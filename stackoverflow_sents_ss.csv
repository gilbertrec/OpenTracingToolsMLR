tool,sentence,neg,pos,neu,com
appdynamics,Is it ok to deploy Performance Monitoring Tool - AppDynamics - in Production?,0.0,0.18,0.82,0.296
appdynamics,The application to be monitored is a standard Java/J2EE Web Application.,0.0,0.0,1.0,0.0
appdynamics,"I have never worked with AppDynamics, and my concern is that it may actually slow down my application.",0.0,0.0,1.0,0.0
appdynamics,Has anyone used AppDynamics in Production?,0.0,0.0,1.0,0.0
appdynamics,Or should it be used only in Test kind of enivornments.,0.0,0.0,1.0,0.0
appdynamics,How do you build a AppDynamic or New Relic kind of system that collects performance metrics of your application including detailed call tree stats by merely installing a software on the servers where your application runs?,0.0,0.0,1.0,0.0
appdynamics,Is it even possible to collect such metrics without compiling your apps with debug information?,0.0,0.0,1.0,0.0
appdynamics,What are the performance trade offs to consider when building such a service?,0.0,0.0,1.0,0.0
appdynamics,How do such software minimize the performance impact they themselves might be having on the application.,0.0,0.0,1.0,0.0
appdynamics,"I'm trying to add Appdynamics into my application, I'm doing those steps:  https://docs.appdynamics.com/display/PRO40/Instrument+an+Android+Application#InstrumentanAndroidApplication-ToaddtheAppDynamicsAndroidagentrepositorytoyourproject  but after all I have error:",0.173,0.0,0.827,-0.5499
appdynamics,This is how my build.gradle (for all project) looks like:,0.0,0.217,0.783,0.3612
appdynamics,and build.gradle (from app module):,0.0,0.0,1.0,0.0
appdynamics,and  adeum-maven-repo  paste into project.,0.0,0.0,1.0,0.0
appdynamics,Any idea what am I doing wrong?,0.383,0.0,0.617,-0.4767
appdynamics,"I followed the instructions on the website of appdynamic, but when I was following this step:",0.0,0.0,1.0,0.0
appdynamics,When I ran standalone.bat this error appears:,0.385,0.0,0.615,-0.481
appdynamics,I have installed Appdynamics lite on my server and it worked fine when I used to run my tomcat instance with root user.,0.0,0.083,0.917,0.2023
appdynamics,"But from the time I have created a new user ""Tomcat"" and start executing my apache tomcat with this user, I am not able to run appdynamics.",0.0,0.098,0.902,0.3612
appdynamics,"I have copied the javaagent at this location with all rights(read,write,execute) to tomcat ""/home/tomcat/profiler/AppServerLite"".",0.0,0.0,1.0,0.0
appdynamics,It throws an exception as follows :,0.0,0.0,1.0,0.0
appdynamics,I wonder what stall means in AppDynamics Pro.,0.231,0.0,0.769,-0.2023
appdynamics,Can you give an example which Appdynamics Pro application name that situation as Stall?,0.122,0.0,0.878,-0.2023
appdynamics,Thanks.,0.0,1.0,0.0,0.4404
appdynamics,"I'm trying to filter on AppDynamics to get all the request to a particular REST URL, the REST URL is not fixed as long as in the URL",0.0,0.0,1.0,0.0
appdynamics,/AppEngine/rest/evac/${id}/createNewActivity,0.0,0.0,1.0,0.0
appdynamics,"On the Transaction Snapshots you have the option to filter results, and in the filters you can filter by URL:",0.0,0.0,1.0,0.0
appdynamics,"If I search for a concrete URL (with ${id} defined) I can search it, but I cannot find how to use a wildcar to find this URL with any ${id}.",0.0,0.0,1.0,0.0
appdynamics,I tried so far to use,0.0,0.0,1.0,0.0
appdynamics,With no results.,0.524,0.0,0.476,-0.296
appdynamics,"The one with works just a bit is using /AppEngine/rest/evac/* which also retrieves other REST which start with the same URL, so I can export the results and filter outside AppDynamics.",0.0,0.0,1.0,0.0
appdynamics,But there is a way to use a wildcard so I can find the desired results in AppDynamics?,0.0,0.159,0.841,0.3919
appdynamics,I am now working on Performance Testing of a Java Application that runs on GlassFish Server 4.1.,0.0,0.0,1.0,0.0
appdynamics,"After going through some statistics that I got from AppDynamics tool, I find that there is no possibility for me to drill down to code/method level issues.",0.084,0.0,0.916,-0.296
appdynamics,"For example, I can see the time taken by each method or function using dotTrace or JProfiler but AppDynamics tool seems to skip all these features.",0.0,0.0,1.0,0.0
appdynamics,"I was also looking for a free solution, hence I choose AppDynamics.",0.0,0.444,0.556,0.6808
appdynamics,Now I feel I am not on the right track.,0.0,0.0,1.0,0.0
appdynamics,Can someone let me know more about this tool if I am missing something or suggest any other quick and easy solution to this.,0.08,0.19,0.73,0.4588
appdynamics,Is there a possibility that the monitors on GlassFish server 4.1 can do the same for no cost?,0.121,0.0,0.879,-0.296
appdynamics,I am trying to a good comparison between AppDynamics and Application Insights in regard to Azure App Service.,0.0,0.162,0.838,0.4404
appdynamics,"I tried to google around but couldn't find any good comparison, if someone can point me or summarize here.",0.155,0.0,0.845,-0.4782
appdynamics,Has anybody solved to monitor Azure functions using AppDynamics ?,0.0,0.208,0.792,0.2732
appdynamics,I don't see any option to add a AppDynamics extension to the Azure functions app.,0.0,0.0,1.0,0.0
appdynamics,For a period of time we might want to have the two analytics together.,0.0,0.098,0.902,0.0772
appdynamics,Could this be problematic?,0.492,0.0,0.508,-0.4404
appdynamics,Would it be degrade the speed?,0.367,0.0,0.633,-0.4404
appdynamics,Would there be any fight between who captures the crash log?,0.371,0.0,0.629,-0.6486
appdynamics,!,0.0,0.0,0.0,0.0
appdynamics,Most popular logging and monitoring stacks like ELK stack or Time series DB-Grafana are designed to be integrated.,0.0,0.259,0.741,0.6801
appdynamics,"Can AppDynamics work with other samplers/DBs, in particular Prometheus?",0.0,0.0,1.0,0.0
appdynamics,I have a problem with my health rule configuration.,0.31,0.0,0.69,-0.4019
appdynamics,All I want is to have health rule which will be checking if service is running or not.,0.0,0.075,0.925,0.0772
appdynamics,I have two types of services:,0.0,0.0,1.0,0.0
appdynamics,IIS,0.0,0.0,1.0,0.0
appdynamics,Standalone services,0.0,0.0,1.0,0.0
appdynamics,The problem is that some services are recognized as critical due to health rule violation.,0.406,0.0,0.594,-0.802
appdynamics,"For example, I have two exactly the same services on two hosts and the only difference is that one of them is in use not so often.",0.0,0.0,1.0,0.0
appdynamics,Due to lack of activity on this service appdynamics pointing me it as critical.,0.277,0.0,0.723,-0.5574
appdynamics,Most probably I have done something wrong.,0.383,0.0,0.617,-0.4767
appdynamics,Any ideas?,0.0,0.0,1.0,0.0
appdynamics,I'm struggling with it as additional task.,0.318,0.0,0.682,-0.4215
appdynamics,Tried appdynamics community website but nothing which could point me solution.,0.0,0.228,0.772,0.4497
appdynamics,Here's my health rule configuration :,0.0,0.0,1.0,0.0
appdynamics,I have a Spring application monitored by Appdynamics.,0.0,0.0,1.0,0.0
appdynamics,"This application has a Service Endpoint recognized by Appdynamics, named lets say:  /general/endpoint",0.0,0.0,1.0,0.0
appdynamics,Now within the application there are multiple endpoints like:,0.0,0.238,0.762,0.3612
appdynamics,Now back to Appdynamics: Within the Service Endpoints menu I can find my  /general/endpoint  and click on it.,0.0,0.0,1.0,0.0
appdynamics,"Then I see a table with the actual REST API calls, their respective execution time, the specific URL (like  /general/endpoint/do-something-1 ) and some more information.",0.0,0.118,0.882,0.4215
appdynamics,If I am interested in monitoring the requests for a specific URL I can do the following:,0.0,0.172,0.828,0.4019
appdynamics,"Now I see what I want, all the requests and their potential problems for a specific URL inside my application.",0.142,0.068,0.789,-0.34
appdynamics,Now comes my actual question:,0.0,0.0,1.0,0.0
appdynamics,How can I achieve the same for Dashboards?,0.0,0.0,1.0,0.0
appdynamics,I was only able to create a dashboard with the  Calls per Minute  for the  /general/endpoint  but not for a specific URL like  /general/endpoint/do-something-1 .,0.0,0.202,0.798,0.5859
appdynamics,Is there a way to apply the more fine grade filter for dashboards as well?,0.0,0.259,0.741,0.4927
appdynamics,I am doing some proof of concept to ingest traces and metrics to AppDynamics without installing Appdynamics agent.,0.0,0.0,1.0,0.0
appdynamics,I have an application emitting prometheus metrics and traces.,0.0,0.0,1.0,0.0
appdynamics,I could not find any Appdynamics documentation talking about opentelemetry Collector.,0.0,0.0,1.0,0.0
appdynamics,I could not find exporter in  https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter  either.,0.0,0.0,1.0,0.0
appdynamics,Can you please advise on how to use opencollector with Appdynamics?,0.0,0.187,0.813,0.3182
appdynamics,"With appdynamics it's easy to setup metrics that ""aggregate"" data from different nodes.",0.0,0.195,0.805,0.4404
appdynamics,"What if you want to aggregate some date, and some other data, show as distinct lines in the graph?",0.0,0.067,0.933,0.0772
appdynamics,Is that possible?,0.0,0.0,1.0,0.0
appdynamics,I am trying to install Appdynamics APM tool.,0.0,0.0,1.0,0.0
appdynamics,It has three components :,0.0,0.0,1.0,0.0
appdynamics,I have few queries:,0.0,0.0,1.0,0.0
appdynamics,How can we capture heap dumps with the help of appdynamics?,0.188,0.188,0.625,0.0
appdynamics,How does AppDynamics and similar problems retrieve data from apps ?,0.231,0.0,0.769,-0.4019
appdynamics,"I read somewhere here on SO that it is based on bytecode injection, but is there some official or reliable source to this information ?",0.0,0.0,1.0,0.0
appdynamics,I am trying to integrate apps performance monitoring tool with my Android Application by my gradle fails saying,0.149,0.0,0.851,-0.4215
appdynamics,Below is my gradle root gradle file,0.0,0.0,1.0,0.0
appdynamics,"And here is my App's gradle file,",0.0,0.0,1.0,0.0
appdynamics,I am already having Multidex flag enabled still it gives me the problem while running the Application.,0.153,0.0,0.847,-0.4019
appdynamics,"And, also I have in my Application class",0.0,0.0,1.0,0.0
appdynamics,I would like to know which existing api in AppDynamics can we use to generate custom reports.,0.0,0.143,0.857,0.3612
appdynamics,"The use case is like, i am consolidating reports from multiple tools so,i would be using the API of app dynamics and doing a backend call to pull the data i need to display and putting it in a csv or excel.All of this will happen in an automated way, would like to know the api or any specific way to do the same in app dynamics.",0.0,0.135,0.865,0.802
appdynamics,How to find TimeStamp difference in App Dynamics Query Language (ADQL)?,0.0,0.174,0.826,0.2732
appdynamics,I tried column1 - column2.,0.0,0.0,1.0,0.0
appdynamics,"Getting error message ""Operator  [SUBTRACTION]  not valid on field types  [DATETIME], [DATETIME] .",0.197,0.0,0.803,-0.4019
appdynamics,I have my java application up and running.,0.0,0.0,1.0,0.0
appdynamics,I want to import that application in Appdynamics for monitoring.,0.0,0.14,0.86,0.0772
appdynamics,Can any one please suggest how to import java applications in appdynamics.,0.0,0.173,0.827,0.3182
appdynamics,"Data is not loading from standalone java applications to agent controller, but only say java agent is connected successfully and waiting for data to load.",0.0,0.152,0.848,0.6486
appdynamics,"In Autodetection and Object tracing dashboard tabs, not able to view the detail or drill down the object tracking instances for standalone Java applications.",0.0,0.0,1.0,0.0
appdynamics,Configured adding some of the fully qualified class names from the application.,0.0,0.0,1.0,0.0
appdynamics,"Please advice, do I need to do any other settings to get the drill down feature?",0.0,0.141,0.859,0.3182
appdynamics,We have cluster of instances whereas each instance has DropWizard metrics gatherer.,0.0,0.0,1.0,0.0
appdynamics,We're also trying to leverage AppDynamics custom metrics and that works so that custom script hits DropWizard exposed endpoint (/metrics) and sends metrics of interest to AppDynamics Controller.,0.043,0.099,0.858,0.4019
appdynamics,AppDynamics has 2 cluster rollout strategies for how the metric is displayed in a whole application view (tier) - SUM and AVG.,0.0,0.0,1.0,0.0
appdynamics,While this works well for stuff like counts (sum is used) and average processing times (avg is used) - we for now don't have any idea of how to aggregate each instance percentiles exposed by DropWizard - neither sum nor avg looks correct.,0.03,0.105,0.866,0.5106
appdynamics,Example:,0.0,0.0,1.0,0.0
appdynamics,sum  will give 1700 what of course isn't useful at all.,0.194,0.0,0.806,-0.3412
appdynamics,avg  will give 600 - which isn't correct either - we're losing track of higher bound.,0.167,0.0,0.833,-0.3818
appdynamics,If AppDynamics had MAX Cluster rollout - that would be more or less fair - still not correct though.,0.0,0.124,0.876,0.3117
appdynamics,But AppDynamics doesn't have that.,0.0,0.0,1.0,0.0
appdynamics,We also understand that the only fully correct way of gathering cluster percentiles is to perform aggregation from all nodes at one place (e.g.,0.0,0.0,1.0,0.0
appdynamics,"logstash, etc..) and not on each instance.",0.0,0.0,1.0,0.0
appdynamics,But for now that's what we have - just sending custom metrics periodically.,0.0,0.0,1.0,0.0
appdynamics,It would be great if anyone suggests something regarding that.,0.0,0.313,0.687,0.6249
appdynamics,"Thanks in advance,",0.0,0.592,0.408,0.4404
appdynamics,"I see that Appdynamics 4.2 claims to  support  Java 8 lambda instrumenting, but this support was  removed  in 4.3.",0.0,0.265,0.735,0.6597
appdynamics,I cannot find anything in  4.3 release notes  that mentions removing support for lambdas.,0.0,0.184,0.816,0.4019
appdynamics,What's happened?,0.0,0.0,1.0,0.0
appdynamics,Is it somehow related to  JDK-8145964 ?,0.0,0.0,1.0,0.0
appdynamics,Getting below error during platform installation:,0.351,0.0,0.649,-0.4019
appdynamics,"""Required libaio package is not found.",0.0,0.0,1.0,0.0
appdynamics,"...""",0.0,0.0,1.0,0.0
appdynamics,"However, above package is already installed:",0.0,0.0,1.0,0.0
appdynamics,Here is output from the installation script:,0.0,0.0,1.0,0.0
appdynamics,I have read about the Appdynamics in Kubernetes but I got confused.,0.247,0.0,0.753,-0.4497
appdynamics,"The scenario is like I am having EC2 under which Kubernetes is running which is having POD and under 1 pod, multiple container is running.",0.0,0.102,0.898,0.3612
appdynamics,Where I have to install machine-agent?,0.0,0.0,1.0,0.0
appdynamics,In EC2 or in daemon set?,0.0,0.0,1.0,0.0
appdynamics,And where I have to install app-agent?,0.0,0.0,1.0,0.0
appdynamics,do I have to add app-agent in each container Dockerfile?,0.0,0.0,1.0,0.0
appdynamics,"And lastly, what would be my hostName and uniqueHostId?",0.0,0.0,1.0,0.0
appdynamics,I explore AppDynamics and other APM solutions to choose right one for my company.,0.0,0.124,0.876,0.1779
appdynamics,I have created simple demo .NET application (WCF service and console client to consume it).,0.0,0.133,0.867,0.25
appdynamics,Then I installed AppDynamics agent on machine and configure it for both client and service as for standalone applications:,0.0,0.0,1.0,0.0
appdynamics,"When I start my client and service I see that AppD agent have ""injected"" code to my applications and write ""Running non-obfuscated client"" to the console",0.0,0.0,1.0,0.0
appdynamics,I want to understand what technics or methods AppDynamics agent use to instrument .NET applications without SDK and being just a separate process (service)?,0.0,0.058,0.942,0.0772
appdynamics,"How does it listen for incoming WCF calls of my service without being directly used by the service (it's not referenced as an assembly, even not mentioned in app.config)?",0.0,0.0,1.0,0.0
appdynamics,Has anyone experienced Webpack dependency compiling issues when using the AppDynamics library?,0.0,0.0,1.0,0.0
appdynamics,And did you find a way to work around it?,0.0,0.0,1.0,0.0
appdynamics,I believe this is an issue stemming from their library.,0.0,0.0,1.0,0.0
appdynamics,"When trying to install the AppDynamics package for monitoring a Node.js/Express application, our Webpack build process is not able to import a handful of dependencies.",0.0,0.0,1.0,0.0
appdynamics,"Specifically, the errors output are:",0.375,0.0,0.625,-0.34
appdynamics,"Our project is set up with:
- Webpack v4.29.0
- Node.js v11.0.0
- Appdynamics v4.5",0.0,0.0,1.0,0.0
appdynamics,The Appdynamics usage is at the top of our server file as:,0.0,0.141,0.859,0.2023
appdynamics,And our Webpack configuration is:,0.0,0.0,1.0,0.0
appdynamics,"So far we have tried downgrading the Webpack version, downgrading the Node environment to 10.15, and using other import methods for the AppDynamics package, but this seems like an issue internal to the Appdynamics library?",0.0,0.087,0.913,0.5023
appdynamics,"The main question is, has anyone experienced Webpack dependency compiling issues when using the Appdynamics library?",0.0,0.0,1.0,0.0
appdynamics,And did you find a way to work around it?,0.0,0.0,1.0,0.0
appdynamics,Any help or clues would be appreciated ❤️,0.0,0.5,0.5,0.7184
appdynamics,I am trying to setup the rabbitmq machine agent for AppDynamics with a standalone RabbitMQ.,0.0,0.0,1.0,0.0
appdynamics,https://www.appdynamics.com/community/exchange/extension/rabbitmq-monitoring-extension/,0.0,0.0,1.0,0.0
appdynamics,itMQ Monitoring Plugin 2.0.2,0.0,0.0,1.0,0.0
appdynamics,A curl on the RabbitMQ API works fine,0.0,0.231,0.769,0.2023
appdynamics,Celery Flower is talking to rabbit fine with the following config options,0.0,0.141,0.859,0.2023
appdynamics,My rabbitMQ Monitoring plugin is configured like so in config.yml,0.0,0.217,0.783,0.3612
appdynamics,"In my troubleshooting, I followed this guide to add /opt/ca/cacert.pem to a Java keystore.",0.0,0.134,0.866,0.1779
appdynamics,https://github.com/MichalHecko/SSLPoke,0.0,0.0,1.0,0.0
appdynamics,I initialize the machine agent as follows,0.0,0.0,1.0,0.0
appdynamics,I am still getting the following error for every RabbitMQ api call by the monitor in machineagent-bundle-64bit-linux-4.5.14.2293/logs/machine-agent.log,0.153,0.0,0.847,-0.4019
appdynamics,What am I missing?,0.524,0.0,0.476,-0.296
appdynamics,Thank you!,0.0,0.736,0.264,0.4199
appdynamics,We are instrumenting java agents on Tibco.,0.0,0.0,1.0,0.0
appdynamics,There are few JVMs ob each server and we are trying to configure unique node name (since which ever JVM starts first we get data only for that JVM).,0.0,0.0,1.0,0.0
appdynamics,"We tried to add the following in the startup command in the tra file:
 -Dappdynamics.agent.tierName=%tibco.deployment% -Dappdynamics.agent.nodeName=$HOSTNAME.%tibco.deployment%",0.0,0.0,1.0,0.0
appdynamics,which didn't work since $HOSTNAME was not translated.,0.0,0.0,1.0,0.0
appdynamics,We need to define this dynamically since at every tibco deployment the configuration is lost and if we indicate a specific node name we have to reconfiure every tra separately.,0.071,0.089,0.839,0.1469
appdynamics,How can we get the hostname dynamically into the tra file so we won't have to redefine every JVM (and we have many) after every deployment ?,0.0,0.091,0.909,0.3612
appdynamics,"Regards,
Yy",0.0,0.0,1.0,0.0
appdynamics,Our existing Infra is hosted on private servers.,0.0,0.0,1.0,0.0
appdynamics,AppDynamics  is used for monitoring hundreds of application &amp; host performances.,0.0,0.0,1.0,0.0
appdynamics,As a move we are moving all our applications on Azure.,0.0,0.0,1.0,0.0
appdynamics,Is this possible to get away from AppDynamics &amp; use any Azure solutions  for the same purpose.,0.0,0.096,0.904,0.1779
appdynamics,Possibly Azure App Insight/Monitor ?,0.0,0.0,1.0,0.0
appdynamics,?,0.0,0.0,0.0,0.0
appdynamics,We have tried Java Application Monitoring on  Azure App Insight ; Azure Monitor is useful there.,0.0,0.172,0.828,0.4404
appdynamics,Also we have used LogAnalytics for creating various performance Dashboards on Azure Monitor.,0.0,0.155,0.845,0.296
appdynamics,Can Application Insight support all the similar features of AppDynamics: Like Workflow Monitoring Performance Monitoring etc etc..,0.0,0.257,0.743,0.6369
appdynamics,I have react app using webpack.,0.0,0.0,1.0,0.0
appdynamics,Building a docker image out of it is failing.,0.32,0.0,0.68,-0.5106
appdynamics,It is failing because appdynamics package.,0.398,0.0,0.602,-0.5106
appdynamics,I am getting the error only during docker build and npm run build seems to work fine without any issues.,0.126,0.084,0.791,-0.2263
appdynamics,"Environment:
React 16
Node 13
appdynamics var 4.5.20
webpack 4
Docker",0.0,0.0,1.0,0.0
appdynamics,package.json,0.0,0.0,1.0,0.0
appdynamics,webpack.config.js,0.0,0.0,1.0,0.0
appdynamics,ERROR,1.0,0.0,0.0,-0.4019
appdynamics,"Im trying to monitor a page availability with Appdynamics 
I have an IIS server with one site and several applications.",0.0,0.0,1.0,0.0
appdynamics,Appdynamics .Net agent 20.4.1 installed on the monitored server,0.0,0.0,1.0,0.0
appdynamics,each application has a appName.svc web page that I can call to check if the service is up.,0.0,0.0,1.0,0.0
appdynamics,Im trying  AppDynamics Extension for URL Monitoring  and followed the installation instructions.,0.0,0.0,1.0,0.0
appdynamics,"I can see in  Metric browser  the URL monitor section, under that I see 'Metric Uploaded'.",0.0,0.0,1.0,0.0
appdynamics,where do I see indication that a URl is down/up?,0.0,0.0,1.0,0.0
appdynamics,"can I monitor multiple URLs, as i did in yml file?",0.0,0.0,1.0,0.0
appdynamics,config.yml  file section looks like this:,0.0,0.333,0.667,0.3612
appdynamics,log:,0.0,0.0,1.0,0.0
appdynamics,"With the Analyze -  Metric browser in AppDynamics, you can go in and look at all the various metrics in the tree, but there's no right-click ""edit"" option.",0.097,0.0,0.903,-0.4215
appdynamics,How do you modify/delete a metric?,0.0,0.0,1.0,0.0
appdynamics,HI I'm wondering if anyone has any thoughts on using  Appdynamics  to monitor WSO2.,0.0,0.0,1.0,0.0
appdynamics,"Out of the box appdynamics detects the servlet request coming in and that it gets written to the database, but beyond that it loses track of the transaction.",0.098,0.0,0.902,-0.4497
appdynamics,"so if anyone could give some help as to what other classes I should instrument, It would be a real help.",0.0,0.241,0.759,0.6597
appdynamics,thanks,0.0,1.0,0.0,0.4404
appdynamics,Sunil Vanmullem,0.0,0.0,1.0,0.0
appdynamics,Normally newer verisons of Appdynamics should display windows services if you add them specificially into the config.xml.,0.0,0.0,1.0,0.0
appdynamics,"I did that, restartet the services and the agent, but nothing happened.",0.0,0.0,1.0,0.0
appdynamics,Did anyone manage to display the Services ?,0.0,0.0,1.0,0.0
appdynamics,"If yes, where do they appear?",0.0,0.351,0.649,0.4019
appdynamics,I develop common java web app.,0.0,0.0,1.0,0.0
appdynamics,"I use  Hystrix  in my app, actually I have a  REST client  whose methods wrapped in  hystrix  commands.",0.0,0.0,1.0,0.0
appdynamics,My web app uses this rest client to communicate with remote server.,0.0,0.0,1.0,0.0
appdynamics,My web app is configured as described in  hystrix   wiki  (it's needed to calculate statistics for  hystrix  dashboard).,0.0,0.0,1.0,0.0
appdynamics,"To monitor my web app I use  AppDynamics  tool, but after I started using  rest client  based on  Hystrix  all calls from my web app aren't being displayed in  AppDynamics .",0.0,0.0,1.0,0.0
appdynamics,When I switched implementation to client without Hystrix everything is working well as expected.,0.0,0.149,0.851,0.2732
appdynamics,Maybe somebody knows what is the problem?,0.31,0.0,0.69,-0.4019
appdynamics,Thanks.,0.0,1.0,0.0,0.4404
appdynamics,I want to analyse my Liferay Server with AppDynamics.,0.0,0.157,0.843,0.0772
appdynamics,Is there special configuration to better analyse Liferay specific things or is the only way to check the JSP related execution and not the internal service calls from the JSPs?,0.0,0.167,0.833,0.6808
appdynamics,System: Liferay 6.2 on Tomcat with a MySQL Database,0.0,0.0,1.0,0.0
appdynamics,I am trying to find suitable entry point for ES client.,0.0,0.0,1.0,0.0
appdynamics,At the moment I have:,0.0,0.0,1.0,0.0
appdynamics,Class that implements an Interface which equals org.elasticsearch.client.ElasticsearchClient,0.0,0.0,1.0,0.0
appdynamics,and method name: prepareSearch,0.0,0.0,1.0,0.0
appdynamics,It seems to collect number of calls but I wonder if there is a better configuration to make ES calls to show up in Tier Flow Map.,0.0,0.179,0.821,0.6124
appdynamics,"When having the AppDynamics performance monitor installed, the servicestack API fails to load with the following exception:",0.149,0.0,0.851,-0.4215
appdynamics,"Could not load type 'd__38' from assembly '###, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null'.",0.0,0.0,1.0,0.0
appdynamics,"StackTrace:
   at ###.BaseService 1.&lt;Any&gt;d__38.MoveNext() in ###\Services\BaseService.cs:line 190
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder 1.Start[TStateMachine](TStateMachine&amp; stateMachine)
   at ###.BaseService 1.Any(T request)
   at ServiceStack.Host.ServiceRunner 1.Execute(IRequest request, Object instance, TRequest requestDto)",0.0,0.0,1.0,0.0
appdynamics,Any help is greatly appreciated.,0.0,0.677,0.323,0.7425
appdynamics,Thank you,0.0,0.714,0.286,0.3612
appdynamics,I want to execute the below command through Java code.,0.0,0.14,0.86,0.0772
appdynamics,This is to create connection to my Appdynamics Contoller,0.0,0.208,0.792,0.2732
appdynamics,curl --user user@customer1:password ' http://192.168.1.11:9090/controller/rest/applications?output=JSON ',0.0,0.0,1.0,0.0
appdynamics,The java code that I have written for this is,0.0,0.0,1.0,0.0
appdynamics,I keep on getting below error,0.403,0.0,0.597,-0.4019
appdynamics,"HTTP/1.1 401 Unauthorized
Error report  HTTP Status 401 -    type  Status report",0.213,0.0,0.787,-0.4019
appdynamics,message,0.0,0.0,1.0,0.0
appdynamics,description This request requires HTTP authentication ().,0.0,0.0,1.0,0.0
appdynamics,Can anyone please help.,0.0,0.714,0.286,0.6124
appdynamics,I Have tried to find if Appdynamics provides any API but I could not find any.,0.0,0.0,1.0,0.0
appdynamics,"So I want to know if there are any APIs available for creating users and project/job in Appdynamics, so that I can automate the process using Python scripts.",0.0,0.136,0.864,0.4201
appdynamics,We are in the process of building a new server infrastructure and will be using Appdynamics for analytics of the Java applications.,0.0,0.0,1.0,0.0
appdynamics,"Appdynamics has a lot of features, so it seems that server metrics via collectd to Graphite will no longer be necessary.",0.104,0.0,0.896,-0.296
appdynamics,Application metrics can also be fed straight into Appdynamics.,0.0,0.192,0.808,0.2263
appdynamics,"How about Logstash, ElasticSearch and Kibana and centralised logging.",0.0,0.0,1.0,0.0
appdynamics,Is there still any reason to build an ELK stack for the Java developers when they can use Appdynamics?,0.0,0.0,1.0,0.0
appdynamics,I have installed AppDynamics's Java Machine Agent along with the URL Monitoring Extension.,0.0,0.0,1.0,0.0
appdynamics,"Every day, for 1 or 2 hours, the metrics are not appearing on my metric browser.",0.0,0.0,1.0,0.0
appdynamics,"I checked the logs corresponding to those time periods, and I see that the HTTP Requests are being made and are getting back HTTP 200 OK responses.",0.0,0.109,0.891,0.4466
appdynamics,"My assumption is that the extension is not sending over the metrics, but I am unable to understand the cause of it.",0.0,0.0,1.0,0.0
appdynamics,Can anyone point me into the right direction?,0.0,0.0,1.0,0.0
appdynamics,I was not able to find any information regarding configuration of  AppDynamics  agent for  JUnit  tests.,0.0,0.0,1.0,0.0
appdynamics,I would like to test performance of Hibernate queries of  Spring  based web service backed by  PostgreSQL  database.,0.0,0.194,0.806,0.3818
appdynamics,Tests must be able to rollback the data at the termination.,0.0,0.0,1.0,0.0
appdynamics,Should it be unit or integration tests?,0.0,0.0,1.0,0.0
appdynamics,What is the best way to accomplish it?,0.0,0.538,0.462,0.7906
appdynamics,How to make  AppDynamics  collect and display graphs of query execution times?,0.0,0.0,1.0,0.0
appdynamics,UPDATE:,0.0,0.0,1.0,0.0
appdynamics,I was not able to set up addDynamics agent for JUnit tests inside IDEA.,0.0,0.0,1.0,0.0
appdynamics,"The VM arguments is pointing to agent  -javaagent:""C:\Tools\AppDynamicsAgent\javaagent.jar"" , the firewall is off but for some reason in appdynamics web based (SaaS) set up dialog shows that no agent able to connect:",0.138,0.0,0.862,-0.5647
appdynamics,"I'm using AppD as APM for my application and in slow transaction reports it shows most of the calls, which is not our application code and we are calling open source libraries method.",0.0,0.0,1.0,0.0
appdynamics,For example :,0.0,0.0,1.0,0.0
appdynamics,com.google.common.reflect.TypeVisitor.visit  method of google library takes almost 155 ms time and  com.google.common.reflect.TypeToken.equals()  method takes almost 60 ms. and  org.apache.tapestry5.internal.services.RenderQueueImpl.render()  takes almost 50 ms.,0.0,0.0,1.0,0.0
appdynamics,I want to highlight that  I've checked and my server is not loaded and both CPU and memory usage is very low as well this time taken is for very small amount of data processing .,0.062,0.158,0.78,0.3961
appdynamics,Let me know the reason behind this and how can I optimize the performance of my application.,0.0,0.176,0.824,0.4939
appdynamics,I have an application that is generating 3 kind of log files,0.0,0.0,1.0,0.0
appdynamics,"and I want to analyse the performance of my server using appdynamics so what kind of data my logs should be generating to generate analytics for server health, performance, throughput, server utilization?",0.0,0.042,0.958,0.0772
appdynamics,JMX used for monitoring and managing the services/components &amp; devices.,0.0,0.0,1.0,0.0
appdynamics,"My question is about monitoring,for the monitoring purpose do we have to change any code if we use JMX.",0.0,0.0,1.0,0.0
appdynamics,"If that is the case, App dynamics will solve this process without doing single line of code change ?",0.0,0.196,0.804,0.4404
appdynamics,"I haven't found anywhere an answer, so I decided to write here.",0.0,0.0,1.0,0.0
appdynamics,Is it possible to display AppDynamics Dashboards on the TV display?,0.0,0.0,1.0,0.0
appdynamics,"Currently I'm using something like  GRUNT  (gruntjs.com), but nowhere can I find whether it is feasible with that?",0.0,0.099,0.901,0.1901
appdynamics,"Currently I'm using GRUNT for displaying tasks from Jenkins, but I don't know how to configure it with AppDynamics.",0.0,0.0,1.0,0.0
appdynamics,"Regards,
Kamil",0.0,0.0,1.0,0.0
appdynamics,I am trying to setup the AppDynamics java agent.,0.0,0.0,1.0,0.0
appdynamics,I am facing issues in loading java agent in the JVM.,0.0,0.0,1.0,0.0
appdynamics,I try to add the below argument to the start.bat jvm options.,0.2,0.0,0.8,-0.3612
appdynamics,-javaagent:C:\javaagent.jar,0.0,0.0,1.0,0.0
appdynamics,"However, the aem do not start after this.",0.0,0.0,1.0,0.0
appdynamics,I have kept AppMachineAgent folder in the same drive as the AEM installation.,0.0,0.0,1.0,0.0
appdynamics,"However, javaagent.jar is not kept in the bin folder of the AEM.",0.0,0.0,1.0,0.0
appdynamics,Do I need to keep it in the bin folder?,0.0,0.0,1.0,0.0
appdynamics,Any suggested steps I am missing?,0.355,0.0,0.645,-0.296
appdynamics,I am having multiple java application configured in my app-dynamics controller and they have their own java agent running and reporting the metrics.,0.0,0.0,1.0,0.0
appdynamics,My problem is that SLA for each application is different and if i change the slow transaction threshold for a single application.,0.124,0.0,0.876,-0.4019
appdynamics,"it changes it for other application as well, which is creating lot of trouble for me.",0.135,0.215,0.65,0.1531
appdynamics,So my question is how to configure separate transaction threshold for each application in AppDynamics controller  ?,0.0,0.0,1.0,0.0
appdynamics,I am able to authenticate and fetch details using local user account using the python SDK of App Dynamics is there a way to authenticate using AD from python API or using the REST/curl.,0.0,0.063,0.937,0.2732
appdynamics,Is it possible to create a stacked area graph in AppDynamics?,0.0,0.189,0.811,0.2732
appdynamics,I want to show the cumulative effects of API response and browser DOM ready to visualize where variance is originating.,0.0,0.183,0.817,0.4215
appdynamics,"I can put both of these on a single graph, but if I choose Area, they overlap.",0.0,0.0,1.0,0.0
appdynamics,How do I get them to stack?,0.0,0.0,1.0,0.0
appdynamics,"I'm on AppDynamics Version 4.3.1.2, build 47",0.0,0.0,1.0,0.0
appdynamics,"I just testing AppDynamics for my database, I am able get it work on MySQL 5 and SQL Server 2014, but I got a JDBC error on SQL Server 2005.",0.129,0.0,0.871,-0.5499
appdynamics,Here is the error log:,0.403,0.0,0.597,-0.4019
appdynamics,"06 6月 2017 00:55:59,461 ERROR [AD Thread Pool-Global0] DBAgentPollingForUpdate:30 - Fatal transport error while connecting to URL [/controller/instance/DBAGENT_MACHINE_ID/db-monit
  or-config/37784]: org.apache.http.NoHttpResponseException: davinci2017060100542331.saas.appdynamics.com:443 failed to respond
  06 6月 2017 00:55:59,473  WARN [AD Thread Pool-Global0] DBAgentPollingForUpdate:62 - Invalid response for configuration request from controller/could not connect.",0.295,0.0,0.705,-0.9333
appdynamics,"Msg: Fatal transp
  ort error while connecting to URL [/controller/instance/DBAGENT_MACHINE_ID/db-monitor-config/37784]
  06 6月 2017 00:56:00,026  INFO [-Scheduler-3] ADBCollector:141 - DB Collector DBSERVER01 is temporarily disabled.",0.228,0.0,0.772,-0.7351
appdynamics,"06 6月 2017 00:56:01,026  INFO [-Scheduler-3] ARelationalDBCollector:59 - (Re)initialize the DB collector 'DBSERVER01'.",0.0,0.0,1.0,0.0
appdynamics,"06 6月 2017 00:56:01,040  INFO [-Scheduler-3] MSSqlCollector:74 - Obtained connection for url jdbc:sqlserver://192.168.1.100:1433
  06 6月 2017 00:56:01,047  INFO [-Scheduler-3] MSSqlCollector:139 - SQL Server Version = 9.00.5057.00 ( 2005.0 )
  06 6月 2017 00:57:00,025 ERROR [-Scheduler-1] ADBCollector:172 -  Error collecting data for database 'DBSERVER01'
  com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near 'sys'.",0.133,0.0,0.867,-0.7297
appdynamics,"at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:216)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1515)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:404)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:350)
          at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:5696)
          at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1715)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:180)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:155)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:285)
          at com.singularity.ee.agent.dbagent.collector.db.relational.mssql.AMSSqlCollectorDelegate.collectDBMSMetrics(AMSSqlCollectorDelegate.java:335)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollectorDelegate.collectPerMinute(ADBCollectorDelegate.java:88)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollector.collect(ADBCollector.java:156)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollector.run(ADBCollector.java:139)
          at com.singularity.ee.util.javaspecific.scheduler.AgentScheduledExecutorServiceImpl$SafeRunnable.run(AgentScheduledExecutorServiceImpl.java:122)
          at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
          at com.singularity.ee.util.javaspecific.scheduler.ADFutureTask$Sync.innerRunAndReset(ADFutureTask.java:335)
          at com.singularity.ee.util.javaspecific.scheduler.ADFutureTask.runAndReset(ADFutureTask.java:152)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.access$101(ADScheduledThreadPoolExecutor.java:119)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.runPeriodic(ADScheduledThreadPoolExecutor.java:206)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.run(ADScheduledThreadPoolExecutor.java:236)
          at com.singularity.ee.util.javaspecific.scheduler.ADThreadPoolExecutor$Worker.runTask(ADThreadPoolExecutor.java:694)
          at com.singularity.ee.util.javaspecific.scheduler.ADThreadPoolExecutor$Worker.run(ADThreadPoolExecutor.java:726)
          at java.lang.Thread.run(Unknown Source)",0.0,0.0,1.0,0.0
appdynamics,I need to fetch the transaction scorecard data of a business application using the REST API of AppDynamics .,0.0,0.0,1.0,0.0
appdynamics,Following is the a sample view of AppDynamics Transaction Scorecard,0.0,0.0,1.0,0.0
appdynamics,"I have done through the AppDynamics REST API documentation to some extent,but not found anything so far .",0.0,0.0,1.0,0.0
appdynamics,Can anybody have any idea on this ?,0.0,0.0,1.0,0.0
appdynamics,I tried to call AppDynamics API using python requests but face an issue.,0.0,0.0,1.0,0.0
appdynamics,I wrote a sample code using the python client as follows...,0.0,0.0,1.0,0.0
appdynamics,It works fine.,0.0,0.474,0.526,0.2023
appdynamics,But if I do a simple call like the following,0.0,0.317,0.683,0.5023
appdynamics,I get the following response:,0.0,0.0,1.0,0.0
appdynamics,Am I doing anything wrong here ?,0.437,0.0,0.563,-0.4767
appdynamics,When I am invoking a REST URI from the browser using an URL like the following,0.0,0.161,0.839,0.3612
appdynamics,http://:/controller/rest/applications//business-transactions?output=JSON,0.0,0.0,1.0,0.0
appdynamics,and this is providing the output as,0.0,0.0,1.0,0.0
appdynamics,"This output is missing the indicator/field for severity information like WARNING,CRITICAL,NORMAL etc.",0.15,0.17,0.68,0.0772
appdynamics,How to get the severity information from the AppDynamics REST call ?,0.0,0.0,1.0,0.0
appdynamics,We have implemented AppDynamics in Jboss application.,0.0,0.0,1.0,0.0
appdynamics,We have load balancer and autoscalling which means we will have node registration when new server comes up.,0.0,0.0,1.0,0.0
appdynamics,The problem here is Java and Machine Agent.,0.278,0.0,0.722,-0.4019
appdynamics,"Java Agent can reuse name with prefix (Appd Controlled Node Names) , but machine agent node name need to be provided at configuration level.",0.0,0.0,1.0,0.0
appdynamics,We are getting two separate agents listed.,0.0,0.0,1.0,0.0
appdynamics,One is 100% with Machine Agent and another with Java agent.,0.0,0.0,1.0,0.0
appdynamics,We need Machine Agent will ping at same line.,0.0,0.0,1.0,0.0
appdynamics,https://i.stack.imgur.com/MhpeU.png,0.0,0.0,1.0,0.0
appdynamics,"What is the meaning and use of environment variables in the Python Agent configuration for App Dynamics, as documented here:",0.0,0.1,0.9,0.2732
appdynamics,https://docs.appdynamics.com/display/PRO42/Python+Agent+Settings,0.0,0.0,1.0,0.0
appdynamics,More specifically:,0.0,0.0,1.0,0.0
appdynamics,"If a value is set in the file and the corresponding environment variable is also set, which one takes precedence?",0.0,0.118,0.882,0.34
appdynamics,"If I want to use environment variables for some of these values, can they be omitted from the file?",0.0,0.2,0.8,0.4588
appdynamics,Can AppDynamics show the request or response being exchanged between different microservices systems.,0.0,0.0,1.0,0.0
appdynamics,"They show the call trace, but couldnt find the details of what is passing between the calls.",0.0,0.0,1.0,0.0
appdynamics,What are the differences in features between AppDynamics and Zipkin apart from the pricing since zipkin is opensource.,0.0,0.0,1.0,0.0
appdynamics,"Can any of them show request or response, in their console?",0.0,0.0,1.0,0.0
appdynamics,Is this comparison even valid?,0.0,0.0,1.0,0.0
appdynamics,Appdynamics does a lot of other things beyond crash logs.,0.252,0.0,0.748,-0.4019
appdynamics,So using Crittercism for crashlogs is a good idea or bad.,0.243,0.201,0.556,-0.1531
appdynamics,Consider below code:,0.0,0.0,1.0,0.0
appdynamics,"I could use AppDynamics ""Java POJO"" rule to create a business transaction to track all the calls to Job.process() method.",0.0,0.11,0.89,0.2732
appdynamics,But the measured response time didn't reflect real cost by the async thread started by java.util.concurrent.ExecutorService.,0.0,0.0,1.0,0.0
appdynamics,This exact problem is also described in AppDynamics document:  End-to-End Latency Performance  that:,0.184,0.0,0.816,-0.4019
appdynamics,"The return of control stops the clock on the transaction in terms of measuring response time, but meanwhile the logical processing for the transaction continues.",0.051,0.0,0.949,-0.0772
appdynamics,The same AppDynamics document tries to give a solution to address this issue but the instructions it provides is not very clear to me.,0.12,0.064,0.815,-0.3509
appdynamics,Could anyone give more executable guide on how to configure AppD to track async calls like the one shown above?,0.0,0.116,0.884,0.3612
appdynamics,We are in the process of configuring AppDynamics for one of our applications.,0.0,0.0,1.0,0.0
appdynamics,"Since there are many instances of the application, we want to add nodename with agentId and hostName so as to identify the different instances.",0.0,0.053,0.947,0.0772
appdynamics,"Below is how we are trying to do, but it does not seems to work:",0.0,0.0,1.0,0.0
appdynamics,Once I start the JVM the node name appears as CalculationEngine_null_null.,0.0,0.0,1.0,0.0
appdynamics,I was hoping nodename to come CalculationEngine_3_a301-564.com where 3 being the agentid and a301-564 being the host name.,0.0,0.157,0.843,0.4215
appdynamics,Also even if the host name parameter is not correct at least it should show CalculationEngine_3_null,0.0,0.0,1.0,0.0
appdynamics,What could be wrong here?,0.437,0.0,0.563,-0.4767
appdynamics,Or is it not possible?,0.0,0.0,1.0,0.0
appdynamics,I am using the ActiveMQ extension of AppDynamics.,0.0,0.0,1.0,0.0
appdynamics,It is good to start.,0.0,0.42,0.58,0.4404
appdynamics,With JMXRemote(enabled in artemis.profile) it is OK.,0.0,0.328,0.672,0.4466
appdynamics,"But, I want it from localhost.",0.0,0.266,0.734,0.1154
appdynamics,JMX is enabled by default for localhost for AMQ.,0.0,0.0,1.0,0.0
appdynamics,AMQ management console use jmx internally and it works without JMXRemote enabled.,0.0,0.0,1.0,0.0
appdynamics,What service URL jolokia use internally to connect using JMX from localhost?,0.0,0.0,1.0,0.0
appdynamics,I have tryed with following URL:,0.0,0.0,1.0,0.0
appdynamics,"serviceUrl: ""service:jmx:rmi:///jndi/rmi://:1099/jmxrmi""",0.0,0.0,1.0,0.0
appdynamics,I'm trying to determine if AppDyanmics support Opentracing.,0.0,0.278,0.722,0.4019
appdynamics,I've looked in the app dynamics site and stack overflow but can't find a clear answer.,0.16,0.089,0.75,-0.3018
appdynamics,"Thanks,
Carlos",0.0,0.744,0.256,0.4404
appdynamics,We have a JBoss fuse ESB instance Running Version 6.3.0.,0.0,0.0,1.0,0.0
appdynamics,"With this, Installed as a Service using tanuki wrapper.",0.0,0.0,1.0,0.0
appdynamics,When passing the javaagent Argument The Application (hawtio)Breaks.,0.263,0.0,0.737,-0.3612
appdynamics,we have tried passing the argument in the following files:,0.217,0.0,0.783,-0.3612
appdynamics,JBOSSHOME/bin/karaf/,0.0,0.0,1.0,0.0
appdynamics,JBOSSHOME/etc/jboss-fuse-wrapper.conf,0.0,0.0,1.0,0.0
appdynamics,-javaagent:/agenthome/javaagent.jar,0.0,0.0,1.0,0.0
appdynamics,"The JVM Loads the argument when it is passing in the wrapper.conf, but as mentioned before the application is not working when the argument is loaded in the JVM.",0.156,0.0,0.844,-0.6124
appdynamics,have anyone instrumented JBoss Fuse ESB with app dynamics before?,0.0,0.189,0.811,0.2732
appdynamics,"I implement AppDynamics in my iOS app, I put everything in doc 
 like initWithKey licence key, but if I write initWithKey in appDelegate my all service request return nil.",0.0,0.065,0.935,0.1901
appdynamics,Can anyone help me Thank you.,0.0,0.565,0.435,0.6369
appdynamics,How to copy an existing dashboard to the new project in appdynamics.,0.0,0.0,1.0,0.0
appdynamics,"How does AppDynamics works internally, In my current company we are planning to use AppDynamics but teams want to know how it actually works such as How it collects data, how does it communicate and how it intercepts java transactions and other related stuff.",0.0,0.033,0.967,0.1154
appdynamics,So I tried looking into AppDynamics knowledge base but did not get accurate technical answer I need.,0.0,0.0,1.0,0.0
appdynamics,not new to coding but I have been working in python before and shaking the rust off my Powershell and working with xml.,0.089,0.0,0.911,-0.2617
appdynamics,We are trying to automate the deployment of AppDynamics's .net agent and we have the deployment piece down as well as upgrade.,0.0,0.091,0.909,0.2732
appdynamics,Now I am trying to include specific applications and tiers running on IIS.,0.0,0.0,1.0,0.0
appdynamics,So again shaking off rust here but I am trying the find a simple script to update the config.xml to add the application and update the tier from IIS.,0.054,0.0,0.946,-0.1253
appdynamics,It's not a complex xml file but any help or direction would be helpful,0.0,0.397,0.603,0.8047
appdynamics,Just trying to find the right syntax to update the nodes properly and arguments,0.172,0.0,0.828,-0.4019
appdynamics,"
 
 #what I need updated
&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;appdynamics-agent xmlns:xsd=""http://stuff.com"" xmlns:xsi=""http://stuff.com""&gt;
  &lt;controller host=""test.saas.appdynamics.com"" port=""123"" ssl=""true"" enable_tls12=""true""&gt;
  &lt;applications&gt; #need to create this
    &lt;application default=""true"" name=""app1"" /&gt; #need to add the default='true' if there is more than one app
    &lt;application name=""App2""/&gt; #this 
  &lt;/applications&gt; #this
    &lt;account name=""testacct"" password=""123456"" /&gt;
  &lt;/controller&gt;
  &lt;machine-agent /&gt;
  &lt;app-agents&gt;
    &lt;IIS&gt;
      &lt;applications&gt; #there by default with the default settings 
        &lt;application controller-application=""app1"" path=""/app1path"" site=""WebSite1""&gt; # need to add this
          &lt;tier name=""app1-1"" /&gt; #and this
        &lt;/application&gt; #this too
        &lt;application controller-application=""app2"" path=""/app2path"" site=""WebSite2""&gt; #some more
          &lt;tier name=""app2-2"" /&gt; #you guessed it
        &lt;/application&gt; #this as well
      &lt;/applications&gt; #ends with this
    &lt;/IIS&gt;
  &lt;/app-agents&gt;
&lt;/appdynamics-agent&gt;",0.0,0.046,0.954,0.5514
appdynamics,We have a spring boot app running in pivotal cloud foundry and we have also configured appdynamics and we can now see our app on appd controller GUI.,0.0,0.0,1.0,0.0
appdynamics,"I would like to build a appd dashboard and call actuator end points like info , health.",0.0,0.294,0.706,0.6124
appdynamics,How do i do this?,0.0,0.0,1.0,0.0
appdynamics,"Or I am also open to other ideas on building appd dashboard for micro services
Please advise",0.0,0.133,0.867,0.3182
appdynamics,"I want to load ""application flow map"" data (that can be seen on the web UI dashboard) from AppDynamics APIs.",0.0,0.067,0.933,0.0772
appdynamics,My goal is to upload the data in Neo4j so we can study our microservices architecture using graph algorithms.,0.0,0.0,1.0,0.0
appdynamics,The AppDynamics Application Model API doesn't seem to provide data up to this level.,0.0,0.0,1.0,0.0
appdynamics,"I'll build a client later but for now I use curl with requests like:
curl --user MyUserName:MyPassword  https://hostname/controller/rest/applications/OurApp/tiers  and variations of this according to the documentation",0.0,0.124,0.876,0.5023
appdynamics,https://docs.appdynamics.com/display/PRO45/Application+Model+API,0.0,0.0,1.0,0.0
appdynamics,Only curl requests for now.,0.0,0.0,1.0,0.0
appdynamics,See point 2.,0.0,0.0,1.0,0.0
appdynamics,I expect a JSON output all the tiers of OurApp with interactions between them.,0.0,0.0,1.0,0.0
appdynamics,I am facing  Magic v1 does not support record headers  while consuming the messages form Kafka.,0.139,0.0,0.861,-0.3089
appdynamics,I understand this comes for the older kafka client version.,0.0,0.0,1.0,0.0
appdynamics,But in my case AppDynamics is injecting a SingularityHeader as given below -,0.0,0.0,1.0,0.0
appdynamics,Kafka Client Version - 0.10.2.0.,0.0,0.0,1.0,0.0
appdynamics,I need suggestions here other than Upgrading Kafka Client version to 0.11.x from 0.10.2.0 (that is not an option).,0.0,0.0,1.0,0.0
appdynamics,Is there a way to disable this from APPD itself ?,0.0,0.0,1.0,0.0
appdynamics,I have a set of webservice endpoints.,0.0,0.0,1.0,0.0
appdynamics,I'd like to use AppDynamics to collect metrics on the performance &amp; error rate of these endpoints.,0.134,0.124,0.743,-0.0516
appdynamics,Are Business Transactions the right tool to use for this?,0.0,0.0,1.0,0.0
appdynamics,"If not, then what  are  Business Transactions useful for?",0.0,0.266,0.734,0.4404
appdynamics,(The documentation explains that Business Transactions monitor a single transaction from end-to-end.,0.0,0.0,1.0,0.0
appdynamics,"I should conceptualize my transactions ""from the end user's perspective"" etc.",0.0,0.0,1.0,0.0
appdynamics,But this doesn't answer my question - what usecase do Business Transactions fulfill that isn't better suited to Information Points or Service Endpoints etc.?),0.111,0.138,0.751,0.1879
appdynamics,I have a web application running on JBoss/Wildfly and using RESTEasy.,0.0,0.0,1.0,0.0
appdynamics,I'm monitoring it with AppDynamics.,0.0,0.0,1.0,0.0
appdynamics,I've configured my business transaction detection to use a Java Servlet.,0.0,0.0,1.0,0.0
appdynamics,"This just about works, but some of my REST paths contain UUIDs, for example:",0.0,0.0,1.0,0.0
appdynamics,"Each time this end-point is invoked with a different UUID, AppD treats it as a different business transaction.",0.0,0.0,1.0,0.0
appdynamics,"Is there a way to make AppD recognise UUIDs within a path, and group these into a single business transaction?",0.0,0.0,1.0,0.0
appdynamics,Something like:,0.0,0.714,0.286,0.3612
appdynamics,I should be able to do it by applying a regex to the request's path info:,0.0,0.0,1.0,0.0
appdynamics,or even just,0.0,0.0,1.0,0.0
appdynamics,but I can't figure out how to escape it properly.,0.0,0.204,0.796,0.2617
appdynamics,"doesn't work, and neither does",0.0,0.0,1.0,0.0
appdynamics,I cloned the image from  https://github.com/Appdynamics/docker-machine-agent,0.0,0.0,1.0,0.0
appdynamics,and executed the  docker-compose up  command after the machine agent installation.,0.0,0.0,1.0,0.0
appdynamics,But I am getting below error while starting the machine agent.,0.283,0.0,0.717,-0.5499
appdynamics,docker-compose up,0.0,0.0,1.0,0.0
appdynamics,Creating docker-machine-agent ... done,0.0,0.423,0.577,0.296
appdynamics,Attaching to docker-machine-agent,0.0,0.0,1.0,0.0
appdynamics,docker-machine-agent | /bin/sh: 1: /opt/appdynamics/machine-agent//start-appdynamics: not found,0.0,0.0,1.0,0.0
appdynamics,docker-machine-agent exited with code 127,0.0,0.0,1.0,0.0
appdynamics,We are using AppDynamics and VisualVM to monitor our application heap memory usage.,0.0,0.0,1.0,0.0
appdynamics,We see similar graph as stated in these questions -  this  and  this .,0.0,0.0,1.0,0.0
appdynamics,the red boxes show idle system heap usage - peaks are seen only when system is in idle state and are even observed when no application is deployed.,0.078,0.0,0.922,-0.296
appdynamics,"the green arrow points to actual application in use state - When system is in use, we see relatively very less heap usage being reported.",0.0,0.0,1.0,0.0
appdynamics,"Based on the clarifications in other SO questions, if we say it is due to garbage collection, why would GC not occur during application use?",0.0,0.0,1.0,0.0
appdynamics,"When system is idle, we see system objects like java.land.String, byte[], int[] etc.",0.0,0.172,0.828,0.3612
appdynamics,"getting reported in AppDynamics, but how to find who is responsible for creating them?",0.0,0.324,0.676,0.6956
appdynamics,"Again, in the heap dumps taken during idle state, we see only 200MB out of 500MB memory used, when the server has dedicated -Xmx4g configuration.",0.094,0.105,0.801,0.0772
appdynamics,How should we make sense of these observations?,0.0,0.0,1.0,0.0
appdynamics,I'm new in LoadRunner.,0.0,0.0,1.0,0.0
appdynamics,My problem is:,0.574,0.0,0.426,-0.4019
appdynamics,"When I'm running the script from LoadRunner (Http/Https protocol), I'm unable to see the browserName(like firefox, Chrome, IE etc)  in monitoring tool (AppDynamics).",0.0,0.0,1.0,0.0
appdynamics,"But when I'm running script from LoadRunner (using truClient protocol), I'm able to see the browserName in AppDynamics.",0.0,0.0,1.0,0.0
appdynamics,My Objective is to see the browser Name in AppDynamics Monitoring tool when I'm using HTTP/HTTPS protocol.,0.0,0.0,1.0,0.0
appdynamics,**,0.0,0.0,1.0,0.0
appdynamics,**,0.0,0.0,1.0,0.0
appdynamics,"I have compared both the header when script was running from    different protocol using Fiddler, Burp-Suite.",0.0,0.0,1.0,0.0
appdynamics,"(No difference) 
User-Agent string is common in both the protocol.",0.0,0.0,1.0,0.0
appdynamics,Can someone suggest/help me on this?,0.0,0.0,1.0,0.0
appdynamics,Thanks in advance.,0.0,0.592,0.408,0.4404
appdynamics,"Note:  
Version of LoadRunner is 12.60.",0.0,0.0,1.0,0.0
appdynamics,Just running with one transaction i.e launching the home page only from both the protocol.,0.0,0.0,1.0,0.0
appdynamics,Please let me know if you need more information from my end.,0.0,0.173,0.827,0.3182
appdynamics,"I'm using Xcode 11.3, my app is in Swift.",0.0,0.184,0.816,0.2023
appdynamics,"When I add AppDynamics pod to my project and try to build, I get ""1222 duplicate symbols for architecture arm64"" error.",0.13,0.0,0.87,-0.4019
appdynamics,"All duplicates are in several .a static libraries that are unrelated to AppDynamics and that are all linked in ""Link Binary With Libraries"" in Build Phases (there are other .framework-s here too but they don't cause any issue).",0.0,0.0,1.0,0.0
appdynamics,"I tried all the standard things like cleaning build folder, deleting derived data, restarting computer, building with Xcode 11.2 version, playing with -ObjC flag, but none of this helped.",0.0,0.108,0.892,0.2846
appdynamics,"Since there are so many duplicates, changing those static libraries is not an option as it has been suggested in some threads.",0.0,0.0,1.0,0.0
appdynamics,Project also has other pods added that all worked fine.,0.0,0.167,0.833,0.2023
appdynamics,"I see that there were other similar questions, but I could not find an answer that worked in my case, I've been stuck on this for more than a day already.",0.085,0.0,0.915,-0.3612
appdynamics,Does anyone have some other suggestion what I could try?,0.0,0.0,1.0,0.0
appdynamics,I would like to understand why this is happening?,0.0,0.263,0.737,0.3612
appdynamics,I have a jar file of java application.,0.0,0.0,1.0,0.0
appdynamics,I have appdynamics agent installed on my machine.,0.0,0.0,1.0,0.0
appdynamics,But not sure how to pass paramters so that appdynamics will monitor this java application.,0.149,0.0,0.851,-0.3491
appdynamics,My requirement is - AppD Health rule should trigger alert when call per hour &lt; x (eg: 10).,0.0,0.128,0.872,0.296
appdynamics,"But while setting this,  I could not find call/hour metric in AppD controller.",0.0,0.0,1.0,0.0
appdynamics,It has all perMin metrics.,0.0,0.0,1.0,0.0
appdynamics,So how to achieve this alerting criteria in AppD health rule?,0.0,0.0,1.0,0.0
appdynamics,I'm trying to get the AppD HTTP Listener working on my Linux system.,0.0,0.0,1.0,0.0
appdynamics,My AppDynamics java process has the required argument running: -Dmetric.http.listener=true,0.217,0.0,0.783,-0.3612
appdynamics,"I ran a curl command and also SoupUI and received ""Couldn't connect to host"" error.",0.184,0.0,0.816,-0.4019
appdynamics,I checked and couldn't find a port being listened too on the Linux server.,0.0,0.0,1.0,0.0
appdynamics,AppD Reference:  https://docs.appdynamics.com/display/PRO45/Standalone+Machine+Agent+HTTP+Listener,0.0,0.0,1.0,0.0
appdynamics,I am trying to configure AppDynamics for a Springboot project and I am getting following exception:,0.0,0.0,1.0,0.0
appdynamics,I can provide more information if anybody is familiar with the exception,0.0,0.0,1.0,0.0
appdynamics,I am working on spring boot and using gradle build tool.I am just wondering how to configure syntectical transaction in my project.,0.0,0.0,1.0,0.0
appdynamics,Any response will be highly appreciated.,0.0,0.418,0.582,0.5563
appdynamics,we installed grafana-appdynamics plugin in the browser.,0.0,0.0,1.0,0.0
appdynamics,appdynamics shows metrics for individual nodes within cluster (it wont sum up for eg: Active sessions),0.0,0.153,0.847,0.4019
appdynamics,I wrote different metrics/queries for each node to fetch from appdynamics and plot in grafana under single panel.. now my question is how to sum up the all the values which are output of all the nodes at a given point?,0.0,0.066,0.934,0.4019
appdynamics,"As per app dynamics android documentation, we can set a userdata using below code in android",0.0,0.13,0.87,0.2732
appdynamics,But is there a way to remove user data?,0.0,0.0,1.0,0.0
appdynamics,"In ios, the App dynamics library has a method to remove.",0.0,0.189,0.811,0.2732
appdynamics,below is the code,0.0,0.0,1.0,0.0
appdynamics,Is there a method to remove user data in android?,0.0,0.0,1.0,0.0
appdynamics,"My current implementation to remove userdata is by setting empty string, is that correct?",0.122,0.0,0.878,-0.2023
appdynamics,or do we have similar method as that of IOS appdynamics method?,0.0,0.0,1.0,0.0
appdynamics,"I've logged in, Created a Tier, and I do not see the &quot;Node&quot; with the name of the server/msmq that I need to add.",0.0,0.091,0.909,0.25
appdynamics,What am I missing?,0.524,0.0,0.476,-0.296
appdynamics,Current setting:,0.0,0.0,1.0,0.0
appdynamics,"• a collection of 30 or so microservices in Java, Node.JS and PHP running on Fargate (a very small portion of them on EC2 instances)
• AppDynamics agents installed on all of them but out of date by several months at best",0.0,0.132,0.868,0.7783
appdynamics,Desired outcome:,0.0,0.677,0.323,0.2732
appdynamics,"• upgrade agents on a schedule (once a month)
• the download and unzip should happen on schedule but the deployment should be OKed once confirmed it doesn't introduce bugs",0.0,0.0,1.0,0.0
appdynamics,What we have explored:,0.0,0.0,1.0,0.0
appdynamics,"• because of the second point above, we've dismissed the option of doing this in the Dockerfile
• creating a cronjob on an existing instance with similar function to download and unzip to EFS, EFS volume is then associated with the task definitions.",0.0,0.053,0.947,0.296
appdynamics,The problem we encountered is that EFS has a limit on associations per availability zone.,0.172,0.0,0.828,-0.4019
appdynamics,"• explored the option of using a Lambda written in Python
• currently exploring the option of automating it with Chef",0.0,0.0,1.0,0.0
appdynamics,With the last two options we don't have very much expertise at all in the team.,0.0,0.0,1.0,0.0
appdynamics,Does anyone have experience with automating this?,0.0,0.0,1.0,0.0
appdynamics,In your experience what would be the best and easiest way to achieve this?,0.0,0.368,0.632,0.7906
appdynamics,I have a java application that writes some events to a cache (vendor product) .,0.0,0.0,1.0,0.0
appdynamics,I would like to set up a health rule violation in AppDynamics to detect when connections to the cache fails.,0.255,0.106,0.638,-0.5423
appdynamics,what would be the best way to do so?,0.0,0.344,0.656,0.6369
appdynamics,We are using AppDynamics to monitor an iOS app.,0.0,0.0,1.0,0.0
appdynamics,In Experience Journey map in AppDynamics what does the &quot;drop-off rate&quot; mean?,0.0,0.0,1.0,0.0
appdynamics,drop-off rate,0.0,0.0,1.0,0.0
appdynamics,"I have an app built with SpringBoot and Spring Cache Abstraction, using Redis through Lettuce.",0.0,0.0,1.0,0.0
appdynamics,"I need to monitor via APM AppDynamics tool, but by default it only gets data from Jedis.",0.0,0.0,1.0,0.0
appdynamics,"I can create an exit point in AppDynamics, but I need to know exactly which class and method is responsible for opening the connection and executing commands to REDIS.",0.0,0.153,0.847,0.5423
appdynamics,Can anyone help me with this issue?,0.0,0.31,0.69,0.4019
appdynamics,I created python script to promote dashboard form one environment to other like from dev to test and then to prod using Custom Import and Export API ( https://docs.appdynamics.com/display/PRO45/Configuration+Import+and+Export+API ),0.0,0.228,0.772,0.7269
appdynamics,"The problem that I am facing is, If I am sending a dashboard which is in DEV to TEST and if the same dashboard already exists in TEST, then ideally it should overwrite it, but is creating a duplicate for the same, which is not what I want, can you suggest something....",0.063,0.092,0.845,0.3647
appdynamics,I am wondering if there is a way I can use AppDynamics to calculate an Apdex score for my APIs and Apps.,0.0,0.0,1.0,0.0
appdynamics,"If so, what would be the best possible way to do so?",0.0,0.276,0.724,0.6369
appdynamics,We're using Appdynamics Java agent for monitoring our production applications.,0.0,0.0,1.0,0.0
appdynamics,We have noticed slow growth in memory and the application eventually stalls.,0.0,0.191,0.809,0.3818
appdynamics,We ran a head dump on one of the JVMs and got the below reports.,0.167,0.0,0.833,-0.3818
appdynamics,"Problem Suspect 1: 
 The thread com.singularity.ee.agent.appagent.kernel.config.xml.a@ 0x1267......",0.495,0.0,0.505,-0.5994
appdynamics,AD thread config Poller keeps local variable config size of 28546.79(15.89%) KB,0.0,0.0,1.0,0.0
appdynamics,"Problem Suspect 2: 
 280561 Instances of
com.singularity.ee.agent.appagent.services.transactionmonitor.com.exitcall.p loaded by com.singularity.ee.agent.appagent.kernel.classloader.d@ 0x6c000....
occupy 503413.3(28.05%) KB.",0.29,0.0,0.71,-0.5994
appdynamics,These instances are referenced from one instance of java.util.HashMap$Node[]...,0.0,0.0,1.0,0.0
appdynamics,We figured that these classes were from the Appdynamics APM that hooks on to the running JVM and sends monitored events to the controller.,0.0,0.0,1.0,0.0
appdynamics,"There is so much convoluted process associated with reaching out to the vendor, so I am wondering if there are any work arounds for this like we enabling our java apps with JMX and Appd getting the monitoring events from JMX rather than directly hooking on to the applications' JVM.",0.0,0.09,0.91,0.5683
appdynamics,Thanks for your suggestions.,0.0,0.492,0.508,0.4404
appdynamics,Do we have any opensource or any proven code which collects  the App Dynamics reports from App Dynamics Servers ?,0.0,0.198,0.802,0.4939
appdynamics,"In regular SQL, I could write a query like:",0.0,0.294,0.706,0.3612
appdynamics,"However in ADQL syntax, the following query will work for a single column, but not for multiple:",0.0,0.0,1.0,0.0
appdynamics,Is there any options in ADQL ( https://docs.appdynamics.com/display/PRO21/ADQL+Reference ) to achieve the same end result?,0.0,0.0,1.0,0.0
appdynamics,I tried the following &quot;hack&quot; (and with a small set of data it seems to work... but will timeout with a large set of data).,0.0,0.0,1.0,0.0
appdynamics,I'm new to APPDYNAMCS and looking for APPDYNAMICS Public Rest APIs for the below data.,0.0,0.0,1.0,0.0
appdynamics,I'm able to find out a few of them but not all.,0.0,0.0,1.0,0.0
appdynamics,Can someone help me with this?,0.0,0.351,0.649,0.4019
appdynamics,Thanks in Advance,0.0,0.592,0.408,0.4404
appdynamics,Looking for REST APIs for the below data.,0.0,0.0,1.0,0.0
appdynamics,"1.Configuration Items( Business Application, servers, business service, etc) and relationship among them.",0.0,0.0,1.0,0.0
appdynamics,2.Service Map data.,0.0,0.0,1.0,0.0
appdynamics,3.Raw Event.,0.0,0.0,1.0,0.0
appdynamics,4.Alert data.,0.0,0.0,1.0,0.0
appdynamics,5.Raw Metrics.,0.0,0.0,1.0,0.0
appdynamics,6.Raw Logs.,0.0,0.0,1.0,0.0
appdynamics,7.Raw Traces.,0.0,0.0,1.0,0.0
appdynamics,8.SLO/SLI data.,0.0,0.0,1.0,0.0
appdynamics,"9.Real User Monitoring / Synthetic Monitoring data
10.User sessions data",0.0,0.0,1.0,0.0
appdynamics,I'm trying to get the list of existing Applications using python script.,0.0,0.0,1.0,0.0
appdynamics,Here is my script which fails.,0.359,0.0,0.641,-0.4215
appdynamics,I'm new to python scripting.,0.0,0.0,1.0,0.0
appdynamics,Basically I want to login to the application first and list the existing applications using the python script.,0.0,0.075,0.925,0.0772
appdynamics,"go build  is unable to find the 'appdynamics' package, even though the GOPATH is properly set.",0.0,0.0,1.0,0.0
appdynamics,"I downloaded the package, copied it onto the GOPATH:  ~/go/src/appdynamics  and ran  go install appdynamics .",0.0,0.0,1.0,0.0
appdynamics,I am using go v.1.10 on Ubuntu 18.4.,0.0,0.0,1.0,0.0
appdynamics,Visual Studio Code is able see the package and code completion works within the IDE.,0.0,0.0,1.0,0.0
appdynamics,"However, running  go get -fix -v appdynamics  produces the following error:",0.213,0.0,0.787,-0.4019
appdynamics,I have also tested this using the github.com/appdynamics namespace per the appdynamics go-sdk instructions.,0.0,0.0,1.0,0.0
appdynamics,"Also, I am aware of all the other go build 'cannot find package'  questions  on S.O.",0.0,0.0,1.0,0.0
appdynamics,My java application is connected to remote webservice application where appdynamics is not installed.,0.0,0.0,1.0,0.0
appdynamics,I am seeing those services as backed services.,0.0,0.155,0.845,0.0258
appdynamics,The remote webservice application has multiple webservices.,0.0,0.0,1.0,0.0
appdynamics,I want to track response time of each webservice separately.,0.0,0.14,0.86,0.0772
appdynamics,Should i create different tier for each service or resolve all services into single tier?,0.0,0.281,0.719,0.5719
appdynamics,Is there any other better way of doing this?,0.0,0.266,0.734,0.4404
appdynamics,has anybody gotten Appdynamics java agent to detect Apache Camel business transactions?,0.0,0.0,1.0,0.0
appdynamics,Picking up files from a directory (polling) and then sending off to activemq.,0.0,0.0,1.0,0.0
appdynamics,"Another case is camel deployed on apache karaf, need to track outgoing http calls using appDynamics",0.0,0.128,0.872,0.296
appdynamics,Best,0.0,1.0,0.0,0.6369
appdynamics,"Hi I am fairly new to appdynamics and using it to configure my server for trial period, I have 3 tomcats, I followed the documentation I got to know that we need to put appagent and machineagent to through data back to controller, If I try to download appagent and machineagent jar file fromir their official site I always end up with the same version and which ever tomcat starts first I get data only for that machine
 
This is what I have used for tomcat  catalina.sh '",0.0,0.0,1.0,0.0
appdynamics,"On our ASP.Net website, we've had some requests timeout.",0.0,0.0,1.0,0.0
appdynamics,"AppDynamics shows that the SQL procedure calls are returning in a matter of seconds, but we're spending 100+ seconds in SNIReadSyncOverAsync.",0.0,0.052,0.948,0.0129
appdynamics,Does anyone know what this method is / does and why it would be taking that much time?,0.0,0.0,1.0,0.0
appdynamics,We're not using EF which is referenced in every question / post I've been able to find about it.,0.0,0.0,1.0,0.0
appdynamics,Thanks in advance,0.0,0.592,0.408,0.4404
appdynamics,Update,0.0,0.0,1.0,0.0
appdynamics,"It's been a while and while we never came to a resolution as to why all of the time was being spent in SNIReadSyncOverAsync, I have a few thoughts.",0.0,0.0,1.0,0.0
appdynamics,"I think that in this case, it may have been the way that specific version of AppDynamics was reporting the time spent on the SQL calls, but I have no real data to back that up, just my guess from what I observed.",0.067,0.0,0.933,-0.4215
appdynamics,We eventually stopped seeing the time reported as being spent in SNIReadSyncOverAsync and it shifted to the queries themselves timing out.,0.087,0.0,0.913,-0.2263
appdynamics,That still didn't make a lot of since because the same queries would run instantly in SSMS on the same database.,0.0,0.0,1.0,0.0
appdynamics,"The ultimate answer ended up being related to ARITHABORT causing our application and SSMS to use two different execution plans (see  https://dba.stackexchange.com/a/9841 ), explaining why we couldn't reproduce the timeouts with SSMS.",0.0,0.0,1.0,0.0
appdynamics,"Once we resolved that, we were able to identify a few portions of the procedure that needed tuning and we haven't run into the unexplained timeouts or SNIReadSyncOverAsync since.",0.0,0.059,0.941,0.1779
appdynamics,I am using,0.0,0.0,1.0,0.0
appdynamics,with G1 garbage collector.,0.0,0.511,0.489,0.4824
appdynamics,JVM argumens are,0.0,0.0,1.0,0.0
appdynamics,"However, I am experiencing following Full GC scans without any apparent reason, how to get rid of them?",0.0,0.0,1.0,0.0
appdynamics,GC log with some tail from preceding events:,0.0,0.0,1.0,0.0
appdynamics,Other similar ones:,0.0,0.0,1.0,0.0
appdynamics,Other similar issue reports:,0.0,0.0,1.0,0.0
appdynamics,"http://grokbase.com/t/openjdk/hotspot-gc-use/1192sy84j5/g1c-strange-full-gc-behavior 
   http://grokbase.com/p/openjdk/hotspot-gc-use/123ydf9c92/puzzling-why-is-a-full-gc-triggered-here 
   http://mail.openjdk.java.net/pipermail/hotspot-gc-use/2013-February/001484.html",0.0,0.0,1.0,0.0
appdynamics,"I have been analyzing the issue using appdynamics profiler and I have found out that every time Full GC occurs, Code Cache (configured to its maximum) is full.",0.0,0.0,1.0,0.0
appdynamics,It seems like a bug in GC.,0.0,0.333,0.667,0.3612
appdynamics,"See also the profiler image, two unnecessary Full GC:s in middle between 24/5 and 25/5.",0.0,0.0,1.0,0.0
appdynamics,"More importantly, they kill the server usability, because they last 60 seconds each:",0.267,0.14,0.593,-0.5221
appdynamics,Profiler log image http://eisler.vps.kotisivut.com/logs/g1gc-code-cache-full-gc-bug-illustration.png,0.0,0.0,1.0,0.0
appdynamics,"See also discussion about Azul's pauseless GC, they seem to have worked out this kind of issues  http://www.artima.com/lejava/articles/azul_pauseless_gc.html",0.0,0.0,1.0,0.0
appdynamics,I'm developing a social-like application which is currently deployed using AWS services.,0.0,0.0,1.0,0.0
appdynamics,"In particular, the DB runs on RDS using MYSQL.",0.0,0.0,1.0,0.0
appdynamics,"So far, we're testing the app using a limited number of users (mostly friends) resulting in an average of 15 Write IOPS/sec.",0.086,0.059,0.856,-0.1531
appdynamics,"The real problem is related to the very high writing latency of the db, which is always above 100ms.",0.13,0.0,0.87,-0.4019
appdynamics,The RDS instance is a db.m3.xlarge which is much more than what we need.,0.0,0.0,1.0,0.0
appdynamics,"I tried to perform a load test in a separate instance (identical configuration of DB and EC2) but i've not been able to reproduce such a high latency, even if I was sending a much higher number of requests.",0.0,0.043,0.957,0.1154
appdynamics,"So I thought it may be due to table fragmentation, but i've not yet run a table optimisation, because the db wouldn't be accessible during this procedure.",0.0,0.124,0.876,0.5267
appdynamics,Do you have any experience with this problem?,0.309,0.0,0.691,-0.481
appdynamics,MORE INFO,0.0,0.0,1.0,0.0
appdynamics,The biggest table (called  Message ) has about 790k rows.,0.0,0.0,1.0,0.0
appdynamics,"Concerning this table, the following query",0.0,0.0,1.0,0.0
appdynamics,took 11s to be executed.,0.0,0.0,1.0,0.0
appdynamics,"Even worse, the query",0.508,0.0,0.492,-0.4767
appdynamics,"took 14s, but the table Comment has about 160k.",0.0,0.0,1.0,0.0
appdynamics,Those two tables are generated by:,0.0,0.0,1.0,0.0
appdynamics,and,0.0,0.0,1.0,0.0
appdynamics,SOME PLOTS,0.0,0.0,1.0,0.0
appdynamics,Using  AppDynamics  I've been able to extract the following plots:,0.0,0.0,1.0,0.0
appdynamics,Wait States : Isn't the query end time too big?,0.0,0.0,1.0,0.0
appdynamics,Page Buffer :,0.0,0.0,1.0,0.0
appdynamics,Write Latency and Queue :,0.0,0.0,1.0,0.0
appdynamics,Query Cache,0.0,0.0,1.0,0.0
appdynamics,Thank for your help!,0.0,0.733,0.267,0.6696
appdynamics,Andrea,0.0,0.0,1.0,0.0
appdynamics,"For one installation of our application we have been seeing issues on production that were reported as ""system is getting slower"" or ""requests never returning"" by the users.",0.0,0.0,1.0,0.0
appdynamics,In the end the server had to be restarted.,0.0,0.0,1.0,0.0
appdynamics,We had several of those incidents and a nightly restart of the server seems to be working as a workaround.,0.0,0.0,1.0,0.0
appdynamics,Our application makes heavy use of dynamic classloading (.jar files stored in database as blobs) and reflection.,0.0,0.14,0.86,0.3818
appdynamics,Environment details:,0.0,0.0,1.0,0.0
appdynamics,We switched to,0.0,0.0,1.0,0.0
appdynamics,but it looks like we are still facing the issue.,0.0,0.265,0.735,0.5023
appdynamics,"What we are seeing in the logfiles, heapdumps, threadumps and gc logs so fare is the following",0.0,0.0,1.0,0.0
appdynamics,What we are seeing is,0.0,0.0,1.0,0.0
appdynamics,See below for an excerpt of the threadump:,0.0,0.0,1.0,0.0
appdynamics,The situation after updating to Java 1.7.80 / G1Gc seems to be similar.,0.0,0.0,1.0,0.0
appdynamics,"Unfortunately no threadump available, just wicket warnings in the log)",0.493,0.0,0.507,-0.7003
appdynamics,We are currently unable to reproduce this (still working on that).,0.0,0.0,1.0,0.0
appdynamics,But maybe somebody in the community has seen something similar and has an idea what could help us to reproduce or solve the issues.,0.0,0.207,0.793,0.6956
appdynamics,"One guess that we currently having is that this is related to native memory consumption (Because of information like  http://www.ibm.com/developerworks/java/library/j-nativememory-linux/ ) but we don't see any hints in this regards in the logs (no OutOfMemory errors, no reports from the linux administrators that the system is running out of memory)",0.11,0.033,0.857,-0.631
appdynamics,Because of a known (fixed) bug in Hazelcast 2.5 we've decided this would be the next upgrade candidate for our project.,0.0,0.0,1.0,0.0
appdynamics,But after dropping in the latest version (3.2.2) we had horrible performance.,0.302,0.0,0.698,-0.6956
appdynamics,The way we are using Hazelcast:,0.0,0.0,1.0,0.0
appdynamics,"Using Hazelcast 2.5 we had great performance when, instead of using  map.values() , we supplied a list of all contained keys  map.getAll(containedKeys) .",0.0,0.177,0.823,0.6249
appdynamics,The way we keep track of the containedKeys by adding an  EntryListener  to the map which stores the containedKeys in a concurrent set.,0.0,0.0,1.0,0.0
appdynamics,"This was added by a colleague and feels like a hack, but works like a charm.",0.0,0.461,0.539,0.8201
appdynamics,"Now when we upgrade to Hazelcast 3.2.2 we instantly see problems with  java.io , for example look at the following snippet from AppDynamics:",0.114,0.0,0.886,-0.4019
appdynamics,"This is something we haven't seen in Hazelcast 2.5, but do have in 3.2.2.",0.0,0.0,1.0,0.0
appdynamics,It grinds our application to a complete standstill.,0.0,0.0,1.0,0.0
appdynamics,Replacing the jar with 2.5 again (and renaming Entry back to MapEntry) and nothing is wrong.,0.0,0.145,0.855,0.3724
appdynamics,What could be causing this?,0.0,0.0,1.0,0.0
appdynamics,Maybe it isn't using the near-cache anymore?,0.0,0.0,1.0,0.0
appdynamics,I have a website written in cakephp on linux server.,0.0,0.0,1.0,0.0
appdynamics,I have a problem with extremly slow download time of my css and js files.,0.184,0.0,0.816,-0.4019
appdynamics,"For example, thats the network tab in chrome when loading my homepage:",0.0,0.0,1.0,0.0
appdynamics,"As you can see, one of my css files took 59 seconds to download!",0.0,0.0,1.0,0.0
appdynamics,Its important to note that it is not always the same css file.,0.0,0.13,0.87,0.2023
appdynamics,"Sometimes its JS file, sometimes other css but they have to be downloaded before other content of the page is displayed, therefore they block the page loading.",0.129,0.0,0.871,-0.5927
appdynamics,"Because of waiting for that one file to download, website is not displayed for 59 seconds.",0.0,0.0,1.0,0.0
appdynamics,"I checked my server and it has a very low load, cpu runs on 10% and there is less than 20% of ram used.",0.102,0.0,0.898,-0.3384
appdynamics,Its an apache server with the following prefork settings:,0.0,0.0,1.0,0.0
appdynamics,This mentioned slow download time happened with maybe 3-4 simultaneous users on the website.,0.0,0.0,1.0,0.0
appdynamics,I have my app under APM with appdynamics and nothing suspicious is shown there.,0.0,0.15,0.85,0.2755
appdynamics,I checked php.ini file with server admin and everything seems to be good there as well.,0.0,0.278,0.722,0.6124
appdynamics,What other software can I use to find the source of this issue?,0.0,0.0,1.0,0.0
appdynamics,There is not much info in apache logs either.,0.0,0.0,1.0,0.0
appdynamics,Any suggestions would be greatly appreciated,0.0,0.418,0.582,0.5563
appdynamics,EDIT:,0.0,0.0,1.0,0.0
appdynamics,I moved all of my assets to webroot and got these results on another domain that is using that same server:,0.0,0.082,0.918,0.1779
appdynamics,"As you can see, this time its jquery file that took 27 seconds to download.",0.0,0.0,1.0,0.0
appdynamics,It is stored in the app/webroot,0.0,0.0,1.0,0.0
appdynamics,We have a production web application running on our intranet which:,0.0,0.0,1.0,0.0
appdynamics,is configured with:,0.0,0.0,1.0,0.0
appdynamics,Each day the heap usage:,0.0,0.0,1.0,0.0
appdynamics,"at which point the heap rises to 55% in about 40 minutes and is collected back to 37%, ad infinitum until the next restart.",0.0,0.0,1.0,0.0
appdynamics,"We have AppDynamics installed on the JVM and can see that Major Garbage Collections take place roughly every minute without much of an impact on the memory (except the falls outlined above of course) until the memory reaches 37%, when the Major collections become much less frequent.",0.0,0.025,0.975,0.0516
appdynamics,"There are obviously hundreds of factors external to the behaviour of a web application, but one avenue of research is the fact that Hotspot JIT information is obviously lost when the JVM is stopped.",0.146,0.0,0.854,-0.6486
appdynamics,Are there GC optimisations/etc which are also lost with the shutdown of the JVM?,0.15,0.0,0.85,-0.3182
appdynamics,Is the JVM effectively consuming more memory than it needs to because certain Hotspot optimisations haven't yet taken place?,0.0,0.328,0.672,0.7783
appdynamics,Is it possible that we would get better memory performance from this application if the JVM wasn't restarted and we found another way to perform a backup of the database?,0.0,0.094,0.906,0.4404
appdynamics,"(Just to reiterate, I know that there are a hundred thousand things that could influence the behaviour of an application, especially an application that hardly anyone else knows!",0.0,0.0,1.0,0.0
appdynamics,I really just want to know whether there are certain things to do with the memory performance of a JVM which are lost when it is stopped),0.082,0.131,0.786,0.0972
appdynamics,I'm monitoring a production system with  AppDynamics  and we just had the system slow to a crawl and almost freeze up.,0.057,0.0,0.943,-0.024
appdynamics,"Just prior to this event, AppDynamics is showing all GC activity (minor and major alike) flatline for several minutes...and then come back to life.",0.0,0.0,1.0,0.0
appdynamics,"Even during periods of ultra low load on the system, we still see our JVMs doing  some  GC activity.",0.104,0.0,0.896,-0.2732
appdynamics,We've never had it totally flatline and drop to 0.,0.208,0.0,0.792,-0.3321
appdynamics,Also - the network I/O flatlined at the same instance of time as the GC/memory flatline.,0.0,0.0,1.0,0.0
appdynamics,"So I ask: can something at the system level cause a JVM to freeze, or cause its garbage collection to hang/freeze?",0.0,0.062,0.938,0.0516
appdynamics,This is on a CentOS machine.,0.0,0.0,1.0,0.0
appdynamics,"I have a nodejs project with a few modules, one of them (appdynamics) serving a native binary depending on the platform/architecture of the requester.",0.0,0.0,1.0,0.0
appdynamics,This in combination with the fingerprinting of yarn in the lockfile creates an issue:,0.0,0.139,0.861,0.2732
appdynamics,Yarn does not allow this out of the box because of the aforementioned fingerprinting.,0.114,0.0,0.886,-0.1695
appdynamics,"For now the only workaround I've found is to mount the codebase into a Docker container and run yarn commands from there, then commit the lockfile changes.",0.0,0.081,0.919,0.296
appdynamics,Is there a better way to achieve this?,0.0,0.326,0.674,0.4404
appdynamics,"We have serious application issue at peak time application get very very slow and when i check on AppDynamics matrix, my heap memory is full and GC kicked in every minute and that make it very very slow.",0.035,0.0,0.965,-0.0772
appdynamics,here is the configuration of my java (tomcat),0.0,0.0,1.0,0.0
appdynamics,Java options are  -Djava.awt.headless=true -Xmx2048m -XX:MaxPermSize=256m -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+DisableExplicitGC,0.0,0.0,1.0,0.0
appdynamics,Major GC collection time spend per min (ms),0.0,0.0,1.0,0.0
appdynamics,CMS Old Gen usage in MB,0.0,0.0,1.0,0.0
appdynamics,Par Eden space in MB,0.0,0.0,1.0,0.0
appdynamics,Any suggestion why par eden space and old gen hitting hard line?,0.113,0.0,0.887,-0.1027
appdynamics,"Here is the last 12 Hour picture of Heap usage and Major GC collection (in Green dots), GC was very high between  3:00AM to 7:00AM  but when i restart application around  7:30AM  everything is good and application response time was very fast, why reboot fixed everything?",0.0,0.08,0.92,0.5927
appdynamics,Major GC collection time spend per min (ms) after 4GB (Zero Major GC),0.0,0.0,1.0,0.0
appdynamics,CMS Old Gen usage in MB after 4GB Heap,0.0,0.0,1.0,0.0
appdynamics,Par Eden space in MB after 4GB Heap,0.0,0.0,1.0,0.0
appdynamics,I am having issues with concurrency when writing JSON out from my Spring Boot WAR app deployed to Tomcat 8.,0.205,0.0,0.795,-0.6841
appdynamics,In the screenshot from AppDynamics there seems to be a considerable wait when the jackson library is performing _flushBuffer.,0.0,0.0,1.0,0.0
appdynamics,This issue arises under load testing for even a small amount (&lt; 10) users.,0.0,0.0,1.0,0.0
appdynamics,I have configured the messageConverters in my configuration class.,0.0,0.0,1.0,0.0
appdynamics,"I am using 
Spring Boot 1.5.4
Java 1.8
Jackson 2.9.7
Tomcat 8.5.33",0.0,0.0,1.0,0.0
appdynamics,Is it possible to monitor Play Framework application performance with Javamelody?,0.0,0.194,0.806,0.34
appdynamics,I'm using Javamelody with Spring apps.,0.0,0.0,1.0,0.0
appdynamics,I find it much better than free version of AppDynamics or Dyna Trace.,0.0,0.383,0.617,0.7351
appdynamics,You can't use filter for HTTP monitoring or aspect for method monitoring.,0.0,0.0,1.0,0.0
appdynamics,I think I should make something like filter or aspect.,0.0,0.263,0.737,0.3612
appdynamics,I have no idea how to add performance monitoring to JDBC queries.,0.18,0.0,0.82,-0.296
appdynamics,Any ideas?,0.0,0.0,1.0,0.0
appdynamics,My multi-jar app runs in Java 11 and shows a warning related to Log4j2:,0.167,0.0,0.833,-0.34
appdynamics,WARNING: sun.reflect.Reflection.getCallerClass is not supported.,0.629,0.0,0.371,-0.6243
appdynamics,This will impact performance.,0.0,0.0,1.0,0.0
appdynamics,"It doesn't crash, but quite bothers me since the Operations team (AppDynamics monitor) has asked me about it.",0.13,0.08,0.789,-0.2525
appdynamics,"I read that I need to use the ""Multi-Release:true"" entry in the manifest, but I don't kow how to tell the Maven Assembly Plugin to add it.",0.0,0.0,1.0,0.0
appdynamics,I don't use any other plugin in the pom.xml.,0.0,0.0,1.0,0.0
appdynamics,Should I use the  Maven Shade Plugin  instead?,0.0,0.0,1.0,0.0
appdynamics,"Anyway, here's the Maven Assembly Plugin section of my pom.xml.",0.0,0.0,1.0,0.0
appdynamics,"The library I'm including (that I also wrote) uses Log4j 2 as a dependency, as shown below:",0.0,0.0,1.0,0.0
appdynamics,How can I get rid of this warning?,0.314,0.0,0.686,-0.4118
appdynamics,We have developed set of APIs using spring boot.,0.0,0.0,1.0,0.0
appdynamics,"When performance test was run and hitting more than 5000 calls/minute, avg response time started increasing.",0.0,0.0,1.0,0.0
appdynamics,"When we investigated with the help of AppDynamics, more than 2% transactions are having slow response time (more than 1.5 seconds).",0.0,0.119,0.881,0.4019
appdynamics,But the CPU usage is still under 20%.,0.0,0.0,1.0,0.0
appdynamics,And all of them are waiting exactly at the same location i. e  org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827.,0.0,0.0,1.0,0.0
appdynamics,Below is the complete call graph.,0.0,0.0,1.0,0.0
appdynamics,What is the root cause for threads handing at this location?,0.0,0.0,1.0,0.0
appdynamics,Are there any SpringBooot properties to be updated to eliminate this?,0.0,0.0,1.0,0.0
appdynamics,"Following is the source code of RequestMappingHandlerAdapter:invokeHandlerMethod and it hangs at invocableMethod.invokeAndHandle(webRequest, mavContainer);",0.0,0.0,1.0,0.0
appdynamics,"protected ModelAndView invokeHandlerMethod(HttpServletRequest request,
            HttpServletResponse response, HandlerMethod handlerMethod) throws Exception {",0.0,0.244,0.756,0.4404
appdynamics,"Recently, our company is focusing on performance of the application we are developing for long time.",0.0,0.0,1.0,0.0
appdynamics,"One thing we noticed during performance test, certain methods are making so many database calls (over 500 queries)",0.0,0.11,0.89,0.2732
appdynamics,"Then this brings a question like, which methods are doing so many calls and how should be prioritize which method to refactor first.",0.0,0.106,0.894,0.3612
appdynamics,"When we initially try to refactor some of those methods, we observed that it is requiring a lot of effort to reduce the number of round-trips.",0.0,0.051,0.949,0.0772
appdynamics,The reason is our data access layer is pretty much depending on NHibernate ORM framework and we figured out that we have totally misused Lazyloading configuration from the beginning of the development.,0.0,0.094,0.906,0.4939
appdynamics,That is why number of round-trips are huge and impacting the performance a lot.,0.0,0.247,0.753,0.3818
appdynamics,And just correcting Lazy Loading configuration creates a lot of regression.,0.198,0.167,0.635,-0.1027
appdynamics,"Thus, we somehow have to figure out a way to collect number of database call per http request.",0.0,0.075,0.925,0.0772
appdynamics,I have seen some tools like Application Insight or AppDynamics provides overall result for all Dependent calls.,0.0,0.143,0.857,0.3612
appdynamics,But I am just wondering is there a way to collect these traces differently than using those frameworks ?,0.0,0.0,1.0,0.0
appdynamics,"For instance, every time http request is made, can we have attribute in the controller that whenever ExecuteQuery() or SqlDataAdapter.Fill method is called within the call stack of the method, can it increase the counter.",0.0,0.063,0.937,0.3182
appdynamics,I am looking for a solution something like this.,0.0,0.49,0.51,0.5859
appdynamics,Any help is greatly appreciated.,0.0,0.677,0.323,0.7425
appdynamics,Thank you in advance for all suggestions.,0.0,0.294,0.706,0.3612
appdynamics,We have a situation where we have a Grails 2.3.11 based application that uses Quartz (version 2.2.1 / Grails Quartz plugin version 1.0.2) jobs to do certain long running processes (1-5 minutes) in background so that a polling service allows the browser to fetch the progress.,0.0,0.109,0.891,0.5994
appdynamics,This is used primarily for import and export of data from the application.,0.0,0.0,1.0,0.0
appdynamics,"For example, when the application first starts, the export for 200,000+ rows takes approx 2 minutes.",0.0,0.0,1.0,0.0
appdynamics,The following day the export takes 3+ minutes.,0.0,0.0,1.0,0.0
appdynamics,The third day the export takes more than 6 minutes.,0.0,0.0,1.0,0.0
appdynamics,We have narrowed the problem down to just the Quartz jobs.,0.213,0.0,0.787,-0.4019
appdynamics,When the system is in the degraded state all other web pages respond with nearly identical response times as when the system is in optimal condition.,0.096,0.085,0.819,-0.0772
appdynamics,It appears that the Quartz jobs tend to slowdown linearly or incrementally over the period of 2 to 3 days.,0.0,0.0,1.0,0.0
appdynamics,"This may be usage related or time, for which we are uncertain.",0.167,0.0,0.833,-0.296
appdynamics,We are familiar with the memory leak bug  reported by Burt Beckwith  and added the fix to our code.,0.118,0.0,0.882,-0.34
appdynamics,"We were experiencing the memory leak before but now memory management appears to be health, even when the job performance is 5-10x slower than",0.069,0.0,0.931,-0.1779
appdynamics,The jobs use GORM for most of the queries.,0.0,0.0,1.0,0.0
appdynamics,We've optimized some to use criterias with projects so they are light weight but haven't been able to change all the logic over so there are a number of Gorm objects.,0.0,0.121,0.879,0.4302
appdynamics,In the case of the exports we've changed the queries to be read-only.,0.0,0.0,1.0,0.0
appdynamics,The logic also clears out the hibernate session appropriately to limit the number of objects in memory.,0.0,0.148,0.852,0.1531
appdynamics,Here are a few additional details:,0.0,0.0,1.0,0.0
appdynamics,Any suggestions would be greatly appreciated.,0.0,0.418,0.582,0.5563
appdynamics,"Thanks,",0.0,1.0,0.0,0.4404
appdynamics,John,0.0,0.0,1.0,0.0
appdynamics,The following code:,0.0,0.0,1.0,0.0
appdynamics,Has started throwing the following exception on SOME machines.,0.0,0.0,1.0,0.0
appdynamics,What could cause this?,0.0,0.0,1.0,0.0
appdynamics,EDIT: the machines that experience the error are running Windows Server 2008 R2.,0.184,0.0,0.816,-0.4019
appdynamics,Windows Server 2012 and desktop machines running windows 7 work fine.,0.0,0.167,0.833,0.2023
appdynamics,"(this is true, but I now think a different issue is the relevant difference... see below).",0.0,0.128,0.872,0.2263
appdynamics,"EDIT: as an additional note, this occurred right after updating our codebase to Entity Framework 6.1.1.-beta1.",0.0,0.0,1.0,0.0
appdynamics,"In the above code, The IDisposable is a class which wraps an EF DbContext.",0.0,0.0,1.0,0.0
appdynamics,EDIT: why the votes to close?,0.0,0.0,1.0,0.0
appdynamics,EDIT: the stack trace of the failure ends at the  WeakReference&lt;T&gt;  constructor called in the above code:,0.171,0.0,0.829,-0.5106
appdynamics,EDIT: it turns out that the machines having issues with this were running  AppDynamics .,0.0,0.0,1.0,0.0
appdynamics,Uninstalling that seems to have removed the issue.,0.0,0.0,1.0,0.0
appdynamics,Are there any tools that can connect to a remote JVM and log to a file the information which is displayed in the Monitor tab of JVisualVM or the Overview tab of JConsole?,0.0,0.0,1.0,0.0
appdynamics,"I am aware of applications such as AppDynamics etc - this is for a little performance test for a machine which is already set up - even though we have AppDynamics licences, using AppDynamics isn't really an option in this scenario.",0.0,0.0,1.0,0.0
appdynamics,I have spring enterprise app running on JDK 1.6 under Windows 2008.,0.0,0.0,1.0,0.0
appdynamics,The app gets slow or unresponsive at random times.,0.0,0.0,1.0,0.0
appdynamics,I suspect it is memory leak and the GC is kicking into over drive.,0.295,0.0,0.705,-0.5574
appdynamics,How can I troubleshoot this without restarting JVM using java.exe -verbose:gc parameter?,0.0,0.153,0.847,0.2023
appdynamics,I really cannot shutdown this app.,0.0,0.0,1.0,0.0
appdynamics,I'm planning on doing AppDynamics on it once I can restart it but for know what can I do?,0.0,0.0,1.0,0.0
appdynamics,What are my options?,0.0,0.0,1.0,0.0
appdynamics,"I have a requirement to pass  cluster, namespace and pod name to AppDynamics agent from my container deployed in Kubernetes cluster.",0.0,0.0,1.0,0.0
appdynamics,"I tried something as below, but that does not work.",0.0,0.0,1.0,0.0
appdynamics,and,0.0,0.0,1.0,0.0
appdynamics,Could anyone please help me here how to collect the detail and pass to AppD.,0.0,0.278,0.722,0.6124
appdynamics,Thanks in advance.,0.0,0.592,0.408,0.4404
appdynamics,"Have a simple method for connection,",0.0,0.0,1.0,0.0
appdynamics,"In case of URL path not being a valid one, FileNotFoundException is getting logged as an error in AppyDynamics.",0.137,0.0,0.863,-0.4019
appdynamics,How to prevent AppDynamics from catching these exceptions since as part of the code its handled as a boolean return but AppDynamics is flooded with FileNotFoundException.,0.0,0.042,0.958,0.0129
appdynamics,Thanks in advance.,0.0,0.592,0.408,0.4404
appdynamics,"Update 
As per AppDynamics documentation  https://docs.appdynamics.com/display/PRO44/Errors+and+Exceptions 
 An HTTP error response, such as a status code 404 or 500 response  get recorded as a transaction snapshot error.",0.197,0.0,0.803,-0.6597
appdynamics,As i know at this point in my code above response 404 is legitimate.,0.0,0.0,1.0,0.0
appdynamics,How can I modify my code to prevent AppDynamics showing it up ?,0.0,0.099,0.901,0.0258
appdynamics,Any Suggestions will be helpful.,0.0,0.412,0.588,0.4215
appdynamics,I am working on the ASP.NET application which is used by 10K people approx.,0.0,0.0,1.0,0.0
appdynamics,Till last week it was working fine and suddenly from past week it's performance is degraded.,0.151,0.097,0.753,-0.25
appdynamics,"Application is taking too long time to respond, also opening each page taking a lot of time(1-2 mins approx.)",0.0,0.0,1.0,0.0
appdynamics,even though no complex calls or functionality is present.,0.216,0.0,0.784,-0.296
appdynamics,I have checked all stored procedures in database and all are working fine and their execution time is less than 5 seconds.,0.0,0.087,0.913,0.2023
appdynamics,"Also using SAAS AppDynamics tools I have checked all calls and request-response time in each page of the application, but it seems everything to be fine there also.",0.0,0.078,0.922,0.296
appdynamics,All calls between application server and data base server are happening in 2-3 seconds.,0.0,0.0,1.0,0.0
appdynamics,I am not able to find where exactly the issue is.,0.0,0.0,1.0,0.0
appdynamics,Is there any applicaiton like AppDynamics which can be helpful?,0.0,0.398,0.602,0.6486
appdynamics,NOTE : I am using linked server in some stored procedures.,0.0,0.0,1.0,0.0
appdynamics,Need help in identifying the issue.,0.0,0.351,0.649,0.4019
appdynamics,Thanks in advance.,0.0,0.592,0.408,0.4404
appdynamics,My node app's CPU usage is gradually increasing.,0.0,0.0,1.0,0.0
appdynamics,I have found that memory leaks are happening.,0.0,0.0,1.0,0.0
appdynamics,"Through AppDynamics, I have found that there is a significant amount of retained memory which keeps increasing over time under  processImmediate  call tree.",0.0,0.132,0.868,0.2263
appdynamics,"As I drilled in, I found the problem was with  settlePromises  function.",0.231,0.0,0.769,-0.4019
appdynamics,I want to get your opinion on one particular usage of promises I have been using.,0.0,0.245,0.755,0.4404
appdynamics,Looping of promises.,0.0,0.565,0.435,0.3818
appdynamics,Below is a sample function structure of such usage.,0.0,0.0,1.0,0.0
appdynamics,The heap growth over an hour is plotted in the below picture,0.0,0.191,0.809,0.3818
appdynamics,The above function has to perform a synchronous update with the objects in data array.,0.0,0.0,1.0,0.0
appdynamics,Is there a chance of memory leak with this?,0.231,0.192,0.577,-0.1027
appdynamics,I'm very new to nodejs.,0.0,0.0,1.0,0.0
appdynamics,"In my dockerized environment, I want to provide appdynamics support to nodejs apps.",0.0,0.286,0.714,0.4588
appdynamics,This mandates every app to require the following as the first line in their app.,0.0,0.0,1.0,0.0
appdynamics,I plan to do that by providing a wrapper called  appdynamics.js  around the app's entry file.,0.0,0.0,1.0,0.0
appdynamics,Details:,0.0,0.0,1.0,0.0
appdynamics,"I run a script in my nodejs docker image to replace the entry file name in the app's package.json with ""appdynamics.js"", where appdynamics.js has the above appdynamics related require statement.",0.0,0.0,1.0,0.0
appdynamics,"Ex :  {scripts { ""start"" : ""node server.js"" }}  will be replaced with 
  {scripts { ""start"" : ""node appdynamics.js""}}",0.0,0.0,1.0,0.0
appdynamics,"Then, i ""require"" the ""server.js"" inside appdynamics.js.",0.0,0.0,1.0,0.0
appdynamics,Invoke npm start.,0.0,0.0,1.0,0.0
appdynamics,My only concern is this:,0.0,0.0,1.0,0.0
appdynamics,"If the  package.json  had something like scripts  { ""start"" : ""coffee server.coffee"" } , my script will replace it to  { ""start"" : ""coffee appdynamics.js"" } .",0.0,0.122,0.878,0.3612
appdynamics,"and then my script will invoke  npm start , which will error out.",0.197,0.0,0.803,-0.4019
appdynamics,What is the best way to solve this?,0.0,0.5,0.5,0.7184
appdynamics,This is a follow up question to  Use &quot;coffee&quot; instead of &quot;node&quot; command in production,0.0,0.0,1.0,0.0
appdynamics,I am creating a REST api to send message to RabbitMQ and was trying to understand what are the best practice for creating/closing channels.,0.0,0.242,0.758,0.7506
appdynamics,I am using RabbitMQ Java client api.,0.0,0.0,1.0,0.0
appdynamics,Currently I have a class  RabbitMQPublisherConnection  where I spring inject RabbitMQ connection.,0.0,0.0,1.0,0.0
appdynamics,This class is then spring injected to another class  RabbitMQPublisherChannel .,0.0,0.0,1.0,0.0
appdynamics,This class has the following function to create a channel:,0.0,0.208,0.792,0.2732
appdynamics,Now I have the third class  RabbitMQPublisher  where I spring inject  RabbitMQPublisherChannel  class.,0.0,0.0,1.0,0.0
appdynamics,My application context looks like this:,0.0,0.333,0.667,0.3612
appdynamics,The class  RabbitMQPublisher  has the function to publish a message to RabbitMQ:,0.0,0.0,1.0,0.0
appdynamics,This application is run through tomcat and I noticed with AppDynamics that the closing the channel takes like 47% of the total time taken to publish message.,0.0,0.091,0.909,0.3612
appdynamics,When I remove the call to close the channel then I save this 47% of time which is like 32ms but then I notice in my RabbitMQ management console that the number of channel is ever increasing for that connection.,0.0,0.135,0.865,0.5106
appdynamics,So my questions are -,0.0,0.0,1.0,0.0
appdynamics,Thanks,0.0,1.0,0.0,0.4404
appdynamics,"So, detailed description.",0.0,0.0,1.0,0.0
appdynamics,I'm working on a microservice framework that uses Rabbit as the event bus.,0.0,0.0,1.0,0.0
appdynamics,"Each service runs on it's own dedicated VM inside a Tomcat container (4 cores, 4GB RAM of which 2 are available to Tomcat).",0.0,0.13,0.87,0.4588
appdynamics,Each service both consumes and publishes messages back to Rabbit.,0.0,0.0,1.0,0.0
appdynamics,"When I crank up the consumers, channel settings and prefetch size I can get an individual service to perform well.",0.0,0.11,0.89,0.2732
appdynamics,"My problem comes when I try to test scalability, i.e.",0.252,0.0,0.748,-0.4019
appdynamics,a 2nd VM instance with the service running on it.,0.0,0.0,1.0,0.0
appdynamics,"Instead of the throughput doubling (or at least increasing), it can actually get slower, and I'm very confused.",0.132,0.0,0.868,-0.3804
appdynamics,"I've checked for errors and exceptions in the service, used analytics tools (AppDynamics) to check the time spent in the service and the resources used, and everything looks fine, so as far as I can tell it's my Rabbit configuration that's the problem.",0.111,0.039,0.85,-0.5106
appdynamics,"The specific settings used to achieve high performance for one service are:
-Consumers: 20.",0.0,0.0,1.0,0.0
appdynamics,"-Channel cache size: 200
-Prefetch: 500",0.0,0.0,1.0,0.0
appdynamics,Using this it seems to work quite well.,0.0,0.255,0.745,0.3384
appdynamics,"However when I add the second service the queues aren't being consumed as fast and they start to back up quite quickly, and I'm at a loss to understand why.",0.078,0.0,0.922,-0.3182
appdynamics,I've experimented a little with the settings above but can't seem to get anywhere.,0.0,0.0,1.0,0.0
appdynamics,"I don't have any access to the Rabbit cluster to change settings so I can't do anything there, but I have full control over the VM my service runs in (Java settings, Tomcat, Rabbit settings..)",0.0,0.0,1.0,0.0
appdynamics,"The service doesn't do anything explicit with connections or ack policies, so it's possible they may need to be tweaked?",0.0,0.0,1.0,0.0
appdynamics,"A few articles mention that there should be one channel per consumer (or even 1 for consume and 1 for publish), but that makes things slower than the larger figure above..",0.0,0.0,1.0,0.0
appdynamics,"I'm at a loss, so any help is appreciated, more details can be provided.",0.124,0.338,0.538,0.6096
appdynamics,"Java: 7
Spring Core: 3.2.2
Rabbit AMQP: 3.1.2
Spring Rabbit: 1.3.5
Spring AMQP: 1.3.5",0.0,0.0,1.0,0.0
appdynamics,"EDIT: I'm using a ConnectionFactory in Spring XML config (defaults to a CachingConnectionFactory I believe) that I set the channel cache size on, and then set the factory into the listener container, dunno if that helps..",0.0,0.0,1.0,0.0
appdynamics,"I have an Spring+Hibernate+Tomcat+MySql application in production, I'm running into a problem.",0.231,0.0,0.769,-0.4019
appdynamics,"I think the application is not closing it's jdbc connections, and when it reaches its limits (currently 200), the application stop responding, and I have to restart tomcat.",0.08,0.044,0.876,-0.25
appdynamics,Do I need to close this connections somewhere ?,0.0,0.0,1.0,0.0
appdynamics,Here is my Datasource:,0.0,0.0,1.0,0.0
appdynamics,"And here is an image of the appdynamics monitoring the connections, from 3 days until now",0.0,0.0,1.0,0.0
appdynamics,Here is a excerpt of the error I get on the catalina.out log file:,0.197,0.0,0.803,-0.4019
appdynamics,type Exception report,0.0,0.0,1.0,0.0
appdynamics,"message Request processing failed; nested exception is
  org.hibernate.exception.JDBCConnectionException: Cannot open
  connection",0.248,0.0,0.752,-0.5106
appdynamics,description  The server encountered an internal error that prevented it from fulfilling this request.,0.171,0.07,0.759,-0.3818
appdynamics,exception,0.0,0.0,1.0,0.0
appdynamics,"org.springframework.web.util.NestedServletException: Request
  processing failed; nested exception is
  org.hibernate.exception.JDBCConnectionException: Cannot open
  connection
  org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:932)
  org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:816)
  javax.servlet.http.",0.202,0.0,0.798,-0.5106
appdynamics,.,0.0,0.0,0.0,0.0
appdynamics,.,0.0,0.0,0.0,0.0
appdynamics,.,0.0,0.0,0.0,0.0
appdynamics,root cause,0.0,0.0,1.0,0.0
appdynamics,"org.hibernate.exception.JDBCConnectionException: Cannot open
  connection
  org.hibernate.exception.SQLStateConverter.convert(SQLStateConverter.java:99)
  org.hibernate.exception.JDBCExceptionHelper.convert(JDBCExceptionHelper.java:66)
  org.hibernate.exception.JDBCExceptionHelper.convert(JDBCExceptionHelper.java:52)
  org.hibernate.jdbc.ConnectionManager.",0.0,0.0,1.0,0.0
appdynamics,.,0.0,0.0,0.0,0.0
appdynamics,.,0.0,0.0,0.0,0.0
appdynamics,.,0.0,0.0,0.0,0.0
appdynamics,root cause,0.0,0.0,1.0,0.0
appdynamics,"com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException:
   Too many connections 
  sun.reflect.GeneratedConstructorAccessor67.newInstance(Unknown Source)
  sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
  java.lang.reflect.Constructor.newInstance(Constructor.java:513)
  com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
  com.mysql.jdbc.Util.getInstance(Util.java:381)
  com.mysql.jdbc.SQLError.",0.0,0.0,1.0,0.0
appdynamics,UPDATE,0.0,0.0,1.0,0.0
appdynamics,The Category domain object is mapped like this:,0.0,0.263,0.737,0.3612
appdynamics,"So my guess is, taking mithridas comments into account, that I´m using the hibernate  session´s manually, And I would need to close them with something like this:",0.0,0.091,0.909,0.3612
appdynamics,Or i could implement @PersistenceContext.,0.0,0.0,1.0,0.0
appdynamics,Would anyone direct me to this implementations so I can evaluate which is best for us to use ?,0.0,0.208,0.792,0.6369
appdynamics,Thank you.,0.0,0.714,0.286,0.3612
appdynamics,"UPDATE2: 
Added more info in answer to James Massey comment:",0.0,0.0,1.0,0.0
appdynamics,"These are my: datasource, sessionFactory, transactionManager, and categoryDAO sessionFactory assignment:",0.0,0.0,1.0,0.0
appdynamics,"I am confused where to set enviroment variables in Ubuntu 12.04
Now I am giving like this {editing 2 files to set path variables }",0.091,0.194,0.714,0.3818
appdynamics,export JAVA_OPTS=&quot;$JAVA_OPTS -Xms1024M -Xmx2048M -XX:MaxPermSize=1024M -XX:PermSize=128M&quot;,0.0,0.0,1.0,0.0
appdynamics,#------------------- PATH SETTINGS ------------------#,0.0,0.0,1.0,0.0
appdynamics,"#-------- Ant Home  
ANT_HOME=/programs/apache-ant-1.8.0",0.0,0.0,1.0,0.0
appdynamics,"#-------- Maven Home  
M2_HOME=/programs/apache-maven-3.2.1",0.0,0.0,1.0,0.0
appdynamics,"# --------- JDK 1.6 Home  
JAVA_HOME=/programs/java/jdk1.6.0_37",0.0,0.0,1.0,0.0
appdynamics,"# ----------JDK 1.7 Home  
#JAVA_HOME=/programs/java/jdk1.7.0_09",0.0,0.0,1.0,0.0
appdynamics,"# ------------- Path Settings  
PATH=$PATH:$JAVA_HOME/bin:$ANT_HOME/bin:$M2_HOME/bin",0.0,0.0,1.0,0.0
appdynamics,"#----------Enabling AppDynamics Viewer---------  
PATH=$PATH:/programs/AppDynamicsLite/LiteViewer",0.0,0.0,1.0,0.0
appdynamics,"I am getting no errors as all paths are set and i can use JAVA, JAVAC, ANT &amp; MAVEN
I am not prefixing export command to set paths in .profile
Only heap settings are put in .bashrc",0.126,0.0,0.874,-0.5574
appdynamics,"but i havenot used ANT_OPTS and MAVEN_OPTS
Hence i am confused whether they are needed or not",0.174,0.0,0.826,-0.4497
appdynamics,I am using a Java web application called AR System.,0.0,0.0,1.0,0.0
appdynamics,"After installing the PHP-Java Bridge, I started seeing  java.lang.OutOfMemoryError: PermGen space error  in the Tomcat logs.",0.162,0.0,0.838,-0.4019
appdynamics,"(I see in Windows Task Manager that there are 6 PHP-CGI.exe processes, all similar in memory footprint, give or take 5 MB).",0.0,0.0,1.0,0.0
appdynamics,"It would occur every other day or so and then shortened to every day, sometimes twice a day.",0.0,0.0,1.0,0.0
appdynamics,"Consequently, the application hangs and I have to restart it.",0.0,0.0,1.0,0.0
appdynamics,And I added a Windows Task to restart Tomcat during non-peak hours to give me some cushion.,0.0,0.0,1.0,0.0
appdynamics,I suspected a memory leak and started doing some research.,0.417,0.0,0.583,-0.5106
appdynamics,"Normally, Tomcat sits at around 300-350 MB.",0.0,0.0,1.0,0.0
appdynamics,"With the PHP-Java Bridge, memory jumped up significantly.",0.0,0.0,1.0,0.0
appdynamics,"In fact, the error has occurred anywhere from 450-600 MB.",0.231,0.0,0.769,-0.4019
appdynamics,"I learned that default PermGen is 64MB and PermGen should be set to 1/4, up to 1/3 of Tomcat memory (sorry, I don't recall the link).",0.0,0.0,1.0,0.0
appdynamics,"Tomcat is running under Windows Services at this point, and I added the following to its properties:",0.0,0.0,1.0,0.0
appdynamics,I enforced GC on PermGen memory and increased the size from the default 64 MB size to 128-256 MB.,0.0,0.11,0.89,0.2732
appdynamics,"Memory went up all the way to 800-850 MB, slowly, but it wasn't hanging during peak hours, albeit I still had Tomcat intentionally restart during non-peak hours, via a Windows Task.",0.0,0.0,1.0,0.0
appdynamics,"If I take off the restart, it  MIGHT  eventually hang but I haven't tried it.",0.0,0.0,1.0,0.0
appdynamics,I still suspected a memory leak.,0.683,0.0,0.317,-0.5106
appdynamics,"I installed a trial version of AppDynamics to monitor the application, its memory, and run leak detection.",0.146,0.0,0.854,-0.34
appdynamics,"Additionally, to use tools like VisualVM and Memory Analyzer (MAT), I disabled The Tomcat Windows service and ran Tomcat from the Windows Command Line, via catalina.bat.",0.0,0.094,0.906,0.3612
appdynamics,"I appended Java Options to the file; I made sure Tomcat memory was 1024 MB, Perm Gen was 128/256 MB, and ensured PHP-Java Bridge and AppDynamics was running.",0.0,0.084,0.916,0.3182
appdynamics,"As of right now, PermGen is holding at 163 MB used, and AppDynamic's Automatic Leak Detection did not detect any leaks with any Java Collections.",0.091,0.0,0.909,-0.34
appdynamics,"I fired up MAT, created a heap dump and analyzed for leaks.",0.408,0.132,0.461,-0.6369
appdynamics,"When I ran it yesterday, it found three possible suspects:",0.231,0.0,0.769,-0.34
appdynamics,"When I ran it today, it found 2 possible suspects:",0.255,0.0,0.745,-0.34
appdynamics,"So, with MAT and AppDynamics, it appears that no memory leaks were detected for classes directly related to the PHP-Java Bridge JAR files.",0.091,0.0,0.909,-0.296
appdynamics,"I haven't tried using Plumbr, but I can't find the free beta version.",0.262,0.0,0.738,-0.5504
appdynamics,"The free version detects leaks, but you have to pay to see it.",0.108,0.146,0.746,0.1406
appdynamics,"Again, I don't have a source link at this time, but I recall reading that Tomcat 5.x  can  have performance and memory leak issues.",0.134,0.0,0.866,-0.4767
appdynamics,"Of course, that doesn't mean everybody will have those issues, just a select number.",0.0,0.098,0.902,0.0772
appdynamics,I know Tomcat 6 and Tomcat 7 redesigned their memory management or how they structure memory.,0.0,0.0,1.0,0.0
appdynamics,"I also did speak with someone from BMC, the maker of AR System, and they said the current version of AR System I'm using could suffer from performance and memory issues.",0.108,0.0,0.892,-0.5423
appdynamics,"But, again, none of this was a problem before the PHP-Java Bridge.",0.262,0.0,0.738,-0.5499
appdynamics,It was only after I installed it that this PermGen memory issue started.,0.0,0.0,1.0,0.0
appdynamics,"Since the tools above did not report any leaks, does that mean there are no leaks and PHP-Java Bridge just needed more than 64 MB PermGen memory?",0.078,0.0,0.922,-0.296
appdynamics,"Or, is there an inherit problem with my version of Tomcat and installing the PHP-Java Bridge just broke the proverbial camel's back?",0.216,0.0,0.784,-0.6705
appdynamics,Upgrading to a newer version of AR System and Tomcat is not an option.,0.0,0.0,1.0,0.0
appdynamics,"If there is a leak, I can uninstall the PHP-Java Bridge or continue trying to find a leak and fix it.",0.231,0.0,0.769,-0.5859
appdynamics,Any help would be appreciated.,0.0,0.667,0.333,0.7184
appdynamics,Thank you.,0.0,0.714,0.286,0.3612
appdynamics,Update 1,0.0,0.0,1.0,0.0
appdynamics,"With MAT, I looked at the thread overview and stacks and you can see below that the PHP-Java Bridge contributes about 2/3 of the total heap memory of Tomcat.",0.0,0.0,1.0,0.0
appdynamics,That's a lot of memory!,0.0,0.0,1.0,0.0
appdynamics,"I think there is a leak, I do.",0.375,0.0,0.625,-0.34
appdynamics,I can't find any information on the PHP-Java Bridge having inherit memory leak issues.,0.167,0.0,0.833,-0.34
appdynamics,"But, to me, it appears that the problem is not that Tomcat is leaking.",0.215,0.0,0.785,-0.5499
appdynamics,Ideas?,0.0,0.0,1.0,0.0
appdynamics,"AppDynamics couldn't find any leaks, even when I manually added classes that were suspected in MAT.",0.119,0.0,0.881,-0.2263
appdynamics,What I'm wondering is perhaps the PermGen error is a symptom of that case where the program has no leak and needs more PermGen memory allotted.,0.249,0.0,0.751,-0.743
appdynamics,"It would be helpful to know if the PHP-Java Bridge is designed to eat a lot of memory, this much memory; maybe it's optimized for 64-bit, since the current setup is a 32-bit Java Web application.",0.0,0.153,0.847,0.7003
appdynamics,"If I knew that this bridge needs a lot of memory, I would say OK, fine, and go from there.",0.0,0.24,0.76,0.5766
appdynamics,But it certainly appears as if there is a memory leak somewhere in the chain.,0.17,0.17,0.659,0.0
appdynamics,Update 2,0.0,0.0,1.0,0.0
appdynamics,I've been running Plumbr now for 2 hours and almost 10 minutes.,0.0,0.0,1.0,0.0
appdynamics,I see that Tomcat memory is shooting up to 960 MB and probably will continue to climb.,0.0,0.0,1.0,0.0
appdynamics,"For those familiar with the program, the Java web application has been analyzed 3 times.",0.0,0.0,1.0,0.0
appdynamics,"So far, no leaks have been reported.",0.292,0.0,0.708,-0.3566
appdynamics,"If it stays this way, then the two conclusions I've arrived at are a) there are no leaks or b) there is a leak and, somehow, both AppDynamics and Plumbr missed it.",0.195,0.0,0.805,-0.7003
appdynamics,"If there are truly no leaks with this set of applications working together, then it must be that the Bridge uses a lot of memory and needs more PermGen memory than Tomcat's default, 64 MB -- at the very least, for 32-bit Java web applications.",0.047,0.062,0.892,0.1779
appdynamics,I've been reading up on  AppDynamics  Lite all morning and absolutely love it!,0.0,0.285,0.715,0.6989
appdynamics,"It's pretty nice to be able to drop a JAR into your web app (WAR), deploy it and have it automatically run perf tests on your app.",0.068,0.193,0.74,0.5994
appdynamics,I was wondering if anything similar exist for full-fledged profiling?,0.0,0.0,1.0,0.0
appdynamics,"Something similar to, say, VisualVM, but that deploys as a JAR and that can be packaged inside a WAR?",0.287,0.0,0.713,-0.8151
appdynamics,Online searches didn't turn up much but then again I might not be searching for the right thing.,0.0,0.0,1.0,0.0
appdynamics,"I call this an ""intra-JVM profiler"" because its profiling the same Java process its running inside of (like AppDynamics).",0.0,0.0,1.0,0.0
appdynamics,I love open source but this is not a mandatory requisite.,0.122,0.238,0.64,0.3109
appdynamics,Thanks in advance for any pointers/recommendations!,0.0,0.39,0.61,0.4926
appdynamics,I have integrated my loopback application with appDynamics library and all the middlewares are not firing from then on.,0.0,0.107,0.893,0.2584
appdynamics,Used the debug command - DEBUG=* npm start to get the logs and found these logs coming up.,0.0,0.0,1.0,0.0
appdynamics,All the middlewares declared through middleware.json and imperatively through app.middleware() command are having this error message -  No matching layer found,0.228,0.0,0.772,-0.6514
appdynamics,Any idea what's going wrong here - Which loopback package is responsible for express routing implementation,0.168,0.125,0.707,-0.2023
appdynamics,"Hello good morning community, I am somewhat confused, I am integrating the Cisco AppDynamics tool, when performing the integration as mentioned in the documentation and when running the project it throws the following error.",0.129,0.079,0.792,-0.204
appdynamics,Event Log:,0.0,0.0,1.0,0.0
appdynamics,build.gradel:,0.0,0.0,1.0,0.0
appdynamics,build.gradel(:app),0.0,0.0,1.0,0.0
appdynamics,dependencies {,0.0,0.0,1.0,0.0
appdynamics,},0.0,0.0,0.0,0.0
appdynamics,I am trying to build a docker image and push it to AWS ECS.,0.0,0.0,1.0,0.0
appdynamics,I am currently integrating an API with appdynamics.,0.0,0.0,1.0,0.0
appdynamics,I need to issue a command to pip to start a proxy before the app opens.,0.0,0.0,1.0,0.0
appdynamics,When running locally I do this with  pyagent proxy start  and it works fine.,0.0,0.13,0.87,0.2023
appdynamics,The problem comes when using that command with docker.,0.252,0.0,0.748,-0.4019
appdynamics,When I attempt to use that command during the build and push process I get the following error stack.,0.144,0.0,0.856,-0.4019
appdynamics,I run the app locally from mac and install the package to a pipfile on a mac.,0.0,0.0,1.0,0.0
appdynamics,I know that darwin corresponds to macOS.,0.0,0.0,1.0,0.0
appdynamics,Is this an issue because docker runs Linux or another issue I missing.,0.167,0.0,0.833,-0.296
appdynamics,The markers the error traces says it is ignoring appear in the pipfile.lock.,0.329,0.0,0.671,-0.6597
appdynamics,I would like some aid in understanding what the error means and any help is appreciated!,0.12,0.391,0.489,0.7263
appdynamics,Thank you!,0.0,0.736,0.264,0.4199
appdynamics,"I need to suppress AppDynamics alerts on every Sunday between 10ma to 3pm and remaining all the time, they should run.",0.0,0.0,1.0,0.0
appdynamics,"To achieve this, i need to write a croj expression to satisfy the condition of ""run all the time except every Sunday 10am to 3pm"".",0.0,0.12,0.88,0.4588
appdynamics,what could be the cron expression for this ?,0.0,0.0,1.0,0.0
appdynamics,"Servers: Weblogic 12.1.1 (being upgraded soon, yes); Database: Oracle 12c",0.0,0.0,1.0,0.0
appdynamics,Our (very old) JSP monitoring page does  SELECT 1 FROM DUAL  to check for the database being up and a very basic check on response time.,0.0,0.0,1.0,0.0
appdynamics,"We also have (newer) AppDynamics monitoring on our servers, which includes monitoring the monitoring page.",0.0,0.0,1.0,0.0
appdynamics,"Lately we have been experiencing overall application slowness, and the monitoring page has been reflecting that.",0.0,0.0,1.0,0.0
appdynamics,"In particular  SELECT 1 FROM DUAL  query which we expect to be very consistent has been intermittently slow...as in 2+ seconds just for that query as reported by AppDynamics, when a normal response time is 50ms.",0.0,0.0,1.0,0.0
appdynamics,"Mostly our focus has been on the network because that seems the most likely, but we haven't found anything.",0.0,0.0,1.0,0.0
appdynamics,What conditions on the database side could cause this?,0.0,0.0,1.0,0.0
appdynamics,"The database is run by a separate team, and they're helpful but they're responsible for a large number of applications, so we get the best effect if we can ask for specific, measurable statistics.",0.0,0.302,0.698,0.9022
appdynamics,"I have a requirement to upload zip file to appDynamics, i need to use the httpsrequest plugin for that from my jenkins pipeline",0.0,0.0,1.0,0.0
appdynamics,upload request for appdynamics :,0.0,0.0,1.0,0.0
appdynamics,we are using a shell to execute the above request now but I am trying to find out how to sent multiple zip files using  httpsRequest plugin,0.0,0.0,1.0,0.0
appdynamics,I am trying to assist in setting up AppDynamics with an Angular 2 app that is hosted in IIS.,0.0,0.0,1.0,0.0
appdynamics,The app is already up and running.,0.0,0.0,1.0,0.0
appdynamics,"There is a part I am having trouble on, the instructions for that part say say:",0.172,0.0,0.828,-0.4019
appdynamics,"1) From the root directory of your Node.js application, run this command:
    npm install appdynamics@4.3.5
   For every Node.js application you are instrumenting, insert the following call in the application source code at the first line of the main module (such as the server.js file), before any other require statements:",0.0,0.0,1.0,0.0
appdynamics,2) Restart you application,0.0,0.0,1.0,0.0
appdynamics,"I did step 1 locally in the console, but I don't know what to do for step 2.",0.0,0.0,1.0,0.0
appdynamics,"If I add that script to the page I get ""The Reference error: require is not defined"".",0.162,0.0,0.838,-0.4019
appdynamics,I learned that that function is not meant to run on the browser.,0.0,0.0,1.0,0.0
appdynamics,"It's meant to be run server-side, but I do not see node js or any server.js files on our dev web server.",0.0,0.0,1.0,0.0
appdynamics,Does anyone have any suggestions on where to put that snippet.,0.0,0.0,1.0,0.0
appdynamics,Will it even work with the current setup?,0.0,0.0,1.0,0.0
appdynamics,I am using supervisor to manage gunicorn process.,0.0,0.0,1.0,0.0
appdynamics,"[program:Test app]
command = /env/bin/pyagent run -c /etc/appdynamics.cfg -- /env/bin/gunicorn app:app --bind 0.0.0.0:8000 --worker-class sanic.worker.GunicornWorker
directory = /projects/app_dir/
autorestart=true",0.0,0.0,1.0,0.0
appdynamics,Appdynamics versions; pip freeze,0.0,0.286,0.714,0.0516
appdynamics,Appdynamics.cfg,0.0,0.0,1.0,0.0
appdynamics,"When i start the procees, i can see that agent is loaded properly and proxy is started as well.",0.0,0.116,0.884,0.2732
appdynamics,But the problem is i dont see any data reported to controller when i generate load to my app.,0.182,0.0,0.818,-0.5499
appdynamics,(using wrk to generate load),0.0,0.0,1.0,0.0
appdynamics,Agent and proxy  Logs does not have any info about app data/metrics.,0.0,0.0,1.0,0.0
appdynamics,Would really appreciate if someone can help me find out  the issue?,0.0,0.363,0.637,0.6901
appdynamics,"Thanks,
Manivasagan",0.0,0.744,0.256,0.4404
appdynamics,I want to Share the  AppDynamic's Dashboards  in an external website under an &lt; iframe   so that the reports(statistics) can be visible without logging into the  AppDynamic  tool on an external website.,0.0,0.108,0.892,0.3612
appdynamics,Requirements :,0.0,0.0,1.0,0.0
appdynamics,"I tried doing this  ""Sharing a Custom Dashboard""",0.0,0.394,0.606,0.5023
appdynamics,"By clicking the ""copy shared URL"" I got the AppDynamics' particular dashboard's URL",0.0,0.179,0.821,0.34
appdynamics,"Whenever I run that URL on  Chrome , it gives the following  error  :",0.213,0.0,0.787,-0.4019
appdynamics,"and when I try on  IE  (Internet Explorer) , it throws this  error  :",0.238,0.0,0.762,-0.481
appdynamics,On Other tools like  Splunk  and  Sitecatalyst  there is a concept of Sharing the reports by embedded URLs.,0.0,0.261,0.739,0.6486
appdynamics,Not sure how  AppDynamic  works,0.329,0.0,0.671,-0.2411
appdynamics,Thanks.,0.0,1.0,0.0,0.4404
appdynamics,"I have a generic question here and I have just started using Openshift enterprise and Origin but I would like to know the details on Cloudforms UI, I know that CloudForms UI can do a lot of things including managing Openshift instances but I would like to know the following in terms of managing Openshift instance, can CloudForms be able to do the following :",0.0,0.106,0.894,0.7579
appdynamics,What I am trying to find here is to see if CloudForms can provide an end to end Openshift solution.,0.0,0.113,0.887,0.3182
appdynamics,"The end user must only have his/her code ready, rest everything could be within the UI.",0.0,0.143,0.857,0.3612
appdynamics,Kindly let me know what all are possible and what all are not.,0.0,0.211,0.789,0.4939
appdynamics,I need advice on the API's which can be used to do end to end monitoring on the Cloud instance.,0.0,0.0,1.0,0.0
appdynamics,"To give the complete picture, I have a model(R/SQL/Scala) running on C3 instance and i want to do end to end monitoring from data fetch to relevant output from the model.",0.0,0.046,0.954,0.0772
appdynamics,The end to end monitoring also needs to be displayed on a dashboard.,0.0,0.0,1.0,0.0
appdynamics,The instance i am using is Linux.,0.0,0.0,1.0,0.0
appdynamics,"When i googled for this, i cam across lots of technologies like Zabbix, AppDynamics etc.",0.0,0.172,0.828,0.3612
appdynamics,which are more sort of products.,0.0,0.0,1.0,0.0
appdynamics,Is there any existing API which is platform independent and can be integrated with multiple technologies like R/Scala etc?,0.0,0.122,0.878,0.3612
appdynamics,"Also,If I am choosing the existing API do i need to be dependent on the developers for implementing that in my model?",0.0,0.0,1.0,0.0
appdynamics,Or Shall i start from the scratch with REST/SOAP web service?,0.0,0.0,1.0,0.0
appdynamics,Any help in this regard will be very much appreciated.,0.0,0.44,0.56,0.7414
appdynamics,I have a dropwizard 0.7.0 service.,0.0,0.0,1.0,0.0
appdynamics,Occasionally (1 in 5000 requests) the service will spend 60 seconds writing its response.,0.0,0.0,1.0,0.0
appdynamics,AppDynamics is showing that 60 seconds are being spent inside com.sun.jersey.spi.container.servlet.WebComponent$Writer:write:300.,0.0,0.0,1.0,0.0
appdynamics,After 60 seconds we get the following exception:,0.0,0.0,1.0,0.0
appdynamics,We also see the following in the logs that shows that the connection is idle for at least 30 seconds:,0.0,0.0,1.0,0.0
appdynamics,What could cause such behaviour?,0.0,0.0,1.0,0.0
appdynamics,A couple of other observations:,0.0,0.0,1.0,0.0
appdynamics,"UPDATED Q : I have tried to run this sample eCommerce app on android studio 1.2.1, build 141.1903!",0.0,0.0,1.0,0.0
appdynamics,[enter image description here][1]..,0.0,0.0,1.0,0.0
appdynamics,https://github.com/Appdynamics/ECommerce-Android,0.0,0.0,1.0,0.0
appdynamics,"and i did what is instructed to run it
but it keeps asking to upgrade gradle to gradle 2.0 or advance.",0.0,0.0,1.0,0.0
appdynamics,(after i downloaded the new version 2.4).,0.0,0.0,1.0,0.0
appdynamics,how to upgrade or integrate gradle in android studio?,0.0,0.0,1.0,0.0
appdynamics,what other problems related to it I have to solve?,0.235,0.157,0.609,-0.2263
appdynamics,error I'm getting,0.574,0.0,0.426,-0.4019
appdynamics,Error:,1.0,0.0,0.0,-0.4019
appdynamics,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0.,0.0,0.0,1.0,0.0
appdynamics,"Searched in the following locations:
    file //ANDRO1/gradle/m2repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom",0.0,0.0,1.0,0.0
appdynamics,file://ANDRO1/gradle/m2repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar,0.0,0.0,1.0,0.0
appdynamics,https // repo1.maven.org/maven2/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom,0.0,0.0,1.0,0.0
appdynamics,https // repo1.maven.org/maven2/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar,0.0,0.0,1.0,0.0
appdynamics,file /c/ Users/Piyush/.m2/repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom,0.0,0.0,1.0,0.0
appdynamics,file /C /Users/Piyush/.m2/repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar,0.0,0.0,1.0,0.0
appdynamics,"Required by:
        ECommerce-Android-master:app:unspecified",0.0,0.0,1.0,0.0
appdynamics,"We are doing a load test of an application, and after some time AppDynamics reports  ""PS Old gen"" at 100% in red .",0.0,0.0,1.0,0.0
appdynamics,Full GC is running every 10 minutes.,0.0,0.0,1.0,0.0
appdynamics,"Memory's ""Current Utilization"" varies between  70-90% , it goes like this for hours and never fails with OOM.",0.0,0.244,0.756,0.5903
appdynamics,"I thought that once old gen utilization is above certain level, GC will try to free/compact old gen area and if nothing is freed it would either fail with OOM and start crazy full GC cycles just before it.",0.184,0.047,0.768,-0.7234
appdynamics,However I don't see any of these.,0.0,0.0,1.0,0.0
appdynamics,The application runs fine with 100% old gen utilization.,0.0,0.184,0.816,0.2023
appdynamics,"We are using Oracle Java 7u14 (64b, 4 cpu cores, 10gb RAM) and JVM is configured with",0.0,0.0,1.0,0.0
appdynamics,Thank you!,0.0,0.736,0.264,0.4199
appdynamics,I'm currently testing platforms that provide a monitoring service for nodejs application.,0.0,0.0,1.0,0.0
appdynamics,So I found (for now) StrongLoop and AppDynamics (recently acquire nodetime).,0.0,0.0,1.0,0.0
appdynamics,Actually I'm testing StrongLoop service.,0.0,0.0,1.0,0.0
appdynamics,"I have followed all the steps describe in the documentation but I can't see any data on the dashboard, only the StrongLoop Demo App.",0.0,0.0,1.0,0.0
appdynamics,Here is all the steps :,0.0,0.0,1.0,0.0
appdynamics,Any idea ?,0.0,0.0,1.0,0.0
appdynamics,Tank you.,0.0,0.0,1.0,0.0
appdynamics,I am looking for something to do for Azure Worker Roles what New Relic and AppDynamics can do for Azure Web Roles.,0.0,0.0,1.0,0.0
appdynamics,I have tried both solutions for my background workers with little success.,0.0,0.362,0.638,0.6258
appdynamics,Am I missing something in configuration for either of these services or is there another service out there that can do what I need?,0.095,0.0,0.905,-0.296
appdynamics,"I am open to hosting my own service if there is an option I run ""locally"".",0.0,0.0,1.0,0.0
appdynamics,What I'm looking for:,0.0,0.0,1.0,0.0
appdynamics,Nice to have:,0.0,0.583,0.417,0.4215
appdynamics,I am looking for an open-source application/tool/technology which can show times of distributed request across distributed system.,0.0,0.0,1.0,0.0
appdynamics,"I have found some wonderful stuff like  AppDynamics , but they are all commercial.",0.0,0.291,0.709,0.4767
appdynamics,"I don't need such a wide functionality, but simple request tracking.",0.0,0.0,1.0,0.0
appdynamics,"I have also had a look on  this list , but I have some difficulties to understand it.",0.177,0.0,0.823,-0.4215
appdynamics,Could you recommend some solutions if you are experienced with APM?,0.0,0.318,0.682,0.4939
appdynamics,I have installed Liferay-Tomcat 6.0.6 on one of my Linux machine having 4GB of RAM and it uses MySQL installed on a different machine.,0.0,0.0,1.0,0.0
appdynamics,The liferay runs really slow even for 10 concurrent users.,0.0,0.0,1.0,0.0
appdynamics,I have attached the screen shot taken from the AppDynamics which shows EhCache and C3PO both are responding slow at times.,0.0,0.0,1.0,0.0
appdynamics,Are there any special config required for EhCache or C3PO?,0.0,0.231,0.769,0.4019
appdynamics,?,0.0,0.0,0.0,0.0
appdynamics,I am currently running with default configurations.,0.0,0.0,1.0,0.0
appdynamics,As I instrument my React native application with appdynamics the react native application gets the runtime error,0.153,0.0,0.847,-0.4019
appdynamics,"'null is not an object (evaluating
'InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY')'",0.0,0.0,1.0,0.0
appdynamics,As I integrate it remains fine with the integration but as soon as I instument the app stops running.,0.104,0.077,0.82,-0.128
appdynamics,After integration I have used,0.0,0.0,1.0,0.0
appdynamics,and,0.0,0.0,1.0,0.0
appdynamics,on the top of the file.,0.0,0.265,0.735,0.2023
appdynamics,Also did all the steps of manual link for android .,0.0,0.0,1.0,0.0
appdynamics,Is there something I am missing here?,0.306,0.0,0.694,-0.296
appdynamics,Is there an option to create a pdf/bi report from the azure app insights data?,0.0,0.139,0.861,0.2732
appdynamics,"After running performance tests , going to individual tabs(failure/performance/availability etc) is time taking exercise.",0.0,0.0,1.0,0.0
appdynamics,Would like to know if there is a plug in/custom controls available like in &quot;CA-Wily&quot; or Appdynamics where you an export all monitoring data to a pdf and share with relevant stakeholders.,0.0,0.211,0.789,0.7351
appdynamics,Our application is built on .NET 4.0 with Oracle 12c Version 1 as database.,0.0,0.0,1.0,0.0
appdynamics,I was assigned to find bottlenecks in the application from the database side.,0.0,0.0,1.0,0.0
appdynamics,I am using Appdynamics to find slowest database calls.,0.0,0.0,1.0,0.0
appdynamics,I have found that in most cases database queries are executed within few seconds but System.Threading.WaitHandle.WaitOneNative took over 20 seconds.,0.0,0.0,1.0,0.0
appdynamics,I do not understand what can be the reason for this.,0.0,0.0,1.0,0.0
appdynamics,We don't have a dedicated front end engineer to look at this.,0.199,0.0,0.801,-0.357
appdynamics,Any help will be appreciated.,0.0,0.667,0.333,0.7184
appdynamics,I have come to know about opentracing and is even working on a POC with Jaeger and Spring.,0.0,0.0,1.0,0.0
appdynamics,We have around 25+ micro services in production.,0.0,0.0,1.0,0.0
appdynamics,I have read about it but is a bit confused as how it can be really used.,0.174,0.0,0.826,-0.4497
appdynamics,I'm thinking to use it as a troubleshooting tool to identify the root cause of a failure in the application.,0.157,0.081,0.762,-0.3818
appdynamics,"For this, we can search for httpStatus codes, custom tags, traceIds and application logs in JaegerUI.",0.0,0.0,1.0,0.0
appdynamics,"Also, we can find areas of bottlenecks/slowness by monitoring the traces.",0.0,0.0,1.0,0.0
appdynamics,What are the other usages?,0.0,0.0,1.0,0.0
appdynamics,Jaeger has a request sampler and I think we should not sample every request in Prod as it may have adverse impact.,0.116,0.0,0.884,-0.3612
appdynamics,Is this true?,0.0,0.583,0.417,0.4215
appdynamics,"If yes, then why and what can be the impact on the application?",0.0,0.184,0.816,0.4019
appdynamics,I guess it can't be really used for troubleshooting in this case as we won't have data on every request.,0.0,0.098,0.902,0.2415
appdynamics,What sampling configuration is recommended for Prod?,0.0,0.231,0.769,0.2023
appdynamics,"Also, how a tool like Jaeger is different from APM tools and where does it fit in?",0.0,0.263,0.737,0.6124
appdynamics,I mean you can do something similar with APM tools as well.,0.0,0.174,0.826,0.2732
appdynamics,"For e.g., one can drill through a service's transaction and jump to corresponding request to other service in AppDynamics.",0.0,0.0,1.0,0.0
appdynamics,Alerts can be put on slow transactions.,0.0,0.0,1.0,0.0
appdynamics,"One can also capture request headers/body so that they can be searched upon, etc.",0.0,0.0,1.0,0.0
appdynamics,"I have a weburl (Appdynamics) which allows SSO, I want to use this website as the data source in Power BI.",0.0,0.071,0.929,0.0772
appdynamics,"If I let the credentials be the default windows one, Power BI still doesn't move past the login page.",0.0,0.0,1.0,0.0
appdynamics,We generally enter the account and get the message on Appdynamics,0.0,0.0,1.0,0.0
appdynamics,and it directly logs us in.,0.0,0.0,1.0,0.0
appdynamics,How to achieve this in Power BI?,0.0,0.0,1.0,0.0
appdynamics,"In our project, we are getting AppDynamics logs(application logs) and machine logs and sometime the the size of the logs increase which eats out the disk size.",0.0,0.081,0.919,0.3182
appdynamics,What I am trying to do it is to get the content between two dates like 10 Nov and 13 Nov and delete the rest.,0.0,0.098,0.902,0.3612
appdynamics,"Since we are working in windows environment, this needs to be done in powershell.",0.0,0.0,1.0,0.0
appdynamics,It is easier to handle such things in linux but I am not good at powershell scripting.,0.164,0.1,0.736,-0.298
appdynamics,Below is the code snippet.,0.0,0.0,1.0,0.0
appdynamics,Code Snippet with file paths,0.0,0.0,1.0,0.0
appdynamics,The ERROR i get while executing the script.,0.364,0.0,0.636,-0.5319
appdynamics,"The powershell and windows version:
Name: Windows PowerShell ISE Host - Version : 5.1.14409.1018
Name: Microsoft Windows Server 2012 R2 Standard 64bit",0.0,0.0,1.0,0.0
appdynamics,your help will be highly obliged.,0.0,0.351,0.649,0.4019
appdynamics,"Best regards,",0.0,0.808,0.192,0.6369
appdynamics,I'm trying to create a user using Appdynamics' Configuration API,0.0,0.208,0.792,0.2732
appdynamics,I'm trying to use this curl command but am not sure what the parameters after --user are.,0.132,0.0,0.868,-0.3491
appdynamics,"This is AppDynamics specific, a better understanding of tenancy or which my users are, or where i can create users would be helpful.",0.0,0.302,0.698,0.7783
appdynamics,Also should I be using the pem key to communicate with my controller host.,0.0,0.0,1.0,0.0
appdynamics,"Here's the link to the Documentation page i'm referring to;
 https://docs.appdynamics.com/display/PRO44/Configuration+API#ConfigurationAPI-CreateandModifyAppDynamicsUsers",0.0,0.0,1.0,0.0
appdynamics,Hi I am developing a plugin to wrap around AppDynamics.,0.0,0.0,1.0,0.0
appdynamics,I need to do the following from this documentation  https://docs.appdynamics.com/display/PRO45/Upload+the+dSYM+File,0.0,0.0,1.0,0.0
appdynamics,"however since the platform files are regenerated each time, can I do this through hooks?",0.0,0.0,1.0,0.0
appdynamics,The  xcode_build_dsym_upload.sh  is part of the podFile and when I try with hooks (which i think is before the xCode build) I'm getting a  no file or directory found  error.,0.164,0.0,0.836,-0.5994
appdynamics,Any ideas?,0.0,0.0,1.0,0.0
appdynamics,I have a website that requires a custom header to access.,0.0,0.0,1.0,0.0
appdynamics,"How do I configure JMeter to only send this custom header to the main site URL/http request sampler, and not send it to any embedded resources such as appdynamics or googleapis?",0.0,0.0,1.0,0.0
appdynamics,"Right now, I have several ""HTTP Request"" samplers trying to act like a browser by using HTTP Defaults and checking the ""Retrieve All Embedded Resources"" box.",0.0,0.098,0.902,0.3612
appdynamics,"The request URL is in the form of "" https://example.com/path/ .""",0.0,0.0,1.0,0.0
appdynamics,This part needs the custom header to access.,0.0,0.0,1.0,0.0
appdynamics,"When retrieving embedded resources (like fonts.googleapis.com), the custom header should not be sent.",0.0,0.0,1.0,0.0
appdynamics,Any ideas on how I can get this configured?,0.0,0.0,1.0,0.0
appdynamics,My organization asked our team to use this new tool AppDynamics for better performance testing results and reports.,0.0,0.146,0.854,0.4404
appdynamics,"For that I have to attach javaagent with running jvm,  on their community this step",0.0,0.0,1.0,0.0
appdynamics,However when I run the same I get following result on cmd (Using windows-8 64 bit),0.0,0.0,1.0,0.0
appdynamics,"java.lang.reflect.InvocationTargetException
Caused by: java.io.IOException: no such process 
Exception in thread ""main"" java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
Caused by: java.lang.reflect.InvocationTargetException
Caused by: java.io.IOException: no such process",0.18,0.0,0.82,-0.5267
appdynamics,This is  the link  of their documentation.,0.0,0.0,1.0,0.0
appdynamics,The .NET web applications we build all integrate with a third party application through a WCF service.,0.0,0.162,0.838,0.4019
appdynamics,Every time a page loads a number of WCF service calls are made to retrieve data that are used to populate some user controls.,0.0,0.058,0.942,0.0772
appdynamics,Through AppDynamics I can tell that there could be up to 8 WCF calls to load a given page.,0.0,0.0,1.0,0.0
appdynamics,AppDynamics tells us that the WCF calls cost up to 85% of the load time.,0.0,0.0,1.0,0.0
appdynamics,This is a serious impact on developer productivity.,0.178,0.0,0.822,-0.0772
appdynamics,Is there a way to intercept all the outbound WCF calls from our .NET web application and stub them with fake data so that pages will not break and load faster?,0.097,0.0,0.903,-0.4767
appdynamics,The pages do not need these data to run in development environment.,0.0,0.0,1.0,0.0
appdynamics,Thanks for your input!,0.0,0.516,0.484,0.4926
appdynamics,John,0.0,0.0,1.0,0.0
appdynamics,"I am trying to obtain statistics for an app which is hosted on my Cloud Foundry Pivotal without using any 3rd party applications like ""AppDynamics"" (or others).",0.0,0.178,0.822,0.6369
appdynamics,"Specifically, I want to find out the  'Requests per second' and 'Response Time'.",0.0,0.106,0.894,0.0772
appdynamics,"I know that it is possible to access memory, disk space and cpu utilization by an app because Pivotal provides these statistics.",0.0,0.0,1.0,0.0
appdynamics,So does Pivotal also provide 'Requests per second' and 'Response Time'?,0.0,0.0,1.0,0.0
appdynamics,I used some good monitoring tools like javamelody and appDynamics to montior apache-tomcat 5.25 servers performance and get some useful statistics for the deployed web application (running on java 1.5 VM).,0.0,0.235,0.765,0.8074
appdynamics,Actually I need to monitor apache-tomcat server version 4.1.24 running on java 1.4.2_12-b03 VM.,0.0,0.0,1.0,0.0
appdynamics,I could not find some useful tools to monitor tomcat version 4 and java 4 web applications similair to javamelody or any other +java5 monitoring tools.,0.099,0.0,0.901,-0.3412
appdynamics,Any idea abt some useful monitoring tools for apache-tomcat 4/Java 1.4?,0.0,0.225,0.775,0.4404
appdynamics,Thanks,0.0,1.0,0.0,0.4404
appdynamics,I am new to Istio service mesh.,0.0,0.0,1.0,0.0
appdynamics,I have to integrate/configure appdynamics in istio.,0.0,0.0,1.0,0.0
appdynamics,I have no clue how to do that.,0.268,0.0,0.732,-0.296
appdynamics,Anything related to this would help.,0.0,0.351,0.649,0.4019
appdynamics,Any example or related links or video...anything.,0.0,0.0,1.0,0.0
appdynamics,I am new to jenkins.,0.0,0.0,1.0,0.0
appdynamics,I am trying to moniter performance test for my project.,0.0,0.0,1.0,0.0
appdynamics,I have my scripts in Jmeter.,0.0,0.0,1.0,0.0
appdynamics,I have created paarmeterize job in jenkins as shown.,0.0,0.222,0.778,0.25
appdynamics,"Threads: 1
RampUp: 1
Loop: 40.",0.0,0.0,1.0,0.0
appdynamics,I am using backend listner to check data in Grafana and Appdynamics.,0.0,0.0,1.0,0.0
appdynamics,"Now when i start the build the scripts run only once, but i am expecting script must run for 40 times (With build success).",0.0,0.0,1.0,0.0
appdynamics,"But when i run it through jmeter, scripts run for 40 times succesfully.",0.0,0.0,1.0,0.0
appdynamics,(Some issue with jenkins i suppose),0.0,0.0,1.0,0.0
appdynamics,Please suggest how can i resolve the issue in jenkins as my project requeiremnt needs it to run script from jenkins.,0.0,0.214,0.786,0.5994
appdynamics,Thank you in advance!,0.0,0.482,0.518,0.4199
appdynamics,I'm trying to create a live connection from our AppDynamics data to PowerBI for reporting purposes.,0.0,0.13,0.87,0.2732
appdynamics,An example of a command I would need to run to get AppD data is below.,0.0,0.0,1.0,0.0
appdynamics,Is it possible to run arbitrary commands like this in PowerBI to return JSON data to PowerBI?,0.0,0.135,0.865,0.3612
appdynamics,"We have a system where Chef has deployed a monitoring agent, AppDynamics, as a specific user - lets call that user sysXYZ for the sake of this post.",0.0,0.0,1.0,0.0
appdynamics,"The AppDynamics agents create a daily log file, all with sysXYZ user ownership.",0.0,0.16,0.84,0.2732
appdynamics,"Tomcat, also being run as sysXYZ user, hosts the application that is being monitored by the AppDynamics agent.",0.0,0.0,1.0,0.0
appdynamics,"Every day, the Tomcat instance is restarted (project has their reasons) and the start-up process includes a step for renaming yesterdays AppDynamics logs.",0.0,0.0,1.0,0.0
appdynamics,"However, this is prevented as a permissions issue.",0.0,0.155,0.845,0.0258
appdynamics,Tomcat running as user sysXYZ cannot amend files owned by user sysXYZ but created by something that is not Tomcat.,0.0,0.116,0.884,0.3612
appdynamics,I get thet SELinux is meant to prevent unexpected access - say a malicious actor has been introduced - and I am fine with that concept.,0.0,0.132,0.868,0.2263
appdynamics,What can we do here to allow the Tomcat instance to rename the files appropriately each time it is restarted?,0.0,0.091,0.909,0.2263
appdynamics,"Requirement  : To display conditional prompts, based on the previous selection ( &quot;Analytics&quot;, in this case ).",0.0,0.0,1.0,0.0
appdynamics,"Problem  : For any selection, it is still displaying the prompt to add Google Analytics's Tracking Id even if i select AppDynamics or none.",0.114,0.0,0.886,-0.4019
appdynamics,Reference  :  Applying Subschemas Conditionally,0.0,0.0,1.0,0.0
appdynamics,"Note : For Simplicity, i have removed &quot;allOf&quot;,&quot;anyOf&quot; etc.",0.0,0.0,1.0,0.0
appdynamics,"and code for any other analytics tool, still it was not working.",0.0,0.0,1.0,0.0
appdynamics,Code  (Schema.json):,0.0,0.0,1.0,0.0
appdynamics,Read many articles on this but still couldn't figure it out.,0.0,0.0,1.0,0.0
appdynamics,Is there any way to add these conditional prompts?,0.0,0.0,1.0,0.0
appdynamics, The above query from Hibernate takes only few 100 milliseconds to execute.,0.0,0.0,1.0,0.0
appdynamics,But the list() call which executes the above query and returns a list takes more than a minute (sometimes   2mins) to execute.,0.0,0.0,1.0,0.0
appdynamics,"I am not sure what is causing this.I was able to track down the query timing from AppDynamics, but could not drill down into why the list() call takes more than a minute 
  The number of records returned is around 8-9k (may be more sometimes).",0.034,0.033,0.933,-0.008
appdynamics,The table has around 800k records and the c3 column referred in the query is indexed.,0.0,0.0,1.0,0.0
appdynamics,I have set the query.setMaxresults value to 50k.,0.0,0.286,0.714,0.34
appdynamics,The number of columns in the actual query is 24.,0.0,0.126,0.874,0.0772
appdynamics,I reduced the column in the query here for simplicity,0.0,0.0,1.0,0.0
appdynamics,The list() method does some processing on the returned results which is time consuming.,0.0,0.0,1.0,0.0
appdynamics,Did someone face this issue before?,0.0,0.0,1.0,0.0
appdynamics,Any help is appreciated.,0.0,0.75,0.25,0.7184
appdynamics,"Hibernate version: 2.0, JDK - 1.8, DB - Oracle 11.2.0.4",0.0,0.0,1.0,0.0
appdynamics,Thanks,0.0,1.0,0.0,0.4404
appdynamics,After updating to Spring Boot (1.5.20.RELEASE) - I'm getting the following error in my latest environment (in AWS) but in local it is working fine.,0.071,0.084,0.845,0.09
appdynamics,ERROR StatusLogger No log4j2 configuration file found.,0.53,0.0,0.47,-0.6841
appdynamics,Using default configuration: logging only errors to the console.,0.231,0.0,0.769,-0.34
appdynamics,pom.xml,0.0,0.0,1.0,0.0
appdynamics,Please suggest.,0.0,0.697,0.303,0.3182
appdynamics,This is working fine in my local environment.,0.0,0.205,0.795,0.2023
appdynamics,"Full Agent Registration Info Resolver using node name [b196dc6a-64de-469b-b70d-78e99a12b504]
Install Directory resolved to[/opt/appdynamics]
ERROR StatusLogger No log4j2 configuration file found.",0.225,0.136,0.639,-0.4995
appdynamics,Using default configuration: logging only errors to the console.,0.231,0.0,0.769,-0.34
appdynamics,"I'm trying to upload a multi-line file to a appdynamics controller, using the ansible uri module.",0.0,0.0,1.0,0.0
appdynamics,Any advice?,0.0,0.0,1.0,0.0
appdynamics,"As you'll notice in the snippets, I'm brand new to Ansible...",0.0,0.0,1.0,0.0
appdynamics,"It works fine with shell: curl, but I can't seem to get the output correct for a more structured play.",0.0,0.235,0.765,0.6017
appdynamics,Expected outcome - I'm able to successfully import the health rule to AppD,0.0,0.225,0.775,0.4939
appdynamics,"Actual - I get a 200 response, and nothing happens.",0.0,0.0,1.0,0.0
appdynamics,Response:,0.0,0.0,1.0,0.0
appdynamics,The AppDynamics java agent requires the JBoss Domain.xml and Host.xml files to be modified in order to run.,0.0,0.0,1.0,0.0
appdynamics,"To make AppDynamics work, I must add a ""property"" element with two attributes.",0.0,0.0,1.0,0.0
appdynamics,"&lt;property name=""jboss.modules.system.pkgs"" value=""com.singularity""/&gt;",0.0,0.0,1.0,0.0
appdynamics,"I have also tried to add the element and attributes int he same code block, but received a ansible error stating the element and attributes are exclusive:",0.188,0.06,0.752,-0.5789
appdynamics,The issue I am running into is this: At the same path there is a duplicate property with different attributes that must remain unchanged,0.0,0.0,1.0,0.0
appdynamics,"&lt;property name=""java.net.preferIPv4Stack"" value=""true""/&gt;",0.0,0.0,1.0,0.0
appdynamics,"When using Ansible, I always seem to overwrite ALL the ""property"" elements.",0.0,0.0,1.0,0.0
appdynamics,I have tried adding the property element with the attributes on the same like  (as if it were the property name) but receive a python error:,0.13,0.064,0.806,-0.4215
appdynamics,The latest version of the Ansible code I am using is as follows:,0.0,0.0,1.0,0.0
appdynamics,I am at a loss on how to deal with duplicated element names with different attributes via ansible.,0.133,0.0,0.867,-0.3182
appdynamics,In the end I would need to end up with  this:,0.0,0.0,1.0,0.0
appdynamics,Any help would be great appreciated.,0.0,0.771,0.229,0.8779
appdynamics,"I know the answer is staring m in the face, but I just don't see it",0.0,0.0,1.0,0.0
appdynamics,And added point would also to ensure I don't have identical elements with identical attributes as well.,0.0,0.251,0.749,0.5719
appdynamics,"Thanks, in advance.",0.0,0.592,0.408,0.4404
appdynamics,I am using Aurora and .NET 3.5 framework in my application which is internally using .NET mysqlconnector 6.9.3 version.,0.0,0.0,1.0,0.0
appdynamics,"When I enable my general_log, I see below logs.",0.0,0.0,1.0,0.0
appdynamics,I am looking forward a way that I can disable the same or Is this how mysql connector designed.,0.0,0.0,1.0,0.0
appdynamics,Any thoughts around this!,0.0,0.0,1.0,0.0
appdynamics,"I use appdynamics to monitor the queries and I see that whenever there is a procedure call, there is a INFORMATION_SCHEMA calls as well.",0.0,0.1,0.9,0.2732
appdynamics,Please have a look at the below logs.,0.0,0.277,0.723,0.3182
appdynamics,"this link  suggested to set  innodb_stats_on_metadata  to  0 , but didn't help me.",0.224,0.0,0.776,-0.438
appdynamics,"At my new assignment, I need to understand a mid level Java application.",0.0,0.0,1.0,0.0
appdynamics,"To understand the flow faster, I had this idea that if I could see at runtime functions are being called, which function finally responded, then I could really get the whole map in my mind.",0.0,0.0,1.0,0.0
appdynamics,I've worked with tools like AppDynamics which tells the latency/DB calls etc.,0.0,0.185,0.815,0.3612
appdynamics,But what I am looking after is something which will tell me the flow at Runtime.,0.0,0.0,1.0,0.0
appdynamics,"Like,  
 Controller.getStudent() -&gt; Service.getStudent() -&gt; Repository.getStudent() -&gt; ....",0.0,0.263,0.737,0.3612
appdynamics,I am wondering if there are any tools/techniques as such.,0.0,0.0,1.0,0.0
appdynamics,Like recording a stacktrace in debug mode.,0.0,0.333,0.667,0.3612
appdynamics,I can imagine a tool doing  Thread.currentThread().getStackTrace() .,0.0,0.0,1.0,0.0
appdynamics,Does anybody have some idea regarding how can do this?,0.0,0.0,1.0,0.0
appdynamics,(I'm using Springboot and Jboss),0.0,0.0,1.0,0.0
appdynamics,"Edit: I'm not really looking after logging, debugging etc.",0.0,0.0,1.0,0.0
appdynamics,I feel there could be something which could tell what functions are being executed.,0.0,0.0,1.0,0.0
appdynamics,And determine if there are any limitations.,0.0,0.0,1.0,0.0
appdynamics,I had to configure AppDynamics alerts in the past for Java applications I worked for.,0.0,0.0,1.0,0.0
appdynamics,"I also heard of Nagios, but I am not very sure how that works.",0.201,0.0,0.799,-0.4153
appdynamics,"Now, I need to configure alerts for a FlowForce Server, but I don't believe it can be integrated with AppDynamics or Nagios.",0.0,0.0,1.0,0.0
appdynamics,"I saw FlowForce allow me to send some alerts, like when a step of a job fails, but I would like to have some server alerts, like, for instance, if the license expires and, as a result, the server is automatically shut down.",0.043,0.217,0.74,0.7783
appdynamics,I am wondering the best way to achieve it.,0.0,0.375,0.625,0.6369
appdynamics,I am running it on a Windows environment BTW.,0.0,0.0,1.0,0.0
appdynamics,Suggestions are welcome.,0.0,0.6,0.4,0.4588
appdynamics,Thank you in advance!,0.0,0.482,0.518,0.4199
appdynamics,"We have instrumented a .Net 4.0 application, running in IIS8.0 on Windows 2012 with an AppDynamics APM agent (v4.5.2).",0.0,0.0,1.0,0.0
appdynamics,"This server also has McAfee Endpoint Protection installed, v10.6.0.542, with Threat Prevention v10.6.0.672.",0.221,0.0,0.779,-0.5267
appdynamics,"With the APM agent installed, CPU is much higher under typical load (~50-60% with agent vs 10% without, across 2 vCPUs).",0.0,0.0,1.0,0.0
appdynamics,"Under heavy load, the application also starts becoming unstable (requests start queuing and timing out, response times become very high, errors begin occurring).",0.198,0.0,0.802,-0.6344
appdynamics,"We have noticed that with McAfee enabled, it injects two DLLs into the w3wp process - EpMPApi.dll and EpMPThe.dll.",0.0,0.0,1.0,0.0
appdynamics,"We checked this using Process Explorer, looking at loaded DLLs for the process.",0.0,0.0,1.0,0.0
appdynamics,We ran various combinations of performance test:,0.0,0.0,1.0,0.0
appdynamics,"We attempted to add w3wp.exe as an exception in McAfee, however we saw that the DLLs were still loaded, and the high CPU and poor performance still occurred.",0.103,0.0,0.897,-0.4767
appdynamics,"In memory dumps, we consistently saw the application threads waiting on critical sections used by EpMPApi.dll.",0.263,0.0,0.737,-0.6124
appdynamics,It seemed to be related to the application attempting to make socket connections (which it does frequently as all requests involves WCF calls to a downstream system).,0.0,0.0,1.0,0.0
appdynamics,"We would like to understand if/how we can configure McAfee to either exclude w3wp.exe fully, or perhaps stop whatever activity it is doing that the APM agent seems to interact badly with.",0.197,0.066,0.738,-0.6077
appdynamics,We are also working on the APM agent side to understand if we can do anything there to prevent or work around the behaviour.,0.0,0.046,0.954,0.0258
appdynamics,Thanks!,0.0,1.0,0.0,0.4926
appdynamics,Having a nodejs application running inside a docker container orchestrated by Rancher.,0.0,0.0,1.0,0.0
appdynamics,Using appdynamics to monitor the service.,0.0,0.0,1.0,0.0
appdynamics,For that we need to add few configuration parameters in main js file.,0.0,0.0,1.0,0.0
appdynamics,One of the configuration has to be unique for all the containers running for this service.,0.0,0.0,1.0,0.0
appdynamics,"So, i would like to pass the rancher container name as that configuration.",0.0,0.202,0.798,0.4173
appdynamics,We are experiencing sporadic long queries execution in our application.,0.0,0.0,1.0,0.0
appdynamics,The database is Oracle 12.1 RDS.,0.0,0.0,1.0,0.0
appdynamics,"I can see in AppDynamics that query was executed for 13s, I'm executing it myself in Oracle SQL Developer and it never takes longer than 0.1s.",0.0,0.0,1.0,0.0
appdynamics,I can't put query here as there are 3 of them that sporadically give execution time longer than 10s and for each of them I can't reproduce it in SQL Developer.,0.0,0.0,1.0,0.0
appdynamics,"We've started to log Execution plan for long running queries using /*+ gather_plan_statistics */ and it is the same as if query executed for 0.1s except the fact that it doesn't have such a record ""1 SQL Plan Directive used for this statement"".",0.0,0.0,1.0,0.0
appdynamics,I'm looking for any ideas that could help to identify the root cause of this behavior.,0.0,0.153,0.847,0.4019
appdynamics,I have this dictionary below,0.0,0.0,1.0,0.0
appdynamics,"I am trying to get the key for the value  ""art"" .",0.0,0.211,0.789,0.34
appdynamics,"But I am only able to search for the value  ""art""  or the list it is in.",0.0,0.171,0.829,0.4767
appdynamics,Here is my code below,0.0,0.0,1.0,0.0
appdynamics,I get the list for the given value,0.0,0.286,0.714,0.34
appdynamics,"How do I get the associated key in this case  ""avgresptime""  for the given list I found for the value and in turn it's given key which is  ""Appdynamics"" ?",0.0,0.085,0.915,0.34
appdynamics,Is there any better way to do it since my approach involves O(n^3) runnning time?,0.0,0.172,0.828,0.4404
appdynamics,We have a functioning  ASP.NET MVC 5.2.2  website running on  .NET Framework 4.5.1  hosted on a web farm with numerous servers.,0.0,0.0,1.0,0.0
appdynamics,The web application integrates with AppyDynamics.,0.0,0.0,1.0,0.0
appdynamics,The  AppDynamics .NET Agent 4.3.2.1  is installed on each server.,0.0,0.0,1.0,0.0
appdynamics,I noticed that it's an outdated version but we are unable to update yet.,0.0,0.0,1.0,0.0
appdynamics,We use StructureMap for our IoC.,0.0,0.0,1.0,0.0
appdynamics,"Sometimes something happens across the entire farm, perhaps by a:",0.0,0.0,1.0,0.0
appdynamics,There is a chance that one of the web applications on an affected node will not function properly.,0.086,0.108,0.806,0.1027
appdynamics,It will start up but the IoC never executed resulting in the following error:,0.215,0.0,0.785,-0.5499
appdynamics,We can't prove it but we think that the AppDynamics Agent intercepts the IIS application just at the wrong time.,0.179,0.0,0.821,-0.631
appdynamics,The web application starts but the IoC is not configured hence the error above.,0.215,0.0,0.785,-0.5499
appdynamics,My Questions,0.0,0.0,1.0,0.0
appdynamics,Is this a known issue?,0.0,0.0,1.0,0.0
appdynamics,I really did search online,0.0,0.0,1.0,0.0
appdynamics,"If you have experienced this, do you have an idea of what the possible cause(s) can be?",0.0,0.0,1.0,0.0
appdynamics,How can we fix this annoying intermittent error for our customers?,0.393,0.0,0.607,-0.7027
appdynamics,Is it as simple as updating something?,0.0,0.0,1.0,0.0
appdynamics,Like StructureMap &amp; AppDynamics?,0.0,0.455,0.545,0.3612
appdynamics,Edit,0.0,0.0,1.0,0.0
appdynamics,The web application is deployed to a network shared folder which feeds all the web servers in the farm.,0.0,0.124,0.876,0.34
appdynamics,:(,1.0,0.0,0.0,-0.4404
appdynamics,The IoC setup must be working if it is working for all the other nodes.,0.0,0.0,1.0,0.0
appdynamics,This problem seemed to have started since the installation of the AppDynamics Agents.,0.184,0.0,0.816,-0.4019
appdynamics,It only happens if IIS is reset across the farm.,0.0,0.0,1.0,0.0
appdynamics,One of the sites on that node will throw the error I mentioned above.,0.184,0.0,0.816,-0.4019
appdynamics,I am still investigating on my side,0.0,0.0,1.0,0.0
appdynamics,Application_Start()  from the  Global.asax .,0.0,0.0,1.0,0.0
appdynamics,"For a crisp view of the image, click on it.",0.0,0.0,1.0,0.0
appdynamics,"We have an issue with our PROD application.. we are seeing random latencies in the transactions(API based, no sessions).",0.109,0.0,0.891,-0.296
appdynamics,"The transactions just freezes b/w the processing, no pattern (not pausing at a section of transaction, or a time of a day), although one pattern would be - It is more occurring when we do more transactions.",0.096,0.0,0.904,-0.3182
appdynamics,"We have checked storage, network, db and for other hardware related issues but couldn't find any.",0.0,0.0,1.0,0.0
appdynamics,"One issue that i could see is - 
GC (Allocation Failure) being written to catalina.out every 2secs - Is this ok?",0.0,0.128,0.872,0.3612
appdynamics,or is this an indication that there are lot of objects created,0.0,0.154,0.846,0.25
appdynamics,"Here is our tomcat/JVM and server config
Xmx is set to 4096m, xss set to 256k,connector is bio, http11protocol, maxthreads=150, acceptcount=100, compression=on, compressionminsize=2048
Hardware - 32G memory, 8cpu.",0.0,0.0,1.0,0.0
appdynamics,"cpu spikes or memory usage are all normal during 
these events.",0.0,0.0,1.0,0.0
appdynamics,Even GC collection time per min looks normal too.,0.0,0.0,1.0,0.0
appdynamics,We use appdynamics and that doesn't point to any issues as well.,0.0,0.16,0.84,0.2732
appdynamics,we use log4j that writes the logging which points to this latency.,0.0,0.0,1.0,0.0
appdynamics,We use SSL/TLS as well but one way to rule this out is - these latencies are recorded b/w the transactions processing statements which happens after the SSL termination.,0.0,0.054,0.946,0.1406
appdynamics,what kind of logging should we enable to better understand the delay?,0.151,0.191,0.658,0.1531
appdynamics,any other recommendation?,0.0,0.0,1.0,0.0
appdynamics,Are they are any PROD ready tools that will show SSL performance?,0.0,0.185,0.815,0.3612
appdynamics,(apparently APPdynamics do not show this),0.0,0.0,1.0,0.0
appdynamics,Does anyone use compression for API based calls when the requests are less than 2kb?,0.0,0.0,1.0,0.0
appdynamics,really appreciate you guys for helping out here!,0.0,0.478,0.522,0.6689
appdynamics,"What is an ""APM solution""?",0.0,0.0,1.0,0.0
appdynamics,I have this term from this blog post:  http://www.myloadtest.com/new-relic-vs-appdynamics/,0.0,0.0,1.0,0.0
appdynamics,I am looking to setup a monitoring alert in AppDynamics.,0.0,0.239,0.761,0.296
appdynamics,I want to generate an email alert whenever the java process in the Centos machine goes down.,0.0,0.2,0.8,0.3612
appdynamics,How do i set this up?,0.0,0.0,1.0,0.0
appdynamics,I am using AppD version 4.2.,0.0,0.0,1.0,0.0
appdynamics,"I have tried installing the process monitoring plugin, but I am still not getting any data in custom metrics.",0.0,0.0,1.0,0.0
appdynamics,https://www.appdynamics.com/community/exchange/extension/process-monitoring-extension/,0.0,0.0,1.0,0.0
appdynamics,"I had created a python file which will pull the data from app dynamics for which i had imported the package from appd.request import AppDynamicsClient
it is working fine but now i want to convert it into .exe for which i am using py2exe in py2exe i am trying to include the package appdynamics but it is giving error package not found",0.057,0.144,0.799,0.3506
appdynamics,"these are the packages i am importing in main file which i am trying to convert it into .exe file
setup.py file",0.0,0.0,1.0,0.0
appdynamics,),0.0,0.0,0.0,0.0
appdynamics,"ERROR MESSAGE:
ImportError: No module named AppDynamicsClient",0.53,0.0,0.47,-0.6841
appdynamics,We have two osb nodes in cluster.,0.0,0.0,1.0,0.0
appdynamics,"One of node osb1 has less ovearall response time ( 1 sec) when measured in appdynamics,  another  node osb2 has high response(20sec).",0.0,0.0,1.0,0.0
appdynamics,We brought down each of this node and tested individually.,0.0,0.0,1.0,0.0
appdynamics,We see same behavior.,0.0,0.0,1.0,0.0
appdynamics,Any suggestions on what to look into to identify the issue.?,0.0,0.0,1.0,0.0
appdynamics,The osb configuration across both the nodes Is identical and jvm configuration also identical.,0.0,0.0,1.0,0.0
appdynamics,Heap usage is same.,0.0,0.0,1.0,0.0
appdynamics,CPU bit differs.,0.0,0.0,1.0,0.0
appdynamics,So I have a code that gets value from Redis using Jedis Client.,0.0,0.194,0.806,0.34
appdynamics,"But at a time, the Redis was at maximum connection and these exceptions were getting thrown:",0.0,0.0,1.0,0.0
appdynamics,"When I check an AppDynamics analysis of this scenario, I saw some iteration of some calls over a long period of time (1772 seconds).",0.0,0.0,1.0,0.0
appdynamics,The calls are shown in the snips.,0.0,0.0,1.0,0.0
appdynamics,Can anyone explain what's happening here?,0.0,0.0,1.0,0.0
appdynamics,And why Jedis didn't stop after the Timeout setting (500ms)?,0.0,0.173,0.827,0.2235
appdynamics,Can I prevent this from happening for long?,0.0,0.155,0.845,0.0258
appdynamics,This is what my Bean definitions for the Jedis look like:,0.0,0.2,0.8,0.3612
appdynamics,I am trying to develope an android ecommerce UI for demo.,0.0,0.0,1.0,0.0
appdynamics,I downloaded the template files from this source at github  https://github.com/Appdynamics/ECommerce-Android .,0.0,0.0,1.0,0.0
appdynamics,When I run the application in android studio everything builds fine but when I go to open the app on my emulator it crashes with this error message in the logcat,0.129,0.043,0.829,-0.5842
appdynamics,I think the problem is coming from this line of code by I'm not sure how to fix it,0.226,0.0,0.774,-0.5664
appdynamics,Here is the preferences.xml file,0.0,0.0,1.0,0.0
appdynamics,Any input would be greatly appreciated thanks.,0.0,0.575,0.425,0.7764
appdynamics,I have a web service which makes readonly call(bunch of select queries) to get data from DB.,0.0,0.0,1.0,0.0
appdynamics,"But it makes call to the database for the first time only, after that, all entities involved are cached in hibernate second level cache using Ehcache.",0.0,0.0,1.0,0.0
appdynamics,"The problem is even if my request is not making any DB call to get data, the application is always making a commit call to the database which is effecting my response time of the web service.",0.069,0.057,0.874,-0.128
appdynamics,"My web service response time is 90ms, out of which the commit call is contributing 35ms(40%) all the time.",0.0,0.109,0.891,0.296
appdynamics,The datasource is configured as spring bean with data source class com.mchange.v2.c3p0.ComboPooledDataSource as shown below.,0.0,0.0,1.0,0.0
appdynamics,It is making a commit call to DB for every call to the web service which I can see it in the call graph taken from appdynamics.,0.0,0.084,0.916,0.296
appdynamics,The cache hit ratio is 100% for all those entries involved in the request.,0.0,0.0,1.0,0.0
appdynamics,I have my cakePHP application (hosted on centOS 7) monitored by appdynamics APM.,0.0,0.0,1.0,0.0
appdynamics,In their monitoring controller I have breakdown of transactions that take too long.,0.0,0.0,1.0,0.0
appdynamics,I also installed a simple chrome page timing plugin.,0.0,0.0,1.0,0.0
appdynamics,On one of my webpages I got the following results:,0.0,0.0,1.0,0.0
appdynamics,As you can see the page loaded after 157 seconds!,0.0,0.0,1.0,0.0
appdynamics,However in my APM the slowest Transaction recorded has 'execution time' at 2.1 seconds.,0.0,0.0,1.0,0.0
appdynamics,If my server serves the pages in under 2 seconds (and usually in around 0.5 second) where does this terrible 157 seconds come from?,0.141,0.0,0.859,-0.561
appdynamics,How can I monitor the source of that load time?,0.0,0.0,1.0,0.0
appdynamics,Thats another example with firefox plugin for page load times:,0.0,0.0,1.0,0.0
appdynamics,This one took nearly 54 seconds and thats a real load time (saw it mysekf).,0.0,0.0,1.0,0.0
appdynamics,However firefox Firebug under Net tab shows that for that same page:,0.0,0.0,1.0,0.0
appdynamics,6 seconds for that same request?,0.0,0.0,1.0,0.0
appdynamics,Why are they so different and why is firebug incorrect?,0.0,0.0,1.0,0.0
appdynamics,I saw myself that the load took over 50 seconds,0.0,0.0,1.0,0.0
appdynamics,"I downloaded Appdynamics agent for Java, which required adding jvm option for glassfish server 3.1.2, for javaagent.jar, giving path of agent.",0.0,0.112,0.888,0.34
appdynamics,user which application server runs on has full permissions on this folder.,0.0,0.0,1.0,0.0
appdynamics,"After adding this jvm in glassfish server 3.1.2, a restart of server is required.",0.0,0.0,1.0,0.0
appdynamics,"After executing restart, server could not start givng error: error opening ZIP file or JAR manifest missing C:AppServerAgent:javaagent.rar.",0.101,0.208,0.691,0.3217
appdynamics,"I noticed that the option was not added in domain.xml file, but still the option is required for starting the machine.",0.0,0.0,1.0,0.0
appdynamics,"I tried to add it manually in the domain.xml file, but still no success.",0.149,0.268,0.584,0.5023
appdynamics,What can I do?,0.0,0.0,1.0,0.0
appdynamics,now the appication hosted by glassfish doesnt start because the server is down.,0.0,0.0,1.0,0.0
appdynamics,Any help?,0.0,0.73,0.27,0.4019
appdynamics,Thank you in advance.,0.0,0.455,0.545,0.3612
appdynamics,We have installed 2 instance of same application in a same datacenter.,0.0,0.0,1.0,0.0
appdynamics,Both the app is using same oracle DB.,0.0,0.0,1.0,0.0
appdynamics,But we are observing performance issue in one application.,0.0,0.0,1.0,0.0
appdynamics,In AppDynamics we can see the response time of one application is much higher that other.,0.0,0.0,1.0,0.0
appdynamics,Is it possible to intentionally prioritise/configure the DB such a way.,0.0,0.0,1.0,0.0
appdynamics,"If yes, where should I look into the database.",0.0,0.278,0.722,0.4019
appdynamics,Any Idea why this is happening?,0.0,0.0,1.0,0.0
appdynamics,I am totally clueless here.,0.482,0.0,0.518,-0.4201
appdynamics,I have a regular expression for the HttpOnly configuration :,0.0,0.0,1.0,0.0
appdynamics,Header edit Set-Cookie ^(.,0.0,0.0,1.0,0.0
appdynamics,*)$ $1;HttpOnly;Secure,0.0,0.0,1.0,0.0
appdynamics,"For Appdynamics EUM, i want to exclude from this regular expression everything that begin with ""ADRUM"" (without quotes).",0.104,0.071,0.824,-0.1531
appdynamics,How can i proceed ?,0.0,0.0,1.0,0.0
appdynamics,Thanks a lot for your help,0.0,0.651,0.349,0.6808
appdynamics,Best regards !,0.0,0.818,0.182,0.6696
appdynamics,Ludo,0.0,0.0,1.0,0.0
appdynamics,My system setup is like: Application A takes requests from outside world and communicates with the backend REST apis.,0.0,0.128,0.872,0.3612
appdynamics,REST api also communicates with mysql database.,0.0,0.0,1.0,0.0
appdynamics,My requirement is to have a tool from which I can just monitor the resource usage and may be the performance of the web server.,0.0,0.0,1.0,0.0
appdynamics,I want to have graphs for the resource usage which means I need historical data otherwise I would have just used the windows task manager to see the resource usage.,0.0,0.048,0.952,0.0772
appdynamics,This means I do not need any load generator(that will be done by the Application A) just a resource monitor.,0.0,0.0,1.0,0.0
appdynamics,"I googled and found tools like appdynamics, Nagios, munin but not sure if they are what I need.",0.134,0.096,0.77,-0.1761
appdynamics,I haven't done performance testing earlier so there's lot of confusion.,0.196,0.0,0.804,-0.296
appdynamics,Just looking for some guidance.,0.0,0.0,1.0,0.0
appdynamics,Thanks,0.0,1.0,0.0,0.4404
appdynamics,I am new to the AppDynamics.,0.0,0.0,1.0,0.0
appdynamics,We want to integrate AppDynamics in our Angular application (It is intranet Single Page Application).,0.0,0.085,0.915,0.0772
appdynamics,I saw this page but this is about AngularJS not Angular.,0.0,0.0,1.0,0.0
appdynamics,https://www.appdynamics.com/supported-technologies/java/angularjs-monitoring,0.0,0.0,1.0,0.0
appdynamics,We are using Cloud Foundry to host our application.,0.0,0.0,1.0,0.0
appdynamics,There is no issues at backend service.,0.268,0.0,0.732,-0.296
appdynamics,Since it comes with property files where we added AppDynamic entries and then when we push our application it will be integrated with AppDynamics.,0.0,0.0,1.0,0.0
appdynamics,But where as Angular doesn't have that configuration.,0.0,0.0,1.0,0.0
appdynamics,So any suggestion about how to integrate AppDynamics to an Angular Application.,0.0,0.0,1.0,0.0
appdynamics,We are facing the below issue on the bundle which is created by auto build.,0.0,0.125,0.875,0.25
appdynamics,The build agent is configured with 32 bit server and our app server is with 64 bit server.,0.0,0.0,1.0,0.0
appdynamics,Some of our node modules such as Appdynamics are not working with two different OS configurations.,0.0,0.0,1.0,0.0
datadog,There does not seem to be a way to replace no data by zeros when using formulas in datadog.,0.115,0.0,0.885,-0.296
datadog,"I've tried fill zero but it doesn't seem to work
I would simply like my dd agent monitor to display 0 instead of no data when it is down",0.09,0.105,0.805,0.1154
datadog,I want to filter metrics on tag value with a regex.,0.0,0.346,0.654,0.4019
datadog,I can do it in Prometheus but I could not find an equivalent way in Datadog.,0.0,0.0,1.0,0.0
datadog,"For example, to select the following metric whose  status  tag value starts with  2 , I can use the query  http.server.requests.count{status=~""^2..$""}",0.0,0.124,0.876,0.34
datadog,"I have the same metric with the same tags in Datadog too, but couldn't find a way to have the same query.",0.0,0.0,1.0,0.0
datadog,Does anyone know how to integrate Spring boot metrics with datadog?,0.0,0.0,1.0,0.0
datadog,Datadog  is a cloud-scale monitoring service for IT.,0.0,0.0,1.0,0.0
datadog,It allows users to easily visualice their data using a lot of charts and graphs.,0.0,0.156,0.844,0.34
datadog,I have a spring boot application that is using  dropwizard  metrics to populate a lot of information about all methods I annotated with  @Timed .,0.0,0.0,1.0,0.0
datadog,On the other hand I'm deploying my application in heroku so I can't install a Datadog agent.,0.0,0.186,0.814,0.4939
datadog,I want to know if there is a way to automatically integrate spring boot metric system reporting with datadog.,0.0,0.075,0.925,0.0772
datadog,Like this one:,0.0,0.556,0.444,0.3612
datadog,[,0.0,0.0,0.0,0.0
datadog,"If yes, how do I create one?",0.0,0.545,0.455,0.5859
datadog,"From all documentation I've read so far, it doesn't seem to support it.",0.158,0.0,0.842,-0.3089
datadog,But I don't see anyone confirming that it's not supported anywhere.,0.213,0.0,0.787,-0.3491
datadog,I installed dd-agent on Amazon linux ec2.,0.0,0.254,0.746,0.1779
datadog,"If I run my python script directly on the host machine (I used the SDK named ""dogstatsd-python""), all the metrics can be sent to datadog (I logged in to datadoghq.com and saw the metrics there).",0.0,0.0,1.0,0.0
datadog,the script is something like:,0.0,0.385,0.615,0.3612
datadog,"However, I launched a docker container and run the same script from inside the container:",0.0,0.111,0.889,0.128
datadog,"'172.14.0.1' is the IP of the host, which was extracted with command",0.0,0.0,1.0,0.0
datadog,No metrics were sent to datadog at all.....,0.239,0.0,0.761,-0.296
datadog,"I'm guessing that maybe it's due to some configuration issue like ""address binding"".",0.0,0.172,0.828,0.3612
datadog,Maybe the dd-agent I installed on the host can only receive metrics from 'localhost'.,0.0,0.0,1.0,0.0
datadog,Hope someone could help me.,0.0,0.651,0.349,0.6808
datadog,Thank you in advance.,0.0,0.455,0.545,0.3612
datadog,"I have a timeseries graph in a time board that displays data for one metric that has multiple tags called ""page"".",0.0,0.0,1.0,0.0
datadog,"The graph has one line for each tag and I'm running functions on the values, so the query for my data is ""ewma_5(avg:client.load_time{env:prod}) by {page}"".",0.0,0.101,0.899,0.4019
datadog,"This query means the tooltip values when I hover on the graph are things like ""ewma_5(avg:client.load_time{env:prod})"".",0.0,0.286,0.714,0.6369
datadog,"I want to know if there is anyway to use the alias function with the tag value in it, so something like ""alias"": ""{page}""?",0.0,0.245,0.755,0.6682
datadog,"If you use a stack like ELK or datadog for collecting server-side logs and events, how do you integrate mobile-side metrics?",0.0,0.116,0.884,0.3612
datadog,"Is there any way to get these out of crashlytics directly, or does this log aggregation need to be implemented separately?",0.0,0.0,1.0,0.0
datadog,We are running a Kubernetes Cluster in AWS and we are collecting the metrics in DataDog using the dd-agent DaemonSet.,0.0,0.0,1.0,0.0
datadog,"We have a Pod being displayed in our metrics tagged as ""no_pod"" and it is using a lot of resources, Memory/CPU/NetworkTx/NetworkRX.",0.0,0.0,1.0,0.0
datadog,"Is there any explanation to what this pod is, how I can find it, kill it, restart it etc?",0.217,0.0,0.783,-0.6908
datadog,"I have found the dd-agent  source code  which seems to define the ""no_pod"" label but I can't make much sense of why it is there, where it is coming from and how I can find it through kubectl etc.",0.0,0.0,1.0,0.0
datadog,I don't understand the difference between  events  and  metrics  in  DataDog .,0.0,0.0,1.0,0.0
datadog,I'm trying to create a count indicator in my  dashboard  so I can now how many times some type of event has happened.,0.0,0.095,0.905,0.2732
datadog,"There is a lot of events named  some.event.name , but no matter what query I use, it always returns  1 .",0.156,0.064,0.78,-0.3919
datadog,"I've tried with this queries,",0.0,0.0,1.0,0.0
datadog,sum:some.event.name{*},0.0,0.0,1.0,0.0
datadog,count_nonzero(sum:some.event.name{*}),0.0,0.0,1.0,0.0
datadog,count_not_null(sum:some.event.name{*}),0.0,0.0,1.0,0.0
datadog,I've also tried with other aggregation functions  avg|max|min|sum  and allways the result is  1 .,0.0,0.0,1.0,0.0
datadog,Any help will be highly appreaciated.,0.0,0.351,0.649,0.4019
datadog,I am using  exometer  and the  exometer_report_statsd  reporter to report Phoenix endpoints response times to Datadog via dogstatsd.,0.0,0.0,1.0,0.0
datadog,"From a Plug, I am calling  :exometer.update/2  to send the response time to Datadog.",0.0,0.0,1.0,0.0
datadog,E.g:,0.0,0.0,1.0,0.0
datadog,":exometer.update [:app_name, :webapp, :resp_time], 25",0.0,0.0,1.0,0.0
datadog,"Now, I want to have only one metric  app_name.webapp.resp_time  instead of one metric per endpoint and version so I thought of using tags.",0.0,0.061,0.939,0.0772
datadog,"The question is, where should I include the tags?",0.0,0.0,1.0,0.0
datadog,I'm looking to report custom metrics from Lambda functions to Datadog.,0.0,0.0,1.0,0.0
datadog,"I need things like counters, gauges, histograms.",0.0,0.333,0.667,0.3612
datadog,Datadog  documentation  outlines two options for reporting metrics from AWS Lambda:,0.0,0.0,1.0,0.0
datadog,"The fine print in the document above mentions that the printing method only supports counters and gauges, so that's obviously not enough for my usecase (I also need histograms).",0.0,0.137,0.863,0.5106
datadog,"Now, the second method - the API - only supports reporting time series points, which I'm assuming are just gauges (right?",0.0,0.122,0.878,0.3612
datadog,"), according to the  API documentation .",0.0,0.0,1.0,0.0
datadog,"So, is there a way to report metrics to Datadog from my Lambda functions, short of setting up a statsd server in EC2 and calling out to it using dogstatsd?",0.0,0.0,1.0,0.0
datadog,Anyone have any luck getting around this?,0.0,0.333,0.667,0.4588
datadog,New to datadog so I'm just really confused.,0.27,0.0,0.73,-0.3804
datadog,First configuration was fast and simple.,0.0,0.0,1.0,0.0
datadog,"However as I want some app specific charts, it doesn't seem as clear as before for my current scenario.",0.112,0.067,0.821,-0.2225
datadog,"We have one host with several docker machines, one for each service:
- nginx
- varnish
- apache
- database (mysql)",0.0,0.0,1.0,0.0
datadog,We've installed datadog client inside the host and also docker integration and everything works fine.,0.0,0.114,0.886,0.2023
datadog,"What I don't get is how get metrics from apache or varnish, or whatever service that is inside docker.",0.0,0.0,1.0,0.0
datadog,Reading the docs in varnish  for example you have to execute:,0.0,0.0,1.0,0.0
datadog,"However, where should I run the command?",0.0,0.0,1.0,0.0
datadog,"dd-agent user exists only in the host, not in the docker container.",0.0,0.0,1.0,0.0
datadog,Varnish is just the other way round.,0.0,0.0,1.0,0.0
datadog,Should I need to install the agent on each container?,0.0,0.0,1.0,0.0
datadog,It would be considered as another host for pricing?,0.0,0.0,1.0,0.0
datadog,"In mysql case, I just have to configure the agent:",0.0,0.0,1.0,0.0
datadog,"But as my host and the container are in separate routes, should I create a new docker container with the agent so it cat get to db container (changing server field)?",0.0,0.086,0.914,0.3919
datadog,Is it considered again as another host?,0.0,0.0,1.0,0.0
datadog,Is there any way to monitor disk usage of docker containers in DataDog?,0.0,0.0,1.0,0.0
datadog,"I can see in DataDog web all the CPU, RAM and IO metrics for my containers.",0.0,0.0,1.0,0.0
datadog,But I can't see any of disk space related metrics.,0.0,0.0,1.0,0.0
datadog,Their page  https://docs.datadoghq.com/integrations/docker/  says about:,0.0,0.0,1.0,0.0
datadog,I can't find these neither in Dashboards   Docker nor in Metrics   Explorer,0.0,0.0,1.0,0.0
datadog,"I'm new to DataDog, so possibly missing something obvious here.",0.216,0.0,0.784,-0.3566
datadog,I am unable to access datadog agent on my host from a docker container.,0.0,0.0,1.0,0.0
datadog,I am using EC2 container service to host my docker containers.,0.0,0.0,1.0,0.0
datadog,I have already set the option  non_local_traffic : yes  in datadog config.,0.0,0.231,0.769,0.4019
datadog,My config looks like this:,0.0,0.385,0.615,0.3612
datadog,To access the host from the docker instance I use this URL from within the docker container :  http://169.254.169.254/latest/meta-data/local-ipv4/  which is discussed here:  http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html,0.0,0.0,1.0,0.0
datadog,This URL gives me the IP of the host machine which is then passed over to python datadog client running in the docker machine.,0.0,0.0,1.0,0.0
datadog,"I've a metric which has 2 tags (it has more but this is for simplicity),  client  and  rule , and its value of course.",0.0,0.134,0.866,0.4767
datadog,"With it I can see the total count of the values for each client, each rule and each ruleXclient.",0.0,0.137,0.863,0.4019
datadog,"Now I want to create a top list which would tell me the amount of unique clients per rule, so if the metric is reporting that 2 clients (2 values of the tag client) have 4 hits each for a rule (single value for the tag rule) I'd like the top list to show me:",0.0,0.253,0.747,0.891
datadog,2: RuleA,0.0,0.0,1.0,0.0
datadog,Is that even possible?,0.0,0.0,1.0,0.0
datadog,How could I approach this if it isn't?,0.0,0.0,1.0,0.0
datadog,I'm using dogstreams to report the metric.,0.0,0.0,1.0,0.0
datadog,Is it possible to export or download Datadog dashboards via Datadog REST API?,0.0,0.0,1.0,0.0
datadog,Export and update of Datadog Monitors works fine.,0.0,0.205,0.795,0.2023
datadog,I need the same functionality for dashboards.,0.0,0.0,1.0,0.0
datadog,I'm running web apps as Docker containers in Azure App Service.,0.0,0.0,1.0,0.0
datadog,"I'd like to add Datadog agent to each container to, e.g., read the log files in the background and post them to Datadog log management.",0.0,0.094,0.906,0.3612
datadog,This is what I have tried:,0.0,0.0,1.0,0.0
datadog,1) Installing Datadog agent as extension as described in  this post .,0.0,0.0,1.0,0.0
datadog,"This option does not seem to be available for App Service apps, only on VMs.",0.0,0.0,1.0,0.0
datadog,2) Using multi-container apps as described  in this post .,0.0,0.0,1.0,0.0
datadog,"However, we have not found a simple way to integrate this with  Azure DevOps release pipelines .",0.0,0.0,1.0,0.0
datadog,I guess it might be possible to create a custom deployment task wrapping Azure CLI commands?,0.0,0.139,0.861,0.2732
datadog,3) Including Datadog agent into our Dockerfiles by following how Datadog Dockerfiles  are built .,0.0,0.0,1.0,0.0
datadog,The process seems quite complicated and add lots of extra dependencies to our Dockerfile.,0.0,0.0,1.0,0.0
datadog,We'd also not like to inherit our Dockerfiles from Datadog Dockerfile with  FROM datadog/agent .,0.14,0.0,0.86,-0.2755
datadog,I'd assume this must be a pretty standard problem for Azure+Datadog users.,0.181,0.215,0.604,0.128
datadog,Any ideas what's the cleanest option?,0.0,0.0,1.0,0.0
datadog,I have a Spring Boot application and want to configure HTTP request tracing via dependency management without having to deal with setting up the java agent.,0.0,0.053,0.947,0.0772
datadog,Can anyone suggest the best way to do this?,0.0,0.344,0.656,0.6369
datadog,"I have the  micrometer-registry-datadog  dependency added to my pom and can see that there are a lot of undocumented  com.datadoghq  dependencies, but am unsure if any of these will solve my problem.",0.172,0.062,0.766,-0.5927
datadog,"I'm getting all of the JVM metrics, but want some more APM-type metrics now.",0.0,0.1,0.9,0.1154
datadog,Ideally I'd like to use the  @Timed  annotation and various others to get detailed metrics around API calls.,0.0,0.249,0.751,0.6486
datadog,We have an elasticsearch cluster deployed to the Elastic Cloud and would like to send monitoring/health metrics to Datadog.,0.0,0.122,0.878,0.3612
datadog,What is the best way to do that?,0.0,0.375,0.625,0.6369
datadog,"It seems like our options are:
* Installing the datadog agent binary via the plugins upload
* Using metric beat -  logstash -  datadog_metrics output",0.0,0.111,0.889,0.3612
datadog,So I have an ongoing metric of events.,0.0,0.0,1.0,0.0
datadog,They are either tagged as success or fail.,0.265,0.28,0.455,0.0516
datadog,"So I have 3 numbers; failed, completed, total.",0.416,0.0,0.584,-0.552
datadog,This is easily illustrated (in Datadog) using a stacked bar graph like so:,0.0,0.329,0.671,0.5994
datadog,So the dark part are the failures.,0.333,0.0,0.667,-0.4588
datadog,"And by looking at the y scale and the dashed red line for scale, this easily tells a human if the rate is a problem and significant.",0.096,0.161,0.743,0.2144
datadog,"Which to mean means that I have a failure rate in excess of 60%, over at least some time (10 minutes?)",0.155,0.0,0.845,-0.5106
datadog,and that there are enough events in this period to consider the rate exceptional.,0.0,0.0,1.0,0.0
datadog,So I am looking for some sort of formula that starts with: failures divided by total (giving me a score between 0 and 1) and then multiplies this somehow again with the total and some thresholds that I decide means that the total is high enough for me to get an automated alert.,0.057,0.042,0.9,-0.2023
datadog,"For extra credit, here is the actual Datadog metric that I am trying to get to work:",0.0,0.148,0.852,0.3818
datadog,"(sum:event{status:fail}.rollup(sum, 300) / sum:event{}.rollup(sum,
  300))",0.0,0.0,1.0,0.0
datadog,And I am watching for 15 minutes and alert of score above 0.75.,0.0,0.167,0.833,0.296
datadog,"But I am not sure about sum, count, avg, rollup or count.",0.196,0.0,0.804,-0.3491
datadog,And ofc this alert will send me mail during the night when the total events goes low enough to were a high failure rate isn't proof of any problem.,0.234,0.072,0.694,-0.6808
datadog,I am trying to create an alert in DataDog that would alert us when disk performance slows down our machines.,0.0,0.289,0.711,0.6705
datadog,"As a business requirement I would say that if the IO is almost saturated (over 90%) for more than 30 minutes, the alert should be triggered.",0.0,0.087,0.913,0.296
datadog,"Here are the current set of metrics that are recorded:
 
sys.cpu.iowait
system.io.avg_q_sz
system.io.avg_rq_sz
system.io.await
system.io.r_await
system.io.r_s
system.io.rkb_s
system.io.rrqm_s
system.io.svctm
system.io.util
system.io.w_await
system.io.w_s
system.io.wkb_s
system.io.wrqm_s",0.0,0.0,1.0,0.0
datadog,"It is possible to use any formulas to combine these, including SUM and AVG values.",0.0,0.162,0.838,0.4019
datadog,I have a time series presenting time values like this one:,0.0,0.426,0.574,0.6369
datadog,"I want to change the y-axis to represent hours instead of milliseconds, i.e.",0.0,0.106,0.894,0.0772
datadog,divide by 3600.,0.0,0.0,1.0,0.0
datadog,Any idea how to do it?,0.0,0.0,1.0,0.0
datadog,I'm running a number of python apps as Replica Sets inside of kubernetes on Google Container Engine (gke).,0.0,0.075,0.925,0.0772
datadog,Along side them I've created the Datadog DaemonSet which launches a dd-agent on each node in my cluster.,0.0,0.111,0.889,0.25
datadog,Now I would like to use that agents dogstatsd for metrics logging from python apps as well as try out the new Datadog APM.,0.0,0.18,0.82,0.5574
datadog,If I just install the ddtrace python package and use it like documented it fills up my logs with,0.0,0.128,0.872,0.3612
datadog,Clearly it don't have magical way to guess how to access port 8126/7777 of the ddagent pods.,0.0,0.144,0.856,0.4019
datadog,Ive tried creating a Service which expose the ports:,0.163,0.224,0.612,0.1531
datadog,but my python pods still don't seem to be able access for example  os.environ['DATADOG_STATSD_PORT_8126_TCP_ADDR']  and  .._PORT .,0.0,0.0,1.0,0.0
datadog,"They are defined and all, I just still get the connection timed out.",0.0,0.0,1.0,0.0
datadog,If I connect to the dd-agent pods and enable tcpdump I also don't see any trafic on ports 8126 etc.,0.0,0.0,1.0,0.0
datadog,The dd-agent DaemonSet is defined like this:,0.0,0.294,0.706,0.3612
datadog,"I'm trying to integrate a Datadog monitor check on sshd process in my terraform codebase, but I'm getting  datadog_monitor.host_is_up2: error updating monitor: API error 400 Bad Request: {""errors"":[""The value provided for parameter 'query' is invalid""]}",0.264,0.069,0.667,-0.8674
datadog,What I did was to copy the monitor's query I created on the Datadog panel and pasted it into the tf file:,0.0,0.095,0.905,0.25
datadog,"ofc the query example  ""avg(last_1h):avg:aws.ec2.cpu{environment:foo,host:foo} by {host} &gt; 2""  works",0.0,0.0,1.0,0.0
datadog,"What's the right way to check via Datadog API or terraform if a specific service, like sshd, is up or not?",0.0,0.116,0.884,0.3612
datadog,I'm trying to use the datadog api but the initialize method keeps giving the error 'INFO No agent or invalid configuration file found'.,0.216,0.105,0.679,-0.5023
datadog,The datadog agent is running:,0.0,0.0,1.0,0.0
datadog,"(PYTHON) daphnepaparis@Daphnes-MBP-2 ~ $ /usr/local/bin/datadog-agent status
Datadog Agent (supervisor) is running all child processes",0.0,0.0,1.0,0.0
datadog,And the configuration file permissions look alright:,0.0,0.25,0.75,0.25
datadog,"(PYTHON) daphnepaparis@Daphnes-MBP-2 ~ $ ls -l ~/.datadog-agent/datadog.conf
lrwxr-xr-x  1 daphnepaparis  staff  35 Mar 22 12:58 /Users/daphnepaparis/.datadog-agent/datadog.conf -  /opt/datadog-agent/etc/datadog.conf",0.0,0.0,1.0,0.0
datadog,Original commands I'm running:,0.0,0.434,0.566,0.3182
datadog,"In [1]: from datadog import initialize, api",0.0,0.0,1.0,0.0
datadog,In [2]: options = {'api_key': '***'},0.0,0.0,1.0,0.0
datadog,In [3]: initialize(**options),0.0,0.0,1.0,0.0
datadog,2017-03-22 13:24:20 INFO No agent or invalid configuration file found,0.196,0.0,0.804,-0.296
datadog,Anyone able to help?,0.0,0.474,0.526,0.4019
datadog,"I was able to follow these instructions carefully and thoroughly  https://docs.datadoghq.com/tracing/setup/python/ ,",0.0,0.143,0.857,0.128
datadog,"I successfully installed DataDog Agent following this guide  https://docs.datadoghq.com/tracing/setup/ ,",0.0,0.314,0.686,0.4939
datadog,"I was also able to install MacOS tracer since it is required for mac user:  https://github.com/DataDog/datadog-trace-agent#run-on-osx ,",0.0,0.0,1.0,0.0
datadog,I enabled apm_config in the configuration file found here:  https://docs.datadoghq.com/agent/faq/agent-configuration-files/?tab=agentv6#agent-main-configuration-file,0.0,0.0,1.0,0.0
datadog,I leave the  env: none  since I only need to run it in on development/debug mode.,0.085,0.0,0.915,-0.0516
datadog,Now Im currently on the step 4:  Instrument your application  guide for Flask and here the steps I took:,0.0,0.0,1.0,0.0
datadog,Add integration for flask:,0.0,0.0,1.0,0.0
datadog,And also my application runs in a docker container and this is what I get from the output log:,0.0,0.0,1.0,0.0
datadog,ERROR:ddtrace.writer:cannot send services to localhost:8126,0.0,0.0,1.0,0.0
datadog,Additional Information,0.0,0.0,1.0,0.0
datadog,On the tracer agent:,0.0,0.0,1.0,0.0
datadog,"According to the  gae_datadog  Github repo , the way to setup datadog in app engine is to clone the repo and add the following in  app.yaml :",0.0,0.0,1.0,0.0
datadog,"However, this doesn't appear to work with their nodejs runtime.",0.0,0.0,1.0,0.0
datadog,Here is my  app.yaml :,0.0,0.0,1.0,0.0
datadog,"It seems like the datadog url handler isn't used at all, because it 404.",0.0,0.161,0.839,0.3612
datadog,"I assume the node.js app takes precedence here, but I don't know how to change that.",0.0,0.0,1.0,0.0
datadog,I have a service that exposes metrics in statsd format and telegraf instance which picks those metrics and sends them to both Prometheus and Datadog (there are two output plugin configurations for both of these).,0.045,0.0,0.955,-0.128
datadog,This works correctly.,0.0,0.0,1.0,0.0
datadog,"However, I have a special requirement where I would need to filter certain metrics that will be sent to Datadog.",0.0,0.242,0.758,0.5859
datadog,My first inclination was to make change in  [[outputs.datadog]]  section of  telegraf.conf .,0.0,0.0,1.0,0.0
datadog,"However, I don't see any specific configuration part where I could, for example, list just metrics that I need to be seen on Datadog.",0.0,0.0,1.0,0.0
datadog,Is there any way to achieve this?,0.0,0.0,1.0,0.0
datadog,Thanks.,0.0,1.0,0.0,0.4404
datadog,I have been working with Datadog log ingestion for about a year now.,0.0,0.0,1.0,0.0
datadog,It's been (mostly) great to work with.,0.0,0.406,0.594,0.6249
datadog,The documentation around running it inside of Kubernetes is a bit lacking though.,0.0,0.0,1.0,0.0
datadog,"Their documentation covers Docker thoroughly, but Kubernetes less so.",0.0,0.0,1.0,0.0
datadog,"When I installed Datadog into our Kubernetes clusters a year ago, there were two ways to do it, you could use a DaemonSet to ensure at least 1 Pod of Datadog runs on every Node.",0.0,0.08,0.92,0.3818
datadog,Or you could install it as a Deployment.,0.0,0.0,1.0,0.0
datadog,I went with the DaemonSet option and used Helm to install it.,0.0,0.0,1.0,0.0
datadog,That worked quite well!,0.0,0.472,0.528,0.3989
datadog,"Then we wanted to start using DogStatsD to ingest metrics about our applications, and it seemed at the time like this required the ""cluster-agent"" to run.",0.0,0.091,0.909,0.3612
datadog,I have serious doubts about this part.,0.467,0.0,0.533,-0.3612
datadog,If I get all of the Datadog-related objects in my cluster I see the DaemonSet ( daemonset.apps/dd-agent-datadog ) and I also see a Deployment ( daemonset..apps/dd-agent-datadog ) on my cluster.,0.0,0.0,1.0,0.0
datadog,Is this right?,0.0,0.0,1.0,0.0
datadog,Do I really need to run both of those things to get log ingestion and metrics?,0.0,0.0,1.0,0.0
datadog,"I  think  this should be possible, but I can't find documented syntax.",0.0,0.0,1.0,0.0
datadog,I would like to construct a DD graph that displays metrics matching tag_one:A or tag_one:B.,0.0,0.172,0.828,0.3612
datadog,Is this possible?,0.0,0.0,1.0,0.0
datadog,"If so, what is the syntax?",0.0,0.0,1.0,0.0
datadog,I want to exclude a path to avoid getting my logs spammed like so:,0.258,0.239,0.503,-0.0772
datadog,"I'm running datadog as a docker agent using the command here:
 https://docs.datadoghq.com/agent/docker/?tab=standard#installation",0.0,0.0,1.0,0.0
datadog,how do I specify files to exclude in the docker run command?,0.16,0.0,0.84,-0.2263
datadog,is it an environment variable?,0.0,0.0,1.0,0.0
datadog,I am using Datadog to monitor my browser console logs.,0.0,0.0,1.0,0.0
datadog,I need different tags in for datadog logs.,0.0,0.0,1.0,0.0
datadog,"The only option I fount is to add attributes to my logger using,",0.0,0.0,1.0,0.0
datadog,"DD_LOGS.addContext('referrer', document.referrer);",0.0,0.0,1.0,0.0
datadog,Is there any way for the frontend client application to have tags in datadog?,0.0,0.0,1.0,0.0
datadog,Or is the attribute and tags are same in Datadog,0.0,0.0,1.0,0.0
datadog,There are a bunch of custom metrics that looks like:,0.0,0.238,0.762,0.3612
datadog,There is a template variables dropdown ( https://docs.datadoghq.com/dashboards/template_variables/ ) on the dashboard that contains all keys.,0.0,0.0,1.0,0.0
datadog,"(Name of the variable is  $key  and values key1, key2..)",0.0,0.231,0.769,0.4019
datadog,The metrics query like this works fine:,0.0,0.462,0.538,0.5106
datadog,But is it possible to parametrize the query with the  $key  variable by concatenation with the part of the name of metric?,0.0,0.0,1.0,0.0
datadog,So it would look like this:,0.0,0.333,0.667,0.3612
datadog,"As I know, it is possible to something like that by using tags but unfortunately, there are no any metrics with the tag &quot;key&quot;.",0.213,0.063,0.723,-0.631
datadog,We are using DataDog and NewRelic to monitor the performance of few DevOps supported systems and we need to provide some uptime reports like:,0.0,0.179,0.821,0.5859
datadog,While we do have URL monitoring configured on DataDog we were not able to find a way to compute the uptime (only to get an alert when the service is down).,0.0,0.071,0.929,0.296
datadog,"NewRelic is also used but it seems that they have an URL monitoring service which works only on publicly accessible sites, making it useless for 9/10 cases.",0.125,0.0,0.875,-0.5719
datadog,"I wanted to add my custom log parser through dogstream, but there was an exception while restarting datadog agent:",0.0,0.0,1.0,0.0
datadog,The parser code:,0.0,0.0,1.0,0.0
datadog,Does anybody know why such thing happend?,0.0,0.0,1.0,0.0
datadog,Any ideas?,0.0,0.0,1.0,0.0
datadog,We are trying to integrate DataDog with our Ruby On Rails app.,0.0,0.0,1.0,0.0
datadog,"Our ROR app will continuously add users, update users and delete users every second.",0.0,0.0,1.0,0.0
datadog,I have integrated Datadog to monitor the no.,0.268,0.0,0.732,-0.296
datadog,"of users added, updated and deleted through the graph provided by Datadog.",0.0,0.0,1.0,0.0
datadog,I installed the datadog agent using the command for Ubuntu Aws instance.,0.0,0.0,1.0,0.0
datadog,I got a free trial for 14 days.,0.0,0.398,0.602,0.5106
datadog,I followed this document for  dogstatd-ruby gem  :  https://github.com/DataDog/dogstatsd-ruby,0.0,0.0,1.0,0.0
datadog,After that i wrote the code in my ruby project like below :,0.0,0.2,0.8,0.3612
datadog,"Here i dont see ""custom.users.updated"" and ""custom.users.added"" graph in the metrics explorer.",0.0,0.0,1.0,0.0
datadog,I would really appreciate if any1 help me out to set the graph for these 2 metrics in Datadog account.,0.0,0.262,0.738,0.6901
datadog,please let me know if i missed anything here.,0.21,0.219,0.571,0.0258
datadog,Is there any default dashboard to monitor Cassandra performance in data dog?,0.0,0.0,1.0,0.0
datadog,https://app.datadoghq.com/account/settings#integrations/cassandra,0.0,0.0,1.0,0.0
datadog,There are lot of metrics listed.,0.0,0.0,1.0,0.0
datadog,How do we construct a monitor?,0.0,0.0,1.0,0.0
datadog,"By default the data dog shows the default system level monitor like CPU, Heap etc... is there anything like it for Cassandra?",0.0,0.2,0.8,0.6124
datadog,Any info would be a great help for me.,0.0,0.531,0.469,0.7783
datadog,"I'm creating my dashboard, and I have the following two metrics:  event.sent  and  event.failed .",0.0,0.155,0.845,0.296
datadog,"Fortunately, I still haven't had any failed events (knock on wood), so this metric does not exist yet on datadog.",0.0,0.131,0.869,0.4023
datadog,But I want to create it so I can add it to my monitors.,0.0,0.291,0.709,0.4767
datadog,How do I manually create this metric?,0.0,0.296,0.704,0.2732
datadog,I've been trying to understand the time aggregation for Datadog monitoring alerts.,0.0,0.0,1.0,0.0
datadog,"The official doc  http://docs.datadoghq.com/guides/monitors/#define-the-conditions 
I understand the idea of time aggregation, but I'm confused about the unit of time as it's not mentioned anywhere.",0.118,0.0,0.882,-0.4497
datadog,Is it aggregating over 1 minute intervals?,0.0,0.0,1.0,0.0
datadog,To rephrase this when I use  sum(last_30m){X}  is it summing the values of  X  for each minute?,0.0,0.162,0.838,0.4019
datadog,What about  sum(last_1h){X} ?,0.0,0.0,1.0,0.0
datadog,Is it still each minute?,0.0,0.0,1.0,0.0
datadog,I am new to Datadog APM.,0.0,0.0,1.0,0.0
datadog,I have read few tutorials but I am unable to find how to to add data in Datadog to create custom dashboard?,0.0,0.122,0.878,0.3919
datadog,I'm looking for a solution to monitor GCP Dataflow pipelines with Datadog to extract the built in metrics as well as Beam custom metrics.,0.0,0.173,0.827,0.5267
datadog,"Currently Datadog offers integration for other GCP services, but not for Dataflow.",0.0,0.0,1.0,0.0
datadog,Has anyone done similar work and can share pointers how to build this as custom solution?,0.0,0.243,0.757,0.5423
datadog,I am new to Datadog and I am trying to implement mute/unmute functions from Datadog on my AWS cloud stack.,0.0,0.0,1.0,0.0
datadog,I want to do that using AWS Lambda Functions.,0.0,0.157,0.843,0.0772
datadog,I am looking for a java based solution.,0.0,0.315,0.685,0.3182
datadog,Is there any Java based sdk provided for the same?,0.0,0.0,1.0,0.0
datadog,I found out that Datadog provides APIs to schedule downtime  here,0.0,0.0,1.0,0.0
datadog,"but the support I can see is either Python, Ruby or Curl.",0.0,0.262,0.738,0.5499
datadog,How can I construct a Java based solution for it?,0.0,0.247,0.753,0.3182
datadog,My application is running in the docker container and it is not able to communicate with dd-trace agent running on host which is ec2,0.0,0.0,1.0,0.0
datadog,I've done all the configurations and still facing  ERROR:ddtrace.writer:cannot send spans to localhost:8126: [Errno 111] Connection refused,0.121,0.0,0.879,-0.296
datadog,Any idea how to fix this?,0.0,0.0,1.0,0.0
datadog,"i'm collect data using Go and want to visualize it, i chose Datadog, but didn't find examples or live projects where Go used for sending metrics to Datadog.",0.0,0.042,0.958,0.0387
datadog,But in offical site says that Go is supported.,0.0,0.269,0.731,0.4497
datadog,"I wanted to ask if anyone has ever saved jmeter test results (sampler names, duration, pass/fail) to Datadog?",0.0,0.149,0.851,0.4215
datadog,Kinda like the backend listener for influx/graphite... but for Datadog.,0.0,0.151,0.849,0.154
datadog,Jmeter-plugins has no such plugin.,0.355,0.0,0.645,-0.296
datadog,"Datadog seems to offer something called ""JMX integration"" but I'm not sure whether that is what I need.",0.132,0.0,0.868,-0.3491
datadog,"I am trying to set up slack monitors with datadog, based on the environment.",0.0,0.0,1.0,0.0
datadog,For e.g.,0.0,0.0,1.0,0.0
datadog,if the environment is production got to slack channel A and if it is uat go to slack channel B and all other environments should go to slack channel C.,0.0,0.0,1.0,0.0
datadog,But I can't find a way to do the last part where all others should go to slack channel B.,0.0,0.0,1.0,0.0
datadog,Looked at the documentation in  https://docs.datadoghq.com/monitors/notifications  and googled but couldn't find anything that can do an else condition.,0.0,0.0,1.0,0.0
datadog,I am utilizing dogstatsd approach to send metrics to datadog using micrometer.,0.0,0.0,1.0,0.0
datadog,I get the normal metrics like counter and gauge but I am not able to generate events.,0.0,0.111,0.889,0.1901
datadog,Is there a way to generate datadog events?,0.0,0.0,1.0,0.0
datadog,I'm trying to send events to my local datadog agent by shell through DataStatsD port.,0.0,0.0,1.0,0.0
datadog,The message is sent without errors but doesn't reach the dashboard.,0.096,0.131,0.774,0.1045
datadog,I use datadog agent in version 6.9 and use datadog documentation:,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/developers/dogstatsd/datagram_shell/#send-metrics-and-events-using-dogstatsd-and-the-shell,0.0,0.0,1.0,0.0
datadog,When I try to send metrics is work fine and I see the metrics in the datadog dashboard but when I send events it's doesn't show in the dashboard.,0.0,0.053,0.947,0.1027
datadog,I also see that when I send event via shell and then check agent status the number of metrics packets go up but the number of events is still 0.,0.0,0.091,0.909,0.1531
datadog,That's the command i run:,0.0,0.0,1.0,0.0
datadog,Edit  When I changed the following configuration properties it's work.,0.0,0.0,1.0,0.0
datadog,"The configuration I changed:
         1) dogstatsd_non_local_traffic: yes
         2) bind_host: localhost",0.0,0.252,0.748,0.4019
datadog,We re trying to eliminate Datadog agents from our infrastructure.,0.0,0.0,1.0,0.0
datadog,I am trying to find a solution to forward the containers standard output logs to be visualised on datadog but without the agents and without changing the dockerfiles because there are hundreds of them.,0.0,0.051,0.949,0.1655
datadog,I was thinking about trying to centralize the logs with rsyslog but I dont know if its a good idea.,0.0,0.194,0.806,0.5927
datadog,Any suggestions ?,0.0,0.0,1.0,0.0
datadog,I am using datadog for monitoring my services on AWS.,0.0,0.0,1.0,0.0
datadog,"On my python application, I use below code to send a data to a metric on Cloudwatch:",0.0,0.0,1.0,0.0
datadog,I can see this data on Cloudwatch -  Metric.,0.0,0.0,1.0,0.0
datadog,But I don't know how I can create a monitor on Datadog to listen on this metric.,0.0,0.169,0.831,0.3919
datadog,"OK, I spent quiet some time figuring out how to configure stuff to have DataDog trace ID in logs but couldn't get it working.",0.0,0.082,0.918,0.2421
datadog,"To be clear what I'm looking for is to see trace IDs in logs message, the same way that adding  spring-cloud-starter-sleuth  to the classpath, automatically configure Slf4j/Logback to show trace IDs in log messages.",0.0,0.073,0.927,0.3818
datadog,Where I've started:,0.0,0.0,1.0,0.0
datadog,What I did so far:,0.0,0.0,1.0,0.0
datadog,Notes:,0.0,0.0,1.0,0.0
datadog,Can someone tell me how exactly I need to configure the application to see trace IDs in log messages?,0.0,0.0,1.0,0.0
datadog,Is there any documentation or samples I can look at?,0.0,0.0,1.0,0.0
datadog,Is there any way to extract the Tags info from DataDog via API for a specific metric?,0.0,0.0,1.0,0.0
datadog,"I need the same info that the  Metrics Explorer  displays (list of hosts and tags), for only one metric.",0.0,0.0,1.0,0.0
datadog,I can retrieve the Tags filtered with a regular expression pattern:,0.0,0.0,1.0,0.0
datadog,And I can get all the Hosts from a Metric with:,0.0,0.0,1.0,0.0
datadog,"Using either the  datadogpy  or the  DataDog API ,  how can I get all the Tags from a Metric?",0.0,0.0,1.0,0.0
datadog,Thanks,0.0,1.0,0.0,0.4404
datadog,I have a unique type of Kubernetes cluster that cannot install the  Kubernetes Datadog agent .,0.0,0.0,1.0,0.0
datadog,I would like to collect the logs of individual docker containers in my Kubernetes pods similar to how the  Docker agent  works.,0.0,0.111,0.889,0.3612
datadog,I am currently collecting docker logs from Kubernetes and then using a script with the  Datadog custom log forwarder  to upload them to Datadog.,0.0,0.0,1.0,0.0
datadog,I was curious if there is a better way to achieve this serverless collection of docker logs from Kubernetes clusters in datadog?,0.0,0.224,0.776,0.6369
datadog,The ideal situation I want is to plug my kubeconfig somewhere and then let Datadog take care of the rest without deploying anything onto my Kubernetes cluster.,0.0,0.256,0.744,0.7845
datadog,Is there an option for that outside of creating a custom script?,0.0,0.18,0.82,0.296
datadog,Is it possible to extract json fields that are nested inside a log?,0.0,0.0,1.0,0.0
datadog,Sample I've been work on:,0.0,0.0,1.0,0.0
datadog,what I wanted to achieve was:,0.0,0.0,1.0,0.0
datadog,"I tried to combine a sample regex ( ""(extract)""\s*:\s*""([^""]+)"",? )",0.0,0.0,1.0,0.0
datadog,"with  example_parser %{data::json}  (using the JSON as a log sample data, for starters) but I haven't managed to get anything working.",0.0,0.0,1.0,0.0
datadog,Thanks in advance!,0.0,0.615,0.385,0.4926
datadog,"Using datadog official docs, I am able to print the K8s  stdout/stderr  logs in DataDog UI, my motive is to print the app logs which are generated by spring boot application at a certain location in my pod.",0.0,0.057,0.943,0.2732
datadog,Configurations done in cluster :,0.0,0.0,1.0,0.0
datadog,Configurations done in App :,0.0,0.0,1.0,0.0
datadog,After doing above configurations I am able to log  stdout/stderr  logs where as I wanted to log application logs in datadog UI,0.0,0.0,1.0,0.0
datadog,If someone has done this please let me know what am I missing here.,0.139,0.166,0.695,0.1091
datadog,"If required, I can share the configurations as well.",0.0,0.417,0.583,0.5106
datadog,Thanks in advance,0.0,0.592,0.408,0.4404
datadog,Question about searching logs in Datadog.,0.0,0.0,1.0,0.0
datadog,Search works on regular strings in the CONTENT portion of the log.,0.0,0.0,1.0,0.0
datadog,"However, if JSON is passed to the CONTENT portion, the JSON elements are automatically parsed into Attributes.",0.0,0.0,1.0,0.0
datadog,But the Attributes are NOT searchable.,0.0,0.0,1.0,0.0
datadog,How do I search for logs by Attribute?,0.0,0.0,1.0,0.0
datadog,"It seems like a step backwards to supply log data in JSON to improve indexing, but then LOSE the ability to search on those elements.",0.149,0.212,0.639,0.0001
datadog,my configuration is:,0.0,0.0,1.0,0.0
datadog,Where:,0.0,0.0,1.0,0.0
datadog,"I'm not getting any errors, but I  am  seeing the subclasses of  ApplicationWorker  in the DataDog dashboard for the APM traces but  not  the  ApplicationJob  subclasses.",0.0,0.059,0.941,0.1326
datadog,"From the documentation I've found this is the correct way to configure  activeJob  tracing when it uses  resque , but I haven't found such great documentation on the subject.",0.146,0.0,0.854,-0.6642
datadog,I'm trying to figure out the difference between the  in-application modifier   as_rate()  and the  rollup function   per_second() .,0.0,0.0,1.0,0.0
datadog,"I want a table with two columns: the left column shows the total number of events submitted to a  Distribution  (in query-speak:  count:METRIC{*} by {tag} ), and the right column shows the average rate of events per second.",0.0,0.073,0.927,0.1531
datadog,"The table visualization applies a sum rollup on left column, and an average rollup on the right column, so that the left column should equal the right column multiplied by the total number of seconds in the selected time period.",0.0,0.033,0.967,0.0772
datadog,From reading the docs I expected either of these queries to work for the right column:,0.0,0.0,1.0,0.0
datadog,count:DISTRIBUTION_METRIC{*} by {tag}.as_rate(),0.0,0.0,1.0,0.0
datadog,per_second(count:DISTRIBUTION_METRIC{*} by {tag}),0.0,0.0,1.0,0.0
datadog,"But, it turns out that these two queries are not the same.",0.0,0.0,1.0,0.0
datadog,as_rate()  is the only one that finds the expected average rate where  left = right * num_seconds .,0.0,0.0,1.0,0.0
datadog,"In fact, the  per_second()  rollup does this extra weird thing where metrics with lower total events have higher average rates.",0.178,0.0,0.822,-0.4404
datadog,Is someone able to clarify why these two functions are not synonymous and what  per_second()  does differently?,0.0,0.0,1.0,0.0
datadog,I have a Postgres dyno on Heroku and I use Datadog.,0.0,0.0,1.0,0.0
datadog,Two postgres dashboards are by default on Datadog: Metrics and Overview.,0.0,0.0,1.0,0.0
datadog,"Metrics is working (CPU usage, memory, I/O,...) but Overview is not (deadlocks, indexes usages)",0.0,0.0,1.0,0.0
datadog,Are Heroku Postgres dyno and Datadog fully compatible?,0.0,0.0,1.0,0.0
datadog,I'm trying to send my ECS Fargate logs to Datadog.,0.0,0.0,1.0,0.0
datadog,To do this I need to pass my Datadog API_KEY as a field in the  logConfiguration  object.,0.0,0.0,1.0,0.0
datadog,I need to secure my API_KEY so I am using AWS Secrets Manager via the  secretOptions  key of the  logConfiguration  object.,0.0,0.118,0.882,0.34
datadog,I'm following the steps from AWS laid out  here .,0.0,0.0,1.0,0.0
datadog,The full steps from the Datadog site can be found  here,0.0,0.0,1.0,0.0
datadog,For some reason I dont see the logs show up in datadog.,0.0,0.0,1.0,0.0
datadog,Here is the log config section of my Terraform code under the  container_definitions  object of the  aws_ecs_task_definition  resource:,0.0,0.0,1.0,0.0
datadog,"If I take out the  secretOptions  and add the apikey in plaintext, the logs show up on the datadog console:",0.0,0.0,1.0,0.0
datadog,I of course cant just send my API_KEY in plaintext.,0.0,0.0,1.0,0.0
datadog,Does the  secretOptions  just not work for Datadog?,0.0,0.0,1.0,0.0
datadog,Any help is appreciated.,0.0,0.75,0.25,0.7184
datadog,I have  datadog agent  running on ubuntu 14.04.,0.0,0.0,1.0,0.0
datadog,And I am trying to monitor page views for the go apps as mentioned in the  link .,0.0,0.0,1.0,0.0
datadog,I have checked everything yaml also it is valid.,0.0,0.0,1.0,0.0
datadog,But still it doesn't even report for upto go_expvar data even after 30 min.,0.0,0.0,1.0,0.0
datadog,I checked in the dashboard it says Last check 29 mins ago.,0.0,0.0,1.0,0.0
datadog,Can anyone tell me how to debug this or reduce this time,0.0,0.0,1.0,0.0
datadog,"I have to set resource limits for my kubernetes apps, and they use the ""milicore"" unity ""m"".",0.0,0.0,1.0,0.0
datadog,"When analyzing my apps in Datadog, I see a unity called M% for CPU usage.",0.0,0.0,1.0,0.0
datadog,How do I convert 1.5M% to m?,0.0,0.0,1.0,0.0
datadog,Kubernetes resources:  http://kubernetes.io/docs/user-guide/compute-resources/,0.0,0.0,1.0,0.0
datadog,Im replacing our existing  NewRelic  java support code with  DataDog  and am wondering about sending error messages.,0.132,0.132,0.735,0.0
datadog,NewRelic has the  .noticeEvent()  call.,0.0,0.0,1.0,0.0
datadog,The  DDog library  Im using has a  .recordEvent()  but doesn't seem to have a way to send a stack trace.,0.0,0.0,1.0,0.0
datadog,Anyone been down this road before?,0.0,0.0,1.0,0.0
datadog,I can send text via the above but I need a bit more info.,0.0,0.0,1.0,0.0
datadog,I am trying to create a dashboard in datadog using the REST API described here:  http://docs.datadoghq.com/api/#timeboards,0.0,0.139,0.861,0.2732
datadog,"Whatever I do, however, I keep getting a 400 response back with a message ""Invalid JSON input"".",0.0,0.0,1.0,0.0
datadog,"I have simplified my json to just a few required fields, and empty ""graphs"" section, and that still doesn't work.",0.096,0.0,0.904,-0.2023
datadog,Does anyone have an idea what could be wrong here?,0.256,0.0,0.744,-0.4767
datadog,"curl -i -X POST 'https://app.datadoghq.com/api/v1/dash?api_key=&lt;key&gt;&amp;application_key=&lt;the_key&gt;' -d '{""dash"":{""title"":""Foo"",""description"":""bar"",""graphs"":[]}}'",0.0,0.0,1.0,0.0
datadog,Response,0.0,0.0,1.0,0.0
datadog,"i am trying to write a php curl for datadog api,but it return internal error.",0.184,0.0,0.816,-0.4019
datadog,this was working in bash script but throwing error while converting in phpcurl.,0.228,0.0,0.772,-0.5499
datadog,can someone help me on this.,0.0,0.351,0.649,0.4019
datadog,I am incrementing a Datadog counter in python:,0.0,0.0,1.0,0.0
datadog,"And have set the metric type to ""count"" and the unit to ""requests per none"" in the metadata for the metric.",0.0,0.0,1.0,0.0
datadog,The code runs in a docker container on a kubernetes node in a Container Engine in Google Cloud...,0.0,0.0,1.0,0.0
datadog,I have docker-dd-agent ( https://github.com/DataDog/docker-dd-agent ) running on each node.,0.0,0.0,1.0,0.0
datadog,I can move the container to any node and it logs around 200 requests per minute.,0.0,0.0,1.0,0.0
datadog,"But as soon as I scale it up and launch a second container, it only logs around 100 requests per minute.",0.0,0.0,1.0,0.0
datadog,"If I scale down to one container again, it spikes to 200 rpm again:",0.0,0.0,1.0,0.0
datadog,What could be causing the requests to drop or get overwritten from other pods?,0.139,0.0,0.861,-0.2732
datadog,"I'm looking for a way to do a ""partial"" update on an existing screenboard/timeboard.",0.0,0.0,1.0,0.0
datadog,"By ""partial"", I mean adding some widget to the existing screenboard/timeboard without wiping out the existing widget that already exist.",0.0,0.0,1.0,0.0
datadog,Consider the following example:,0.0,0.0,1.0,0.0
datadog,Create screenboard:,0.0,0.677,0.323,0.2732
datadog,Update screenboard:,0.0,0.0,1.0,0.0
datadog,"When I run the create and then the update example, the  update_image  widget will overwrite the  create_image  widget and that is the problem I'm trying to avoid.",0.163,0.07,0.767,-0.4215
datadog,I have a  AWS Lambda  function which filters AWS log events from Cloud-trail and give only my AWS ROLE's events.,0.0,0.0,1.0,0.0
datadog,Can I send this records only to Data-dog?,0.0,0.0,1.0,0.0
datadog,Is there an API in which I can pass this filtered events directly?,0.0,0.0,1.0,0.0
datadog,Where does the  userstats.o1.daus  metric take the data from?,0.0,0.0,1.0,0.0
datadog,"I looked in the metrics list and in the app, but I don't seem to find the source of the metric.",0.0,0.0,1.0,0.0
datadog,The application infrastructure relies on:,0.0,0.0,1.0,0.0
datadog,"I have an application that publishes a metric to DataDog with multiple tags, and my DataDog agent has a line that looks like",0.0,0.116,0.884,0.3612
datadog,So my metric (lets call it  ResponseTime ) has a metric in the DataDog viewer for each of those (i.e.,0.0,0.0,1.0,0.0
datadog,ResponseTime.90perentile ).,0.0,0.0,1.0,0.0
datadog,However if you look at this metric carefully it appears to be calculating these percentiles on a short range (not sure what) and for each tuple of the tags that exist.,0.0,0.119,0.881,0.4215
datadog,Ideally what I'd like to get is a 95th percentile of the  ResponseTime  metric over all the tags (maybe I filter it down by 1 or 2 and have a couple of different graphs) but over the last week or so.,0.0,0.097,0.903,0.3919
datadog,Is there an easy way to do this?,0.0,0.293,0.707,0.4404
datadog,"I have configured DD agent on AWS Ubuntu machine and defined CPU Usage, RAM monitors, and metric is correctly reflecting in the dashboard.",0.0,0.0,1.0,0.0
datadog,Inside  /etc/dd-agent/conf.d  in file  process.yaml :,0.0,0.0,1.0,0.0
datadog,"On the same machine, I have a JAR running as a process with name  ecommerce-order-0.0.1-SNAPSHOT.jar  as a process.",0.0,0.0,1.0,0.0
datadog,When I do:,0.0,0.0,1.0,0.0
datadog,I get:,0.0,0.0,1.0,0.0
datadog,But when I do:,0.0,0.0,1.0,0.0
datadog,I get:,0.0,0.0,1.0,0.0
datadog,I want a process monitor who can check if a  JAR  with some name is currently running or not.,0.0,0.08,0.92,0.0772
datadog,What is it I am doing wrong?,0.383,0.0,0.617,-0.4767
datadog,I am looking at some alternatives for APM and I like the extensive list of Datadog integration points.,0.0,0.143,0.857,0.3612
datadog,"However, it seems that I would have to make code changes to explicitly send stats to Datadog.",0.0,0.0,1.0,0.0
datadog,Doesn't Datadog support runtime instrumentation?,0.361,0.0,0.639,-0.3089
datadog,My tech stack is MS .NET/C# and SQL Server backend.,0.0,0.0,1.0,0.0
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,"we use Jenkins 2.60.1 , with Datadog plugin 0.6.1",0.0,0.0,1.0,0.0
datadog,"after first installation all works well , but after some time we stop get events in Datadog.",0.153,0.084,0.763,-0.3071
datadog,restart Jenkins solve the issue.,0.0,0.31,0.69,0.2023
datadog,any idea ?,0.0,0.0,1.0,0.0
datadog,I am new to Datadog and NGiNX.,0.0,0.0,1.0,0.0
datadog,I noticed when I was creating a monitor for some integrations several of the integrations were labeled as misconfigured.,0.0,0.128,0.872,0.296
datadog,My guess is someone clicked the install button but did finish the remaining integration steps.,0.0,0.0,1.0,0.0
datadog,I started to work with NGiNX and quickly hit a roadblock.,0.0,0.0,1.0,0.0
datadog,I verified it is running http status module,0.0,0.0,1.0,0.0
datadog,"The NGiNX install is under a different directory than is usual
and the configuration file is under",0.0,0.0,1.0,0.0
datadog,I created the status.conf file there.,0.0,0.333,0.667,0.25
datadog,When I reload the NGINX I get a failure.,0.398,0.0,0.602,-0.5106
datadog,I don't understand what it means or how to proceed from here.,0.0,0.0,1.0,0.0
datadog,There is a logs directory with nothing in it.,0.0,0.0,1.0,0.0
datadog,ps -ef|grep nginx,0.0,0.0,1.0,0.0
datadog,I think the issue is that our install doesn't seem to be following the same defaults as the instructions and I'm pretty sure I'm not doing this correctly.,0.0,0.18,0.82,0.6705
datadog,If anyone has any insights that would be great!,0.0,0.354,0.646,0.6588
datadog,Chris,0.0,0.0,1.0,0.0
datadog,Is it possible to configure datadog to notify about each new error that got logged?,0.162,0.0,0.838,-0.4019
datadog,I know how to set a threshold for a specified period and how to send the error-count for instance to slack.,0.0,0.0,1.0,0.0
datadog,But I am searching for a possibility to send the actual error rather than the number of errors.,0.315,0.069,0.616,-0.7351
datadog,"I am currently using this  link  for writing a program in Python that will send out curl commands for  POST  ,  PUT , and  DELETE  requests using the Datadog API.",0.0,0.0,1.0,0.0
datadog,"So far, the request seems to be firing as I'd like it to, but it won't take my credentials.",0.083,0.086,0.831,0.0129
datadog,"I'm not entirely sure what a service hook url is, but I believe it may be the culprit.",0.096,0.0,0.904,-0.1505
datadog,Could anyone tell me how to find the following Slack specific elements for this?,0.0,0.0,1.0,0.0
datadog,This is my test script in Python:,0.0,0.0,1.0,0.0
datadog,The results were:,0.0,0.0,1.0,0.0
datadog,I would really appreciate any help in finding this information!,0.0,0.472,0.528,0.739
datadog,"Using datadog docker image, with the following in docker-compos",0.0,0.0,1.0,0.0
datadog,I am getting the following errors continuously,0.324,0.0,0.676,-0.34
datadog,"2018-07-14 16:10:04 UTC | ERROR | (runner.go:277 in work) | Error running check disk: [{""message"": ""[Errno 2] No such file or directory: '/host/proc/filesystems'"", ""traceback"": ""Traceback (most recent call last):\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/checks/base.py\"", line 294, in run\n    self.check(copy.deepcopy(self.instances[0]))\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/disk/disk.py\"", line 43, in check\n    self.collect_metrics_psutil()\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/disk/disk.py\"", line 90, in collect_metrics_psutil\n    for part in psutil.disk_partitions(all=True):\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/ init .py\"", line 1839, in disk_partitions\n    return _psplatform.disk_partitions(all)\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/_pslinux.py\"", line 1000, in disk_partitions\n    with open_text(\""%s/filesystems\"" % get_procfs_path()) as f:\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/_pslinux.py\"", line 194, in open_text\n    return open(fname, \""rt\"", **kwargs)\nIOError: [Errno 2] No such file or directory: '/host/proc/filesystems'\n""}]",0.11,0.0,0.89,-0.8602
datadog,and another,0.0,0.0,1.0,0.0
datadog,"2018-07-14 16:10:04 UTC | WARN | (cgroup.go:510 in
  parseCgroupMountPoints) | No mountPoints were detected, current cgroup
  root is: /host/sys/fs/cgroup/",0.236,0.0,0.764,-0.516
datadog,Any ideas what it means or how to debug it?,0.0,0.0,1.0,0.0
datadog,Expecting to get logs into datadog from other containers sysout so I have all logs in one place.,0.0,0.0,1.0,0.0
datadog,I can see it successfully detects the other containers,0.0,0.314,0.686,0.4939
datadog,"Note the docker image is using version 6 of datadog
Thanks",0.0,0.244,0.756,0.4404
datadog,"I set up datadog and kubernetes to test to out monitoring, although in datadog i can see some logs and metrics, in the agent in kubernetes I have the following errors:",0.082,0.0,0.918,-0.34
datadog,"As the logs state the agent cannot connect to Kubectl, has anyone come across this?",0.0,0.0,1.0,0.0
datadog,I am trying to add new monitor to datadog.,0.0,0.0,1.0,0.0
datadog,I added the metric to my code.,0.0,0.0,1.0,0.0
datadog,And I can see this metric on datadog (goto Metrics -  explorer -  Graph).,0.0,0.0,1.0,0.0
datadog,Now I am trying to create monitoring on datadog that will alert me if the value of metric don't change for three days in a row.,0.0,0.242,0.758,0.6908
datadog,Is it possible to create this kind of monitoring?,0.0,0.208,0.792,0.2732
datadog,Thanks.,0.0,1.0,0.0,0.4404
datadog,"I have a microservice based project with  kafka , which I used for event bus.",0.0,0.0,1.0,0.0
datadog,"I have a business process, which contains multiple microservices.",0.0,0.0,1.0,0.0
datadog,Microservices asynchronous communicate with each other with help kafka.,0.0,0.252,0.748,0.4019
datadog,Each instance of business process has unique  process_id .,0.0,0.0,1.0,0.0
datadog,Let's consider a example of some process:,0.0,0.0,1.0,0.0
datadog,"So, I need to measure execution time bwtween differ steps of this process.",0.0,0.0,1.0,0.0
datadog,"For example, I want to know, duration between steps  3  and  5 , or  2  and  6 .",0.0,0.115,0.885,0.0772
datadog,"So, I don't know, how to measure it.",0.0,0.0,1.0,0.0
datadog,I have only one workaround solution.,0.0,0.365,0.635,0.3182
datadog,"I can have share memory (e.g., redis), where I can store points of time for each stem of process.",0.0,0.121,0.879,0.296
datadog,"In the end of process, I can calculate all metrics and push it datadog.",0.0,0.0,1.0,0.0
datadog,I am using ansible version 2.7 for kubernetes deployment.,0.0,0.0,1.0,0.0
datadog,"For sending logs to datadog on kubernetes one of the way is to configure annotations like below,",0.0,0.135,0.865,0.3612
datadog,this works fine and I could see logs in DataDog.,0.0,0.184,0.816,0.2023
datadog,However I would like to achieve above configuration via ansible deployment on kubernetes for which I have used below code,0.0,0.128,0.872,0.3612
datadog,and datadog.json.j2 looks like below,0.0,0.385,0.615,0.3612
datadog,However the resulting config on deployment is below,0.0,0.0,1.0,0.0
datadog,and this config does not allow datadog agent to parse logs failing with below error,0.39,0.0,0.61,-0.7695
datadog,if I use ansible code as below (using replace),0.0,0.0,1.0,0.0
datadog,it generates deployment config as below,0.0,0.0,1.0,0.0
datadog,"Which also fails,",0.583,0.0,0.417,-0.4215
datadog,"to configure the working config with ansible, I have to either remove leading pipe (|) or three quotes coming when using replace).",0.0,0.0,1.0,0.0
datadog,I would like to have jinja variables substitution in place so that I could configure deployment with desired source and service at deployment time.,0.0,0.187,0.813,0.5574
datadog,kindly suggest,0.0,0.762,0.238,0.4939
datadog,"I have Golang app, it writes logs to Stdout with Logrus.",0.0,0.0,1.0,0.0
datadog,"I was trying to recreate this  https://github.com/DataDog/docker-compose-example  scenario, and replace python app with my app.",0.0,0.0,1.0,0.0
datadog,"But logs aren't coming to Datadog dashboad
This is docker-compose I'm trying to make work",0.0,0.0,1.0,0.0
datadog,"I also tried non-compose, but simple docker container installation for the agent by this  https://docs.datadoghq.com/logs/log_collection/docker/?tab=containerinstallation  instructions.",0.0,0.0,1.0,0.0
datadog,I run my golang app container with,0.0,0.0,1.0,0.0
datadog,with Dockerfile,0.0,0.0,1.0,0.0
datadog,and DD agent can see app container ups and downs but receive no logs,0.177,0.0,0.823,-0.4215
datadog,I installed datadog agent locally on my windows 10  machine.,0.0,0.0,1.0,0.0
datadog,By default it stored data in ProgramData folder in C drive.,0.0,0.0,1.0,0.0
datadog,It does not give option to select different drive while installation.,0.0,0.0,1.0,0.0
datadog,"Now when I run any command, it gives me below error.",0.231,0.0,0.769,-0.4019
datadog,Can we edit permissions to it allow access to ProgramData folder.,0.0,0.16,0.84,0.2263
datadog,I have one Datadig dashboard to monitor a particular service.,0.0,0.0,1.0,0.0
datadog,"To use the same dashboard for other services, I added a couple of template variables to change the queries in the dashboard.",0.0,0.0,1.0,0.0
datadog,"But, I could not use these variables in the query section of Datadog SLO widget.",0.0,0.0,1.0,0.0
datadog,"The following query with variables works in the other type of widgets, but not in the SLO widget.",0.0,0.0,1.0,0.0
datadog,"Is there a way to use variables in SLO widget too, or not possible because it is in beta version or something?",0.0,0.0,1.0,0.0
datadog,I'm trying to record my website sales $ amount in datadog.,0.0,0.0,1.0,0.0
datadog,However I'm getting way more than the actual value.,0.0,0.231,0.769,0.34
datadog,I'm using java-dogstatsd client and spring.,0.0,0.0,1.0,0.0
datadog,My application is running on 3 hosts.,0.0,0.0,1.0,0.0
datadog,I recorded all metrics (using sendWebOrder method) but no luck.,0.203,0.29,0.507,0.296
datadog,I'm trying to generate a datadog toplist by transactiontype.,0.0,0.0,1.0,0.0
datadog,"I'm not getting the correct amount in any of the metrics (tried mainly count, gauge and histogram.sum).",0.0,0.0,1.0,0.0
datadog,Here is my datadog config:,0.0,0.0,1.0,0.0
datadog,What am I missing?,0.524,0.0,0.476,-0.296
datadog,Is this the correct way to record money value?,0.0,0.231,0.769,0.34
datadog,Do I've to do any rollup in config?,0.0,0.0,1.0,0.0
datadog,Any help is appreciated.,0.0,0.75,0.25,0.7184
datadog,Is anyone aware if there is any telegraf output plugin to submit metric to datadog agent?,0.0,0.0,1.0,0.0
datadog,I can see a  datadog output plugin  which calls datadog metric api but not anything to submit data to datadog agent.,0.0,0.0,1.0,0.0
datadog,I am very new to the monitoring of microservices using prometheus and datadog.,0.0,0.0,1.0,0.0
datadog,"I am trying to monitor the rate of event callback requests per second using PromQL queries and datadog queries but when i compared the values from both the tools, they came out to be very different.",0.0,0.097,0.903,0.5499
datadog,I want to know if the two values need to be same.,0.0,0.308,0.692,0.4588
datadog,"If yes, then how do i write my queries to obtain the correct values",0.0,0.329,0.671,0.6597
datadog,PromQL query,0.0,0.0,1.0,0.0
datadog,Datadog query(json file),0.0,0.0,1.0,0.0
datadog,Please tell me how can i make the two queries equivalent?,0.0,0.204,0.796,0.3182
datadog,"I have been using mongo-driver in my project, deploying with gcloud app deploy for a while now.",0.0,0.0,1.0,0.0
datadog,"I recently rebuild my machine, and simply ran  go get  to get fetch ally my dependencies.",0.0,0.0,1.0,0.0
datadog,"Everything is compiling fine locally, however,  gcloud app deploy  fails:",0.222,0.143,0.635,-0.25
datadog,Any ideas?,0.0,0.0,1.0,0.0
datadog,app.yaml is just  runtime: go113,0.0,0.0,1.0,0.0
datadog,We have multiple applications sending logs to  Datadog  via  syslog .,0.0,0.0,1.0,0.0
datadog,Every team has created  facets  /  measures  for their respective applications under a particular group.,0.0,0.304,0.696,0.5859
datadog,Is there a direct way to explore the list of  facets  /  measures  under a specific group?,0.0,0.0,1.0,0.0
datadog,I am trying to create a document for our support team to include the list of  facets  and  measures .,0.0,0.242,0.758,0.5859
datadog,"I am able to view them in the Log Search page, but, cannot copy.",0.0,0.0,1.0,0.0
datadog,I am looking something as export the list of  facets / measures  to excel / csv.,0.0,0.2,0.8,0.4588
datadog,Note: I am restricted to use datadog api.,0.302,0.0,0.698,-0.3818
datadog,I am trying to query the datadog server for some specific metrics ie.,0.0,0.0,1.0,0.0
datadog,"""max mem used"" over some period x and I'm doing the following:",0.0,0.0,1.0,0.0
datadog,"I would have expect a single timestamped value to be returned, however I get datapoint for every minute like so:
 [
          1581084600000,
          1339840512
        ],
        [
          1581084660000,
          1339883520
        ],
        [
          1581084720000,
          1339740160
        ]",0.0,0.176,0.824,0.5994
datadog,"Is there a way to get a specific result, i.e.",0.0,0.0,1.0,0.0
datadog,the maximum out of all these results?,0.0,0.0,1.0,0.0
datadog,Thanks.,0.0,1.0,0.0,0.4404
datadog,"In Quarkus, the default logging library is JBoss and using the  quarkus-logging-json  allows you to encode your logs as JSON.",0.0,0.0,1.0,0.0
datadog,"However, Datadog integration requires custom fields such as  service ,  dd.span_id  and  dd.trace_id  to have the logs associated with the correct syntax.",0.0,0.0,1.0,0.0
datadog,"Currently, I've tried adding this in  application.properties :",0.0,0.0,1.0,0.0
datadog,"However, this seems not to show up in Datadog as expected.",0.0,0.0,1.0,0.0
datadog,"When we use log4j2, we simply configure it like so.",0.0,0.217,0.783,0.3612
datadog,"Again, cannot find any documentation how to accomplish the same result in Quarkus configs.",0.0,0.177,0.823,0.4215
datadog,Does anyone know how I could inject these custom properties into the JSON logs with Quarkus or how to integrate it with Datadog properly?,0.0,0.0,1.0,0.0
datadog,How can I automate configuring Log Archives on GCP?,0.0,0.0,1.0,0.0
datadog,"I can do it manually by following steps
 https://docs.datadoghq.com/logs/archives/?tab=googlecloudstorage",0.0,0.0,1.0,0.0
datadog,"I guess selenium can help this 
but I looking for a more programmatic way like Terraform or REST API",0.0,0.282,0.718,0.67
datadog,Thank you.,0.0,0.714,0.286,0.3612
datadog,I am working on to create some custom metrics for my spring boot 2 rest api.,0.0,0.139,0.861,0.2732
datadog,I have added the required micro meter and datadog dependency.,0.0,0.0,1.0,0.0
datadog,My office machine works behind a proxy.,0.0,0.0,1.0,0.0
datadog,I have setup proxy through spring boot plugin.,0.0,0.0,1.0,0.0
datadog,below are in my application.properties file.,0.0,0.0,1.0,0.0
datadog,management.metrics.export.datadog.apiKey=mykey,0.0,0.0,1.0,0.0
datadog,But I am getting the socket connection timeout.,0.0,0.0,1.0,0.0
datadog,As far as I debugged the io.micrometer.core.ipc.http.HttpUrlConnectionSender.send method is failing and I dont under how the micro meter data dog takes the proxy details.,0.136,0.0,0.864,-0.5106
datadog,The micrometer doc says,0.0,0.0,1.0,0.0
datadog,But I dont understand what it means?,0.0,0.0,1.0,0.0
datadog,should I replace this url with my proxy url or is there any specific uri pattern with the proxy?,0.0,0.0,1.0,0.0
datadog,I am using spring boot 2.2.4.RELEASE,0.0,0.0,1.0,0.0
datadog,Splunk has  transaction  command which can produce  duration  between logs grouped by id:,0.0,0.0,1.0,0.0
datadog,as it is decribed on,0.0,0.0,1.0,0.0
datadog,How to calculate duration between events in Datadog?,0.0,0.0,1.0,0.0
datadog,I'm setting up a monitor that looks like this:,0.0,0.263,0.737,0.3612
datadog,I've also got a widget that looks like this:,0.0,0.263,0.737,0.3612
datadog,"I'd like to define two different messages, one of which will be sent when the &quot;warning&quot; threshold is crossed, and the other which will be sent when the &quot;critical&quot; threshold is crossed.",0.0,0.075,0.925,0.3612
datadog,How can I do this?,0.0,0.0,1.0,0.0
datadog,Is this correct?,0.0,0.0,1.0,0.0
datadog,"I have an index created on the log and the paths have special character :
 for example:",0.0,0.266,0.734,0.5719
datadog,Sample URL:,0.0,0.0,1.0,0.0
datadog,grok parser:,0.0,0.0,1.0,0.0
datadog,when I try to add facet for  @params.rs:orgId,0.0,0.0,1.0,0.0
datadog,I am getting error as,0.474,0.0,0.526,-0.4019
datadog,"An error occurred while saving the facet: The Facet path must contain
only letters, digits, or the characters - _ .",0.137,0.0,0.863,-0.4019
datadog,@ $,0.0,0.0,0.0,0.0
datadog,Is there a DataDog metric to report the space used or remaining in a GCP PersistentVolume.,0.0,0.0,1.0,0.0
datadog,"I have found disk use metrics for the container itself, but not for a PersistentVolume.",0.0,0.0,1.0,0.0
datadog,I am working in GoogleCloudPlatform.,0.0,0.0,1.0,0.0
datadog,Using:,0.0,0.0,1.0,0.0
datadog,I am deploying Datadog as a DaemonSet and with the cluster-agent enabled to a Kubernetes cluster using the instructions provided  here .,0.0,0.0,1.0,0.0
datadog,I'm configuring Datadog using the  values.yaml  file as specified.,0.0,0.0,1.0,0.0
datadog,"I want to do some custom metrics, specifically using the integration formerly known as  postgres.yaml .",0.0,0.091,0.909,0.0772
datadog,"I have tried to do this as specified in the  values.yaml  template found  here , like this (putting it in the cluster-agent, since these are cluster-wide metrics):",0.0,0.094,0.906,0.3612
datadog,"As per the documentation, I can confirm that using the  |-  prefix this indeed creates a file in the path  /etc/datadog-agent/conf.d/postgres.yaml  on the node, where I would expect it to.",0.0,0.075,0.925,0.2732
datadog,"The file correctly has all the contents in the block, i.e.",0.225,0.0,0.775,-0.4404
datadog,starting with  init_config:...,0.0,0.0,1.0,0.0
datadog,"Now, when starting the node I see this in the logs (DEBUG):",0.0,0.0,1.0,0.0
datadog,"'/conf.d/postgres.yaml' -&gt; '/etc/datadog-agent/conf.d/postgres.yaml'
/conf.d/..2020_10_22_10_22_27.239825358 -&gt;
/etc/datadog-agent/conf.d/..2020_10_22_10_22_27.239825358
'/conf.d/..2020_10_22_10_22_27.239825358/postgres.yaml' -&gt;
'/etc/datadog-agent/conf.d/..2020_10_22_10_22_27.239825358/postgres.yaml'",0.0,0.0,1.0,0.0
datadog,"2020-10-22 10:22:29 UTC | CLUSTER | DEBUG |
(pkg/autodiscovery/providers/file.go:196 in collectEntry) | Found
valid configuration in file: /etc/datadog-agent/conf.d/postgres.yaml",0.0,0.0,1.0,0.0
datadog,"2020-10-22 10:22:29 UTC | CLUSTER | DEBUG |
(pkg/collector/scheduler.go:154 in getChecks) | Unable to load a check
from instance of config 'postgres': Core Check Loader:  Check postgres
not found in Catalog",0.0,0.0,1.0,0.0
datadog,"2020-10-22 10:22:29 UTC | CLUSTER | ERROR |
(pkg/collector/scheduler.go:201 in GetChecksFromConfigs) |  Unable to
load the check: unable to load any check from config 'postgres'",0.147,0.0,0.853,-0.5319
datadog,"The documentation  here  states, that the postgres yaml-contents in agents v7.x should actually be in  /etc/datadog-agent/conf.d/postgres.d/conf.yaml  and not in  /etc/datadog-agent/conf.d/postgres.yaml .",0.0,0.0,1.0,0.0
datadog,"It is not possible to create a subfolder / use forward slashes in the config key (internally, the file is created using ConfigMap).",0.153,0.085,0.762,-0.1566
datadog,I'm not even sure if the problem is the yaml-file path or if a core integration is missing.,0.329,0.0,0.671,-0.7061
datadog,So my broad quest is: how do I enable Datadog postgres-integration correctly in my setup?,0.0,0.0,1.0,0.0
datadog,"I have been asked to implement a centralized monitoring and logging system using DataDog that will receive information from various services and applications, some running as Windows Services on virtual machines and some running inside a Kubernetes cluster.",0.0,0.0,1.0,0.0
datadog,"In order to implement the logging aspect so that DataDog can correctly ingest the logs, I'm using Serilog to do the logging.",0.0,0.0,1.0,0.0
datadog,My plan is currently to write the logs to the console in json format and have the DataDog agent installed on each server or k8s node capture and ship them to DataDog.,0.0,0.0,1.0,0.0
datadog,"This works, at least for the k8s node where I've implemented it so far.",0.0,0.0,1.0,0.0
datadog,(I'm trying to avoid using the custom Serilog sink for DataDog as that's discouraged in the DataDog documentation).,0.234,0.0,0.766,-0.5994
datadog,My problem is that I cannot get logs ingested correctly on the DataDog side.,0.184,0.0,0.816,-0.4019
datadog,DataDog expects the json to contain a property call Message but Serilog names this property RenderedMessage (if I use JsonFormatter(renderMessage: true)) or @m (if I use RenderedCompactJsonFormatter()).,0.0,0.0,1.0,0.0
datadog,How can I get my logs shipped to DataDog and ingested correctly on the DataDog end?,0.0,0.0,1.0,0.0
datadog,My current organization is migrating to DataDog for Application Performance Monitoring.,0.0,0.0,1.0,0.0
datadog,I am deploying a Python Flask web application using docker to Azure Container Registry.,0.0,0.0,1.0,0.0
datadog,After the deployment to Azure the app should be listed/available on Datadog portal.,0.0,0.0,1.0,0.0
datadog,Please note I just started learning Docker containers.,0.0,0.277,0.723,0.3182
datadog,There is a high chance I could do completely wrong.,0.298,0.176,0.527,-0.3384
datadog,Please bear with me,0.0,0.434,0.566,0.3182
datadog,Steps followed,0.0,0.0,1.0,0.0
datadog,Option 1: Create a docker container on local machine and push to ACR,0.0,0.16,0.84,0.2732
datadog,Added  dd-trace  python library to the docker image,0.0,0.0,1.0,0.0
datadog,Added dd-trace run command the docker file,0.0,0.0,1.0,0.0
datadog,build the image,0.0,0.0,1.0,0.0
datadog,run the container on local,0.0,0.0,1.0,0.0
datadog,Getting OSError: [Errno 99] Cannot assign requested address,0.0,0.0,1.0,0.0
datadog,Option 2: Forward logs to Azure Blob Storage but a heavy process,0.0,0.0,1.0,0.0
datadog,"Option 3: using Serilog but, my organization does not want to use third party logging framework, we have our own logging framework",0.054,0.143,0.804,0.4968
datadog,"Any help is highly appreciated, I am looking for a solution using  Option 1 .",0.0,0.518,0.482,0.8221
datadog,"I went through the Microsoft articles, Datadog documentation but, no luck.",0.189,0.27,0.541,0.296
datadog,"I setup app registrations, Manage reader permissions on Subscription, created ClientID and app secrets on Azure portal.",0.0,0.118,0.882,0.25
datadog,none of them helped,0.0,0.0,1.0,0.0
datadog,Could you confirm whether is there a way to collect the APM logs on datadog with out installing agent on Azure.,0.0,0.0,1.0,0.0
datadog,Thank you in advance.,0.0,0.455,0.545,0.3612
datadog,"I am little embarrased, as this is probably very banal, but it's really not making any sense to me.",0.0,0.0,1.0,0.0
datadog,"I have a little golang web-application, that I for now am just localhosting.",0.0,0.0,1.0,0.0
datadog,"I have some logging with logrus going on in the project, and want to use datadog to get a nice and visual dashboard for my application.",0.0,0.157,0.843,0.4767
datadog,"So I follow a bunch of the steps on their page, download their dockerimage and run it, and i am able to see some data!",0.0,0.0,1.0,0.0
datadog,"I then follow the instructions further on, under a headline called &quot;docker integration&quot;.",0.0,0.0,1.0,0.0
datadog,here I am asked to run the following command:,0.0,0.0,1.0,0.0
datadog,To which the response &quot;cannot find user &quot;dd-agent&quot;&quot; is given.,0.0,0.0,1.0,0.0
datadog,But what is this &quot;user&quot; value?,0.0,0.383,0.617,0.4767
datadog,Is it the user to which I have signed up to datadog with?,0.0,0.0,1.0,0.0
datadog,something else?,0.0,0.0,1.0,0.0
datadog,I find it very unspecified?,0.0,0.0,1.0,0.0
datadog,would very much appreciate a helping hand,0.0,0.742,0.258,0.8244
datadog,I want to get Confluent cloud metrics into Datadog so I followed the this  instruction .,0.0,0.098,0.902,0.0772
datadog,Instead of using CCLOUD_USER: ${CCLOUD_USER} and CCLOUD_PASSWORD: ${CCLOUD_PASSWORD} I used CCLOUD_API_KEY and CCLOUD_API_SECRET as environment variables for the exporter container.,0.0,0.0,1.0,0.0
datadog,I get a Failed to establish a new connection: [Errno 111] Connection refused error:,0.506,0.0,0.494,-0.802
datadog,When I tried to curl http://ccloudexporter_ccloud_exporter_1:2112/metrics I got no reply but I did with a curl to http://localhost:2112/metrics.,0.11,0.0,0.89,-0.1531
datadog,So I adjusted the openmetrics.yml to use prometheus url http://localhost:2112/metrics.,0.0,0.0,1.0,0.0
datadog,Still same error in the DD container.,0.31,0.0,0.69,-0.4019
datadog,When I go to http://localhost:2112/metrics in my browser I see metrics.,0.0,0.0,1.0,0.0
datadog,No clue on why DD cannot connect to /metrics.,0.216,0.0,0.784,-0.296
datadog,Background:,0.0,0.0,1.0,0.0
datadog,I'm trying SLO feature from micrometer and I expect that I can get the number of requests that fulfill the SLO.,0.0,0.198,0.802,0.4939
datadog,"For example if I set the SLO to 500ms, then I want to know how many requests &lt;= 500ms.",0.0,0.075,0.925,0.0772
datadog,Also I want to know the total requests.,0.0,0.178,0.822,0.0772
datadog,Problem:,1.0,0.0,0.0,-0.4019
datadog,"http.server.requests.count says 24
http.server.requests.histogram with tag le:_inf says 126",0.0,0.0,1.0,0.0
datadog,I believe both of them should have the same (or at least similar) value,0.0,0.167,0.833,0.34
datadog,I'm using:,0.0,0.0,1.0,0.0
datadog,"Spring Boot 2.3.2.RELEASE
Micrometer: 1.5.2",0.0,0.0,1.0,0.0
datadog,application.properties,0.0,0.0,1.0,0.0
datadog,Meter Filter,0.0,0.0,1.0,0.0
datadog,Datadog,0.0,0.0,1.0,0.0
datadog,Any clue on what's happening here?,0.0,0.0,1.0,0.0
datadog,Thanks,0.0,1.0,0.0,0.4404
datadog,I was reading the  Datadog docs  on how to monitor AWS Elastic Map Reduce using Datadog because I need to get metrics for failed EMR steps.,0.125,0.0,0.875,-0.5106
datadog,LIKE HERE,0.0,0.714,0.286,0.3612
datadog,"I think the most accurate metric is  aws.elasticmapreduce.jobs_failed  but as the image says, is only available for Hadoop V1, but I'm using Hadoop V2... so I don't see it in my  Datadog Metric Explorer",0.0,0.0,1.0,0.0
datadog,"I want to trace a request path that has started in the Web application React JS (frontend), then passed to the backend and returned as a response.",0.0,0.053,0.947,0.0772
datadog,Can  dd-trace-js  start the span and pass it to the server over HTTP HEADERS?,0.0,0.0,1.0,0.0
datadog,"Hikaricp , Tomcat and jdbc metrics are not being exported to DataDog",0.0,0.0,1.0,0.0
datadog,"we have setup springboot app to push the metrics to datadoghq, it does export 60 metrics, however the metrics like hikaricp, tomcat and jdbc are missing.",0.077,0.087,0.836,0.0772
datadog,"hikaricp, tomcat and jdbc - these mertics are listed under  /actuator/metrics  endpoint, but not exported to datadog.",0.0,0.0,1.0,0.0
datadog,"Is there any additional settings required to push hikaricp, tomcat and jdbc metrics ?",0.0,0.0,1.0,0.0
datadog,"How can logback be configured to add tags,so that datadog can recognize the source?",0.0,0.0,1.0,0.0
datadog,I have the following  logback.xml :,0.0,0.0,1.0,0.0
datadog,Where the custom field  ddtags  is supposed to set tags for datadog.,0.0,0.0,1.0,0.0
datadog,"The logs show up in datadog and everything works as expected, despite the  source -tag.",0.0,0.0,1.0,0.0
datadog,The log messages sent from my service show up with two tags in datadog:  source:java  and  source:undefined :,0.0,0.0,1.0,0.0
datadog,How do I get rid of the  source:undefined  tag so that datadog correctly recognizes the source?,0.0,0.0,1.0,0.0
datadog,I'm working with express + graphql environment.,0.0,0.0,1.0,0.0
datadog,I want to add tags to express span with the value I derive while resolving the graphql query.,0.0,0.326,0.674,0.6486
datadog,"Currently, tags get added to the graphql span with the following code.",0.0,0.0,1.0,0.0
datadog,Let me know if there is a way to add these tags to the parent span instead of the current span.,0.0,0.0,1.0,0.0
datadog,This is required as I don't want to enable analytics on graphql since I already have it enabled in the express app.,0.06,0.0,0.94,-0.0572
datadog,Basically I want tags to root trace(express) instead of current span(graphql.execute).,0.0,0.126,0.874,0.0772
datadog,"Anyway, how can I search for all messages(errors) where stacktrace contains specific piece of code?",0.0,0.0,1.0,0.0
datadog,According to datadog documentation it search only by message attribute(it's infered from the json-like object sent to datadog when you log something).,0.0,0.0,1.0,0.0
datadog,Stacktrace is a separate property and I cannot understand how to search it.,0.0,0.0,1.0,0.0
datadog,My current situation is that I have two different data feeds (Feed A &amp; Feed B) and I have created custom metrics for both feeds:,0.0,0.087,0.913,0.25
datadog,Next steps is to create alert monitoring for the agreed upon threshold of difference between the two metrics.,0.0,0.299,0.701,0.6597
datadog,Say we have agreed that it is acceptable for Order Counts from Feed A to be within ~5% of Order Counts from Feed B.,0.0,0.173,0.827,0.5267
datadog,How can I go about creating that threshold and comparison between the two metrics that I have already developed in Datadog?,0.0,0.109,0.891,0.296
datadog,I would like to send alerts to myself when the % difference between the two data feeds is &gt; 5 % for a daily validation.,0.0,0.116,0.884,0.3612
datadog,"I have an application running on Kubernetes and this app has log files that I want to stream to datadog log, then set up an alert.",0.0,0.137,0.863,0.3612
datadog,"Previously, this app run on bare-metal server, I installed datadog agent on that server, and I used custom log collection to retrieve that logs.",0.0,0.0,1.0,0.0
datadog,It worked perfectly well.,0.0,0.759,0.241,0.743
datadog,"Now, I have an obstacle on how to read the log files in the container.",0.161,0.0,0.839,-0.3612
datadog,"I have googled and it said I can use annotations and auto discovery, but I can't see where I am supposed to define the log path.",0.0,0.0,1.0,0.0
datadog,Does anyone have an idea how to resolve this or have a similar case with mine?,0.0,0.157,0.843,0.3818
datadog,Thank you in advance.,0.0,0.455,0.545,0.3612
datadog,"On DataDog log search, I want to search for logs with empty string for a specific facet, e.g.",0.105,0.076,0.819,-0.128
datadog,logs with userId is empty.,0.31,0.0,0.69,-0.2023
datadog,"@userId:''  ,  @userId:"""" ,  -@userId:*  non worked.",0.0,0.0,1.0,0.0
datadog,I have a metric which has a tag with lots of different values (the value is a file name).,0.0,0.282,0.718,0.6249
datadog,How can I create a query that determines the number of different values of that tag exist on a metric?,0.0,0.303,0.697,0.6249
datadog,"For example if 4 metrics are received during a time frame, with the following tags ""file_name:dir/file1"", ""file_name:dir/file2"", ""file_name:dir/file3"", ""file_name:dir/file1""",0.0,0.0,1.0,0.0
datadog,"I want the query to return the value 3, since of all the metrics received during this timeframe there were 3 distinct values for the file_name tag.",0.0,0.225,0.775,0.6597
datadog,"This is the installation path of datadog  /etc/datadog-agent 
Under this we have checks under folder  /etc/datadog-agent/conf.d",0.0,0.0,1.0,0.0
datadog,"Under this we have defined a service to report disk space alert under disk.d
 /etc/datadog-agent/conf.d/disk.d",0.0,0.145,0.855,0.296
datadog,We have the file ready in the configuration.,0.0,0.263,0.737,0.3612
datadog,We did tried to reload the datadog agent to reflect the changes.,0.0,0.0,1.0,0.0
datadog,"The expected scenario is it should reflect in datadog console under the service defined 
The query we are using is",0.0,0.0,1.0,0.0
datadog,But we are unable to establish anything.,0.0,0.0,1.0,0.0
datadog,Nutshell none of the alerts configured for this host are not reflecting in datadog console.,0.0,0.0,1.0,0.0
datadog,Our applications log in JSON format.,0.0,0.0,1.0,0.0
datadog,According to Datadog's documentation JSON logs are not processed by pipelines.,0.0,0.0,1.0,0.0
datadog,How can I enrich the JSON logs with an additional field that is based on a different value of that same log line?,0.0,0.107,0.893,0.34
datadog,I have this line:,0.0,0.0,1.0,0.0
datadog,And I want this line:,0.0,0.302,0.698,0.0772
datadog,Is this possible with Datadog?,0.0,0.0,1.0,0.0
datadog,I do not want to change our loggers to a the  customerId  to the log output.,0.086,0.0,0.914,-0.0572
datadog,I cannot find any article that describes the advantages of using datadog histogram compared to datadog distribution for apps that run on multi instance.,0.0,0.102,0.898,0.3612
datadog,Would someone kindly help me on deciding the best choice between those two?,0.0,0.502,0.498,0.8779
datadog,I want to be able to parameterised my datadog dashboard.,0.0,0.14,0.86,0.0772
datadog,I have already introduced a template variable  flavor  which to indicate if it is  dev  or  prod  environment.,0.0,0.0,1.0,0.0
datadog,What I wish to achieve is to switch data from one environment o another when I select a different environment (e.g.,0.0,0.144,0.856,0.4019
datadog,from  dev-db-master  to  prod-db-master ).,0.0,0.0,1.0,0.0
datadog,The string interpolation is necessary because I want to display multiple time series within a single chart.,0.0,0.085,0.915,0.0772
datadog,However the chart is basically blank,0.0,0.0,1.0,0.0
datadog,The Json tab also shows a pink background which indicates either the json is malformed or the query is too complex.,0.0,0.0,1.0,0.0
datadog,"My goal is to be able to, by changing the template variable  flavor ,",0.0,0.0,1.0,0.0
datadog,"I can change a group of time series from, says,  'dev-db-master', 'dev-db1-master' and 'dev-db2-master'  to  'prod-db-master', 'prod-db1-master' and 'prod-db2-master' .",0.0,0.0,1.0,0.0
datadog,Can you suggest a way to construct a string with a template variable?,0.0,0.0,1.0,0.0
datadog,"I have a message like 'Service is running' that i'm not able to change, so in log Grok Parser I want to replace it to 'INFO | Service is running' or manually or somehow manually assign like  %{level=INFO}  .",0.0,0.169,0.831,0.6486
datadog,Please kindly advice.,0.0,0.846,0.154,0.6705
datadog,I have created a event monitor( for example,0.0,0.286,0.714,0.25
datadog,events('sources:rds event_source:db-instance').by('dbinstanceidentifier').rollup('count').last('1d') &gt;= 1 ),0.0,0.0,1.0,0.0
datadog,"but it returns ""NO DATA"" when there are no any events.",0.427,0.0,0.573,-0.7717
datadog,How to make it return 0 when there are no any events?,0.18,0.0,0.82,-0.296
datadog,Is it possible to migrate existing data (Like in Prometheus and elk )to datadog?,0.0,0.0,1.0,0.0
datadog,There is a setup for live streaming of Prometheus metrics to Datadog by configuring datadog config.,0.0,0.0,1.0,0.0
datadog,But what could be done with the past data?,0.0,0.0,1.0,0.0
datadog,I have a question about the configuration setting for datadog for postgres 9.6.,0.0,0.0,1.0,0.0
datadog,(1) How do I get all databases monitored in datadog?,0.0,0.0,1.0,0.0
datadog,(2) How do I get all table level metrics from each database/schema?,0.0,0.0,1.0,0.0
datadog,Here is conf file.,0.0,0.0,1.0,0.0
datadog,Datadog documents are not really helpful.,0.338,0.0,0.662,-0.3713
datadog,"Instead of listing all the dbs, I want all databases, so if we add a new db, we don't have to change the conf file and same goes for table_name.",0.0,0.046,0.954,0.0772
datadog,"According to datadog docs, the table level metrics are collected using pg_stat_user_tables, pg_statio_user_tables etc.",0.0,0.0,1.0,0.0
datadog,And these postgres tables are database specific unlike pg_stat_activity or pg_stat_statements.,0.0,0.0,1.0,0.0
datadog,I have following function:,0.0,0.0,1.0,0.0
datadog,I use it as following:,0.0,0.0,1.0,0.0
datadog,In logs I see spans as following:,0.0,0.0,1.0,0.0
datadog,Agent works fine but on datadog server I don't see the trace.,0.0,0.123,0.877,0.1027
datadog,What is wrong?,0.608,0.0,0.392,-0.4767
datadog,I am reading this  tutorial :,0.0,0.0,1.0,0.0
datadog,I would like to set  traceId  be equal  requestId .,0.0,0.263,0.737,0.3612
datadog,How can I do it using  ddtrace  api?,0.0,0.0,1.0,0.0
datadog,I have a situation where I'm trying to count the number of files loaded into the system I am monitoring.,0.0,0.075,0.925,0.0772
datadog,"I'm sending a ""load time"" metric to Datadog each time a file is loaded, and I need to send an alert whenever an expected file does not appear.",0.0,0.084,0.916,0.296
datadog,"To do this, I was going to count up the number of ""load time"" metrics sent to Datadog in a 24 hour period, then use anomaly detection to see whether it was less than the normal number expected.",0.0,0.071,0.929,0.1531
datadog,"However, I'm having some trouble finding a way to consistently pull out this count for use in the alert.",0.129,0.105,0.766,-0.128
datadog,"I can't use the count_nonzero function, as some of my files are empty and have a load time of 0.",0.096,0.0,0.904,-0.2023
datadog,"I do know about .as_count() and count:metric{tags}, but I haven't found a way to include an evaluation interval with either of these.",0.0,0.0,1.0,0.0
datadog,"I've tried using .rollup(count, time) to count up the metrics sent, but this call seems to return variable results based on the rollup interval.",0.0,0.0,1.0,0.0
datadog,"For instance, if I compare intervals of 2000 and 4000 seconds, I would expect each 4000 second interval to count up about the sum of two 2000 second intervals over the same time period.",0.0,0.0,1.0,0.0
datadog,This does not seem to be what happens at all - the counts for the smaller intervals seem to add up to much more than the count for the larger one.,0.0,0.0,1.0,0.0
datadog,"Additionally some rollup intervals display decimal numbers as counts, which does not make any sense to me if this function is doing what I thought it was.",0.0,0.0,1.0,0.0
datadog,Does anyone have any thoughts on how to accomplish this?,0.0,0.237,0.763,0.4215
datadog,I'd really appreciate any new ideas.,0.0,0.374,0.626,0.4576
datadog,I'm trying to make a dashboard to monitor a process which runs on 5 remote machines simultaneously.,0.0,0.0,1.0,0.0
datadog,"I want the dashboard to display the metrics for each machine separately - basically, I want to create five separate graphs, one for each machine that runs the process.",0.0,0.17,0.83,0.4019
datadog,"My problem is that the remote machines are reassigned periodically, so I have no way of knowing the name of the host at any given time.",0.184,0.0,0.816,-0.6344
datadog,"I've tried creating five separate graphs, with each one filtered by a different host name tag, but the graphs do not seem to pick up the new host when the lease for the process is changed.",0.0,0.045,0.955,0.1531
datadog,"I also know you can split out one graph for each host using metrics explorer, but I haven't found any way to automatically do that on a dashboard.",0.0,0.0,1.0,0.0
datadog,Does anyone know if this is possible?,0.0,0.0,1.0,0.0
datadog,"Leases for the process are assigned through AWS, if that is helpful.",0.0,0.203,0.797,0.4215
datadog,Thanks in advance for any suggestions.,0.0,0.367,0.633,0.4404
datadog,I'd like to be able to send logs to datadog and have the message be a JSON object rather than a string.,0.0,0.116,0.884,0.3612
datadog,"The metadata fields aren't searchable unless a facet is created, which I would like to avoid doing.",0.118,0.241,0.642,0.3182
datadog,I'm currently using  winston  +  winston-datadog-logs-transporter  to send the logs.,0.0,0.0,1.0,0.0
datadog,"If I do:  logger.info(JSON.stringify(message)) , datadog records the message as blank and adds the stringified message as metadata.",0.0,0.0,1.0,0.0
datadog,"If I do:  logger.info('foo' + JSON.stringify(message) , then the message is interpreted as a string and I can search on it.",0.0,0.0,1.0,0.0
datadog,"If I do:  logger.info('foo', message) , the body is set to  foo  and  message  is interpreted as metadata, which I cannot search for without creating a facet.",0.079,0.0,0.921,-0.2235
datadog,"Any help is appreciated, thanks!",0.0,0.821,0.179,0.8478
datadog,I am not able to see traces for my application under APM --  Service in Datadog.,0.0,0.0,1.0,0.0
datadog,I found some sample code from Datadog docs but don't know exactly where it should go inside my application.,0.0,0.0,1.0,0.0
datadog,Please let me know if anyone has any idea regarding it.,0.0,0.187,0.813,0.3182
datadog,I have already tried with following code in my js file.,0.0,0.0,1.0,0.0
datadog,My application is based on node js which is serverless.,0.0,0.0,1.0,0.0
datadog,"I have also added dependencies for dd-trace in package.json as  ""dd-trace"": ""^0.11.0""",0.0,0.0,1.0,0.0
datadog,I expected to list my application with proper name in APM Services in Datadog.,0.0,0.0,1.0,0.0
datadog,"I have a use case where I want to report all HTTP 500 events when they occur as integer counts, but also send a default value of 0 if no 500 event occurred during a request.",0.082,0.125,0.793,0.1154
datadog,How can I achieve this with the Datadog adapter?,0.0,0.0,1.0,0.0
datadog,"As a first pass, I attempted to create a rule that has  match: true , and then a metric that, for value, sets it to read  conditional(response.code.startsWith(""5""), ""1"", ""0"") .",0.0,0.258,0.742,0.743
datadog,"I then told the rule to use the Datadog adaptor, and registered this metric with said adaptor in the rule.",0.0,0.0,1.0,0.0
datadog,"This threw errors in the mixer logs, likely because request.code is an integer and startsWith is probably a function that expects a string - we lost all metrics as a consequence.",0.158,0.0,0.842,-0.5719
datadog,How can I create a threshold alert by comparing aggregate of 2 metrics ?,0.0,0.35,0.65,0.5106
datadog,"For example, if m1=[2,3,1,5] and m2=[6,7], I want to create an alert when sum(m1)   sum(m2).",0.0,0.337,0.663,0.5574
datadog,sum method here I assume will add all the data points returned by a query.,0.0,0.0,1.0,0.0
datadog,"From what I have observed, datadog flow of alert creating is like, define the metric  -  set alert condition on metric.",0.0,0.378,0.622,0.7964
datadog,"That is, it looks like the alert condition will be some condition on metric data type only.",0.0,0.239,0.761,0.5719
datadog,But I am looking for something like storing the aggregate of metrics in a variables and comparing those variables.,0.0,0.169,0.831,0.5023
datadog,How can it be done in datadog ?,0.0,0.0,1.0,0.0
datadog,"I have configured datadog agent on Amazon ECS, Fargate.",0.0,0.195,0.805,0.1779
datadog,"I can send all the intended metrics but I cannot send ""tags"".",0.0,0.0,1.0,0.0
datadog,I've set Environment variables in ECS task definitions.,0.0,0.0,1.0,0.0
datadog,I think most of the settings are all right because I can see the metrics which I want to see.,0.0,0.075,0.925,0.0772
datadog,"But tags, especially env:stg is missing in datadog UI and because of this weired error, some metrics is missing.",0.383,0.0,0.617,-0.8733
datadog,Does anyone know the reason of this error and the way to solve this?,0.185,0.106,0.709,-0.3237
datadog,Thanks.,0.0,1.0,0.0,0.4404
datadog,I am attempting to use Datadog to monitor my application via JMX...,0.0,0.0,1.0,0.0
datadog,"I have successfully deployed my app in a docker container, and exposed the JMX port and confirmed I can indeed attach to the port from anywhere and get information.",0.046,0.112,0.842,0.4404
datadog,So I am attempting to set up the datadog docker image to use JMX and connect to the server...,0.0,0.0,1.0,0.0
datadog,"I have it all configured, but at runtime the datadog image attempts to start utilizing JMX, but fails saying it can't find Java on its image...",0.134,0.0,0.866,-0.5719
datadog,I log into the image and sure enough it has no java installed.,0.152,0.159,0.69,0.0258
datadog,From the datadog documentation:,0.0,0.0,1.0,0.0
datadog,"Well that's all nice and well, but if I attempt to expose my host machine java to the image via a volume mount, it doesn't work, as the host machine is Apple and if the image attempts to run the java binary it throws an invalid format for the binary file.. not surprising since its a MACOS binary not a Debian Linux Binary (which the datadog image is)....",0.06,0.073,0.866,-0.0312
datadog,"So, I have been attempting to take the datadog image and build a new image with it as the base with Java... but I have been completely unsuccessful, every attempt to install java during docker build fails..",0.101,0.0,0.899,-0.5704
datadog,"I have tried every example of how to install java into a debian docker image, but none work... Every one dies with apt-get line returned a non zero",0.0,0.0,1.0,0.0
datadog,How the heck do I get JAVA installed on a debian image?,0.0,0.0,1.0,0.0
datadog,"Or better yet, how do I get the datadog image with JMX to run properly?",0.0,0.182,0.818,0.4404
datadog,I am having a hard time to collect logs from an python app deployed in ECS using DataDog Agent.,0.08,0.0,0.92,-0.1027
datadog,I have a dockerized Flask app deployed in ECS.,0.0,0.0,1.0,0.0
datadog,The app spits logs to stdout.,0.0,0.0,1.0,0.0
datadog,I now need to monitor them in DataDog.,0.0,0.0,1.0,0.0
datadog,"I've added a new DataDog agent container (Fargate compatible, since I am using Fargate), which runs as part of the same task as the app.",0.0,0.0,1.0,0.0
datadog,"I can see the CPU and memory metrics for both container in app.datadoghq.com/containers, so that means that DataDog Agent is working.",0.0,0.0,1.0,0.0
datadog,I now need the app logs.,0.0,0.0,1.0,0.0
datadog,"I went through the documentation in  https://app.datadoghq.com/logs/onboarding/container , added",0.0,0.0,1.0,0.0
datadog,to the app container and the following env.vars to the DataDog container :,0.0,0.0,1.0,0.0
datadog,But that seems to be insufficient.,0.0,0.0,1.0,0.0
datadog,Am I going in the right direction ?,0.0,0.0,1.0,0.0
datadog,What am I missing ?,0.524,0.0,0.476,-0.296
datadog,I wanted to create a graph in Datadog to display iddle connections per user.,0.0,0.16,0.84,0.2732
datadog,Following this example:  http://www.miketheman.net/tag/postgres/  I changed my postgres.yaml configuration to:,0.0,0.0,1.0,0.0
datadog,"I can see the metric is appearing in Datadog, but I can just see one of the rows that should appear (has I have several databases in my PostgreSQL).",0.0,0.0,1.0,0.0
datadog,"Here is the datadog connection graph 
Am I missing any step?",0.196,0.0,0.804,-0.296
datadog,Is postgres.yaml missing any configuration?,0.355,0.0,0.645,-0.296
datadog,"Running that query in psql, I get this (modifying names but not data):",0.0,0.0,1.0,0.0
datadog,"From the datadog guide, want to integrate aws:",0.0,0.157,0.843,0.0772
datadog,https://docs.datadoghq.com/integrations/amazon_web_services/,0.0,0.0,1.0,0.0
datadog,Created a new policy named  DatadogAWSIntegrationPolicy :,0.0,0.333,0.667,0.25
datadog,"However, when clicked  Review policy  button, it said:",0.0,0.0,1.0,0.0
datadog,The syntax was followed the  datadog  service:,0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/360002042531-Error-Datadog-is-not-authorized-to-peform-sts-AssumeRole,0.0,0.0,1.0,0.0
datadog,What did I do?,0.0,0.0,1.0,0.0
datadog,via command line on my debian 8.x host.,0.0,0.0,1.0,0.0
datadog,Expected behaviour:,0.0,0.0,1.0,0.0
datadog,Actual behaviour:,0.0,0.0,1.0,0.0
datadog,Problem:,1.0,0.0,0.0,-0.4019
datadog,I can not start the service due to Result state: start-limit.,0.0,0.0,1.0,0.0
datadog,I waited about 9 hours.,0.0,0.0,1.0,0.0
datadog,My idea was the service state will recover from to many restart attempts.,0.0,0.0,1.0,0.0
datadog,It did not.,0.0,0.0,1.0,0.0
datadog,Any ideas?,0.0,0.0,1.0,0.0
datadog,According to the  DataDog Docker Integration Docs :,0.0,0.0,1.0,0.0
datadog,"There are two ways to run the [DataDog] Agent: directly on each host, or within a docker-dd-agent container.",0.0,0.0,1.0,0.0
datadog,We recommend the latter.,0.0,0.455,0.545,0.3612
datadog,Why is a Docker-based agent installation preferred over just installing the DataDog agent directly as a service on the box that's running the Docker containers?,0.0,0.0,1.0,0.0
datadog,I have my application integrated with Datadog for monitoring purpose.,0.0,0.0,1.0,0.0
datadog,At the same time I want the notifications/calls to be sent to the team if any of the metric fails to achieve the desires value.,0.102,0.135,0.764,-0.0258
datadog,I used Webhooks integration in Datadog for this purpose.,0.0,0.0,1.0,0.0
datadog,In the webhooks configuration I have set the URL (Twilio request) and I do get a call on my number.,0.0,0.075,0.925,0.0772
datadog,Now I am looking for an scenario wherein if the user doesn't pick the call for say 30secs then try calling the second number.,0.0,0.056,0.944,0.0772
datadog,How do I achieve this?,0.0,0.0,1.0,0.0
datadog,"I have two AWS account , I was able to set AWS integration for the first account using Terraform, but when I try to create AWS integration for my second account I am having an error.",0.101,0.075,0.824,-0.2263
datadog,I have created a role with in-line policy and we do not have a cross account set up.,0.0,0.125,0.875,0.25
datadog,Trust Relationship:,0.0,0.767,0.233,0.5106
datadog,Can anyone please guide me how to solve this error?,0.22,0.288,0.492,-0.0065
datadog,I have set the threshold value to get the alert in data dog for infrastructure.,0.0,0.277,0.723,0.5574
datadog,Alert is coming on  data dog UI but how to get this all alert data through API call either using JAVA or python.,0.0,0.173,0.827,0.5267
datadog,I need only alert data.,0.0,0.423,0.577,0.296
datadog,I am trying to integrate datadog to elasticsearch but the datadog collector shows an error .,0.215,0.0,0.785,-0.5499
datadog,i am not able to troubleshoot this.,0.242,0.0,0.758,-0.1511
datadog,pls help,0.0,1.0,0.0,0.4588
datadog,My elastic.yaml,0.0,0.0,1.0,0.0
datadog,"I want to use dataDog to see how many times a java method has been called, this is my example code.",0.0,0.067,0.933,0.0772
datadog,enter image description here,0.0,0.0,1.0,0.0
datadog,"In this code, I simply want to count how many times that 'multiply' has been called, and I used DogStatsD to record the this metrics.",0.0,0.056,0.944,0.0772
datadog,"However, when I go to the infrastructure of my dataDog, I can not find a metrics with a name similar like ""multiply"".",0.0,0.128,0.872,0.3612
datadog,What the name of the metrics should be if I set it up correctly?,0.0,0.0,1.0,0.0
datadog,Can anyone help me with how can I get the metrics of the 'multiply'?,0.0,0.184,0.816,0.4019
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,enter image description here,0.0,0.0,1.0,0.0
datadog,Is it possible to change JMX attribute name in JMXFetch so that a different name shows up in DataDog?,0.0,0.0,1.0,0.0
datadog,I currently have the following:,0.0,0.0,1.0,0.0
datadog,This would report two metrics in DataDog:,0.0,0.0,1.0,0.0
datadog,Is it possible to rename it in the yaml script to something more like:,0.0,0.177,0.823,0.4201
datadog,without the original names ever showing up in DataDog?,0.197,0.0,0.803,-0.2411
datadog,I'm new to datadog.,0.0,0.0,1.0,0.0
datadog,"I followed this  post , and replaced in my app/api keys.",0.0,0.0,1.0,0.0
datadog,I have :  nginx_dd.py,0.0,0.0,1.0,0.0
datadog,"When I run it  python nginx_dd.py , I kept getting",0.0,0.0,1.0,0.0
datadog,ImportError: No module named datadog,0.355,0.0,0.645,-0.296
datadog,Any hints / suggestions on this will be a huge helps !,0.0,0.426,0.574,0.636
datadog,I have a C# service installed on a machine that I publish stats to a DataDogAgent (which I later monitor).,0.0,0.0,1.0,0.0
datadog,"I uses StatsD as the library to publish the stats, the DataDogAgent is installed locally on the machine.",0.0,0.0,1.0,0.0
datadog,When I first set this up it worked great and no issues.,0.144,0.268,0.588,0.4404
datadog,When I released new code to the box and re-installed the service I stopped getting updated stats to DataDog.,0.106,0.0,0.894,-0.2263
datadog,When I look in the DataDogAgent in the collection log I see,0.0,0.0,1.0,0.0
datadog,I have verified that my code that submits via StatsD has not changed and is correctly sending out the stats I want.,0.0,0.064,0.936,0.0772
datadog,"I also repaired, uninstalled and re-installed and restarted the DataDogAgent all with no success.",0.13,0.219,0.651,0.3612
datadog,The timing is suspect to when I released code but I don't see what I would have changed that would cause this.,0.082,0.0,0.918,-0.1531
datadog,Everything is set to go to port 8125 for StatsD,0.0,0.0,1.0,0.0
datadog,I am trying to create a change  monitor using terraform .,0.0,0.231,0.769,0.2732
datadog,To create a monitor that checks that overtime a count stays at 0 for example every day (the value will go up to one some times and get back to 0).,0.0,0.148,0.852,0.5423
datadog,I found on the UI the capacity to create a  change alert .,0.0,0.35,0.65,0.5106
datadog,I cant seem to find a way to define the configuration for this type.,0.0,0.0,1.0,0.0
datadog,Is terraform just supporting only a subset of the monitors?,0.0,0.266,0.734,0.4404
datadog,or does the query need to be change in some specific way that I cant find documentation for?.,0.0,0.0,1.0,0.0
datadog,I have a script that queries our CI (Buildkite)'s API once per minute to fetch details of all build agents and emit metrics to Datadog for analysis.,0.0,0.0,1.0,0.0
datadog,"Getting an accurate  count  of these agents in the Datadog UI has proven challenging, however.",0.0,0.103,0.897,0.1531
datadog,"If the script emits a COUNT metric for each agent it sees, then agents will be double-counted in the Datadog UI when the interval is longer than a minute, because the script runs once per minute and sees (mostly) the same agents each time.",0.0,0.0,1.0,0.0
datadog,"The script could total up the number of agents it sees each run and emit that as a GAUGE, but then I lose the ability to break down the count in the Datadog UI by agent-specific tags (queue, etc).",0.085,0.098,0.816,-0.1154
datadog,"I suppose I could emit a GAUGE with a value of 1 for each agent on each run, and add an artificial  index  tag with a value of the numeric index in the agent array, and rely on the Datadog UI to do the summation across  index  values?",0.0,0.161,0.839,0.7579
datadog,"I could use the agent ID/host, of course, but Datadog charges by number of tag values and we've got our agents in an auto-scaling group, so hosts change frequently.",0.081,0.153,0.766,0.3291
datadog,This seems hacky - is there a better solution?,0.0,0.51,0.49,0.6369
datadog,Am I overthinking this?,0.0,0.0,1.0,0.0
datadog,"I want to setup datadog alert threshold different for each tag value passed, how can I do this?",0.0,0.312,0.688,0.5994
datadog,One possibility is I create separate monitor for each tag value and use IN query for a perticular tag value.,0.0,0.315,0.685,0.7096
datadog,it will be bit hectic to manage those numbers of monitors.,0.0,0.0,1.0,0.0
datadog,DataDog is so useless in its querying and its intuitiveness ...,0.266,0.0,0.734,-0.5598
datadog,I'm looking for a custom exception in the stack trace.,0.0,0.0,1.0,0.0
datadog,"I found individual log entries in the last 18 hours that contain my exception class name, but attempting to write a log query that will find me all the occurrences is returning nothing.",0.0,0.0,1.0,0.0
datadog,E.g.,0.0,0.0,1.0,0.0
datadog,:,0.0,0.0,0.0,0.0
datadog,environment:prod @thrown.extendedStackTrace:UserDoesNotExistException,0.0,0.0,1.0,0.0
datadog,"I'd like to include more words in the query, but even reducing down a single word fails to find anything.",0.165,0.078,0.757,-0.4497
datadog,"I've looked at their documentation, which is zero help.",0.0,0.252,0.748,0.4019
datadog,Do Datadog has the ability to exclude some of the websites from profiling?,0.125,0.151,0.724,0.1027
datadog,"ie, Suppose client has 10 websites hosted in IIS but he needs only 3 websites to be profiled.Is it possible to do so?",0.0,0.0,1.0,0.0
datadog,Any help is highly appreciated :),0.0,0.761,0.239,0.8615
datadog,I am currently working on setting up a monitor to monitor slow queries in the Cloud SQL DB.,0.0,0.0,1.0,0.0
datadog,"I built a custom query to get the processes running on the SQL server, because currently slow query monitoring doesn't report until the process is completed.",0.0,0.0,1.0,0.0
datadog,To get a check every 15-20 seconds (or whatever is configured in DD) of currently running queries over 5 minutes I have this in my DD agent's config.,0.0,0.0,1.0,0.0
datadog,And my results in DD are:,0.0,0.0,1.0,0.0
datadog,As you can see it shows the count of queries that have been running for over 5 minutes.,0.0,0.0,1.0,0.0
datadog,How would I be able to get more information about each query.,0.0,0.0,1.0,0.0
datadog,For example I would like to see the exact query statement that is being executed.,0.0,0.161,0.839,0.3612
datadog,"I know I can use the query:
 SELECT INFO as QUERY FROM INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query'; 
to get the Query statement, but Id like to be able to click on the graph in DD and dig further into each process to see the statements.",0.0,0.075,0.925,0.5023
datadog,Is there a way to add this information or another feature in Datadog where I can  the processes being queried to each process individually?,0.0,0.0,1.0,0.0
datadog,My first thought was to change the custom query to this:,0.0,0.0,1.0,0.0
datadog,But this only returns one row with the count number of all process and the ID and Query of the first result.,0.0,0.065,0.935,0.1154
datadog,I was wondering to know if there is a way to exclude a site from Datadog automatic tracing on IIS.,0.106,0.0,0.894,-0.2263
datadog,I've read the docs but didn't find anything about.,0.0,0.0,1.0,0.0
datadog,"Currently, I see error status for all the authentication errors and it feels like a lot of extra noise in the total errors chart.",0.268,0.089,0.643,-0.6124
datadog,I looked at  https://github.com/DataDog/dd-trace-js/pull/909  and tried to use the custom execute provided for graphql,0.0,0.0,1.0,0.0
datadog,"But still, res with only 403 error is going into error status.",0.415,0.0,0.585,-0.7964
datadog,Please help me with how can I achieve this.,0.0,0.455,0.545,0.6124
datadog,"I'm setting up a timeseries to monitor system problems with the standard levels: Critical, Error, Warn, etc..",0.431,0.0,0.569,-0.7964
datadog,I want to set the colors as follows:,0.0,0.178,0.822,0.0772
datadog,I can't seem to do this.,0.0,0.0,1.0,0.0
datadog,"There is a color selection drop-down, but options aren't colors... they're themes, like &quot;Classic,&quot; &quot;Cool,&quot; &quot;Warm,&quot; etc.",0.0,0.178,0.822,0.5023
datadog,How do I set the color on a line in a time series?,0.0,0.0,1.0,0.0
datadog,"Actually, we have two datadog accounts: Let me consider it has account A and account B.
when I push the message to data dog event using API I am able to see the events in events stream and I am able to see the same thing in logs also in the account A.",0.0,0.0,1.0,0.0
datadog,But when I do the same thing in account B I am able to see the data in event stream but not in logs.,0.0,0.0,1.0,0.0
datadog,can I know what might be the reason ???,0.0,0.0,1.0,0.0
datadog,Am I missing something to enable ??,0.339,0.0,0.661,-0.3736
datadog,if so can someone help me with this?,0.0,0.297,0.703,0.4522
datadog,BELOW CODE IS USED TO PUSH THE EVENT TO DATA EVENTS STREAM.,0.0,0.0,1.0,0.0
datadog,I query for a particular URL string like  https://docs.python.org/3/reference/datamodel.html  in my logs.,0.0,0.217,0.783,0.3612
datadog,The URL contains lots of special reserved characters.,0.0,0.278,0.722,0.4019
datadog,"Escaping each char as stated by  official doc  works, but requires too much hustle:",0.0,0.078,0.922,0.0258
datadog,https\:\/\/docs.python.org\/3\/reference\/datamodel.html,0.0,0.0,1.0,0.0
datadog,Is there any easier way to escape all special chars in the string?,0.0,0.419,0.581,0.7351
datadog,As the title suggests  @Trace  annotation is not working with Kotlin Grpc Coroutines.,0.0,0.0,1.0,0.0
datadog,Is there a way to make it work?,0.0,0.0,1.0,0.0
datadog,Unfortunately this gives no error or warning.,0.764,0.0,0.236,-0.8271
datadog,Can I construct Trace programatically and would it work for a  suspend  function?,0.187,0.0,0.813,-0.3182
datadog,I want to create a monitor in datadog that fires an alert if the current value is larger than the maximum value of last month.,0.0,0.366,0.634,0.8126
datadog,something like this:,0.0,0.556,0.444,0.3612
datadog,The purpose of this monitor is to check if the monthly increment is larger than 15%.,0.0,0.0,1.0,0.0
datadog,anyone know if this is possible?,0.0,0.0,1.0,0.0
datadog,"I am using statsD ( hot-shots ) to try log events in datadog such as when a new item is created and by who but, when I am calling  statsD.event('title', 'description')  I do not see any events in datadog metrics.",0.0,0.043,0.957,0.128
datadog,My statsD client is setup like this:,0.0,0.294,0.706,0.3612
datadog,and then I call the event method like so:,0.0,0.263,0.737,0.3612
datadog,"In the metrics section of datadog, I do not appear to be seeing any events come through.",0.0,0.0,1.0,0.0
datadog,The only way I can see some logs there is using  increment,0.0,0.0,1.0,0.0
datadog,But increment only seems to tell me how many times that action is called and I am unable to log additional data such as username and alias.,0.0,0.0,1.0,0.0
datadog,I am pretty sure my statsD client and datadog configs are setup correctly as I am able to see data from increment so I suspect it is to do with the way I am trying to use the event method.,0.069,0.133,0.798,0.3887
datadog,Am I using the event method incorrectly?,0.0,0.0,1.0,0.0
datadog,Perhaps I am checking in the wrong place in datadog?,0.279,0.0,0.721,-0.4767
datadog,Perhaps I should be using increment?,0.0,0.0,1.0,0.0
datadog,How can I log events along with associated data in datadog using statsD?,0.0,0.0,1.0,0.0
datadog,"In Datadog AP, service map view, we can see a lines connecting services.",0.0,0.0,1.0,0.0
datadog,Is there a way to filter the traces going from one service to the other?,0.0,0.0,1.0,0.0
datadog,I've tried to &quot;Inspect&quot; one service and then click &quot;View related Traces&quot; on the other service but that returns all traces for the other service.,0.0,0.0,1.0,0.0
datadog,"From everything I have read, I should be able to do this:",0.0,0.0,1.0,0.0
datadog,It looks like it built correctly:,0.0,0.333,0.667,0.3612
datadog,"But when I check in the container to see if it installed in  C:\Program Files\Datadog , I am not seeing any of the files I am expecting from the installation.",0.0,0.0,1.0,0.0
datadog,I added the flag to give the install some extra logs (/L*V C:\install.log) but didn't see much in there.,0.0,0.0,1.0,0.0
datadog,"I have confirmed the msiexec command works on a Windows host, just not in the Docker build from what I can tell.",0.0,0.0,1.0,0.0
datadog,Is there something simple that I'm missing?,0.268,0.0,0.732,-0.296
datadog,"Using Nlog to log from my asp.net Core application, I would like to review the logs in Datadog.",0.0,0.135,0.865,0.3612
datadog,"Datadog allows me to visualize the log data, and slice, search, select and sort logs in a convenient way to provide support to my customers.",0.0,0.105,0.895,0.4019
datadog,"I was looking for a way to use NLog to directly post to the Datadog API, so I do not need to use the Windows Agent to collect the logs.",0.0,0.0,1.0,0.0
datadog,"Below how to do this, as I could not find the answer anywhere.",0.0,0.0,1.0,0.0
datadog,"I need to develop a Datadog dashboard which will monitor metrics, logs of the applications running in AWS EC2.",0.0,0.0,1.0,0.0
datadog,At the same time i have some need to send some messages to Application from Datadog Dashboard.,0.0,0.0,1.0,0.0
datadog,Is it possible to do that?,0.0,0.0,1.0,0.0
datadog,If it is not what are the alternative i can use to achieve this.,0.0,0.0,1.0,0.0
datadog,I'm having trouble setting up monitor that will alert me when an event hasn't happened since some period of time following another event.,0.104,0.085,0.811,-0.128
datadog,"Basically, for a given task in my application, I have a log that indicates a state of &quot;running&quot; and another log that indicates a state of &quot;finished&quot;.",0.0,0.0,1.0,0.0
datadog,"From these logs, I've defined two custom metrics in datadog.",0.0,0.0,1.0,0.0
datadog,I'm trying to set up a monitor that will alert me when a task has not finished within 2 hours of when it started running.,0.0,0.095,0.905,0.296
datadog,"So for example, if the running metric is observed at 2:00, the monitor shouldn't alert for the absence of the finished until 4:00.",0.079,0.0,0.921,-0.2235
datadog,"If the finished metric is observed before 4:00, the monitor will not alert for this task.",0.112,0.0,0.888,-0.2235
datadog,"The way that I've tried implementing this is by using a threshold monitor, and subtracting the count of my running metric from the count of my finished metric.",0.0,0.0,1.0,0.0
datadog,"However, what's challenging here is the time-delta piece.",0.0,0.186,0.814,0.1531
datadog,"I've tried using the delay evaluation (delaying by 2 hrs), however, at the time when it starts evaluating, it will only take into account the first metric.",0.084,0.0,0.916,-0.3182
datadog,"It basically, just slides the window back.",0.0,0.0,1.0,0.0
datadog,I want to take some action on the app server based on the server utilisation.,0.0,0.091,0.909,0.0772
datadog,The monitoring on the server is done by datadog.,0.0,0.0,1.0,0.0
datadog,So is it possible to take an action on the server using datadog ?,0.0,0.0,1.0,0.0
datadog,I try to setup a postgres check using DD Agent and i'm getting an error thrown by postgres.py script.,0.144,0.0,0.856,-0.4019
datadog,"As you can see in the screenshot, i'm using this simple query to  get the number of active connections to a db.",0.0,0.174,0.826,0.4588
datadog,I've put it inside the /etc/datadog-agent/conf.d/postgres.d/conf.yaml like this :,0.0,0.263,0.737,0.3612
datadog,The error i get when i run a config check is the following :,0.231,0.0,0.769,-0.4019
datadog,If i understood correctly the conf.yaml file is used to call the postgres.py script with certain parameters.,0.0,0.123,0.877,0.2732
datadog,"The postgres.py script can be found here :
 https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/postgres.py",0.0,0.0,1.0,0.0
datadog,I have a problem with reading a DataDog chart.,0.351,0.0,0.649,-0.4019
datadog,In DataDog latency breakdown chart I see that one method call took 24.3s.,0.0,0.0,1.0,0.0
datadog,"In that method I have DataDog scope logging (operation1, operation2 and operation3) - sum of these three operations is about 1.61% of total time while whole method took about 97.7%.",0.0,0.0,1.0,0.0
datadog,Also SQL operations listed below have a low impact on that method.,0.174,0.0,0.826,-0.2732
datadog,Where does that difference come from?,0.0,0.0,1.0,0.0
datadog,We want to integrate our Kafka Server with our remote Datadog server.,0.0,0.106,0.894,0.0772
datadog,"Due some policies, we decide to use tunnel instead Datadog Agent.",0.0,0.0,1.0,0.0
datadog,"We have set JMX port for each instance (3 Zk, 3 Brokers and 12 Kafka Connect Workers) with same format service background like this:",0.0,0.102,0.898,0.3612
datadog,Each instance has its own port.,0.0,0.0,1.0,0.0
datadog,We found that When we try to curl   curl localhost:19999\metrics  to test the JMX it returns empty which indicate we miss something to collect the JMX report,0.12,0.0,0.88,-0.34
datadog,"However, from  Datadog tutorial to integrate kafka with DD , they use jmxfetch which the instalation need DDAgent.",0.0,0.0,1.0,0.0
datadog,We want to know if there's another alternatives to integrate Kafka Server into Datadog without Agent and rely on tunneling.,0.0,0.064,0.936,0.0772
datadog,I'm trying to test out creating a monitor for google pub sub and am getting an &quot;Invalid Query&quot; error.,0.129,0.105,0.766,-0.128
datadog,"This is the query text when i view source of another working monitor, so i'm confused as to why this isn't working.",0.114,0.0,0.886,-0.3774
datadog,Error:   Error: error creating monitor: 400 Bad Request: {&quot;errors&quot;:[&quot;The value provided for parameter 'query' is invalid&quot;]},0.443,0.176,0.382,-0.7906
datadog,Terraform:,0.0,0.0,1.0,0.0
datadog,I am parsing my log statement like below,0.0,0.294,0.706,0.3612
datadog,"I want to extract all values of &quot;ADD&quot;,&quot;UPDATE&quot; under &quot;XE&quot;; Add (sum) them up ; and convert it into a metric.",0.0,0.2,0.8,0.4588
datadog,"Also the depth of ADD within XE can vary with logs and the number of add, update statements can be zero or more.",0.0,0.056,0.944,0.0772
datadog,"I was successful in parsing and displaying values under JSON tree, but since the child depth and a number of occurrences vary, I am not able to achieve what I want.",0.0,0.237,0.763,0.631
datadog,Any help here would be highly appreciated.,0.0,0.557,0.443,0.7425
datadog,"When a message is formatted as json, it is automatically turned into attributes.",0.0,0.0,1.0,0.0
datadog,"It seems like attributes cant be queried without first being turned into facets (which only applies to new log lines, and means you sometimes have to see something show up, then facetize it, then debug it).",0.0,0.067,0.933,0.3612
datadog,"Is there a way to query the message directly, bypassing the attribute facet requirement?",0.0,0.0,1.0,0.0
datadog,i'm using datadog-agent Agent 7.21.1.,0.0,0.0,1.0,0.0
datadog,Currently i'm working on gathering SNMP data from a Nimble storage device.,0.0,0.0,1.0,0.0
datadog,I already converted the mib file into Python format and i'm able to retrieve metrics using SNMP GET.,0.0,0.0,1.0,0.0
datadog,Inside /etc/datadog-agent/snmp.d/conf.yaml i've setup the following (Some values are censored):,0.0,0.231,0.769,0.4019
datadog,DataDog retrieves the all the metrics until the last one &quot;volIoReads&quot;.,0.0,0.0,1.0,0.0
datadog,Using a cli tool i can read out the values using the OID:,0.0,0.213,0.787,0.4019
datadog,Using the OID inside snmp conf it doesn't work either:,0.0,0.0,1.0,0.0
datadog,Tried also:,0.0,0.0,1.0,0.0
datadog,But no luck.,0.359,0.513,0.128,0.296
datadog,Does anybody know what's going on?,0.0,0.0,1.0,0.0
datadog,We have quite a lot of those endpoints and am trying to set my expectations before i start the POC.,0.0,0.0,1.0,0.0
datadog,Were the steps in the documentation enough to integrate the DDagent to the websphere startup ?,0.0,0.0,1.0,0.0
datadog,"Is there a way of getting Jaeger traces to Datadog, whether it be through a proxy, scraping traces from Jager and converting them to DD Traces, etc...",0.0,0.0,1.0,0.0
datadog,"We have a vendor provided backend that only supports Jaeger, but the enterprise APM solution is Datadog.",0.0,0.251,0.749,0.5719
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,"I'm trying to integrate DataDog with my NodeJS/Express application, however it appears that when a POST request is sent to my app the body of the POST is not being passed along to datadog, how can I fix this?",0.0,0.0,1.0,0.0
datadog,I have a file called  Winston.js  which looks like so:,0.0,0.263,0.737,0.3612
datadog,And then I'm attaching them to my app using the following:,0.0,0.0,1.0,0.0
datadog,Currently my logs in DataDog look like this:,0.0,0.263,0.737,0.3612
datadog,I created an event monitor that catches events with errors and notifies about the alert in a special messenger.,0.108,0.309,0.583,0.5423
datadog,"Everything worked out for me, but I noticed that such alerts are auto-recovered on their own for some time.",0.0,0.0,1.0,0.0
datadog,As I understand it is because of this parameter:,0.0,0.0,1.0,0.0
datadog,"So, datadog catches event, then sets event-monitor in alert status, then wait 5min-48hours and if there are no new events, it is auto-recovered and set status from ""Alert"" to ""OK"".",0.068,0.068,0.864,0.0
datadog,It absolutely does not suit me.,0.0,0.0,1.0,0.0
datadog,"Can I somehow configure the monitor that the monitor's status does not change automatically from ""Alert"" to ""OK"" until I change it manually?",0.0,0.0,1.0,0.0
datadog,I want to set a threshold alert for my datadog metric monitor trigger it after 12 hours.,0.0,0.212,0.788,0.3612
datadog,However none of the options are 12 hours.,0.0,0.0,1.0,0.0
datadog,I cant seem to add it in.,0.0,0.0,1.0,0.0
datadog,"I am using  serverless-plugin-datadog , which uses  datadog-lambda-layer  under the hood.",0.0,0.0,1.0,0.0
datadog,"The  docs state , that by using this plugin it is not necessary to wrap a handler anymore.",0.0,0.0,1.0,0.0
datadog,"This is, by the way, the main reason why I decided to go for it.",0.0,0.0,1.0,0.0
datadog,"The lambda itself is a REST API, which responds with dedicated status codes.",0.0,0.214,0.786,0.4588
datadog,"My question now is, how can I monitor the number of  4xx  and  5xx  http status codes?",0.0,0.08,0.92,0.0772
datadog,Do I have to define custom metrics in datadog for this to work?,0.0,0.0,1.0,0.0
datadog,"I was under the assumption that the plugin comes with those data out-of-the-box, but it looks like I'm missing an important part here.",0.103,0.2,0.697,0.3919
datadog,"In my understand, usual case is using Datadog agent to send error to Datadog.",0.172,0.0,0.828,-0.4019
datadog,"However, I'd like to know there are some ways to send error to Datagog without Datadog agent.",0.134,0.124,0.743,-0.0516
datadog,"For example, can we send by using Datadog webhooks?",0.0,0.0,1.0,0.0
datadog,"My organization is setting up dashboards for our backend services and after performance testing that we ran, we have noticed that some API calls report http status  N\A .",0.0,0.0,1.0,0.0
datadog,"It is not very helpful, anyone seen something like that?",0.195,0.192,0.613,-0.0126
datadog,Is that a configuration issue?,0.0,0.0,1.0,0.0
datadog,"we are facing a problem using NUnit, Serilog, and Datadog.",0.252,0.0,0.748,-0.4019
datadog,We are working with:,0.0,0.0,1.0,0.0
datadog,All packages are NuGet lastest.,0.0,0.0,1.0,0.0
datadog,This is the Serilog configuration we are using:,0.0,0.0,1.0,0.0
datadog,We are testing this configuration both in debug run (F5 key in Visual Studio) and under the NUnit test environment (in Visual Studio).,0.0,0.0,1.0,0.0
datadog,The problem we are facing is that while in debug run all work fine:,0.164,0.109,0.727,-0.2263
datadog,when we run this code in the NUnit environment:,0.0,0.0,1.0,0.0
datadog,but  no logs arrive at Datadog .,0.359,0.0,0.641,-0.4215
datadog,"Checking the network stream with Fiddler we notice that while in debug run, logs are sent to Datadog, under the NUnit environment  logs are NOT sent to Datadog .",0.0,0.0,1.0,0.0
datadog,Any ideas or suggestions?,0.0,0.0,1.0,0.0
datadog,Thank you,0.0,0.714,0.286,0.3612
datadog,I need to monitor fifty application.,0.0,0.0,1.0,0.0
datadog,As apart of which I need to perform healthcheck on datadog dashboard to all the application everyday.,0.0,0.0,1.0,0.0
datadog,"So, Is it possible to collect the metrics collected in datadog from Java code
..",0.0,0.0,1.0,0.0
datadog,Thanks in advance.,0.0,0.592,0.408,0.4404
datadog,"I want to filter logs that either don't have a facet, say half of my logs have some  @facet  but I want the other half",0.0,0.115,0.885,0.1531
datadog,"I tried  -@facet ,  @facet:""""  and  NOT @facet   but doesn't work and google doesn't help",0.194,0.0,0.806,-0.438
datadog,"Feels like there is an easy way for doing this, halp",0.0,0.375,0.625,0.6597
datadog,I'm currently running a spring boot application as container into a kubernetes cluster.,0.0,0.0,1.0,0.0
datadog,Datadog agent is running as containers on the cluster.,0.0,0.0,1.0,0.0
datadog,I have modified the container image build to include the datadog agent before running the application:,0.0,0.0,1.0,0.0
datadog,I also setup the environment variable to indicate the HOST IP of the agent to my container via the Deployment file.,0.0,0.0,1.0,0.0
datadog,The problem now is i'm getting this class not found exception when the application starts:,0.162,0.0,0.838,-0.4019
datadog,"Quite straigtforward, i need to include some dependencies into the application package.",0.0,0.0,1.0,0.0
datadog,But i could not find anything useful on datadog website nor maven central repository.,0.206,0.0,0.794,-0.4782
datadog,Including the agent itself or the api libraries fix nothing.,0.0,0.0,1.0,0.0
datadog,This class is present on the agent but under a different path.,0.0,0.0,1.0,0.0
datadog,Does anybody know which dependencies should be included in the classpath of the application to fix that ?,0.0,0.0,1.0,0.0
datadog,I have the above datadog json template with me which I have to just import in terraform instead of recreating it as terraform dsl.,0.0,0.0,1.0,0.0
datadog,"Okay, here is my setup:",0.0,0.322,0.678,0.2263
datadog,"Question: So I am running Ubuntu 18.04LTS instances and as time goes on, it seems to spawn additional devices periodically:",0.0,0.0,1.0,0.0
datadog,"device:/dev/loop1, /dev/loop2 and so on.",0.0,0.0,1.0,0.0
datadog,"When I first spun up these instances, there were only 3 /dev/loop(1-3) devices, however, over time, a /dev/loop4 showed up and our drive space alert paged me since these are 100% utilized when created.",0.0,0.127,0.873,0.4939
datadog,"So, I have to go into each of the monitors (one per environment) and add an exclusion for the new /dev/loop4, but I cannot set the exclusion until it has been created by at least one of the monitored instances.",0.105,0.06,0.835,-0.2263
datadog,Is there a way in DataDog that you can just add a blanket exclusion like:,0.14,0.159,0.701,0.0772
datadog,device:/dev/loop*?,0.0,0.0,1.0,0.0
datadog,"I have been combing through documentation and have not been able to find anything, so I thought I would ask here.",0.0,0.0,1.0,0.0
datadog,"I have a datadog count metric that I want to create a new metric from which shows the difference between two agent points on the metric, so I can see the change between points.",0.0,0.112,0.888,0.34
datadog,Is there a way to create a metric from another metric using the datadog dashboard.,0.0,0.149,0.851,0.2732
datadog,I'm using the DataDog Helm chart to install the DataDog agent on my EKS Kubernetes clusters ( https://github.com/helm/charts/tree/master/stable/datadog ).,0.0,0.0,1.0,0.0
datadog,The problem I'm having now is that I am not able to filter logs by cluster name.,0.153,0.0,0.847,-0.4019
datadog,I have also set the  DD_CLUSTER_NAME  environment variable but it does not seem to do anything.,0.0,0.0,1.0,0.0
datadog,I have set the following in my values.yml file:,0.0,0.0,1.0,0.0
datadog,I've installed the DataDog agent on my Kubernetes cluster using the Helm chart ( https://github.com/helm/charts/tree/master/stable/datadog ).,0.0,0.0,1.0,0.0
datadog,This works very well except for one thing.,0.0,0.255,0.745,0.3384
datadog,I have a number of Redis containers that have passwords set.,0.0,0.14,0.86,0.0772
datadog,This seems to be causing issues for the DataDog agent because it can't connect to Redis without a password.,0.0,0.0,1.0,0.0
datadog,I would like to either disable monitoring Redis completely or somehow bypass the Redis authentication.,0.0,0.161,0.839,0.3612
datadog,If I leave it as is I get a lot of error messages in the DataDog container logs and the redisdb integration shows up in yellow in the DataDog dashboard.,0.135,0.0,0.865,-0.4404
datadog,What are my options here?,0.0,0.0,1.0,0.0
datadog,Recent Tenable scan highlighted an issue with certain versions of datadog versions.,0.0,0.16,0.84,0.2732
datadog,This is also brought to attention in Datadog monitor.,0.0,0.0,1.0,0.0
datadog,Critical bug in Windows Agent versions 6.14.0 and 6.14.1.,0.223,0.0,0.777,-0.3182
datadog,See --   http://dtdg.co/win-614-fix  &lt;-- for steps to fix the issue.,0.0,0.0,1.0,0.0
datadog,As the bulk of our servers are hosted on AWS - just wondered if I could query this through AWS CLI to list which servers were using the affected versions.,0.056,0.0,0.944,-0.1531
datadog,"I want to disable all builtin metrics (jvm, cpu, etc) but keep my custom metrics.",0.0,0.081,0.919,0.0387
datadog,When I enabled Spring Boot Actuator metrics together with Datadog I end up with +320 metrics sent to datadog.,0.0,0.0,1.0,0.0
datadog,"Most of these metrics are from the  builtin core metrics  (JVM metrics, CPU metrics, File description metrics) only 5 of those metrics are my custom metrics that are the ones that I want to send to datadog.",0.0,0.037,0.963,0.0772
datadog,According to  this section of the Spring Boot documentation :,0.0,0.0,1.0,0.0
datadog,Spring Boot also configures built-in instrumentation (i.e.,0.0,0.0,1.0,0.0
datadog,"MeterBinder
  implementations) that  you can control via configuration or dedicated
  annotation markers",0.0,0.214,0.786,0.4588
datadog,but there is no direct example on how to exclude the those metrics,0.319,0.0,0.681,-0.631
datadog,From what I found in  this other SO question  one way to control it is:,0.0,0.0,1.0,0.0
datadog,and that removes all the metrics except the JVM ones.,0.0,0.0,1.0,0.0
datadog,But it  also removes my custom metrics .,0.0,0.0,1.0,0.0
datadog,I don't see how can I reenable my custom metrics.,0.0,0.0,1.0,0.0
datadog,Just for the record the way I register the custom metrics is this way:,0.0,0.0,1.0,0.0
datadog,"This works ok, as long as `management.metrics.enable.all=true",0.0,0.268,0.732,0.296
datadog,"So how can I disable all core metrics , but keep my custom metrics?",0.0,0.0,1.0,0.0
datadog,I need to implement tracing opentracing (opentelementary) for datadog in my Spring Boot application with rest controller.,0.0,0.0,1.0,0.0
datadog,"I have a given kubernetes endpoint, to which I should send traces.",0.0,0.0,1.0,0.0
datadog,I am running the datadog agent using docker,0.0,0.0,1.0,0.0
datadog,I want to send custom metrics using dogstatsd.,0.0,0.178,0.822,0.0772
datadog,When I run,0.0,0.0,1.0,0.0
datadog,I can see in wireshark that the udp packet was successful from the source to the destination but this metric is not being submitted to datadog.,0.0,0.091,0.909,0.34
datadog,Am I missing some configuration?,0.423,0.0,0.577,-0.296
datadog,I have an alert in Datadog when CPU Credits are low.,0.152,0.341,0.507,0.3818
datadog,"The problem is when I create a new RDS in Amazon, initially it has 0 CPU credits and I receive this alert.",0.11,0.359,0.531,0.6249
datadog,How can I avoid this case?,0.355,0.0,0.645,-0.296
datadog,"I tried to find ""time since creation"" metric, but with no success.",0.161,0.379,0.46,0.5859
datadog,I have a service and send Datadog events from it using com.github.arnabk.java-dogstatsd-client.,0.0,0.0,1.0,0.0
datadog,In order to send json string I use  JsonObject  where I put all properties which I need then convert it to string using  toString()  method on  JsonObject  and send string as a message body.,0.0,0.0,1.0,0.0
datadog,Everything works perfect unless I have a character in a string which is not from english alphabet.,0.0,0.222,0.778,0.5719
datadog,Example: µ.,0.0,0.0,1.0,0.0
datadog,"In this case instead of having correct json  {""Smth"":""µ""}  in Datadog I'm getting incorrect string without closing curly brace  {""Smth"":""µ"" .",0.0,0.0,1.0,0.0
datadog,Has anybody experienced the same and knows how to deal with this?,0.0,0.0,1.0,0.0
datadog,"I use datadog agent 6.9 that run on my host(not on a docker), 
and i also run several application on my host (docker images).",0.0,0.0,1.0,0.0
datadog,I try to avoid sending specific logs to the datadoghq from my mongodb.,0.167,0.0,0.833,-0.296
datadog,"So according to  https://docs.datadoghq.com/logs/log_collection/?tab=tailexistingfiles 
I create mongo.d directory and conf.yaml inside that look like:",0.0,0.295,0.705,0.5574
datadog,But when i restart my agent it's still send the unwanted logs to my datadoghq.,0.153,0.0,0.847,-0.3291
datadog,"Thanks in advance for the help,
Baruch",0.0,0.528,0.472,0.6808
datadog,I'm using  Datadog dashboard to monitor Aurora clusters  I have in my account.,0.0,0.0,1.0,0.0
datadog,"The "" query-volume "" section is always empty, even if I go the mysql shell and do a couple of selects.",0.101,0.0,0.899,-0.2023
datadog,I'd like to make sure it works before I put high load on my db in production.,0.0,0.255,0.745,0.5859
datadog,for now I only see changes in query-volume section in the charts of  Select latency and DML latency  and in AWS resource metrics in all charts.,0.0,0.0,1.0,0.0
datadog,"whereas DiskIO section is totally empty, ( Connection and Replication is empty as well, but I know that because I don't have a replica )",0.143,0.072,0.785,-0.1388
datadog,Any idea how can I make sure it works?,0.0,0.247,0.753,0.3182
datadog,I'm running an Azure Cloud Service with a WebRole.,0.0,0.0,1.0,0.0
datadog,"We run the DataDog Agent on each of our server instances, by running a startup task that executes a .cmd file.",0.0,0.0,1.0,0.0
datadog,"Previously we have been using the latest version of DataDog Agent 5, and installing it using this -",0.0,0.0,1.0,0.0
datadog,"Now we are trying to upgrade to the latest version of DataDog Agent 6 using this, which is failing to install and register the instance as an available host in DataDogs dashboard -",0.099,0.0,0.901,-0.5106
datadog,The URL is of course different in each case.,0.0,0.0,1.0,0.0
datadog,I used the  DataDog Audit bundle  in order to log every action that happens in my MySQL database.,0.0,0.0,1.0,0.0
datadog,However when I check the diff column in the audit_log table I can't find the ID of the respective entity that have been updated/inserted/deleted etc.,0.0,0.113,0.887,0.4215
datadog,I also can't find which user is responsible for a certain action.,0.0,0.355,0.645,0.5267
datadog,Does anyone know if the DataDog audit bundle saves the ID of the entity to which action are performed and if this is the case where I can retrieve this data?,0.0,0.0,1.0,0.0
datadog,I have a MongoDB using the database profiler to collect the slowest queries.,0.0,0.0,1.0,0.0
datadog,How can I send this information to Datadog and analyze it in my Datadog dashboard?,0.0,0.0,1.0,0.0
datadog,"i have some problem to settings my dashboard metrics in datadog, the case is about current connection of my apps, for example when there is user connected my app the value goes add by 1, but when its disconnected it will reduced the value by 1. the problem when im using datadog, they will evaluate based on timestamp, so for example if i want to check per 5 minutes, when first 5 minutes there is 10 users connected it will add by 10 the monitoring show 10, it should not be a problem, but the problem when the next 5 minutes when there is 5 disconnected users, it will reduce the value by 5, and it should be  5  not  -5 .",0.099,0.074,0.828,-0.631
datadog,is there any function that i used to somehow ignore the timestamp in datadog ?,0.172,0.0,0.828,-0.3612
datadog,additional information with last case i mention earlier if the next 5 hours there is no user that connected / disconnected again it should be show as 5 users regardless what time batch series i take.,0.068,0.0,0.932,-0.296
datadog,is that possible to do that in datadog ?,0.0,0.0,1.0,0.0
datadog,I am trying to set up hazelcast metrics pushed to datadog.,0.0,0.0,1.0,0.0
datadog,I followed below documents.,0.0,0.0,1.0,0.0
datadog,DD for HazelCast,0.0,0.0,1.0,0.0
datadog,This is what I did:,0.0,0.0,1.0,0.0
datadog,"Now, I have to do same in ci pipeline in my org.",0.0,0.0,1.0,0.0
datadog,So I have to pass these annotation in --set,0.0,0.0,1.0,0.0
datadog,You can see it is little bit complicated:,0.0,0.0,1.0,0.0
datadog,It shows pods running with annotations like this,0.0,0.263,0.737,0.3612
datadog,What I also tried:,0.0,0.0,1.0,0.0
datadog,I see pods running but no metrics in Datadog.,0.286,0.0,0.714,-0.4215
datadog,I am 99% sure that json is screwing things up.,0.17,0.205,0.625,0.1027
datadog,Also I tried jq.,0.0,0.0,1.0,0.0
datadog,"where,",0.0,0.0,1.0,0.0
datadog,and,0.0,0.0,1.0,0.0
datadog,and I got,0.0,0.0,1.0,0.0
datadog,Any help is appreciated.,0.0,0.75,0.25,0.7184
datadog,We have a requirement where we need to send airflow metrics to datadog.,0.0,0.0,1.0,0.0
datadog,"I tried to follow the steps mentioned here
 https://docs.datadoghq.com/integrations/airflow/?tab=host",0.0,0.0,1.0,0.0
datadog,"Likewise, I included statsD in airflow installation and updated the airflow configuration file (Steps 1 and 2)",0.0,0.0,1.0,0.0
datadog,"After this point, I am not able to figure out how to send my metrics to datadog.",0.0,0.0,1.0,0.0
datadog,Do I follow the Host configurations or containarized configurations?,0.0,0.0,1.0,0.0
datadog,"For the Host configurations, we have to update the datadog.yaml file which is not in our repo and for containerized version, they have specified how to do it for Kubernetics but we don't use Kubernetics.",0.0,0.0,1.0,0.0
datadog,We are using airflow by creating a docker build and running it over on Amazon ECS.,0.0,0.231,0.769,0.4404
datadog,We also have a datadog agent running parallely in the same task (not part of our repo).,0.0,0.0,1.0,0.0
datadog,However I am not able to figure out what configurations I need to make in order to send the StatsD metrics to datadog.,0.0,0.0,1.0,0.0
datadog,Please let me know if anyone has any answer.,0.0,0.223,0.777,0.3182
datadog,I am new to DataDog and getting back into working with Windows Servers.,0.0,0.0,1.0,0.0
datadog,"I am trying to push Event Viewer logs (Security, System, etc) to Datadog logs.",0.0,0.0,1.0,0.0
datadog,I have been successful in terms of setting it up (used their documentation -  https://docs.datadoghq.com/integrations/win32_event_log/ ).,0.0,0.226,0.774,0.5859
datadog,I am getting logs into my DD for that server for my System and Security:,0.0,0.156,0.844,0.34
datadog,I know that you can push items from the Event Viewer to Events in DD by using  Instances  and you can be more granular there.,0.0,0.0,1.0,0.0
datadog,But I want that granularity in the logs sections since we rarely view Events.,0.0,0.108,0.892,0.1154
datadog,"Right now it is showing me all the items in the logs, success, etc.",0.0,0.222,0.778,0.5719
datadog,I am looking to only get the Errors and Warnings piped to the Logs.,0.295,0.0,0.705,-0.5574
datadog,Thanks for the help.,0.0,0.737,0.263,0.6808
datadog,D,0.0,0.0,0.0,0.0
datadog,I am having trouble figuring out how the datadog forward encodes/encrypts its messages from the datadog forwarder.,0.153,0.0,0.847,-0.4019
datadog,We are utilizing the forwarder on datadog using the following documentation:  https://docs.datadoghq.com/serverless/forwarder/  .,0.0,0.0,1.0,0.0
datadog,"On that page there, Datadog has an option to send the same event to another lambda that it invokes via the AdditionalTargetLambdaARNs flag.",0.0,0.0,1.0,0.0
datadog,"We are doing this and having the other lambda invoke but the event input that we are getting is long string that looks like it is base64 encoded but when I put it into a base64 decoder, I get gibberish back.",0.0,0.081,0.919,0.5023
datadog,I was wondering if anyone knew how datadog is compressing/encoding/encrypting their data/logs that they send so that I can read the logs in my lambda and be able to preform actions off of the data being forwarded?,0.0,0.0,1.0,0.0
datadog,I have been searching google and the datadog site for documentation on this but I can't find any.,0.0,0.0,1.0,0.0
datadog,"I'm Using,",0.0,0.0,1.0,0.0
datadog,But I'm not able to get the operation name and endpoints associated with service.,0.0,0.0,1.0,0.0
datadog,Help me to find the correct url link.,0.0,0.278,0.722,0.4019
datadog,"Getting query invalid for below monitor, Please suggest.",0.0,0.247,0.753,0.3182
datadog,"Taking a look at  Redis metrics for Datadog  , we can see that  redis.cpu.sys  refers to  System CPU consumed by the Redis server.",0.0,0.0,1.0,0.0
datadog,"But this is metric for CPU usage time, not CPU usage.",0.0,0.0,1.0,0.0
datadog,How do we do if we want to create alerts based on the actual CPU usage?,0.0,0.195,0.805,0.34
datadog,Am looking for information on configuring Datadog to perform event correlation using custom event attributes (application components output events records in JSON format).,0.0,0.0,1.0,0.0
datadog,"Also, is it possible in Datadog to then configure notifications based on correlated events?",0.0,0.0,1.0,0.0
datadog,Appreciate any pointers on the above or where I can get the above information.,0.0,0.184,0.816,0.4019
datadog,TIA.,0.0,1.0,0.0,0.5106
datadog,RKH,0.0,0.0,1.0,0.0
datadog,I want to monitor our application's usage of a 3rd party API with datadog.,0.0,0.286,0.714,0.4588
datadog,We have a daily quota of x calls we are allowed to perform every day.,0.0,0.0,1.0,0.0
datadog,The number of calls resets every day at midnight.,0.0,0.14,0.86,0.0772
datadog,I have added a datadog metric in our code that increases every time we make a call to that API.,0.0,0.0,1.0,0.0
datadog,Now I want to create a monitor that will alert us whenever we reach 80% of our allowed daily calls.,0.0,0.324,0.676,0.5719
datadog,"In other words, I want to get the calls we made from the beginning of the day.",0.0,0.08,0.92,0.0772
datadog,Is that possible with datadog?,0.0,0.0,1.0,0.0
datadog,Hey I am trying to logging within a script and send a log message to Datadog via their api: POST  https://http-intake.logs.datadoghq.com/v1/input .,0.0,0.0,1.0,0.0
datadog,"My issue is that the code works perfectly well when running on the team's machine but once it's in the pipeline, it keeps throwing 400 errors as a response from Datadog.",0.091,0.121,0.788,0.0129
datadog,The Key is perfectly valid and the log message works on our machine so I cannot see why I am getting a 400 error.,0.104,0.162,0.734,0.3612
datadog,Wondering if anyone else has run into this problem,0.281,0.0,0.719,-0.481
datadog,pic of gitlab-cl.yml,0.0,0.0,1.0,0.0
datadog,"Thanks,",0.0,1.0,0.0,0.4404
datadog,"Java app, built with Gradle, implementing SLF4J and Logback, exporting with Logstash to Datadog agentless logging.",0.0,0.0,1.0,0.0
datadog,"Can't seem to get the  host ,  service , or  source  properties to transmit:",0.0,0.0,1.0,0.0
datadog,Note where I've included the  &lt;host&gt;  and  &lt;service&gt;  tags.,0.0,0.0,1.0,0.0
datadog,I also tried  &lt;property name=&quot;..&quot; value=&quot;..&quot;&gt;  and  &lt;KeyValuePair key=&quot;service&quot; value=&quot;java-app&quot; /&gt;  to no avail.,0.155,0.0,0.845,-0.296
datadog,Here are the docs I'm reading from Datadog:,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/logs/log_collection/java/?tab=log4j#agentless-logging,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/tracing/connect_logs_and_traces/java?tab=log4j2,0.0,0.0,1.0,0.0
datadog,https://www.datadoghq.com/blog/java-logging-guide/,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/logs/log_collection/?tab=host,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/getting_started/tagging/unified_service_tagging/?tab=kubernetes,0.0,0.0,1.0,0.0
datadog,"Also, the docs  for logstash-logback-encoder  itself states:",0.0,0.0,1.0,0.0
datadog,"By default, each property of Logback's Context (ch.qos.logback.core.Context) will appear as a field in the LoggingEvent.",0.0,0.0,1.0,0.0
datadog,"By default, each property of Logback's Context  (ch.qos.logback.core.Context)  will appear as a field in the LoggingEvent.",0.0,0.0,1.0,0.0
datadog,"So, how do I add a property to Logback's Context?",0.0,0.0,1.0,0.0
datadog,"I'd like to export a few metrics from  k8s.io/kubernetes/pkg/proxy/metrics : e.g.,  sync_proxy_rules_duration_seconds  and  sync_proxy_rules_last_queued_timestamp_seconds , what datadog integration shall I use for it?",0.0,0.122,0.878,0.3612
datadog,There's a section of  K8s proxy metrics  that are collected by default but there're not these 2 metrics I'm interested in on that list.,0.0,0.145,0.855,0.5499
datadog,"Moreover, I can't  find  kubernetes proxy integration as well even though there's this  kube-proxy repo  on GitHub.",0.0,0.123,0.877,0.2732
datadog,What is the difference between the  count  and the  gauge  metric types in DataDog?,0.0,0.0,1.0,0.0
datadog,"Or rather, when should I prefer one over the other?",0.0,0.0,1.0,0.0
datadog,The definitions from their website don't help me much:,0.22,0.0,0.78,-0.3089
datadog,Count:,0.0,0.0,1.0,0.0
datadog,The COUNT metric submission type represents the total number of event occurrences in one time interval.,0.0,0.08,0.92,0.0772
datadog,A COUNT can be used to track the total number of connections made to a database or the total number of requests to an endpoint.,0.0,0.11,0.89,0.1531
datadog,This number of events can accumulate or decrease over time—it is not monotonically increasing.,0.0,0.091,0.909,0.0772
datadog,Gauge:,0.0,0.0,1.0,0.0
datadog,The GAUGE metric submission type represents a snapshot of events in one time interval.,0.0,0.0,1.0,0.0
datadog,This representative snapshot value is the last value submitted to the Agent during a time interval.,0.0,0.27,0.73,0.5859
datadog,A GAUGE can be used to take a measure of something reporting continuously—like the available disk space or memory used.,0.0,0.0,1.0,0.0
datadog,"The  count  type seems to be somewhat related to the  rate  type, but for me it is unclear why or when I should use  count  instead of  gauge .",0.088,0.0,0.912,-0.3612
datadog,"I mean in principle a measurement of &quot;something&quot; could always be presented as a gauge, couldn't it?",0.0,0.0,1.0,0.0
datadog,how to write pytest for the below module?,0.0,0.0,1.0,0.0
datadog,I have been trying to write the unit test for the below datadog API monitor creation using python language.,0.0,0.11,0.89,0.2732
datadog,Assume the create method is going to send 200 status.,0.0,0.189,0.811,0.2732
datadog,how do I mock  json[&quot;monitors&quot;][0]['type'] .,0.483,0.0,0.517,-0.4215
datadog,I get,0.0,0.0,1.0,0.0
datadog,Datadog API reference -  https://docs.datadoghq.com/api/latest/monitors/,0.0,0.0,1.0,0.0
datadog,json content:,0.0,0.0,1.0,0.0
datadog,"Is there a way to easily generate reports of alerts from certain monitors in Datadog, on a weekly or biweekly basis?",0.0,0.209,0.791,0.5423
datadog,"Context: At the moment, these alerts go to a Slack channel.",0.0,0.0,1.0,0.0
datadog,Folks have to scroll through the channel to see all the issues and prioritize investigations (during sprint planning).,0.0,0.0,1.0,0.0
datadog,I am trying to make it easy for sprint planners to pull up the alerts report.,0.0,0.172,0.828,0.4404
datadog,I found only a couple related things after googling:,0.0,0.0,1.0,0.0
datadog,"Datadog has a CSV with 6 months of alerts, that you can curl to download.",0.0,0.0,1.0,0.0
datadog,"I guess I could curl, diff with prior week's csv and filter for interesting monitors.",0.0,0.184,0.816,0.4019
datadog,But does not seem like the best solution.,0.162,0.533,0.305,0.7955
datadog,https://docs.datadoghq.com/monitors/faq/how-can-i-export-alert-history/?tab=us,0.0,0.0,1.0,0.0
datadog,An old article about Monitor Trends Report which I can't find in the app.,0.0,0.0,1.0,0.0
datadog,https://www.datadoghq.com/blog/monitor-alert-status/,0.0,0.0,1.0,0.0
datadog,I'm using V6 of the Datadog agent on Ubuntu 18.04.,0.0,0.0,1.0,0.0
datadog,I'll like to change the min_collection_interval for all checks from the default 15 seconds to 30 seconds.,0.0,0.135,0.865,0.3612
datadog,It's unclear from the documentation.,0.333,0.0,0.667,-0.25
datadog,Is this possible?,0.0,0.0,1.0,0.0
datadog,I’m trying to group logs from a source so I can filter them in or out in DataDog logs.,0.0,0.0,1.0,0.0
datadog,"There is already a grok parser that formats the messages, but how can I add a tag to them?",0.0,0.0,1.0,0.0
datadog,DataDog seem to use a subset of LogStash grok parsing rules:  https://docs.datadoghq.com/logs/processing/processors/,0.0,0.0,1.0,0.0
datadog,Eg from Heroku:,0.0,0.0,1.0,0.0
datadog,as,0.0,0.0,1.0,0.0
datadog,"What I'd like is to add something like a  type , so I know they are not from the app, eg",0.0,0.238,0.762,0.6124
datadog,Then I guess I could add the same thing to app logs and add  type: 'app'  to them.,0.0,0.0,1.0,0.0
datadog,I use k6 on my local machine to perform load-testing as well as a  Datadog agent  to visualize the metrics in Datadog.,0.0,0.1,0.9,0.2732
datadog,I'd like to filter k6 metrics in Datadog as the tests aren't distinguishable.,0.0,0.172,0.828,0.3612
datadog,At this point the  $test_run_id  only shows  *  (refer to the screenshot below):,0.0,0.0,1.0,0.0
datadog,"I followed  this the official doc  that suggests to set  include_test_run_id  flag to  true  in k6 config, but I was unsuccessful.",0.147,0.086,0.767,-0.3291
datadog,Here's a k6 config I currently use ( &lt;YOUR_DATADOG_API_KEY&gt;  is replaced with an actual Datadog API key):,0.0,0.0,1.0,0.0
datadog,I have an nginx-pod which redirects traffic into Kubernetes services and stores related certificates insides its volume.,0.0,0.0,1.0,0.0
datadog,I want to monitor these certificates - mainly their expiration.,0.0,0.157,0.843,0.0772
datadog,I found out that there is a TLS integration in Datadog (we use Datadog in our cluster):  https://docs.datadoghq.com/integrations/tls/?tab=host .,0.0,0.0,1.0,0.0
datadog,"They provide sample file, which can be found here:  https://github.com/DataDog/integrations-core/blob/master/tls/datadog_checks/tls/data/conf.yaml.example",0.0,0.0,1.0,0.0
datadog,"To be honest, I am completely lost and do not understand comments of the sample file - such as:",0.124,0.158,0.718,0.1796
datadog,"I want to monitor certificates that are stored in the pod, does it mean this value should be localhost or do I need to somehow iterate over all the certificates that are stored using this value (such as server_names in nginx.conf)?",0.0,0.159,0.841,0.7003
datadog,"If anyone could help me with setting sample configuration, I would be really grateful - if there are any more details I should provide, that is not a problem at all.",0.0,0.256,0.744,0.8048
datadog,i am trying to send kafka consumer metrics to datadog but its not showing in monitoring when I select the node.,0.0,0.0,1.0,0.0
datadog,The server is giving below check in status,0.0,0.255,0.745,0.34
datadog,JMX is as above.,0.0,0.0,1.0,0.0
datadog,Please help in finding what could be wrong.,0.237,0.382,0.382,0.2263
datadog,"I am looking for the criteria which decide whether we could use the built-in feature of Google cloud i.e Stack driver or we need to go for third-party tools like Grafana, Datadog etc in order to carry out Monitoring and logging.",0.0,0.06,0.94,0.3612
datadog,I was trying to install  data-dog  agent in my  Ubuntu 20.04  for monitoring a python backend with the following command,0.0,0.0,1.0,0.0
datadog,From the  official documentation  it says,0.0,0.0,1.0,0.0
datadog,But haven't found any  python.d  inside  /etc/datadog-agent/conf.d .,0.0,0.0,1.0,0.0
datadog,If I create the  python.d/con.yaml  do I need to do anything else for enabling sending logs?,0.0,0.139,0.861,0.2732
datadog,"I have a scheduled Celery task that queries  x  number of rows, does some processing and upon success (or error) it increments a specific metric using ThreadStats.",0.0,0.192,0.808,0.6124
datadog,"For each execution of this task, the metric should be incremented by  x  at a specific time.",0.0,0.0,1.0,0.0
datadog,The problem is that some of these increments are not posted to DataDog.,0.184,0.0,0.816,-0.4019
datadog,Ex.,0.0,0.0,1.0,0.0
datadog,"if the total number of rows is 100 and the task processes  x=10  at a time, some of those task executions fails to increment the metric and it ends up displaying only 60.",0.082,0.038,0.88,-0.3612
datadog,This is what I tried to do without success:,0.3,0.0,0.7,-0.4585
datadog,"I'm trying to collect logs from cron jobs running on our self hosted Github runners, but so far can only see the actual github-runner host logs.",0.0,0.0,1.0,0.0
datadog,I've created a self-hosted Github Runner in AWS running on Unbtu with a standard config.,0.0,0.143,0.857,0.25
datadog,"We've also installed the Datadog agent v7 with their script and basic configuration, and added log collection from files using  these instructions",0.0,0.0,1.0,0.0
datadog,Our config for log collection is below.,0.0,0.0,1.0,0.0
datadog,"After these steps, I can see logs from our Github runners servers.",0.0,0.0,1.0,0.0
datadog,"However, on those runners we have several python cron jobs running in Docker containers, logging to stdout.",0.0,0.0,1.0,0.0
datadog,"I can see those logs in the Github Runner UI, but they're not available in Datadog, and those are the logs I'd really like to capture, so I can extract metrics from.",0.0,0.113,0.887,0.5704
datadog,Do the docker containers for the python scripts need some special datadog setup as well?,0.0,0.27,0.73,0.5859
datadog,Do they need to log to a file that the datadog agents registers as a log file in the setup above?,0.0,0.0,1.0,0.0
datadog,I need to integrate Datadog monitoring on Apache web servers which are on Windows servers.,0.0,0.0,1.0,0.0
datadog,Is there a link/blog available detailing the same for Windows server specifically ?,0.0,0.0,1.0,0.0
datadog,I got a blog link from Datadog but it seems not to cover Windows servers.,0.0,0.0,1.0,0.0
datadog,Need it specifically for Windows servers.,0.0,0.0,1.0,0.0
datadog,Please advise me about Datadog metrics.,0.0,0.315,0.685,0.3182
datadog,"I'm using the  increment  method to send data to Datadog, but I can't see the total number on the Datadog side.",0.0,0.071,0.929,0.1154
datadog,I have specified a sample rate of 1 and am sending everything.,0.0,0.0,1.0,0.0
datadog,"If you look at the following documentation, you will see that &quot;COUNT type metrics can show a decimal value within Datadog since they are normalized over the flush interval to report per-second units .&quot; and stated.",0.0,0.066,0.934,0.34
datadog,https://docs.datadoghq.com/developers/metrics/dogstatsd_metrics_submission/#count,0.0,0.0,1.0,0.0
datadog,Isn't there a way to see the total number on Datadog?,0.0,0.126,0.874,0.0772
datadog,Please let me know if you know.,0.0,0.277,0.723,0.3182
datadog,I have a piece of code:,0.0,0.0,1.0,0.0
datadog,I have to show this time difference in Datadog dashboard.,0.0,0.0,1.0,0.0
datadog,Can anyone help me with this?,0.0,0.351,0.649,0.4019
datadog,P.S.,0.0,0.0,1.0,0.0
datadog,I have the io.micrometer setup in place and MeterRegistry is autowired in the class.,0.0,0.0,1.0,0.0
datadog,Just need to know the method to show such a metric on the dashboard.,0.0,0.0,1.0,0.0
datadog,We are using the eBPF via the Datadog-agent which is installed in a linux server.,0.0,0.0,1.0,0.0
datadog,"More precisely, we are using the nprobe for gathering “NetFlow data” in the Linux server, and then Datadog via eBPF illustrates these flows on a dashboard.",0.0,0.0,1.0,0.0
datadog,"However, we got an issue as the IP-source is always remains the same.",0.0,0.0,1.0,0.0
datadog,Actually is the IP address where the Linux-Server is receiving the “Netflow data”.,0.0,0.0,1.0,0.0
datadog,That's not normal as Netflow is based on a unique pair of ip.source / ip.destination.,0.0,0.0,1.0,0.0
datadog,It seems that eBPF takes as source/reference the traffic that is receiving on linux-NIC,0.0,0.0,1.0,0.0
datadog,Is there any way to modify this behaviour?,0.0,0.0,1.0,0.0
datadog,Re,0.0,0.0,1.0,0.0
datadog,I want to send custom metrics using io.micrometer.datadog.DatadogMeterRegistry to datadog.,0.0,0.14,0.86,0.0772
datadog,Below is the code snippet of the method where I am emitting metrics to Datadog.,0.0,0.0,1.0,0.0
datadog,I am able to see logs &quot;metric sent successfully&quot; with no error but this custom metric is not showing up in Datadog UI under metrics summary.,0.13,0.0,0.87,-0.3506
datadog,Am I missing anything?,0.524,0.0,0.476,-0.296
datadog,Is it possible to graph an SLO as a time-series graph using just native Datadog components?,0.0,0.0,1.0,0.0
datadog,"If so, how?",0.0,0.0,1.0,0.0
datadog,"I can only find a way to show an SLO as a number, I'd like to show how it changes over time in a graph-format.",0.0,0.167,0.833,0.4215
datadog,I am trying to create an alert Datadog using Terraform for when multiple hosts (1 or more)  are at &gt;= 95% CPU usage.,0.0,0.177,0.823,0.5106
datadog,"So far, with the code I have, the alert would trigger anytime a host exceeds the threshold and that is a little too noisy.",0.063,0.097,0.84,0.197
datadog,Would you happen to know how to create the logic to satisfy both conditions before the alert gets triggered?,0.0,0.313,0.687,0.743
datadog,(Alert when Multiple hosts at 95% CPU or higher),0.0,0.0,1.0,0.0
datadog,I am new to the datadog and I have followed the official docs and installed datadog metric in EC2 and collect the metrics.,0.0,0.0,1.0,0.0
datadog,"I am able to successfully view the metrics like EC2, RDS,S3.",0.0,0.416,0.584,0.6908
datadog,I am now trying to integrate API gateway and AWS glue but didnot know how to do that also I couldnot find useful article too.,0.0,0.149,0.851,0.5927
datadog,It would be great if some one help me to achieve my requirement.,0.0,0.382,0.618,0.7783
datadog,Thanks in Advance,0.0,0.592,0.408,0.4404
datadog,How do I set custom &quot;trace_id&quot; for Datadog tracing?,0.0,0.0,1.0,0.0
datadog,I searched high and low but can't find an answer to this.,0.134,0.0,0.866,-0.1406
datadog,I suspect it's not supported.,0.675,0.0,0.325,-0.4874
datadog,Would really appreciate it if I can get some help here.,0.0,0.416,0.584,0.6901
datadog,"As an example, if I can do the following in multiple files, then I can view these spans together in the Datadog UI since they all have the same trace ID:",0.0,0.0,1.0,0.0
datadog,Is there a way to create a delay of n seconds when making datadog event monitors?,0.149,0.136,0.714,-0.0516
datadog,I have a monitor set up to ensure that there are 120 events of a kind received in every 24 hour period.,0.0,0.261,0.739,0.7184
datadog,"The problem is that the monitor goes off when there are ~117-119 events present, and then it resolves immediately after that - when the remaining events come through in a few minutes.",0.083,0.052,0.864,-0.25
datadog,"I want to add a delay of sorts, that will only trigger the monitor if it remains in alarm state for more than 10 minutes at a stretch, rather than triggering alerts as soon as the count dips below 120.",0.117,0.033,0.85,-0.5267
datadog,i am using datadog to monitor my cloud infrastructure(AWS).,0.0,0.0,1.0,0.0
datadog,"at present, i am sending aws-logs to datadog and datadog keeping those log data for some default timeframe.",0.0,0.0,1.0,0.0
datadog,How i can set some limit so that after that particular limit logs will be deleted from datadog?,0.0,0.0,1.0,0.0
datadog,I want to delete datadog logs after 7 days,0.0,0.178,0.822,0.0772
datadog,Can anyone suggest a solution for this.,0.0,0.315,0.685,0.3182
datadog,AWS EC2 instance with Talent Server and agents hosted.two agents are on-perm from where data is pushed to cloud thorugh Talent Job.,0.0,0.219,0.781,0.6808
datadog,Please suggest is there integration avaiable for monitoring Talend job for monitoring on DataDog which is again hosted in Same AWS account.,0.0,0.099,0.901,0.3182
datadog,"I have a bunch of logs that are supposed to be summed up into a value, that I would like to monitor.",0.0,0.234,0.766,0.5994
datadog,I tried sum function in the query widget and in table widget but nether seems to work with logs and not metrics.,0.0,0.0,1.0,0.0
datadog,Anyone did something similar?,0.0,0.0,1.0,0.0
datadog,Thanks,0.0,1.0,0.0,0.4404
datadog,I'm trying to send events to dataDog from c# (unity specifically) but it returns an error 400 every time and I'm at a loss at what else to attempt..,0.2,0.0,0.8,-0.7579
datadog,This is the error it gives me,0.31,0.0,0.69,-0.4019
datadog,"The error is actually on the response line
 var httpResponse = (HttpWebResponse) httpWebRequest.GetResponse();",0.197,0.0,0.803,-0.4019
datadog,"If I comment that line out, the error goes away but I never receive the events on dataDog, which is understandable considering it's saying it's rejecting my post",0.196,0.0,0.804,-0.705
datadog,"Given monitor that displays values grouped by language, e.g.",0.0,0.252,0.748,0.4019
datadog,:,0.0,0.0,0.0,0.0
datadog,"Is it possible to retrieve the actual value of a language to display it in the Monitor Name (to see it in the potential alert,  here )?",0.0,0.161,0.839,0.5574
datadog,"I've tried putting into the name the something like:  Errors found in {{language}}  and  {{language.name}} , but it didn't work.",0.083,0.086,0.831,0.0129
datadog,Maybe variable evaluation is not expected to work in the name of a monitor?,0.0,0.0,1.0,0.0
datadog,Id like to show a sum of all my monitors that are in alert status on my dashboard.,0.0,0.239,0.761,0.5719
datadog,"so for instance if i have
monitor 1, monitor 2, monitor 3",0.0,0.0,1.0,0.0
datadog,"and monitor 1 and monitor 2 are alerting, the dashboard would show",0.0,0.0,1.0,0.0
datadog,Current number of alerts: 2,0.0,0.302,0.698,0.0772
datadog,does anyone know if this is possible?,0.0,0.0,1.0,0.0
datadog,Given following scenario:,0.0,0.0,1.0,0.0
datadog,Right now we monitor a custom error-count metric like  myService.errorType .,0.0,0.238,0.762,0.3612
datadog,"Which gives us an exact number of how many times an error occurred - independent from a specific entity: If an entity can't be processed like 100 times, then the metric value will be  100 .",0.128,0.099,0.773,-0.2755
datadog,"What I'd like to have, though, is a distinct metric based on the UUID.",0.0,0.172,0.828,0.3612
datadog,Example:,0.0,0.0,1.0,0.0
datadog,"Then I'd like to have a metric with the value of  2  - because the processes failed for two entities only (and not for 30, as it would be reported right now).",0.096,0.143,0.76,0.1531
datadog,While searching for a solution I found the possibility of using tags.,0.0,0.204,0.796,0.3182
datadog,But  as the docs point  out they are not meant for such a use-case:,0.0,0.0,1.0,0.0
datadog,"Tags shouldn’t originate from unbounded sources, such as epoch timestamps, user IDs, or request IDs.",0.0,0.0,1.0,0.0
datadog,Doing so may infinitely increase the number of metrics for your organization and impact your billing.,0.0,0.216,0.784,0.4336
datadog,So are there any other possibilities to achieve my goals?,0.0,0.0,1.0,0.0
datadog,I am trying to connect to datadog from my fastapi backend.,0.0,0.0,1.0,0.0
datadog,I am currently trying to do this on localhost using a docker-compose file to let both my datadog-agent and my backend-container run in the same network.,0.0,0.0,1.0,0.0
datadog,Here is a minimal example,0.0,0.0,1.0,0.0
datadog,docker-compose.yml,0.0,0.0,1.0,0.0
datadog,Dockerfile,0.0,0.0,1.0,0.0
datadog,main.py,0.0,0.0,1.0,0.0
datadog,I run docker-compose up and then check the ip of my dd-container with,0.0,0.0,1.0,0.0
datadog,and update it in the compose file.,0.0,0.0,1.0,0.0
datadog,"When I then again run docker-compose up, I get the following error",0.231,0.0,0.769,-0.4019
datadog,Any help would be very appreciated,0.0,0.611,0.389,0.7425
datadog,I am trying to test Datadog and see if it works for my needs... the problem is that I cannot make it works...,0.119,0.0,0.881,-0.4019
datadog,I have been following the guide here:  https://docs.datadoghq.com/integrations/amazon_lambda/  nut no luck... is there anybody who has a step by step guide?,0.109,0.0,0.891,-0.296
datadog,(I have trouble on getting the IAM datadog policy (should I create it manually or AWS coludformation does that for me?),0.118,0.092,0.789,-0.1531
datadog,"I need to test the AMP mainly, because I am curious to see how the service map will work...",0.0,0.126,0.874,0.3182
datadog,any suggestion please?,0.0,0.535,0.465,0.3182
datadog,Thank you,0.0,0.714,0.286,0.3612
datadog,We're using the DataDog agent with some .Net Core micro-services on Linux.,0.0,0.0,1.0,0.0
datadog,We want to send our statsd metrics directly through the DataDog agent but we need to do some filtering before the agent sends the metrics to DataDog.,0.0,0.042,0.958,0.0387
datadog,All I could find was the following:  https://docs.datadoghq.com/tracing/custom_instrumentation/agent_customization/?tab=mongodb,0.0,0.0,1.0,0.0
datadog,"That all appears to be about logging and spans, not metrics.",0.0,0.0,1.0,0.0
datadog,Is it possible to filter the statsd metrics?,0.0,0.0,1.0,0.0
datadog,The question is about DataDog - OpsGenie integration.,0.0,0.0,1.0,0.0
datadog,"Whenever a DataDog monitor triggers an alert an incident is opened in OpsGenie (which is good), but when the monitor recovers back to a healthy state the OpsGenie incident is auto-closed (which is bad).",0.0,0.147,0.853,0.631
datadog,Is there any way to prevent this behavior?,0.0,0.136,0.864,0.0258
datadog,I want to keep incidents open until they are acked and resolved.,0.0,0.25,0.75,0.25
datadog,I am using the dependency  'io.micrometer:micrometer-registry-prometheus'  to Calculate Average request processing time.,0.0,0.0,1.0,0.0
datadog,For that I wrote the following java code,0.0,0.0,1.0,0.0
datadog,"So with that code,  timer  measures the time taken by the runnable (request method) and also counts the number of times the method was called.",0.0,0.051,0.949,0.0772
datadog,As a result two metrics are generated which I can use in Datadog:,0.0,0.0,1.0,0.0
datadog,"Then in order to calculate the average request processing time , I have created a &quot;Query value&quot; graph in datadog and uses the datadog  avg by  function.",0.0,0.08,0.92,0.25
datadog,"But instead of resulting in the  average time , DataDog seems to calculate the  sum of the average request time per event type  and shows an always increasing average request time in seconds (which is wrong because I logged in java code the request times and they are all under 100 milliseconds; so an average must be in a range of milliseconds).",0.067,0.0,0.933,-0.631
datadog,So I suppose that I am doing something wrong.,0.341,0.0,0.659,-0.4767
datadog,My question How do you compute average response time using DataDog graphs?,0.0,0.0,1.0,0.0
datadog,N.B.,0.0,0.0,1.0,0.0
datadog,what I also tried as a solution is to divide count by sum using the datadog function &quot;Add Query&quot;.,0.0,0.126,0.874,0.3182
datadog,It means in short,0.0,0.0,1.0,0.0
datadog,But some colleagues argued that it is not a proper way to calculate the average and that it should be able to calculate the average out-of-the-box using DataDog.,0.111,0.0,0.889,-0.5023
datadog,What do you think about that solution?,0.0,0.277,0.723,0.3182
datadog,Regards,0.0,0.0,1.0,0.0
datadog,Is there a way to connect a Snowflake Snowpipe logging to DataDog.,0.0,0.0,1.0,0.0
datadog,"I would like to setup live monitoring for a Snowpipe, I only know of the debugging tools such as  SYSTEM$PIPE_STATUS   information_schema.copy_histor   information_schema.pipe_usage_histor  and  information_schema.validate_pipe_load  but I would like it to be proactive monitoring and not ad-hoc debugging.",0.0,0.225,0.775,0.8271
datadog,From what I understand I could poll the Snowpipe REST API in an automated way and push the logs to a logging system but I'm wondering if there is no easier access to the Snowpipe logs.,0.075,0.099,0.827,0.2263
datadog,For example to easier load them into DataDog and be alerted whenever something goes wrong.,0.164,0.148,0.688,-0.0772
datadog,Thanks for any pointers!,0.0,0.516,0.484,0.4926
datadog,Cédric,0.0,0.0,1.0,0.0
datadog,I've installed the free trial version of DataDog on my Windows 10 box.,0.0,0.216,0.784,0.5106
datadog,"I'm trying to follow the instructions to monitor a custom file, from  https://docs.datadoghq.com/getting_started/logs/#monitor-a-custom-file",0.0,0.0,1.0,0.0
datadog,I've created a test log file in  C:\dev\tmp\datadog_logs\first.log .,0.0,0.25,0.75,0.25
datadog,"I've edited  C:\ProgramData\Datadog\datadog.yaml , setting  logs_enabled: true .",0.0,0.359,0.641,0.4215
datadog,"I've created a yaml file for custom log collection in  C:\ProgramData\Datadog\conf.d\custom_log_collection.d\conf.yaml , containing:",0.0,0.167,0.833,0.25
datadog,"Then I've restarted the agent, and checked its status (from an Administrator Command Prompt):",0.0,0.0,1.0,0.0
datadog,"From the doc I linked above, I would expect to see a &quot;custom_log_collection&quot; entry under &quot;Logs Agent&quot;.",0.0,0.0,1.0,0.0
datadog,I do not:,0.0,0.0,1.0,0.0
datadog,The obvious place where I might be having problems is in the path to the file.,0.162,0.0,0.838,-0.4019
datadog,"I've tried several variants, e.g., a unix-style path:  /dev/tmp/datadog_logs/first.log , and nothing works.",0.0,0.0,1.0,0.0
datadog,Any ideas as to what is going wrong?,0.307,0.0,0.693,-0.4767
datadog,Or where to find out what might be going wrong?,0.256,0.0,0.744,-0.4767
datadog,I am trying to generate graphs for the AWS Lambdas in Datadog.,0.0,0.0,1.0,0.0
datadog,I want to add titles/units to the x-axis and y-axis.,0.0,0.14,0.86,0.0772
datadog,Please help on this,0.0,0.714,0.286,0.6124
datadog,"Below is the deployment yaml file spec parts, I am trying to update for datadog logging of containers",0.0,0.0,1.0,0.0
datadog,How to use variable &quot;LAUNCH_ID&quot; in the annotations?,0.0,0.0,1.0,0.0
datadog,I am using stats-d [ https://www.npmjs.com/package/node-statsd ] and datadog is connected to it.,0.0,0.0,1.0,0.0
datadog,"I would see metrics which I sent to stat-d, being captured on the datadog UI.",0.0,0.0,1.0,0.0
datadog,However I was asked to add tags.,0.0,0.0,1.0,0.0
datadog,I changed:,0.0,0.0,1.0,0.0
datadog,client.increment(somemetric);,0.0,0.0,1.0,0.0
datadog,to,0.0,0.0,1.0,0.0
datadog,"client.increment(somemetric, [incrementTag]);",0.0,0.0,1.0,0.0
datadog,Soon after I did that nothing showed up on datadog.,0.0,0.0,1.0,0.0
datadog,Looks like I have followed the stats-d doc.,0.0,0.294,0.706,0.3612
datadog,What would be my next steps to figure out why datadog cannot read it ?,0.0,0.0,1.0,0.0
datadog,I'm trying to setup a notification message on Slack for a monitor on a custom metric that we created.,0.0,0.118,0.882,0.25
datadog,"I would like the message to include a  timestamp  of the event, and also a link that redirect to the log, to analyze it immediately.",0.0,0.106,0.894,0.3612
datadog,"Are there any template variable like {{var}} that let me insert the timestamp and the link to the log, or maybe that let me build the log search query string
dynamically like: 
 https://app.datadoghq.com/logs ?....",0.0,0.195,0.805,0.7579
datadog,(so I will need the timestamp at least)?,0.0,0.0,1.0,0.0
datadog,At the moment we only have this in the message:,0.0,0.0,1.0,0.0
datadog,CHANNEL: {{channel.name}},0.0,0.0,1.0,0.0
datadog,ENVIRONMENT: {{environment.name}}.,0.0,0.0,1.0,0.0
datadog,"I am trying to build Datadog integration extras plugin which used to connect Neo4J to Datadog, for monitoring.",0.0,0.0,1.0,0.0
datadog,I am following this post  https://docs.datadoghq.com/integrations/neo4j/,0.0,0.0,1.0,0.0
datadog,And the integration plugin code is in this github location  https://github.com/DataDog/integrations-extras,0.0,0.0,1.0,0.0
datadog,I am using Mackbook pro for building the application.,0.0,0.0,1.0,0.0
datadog,As per the article i am not able to execute the following command.,0.0,0.0,1.0,0.0
datadog,"While executing the above command i am getting the following error
 ""Error: accepts 0 arg(s), received 3""",0.15,0.128,0.722,-0.1027
datadog,Can anyone please help me what i am missing.,0.18,0.41,0.41,0.4215
datadog,We are using JMS (tibco EMS) as messaging service can We visualize the performance using Datadog?,0.0,0.0,1.0,0.0
datadog,If it is not possible do we have any alternate?,0.0,0.0,1.0,0.0
datadog,I'm trying to import my whole Datadog environment to the Terraform configuration.,0.0,0.0,1.0,0.0
datadog,My account has access to multiple organizations.,0.0,0.0,1.0,0.0
datadog,I want to import it to the single Monolithics repository.,0.0,0.14,0.86,0.0772
datadog,"Unfortunately, I met the issue with directory layout startegy - I'm not sure how it should look based on  Terraform best practices .",0.171,0.164,0.665,0.2115
datadog,I suggested:,0.0,0.0,1.0,0.0
datadog,or,0.0,0.0,1.0,0.0
datadog,Does somebody have experience with the issue?,0.0,0.0,1.0,0.0
datadog,What do you think?,0.0,0.0,1.0,0.0
datadog,Could you provide me your experience?,0.0,0.0,1.0,0.0
datadog,Thanks in advance!,0.0,0.615,0.385,0.4926
datadog,"Basically, i created aws cloudwatch metrics from cloudwatch logs.",0.0,0.222,0.778,0.25
datadog,when i create a chart using these metrics i can see correct chart in the cloudwatch Dashboard.,0.0,0.139,0.861,0.2732
datadog,Problem,1.0,0.0,0.0,-0.4019
datadog,"I want to create the same chart in the DataDog Monitoring environment, i can able to create the chart using those metrics but i didn't see accurate results.",0.0,0.162,0.838,0.3071
datadog,"In Datadog  I am seeing decimal points as metric values(0.1,0.2,0.25 etc).",0.0,0.0,1.0,0.0
datadog,can anyone tell me how i can resolve this issue?,0.0,0.245,0.755,0.3818
datadog,I would like to use the  Datadog Oracle Integration  via the  Helm Chart Datadog .,0.0,0.172,0.828,0.3612
datadog,"Oracle Integration states  To use the Oracle integration, either install the Oracle Instant Client libraries, or download the Oracle JDBC Driver.",0.0,0.0,1.0,0.0
datadog,"I do not want to use a custom image to package the JDBC-driver, I want to use a standard image such as tag:7-jmx.",0.126,0.0,0.874,-0.1139
datadog,Other options that come to mind (e.g.,0.0,0.0,1.0,0.0
datadog,EFS volume with the driver inside) seem to be an overkill also.,0.0,0.0,1.0,0.0
datadog,Best option to me seems to be an init container that downloads the JDBC driver.,0.0,0.231,0.769,0.6369
datadog,But Datadog Helm Chart does not support custom init containers for the agents.,0.194,0.0,0.806,-0.438
datadog,What's the best way to do this?,0.0,0.412,0.588,0.6369
datadog,To get an Datadog Agent with a JDBC driver via Helm?,0.0,0.0,1.0,0.0
datadog,I run mysql  Ver 14.14 Distrib 5.7.30 on Debian 10 with the latest DataDog agent,0.0,0.0,1.0,0.0
datadog,"In  datadog-agent status  I see warning:
 Warning: Failed to fetch records from the perf schema 'events_statements_summary_by_digest' table.",0.384,0.0,0.616,-0.7964
datadog,During setup recommended permissions were added:  GRANT SELECT ON performance_schema.,0.0,0.386,0.614,0.6166
datadog,* TO 'datadog'@'localhost';,0.0,0.0,1.0,0.0
datadog,"..and in fact datadog user can read that table:  mysql -u datadog -p performance_schema -e ""select * from events_statements_summary_by_digest"" :",0.0,0.0,1.0,0.0
datadog,My DataDog agent's config:,0.0,0.0,1.0,0.0
datadog,What am I doing wrong?,0.508,0.0,0.492,-0.4767
datadog,I installed Datadog on my Django 1.8 application.,0.0,0.0,1.0,0.0
datadog,"I am getting all the needed services which are running and apart from that I am also getting a service called ""unnamed-python-service"" popping up, inside of which are some HTML resources rendered by  django.template",0.0,0.0,1.0,0.0
datadog,I have the below loaders configured and use the default templating engine,0.0,0.0,1.0,0.0
datadog,Can anyone help me understand more about the service and also how to rename it?,0.0,0.162,0.838,0.4019
datadog,Here's how it shows,0.0,0.0,1.0,0.0
datadog,"Is it possible to query events and it's properties and make chart or dashboard and download the results, like as we do in amplitude analytics or heap analytics?",0.0,0.085,0.915,0.3612
datadog,"I have installed Datadog directly on 2 of my Ubuntu servers;
however in my third server I am facing problems.",0.144,0.0,0.856,-0.4019
datadog,"Same procedure same steps, only difference between these two are that the 3rd server contains multiple virtualhost entries.",0.0,0.0,1.0,0.0
datadog,Datadog installation: using helm was successful,0.0,0.432,0.568,0.5859
datadog,Docs used,0.0,0.0,1.0,0.0
datadog,Agent version (7.19.0),0.0,0.0,1.0,0.0
datadog,Agent Status,0.0,0.0,1.0,0.0
datadog,Error in Logs,0.574,0.0,0.426,-0.4019
datadog,ERROR which i see in log agent and the same is true for all pods,0.188,0.154,0.658,-0.1613
datadog,Not sure where I am going wrong.,0.559,0.0,0.441,-0.6202
datadog,Any help is highly appreciated.,0.0,0.677,0.323,0.7425
datadog,We want to monitor an individual deployment of datadog.,0.0,0.14,0.86,0.0772
datadog,"By default, the agent gets installed on all nodes and we start paying for each node as a host.",0.0,0.0,1.0,0.0
datadog,"However, we're interested in using datadog to monitor our ""agents"" running in customer clusters, therefore we only want to monitor that one deployment.",0.0,0.16,0.84,0.4588
datadog,Paying 15$ per node for each client would blow up our costs.,0.0,0.0,1.0,0.0
datadog,"Is there a way to only monitor a specific deployment on K8S with datadog and only pay a fee linked to that deployment, not to the size of the entire cluster?",0.049,0.0,0.951,-0.1027
datadog,I have logs that contain this kind of line:,0.0,0.0,1.0,0.0
datadog,"I want to filter out ones with  ""event_artist_id"": 100",0.0,0.157,0.843,0.0772
datadog,How do I do that?,0.0,0.0,1.0,0.0
datadog,"I tried many options, but no luck yet, for example:  \/""event_artist_id"": 100\/*",0.177,0.253,0.57,0.296
datadog,I have DataDog with Amazon AWS RDS integration configured.,0.0,0.195,0.805,0.1779
datadog,Is it possible to create a graph and use a tag to exclude some hosts from the result.,0.106,0.117,0.778,0.0516
datadog,I have let's say 100 hosts with tag  environment:live  and 10 of them are also tagged with tag  importance:ignore .,0.0,0.0,1.0,0.0
datadog,So I need to create a graph which will include metrics for 90 hosts that are tagged with first tag but don't tagged with a second one.,0.0,0.068,0.932,0.1734
datadog,Is it possible?,0.0,0.0,1.0,0.0
datadog,"I have a function that runs in a thread pool, but it only shows up in the Datadog tracing UI when I run it outside of my threadpool.",0.0,0.0,1.0,0.0
datadog,In the screenshot below you can see it show up in  sync_work  but not in  async_work .,0.0,0.0,1.0,0.0
datadog,"Here is my code, contained in a script called  ddtrace_threadpool_example.py :",0.0,0.0,1.0,0.0
datadog,I run the script like this:  python ddtrace_threadpool_example.py .,0.0,0.294,0.706,0.3612
datadog,"I'm using Python 3.7, and  pip freeze  shows  ddtrace==0.29.0 .",0.0,0.13,0.87,0.0516
datadog,I am running a  Python Pyramid  app and I want to measure the number of requests coming in per second.,0.0,0.148,0.852,0.1531
datadog,"For this, I am using  datadog .",0.0,0.0,1.0,0.0
datadog,We have a company wide datadog server.,0.0,0.0,1.0,0.0
datadog,On my instance where my app is running we have a  dd-agent  running.,0.0,0.0,1.0,0.0
datadog,I have initialized datadog client in my app and I am using the following call to capture number of requests per second.,0.0,0.064,0.936,0.0772
datadog,I am trying to capture all requests and also specific gameplay requests like for  game1  and  game2 .,0.0,0.143,0.857,0.3612
datadog,So when the incoming request is for either of these two games I added the following tags depending on the game being requested:,0.0,0.0,1.0,0.0
datadog,But for requests other than these two games the tags is empty as  tags=[],0.145,0.0,0.855,-0.296
datadog,"My question is:
Is this the right way to do it?",0.0,0.0,1.0,0.0
datadog,So when I go to the datadog for this metric and I don't give any tags to filter I should be able to capture all incoming requests per second?,0.0,0.0,1.0,0.0
datadog,"And when I use the filter as  game:game1 , I should be able to see incoming requests for only  game1  ?",0.0,0.0,1.0,0.0
datadog,"I am trying to create a ""Top List"" visualization in DataDog and I would like to graph my data which should be grouped by error code.",0.096,0.228,0.676,0.4019
datadog,This error code is a substring in logs.,0.31,0.0,0.69,-0.4019
datadog,An example of a line in the log is given below.,0.0,0.0,1.0,0.0
datadog,"I have tried to group my data by message but this is not working, I would like to group my data by substring of the message.",0.104,0.0,0.896,-0.395
datadog,Can someone guide me on this?,0.0,0.0,1.0,0.0
datadog,"...Server Error {""error"":{""code"":1001,""type"":""MATCH"",""message"":""Invoke
  failed: Failed...
  ...Server Error {""error"":{""code"":2001,""type"":""MATCH"",""message"":""Invoke
  failed: Failed...",0.667,0.0,0.333,-0.9001
datadog,Currently I get the visualization as follows,0.0,0.0,1.0,0.0
datadog,1.0 is the number of occurrence,0.0,0.206,0.794,0.0772
datadog,Rather I want the visualization as follows,0.0,0.206,0.794,0.0772
datadog,2.0 will be total 2 occurrences of error 1001 and 1.0 will be the occurrences of error 2001,0.265,0.0,0.735,-0.6597
datadog,Does anyone know if Datadog agent works on snowflake?,0.0,0.0,1.0,0.0
datadog,"We want to use Datadog to collect snowflake metrics, traces and logs and create dashboards, graphs, and monitors.",0.0,0.175,0.825,0.34
datadog,I have a the following function to publish datadog metrics,0.0,0.0,1.0,0.0
datadog,"I am now trying to change it so it will go through a socket, as I find in this following page:",0.0,0.0,1.0,0.0
datadog,https://github.com/garrettsickles/DogFood   under  UDS - Unix Domain Sockets (Custom) .,0.0,0.0,1.0,0.0
datadog,Now the function looks like this:,0.0,0.333,0.667,0.3612
datadog,"When I try to compile it, I get 'not a member' errors.",0.0,0.203,0.797,0.2584
datadog,How can I include them as members?,0.0,0.0,1.0,0.0
datadog,I have the following  curl  command which works fine:,0.0,0.205,0.795,0.2023
datadog,"Then I convert this requests to Python Requests, and the  curl  method works but Python returns a 500 error without any details.",0.157,0.0,0.843,-0.5499
datadog,"I tried it outside my Docker guessing that maybe connection was the key, but it doesn't work either.",0.0,0.0,1.0,0.0
datadog,I am searching for the occurrence of character  $  in a log file on datadog log explorer which uses lucene syntax.,0.0,0.0,1.0,0.0
datadog,It's pretty similar to kibana.,0.0,0.444,0.556,0.4939
datadog,I have logged a string for testing  Testing $ pattern datadog  but when I search for  $  it doesn't show any results.,0.0,0.0,1.0,0.0
datadog,On searching for  Testing  I get  Testing $ pattern datadog  in response.,0.0,0.0,1.0,0.0
datadog,Please tell me how can list the occurrences of  $  any help is much appreciated.,0.0,0.43,0.57,0.8074
datadog,I have created a Multi alert event monitor,0.0,0.512,0.488,0.4939
datadog,"I wanted it to be aggregated by ""dbinstanceidentifier"" but it shows the accumulative count for ""* (Entire Infrastructure)"".",0.0,0.0,1.0,0.0
datadog,Basically it doesn't see any groups.,0.0,0.0,1.0,0.0
datadog,But I can see them in the infrastructure.,0.0,0.0,1.0,0.0
datadog,Is it a problem of datadog?,0.403,0.0,0.597,-0.4019
datadog,"May be it's only available in a kind of ""premium"" subscription?",0.0,0.0,1.0,0.0
datadog,"I've created a custom Datadog metric in a Springboot Java App, and turned on the management end-points.",0.0,0.125,0.875,0.25
datadog,"I am incrementing a MeterRegistry Counter with a double value (relating to the monetary value of an order)
When I use the /management/metrics end-point, I can see the correct value being stored.",0.0,0.231,0.769,0.7351
datadog,"However, when I create a widget in my Datadog dashboard, it is only displaying the pre-decimal point value of the data.",0.0,0.209,0.791,0.5423
datadog,"e.g the order value is 61.67 and in Datadog it is displaying 61, so it's not even doing any rounding !",0.0,0.124,0.876,0.4003
datadog,Is there any way to display the raw value of the counter in a Datadog Dashboard widget?,0.0,0.138,0.862,0.34
datadog,Thanks in advance,0.0,0.592,0.408,0.4404
datadog,Im trying to deploy my service and read my local logfile from the inside pod.,0.0,0.0,1.0,0.0
datadog,Using DataDog's helm chart values with the following configs :,0.0,0.252,0.748,0.4019
datadog,as you can see I expect my logs to be available at /app/logs/service.log and thats what Im supplying to my conf.d :,0.0,0.0,1.0,0.0
datadog,"In my service, I use WinstonLogger using file transport with the JSON format.",0.0,0.0,1.0,0.0
datadog,process.env.LOGS_PATH = '/app/logs',0.0,0.0,1.0,0.0
datadog,"After all, exploring my pod and tail -f my service.log in the expected /app/logs folder I see that the application actually writes the logs in a JSON format as expected.",0.0,0.0,1.0,0.0
datadog,My DataDog doesn't pick up the logs and they are not showing in the log section ..,0.0,0.0,1.0,0.0
datadog,NOTE:: I do not mount any volume to and from my service ..,0.0,0.0,1.0,0.0
datadog,What am I missing?,0.524,0.0,0.476,-0.296
datadog,Should I mount my local log to /var/log/pods/[service_name]/   ?,0.0,0.0,1.0,0.0
datadog,My team and I are trying to add a table to summarize our logs.,0.0,0.0,1.0,0.0
datadog,Our system logs to datadog (not always but sometimes) as follows:,0.0,0.0,1.0,0.0
datadog,To group a set of logs (Large operation) and know they are related I added to each a  trace_id  facet (to each log in between).,0.0,0.0,1.0,0.0
datadog,Eventually I want to quickly be able to see,0.0,0.157,0.843,0.0772
datadog,What's the best approach?,0.0,0.583,0.417,0.6369
datadog,I don't want to query each time I need the information - I want to create an overview and to apply the system's analytic tool to this summary,0.096,0.082,0.822,0.167
datadog,"Is it possible to automatically mute created Datadog Monitor(which is monitoring Windows Service), when Windows service state was changed from Automatic to Disabled?",0.0,0.083,0.917,0.25
datadog,"How can it be configured
Thanks.",0.0,0.367,0.633,0.4404
datadog,I'm trying to create a datadog monitor that only alerts on Wednesdays and Fridays.,0.0,0.149,0.851,0.2732
datadog,"I have created the metric and monitor, and I think the best solution is to create a schedualed downtime that repeats for the days I'm not interessted in.",0.0,0.335,0.665,0.8625
datadog,Ive created the downtime window as:,0.0,0.286,0.714,0.25
datadog,"This creates a window for only 1hr, ideally this should be 24hr",0.0,0.353,0.647,0.5994
datadog,"I was installed the ""datadog-php-tracer_0.14.1-beta_amd64.deb"" on my server and after installed my application return 500 error.",0.162,0.0,0.838,-0.4019
datadog,Below is the things which I have configured or my server related information:,0.0,0.0,1.0,0.0
datadog,"I am using Ubuntu, NGINX and php-fpm 7.0.",0.0,0.0,1.0,0.0
datadog,I have installed datadog agent v6.,0.0,0.0,1.0,0.0
datadog,"When I am checking my php-fpm log file, it shows the PDO error about ""Slim\PDO\Statement\StatementContainer- execute()"".",0.162,0.0,0.838,-0.4019
datadog,But when I disabled the Datadog Agent or APM trace then my application working normally.,0.0,0.0,1.0,0.0
datadog,In short when I am enable ddtrace my app not working and return 500 error.,0.172,0.0,0.828,-0.4019
datadog,Can you please look in it and let me know how can resolved the issue and APM work well with my app.,0.0,0.243,0.757,0.6249
datadog,In our infra CPU utilization is being monitored by datadog SAS.,0.0,0.0,1.0,0.0
datadog,The dashboard shows  CPU utilization over a period time graphically.,0.0,0.0,1.0,0.0
datadog,How do I find the average CPU utilization over that period of time?,0.0,0.0,1.0,0.0
datadog,I'm trying to figure out how to create an alert around a process that may be crashing and restarting repeatedly.,0.0,0.202,0.798,0.5106
datadog,"It might be providing some data to Datadog while it's up, so a ""no data"" alert won't do because the lack of data never hits the duration threshold as the process restarts.",0.145,0.069,0.786,-0.4024
datadog,"I was thinking of alerting on a changing PID, but I cannot for the life of me figure out how to create a PID-based Monitor.",0.0,0.117,0.883,0.3919
datadog,Is it possible?,0.0,0.0,1.0,0.0
datadog,And how?,0.0,0.0,1.0,0.0
datadog,Does anyone have any other suggestions for this situation?,0.0,0.0,1.0,0.0
datadog,We want to collect metrics from machines running AWS lambda in AWS.,0.0,0.106,0.894,0.0772
datadog,How can I get access to these machines and get DD agent installed on them.,0.0,0.0,1.0,0.0
datadog,We are not willing to use DD agent.,0.0,0.0,1.0,0.0
datadog,How can we know if PostgresSQL is up and running on my Redhat linux server so that I can create an alert when postgres is down.,0.0,0.166,0.834,0.552
datadog,I am using Kamon DatadogAgentReporter to record different metrics in my application.,0.0,0.0,1.0,0.0
datadog,"After migrating Kamon from 0.6.x to 1.x, I can see only the list of metrics with tags without any service name.",0.0,0.0,1.0,0.0
datadog,"I added the reporter like this, Kamon.addReporter(new DatadogAgentReporter()) and the config as given below,",0.0,0.172,0.828,0.3612
datadog,Did I miss something?,0.444,0.0,0.556,-0.1531
datadog,How do I get the display service-name prefix for my metrices?,0.0,0.0,1.0,0.0
datadog,Thanks in advance!,0.0,0.615,0.385,0.4926
datadog,I have written a Datadog Agent check in Python following the instructions on this page:  https://docs.datadoghq.com/developers/agent_checks/ .,0.0,0.0,1.0,0.0
datadog,The agent check is supposed to read all files in a specified network folder and then send certain metrics to Datadog.,0.0,0.1,0.9,0.2732
datadog,The folder to be read is specified like this in the Yaml file:,0.0,0.172,0.828,0.3612
datadog,"This is the code used to read the folder, it is Python 2.7 because that is required by Datadog",0.0,0.0,1.0,0.0
datadog,If I just run the Python script in my IDE everything works correctly.,0.0,0.0,1.0,0.0
datadog,When the check is added to the Datadog Agent Manager on the same machine that the IDE is on and the check is run an error is thrown in the Datadog Agent Manager Log saying:,0.074,0.0,0.926,-0.4019
datadog,"2018-08-14 14:33:26 EEST | ERROR | (runner.go:277 in work) | Error running check TaskResultErrorReader: [{""message"": ""[Error 3] The system cannot find the path specified: 'Z:/TaskResults/ .",0.235,0.0,0.765,-0.7297
datadog,"'"", ""traceback"": ""Traceback (most recent call last):\n File \""C:\Program Files\Datadog\Datadog Agent\embedded\lib\site-packages\datadog_checks\checks\base.py\"", line 294, in run\n self.check(copy.deepcopy(self.instances[0]))\n File \""c:\programdata\datadog\checks.d\TaskResultErrorReader.py\"", line 42, in check\n for file in os.listdir(task_result_location):\nWindowsError: [Error 3] The system cannot find the path specified: 'Z:/TaskResults/ .",0.0,0.0,1.0,0.0
datadog,"'\n""}]",0.0,0.0,1.0,0.0
datadog,"I have tried specifying the folder location in multiple ways with single and double quotes, forward and back slashes and double slashes but the same error is thrown.",0.209,0.0,0.791,-0.6542
datadog,Would anyone know if this is a Yaml syntax error or some sort of issue with Datadog or the Python?,0.13,0.0,0.87,-0.4019
datadog,I have file full of metrics:,0.0,0.0,1.0,0.0
datadog,"I would like to send them (once) to DataDog to create dashboard, because our infrastructure team didn't install DogStatsD agents on our containers yet.",0.0,0.18,0.82,0.5574
datadog,"I have a Spring Boot app which due to weird restrictions needs to run once every three hours, and won't work with Quartz, so I've been running it once every three hours from OS cron and it quits when it's done.",0.043,0.0,0.957,-0.1779
datadog,"After adding micrometer-registry-datadog (and spring-legacy) however, it never quits, it just sends metrics every 20 seconds or whatever the default period is, even after calling registry.close().",0.0,0.0,1.0,0.0
datadog,"Am I doomed like the dutchman to sail the seas of processing forever, or is there an obvious error I have made?",0.261,0.095,0.644,-0.6597
datadog,"Code: It reaches SpringApplication.exit(ctx), but it does not actually exit cleanly.",0.0,0.099,0.901,0.0258
datadog,(service is a TimedExecutorService.),0.0,0.0,1.0,0.0
datadog,My daemon keeps querying db on a cronly basis.,0.0,0.0,1.0,0.0
datadog,"In every iteration, (a) the deamon makes a DB query (b) receives some documents from db (c) processes those results.",0.0,0.0,1.0,0.0
datadog,I want to emit  the number of documents returned for the query  on Datadog.,0.0,0.191,0.809,0.1531
datadog,What is the right metric type?,0.0,0.0,1.0,0.0
datadog,I created a wrapper cookbook to retrieve my datadog api keys from an encrypted data bag but it looks like it is not running during the execution.,0.0,0.171,0.829,0.5789
datadog,Here is my code:,0.0,0.0,1.0,0.0
datadog,attributes/default.rb,0.0,0.0,1.0,0.0
datadog,recipes/set_key.rb:,0.0,0.0,1.0,0.0
datadog,and del_key:,0.0,0.0,1.0,0.0
datadog,I created a role named datadog and run list of this role looks like:,0.0,0.31,0.69,0.5423
datadog,"I'm expecting this wrapper recipe load datadog keys, then datadog recipes to run and finally another wrapper recipe to remove keys.",0.0,0.0,1.0,0.0
datadog,"But when Chef is running, I receive an error message like:",0.24,0.22,0.541,-0.0772
datadog,"Since I'm new to Chef and data bags use, I'm a bit confused.",0.173,0.0,0.827,-0.3182
datadog,Why my setter recipe is not running?,0.0,0.0,1.0,0.0
datadog,Thanks.,0.0,1.0,0.0,0.4404
datadog,statd.yaml  in  conf.d  is configured as follows,0.0,0.0,1.0,0.0
datadog,after starting Datadog-agent I get one error as shown below,0.252,0.0,0.748,-0.4019
datadog,"everything else runs fine, Also in  datadog-conf  I have mentioned the forwarder's IP and API key as well, but it is not showing in host map in Datadog webUI",0.0,0.102,0.898,0.2382
datadog,I am having several issues regarding Flink and Datadog integration.,0.0,0.0,1.0,0.0
datadog,"First, the issue is that Datadog uses dogstatsD instead of statsD which is not included in Flink documentation",0.0,0.0,1.0,0.0
datadog,"Another issue is that if you go to Datadog's  Integrations  page, Flink integration is missing.",0.136,0.0,0.864,-0.296
datadog,"I have tried installing graphite but I am having several issues with that as well due to python 3.6, I tried virtualenv as well, but thought of going with datadog, which is giving me hard time as well.",0.038,0.259,0.703,0.8573
datadog,"Not sure if there are any pro DataDog users on here, but I'm hoping.",0.086,0.215,0.698,0.4971
datadog,I've created a template DataDog dashboard template that captures the memory usage of a host by docker container.,0.0,0.118,0.882,0.25
datadog,"The ""hostname"" appears in 5 or so places:",0.0,0.0,1.0,0.0
datadog,"I'm trying to set up a dashboard right now that displays this template for each of my 20 or so hosts, but it's a painful process of cloning the chart and editing the host name in all 5 places.",0.099,0.0,0.901,-0.5927
datadog,"Whenever I make a change to the template, I have to painfully paste the changes into each host chart and change the hostname in applicable places.",0.134,0.0,0.866,-0.5267
datadog,Is there a way I can set up this template (perhaps with a variable in place of the host name) and have a dashboard automatically create a chart for each host from this template?,0.0,0.07,0.93,0.2732
datadog,"Failing that, is there a way this can be scripted?",0.292,0.0,0.708,-0.5106
datadog,Thank you.,0.0,0.714,0.286,0.3612
datadog,"i am struggling with importing metrics with datadog...I am getting below error in spite of installing all required packages...
( - instance #0 [ERROR]: Exception('You need the ""psutil"" package to run this check',)",0.248,0.0,0.752,-0.836
datadog,request you to please help me out here as this is prove to be a major showstopper.,0.0,0.263,0.737,0.6124
datadog,[root@mudcsftpup01 init.d]# ./datadog-agent info,0.0,0.0,1.0,0.0
datadog,"Status date: 2017-08-31 11:31:19 (1s ago)
  Pid: 32028
  Platform: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
  Python Version: 2.7.5
  Logs: , /var/log/datadog/collector.log, syslog:/dev/log",0.0,0.0,1.0,0.0
datadog,"Clocks
  ======",0.0,0.0,1.0,0.0
datadog,"Paths
  =====",0.0,0.0,1.0,0.0
datadog,"Hostnames
  =========",0.0,0.0,1.0,0.0
datadog,"Checks
  ======",0.0,0.0,1.0,0.0
datadog,"Emitters
  ========",0.0,0.0,1.0,0.0
datadog,"Status date: 2017-08-31 11:31:23 (2s ago)
  Pid: 32053
  Platform: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
  Python Version: 2.7.5
  Logs: , /var/log/datadog/dogstatsd.log, syslog:/dev/log",0.0,0.0,1.0,0.0
datadog,"Flush count: 1
  Packet Count: 0
  Packets per second: 0.0
  Metric count: 0
  Event count: 0",0.0,0.0,1.0,0.0
datadog,Unfortunately there is not an official Go Datadog API.,0.231,0.0,0.769,-0.34
datadog,I am currently using this one instead  https://github.com/zorkian/go-datadog-api .,0.0,0.0,1.0,0.0
datadog,Datadog forked the first version of it and recommend using it.,0.0,0.2,0.8,0.3612
datadog,I am able to connect to my Dashboard:,0.0,0.0,1.0,0.0
datadog,But I do not know how to send create/track an event.,0.0,0.0,1.0,0.0
datadog,This is my current approach but if fails badly.,0.529,0.0,0.471,-0.8338
datadog,"From my understanding and the missing documentation, I would have to fill out some of these variables in this struct ( https://github.com/zorkian/go-datadog-api/blob/master/events.go )",0.104,0.0,0.896,-0.296
datadog,Can you please help me with that?,0.0,0.5,0.5,0.6124
datadog,I’m getting a connection refused from my datadog-agent that is trying to collect JMX (via RMI) metrics from an in-house application that exists in its own docker container.,0.078,0.0,0.922,-0.296
datadog,"However, jconsole is able to collect the metrics from the application that exists within its own docker container.",0.0,0.0,1.0,0.0
datadog,The datadog-agent exists within a container of its own.,0.0,0.0,1.0,0.0
datadog,Both containers exist within the same network on the same host.,0.0,0.0,1.0,0.0
datadog,Any ideas?,0.0,0.0,1.0,0.0
datadog,I have looked at the other stack overflow questions.,0.0,0.0,1.0,0.0
datadog,"Docker Container 0: 
* Runs the my_streams_app that outputs kafka streams metrics 
* Executed via:",0.0,0.0,1.0,0.0
datadog,"Docker Container 1: 
* Runs datadog-agent within container
* Datadog-agent uses JMX default (RMI) to fetch the metrics from my_streams_app that exists in container 0, above.",0.0,0.0,1.0,0.0
datadog,"* both containers run on the same network within the same host (my laptop MAC OSX) 
* able to netcat from within datadog-agent in docker container to the my_streams_app ip and port in the other container.",0.0,0.0,1.0,0.0
datadog,"Using 0.0.0.0 and 9998, can also use specific IP addresses 
* command to run the datadog agent from within a container",0.0,0.0,1.0,0.0
datadog,jmx configuration for collecting metrics by datadog jmx from within the container:,0.0,0.0,1.0,0.0
datadog,instances:,0.0,0.0,1.0,0.0
datadog,"docker_images:
        - my_streams_app",0.0,0.0,1.0,0.0
datadog,"init_config:
    is_jmx: true
    conf:
        - include:
            domain: '""kafka.streams""'
            bean: '""kafka.streams"":type=""stream-metrics"",client-id=“my_test-1-StreamThread-1""'
            attribute:
                commit-calls-rate:
                    metric_type: gauge
                commit-time-avg: 
                    metric_type: gauge
                commit-time-max:
                    metric_type: gauge
                poll-calls-rate:
                    metric_type: gauge",0.0,0.118,0.882,0.4215
datadog,"JConsole: 
* collects metrics from my_streams_app within the docker container 0, above via:",0.0,0.0,1.0,0.0
datadog,Error output:,0.73,0.0,0.27,-0.4019
datadog,rmiregistry has been started as per  Failed to retrieve RMIServer stub,0.248,0.0,0.752,-0.5106
datadog,"Iam using a puppetized datadog agent install(version 1.10.0), and looks like i'm on default dd-agent version - 5.9.1.",0.0,0.143,0.857,0.3612
datadog,Would like to upgrade.,0.0,0.455,0.545,0.3612
datadog,I dont see any docs related to that.,0.0,0.0,1.0,0.0
datadog,Can someone point me on how to do this?,0.0,0.0,1.0,0.0
datadog,I'm attempting to query our DATADOG hub and display some metric graphs.,0.0,0.0,1.0,0.0
datadog,"However, it appears the default way to do this is using an embed script generated by DATADOG and utilizing that in your app.",0.0,0.0,1.0,0.0
datadog,"I'm actually wanting to draw the graphs on my side, using their API data so I'm better able to control the size, look and flexibility of the graphs.",0.0,0.177,0.823,0.6786
datadog,Is this something that is possible?,0.0,0.0,1.0,0.0
datadog,"Rather new to DATADOG and everything seems to be done in an iFrame, which I do not want.",0.071,0.0,0.929,-0.0572
datadog,"Additionally, I found a package which I believe may be of use for Node?",0.0,0.0,1.0,0.0
datadog,:  http://brettlangdon.github.io/node-dogapi/#embed-create,0.0,0.0,1.0,0.0
datadog,D.D.,0.0,0.0,1.0,0.0
datadog,Graphs Docs:  http://docs.datadoghq.com/api/#graphs,0.0,0.0,1.0,0.0
datadog,"Any advice would be greatly appreciated, I have not seen anything similar on S.O.",0.0,0.23,0.77,0.5563
datadog,"Graphing Primer  and  Getting started with tags – Datadog  suggest that Tags are  the  way to filter data, but the latter article cautions:",0.0,0.0,1.0,0.0
datadog,"Please don't include endlessly growing tags in your metrics, like timestamps or user ids.",0.088,0.277,0.635,0.5076
datadog,Please limit each metric to 1000 tags.,0.0,0.277,0.723,0.3182
datadog,"So, if I want to filter by user id, how can I do that without using Tags?",0.0,0.101,0.899,0.1477
datadog,"There are logs with &quot;events&quot; that have attributes such as names, statuses, etc as well as &quot;amount&quot; which corresponds to a dollar amount.",0.0,0.091,0.909,0.2732
datadog,I'm sending requests to datadog's timeseries api to get back data about these logs.,0.0,0.0,1.0,0.0
datadog,"I'm starting with a base query:
 sum:events{event_name:reload,event_result:success,event_environment:main,event_market:gb}.as_count()",0.0,0.0,1.0,0.0
datadog,This query just returns the total number of these &quot;reload&quot; events that have occurred in the timeframe.,0.0,0.075,0.925,0.0772
datadog,Each of these events has an attribute &quot;amount&quot; which I want returned instead.,0.0,0.106,0.894,0.0772
datadog,I can't figure out how to format the syntax for the query to get it to return the sum of these amounts instead of the sum of the occurrences of events.,0.0,0.0,1.0,0.0
datadog,Here are some queries I have tried which do not work:,0.0,0.0,1.0,0.0
datadog,I've had difficulty finding anything about this in the datadog documentation.,0.194,0.0,0.806,-0.34
datadog,If anyone understands how to perform this with datadog's query syntax I would very much appreciate the help.,0.0,0.275,0.725,0.6887
datadog,Thanks,0.0,1.0,0.0,0.4404
datadog,I've been using the Datadog dashboard and I really like how strong and flexible it is.,0.0,0.408,0.592,0.8051
datadog,I am wondering what type of API they are using,0.0,0.0,1.0,0.0
datadog,Here's a  list  of DataDog metrics of Azure Load Balancer that are available to use.,0.0,0.0,1.0,0.0
datadog,It seems like,0.0,0.556,0.444,0.3612
datadog,are the most releveant.,0.0,0.0,1.0,0.0
datadog,"When SNAT port resources are exhausted, outbound flows fail.",0.462,0.0,0.538,-0.7184
datadog,You could observe failing outbound connections or are advised by support that you're exhausting SNAT ports.,0.27,0.126,0.605,-0.4767
datadog,Simply seeing failed connections does not confirm SNAT exhaustion.,0.266,0.17,0.564,-0.2937
datadog,Seeing the failed connections was a clue we were having an issue but there is no way to confirm SNAT exhaustion w/o a ticket to Microsoft it turns out.,0.255,0.0,0.745,-0.802
datadog,"We have a VM with DataDogAgent in azure, on this VM we host  ASP.Net core API  application.",0.0,0.0,1.0,0.0
datadog,We collect these kind of metrics:,0.0,0.0,1.0,0.0
datadog,Here is the code of stub API:,0.0,0.0,1.0,0.0
datadog,In data dog dashboard these 2 kind of metrics are stacked together for one request into one trace:,0.0,0.0,1.0,0.0
datadog,"After application migrated to  Azure AppService ( .NET Datadog APM  is used to collect metrics here), spans are not stacked into one trace and appears in dashborad as separate traces.",0.0,0.0,1.0,0.0
datadog,How to merge them into one trace in  Azure App Service ?,0.0,0.0,1.0,0.0
datadog,I'm using datadog to monitor the health of several pods deployed in a kubernetes cluster.,0.0,0.0,1.0,0.0
datadog,I use a query like this to check the pods,0.0,0.263,0.737,0.3612
datadog,"If I stop the pod, there ins't any data for kubernetes.pods.running (so the value is not zero, I don't have any value) .",0.097,0.106,0.796,0.0516
datadog,I don't know if it's possible to check from datadog that no pods has kube_service running.,0.136,0.0,0.864,-0.296
datadog,"Using the @datadog/browser-rum package from  RUM Browser Monitoring , with Node apollo web client fails on IE11.",0.157,0.0,0.843,-0.4215
datadog,Seeing multiple requests to  https://run-http-intake.logs.datadoghq.com  on IE11 that are different than Chrome.,0.0,0.0,1.0,0.0
datadog,Unable to login to the application on IE11.,0.0,0.0,1.0,0.0
datadog,Working theory is that datadogRUM is blocking other application requests on IE11.,0.191,0.0,0.809,-0.3818
datadog,"When datadogRUM is removed, the application is working correctly.",0.0,0.0,1.0,0.0
datadog,Chrome and IE11 requests:,0.0,0.0,1.0,0.0
datadog,Request Method: POST  RequestURL:  https://rum-http-intake.logs.datadoghq.com/v1/input/pub{id}?_dd.application_id={id}&amp;ddsource=browser&amp;&amp;ddtags=sdk_version:1.25.2.env:local&amp;batch_time={timestamp},0.0,0.0,1.0,0.0
datadog,Only IE11 request:,0.0,0.0,1.0,0.0
datadog,Request Method: CONNECT RequestURL:  https://rum-http-intake.logs.datadoghq.com  Proxy-Connection: Keep-Alive,0.0,0.0,1.0,0.0
datadog,Please let me know what additional information I should find to help with this issue.,0.0,0.294,0.706,0.6124
datadog,IE11 issues are no fun.,0.259,0.388,0.353,0.2732
datadog,Thank you!,0.0,0.736,0.264,0.4199
datadog,I'm creating alerts in Prometheus and migrating from Datadog.,0.0,0.216,0.784,0.296
datadog,I have two metrics queries that I'm not able to understand yet.,0.0,0.0,1.0,0.0
datadog,"In this query I understand the  avg:default.burrow_kafka_consumer_lag_total{*} by {consumer_group,env}  part but not the rest and how to translate it to PromQL.",0.0,0.0,1.0,0.0
datadog,Second,0.0,0.0,1.0,0.0
datadog,"I don't understand the rollup part, how to translate it into PromQL?",0.0,0.0,1.0,0.0
datadog,"how is  pct_change(avg(last_1h),last_1h)  part of the query?",0.0,0.0,1.0,0.0
datadog,I'm new to this.,0.0,0.0,1.0,0.0
datadog,I have translated other queries but these I don't understand.,0.0,0.0,1.0,0.0
datadog,We are using DataDog's Distributed tracing in a rails application and would like to write the trace_id (for a controller#action) so that we could access the url later to our Rails logs.,0.0,0.079,0.921,0.3612
datadog,How could I do this?,0.0,0.0,1.0,0.0
datadog,something like?,0.0,0.714,0.286,0.3612
datadog,:,0.0,0.0,0.0,0.0
datadog,I would like to integrate my  Sanic==19.9.0  application with Datadog  ddtrace==0.34.1  tracer.,0.0,0.2,0.8,0.3612
datadog,So far I have following code:,0.0,0.0,1.0,0.0
datadog,But trace logs are empty:,0.355,0.0,0.645,-0.296
datadog,What do I do wrong?,0.508,0.0,0.492,-0.4767
datadog,Very new to Datadog and need some help.,0.0,0.278,0.722,0.4019
datadog,I have crafted 2 SQL queries (one for on-prem database and one for cloud database) and I would like to run those queries through Datadog and be able display the query results and validate that the daily results fall within an expected variance between the two systems.,0.0,0.106,0.894,0.6124
datadog,I have already set up Datadog on the cloud environment and believe I should use DogStatsD to create a custom metric but I am pretty lost with how I can incorporate my necessary SQL queries in the code to create the metric for eventual display on a dashboard.,0.06,0.172,0.768,0.6757
datadog,Any help will be greatly appreciated!!,0.0,0.632,0.368,0.7831
datadog,!,0.0,0.0,0.0,0.0
datadog,"For example, suppose I have a Node library that I could use something like:",0.0,0.2,0.8,0.3612
datadog,"The few I've looked at all seem to be good for posting new data to Datadog, however, are there any that can pull data from Datadog?",0.0,0.104,0.896,0.4404
datadog,Does anything like this exist?,0.0,0.385,0.615,0.3612
datadog,"I like the  Datadog API  but it seems as though that is only in Curl, Python, and Ruby.",0.0,0.099,0.901,0.1901
datadog,I'm trying to monitor several applications within the same site in IIS.,0.0,0.0,1.0,0.0
datadog,"With just running the  msi  of the tracer  dd-trace-dotnet , I started to see the events, but these are registered as  [site name]/[application]  e.g  default_web_site/docs_webhook 
I would love to be able to logs them under a custom service name for each application, but according to the  documentation , this is only possible at the site level.",0.0,0.104,0.896,0.7783
datadog,"Manual instrumentation is described for windows services, setting the environment variable  DD_SERVICE_NAME  in the registry entry  HKLM\System\CurrentControlSet\Services\{service name}\Environment  is enough, but does not apply to IIS applications.",0.0,0.0,1.0,0.0
datadog,NOTE: Creating separate sites for each application is not an option right now.,0.0,0.155,0.845,0.296
datadog,I'm using a  statsd.timed  to send some time metrics to datadog.,0.0,0.0,1.0,0.0
datadog,These metrics are being used in a few Datadog dashboards.,0.0,0.0,1.0,0.0
datadog,"Changing the metric name being sent is straightforward and can be done by updating the name of the metric in the statsd.timed call/decorator in the code, however, the old metric name may already be in use in existing datadog dashboards.",0.0,0.0,1.0,0.0
datadog,"Is there a quick and easy way to rename a metric in Datadog so that all dependencies such as Dashboards using the metric are also updated, without having to go through each dashboard and updating them independently?",0.0,0.079,0.921,0.4404
datadog,I currently have a program that allows me to create and post dashboards to Datadog programmatically.,0.0,0.139,0.861,0.2732
datadog,"Using the API functions  here , I was successfully able to create, update, and remove dashboards as I please.",0.0,0.369,0.631,0.765
datadog,"However, now I'd like to extract the skeleton of existing dashboards that I have already created from Datadog to see what has been added or removed.",0.0,0.164,0.836,0.5423
datadog,"To do this, I need to figure out how to send the API key along with a request.",0.0,0.0,1.0,0.0
datadog,"I have no problem getting the higher level information about the boards, but I'd like to go a step further.",0.159,0.15,0.691,0.2023
datadog,This is what I get by calling  api.ScreenBoard.get_all(),0.0,0.0,1.0,0.0
datadog,"Now, the end goal is simply to pull JSON from the ""resource"" link given from this command.",0.0,0.0,1.0,0.0
datadog,"I've tried to use urllib and urllib2 to merge that link with the host site (like  https://www.foo.com/{resource-link} ), but I keep getting the following results:",0.0,0.0,1.0,0.0
datadog,OR,0.0,0.0,1.0,0.0
datadog,The code that triggered this error is:,0.342,0.0,0.658,-0.481
datadog,"As you can see, my ""data"" variable returns the error.",0.231,0.0,0.769,-0.4019
datadog,"So, all I need is to figure out how to send the API key along with my request to resolve the issue.",0.0,0.115,0.885,0.3818
datadog,"If anyone knows how to perform this task, I would really appreciate it.",0.0,0.214,0.786,0.4576
datadog,Is there a way to display a service version (as a string) in Datadog?,0.0,0.0,1.0,0.0
datadog,(Like sending an API request with the text to display)?,0.0,0.0,1.0,0.0
datadog,Does  kafka.messages_in.rate  represents the number of events from producer to broker or it also includes the replication events from other brokers.,0.0,0.061,0.939,0.0772
datadog,The official doc is useless just presents the same metric in plain English without an possible explanation,0.149,0.0,0.851,-0.4215
datadog,"I am trying to install the Datadog agent on windows using powershell only, not manualHowever, the APIKEY is not being setup.",0.0,0.0,1.0,0.0
datadog,Is there a way to update/set the APIKEY after installation?,0.0,0.0,1.0,0.0
datadog,I am trying to integrate activemq with datadog.,0.0,0.0,1.0,0.0
datadog,I have modified /Users//.datadog-agent/conf.d/activemq_58.yaml.,0.0,0.0,1.0,0.0
datadog,Changes are:,0.0,0.0,1.0,0.0
datadog,"instances:
   - host: localhost
     port: 8161
      user: admin
      password: admin",0.0,0.0,1.0,0.0
datadog,activemq is running in localhost at default port with jmx enabled.,0.0,0.0,1.0,0.0
datadog,"Restarted datadog agent 
I could see error after running info command.",0.231,0.0,0.769,-0.4019
datadog,Error is,0.73,0.0,0.27,-0.4019
datadog,activemq_58,0.0,0.0,1.0,0.0
datadog,Can anybody suggest that why I am getting this error?,0.281,0.0,0.719,-0.481
datadog,"In my case scenario, Flink is sending the metrics to Datadog.",0.0,0.0,1.0,0.0
datadog,Datadog Host map is as shown below { I have no Idea why is showing me latency here },0.128,0.0,0.872,-0.296
datadog,Flink metrics are sent to localhost.,0.0,0.0,1.0,0.0
datadog,The issue here is that when,0.0,0.0,1.0,0.0
datadog,flink-conf.yaml  file configuration is as follows,0.0,0.0,1.0,0.0
datadog,"The issue is that Datadog is showing 163 metrics which I don't understand,  which I will explain in a while",0.0,0.0,1.0,0.0
datadog,I don't understand the metrics format in datadog as it shows me metrics something like this,0.0,0.152,0.848,0.3612
datadog,Now as shown in above Image,0.0,0.0,1.0,0.0
datadog,So my question is that which metric is this?,0.0,0.0,1.0,0.0
datadog,"Also, the execution plan of my job is something like this",0.0,0.2,0.8,0.3612
datadog,How do  I relate the metrics in Datadog with execution plan operators in Flink?,0.0,0.0,1.0,0.0
datadog,"I have read in   Flink API 1.3.2  that I can use tags, I have tried to use them in flink-conf.yaml file but I don't have complete Idea what sense they make here.",0.0,0.0,1.0,0.0
datadog,"My ultimate goal is to find operator latency, number of records out and in /second at each operator in this case",0.0,0.061,0.939,0.0772
datadog,Just wanted to check weather Datadog agent is installed in UNIX box or not.,0.0,0.0,1.0,0.0
datadog,"I ran a command  sudo /etc/init.d/datadog-agent status  but got below output
 sudo: /etc/init.d/datadog-agent: command not found",0.0,0.0,1.0,0.0
datadog,Please advice,0.0,0.697,0.303,0.3182
datadog,I'm trying to get DataDog to display a dashboard of system information.,0.0,0.0,1.0,0.0
datadog,One piece of information is the percentage of time the system is reading/writing from the disk expressed in the metrics  system.disk.read_time_pct  and  system.disk.write_time_pct,0.0,0.0,1.0,0.0
datadog,"However, when I put this graph on my dashboard it shows some parts at well over 5000%, which clearly can't be right.",0.0,0.202,0.798,0.5859
datadog,"As you can see from the preview above, it is showing a disk read time of 5430%.",0.0,0.0,1.0,0.0
datadog,If I constrain the Y-axis to 100 it regularly goes above 100%.,0.0,0.0,1.0,0.0
datadog,I can't find anything to explain this or how to graph it properly.,0.0,0.0,1.0,0.0
datadog,"So, how do I properly graph  system.disk.read_time_pct  and  system.disk.write_time_pct  with DataDog?",0.0,0.0,1.0,0.0
datadog,I use  dropwizard metrics  with  metrics-datadog .,0.0,0.0,1.0,0.0
datadog,Create reported like this:,0.0,0.697,0.303,0.5574
datadog,But there is no host(server name) param in datadog.,0.259,0.0,0.741,-0.4215
datadog,How can I add host (server name) for metrics to filter them in datadog control panel?,0.0,0.0,1.0,0.0
datadog,Metrics from default datadog agent has server name attribute.,0.0,0.0,1.0,0.0
datadog,I have a server that processes packets from different devices.,0.0,0.0,1.0,0.0
datadog,Devices can report in different intervals.,0.0,0.0,1.0,0.0
datadog,"I would like to make a chart showing the distribution of intervals by the count of devices (how many devices are reporting within 5 sec/10 sec/60 sec ...)
Intervals for each device can vary.",0.0,0.077,0.923,0.3612
datadog,"Now I'm sending metric with  Set  using deviceId with tags that represent interval (5 sec, 10 sec, 30 sec, and more) but I'm not sure that it is correct.",0.08,0.0,0.92,-0.3491
datadog,What is the best way to realize it?,0.0,0.375,0.625,0.6369
datadog,"I'm using  this built-in dashboard  for monitoring Aurora and was wondering how can I have as code, as cloud formation stack precisely.",0.0,0.0,1.0,0.0
datadog,"I'm aware of those three repos which do backup and monitoring of changing of the dashboard in the API and then commit back to GitHub, but I only want to export it once.",0.0,0.092,0.908,0.2617
datadog,I have an API with 10 endpoints(contracts).,0.0,0.0,1.0,0.0
datadog,and i am shipping logs to IIS to data-dog from the API.,0.0,0.0,1.0,0.0
datadog,I also installed data-dog agent on the server.,0.0,0.0,1.0,0.0
datadog,Now i am trying to create graph for all the endpoint hits per second.,0.0,0.149,0.851,0.2732
datadog,there will be only one graph and all the endpoints TPS will be shown on the graph.,0.0,0.0,1.0,0.0
datadog,How can I achieve this?,0.0,0.0,1.0,0.0
datadog,any suggestions?,0.0,0.0,1.0,0.0
datadog,I tried to create different matrix but not able to achieve this.,0.0,0.134,0.866,0.1406
datadog,I've read that parser file needs to be created.,0.0,0.2,0.8,0.25
datadog,Can any one help me in getting the Datastax driver Metrics to Datadog.,0.0,0.184,0.816,0.4019
datadog,Tried to search but no luck and was not able find any.,0.167,0.238,0.595,0.296
datadog,The following are the  metrics  that we need to se in the dataDog.,0.0,0.0,1.0,0.0
datadog,"So I use  StatsD  to receive all my metrics from my service, but unfortunately Datadog can't seem to pick up those, how can I debug it?",0.119,0.0,0.881,-0.4767
datadog,I think it'd be great if there's  metrics  endpoint or something for  StatsD  such that I'd be able to curl it and see which metrics has been collected so far.,0.0,0.128,0.872,0.6249
datadog,I am getting this stacktrace when running a go program:,0.0,0.0,1.0,0.0
datadog,The signature of the Event function is:,0.0,0.0,1.0,0.0
datadog,which can also be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L285,0.0,0.0,1.0,0.0
datadog,The type definition for  Event  can be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L333,0.0,0.0,1.0,0.0
datadog,The type definition for  Client  can be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L59,0.0,0.0,1.0,0.0
datadog,"My question is, how do I interpret the memory addresses on this line, and more generally, any stack traces which involve typed variables as targets and as arguments?",0.094,0.0,0.906,-0.4019
datadog,"When I looked at  http://www.goinggo.net/2015/01/stack-traces-in-go.html  (which is the only information I was able to find on the subject), I didn't see anything about how to interpret the output when structs were involved.",0.0,0.0,1.0,0.0
datadog,We have a counter metric in one our micro services which pushes data to DataDog.,0.0,0.0,1.0,0.0
datadog,"I want to display the total count for given time frame, and also the count per day (X axis would have the date and Y axis would have count).",0.0,0.048,0.952,0.0772
datadog,How do we achive this?,0.0,0.0,1.0,0.0
datadog,I tried using  sum by  and  diff  with Query value representation.,0.0,0.211,0.789,0.34
datadog,It gives the total number of the count for given time frame.,0.0,0.106,0.894,0.0772
datadog,But I would like to get a bar graph with the X axis as the date and the Y axis as the count.,0.0,0.153,0.847,0.5023
datadog,Is this possible in DataDog?,0.0,0.0,1.0,0.0
datadog,"I want to control the reading and writing speed to an RDB by  Spark  directly, yet the related parameters as the title already revealed seemingly were not working.",0.0,0.113,0.887,0.296
datadog,Can I conclude that  fetchsize  and  batchsize  didn't work with my testing method?,0.0,0.0,1.0,0.0
datadog,Or they do affect on the facet of reading and writing since the measure result is reasonable based on scale.,0.0,0.0,1.0,0.0
datadog,"I created two  m4.xlarge  Linux entities on  AWS , one is for the execution of  Spark , the other is for data storage on an RDB, using  Datadog  to watch the performance of the  Spark  application, especially on the reading and writing to the RDB.",0.0,0.129,0.871,0.5859
datadog,"Spark  was in the standalone mode, and the application for test is simply pulling some data from a  MySQL  RDB, doing some computation, then pushing back to the  MySQL .",0.0,0.066,0.934,0.2263
datadog,Some details are as the following:,0.0,0.0,1.0,0.0
datadog,"JDBC properties are put in a file,  application.conf , like the following:",0.0,0.217,0.783,0.3612
datadog,"Logging while executing the application is enabled by  log4jx2 , within it, time for writing is measured.",0.0,0.0,1.0,0.0
datadog,The problem:,0.73,0.0,0.27,-0.4019
datadog,I have three instances of a java application running in Kubernetes.,0.0,0.0,1.0,0.0
datadog,My application uses Apache Camel to read from a Kinesis stream.,0.0,0.0,1.0,0.0
datadog,I'm currently observing two related issues:,0.0,0.0,1.0,0.0
datadog,"Each of the three running instances of my application is processing the records coming into the stream, when I only want each record to be processed once (I want three up and running for scaling purposes).",0.0,0.073,0.927,0.1531
datadog,"I was hoping that while one instance is processing record A, a second could be picking up record B, etc.",0.0,0.141,0.859,0.4215
datadog,"Every time my application is re-deployed in Kubernetes, each instance starts every record all over again (in other words, it has no idea where it left off or which records have previously been processed).",0.062,0.0,0.937,-0.296
datadog,"After 5 minutes, the shard iterator that my application is using to poll kinesis times out.",0.0,0.0,1.0,0.0
datadog,"I know that this is normal behavior, but what I don't understand is why my application is not grabbing a new iterator.",0.0,0.0,1.0,0.0
datadog,This screenshot shows the error from DataDog.,0.31,0.0,0.69,-0.4019
datadog,"What I've tried: 
First off, I believe that this issue is caused by inconsistent shard iterator ids, and kinesis consumer ids across the three instances of my application, and across deploys.",0.0,0.0,1.0,0.0
datadog,"However, I have been unable to locate where these values are set in code, and how I could go about setting them.",0.0,0.124,0.876,0.4019
datadog,"Of course, there may also be a better solution altogether.",0.0,0.426,0.574,0.6369
datadog,"I have found very little documentation on Kinesis/Kubernetes/Camel working together, and so very little outside sources have been helpful.",0.0,0.141,0.859,0.4215
datadog,"The documentation on  AWS Kinesis :: Apache Camel  is very limited, but what I have tried playing around with the iterator type and building a custom  Client Configuration .",0.057,0.079,0.863,0.154
datadog,"Let me know if you need any additional information, thanks.",0.0,0.244,0.756,0.4404
datadog,Configuring the client:,0.0,0.0,1.0,0.0
datadog,My route:,0.0,0.0,1.0,0.0
datadog,Edit: Tarun's answer does exactly what I asked for.,0.0,0.0,1.0,0.0
datadog,Eugen's answer is also a very good solution.,0.0,0.536,0.464,0.6976
datadog,"I ended up accepting Tarun's answer as correct, but using Eugen's.",0.0,0.167,0.833,0.2023
datadog,"If you have a similar issue and are worried about other containers accessing the nginx status server, use Tarun's answer.",0.109,0.0,0.891,-0.296
datadog,"If you'd rather stick to Docker's normal hostname scheme, use Eugen's.",0.0,0.0,1.0,0.0
datadog,+++ Original Question +++,0.0,0.434,0.566,0.3182
datadog,I have an application that I build with docker-compose.,0.0,0.0,1.0,0.0
datadog,I am trying to integrate monitoring through DataDog.,0.0,0.0,1.0,0.0
datadog,"I'm using DataDog's Agent container, and so far everything is working.",0.0,0.0,1.0,0.0
datadog,I am trying to get nginx monitoring up and running by adapting  this tutorial .,0.0,0.0,1.0,0.0
datadog,My application is defined in a docker-compose file like this:,0.0,0.238,0.762,0.3612
datadog,"Per the tutorial, I've added a server block to nginx that looks like this:",0.177,0.152,0.671,-0.1027
datadog,"With this configuration, I can check the nginx status from within the nginx container.",0.0,0.0,1.0,0.0
datadog,"So far, so good.",0.0,0.576,0.424,0.6213
datadog,"Now I would like to change the ""allow"" directive in the location block to allow access to the datadog-agent service only.",0.119,0.181,0.7,0.128
datadog,"However, I don't know the IP of the datadog-agent.",0.0,0.0,1.0,0.0
datadog,"When configuring access to the Flask uwsgi server, I was able to use directives like this:",0.0,0.152,0.848,0.3612
datadog,But this doesn't seem to work for allow directives; if I try:,0.0,0.19,0.81,0.3291
datadog,I get the following error:,0.474,0.0,0.526,-0.4019
datadog,How can I safely expose the nginx status to my monitoring container?,0.116,0.232,0.652,0.3818
datadog,I am trying to connect to a MySQL database using python but I am getting a strange error.,0.324,0.0,0.676,-0.6956
datadog,It is compounded by the fact that I can use the same connection values from the  mysql  console command and it connects with no problems.,0.171,0.094,0.734,-0.296
datadog,Here is the exact code I am using:,0.0,0.0,1.0,0.0
datadog,"import pymysql
    from checks import AgentCheck",0.0,0.0,1.0,0.0
datadog,This is the error that I am getting:,0.31,0.0,0.69,-0.4019
datadog,I am running this code on a Ubuntu box and I though initially that it might be because the SSL CA is a self generated cert.,0.0,0.0,1.0,0.0
datadog,"So I followed the steps  here  But, it did not make any difference.",0.0,0.0,1.0,0.0
datadog,Also I have verified that the process that is running this code has full access to the cert files,0.0,0.0,1.0,0.0
datadog,Any ideas what else might be causing this?,0.0,0.0,1.0,0.0
datadog,Datadog Tracing API  requires 64-bit integers serialized as JSON numbers.,0.0,0.0,1.0,0.0
datadog,How can I create JSON with 64-bit integer numbers using JavaScript?,0.0,0.189,0.811,0.2732
datadog,I have a graphql server with multiple endpoints.,0.0,0.0,1.0,0.0
datadog,"It is basically just a CRUD app, so I'm honestly not sure why there's a memory leak.",0.222,0.167,0.611,-0.0216
datadog,The only potentially leaky endpoint I have is one that uploads pics to S3.,0.0,0.0,1.0,0.0
datadog,I've been looking around and have tried taking heap snapshots and comparing them but I'm not even sure which endpoint is the culprit.,0.1,0.0,0.9,-0.3491
datadog,This is the flow I've been following:,0.0,0.0,1.0,0.0
datadog,Is this the correct flow for finding a memory leak?,0.231,0.0,0.769,-0.34
datadog,Is there a better way of doing this without having to guess which endpoint it is coming from?,0.0,0.153,0.847,0.4404
datadog,Are there perhaps tools I can use online that can help me find the source of the memory leak in production without having to guess like this?,0.078,0.17,0.752,0.4215
datadog,Perhaps something like Datadog or something?,0.0,0.333,0.667,0.3612
datadog,"Update: From Heroku's metrics, it looks like the memory usage increases every time a request is made?",0.0,0.143,0.857,0.3612
datadog,But my src/index.js file doesn't do anything special:,0.292,0.0,0.708,-0.438
datadog,I'm trying to setup a Stackdriver dashboard for my custom metrics that my services provide.,0.0,0.0,1.0,0.0
datadog,In particular I'm starting with general  custom/grpc/time_ms  metric that is a gauge and have  status  label on it.,0.0,0.0,1.0,0.0
datadog,I'd love to be able to set up a chart and alert for success rate of the metric(something like  count:custom/grpc/time_ms{status:OK} / count:custom/grpc/time_ms{*} ).,0.0,0.426,0.574,0.9118
datadog,With my previous project I used Datadog and it was  pretty easy to do so there .,0.0,0.319,0.681,0.7269
datadog,But I don't see any similar functionality neither in the UI nor in Stackdriver documentation.,0.0,0.0,1.0,0.0
datadog,So I was wondering if it's not documented or simply not supported?,0.164,0.0,0.836,-0.2411
datadog,"In my  config\environments\development.rb  and  config\environments\production.rb  files, I set some global variables.",0.0,0.0,1.0,0.0
datadog,"In the example below, I have a a  Redis  instance that points to our cache, and a  Statsd  instance that points to the DataDog agent.",0.0,0.0,1.0,0.0
datadog,"In the case of Redis, I added  gem 'redis'  to my gem file, ran  bundle install  and everything worked fine.",0.0,0.091,0.909,0.2023
datadog,"In the case of StatsD, however, it seems that I need to also add  require 'statsd'  at the top of the  development.rb  and  production.rb  files in order to be able to create the instance.",0.0,0.112,0.888,0.4404
datadog,"Of course, I also added  gem 'dogstatsd-ruby'  to my gem file and ran  bundle install , but that didn't seem to be enough.",0.0,0.0,1.0,0.0
datadog,"If I don't add the  require  statement at the top of the config files, I get the following error when I try to run my Rails app:",0.102,0.068,0.83,-0.2263
datadog,"Can anyone explain why I have to add the  require  statement only in this particular case (StatsD), or is there is a better way to do this?",0.0,0.108,0.892,0.4404
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,"Note: according to tests (see Edit below), this occurs only on a Linux machine.",0.0,0.0,1.0,0.0
datadog,I have an ASP.NET Core Blazor application (using server-side hosting model) running on a Raspberry Pi.,0.0,0.0,1.0,0.0
datadog,Part of the application's functionality is to dim/brighten screen based on when system was last interacted with.,0.0,0.0,1.0,0.0
datadog,"To do that, every 1 second or so I spawn a terminal child-process to run  xprintidle , parse its output, and act accordingly.",0.0,0.0,1.0,0.0
datadog,"I use DataDog for monitoring, and I am having a memory leak until the system crashes (it takes a few days to use up all memory, but it does occur eventually):",0.061,0.0,0.939,-0.1779
datadog,"I have pinpointed that the following method is what leaks memory - if I skip calling it and use some constant timespan, the memory does not leak:
I have following code to do so:",0.0,0.066,0.934,0.2584
datadog,"I spent a lot of time (over span of months - literally) trying various changes to solve this issue -  WaitForExitAsync  was overhauled a lot, tried different ways of disposing.",0.0,0.07,0.93,0.2023
datadog,I attempted to call GC.Collect() periodically.,0.0,0.0,1.0,0.0
datadog,Also tried running the application with both Server and Workstation GC mode.,0.0,0.0,1.0,0.0
datadog,"As I mentioned earlier, I am pretty sure it's this code that leaks - if I don't call  ExecuteAndWaitAsync , there's no memory leak.",0.183,0.219,0.598,0.2263
datadog,The result class is also not stored by the caller - it simply parses a value and uses it right away:,0.0,0.118,0.882,0.34
datadog,Am I missing something?,0.524,0.0,0.476,-0.296
datadog,"Is it Process class leaking, or the way I read output?",0.0,0.0,1.0,0.0
datadog,"EDIT : As people in comments asked, I created minimum runnable code, basically fetching all relevant methods in a single class and execute in a loop.",0.0,0.087,0.913,0.25
datadog,"The code is available as a gist:  https://gist.github.com/TehGM/c953b670ad8019b2b2be6af7b14807c2 
I ran it both on my Windows machine and Raspberry Pi.",0.0,0.0,1.0,0.0
datadog,"On Windows, memory seemed stable, however on Raspberry Pi it was clearly leaking.",0.0,0.308,0.692,0.5994
datadog,I tried both  xprintidle  and  ifconfig  to make sure it's not an issue with xprintidle only.,0.0,0.141,0.859,0.3182
datadog,"Tried both .NET Core 3.0 and .NET Core 3.1, and effect was largely the same.",0.0,0.0,1.0,0.0
datadog,I have a springboot application which I'm trying to instrument using bytebuddy.,0.0,0.0,1.0,0.0
datadog,I'm running into classpath issues which I'm not able to understand.,0.0,0.0,1.0,0.0
datadog,"Firstly, the following is other literature on this:",0.0,0.0,1.0,0.0
datadog,https://github.com/raphw/byte-buddy/issues/473,0.0,0.0,1.0,0.0
datadog,https://github.com/raphw/byte-buddy/issues/87,0.0,0.0,1.0,0.0
datadog,Unable to instrument apache httpclient using javaagent for spring boot uber jar application,0.0,0.0,1.0,0.0
datadog,https://github.com/raphw/byte-buddy/issues/109,0.0,0.0,1.0,0.0
datadog,https://github.com/raphw/byte-buddy/issues/473,0.0,0.0,1.0,0.0
datadog,https://github.com/raphw/byte-buddy/issues/489,0.0,0.0,1.0,0.0
datadog,https://github.com/spring-projects/spring-boot/issues/4868,0.0,0.0,1.0,0.0
datadog,https://github.com/alibaba/transmittable-thread-local/issues/161,0.0,0.0,1.0,0.0
datadog,"Problem is that Spring-boot bundles the application into one uber-jar, which contains other jars inside it",0.153,0.0,0.847,-0.4019
datadog,"A thing to note here is, that if I run the application using IntelliJ, it doesn't use the uber-jar and runs via main class with a bunch of jars as classpath arguments.",0.088,0.0,0.912,-0.4019
datadog,"Due to this difference, When running via uber-jar( java -jar target/demo-0.0.1-SNAPSHOT.jar ), some classes are not available at the time the Agent runs.",0.0,0.0,1.0,0.0
datadog,"The classes are loadable at the time of application main, as spring-boot uses its own classloader, which is created at some time b/w agent and application main methods.",0.0,0.069,0.931,0.25
datadog,I'll describe the behaviour at various points of time below:,0.0,0.0,1.0,0.0
datadog,Both spring and javax classes are not loaded as they are not directly in the classpath as per the App Classloader.,0.0,0.0,1.0,0.0
datadog,"It's part of an inner jar, something like  app.jar!/BOOT-INF/lib/some.jar 
App Classloader won't be able to load this.",0.0,0.149,0.851,0.4199
datadog,"After loading,  Class.forName(""javax.servlet.http.HttpServletRequest"").classLoader  is also the same  LaunchedURLClassLoader .",0.0,0.0,1.0,0.0
datadog,"I have two classes,  SpringBootInterceptor  (which contains the intercept method) and  SpringBootInterceptorOne  (which contains the entry and exit method)",0.0,0.0,1.0,0.0
datadog,The function  intercept()  is called in the premain path.,0.0,0.0,1.0,0.0
datadog,The function  visit  is called when the type is actually attempted to load.,0.0,0.0,1.0,0.0
datadog,At that point bytebuddy tris to intercept those methods and modify bytecode.,0.0,0.0,1.0,0.0
datadog,"In the current version of this code, the function  intercept()  throws  ClassNotFoundException  and interception doesn't happen.",0.0,0.0,1.0,0.0
datadog,"So, I tried to change the code, to the following:",0.0,0.0,1.0,0.0
datadog,"Here, basically loading the class lazily.",0.0,0.0,1.0,0.0
datadog,Helper functions from datadog:,0.0,0.444,0.556,0.34
datadog,https://github.com/DataDog/dd-trace-java/blob/master/dd-java-agent/agent-tooling/src/main/java/datadog/trace/agent/tooling/ByteBuddyElementMatchers.java,0.0,0.0,1.0,0.0
datadog,"After doing this, the  intercept  function doesn't fail, but the  visit()  function behaves strangely:",0.167,0.115,0.717,-0.2204
datadog,"At this point,",0.0,0.0,1.0,0.0
datadog,"Next, I tried the following:",0.0,0.0,1.0,0.0
datadog,"This causes the interceptor to be ""installed"".",0.0,0.0,1.0,0.0
datadog,The entry and exit methods are called okay.,0.0,0.213,0.787,0.2263
datadog,But casting the arguments to proper types gives error for the types of the arguments,0.47,0.0,0.53,-0.8922
datadog,Here is SpringBootInterceptorOne for reference:,0.0,0.0,1.0,0.0
datadog,UPDATE:,0.0,0.0,1.0,0.0
datadog,"As per the advice offered in an answer, I tried the following:",0.0,0.0,1.0,0.0
datadog,It again throws Error:,0.474,0.0,0.526,-0.4019
datadog,I have a project built in Lumen (php Framework) hosted on a docker container built from alpine as base image using apache2 server with php 7.x,0.0,0.0,1.0,0.0
datadog,Here's part of my Dockerfile:,0.0,0.0,1.0,0.0
datadog,The purpose of this project is to receive http post requests (i.e.,0.0,0.0,1.0,0.0
datadog,webhook events from external system) and process them.,0.0,0.0,1.0,0.0
datadog,"When the project is deployed, it runs fine for several days before this error starts showing up in our datadog logs:",0.131,0.075,0.794,-0.3237
datadog,[core:warn] [pid 9] (99)Address not available: AH00056: connect to listener on [::]:80,0.0,0.0,1.0,0.0
datadog,"When this error occurs, the site/project is not publicly accessible but the apache is still running.",0.11,0.0,0.89,-0.2144
datadog,"If i restart the container, everything goes back to normal.",0.0,0.0,1.0,0.0
datadog,"Upon investigating this further, I noticed this happens every time my api is hit simultaneously.",0.0,0.0,1.0,0.0
datadog,I.e.,0.0,0.0,1.0,0.0
datadog,"3 days ago, project was hit with 145 request simultaneously and since then the app is no longer accessible.",0.115,0.0,0.885,-0.296
datadog,Apache is refusing to serve any new request but the container is up and running and there's plenty of memory/disk-space available to the container.,0.074,0.0,0.926,-0.2144
datadog,Any idea what causes this?,0.0,0.0,1.0,0.0
datadog,do I need to optimise the mpm.conf to allow for more workers / child processes etc.?,0.0,0.286,0.714,0.5859
datadog,I am currently using stock config.,0.0,0.0,1.0,0.0
datadog,I'm trying to learn how to use docker and am having some troubles.,0.2,0.0,0.8,-0.4588
datadog,I'm using a  docker-compose.yaml  file for running a python script that connects to a mysql container and I'm trying to use  ddtrace  to send traces to datadog.,0.0,0.0,1.0,0.0
datadog,I'm using the following image from  this github page from datadog,0.0,0.0,1.0,0.0
datadog,And my  docker-compose.yaml  looks like,0.0,0.385,0.615,0.3612
datadog,"So then I'm running the command  docker-compose run --rm ddtrace-test python test.py , where  test.py  looks like",0.0,0.143,0.857,0.3612
datadog,"And when I run the command, I'm returned with",0.0,0.0,1.0,0.0
datadog,I'm not sure what this error means.,0.483,0.0,0.517,-0.5664
datadog,"When I use my key and run from local instead of over a docker image, it works fine.",0.0,0.107,0.893,0.2023
datadog,What could be going wrong here?,0.383,0.0,0.617,-0.4767
datadog,Are there some broker metrics we can use to monitor Kafka broker if acknowledgment lag is very high in the producer side.,0.103,0.0,0.897,-0.34
datadog,We are using datadog to monitor producer and Kafka broker side.,0.0,0.0,1.0,0.0
datadog,It can be seen that the producer ack lag is more than 10 secs.,0.156,0.0,0.844,-0.34
datadog,"However, on the broker side, I feel like only using  message.in.rate  and  kafka.net.bytes_in.rate  are not very efficient.",0.134,0.131,0.735,-0.0126
datadog,It would be better we can have some LAG metrics in the broker side to indicate  the broker is fully loaded to acknowledge back the producer.,0.104,0.097,0.799,-0.0601
datadog,"Also, we only use  kafka.acks = 1  for partition leader.",0.0,0.0,1.0,0.0
datadog,I wonder does anyone has some experience about it and any advice is welcome.,0.0,0.2,0.8,0.4588
datadog,:) Thanks in advance.,0.0,0.747,0.253,0.7096
datadog,"I have an agent (datadog agent but could be something else) running on all the nodes of my cluster, deployed through a DaemonSet.",0.0,0.0,1.0,0.0
datadog,"This agent is collecting diverse metrics about the host: cpu and memory usage, IO, which containers are running.",0.0,0.0,1.0,0.0
datadog,"It can also collect custom metrics, by listening on a specific port 1234.",0.0,0.0,1.0,0.0
datadog,How can I send data from a pod to the instance of the agent running on the same node than the pod?,0.0,0.0,1.0,0.0
datadog,If I use a Kubernetes service the calls to send the metric will be load balanced across all my agents and I'll lose the correlation between the pod emitting the metric and the host it's running on.,0.074,0.0,0.926,-0.4019
datadog,I'd like to use Airflow with Statsd and DataDog to monitor if DAG takes e.g.,0.0,0.152,0.848,0.3612
datadog,twice time as its previous execution(s).,0.0,0.0,1.0,0.0
datadog,"So, I need some kind of a real-time timer for a DAG (or  operator ).",0.0,0.0,1.0,0.0
datadog,I'm aware that Airflow supports  some metrics .,0.0,0.294,0.706,0.3612
datadog,"However, to my understanding all of the metrics are related to finished tasks/DAGs, right?",0.0,0.0,1.0,0.0
datadog,"So, It's not the solution, because I'd like to monitor running DAGs.",0.136,0.173,0.691,0.1376
datadog,"I also considered the  timeout_execution / SLA  features, but they are not suitable for this use-case",0.0,0.0,1.0,0.0
datadog,"I'd like to be notified that some DAG hangs, but I don't want to kill it.",0.066,0.34,0.594,0.7596
datadog,"What is the difference between ""hikaricp.connections. """,0.0,0.0,1.0,0.0
datadog,"and ""jdbc.connections. """,0.0,0.0,1.0,0.0
datadog,meter names?,0.0,0.0,1.0,0.0
datadog,I have a Spring Boot 2 application that is defaulting to the Hikari connection pool mechanism and I am tring to understand how to best monitor the database connections in production.,0.0,0.139,0.861,0.6369
datadog,"After visualizing my metrics in Datadog, I am seeing a slight difference in the metric data for both hikariCP.connections.active and jdbc.connections.active.",0.0,0.0,1.0,0.0
datadog,Are the JDBC meter names duplicates?,0.0,0.0,1.0,0.0
datadog,Should one be used over the other or does it not matter.,0.089,0.0,0.911,-0.0191
datadog,I have been struggling to find more detailed documentation on this.,0.237,0.0,0.763,-0.4215
datadog,Any help is much appreciated.,0.0,0.667,0.333,0.7184
datadog,My task definition:,0.0,0.0,1.0,0.0
datadog,My service defintion:,0.0,0.0,1.0,0.0
datadog,I get the following error -,0.474,0.0,0.526,-0.4019
datadog,Is there something I am missing here?,0.306,0.0,0.694,-0.296
datadog,Looking at the Terraform docs and GitHub issues this should have worked.,0.0,0.0,1.0,0.0
datadog,Is it related to running Datadog as a daemon?,0.0,0.0,1.0,0.0
datadog,I trying to run 3 containers and get the error for all of them.,0.197,0.0,0.803,-0.4019
datadog,"I found the source code, but can't find the answer to why this is happening.",0.0,0.0,1.0,0.0
datadog,"Also, tried to look at the internet and can't find anyone with the same error.",0.162,0.0,0.838,-0.4019
datadog,"Source code:
 https://github.com/DataDog/datadog-agent/blob/eb35254e9e13165b4148fc9280ef79e2d6bf8235/pkg/util/docker/containers.go",0.0,0.0,1.0,0.0
datadog,The error from /var/log/syslog:,0.474,0.0,0.526,-0.4019
datadog,"process-agent[30759]: 2019-12-11 08:58:17 UTC | PROCESS | ERROR |
  (pkg/util/docker/containers.go:110 in ListContainers) | Failed to get
  host IPs.",0.359,0.0,0.641,-0.7739
datadog,Container XXXXX will be missing network info: %!s(),0.263,0.0,0.737,-0.3595
datadog,The containers running on the Host network.,0.0,0.0,1.0,0.0
datadog,Thanks,0.0,1.0,0.0,0.4404
datadog,We developed our team's new service with spring-webflux.,0.0,0.0,1.0,0.0
datadog,It has been working well.,0.0,0.344,0.656,0.2732
datadog,Only one thing we could not figure out is below log,0.0,0.0,1.0,0.0
datadog,"Error [reactor.netty.ReactorNetty$InternalNettyException: io.netty.channel.ExtendedClosedChannelException] for HTTP GET ""[TARGET_URL]"", but ServerHttpResponse already committed (200 OK)",0.119,0.171,0.71,0.2023
datadog,"Although it does not frequently happen, it is logged in our datadog log.",0.0,0.0,1.0,0.0
datadog,The service is a kind of middle man.,0.0,0.0,1.0,0.0
datadog,"One service sends ""get"" request to it, and then it calls other backends to gather data, then return.",0.0,0.0,1.0,0.0
datadog,"In the service layer, we use Mono.zip to combine all response from various backends.",0.0,0.0,1.0,0.0
datadog,Both clients and backends are traditional thread base sprint applications.,0.0,0.0,1.0,0.0
datadog,Our code to build webclient is:,0.0,0.0,1.0,0.0
datadog,connection timeout is set to 2500 milliseconds.,0.0,0.0,1.0,0.0
datadog,read timeout is 50-90ms depends on backends.,0.0,0.0,1.0,0.0
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,Docker daemon got crashed after short span of time.,0.0,0.0,1.0,0.0
datadog,Lately Docker services of stacks doesn't get properly up and resulted in app crash which only got fixed when i removed all the stacks and redeployed them.,0.097,0.0,0.903,-0.4019
datadog,I'm running my whole Android app and Other APIs on docker swarm cluster.,0.0,0.0,1.0,0.0
datadog,*I have my machine running on Google Cloud platform with around 75 CPUs and 250G memory which is more than enough for all the services I'm running on my machine.,0.0,0.0,1.0,0.0
datadog,"I have haproxy in frontend which does reverse proxy, backend as python flask api with 5 replicas, Database connectivity through pgbouncer.",0.0,0.0,1.0,0.0
datadog,"Else Logspout, datadog, portainer, redis, etc.",0.0,0.0,1.0,0.0
datadog,*,0.0,0.0,0.0,0.0
datadog,"I couldn't understand that even if i have enough resources, proper setup system with enough max_pids still the daemon crashed.",0.0,0.0,1.0,0.0
datadog,"
Generally Necessary:
- cgroup hierarchy: properly mounted [/sys/fs/cgroup]",0.0,0.0,1.0,0.0
datadog,apparmor: enabled and tools installed,0.0,0.0,1.0,0.0
datadog,CONFIG_NAMESPACES: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_NET_NS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_PID_NS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_IPC_NS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_UTS_NS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUPS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUP_CPUACCT: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUP_DEVICE: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUP_FREEZER: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUP_SCHED: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CPUSETS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_MEMCG: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_KEYS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_VETH: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_BRIDGE: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_BRIDGE_NETFILTER: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NF_NAT_IPV4: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_IP_NF_FILTER: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_IP_NF_TARGET_MASQUERADE: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NETFILTER_XT_MATCH_ADDRTYPE: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NETFILTER_XT_MATCH_CONNTRACK: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NETFILTER_XT_MATCH_IPVS: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_IP_NF_NAT: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NF_NAT: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NF_NAT_NEEDED: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_POSIX_MQUEUE: enabled,0.0,0.0,1.0,0.0
datadog,Optional Features:,0.0,0.0,1.0,0.0
datadog,CONFIG_USER_NS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_SECCOMP: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUP_PIDS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_MEMCG_SWAP: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_MEMCG_SWAP_ENABLED: missing,0.688,0.0,0.312,-0.296
datadog,(cgroup swap accounting is currently enabled),0.0,0.0,1.0,0.0
datadog,CONFIG_LEGACY_VSYSCALL_EMULATE: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_BLK_CGROUP: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_BLK_DEV_THROTTLING: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_IOSCHED_CFQ: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CFQ_GROUP_IOSCHED: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUP_PERF: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUP_HUGETLB: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_NET_CLS_CGROUP: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_CGROUP_NET_PRIO: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CFS_BANDWIDTH: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_FAIR_GROUP_SCHED: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_RT_GROUP_SCHED: missing,0.688,0.0,0.312,-0.296
datadog,CONFIG_IP_NF_TARGET_REDIRECT: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_IP_VS: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_IP_VS_NFCT: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_IP_VS_PROTO_TCP: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_IP_VS_PROTO_UDP: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_IP_VS_RR: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_EXT4_FS: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_EXT4_FS_POSIX_ACL: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_EXT4_FS_SECURITY: enabled,0.0,0.0,1.0,0.0
datadog,Network Drivers:,0.0,0.0,1.0,0.0
datadog,"""overlay"":",0.0,0.0,1.0,0.0
datadog,CONFIG_VXLAN: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_BRIDGE_VLAN_FILTERING: enabled,0.0,0.0,1.0,0.0
datadog,Optional (for encrypted networks):,0.0,0.0,1.0,0.0
datadog,CONFIG_CRYPTO: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CRYPTO_AEAD: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CRYPTO_GCM: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CRYPTO_SEQIV: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_CRYPTO_GHASH: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_XFRM: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_XFRM_USER: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_XFRM_ALGO: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_INET_ESP: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_INET_XFRM_MODE_TRANSPORT: enabled (as module),0.0,0.0,1.0,0.0
datadog,"""ipvlan"":",0.0,0.0,1.0,0.0
datadog,CONFIG_IPVLAN: enabled (as module),0.0,0.0,1.0,0.0
datadog,"""macvlan"":",0.0,0.0,1.0,0.0
datadog,CONFIG_MACVLAN: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_DUMMY: enabled (as module),0.0,0.0,1.0,0.0
datadog,"""ftp,tftp client in container"":",0.0,0.0,1.0,0.0
datadog,CONFIG_NF_NAT_FTP: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NF_CONNTRACK_FTP: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NF_NAT_TFTP: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_NF_CONNTRACK_TFTP: enabled (as module),0.0,0.0,1.0,0.0
datadog,Storage Drivers:,0.0,0.0,1.0,0.0
datadog,"""aufs"":",0.0,0.0,1.0,0.0
datadog,CONFIG_AUFS_FS: enabled (as module),0.0,0.0,1.0,0.0
datadog,"""btrfs"":",0.0,0.0,1.0,0.0
datadog,CONFIG_BTRFS_FS: enabled (as module),0.0,0.0,1.0,0.0
datadog,CONFIG_BTRFS_FS_POSIX_ACL: enabled,0.0,0.0,1.0,0.0
datadog,"""devicemapper"":",0.0,0.0,1.0,0.0
datadog,CONFIG_BLK_DEV_DM: enabled,0.0,0.0,1.0,0.0
datadog,CONFIG_DM_THIN_PROVISIONING: enabled (as module),0.0,0.0,1.0,0.0
datadog,"""overlay"":",0.0,0.0,1.0,0.0
datadog,CONFIG_OVERLAY_FS: enabled (as module),0.0,0.0,1.0,0.0
datadog,"""zfs"":",0.0,0.0,1.0,0.0
datadog,/dev/zfs: missing,0.688,0.0,0.312,-0.296
datadog,zfs command: missing,0.524,0.0,0.476,-0.296
datadog,zpool command: missing,0.524,0.0,0.476,-0.296
datadog,Limits:,0.0,0.0,1.0,0.0
datadog,NEED HELP ON UNDERSTANDING THE ISSUE HERE.,0.0,0.31,0.69,0.4019
datadog,RESULT OF  docker info  here.,0.0,0.0,1.0,0.0
datadog,"
Containers: 170",0.0,0.0,1.0,0.0
datadog,Running: 167,0.0,0.0,1.0,0.0
datadog,Paused: 0,0.0,0.0,1.0,0.0
datadog,Stopped: 3,1.0,0.0,0.0,-0.2263
datadog,Images: 144,0.0,0.0,1.0,0.0
datadog,Server Version: 18.09.7,0.0,0.0,1.0,0.0
datadog,Storage Driver: overlay2,0.0,0.0,1.0,0.0
datadog,Backing Filesystem: extfs,0.0,0.355,0.645,0.0258
datadog,Supports d_type: true,0.0,0.841,0.159,0.6486
datadog,Native Overlay Diff: true,0.0,0.483,0.517,0.4215
datadog,Logging Driver: json-file,0.0,0.0,1.0,0.0
datadog,Cgroup Driver: cgroupfs,0.0,0.0,1.0,0.0
datadog,Plugins:,0.0,0.0,1.0,0.0
datadog,Volume: local,0.0,0.0,1.0,0.0
datadog,Network: bridge host macvlan null overlay,0.0,0.0,1.0,0.0
datadog,Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog,0.0,0.0,1.0,0.0
datadog,Swarm: active,0.0,0.73,0.27,0.4019
datadog,NodeID: uqkfe247qfql50b1bft3r205b,0.0,0.0,1.0,0.0
datadog,Is Manager: true,0.0,0.583,0.417,0.4215
datadog,ClusterID: fe79jmqus0l6zsa7kl41cbqa9,0.0,0.0,1.0,0.0
datadog,Managers: 1,0.0,0.0,1.0,0.0
datadog,Nodes: 2,0.0,0.0,1.0,0.0
datadog,Default Address Pool: 10.0.0.0/8,0.0,0.0,1.0,0.0
datadog,SubnetSize: 24,0.0,0.0,1.0,0.0
datadog,Orchestration:,0.0,0.0,1.0,0.0
datadog,Task History Retention Limit: 5,0.0,0.0,1.0,0.0
datadog,Raft:,0.0,0.0,1.0,0.0
datadog,Snapshot Interval: 10000,0.0,0.0,1.0,0.0
datadog,Number of Old Snapshots to Retain: 0,0.0,0.206,0.794,0.0772
datadog,Heartbeat Tick: 1,0.0,0.0,1.0,0.0
datadog,Election Tick: 10,0.0,0.0,1.0,0.0
datadog,Dispatcher:,0.0,0.0,1.0,0.0
datadog,Heartbeat Period: 5 seconds,0.0,0.0,1.0,0.0
datadog,CA Configuration:,0.0,0.0,1.0,0.0
datadog,Expiry Duration: 3 months,0.0,0.0,1.0,0.0
datadog,Force Rotate: 0,0.0,0.0,1.0,0.0
datadog,Autolock Managers: false,0.0,0.0,1.0,0.0
datadog,Root Rotation In Progress: false,0.0,0.412,0.588,0.4215
datadog,Node Address: 10.160.0.30,0.0,0.0,1.0,0.0
datadog,"Manager Addresses:
  10.160.0.30:2377",0.0,0.0,1.0,0.0
datadog,Runtimes: runc,0.0,0.0,1.0,0.0
datadog,Default Runtime: runc,0.0,0.0,1.0,0.0
datadog,Init Binary: docker-init,0.0,0.0,1.0,0.0
datadog,containerd version:,0.0,0.0,1.0,0.0
datadog,runc version: N/A,0.0,0.0,1.0,0.0
datadog,init version: v0.18.0 (expected: fec3683b971d9c3ef73f284f176672c44b448662),0.0,0.0,1.0,0.0
datadog,Security Options:,0.0,0.706,0.294,0.34
datadog,apparmor,0.0,0.0,1.0,0.0
datadog,seccomp,0.0,0.0,1.0,0.0
datadog,Profile: default,0.0,0.0,1.0,0.0
datadog,Kernel Version: 4.15.0-1040-gcp,0.0,0.0,1.0,0.0
datadog,Operating System: Ubuntu 18.04.3 LTS,0.0,0.455,0.545,0.516
datadog,OSType: linux,0.0,0.0,1.0,0.0
datadog,Architecture: x86_64,0.0,0.0,1.0,0.0
datadog,CPUs: 76,0.0,0.0,1.0,0.0
datadog,Total Memory: 246GiB,0.0,0.0,1.0,0.0
datadog,Name: rc-manager-instance,0.0,0.0,1.0,0.0
datadog,ID: 2PEM:4AF6:47RA:EMDM:CIMD:H4OC:5MNG:SXNI:ERFB:ML5G:O3YI:6VWA,0.0,0.0,1.0,0.0
datadog,Docker Root Dir: /var/lib/docker,0.0,0.0,1.0,0.0
datadog,Debug Mode (client): false,0.0,0.0,1.0,0.0
datadog,Debug Mode (server): false,0.0,0.0,1.0,0.0
datadog,Registry:  https://index.docker.io/v1/,0.0,0.0,1.0,0.0
datadog,Labels:,0.0,0.0,1.0,0.0
datadog,Experimental: false,0.0,0.0,1.0,0.0
datadog,Insecure Registries:,0.737,0.0,0.263,-0.4215
datadog,10.160.0.30:7000,0.0,0.0,1.0,0.0
datadog,127.0.0.0/8,0.0,0.0,1.0,0.0
datadog,Live Restore Enabled: false,0.0,0.423,0.577,0.296
datadog,i am trying to develop generic terraform modules to support data-dog monitors and let the user of the modules to append resources and/or override resources in side generic modules.,0.0,0.091,0.909,0.4019
datadog,"terraform  overrides  feature works fine without modules, But not working when using modules.",0.0,0.104,0.896,0.1027
datadog,how to override some of the resource parameters inside modules?,0.0,0.0,1.0,0.0
datadog,Requirements:,0.0,0.0,1.0,0.0
datadog,"/modules/datadog/monitors.tf  contains list of resources, each resource represents a generic datadog monitor with default parameter values.",0.0,0.162,0.838,0.4019
datadog,Each  individual application may choose to override one or more parameters inside each resource .,0.0,0.0,1.0,0.0
datadog,"/application-1/monitors.tf  contains module with source as  /modules/datadog/  , some more monitors that are not covered in generic monitors and some variables.",0.0,0.0,1.0,0.0
datadog,/application-1/monitors.tf,0.0,0.0,1.0,0.0
datadog,/modules/datadog/monitors.tf,0.0,0.0,1.0,0.0
datadog,Solution 1 : Add overrides.tf to  /modules/datadog  Directory.,0.0,0.315,0.685,0.3182
datadog,"terraform  override feature  merges content in overrides.tf to 
 configuration defined in monitors.tf.",0.0,0.0,1.0,0.0
datadog,But the problem with this solution is each application specific overrides.tf needs to be copied over to /modules/datadog Directory before running apply command.,0.129,0.107,0.764,-0.1531
datadog,overrides.tf,0.0,0.0,1.0,0.0
datadog,Solution 2 : can i use overrides with modules?,0.0,0.315,0.685,0.3182
datadog,"i tried to override resource parameters by copying overrides.tf to /application-1/ Directory, But terraform is not overriding resources, instead it is considering both as different resources.",0.0,0.0,1.0,0.0
datadog,When I start consumer with single instance it is show in consumer group but it is not consuming data from topic.,0.0,0.0,1.0,0.0
datadog,After that if I start another consumer and my first consumer start consuming data but latest consumer instance doesn't have any partition assigned to it.,0.0,0.0,1.0,0.0
datadog,below is the info log when first consumer instance starts.,0.0,0.0,1.0,0.0
datadog,"INFO:kafka.client:Bootstrapping cluster metadata from [(u'kafka-broker1.ap-south-1.staging.internal', 9092, 0)]
  INFO:kafka.conn:: connecting to 172.31.1.66:9092
  INFO:kafka.client:Bootstrap succeeded: found 3 brokers and 19 topics.",0.0,0.141,0.859,0.4215
datadog,INFO:kafka.conn:: Closing connection.,0.0,0.0,1.0,0.0
datadog,"INFO:kafka.conn:: connecting to 172.31.1.148:9092
  INFO:kafka.conn:Broker version identifed as 0.11.0
  INFO:kafka.conn:Set configuration api_version=(0, 11, 0) to skip auto check_version requests on startup
  INFO:kafka.consumer.subscription_state:Subscribing to pattern: /events/
  INFO:kafka.conn:: connecting to 172.31.1.70:9092
  INFO:kafka.cluster:Group coordinator for datadog is BrokerMetadata(nodeId=1, host=u'kafka-broker1.ap-south-1.staging.internal', port=9092, rack=None)
  INFO:kafka.coordinator:Discovered coordinator 1 for group datadog
  INFO:kafka.conn:: connecting to 172.31.1.66:9092
  INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set([]) for group datadog
  INFO:kafka.coordinator:(Re-)joining group datadog
  INFO:kafka.consumer.subscription_state:Updating subscribed topics to: [u'events']
  INFO:kafka.coordinator:Joined group 'datadog' (generation 843) with member_id kafka-python-1.3.5-e3c25fb3-39ea-4550-845f-9b663355b4f5
  INFO:kafka.coordinator:Successfully joined group datadog with generation 843
  INFO:kafka.consumer.subscription_state:Updated partition assignment: []
  INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([]) for group datadog",0.0,0.0,1.0,0.0
datadog,When I start second instance first instance get partition assigned and second instance have 0 partition assigned and have same info log as first instance before starting second instance.,0.0,0.0,1.0,0.0
datadog,below is the info log of first instance after second instance started.,0.0,0.0,1.0,0.0
datadog,"INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([]) for group datadog
  WARNING:kafka.coordinator:Heartbeat failed for group datadog because it is rebalancing
  WARNING:kafka.coordinator:Heartbeat failed ([Error 27] RebalanceInProgressError); retrying
  INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set([]) for group datadog
  INFO:kafka.coordinator:(Re-)joining group datadog
  INFO:kafka.coordinator:Skipping heartbeat: no auto-assignment or waiting on rebalance
  INFO:kafka.coordinator:Joined group 'datadog' (generation 843) with member_id kafka-python-1.3.5-ddb66185-c615-4f31-9729-9384131f24c9
  INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range
  INFO:kafka.coordinator:Successfully joined group datadog with generation 843
  INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic=u'events', partition=0), TopicPartition(topic=u'events', partition=1), TopicPartition(topic=u'events', partition=2), TopicPartition(topic=u'events', partition=3), TopicPartition(topic=u'events', partition=4), TopicPartition(topic=u'events', partition=5), TopicPartition(topic=u'events', partition=6), TopicPartition(topic=u'events', partition=7), TopicPartition(topic=u'events', partition=8), TopicPartition(topic=u'events', partition=9)]
  INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([TopicPartition(topic=u'events', partition=6), TopicPartition(topic=u'events', partition=7), TopicPartition(topic=u'events', partition=8), TopicPartition(topic=u'events', partition=9), TopicPartition(topic=u'events', partition=0), TopicPartition(topic=u'events', partition=1), TopicPartition(topic=u'events', partition=2), TopicPartition(topic=u'events', partition=3), TopicPartition(topic=u'events', partition=4), TopicPartition(topic=u'events', partition=5)]) for group datadog",0.072,0.0,0.928,-0.8316
datadog,"We always have 1 consumer instance with unassigned partitions on all the cases,  last consumer instance always have 0 partition assigned",0.0,0.0,1.0,0.0
datadog,**Below are the screenshot for the same **,0.0,0.0,1.0,0.0
datadog,No partitions assigned to first consumer instance,0.268,0.0,0.732,-0.296
datadog,No partitions assigned to last consumer instance,0.268,0.0,0.732,-0.296
datadog,We also suspect clustering of kafka.,0.306,0.0,0.694,-0.296
datadog,When we had only 1 kafka node partition assignment to consumer instance and re-balancing was working fine.,0.0,0.107,0.893,0.2023
datadog,But after going into multi-node cluster we are facing this issue,0.0,0.0,1.0,0.0
datadog,I have a very simple case where I want to see how many time a user click on the ButtonA in my app.,0.0,0.067,0.933,0.0772
datadog,I'm using DropWizard metrics counter to archive this and the coursera reporter to report them to DataDog every 1 minutes.,0.0,0.0,1.0,0.0
datadog,But what is happening is that this counter doesn't behave like I thought it would.,0.17,0.0,0.83,-0.395
datadog,"so for example if the buttonA has been clicked 4 times, the counter will keep the value 4 until the app restart which is not very useful.",0.094,0.086,0.821,-0.0574
datadog,Is there an other metrics I'm not aware about that would keep a count and at each reports reset to 0 ?,0.0,0.0,1.0,0.0
datadog,So that on Datadog dashboard I can easily sum all the count and manage to get the exact numbers even if the app is restarted it will not affect the metrics.,0.0,0.076,0.924,0.34
datadog,it is possible to install some special sub-package from package?,0.0,0.231,0.769,0.4019
datadog,"For example, I want to create package with slack, datadog, sentry plugins (wrappers).",0.0,0.254,0.746,0.34
datadog,But I want to allow user what he wants to install.,0.0,0.322,0.678,0.4215
datadog,Like:,0.0,1.0,0.0,0.3612
datadog,Can it be done without separating all plugins to different packages?,0.0,0.0,1.0,0.0
datadog,I am getting Cassandra timeouts using the Phantom-DSL with the Datastax Cassandra driver.,0.0,0.0,1.0,0.0
datadog,"However, Cassandra does not seem to be overloaded.",0.0,0.0,1.0,0.0
datadog,Below is the exception I get:,0.0,0.0,1.0,0.0
datadog,And here are the statistics I get from the Cassandra Datadog connector over this time period:,0.0,0.0,1.0,0.0
datadog,You can see our read rate (per second) on the top-center graph.,0.0,0.0,1.0,0.0
datadog,Our CPU and memory usage are very low.,0.255,0.0,0.745,-0.3384
datadog,Here is how we are configuring the Datastax driver:,0.0,0.0,1.0,0.0
datadog,Our  nodetool cfstats  looks like this:,0.0,0.333,0.667,0.3612
datadog,"When we ran  cassandra-stress , we didn't experience any issues: we were getting a steady 50k reads per second,  as expected .",0.0,0.0,1.0,0.0
datadog,Cassandra has this error whenever I make my queries:,0.309,0.0,0.691,-0.481
datadog,Why are we getting timeouts?,0.0,0.0,1.0,0.0
datadog,EDIT:  I had the wrong dashboard uploaded.,0.383,0.0,0.617,-0.4767
datadog,Please see the new image.,0.0,0.365,0.635,0.3182
datadog,I'm using a SaaS for my AWS instance monitoring and Mandrill for email sending/campaigns.,0.0,0.0,1.0,0.0
datadog,I had created a simple chart with  Zapier  but I'd rather like to host it myself.,0.0,0.284,0.716,0.5789
datadog,So my question is:,0.0,0.0,1.0,0.0
datadog,How can I receive a webhook signal from Mandrill and then send it to Datadog from my server?,0.0,0.0,1.0,0.0
datadog,Then again I guess hosting this script right on the same server I'm monitoring would be a terrible idea...,0.162,0.0,0.838,-0.4767
datadog,"Basically I don't know how to ""receive the webhook"" so I can report it back to my Datadog service agent so it gets updated on their website.",0.0,0.0,1.0,0.0
datadog,I get how to actually report the data to Datadog as explained here  http://docs.datadoghq.com/api/  but I just don't have a clue  how to host a listener for web hooks ?,0.0,0.0,1.0,0.0
datadog,"Programming language isn't important, I don't have a preference for that case.",0.15,0.0,0.85,-0.1511
datadog,"I've been using Copperegg for a while now and have generally been happy with it until lately, where I have had a few issues.",0.0,0.156,0.844,0.5719
datadog,It's being used to monitor a number of EC2 instances that must be up 24/7.,0.0,0.091,0.909,0.0772
datadog,"Last week I was getting phantom alerts that servers had gone down when they hadn't, which I can cope with, but also I didn't get an alert when I should have done.",0.08,0.0,0.92,-0.3252
datadog,One server had high CPU for over 5 mins when the alert should be triggered after 1 minute.,0.0,0.128,0.872,0.296
datadog,"The Copperegg support weren't not all that helpful, merely agreeing that an alert should have been triggered.",0.103,0.323,0.574,0.6083
datadog,The latter of those problems is unacceptable and if it were to happen again outside of working hours then serious problems will follow.,0.338,0.0,0.662,-0.8271
datadog,"So, I'm looking for alternative services that will do that same job.",0.0,0.0,1.0,0.0
datadog,"I've looked at Datadog and New Relic, but both have a significant problem in that they will only alert me of a problem 5 minutes after it's occurred, rather than the 1 minute I can get with Copperegg.",0.173,0.122,0.706,-0.4767
datadog,What else is out there that can do the same job and will also integrate with Pager Duty?,0.0,0.0,1.0,0.0
datadog,I am migrating an application to use the latest version of Spring Boot.,0.0,0.0,1.0,0.0
datadog,Currently all the camel routes are in XML and I have it running using this approach.,0.0,0.0,1.0,0.0
datadog,All the routes currently log a message at the end of processing.,0.0,0.0,1.0,0.0
datadog,I was wondering is there a way to detect how long it took a particular route to execute and add to the log message.,0.0,0.0,1.0,0.0
datadog,"With this information, we can then create datadog dashboards to show stats on our camel routes",0.0,0.123,0.877,0.2732
datadog,"Thanks in advance
Damien",0.0,0.492,0.508,0.4404
datadog,I'm running datadog agent container in EC2 by configuring task definition in AWS ECS.,0.0,0.0,1.0,0.0
datadog,"But at this time, the huge amount of logs is stored in  /var/lib/docker/containers/ ContainerID / ContainerID .json so that I want to rotate it.",0.0,0.194,0.806,0.5883
datadog,"In Docker documents, I saw this  link .",0.0,0.0,1.0,0.0
datadog,There are,0.0,0.0,1.0,0.0
datadog,Now I want to config these options through task definition but I don't know the convention of them.,0.0,0.071,0.929,0.0387
datadog,Did anyone have any ideas?,0.0,0.0,1.0,0.0
datadog,I'm currently leveraging celery for periodic tasks.,0.0,0.0,1.0,0.0
datadog,I am new to celery.,0.0,0.0,1.0,0.0
datadog,I have two workers running two different queues.,0.0,0.0,1.0,0.0
datadog,One for slow background jobs and one for jobs user's queue up in the application.,0.0,0.0,1.0,0.0
datadog,I am monitoring my tasks on datadog because it's an easy way to confirm my workers a running appropriately.,0.0,0.153,0.847,0.4404
datadog,"What I want to do is after each task completes, record which queue the task was completed on.",0.0,0.075,0.925,0.0772
datadog,"The following function is something that I implemented after researching the celery docs and some StackOverflow posts, but it's not working as intended.",0.0,0.0,1.0,0.0
datadog,I get the first statsd increment but the remaining code does not execute.,0.0,0.0,1.0,0.0
datadog,"I am wondering if there is a simpler way to inspect inside/after each task completes, what queue processed the task.",0.0,0.0,1.0,0.0
datadog,I'm using micrometer to publish several different metrics to Datadog; among these there is the number of items processed by several different batch jobs which can be pretty sparse.,0.0,0.143,0.857,0.5423
datadog,"Some batch jobs have different intervals, some others are triggered by external events so they don't have a fixed interval at all.",0.0,0.0,1.0,0.0
datadog,"However, my micrometer configuration has a  step  of 30s, which I don't want to change, as suggested in  this question , because I have denser metrics that need to be tracked every 30s.",0.042,0.0,0.958,-0.0572
datadog,"This means that every 30s, unless a batch job has just run, my application is publishing a zero for each batch job metric, polluting them.",0.0,0.0,1.0,0.0
datadog,"I've been trying to handle this on Datadog by using its  rollup  function, but i don't know the rollup interval beforehand because the interval may be variable for jobs triggered by external events.",0.0,0.0,1.0,0.0
datadog,"Another solution i'm considering is to extend the  DataDogMeterRegistry  to have it clear all metrics right after publishing, so that it will only publish metrics that registered in the latest 30s interval, but I was wondering if there was a clearer way to prevent micrometer from sending zeroes for sparse metrics.",0.0,0.117,0.883,0.4497
datadog,I have a logrus log handler in my Golang application.,0.0,0.0,1.0,0.0
datadog,"Logs are formatted with JSONFormatter and are submitted as a single line to Datadog, which aggregates them and displays them nicely.",0.0,0.132,0.868,0.4404
datadog,"However, I recently discovered a case where there's an unhandled panic, and this is  not  captured with the logrus logger.",0.163,0.0,0.837,-0.5106
datadog,"This results in the actual panic and stack trace being spread across multiple output lines, which Datadog collects individually.",0.155,0.0,0.845,-0.5106
datadog,This costs us money and makes the logs very difficult to read.,0.202,0.0,0.798,-0.4201
datadog,"I'm going to fix the issue, but in the event that any further unhandled panics happen, I'd like to be able to capture them using the logrus JSONFormatter.",0.116,0.098,0.785,-0.1531
datadog,Something like this:,0.0,0.556,0.444,0.3612
datadog,This produces the following output.,0.0,0.0,1.0,0.0
datadog,"As you can see, the first two logs use logrus, but the unhandled panic does not.",0.229,0.0,0.771,-0.6652
datadog,Is it possible to get those last several lines to log using logrus?,0.0,0.0,1.0,0.0
datadog,I've been working on adding monitoring metrics in our GraphQL gateway recently.,0.0,0.0,1.0,0.0
datadog,We're using  graphql-spring-boot  starter for the gateway.,0.0,0.0,1.0,0.0
datadog,"After reading the following documentations, I manage to send the basic graphql.timer.query.",0.0,0.0,1.0,0.0
datadog,* metrics to Datadog,0.0,0.0,1.0,0.0
datadog,"What I've achieved so far is, when I send a GraphQL query/mutation, I'd collect the request count and time accordingly.",0.0,0.0,1.0,0.0
datadog,e.g.,0.0,0.0,1.0,0.0
datadog,sending the query below,0.0,0.0,1.0,0.0
datadog,I'll see metrics  graphql.timer.query.count  /  graphql.timer.query.sum  with tags  operationName=HelloWorldQuery,0.0,0.0,1.0,0.0
datadog,"It works like perfectly, until I want to test a query with errors.",0.138,0.46,0.402,0.6808
datadog,I realise there is no metrics/tags related to a failed query.,0.44,0.0,0.56,-0.6705
datadog,"For example, if I the above query returns null data and some GraphQL errors, I'd still collect  graphql.timer.query.count (operationName=HelloWorldQuery) , but there's no additional tags for me to tell there is an error for that query.",0.206,0.0,0.794,-0.7935
datadog,"In the gateway, I have implemented a custom  GraphQLErrorHandler , so I was thinking maybe I should add error counter (via MeterRegistry) in that class, but I am unable to get the  operationName  simply from GraphQLError type.",0.058,0.0,0.942,-0.2144
datadog,the best I can get is error.getPath() which gives the method name (e.g.,0.0,0.276,0.724,0.6369
datadog,greeting ) rather than the custom query name ( HelloWorldQuery  - to be consistent with what  graphql.timer.query.,0.0,0.167,0.833,0.3818
datadog,*  provides).,0.0,0.0,1.0,0.0
datadog,"My question is, how to solve the above problem?",0.235,0.157,0.609,-0.2263
datadog,And generally what is the best way of collecting GraphQL query metrics (including errors)?,0.0,0.244,0.756,0.6369
datadog,-------------------  Update  -------------------,0.0,0.0,1.0,0.0
datadog,"2019-12-31 
I read a bit more about GraphQL Instrumentation  here  and checked the  MetricsInstrumentation  implementation in graphql-spring-boot repo, the I have an idea of extending the MetricsInstrumentation class by adding error metrics there.",0.085,0.0,0.915,-0.4019
datadog,"2020-01-02 
I tried to ingest my CustomMetricsInstrumentation class, but with no luck.",0.177,0.253,0.57,0.296
datadog,"There is internal AutoConfiguration wiring, which I cannot insert my auto configuration in the middle.",0.0,0.0,1.0,0.0
datadog,"I am using Datadog integration with elasticsearch to monitor the ES clusters, one important metric which it shows on its dashboard is the no of active and waiting for search threads.",0.065,0.134,0.801,0.3182
datadog,"Referring to  this  ES docs, I understand that search threads work on a request queue in ES which is of the fixed size of 1000.",0.0,0.0,1.0,0.0
datadog,"I am seeing a lot of waiting for threads as shown in the image, but there is no rejected queue exception explained  here .",0.276,0.0,0.724,-0.8047
datadog,So it means ES is not rejecting the requests but still search threads are not able to execute the request fast enough hence ended up in waiting status for a long time.,0.0,0.055,0.945,0.1877
datadog,Questions,0.0,0.0,1.0,0.0
datadog,"I know its a board question, hence let me know if any additional information is required.",0.0,0.0,1.0,0.0
datadog,I deployed the Datadog agent using the  Datadog Helm chart  which deploys a  Daemonset  in Kubernetes.,0.0,0.0,1.0,0.0
datadog,However when checking the state of the Daemonset I saw it was not creating all pods:,0.119,0.0,0.881,-0.2235
datadog,When describing the  Daemonset  to figure out what was going wrong I saw it did not have enough resources:,0.154,0.0,0.846,-0.4767
datadog,"However, I have the  Cluster-autoscaler  installed in the cluster and configured properly (It does trigger on regular  Pod  deployments that do not have enough resources to schedule), but it does not seem to trigger on the  Daemonset :",0.0,0.0,1.0,0.0
datadog,The AutoScalingGroup has enough nodes left:,0.0,0.0,1.0,0.0
datadog,Did I miss something in the configuration of the Cluster-autoscaler?,0.167,0.0,0.833,-0.1531
datadog,What can I do to make sure it triggers on  Daemonset  resources as well?,0.0,0.286,0.714,0.5267
datadog,"Edit:
Describe of the Daemonset",0.0,0.0,1.0,0.0
datadog,Example json:,0.0,0.0,1.0,0.0
datadog,"I'd like to return a list of services that have the ""build"" key, and not the ""image"" key.",0.0,0.135,0.865,0.3612
datadog,Note that the value for the build key isn't something I can key off of.,0.0,0.156,0.844,0.34
datadog,"Output should be: [""web"", ""datadog""]",0.0,0.0,1.0,0.0
datadog,In my implementation script I have a line which logs a metric:,0.0,0.0,1.0,0.0
datadog,"From my test script, I assert that statsd.increment() is called by mocking out the datadog module:",0.162,0.0,0.838,-0.4019
datadog,This works fine and passes.,0.0,0.31,0.69,0.2023
datadog,"But as soon as I add ANOTHER script which calls  some_function()  without mocking datadog, that script runs beforehand and loads the real datadog module into the cache.",0.0,0.104,0.896,0.438
datadog,"The above test then fails because  some_function()  is no longer using the mock datadog, it uses the real (cached) datadog.",0.315,0.0,0.685,-0.7783
datadog,How can I address this?,0.0,0.0,1.0,0.0
datadog,Is it possible to remove the module from the cache?,0.0,0.0,1.0,0.0
datadog,"We need centralized logging in order to monitor, store, manage, and visualize logs for our infrastructure.",0.0,0.0,1.0,0.0
datadog,"The logging solution must be able to capture messages from projects written in different languages such as Java, Angular, Scala, and Python.",0.0,0.099,0.901,0.3182
datadog,"Implementing a custom-built solution would lead to additional tasks, costs, and dependencies associated with managing and maintaining its components.",0.0,0.119,0.881,0.3182
datadog,"So instead, we are thinking about using AWS Partner Network (APN) offerings.",0.0,0.0,1.0,0.0
datadog,"What would be the best managed solution out of Splunk, Sumo Logic, Datadog, Elastic and Loggly?",0.0,0.317,0.683,0.7579
datadog,I'm currently trying to run a playbook that uses a callback plugin.,0.0,0.0,1.0,0.0
datadog,This plugin uses a module called datadog:,0.0,0.0,1.0,0.0
datadog,"When I try to run the playbook, I get an error message:",0.231,0.0,0.769,-0.4019
datadog,ImportError: No module named datadog,0.355,0.0,0.645,-0.296
datadog,"I order to work around this, I created a virtualenv, activated it and installed the datadog module:",0.0,0.148,0.852,0.3071
datadog,"Then when I launch python and import the module, everything is fine:",0.0,0.153,0.847,0.2023
datadog,Python 2.7.15,0.0,0.0,1.0,0.0
datadog,"Therefore, I believe that the module is used properly.",0.0,0.0,1.0,0.0
datadog,"However, when launching the ansible playbook, the error remains:",0.252,0.0,0.748,-0.4019
datadog,"From what I can tell, Ansible is not taking the virtualenv into consideration.",0.0,0.0,1.0,0.0
datadog,I would expect Ansible to use the path of the virtualenv in the  ansible python module location .,0.0,0.0,1.0,0.0
datadog,How can I make ansible use the virtualenv?,0.0,0.0,1.0,0.0
datadog,I didn't find anything related to the python path in the ansible documentation:  https://docs.ansible.com/ansible/2.5/reference_appendices/config.html,0.0,0.0,1.0,0.0
datadog,Note: the issue occurs on the machine running Ansible.,0.0,0.0,1.0,0.0
datadog,Not the machine being provisioned.,0.0,0.0,1.0,0.0
datadog,Im new to kibana but am hoping to migrate away from Datadog.,0.0,0.252,0.748,0.5719
datadog,In DD I can create 'visualizations' that are specific to a single data source (host in this case) and combine several into one dashboard.,0.0,0.091,0.909,0.2732
datadog,EG: view CPU load for n hosts on one page,0.0,0.0,1.0,0.0
datadog,Am not seeing (yet) how to accomplish this sort of thing via Kibana.,0.0,0.189,0.811,0.4215
datadog,Suggestions on where to look?,0.0,0.0,1.0,0.0
datadog,Using v6.x Kibana/logstash/elasticsearch/metricbeat,0.0,0.0,1.0,0.0
datadog,I have two MySQL server which are running on same group replication.,0.0,0.0,1.0,0.0
datadog,The setup had been done by below steps:,0.0,0.0,1.0,0.0
datadog,"But I found an other error: Every time I reload the fallback MySQL (by restart services) it will auto join to group but stuck at RECOVERING forever, I waited for 3 days but it still in RECOVERING.",0.156,0.072,0.772,-0.5023
datadog,"I checked on the log and don't see any error on both server, everything look good except the fallback is running as readonly and stay at RECOVERING.",0.0,0.177,0.823,0.6319
datadog,What step did I missed?,0.423,0.0,0.577,-0.296
datadog,My group configuration is (I followed the instruction from DigitalOcean help  page at  https://www.digitalocean.com/community/tutorials/how-to-configure-mysql-group-replication-on-ubuntu-16-04 ):,0.151,0.146,0.703,-0.0258
datadog,"sync_binlog                    = 1 binlog_format                  =
ROW",0.0,0.0,1.0,0.0
datadog,"loose-group_replication_start_on_boot = ON
loose-group_replication_ssl_mode = REQUIRED
loose-group_replication_recovery_use_ssl = 1",0.0,0.0,1.0,0.0
datadog,"&quot;9dc4ae01-6664-437a-83f8-80546d58e025&quot;
loose-group_replication_ip_whitelist =
&quot;172.AAA.BBB.166,138.AAA.BBB.199&quot; loose-group_replication_group_seeds
= &quot;172.AAA.BBB.166:33061,138.AAA.BBB.199:33061&quot;",0.0,0.0,1.0,0.0
datadog,loose-group_replication_enforce_update_everywhere_checks = ON,0.0,0.0,1.0,0.0
datadog,&quot;138.AAA.BBB.199:33061&quot;,0.0,0.0,1.0,0.0
datadog,Below is MySQL log on first server:,0.0,0.0,1.0,0.0
datadog,2018-06-08T06:10:12.167400Z 0 [Warning] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: 'Members removed from the group: 138.AAA.BBB.199:3306',0.0,0.0,1.0,0.0
datadog,2018-06-08T06:10:12.167475Z 0 [Note] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: 'Group membership changed to 172.AAA.BBB.166:3306 on view,0.0,0.0,1.0,0.0
datadog,15271181169364149:11.',0.0,0.0,1.0,0.0
datadog,2018-06-08T06:11:59.032666Z 0 [Note] Plugin,0.0,0.0,1.0,0.0
datadog,group_replication reported: 'Members joined the group:,0.0,0.0,1.0,0.0
datadog,138.AAA.BBB.199:3306' 2018-06-08T06:11:59.032722Z 0 [Note] Plugin group_replication reported: 'Group membership changed to,0.0,0.0,1.0,0.0
datadog,"172.AAA.BBB.166:3306, 138.AAA.BBB.199:3306 on view 15271181169364149:12.'",0.0,0.0,1.0,0.0
datadog,Below is the MySQL log on fallback server:,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:57.490896Z 0 [Warning] option 'max_allowed_packet':,0.0,0.0,1.0,0.0
datadog,unsigned value 3221225472 adjusted to 1073741824,0.0,0.324,0.676,0.34
datadog,2018-06-11T09:22:57.490942Z 0 [Warning] The use of InnoDB is mandatory,0.0,0.157,0.843,0.0772
datadog,since MySQL 5.7.,0.0,0.0,1.0,0.0
datadog,The former options like '--innodb=0/1/OFF/ON' or,0.0,0.333,0.667,0.3612
datadog,'--skip-innodb' are ignored.,0.535,0.0,0.465,-0.3182
datadog,2018-06-11T09:22:57.491057Z 0 [Warning],0.0,0.0,1.0,0.0
datadog,The syntax '--log_warnings/-W' is deprecated and will be removed in a,0.0,0.0,1.0,0.0
datadog,future release.,0.0,0.0,1.0,0.0
datadog,Please use '--log_error_verbosity' instead.,0.0,0.434,0.566,0.3182
datadog,2018-06-11T09:22:57.491098Z 0 [Warning] TIMESTAMP with implicit,0.0,0.0,1.0,0.0
datadog,DEFAULT value is deprecated.,0.0,0.444,0.556,0.34
datadog,Please use,0.0,0.697,0.303,0.3182
datadog,--explicit_defaults_for_timestamp server option (see documentation for more details).,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:57.492972Z 0 [Note] /usr/sbin/mysqld,0.0,0.0,1.0,0.0
datadog,(mysqld 5.7.22-log) starting as process 31633 ...,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:57.500063Z 0 [Warning] InnoDB: Using,0.0,0.0,1.0,0.0
datadog,innodb_locks_unsafe_for_binlog is DEPRECATED.,0.0,0.0,1.0,0.0
datadog,This option may be,0.0,0.0,1.0,0.0
datadog,removed in future releases.,0.0,0.0,1.0,0.0
datadog,Please use READ COMMITTED transaction,0.0,0.631,0.369,0.6289
datadog,isolation level instead; Please refer to,0.3,0.256,0.444,-0.1027
datadog,http://dev.mysql.com/doc/refman/5.7/en/set-transaction.html,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:57.500175Z 0 [Note] InnoDB: PUNCH HOLE support,0.0,0.351,0.649,0.4019
datadog,available 2018-06-11T09:22:57.500191Z 0 [Note] InnoDB: Mutexes and,0.0,0.0,1.0,0.0
datadog,rw_locks use GCC atomic builtins 2018-06-11T09:22:57.500200Z 0 [Note],0.0,0.0,1.0,0.0
datadog,InnoDB: Uses event mutexes 2018-06-11T09:22:57.500205Z 0 [Note],0.0,0.0,1.0,0.0
datadog,InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier,0.158,0.0,0.842,-0.128
datadog,2018-06-11T09:22:57.500209Z 0 [Note] InnoDB: Compressed tables use,0.0,0.0,1.0,0.0
datadog,zlib 1.2.3 2018-06-11T09:22:57.500213Z 0 [Note] InnoDB: Using Linux,0.0,0.0,1.0,0.0
datadog,native AIO 2018-06-11T09:22:57.500430Z 0 [Note] InnoDB: Number of,0.0,0.178,0.822,0.0772
datadog,pools: 1 2018-06-11T09:22:57.500575Z 0 [Note] InnoDB: Using CPU crc32,0.0,0.0,1.0,0.0
datadog,instructions 2018-06-11T09:22:57.501015Z 0 [ERROR] InnoDB: Failed to,0.398,0.0,0.602,-0.5106
datadog,"create check sector file, errno:13 Please confirm O_DIRECT is",0.0,0.386,0.614,0.5267
datadog,supported and remove the file /data/check_sector_size if it exists.,0.0,0.223,0.777,0.3182
datadog,"2018-06-11T09:22:57.502305Z 0 [Note] InnoDB: Initializing buffer pool,",0.0,0.0,1.0,0.0
datadog,"total size = 4G, instances = 8, chunk size = 128M",0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:57.799065Z 0 [Note] InnoDB: Completed initialization,0.0,0.0,1.0,0.0
datadog,of buffer pool 2018-06-11T09:22:57.857325Z 0 [Note] InnoDB: If the,0.0,0.0,1.0,0.0
datadog,"mysqld execution user is authorized, page cleaner thread priority can",0.0,0.159,0.841,0.1779
datadog,be changed.,0.0,0.0,1.0,0.0
datadog,See the man page of setpriority().,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:57.870317Z 0 [Note] InnoDB: Highest supported file,0.0,0.315,0.685,0.3182
datadog,format is Barracuda.,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.081570Z 0 [Note] InnoDB:,0.0,0.0,1.0,0.0
datadog,Creating shared tablespace for temporary tables,0.0,0.535,0.465,0.5574
datadog,2018-06-11T09:22:58.081656Z 0 [Note] InnoDB: Setting file,0.0,0.0,1.0,0.0
datadog,'/data/databases/ibtmp1' size to 12 MB.,0.0,0.0,1.0,0.0
datadog,Physically writing the file,0.0,0.0,1.0,0.0
datadog,full; Please wait ... 2018-06-11T09:22:58.116190Z 0 [Note] InnoDB:,0.0,0.277,0.723,0.3182
datadog,File '/data/databases/ibtmp1' size is now 12 MB.,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.117279Z 0 [Note] InnoDB: 96 redo rollback,0.0,0.0,1.0,0.0
datadog,segment(s) found.,0.0,0.0,1.0,0.0
datadog,96 redo rollback segment(s) are active.,0.0,0.351,0.649,0.4019
datadog,2018-06-11T09:22:58.117293Z 0 [Note] InnoDB: 32 non-redo rollback,0.0,0.0,1.0,0.0
datadog,segment(s) are active.,0.0,0.574,0.426,0.4019
datadog,2018-06-11T09:22:58.117670Z 0 [Note] InnoDB:,0.0,0.0,1.0,0.0
datadog,Waiting for purge to start 2018-06-11T09:22:58.168094Z 0 [Note],0.0,0.0,1.0,0.0
datadog,InnoDB: 5.7.22 started; log sequence number 51745666191,0.0,0.178,0.822,0.0772
datadog,2018-06-11T09:22:58.168309Z 0 [Note] InnoDB: Loading buffer pool(s),0.0,0.0,1.0,0.0
datadog,from /data/databases/ib_buffer_pool 2018-06-11T09:22:58.168558Z 0,0.0,0.0,1.0,0.0
datadog,[Note] Plugin 'FEDERATED' is disabled.,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.183268Z 0,0.0,0.0,1.0,0.0
datadog,[Warning] CA certificate /etc/mysql/mysql-ssl/ca-cert.pem is self,0.0,0.0,1.0,0.0
datadog,signed.,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.184615Z 0 [Note] Server hostname,0.0,0.0,1.0,0.0
datadog,(bind-address): '138.AAA.BBB.199'; port: 3306,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.184636Z 0 [Note]   - '138.AAA.BBB.199' resolves to,0.0,0.298,0.702,0.1779
datadog,'138.AAA.BBB.199'; 2018-06-11T09:22:58.184668Z 0 [Note] Server socket,0.0,0.0,1.0,0.0
datadog,created on IP: '138.AAA.BBB.199'.,0.0,0.4,0.6,0.25
datadog,2018-06-11T09:22:58.186203Z 0,0.0,0.0,1.0,0.0
datadog,[Warning] 'user' entry 'mysql.session@localhost' ignored in,0.315,0.0,0.685,-0.3182
datadog,--skip-name-resolve mode.,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.186220Z 0 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve,0.247,0.0,0.753,-0.3182
datadog,mode.,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.186238Z 0 [Warning] 'user' entry,0.0,0.0,1.0,0.0
datadog,'phpmadsys@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.186260Z 0 [Warning] 'user' entry,0.0,0.0,1.0,0.0
datadog,'phpmyadmin@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.186308Z 0 [Warning] 'db' entry 'performance_schema,0.0,0.0,1.0,0.0
datadog,mysql.session@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.186313Z 0 [Warning] 'db' entry 'sys,0.0,0.0,1.0,0.0
datadog,mysql.sys@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.186318Z 0 [Warning] 'db' entry 'phpmyadmin,0.0,0.0,1.0,0.0
datadog,phpmadsys@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.186322Z 0 [Warning] 'db' entry 'performance_schema,0.0,0.0,1.0,0.0
datadog,datadog@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.186327Z 0 [Warning] 'db' entry 'phpmyadmin,0.0,0.0,1.0,0.0
datadog,phpmyadmin@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.186340Z 0 [Warning] 'proxies_priv' entry '@,0.0,0.0,1.0,0.0
datadog,root@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.188628Z 0 [Warning] 'tables_priv' entry 'user,0.0,0.0,1.0,0.0
datadog,mysql.session@localhost' ignored in --skip-name-resolve mode.,0.365,0.0,0.635,-0.3182
datadog,2018-06-11T09:22:58.188649Z 0 [Warning] 'tables_priv' entry,0.0,0.0,1.0,0.0
datadog,'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.,0.315,0.0,0.685,-0.3182
datadog,2018-06-11T09:22:58.192624Z 0 [Warning] Neither --relay-log nor,0.0,0.0,1.0,0.0
datadog,--relay-log-index were used; so replication may break when this MySQL server acts as a slave and has his hostname changed!!,0.0,0.0,1.0,0.0
datadog,Please use,0.0,0.697,0.303,0.3182
datadog,'--relay-log=dvm02-relay-bin' to avoid this problem.,0.64,0.0,0.36,-0.6514
datadog,2018-06-11T09:22:58.206545Z 0 [Note] Event Scheduler: Loaded 0 events,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.206745Z 0 [Note] /usr/sbin/mysqld: ready for,0.0,0.385,0.615,0.3612
datadog,connections.,0.0,0.0,1.0,0.0
datadog,Version: '5.7.22-log'  socket:,0.0,0.0,1.0,0.0
datadog,'/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server,0.0,0.0,1.0,0.0
datadog,(GPL) 2018-06-11T09:22:58.207175Z 2 [Note] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: 'Group communication SSL configuration:,0.0,0.0,1.0,0.0
datadog,group_replication_ssl_mode: &quot;REQUIRED&quot;; server_key_file:,0.0,0.0,1.0,0.0
datadog,&quot;/etc/mysql/mysql-ssl/server-key.pem&quot;; server_cert_file:,0.0,0.0,1.0,0.0
datadog,&quot;/etc/mysql/mysql-ssl/server-cert.pem&quot;; client_key_file:,0.0,0.0,1.0,0.0
datadog,&quot;/etc/mysql/mysql-ssl/server-key.pem&quot;; client_cert_file:,0.0,0.0,1.0,0.0
datadog,&quot;/etc/mysql/mysql-ssl/server-cert.pem&quot;; ca_file:,0.0,0.0,1.0,0.0
datadog,&quot;/etc/mysql/mysql-ssl/ca-cert.pem&quot;; ca_path: &quot;&quot;; cipher: &quot;&quot;;,0.0,0.0,1.0,0.0
datadog,"tls_version: &quot;TLSv1,TLSv1.1&quot;; crl_file: &quot;&quot;; crl_path: &quot;&quot;'",0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.207378Z 2 [Warning] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: '[GCS] Automatically adding IPv4 localhost address to the,0.0,0.0,1.0,0.0
datadog,whitelist.,0.0,0.0,1.0,0.0
datadog,It is mandatory that it is added.',0.0,0.178,0.822,0.0772
datadog,2018-06-11T09:22:58.207820Z 2 [Note] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: 'Initialized group communication with configuration:,0.0,0.0,1.0,0.0
datadog,group_replication_group_name: &quot;9dc4ae01-6664-437a-83f8-80546d58e025&quot;;,0.0,0.0,1.0,0.0
datadog,group_replication_local_address: &quot;138.AAA.BBB.199:33061&quot;;,0.0,0.0,1.0,0.0
datadog,group_replication_group_seeds:,0.0,0.0,1.0,0.0
datadog,"&quot;172.AAA.BBB.166:33061,138.AAA.BBB.199:33061&quot;;",0.0,0.0,1.0,0.0
datadog,group_replication_bootstrap_group: false;,0.0,0.0,1.0,0.0
datadog,group_replication_poll_spin_loops: 0;,0.0,0.0,1.0,0.0
datadog,group_replication_compression_threshold: 1000000;,0.0,0.0,1.0,0.0
datadog,"group_replication_ip_whitelist: &quot;172.AAA.BBB.166,138.AAA.BBB.199&quot;'",0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.207853Z 2 [Note] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: '[GCS] Configured number of attempts to join: 0',0.0,0.333,0.667,0.3612
datadog,2018-06-11T09:22:58.207859Z 2 [Note] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: '[GCS] Configured time between attempts to join: 5 seconds',0.0,0.216,0.784,0.296
datadog,2018-06-11T09:22:58.207878Z 2 [Note] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: 'Member configuration: member_id: 2; member_uuid:,0.0,0.0,1.0,0.0
datadog,&quot;822868f9-52a0-11e8-aa0e-1e45f9551f27&quot;; single-primary mode: &quot;false&quot;;,0.0,0.0,1.0,0.0
datadog,group_replication_auto_increment_increment: 7; ',0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.209024Z 3 [Note] 'CHANGE MASTER TO FOR CHANNEL,0.0,0.0,1.0,0.0
datadog,'group_replication_applier' executed'.,0.0,0.0,1.0,0.0
datadog,Previous state,0.0,0.0,1.0,0.0
datadog,"master_host='', master_port= 0, master_log_file='',",0.0,0.0,1.0,0.0
datadog,"master_log_pos= 4, master_bind=''.",0.0,0.0,1.0,0.0
datadog,"New state master_host='',",0.0,0.0,1.0,0.0
datadog,"master_port= 0, master_log_file='', master_log_pos= 4, master_bind=''.",0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.216904Z 6 [Note] Slave SQL thread for channel,0.0,0.0,1.0,0.0
datadog,"'group_replication_applier' initialized, starting replication in log",0.0,0.0,1.0,0.0
datadog,"'FIRST' at position 0, relay log",0.0,0.0,1.0,0.0
datadog,'./dvm02-relay-bin-group_replication_applier.000071' position: 4,0.0,0.0,1.0,0.0
datadog,2018-06-11T09:22:58.216931Z 2 [Note] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: 'Group Replication applier module successfully initialized!',0.0,0.368,0.632,0.5411
datadog,2018-06-11T09:22:58.241357Z 0 [Note] Plugin group_replication,0.0,0.0,1.0,0.0
datadog,reported: 'XCom protocol version: 3' 2018-06-11T09:22:58.241397Z 0,0.0,0.0,1.0,0.0
datadog,[Note] Plugin group_replication reported: 'XCom initialized and ready,0.0,0.263,0.737,0.3612
datadog,to accept incoming connections on port 33061',0.0,0.302,0.698,0.3818
datadog,2018-06-11T09:22:59.213826Z 0 [Note] InnoDB: Buffer pool(s) load,0.0,0.0,1.0,0.0
datadog,completed at 180611 11:22:59 2018-06-11T09:23:00.316791Z 0 [Note],0.0,0.0,1.0,0.0
datadog,Plugin group_replication reported: 'Group membership changed to,0.0,0.0,1.0,0.0
datadog,"172.AAA.BBB.166:3306, 138.AAA.BBB.199:3306 on view 15271181169364149:16.'",0.0,0.0,1.0,0.0
datadog,What is the best way/tool to monitor an EBS volume available space when mounted inside a Docker container?,0.0,0.208,0.792,0.6369
datadog,I really need to monitor the available disk space in order to prevent crash because of no  space left on device .,0.213,0.048,0.739,-0.5859
datadog,"Do you know of any tool that can monitor that, like datadog, newrelic grafana, prometheus or something opensource?",0.0,0.128,0.872,0.3612
datadog,"My scenario is that currently, I'm running my application as Daemon sets and want to integrate Datadog into my infrastructure.",0.0,0.064,0.936,0.0772
datadog,As my understanding is that Daemon sets purpose is to make sure one pod of each set is ran on each node.,0.0,0.099,0.901,0.3182
datadog,Here I wanted to point my application at datadog agent so it will feed data into it.,0.0,0.0,1.0,0.0
datadog,I've defined a  Service  of  Nodeport  type to expose the port of the agent.,0.118,0.0,0.882,-0.1531
datadog,I provided the service name in my application definition and it works.,0.0,0.0,1.0,0.0
datadog,For one node.,0.0,0.0,1.0,0.0
datadog,What happens now when I will have more nodes?,0.0,0.0,1.0,0.0
datadog,Will k8s be clever enough to route to the agent on the same nodes or there is a situation where a pod with my application might call the agent on a different node?,0.0,0.094,0.906,0.4588
datadog,Is this a correct setup?,0.0,0.0,1.0,0.0
datadog,"I want to monitor Undertow (mainly IO and worker threads) using statsd, I had in place tomcat and a lot of metrics were exported automatically to jmx but it seems not the case for Undertow.",0.0,0.036,0.964,0.0387
datadog,"I can see XNIO on JConsole for the application(Spring-boot app), but no metrics appear when I try to see them anywhere else.",0.128,0.0,0.872,-0.4215
datadog,"(Graphite, Datadog)",0.0,0.0,1.0,0.0
datadog,I would like to use Datadog to monitor the queue length of some background jobs.,0.0,0.161,0.839,0.3612
datadog,"Basically  I need to know the name of the key that represents a queue in Sidekiq , so that I can monitor it as described here:
 https://docs.datadoghq.com/integrations/redisdb/",0.0,0.0,1.0,0.0
datadog,I've read  that the Sidekiq keys have the form  sidekiq:queue:myqueuename .,0.0,0.0,1.0,0.0
datadog,However I have tried to execute  KEYS *myqueuename*  and I can't find anything.,0.0,0.0,1.0,0.0
datadog,I have also tried to search  KEYS *sidekiq*  but I don't get anything.,0.0,0.0,1.0,0.0
datadog,If I search  KEYS *queue*  I get the key  queues  which represents a set with the names of the queues (e.g.,0.0,0.0,1.0,0.0
datadog,"deliveries, default, low).",0.0,0.0,1.0,0.0
datadog,However those are only the names: I need the actual queues.,0.0,0.0,1.0,0.0
datadog,What is the key of a queue?,0.0,0.0,1.0,0.0
datadog,Is there a way to name containers when they start through ECS?,0.0,0.0,1.0,0.0
datadog,Wondering because I'm currently using Datadog to monitor the system usage and the containers are named something long etc like,0.0,0.116,0.884,0.3612
datadog,ecs-datadog-agent-task-1-datadog-agent-c0a1f3e8d9e58dd5e901,0.0,0.0,1.0,0.0
datadog,would like to set my own name,0.0,0.294,0.706,0.3612
datadog,"I want to monitor quota level usage for openstack projects and I need to be able to monitor  current and max levels for networks, ports and routers  (from Python code).",0.0,0.046,0.954,0.0772
datadog,Please note that I am talking about project-level access so the user performing the monitoring is not an open-stack admin.,0.0,0.113,0.887,0.3182
datadog,I was able to successfully read current level and max level for nova metrics (compute) but for those related to neutron (networking) it seems that API and command line returns only max-limits and not current levels.,0.0,0.058,0.942,0.2732
datadog,It very easy to test it yourself:,0.0,0.347,0.653,0.4927
datadog,I should mention that the web interface (horizon) does already report correctly the Floating IPs.,0.0,0.0,1.0,0.0
datadog,It seems that it does not display any gauges for networks.,0.0,0.0,1.0,0.0
datadog,The solution should work with openstack  kilo (7) releases or newer.,0.0,0.187,0.813,0.3182
datadog,I am very new to writing code.,0.0,0.0,1.0,0.0
datadog,I've been looking at every way I can find of finding a string in a text document and then returning part of the string on the following line.,0.0,0.0,1.0,0.0
datadog,Ideally with the end goal of putting this extracted string into an excel file but I'm no where near that step yet.,0.109,0.152,0.739,0.0258
datadog,I've been playing around with a lot of different options and I can not for the life of me get it to work.,0.0,0.083,0.917,0.2023
datadog,I feel like I'm close and it's killing me because I just can't figure out where I'm going wrong here.,0.3,0.1,0.6,-0.7184
datadog,Goal: to extract the name of the person who posted the job from the text below without knowing the person's name.,0.0,0.0,1.0,0.0
datadog,"I know the string ""Job posted by"" will immediately preseed the name I'm looking for and I know "" · "" will immediately follow the name.",0.0,0.0,1.0,0.0
datadog,no where else in the text document do either of these surround strings appear.,0.145,0.0,0.855,-0.296
datadog,my attempts at this so far are the following (my issue is that it appears to simply return the entire text document as opposed to just the name I'm looking for),0.0,0.0,1.0,0.0
datadog,any and all help and ideas is enormously appreciated.,0.0,0.473,0.527,0.7425
datadog,"sample text in the file:
 9/2/2016 Application Security Engineer Job at Datadog in Greater New York City Area | LinkedIn
    60
 Home Profile
Job description
My Network Jobs
 Search for people, jobs, companies, and more...",0.0,0.133,0.867,0.5994
datadog,"Interests
 Advanced
 
Business Services

Go to Lynda.c
  Application Security Engineer
Datadog
Greater New York City Area
    Posted 15 days ago 93 views
1 alum works here
   Apply on company website
  We’re on a mission to bring sanity to cloud operations and we need you to build resilient and secure applications on our platform.",0.0,0.197,0.803,0.8519
datadog,"What you will do
Perform code and design reviews, contribute code that improves security throughout Datadog's products Educate your fellow engineers about security in code and infrastructure
Monitor production applications for anomalous activity
Prioritize and track application security issues across the company
    Help improve our security policies and processes
Job posted by
Ryan Elberg · 2nd
Head of Tech Talent Acquisition at Datadog Greater New York City Area
Send Inmail",0.0,0.28,0.72,0.9652
datadog,"It's a pretty simple question, really.",0.0,0.444,0.556,0.4939
datadog,"I want to report the number of running instances to datadog, along with a bunch of my other stats.",0.0,0.148,0.852,0.1531
datadog,"There's an irony to the fact that I search Google Web Search for how to do something in Google App Engine and get the crappiest possible result, every time: The Google App Engine documentation pages.",0.035,0.0,0.965,-0.0516
datadog,There are some not labeled corpus.,0.0,0.0,1.0,0.0
datadog,"I extracted from it triples (OBJECT, RELATION, OBJECT).",0.0,0.0,1.0,0.0
datadog,For relation extraction I use Stanford OpenIE.,0.0,0.0,1.0,0.0
datadog,But I need only some of this triples.,0.0,0.0,1.0,0.0
datadog,"For example, I need relation "" funded "".",0.0,0.0,1.0,0.0
datadog,"Text :
 Every startup needs a steady diet of funding to keep it strong and growing.",0.0,0.294,0.706,0.6124
datadog,"Datadog, a monitoring service that helps customers bring together data from across a variety of infrastructure and software is no exception.",0.101,0.119,0.78,0.1027
datadog,Today it announced a massive $94.5 million Series D Round.,0.0,0.0,1.0,0.0
datadog,The company would not discuss valuation.,0.0,0.0,1.0,0.0
datadog,"From this text i want to extract relation  (Datadog, announced, $94.5 million Round)",0.0,0.106,0.894,0.0772
datadog,I have only one idea:,0.0,0.0,1.0,0.0
datadog,May be there are better approach?,0.0,0.367,0.633,0.4404
datadog,May be I need labeled corpus(i haven't it)?,0.0,0.0,1.0,0.0
datadog,"I want to write an DataDog Check to monitor some process like Puma, delayed_job etc, I can see there are ready plugins available for these for nagios and Sensu but not for DataDog, But can I write my own check/plugin for this services in datadog ?",0.0,0.107,0.893,0.3919
datadog,or can I use existing Nagios/sensu plugins with DataDog ?,0.0,0.0,1.0,0.0
datadog,If yes How should I proceed ?,0.0,0.403,0.597,0.4019
datadog,I want to integrate server monitoring system like DataDog that implements APM standards.,0.0,0.275,0.725,0.4215
datadog,"I want to achieve this without using the custom server like Express or Koa, just using the out-of-box NextJS server.",0.0,0.183,0.817,0.4215
datadog,"My NextJS server is only being used for pages, not APIs.",0.0,0.0,1.0,0.0
datadog,It would help if NextJS allows us to insert middleware for all requests.,0.0,0.184,0.816,0.4019
datadog,However it doesn't look like that's an option.,0.232,0.0,0.768,-0.2755
datadog,Do you know a good way to do this?,0.0,0.293,0.707,0.4404
datadog,I set up datadog trace client in my kubernetes cluster to monitor my deployed application.,0.0,0.0,1.0,0.0
datadog,"It was working fine with the kubernetes version 1.15x but as soon as I upgraded the version to 1.16x, the service itself is not showing in the Datadog Dashboard.",0.0,0.049,0.951,0.1027
datadog,Currently using:,0.0,0.0,1.0,0.0
datadog,Kubernetes 1.16.9,0.0,0.0,1.0,0.0
datadog,Datadog 0.52.0,0.0,0.0,1.0,0.0
datadog,When checked for agent status.,0.0,0.0,1.0,0.0
datadog,It is giving following exception :,0.0,0.375,0.625,0.34
datadog,This looks like a version issue to me.,0.0,0.294,0.706,0.3612
datadog,If it is which Datadog version I need to use for monitoring?,0.0,0.0,1.0,0.0
datadog,Per  this spec  on github and these  helm instructions   I'm trying to upgrade our Helm installation of datadog using the following syntax:,0.0,0.0,1.0,0.0
datadog,"However I'm getting the error below regardless of any attempt at altering the syntax of the  prometheus_url  value (putting the url in quotes, escaping the quotes, etc):",0.089,0.119,0.792,-0.0258
datadog,"Error: UPGRADE FAILED: failed to create resource: ConfigMap in version ""v1"" cannot be handled as a ConfigMap: v1.ConfigMap.Data: ReadString: expects "" or n, but found {, error found in #10 byte of ...|er.yaml"":{""instances|..., bigger context ...|{""apiVersion"":""v1"",""data"":{""kube_scheduler.yaml"":{""instances"":[{""prometheus_url"":""\"" http://localhost| ...",0.236,0.036,0.727,-0.8184
datadog,If I add the  --dry-run --debug  flags I get the following yaml output:,0.0,0.0,1.0,0.0
datadog,The Yaml output appears to mesh with the integration as specified on this  github page .,0.0,0.0,1.0,0.0
datadog,I have a node.js app already deployed on Elastic Beanstalk.,0.0,0.0,1.0,0.0
datadog,The EC2 instance on which the node app is deployed is running Ubuntu 16.04.5 LTS.,0.0,0.192,0.808,0.516
datadog,I am trying to integrate Datadog APM with Elastic Beanstalk using datadog's config file in .ebextensions folder.,0.0,0.0,1.0,0.0
datadog,I am following the instructions given on their docs page( https://docs.datadoghq.com/integrations/amazon_elasticbeanstalk/#alternate-datadog-agent-configuration ),0.0,0.0,1.0,0.0
datadog,Even though I am following all the mentioned steps I keep getting the following error on AWS ELB.,0.153,0.0,0.847,-0.4019
datadog,My Datadog config file code:,0.0,0.0,1.0,0.0
datadog,"Even after replacing the  initctl  with  systemctl  in the start &amp; stop scripts, I am still getting the same error.",0.224,0.0,0.776,-0.5994
datadog,Can't understand where I'm going wrong.,0.383,0.0,0.617,-0.4767
datadog,Please Help!,0.0,1.0,0.0,0.6476
datadog,I'm just starting out using Apache Beam on Google Cloud Dataflow.,0.0,0.0,1.0,0.0
datadog,I have a project set up with a billing account.,0.0,0.0,1.0,0.0
datadog,"The only things I plan on using this project for are:
1. dataflow - for all data processing
2. pubsub - for exporting stackdriver logs to be consumed by Datadog",0.0,0.0,1.0,0.0
datadog,"Right now, as I write this, I am not currently running any dataflow jobs.",0.0,0.0,1.0,0.0
datadog,"Looking at the past month, I see ~$15 in dataflow costs and ~$18 in Stackdriver Monitor API costs.",0.0,0.0,1.0,0.0
datadog,It looks as though Stackdriver Monitor API is close to a fixed $1.46/day.,0.0,0.0,1.0,0.0
datadog,I'm curious how to mitigate this.,0.0,0.315,0.685,0.3182
datadog,I do not believe I want or need Stackdriver Monitoring.,0.149,0.0,0.851,-0.0572
datadog,Is it mandatory?,0.0,0.394,0.606,0.0772
datadog,"Further, while I feel I have nothing running, I see this over the past hour:",0.0,0.0,1.0,0.0
datadog,"So I suppose the questions are these:
1. what are these calls?",0.0,0.0,1.0,0.0
datadog,2. is it possible to disable Stackdriver Monitoring for dataflow or otherwise mitigate the cost?,0.0,0.0,1.0,0.0
datadog,I am currently exporting Actuator metrics for my Spring Boot Webflux project to DataDog with 10 seconds interval.,0.0,0.0,1.0,0.0
datadog,I would like to add another exporter for one of our internal system that is not in the list of supported backends.,0.0,0.202,0.798,0.5859
datadog,Looking at the implementation from  DataDogMeterRegistry  I came up with the following.,0.0,0.0,1.0,0.0
datadog,However this is not working since no logs are printed.,0.0,0.173,0.827,0.2235
datadog,My question is how can I add and implement another MeterRegistry for Spring Boot Micrometer?,0.0,0.0,1.0,0.0
datadog,We're running NodeJS application inside docker container hosted on Amazon EC2 instance.,0.0,0.134,0.866,0.1779
datadog,To,0.0,0.0,1.0,0.0
datadog,To enable Monitoring for Node.js app with Datadog we are using datadog-metrics library and integrate it with our application.,0.0,0.0,1.0,0.0
datadog,We basically require to save the below Javascript code into a file called example_app.js,0.0,0.211,0.789,0.4939
datadog,"
 
 var metrics = require('datadog-metrics');
metrics.init({ **host: 'myhost', prefix: 'myapp.",0.0,0.0,1.0,0.0
datadog,"'** });

function collectMemoryStats() {
    var memUsage = process.memoryUsage();
    metrics.gauge('memory.rss', memUsage.rss);
    metrics.gauge('memory.heapTotal', memUsage.heapTotal);
    metrics.gauge('memory.heapUsed', memUsage.heapUsed);
    metrics.increment('memory.statsReported');
}

setInterval(collectMemoryStats, 5000);",0.0,0.0,1.0,0.0
datadog,"Although, we are able to successfully publish metrics to datadog but we're wondering if this can be automated.",0.0,0.11,0.89,0.2732
datadog,"We want build this into our docker image, hence require an automatic way to pick up the hostname, at the very least be able to use the docker hosts name if possible..Because till now we're manually specifying ""myhost"" and ""myapp"" values manually.",0.0,0.091,0.909,0.4588
datadog,Any better way to fetch the AWS instance hostname value into %myhost?,0.0,0.346,0.654,0.6486
datadog,We're implementing logging &amp; monitoring for a Vue/Node application which is using a REST Api.,0.0,0.0,1.0,0.0
datadog,"Oftentimes the API returns 4xx reponses (401s, 404s) which are currently caught by Axios and returned as &quot;Errors&quot;.",0.0,0.0,1.0,0.0
datadog,"These end up in our logging solutions (Datadog, Sentry) but dont bring much actionable points.",0.0,0.088,0.912,0.09
datadog,Should in general  status codes like these be considered Errors?,0.186,0.194,0.62,0.0258
datadog,Are there any best practices for SPA logging and monitoring?,0.0,0.318,0.682,0.6369
datadog,(couldn't find any resources),0.0,0.0,1.0,0.0
datadog,I have a use case wherein I want to publish my spring boot API metrics to Datadog &amp; CloudWatch simultaneously,0.0,0.075,0.925,0.0772
datadog,I have added the below dependencies to my pom,0.0,0.0,1.0,0.0
datadog,Main Application class,0.0,0.0,1.0,0.0
datadog,I have added all required properties in the  application.properties  as well.,0.0,0.189,0.811,0.2732
datadog,"I can see metrics are being published to both datadog &amp; CloudWatch with default metrics name  http.server.request 
But I want the metrics name for datadog to be different &amp; for this, I have added the below property as well",0.0,0.108,0.892,0.4767
datadog,management.metrics.web.server.requests-metric-name = i.want.to.be.different,0.0,0.0,1.0,0.0
datadog,But this is changing the name for both CloudWatch &amp; datadog,0.0,0.0,1.0,0.0
datadog,My question is how can I change the default metrics name for datadog only or keep names different for both,0.0,0.0,1.0,0.0
datadog,1.16 deprecation notice:,0.0,0.0,1.0,0.0
datadog,"I have about 10 helm charts that contain the old api versions - datadog, nginx-ingress and more.",0.0,0.0,1.0,0.0
datadog,I don't want to upgrade these different services.,0.169,0.0,0.831,-0.0572
datadog,are there any known work arounds?,0.0,0.0,1.0,0.0
datadog,I'm new to Kafka.,0.0,0.0,1.0,0.0
datadog,"During study to kafka, I think monitoring consumer's lag is needed.",0.211,0.0,0.789,-0.34
datadog,"When I search from google and docs, I found few ways.",0.0,0.0,1.0,0.0
datadog,I just trying to get less pipeline steps.,0.0,0.0,1.0,0.0
datadog,What should be the simple way to visualize consumer's lag?,0.211,0.0,0.789,-0.34
datadog,I want to see the remaining lag in near real-time from Kafka for a particular consumer group.,0.144,0.078,0.778,-0.2732
datadog,"The closest thing I've done is run the  describe  script from Kafka binaries, but it's slow and unreliable.",0.0,0.0,1.0,0.0
datadog,We are trying to programmatically do this to perform some conditional logic within our ETL pipeline.,0.0,0.0,1.0,0.0
datadog,My first thought is to garner metrics within the consumer and publish over statsD to new relic or datadog then poll over HTTP.,0.0,0.0,1.0,0.0
datadog,This is something I would do long-term.,0.0,0.0,1.0,0.0
datadog,"Is there a shorter-term, simpler approach to poll the consumer lag for a particular group?",0.167,0.0,0.833,-0.34
datadog,I’m looking to enable JMX to allow datadog to monitor our java JBoss wildfly systems but keep hitting runtime errors,0.137,0.064,0.798,-0.3919
datadog,I have set up the standalone.xml with,0.0,0.0,1.0,0.0
datadog,And,0.0,0.0,1.0,0.0
datadog,As well as,0.0,0.512,0.488,0.2732
datadog,Then in my startup.sh i have added,0.0,0.0,1.0,0.0
datadog,But this gives me,0.0,0.0,1.0,0.0
datadog,"java.lang.IllegalStateException: The LogManager was not properly
  installed (you must set the ""java.util.logging.manager"" system
  property to ""org.jboss.logmanage r.LogManager"")",0.0,0.0,1.0,0.0
datadog,This seems to be fairly common if I look at both here and on google but there seem to be different solutions depending on the version of wildfly.,0.0,0.073,0.927,0.2617
datadog,"I think I need to do something like
Set at the start of the standalone.conf",0.0,0.172,0.828,0.3612
datadog,And then,0.0,0.0,1.0,0.0
datadog,At the end.,0.0,0.0,1.0,0.0
datadog,"But I still get errors “Could not load Logmanager ""org.jboss.logmanager.LogManager""”",0.279,0.0,0.721,-0.4767
datadog,Any advice would be appreciated.,0.0,0.452,0.548,0.5106
datadog,I am deploying my flask application in AWS Elasticbeanstalk and I want to add a command for running datadog tracing when executing entry-point command.,0.0,0.061,0.939,0.0772
datadog,How can I do that?,0.0,0.0,1.0,0.0
datadog,This is the entry-point command to start my flask app in local machine:,0.0,0.0,1.0,0.0
datadog,This is how to add a command before that entry-point command (using datadog as example):,0.0,0.0,1.0,0.0
datadog,How to do the same in AWS elasticbeanstalk?,0.0,0.0,1.0,0.0
datadog,Seems like beanstalk is using apache + mod_wsgi to run python-flask application but I am not sure how to add a command before the entry-point command.,0.097,0.069,0.834,-0.1761
datadog,I've a Spring Boot REST application.,0.0,0.0,1.0,0.0
datadog,"I use many Spring libraries as: Spring Data REST, HATEOAS, Spring JPA, Hibernate, Redis, ElasticSearch...",0.0,0.0,1.0,0.0
datadog,I want to track metrics of my application and I did a research to find the best tool to do it.,0.0,0.256,0.744,0.6705
datadog,"After have given a try to Micrometer+DataDog, becuase I already use ElasticSearch I did try  APM Java Agent  and I found quite impressive the amount of data I get in Kibana Dashboard.",0.0,0.121,0.879,0.5563
datadog,I can see my endpoints and investigate where the time was spent (Mysql queries and others stuff).,0.0,0.0,1.0,0.0
datadog,"I didn't try yet Micrometer+ElasticSearch, but from the documentation it seems to collect less data out of the box.",0.0,0.0,1.0,0.0
datadog,I'd like to know your advice about and what you think is the best tool to collect metric for an application in production.,0.0,0.242,0.758,0.7717
datadog,Last month we had an outage caused by the AKS Scheduler going down.,0.0,0.0,1.0,0.0
datadog,Commands such as  kubectl  were still working but pods weren't starting.,0.0,0.0,1.0,0.0
datadog,"When we contacted AKS, they eventually ""restarted the API server"" which resolved this issue.",0.0,0.116,0.884,0.1779
datadog,It definitely makes me a little worried that we could lose something as critical as the scheduler and we have to call to ask Azure to fix it.,0.212,0.083,0.705,-0.4951
datadog,Azure has made the Control Plane opaque from within the cluster.,0.0,0.0,1.0,0.0
datadog,"The API server, scheduler, and controller are not even listed as objects.",0.0,0.0,1.0,0.0
datadog,"We are working on a simple healthcheck pod that would start up and send a ping to Datadog saying ""I'm alive"", however, I tend to think that Azure should be providing someway to monitor or view the health of these services.",0.0,0.0,1.0,0.0
datadog,Has anyone come up with a better method of monitoring these processes?,0.0,0.225,0.775,0.4404
datadog,The app has the following containers,0.0,0.0,1.0,0.0
datadog,In the dev process many feature branches are created to add new features.,0.0,0.143,0.857,0.25
datadog,such as,0.0,0.0,1.0,0.0
datadog,I have an AWS EC2 instance per feature branches running docker engine V.18 and docker compose to build the and run the docker stack that compose the php app.,0.0,0.0,1.0,0.0
datadog,To save operation costs 1 AWS EC2 instance can have 3 feature branches at the same time.,0.0,0.186,0.814,0.4939
datadog,I was thinking that there should be a custom docker-compose with special port mapping and docker image tag for each feature branch.,0.0,0.124,0.876,0.4019
datadog,The goal of this configuration is to be able to test 3 feature branches and access the app through different ports while saving money.,0.0,0.0,1.0,0.0
datadog,I also thought about using  docker networks  by keeping the same ports and using an nginx to redirect traffic to the different docker network ports.,0.0,0.0,1.0,0.0
datadog,What recommendations do you give?,0.0,0.0,1.0,0.0
datadog,We have an app that was initially created in 2008 on Rails 2.3.,0.0,0.143,0.857,0.25
datadog,"This app runs on top of JRuby version 1.7, in Ruby 1.8 compatibility mode.",0.0,0.122,0.878,0.2023
datadog,This is a largish app with 350 controllers and 450 models.,0.0,0.0,1.0,0.0
datadog,"We're (finally) upgrading the app to Rails 5.2, upgrading JRuby to latest version 9.2.8.0.",0.0,0.0,1.0,0.0
datadog,"We're now noticing a significant performance regression compared to the old app, running on the same server and database.",0.0,0.096,0.904,0.2023
datadog,Most requests are 30% to 200% slower than in the Rails 2.3 version.,0.0,0.0,1.0,0.0
datadog,The slowdown seems to be linear to the amount of data rendered.,0.0,0.0,1.0,0.0
datadog,I.e.,0.0,0.0,1.0,0.0
datadog,"when we render a large table of data, the request may take 3000ms vs 1000ms on the old version, even when there is no DB access happening in the view.",0.073,0.0,0.927,-0.296
datadog,"When rendering a request with little data, the slowdown is much less pronounced.",0.0,0.0,1.0,0.0
datadog,"For instance, Rails 2.3 will render a certain ""heavy"" page in 3 sec:",0.0,0.174,0.826,0.2732
datadog,"Completed in 2962ms (View: 1565, DB: 1384)",0.0,0.0,1.0,0.0
datadog,Compared to 7 sec for the new version:,0.0,0.0,1.0,0.0
datadog,Completed 200 OK in 7254ms (Views: 5808.5ms | ActiveRecord: 1389.3ms),0.0,0.268,0.732,0.4466
datadog,"The DB access time is very similar, but the ""view"" time is much much higher.",0.0,0.0,1.0,0.0
datadog,"We've tried running our app on MRI Ruby, and it's 3 times faster than on JRuby 9.",0.0,0.0,1.0,0.0
datadog,"Unfortunately, we make extensive use of Java libraries in our code, so we can't easily switch to MRI.",0.227,0.0,0.773,-0.5718
datadog,Some questions:,0.0,0.0,1.0,0.0
datadog,"Is JRuby still a viable option for Rails 5, is anyone using it with good performance?",0.0,0.172,0.828,0.4404
datadog,I couldn't find a recommendation which JDK to run JRuby on.,0.0,0.0,1.0,0.0
datadog,We're now running on OpenJDK 8.,0.0,0.0,1.0,0.0
datadog,I've briefly tried OpenJDK 13 but that seemed even slower.,0.0,0.0,1.0,0.0
datadog,Is there a recommended version?,0.0,0.375,0.625,0.2023
datadog,We're using these options:  JRUBY_OPTS=-J-Xmx4g -XX:ReservedCodeCacheSize=300M --server .,0.0,0.0,1.0,0.0
datadog,Any recommendations on other options that may improve performance?,0.0,0.266,0.734,0.4404
datadog,"We've used ""Datadog"" to produce flame-graphs of our requests, which confirms that most of the time is spent rendering views outside of data access, but we can't really tell what is costing time in the view.",0.0,0.0,1.0,0.0
datadog,"We've tried attaching JVisualVM to the app in sampling mode, but it's very hard to get useful metrics out of this.",0.082,0.155,0.763,0.4235
datadog,Is there a better way of profiling JRuby on Rails applications?,0.0,0.244,0.756,0.4404
datadog,I want to use mongodb driver.But I get the following error:,0.245,0.118,0.636,-0.34
datadog,"go.mongodb.org/mongo-driver/vendor/github.com/DataDog/zstd
  exec: ""gcc"": executable file not found in %PATH%",0.0,0.0,1.0,0.0
datadog,I am trying to trace .NET application using Datadog .NET Tracer.,0.0,0.0,1.0,0.0
datadog,"https://github.com/DataDog/dd-trace-dotnet/releases 
The application and the tracer are installed on Windows 2008R2SP1x64 Std with .NET Framework 4.6.1.",0.0,0.0,1.0,0.0
datadog,It fails to trace with the following warning messages:,0.426,0.0,0.574,-0.6369
datadog,".net tracer log:
""Failed to attach profiler: interface ICorProfilerInfo3 not found.""",0.248,0.0,0.752,-0.5106
datadog,"Windows Application Event log:
""NET Runtime version 2.0.50727.8800 - Failed to CoCreate profiler.""",0.231,0.0,0.769,-0.5106
datadog,The requirement for the .net tracer is .NET CLR 4.5 and above.,0.0,0.0,1.0,0.0
datadog,"My understanding of .NET is that CLR is a component of the framework, hence CLR version is same as framework version.",0.0,0.0,1.0,0.0
datadog,I'm trying to understand why .NET runtime version (2.0.50727.8800 according to Windows event log) is older than the framework (4.6.1 according to the Windows control panel).,0.0,0.0,1.0,0.0
datadog,I have a gauge in datadog and want to see the 'last' value.,0.0,0.291,0.709,0.4019
datadog,But when I select  the last day  in the dashboard time-picker I see a different value than when I select  the last week .,0.0,0.147,0.853,0.4767
datadog,My natural understanding is that 'last' gauge value should be the same regardless of how far back in time I go.,0.0,0.214,0.786,0.5994
datadog,Am I missing some understanding of how gauges or datadog works?,0.196,0.0,0.804,-0.296
datadog,I have a ResponseTimeMiddleware.cs responsible for getting response time metrics (I am using datadog) for every request made.,0.0,0.133,0.867,0.3182
datadog,Which is tagged by controller and action names.,0.0,0.0,1.0,0.0
datadog,"However when we hit the ""connect/token"" endpoint, the context.GetRouteData() is null, probably because identity server is doing it behind the scenes.",0.0,0.0,1.0,0.0
datadog,Is there a way I could get this information or some other unique information where I could tag with?,0.0,0.0,1.0,0.0
datadog,here's my code:,0.0,0.0,1.0,0.0
datadog,This is my Startup:,0.0,0.0,1.0,0.0
datadog,Is there a simple way it to report custom defined stats to our statsd / Datadog infrastructure from a Google Cloud Function written in Node.js?,0.0,0.0,1.0,0.0
datadog,"Since it's a high-traffic Javascript Cloud Function, I'd like to avoid heavy initialization of additional libraries every time the cloud function is invoked.",0.089,0.101,0.81,0.0772
datadog,"Also, by custom stats I mean stats of our own definition (not boilerplate summary statistics via StackDriver or DataDog GCP integration).",0.0,0.0,1.0,0.0
datadog,Bundler is causing a build fail for my Heroku app.,0.304,0.0,0.696,-0.5423
datadog,It is a ruby on rails app and was working perfectly and deployed on Heroku.,0.0,0.244,0.756,0.6369
datadog,I started getting  the following build error only after I installed the datadog agent.,0.197,0.0,0.803,-0.4019
datadog,"The build error seemed easy enough to fix, but it has grown difficult.",0.299,0.114,0.587,-0.4854
datadog,"After removing my Gemfile.lock and reinstalling to make sure bundler version was   2.0, I then started getting another build error.",0.123,0.105,0.773,-0.1027
datadog,This time it was that the bundler version my project requires is &lt; 2:,0.0,0.0,1.0,0.0
datadog,"I updated the latest version on my system, like it suggests.",0.0,0.217,0.783,0.3612
datadog,But then I run back into the first error that Bundler V2 is required for the Heroku build step.,0.173,0.0,0.827,-0.5499
datadog,This is my Gemfile.lock:,0.0,0.0,1.0,0.0
datadog,Here is my Gemfile if that helps,0.0,0.302,0.698,0.3818
datadog,I'm really at a loss an appreciate your help.,0.199,0.416,0.385,0.4256
datadog,ANY SUGGESTIONS APPRECIATED!,0.0,0.642,0.358,0.5562
datadog,I've recently made some updates to a Ruby on Rails Heroku app that was working fine.,0.0,0.114,0.886,0.2023
datadog,I tried updating the bundler version so that it could be configured with a datadog agent.,0.0,0.0,1.0,0.0
datadog,Now when I try to push to heroku master I get the following error:,0.197,0.0,0.803,-0.4019
datadog,"
 
 git push heroku master
Counting objects: 28, done.",0.0,0.0,1.0,0.0
datadog,Delta compression using up to 4 threads.,0.0,0.0,1.0,0.0
datadog,"Compressing objects: 100% (28/28), done.",0.0,0.0,1.0,0.0
datadog,"Writing objects: 100% (28/28), 3.33 KiB | 1.66 MiB/s, done.",0.0,0.0,1.0,0.0
datadog,"Total 28 (delta 19), reused 0 (delta 0)
remote: Compressing source files... done.",0.0,0.0,1.0,0.0
datadog,"remote: Building source:
remote: 
remote: -----&gt; ActiveStorage Preview app detected
remote: -----&gt; Installing binary dependencies for ActiveStorage Preview
remote:        Downloading packages..
remote:        Installing packages.....
remote: -----&gt; Ruby app detected
remote: 
remote:  !",0.0,0.0,1.0,0.0
datadog,remote:  !,0.0,0.0,1.0,0.0
datadog,"different prefix: """" and ""/tmp/build_2f915e77052b7fa5cef9531ffdd277e6""
remote:  !",0.0,0.0,1.0,0.0
datadog,"remote: /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/pathname.rb:522:in `relative_path_from': different prefix: """" and ""/tmp/build_2f915e77052b7fa5cef9531ffdd277e6"" (ArgumentError)
remote: 	from /tmp/d20190215-459-drze92/bundler-1.15.2/gems/bundler-1.15.2/lib/bundler/lockfile_parser.rb:98:in `rescue in initialize'
remote: 	from /tmp/d20190215-459-drze92/bundler-1.15.2/gems/bundler-1.15.2/lib/bundler/lockfile_parser.rb:61:in `initialize'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:130:in `new'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:130:in `block in parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:128:in `parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:112:in `lockfile_parser'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:74:in `specs'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:59:in `block in gem_version'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:58:in `gem_version'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/rails5.rb:9:in `block in use?'",0.0,0.0,1.0,0.0
datadog,"remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/base.rb:48:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/rails5.rb:8:in `use?'",0.0,0.0,1.0,0.0
datadog,"remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:17:in `block (2 levels) in detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `each'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `block in detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:13:in `detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/bin/support/ruby_compile:17:in `block in &lt;main&gt;'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:35:in `block in trace'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:35:in `trace'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/bin/support/ruby_compile:15:in `&lt;main&gt;'
remote:  !",0.0,0.0,1.0,0.0
datadog,"Push rejected, failed to compile Ruby app.",0.569,0.0,0.431,-0.765
datadog,"remote: 
remote:  !",0.0,0.0,1.0,0.0
datadog,"Push failed
remote: Verifying deploy...
remote: 
remote: !",0.374,0.0,0.626,-0.5562
datadog,Push rejected to codereader-backend.,0.524,0.0,0.476,-0.5106
datadog,"remote: 
To https://git.heroku.com/codereader-backend.git
 !",0.0,0.0,1.0,0.0
datadog,"[remote rejected] master -&gt; master (pre-receive hook declined)
error: failed to push some refs",0.333,0.0,0.667,-0.7184
datadog,Before this error I was getting an error about the my lockfile is unreadable.,0.329,0.0,0.671,-0.6597
datadog,I removed the lockfile and rebundled to no avail.That error is below.,0.353,0.0,0.647,-0.5994
datadog,"
 
 git push heroku master
Counting objects: 27, done.",0.0,0.0,1.0,0.0
datadog,Delta compression using up to 4 threads.,0.0,0.0,1.0,0.0
datadog,"Compressing objects: 100% (27/27), done.",0.0,0.0,1.0,0.0
datadog,"Writing objects: 100% (27/27), 3.24 KiB | 1.62 MiB/s, done.",0.0,0.0,1.0,0.0
datadog,"Total 27 (delta 18), reused 0 (delta 0)
remote: Compressing source files... done.",0.0,0.0,1.0,0.0
datadog,"remote: Building source:
remote: 
remote: -----&gt; ActiveStorage Preview app detected
remote: -----&gt; Installing binary dependencies for ActiveStorage Preview
remote:        Downloading packages..
remote:        Installing packages.....
remote: -----&gt; Ruby app detected
remote: 
remote:  !",0.0,0.0,1.0,0.0
datadog,remote:  !,0.0,0.0,1.0,0.0
datadog,Your lockfile is unreadable.,0.0,0.0,1.0,0.0
datadog,Run `rm Gemfile.lock` and then `bundle install` to generate a new lockfile.,0.0,0.0,1.0,0.0
datadog,remote:  !,0.0,0.0,1.0,0.0
datadog,remote: /tmp/d20190215-478-1u5qan1/bundler-2.0.1/gems/bundler-2.0.1/lib/bundler/lockfile_parser.rb:98:in `rescue in initialize': Your lockfile is unreadable.,0.0,0.0,1.0,0.0
datadog,Run `rm Gemfile.lock` and then `bundle install` to generate a new lockfile.,0.0,0.0,1.0,0.0
datadog,"(Bundler::LockfileError)
remote: 	from /tmp/d20190215-478-1u5qan1/bundler-2.0.1/gems/bundler-2.0.1/lib/bundler/lockfile_parser.rb:61:in `initialize'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:130:in `new'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:130:in `block in parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:128:in `parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:112:in `lockfile_parser'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:74:in `specs'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:59:in `block in gem_version'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:58:in `gem_version'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/rails5.rb:9:in `block in use?'",0.0,0.0,1.0,0.0
datadog,"remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/base.rb:48:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/rails5.rb:8:in `use?'",0.0,0.0,1.0,0.0
datadog,"remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:17:in `block (2 levels) in detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `each'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `block in detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:13:in `detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/bin/support/ruby_compile:17:in `block in &lt;main&gt;'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:35:in `block in trace'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:35:in `trace'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/bin/support/ruby_compile:15:in `&lt;main&gt;'
remote:  !",0.0,0.0,1.0,0.0
datadog,"Push rejected, failed to compile Ruby app.",0.569,0.0,0.431,-0.765
datadog,"remote: 
remote:  !",0.0,0.0,1.0,0.0
datadog,"Push failed
remote: Verifying deploy...
remote: 
remote: !",0.374,0.0,0.626,-0.5562
datadog,Push rejected to codereader-backend.,0.524,0.0,0.476,-0.5106
datadog,remote:,0.0,0.0,1.0,0.0
datadog,Any help is appreciated.I'm not sure what steps to take and there aren't any similar errors that I've found.,0.086,0.209,0.705,0.4164
datadog,Thank you.,0.0,0.714,0.286,0.3612
datadog,Is it possible to trigger alarm in zabbix or datadog based on output of bash command.,0.138,0.0,0.862,-0.34
datadog,I.e.,0.0,0.0,1.0,0.0
datadog,I have a bash command that returns value and I want to trigger an alarm if value rises above some level.,0.107,0.271,0.622,0.4019
datadog,I am investigating options for monitoring our installation in Swisscom's cloud-foundry.,0.0,0.0,1.0,0.0
datadog,My objectives are the following:,0.0,0.0,1.0,0.0
datadog,"So far, I understand the options are the following (including some BUTs):",0.0,0.0,1.0,0.0
datadog,"That is very useful for tracing / ad-hoc monitoring, but not very good for a serious infrastructure monitoring.",0.259,0.103,0.638,-0.4911
datadog,"This can be deployed as an app to (as far as I understand) do the job in similar way, as the TOP cf plugin.",0.0,0.103,0.897,0.368
datadog,"The problem is, that it requires registered client, so it can authenticate with the doppler endpoint.",0.153,0.0,0.847,-0.4019
datadog,"For some reason, the top-cf-plugin does that automatically / in another way.",0.0,0.0,1.0,0.0
datadog,That can be for example done with  Datadog .,0.0,0.0,1.0,0.0
datadog,But it seems to also require a dedicated uaa client to register the Nozzle.,0.0,0.25,0.75,0.6124
datadog,"I would like to check, if somebody is (was) on the similar road, has some findings.",0.0,0.152,0.848,0.3612
datadog,Eventually I would like to raise the following questions towards the swisscom community support:,0.0,0.321,0.679,0.6369
datadog,In a AWS ECS cluster each cluster instance runs the ecs-agent [1] as a docker container.,0.0,0.0,1.0,0.0
datadog,Next to that container I run datadog-agent [2] also as a container.,0.0,0.0,1.0,0.0
datadog,The datadog-agent monitors all other containers and ship their logs to DataDog.,0.0,0.0,1.0,0.0
datadog,In order to have the log of each container tagged by name I've added a specific docker label [3] to each container with the respective name.,0.0,0.104,0.896,0.4215
datadog,"However, I'm not been able to add a docker label to the ecs-agent itself.",0.0,0.0,1.0,0.0
datadog,Is there a way to add custom docker labels to the ecs-agent container?,0.0,0.0,1.0,0.0
datadog,[1] -  https://github.com/aws/amazon-ecs-agent,0.0,0.0,1.0,0.0
datadog,[2] -  https://github.com/DataDog/datadog-agent,0.0,0.0,1.0,0.0
datadog,[3] -  https://docs.datadoghq.com/logs/log_collection/docker/,0.0,0.0,1.0,0.0
datadog,"We are Moving from datadog to prometheus,
Datadog have google cloud metrics, for example:
gcp.gcp.instance.is_running 
gcp.gcp.project.quota.networks.limit
and more .. (all metrics starting with gcp)",0.0,0.0,1.0,0.0
datadog,"i want to have those google cloud metrics using prometheus - 
is there an exporter to get them?couldn't find anything .",0.0,0.071,0.929,0.0772
datadog,Is there any way to install exe/MSI agents in AWS EC2 instances in an automated way??,0.0,0.0,1.0,0.0
datadog,"In specific, I am looking for a counterpart of Azure's Custom Script Extension.",0.0,0.0,1.0,0.0
datadog,[Free of cost],0.0,0.0,1.0,0.0
datadog,"Scenario:
I want to install BigFix and Datadog agents on 1000 Ec2 instances, this is a one time job, so I am not looking for any solution that involves Chef / Puppet, etc.,",0.0,0.118,0.882,0.3818
datadog,I am looking for any API or any way to access Microsoft's Cloudyn service.,0.0,0.0,1.0,0.0
datadog,"I want to extract data - Azure Storage Cost and utilization per month, this is available under the ""Management Dashboard of Cloudyn"" and want to integrate with Datadog.",0.0,0.098,0.902,0.1531
datadog,I'm using Datadog and NewRelic to try and track down odd behavior that seems to happen at random times.,0.113,0.0,0.887,-0.3182
datadog,Recently I have noticed huge spikes in REDIS latency to my application in NewRelic.,0.0,0.161,0.839,0.3182
datadog,I added Datadog to the Redis server and saw these spikes of commands/second going from around ~0.5-2k to over 40-60k!,0.0,0.0,1.0,0.0
datadog,"Along with those is a spike in bandwidth and load, but only very mindor CPU changes.",0.0,0.0,1.0,0.0
datadog,"When these were occurring, GoogleAnalytics (GA) was actually showing a rather slow day in comparison.",0.0,0.0,1.0,0.0
datadog,"In fact, the overall app load today is about 2-3x higher than the day shown in the image below, but today has had perfect REDIS performance without any latency/commands spikes.",0.0,0.148,0.852,0.7227
datadog,Could it be bots/crawlers hitting stale caches that are causing large chunks of data to be inserted at once?,0.0,0.0,1.0,0.0
datadog,"My app heavily relies on an external API, which does occasionally spike in response time as well, but why would a slow API call cause slower redis calls or massive spikes in redis commands?",0.0,0.046,0.954,0.1406
datadog,Understand the options to secure the docker.sock.,0.0,0.286,0.714,0.34
datadog,"As in those articles, giving access to docker.sock is a risk.",0.168,0.192,0.64,0.0772
datadog,However there could be cases where we need to deploy a pod such which needs to talk to docker daemon via the socket for monitoring or controlling.,0.0,0.0,1.0,0.0
datadog,For example  datadog  which mounts the socket via hostPath mount.,0.0,0.0,1.0,0.0
datadog,OpenShift requires explicit grant of SCC e.g.,0.0,0.294,0.706,0.3612
datadog,"hostaccess to the service account which runs the pod for the pod to use hostPath, but it is OpenShift proprietary.",0.0,0.0,1.0,0.0
datadog,I suppose SELinux can be used so that any pods who access the docker socker are required to have a certain label.,0.0,0.1,0.9,0.2732
datadog,"I would like to know if my understanding of SELinux label is valid, and what other options would be available.",0.0,0.122,0.878,0.3612
datadog,volume-mounting the docker socket into a container is unsupported by Red Hat.,0.213,0.0,0.787,-0.4019
datadog,"This means that while it is entirely possible to do so (as with any other volume mount), Red Hat is unable to assist with configurations using this setup, problems that arise because of this setup or the security implications/concerns surrounding this setup.",0.06,0.053,0.887,-0.0772
datadog,I'm pushing gunicorn metrics from multiple applications into datadog from the same host however I cannot find a way to group the statsd metrics using either a tag or proc_name.,0.0,0.0,1.0,0.0
datadog,Datadog gunicorn integration,0.0,0.0,1.0,0.0
datadog,https://app.datadoghq.com/account/settings#integrations/gunicorn,0.0,0.0,1.0,0.0
datadog,Datadog agent checks are being updated automatically with the  app:proc_name  tag.,0.0,0.0,1.0,0.0
datadog,I can use this to group and select the data for a specific service.,0.0,0.0,1.0,0.0
datadog,https://github.com/DataDog/dd-agent/blob/5.2.x/checks.d/gunicorn.py#L53,0.0,0.0,1.0,0.0
datadog,"For the statsd metrics however, I do not see how to assign a tag or proc_name.",0.0,0.0,1.0,0.0
datadog,This is not being done automatically nor do I see a way to specify a tag.,0.0,0.0,1.0,0.0
datadog,https://github.com/benoitc/gunicorn/blob/19.6.0/gunicorn/instrument/statsd.py#L90,0.0,0.0,1.0,0.0
datadog,Datadog config:,0.0,0.0,1.0,0.0
datadog,Gunicorn config:,0.0,0.0,1.0,0.0
datadog,Any ideas on how this might be achieved?,0.0,0.0,1.0,0.0
datadog,Examples using notebooks:,0.0,0.0,1.0,0.0
datadog,"In this example, I am able to select  app:service  in either the 'from' or 'avg by' drop downs.",0.116,0.0,0.884,-0.2732
datadog,For the metrics with the my_namespace prefix I am unable to reference the same application name.,0.0,0.0,1.0,0.0
datadog,Only host and environment related tags are available.,0.0,0.0,1.0,0.0
datadog,I have a squid proxy container on my local Docker for Mac (datadog/squid image).,0.0,0.0,1.0,0.0
datadog,Essentially I use this proxy so that app containers on my local docker and the browser pod (Selenium) on another host use the same network for testing (so that the remote browser can access the app host).,0.0,0.0,1.0,0.0
datadog,"But with my current setup, when I run my tests the browser starts up on the remote host and then after a bit fails the test.",0.139,0.0,0.861,-0.5719
datadog,The message on the browser is  ERR_PROXY_CONNECTION_FAILED  right before it closes.,0.0,0.0,1.0,0.0
datadog,So I assume that there is an issue with my squid proxy config.,0.0,0.0,1.0,0.0
datadog,I use the default config and on the docker hub site it says,0.0,0.0,1.0,0.0
datadog,"Please note that the stock configuration available with the container is set for local access, you may need to tweak it if your network scenario is different.",0.0,0.081,0.919,0.3182
datadog,I'm not really sure how my network scenario is different.,0.195,0.0,0.805,-0.2912
datadog,What should I be looking into for more information?,0.0,0.0,1.0,0.0
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,I would like my pods in my minikube-vm to use the network that my docker-machine uses.,0.0,0.152,0.848,0.3612
datadog,"Inside the docker-machine I set up a proxy container(datadog/squid) for minikube to access, but I'm not sure exactly how to allow my minikube/pods to use this proxy.",0.088,0.085,0.828,-0.024
datadog,Should I be setting the env vars HTTP_PROXY or is there something else I need to do?,0.0,0.0,1.0,0.0
datadog,"Not sure what I should be looking into, any help would be appreciated, thanks!",0.097,0.456,0.447,0.8036
datadog,For me it is very strange that jmeter does not bring response time for restcall.,0.13,0.0,0.87,-0.2716
datadog,These are all possibilites to be saved on jtl/csv file:,0.0,0.237,0.763,0.4215
datadog,My question is either if response time is equal one of the data above or if I can manually calculate sum some values and get to it.,0.0,0.097,0.903,0.4019
datadog,PS: The reason while I don't simply use Jmeter Response Time graph is due to the fact that I send data to Datadog (measurement tool) instead.,0.0,0.0,1.0,0.0
datadog,I followed a Youtube video by Chris Pettus called PostgreSQL Proficiency for Python People to edit some of my postgres.conf settings.,0.0,0.0,1.0,0.0
datadog,"My server has 28 gigs of RAM and prior to making the changes, my system memory was averaging around 3GB.",0.0,0.0,1.0,0.0
datadog,Now it hovers around 10GB.,0.0,0.0,1.0,0.0
datadog,"I am not having any issues right now, but I would like to understand the pros and cons of the changes I made.",0.0,0.146,0.854,0.5023
datadog,I assume that there must be some tangible benefits of tripling the average memory being used in my system (measured with Datadog).,0.0,0.115,0.885,0.3818
datadog,My server is used to perform ETL (Airflow) and hosts the database.,0.0,0.0,1.0,0.0
datadog,"Airflow has a lot of connections but typically the files are pretty small (a few mb) which are processed with pandas, compared with the database to find new rows, and then loaded.",0.0,0.125,0.875,0.6486
datadog,I have several applications running on a Glassfish application server (4.0).,0.0,0.0,1.0,0.0
datadog,"I have recorded some statistics of the java memory usage with DataDog, so I am able to see the historic of the used heap memory along with the  initial  and  maximum  constant heap sizes.",0.0,0.0,1.0,0.0
datadog,"
 The image shows the initial (yellow), maximum (blue) and real (purple) heap values.",0.0,0.184,0.816,0.4019
datadog,"As you can see above, the real heap size is always bellow the initial heap value, so I'm planning to move these parameters to improve the server's performance, but I'm not sure if this is really necessary.",0.061,0.091,0.848,0.0534
datadog,"So, I have this doubts:",0.485,0.0,0.515,-0.4271
datadog,"I guess this questions hold true talking of a tomcat, JBoss or any servlet-oriented server.",0.0,0.189,0.811,0.4215
datadog,Any help will be gratefully received.,0.0,0.592,0.408,0.7003
datadog,"I recently watched the  OSCON Austin 2016 talk  on  ""Detecting outliers and anomalies in realtime at Datadog""  by  Homin Lee  and found proper motivation to ask the following question.",0.0,0.082,0.918,0.34
datadog,Basically what I am trying to do is find anomalies in graphs that do not necessarily start at the same time ( t ) but are quite similar (in family) in shape.,0.0,0.0,1.0,0.0
datadog,Separated:,0.0,0.0,1.0,0.0
datadog,Combined:,0.0,0.0,1.0,0.0
datadog,"As depicted in my (rough) concept drawing, given two similar frequency ( f ) functions of time, I want to line them up on top of each other on the basis of where each of their inflection points are.",0.0,0.086,0.914,0.2732
datadog,One of the frequency graphs starts at  t=-2  and the other  t=5 .,0.0,0.0,1.0,0.0
datadog,They both have inflection points around  t_1=8.5  and  t_2=1.5 .,0.0,0.0,1.0,0.0
datadog,This is where I want to line them up.,0.0,0.157,0.843,0.0772
datadog,"Essentially, the image drawn should be the final output from my algorithm and listing any anomalies that trigger like say for the green curve, if  f=0.2  at  t_1=12 , then that should be an anomaly because it is not in family.",0.0,0.06,0.94,0.3612
datadog,"And as Homin Lee says it, the graph would not be ""within the trained envelope.""",0.0,0.0,1.0,0.0
datadog,Now I want to lay out what my particular approach would be and see if you think the same or have a better approach to develop this algorithm.,0.0,0.149,0.851,0.4939
datadog,"Before we choose which anomaly detection algorithm to use, we need to discuss how to prepare this data.",0.0,0.0,1.0,0.0
datadog,We will continue to use frequency-versus-time data for example purposes.,0.0,0.0,1.0,0.0
datadog,"To prepare the data, we need to (1) find the inflection points, (2) scale the data so that the data all have the same time domain duration (i.e.,  12-5=7=7=5-(-2) ), and (3) find a way to match (line up) the times of each graph (i.e., 5 to -2, 6 to -1, and so on).",0.0,0.0,1.0,0.0
datadog,"Once the data is prepared, now it is onto the algorithm.",0.0,0.16,0.84,0.2263
datadog,"For (robust) anomaly detection, I was thinking about using  one-class / multi-class  Support Vector Machines (SVM) because we are going to be training a huge set of graphs to form the ""envelope.""",0.0,0.156,0.844,0.6124
datadog,This section is also open for suggestions.,0.0,0.0,1.0,0.0
datadog,"As a moonshot thought, I would like to eventually be able to put all of the graphs onto one huge plot and point out the anomalies from there.",0.0,0.167,0.833,0.5859
datadog,The problem is that there would be so many different time intervals.,0.197,0.0,0.803,-0.4019
datadog,"So one solution would be to create a singular universal ( u ) time interval so that you don't have to deal with the differing intervals (e.g.,  t_1=5,9  would become  t_u=1,5  and same goes for  t_2 ).",0.0,0.131,0.869,0.5688
datadog,"So to recap, I am looking to analyze similar graphs on different time intervals for anomalies.",0.0,0.0,1.0,0.0
datadog,"Find critical/key points (not necessarily inflection points), scale, plot graphs, and check for anomalies.",0.0,0.0,1.0,0.0
datadog,"I have rambled on for long enough but if something does not make sense and you would like me to clarify or elaborate, let me know and I will.",0.0,0.111,0.889,0.5023
datadog,"Feel free to make suggestions, submit to me some code, and/or any other ideas or approaches that I did not necessary think of before.",0.0,0.13,0.87,0.5106
datadog,Thank you.,0.0,0.714,0.286,0.3612
datadog,"P.S., sorry about the drawings; I tried my best.",0.113,0.365,0.522,0.5994
datadog,:P,0.0,1.0,0.0,0.34
datadog,I'm using Datadog for Ansible.,0.0,0.0,1.0,0.0
datadog,I have a role which installs the Datadog package but doesn't run the datadog role automatically after the package installation.,0.0,0.0,1.0,0.0
datadog,"Currently, we need in each project to call Datadog role manually.",0.0,0.0,1.0,0.0
datadog,"Is it possible to call Datadog role in my role1 instead of having to write ""datadog.datadog"" everywhere after role1.",0.0,0.0,1.0,0.0
datadog,"Precisely, can we execute a role after a task which is responsible to install this role ?",0.0,0.15,0.85,0.3182
datadog,Thank you in advance :),0.0,0.647,0.353,0.6705
datadog,In DockerCloud I am trying to get my container to speak with the other container.,0.0,0.0,1.0,0.0
datadog,I believe the problem is the hostname not resolving (this is set in  /conf.d/kafka.yaml  shown below).,0.273,0.0,0.727,-0.5972
datadog,"To get DockerCloud to have the two containers communicate, I have tried many variations including the full host-name  kafka-development-1  and  kafka-development-1.kafka , etc.",0.0,0.0,1.0,0.0
datadog,Within the container I run  ./etc/init.d/datadog-agent info  and receive:,0.0,0.0,1.0,0.0
datadog,SSH Into Docker Node:,0.0,0.0,1.0,0.0
datadog,"I log into the containers to see their values, this is the  datadog-agent :",0.0,0.197,0.803,0.4019
datadog,This is the  kafka container :,0.0,0.0,1.0,0.0
datadog,Datadog  conf.d/kafka.yaml :,0.0,0.0,1.0,0.0
datadog,Can anyone see what I am doing wrong?,0.341,0.0,0.659,-0.4767
datadog,"I am implementing a recommendation engine in .Net C#, I am using Cassandra to store the data.",0.0,0.0,1.0,0.0
datadog,"I am still new in using C*, just started using it 2 months ago.",0.0,0.0,1.0,0.0
datadog,"At the moment I have only 2 nodes in my cluster (single DC), deployed in Azure DS2 VM (each has 7Gb RAM, 2 Cores).",0.0,0.0,1.0,0.0
datadog,"I set  RF=2, CL=1  for both read and write.",0.0,0.0,1.0,0.0
datadog,I set the timeouts in yaml config file as below,0.0,0.0,1.0,0.0
datadog,I set lower read query timeout in client side (30 secs each).,0.18,0.0,0.82,-0.296
datadog,"The data stored in cassandra is user history, item counter, and recommended items data.",0.0,0.122,0.878,0.2023
datadog,"I created an API (stands in equinix DC) for my recommendation engine, its work is very simple, only reading all recommended_items Id from recommended_items table in C* everytime a user opens the website page.",0.0,0.061,0.939,0.25
datadog,It means that the query is very simple for each user :,0.0,0.0,1.0,0.0
datadog,"When I did load testing for up to 500 users/threads, it was fine and very fast.",0.0,0.114,0.886,0.2023
datadog,"But when the online site calls API to read from C* table, I got read timeouts very often.",0.0,0.0,1.0,0.0
datadog,There were usually only less than 20 users at the same time though.,0.0,0.0,1.0,0.0
datadog,"I monitor the cassandra nodes activity using DataDog and I found that only node #2 that keeps getting timeouts (the seed node is node #1, though what I understand is seed doesn't really matter except during bootstrapping step).",0.037,0.0,0.963,-0.0749
datadog,"However, everytime the timeout happens, I tried to query using cqlsh in both nodes, and node #1 is the one that return",0.0,0.0,1.0,0.0
datadog,OperationTimeOut Exception.,0.0,0.0,1.0,0.0
datadog,I have been trying to find the main root of this issue.,0.0,0.0,1.0,0.0
datadog,Does that have anything to do with coordinator node being down ( I read this article ) ?,0.0,0.0,1.0,0.0
datadog,Or is that because I have only 2 nodes?,0.0,0.0,1.0,0.0
datadog,"When the timeout happens (the webpage shows nothing), then I tried to refresh the page that calls the API, it will be loading for long time before showing nothing again (because of the timeout).",0.0,0.0,1.0,0.0
datadog,"But surprisingly, I will get the log that all those requests were actually successful after few minutes even though the web page has been closed.",0.0,0.267,0.733,0.8402
datadog,It's like the read request was still running even though the page has been closed.,0.0,0.152,0.848,0.3612
datadog,The exception are like these (they didn't happen together) :,0.0,0.238,0.762,0.3612
datadog,OR,0.0,0.0,1.0,0.0
datadog,Does anyone have any suggestion about my problem?,0.278,0.0,0.722,-0.4019
datadog,thank you.,0.0,0.714,0.286,0.3612
datadog,output of cfstats .recommended_items,0.0,0.0,1.0,0.0
datadog,NODE #1,0.0,0.0,1.0,0.0
datadog,NODE #2,0.0,0.0,1.0,0.0
datadog,"My name is Daniel, 
I'm a newcomer accountwise but a long time lurker.",0.0,0.0,1.0,0.0
datadog,"I decided to learn Apache Cassandra for my next ""lets write some code while the kids are sleeping"" project.",0.0,0.0,1.0,0.0
datadog,What i'm writing is a neat little api that will do read and writes against a cassandra database.,0.0,0.167,0.833,0.4588
datadog,"I had a lot of the db layout figured out in mongodb, but for me it's time to move on and grow as a engineer :)",0.0,0.154,0.846,0.6124
datadog,"Mission:
I will collect metrics from the servers in my rack, an agent will send a payload of metrics every minute.",0.0,0.0,1.0,0.0
datadog,"I have the api part pretty much figured out, will use JWT tokens signing the payloads.",0.0,0.186,0.814,0.4939
datadog,The type of data i will store can be seen below.,0.0,0.0,1.0,0.0
datadog,"cpuload, cpuusage, memusage, diskusage etc.",0.0,0.0,1.0,0.0
datadog,"The part where i am confused with cassandra is how to write the actual model, i understand the storagengines sort of writes it all as a time serie
on disk for me making reads quite amazing.",0.062,0.109,0.829,0.4201
datadog,"i know anything i would whip together now would work for my lab since it's jsut 30 machines, 
but i'm trying to understand how these things are done properly and how it could be done for a real life scenario like server density, datadog , ""insert your prefered server monitoring service"".",0.0,0.066,0.934,0.5023
datadog,:),0.0,1.0,0.0,0.4588
datadog,But how are you more experienced engineers designing a schema like this ?,0.0,0.245,0.755,0.5023
datadog,Usage scenarios for the database:,0.0,0.0,1.0,0.0
datadog,Read the assets associated with ones userid,0.0,0.221,0.779,0.1779
datadog,Generate monthly pdf reports showing uptime and such.,0.0,0.0,1.0,0.0
datadog,"Should i insert the rows containing the full payload or am i better of inserting them per service basis: timeuid|cpuusage 
Per service row",0.0,0.127,0.873,0.4404
datadog,All in one,0.0,0.0,1.0,0.0
datadog,"In mongo i would preallocate the buckets, and also keep a quick read avg inside of the document.",0.0,0.0,1.0,0.0
datadog,So in the webgui i could simply show the avg stats for pre-defined time periods.,0.0,0.0,1.0,0.0
datadog,Examples for dumbasses are highly appreciated.,0.0,0.418,0.582,0.5563
datadog,Hope you can decipher my rather poor english.,0.258,0.242,0.5,-0.0516
datadog,"Just found this url in the SO suggestions:
 Cassandra data model for time series 
i guess that is something that applies to me aswell.",0.0,0.0,1.0,0.0
datadog,"Sincerly
Daniel Olsson",0.0,0.0,1.0,0.0
datadog,I am using  ElasticBeanstalk with single docker container .,0.0,0.0,1.0,0.0
datadog,I am using DataDog(statsd client) for pushing metrics from the docker container.,0.0,0.0,1.0,0.0
datadog,I have a running datadog-agent which is technically a statsd client on the host machine.,0.0,0.0,1.0,0.0
datadog,The issue I am facing is to connect that client running at port 8125 from the container.,0.0,0.0,1.0,0.0
datadog,What I have tried is:,0.0,0.0,1.0,0.0
datadog,Thanks in Advance,0.0,0.592,0.408,0.4404
datadog,"We have been handling a rather peculiar issue with  clock offsets between VM's (Windows Server 2019) on Azure, that are hosted in the same region and datacenter, moreover in a VMSS .",0.0,0.054,0.946,0.1531
datadog,"Several facts regarding the issue, after doing some experiments in the last three months:",0.0,0.0,1.0,0.0
datadog,Would appreciate any ideas.,0.0,0.474,0.526,0.4019
datadog,I am using cloudwatch subscription filter which automatically sends logs to elasticsearch aws and then I use Kibana from there.,0.0,0.0,1.0,0.0
datadog,The issue is that everyday cloudwatch creates a new indice due to which I have to manually create the new index pattern each day in kibana.,0.0,0.16,0.84,0.4939
datadog,Accordingly I will have to create new monitors and alerts in kibana as well each day.,0.0,0.244,0.756,0.4939
datadog,I have to automate this somehow.,0.0,0.0,1.0,0.0
datadog,Also if there is better option with which I can go forward would be great.,0.0,0.368,0.632,0.7906
datadog,I know datadog is one good option.,0.0,0.367,0.633,0.4404
datadog,I have setup a 4 broker Kafka cluster on AWS MSK (version 2.2.1).,0.0,0.0,1.0,0.0
datadog,I am monitoring the same through datadog (crawler setup from  https://docs.datadoghq.com/integrations/amazon_msk/ ).,0.0,0.0,1.0,0.0
datadog,"Now as per my understanding, the fetch follower total time is the sum of the other metrics.",0.0,0.0,1.0,0.0
datadog,But you can see all of them &lt;1ms while fetch follower total time is over 200ms.,0.0,0.0,1.0,0.0
datadog,I have not really changed the default Kafka config much:,0.0,0.0,1.0,0.0
datadog,Can someone suggest a possible cause of this high follower fetch time and how to reduce it?,0.0,0.0,1.0,0.0
datadog,Any approches to identify the cause are also welcome.,0.0,0.273,0.727,0.4588
datadog,I'm sending UDP packets (statsd) from pods on a host to  &lt;hostIP&gt;:8125 .,0.0,0.0,1.0,0.0
datadog,"On the other end, a collector (datadog-agent using  hostPort ; one per host via DaemonSet) picks up the packets and does it's thing.",0.0,0.0,1.0,0.0
datadog,"Generally this works fine, but if I ever delete + re-create the collector ( kubectl delete pod datadog-agent-xxxx ; new pod is started on same IP/port a few seconds later), traffic from  existing  client-sockets stop arriving at the collector (UDP sockets created  after  the pod-rescheduling works fine).",0.061,0.085,0.853,0.0258
datadog,Re-starting just the agent inside the collector pod ( kubectl exec -it datadog-agent-xxxxx agent stop ; auto-restarts after ~30s) the same old traffic  does  show up.,0.087,0.0,0.913,-0.296
datadog,So containers somehow must have an impact.,0.0,0.0,1.0,0.0
datadog,"While UDP are (supposedly) stateless, something, somewhere is obviously keeping state around!?",0.0,0.0,1.0,0.0
datadog,Any ideas/pointers?,0.0,0.0,1.0,0.0
datadog,"Each ""client"" pod has something like this in the deployment/pod:",0.0,0.217,0.783,0.3612
datadog,On the collector (following  datadog's k8s docs ):,0.286,0.0,0.714,-0.4215
datadog,This happens on Kubernetes 1.12 on Google Kubernetes Engine.,0.0,0.0,1.0,0.0
datadog,"I am looking for JMX metric(s) for Kafka Broker [Not more than 1 or 2, if possible] which at a high level can identify the health of the cluster?",0.0,0.0,1.0,0.0
datadog,"I have referred to the list compiled by datadog and confluent, but couldn't find anything similar.",0.0,0.0,1.0,0.0
datadog,Absolutely love micrometer.,0.0,0.692,0.308,0.6697
datadog,Running a spring boot JAX RS app.,0.0,0.0,1.0,0.0
datadog,Am using datadog registry and wanted to time some service methods.,0.0,0.0,1.0,0.0
datadog,I saw all rest end points are timed and detail sent - thats great.,0.0,0.272,0.728,0.6249
datadog,However here is the issue I am facing.,0.0,0.0,1.0,0.0
datadog,"If I have a GET call that takes a path param, for example  /employees/{empId} , And If I have a million users in the system, then I guess we will have 1 million tag combinations pushed to datadog.",0.0,0.0,1.0,0.0
datadog,So question is how to get around this.,0.0,0.0,1.0,0.0
datadog,In this case I am only interested in all of the GET calls and not a specific call to get a specific employee.,0.0,0.124,0.876,0.4019
datadog,So how do I also tell micrometer/spring-boot to use the pattern rather than actual values.,0.0,0.172,0.828,0.4019
datadog,(I am not using @Timed on the JAX-RS resource classes.,0.0,0.0,1.0,0.0
datadog,Is that the way to go ?),0.0,0.0,1.0,0.0
datadog,"My function scrapes my servers for the command and outputs something along the lines of  offset=1.3682  which  metrics_emit  uses to send to our metrics collector/visualizer, datadog.",0.0,0.0,1.0,0.0
datadog,What I need to do is strip off the  offset=  part because  metrics_emit  only wants the numerical value.,0.0,0.13,0.87,0.34
datadog,What would be the best way of stripping  offset=  as well as calling  strip()  on  i  so that it gets rid of all newlines and trailing/leading whitespaces?,0.0,0.208,0.792,0.743
datadog,We have a requeriment to monitor certain counters on our production applications.,0.0,0.174,0.826,0.2732
datadog,Each application has it's own AppPool.,0.0,0.0,1.0,0.0
datadog,"We know how to attach and monitor the performance counters, but the problem we face i sthat IIS worker processes are recycled, and thus, the instance names for those performance counters, invariably change.",0.103,0.0,0.897,-0.5499
datadog,Our operations and sysadmins use external tools to monitor several counters and metrics.,0.0,0.0,1.0,0.0
datadog,"In order to include .NET CLR counters, we need to rely on the instanceId being stable, or else the monitoring needs to be setup again if the worker process dies or gets recycled.",0.0,0.064,0.936,0.296
datadog,Does anybody know a way around this limitation?,0.294,0.0,0.706,-0.3612
datadog,Maybe keeping the instanceIds stable (same PID!?),0.0,0.293,0.707,0.3595
datadog,"or by hooking these external tools (Datadog, WhatsUp, PerfMon, etc) to something that doesn't change?",0.0,0.0,1.0,0.0
datadog,"I am new to writing tests in java, and seem to be unable to test if a method of a class is called.",0.0,0.0,1.0,0.0
datadog,"I am sending metrics to datadog, and want to test in the code if a function of another class was called.",0.0,0.067,0.933,0.0772
datadog,"It says I need to mock first, but I couldn't get it to work.",0.147,0.0,0.853,-0.2263
datadog,MetricRecorder.java,0.0,0.0,1.0,0.0
datadog,MetricRecorderTest.java,0.0,0.0,1.0,0.0
datadog,"When I run the test I get this =  org.mockito.exceptions.misusing.NotAMockException: 
Argument passed to verify() is of type NonBlockingStatsDClient and is not a mock!",0.126,0.105,0.768,-0.1179
datadog,"Any idea of how I should be testing if recordHistogramValue was called, and if so with what arguments?",0.156,0.0,0.844,-0.4522
datadog,I'm trying to send data to datadog using kamon.,0.0,0.0,1.0,0.0
datadog,My setup is the following:,0.0,0.0,1.0,0.0
datadog,I'm getting the following exception at akka startup:,0.0,0.0,1.0,0.0
datadog,"I am using Ansible  Datadog role  and trying to install and configure datadog agents in target servers however, i am stuck at a point where i need to use host variables and update a section of the playbook using these variables.",0.054,0.0,0.946,-0.25
datadog,The variable has got multiple values separated by a space.,0.0,0.252,0.748,0.4019
datadog,I want to ensure that these values are added in the playbook based on the variable values.,0.0,0.437,0.563,0.8074
datadog,Following example will help in understanding the requirement.,0.0,0.278,0.722,0.4019
datadog,Playbook:,0.0,0.0,1.0,0.0
datadog,"Here, the tag value AID is using a host variable with the same name i.e., AID and in some cases this host variable can have values like the following:",0.0,0.233,0.767,0.765
datadog,AID: 100 101 102 103,0.0,0.0,1.0,0.0
datadog,Is there a way that the while executing tag section of the playbook is parsed based on the variable values in following format.,0.0,0.114,0.886,0.4019
datadog,I believe i cannot use templates for such requirements since the configurations are used under vars in the role.,0.0,0.0,1.0,0.0
datadog,Any suggests would be appreciated.,0.0,0.452,0.548,0.5106
datadog,I'm trying to set up a Datadog PostgreSQL integration that requires a user with  pg_monitor  role and  SELECT  permission on  pg_stat_database  as described on their own  documentation .,0.0,0.0,1.0,0.0
datadog,"My database is currently hosted on Heroku and it seems the default user doesn't have  SUPERUSER permissions because, when I try to apply the above role and permission to a &quot;monitor&quot; user I have the following error message:",0.074,0.0,0.926,-0.4019
datadog,ERROR:  must have admin option on role &quot;pg_monitor&quot;,0.329,0.0,0.671,-0.5319
datadog,So I'm looking for some way of:,0.0,0.0,1.0,0.0
datadog,Someone has ever faced this issue?,0.0,0.0,1.0,0.0
datadog,There is a way to handle this case?,0.0,0.0,1.0,0.0
datadog,Our infra for web application looks like this,0.0,0.263,0.737,0.3612
datadog,Nodejs Web application -&gt; GraphQL + Nodejs as middleware (BE for FE) -&gt; Lot's of BE services in ROR -&gt; DB/ES etc etc,0.0,0.0,1.0,0.0
datadog,We have witness the whole middleware layer of GrpahQL+Nodejs gets latent whenever any of the multiple crucial BE service gets latent and request queuing starts happening.,0.0,0.0,1.0,0.0
datadog,When we tried to compare it with number of requests during the period it got latent it was &lt;1k request which is much lower than the claimed 10k concurrent request handling of nodejs.,0.064,0.038,0.899,-0.2263
datadog,Looking for pointers to debug this issue further.,0.0,0.0,1.0,0.0
datadog,Analysis done so far from our end:,0.0,0.0,1.0,0.0
datadog,We have a Rails 4.2 app and are currently using a shared cache across several apps.,0.0,0.156,0.844,0.34
datadog,Our memcached miss rate is pretty high (like 85% hits and 15% misses) but this is complicated by the fact that multiple apps are sharing the same memcached instance.,0.039,0.175,0.785,0.6705
datadog,So we might be getting a high miss rate for a couple of critical cache processes (our DataDog data would support this).,0.165,0.114,0.72,-0.0516
datadog,Is it possible to specify a cache store on a fragement cache call like:,0.0,0.185,0.815,0.3612
datadog,I think this is possible with object caching by doing something like:,0.0,0.2,0.8,0.3612
datadog,Would there be other ways to untangle the hit / miss ratio of specific cache actions?,0.103,0.0,0.897,-0.1531
datadog,I set up a simple health check in nestjs using terminus.,0.0,0.0,1.0,0.0
datadog,"In Datadog APM, I see dozens of failed fs.stat() calls for various paths under /dist",0.202,0.0,0.798,-0.5106
datadog,But the health check seems to complete fine after those calls.,0.0,0.18,0.82,0.296
datadog,Any idea why this is happening?,0.0,0.0,1.0,0.0
datadog,"Let's say I have a daemonset running in k8s cluster, and the cluster has other deployments pods (e.g.",0.0,0.0,1.0,0.0
datadog,nginx webserver).,0.0,0.0,1.0,0.0
datadog,I would like the daemonset change/update config files in the deployment pods (e.g.,0.0,0.185,0.815,0.3612
datadog,change nginx worker_processes or worker_connections in /etc/nginx/nginx.conf of the nginx pods from the daemonset).,0.0,0.0,1.0,0.0
datadog,This process is part of the automated or continuous performance tuning workflow that I would like to implement.,0.0,0.135,0.865,0.3612
datadog,"In short, I'm looking for ways to have the daemonset change config files in other pods depending on performance metric continuously.",0.0,0.0,1.0,0.0
datadog,"I will have the daemonset continuously measure the performance or get performance metrics from another metric servers (Prometheus, new-relic, datadog, or others).",0.0,0.0,1.0,0.0
datadog,"Depending on the performance measures, I want to take action (tune the pods) from the daemonset.",0.0,0.085,0.915,0.0772
datadog,So the only problem I have from this workflow is how to access pods from the daemonset.,0.165,0.0,0.835,-0.4522
datadog,I appreciate any feedback or solutions on how to achieve this.,0.0,0.355,0.645,0.5267
datadog,C++ and CMake newbie question regarding how to integrate a third-party library into my own code.,0.0,0.0,1.0,0.0
datadog,I'm trying to add Datadog metrics to C++ application.,0.0,0.0,1.0,0.0
datadog,The  officially-endorsed library  doesn't state how it can be integrated.,0.0,0.0,1.0,0.0
datadog,I imagine it should tell me how to import it like this:,0.0,0.2,0.8,0.3612
datadog,This is my understanding of how to integrate a third-party library (don't you wish there is &quot;pip&quot; in C++?).,0.117,0.0,0.883,-0.3089
datadog,But the names in &lt;&gt; are not provided in the README.,0.0,0.0,1.0,0.0
datadog,I certainly don't have to do it like this as long as I can use CMake.,0.0,0.29,0.71,0.5994
datadog,Any help is appreciated!,0.0,0.759,0.241,0.7424
datadog,We have a consumer using librdkafka (via confluent-kafka-python) that we would like to monitor with DataDog.,0.0,0.152,0.848,0.3612
datadog,We are trying to get  records-consumed-rate  which is a recommended thing to watch.,0.0,0.141,0.859,0.2023
datadog,"The problem is, I can't seem to find it in the librdkafka stats object.",0.184,0.0,0.816,-0.4019
datadog,Is there a way to get this metric?,0.0,0.0,1.0,0.0
datadog,librdkafka stats docs,0.0,0.0,1.0,0.0
datadog,Confluent docs showing that this indeed a thing,0.0,0.0,1.0,0.0
datadog,Datadog recommendation to watch this,0.0,0.0,1.0,0.0
datadog,I have reverse proxy on running k8s.,0.0,0.0,1.0,0.0
datadog,I want to log request and response headers log.,0.0,0.157,0.843,0.0772
datadog,I did not manage to it.,0.0,0.0,1.0,0.0
datadog,Could anyone help me ?,0.0,0.474,0.526,0.4019
datadog,It could be console log.,0.0,0.0,1.0,0.0
datadog,I have Datadog agent on my Cluster.,0.0,0.0,1.0,0.0
datadog,So can can get request logs from NGINX.,0.0,0.0,1.0,0.0
datadog,I have recently been creating a POC using the new  DataDog/Azure DevOps Integration .,0.0,0.18,0.82,0.296
datadog,"The purpose of doing this is to aggregate all of my build/release logs, PR data, etc into DataDog to build insights, alerts, dashboards, etc.",0.0,0.0,1.0,0.0
datadog,"The DataDog charts are very nice, but I would prefer to use Azure Log Analytics as this is where most of my company's log and metric data is aggregated already and the ability to correlate it would be helpful.",0.0,0.199,0.801,0.827
datadog,"Note, I realize that Azure DevOps has Analytics charting and PowerBI integration, but I would like to use Log Analytics to store the metric and log data, if possible.",0.0,0.111,0.889,0.5023
datadog,The Azure DevOps ServiceHooks do not have Azure Log Analytics as an option (see image below).,0.0,0.0,1.0,0.0
datadog,Maybe the trick is to push it to Azure Service Bus and then push it to Log Analytics?,0.066,0.0,0.934,-0.0516
datadog,I have looked at the  Azure DevOps Reporting documentation  and I didn't see anything obvious.,0.0,0.0,1.0,0.0
datadog,"If anyone knows of any good blogs on pushing data from Azure DevOps to Log Analytics, I'd really appreciate it.",0.0,0.247,0.753,0.7089
datadog,Unfortunately most of my searches come back with advice on how to use Azure DevOps to provision monitoring of external applications with App Insights and Log Analytic rather than the other way around.,0.07,0.0,0.93,-0.34
datadog,"I can imagine using a scheduled task calling the Azure DevOps API and pushing it into Log Analytics API, but that seems like the least elegant and most error prone solution.",0.188,0.169,0.642,-0.184
datadog,Any thoughts on the best ways to monitor this data are appreciated.,0.0,0.429,0.571,0.8176
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,"I have a custom metrics which is pushed to the metrics endpoint, I use the metrics endpoint to push to datadog.",0.0,0.0,1.0,0.0
datadog,I wanted to know how to test DatadogMeterRegistry in spring boot application.,0.0,0.0,1.0,0.0
datadog,I found something in  GitHub  but it was dead-end,0.0,0.0,1.0,0.0
datadog,Scenario:,0.0,0.0,1.0,0.0
datadog,I have a remote server which is monitored (via DataDog) and sends out a warning when some anomaly is detected.,0.13,0.0,0.87,-0.34
datadog,This warning can be fetched via a webhook.,0.286,0.0,0.714,-0.34
datadog,"Now I want to connect that webhook ( https://docs.datadoghq.com/integrations/webhooks/ ) with MS Teams (probably via Bot), to receive a warning.",0.136,0.073,0.791,-0.2732
datadog,Then I want to send a command back to the remote server to resolve the warning.,0.139,0.225,0.636,0.128
datadog,Technology:,0.0,0.0,1.0,0.0
datadog,"MS Teams, Python flask/Django, remote server",0.0,0.0,1.0,0.0
datadog,Expected Results:,0.0,0.0,1.0,0.0
datadog,I can receive a warning from my remote server to MS Teams via a bot.,0.179,0.0,0.821,-0.34
datadog,Then send a command back to the remote server.,0.0,0.0,1.0,0.0
datadog,My initial plan is doing this using Python Flask/Django but not tied to a specific language.,0.0,0.0,1.0,0.0
datadog,Environment:,0.0,0.0,1.0,0.0
datadog,Remote server is a LINUX based system.,0.0,0.0,1.0,0.0
datadog,"we have a internal network that is used within our company, so might need to resolve a firewall problem potentially (idk whole lot about it tho).",0.099,0.095,0.806,-0.0258
datadog,Things I have tried:,0.0,0.0,1.0,0.0
datadog,"I just want to see if this is possible or not, so i havent coded up any.",0.0,0.085,0.915,0.0772
datadog,But I found some information relevant to our problem:,0.336,0.0,0.664,-0.5499
datadog,https://docs.datadoghq.com/integrations/webhooks/,0.0,0.0,1.0,0.0
datadog,https://docs.microsoft.com/en-us/microsoftteams/platform/bots/how-to/create-a-bot-for-teams,0.0,0.0,1.0,0.0
datadog,https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/what-are-webhooks-and-connectors,0.0,0.0,1.0,0.0
datadog,"One last note, I am not also tied down to Teams bot.",0.0,0.0,1.0,0.0
datadog,"Our company also uses Azure Devops, so that is another resource I can use to realize the solution.",0.0,0.126,0.874,0.3182
datadog,"Btw, this question was posted on  here  but was told to post on MSDN, but I couldn't find an appropriate forum.",0.0,0.0,1.0,0.0
datadog,"Hence, I am posting on stack overflow instead",0.0,0.0,1.0,0.0
datadog,"We recently started using SCDF on Kubernetes, and we are trying to workout the kinks.",0.0,0.0,1.0,0.0
datadog,"One of thing things that i was'nt able to find is whether there is a way to affect logging format,for ex.",0.0,0.0,1.0,0.0
datadog,switch to using Json format.,0.0,0.0,1.0,0.0
datadog,"Reason for this is simple, we are using Datadog as our logging platform, and with Json, you don't have to write custom log parsing rules.",0.0,0.0,1.0,0.0
datadog,"With regular log format, you will endup with something like this",0.0,0.2,0.8,0.3612
datadog,"For example in Node.js container I do:
 throw new Error('lol');  or  console.error('lol'); 
But when I open container logs:  docker-compose logs -f  nodejs 
there are no any statuses or colors like all logs have info status.",0.076,0.088,0.837,0.1154
datadog,I use Datadog to collect logs from container - it also mark all logs as 'info'.,0.0,0.0,1.0,0.0
datadog,I'm trying to monitor my celery queues and which tasks they are running.,0.0,0.0,1.0,0.0
datadog,The main idea is to get a better understanding of how everything works.,0.0,0.209,0.791,0.4404
datadog,I'm fairly new to celery outside of calling  delay  or setting up a  periodic_task .,0.161,0.0,0.839,-0.3182
datadog,"I'm running into a bit of a pickle, no pun indented and I'm using datadog to monitor some of this information.",0.109,0.0,0.891,-0.296
datadog,What I setup is an  after_task_publish  function that I'd like to use to track the task and queue it was processed on.,0.0,0.111,0.889,0.3612
datadog,I actually would like the worker as well.,0.0,0.479,0.521,0.5574
datadog,I have extended my celery results so I can look things up in redis and see the queue.,0.0,0.0,1.0,0.0
datadog,Currently the tasks  _exec_options  queue is always  None .,0.0,0.0,1.0,0.0
datadog,Below is an example.,0.0,0.0,1.0,0.0
datadog,"Now this I don't understand because I set the queue, and also know the job ran so it had to be queued.",0.0,0.0,1.0,0.0
datadog,I'm trying to figure out if my configuration of celery is not doing what I anticipate or if I am going about trying to find the queue and worker that processed the job in the wrong way.,0.084,0.0,0.916,-0.4767
datadog,Any direction would be appreciated.,0.0,0.452,0.548,0.5106
datadog,I am currently setting up my microservice application and am currently in the phase of error tracking and logging.,0.137,0.0,0.863,-0.4019
datadog,I am unsure on what the best practices are when it comes to this.,0.116,0.244,0.64,0.4939
datadog,I have done some research on services that provide this.,0.0,0.0,1.0,0.0
datadog,"I feel there is a difference between application logging, like user actions and error tracking.",0.167,0.154,0.679,-0.0516
datadog,When it comes to error tracking I was planning on using a service like  Bugsnag  or  Sentry  which will allow me to follow a stack trace to pin down the error.,0.16,0.13,0.71,-0.25
datadog,However when it comes to application logging I am debating whether to build that in house or also use a service like Datadog or Papertrail.,0.0,0.102,0.898,0.3612
datadog,"However it seems like Datadog and Papertrail help with lower level logs like performance, etc.",0.105,0.368,0.526,0.6705
datadog,Stripe is a great example of clear logging.,0.0,0.573,0.427,0.7717
datadog,You can see the chain of events for each user in an easy to read format.,0.0,0.162,0.838,0.4404
datadog,"To achieve a similar experience, I was thinking of creating a logging microservice which will be a consumer on a kafka broker and every microservice will produce a message when a certain function/user action is invoked and I can store in my db the user info and what action was taken.",0.0,0.095,0.905,0.5106
datadog,Is this standard industry parctice?,0.0,0.0,1.0,0.0
datadog,Is there a service that can acheive this experience?,0.0,0.0,1.0,0.0
datadog,I'm running Postgres 11.,0.0,0.0,1.0,0.0
datadog,I have a table with 1.000.000 (1 million) rows and each row has a size of 40 bytes (it contains 5 columns).,0.0,0.0,1.0,0.0
datadog,That is equal to 40MB.,0.0,0.0,1.0,0.0
datadog,"When I execute (directly executed on the DB via DBeaver, DataGrid ect.- not called via Node, Python ect.",0.0,0.0,1.0,0.0
datadog,):,1.0,0.0,0.0,-0.4215
datadog,it takes 40 secs first time (is this not very slow even for the first time).,0.0,0.0,1.0,0.0
datadog,The CREATE statement of my tables:,0.0,0.362,0.638,0.4278
datadog,"On 5 random tables: EXPLAIN (ANALYZE, BUFFERS) select * from [table_1...2,3,4,5]",0.0,0.0,1.0,0.0
datadog,When I add a LIMIT 1.000.000 to table_5 (it contains 1.7 million rows),0.0,0.0,1.0,0.0
datadog,When I add a WHERE clause between 2 dates (I'm monitored the query below with DataDog software and the results are here (max.~ 31K rows/sec when fetching):  https://www.screencast.com/t/yV0k4ShrUwSd ):,0.101,0.0,0.899,-0.4215
datadog,All tables has an unique index on the c3 column.,0.0,0.0,1.0,0.0
datadog,The size of the database is like 500GB in total.,0.0,0.217,0.783,0.3612
datadog,The server has 16 cores and 112GB M2 memory.,0.0,0.0,1.0,0.0
datadog,"I have tried to optimize Postgres system variables - Like: WorkMem(1GB), shared_buffer(50GB), effective_cache_size (20GB) - But it doesn't seems to change anything (I know the settings has been applied - because I can see a big difference in the amount of idle memory the server has allocated).",0.0,0.09,0.91,0.431
datadog,I know the database is too big for all data to be in memory.,0.0,0.0,1.0,0.0
datadog,But is there anything I can do to boost the performance / speed of my query?,0.0,0.215,0.785,0.5499
datadog,How do I get all the metrics including status codes and exceptions using micrometer and statsd with flavor datadog.,0.0,0.0,1.0,0.0
datadog,I am using maven dependency for micrometer-statsd and spring-boot actuator ?,0.0,0.0,1.0,0.0
datadog,I have added @Timed annotation according to spring boot actuator configuration to a controller.,0.0,0.0,1.0,0.0
datadog,But in the graphite I only see http.server.requests.max BUT no exceptions or status codes.,0.189,0.0,0.811,-0.4215
datadog,Can someone point out what config am I missing ?,0.239,0.0,0.761,-0.296
datadog,I would like to create a file to which I can write as described in the  Datadog Datagram docs :,0.0,0.247,0.753,0.5574
datadog,Everything that is written to that file should be – instead of being handled by Datadog and sent to them via the agent – written to a log file.,0.0,0.0,1.0,0.0
datadog,After executing the three lines above the log file should contain the following:,0.0,0.0,1.0,0.0
datadog,I thought that a named pipe and a background process that handles that would be perfect.,0.0,0.236,0.764,0.5719
datadog,"However, it does not work as expected and the background process never writes anything, even though writing seems to work.",0.0,0.0,1.0,0.0
datadog,I created the following script:,0.0,0.4,0.6,0.25
datadog,And the following systemd service:,0.0,0.0,1.0,0.0
datadog,"The service is started correctly after executing  systemctl enable --now datadog-agent , however, as I said, nothing is ever being written to the log file.",0.0,0.0,1.0,0.0
datadog,This is very strange to me because opening two shell instances where I write the following in the first shell:,0.104,0.0,0.896,-0.2716
datadog,And then start sending data in the second shell prints the lines correctly.,0.0,0.0,1.0,0.0
datadog,I am looking for any tips or advice on how to troubleshoot our Elasticsearch cluster.,0.0,0.122,0.878,0.2023
datadog,This cluster has been running flawlessly with only minor maintenance for a couple years.,0.0,0.13,0.87,0.2023
datadog,We suddenly experienced an outage a week ago today.,0.0,0.0,1.0,0.0
datadog,"I have been struggling to keep it running ever since (it is actually our dev environment, so not production, but our devs are impacted, and we are worried the same thing can happen in production).",0.128,0.0,0.872,-0.5719
datadog,The symptom is this:,0.0,0.0,1.0,0.0
datadog,We have three client nodes that coordinate requests with the data nodes.,0.0,0.0,1.0,0.0
datadog,"As soon as there is any kind of traffic, I see constant Garbage Collection in the logs.",0.0,0.0,1.0,0.0
datadog,Example:,0.0,0.0,1.0,0.0
datadog,"[2019-08-09T00:38:15,835][WARN ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1034] overhead, spent [694ms] collecting in the last [1s]",0.0,0.0,1.0,0.0
datadog,"[2019-08-09T00:38:17,066][WARN ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1035] overhead, spent [693ms] collecting in the last [1.2s]",0.0,0.0,1.0,0.0
datadog,"[2019-08-09T00:38:18,079][INFO ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1036] overhead, spent [352ms] collecting in the last [1s]",0.0,0.0,1.0,0.0
datadog,At some point the client loses communication with the master:,0.204,0.0,0.796,-0.3182
datadog,"[2019-08-06T14:38:54,403][INFO ][o.e.d.z.ZenDiscovery ] [client-vm0] master_left [{master-vm1}{PZJChTgxT46h4YYOqMr2fg}{1G3fXiSMQ5auVH-i5RH10w}{10.0.0.11}{10.0.0.11:9300}{ml.machine_memory=30064300032, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true}], reason [failed to ping, tried [3] times, each with maximum [30s] timeout]",0.0,0.0,1.0,0.0
datadog,"[2019-08-06T14:38:54,419][WARN ][o.e.d.z.ZenDiscovery ] [client-vm0] master left (reason = failed to ping, tried [3] times, each with maximum [30s] timeout), current nodes: nodes:…",0.148,0.0,0.852,-0.5106
datadog,"[2019-08-06T14:38:54,434][INFO ][o.e.x.w.WatcherService ] [client-vm0] stopping watch service, reason [no master node]",0.151,0.0,0.849,-0.1531
datadog,"It tries to find another master, but is unable to:",0.0,0.0,1.0,0.0
datadog,"[2019-08-06T14:41:56,528][WARN ][o.e.d.z.ZenDiscovery ] [client-vm0] not enough master nodes discovered during pinging (found [[]], but needed  2 ), pinging again",0.0,0.0,1.0,0.0
datadog,"During this time, the masters and all the data nodes are perfectly find.",0.0,0.259,0.741,0.6369
datadog,I have usually seen the above when there is a large GC and the *  master  * loses contact with the *  client  * because it is too busy with GC to respond.,0.081,0.0,0.919,-0.3182
datadog,"But in this case, the client can’t find the master.",0.0,0.0,1.0,0.0
datadog,Eventually the client suffers an Out Of Memory failure and the JVM crashes.,0.368,0.0,0.632,-0.7506
datadog,"I am assuming that the memory issues, the GC and the crashing are all related, but I am having a problem figuring out what the cause is, and why so sudden.",0.116,0.0,0.884,-0.5499
datadog,Cluster details:,0.0,0.0,1.0,0.0
datadog,ElasticSearch,0.0,0.0,1.0,0.0
datadog,Version: 6.3.2,0.0,0.0,1.0,0.0
datadog,License: Open Source,0.0,0.0,1.0,0.0
datadog,Nodes:,0.0,0.0,1.0,0.0
datadog,Client (3) (D13_v2): 8 CPU; 56 GB RAM; HDD Drives,0.0,0.0,1.0,0.0
datadog,Master (3) (D4_v2): 8 CPU; 28 GB RAM; HDD Drives,0.0,0.0,1.0,0.0
datadog,Data (35) (DS13): 8 CPU; 56 GB RAM; SSD OS &amp; 3x1TB SSD data drives,0.0,0.0,1.0,0.0
datadog,Indexes:,0.0,0.0,1.0,0.0
datadog,Taxonomy:,0.0,0.0,1.0,0.0
datadog,Size: 1.45 GB,0.0,0.0,1.0,0.0
datadog,Shards: 2,0.0,0.0,1.0,0.0
datadog,Replicas: 1,0.0,0.0,1.0,0.0
datadog,Support:,0.0,1.0,0.0,0.4019
datadog,Size: 365 GB,0.0,0.0,1.0,0.0
datadog,Shards: 30,0.0,0.0,1.0,0.0
datadog,Replicas: 1,0.0,0.0,1.0,0.0
datadog,NonSupport:,0.0,0.0,1.0,0.0
datadog,Size: 2 TB,0.0,0.0,1.0,0.0
datadog,Shards: 80,0.0,0.0,1.0,0.0
datadog,Replicas: 1,0.0,0.0,1.0,0.0
datadog,JVM,0.0,0.0,1.0,0.0
datadog,Java Version: (build 1.8.0_144-b01) Note: we stayed with this build as it was what had been running for most of the last year and we wanted to start from a good state.,0.0,0.088,0.912,0.4404
datadog,ES_HEAP_SIZE: 28672m (roughly half available memory),0.0,0.0,1.0,0.0
datadog,Note also that we have a current issue with field mapping explosion that has grown.,0.0,0.0,1.0,0.0
datadog,This may be a culprit that we are investigating.,0.0,0.0,1.0,0.0
datadog,"If I shut off external access to the cluster, then everything is roses.",0.0,0.0,1.0,0.0
datadog,"When I open back up again, the clients go down in minutes.",0.0,0.0,1.0,0.0
datadog,"If I clear out all pending requests that I can see (we have a lot of queue based traffic), then it seems to be fine for some measure of time (~12 hours), but then it must reach some load where things fall over again.",0.0,0.103,0.897,0.3291
datadog,I have tried modifying the heap size to try to adjust GC time.,0.0,0.0,1.0,0.0
datadog,"I have reduced mappings in the taxonomy index, and that seemed to help.",0.0,0.197,0.803,0.4019
datadog,My current questions:,0.0,0.0,1.0,0.0
datadog,"I also find that I am unable to inspect the hprof crash dumps as they are just too huge (~45GB), so I can’t get any info there on why the JVM might be crashing.",0.151,0.064,0.784,-0.4767
datadog,"I set up DataDog when I started the investigation, but it has a lot of data I don’t know how to interpret",0.0,0.0,1.0,0.0
datadog,Any advice would be appreciated.,0.0,0.452,0.548,0.5106
datadog,"Thanks,",0.0,1.0,0.0,0.4404
datadog,~john,0.0,0.0,1.0,0.0
datadog,We're utilizing the  django-rest-framework  to create a RESTful API and using generic views or view sets to create the endpoint views.,0.0,0.283,0.717,0.6908
datadog,"There is no templating happening, all the frontend is in React.",0.18,0.0,0.82,-0.296
datadog,"However, upon watching the traces in Datadog, we're seeing that SOMETIMES (not every time), Jinja2 is rendering, causing a 500-800ms latency.",0.0,0.0,1.0,0.0
datadog,Does anyone have any clues to why this might be happening and how to turn it off?,0.0,0.0,1.0,0.0
datadog,I'm setting up datadog monitors/alerts and want to have alerts routed to slack or pagerduty depending on if the issue is in our production environment or not.,0.0,0.048,0.952,0.0772
datadog,"I've created multi-alert monitors that alert correctly, but I can't figure out how to make only ones where  environment.name  is equal to  prod  send an alert to pagerduty, and always send them to Slack.",0.0,0.164,0.836,0.5994
datadog,I was hoping to be able to do something like the following in the alert message but haven't been able to figure out a syntax that works:,0.0,0.193,0.807,0.5023
datadog,"For now, I've found a work around of having two monitors that are duplicates of each other where one has is scoped to production only and alerts pagerduty only and the second is for all environments and alerts slack only.",0.0,0.0,1.0,0.0
datadog,"However, I know this is going to become a maintenance nightmare as we grow and I'd like to know if there's a better solution.",0.0,0.3,0.7,0.7717
datadog,So i need to figure out how to write the following path into regexp,0.0,0.0,1.0,0.0
datadog,/private/toolbox/*,0.0,0.0,1.0,0.0
datadog,I'm not sure how to do it because of the *,0.179,0.0,0.821,-0.2411
datadog,I have add the following 2 paths with no problem,0.45,0.0,0.55,-0.5994
datadog,/private/healthcheck,0.0,0.0,1.0,0.0
datadog,/private/datadog/dashboards,0.0,0.0,1.0,0.0
datadog,I'm using Datadog's  statsd client  to record the duration of a certain server response.,0.0,0.149,0.851,0.2732
datadog,I used to pass in quite a few number of custom tags when  time -ing these responses.,0.0,0.101,0.899,0.1477
datadog,So I'm in the process of reducing the number of custom tags.,0.0,0.106,0.894,0.0772
datadog,"However, the problem is that when I reduce the number of tags passed in, there is extra latency of server response, which isn't intuitive because I'm passing in fewer tags and the implementation hasn't changed.",0.075,0.036,0.889,-0.34
datadog,"According to Datadog and Etsy (which originally released  statsd ), these methods that record these metrics aren't blocking.",0.0,0.114,0.886,0.2924
datadog,"However, they must be using some extra threads to perform this.",0.0,0.0,1.0,0.0
datadog,What could be the issue?,0.0,0.0,1.0,0.0
datadog,Are there possible any side effects associated with using this client?,0.0,0.0,1.0,0.0
datadog,"I have an application running under spring boot utilizing SMBJ to mount and read remote files, and it works perfectly.",0.0,0.189,0.811,0.6369
datadog,However I am trying to set up some datadog reporting and trying to use JMX as a datasource for datadog...,0.0,0.0,1.0,0.0
datadog,TO do this I am staring the springboot jar with the following:,0.0,0.0,1.0,0.0
datadog,"And once I do this, SMBJ no longer creates the mount.",0.179,0.171,0.65,-0.0258
datadog,"If I remove these parameters the code works fine again and SMBJ is able to create/mount to the share, If I have them it simply times out trying to create the share.",0.0,0.242,0.758,0.743
datadog,"I thought maybe it was the RMI hostname change, but removing just this this doesn't seem to fix it.",0.0,0.0,1.0,0.0
datadog,Can anyone offer any help on this?,0.0,0.31,0.69,0.4019
datadog,Is SMBJ really dependent on the jmxremote settings?,0.0,0.0,1.0,0.0
datadog,"It certainly seems to be..I have tried removing the overriding of the ports, so they go to their default ports as well, but this didn't fix it either.",0.0,0.111,0.889,0.3071
datadog,Any help would be appreciated.,0.0,0.667,0.333,0.7184
datadog,We need to separate logs generated by a REST web-service on a user-specific basis and eventually import these logs into an aggregation framework like Datadogs.com.,0.0,0.102,0.898,0.3612
datadog,There are several ways to approach this and I’m interested in getting feedback before selecting an approach.,0.0,0.144,0.856,0.4019
datadog,The basics would go something like this:,0.0,0.294,0.706,0.3612
datadog,Depending on the stage,0.0,0.0,1.0,0.0
datadog,"For development, use the NLOG File Logger, so the developer can simply “tail” the log file.",0.0,0.0,1.0,0.0
datadog,"Use the variable in the nlog filename target as 
 &lt;target filename=""/path/file-${var:userid}.log""/&gt;.",0.0,0.0,1.0,0.0
datadog,"Or use the, it in the nlog target as:
 layout=""${var:userid}-${OtherLayout} "", and have developers do a  tail -f masterFile.log | grep USERID.",0.0,0.0,1.0,0.0
datadog,"In production switch to using the nlog JsonLayout, so a system like DataDog can read Time, Threadid, Userid and message data.",0.0,0.128,0.872,0.4173
datadog,Use the JsonFormat and specify a userid attributes.,0.0,0.0,1.0,0.0
datadog,I see that the JsonFormat has supports for MappedDiagnosticsLogicalContext but I would prefer the simplicity of the ${var:xxx} format to specify the value.,0.0,0.203,0.797,0.5927
datadog,Problems:,1.0,0.0,0.0,-0.4019
datadog,I’ve used the MappedDiagnosticContext in the past and then specified it in the target filename.,0.0,0.0,1.0,0.0
datadog,I just attempted to use the ${var} approach and it did not appear to work?,0.0,0.0,1.0,0.0
datadog,?,0.0,0.0,0.0,0.0
datadog,Concerns:,0.0,0.0,1.0,0.0
datadog,I like the filename target approach for development.,0.0,0.294,0.706,0.3612
datadog,It should work well with 10-100 of users but not scale with 1000's of users.,0.0,0.1,0.9,0.1406
datadog,"Clearly with 1000’s of users we would need to close the log file after each line is written, as we don’t want to keep 1000’s of files open.",0.0,0.133,0.867,0.4588
datadog,A major concern is threading.,0.0,0.0,1.0,0.0
datadog,It’s possible that each webservice is called multiple times by different users and all approaches require Nlog MappedDiagnosticContext or ${var} capability’s to be thread safe.,0.0,0.108,0.892,0.4404
datadog,Is it?,0.0,0.0,1.0,0.0
datadog,Any issues to consider?,0.0,0.0,1.0,0.0
datadog,"Eventually we would like to introduce some structured logging into the system, but the majority of the code base was built using standard logging techniques.",0.0,0.068,0.932,0.1901
datadog,"If the objects being logged in the structured logging included userid then much of this complexity could be avoided, but that would require a lot of work rewriting for structured logging.",0.055,0.0,0.945,-0.1779
datadog,"Clearly, there is a lot to think about and I know I’m not the first to ponder this.",0.0,0.153,0.847,0.4019
datadog,Your input will be appreciated.,0.0,0.452,0.548,0.5106
datadog,I'm trying to monitor our postgresql DB and identify the 20 largest tables and than see when was the last vacuum and analyse took place.,0.0,0.0,1.0,0.0
datadog,I have this query that shows me the largest 20 schema name/relname which is good and that's what I was looking for:,0.0,0.132,0.868,0.4404
datadog,I also have this query that shows me all the analysis I want to see with schema name and relname:,0.0,0.071,0.929,0.0772
datadog,But I'm having a real hard time combining them together to one query that will show me when those analysis only for those 20 tables.,0.065,0.0,0.935,-0.1531
datadog,"Once this is done I'm looking to view the results in some sort of a graphic view in datadog, so If anyone have a good idea how to run this query as a datadog posgres query it will be amazing as well.",0.0,0.196,0.804,0.8316
datadog,I have a metric client that looks something like:,0.0,0.294,0.706,0.3612
datadog,Then in my application I import it and use it,0.0,0.0,1.0,0.0
datadog,This publishes the log  Updating metric metric1  in datadog and I can see it.,0.0,0.0,1.0,0.0
datadog,But this will only publish the first instance.,0.0,0.0,1.0,0.0
datadog,"Until I restart  service nginx restart , I will not get any more increments.",0.0,0.0,1.0,0.0
datadog,Update,0.0,0.0,1.0,0.0
datadog,So I have a  start.lua  in  /etc/nginx/conf.d/start.lua  that is:,0.0,0.0,1.0,0.0
datadog,And the nginx config is,0.0,0.0,1.0,0.0
datadog,"If I were to copy/paste the metric code into  start.lua , then the metric is updated every time.",0.0,0.0,1.0,0.0
datadog,Why is this?,0.0,0.0,1.0,0.0
datadog,!,0.0,0.0,0.0,0.0
datadog,Update,0.0,0.0,1.0,0.0
datadog,I noticed this in the error logs:,0.351,0.0,0.649,-0.4019
datadog,"This happens on the 2nd request to the nginx; the first time after restart, this is all fine ...",0.0,0.091,0.909,0.2023
datadog,Update 2,0.0,0.0,1.0,0.0
datadog,This happens only if I have a  metrics  file and  require  it in my other.,0.0,0.0,1.0,0.0
datadog,"So if I instantiate the  resty_dogstatsd  client inside the main lua file, then everything is fine ...",0.0,0.107,0.893,0.2023
datadog,"I have installed data dog agent on one of my virtual machines When I have altered my NSG so that all ""Outbound-connections"" are denied, I am still able to see ""CPU metric"" getting updated on Data dog dashboard.",0.079,0.0,0.921,-0.4404
datadog,I would like to know where this information is going from Azure to Datadog.,0.0,0.172,0.828,0.3612
datadog,We have an application hosted on Heroku and logs are getting redirected to Sumologic.,0.0,0.0,1.0,0.0
datadog,I see some options can be passed to JVM where log files will be generated locally.,0.0,0.0,1.0,0.0
datadog,"Question: Is there a way we can redirect these logs to cloud log analyzers like Splunk, SumoLogic, or Datadog?",0.0,0.128,0.872,0.3612
datadog,"I have configured the docker-daemon,and also added modified the auto_conf.",0.0,0.0,1.0,0.0
datadog,How should i pass the  %%host%%  variable?,0.0,0.0,1.0,0.0
datadog,changed the etcd.yaml,0.0,0.0,1.0,0.0
datadog,but when i try to do,0.0,0.0,1.0,0.0
datadog,sudo docker exec -it dd-agent /etc/init.d/datadog-agent configcheck,0.0,0.0,1.0,0.0
datadog,the collector logs show,0.0,0.0,1.0,0.0
datadog,I'm sending metrics from a C# web service to datadog.,0.0,0.0,1.0,0.0
datadog,I need to track the length of words that are being searched in an api call and display this in a histogram.,0.0,0.0,1.0,0.0
datadog,But datadog is averaging these values which is not what I want.,0.096,0.256,0.648,0.4968
datadog,"If one string is 1 character in length and another string is 10 characters in length it records a metric of 5.5, which isn't much use to me.",0.0,0.0,1.0,0.0
datadog,Ideally I would like a histogram graph over a time period e.g.,0.0,0.431,0.569,0.6486
datadog,"an hour, showing the number of instances of 1, 2, 3 etc.",0.0,0.115,0.885,0.0772
datadog,that were recorded during that time period.,0.0,0.0,1.0,0.0
datadog,Is that possible in datadog?,0.0,0.0,1.0,0.0
datadog,This is the call I'm making in the code:,0.0,0.0,1.0,0.0
datadog,I have a python program that crunches a large dataset using Pandas.,0.0,0.0,1.0,0.0
datadog,It currently takes about 15 minute to complete.,0.0,0.0,1.0,0.0
datadog,I want to log (stdout &amp; send the metric to Datadog) about the progress of the task.,0.0,0.227,0.773,0.4767
datadog,Is there a way to get the %-complete of the task (or a function)?,0.0,0.0,1.0,0.0
datadog,"In the future, I might be dealing with larger datasets.",0.0,0.0,1.0,0.0
datadog,The Python task that I am doing is a simple grouping of a large pandas data frame.,0.0,0.0,1.0,0.0
datadog,Something like this:,0.0,0.556,0.444,0.3612
datadog,"here, the categoryList has about 20000 items, and df is a large data frame having (say) a 5 million rows.",0.0,0.0,1.0,0.0
datadog,I am not looking for anything fancy (like progress-bars..).,0.0,0.0,1.0,0.0
datadog,Just percentage complete value.,0.0,0.444,0.556,0.34
datadog,Any ideas?,0.0,0.0,1.0,0.0
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,I would like to detect bad/faulty aws instances using datadog's outlier detection.,0.0,0.2,0.8,0.3612
datadog,Is that possible?,0.0,0.0,1.0,0.0
datadog,I'm trying to create an automatic failover scenario using datadog.,0.0,0.189,0.811,0.2732
datadog,Any suggestions would be appreciated.,0.0,0.452,0.548,0.5106
datadog,"We're setting up monitoring for SphinxQL query_log in DataDog, and we'd like to understand what each value represents in the logging format.",0.0,0.197,0.803,0.5994
datadog,"We understand all of the values except for  conn , which we're not seeing an explicit definition, but are making an educated guess that it might be a connection id.",0.0,0.064,0.936,0.2144
datadog,We'd like to know for sure.,0.0,0.545,0.455,0.5859
datadog,The standard log format for Sphinx:,0.0,0.0,1.0,0.0
datadog,The SphinxQL logging format:,0.0,0.0,1.0,0.0
datadog,"You can see the SphinxQL format adds a  conn  param after the query date, but the docs and referencing the standard log format do not make clear what  conn  is.",0.09,0.0,0.91,-0.4168
datadog,"We think it's a connection id, but I'm hoping someone with more expert knowledge of Sphinx can help clarify.",0.0,0.312,0.688,0.8047
datadog,"I have terraform config as below, which I am using to bring up ECS cluster in various prod/uat/dev environments.",0.0,0.0,1.0,0.0
datadog,"In our infra, we configure the  logConfiguration  of container definition, based on targeted environment.",0.0,0.0,1.0,0.0
datadog,"if infra being brought up by developer and  env:dev  , we configure the  logConfiguration  to use  awslogs  as logging driver and its subsequent log driver options ( i.e awslogs-region, awslogs-group, awslogs-stream-prefix )",0.0,0.0,1.0,0.0
datadog,"if infra being brought up by our continuous deployment pipeline with  env:prod or uat , we configure the  logConfiguration  to use  awsfirelens  logDriver and its subsequent options , we also add fluentbit container definition ( we use DataDog as logging / monitoring solutions, so we bring up the fluentbit container beside our service container to forward the logs to datadog endpoint )",0.0,0.029,0.971,0.1779
datadog,"As of today, below configuration is working out just fine.",0.0,0.167,0.833,0.2023
datadog,however it has the code duplication of my service container definition for each log configuration use-case as I am using ternary operator to decide which container definition to generate based on env.,0.0,0.0,1.0,0.0
datadog,"This makes my code un-necessary long and error prone to work with ( If I add/substract environment variables for my service, I have to do that in both the use-case.",0.094,0.0,0.906,-0.4019
datadog,"If I change my service config in the future, I have to keep maintaining that in both the use-case etc etc. )",0.0,0.0,1.0,0.0
datadog,"How can I improve above TF config, so that terraform will conditionally generates the container definition based on env: I passed in?",0.0,0.132,0.868,0.4404
datadog,"If I pass env:dev it will log to awslogs logging driver and its options, if I pass env:uat/prod , it will log to awsfirelens log driver and also add fluentbit container definition in task definition.",0.0,0.0,1.0,0.0
datadog,During years I used to work with SUMO.,0.0,0.0,1.0,0.0
datadog,I used to do things like:,0.0,0.385,0.615,0.3612
datadog,"And from there you would add time slices, to count on time basis.",0.0,0.0,1.0,0.0
datadog,"And then you could also integrate the results with some nice charts like time series, pie chart.",0.0,0.261,0.739,0.6486
datadog,All within seconds.,0.0,0.0,1.0,0.0
datadog,"Now, I forced to use DataDog.",0.429,0.0,0.571,-0.4588
datadog,Imagine you have errors like:,0.304,0.316,0.38,0.0258
datadog,I would like to count errors by place.,0.242,0.253,0.505,0.0258
datadog,In this case:,0.0,0.0,1.0,0.0
datadog,Well I can't do it with Data Dog.,0.0,0.259,0.741,0.2732
datadog,I hate it.,0.787,0.0,0.213,-0.5719
datadog,"I read about Pipeline, Processors, Metrics.",0.0,0.0,1.0,0.0
datadog,I can't believe this is so difficult compared with SUMO.,0.288,0.0,0.712,-0.5009
datadog,"I have a Pipeline, who parses my logs.",0.0,0.0,1.0,0.0
datadog,Then I can see all the fields being parsed.,0.0,0.0,1.0,0.0
datadog,But how can I create a metric from there?,0.0,0.306,0.694,0.3919
datadog,"My fields are strings, so I can't &quot;measure&quot; them.",0.0,0.0,1.0,0.0
datadog,I just need to count how many &quot;house&quot; errors happened.,0.231,0.0,0.769,-0.34
datadog,And so on with some other strings.,0.0,0.0,1.0,0.0
datadog,"In Laravel, the general way to debug performance is to use  Laravel Debugbar",0.0,0.0,1.0,0.0
datadog,What that doesn't include is things like:,0.0,0.294,0.706,0.3612
datadog,"With all of this, it's difficult to debug where the performance is being slowed down.",0.152,0.0,0.848,-0.3612
datadog,Does anyone have any tools or details on how to debug FULLY end to end and each little area?,0.0,0.0,1.0,0.0
datadog,Visual tools preferred.,0.0,0.0,1.0,0.0
datadog,"Haven't looked as much into Xdebug, but I see tools like Datadog/Newrelic but haven't played with those much either.",0.117,0.149,0.734,0.1769
datadog,Would be great to be able to debug it locally rather than in production environment.,0.0,0.227,0.773,0.6249
datadog,"I want to be able to see that from all my middleware, view composers, included partials that there's one particular function that's causing 200ms due to a query or inefficient piece of code and I haven't found anything that can do that easily without adding breakpoints or start/stop timers everywhere but the app is too big for that.",0.0,0.051,0.949,0.2144
datadog,Any help appreciated!,0.0,0.863,0.137,0.7424
datadog,"Background of the issue :
We are using Magnolia CMS with customized UI.",0.0,0.0,1.0,0.0
datadog,"As a first step of using Magnolia, we are migrating old content including documents to Magnolia.",0.0,0.0,1.0,0.0
datadog,Content migration is working fine.,0.0,0.31,0.69,0.2023
datadog,Issue with DAM.,0.0,0.0,1.0,0.0
datadog,We have more than 50 GB of historical content to be stored in Magnolia.,0.0,0.0,1.0,0.0
datadog,Storing in DB became very expensive.,0.0,0.0,1.0,0.0
datadog,We decided to save them in File system.,0.0,0.314,0.686,0.4939
datadog,Magnolia running in Kubernetes cluster as a service.,0.0,0.0,1.0,0.0
datadog,"Problem: After we migrate dam from old system to magnolia, we are trying to publish asset from author to public instance.",0.112,0.103,0.785,-0.0516
datadog,"After MAx 200 documents published, It is killing the pod.",0.328,0.0,0.672,-0.6597
datadog,"In logs, we are not seeing anything.",0.0,0.0,1.0,0.0
datadog,Last message is Catalish.sh killed...,0.0,0.0,1.0,0.0
datadog,Could you please suggest if anyone come across such scenarios.,0.0,0.204,0.796,0.3182
datadog,Best practices to implement dam and other configuration.,0.0,0.375,0.625,0.6369
datadog,"In Datadog, have seen that threadcounts are always more and GC is going crazy.",0.156,0.0,0.844,-0.34
datadog,Thank you in advance.,0.0,0.455,0.545,0.3612
datadog,For e.g.,0.0,0.0,1.0,0.0
datadog,tags:,0.0,0.0,1.0,0.0
datadog,When using above syntax not getting logs in Datadog for component2 only getting logs only for component1 so we want to parse logs for both the component i.e component1 and component2 so how would get it?,0.0,0.043,0.957,0.1477
datadog,In my project haproxy is used.,0.0,0.0,1.0,0.0
datadog,But time to time I'm getting backend session limit exceed in haproxy when check with datadog graphs.,0.0,0.0,1.0,0.0
datadog,At the same time when check the frontend session count it also bit high than the normal time.,0.0,0.0,1.0,0.0
datadog,I need to know following things.,0.0,0.0,1.0,0.0
datadog,What is the connection between frontend sessions and user requests come to the frontend?,0.0,0.0,1.0,0.0
datadog,Single user getting one session for all the requests or multiple sessions?,0.0,0.0,1.0,0.0
datadog,For one frontend session backend going to create one session or multiple sessions?,0.0,0.149,0.851,0.2732
datadog,If the frontend sessions are hight then can we say requests count to the haproxy is high?,0.0,0.0,1.0,0.0
datadog,"If possible can anyone share a tutorial to clearly understand connections between user requests, frontend sessions and backend sessions.",0.0,0.234,0.766,0.5994
datadog,Thank you.,0.0,0.714,0.286,0.3612
datadog,"I am using datadog with the agent, parsing logs in a file in json format.",0.0,0.0,1.0,0.0
datadog,The way I found to tag json attributes is to edit a pipeline configuration and do something like,0.0,0.143,0.857,0.3612
datadog,"But I have to do it for each individual tag key, which implies to know the list of tags in advance and maintain it over time.",0.0,0.0,1.0,0.0
datadog,It's not ideal.,0.581,0.0,0.419,-0.4168
datadog,I'd like to know if there is a way to log things in this format,0.0,0.161,0.839,0.3612
datadog,"And have a pipeline configuration taking all key/value pairs in the &quot;tags&quot; value of the log, and tag each one of them.",0.0,0.107,0.893,0.34
datadog,What I thought about was doing this,0.0,0.0,1.0,0.0
datadog,But datadog's documentation is not clear about native attributes and their names so I don't know what would be the target attribute name.,0.117,0.0,0.883,-0.4168
datadog,I'm using NestJS (with Express Server) for a project and trying to optimize the performance on some of the endpoints.,0.0,0.151,0.849,0.4939
datadog,Using Datadog I noticed that about 83% of the response time of all endpoints is spent in an anonymous middleware.,0.0,0.0,1.0,0.0
datadog,Does anyone know what middleware this is and why it's taking this long?,0.0,0.0,1.0,0.0
datadog,I suspect that it has to do with the framework itself due to the similar unanswered question  here .,0.121,0.0,0.879,-0.296
datadog,My company has a cluster that's already been monitoring by a datadog agent.,0.0,0.0,1.0,0.0
datadog,But my team needs a monitoring just for us.,0.0,0.0,1.0,0.0
datadog,"I already looked on fluentd, prometheus and so on, but I cound't find an option for use a tool that I don't need to install in my namespace.",0.0,0.0,1.0,0.0
datadog,Does anyone know an option that I can collect the logs of my pods just in my namespace?,0.0,0.0,1.0,0.0
datadog,"Like, up a pod for grafana and another for collect logs and send to grafana or something like that?",0.0,0.238,0.762,0.6124
datadog,"I´am trying to setup serilog for for a project, but have some trouble understanding how it should be setup according to best practice for production.",0.113,0.185,0.702,0.5023
datadog,"I have followed this  https://benfoster.io/blog/serilog-best-practices/  , but I have some questions regarding the section  Configuration .",0.0,0.0,1.0,0.0
datadog,There he says;,0.0,0.0,1.0,0.0
datadog,"In .NET writing to Console is a blocking call and can have a
significant performance impact.",0.159,0.11,0.732,-0.2023
datadog,and he links to  https://weblog.west-wind.com/posts/2018/Dec/31/Dont-let-ASPNET-Core-Default-Console-Logging-Slow-your-App-down  .,0.0,0.0,1.0,0.0
datadog,Which again says that console is bad for perfomance and has this solution to the problem:,0.284,0.12,0.596,-0.5537
datadog,Which only adds console if in development.,0.0,0.0,1.0,0.0
datadog,My question is: If I´am not logging to console for production what should i do?,0.0,0.0,1.0,0.0
datadog,Should i write to file?,0.0,0.0,1.0,0.0
datadog,RollingFile?,0.0,0.0,1.0,0.0
datadog,What is the best perfomance vice and will it work on fargate?,0.0,0.276,0.724,0.6369
datadog,My service is running on fargate with firelens sending logs to datadog.,0.0,0.0,1.0,0.0
datadog,I recently read an article regarding Tomcat architecture and a high level overview of its working and monitoring.,0.0,0.0,1.0,0.0
datadog,Key metrics for monitoring Tomcat - DataDog,0.0,0.0,1.0,0.0
datadog,"In this article, it mentions Tomcat having a pool of worker threads per connector that can be configured.",0.0,0.0,1.0,0.0
datadog,It also mentions about Executors and how it is mainly a thread pool that is shared with multiple connectors.,0.0,0.124,0.876,0.34
datadog,I have some doubts regarding Spring Boot and its Embedded Tomcat Server,0.18,0.0,0.82,-0.296
datadog,I would be grateful if someone could shed some light on the above.,0.0,0.214,0.786,0.4588
datadog,"In short, I just wanted to know if the server configuration via application.propeties is for an Executor or for the Connector specific pool of worker threads.",0.0,0.0,1.0,0.0
datadog,So here's the scenario..,0.0,0.0,1.0,0.0
datadog,"I'm seeing higher network packets per second on the older 2 webservers, and lower on the newer one.",0.121,0.0,0.879,-0.296
datadog,"Datadog defines this metric as ""The number of packets of data received by the interface"".",0.0,0.085,0.915,0.0772
datadog,The graph looks like this,0.0,0.385,0.615,0.3612
datadog,To me this looks like there is higher throughput on the older servers.,0.0,0.172,0.828,0.3612
datadog,"Looking at the nginx requests per 24 hour period on the 3 boxes, I see about  12.5% increase on the new box .",0.0,0.108,0.892,0.3182
datadog,This tells me that..,0.0,0.0,1.0,0.0
datadog,Can someone help me makes sense of this,0.0,0.278,0.722,0.4019
datadog,I'm trying to deploy a build Jenking ANSIBLE and i receive this error message.,0.221,0.0,0.779,-0.481
datadog,The target is to install Datadog Agent on Ubuntu ANSIBLE via playbook that i build.,0.0,0.0,1.0,0.0
datadog,the message i receive is the following:,0.0,0.0,1.0,0.0
datadog,10:39:16 [INFO] les credentials Conjur n'ont pas été récupérés : Could not find credentials entry with ID 'conjur_api_key',0.0,0.0,1.0,0.0
datadog,and :,0.0,0.0,1.0,0.0
datadog,"&lt;10.145.99.222  (0, b'', b'')
10:40:00 fatal: [uamudaxd07]: FAILED!",0.566,0.0,0.434,-0.8327
datadog,"=  {
10:40:00     ""changed"": false,
10:40:00     ""invocation"": {
10:40:00         ""module_args"": {
10:40:00             ""_original_basename"": null,
10:40:00             ""attributes"": null,
10:40:00             ""backup"": false,
10:40:00             ""checksum"": null,
10:40:00             ""content"": null,
10:40:00             ""delimiter"": null,
10:40:00             ""dest"": ""/tmp/datadoginstall/datadog-agent-base_1.0_all.deb"",
10:40:00             ""directory_mode"": null,
10:40:00             ""follow"": false,
10:40:00             ""force"": true,
10:40:00             ""group"": null,
10:40:00             ""local_follow"": null,
10:40:00             ""mode"": null,
10:40:00             ""owner"": null,
10:40:00             ""regexp"": null,
10:40:00             ""remote_src"": true,
10:40:00             ""selevel"": null,
10:40:00             ""serole"": null,
10:40:00             ""setype"": null,
10:40:00             ""seuser"": null,
10:40:00             ""src"": ""/S3binaries/binaires_mdw_dax/DATADOG_AGENT/Linux/datadog-agent-base_1.0_all.deb"",
10:40:00             ""unsafe_writes"": null,
10:40:00             ""validate"": null
10:40:00         }
10:40:00     },
10:40:00     ""msg"": ""Source /S3binaries/binaires_mdw_dax/DATADOG_AGENT/Linux/datadog-agent-base_1.0_all.deb not found""
10:40:00 }
10:40:00 
10:40:00 PLAY RECAP *********************************************************************
10:40:00 uamudaxd07                 : ok=4    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0 
10:40:00",0.0,0.082,0.918,0.8286
datadog,is there someone to help me :)?,0.0,0.31,0.69,0.4019
datadog,We have a custom Golang script to publish messages to PubSub.,0.0,0.0,1.0,0.0
datadog,We then use the same client to publish to upto 40 topics.,0.0,0.0,1.0,0.0
datadog,and then based on a certain condition publish to 1 of the topic.,0.0,0.174,0.826,0.2732
datadog,Our publisher loop looks like this,0.0,0.333,0.667,0.3612
datadog,"We use 3000 Goroutines to publish messages to the topics and synchronously wait for messages to get acknowledged, that means there are at a time only 3000 in flight/waiting for acknowledgement at client.",0.0,0.0,1.0,0.0
datadog,Our current rate of publishing is close to 5K RPS but our latencies are as high as 30 seconds.,0.0,0.0,1.0,0.0
datadog,Below are the stats that I compiled from our Datadog dashboard.,0.0,0.0,1.0,0.0
datadog,When I wrote a small benchmark script to publish messages to a single topic the average latency was 147ms from the same machine.,0.0,0.0,1.0,0.0
datadog,I've tried to tweak the publisher settings for each topic but that did not help.,0.171,0.0,0.829,-0.438
datadog,Now I have couple of question.,0.0,0.0,1.0,0.0
datadog,I'm trying to send logs from td-agent to Datadog using the below configuration.,0.0,0.0,1.0,0.0
datadog,My expectation is filtering some keywords and formating that logs with using CSV format type.,0.0,0.0,1.0,0.0
datadog,How can I do this?,0.0,0.0,1.0,0.0
datadog,I tried to grep and format plugin in the filter section as below but it doesn't work as expected.,0.0,0.0,1.0,0.0
datadog,The current and expected situation as below picture.,0.0,0.0,1.0,0.0
datadog,How can I solve this situation?,0.0,0.31,0.69,0.2023
datadog,current,0.0,0.0,1.0,0.0
datadog,expected,0.0,0.0,1.0,0.0
datadog,"I am trying to install datadog agent via runbook on multiple Azure Virtual Machine (VM), I have uploaded binaries on Blob from where I can download on my local computer (for testing, it is working fine), but when I am trying to connect to Azure Vm via  $Session = New-PSSession -ComputerName $vm  -Credential $cred , I am getting an error that Winrm server is having an issue.",0.057,0.0,0.943,-0.5499
datadog,Even winrm is running fine on that server.,0.0,0.205,0.795,0.2023
datadog,Just I wanted to know is there any other way to download binaries on a remote VM and install on it via Powershell or runbook.,0.0,0.0,1.0,0.0
datadog,"If there is an option, please suggest it.",0.0,0.247,0.753,0.3182
datadog,we use kubenrnetes nginx ingress controller version 0.25.1 on aws eks (kubernetes version 1.13).,0.0,0.0,1.0,0.0
datadog,we enable opentracing as per the documentation and use Datadog to view the traces.,0.0,0.0,1.0,0.0
datadog,We have a general ingress rule to catch every path:,0.0,0.0,1.0,0.0
datadog,"In the Datadog ui we see the nginx traces, however the ""resource"" column always shows ""/"" rather than the full path which is ""/test"" or ""/ping"".",0.0,0.0,1.0,0.0
datadog,"If we create a separate ingress rule for each resource path, then we see the full path as expected (i.e.",0.0,0.104,0.896,0.2732
datadog,"""/test"" or ""//ping"") but it is very inconvenient and tedious to create a ingress rule for each path.",0.167,0.125,0.708,-0.2238
datadog,is there any way we can see the full resource path in datadog UI without creating a separate ingress rule for each resource path?,0.079,0.0,0.921,-0.2235
datadog,I am in the process of migrating our Laravel application to EKS Kubernetes which is currently running on Docker however the response time is significantly slower.,0.0,0.0,1.0,0.0
datadog,"The current response time is roughly (Docker): 350-450 ms 
The new response time is roughly (Kubernetes): 750-1100 ms",0.0,0.0,1.0,0.0
datadog,Notable Environment Differences:,0.0,0.0,1.0,0.0
datadog,APM Findings:,0.0,0.0,1.0,0.0
datadog,"I am running DataDog which shows that a lot of time is being spent on Laravel, rather than DB or Redis which doesn't give me much to work with.",0.0,0.0,1.0,0.0
datadog,"At this point, I am thinking it is infrastructure related rather than an issue with Laravel as the Docker environment already preforms (decently).",0.0,0.0,1.0,0.0
datadog,I am running this as an init container (which occurs every deployment or pod restart):,0.0,0.0,1.0,0.0
datadog,I am unsure where to start troubleshooting.,0.26,0.221,0.519,-0.0772
datadog,Any advice will be helpful.,0.0,0.412,0.588,0.4215
datadog,"I want to get a list of queries executed against my mysql instance, I also want to get list of executions counts for them and duration,",0.0,0.11,0.89,0.1531
datadog,"I can get these stats in something like datadog APM, but I would like to be able to run a query for them locally.",0.0,0.208,0.792,0.6124
datadog,is there a table or schema I need to look at?,0.0,0.0,1.0,0.0
datadog,I have a Flink job which reads from Kafka (v0.9) and writes to Redis.,0.0,0.0,1.0,0.0
datadog,I want to monitor the  records-consumed-rate  and  records-lag-max  metrics emitted by Kafka which Flink should be able to forward.,0.0,0.071,0.929,0.0772
datadog,"In this case, I am forwarding to Datadog.",0.0,0.0,1.0,0.0
datadog,"When I start the job with a parallelism of 1, I see this metric emitted just fine.",0.0,0.122,0.878,0.2023
datadog,"However, if I make the parallelism greater than 1, this metric is no longer forwarded.",0.132,0.15,0.719,0.0772
datadog,The job is running when parallelism   1 because I can see the entries being written to Redis.,0.0,0.0,1.0,0.0
datadog,I'm running Flink (v1.6.2) on AWS EMR:,0.0,0.0,1.0,0.0
datadog,The parallelism is set by streamExecutionEnvironment.setParallelism().,0.0,0.0,1.0,0.0
datadog,Each Kafka Consumer is instantiated with the same group.id and a unique client.id.,0.0,0.0,1.0,0.0
datadog,The DD agent is running just fine on the cluster.,0.0,0.167,0.833,0.2023
datadog,Many metrics are being emitted such as numberOfCompletedCheckpoints and upTime etc.,0.0,0.0,1.0,0.0
datadog,Is there any reason Flink would not be forwarding these metrics from Kafka if the parallelism is greater than 1?,0.0,0.116,0.884,0.3612
datadog,"Update: 
I also tried sending a custom DD metric ( counter.inc() ) from the Redis RichSinkFunction.",0.0,0.0,1.0,0.0
datadog,"When the parallelism=1, the metric is sent fine.",0.0,0.205,0.795,0.2023
datadog,"When parallelism=7, the metric is not sent however it is being called (added a debug line).",0.0,0.0,1.0,0.0
datadog,So it seems its not limited to the forwarded metrics from Kafka.,0.0,0.132,0.868,0.1695
datadog,I'm using Nginx and I want to keep track how many hits we get to what endpoint.,0.0,0.08,0.92,0.0772
datadog,"We have few services in our website, how we can track the number of hits each one gets (not only number of connections but to what path of the platform)?",0.0,0.076,0.924,0.0772
datadog,That way for example we can see what each point of our API get's the most hits and to improve things there.,0.0,0.121,0.879,0.4404
datadog,If there is a way to get this even further with origin of the request it will be great.,0.0,0.194,0.806,0.6249
datadog,"I've installed Datadog agent but didn't installed anything related to NGINX, there is better tool for this task?",0.0,0.185,0.815,0.5927
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,I can't seem to configure my rails app properly.,0.0,0.0,1.0,0.0
datadog,I had a perfectly working rails app deployed on Heroku.,0.0,0.375,0.625,0.6369
datadog,I added the datadog buildpack and then started getting the following build error:,0.197,0.0,0.803,-0.4019
datadog,"After removing my Gemfile.lock and reinstalling and making sure bunder version was   2.0, I then started getting another build error.",0.123,0.105,0.773,-0.1027
datadog,This time it was that the bundler version my project needs is &lt; 2:,0.0,0.0,1.0,0.0
datadog,I'm not sure how to move forward debugging this error.,0.389,0.0,0.611,-0.6233
datadog,I don't know how to access the /tmp/build folder and I'm not sure what I would do if I did.,0.109,0.0,0.891,-0.2411
datadog,Please any direction or suggestions would be greatly appreciated.,0.0,0.457,0.543,0.7089
datadog,I've already tried pushing to heroku master with various versions of bundler.,0.0,0.0,1.0,0.0
datadog,There is always a problem with the build.,0.31,0.0,0.69,-0.4019
datadog,"I feel like there are multiple areas where bundler is required in the build and in the actual project, and that they require different versions, but I don't know how to find them.",0.0,0.055,0.945,0.1901
datadog,Here is my Gemfile:,0.0,0.0,1.0,0.0
datadog,and here is my Gemfile.lock,0.0,0.0,1.0,0.0
datadog,I'd like to be able to find where the areas are in my code that are requiring different versions of bundler.,0.0,0.111,0.889,0.3612
datadog,Or be able to circumvent the issue with a different gem.,0.0,0.0,1.0,0.0
datadog,If anyone can help me get the Heroku build to work I would be very grateful.,0.0,0.316,0.684,0.7178
datadog,I wonder if this is possible to achieve in Datadog.,0.0,0.0,1.0,0.0
datadog,"I have a data collected under 1 metric  entity.count  - now the data are being posted to Datadog with multiple tags, for example  entity.count.visits ,  entity.count.payment  and probably another 10 different tags.",0.0,0.0,1.0,0.0
datadog,"I'm trying to create Datadog chart in a dashboard, which would display top 5 tags of the entity counts in a stacked bar chart.",0.0,0.17,0.83,0.4404
datadog,"I know about the option of adding more queries, but since I'm not sure what entities will be available in the future, I would like datadog to always just display dynamically the top 5 entities in the dashboard (Insted of me specifying in the queries what tag to display).",0.046,0.164,0.79,0.7397
datadog,"This is what I currently have (and it does the job, it's just not dynamic):",0.0,0.0,1.0,0.0
datadog,I've decided to use zstd library for compression in my code using the  Go wrapper from datadog .,0.0,0.0,1.0,0.0
datadog,My build command for app is (I build my app in gitlab-ci using  image: golang:1.10.2-alpine ),0.0,0.0,1.0,0.0
datadog,Which fails with,0.583,0.0,0.417,-0.4215
datadog,When I try to enable  CGO_ENABLED  (and keep the rest of the build commands unchanged) I end up with warnings:,0.115,0.0,0.885,-0.296
datadog,Which doesn't seem ok as I want to have my build statically linked.,0.143,0.099,0.758,-0.1501
datadog,Do I have to firstly build zstd library and then build my app?,0.0,0.0,1.0,0.0
datadog,I created a sample service in Akka for testing Kamon + DataDog monitoring.,0.0,0.182,0.818,0.25
datadog,Here are dependencies which I added:,0.0,0.0,1.0,0.0
datadog,Here are plugins enabled in  build.sbt :,0.0,0.0,1.0,0.0
datadog,Then  application.conf :,0.0,0.0,1.0,0.0
datadog,Finally in the  Main  class I invoke:,0.0,0.0,1.0,0.0
datadog,On EC2 I installed datadog-agent for docker.,0.0,0.0,1.0,0.0
datadog,"When I run the service container on the EC2 instance and then look into the DataDog interface I don't see any related metrics to akka, just a list of standard metrics like:  datadog.process.agent ,  docker.cpu.usage ,  system.io.await  etc",0.0,0.072,0.928,0.3612
datadog,How to enable akka related metrics in case when an akka app is packaged into docker and deployed on EC2?,0.0,0.0,1.0,0.0
datadog,I've a 4 node kafka cluster in my production where we are using custom partitioner which does mod 64 of an id to determine the partition.,0.0,0.0,1.0,0.0
datadog,"since last week, there has been imbalanced kafka messages_in rate on 1 of our nodes as can been seen in the graph attached .",0.0,0.0,1.0,0.0
datadog,The pink line shows the message in rate on kafka01 node and bluish yellow line shows the message in rate on all other 3 boxes .,0.0,0.0,1.0,0.0
datadog,I'm using datadog for monitoring and using the metric kafka.messages_in.rate .,0.0,0.0,1.0,0.0
datadog,"Assuming that there has been no change in the id distribution , there should have been no change in distribution of message in rate .",0.173,0.0,0.827,-0.5267
datadog,Steps I've taken to debug the issue are,0.0,0.0,1.0,0.0
datadog,Requesting any help or areas/metrics one can look into to debug this anomaly.,0.0,0.184,0.816,0.4019
datadog,"For people who are searching about this in future
 https://mail-archives.apache.org/mod_mbox/kafka-users/201710.mbox/%3CCALaekbwkSKapqPwsyuAoHGiSnc1+3jF2wF+2FDZbAVx61E+c2w@mail.gmail.com%3E",0.0,0.0,1.0,0.0
datadog,"I am working on a project where i need to display the database mssql server's performance metrics for example memory consumed/free, storage free space etc.",0.0,0.136,0.864,0.5106
datadog,I have researched for this purpose and one thing came up was  DOGSTATSD .,0.0,0.0,1.0,0.0
datadog,Datadog  provides the library for .net project to get custom metrics but that was not the solution for me because the metrics works on datadog website.,0.089,0.0,0.911,-0.3491
datadog,"I have to display the all (in graph or whatever suited) data, received from MSSQL SERVER.",0.0,0.0,1.0,0.0
datadog,There will be multiple servers/instances.,0.0,0.0,1.0,0.0
datadog,"Is there a way to do that, our WebApp connected with multiple databases and we receive/display information.",0.0,0.0,1.0,0.0
datadog,I cannot use already available tools for the insights.,0.0,0.0,1.0,0.0
datadog,I got error with the latest heapster version:  v1.5.1 .,0.278,0.0,0.722,-0.4019
datadog,I've described in detail in this github issue link:  https://github.com/kubernetes/heapster/issues/1969,0.0,0.0,1.0,0.0
datadog,The error message:,0.574,0.0,0.426,-0.4019
datadog,Anybody knows how to solve it?,0.0,0.265,0.735,0.2023
datadog,Perhaps someone who already successfully integrated the heapster to datadog statsd agent in Kubernetes?,0.0,0.198,0.802,0.4939
datadog,Thanks before,0.0,0.744,0.256,0.4404
datadog,Is there any way to send matrics graph over email for a specific period of time from AWS Cloudwatch or by using datadog.,0.0,0.0,1.0,0.0
datadog,I want to send ec2 system check matrics for a specific time period.,0.0,0.115,0.885,0.0772
datadog,Thanks in advance.,0.0,0.592,0.408,0.4404
datadog,Is there a way to alert / detect in elasticsearch when the primary shard and replica shard land up on the same data node ?,0.0,0.095,0.905,0.296
datadog,May be via datadog or any other way ?,0.0,0.0,1.0,0.0
datadog,Looking for an automatic way not manual way like monitoring through head plugin etc.,0.14,0.0,0.86,-0.2755
datadog,I am on a RHEL system and I would like to add the following parameters so that I can have my datadog + docker integration as described here ( https://github.com/DataDog/docker-dd-agent#cgroups ).,0.0,0.094,0.906,0.3612
datadog,I need to set the following kernel parameters:,0.0,0.0,1.0,0.0
datadog,cgroup_enable=memory swapaccount=1,0.0,0.0,1.0,0.0
datadog,I was planning to use something like:,0.0,0.333,0.667,0.3612
datadog,cgset -r cgroup_memory=enable,0.0,0.0,1.0,0.0
datadog,but I get the error  wrong -r  parameter (cgroup_enable=memory),0.562,0.0,0.438,-0.8271
datadog,I am planning to setup a 80 nodes cassandra cluster (current version 2.1 but will upgrade to 3 in future).,0.0,0.0,1.0,0.0
datadog,I have gone though  http://graphite.readthedocs.io/en/latest/tools.html  which has list of tools that graphite supports.,0.0,0.185,0.815,0.3612
datadog,I want to decide which tools to choose as listener and storage so that it could scale.,0.0,0.08,0.92,0.0772
datadog,As a listener should i use the default carbon or should i choose graphite-ng ?,0.0,0.0,1.0,0.0
datadog,"However as storage component, i am confused that whether default whisper is enough?",0.173,0.0,0.827,-0.3182
datadog,"Or should I look at ohter option (like Influxdata,cynite or some rdms db (postgres/mysql))?",0.0,0.0,1.0,0.0
datadog,As gui component i have finalized to use grafana for better visulization.,0.0,0.225,0.775,0.4404
datadog,I think datadog + grafana will work fine but datadog is not opensource.So Please suggest an opensource scalable up to 100 cassandra nodes alternative.,0.102,0.059,0.839,-0.26
datadog,It seems like we ran into a OutOfMemoryError: Metaspace before actually running out of available memory for that pool.,0.0,0.128,0.872,0.3612
datadog,"More specifically, we appeared to hit that error as soon as the  committed  amount for that pool reached the maximum, instead of when the  used  amount did.",0.089,0.116,0.795,-0.0516
datadog,Here's the setup:,0.0,0.0,1.0,0.0
datadog,"We have a Jenkins server running on Oracle Java 8 update 121, and have the following metaspace arguments  -XX:MetaspaceSize=10G -XX:MaxMetaspaceSize=10G .",0.137,0.0,0.863,-0.4019
datadog,We also have Datadog monitoring heap and non-heap pools.,0.0,0.0,1.0,0.0
datadog,We hit an issue where the Jenkins log indicated that some thread threw an OutOfMemoryError: Metaspace.,0.0,0.0,1.0,0.0
datadog,"However, in Datadog at the time of incident, the amount of non-heap used is shown to be very low (graph below).",0.107,0.0,0.893,-0.3384
datadog,"At first I thought Datadog might be measuring it wrong, but using jconsole I get matching results for current usage (I didn't have jconsole open at the time of incident).",0.071,0.0,0.929,-0.2617
datadog,"My only other conclusion was that the error originated from trying to allocate more  committed  metaspace, even though there's still plenty of gap between that and the used amount.",0.084,0.075,0.841,-0.079
datadog,Am I missing something about how these memory pools are supposed to work?,0.167,0.0,0.833,-0.296
datadog,"Note:  I'm perfectly aware that this is a pretty large metaspace to have to begin with, and that we likely have a classloader leak somewhere.",0.081,0.248,0.671,0.7184
datadog,This is something we hit while trying to investigate that leak.,0.194,0.0,0.806,-0.34
datadog,Does anyone know why we are experiencing on our kubernetes master node some system load peaks.,0.0,0.0,1.0,0.0
datadog,I thought that the master node is not doing anything except monitoring our agent nodes.,0.0,0.0,1.0,0.0
datadog,"Each time we have a system load peak of 1.8-2 on our dual-core machine, I see in the kube-controller-manager log that the master tries to start 3 things:",0.0,0.0,1.0,0.0
datadog,Our kubernetes version is 1.4.6 and is created via the azure portal.,0.0,0.154,0.846,0.25
datadog,The system peaks can we see via datadog monitoring.,0.0,0.0,1.0,0.0
datadog,I'm using Datadog to collect metrics from Kafka running on my localhost.,0.0,0.0,1.0,0.0
datadog,When I run the -info command on my Datadog agent this is the error I get for Kafka.,0.153,0.0,0.847,-0.4019
datadog,Any ideas whats causing this?,0.0,0.0,1.0,0.0
datadog,I have some doubts regarding monitoring nexus OSS 3.0.1 server.,0.216,0.0,0.784,-0.296
datadog,Can some please let me know the following:-,0.0,0.247,0.753,0.3182
datadog,I have an application hosted in openshift.,0.0,0.0,1.0,0.0
datadog,Now I want figure out how many request can handle in order to check the speed and availability.,0.0,0.075,0.925,0.0772
datadog,So my first attempt will be generate a multiple HTTP GET requests to my Rest Service(made in python and hosted in openshift).,0.0,0.0,1.0,0.0
datadog,My fear is can get my IP workplace banned regarding this looks like an attack.,0.408,0.11,0.482,-0.7783
datadog,"In the other hand I see there are tools like  New Relic  or  DataDog  to check metrics, but I don't know if I can simulate http requests and then check the response times.",0.0,0.121,0.879,0.431
datadog,I finally wrote to Openshift support and they told me I can simulate http requests without worries.,0.0,0.279,0.721,0.6164
datadog,"I'm adding logging to my application, and was wondering what the correct way is to add a tag that may or may not have a value.",0.081,0.0,0.919,-0.2584
datadog,My code is,0.0,0.0,1.0,0.0
datadog,The  config  is a dict of configuration variables constructed from another class.,0.0,0.0,1.0,0.0
datadog,Is it okay to pass objects that might not have a value like the  sometimes_here  config element?,0.218,0.1,0.683,-0.3063
datadog,Or do I need to build the tags list separately like,0.0,0.217,0.783,0.3612
datadog,"Since there might potentially be even more optional elements in config, I'm hoping that the more compact code will be suitable, but I can't find anything in the Datadog documentation on what it would do about those tags.",0.0,0.05,0.95,0.2263
datadog,"We have the below Category processor pipeline for Lambdas such that when there is a log with (@error.kind:* or @error.message:*) , the logs will be defined as an error.",0.094,0.0,0.906,-0.4019
datadog,"To test above Category process rule, I have create a simple Lambda called python-test as below",0.0,0.139,0.861,0.2732
datadog,"When I run this lambda with below test event, I don’t see the log messages being converted to error",0.144,0.0,0.856,-0.4019
datadog,"Eg: In Datadog as you can see below,
it still remains as log ie.",0.0,0.0,1.0,0.0
datadog,"it is not converted to error (ErrorType –error.kind is not set)
Can you please advise If I am missing something?",0.097,0.2,0.703,0.3309
datadog,I'm trying to implement distributed tracing in my kotlin app using spring cloud sleuth.,0.0,0.0,1.0,0.0
datadog,I'm sending those data to the datadog.,0.0,0.0,1.0,0.0
datadog,Now I'm able to trace my logs but I want to add some extra data to spans.,0.0,0.088,0.912,0.1154
datadog,Let's say I want to add info about user and be able to see it in datadog.,0.0,0.08,0.92,0.0772
datadog,Am I right that span tags are good for it?,0.0,0.266,0.734,0.4404
datadog,I'm sending the logs in json format to datadog but I cannot add tags here.,0.0,0.0,1.0,0.0
datadog,(traceId and spanId are injected).,0.0,0.0,1.0,0.0
datadog,Logback config:,0.0,0.0,1.0,0.0
datadog,gradle:,0.0,0.0,1.0,0.0
datadog,and to add the tag I'm trying,0.0,0.0,1.0,0.0
datadog,example log:,0.0,0.0,1.0,0.0
datadog,shouldn't be that 'user' injected into MDC and then into logs?,0.0,0.0,1.0,0.0
datadog,I'm trying to configure spring actuator metrics along with micrometer to be sent to Datadog stastd agent.,0.0,0.0,1.0,0.0
datadog,"Still, I'd like to get them all sent with a tag, so that I can filter in my Datadog dashboard just my service metrics, and not considering other services metrics.",0.0,0.085,0.915,0.3612
datadog,I've added:,0.0,0.0,1.0,0.0
datadog,"to my service metrics configuration, but I can't see this tag value in Datadog dashboard.",0.0,0.193,0.807,0.4767
datadog,I'm not seeing anything weird in app logs nor actuator logfile neither.,0.0,0.121,0.879,0.1326
datadog,"I have nothing else regarding metrics in my service, as I don't want to implement custom metrics, just want to use the one provided by actuator.",0.1,0.0,0.9,-0.1139
datadog,This is how the whole metrics configuration looks like:,0.0,0.238,0.762,0.3612
datadog,Versions:,0.0,0.0,1.0,0.0
datadog,micrometer version: 1.6.4,0.0,0.0,1.0,0.0
datadog,actuator version: 2.4.3,0.0,0.0,1.0,0.0
datadog,spring version: 2.3.8,0.0,0.0,1.0,0.0
datadog,Any clue about what I could be missing to get the tag reaching Datadog?,0.147,0.12,0.733,-0.1027
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,I'm exploring the Open Telemetry Collector Project and how this could work with containerized .NET Core apps (or any other apps for that matter).,0.0,0.0,1.0,0.0
datadog,"Currently we're using DynaTrace at the company I work for, which requires the DynaTrace 'OneAgent' agent to be installed on hosts.",0.0,0.0,1.0,0.0
datadog,The DynaTrace agent somehow hooks into the dotnet CLR and does bytecode/MSIL instrumentation.,0.0,0.0,1.0,0.0
datadog,Basically this approach allows us to capture APM data to DynaTrace without having to do any code changes in our apps whatsoever.,0.0,0.0,1.0,0.0
datadog,"Contrast this with the Open Telemetry approach, which (as far as I can tell) requires additional (nuget) packages to be installed into the services we want instrumented.",0.0,0.049,0.951,0.0772
datadog,"In .NET Core land, I suspect this is using DiagnosticSource based instrumentation, which I would describe as a type of AOP.",0.115,0.0,0.885,-0.296
datadog,"That said, it's mostly automatic and integrated into various .NET libraries/frameworks such as ASP.NET, Entity Framework, etc; so the only code changes are; a) installing the Open Telemetry nuget packages, b) some basic Startup.cs configuration, and c) optionally adding additional spans if/when needed.",0.0,0.0,1.0,0.0
datadog,"It's minimal code, but it's NOT no-code like the DynaTrace approach.",0.21,0.0,0.79,-0.395
datadog,I'd also assume the granularity of spans to be a lot courser than bytecode/MSIL instrumentation approach used by DynaTrace.,0.0,0.0,1.0,0.0
datadog,WRT the Open Telemetry Collecter; I really like the fact that we can configure exporters to send instrumentation data to any supported third party monitoring solution (e.g.,0.0,0.314,0.686,0.8439
datadog,"DataDog, Elastic, Kafka, etc) without having to install any proprietary agents.",0.0,0.0,1.0,0.0
datadog,IMO this approach means we can more easily change the monitoring service(s) we might be using and therefore mitigates vendor lock.,0.0,0.119,0.881,0.4005
datadog,I'm hoping I can find a way to get the best of both worlds;,0.0,0.412,0.588,0.7906
datadog,Is this possible?,0.0,0.0,1.0,0.0
datadog,I was looking at how the DataDog agent works  here .,0.0,0.0,1.0,0.0
datadog,It looks like this might be a similar approach to DynaTrace.,0.0,0.217,0.783,0.3612
datadog,"For DataDog, it looks like you need to set some some CLR environment variables in the app container - presumably this extends the CLR to send bytecode/MSIL instrumentation data to the DataDog agent (running as a sidecar or something)...",0.0,0.105,0.895,0.481
datadog,"Would we be able to use this DataDog approach, but instead of sending the instrumentation data to a DataDog agent, we send it instead to an Open Telemerty Collecter agent?",0.0,0.0,1.0,0.0
datadog,Technically the Open Telemerty Collecter agent could still send to DataDog if we wanted (i.e.,0.0,0.0,1.0,0.0
datadog,"using the  DataDog exporter ), but at least we'd mitigate the vendor lock using this approach.",0.0,0.0,1.0,0.0
datadog,Thoughts...,0.0,0.0,1.0,0.0
datadog,Our team has a production Django app that's been experiencing latency spikes throughout the day.,0.0,0.0,1.0,0.0
datadog,"Drilling into the requests during those time periods in Datadog,  the recurring theme is that the Django middleware is running very slowly , which you can see in the flame graph.",0.0,0.0,1.0,0.0
datadog,"Compared to normal-latency periods, that's the main difference in performance, and this happens for all endpoints.",0.0,0.0,1.0,0.0
datadog,You can see from the Heroku metrics that it occurs in an almost regular pattern.,0.0,0.0,1.0,0.0
datadog,We haven't added any new middleware recently.,0.0,0.0,1.0,0.0
datadog,"Has anyone seen this before, or have any guidance on what could be causing this issue with the middleware?",0.0,0.0,1.0,0.0
datadog,"I'm using scala, sbt, sbt-native-package, and potentially sbt-java-agent to conditionally activate a datadog java agent at runtime w/ kubernetes.",0.0,0.0,1.0,0.0
datadog,"By adding the  dd-java-agent  as a dependency and adding a script snippet, I'm able to activate datadog only when a specific env.",0.0,0.0,1.0,0.0
datadog,"variable is set, but this is also adding the dd-java-agent to the classpath, which I'm trying to avoid:",0.141,0.0,0.859,-0.4215
datadog,"Is there a way to have sbt manage the downloading of dd-java-agent.jar, include this jar in the  lib  directory (or a different directory if that's what it takes), but exclude from classpath?",0.075,0.0,0.925,-0.3291
datadog,"I've tried using  sbt-java-agent  which puts the jar in a  dd-java-agent  directory and excludes the jar from the classpath, but I can not figure out how to wrap the  addJava  statement in an  if  check when using that plugin.",0.0,0.0,1.0,0.0
datadog,Thanks for any help you can provide!,0.0,0.541,0.459,0.7088
datadog,"I’m integrating Istio v1.8.0 with DataDog and their integrations works well, Im getting most of the metrics.",0.0,0.116,0.884,0.2732
datadog,"But, I’m not getting tcp related metrics for my services:",0.0,0.0,1.0,0.0
datadog,"My services communicate using HTTP with each other, but I want to see tcp metrics as well for open connections to my envoy-proxy and other tcp related metrics.",0.0,0.141,0.859,0.4767
datadog,Are there any metrics of this kind that I can use?,0.0,0.308,0.692,0.6124
datadog,I don't see any here -  https://istio.io/latest/docs/reference/config/metrics/,0.0,0.0,1.0,0.0
datadog,I'm trying Migrating from DataDog to prometheus.,0.0,0.0,1.0,0.0
datadog,and this one query i'm having some difficulties with.,0.216,0.0,0.784,-0.296
datadog,I'm unable to find the pct_change functionality available in DD in prometheus.,0.0,0.0,1.0,0.0
datadog,I tried using (A-B)/B or A/B but they don't work.,0.0,0.0,1.0,0.0
datadog,not appropriate results.,0.0,0.0,1.0,0.0
datadog,e.g.,0.0,0.0,1.0,0.0
datadog,But it's not correct.,0.0,0.0,1.0,0.0
datadog,The alert fires very often.,0.0,0.355,0.645,0.296
datadog,Any leads on this would be appreciated.,0.0,0.355,0.645,0.5106
datadog,How can I write the PromQl alternative of DD?,0.0,0.0,1.0,0.0
datadog,We are using Datadog and AWS ECS.To collect ECS host network related metrices in  DataDog we are following below documentation,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/network_performance_monitoring/installation/?tab=docker,0.0,0.0,1.0,0.0
datadog,I was able to add all mentioned parms in DataDog agent container definition except below,0.0,0.0,1.0,0.0
datadog,I tried to update same in my ecs-taskdefination cloudformation,0.0,0.0,1.0,0.0
datadog,but getting below error when deploying cloudformation,0.372,0.0,0.628,-0.5499
datadog, Model validation failed (#/conatinerdefination/0/Privilaged: expected type: Boolean found: string),0.292,0.0,0.708,-0.5106
datadog,"We have an electron app that connects to our servers on AWS (ALB, Fargate, lambdas) its a meteor based app (ddp, socks underneath) for the past month we've seen lots of XHR error GET on our datadog logs, it seems it's not disrupting our service, but it's really polluting all of our logs as we get a lot of those, could it be its timing out due to ALB not letting the connection in?",0.025,0.0,0.975,-0.2144
datadog,"it's odd as I stated our users haven't reported any connection issues and are able to use the app, we have ports 80, 443 open and redirect any calls from Http to Https, so not really sure what would be causing the error.",0.16,0.0,0.84,-0.7538
datadog,Do XHR sockjs are using a different port?,0.0,0.0,1.0,0.0
datadog,"We assume it has to be something on our network but we haven't figured it out, we don't experience any of these errors when we use our app locally on development mode, only happens if we have the electron app running and we turn off our server instance, as the connection cannot be established.",0.055,0.0,0.945,-0.4767
datadog,Any insights would be great!,0.0,0.523,0.477,0.6588
datadog,thanks!,0.0,1.0,0.0,0.4926
datadog,I'm literarlly new to DataDog.,0.0,0.0,1.0,0.0
datadog,I have been assigned into a project where they have set up DataDog to log everything within the code.,0.0,0.0,1.0,0.0
datadog,After spending 1hr or so trying to do something like:,0.0,0.217,0.783,0.3612
datadog,"I concluded that to do a simple &quot;count by&quot;, I need to go over the process of creating a Processor, then a Metric.",0.0,0.115,0.885,0.296
datadog,Really is that cumbersome?,0.0,0.0,1.0,0.0
datadog,I miss so much SUMO Logic simplicity for this scenario.,0.167,0.0,0.833,-0.1531
datadog,"In my company, we have Datadog dashboards and monitors which we often make changes to and thus we wish to have some version control.",0.0,0.105,0.895,0.4019
datadog,After discussions it was decided that Terraform is the way to go with this.,0.0,0.0,1.0,0.0
datadog,I've successfully been able to manually back up our current Datadog infrastructure using Google's  Terraformer  (to import the infrastructure as resources) and Terraform's CLI commands to import the corresponding infrastructure's states.,0.0,0.096,0.904,0.4939
datadog,It is also possible to make changes to the resources through commands such as  terraform plan  and  terraform apply .,0.0,0.0,1.0,0.0
datadog,The manual changes to the resources can then be backed up with Git.,0.0,0.084,0.916,0.0258
datadog,Now to the issue I'm having.,0.0,0.0,1.0,0.0
datadog,"We wish to automate the the back-up of our resources, that is, through making changes in the Datadog GUI and to have those changes be reflected in our resource files.",0.0,0.085,0.915,0.4019
datadog,That means the other way around of what I've managed to accomplish above.,0.0,0.189,0.811,0.4215
datadog,"It is rather straight forward to do this manually, where one imports the infrastructure resources that have been changed using the GUI (using terraformer).",0.0,0.076,0.924,0.2263
datadog,"If &quot;conflicts&quot; arise, one needs to manually remove certain resources and re-import them which I've found difficult to automate.",0.116,0.097,0.787,-0.1027
datadog,My question is: is there a straight forward way of automating back-up of changes made &quot;remotely&quot; (e.g.,0.0,0.112,0.888,0.2263
datadog,in the Datadog GUI) to resource files?,0.0,0.0,1.0,0.0
datadog,My research for Terraform CLI's for this purpose has lead me to nothing thus far.,0.0,0.0,1.0,0.0
datadog,Any suggestions are appreciated!,0.0,0.545,0.455,0.5562
datadog,Thanks,0.0,1.0,0.0,0.4404
datadog,"Yesterday my JVM application was broken due to high CPU usage, when checking the root cause this is because GC stop the world.",0.202,0.0,0.798,-0.6486
datadog,"The GC require more time to clean up the memory, this is because I introduce new LRU cache at the app.",0.0,0.135,0.865,0.4522
datadog,So my question is:,0.0,0.0,1.0,0.0
datadog,Is there any way to track how big memory group by JVM class?,0.0,0.0,1.0,0.0
datadog,"For example, I have 3 classes,  Foo ,  Bar ,  Baz .",0.0,0.0,1.0,0.0
datadog,I want to know how big memory used by each class at run time.,0.0,0.098,0.902,0.0772
datadog,"I have a Datadog account, so I plan to use tools to send those metrics to Datadog.",0.0,0.0,1.0,0.0
datadog,I'm trying to import a 3rd party library (datadog) for use with a glue shell script and I'm running into issues.,0.0,0.13,0.87,0.4019
datadog,"I've packaged the file as a .egg and given the path to it in the glue job, as instructed  here .",0.0,0.0,1.0,0.0
datadog,This ends up throwing an error saying zipimport.ZipImportError: not a Zip file: '/tmp/glue-python-libs/datadog.egg'.,0.197,0.0,0.803,-0.4019
datadog,"When I try using a zip file instead, it throws ModuleNotFoundError: No module named 'datadog'.",0.155,0.0,0.845,-0.296
datadog,How do I go about importing the library?,0.0,0.0,1.0,0.0
datadog,"Currently have some task-based automation for ECS that run on a scheduled basis, however sometimes there is a need to run only run task or re-run tasks for only a certain kinds of tasks (for example sql tasks or datadog tasks).",0.0,0.054,0.946,0.2732
datadog,"I know this can be done via console, but it's inefficient.",0.0,0.0,1.0,0.0
datadog,Was thinking of a bash script that calls to start a task from a CLI.,0.0,0.0,1.0,0.0
datadog,"Currently I know I can do this with the AWS CLI using '--task-definition', but it's not much better.",0.172,0.0,0.828,-0.4782
datadog,"I don't usually write scripts, so I'm basically here to help with brainstorming.",0.0,0.197,0.803,0.4019
datadog,I'm wondering if there is a way to make an API call to start tasks.,0.0,0.0,1.0,0.0
datadog,Would I need to type in the ARN every time?,0.0,0.0,1.0,0.0
datadog,Can I just list the tasks on the AWS CLI and have the exported to the script?,0.0,0.0,1.0,0.0
datadog,Would network-config need to be hard-coded?,0.0,0.0,1.0,0.0
datadog,Thanks!,0.0,1.0,0.0,0.4926
datadog,I use  Laravel Horizon  to process jobs in my Laravel application with the help of Supervisor.,0.0,0.162,0.838,0.4019
datadog,The whole setup runs on docker in ECS.,0.0,0.0,1.0,0.0
datadog,"On the host node of ECS (managed by Fargate) I have datadog agents running on it, which will grab the stdout/stderr of the container and put it into datadog (so I can access the logs centrally).",0.0,0.0,1.0,0.0
datadog,What I noticed is that I am not getting any logs out of the jobs that is processing.,0.0,0.0,1.0,0.0
datadog,"Even simple  print(""test"");  in the job code does not go to the stdout.",0.0,0.0,1.0,0.0
datadog,Here's my setup:,0.0,0.0,1.0,0.0
datadog,Dockerfile,0.0,0.0,1.0,0.0
datadog,The service definition for this in yaml looks like this:,0.0,0.217,0.783,0.3612
datadog,And here's the content of my  horizon.conf :,0.0,0.0,1.0,0.0
datadog,Any idea what I might be missing?,0.306,0.0,0.694,-0.296
datadog,What can't I get logs out of the jobs processed by horizon in docker via supervisor?,0.0,0.0,1.0,0.0
datadog,I've installed the datadog app on my linux vm but i can't seem to read the datadog.yaml agent file.,0.0,0.0,1.0,0.0
datadog,[ Error reading /etc/datadog-agent/datadog.yaml: Permission denied ],0.651,0.0,0.349,-0.6808
datadog,"My linux box is hosted on GCP, do i need to configure permissions?",0.0,0.0,1.0,0.0
datadog,I have a Spring Boot (2.2.6) application that uses Log4j2 (with Slf4j).,0.0,0.0,1.0,0.0
datadog,Log4j is configured to use the json layout and in the end I want to ingest the logs in Datadog.,0.0,0.067,0.933,0.0772
datadog,For that the 'serviceName' is important as a field in the json.,0.0,0.153,0.847,0.2023
datadog,Now according to the log4j docu ( https://logging.apache.org/log4j/2.x/manual/layouts.html#JSONLayout ) one can add a custom field with the 'KeyValuePair' tags and that works.,0.0,0.0,1.0,0.0
datadog,Unfortunately this breaks the normal structure of the spring logs.,0.211,0.0,0.789,-0.34
datadog,Log4j2.xml config:,0.0,0.0,1.0,0.0
datadog,Log w/o custom field:,0.0,0.0,1.0,0.0
datadog,log w/ custom field:,0.0,0.0,1.0,0.0
datadog,"Spring docu mentions how this might work with logback, but not log4j ( https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-custom-log-configuration , end of chapter)",0.0,0.0,1.0,0.0
datadog,I've searched but couldn't find anything useful.,0.341,0.0,0.659,-0.4782
datadog,Any ideas how i can add a custom field to the json log while still preserving all fields coming from Spring?,0.0,0.0,1.0,0.0
datadog,"Thanks, 
Felix",0.0,0.744,0.256,0.4404
datadog,I want to expose all metrics on the metrics endpoint but publish some of them to a remote meter registry.,0.07,0.062,0.867,-0.0387
datadog,"For doing so, I have a SimpleMeterRegistry for the metrics endpoint and added a  MeterRegistryCustomizer  for the remote meter registry(Datadog) to add some  MeterFilter  to avoid specific metrics using  MeterFilter's DENY  function.",0.165,0.0,0.835,-0.6523
datadog,For example :,0.0,0.0,1.0,0.0
datadog,"However, all jvm related metrics are visible in Datadog.",0.0,0.0,1.0,0.0
datadog,I tried  MeterFilterReply  but no use.,0.412,0.0,0.588,-0.4215
datadog,Please suggest how this can be achieved.,0.0,0.277,0.723,0.3182
datadog,I have a climate control automation tool running with Home Assistant.,0.0,0.0,1.0,0.0
datadog,"Hass supports many types of long-term db storage of it's entity states (sensor data etc), like datadog, influbdb, graphite etc.",0.0,0.217,0.783,0.6124
datadog,So far I've tried influxdb and graphite.,0.0,0.0,1.0,0.0
datadog,I've been using grafana to visualize the data.,0.0,0.0,1.0,0.0
datadog,"I want to store threshold values in the database, such as min/max temperature.",0.0,0.286,0.714,0.4588
datadog,These temperatures can be set using an input slider on the hass UI.,0.0,0.0,1.0,0.0
datadog,"Once set, these controls can be left for days, even weeks.",0.0,0.0,1.0,0.0
datadog,So there may be only one data point in the db for very long periods of time.,0.0,0.0,1.0,0.0
datadog,"If I want to display these on grafana, they disappear from the time range being looked at pretty quickly, and grafana simply removes the entity from the graph.",0.062,0.148,0.789,0.3818
datadog,"Influx has a ""use previous value"", and graphite has a ""keepLastValue"" function that I thought I could use to pull the last value of the threshold from the db, but in both cases, the values must exist in the time range selected.",0.0,0.127,0.873,0.6428
datadog,"If the previous value for the control was days before the time range, too bad, so sad.",0.314,0.1,0.586,-0.7262
datadog,"I thought this would be a very common requirement, but perhaps not.",0.0,0.0,1.0,0.0
datadog,Does anyone know a combination of database and dashboard that can display the last value for an entity even if said last value was recorded far out of the time range selected?,0.0,0.142,0.858,0.5859
datadog,So I'm sending my Nginx access logs to Datadog (an APM solution).,0.0,0.0,1.0,0.0
datadog,My log format looks like this,0.0,0.333,0.667,0.3612
datadog,I can extract the url from the  referrer  field and it looks like this,0.0,0.172,0.828,0.3612
datadog,I only want  /foo/bar  though.,0.0,0.302,0.698,0.0772
datadog,Is this something I have to modify in the  log_format ?,0.0,0.0,1.0,0.0
datadog,"I saw an example from datadog docs where they're able to extract a url path attribute, but there's no example config.",0.135,0.0,0.865,-0.4215
datadog,I am trying to install the chart  stable/efs-provisioner  and I would like to apply an annotation so that the deployment is correctly tagged in datadog.,0.0,0.102,0.898,0.3612
datadog,"Datadog requires the  annotation :  ad.datadoghq.com/tags: '{""env"": ""staging""}'",0.0,0.0,1.0,0.0
datadog,"I have tried various incantations of the following, but I keep getting the error below.",0.228,0.0,0.772,-0.5499
datadog,Error:,1.0,0.0,0.0,-0.4019
datadog,Tomcat getting hang with high load.,0.0,0.0,1.0,0.0
datadog,Tomcat threads DB connections are showing a sudden increase.,0.0,0.247,0.753,0.3182
datadog,ex: tomcat threads reach 1000 from 200 within a minute.,0.0,0.121,0.879,0.0258
datadog,normally tomcat thread shows 150 - 250 stable pattern,0.0,0.239,0.761,0.296
datadog,We have checked all application metrics and JVM metrics.,0.0,0.0,1.0,0.0
datadog,But cannot identifies the root cause for this sudden instability.,0.0,0.0,1.0,0.0
datadog,below is the list that we have checked.,0.0,0.0,1.0,0.0
datadog,"App - tomcat thread, tomcat CPU, tomcat memory, server memory, server CPU",0.0,0.0,1.0,0.0
datadog,"DB - CPU,process,connections,memory",0.0,0.0,1.0,0.0
datadog,JVM - GC calls,0.0,0.0,1.0,0.0
datadog,TomcatThreadGraph-Nagios -  https://imgur.com/NItgPjp,0.0,0.0,1.0,0.0
datadog,TomcatThreadGraph-DataDog -  https://imgur.com/4RH570B,0.0,0.0,1.0,0.0
datadog,"I have  elastic-search cluster which hosts more than 15 indices, I have a Datadog integration which shows me the below view of my elastic-search cluster.",0.0,0.0,1.0,0.0
datadog,We have alert integration with DD(datadog) which gives us alert if overall CPU usage goes beyond 60% and also in our application we start getting alerts when  elasticsearch cluster is under stress  as in this case our response time increases beyond a configures threshold.,0.059,0.093,0.847,0.1531
datadog,"Now my problem is how to know which indices are consuming the ES cluster resources most, so that we can fine either throttle the request from those indices or optimize their requests.",0.074,0.136,0.79,0.3182
datadog,Some things which we did:,0.0,0.0,1.0,0.0
datadog,So my problem is very simple and all I want some metric from DD or elastic which can easily tell me which indices are consuming the most resources on a elastic-search cluster.,0.088,0.11,0.802,-0.0717
datadog,"I've started a new project, and am building my first micro(ish) service.",0.0,0.0,1.0,0.0
datadog,"It has a  /health  endpoint, which currently just replies with HTTP status  200  and a  { ""status"": ""OK"" }  body.",0.0,0.0,1.0,0.0
datadog,"My service relies on several other (external) services, and I was wondering if there's a standard I can follow for exposing information about the connection between my services and the external ones?",0.07,0.0,0.93,-0.2732
datadog,I'd like to not invent my own patterns here if there are industry best practices or standards I can follow.,0.0,0.283,0.717,0.7717
datadog,Something like:,0.0,0.714,0.286,0.3612
datadog,"My service will be running in Kubernetes on GCP, if that helps.",0.0,0.191,0.809,0.3818
datadog,We use Datadog for observability.,0.0,0.0,1.0,0.0
datadog,I need to identify in my infrastructure which hosts have tag1 and tag2 and tag3.,0.0,0.0,1.0,0.0
datadog,I'm new to datadog but it seems that when filtering I have to specify a value for a specific tag.,0.0,0.162,0.838,0.4767
datadog,"I also need to identify the inverse, list hosts that have tag1 but are missing tag2 OR tag3.",0.149,0.0,0.851,-0.4215
datadog,"I have setup a dashboard for each of the tags, I seem to be limited by up 2 tags.",0.119,0.0,0.881,-0.2263
datadog,"e.g filter by -  env:dev 
    group by -  tag1 , tag2",0.0,0.0,1.0,0.0
datadog,I would expect to be able to see what hosts have tag1 AND tag2 AND tag3,0.0,0.0,1.0,0.0
datadog,And the inverse -  what hosts do not have all 3 tags.,0.0,0.0,1.0,0.0
datadog,Our project is responsible for migrating data from one system to another.,0.0,0.173,0.827,0.3182
datadog,"We are going to run transformation, validation and migration scripts using Jenkins.",0.0,0.0,1.0,0.0
datadog,It's unclear for me how to aggregate logs from several Jobs or Pipelines in Jenkins.,0.125,0.0,0.875,-0.25
datadog,How it can be done?,0.0,0.0,1.0,0.0
datadog,We'll rely on logs heavily to identify any issues found during validation etc.,0.0,0.0,1.0,0.0
datadog,In terms of our planned setup we'll have AWS EC2 instances + we can use Datadog (our company uses it).,0.0,0.0,1.0,0.0
datadog,Can we use Datadog for this purpose?,0.0,0.0,1.0,0.0
datadog,I've executed this query on production with load on the server.,0.0,0.0,1.0,0.0
datadog,It took 35 minutes according to the datadog.,0.0,0.0,1.0,0.0
datadog,this  table1  has around 100 million rows.,0.0,0.0,1.0,0.0
datadog,Is 35 minutes normal?,0.0,0.0,1.0,0.0
datadog,Is there any way I can execute such simple migrations (adding a nullable column) without locking the table?,0.0,0.0,1.0,0.0
datadog,I have created a windows cmd file that calls three independent bat files.,0.0,0.167,0.833,0.25
datadog,I want to create a windows task that calls this cmd file and runs every 5 minutes.,0.0,0.221,0.779,0.34
datadog,The problem is that this task runs perfectly fine only when I'm logged into the system.,0.124,0.276,0.599,0.5106
datadog,"But I'm unable to make this task continue to run ""whether I'm logged in or not"" .",0.0,0.0,1.0,0.0
datadog,I even asked my colleague to login to that machine and run this task under his account - it worked.,0.0,0.0,1.0,0.0
datadog,"I created a local admin user on that machine, logged in as that user, tried to run this task - it did not work - the script waits forever while post_results.bat.",0.0,0.071,0.929,0.25
datadog,I even tried to schedule a jenkins job that basically does the same thing - it did not work - the jenkins job waits forever while post_results.bat (I killed the jenkins job after waiting for ~20 min).,0.123,0.0,0.877,-0.6705
datadog,Here is a summary of what these tasks are doing:,0.0,0.0,1.0,0.0
datadog,run_all.cmd,0.0,0.0,1.0,0.0
datadog,run_test.bat  - executes a jmeter script,0.0,0.0,1.0,0.0
datadog,post_results.bat  - calls a python script that posts the jmeter test results to datadog,0.0,0.0,1.0,0.0
datadog,post_jmeter_results_to_datadog.py  - uses the datadog python api to post metrics to datadog,0.0,0.0,1.0,0.0
datadog,clean.bat  - deletes the jmeter test result files,0.0,0.0,1.0,0.0
datadog,All I need is to be able to run this task every 5 minutes.,0.0,0.0,1.0,0.0
datadog,If anyone is able to see what I'm doing wrong and points that out...,0.193,0.0,0.807,-0.4767
datadog,I'd be really grateful.,0.0,0.523,0.477,0.5095
datadog,We have a cookbook with multiple recipes where we select features.,0.0,0.0,1.0,0.0
datadog,"In this case, it's Couchbase and we want to be able to have  data ,  query , and  index  nodes tagged in Datadog, but that's probably more than you need to know...",0.0,0.038,0.962,0.0387
datadog,"Anyhow, one or more features can be selected.",0.0,0.0,1.0,0.0
datadog,"So, we have 3 recipes, one for each concern.",0.0,0.0,1.0,0.0
datadog,Each recipe adds the feature name to an array and then  include_recipe cookbook::default,0.0,0.0,1.0,0.0
datadog,"With Chef 12, we could select multiple feature recipes and then it seemed to wait until all of them were processed to run the default cookbook, so it could aggregate the array and process all the chosen features together.",0.0,0.0,1.0,0.0
datadog,"With Chef 13, it appears to be run immediately after the first feature recipe is processed, so that subsequent  include_recipe  are skipped.",0.0,0.0,1.0,0.0
datadog,"As a workaround, of course, I've changed some of the logic, but finding details on this behavior change hasn't come up with anything.",0.0,0.0,1.0,0.0
datadog,Thanks for any help...,0.0,0.492,0.508,0.4404
datadog,-H,0.0,0.0,1.0,0.0
datadog,"I have a linux trusty on aws m4.xlarge so 4 CPU, 16 GB RAM.",0.0,0.242,0.758,0.4939
datadog,It's running a java application on tomcat7 and oracle java 8.,0.0,0.0,1.0,0.0
datadog,Very frequently the app will hang and won't accept any other connection.,0.166,0.0,0.834,-0.2924
datadog,Status cake will report it as down since the response times out.,0.0,0.0,1.0,0.0
datadog,Datadog will show threads are maxed out.,0.0,0.0,1.0,0.0
datadog,But there is no increase in CPU (barely 10% of usage).,0.19,0.2,0.61,0.0387
datadog,RAM usage remains unchanged during that period.,0.0,0.0,1.0,0.0
datadog,Only a tomcat restart fixes the problem temporarily(12h approx ).,0.252,0.0,0.748,-0.4019
datadog,So I have taken a thread dump and seen so many threads in a waiting state.,0.178,0.0,0.822,-0.3818
datadog,"Since this is very new to me, I am blind even with the data.",0.184,0.0,0.816,-0.4019
datadog,I was hoping I could get help here and eventually master the art of ciphering a thread dump file.,0.123,0.261,0.616,0.4404
datadog,I have attached it here and I have as well uploaded it to  fastthread.io and it says there is no problem .,0.213,0.091,0.696,-0.4215
datadog,I have also uploaded the full  threadump on zerobin,0.0,0.0,1.0,0.0
datadog,I would be very grateful if anyone here can shed some lights on this and I hope it will help others in the same situation.,0.0,0.308,0.692,0.8357
datadog,Thanks in advance.,0.0,0.592,0.408,0.4404
datadog,"Having followed  Spring Boot Metrics documentation , I was able to set metrics logging for datadog easily.",0.0,0.146,0.854,0.34
datadog,The only remaining stuff is to set custom tags for my instances.,0.0,0.0,1.0,0.0
datadog,"With Spring Boot, you can do it by registering a new bean:",0.0,0.0,1.0,0.0
datadog,"However, I'm not able to register it in Grails 3.",0.0,0.0,1.0,0.0
datadog,Not in  resource.groovy  nor in application main class  Application.groovy .,0.0,0.0,1.0,0.0
datadog,Is there any way how to set this in Grails 3?,0.0,0.0,1.0,0.0
datadog,I try to use a wrapper Chef recipe to read Datadog API keys from an encrypted data bag and override node default attribute.,0.0,0.0,1.0,0.0
datadog,My confusion here is with  Chef::EncryptedDataBagItem.load method use.,0.239,0.0,0.761,-0.296
datadog,I created an encrypted bag with name datadog with an item datadog_keys inside of it.,0.0,0.133,0.867,0.25
datadog,I would like to get api key and app key from inside of this data bag item.,0.0,0.143,0.857,0.3612
datadog,So I'm using:,0.0,0.0,1.0,0.0
datadog,"My question, this usage is it correct or should I use:",0.0,0.0,1.0,0.0
datadog,"Chef::EncryptedDataBagItem.load(""datadog_keys"", ""api_key"")",0.0,0.0,1.0,0.0
datadog,or,0.0,0.0,1.0,0.0
datadog,"Chef::EncryptedDataBagItem.load(""datadog::datadog_keys"", ""api_key"")",0.0,0.0,1.0,0.0
datadog,I have netcat listening for udp traffic on port 8125 in terminal 1,0.0,0.0,1.0,0.0
datadog,nc -ul 8125,0.0,0.0,1.0,0.0
datadog,and in terminal 2 I run the following (a test dogstatsd message for troubleshooting a datadog client connection):,0.0,0.108,0.892,0.1779
datadog,"I would expect to see  test_metric:1|c  show up in the output of terminal 1, but there is no output at all.",0.128,0.0,0.872,-0.4215
datadog,Can you help me understand why the udp message is not showing up and how to successfully send the udp message?,0.0,0.237,0.763,0.7096
datadog,I'm running DataDog agent as a container within my AWS CoreOS instance.,0.0,0.0,1.0,0.0
datadog,This is done via running dd-agent as a container.,0.0,0.0,1.0,0.0
datadog,To automate this I have written a systemd unit for enabling and running data dog agent within AWS CoreOS instance.,0.0,0.0,1.0,0.0
datadog,But none of the metrics are being sent into the DataDog side.,0.0,0.0,1.0,0.0
datadog,But the Docker container is running without any issue.,0.0,0.0,1.0,0.0
datadog,Here is my Systemd unit file,0.0,0.0,1.0,0.0
datadog,EDIT - Adding More Info,0.0,0.0,1.0,0.0
datadog,Initially when I ran this on a single CoreOS instance I was able to see docker related metrics of the instance within DataDog dashboard.,0.0,0.0,1.0,0.0
datadog,Then I enabled this on multiple CoreOS AWS instances.,0.0,0.0,1.0,0.0
datadog,From that point on wards none of the metrics which are related to the CoreOS instances or Docker containers are not visible.,0.0,0.0,1.0,0.0
datadog,EDIT - Adding docker logs,0.0,0.0,1.0,0.0
datadog,"I have three slaves (jmeter-servers) running on EC2 instances, and in one case – (1) JMeter GUI on a local laptop, on another – same test plan (2) running from a command line on yet another EC2 instance.",0.0,0.0,1.0,0.0
datadog,"In case of GUI I can see all the aggregated numbers for Throughput, 99%, etc.",0.0,0.0,1.0,0.0
datadog,"in – well, GUI.",0.0,0.512,0.488,0.2732
datadog,I'm creating a jtl file with Aggregate Report listener.,0.0,0.239,0.761,0.296
datadog,"From watching Datadog charts monitoring the application server parameters (CPU usage, memory, etc.)",0.0,0.0,1.0,0.0
datadog,"I see that in case of a command line and everything on EC2 load is more than twice higher than when my local laptop is communicating with the jmeter-servers, meaning probably that the network becomes a bottleneck.",0.0,0.0,1.0,0.0
datadog,So I want to run everything on EC2.,0.0,0.21,0.79,0.1513
datadog,But then – how do I get access to the same aggregated numbers when I'm running from the command line when all four machines are EC2 instances?,0.0,0.0,1.0,0.0
datadog,"The huge jtl file contains records for each transaction, not the aggregated one line of the entire run result.",0.0,0.113,0.887,0.3182
datadog,On an attempt to download that jtl from EC2 and open it in GUI on a local laptop it generates some error instead of showing aggregated data.,0.097,0.0,0.903,-0.4019
datadog,Am I using a wrong listener to get to the summary data?,0.256,0.0,0.744,-0.4767
datadog,"(Tried Summary Report – it creates even larger jtl file, not the one line I'm looking for.)",0.0,0.123,0.877,0.2732
datadog,I'm trying to get the version of program that is installed on a windows server and I want it as a variable inside the recipe.,0.0,0.058,0.942,0.0772
datadog,Basically I'm trying to find the version and if it is not what I want it will be removed and the correct version of the program will be installed.,0.043,0.0,0.957,-0.0572
datadog,I can't figure out a way to get the version though.,0.0,0.0,1.0,0.0
datadog,The program I want the version for is the Datadog agent.,0.0,0.126,0.874,0.0772
datadog,I have a requirement in my program to send metrics to datadog indefinitely (for continuous app monitoring in datadog).,0.0,0.0,1.0,0.0
datadog,The program runs for a while and exits with the error &quot;dial udp 127.0.0.1:18125: socket: too many open files&quot;.,0.137,0.0,0.863,-0.4019
datadog,The app names are read from a config.toml file,0.0,0.0,1.0,0.0
datadog,"I'm trying to figure out how to take the results of ""repadmin /syncall /d /e"" and put the results into an if else statement.",0.0,0.0,1.0,0.0
datadog,"I've considered trying to just look for the success string it outputs for the if and, but I'm wondering if there is a more official way to pull the status code?",0.0,0.075,0.925,0.3291
datadog,So if successful use some built in PowerShell feature to know the status is successful.,0.0,0.386,0.614,0.8464
datadog,I'm doing this so I can publish a metric to DataDog giving a pass or fail count for cross-site AD Replications.,0.16,0.11,0.731,-0.2732
datadog,Any ideas?,0.0,0.0,1.0,0.0
datadog,"In Linux shell, how to use regex to filter output of other command.",0.0,0.0,1.0,0.0
datadog,"like in cisco devices we use sh ver | b interface, which will dispaly info about int only.",0.0,0.143,0.857,0.3612
datadog,"my requirement is filter output of below command to display from ""Dogstatsd (v 5.12.0)"" and status date &amp; time.",0.0,0.0,1.0,0.0
datadog,So that i can use this o/p with certain criteria to write a script to auto restart the agent.,0.0,0.116,0.884,0.2732
datadog,"Status date: 2017-05-30 08:20:13 (17s ago)
  Pid: 7864
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/collector.log, syslog:/dev/log",0.0,0.0,1.0,0.0
datadog,"Clocks
  ======",0.0,0.0,1.0,0.0
datadog,"NTP offset: 0.018 s
    System UTC time: 2017-05-30 06:20:31.535928",0.0,0.0,1.0,0.0
datadog,"Paths
  =====",0.0,0.0,1.0,0.0
datadog,"conf.d: /etc/dd-agent/conf.d
    checks.d: /opt/datadog-agent/agent/checks.d",0.0,0.0,1.0,0.0
datadog,"Hostnames
  =========",0.0,0.0,1.0,0.0
datadog,"socket-hostname: adcd
    hostname: adcd
    socket-fqdn: adcd",0.0,0.0,1.0,0.0
datadog,"Checks
  ======",0.0,0.0,1.0,0.0
datadog,"apache (5.0)
    ---------------
      - instance #0 [OK]
      - Collected 12 metrics, 0 events &amp; 1 service check",0.0,0.0,1.0,0.0
datadog,"network (5.0)
    ----------------
      - instance #0 [OK]
      - Collected 16 metrics, 0 events &amp; 0 service checks",0.0,0.0,1.0,0.0
datadog,"directory (5.0)
    ------------------
      - instance #0 [OK]
      - Collected 17 metrics, 0 events &amp; 0 service checks",0.0,0.0,1.0,0.0
datadog,"ntp (5.0)
    ------------
      - Collected 0 metrics, 0 events &amp; 0 service checks",0.0,0.0,1.0,0.0
datadog,"disk (5.0)
    -------------
      - instance #0 [OK]
      - Collected 24 metrics, 0 events &amp; 0 service checks",0.0,0.0,1.0,0.0
datadog,"Emitters
  ========",0.0,0.0,1.0,0.0
datadog,====================,0.0,0.0,1.0,0.0
datadog,"Status date: 2017-05-30 08:20:24 (7s ago)
  Pid: 7859
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/dogstatsd.log, syslog:/dev/log",0.0,0.0,1.0,0.0
datadog,"Flush count: 583466
  Packet Count: 333155
  Packets per second: 0.0
  Metric count: 1
  Event count: 0
  Service check count: 1",0.0,0.0,1.0,0.0
datadog,====================,0.0,0.0,1.0,0.0
datadog,"Status date: 2017-05-30 08:20:29 (2s ago)
  Pid: 8868
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/forwarder.log, syslog:/dev/log",0.0,0.0,1.0,0.0
datadog,"Queue Size: 422 bytes
  Queue Length: 1
  Flush Count: 1102592
  Transactions received: 879956
  Transactions flushed: 879955
  Transactions rejected: 0
  API Key Status: API Key is valid",0.125,0.0,0.875,-0.5106
datadog,======================,0.0,0.0,1.0,0.0
datadog,Not running (port 8126),0.0,0.0,1.0,0.0
datadog,root@adcd:~#,0.0,0.0,1.0,0.0
datadog,"We have several Tomcat servers (in AWS) running under Debian, we have all of them instrumented with Cloudwatch metrics for overall performance (Memory, CPU and others).",0.0,0.0,1.0,0.0
datadog,"We've detected that in a few of them we have ""spikes"" of either CPU or Memory utilization, and we'd like do detect what is actually clogging those resources.",0.0,0.088,0.912,0.3612
datadog,"As all the server runs is java based inside a Tomcat container, the logical would be to hook up some kind of JVM profiler and visually monitor the threads in it, but as we do have Cloudwatch alerts enabled when exceeding a certain threshold (for example CPU over 90%), we'd like to trigger some kind of automated stats collection to see what actual Java thread/code is the root cause of such consumption.",0.0,0.08,0.92,0.7096
datadog,Is there any monitoring agent and/or performance collection tool that might help to diagnose those specific spikes and not needing to collect stats for an actually long running process?,0.0,0.088,0.912,0.4019
datadog,"We've already tried trial versions of New Relic, DataDog, and Dynatrace (the latest being the most useful, prohibitively expensive due to its business model not suitable for small companies.",0.0,0.102,0.898,0.4927
datadog,"), but these solutions gather EVERYTHING, not only required timing windows, as I've asked above...these could work but introduces quite an overhead to the servers if being used 100% time in production servers (where the problem arises and, not in pre-production ones.",0.078,0.045,0.877,-0.3612
datadog,).,0.0,0.0,1.0,0.0
elasticapm,Are there any npm modules/plugins exists so as integrate Elastic APM's RUM JS Agent with the application ?,0.0,0.0,1.0,0.0
elasticapm,"By integration i mean good coupling with application so as to record all the things /events(such as route loading, all service requests, actions etc.)",0.0,0.116,0.884,0.4404
elasticapm,and provide it to elastic search.,0.0,0.0,1.0,0.0
elasticapm,I am using the elastic apm java agent to publish tracing information to an elastic apm server.,0.0,0.0,1.0,0.0
elasticapm,"I am setting the  enable_log_correlation  property to true which makes the span, trace and transaction ids available in the MDC and using logstash to capture all that in an elastic index.",0.0,0.088,0.912,0.4215
elasticapm,The logs are visible in the Discover tab in elastic and tracing information is visible in the APM tab.,0.0,0.0,1.0,0.0
elasticapm,I have seen in some screenshots a link on the APM   Transactions page to  View Transaction in Discover  to be able the see all logs related to a single transaction.,0.0,0.0,1.0,0.0
elasticapm,This link is not showing up for me.,0.0,0.0,1.0,0.0
elasticapm,The span link  Show span in Discover  is showing up and it takes you to Discover and opens up the apm index for this span.,0.0,0.0,1.0,0.0
elasticapm,So two questions:,0.0,0.0,1.0,0.0
elasticapm,Thanks!,0.0,1.0,0.0,0.4926
elasticapm,Kibana response is,0.0,0.0,1.0,0.0
elasticapm,"APM server returns 503 - Internal Server Error, 
Having hard time identifying the root cause.",0.255,0.0,0.745,-0.4767
elasticapm,Is it ES queue is full or ran out of memory or cluster is not being setup correctly?,0.0,0.0,1.0,0.0
elasticapm,According to ES documentation:  https://www.elastic.co/guide/en/apm/server/master/common-problems.html#queue-full,0.0,0.0,1.0,0.0
elasticapm,"A full queue generally means that the agents collect more data than
  APM server is able to process.",0.0,0.0,1.0,0.0
elasticapm,"This might happen when APM Server is
  not configured properly for the size of your Elasticsearch cluster, or
  because your Elasticsearch cluster is underpowered or not configured
  properly for the given workload.",0.0,0.0,1.0,0.0
elasticapm,The queue can also fill up if Elasticsearch runs out of disk space.,0.0,0.0,1.0,0.0
elasticapm,Documentation doesn't help in identifying what could be the root cause.,0.184,0.0,0.816,-0.3089
elasticapm,How do we identify the root cause?,0.0,0.0,1.0,0.0
elasticapm,Restarting Kibana and Elasticsearch helps but that does not help to identify the root cause,0.163,0.102,0.735,-0.2702
elasticapm,I am currently doing a PoC to integrate Elastic APM into my spring application.,0.0,0.0,1.0,0.0
elasticapm,"I was following this page :-  https://www.elastic.co/guide/en/apm/agent/java/1.x/setup-attach-api.html  
to programatically attach elastic-apm jar.",0.0,0.0,1.0,0.0
elasticapm,I have added the required jar into pom.xml but i am not getting how should i attach Elastic Apm (ElasticApmAttacher.attach())into my normal spring code.,0.0,0.0,1.0,0.0
elasticapm,Example given is for SpringBoot.,0.0,0.0,1.0,0.0
elasticapm,"But my application is on Spring core ( spring-core, spring-web ..) with rest services exposed using Jax-Rs.",0.088,0.0,0.912,-0.1154
elasticapm,After adding elastic-apm-node on our backend server we receive the below error hundreds of times a day.,0.153,0.0,0.847,-0.4019
elasticapm,Can anyone suggest what might cause this?,0.0,0.0,1.0,0.0
elasticapm,The error we see is this:,0.351,0.0,0.649,-0.4019
elasticapm,I think we run node v10.x but I'm not absolutely sure.,0.235,0.0,0.765,-0.4153
elasticapm,The service is built on feathersJS which again is built on Express.,0.0,0.0,1.0,0.0
elasticapm,We start APM like this:,0.0,0.385,0.615,0.3612
elasticapm,The dependencies listed in package.json are:,0.0,0.0,1.0,0.0
elasticapm,I'm using  elastic apm  to profiling my  NestJS  application and my apm agent is  elastic-apm-node .,0.0,0.0,1.0,0.0
elasticapm,My ORM is  typeOrm  and my database is  Oracle .,0.0,0.0,1.0,0.0
elasticapm,My problem is apm agent does not record database query spans and I can't see  database query spans  in kibana ui.,0.124,0.0,0.876,-0.4019
elasticapm,Can anyone help me?,0.0,0.474,0.526,0.4019
elasticapm,We want to track our elixir phoenix app  using elastic apm.,0.0,0.115,0.885,0.0772
elasticapm,But I could not find an apm agent from elastic.,0.0,0.0,1.0,0.0
elasticapm,Someone suggested to use  opentelemetry  along with exporter but I am unable to understand how to use that from the docs.,0.0,0.0,1.0,0.0
elasticapm,I want to track details like the new relic does like errors and all things.,0.128,0.337,0.535,0.4404
elasticapm,Previously we used new relic for which there is an open source apm agent but now we want to switch to elastic.I am unable to understand how to use span in the app and how to handle multiple span and where to put them.,0.0,0.033,0.967,0.1154
elasticapm,If anyone can help with that or provide alternate solution to use elastic apm it would be great.,0.0,0.378,0.622,0.8442
elasticapm,I'd like to measure avg request-response time for my webserver.,0.0,0.217,0.783,0.3612
elasticapm,Apm has  transaction.duration.us  and it seems this could be the metric I'm looking for.,0.0,0.0,1.0,0.0
elasticapm,But I coulnd't find the documentation what it is.,0.0,0.0,1.0,0.0
elasticapm,Where can I find the meaning of the variable?,0.0,0.0,1.0,0.0
elasticapm,"I have a problem when I want to start the app, when i type  npm run server .",0.18,0.087,0.733,-0.34
elasticapm,And the error information are like this :,0.265,0.245,0.49,-0.0516
elasticapm,"Elastic APM agent is inactive due to configuration, what does it mean?",0.0,0.0,1.0,0.0
elasticapm,"Maybe someone can help me, please...",0.0,0.351,0.649,0.4019
elasticapm,This is the package.json :,0.0,0.0,1.0,0.0
elasticapm,"{ 
    ""name"": ""opbeans"", 
    ""version"": ""1.0.0"", 
    ""description"": ""The Opbeans inventory management system"", 
    ""main"": ""server.js"", 
    ""dependencies"": { 
      &nbsp;&nbsp;&nbsp;""after-all-results"": ""^2.0.0"", 
      &nbsp;&nbsp;&nbsp;""body-parser"": ""^1.15.2"", 
      &nbsp;&nbsp;&nbsp;""concurrently"": ""^3.1.0"", 
      &nbsp;&nbsp;&nbsp;""detect-port"": ""^1.2.2"", 
      &nbsp;&nbsp;&nbsp;""elastic-apm-node"": ""^1.3.0"", 
      &nbsp;&nbsp;&nbsp;""express"": ""^4.14.0"", 
      &nbsp;&nbsp;&nbsp;""pg"": ""^6.1.2"", 
      &nbsp;&nbsp;&nbsp;""redis"": ""^2.6.3"", 
      &nbsp;&nbsp;&nbsp;""request"": ""^2.79.0"", 
      &nbsp;&nbsp;&nbsp;""webpack"": ""^4.3.0"", 
      &nbsp;&nbsp;&nbsp;""webpack-dev-server"": ""^3.1.1"", 
      &nbsp;&nbsp;&nbsp;""workload"": ""^2.3.0"" 
    }, 
    ""devDependencies"": { 
      &nbsp;&nbsp;&nbsp;""dotenv"": ""^4.0.0"", 
      &nbsp;&nbsp;&nbsp;""react-dev-utils"": ""^5.0.0"", 
      &nbsp;&nbsp;&nbsp;""react-scripts"": ""^1.1.1"", 
      &nbsp;&nbsp;&nbsp;""standard"": ""^10.0.2"" 
    }, 
    ""scripts"": { 
      &nbsp;&nbsp;&nbsp;""db-setup"": ""./db/setup.sh"", 
      &nbsp;&nbsp;&nbsp;""test"": ""standard  .js server/ / .js db/ /*.js"", 
      &nbsp;&nbsp;&nbsp;""server"": ""node server.js"", 
      &nbsp;&nbsp;&nbsp;""client"": ""npm run start --prefix client/"", 
      &nbsp;&nbsp;&nbsp;""client-build"": ""npm run build --prefix client/"", 
      &nbsp;&nbsp;&nbsp;""client-install"": ""npm install --prefix client/"", 
      &nbsp;&nbsp;&nbsp;""postinstall"": ""./client/build-client.sh"", 
      &nbsp;&nbsp;&nbsp;""start"": ""concurrently \""npm run server\"" \""npm run &nbsp;&nbsp;&nbsp;client\"""", 
      &nbsp;&nbsp;&nbsp;""workload"": ""workload -f .workload.js"" 
    }",0.0,0.0,1.0,0.0
elasticapm,"im trying to install the Elastic APM with Elasticsearch, Kibana and the APM server as 3 services with docker-compose.",0.0,0.0,1.0,0.0
elasticapm,Now im getting confused on how to set the IPs in the app-server.yml file with the documentation  APM Server Configuration .,0.108,0.0,0.892,-0.3182
elasticapm,The file should look like this:,0.0,0.333,0.667,0.3612
elasticapm,"I tried to set ElasticsearchAddress to localhost or 127.0.0.1 but I always get errors like
 Failed to connect: Get http://127.0.0.1:9200: dial tcp 127.0.0.1:9200: getsockopt: connection refused  or  Failed to connect: Get http://localhost:9200: dial tcp [::1]:9200: connect: cannot assign requested address .",0.29,0.064,0.646,-0.9109
elasticapm,I also tried it with several other ips.,0.0,0.0,1.0,0.0
elasticapm,Does anyone know how to configure the app server correctly or are there any docker-compose files to do the installation correctly?,0.0,0.0,1.0,0.0
elasticapm,Thanks for ur help,0.0,0.737,0.263,0.6808
elasticapm,I have downloaded elastic-apm-agent.jar on my local which is monitoring locally deployed spring boot micro service.,0.0,0.0,1.0,0.0
elasticapm,Now I want to add same jar to PCF.,0.0,0.157,0.843,0.0772
elasticapm,I am using a TypeScript setup with webpack and babel and get the following error when trying to include  elastic-apm-node .,0.137,0.0,0.863,-0.4019
elasticapm,I have the settings in environment variables.,0.0,0.0,1.0,0.0
elasticapm,Errors:,1.0,0.0,0.0,-0.34
elasticapm,Any idea how I can prevent this?,0.0,0.18,0.82,0.0258
elasticapm,"When using just TypeScript with Meteor I don't get the errors, so I think it is connected to Babel / Webpack.",0.0,0.107,0.893,0.2584
elasticapm,I have setup of Elastic with APM server on single machine.,0.0,0.0,1.0,0.0
elasticapm,I've configured APM java agent to push traces to APM server on localhost.,0.0,0.0,1.0,0.0
elasticapm,Everything works fine with localhost configuration on Windows.,0.0,0.205,0.795,0.2023
elasticapm,"Now, I'm looking to run apm java agent for application running on different machine on the same network.",0.0,0.0,1.0,0.0
elasticapm,That is apm java agent on linux &amp; apm server running on windows machine.,0.0,0.0,1.0,0.0
elasticapm,Default APM-server listen to localhost.,0.0,0.0,1.0,0.0
elasticapm,I tried to change setting on apm-server.yml file with -,0.0,0.0,1.0,0.0
elasticapm,default is:,0.0,0.0,1.0,0.0
elasticapm,"After making apm-server.yml change, process explorer show apm-server.exe process listening to IP- host-ip port- 8200 protocol- TCP.",0.0,0.0,1.0,0.0
elasticapm,"But, still  http://host-ip:8200  is not accessible from other machine on network.",0.0,0.0,1.0,0.0
elasticapm,While on the same machine (windows)  http://localhost:8200  &amp;  http://host-ip:8200  works fine &amp; give below response.,0.0,0.114,0.886,0.2023
elasticapm,Thanks for help.,0.0,0.848,0.152,0.6808
elasticapm,"Given this docker file to setup the backend services that includes: elasticsearch, apm-server, kibana, jaeger-collector, jaeger-agent, jaeger-query, grafana.",0.0,0.0,1.0,0.0
elasticapm,I am running Elastic APM with Opentracing from my Angular client:,0.0,0.0,1.0,0.0
elasticapm,I am encountering CORS issue:,0.0,0.0,1.0,0.0
elasticapm,My goal is to hook up Angular the elastic APM's opentracing client to the services inside docker.,0.0,0.0,1.0,0.0
elasticapm,There are some additional issues and documents that covers CORS for apm-server:,0.0,0.0,1.0,0.0
elasticapm,Distributed Tracing Guide,0.0,0.0,1.0,0.0
elasticapm,Config with RUM enabled,0.0,0.0,1.0,0.0
elasticapm,"It looks like the config should work, since  Default value is set to ['*'], which allows everything.",0.0,0.246,0.754,0.5994
elasticapm,"I am quite new to Elastic APM, Kibana, Elasticsearch and APM in general and did not come across any information pointing towards my needs.",0.0,0.0,1.0,0.0
elasticapm,I set up the  elastic-apm[flask]  module and followed the tutorial.,0.0,0.0,1.0,0.0
elasticapm,"In the Kibana dashboard I get information like response times and server name, but the fields for client.ip etc.",0.0,0.093,0.907,0.1901
elasticapm,are empty.,0.643,0.0,0.357,-0.2023
elasticapm,I would like to track the IP addresses (more exactly where my website visitors come from).,0.0,0.152,0.848,0.3612
elasticapm,"So, how do I get the user's IP address into the client.ip field in Elastic APM?",0.0,0.0,1.0,0.0
elasticapm,I don't want to issue an  app.logger.debug  statement everytime a route is being requested.,0.1,0.0,0.9,-0.0572
elasticapm,I'm currently checking why the APM UI in Kibana doesn't output me the information when the timeframe is set to more than 24 hours.,0.0,0.0,1.0,0.0
elasticapm,When checking the config I did notice that we haven't had the Kibana endpoint set for the APM server.,0.0,0.0,1.0,0.0
elasticapm,Checking the APM logs I can't see an error but when going to the APM UI I can find this error in the Kibana logs:,0.0,0.177,0.823,0.5448
elasticapm,From the output it looks like that Kibana can't find any server configuration but the setup was done successfully but I can't as well access the APM settings that were moved in Kibana (timeout with error 404).,0.132,0.138,0.73,0.0719
elasticapm,Other queries or the query from the same indices work well without any increased delay.,0.101,0.227,0.671,0.3067
elasticapm,The APM agent is the latest version of the Django (Python) agent.,0.0,0.0,1.0,0.0
elasticapm,"And the resource metrics from clusters as well as status are really good, so the cluster should be powerful enough.",0.0,0.323,0.677,0.796
elasticapm,We have had as well testing deploy in Kubernetes and the services sometimes show up in the APM UI but it's turned off currently.,0.0,0.063,0.937,0.1406
elasticapm,Environment and versions:,0.0,0.0,1.0,0.0
elasticapm,"Elasticsearch deployed on GCP in Docker containers (4 nodes on different VMs + 1 VM with APM Server, Kibana, and Elasticsearch client node).",0.0,0.0,1.0,0.0
elasticapm,"ES, Kibana, APM-Server versions: 7.3.1",0.0,0.0,1.0,0.0
elasticapm,"I'm using Elastic APM, and want to find out how long the garbage collector has been running during a period of time.",0.0,0.061,0.939,0.0772
elasticapm,"This is to understand if the application is running out of memory, which seems more accurate than just checking heap used, as garbage collection could trigger when heap space is limited, and then free up a large amount.",0.047,0.082,0.871,0.34
elasticapm,"Elastic APM will track  jvm.gc.time , which the Elastic site defines as:",0.0,0.0,1.0,0.0
elasticapm,The approximate accumulated collection elapsed time in milliseconds.,0.0,0.0,1.0,0.0
elasticapm,Source,0.0,0.0,1.0,0.0
elasticapm,I assumed this meant how much time has been spent garbage collecting since the application started.,0.0,0.0,1.0,0.0
elasticapm,"My plan was to read this value periodically, and determine how much of the time interval was spent garbage collecting.",0.0,0.126,0.874,0.4118
elasticapm,"When I read this value two different times, it turns out the second, and  later  reading, is actually lower than the first.",0.092,0.115,0.793,0.1406
elasticapm,First Reading,0.0,0.0,1.0,0.0
elasticapm,Second Reading,0.0,0.0,1.0,0.0
elasticapm,Can anyone help me understand what  jvm.gc.time  captures?,0.0,0.278,0.722,0.4019
elasticapm,"I am interested in using Elastic APM within an ASP.NET Core to instrument traces of a set of services which communicate over a mix of protocols (HTTP, SQS, SNS).",0.0,0.097,0.903,0.4019
elasticapm,"Despite reviewing the documentation, I am not clear how I can use the  Elastic APM Public API  to connect transactions to one another which occur outside of HTTP (HttpClient is automatically instrumented for trace by Elastic APM).",0.06,0.0,0.94,-0.2924
elasticapm,"According to the documentation, I should be able to serialize the  CurrentTransaction.OutgoingDistributedTracingData  on the caller and then deserialize it to resume the transaction on the callee, but despite implementing this pattern in memory, my traces in Kibana are missing spans from all but the final transaction.",0.06,0.0,0.94,-0.4215
elasticapm,My implementation spike can be found on  Github .,0.0,0.0,1.0,0.0
elasticapm,I add ElasticAPM to my startup on AspNetCore 3.1,0.0,0.0,1.0,0.0
elasticapm,"in my project, rest api services logs as transaction tab of kibana-apm.",0.0,0.0,1.0,0.0
elasticapm,but my background services dos not logged by apm agent &amp; only metrics tab work for me.,0.0,0.0,1.0,0.0
elasticapm,"I have set up an elastic stack with elasticsearch, filebeat, kibana and apm server, and an spring-boot-application with the apm java agent and started my setup in a docker compose file.",0.0,0.0,1.0,0.0
elasticapm,I have enabled the dashboard and I can see traces about processes in the application.,0.0,0.0,1.0,0.0
elasticapm,"But I cannot filter for container id, because there is no id.",0.219,0.0,0.781,-0.4215
elasticapm,"How do I enable my stack apm server/apm agent to receive the metadata about container id, pod id and so on.",0.0,0.0,1.0,0.0
elasticapm,Where can I enable metadata for apm server / apm agent to receive the container id for instance.,0.0,0.0,1.0,0.0
elasticapm,My apm server can't connect to ES with the following log,0.0,0.0,1.0,0.0
elasticapm,"I tried to 'reset' the index by the following command, it won't work either",0.0,0.0,1.0,0.0
elasticapm,"I tried to setup a policy where apm data is deleted after 3 month, and I think I messed up the index setup.. (I can't remember what I did exactly)",0.094,0.0,0.906,-0.34
elasticapm,How do I reset the index and start using apm again?,0.0,0.0,1.0,0.0
elasticapm,"(It's a plus if I can retain the data, but I can sacrifice it)",0.0,0.0,1.0,0.0
elasticapm,I am trying to instrument our java webapp based on tomcat using Elastic APM.,0.0,0.0,1.0,0.0
elasticapm,Tomcat starts fine without the javaagent but refuses to start with it.,0.0,0.113,0.887,0.1027
elasticapm,I do not see any logs in /var/log/tomcat.,0.0,0.0,1.0,0.0
elasticapm,The following is the log:,0.0,0.0,1.0,0.0
elasticapm,Has anyone faced this before?,0.0,0.0,1.0,0.0
elasticapm,Any ideas what I could do to fix it?,0.0,0.0,1.0,0.0
elasticapm,Getting error while integrating Elastic APM in the Node Express application.,0.213,0.0,0.787,-0.4019
elasticapm,"Nodejs: v6.4.2,",0.0,0.0,1.0,0.0
elasticapm,app.js,0.0,0.0,1.0,0.0
elasticapm,package.json,0.0,0.0,1.0,0.0
elasticapm,Error:,1.0,0.0,0.0,-0.4019
elasticapm,"You have triggered an unhandledRejection, you may have forgotten to catch a Promise rejection:
  Error: Can't set headers after they are sent.",0.296,0.084,0.62,-0.7003
elasticapm,"at ServerResponse.OutgoingMessage.setHeader (_http_outgoing.js:335:11)
      at /home/ubuntu/depanels/server/api/user/user.controller.js:22:25
      at /home/ubuntu/depanels/server/auth/auth.service.js:149:28
      at _fulfilled (/home/ubuntu/depanels/node_modules/q/q.js:854:54)
      at self.promiseDispatch.done (/home/ubuntu/depanels/node_modules/q/q.js:883:30)
      at Promise.promise.promiseDispatch (/home/ubuntu/depanels/node_modules/q/q.js:816:13)
      at /home/ubuntu/depanels/node_modules/q/q.js:624:44
      at runSingle (/home/ubuntu/depanels/node_modules/q/q.js:137:13)
      at flush (/home/ubuntu/depanels/node_modules/q/q.js:125:13)
      at elasticAPMCallbackWrapper (/home/ubuntu/depanels/node_modules/elastic-apm-node/lib/instrumentation/index.js:191:27)
      at nextTickCallbackWith0Args (node.js:419:9)
      at process._tickDomainCallback [as _tickCallback] (node.js:389:13)
  Elastic APM HTTP error (404): 404 page not found
  Elastic APM HTTP error (404): 404 page not found",0.096,0.0,0.904,-0.6597
elasticapm,Please suggest where I am doing wrong?,0.33,0.245,0.426,-0.2023
elasticapm,I need to send span to elastic-apm after sending express response.,0.0,0.0,1.0,0.0
elasticapm,"In my code example, i use setInterval.",0.0,0.0,1.0,0.0
elasticapm,"First express response, after 1 second, i try use startspan, but i got error: Cannot read property 'end' of null",0.182,0.0,0.818,-0.5499
elasticapm,My elastic stack version is 7.6.2,0.0,0.0,1.0,0.0
elasticapm,I am using Elastic APM.,0.0,0.0,1.0,0.0
elasticapm,I find that mongoose integration is not working when used with GraphQL/Apollo Server somehow.,0.0,0.0,1.0,0.0
elasticapm,I wrote an apollo-server plugin like this to start/stop transactions:,0.0,0.238,0.762,0.3612
elasticapm,"It works, but I am missing spans from mongoose, when I enable trace, this is what I see:",0.167,0.0,0.833,-0.4215
elasticapm,"Notice the spans created fine before and after those mongoose calls, but the mongoose calls seem to be unable to find the active transaction somehow.",0.0,0.227,0.773,0.6652
elasticapm,no active transaction found - cannot build new span,0.202,0.248,0.55,0.128
elasticapm,I am adding the elastic-apm-node package to our nestjs backend.,0.0,0.0,1.0,0.0
elasticapm,I am using the graphql feature of nestjs.,0.0,0.0,1.0,0.0
elasticapm,"Because of this, all requests are merged together as  /graphql  in elastic.",0.0,0.0,1.0,0.0
elasticapm,Is this how it is supposed to be?,0.0,0.0,1.0,0.0
elasticapm,"I imagined that since apollo-server-express is supported by elastic-apm-node, which is also used by nestjs, it should be displaying it better.",0.0,0.224,0.776,0.6369
elasticapm,Am I missing something?,0.524,0.0,0.476,-0.296
elasticapm,UPDATE,0.0,0.0,1.0,0.0
elasticapm,The graphql feature is set up using the docs for nestjs:  https://docs.nestjs.com/graphql/quick-start  it is basically their recommended setup I am using.,0.0,0.087,0.913,0.2023
elasticapm,Has anyone already tried to connect wso2 production with elastic apm server ?,0.0,0.0,1.0,0.0
elasticapm,I have done this,0.0,0.0,1.0,0.0
elasticapm,But i dont have the http request of the API in the APM in kibana,0.0,0.0,1.0,0.0
elasticapm,I am running a sample application jar on local system using elasticAPM agent.,0.0,0.0,1.0,0.0
elasticapm,Elastic APM show 2 different cpu stats (system/process).,0.0,0.0,1.0,0.0
elasticapm,"Metrics explanation on official site says the same thing for both stats
 https://www.elastic.co/guide/en/apm/server/current/exported-fields-system.html",0.0,0.0,1.0,0.0
elasticapm,"Please explain, Is the &quot;system cpu stats&quot; is of my system even when the agent is connected to application.jar only using java command?",0.0,0.095,0.905,0.3182
elasticapm,"If so, how can I check on elastic apm what else on my system in consuming cpu since only application is running during the load test.",0.0,0.0,1.0,0.0
elasticapm,java -javaagent:&lt;agent.jar&gt; -jar &lt;app.jar&gt;,0.0,0.0,1.0,0.0
elasticapm,The CPU usage shown below,0.0,0.0,1.0,0.0
elasticapm,"Elastic APM uses a Java agent to collect application performance metrics and sends it to the Elastic APM server, which means it will use Java instrumentation for underlying metrics in JVM.",0.0,0.0,1.0,0.0
elasticapm,My question is:,0.0,0.0,1.0,0.0
elasticapm,Is there any vulnerability issue or security risk for using it?,0.278,0.167,0.556,-0.1531
elasticapm,I am very new to Elastic APM and not sure how it can support different frameworks.,0.111,0.153,0.736,0.1872
elasticapm,I can see that from the documentation APM supports Spring Boot.,0.0,0.217,0.783,0.3612
elasticapm,I have tested a Spring Boot application with the APM and it looks promising.,0.0,0.197,0.803,0.4019
elasticapm,I was wondering if APM supports Spring Cloud Stream as well.,0.0,0.365,0.635,0.5574
elasticapm,Spring Cloud Stream provides Event Driven Architecture by using Spring Boot and messaging middleware.,0.0,0.0,1.0,0.0
elasticapm,"Middleware can be Kafka, RabbitMQ, etc.",0.0,0.0,1.0,0.0
elasticapm,Is there a way for me to track garbage collection of my Java application using Elastic APM and the associated Java APM agent?,0.0,0.0,1.0,0.0
elasticapm,"I'm using Spring Boot, if that makes a difference.",0.0,0.0,1.0,0.0
elasticapm,"Out-of-the-box I'm able to see the heap and non-heap memory utilization, but I'm not sure if there is also a way to view garbage collection.",0.096,0.0,0.904,-0.3491
elasticapm,When nesting spans in Elsatic APM through either the Opentracing API or Elastic APM's API.,0.0,0.0,1.0,0.0
elasticapm,Some spans are never recorded.,0.0,0.0,1.0,0.0
elasticapm,Using  import * as apm from '@elastic/apm-rum'; :,0.0,0.0,1.0,0.0
elasticapm,Using Elastic's OpenTracing API:,0.0,0.0,1.0,0.0
elasticapm,The behavior for spans are just as inconsistent.,0.0,0.0,1.0,0.0
elasticapm,It is unclear when a transaction begins or ends.,0.222,0.0,0.778,-0.25
elasticapm,"Some spans are translated into transactions, and nested spans may not be recorded.",0.0,0.0,1.0,0.0
elasticapm,"If I declare a page wide transaction, Angular's  ngOnInit  can be recorded by a span, but other event hooks are never recorded.",0.0,0.0,1.0,0.0
elasticapm,I have tried variations of this.,0.0,0.0,1.0,0.0
elasticapm,"Wrapping span in span, childOf, app-level span, individual instances of span.",0.0,0.0,1.0,0.0
elasticapm,I am trying out Elastic APM.,0.0,0.0,1.0,0.0
elasticapm,I used automatic setup with  apm-agent-attach-standalone.jar .,0.0,0.0,1.0,0.0
elasticapm,"(I also used  -javaagent  flag (manual setup), and it worked fine)",0.0,0.0,1.0,0.0
elasticapm,"Data was successfully recieved from the agents, and I used APM UI to monitor.",0.0,0.211,0.789,0.4939
elasticapm,How do I detach this agent from the process?,0.0,0.0,1.0,0.0
elasticapm,What do I do if I want to stop this agent?,0.21,0.124,0.667,-0.2263
elasticapm,I'm looking to use Elastic APM to monitor my own custom events only (user action events) in Node.js,0.0,0.0,1.0,0.0
elasticapm,I can't find in the docs how to turn off all HTTP monitoring and also how to send a custom event.,0.0,0.0,1.0,0.0
elasticapm,They have custom span and custom transaction in the docs but I'm not sure when or how to use either for my use case.,0.096,0.0,0.904,-0.3491
elasticapm,For example in the docs they describe a custom transaction as follows:,0.0,0.0,1.0,0.0
elasticapm,"I'm not looking to send an error, I'm looking to send a custom event so I'm not sure what's up with that.",0.206,0.0,0.794,-0.6028
elasticapm,I'm trying out Elastic APM.,0.0,0.0,1.0,0.0
elasticapm,I have successfully created a service with data flowing in.,0.0,0.464,0.536,0.6369
elasticapm,I wanted to see if I can have multiple services.,0.0,0.0,1.0,0.0
elasticapm,"Somehow, I ran into problems, so I wanted to delete some services.",0.231,0.0,0.769,-0.4019
elasticapm,"However, I couldn't find a way to delete a service.",0.0,0.0,1.0,0.0
elasticapm,Question : How can I delete a service in APM?,0.0,0.0,1.0,0.0
elasticapm,Indexes related to APM :,0.0,0.0,1.0,0.0
elasticapm,Above contains the service that I want to remove.,0.0,0.157,0.843,0.0772
elasticapm,I've seen some APM that only measures web applications which run on WAS.,0.0,0.0,1.0,0.0
elasticapm,Can Elastic APM meausure the performance of other applications like pure Java application and etc?,0.0,0.152,0.848,0.3612
elasticapm,"If not,  can I use  https://www.elastic.co/guide/en/apm/agent/java/1.x/public-api.html  (Public API) so that it can measure the performance of non web applications?",0.0,0.0,1.0,0.0
elasticapm,I will appreciate any advice.,0.0,0.474,0.526,0.4019
elasticapm,Cheers.,0.0,1.0,0.0,0.4767
elasticapm,I integrated  Elastic APM  as follows to my Vue.js App.,0.0,0.0,1.0,0.0
elasticapm,It logs successfully to Elastic APM.,0.0,0.39,0.61,0.4939
elasticapm,However it is not showing the current page name in logs correctly.,0.0,0.0,1.0,0.0
elasticapm,It seems to always show the first page on which the App has been mounted in APM.,0.0,0.0,1.0,0.0
elasticapm,I would like to always see the current page to be linked to the corresponding APM events.,0.0,0.143,0.857,0.3612
elasticapm,Any suggestions how to achieve this?,0.0,0.0,1.0,0.0
elasticapm,The documentation says to set  pageLoadTransactionName .,0.0,0.0,1.0,0.0
elasticapm,"However it seems to not update this on route change:
 https://www.elastic.co/guide/en/apm/agent/rum-js/current/custom-transaction-name.html",0.0,0.0,1.0,0.0
elasticapm,App.js,0.0,0.0,1.0,0.0
elasticapm,I can't understand if Elastic APM for Java should capture logs from slf4j or it can track only exceptions?,0.0,0.0,1.0,0.0
elasticapm,I have Spring Boot service that uses slf4j but I can't find log entries in apm index.,0.0,0.0,1.0,0.0
elasticapm,Can anybody clarify the expectation?,0.0,0.0,1.0,0.0
elasticapm,Thank you,0.0,0.714,0.286,0.3612
elasticapm,flushing due to time since last flush 9.060s   max_flush_time 9.060s,0.0,0.0,1.0,0.0
elasticapm,I 'm getting tone of those message in django debug.,0.0,0.0,1.0,0.0
elasticapm,I tried changing to their default setting,0.0,0.0,1.0,0.0
elasticapm,Still getting lots of logs.,0.0,0.0,1.0,0.0
elasticapm,How to turn this off?,0.0,0.0,1.0,0.0
elasticapm,"I'm having trouble getting the APM ""button"" and dashboard to appear on the Kibana page.",0.162,0.0,0.838,-0.4019
elasticapm,"Yes, there is the ""Add APM"" button which tells you what to do, but it doesn't seem to actually work.",0.0,0.089,0.911,0.2144
elasticapm,"Actually, this is not entirely true - I was able to get the APM ""button"" and corresponding dashboard ""installed"" in my Kibana view,  but I can't remember what I had to do to make that happen.",0.054,0.0,0.946,-0.1961
elasticapm,"I believe that I have the various components (Elasticsearch, Kibana, APM server) installed and running.",0.0,0.0,1.0,0.0
elasticapm,"The ""check APM server status"" button indicates that it's been set up properly.",0.0,0.0,1.0,0.0
elasticapm,"If I click the ""APM dashboard"" button at the bottom of the page, it gives me a list of items, but I don't know what they are or whether they have anything to do with APM.",0.0,0.0,1.0,0.0
elasticapm,I am at a loss as to how to get APM to appear in Kibana.,0.161,0.0,0.839,-0.3182
elasticapm,Does anybody have any ideas?,0.0,0.0,1.0,0.0
elasticapm,UPDATE :,0.0,0.0,1.0,0.0
elasticapm,https://www.elastic.co/guide/en/apm/server/current/getting-started-apm-server.html,0.0,0.0,1.0,0.0
elasticapm,then,0.0,0.0,1.0,0.0
elasticapm,https://www.elastic.co/guide/en/apm/server/current/installing.html,0.0,0.0,1.0,0.0
elasticapm,then,0.0,0.0,1.0,0.0
elasticapm,https://www.elastic.co/guide/en/apm/server/current/apm-server-configuration.html,0.0,0.0,1.0,0.0
elasticapm,This seems to provide specific information I haven't been able to find elsewhere.,0.0,0.0,1.0,0.0
elasticapm,The usage of  apm-server setup &lt;flags&gt;  seems promising.,0.0,0.278,0.722,0.4019
elasticapm,I'm not sure which flags (if any) I should use?,0.197,0.0,0.803,-0.2411
elasticapm,I have integrated  Elastic APM  to my Vue.js App accordingly to the documentation ( https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html ),0.0,0.0,1.0,0.0
elasticapm,In addition to the default events  page-load  and  route-change  I want to add custom transactions/spans for some button clicks.,0.0,0.071,0.929,0.0772
elasticapm,I am stucked with checking if there is already an existing transaction start which I could use to add a custom span:,0.0,0.0,1.0,0.0
elasticapm,However getting the current transaction fails (first line).,0.286,0.0,0.714,-0.4215
elasticapm,I am trying to integrate Elastic APM and Sentry into my website using Buffalo.,0.0,0.0,1.0,0.0
elasticapm,The interesting files are as follows:,0.0,0.351,0.649,0.4019
elasticapm,handlers/sentryHandler.go,0.0,0.0,1.0,0.0
elasticapm,handlers/elasticAPMHandler.go,0.0,0.0,1.0,0.0
elasticapm,actions/app.go,0.0,0.0,1.0,0.0
elasticapm,"The problem I'm running into is if I have the Sentry/APM handlers at the top, then I get errors like  application.html: line 24: ""showPagePath"": unknown identifier .",0.173,0.146,0.68,-0.2023
elasticapm,"However, if I move it to just before I set up the routes, then I get a no transaction found error.",0.246,0.0,0.754,-0.5994
elasticapm,"So, I'm guessing that the handler wrappers are dropping the  buffalo.Context  information.",0.0,0.0,1.0,0.0
elasticapm,"So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?",0.0,0.0,1.0,0.0
elasticapm,I am using Elastic APM agent ( https://www.elastic.co/guide/en/apm/agent/dotnet/current/index.html ) to instrument an ASP.NET MVC Application.,0.0,0.0,1.0,0.0
elasticapm,I added the nuget packages and added the module entry in the web.config.,0.0,0.0,1.0,0.0
elasticapm,I am able to get data in the Kibana APM tab and nicely shows the time spent by each call.,0.0,0.139,0.861,0.4404
elasticapm,(see screenshot below).,0.0,0.0,1.0,0.0
elasticapm,Mu Question is: How can I drill down inside each of these calls to see where the time is spent in the stackstace?,0.0,0.0,1.0,0.0
elasticapm,Is there something I am missing?,0.355,0.0,0.645,-0.296
elasticapm,We have newly introduced elastic APM to monitor application performance.,0.0,0.0,1.0,0.0
elasticapm,"After the deployment, we have an issue with Kibana APM UI that doesn't show up any transactions for few services.",0.0,0.0,1.0,0.0
elasticapm,we are having data in the APM indice but doesn't transactions show up in the Kibana APM dashboard   UI we get Avg.response time N/A and Trans.per minute is 0.,0.0,0.0,1.0,0.0
elasticapm,"APM-Server 7.6.1
Elastic Agent - 1.15.0
ELK Stack - 7.6.1",0.0,0.0,1.0,0.0
elasticapm,Please help me on this to identify the issue.,0.0,0.417,0.583,0.6124
elasticapm,Thanks in advance.,0.0,0.592,0.408,0.4404
elasticapm,I try to implement profile based application in Spring Boot and this works for Spring Boot clearly.,0.0,0.153,0.847,0.4019
elasticapm,"But when I try to implement it for elastic-search APM, it doesn't work.",0.0,0.0,1.0,0.0
elasticapm,According to  here : We can describe properties like with elastic.apm prefix:,0.0,0.2,0.8,0.3612
elasticapm,but it doesn't work.,0.0,0.0,1.0,0.0
elasticapm,"When I call it with elasticapm.properties, it works.",0.0,0.0,1.0,0.0
elasticapm,Do you have any suggestion?,0.0,0.0,1.0,0.0
elasticapm,We're using the .NET agent (1.4) and Elastic ECE 7.6.,0.0,0.0,1.0,0.0
elasticapm,Activating the APM server and instrumenting our application was quite easy.,0.0,0.242,0.758,0.4927
elasticapm,Everything seems to work up until the point where it needs to show the code snippet related to a particular span.,0.0,0.0,1.0,0.0
elasticapm,Below an example of what I'm looking for from a Node application:,0.0,0.0,1.0,0.0
elasticapm,My question:,0.0,0.0,1.0,0.0
elasticapm,"Is this not yet included in the .NET agent, or is there additional configuration necessary to get this working?",0.0,0.0,1.0,0.0
elasticapm,I have the Elastic Stack running with Docker.,0.0,0.0,1.0,0.0
elasticapm,I've also added an Angular 9 App via Docker and enabled CORS on Elastic.,0.0,0.0,1.0,0.0
elasticapm,"I am trying to send logs from the Angular 9 app to APM (on port 9200), using the  APM Angular Package .",0.0,0.0,1.0,0.0
elasticapm,However I'm getting a 400 error.,0.403,0.0,0.597,-0.4019
elasticapm,"I'm doing what the docs say, their NPM package and running:",0.0,0.0,1.0,0.0
elasticapm,But still getting the error in the screenshot  [Elastic APM] Failed sending events!,0.43,0.0,0.57,-0.8516
elasticapm,"Error: ""http://localhost:9200/intake/v2/rum/events HTTP status: 400""",0.403,0.0,0.597,-0.4019
elasticapm,Any advice on how to resolve this?,0.0,0.302,0.698,0.3818
elasticapm,docker-compose.yml  file:,0.0,0.0,1.0,0.0
elasticapm,Elastic yml:,0.0,0.0,1.0,0.0
elasticapm,APM Server yml:,0.0,0.0,1.0,0.0
elasticapm,I am currently new to Elastic APM.,0.0,0.0,1.0,0.0
elasticapm,"I am currently developing an application using spring-webflux and want to monitor my application using Elastic APM, but unfortunately, it's not working for me.",0.123,0.046,0.832,-0.4497
elasticapm,Dependecies,0.0,0.0,1.0,0.0
elasticapm,"APM Java Agent Version -  1.8.0
Elastic search - 7.2.0
APM server - 7.2.0",0.0,0.0,1.0,0.0
elasticapm,Exception observed -,0.0,0.0,1.0,0.0
elasticapm,Could someone please suggest what I am missing ?,0.232,0.242,0.526,0.0258
elasticapm,I am trying to check for the instrumentation of a function which runs asynchronously through the @Async annotation in spring boot.,0.0,0.0,1.0,0.0
elasticapm,"I am using the following elastic APM library : 
  co.elastic.apm:elastic-apm-agent:1.9.0",0.0,0.0,1.0,0.0
elasticapm,"As of now, with general configurations, I am able to see the instrumentation of API requests and scheduled jobs (@Scheduled annotation in spring boot), but I am not able to check the details of @Async annotated functions.",0.0,0.0,1.0,0.0
elasticapm,Is there any specific configuration that I need to do?,0.0,0.0,1.0,0.0
elasticapm,"As far as I know, it is available in newrelic.",0.0,0.0,1.0,0.0
elasticapm,It would be great if someone can point to a documentation / example for the same in elastic APM.,0.0,0.204,0.796,0.6249
elasticapm,Thanks in advance.,0.0,0.592,0.408,0.4404
elasticapm,I am getting errors with ElasticAPM version 5.5.2 while running,0.231,0.0,0.769,-0.34
elasticapm,"Error: logging_config_func(self.LOGGING) File &quot;/usr/lib/python2.7/logging/config.py&quot;, line 795, in dictConfig dictConfigClass(config).configure() File &quot;/usr/lib/python2.7/logging/config.py&quot;, line 577, in configure '%r: %s' % (name, e)) ValueError: Unable to configure handler 'elasticapm': Cannot resolve 'elasticapm.contrib.django.handlers.LoggingHandler': No module named apps",0.137,0.053,0.811,-0.4581
elasticapm,"Django: 1.6.11
Python: 2.7
ElastcAPM: 5.5.2",0.0,0.0,1.0,0.0
elasticapm,I have a JavaScript application and configured Elastic APM/RUM on that.,0.0,0.0,1.0,0.0
elasticapm,As soon as I start to interact with the application some metrics start sending to APM Server (page_load for example).,0.0,0.0,1.0,0.0
elasticapm,I want to know if it's possible to send a specific text message to APM Server.,0.0,0.091,0.909,0.0772
elasticapm,Example:,0.0,0.0,1.0,0.0
elasticapm,I'm trying to inject myDebugMessage this way:,0.0,0.0,1.0,0.0
elasticapm,payload.myDebugMessage = debugMessage,0.0,0.0,1.0,0.0
elasticapm,See the code below:,0.0,0.0,1.0,0.0
elasticapm,I have a standalone JAVA application.,0.0,0.0,1.0,0.0
elasticapm,And have integrated it successfully with Elastic APM (+ElasticSearch +Kibana) for capturing telemetries.,0.0,0.211,0.789,0.4939
elasticapm,"Java Version:  8 - OpenJDK 
 Elastic Agent &amp; Library Version:  1.16 
 Elastic Search, APM and Kibana Version:  7.7.1",0.0,0.0,1.0,0.0
elasticapm,Below are the relevant JVM Options being used:,0.0,0.0,1.0,0.0
elasticapm,However some out of the box transactions from Quartz(which is being used in the application) are shown as expected.,0.0,0.0,1.0,0.0
elasticapm,Which should mean the integration with the Elastic APM Server is fine.,0.0,0.141,0.859,0.2023
elasticapm,"It appears to me, even though the transactions are being captured successfully, those are not reported(sent) to the APM Server",0.0,0.144,0.856,0.4939
elasticapm,Refer some relevant apm logs:,0.0,0.0,1.0,0.0
elasticapm,I have a standalone Python application.,0.0,0.0,1.0,0.0
elasticapm,The python process is not using any framework.,0.0,0.0,1.0,0.0
elasticapm,And is a simple standalone python process.,0.0,0.0,1.0,0.0
elasticapm,This has been successfully integrated with Elastic APM (+ElasticSearch +Kibana) for capturing telemetries.,0.0,0.211,0.789,0.4939
elasticapm,"Python version:  3.7 
 elastic-apm python agent:  5.8.0 
 Elastic Search, APM and Kibana Version:  7.7.1 
 
As mentioned in the  official doc , I have used the following statements to start capturing metrics from my python process",0.0,0.0,1.0,0.0
elasticapm,"But on Kibana, I am able to see only the following 3 system metrics (under 2 visualizations):",0.0,0.0,1.0,0.0
elasticapm,"As per the  python code  analysis, as well as per what I have  read .",0.0,0.149,0.851,0.2732
elasticapm,Elastic APM Agent collects other process related metrics like:,0.0,0.238,0.762,0.3612
elasticapm,"Refer the screenshot 
 
 
 
 Additionally, I expect the Elastic APM Python agent to collect other informations like :",0.0,0.152,0.848,0.3612
elasticapm,Which are already available for Elastic APM Java agent.,0.0,0.0,1.0,0.0
elasticapm,Refer the screenshot,0.0,0.0,1.0,0.0
elasticapm,"As of a few weeks ago we added filebeat, metricbeat and apm to our dotnet core application ran on our kubernetes cluster.",0.0,0.0,1.0,0.0
elasticapm,It works all nice and recently we discovered filebeat and metricbeat are able to write a different index upon several rules.,0.0,0.128,0.872,0.4215
elasticapm,"We wanted to do the same for APM, however  searching the documentation  we can't find any option to set the name of the index to write to.",0.0,0.0,1.0,0.0
elasticapm,"Is this even possible, and if yes how is it configured?",0.0,0.213,0.787,0.4019
elasticapm,I also tried finding the current name  apm-*  within the codebase but couldn't find any matches upon configuring it.,0.0,0.0,1.0,0.0
elasticapm,The problem which we'd like to fix is that every space in kibana gets to see the apm metrics of every application.,0.107,0.099,0.794,-0.0516
elasticapm,Certain applications shouldn't be within this space so therefore i thought a new  apm-application-*  index would do the trick...,0.0,0.116,0.884,0.2732
elasticapm,Since it shouldn't be configured on the agent but instead in the cloud service console.,0.0,0.0,1.0,0.0
elasticapm,I'm having troubles to 'user-override' the settings to my likings.,0.25,0.0,0.75,-0.4588
elasticapm,The rules i want to have:,0.0,0.245,0.755,0.0772
elasticapm,I see you can add  output.elasticsearch.indices  to make this happen:  Array of index selector rules supporting conditionals and formatted string.,0.0,0.139,0.861,0.4404
elasticapm,I tried this by copying the same i had for metricbeat and updated it to use the apm syntax and came to the following 'user-override',0.0,0.0,1.0,0.0
elasticapm,but when i use this setup it tells me:,0.0,0.0,1.0,0.0
elasticapm,Then i updated the example but came to the same conclusion as it was not valid either..,0.0,0.0,1.0,0.0
elasticapm,Currently spring boot micorservices are enabled in Elastic APM.,0.0,0.0,1.0,0.0
elasticapm,We can also trace at method level and DB queries are shown.,0.0,0.0,1.0,0.0
elasticapm,But Spring batch job(Spring boot based) does not show any method level details and Oracle transaction details.,0.0,0.0,1.0,0.0
elasticapm,Does anything needs to be explicitly configured in Elastic APM for Spring batch applications.,0.0,0.0,1.0,0.0
elasticapm,logstash 's elasticsearch output has option to turn off SSL verification,0.0,0.0,1.0,0.0
elasticapm,https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-ssl_certificate_verification,0.0,0.0,1.0,0.0
elasticapm,Is there a similar option for apm?,0.0,0.0,1.0,0.0
elasticapm,"Elasticsearch is using self signed certificate, and apm is complaining when connecting to ES.",0.122,0.0,0.878,-0.2023
elasticapm,I have setup two Elastic APM servers which sends data to Elastic-search.,0.0,0.0,1.0,0.0
elasticapm,"When I setup Elastic-search with single instance, Elastic APM server working fine responding 202 response.",0.0,0.122,0.878,0.2023
elasticapm,"When I setup Elastic-search cluster(3 master, 3 data nodes, 2 ingest, 2 coordinate nodes) , APM server server misbehaves responding interleaved 202 and 503
Using coordinate host in Elastic APM.",0.098,0.0,0.902,-0.3818
elasticapm,This link  https://www.elastic.co/guide/en/apm/server/master/common-problems.html#queue-full   has some info but I am unable to resolve issue.,0.0,0.236,0.764,0.5267
elasticapm,"Edited: 
Problem is one of Elastic APM server is not able to push data on ES cluster due to index.",0.124,0.0,0.876,-0.4019
elasticapm,But I am still don't know why index name is different on single ES node vs Cluster,0.0,0.0,1.0,0.0
elasticapm,Currently have Elastic Apm setup with:  app.UseAllElasticApm(Configuration);  which is working correctly.,0.0,0.0,1.0,0.0
elasticapm,I've just been trying to find a way to record exactly how many SQL Queries are run via Entity Framework for each transaction.,0.0,0.0,1.0,0.0
elasticapm,Ideally when viewing the Apm data in Kibana the metadata tab could just include an  EntityFramework.ExecutedSqlQueriesCount .,0.0,0.157,0.843,0.4215
elasticapm,Currently on .Net Core 2.2.3,0.0,0.0,1.0,0.0
elasticapm,I have a react application with SSR implementation using node js Express.,0.0,0.0,1.0,0.0
elasticapm,I am trying to instrument the application with elastic APM.,0.0,0.0,1.0,0.0
elasticapm,Following  Elastic APM   docs I added below changes to index js file to start APM agent.,0.0,0.0,1.0,0.0
elasticapm,index.js :,0.0,0.0,1.0,0.0
elasticapm,"Getting below message on local env console: APM Server transport error
(ECONNREFUSED): connect ECONNREFUSED 127.0.0.1:8200",0.162,0.0,0.838,-0.4019
elasticapm,On Kibana APM dashboard UI we get Avg.response time N/A and Trans.per minute is 0.,0.0,0.0,1.0,0.0
elasticapm,Please suggest how this can be debugged.,0.0,0.277,0.723,0.3182
elasticapm,Reference Article:  Distributed Tracing in your Kibana with Node.JS,0.0,0.0,1.0,0.0
elasticapm,I am using elastic-apm with spring application to monitor API requests and track all SQL's executed for given endpoint.,0.0,0.0,1.0,0.0
elasticapm,The problem is give the amount of traffic elastic search is collecting huge magnitude of data and I would like to enable capturing span only for specific endpoints.,0.086,0.152,0.762,0.2732
elasticapm,"I tried using public api of elastic-apm  https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html 
I can customize a transaction and span but I couldn't find a way to enable/disable to specific endpoints.",0.0,0.0,1.0,0.0
elasticapm,I have tried this but no luck -,0.259,0.37,0.37,0.296
elasticapm,I have a very basic Django APM agent setup:,0.0,0.0,1.0,0.0
elasticapm,My APM server is up and running on localhost:8200.,0.0,0.0,1.0,0.0
elasticapm,But it seems my APM agent can't make a connection to the APM server.,0.0,0.0,1.0,0.0
elasticapm,Here is a part of my log file that I think cause the problem:,0.197,0.0,0.803,-0.4019
elasticapm,Here are my APM logs:,0.0,0.0,1.0,0.0
elasticapm,"On my APM server, I'm not receiving any requests from my agent.",0.0,0.0,1.0,0.0
elasticapm,I checked the APM server log files.,0.0,0.0,1.0,0.0
elasticapm,I'm trying out the .Net agent in Elastic APM and I'm using a C# application which is created using a framework called ASP.net Boilerplate.,0.0,0.087,0.913,0.25
elasticapm,I've added the core libraries as mentioned in the documentation and added the settings in appsettings.json.,0.0,0.0,1.0,0.0
elasticapm,This enables the default instrumentation and I got traces in the APM visualized through Kibana.,0.0,0.0,1.0,0.0
elasticapm,Currently I've got a node.js application running and I publish a message to a RabbitMQ queue with the traceparent in the message payload.,0.0,0.0,1.0,0.0
elasticapm,The C# app reads the published message.,0.0,0.0,1.0,0.0
elasticapm,I need to create a transaction or span using this traceparent / trace id so that Kibana would show the trace among the distributed systems.,0.0,0.091,0.909,0.2732
elasticapm,I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.,0.0,0.129,0.871,0.34
elasticapm,I've checked the Elastic APM agent documentation -&gt; Public API for information but couldnt find any information on this.,0.0,0.0,1.0,0.0
elasticapm,Is there a way?,0.0,0.0,1.0,0.0
elasticapm,Thanks.,0.0,1.0,0.0,0.4404
elasticapm,I'm trying to figure out a way to add a custom implementation to the Elastic APM .Net Agent code.,0.0,0.0,1.0,0.0
elasticapm,Does anyone know how to set it up in Visual Studio 2019?,0.0,0.0,1.0,0.0
elasticapm,Any references that I could use to refer on setting it up.,0.0,0.0,1.0,0.0
elasticapm,Thanks.,0.0,1.0,0.0,0.4404
elasticapm,I'm creating custom spans for outgoing requests for my java app.,0.0,0.328,0.672,0.5267
elasticapm,"It works and it's great ;)
But... when I compare my custom span on Kibana's APM board with other spans from springboot applications, which are created by elastic-apm-agent, then I can see that my spans are very low on additional details.",0.053,0.176,0.771,0.6815
elasticapm,I would like to have at least the URL details included in my custom span.,0.0,0.161,0.839,0.3612
elasticapm,The apm-agent-api doesn't allow this.,0.294,0.0,0.706,-0.1695
elasticapm,"Is there a way to set additional details, like url, to custom span?",0.0,0.185,0.815,0.3612
elasticapm,(I don't want to use labels for this),0.149,0.0,0.851,-0.0572
elasticapm,Thanks,0.0,1.0,0.0,0.4404
elasticapm,I want to install apm elastic for log errors and transactions in lumen.,0.175,0.095,0.73,-0.2732
elasticapm,I found a package for elastic called anik/elastic-apm-php but when I install with composer I see below error,0.215,0.0,0.785,-0.5499
elasticapm,How to use elastic log monitoring?,0.0,0.0,1.0,0.0
elasticapm,I am planning to deploy ELK to monitor my application running in AWS.,0.0,0.0,1.0,0.0
elasticapm,My apps are using AWS xray for trace data.,0.0,0.0,1.0,0.0
elasticapm,I am reading the doc about elastic APM to see how to ingest AWS xray to elasticsearch but I can't find any solution.,0.109,0.0,0.891,-0.3491
elasticapm,I have read the agent doc  https://www.elastic.co/guide/en/apm/agent/nodejs/3.x/intro.html  but xray is not listed as supported framework.,0.158,0.0,0.842,-0.3491
elasticapm,Does this mean I need to build a xray agent and send the trace data to APM server?,0.0,0.0,1.0,0.0
elasticapm,Or is there an easier way to do that?,0.0,0.259,0.741,0.4215
elasticapm,I am using  https://github.com/elastic/apm-agent-nodejs  in a node application to send trace data to Elastic APM Server.,0.0,0.0,1.0,0.0
elasticapm,I will send trace data to APM server and I am able to view them via Kibana.,0.0,0.0,1.0,0.0
elasticapm,"The trace id, start/end time is calculated by APM client or server.",0.0,0.0,1.0,0.0
elasticapm,How can I provide my own trace id and start/end time for each transaction and its spans?,0.0,0.0,1.0,0.0
elasticapm,"I'm able to install elasticseach and kibana, both are up and running.",0.0,0.0,1.0,0.0
elasticapm,"In Kibana dashboard APM server is setup, and indices are showing up.",0.0,0.0,1.0,0.0
elasticapm,I am getting the following error for APM-Agent when I trace the log.,0.213,0.0,0.787,-0.4019
elasticapm,ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Error trying to connect to APM Server.,0.467,0.0,0.533,-0.7297
elasticapm,Some details about SSL configurations corresponding the current connection are logged at INFO level.,0.0,0.0,1.0,0.0
elasticapm,ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Failed to handle event of type JSON_WRITER withthis error: connect timed out,0.462,0.0,0.538,-0.8567
elasticapm,APM-Agent Yaml File,0.0,0.0,1.0,0.0
elasticapm,ElasticSearch Yaml,0.0,0.0,1.0,0.0
elasticapm,Kibana Yaml,0.0,0.0,1.0,0.0
elasticapm,APM Server Yaml,0.0,0.0,1.0,0.0
elasticapm,"I receive logs from opentelemetry-collector in Elastic APM
logs structure :",0.0,0.0,1.0,0.0
elasticapm,&quot;{Timestamp:HH:mm:ss} {Level:u3} trace.id={TraceId} transaction.id={SpanId}{NewLine}{Message:lj}{NewLine}{Exception}&quot;,0.0,0.0,1.0,0.0
elasticapm,example:,0.0,0.0,1.0,0.0
elasticapm,I tried use pipeline,0.0,0.0,1.0,0.0
elasticapm,My goal see logs in Elastic APM,0.0,0.0,1.0,0.0
elasticapm,I have the following simple code below to run APM with logs.,0.0,0.0,1.0,0.0
elasticapm,APM is firing stats and looks like it works great.,0.15,0.412,0.438,0.6369
elasticapm,"But the problems that the errors are not appearing anywhere in Kibana 
What am I missing?",0.441,0.0,0.559,-0.8573
elasticapm,Thanks,0.0,1.0,0.0,0.4404
elasticapm,"Elasticsearch,kibana and apm-server are installed in a ec2 server
I have installed automatic java agent attach to another server to track jenkins app",0.0,0.0,1.0,0.0
elasticapm,Agent is getting attached to the process but dynamic configuration options are not working,0.0,0.207,0.793,0.5267
elasticapm,Apmagent directory: (command ls),0.0,0.0,1.0,0.0
elasticapm,apm-agent-attach-standalone.jar elasticapm.properties,0.0,0.0,1.0,0.0
elasticapm,elasticapm.properties file,0.0,0.0,1.0,0.0
elasticapm,Attach Command:,0.0,0.0,1.0,0.0
elasticapm,sudo java -jar apm-agent-attach-standalone.jar --include '.,0.0,0.0,1.0,0.0
elasticapm,jenkins. ',0.0,0.0,1.0,0.0
elasticapm,-&gt;This doesn't pick configuration file but attached the agent,0.0,0.0,1.0,0.0
elasticapm,so i used below command to update,0.0,0.0,1.0,0.0
elasticapm,Log:,0.0,0.0,1.0,0.0
elasticapm,Query:,0.0,0.0,1.0,0.0
elasticapm,1.Which is the right way to use the configuration options in command line?,0.0,0.0,1.0,0.0
elasticapm,2.Do we need to create a log file or it will create if log_file is used..now its polluting the application log,0.0,0.189,0.811,0.4939
elasticapm,I am implementing apm agent to sonarqube and I can connect to apm server using curl,0.0,0.0,1.0,0.0
elasticapm,curl -v &quot;&quot; returns 200,0.0,0.0,1.0,0.0
elasticapm,Command:,0.0,0.0,1.0,0.0
elasticapm,sudo java -jar apm-agent-attach-standalone.jar --include '.,0.0,0.0,1.0,0.0
elasticapm,sonar. ',0.0,0.0,1.0,0.0
elasticapm,---config config_file=elasticapm.properties,0.0,0.0,1.0,0.0
elasticapm,Command output:,0.0,0.0,1.0,0.0
elasticapm,It shows agent as attached,0.0,0.0,1.0,0.0
elasticapm,Logfile:,0.0,0.0,1.0,0.0
elasticapm,Query:,0.0,0.0,1.0,0.0
elasticapm,It shows access denied exception ... How to fix this?,0.244,0.0,0.756,-0.4404
elasticapm,"Note: 
For server_url i tried using properties file, --config server_url, --args server_url which usually works",0.0,0.0,1.0,0.0
elasticapm,"for the code below I wanted to create a new tab which would store all the external method calls and would be aggregated, but I am not getting any such results.",0.0,0.054,0.946,0.1406
elasticapm,I was looking at APM tools.,0.0,0.0,1.0,0.0
elasticapm,Essentially Dynatrace and I could see that it also provides tracing capabilities that seem to be language agnostic and also without code modifications.,0.0,0.0,1.0,0.0
elasticapm,Where would jaeger/open tracing be a better option than a tool like dynatrace?,0.0,0.375,0.625,0.6597
elasticapm,"Yes, dynatrace (or others like Elastic APM) is capable of providing a lot more insight into an application other than tracing.",0.0,0.315,0.685,0.7783
elasticapm,"But just from tracing perspective...
What advantages or capabilities does jaeger have that are better than APM tooling or not available in APMs.",0.0,0.253,0.747,0.7964
elasticapm,ONLY from the tracing perspective.,0.0,0.0,1.0,0.0
elasticapm,"I'm using Kafka for a queue, with Node services producing and consuming messages to Kafka topics using  Kafka-Node .",0.0,0.0,1.0,0.0
elasticapm,"I've been using a home-brewed distributed tracing solution, but now we are moving to the Elastic APM.",0.0,0.099,0.901,0.1655
elasticapm,"This seems to be tailored to HTTP servers, but how do I configure it to work with Kafka?",0.0,0.0,1.0,0.0
elasticapm,"I want to be able to track transactions like the following: Service A sends an HTTP request to Service B, which produces it to Kafka Topic C, from which it is consumed by Service D, which puts some data into Kafka Topic E, from which it is consumed by Service B.",0.0,0.075,0.925,0.4215
elasticapm,I have a very simple piece of code written in node.js which runs on Kubernetes and AWS.,0.0,0.0,1.0,0.0
elasticapm,The app just does POST/GET request to create and get data from other services.,0.0,0.139,0.861,0.2732
elasticapm,service1-- service2- service3,0.0,0.0,1.0,0.0
elasticapm,"Service1 get post request and call service2, service2 calls postgres DB (using sequlize) and create a new row and then call service3, service3 get data from the DB and returns the response to service2, service2 returns the response to service1.",0.0,0.052,0.948,0.2732
elasticapm,"Most of the times it works, but once in 4-5 attempts + concurrency, it dropped and I got a timeout.",0.0,0.0,1.0,0.0
elasticapm,the problem is that the service1 receives the response back (according to the logs and network traces) but it seems that the connection was dropped somewhere between the services and I got a timeout ( ESOCKETTIMEDOUT ).,0.053,0.0,0.947,-0.2144
elasticapm,Is it possible Kubernetes drops my connection?,0.0,0.0,1.0,0.0
elasticapm,I'm working on a corporate ASP.NET Core application that needs to reach out to a cloud resource (Elastic APM).,0.0,0.064,0.936,0.0258
elasticapm,"Unfortunately, our corporate proxy is swatting down the request before it can complete.",0.167,0.0,0.833,-0.34
elasticapm,"It's not my code that making the requests but code within a NuGet package, so I can't easily change how it's making the connection.",0.124,0.0,0.876,-0.4537
elasticapm,I'm hoping to use middleware to inject the proxy details before it goes out the door.,0.0,0.157,0.843,0.4215
elasticapm,"I've tried this below, but it doesn't seem to be working:",0.0,0.0,1.0,0.0
elasticapm,"Long-term, I am going to whitelist the cloud resource.",0.0,0.0,1.0,0.0
elasticapm,"But as I prototype this solution, I'd prefer not to go through that red tape...",0.0,0.209,0.791,0.5326
elasticapm,"I am using elastic apm agent for monitoring, I have to download the apm-agent.jar and included it in my start entry point like  java -javaagent:/path/to/apm-agent.jar app.jar .",0.0,0.098,0.902,0.3612
elasticapm,"The problem is I have to manually download the apm-agent.jar, is there a way that I can configure the apm agent in my Gradle dependencies?",0.114,0.0,0.886,-0.4019
elasticapm,and then refer to the path of the jar file that was downloaded by gradle in the Dockerfile?,0.0,0.0,1.0,0.0
elasticapm,What is the proper way of dependency management for jar files like java agent?,0.0,0.161,0.839,0.3612
elasticapm,I'm using the  Elastic APM Agent  as a Java Agent to monitor usages of various methods in my spring boot microservice.,0.0,0.0,1.0,0.0
elasticapm,This all works fine and we're able to graph various metrics in Kibana.,0.0,0.13,0.87,0.2023
elasticapm,Unfortunately what it doesn't do is consistently attach the same labels to all spans within a transaction e.g.,0.13,0.0,0.87,-0.34
elasticapm,details of the user who made the original request.,0.0,0.223,0.777,0.3182
elasticapm,To work around this I thought I could use ByteBuddy (which I've never used before) to wrap any usages of the APM Span class and attach that information (as it's readily available from ThreadLocal) to each instance.,0.0,0.0,1.0,0.0
elasticapm,I am however having issues accessing the Span class as it's in the APM Java Agent and with the following code I get the following logs where it appears that it's not able to find the Span class...,0.0,0.0,1.0,0.0
elasticapm,"I've tried using ByteBuddy for my own classes and it all works without issue, but I'm getting very confused around which classloader has loaded what and how to point ByteBuddy at them.",0.099,0.0,0.901,-0.5251
elasticapm,What is the right way to configure/enable an Elastic APM agent in a Nuxtjs project?,0.0,0.0,1.0,0.0
elasticapm,I referred  this documentation  for a custom NodeJS app.,0.0,0.0,1.0,0.0
elasticapm,The key takeaway was:,0.0,0.0,1.0,0.0
elasticapm,"It’s important that the agent is started before you require any other
modules in your Node.js application - i.e.",0.0,0.096,0.904,0.2023
elasticapm,"before http and before your
router etc.",0.0,0.0,1.0,0.0
elasticapm,"I added the following snippet in nuxt.config.js, but the APM agent is not started or working.",0.0,0.0,1.0,0.0
elasticapm,I do not see any errors in the app logs.,0.0,0.203,0.797,0.2584
elasticapm,Is there any other way to do this?,0.0,0.0,1.0,0.0
elasticapm,"I monitor my jar using Elastic APM Agent, i run these commands manually :",0.0,0.0,1.0,0.0
elasticapm,"Now , i want to pass these parameters using docker run , i create the image and i try with this command to pass these settings , but the application is not starting",0.0,0.097,0.903,0.1779
elasticapm,any idea to resolve this ?,0.0,0.394,0.606,0.3818
elasticapm,Thanks,0.0,1.0,0.0,0.4404
elasticapm,"I just created one application with springboot , used the Elastic APM attacher of APM tool.",0.0,0.133,0.867,0.25
elasticapm,"When i run the apm attacher, it generates error exception as shown below.",0.197,0.0,0.803,-0.4019
elasticapm,Code used to generate the error:,0.351,0.0,0.649,-0.4019
elasticapm,"But if I try to run the application commenting the line i.e //ElasticApmAttacher.attach(); from same code, it runs successfully",0.0,0.202,0.798,0.6486
elasticapm,i am searching for solution but so far clueless.,0.324,0.146,0.53,-0.462
elasticapm,Can someone please suggest how to resolve it,0.0,0.45,0.55,0.5994
elasticapm,I wish to make a change to a standard singleton design pattern that follows the  System.Lazy&lt;T&gt;  design as described  here .,0.0,0.144,0.856,0.4019
elasticapm,The change is to  Agent  in Elastic APM Agent which can be seen in GitHub  here .,0.0,0.0,1.0,0.0
elasticapm,This is the code broken down for brevity:,0.307,0.0,0.693,-0.4767
elasticapm,"The clear problem with this is that if  Agent.Instance  is accessed before  Agent.Setup  is called, the  Foo  object in  Agent.Lazy  is instantiated with null ( _bar ) passed to its constructor.",0.084,0.08,0.836,-0.0258
elasticapm,"Therefore, the expectation that the  Bar  object passed to  Setup  will be used for the underlying  Foo  will not be met.",0.0,0.0,1.0,0.0
elasticapm,"The problem of course is that this is an antipattern as described  here , because this singleton is encapsulating global state.",0.124,0.0,0.876,-0.4019
elasticapm,As this link describes:,0.0,0.0,1.0,0.0
elasticapm,A singleton is a convenient way for accessing the service from anywhere in the application code.,0.0,0.0,1.0,0.0
elasticapm,"The model quickly falls apart when the service not only provides access to operations but also encapsulates state, which affects how other code behaves.",0.0,0.0,1.0,0.0
elasticapm,Application configuration is a good example of this.,0.0,0.326,0.674,0.4404
elasticapm,"In the best case, the configuration is read once at the application start and does not change for the entire lifetime of the application.",0.0,0.154,0.846,0.6369
elasticapm,"However, different configuration can cause a method to return different results although no visible dependencies have changed, i.e.",0.121,0.0,0.879,-0.296
elasticapm,the constructor and the method have been called with the same parameters.,0.0,0.0,1.0,0.0
elasticapm,"This can become an even bigger problem if the singleton state can change at runtime, either by rereading the configuration file or by programmatic manipulation.",0.176,0.0,0.824,-0.5994
elasticapm,Such code can quickly become very difficult to reason with:,0.237,0.0,0.763,-0.4201
elasticapm,"Without comments, an uninformed reader of the code above could not expect the values of  before  and  after  to be different, and could only explain it after looking into the implementation of the individual methods, which read and modify global state hidden in  Configuration  singleton.",0.049,0.0,0.951,-0.3089
elasticapm,The  article  advocates using DI to resolve this.,0.0,0.271,0.729,0.3818
elasticapm,"However, is there a simpler way to resolve this situation where DI is not possible or would involve too much of a refactor?",0.0,0.115,0.885,0.3818
elasticapm,I would like to send some labels along with the stacktrace to the Elastic APM server from the client side.,0.0,0.122,0.878,0.3612
elasticapm,"Using the  @elastic/apm-rum  javascript agent, I'm calling  apm.addLabels({ ... })  and passing my object.",0.0,0.0,1.0,0.0
elasticapm,"Looking at the request in my browser, the labels look like they are being sent properly.",0.0,0.143,0.857,0.3612
elasticapm,Here's the relevant part of the request.,0.0,0.0,1.0,0.0
elasticapm,"I am expecting that the  browser  and  os  labels show under the  Labels  tab in the error report, but instead I'm seeing this.",0.081,0.0,0.919,-0.2144
elasticapm,"So my question as an elk novice is, what additional configuration do I need to get the labels in the report?",0.0,0.0,1.0,0.0
elasticapm,I installed latest version of ELK stack on Azure (7.0.1).,0.0,0.0,1.0,0.0
elasticapm,I have apm-server on kubernetes with this docker image:  docker.elastic.co/apm/apm-server:7.0.1,0.0,0.0,1.0,0.0
elasticapm,"But, it's not connecting with elasticsearch server.",0.0,0.0,1.0,0.0
elasticapm,Error:,1.0,0.0,0.0,-0.4019
elasticapm,ERROR   pipeline/output.go:100  Failed to connect to backoff(elasticsearch( http://x.x.x.x:9200 )): Connection marked as failed because the onConnect callback failed: This Beat requires the default distribution of Elasticsearch.,0.377,0.0,0.623,-0.9236
elasticapm,"Please upgrade to the default distribution of Elasticsearch from elastic.co, or downgrade to the oss-only distribution of beats",0.0,0.119,0.881,0.3182
elasticapm,INFO    pipeline/output.go:93   Attempting to reconnect to backoff(elasticsearch( http://x.x.x.x:9200 )) with 11 reconnect attempt(s),0.0,0.0,1.0,0.0
elasticapm,INFO    [publisher]     pipeline/retry.go:189   retryer: send unwait-signal to consumer,0.0,0.0,1.0,0.0
elasticapm,INFO    [publisher]     pipeline/retry.go:191     done,0.0,0.0,1.0,0.0
elasticapm,INFO    [publisher]     pipeline/retry.go:166   retryer: send wait signal to consumer,0.0,0.0,1.0,0.0
elasticapm,INFO    [publisher]     pipeline/retry.go:168     done,0.0,0.0,1.0,0.0
elasticapm,INFO    elasticsearch/client.go:734     Attempting to connect to Elasticsearch version 7.0.1,0.0,0.0,1.0,0.0
elasticapm,"INFO    [request]       beater/common_handler.go:185    handled request {&quot;request_id&quot;: &quot;2e79d623-b8fb-4743-8b50-b516db256d5b&quot;, &quot;method&quot;: &quot;POST&quot;, &quot;URL&quot;: &quot;/intake/v2/events&quot;, &quot;content_length&quot;: -1, &quot;remote_address&quot;: &quot;10.0.11.11&quot;, &quot;user-agent&quot;: &quot;elastic-apm-node/2.11.0 elastic-apm-http-client/7.3.0&quot;, &quot;response_code&quot;: 202}",0.0,0.0,1.0,0.0
elasticapm,For logging in our microservice applications we simply log to stdout/console and the docker logging driver handles and redirects these logs somewhere e.g.,0.0,0.0,1.0,0.0
elasticapm,"gelf/logstash, fluentd, etc.",0.0,0.0,1.0,0.0
elasticapm,"Basically, we're following  12 factor  guidelines for logging.",0.0,0.0,1.0,0.0
elasticapm,This means that developers working on the application code don't need to know anything about the underlying logging solution (e.g.,0.0,0.108,0.892,0.3182
elasticapm,"Elasticsearch, Graylog, Splunk, etc) - it's entirely an ops/configuration concern.",0.0,0.0,1.0,0.0
elasticapm,In theory we should be able to change the underlying logging solution without any code changes.,0.0,0.133,0.867,0.3182
elasticapm,I'd like something similar for traces and my research has led me to OpenTracing.,0.0,0.161,0.839,0.3612
elasticapm,Developers shouldn't need to know the underlying tracing solution (e.g.,0.0,0.204,0.796,0.3182
elasticapm,"Jaeger, Zipkin, Elastic APM, etc) and as per logging; in theory we should be able to change the underlying tracing solution without any code changes.",0.0,0.087,0.913,0.3182
elasticapm,I've successfully got a .NET core POC sending traces to Jaeger using the  opentracing/opentracing-csharp  and  jaegertracing/jaeger-client-csharp  libraries.,0.0,0.176,0.824,0.4939
elasticapm,"I'm still trying to fully get my head around OpenTracing, but I'm wondering if there's a way to send traces to an OpenTracing compliant API without having to take a hard dependency on a particular solution such as Jaeger (i.e.",0.04,0.075,0.885,0.3291
elasticapm,the jaeger-client-csharp library).,0.0,0.0,1.0,0.0
elasticapm,Based on my understanding OpenTracing is just a standard.,0.0,0.0,1.0,0.0
elasticapm,Shouldn't I just be able to configure an OpenTracing endpoint with some sampling options without needing the jaeger-client-csharp library?,0.0,0.0,1.0,0.0
elasticapm,Or is it that the jaeger-client-csharp is not actually Jaeger specific and can actually send traces to any OpenTracing API?,0.0,0.0,1.0,0.0
elasticapm,"Example configuration shown below, which uses jaeger client library:",0.0,0.0,1.0,0.0
elasticapm,"I have the elk setup in a ec2 server.With Beats like metricbeat,filebeat,heartbeat.",0.0,0.217,0.783,0.3612
elasticapm,I have setup the elastic apm for some applications like jenkins &amp; sonarqube.,0.0,0.185,0.815,0.3612
elasticapm,"Now In uptime I can see only few monitors like sonarqube and jenkins
Other application are missing..",0.0,0.143,0.857,0.3612
elasticapm,When I see data from yesterday not available in elasticsearch for particular application,0.0,0.0,1.0,0.0
elasticapm,I have been using elastic APM for tracing a application.,0.0,0.0,1.0,0.0
elasticapm,I have a main Spring boot application and another spring based jar file(my own) which is included as a dependency in main Spring boot project.,0.0,0.0,1.0,0.0
elasticapm,"When I add add custom context &amp; indexed labels from main Spring boot project , elastic APM console does shows up that trace data.",0.0,0.0,1.0,0.0
elasticapm,"However , when I write some tracing code inside ( adding index label or adding custom context) inside that spring based project which is then included as a jar libaray inside my spring boot project , that is not shown up on console.",0.0,0.0,1.0,0.0
elasticapm,Does this mean I can only instrument only main project and not instrument included jar library&gt; I have configured packages for both main spring boot project as well included spring dependency.,0.0,0.07,0.93,0.2732
elasticapm,Any help is highly appreciated.,0.0,0.677,0.323,0.7425
elasticapm,"ElasticApm.currentTransaction().setLabel(&quot;test1&quot;, &quot;test2&quot;) 
 ElasticApm.currentTransaction().addCustomContext(&quot;test3&quot;, &quot;test4&quot;)",0.0,0.0,1.0,0.0
elasticapm,"Attached ELastic APM before starting spring boot app as :
 ElasticApmAttacher.attach(configMap);",0.0,0.0,1.0,0.0
elasticapm,I'm currently facing a very strange issue.,0.295,0.0,0.705,-0.2716
elasticapm,"I did some optimizations in my queries, which improved quite a lot the overall performance for my Django application's GET requests.",0.0,0.227,0.773,0.6124
elasticapm,"However, I'm still facing a few very slow ones (1000ms or more).",0.0,0.0,1.0,0.0
elasticapm,"Checking on Elastic APM, I noticed that for all those cases, there was a DB reconnection.",0.0,0.0,1.0,0.0
elasticapm,"While it's obvious those requests would take more time, it's still takes WAY more time than the amount required for the reconnection itself.",0.0,0.0,1.0,0.0
elasticapm,"I have set the DB_CONN_MAX_AGE to  None , therefore the connections themselves shouldn't be closed at all.",0.0,0.0,1.0,0.0
elasticapm,Which makes me think the reason for the disconnection itself could also indicate the cause for this.,0.0,0.0,1.0,0.0
elasticapm,The blue bars represent the amount of time a given transaction took.,0.0,0.0,1.0,0.0
elasticapm,The total amount of time for this particular request was 1599ms.,0.0,0.0,1.0,0.0
elasticapm,"The DB reconnection took 100ms, the the queries about ~20ms.",0.0,0.0,1.0,0.0
elasticapm,"Adding those up, gives a total time of 120ms.",0.0,0.0,1.0,0.0
elasticapm,I'm a bit clueless how to find out where the rest of the 1479ms.,0.172,0.0,0.828,-0.3612
elasticapm,"I did some load tests locally, but couldn't reproduce this particular issue.",0.0,0.0,1.0,0.0
elasticapm,"Of course is serializations, middlewares, authentication and other things that might add up some time to requests, but not nearly the 1479ms shown here.",0.0,0.0,1.0,0.0
elasticapm,"It certainly related to the DB connection itself, or better yet, something that happens before that.",0.0,0.275,0.725,0.6486
elasticapm,But I'm not sure how to diagnose it.,0.259,0.0,0.741,-0.3491
elasticapm,Specially being unable to reproduce this locally.,0.0,0.0,1.0,0.0
elasticapm,I'm open to any ideas that could lead to more information on how to solve this problem.,0.157,0.09,0.753,-0.3237
elasticapm,Or maybe someone had a similar experience and could share it with me?,0.0,0.167,0.833,0.296
elasticapm,I change some settings to elastic-apm.,0.0,0.0,1.0,0.0
elasticapm,https://www.elastic.co/guide/en/apm/server/current/configuration-process.html,0.0,0.0,1.0,0.0
elasticapm,I want to verify if the setting is actually changed.,0.0,0.14,0.86,0.0772
elasticapm,but not sure how to check ..,0.289,0.0,0.711,-0.3491
elasticapm,Is there an endpoint where I can view the current configuration?,0.0,0.0,1.0,0.0
elasticapm,I am using Elastic Search in my MVC Application and getting en error when adding migration.,0.162,0.0,0.838,-0.4019
elasticapm,Flow is:,0.0,0.0,1.0,0.0
elasticapm,There is an warnin message like that,0.0,0.294,0.706,0.3612
elasticapm,"The type 'Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6' registered in the application config file as an IDbInterceptor not be loaded.",0.0,0.0,1.0,0.0
elasticapm,Make sure that the assembly-qualified name is used and that the assembly is available to the running application.,0.0,0.119,0.881,0.3182
elasticapm,Running a .NET Core 3.1 API with an async Controller Method that runs multiple DatabaseRepository async methods.,0.0,0.0,1.0,0.0
elasticapm,I'm calling DatabaseRepository Tasks like this:,0.0,0.333,0.667,0.3612
elasticapm,DatabaseRepository method all look like this:,0.0,0.333,0.667,0.3612
elasticapm,"What I expected is that most of these async methods will run concurrently, but that doesn't happen.",0.0,0.0,1.0,0.0
elasticapm,How do I know?,0.0,0.0,1.0,0.0
elasticapm,With Elastic APM.,0.0,0.0,1.0,0.0
elasticapm,This is what I see:,0.0,0.0,1.0,0.0
elasticapm,https://cdn.discordapp.com/attachments/195830344715337728/757569429683830795/unknown.png,0.0,0.0,1.0,0.0
elasticapm,What do I do wrong?,0.508,0.0,0.492,-0.4767
elasticapm,"I have gke clusters, and I have elasticsearch deployments on elastic.co.",0.0,0.0,1.0,0.0
elasticapm,Now on my gke cluster I have network policies for each pod with egress and ingress rules.,0.0,0.0,1.0,0.0
elasticapm,My issue is that in order to use elastic APM I need to allow egress to my elastic deployment.,0.0,0.101,0.899,0.2263
elasticapm,Anyone has an idea how to do that?,0.0,0.0,1.0,0.0
elasticapm,"I am thinking either a list of IPs for elastic.co on the gcp instances to be able to whitelist them in my egress rules, or some kind of proxy between my gke cluster and elastic apm.",0.0,0.0,1.0,0.0
elasticapm,"I know a solution can be to have a local elastic cluster on gcp, but I prefer not to go this way.",0.0,0.088,0.912,0.1655
elasticapm,I have an Elastic APM-Server up and running and it has successfully established connection with Elasticsearch.,0.0,0.186,0.814,0.4939
elasticapm,Then I installed an Elastic APM Go agent:,0.0,0.0,1.0,0.0
elasticapm,It returned the following:,0.0,0.0,1.0,0.0
elasticapm,Then I setup the  ELASTIC_APM_SERVER_URL  and  ELASTIC_APM_SERVICE_NAME :,0.0,0.0,1.0,0.0
elasticapm,"However, I don't see the agent getting registered in the APM dashboard.",0.0,0.0,1.0,0.0
elasticapm,It isn't sending any data to the APM Server.,0.0,0.0,1.0,0.0
elasticapm,How do I make sure that the agent is running?,0.0,0.223,0.777,0.3182
elasticapm,How do I check the agent log as to why it isn't able to connect to the APM server?,0.0,0.0,1.0,0.0
elasticapm,I am trying to run  Java APM agent on Kubernetes with Springboot 2.3.1.RELEASE,0.0,0.0,1.0,0.0
elasticapm,I get the following error,0.474,0.0,0.526,-0.4019
elasticapm,[elastic-apm-server-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler -  Failed to handle event of type METRICS with this error: / by zero,0.451,0.0,0.549,-0.8707
elasticapm,It works well if I run it on VM with same Java version,0.0,0.16,0.84,0.2732
elasticapm,How I am using,0.0,0.0,1.0,0.0
elasticapm,"APM Agent language and version : Java, elastic-apm-agent-1.17.0.jar,1.16.0.jar, 1.15.0.jar",0.0,0.0,1.0,0.0
elasticapm,Java version,0.0,0.0,1.0,0.0
elasticapm,More logs,0.0,0.0,1.0,0.0
elasticapm,I tried with previous agent versions 1.16.0 &amp; 1.15.0 but I still get the same error.,0.215,0.0,0.785,-0.5499
elasticapm,Can anyone please help me.,0.0,0.625,0.375,0.6124
elasticapm,Thank you very much in advance,0.0,0.333,0.667,0.3612
elasticapm,"I'm working on a django project with postgres where table and field names are generated with
double quotes.",0.0,0.0,1.0,0.0
elasticapm,Anyone knows how can I disable this behavior?,0.0,0.0,1.0,0.0
elasticapm,[Model definition],0.0,0.0,1.0,0.0
elasticapm,[Migtation],0.0,0.0,1.0,0.0
elasticapm,[DDL Generated],0.0,0.0,1.0,0.0
elasticapm,[DDL Expected],0.0,0.0,1.0,0.0
elasticapm,[requirements],0.0,0.0,1.0,0.0
elasticapm,We are using  Elastic APM  for monitoring our APIs.,0.0,0.0,1.0,0.0
elasticapm,It shows queries status and useful information about the queries.,0.0,0.244,0.756,0.4404
elasticapm,I want to have the same information about the queries which are sent to Elasticsearch server.,0.0,0.085,0.915,0.0772
elasticapm,"I want to have information about queries, time, status code, etc.",0.0,0.126,0.874,0.0772
elasticapm,Is there any plugin in Elastic stack that I can use for this purpose?,0.0,0.0,1.0,0.0
elasticapm,"I'm using elastic apm for my go app during
 development i export env variable through the terminal and it's working.",0.0,0.0,1.0,0.0
elasticapm,but now i want to deploy the app so i need to read variables from the  .env file,0.0,0.088,0.912,0.1154
elasticapm,explanation,0.0,0.0,1.0,0.0
elasticapm,"i use  go.elastic.co/apm/module/apmhttp  at my app
and when  go.elastic.co/apm  read env variables, 
it can't see  ELASTIC_APM_SERVICE_NAME ,  ELASTIC_APM_SERVER_URL  or  ELASTIC_APM_SECRET_TOKEN 
that existed at my .env on my app files.",0.0,0.0,1.0,0.0
elasticapm,I'm trying out different approaches for microservices tracing (i'm mostly working with event-driven services using RabbitMQ).,0.0,0.0,1.0,0.0
elasticapm,What I'm testing:,0.0,0.0,1.0,0.0
elasticapm,Given the code,0.0,0.0,1.0,0.0
elasticapm,My questions / remarks / issues,0.0,0.0,1.0,0.0
elasticapm,"To have this new span, I'm using  ScopedSpan sp = tracer.startScopedSpanWithParent(""getUrlConnection"", tracer.currentSpan().context());  and  sp.finish() .",0.0,0.0,1.0,0.0
elasticapm,The span is visible in ZipKin but it's not really appealing compared to only putting a  @NewSpan .,0.0,0.0,1.0,0.0
elasticapm,Am I missing something ?,0.524,0.0,0.476,-0.296
elasticapm,Elasticsearch APM agent + API seems to handle this properly by just requiring the addition of  @CaptureTransaction  and  @CaptureSpan .,0.0,0.0,1.0,0.0
elasticapm,I know it's not perfect because it doesn't hook directly on the consumer call nor support tracing effectively with my use case.,0.299,0.0,0.701,-0.7692
elasticapm,But it also requires to add the agent.,0.0,0.0,1.0,0.0
elasticapm,Thank you :).,0.0,0.556,0.444,0.3612
jaeger,I have tried executing this docker command to setup Jaeger Agent and jaeger collector with elasticsearch.,0.0,0.0,1.0,0.0
jaeger,but this command gives the below error.,0.372,0.0,0.628,-0.5499
jaeger,How to configure Jaeger with ElasticSearch?,0.0,0.0,1.0,0.0
jaeger,There is an existing Spring Boot app which is using SLF4J logger.,0.0,0.0,1.0,0.0
jaeger,I decided to add the support of distributed tracing via standard  opentracing  API with Jaeger as the tracer.,0.0,0.144,0.856,0.4019
jaeger,It is really amazing how easy the initial setup is - all that is required is just adding two dependencies to the  pom.xml :,0.0,0.266,0.734,0.8051
jaeger,and providing the  Tracer  bean with the configuration:,0.0,0.0,1.0,0.0
jaeger,All works like a charm - app requests are processed by Jaeger and spans are created:,0.0,0.396,0.604,0.7351
jaeger,"However, in the span  Logs  there are only  preHandle  &amp;  afterCompletion  events with info about the class / method that were called during request execution (no logs produced by  slf4j  logger are collected) :",0.0,0.0,1.0,0.0
jaeger,The question is if it is possible to configure the Tracer to pickup the logs produced by the app logger ( slf4j  in my case) so that all the application logs done via:  LOG.info  /  LOG.warn  /  LOG.error  etc.,0.0,0.0,1.0,0.0
jaeger,would be also reflected in Jaeger,0.0,0.0,1.0,0.0
jaeger,NOTE : I have figured out how to log to span  manually  via  opentracing  API e.g.,0.0,0.0,1.0,0.0
jaeger,:,0.0,0.0,0.0,0.0
jaeger,And do some  manual  manipulations with the  ERROR  tag for exception processing in filters e.g.,0.197,0.0,0.803,-0.5319
jaeger,"But, I'm still wondering if it is possible to configure the tracer to pickup the application logs  automatically :",0.0,0.0,1.0,0.0
jaeger,UPDATE : I was able to add the application logs to the tracer by adding wrapper for the logger e.g.,0.0,0.0,1.0,0.0
jaeger,"However, so far I was not able to find opentracing configuration options that would allow to add the application logs to the tracer automatically by default.",0.0,0.073,0.927,0.2263
jaeger,"Basically, it seems that it is expected that dev would add extra logs to tracer programmatically if needed.",0.0,0.0,1.0,0.0
jaeger,"Also, after investigating tracing more it appeared to be that normally  logging  and  tracing  are handled separately and adding all the application logs to the tracer is not a good idea (tracer should mainly include sample data and tags for request identification)",0.057,0.0,0.943,-0.3412
jaeger,We're using Opentracing/Jaeger in Istio for tracing multiple Spring Boot/Spring Cloud based microservices.,0.0,0.0,1.0,0.0
jaeger,I'm currently wondering if there's an option to enrich the tracing spans by providing information about executed query (i.e.,0.0,0.0,1.0,0.0
jaeger,SQL statement)?,0.0,0.0,1.0,0.0
jaeger,Tracing JDBC connection info is working fine using  opentracing-contrib/java-spring-cloud  but additional information is missing.,0.173,0.086,0.741,-0.34
jaeger,I know that e.g.,0.0,0.0,1.0,0.0
jaeger,glowroot  is capable of tracing the statements itself but haven't found anything related to Opentracing or Jaeger.,0.0,0.101,0.899,0.2023
jaeger,Would be great if anybody could show some directions for research!,0.0,0.305,0.695,0.6588
jaeger,"I want to use istio with existing jaeger tracing system in K8S, I began with installing jaeger system following  the official link  with cassandra as backend storage.",0.0,0.051,0.949,0.0772
jaeger,"Then installed istio by  the helm way , but with only some selected components enabled:",0.0,0.0,1.0,0.0
jaeger,"Jaeger and istio are installed inside the same namespace  istio-sytem , after all done, all pods inside it looks like this:",0.0,0.116,0.884,0.3612
jaeger,"Then I followed  the link  to deploy the bookinfo sample into another namespace  istio-play , which has label  istio-injection=enabled , but no matter how I flush the  productpage  page, there's no tracing data be filled into jaeger.",0.152,0.031,0.816,-0.6652
jaeger,"I guess maybe tracing spans are sent to jaeger by mixer, like the way istio do all other telementry stuff, so I  -set mixer.enabled=true , but unfortunately only some services like  istio-mixer  or  istio-telementry  are displayed.",0.081,0.131,0.787,0.2263
jaeger,"Finally I cleaned up all the above installation and followed  this task  step by step, but the tracing data of bookinfo app are still not there.",0.0,0.0,1.0,0.0
jaeger,My questions is: How indeed istio send tracing data to jaeger?,0.0,0.0,1.0,0.0
jaeger,"Does sidecar proxy send it directly to jaeger-collector( zipkin.istio-system:9411 ) like  how envoy does , or the data flows like this:  sidecar-proxy -&gt; mixer -&gt; jaeger-collector ?",0.0,0.185,0.815,0.6124
jaeger,And how could I debug how the data flow between all kinds of components inside the istio mesh?,0.0,0.0,1.0,0.0
jaeger,Thanks for any help and info :-),0.0,0.664,0.336,0.7845
jaeger,"Update : I tried again by installing istio without helm:  kubectl -n istio-system apply -f install/kubernetes/istio-demo.yaml , this time everything works just fine, there must be something different between  kubectl way  and  helm way .",0.0,0.057,0.943,0.2023
jaeger,"I instrumented a simple Spring-Boot application with Jaeger, but when I run the application within a Docker container with docker-compose, I can't see any traces in the Jaeger frontend.",0.0,0.0,1.0,0.0
jaeger,I'm creating the tracer configuration by reading the properties from environment variables that I set in the docker-compose file.,0.0,0.115,0.885,0.296
jaeger,This is how I create the tracer:,0.0,0.296,0.704,0.2732
jaeger,And this is my docker-compose file:,0.0,0.0,1.0,0.0
jaeger,You can also find my project on  GitHub .,0.0,0.0,1.0,0.0
jaeger,What am I doing wrong?,0.508,0.0,0.492,-0.4767
jaeger,Created a deployment on  Google Kubernetes  using  jaegertracing/all-in-one  public image from  Docker Hub,0.0,0.154,0.846,0.25
jaeger,"Then, exposed the deployment with Service type as  LoadBalancer .",0.14,0.0,0.86,-0.0772
jaeger,"Now, launched the Jagger UI and it is working, but it do not show any service  except jagger-query .",0.0,0.068,0.932,0.0644
jaeger,I have deployed my .net web api application for testing on kubernetes.,0.0,0.0,1.0,0.0
jaeger,My application has only one single web API which is successfully running on Google Kubernetes engine and exposed via load balancer service type.,0.051,0.125,0.824,0.4404
jaeger,The API has all the data hard-coded and do not call any other service or database.,0.0,0.0,1.0,0.0
jaeger,Used following nuget packages in the project:,0.0,0.0,1.0,0.0
jaeger,"Jaeger -Version 0.2.2 
OpenTracing.Contrib.NetCore -Version 0.5.0",0.0,0.0,1.0,0.0
jaeger,I have added following code in  Startup.cs  file of my API:,0.0,0.0,1.0,0.0
jaeger,jaegerHost  environment variable is assigned the  IP  of the jaeger service created.,0.0,0.154,0.846,0.25
jaeger,"The question is how and where to make changes so that my application service gets available in the Jaeger UI, so that I can see it's traces.",0.0,0.0,1.0,0.0
jaeger,"I am stuck here, can anyone please help how to proceed ahead?",0.133,0.333,0.533,0.4588
jaeger,We are re-building our software platform using a microservice architecture approach.,0.0,0.0,1.0,0.0
jaeger,"Most of it using  Spring Boot  libraries, and for our entry points we are using  Spring Cloud Gateway  which can be easily integrated with  Jaeger  to have tracing in the platform.",0.0,0.074,0.926,0.34
jaeger,We do that using the following Maven dependency:,0.0,0.0,1.0,0.0
jaeger,"And it is working pretty well, we can see the trace on the  Jaeger UI  web console.",0.0,0.261,0.739,0.6486
jaeger,The problem now is how to send the tracking information to the next inner service in the platform to have every service correlated.,0.109,0.0,0.891,-0.4019
jaeger,Any ideas?,0.0,0.0,1.0,0.0
jaeger,We have tried  to inject the tracking information as HTML Headers using a  GlobalFilter  but it is using the incoming request and we need to put them on the downstream request... any clue on how to override  HTTPClient  used underneath.,0.0,0.0,1.0,0.0
jaeger,Tracing tasks in celery 4.1.1 using sample code.,0.0,0.0,1.0,0.0
jaeger,Each worker runs:,0.0,0.0,1.0,0.0
jaeger,When I first start celery and run tasks each worker gets a working tracer and there is a log output for each of:,0.0,0.0,1.0,0.0
jaeger,"Any task that runs after the initial gets the global tracer 
 from  Config.initialze_tracer  (which returns  None ) and a log warning  Jaeger tracer already initialized, skipping .",0.0,0.081,0.919,0.2584
jaeger,"Watching tcpdump on the console shows that the UDP packets aren't being sent, I think I'm getting an uninitialized default tracer and it's using the noop reporter.",0.0,0.0,1.0,0.0
jaeger,I've pored over the code in opentracing and jaeger_client and I can't find a canonical way around this.,0.0,0.0,1.0,0.0
jaeger,I have a problem using the jaeger open tracing project within our microservice system.,0.197,0.0,0.803,-0.4019
jaeger,The config I use is as below.,0.0,0.0,1.0,0.0
jaeger,The origin of the trace is the same as this.,0.0,0.0,1.0,0.0
jaeger,This works fine and is logged within the UI and with printed results below.,0.0,0.122,0.878,0.2023
jaeger,"But then when I extract {'uber-trace-id': '2b55203a8773aa14:77cceca94f4cfe74:0:1'} within another service it says a new span has been created as expected, but nothing is logged within the UI.",0.0,0.094,0.906,0.3612
jaeger,Print result:,0.0,0.0,1.0,0.0
jaeger,With both Format.HTTP_HEADERS and Format.TEXT_MAP it is the same result.,0.0,0.0,1.0,0.0
jaeger,Does anyone know why nothing is logged in the UI and how to fix this?,0.0,0.0,1.0,0.0
jaeger,Thanks in advance.,0.0,0.592,0.408,0.4404
jaeger,I am looking at open tracing implementations to trace the application to JaegerUI.,0.0,0.0,1.0,0.0
jaeger,"Our application Front end is angular, backend end is Asp.net Web api.",0.0,0.0,1.0,0.0
jaeger,I am able to trace the webapi using Jaeger C# nuget packages.,0.0,0.0,1.0,0.0
jaeger,"However, I have not found a way/ npm package to trace the angular trace messages to Jaeger.",0.0,0.0,1.0,0.0
jaeger,I understand there's a Jaeger npm package for node.js(server) but not for client javascript/typescript that runs in browser.,0.0,0.0,1.0,0.0
jaeger,Could you let me know how can we implement opentracing with Jaeger at front end(Javascript/angular)- so that we can see the full span traced (front end to backend) in Jaeger,0.0,0.0,1.0,0.0
jaeger,I installed the jaeger all in one in Docker with:,0.0,0.0,1.0,0.0
jaeger,And below is the sample code on how I initialize the tracer and spans.,0.0,0.0,1.0,0.0
jaeger,I get the logs in my console but it does not reflect in my Jaeger UI.,0.0,0.0,1.0,0.0
jaeger,Could anyone please help me with this?,0.0,0.5,0.5,0.6124
jaeger,"I'm trying to get a little example of Jaeger working using Node.js, but I can't get the Jaeger UI to display any data or show anything.",0.0,0.0,1.0,0.0
jaeger,I have read this question:  uber/jaeger-client-node: backend wont receive data  but this hasn't helped in my case.,0.0,0.0,1.0,0.0
jaeger,I'm running the Jaeger back end in a docker container using:,0.0,0.0,1.0,0.0
jaeger,The code for my example is:,0.0,0.0,1.0,0.0
jaeger,Any help as to what I am doing wrong would be much appreciated!,0.169,0.342,0.489,0.4926
jaeger,In a spring boot application (just one at the moment) I included jaeger by adding dependency  opentracing-spring-jaeger-web-starter  and the below beans,0.0,0.0,1.0,0.0
jaeger,After starting Jaeger in docker  docker run -d --name jaeger -p 16686:16686 -p 6831:6831/udp jaegertracing/all-in-one:1.9  I get the traces.,0.0,0.0,1.0,0.0
jaeger,I found now another dependency and read different tutorials which made me somehow unsure on what is the right way to use Jaeger with spring boot.,0.077,0.0,0.923,-0.25
jaeger,Which dependency would I use?,0.0,0.0,1.0,0.0
jaeger,https://github.com/opentracing-contrib/java-spring-cloud,0.0,0.0,1.0,0.0
jaeger,https://github.com/signalfx/tracing-examples/tree/master/jaeger-java-spring-boot-web,0.0,0.0,1.0,0.0
jaeger,Following the  Jaeger documentation  possibly,0.0,0.0,1.0,0.0
jaeger,would be enough!,0.0,0.0,1.0,0.0
jaeger,?,0.0,0.0,0.0,0.0
jaeger,Before trying Jaeger I used Zipkin which is very easy to integrate in Spring since there is a starter for sleuth.,0.0,0.151,0.849,0.4927
jaeger,The logs contain trace and span id's in separate fields so they can be searched for e.g.,0.0,0.0,1.0,0.0
jaeger,in Kibana.,0.0,0.0,1.0,0.0
jaeger,Jaeger does  not .,0.0,0.0,1.0,0.0
jaeger,Can that be customized and if so - how?,0.0,0.0,1.0,0.0
jaeger,Is it possibly to use Jaeger with Brave for instrumentation.,0.0,0.274,0.726,0.5267
jaeger,The project e.g.,0.0,0.0,1.0,0.0
jaeger,includes  spring-cloud-starter-sleuth  as a dependency.,0.0,0.0,1.0,0.0
jaeger,There are some conflicts with already existing beans.,0.271,0.0,0.729,-0.3818
jaeger,Can Jaeger be used with brave at all?,0.0,0.327,0.673,0.5267
jaeger,I have added these fields in application.yml of microservices and dependency in pom.xml.Jaeger running on my local is abl to identify the services as well,0.0,0.084,0.916,0.2732
jaeger,I have deployed all my microservices on kubernetes.,0.0,0.0,1.0,0.0
jaeger,Please help me in deploying jaeger on kubernetes.,0.0,0.455,0.545,0.6124
jaeger,"UPDATE: 
I have reached this step.",0.0,0.259,0.741,0.1027
jaeger,I have a load balancer IP for jaeger-query.,0.0,0.0,1.0,0.0
jaeger,But on which host and port will my microservice send the logs to  ?,0.0,0.0,1.0,0.0
jaeger,?,0.0,0.0,0.0,0.0
jaeger,"So, I am trying to trace logs of my spring boot application with jaeger so what are the steps that should be perform if my application and jaeger is deploy on kubernetes.",0.0,0.0,1.0,0.0
jaeger,"I have successfully deployed jaeger and spring boot application 
now how will I configure jaeger in my service.",0.0,0.176,0.824,0.4939
jaeger,My services are not visible in the jaegar console.,0.0,0.0,1.0,0.0
jaeger,I have added the following configuration to the yml:,0.0,0.0,1.0,0.0
jaeger,Output og kubectl get service jaeger-query,0.0,0.0,1.0,0.0
jaeger,"I'm using jaeger with spring boot to trace a test application, sometimes I get some extra space or overlap that appears between spans in a single-threaded trace that takes up to 20ms.",0.0,0.0,1.0,0.0
jaeger,I am confused about this extra space because there aren't any codes between these spans and I expected to see spans starting after each other.,0.095,0.0,0.905,-0.3182
jaeger,Here are my output results.,0.0,0.0,1.0,0.0
jaeger,We lately setup a jaeger server in order to trace all requests throughout our system.,0.0,0.0,1.0,0.0
jaeger,The initial setup worked pretty nicely by simply adding the necessary spring (cloud) starter dependencies to our build files.,0.0,0.264,0.736,0.7269
jaeger,"Each time, a request hits one of our servers, a new span is created and reported to the jaeger server which was setup by using the all-in-one docker image.",0.0,0.071,0.929,0.25
jaeger,The most important dependencies are the following:,0.0,0.259,0.741,0.2716
jaeger,"While spans are created on the server, the necessary headers are not forwarded to the feign clients.",0.0,0.111,0.889,0.25
jaeger,"According to the documentation, the addition of  opentracing-spring-cloud-feign-starter  dependency should to the trick, but so far, none of the feign clients worked.",0.05,0.0,0.95,-0.0258
jaeger,I also added a breakpoint to the auto configure class provided by opentracing,0.0,0.0,1.0,0.0
jaeger,"and this method is invoked, when the application starts up.",0.0,0.0,1.0,0.0
jaeger,There are also is also some information in the logs regarding the initialization of jaeger/opentracing:,0.0,0.0,1.0,0.0
jaeger,"I spent quite some time, reading into documentation and looking for examples how to configure a spring boot/cloud application correctly in order to work with feign clients but so far I had no luck.",0.088,0.111,0.801,0.2034
jaeger,Most examples out there use Springs' RestTemplate instead of Feign clients.,0.0,0.0,1.0,0.0
jaeger,"I would be very happy, if somebody could point me towards the right direction.",0.0,0.25,0.75,0.6115
jaeger,We are trying to go reactive with Webflux.,0.0,0.0,1.0,0.0
jaeger,We are using Jaegar with Istio for instrumentation purposes.,0.0,0.0,1.0,0.0
jaeger,"Jaegar understands Spring MVC endpoints well, but don't seem to work at all for WebFlux.",0.0,0.1,0.9,0.1406
jaeger,I am looking for suggestions to make my webflux endpoints appear in Jaeger.,0.0,0.0,1.0,0.0
jaeger,Thanks in advance.,0.0,0.592,0.408,0.4404
jaeger,I'm trying to set up a local k8s cluster and on  minikube  with installed  istio  and I have an issue with enabling distributed tracing with Jaeger.,0.0,0.0,1.0,0.0
jaeger,I have 3 microservices  A -&gt; B -&gt; C .,0.0,0.0,1.0,0.0
jaeger,I am propagating the all the headers that are needed:,0.0,0.0,1.0,0.0
jaeger,"But on Jaeger interface, I can only see the request to the service A and I cannot see the request going to service B.",0.0,0.0,1.0,0.0
jaeger,I have logged the headers that are sent in the request.,0.0,0.0,1.0,0.0
jaeger,Headers from service A:,0.0,0.0,1.0,0.0
jaeger,Headers from service B:,0.0,0.0,1.0,0.0
jaeger,"So the  x-request-id ,  x-b3-traceid ,  x-b3-sampled , and  x-b3-spanid  mathces.",0.0,0.0,1.0,0.0
jaeger,There are some headers that aren't set.,0.0,0.0,1.0,0.0
jaeger,"Also, I'm accessing service A via k8s Service IP of type LoadBalancer, not via ingress.",0.0,0.0,1.0,0.0
jaeger,Don't know if this could be the issue.,0.0,0.0,1.0,0.0
jaeger,UPD: I have setup istio gateway so now I'm accessing service  A  via istio gateway.,0.0,0.0,1.0,0.0
jaeger,"However the result is the same, I can see the trace for  gateway-&gt;A  but no any further tracing",0.149,0.0,0.851,-0.4215
jaeger,"After login to the Keycloak Jaeger(realm) client, the keycloak server doesn't navigate to the Jaeger UI path -  localhost:16686.",0.0,0.0,1.0,0.0
jaeger,It seems keycloak verifies the user (see below code),0.0,0.0,1.0,0.0
jaeger,proxy.json,0.0,0.0,1.0,0.0
jaeger,keycloak.json,0.0,0.0,1.0,0.0
jaeger,We have selected to use Jaeger API is used for tracing.,0.0,0.0,1.0,0.0
jaeger,"There , we have setup the Jaeger locally using docker as mentioned below.",0.0,0.0,1.0,0.0
jaeger,"In the  ServletContextListener , we created new cofigurations like below.",0.0,0.391,0.609,0.5423
jaeger,Now this works fine and I can see the tracing in  http://localhost:16686,0.0,0.153,0.847,0.2023
jaeger,"Problem :  
I want to make the Jager setup in an external environment and connect from another application server (application server is running on wildfly 10 docker under host mode).",0.09,0.043,0.867,-0.34
jaeger,In future that Jaeger instance may used by more than one server instances for tracings.,0.0,0.0,1.0,0.0
jaeger,After looking at the source and various references as mentioned below I tried below options.,0.0,0.0,1.0,0.0
jaeger,But it connects to local always.,0.0,0.0,1.0,0.0
jaeger,"I tried various ports like 5775, 6831, 6832 also but result was same.",0.0,0.137,0.863,0.1901
jaeger,Further I tried setting JAEGER_ENDPOINT and JAEGER_SAMPLER_MANAGER_HOST_PORT as environment variables also.,0.0,0.0,1.0,0.0
jaeger,But failed.,0.817,0.0,0.183,-0.6652
jaeger,"In one  reference  I found that  ""Jaeger client libraries expect jaeger-agent process to run locally on each host..."" .",0.0,0.0,1.0,0.0
jaeger,Is that means I can't use it in cebtrelized manner and I need to setup Jaeger in each application server instances?,0.0,0.0,1.0,0.0
jaeger,Otherwise how to do that?,0.0,0.0,1.0,0.0
jaeger,I am trying to integrate jaeger tracing in my java spring application.,0.0,0.0,1.0,0.0
jaeger,I have added following code in the configuration file :,0.0,0.0,1.0,0.0
jaeger,"@Bean
    public io.opentracing.Tracer jaegerTracer(){",0.0,0.0,1.0,0.0
jaeger,and used following docker command :,0.0,0.0,1.0,0.0
jaeger,docker run -d -p 5775:5775/udp -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14268:14268 jaegertracing/all-in-one:latest,0.0,0.0,1.0,0.0
jaeger,"Still, I am not able to find my service in jaeger-ui",0.0,0.0,1.0,0.0
jaeger,"Upon hitting this url :
 http://localhost:5778/?service=pilot-tracking 
Output is :
tcollector error: tchannel error ErrCodeBadRequest: no handler for service ""jaeger-collector"" and method ""SamplingManager::getSamplingStrategy""",0.309,0.0,0.691,-0.765
jaeger,Please help!,0.0,1.0,0.0,0.6476
jaeger,!,0.0,0.0,0.0,0.0
jaeger,I can only find old and incomplete examples of using opentracing/jaeger with Kafka.,0.0,0.0,1.0,0.0
jaeger,I want to run an example locally as a proof of concept - opentracing spans to kafka.,0.0,0.091,0.909,0.0772
jaeger,"I managed to get some of this working, but on  jeager-query  service I keep getting:",0.0,0.0,1.0,0.0
jaeger,I'm not sure if I need to use some sort of storage i.e.,0.151,0.0,0.849,-0.2411
jaeger,cassandra too?,0.0,0.0,1.0,0.0
jaeger,"I'm trying to instrument a Spring Cloud RxJava sample app with Jaeger, and for some reason I'm failing!",0.183,0.0,0.817,-0.5562
jaeger,"I have a couple of other SpringCloud apps, like Hystrix, JDBC and JMS working fine with the tracing being reported to Jeager by just adding the maven dependency to it.",0.0,0.142,0.858,0.5106
jaeger,"For RxJava, on the other hand, I can't figure it out why I'm not able to follow the same approach...",0.0,0.151,0.849,0.4939
jaeger,"When I leave the App without a  Tracer @Bean , I don't get anything in Jaeger and I get this message:",0.074,0.0,0.926,-0.0516
jaeger,"All the other SpringCloud apps are working without the  Tracer @Bean , so I was expecting the same behavior for the RxJava...",0.0,0.0,1.0,0.0
jaeger,"The worst part is that whenever I add the  Tracer @Bean , the bean is initialized, but still no data is sent to Jaeger...
Not sure if it is related to this message:",0.218,0.0,0.782,-0.7778
jaeger,Does anyone have any idea?,0.0,0.0,1.0,0.0
jaeger,Do I need to set anything in the  application.properties ?,0.0,0.0,1.0,0.0
jaeger,I'm posting below my  pom.xml  file:,0.0,0.0,1.0,0.0
jaeger,Sample app committed here:  https://github.com/julianocosta89/rxjava-jeager,0.0,0.344,0.656,0.2732
jaeger,I am trying to trace in a front end app.,0.0,0.0,1.0,0.0
jaeger,I am not be able to use  @opentelemetry/exporter-jaeger  since  I believe it is for Node.js back end app only .,0.0,0.0,1.0,0.0
jaeger,So I am trying to use  @opentelemetry/exporter-collector .,0.0,0.0,1.0,0.0
jaeger,First I tried to print the trace data in the browser console.,0.0,0.0,1.0,0.0
jaeger,"And
the code below succeed printing the trace data.",0.0,0.286,0.714,0.4939
jaeger,Now I want to forward them to Jaeger.,0.0,0.178,0.822,0.0772
jaeger,I am running  Jaeger all-in-one  by,0.0,0.0,1.0,0.0
jaeger,"Based on the  Jaeger port document , I might be able to use these two ports (if other ports work, that will be great too!",0.0,0.166,0.834,0.6588
jaeger,):,1.0,0.0,0.0,-0.4215
jaeger,Then I further found  more info about this port :,0.0,0.0,1.0,0.0
jaeger,Zipkin Formats (stable),0.0,0.0,1.0,0.0
jaeger,"Jaeger Collector can also accept spans in several Zipkin data format,
namely JSON v1/v2 and Thrift.",0.0,0.148,0.852,0.3818
jaeger,"The Collector needs to be configured to
enable Zipkin HTTP server, e.g.",0.0,0.0,1.0,0.0
jaeger,"on port 9411 used by Zipkin
collectors.",0.0,0.0,1.0,0.0
jaeger,"The server enables two endpoints that expect POST
requests:",0.0,0.0,1.0,0.0
jaeger,I updated my codes to,0.0,0.0,1.0,0.0
jaeger,"However, I got bad request for both v1 and v2 endpoints without any response body returned",0.2,0.0,0.8,-0.5423
jaeger,POST http://localhost:9411/api/v1/spans 400 (Bad Request),0.0,0.0,1.0,0.0
jaeger,POST http://localhost:9411/api/v2/spans 400 (Bad Request),0.0,0.0,1.0,0.0
jaeger,Any idea how can I make the request format correct?,0.0,0.0,1.0,0.0
jaeger,Thanks,0.0,1.0,0.0,0.4404
jaeger,I think Andrew is right that I should use OpenTelemetry collector.,0.0,0.0,1.0,0.0
jaeger,"I also got some help from Valentin Marchaud and Deniz Gurkaynak
at Gitter.",0.0,0.197,0.803,0.4019
jaeger,"Just add the link here for further people who meet same issue:
 https://gitter.im/open-telemetry/opentelemetry-node?at=5f3aa9481226fc21335ce61a",0.0,0.0,1.0,0.0
jaeger,My final working solution posted at  https://stackoverflow.com/a/63489195/2000548,0.0,0.277,0.723,0.3182
jaeger,I am following all the instructions mentioned here:  https://github.com/opentracing-contrib/java-jdbc,0.0,0.0,1.0,0.0
jaeger,I assumed that with these steps the traces related to JDBC operations will automatically start getting reported.,0.0,0.0,1.0,0.0
jaeger,"However, in the logs, I only see this below and nothing in Jaeger UI.",0.0,0.0,1.0,0.0
jaeger,Can someone please show an example of how to achieve this?,0.0,0.187,0.813,0.3182
jaeger,I am using below versions:,0.0,0.0,1.0,0.0
jaeger,"opentracing-api-0.33.0
opentracing-jdbc-0.2.10
opentracing-util-0.33.0
jaeger-core-0.35.4",0.0,0.0,1.0,0.0
jaeger,Here's the abstract that I have attempted.,0.0,0.0,1.0,0.0
jaeger,Is there anything else I need to do?,0.0,0.0,1.0,0.0
jaeger,And then this is the code around database call:,0.0,0.0,1.0,0.0
jaeger,The database config is:,0.0,0.0,1.0,0.0
jaeger,I would like to know what's the minimal configuration for a spring-boot app in terms of dependencies if I need to report traces to Jaeger in Istio.,0.0,0.098,0.902,0.3612
jaeger,I was expecting that by adding only,0.0,0.0,1.0,0.0
jaeger,the traces will be present in Jaeger.,0.0,0.0,1.0,0.0
jaeger,But it's true that the envoy can not correlate the requests that go to the service with the response if the headers with the tracing details are not in the response.,0.0,0.11,0.89,0.5719
jaeger,It seems that sleuth only propagates the headers when calling by RestTemplate or Feign or Spring Integration... but the headers are not there in the response for an API.,0.0,0.0,1.0,0.0
jaeger,"In order to make it work, I added this other dependency to the service",0.0,0.0,1.0,0.0
jaeger,Or even propagating the headers manually I can see traces in Jaeger.,0.0,0.0,1.0,0.0
jaeger,I am confused because it seems that sleuth should be doing this task without additional dependencies as I understood from @spencergibb in this video  https://youtu.be/AMJQO9zs2eo?t=1045,0.095,0.0,0.905,-0.3182
jaeger,In case of Sleuth dependency is not enough.,0.0,0.0,1.0,0.0
jaeger,What dependencies will be required?,0.0,0.0,1.0,0.0
jaeger,"I can see several dependencies that seem to do similar things like the previous one, like",0.0,0.278,0.722,0.6124
jaeger,Thanks in advance.,0.0,0.592,0.408,0.4404
jaeger,"So I am exploring Jaeger for Tracing and I saw that we can directly send spans from the client to the collector in HTTP (PORT: 14268), if so then what is the advantage of using the jaeger agent.",0.0,0.054,0.946,0.25
jaeger,When to go for the Jaeger Agent Approach and when to go with the direct HTTP Approach.,0.0,0.0,1.0,0.0
jaeger,What is the disadvantage of going with the Direct approach to the collector,0.189,0.0,0.811,-0.4215
jaeger,"I've been trying to run basic shell commands, ls as an example, but any of them work.",0.0,0.0,1.0,0.0
jaeger,"So, I've tried to validate if the container has a bash enabled, and answers to similar posts say to run:",0.0,0.122,0.878,0.3612
jaeger,But any of them work (neither docker exec -it amazing_robinson ls).,0.0,0.0,1.0,0.0
jaeger,This is the error:,0.474,0.0,0.526,-0.4019
jaeger,The container is,0.0,0.0,1.0,0.0
jaeger,"We are using Jaeger with camel open tracing to get metrics, we are able to see all the latency metrics in Jaeger UI.",0.0,0.0,1.0,0.0
jaeger,"In Prometheus we are able to see few jaeger metrics request count, but the some metrics like latency we are not able to found in prometheus dashboard.",0.0,0.111,0.889,0.5023
jaeger,Prometheus dashborad,0.0,0.0,1.0,0.0
jaeger,"We are exposing metrics using below:
host:14269/metrics, host:16687/metrics",0.231,0.0,0.769,-0.2732
jaeger,Can some one help me to get the jaeger service latency metrics in prometheus.,0.0,0.172,0.828,0.4019
jaeger,"I was trying to implement the OpenTracing + Jaeger to my PHP project, following the ""Get started"" example there  https://github.com/jonahgeorge/jaeger-client-php",0.0,0.0,1.0,0.0
jaeger,"Tracer, spans and scopes have been created successfully, but Jaeger does not see my service.",0.0,0.217,0.783,0.3818
jaeger,There are my  .php  and  docker-compose.yml  files below:,0.0,0.0,1.0,0.0
jaeger,I have distribute application that consists of several Go services.,0.0,0.0,1.0,0.0
jaeger,Some of those use Kafka as data bus.,0.0,0.0,1.0,0.0
jaeger,I was able trace down calls between services using  opentracing  with Jaeger.,0.0,0.0,1.0,0.0
jaeger,"I have problem plotting Kafka spans on graph, them appear as gaps.",0.213,0.0,0.787,-0.4019
jaeger,Here is what i was able to do.,0.0,0.0,1.0,0.0
jaeger,Initial spans been created by gRPC middleware.,0.0,0.25,0.75,0.25
jaeger,Producer side:,0.0,0.0,1.0,0.0
jaeger,Consumer side:,0.0,0.0,1.0,0.0
jaeger,How should i modify this to plot span on graph when message was in Kafka?,0.0,0.0,1.0,0.0
jaeger,I am trying to setup jaeger-collector on one server with jaeger-agent running in another server.,0.0,0.0,1.0,0.0
jaeger,"If I run the exe jaeger-all-in-one, everything works as expected (using in memory).",0.0,0.0,1.0,0.0
jaeger,"In order to see the options available with ES, i am not able to run a help command.",0.0,0.153,0.847,0.4019
jaeger,"When I run jaeger-collector --help, it shows only cassandra related flags.",0.0,0.0,1.0,0.0
jaeger,How do I check the elastic search specific details.,0.0,0.0,1.0,0.0
jaeger,"Now, my requirement is to specify and elastic search url.",0.0,0.0,1.0,0.0
jaeger,"I have set up the Environment variables SPAN_STORAGE_TYPES and ES_SERVER_URLS, but couldn't find how to run jaeger-collector.exe by asking it to take in these environment variables.",0.0,0.0,1.0,0.0
jaeger,"Thanks,
Minu",0.0,0.744,0.256,0.4404
jaeger,I would like to configure Jaeger in my Spring application.,0.0,0.238,0.762,0.3612
jaeger,Somehow I cannot find a proper way to do this.,0.0,0.0,1.0,0.0
jaeger,Almost all Spring-Jaeger-related documentation is for Spring Boot where most of the properties are auto configured.,0.0,0.0,1.0,0.0
jaeger,Here's my approach.,0.0,0.0,1.0,0.0
jaeger,Maven dependency:,0.0,0.0,1.0,0.0
jaeger,Spring config for Jaeger:,0.0,0.0,1.0,0.0
jaeger,Jaeger is running locally in docker on port 6831.,0.0,0.0,1.0,0.0
jaeger,"Once my application starts, I noticed that application slows down considerably, I assume that is because of metrics logged heavily to console by LoggingReporter.",0.0,0.0,1.0,0.0
jaeger,"However, My Spring app does not show up in Jaeger UI.",0.0,0.0,1.0,0.0
jaeger,In the beginning I would like to trace my REST endpoints.,0.0,0.217,0.783,0.3612
jaeger,Can someone point me in the right direction why my app is missing from UI and how I configure Jaeger properly?,0.104,0.0,0.896,-0.296
jaeger,Is there perhaps a sample project with Spring+Jaeger that does not rely on outdated Jaeger?,0.0,0.0,1.0,0.0
jaeger,I have installed ISTIO using Helm .,0.0,0.0,1.0,0.0
jaeger,"I forgot to enable grafana, kiali and jaeger.",0.0,0.0,1.0,0.0
jaeger,How can i enable all these above services after i have installed istio?,0.0,0.0,1.0,0.0
jaeger,I had setup Jaeger in Azure Kubernetes Cluster in monitoring namespace and I deployed my container which is instrumented with jaeger client libraries in monitoring domain.,0.0,0.0,1.0,0.0
jaeger,The service is up and running and I'm able to see the traces using actuator when I specify the :/actuator in the browser.,0.0,0.0,1.0,0.0
jaeger,But the same microservice is not populating in the service dropdown in Jaeger UI.,0.0,0.0,1.0,0.0
jaeger,Below are the files i'm using.,0.0,0.0,1.0,0.0
jaeger,DemoOpentracingApplication.java,0.0,0.0,1.0,0.0
jaeger,Why the instrumented service is not populating in Jaeger UI in Kubernetes?,0.0,0.0,1.0,0.0
jaeger,I want to inject x-b3-traceid and x-b3-spanid in logs  with pattern as shown-,0.0,0.106,0.894,0.0772
jaeger,"For zipkins, there are libraries available like",0.0,0.294,0.706,0.3612
jaeger,"brave-context-log4j2 –
  ( https://github.com/openzipkin/brave/tree/master/context/log4j2 )",0.0,0.0,1.0,0.0
jaeger,Spring cloud sleuth.,0.0,0.0,1.0,0.0
jaeger,"( https://cloud.spring.io/spring-cloud-sleuth/ )
How can I add that while using jaeger?",0.0,0.0,1.0,0.0
jaeger,Jaeger provides  an all-in-one  configuration for a development setup of Jaeger that doesn't use tons of memory.,0.0,0.0,1.0,0.0
jaeger,The instructions  show how to easily install this via:,0.0,0.231,0.769,0.34
jaeger,However I manage my development environment using Helm.,0.0,0.0,1.0,0.0
jaeger,Is there a Helm chart for this setup that I can use instead?,0.0,0.0,1.0,0.0
jaeger,I really read many articles.,0.0,0.0,1.0,0.0
jaeger,I figure out that need to just include a starters in spring boot ))),0.0,0.0,1.0,0.0
jaeger,Can anyone sort it out: is Sleuth create MDC (Mapped Diagnostic Context)?,0.0,0.16,0.84,0.2732
jaeger,Is sleuth create a record's ID which used by Zipkin?,0.0,0.208,0.792,0.2732
jaeger,Can I see this ID in Kibana?,0.0,0.0,1.0,0.0
jaeger,Or do I need to use zipkin API?,0.0,0.0,1.0,0.0
jaeger,Are there best practice to use all of this together?,0.0,0.318,0.682,0.6369
jaeger,Is Jaeger substitute both Zipkin and Sleuth or how?,0.0,0.0,1.0,0.0
jaeger,I'm trying to run  istioctl install istio-config.yaml  command within CodeBuild on AWS but I get this error:,0.218,0.0,0.782,-0.6355
jaeger,"error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid:
spec.resources.requests.storage: Forbidden: field can not be less than
previous value",0.267,0.104,0.63,-0.5209
jaeger,even though I don't have the path  spec.resources.requests.storage  in my configuration file!,0.0,0.0,1.0,0.0
jaeger,This is the content of my file:,0.0,0.0,1.0,0.0
jaeger,and this is the whole log of the command:,0.0,0.0,1.0,0.0
jaeger,This is more details about the pvc  istio-jaeger-pvc :,0.0,0.0,1.0,0.0
jaeger,I'm trying to install Jaeger into my K8s cluster using the streaming strategy.,0.0,0.0,1.0,0.0
jaeger,I need to use the existing Kafka cluster from my cloud provider.,0.0,0.0,1.0,0.0
jaeger,It requires a username and password.,0.0,0.0,1.0,0.0
jaeger,Jaeger documentation mentions only broker and topic:,0.0,0.0,1.0,0.0
jaeger,How can I configure Kafka credentials in CRD?,0.0,0.0,1.0,0.0
jaeger,-Thanks in advance!,0.0,0.615,0.385,0.4926
jaeger,"In the Jaeger UI  http://localhost:16686/search , there is an option to upload JSON files for traces.",0.0,0.0,1.0,0.0
jaeger,I wonder can we download the traces from Jaeger itself and use them in the future for finding performance issues?,0.0,0.0,1.0,0.0
jaeger,"How can we do that, I see no option to download traces from Jaeger Ui.",0.145,0.0,0.855,-0.296
jaeger,I am using the Jaeger Operator to deploy the Jaeger Query and Collector services to Kubernetes (K3S actually) along with an ElasticSearch instance for the storage backend.,0.0,0.0,1.0,0.0
jaeger,The Jaeger Operator creates an Ingress instance for the Jaeger Query service but it assumes that all of your Jaeger Agents will also be running inside the Kubernetes cluster.,0.0,0.052,0.948,0.1406
jaeger,"Unfortunately, that is not the case for me as some applications that I am tracing are not run within the cluster so I need my Jaeger Collector to be accessible from outside.",0.076,0.0,0.924,-0.34
jaeger,This Jaeger GitHub issue discusses a potential enhancement to the Jaeger Operator for this functionality  and it suggests creating your own Ingress outside of the Operator to expose the Jaeger Collector but doesn't go into details.,0.036,0.045,0.919,0.0772
jaeger,I also want to utilize gRPC for the communication between the Agent outside the cluster and the Collector in the cluster and  this article describes how to set up an Ingress for gRPC  (though it is not specific to Jaeger).,0.0,0.033,0.967,0.0772
jaeger,"I used the  example ingress spec there , made some tweaks for my scenario, and deployed it to my cluster:",0.0,0.0,1.0,0.0
jaeger,This creates an Ingress for me alongside the simple-prod-query ingress that is created by the Jaeger Operator:,0.0,0.215,0.785,0.4767
jaeger,Here are the services behind the ingress:,0.0,0.0,1.0,0.0
jaeger,"Unfortunately, my Jaeger Agent can't seem to speak to it still...",0.194,0.0,0.806,-0.34
jaeger,"I am actually deploying my Jaeger Agent via docker-compose and as you can see here, I am configuring it to connect to jaeger-collector.my-container-dev:80:",0.0,0.0,1.0,0.0
jaeger,"I can see that something is wrong with the connection because when I hit the Jaeger Agent's Sampling Strategy service with an  HTTP GET to http://localhost:5778/sampling?service=myservice , I get an error back that says the following:",0.162,0.0,0.838,-0.7003
jaeger,Is there something wrong with my Ingress spec?,0.307,0.0,0.693,-0.4767
jaeger,No trace data seems to be making it from my Agent to the Collector and I get errors when hitting the Jaeger Agent Sampling Service.,0.173,0.0,0.827,-0.5574
jaeger,"Also, I find it a little strange that there is no IP Address listed in the  kubectl get ing  output but perhaps that is a red herring.",0.115,0.0,0.885,-0.2152
jaeger,"As mentioned above, I am using K3S which seems to use traefik for its ingress controller (as opposed to nginx).",0.0,0.0,1.0,0.0
jaeger,I checked the logs for the traefik controller and I didn't see anything helpful there either.,0.152,0.0,0.848,-0.3252
jaeger,I am prototyping the use of  Jaeger  in an ASP.NET Core (3.1) Web API using the  Jaeger C# Client  and I got it working with the  All in One approach they mention in their Getting Started documentation .,0.0,0.0,1.0,0.0
jaeger,This works fine for initial prototyping but I also wanted to test with storing to an instance of ElasticSearch.,0.0,0.076,0.924,0.1027
jaeger,"Luckily, I found  another Stack Overflow post about this which contains a docker-compose.yaml for deploying Elastic Search and all the Jaeger components  and I got that working after a few tweaks to the slightly outdated docker-compose ( details in my answer for that post ).",0.0,0.078,0.922,0.5106
jaeger,"However, while digging through the Jaeger documentation, I found the  CLI Flags reference for the jaeger-all-in-one distribution  that seems to contradict itself.",0.103,0.0,0.897,-0.3182
jaeger,"First, it says",0.0,0.0,1.0,0.0
jaeger,"Jaeger all-in-one distribution with agent, collector and query.",0.0,0.0,1.0,0.0
jaeger,Use with caution this version by default uses only in-memory database.,0.0,0.0,1.0,0.0
jaeger,But then it also proceeds to say,0.0,0.0,1.0,0.0
jaeger,jaeger-all-in-one can be used with these storage backends:,0.0,0.0,1.0,0.0
jaeger,and then lists jager-all-in-one distribution CLI Flag details for:,0.0,0.0,1.0,0.0
jaeger,"So this implies that the Jaeger All in One distribution can be used with Elastic Search, etc.",0.0,0.0,1.0,0.0
jaeger,I am guessing the initial comment about the all-in-one distribution only supporting an in-memory database applies to the  jaeger-all-in-one with memory  option and not the others as otherwise it doesn't make sense.,0.0,0.088,0.912,0.4404
jaeger,Can someone with Jaeger experience clarify?,0.0,0.0,1.0,0.0
jaeger,I have used the following configuration to setup the Istio,0.0,0.0,1.0,0.0
jaeger,and exposed the jaeger-query service as mentioned below,0.157,0.0,0.843,-0.0772
jaeger,I couldn't see the below deployed application in Jaeger,0.0,0.0,1.0,0.0
jaeger,and have deployed the application as mentioned below,0.0,0.0,1.0,0.0
jaeger,I could access the service as shown below,0.0,0.0,1.0,0.0
jaeger,I do know why the service is not listed in the Jaeger UI?,0.0,0.0,1.0,0.0
jaeger,Please take a look at  https://github.com/winster/jaeger-trace-reactive/blob/master/src/main/java/com/example/demo/JaegerTraceReactiveApplication.java  (readme might help to understand the problem better  https://github.com/winster/jaeger-trace-reactive ),0.131,0.383,0.485,0.6369
jaeger,This is a spring boot application with opentracing-jaeger.,0.0,0.0,1.0,0.0
jaeger,"As per the doc, jaeger supports webflux and webclient.",0.0,0.238,0.762,0.3612
jaeger,"But it has been noted that, the trace skips the reactive flow when there is a webclient call.",0.0,0.0,1.0,0.0
jaeger,Is there a way to fix this?,0.0,0.0,1.0,0.0
jaeger,I am doing some prototyping of Jaeger Tracing for an ASP.NET Core Web API and I am able to get it working using the  All in One instance of Jaeger described in the Getting Started documentation  and the following code in my  Startup.ConfigureServices()  method:,0.0,0.0,1.0,0.0
jaeger,"To use all this, you need to add a few packages to your project:",0.0,0.0,1.0,0.0
jaeger,So this works OK and I get Traces with their Spans showing up in my Jaeger Search UI (http://localhost:16686/search) but it just shows the Trace with my service name (in this case &quot;MySuperCoolWebAPI&quot;) followed by &quot;HTTP GET&quot;:,0.0,0.057,0.943,0.2728
jaeger,This is not terribly useful to see &quot;HTTP GET&quot; there.,0.18,0.219,0.6,0.1326
jaeger,"Instead, I want to see the Web API action name or something else that lets me know what kind of request this really is.",0.0,0.056,0.944,0.0772
jaeger,"As you can see from my sample code above, I have tried setting the  HttpHandlerDiagnosticOptions.OperationNameResolver  but this only affects  HttpClient  calls I make from within my web service.",0.0,0.0,1.0,0.0
jaeger,It does not seem to affect how the Trace/Span associated with the request I received is named.,0.0,0.0,1.0,0.0
jaeger,I also tried setting the Span OperationName in my Web API Controller method using the GlobalTracer like this but this affects an inner span and NOT the main Trace/Span that shows up on the main Jaeger UI search results page:,0.0,0.044,0.956,0.1901
jaeger,Here you can see the first child Span has its name set to what I forced it to but the main level Span (the parent of the one I changed) is not affected:,0.061,0.051,0.888,-0.0859
jaeger,Is there a way I can set the operation name of the main Span with the Jaeger C# Client?,0.0,0.0,1.0,0.0
jaeger,"Also, I am using .NET Core 3.1 in case that is relevant.",0.0,0.0,1.0,0.0
jaeger,I have 2 services A and B. I'm calling an endpoint in B from A using Spring Integration with Spring Boot 2.1.4.,0.0,0.0,1.0,0.0
jaeger,In service A logs:,0.0,0.0,1.0,0.0
jaeger,In service  B logs:,0.0,0.0,1.0,0.0
jaeger,Clearly traceId is different in service B.,0.0,0.31,0.69,0.4019
jaeger,"I think due to this, I'm not seeing the service B span under service A span in JaegerUi.",0.0,0.0,1.0,0.0
jaeger,Any idea what I'm doing wrong?,0.383,0.0,0.617,-0.4767
jaeger,"I have configured  JaegerGrpcSpanExporter  , so that it can export the created spans to Jaeger-Collector.",0.0,0.143,0.857,0.25
jaeger,I don't want to export the spans to Jaeger-Agent.,0.149,0.0,0.851,-0.0572
jaeger,I have written down below code.,0.0,0.0,1.0,0.0
jaeger,when i change the port to 14250 i.e.,0.0,0.0,1.0,0.0
jaeger,Jaeger-agent port spans are exported to UI but with 14268 I am not able to find any trace at Jaeger UI.,0.0,0.0,1.0,0.0
jaeger,Do i need to change the above code?,0.0,0.0,1.0,0.0
jaeger,I'm working on a POC and was able to integrate 2 microservices with JaegerUI.,0.0,0.0,1.0,0.0
jaeger,Request to an endpoint in serviceA calls an endpoint in serviceB and returns a response.,0.0,0.0,1.0,0.0
jaeger,I have used below dependencies:,0.0,0.0,1.0,0.0
jaeger,spring.boot.version : 2.1.4.RELEASE,0.0,0.0,1.0,0.0
jaeger,Spring autoconfiguration takes care of everything so just added the required properties:,0.0,0.225,0.775,0.4939
jaeger,I want to achieve the below:,0.0,0.245,0.755,0.0772
jaeger,Based on the answer in below SO question:,0.0,0.0,1.0,0.0
jaeger,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,0.0,0.0,1.0,0.0
jaeger,opentracing-spring-cloud-starter dependency should automatically take care of sending app logs to span in JaegerUI.,0.0,0.198,0.802,0.4939
jaeger,I have a log statement like below in serviceA:,0.0,0.294,0.706,0.3612
jaeger,logger.info(&quot;sending request to serviceB.&quot;);,0.0,0.0,1.0,0.0
jaeger,But above log is not getting captured in corresponding span and not visible in JaegerUI.,0.0,0.0,1.0,0.0
jaeger,Any suggestions on how to achieve the above scenarios are appreciated!,0.0,0.264,0.736,0.5562
jaeger,I'm using this library ( grpc-spring-boot-starter ) so I can have gRPC capabilities in a Spring Boot app.,0.0,0.0,1.0,0.0
jaeger,I want to know how to properly integrate this with Istio + Jaeger tracing.,0.0,0.106,0.894,0.0772
jaeger,I need to know what are the needed dependencies for this to happen.,0.0,0.0,1.0,0.0
jaeger,"I have two (2) apps, one serves as the gRPC client and one serves as the gRPC server,",0.0,0.0,1.0,0.0
jaeger,The expectation is that the trace between the gRPC client and the gRPC server must be reflected in Jaeger.,0.0,0.0,1.0,0.0
jaeger,But it's not happening.,0.0,0.0,1.0,0.0
jaeger,I am inside a Kubernetes cluster that has Istio.,0.0,0.0,1.0,0.0
jaeger,What really happens is HTTP request -  Envoy sidecar -  gRPC Client's Spring Boot @RestController -  get the Headers from HTTP request -  copy those to gRPC call before making the call -  gRPC Service.,0.0,0.0,1.0,0.0
jaeger,How can I make the gRPC client &lt;-  gRPC Service trace shown to Jaeger?,0.0,0.0,1.0,0.0
jaeger,Are there any dependencies that needs to be imported?,0.0,0.0,1.0,0.0
jaeger,Right now I have:,0.0,0.0,1.0,0.0
jaeger,"I also have done something like this to ""propagate the headers"":",0.0,0.217,0.783,0.3612
jaeger,But it doesn't seem to work..,0.0,0.0,1.0,0.0
jaeger,I've added opentracing to my web app and am using the Jaeger all-in-one docker image as the collector.,0.0,0.0,1.0,0.0
jaeger,I'm running docker on windows 10 (hyper-v) and am using the Jaeger java client.,0.0,0.0,1.0,0.0
jaeger,When I test the web app locally on the host machine it sends traces successfully to the Jaeger collector docker instance.,0.0,0.144,0.856,0.4939
jaeger,"However, when I run the web app in another docker container no traces are registered in the Jaeger UI.",0.115,0.0,0.885,-0.296
jaeger,I've tried both containers on the same docker network with no success.,0.138,0.233,0.629,0.3612
jaeger,The web app in the docker container can access other services in dockers contains such as a DB and an ETCD server with no issues.,0.087,0.0,0.913,-0.296
jaeger,I thought I might have the wrong ports but given the fact it works from the host environment I'm assuming these are correct and that it is a docker configuration issue.,0.071,0.0,0.929,-0.2617
jaeger,I've also set  JAEGER_SAMPLER_TYPE environment variable to const and the JAEGER_SAMPLER_PARAM to 1 to ensure all traces are logged.,0.0,0.133,0.867,0.3818
jaeger,Edit  - when I look at the metrics it seems like the jaeger is receiving the spans.,0.0,0.152,0.848,0.3612
jaeger,Every call I make to the app increments this count by one.,0.0,0.0,1.0,0.0
jaeger,I also tried the sample project  Hotrod  as suggested by Yuri Shkuro on a similar issue someone had.,0.0,0.0,1.0,0.0
jaeger,Exact same result as above.,0.0,0.0,1.0,0.0
jaeger,Metrics shows spans being received but nothing is displayed in UI.,0.0,0.0,1.0,0.0
jaeger,Edit 2  - I've narrowed it down to happening in a hyper-v windows 10 VM.,0.0,0.0,1.0,0.0
jaeger,Any help would be appreciated.,0.0,0.667,0.333,0.7184
jaeger,Thanks,0.0,1.0,0.0,0.4404
jaeger,192.168.0.15 is host machine,0.0,0.0,1.0,0.0
jaeger,Setting up Jaeger tracer in Java,0.0,0.0,1.0,0.0
jaeger,Unable to trace a services for springboot application on Jaeger UI(localhost:16686/search).,0.0,0.0,1.0,0.0
jaeger,"Here I am able to run an application successfully, but unable to get a services in jaeger ui(Except defalut one jaeger-query).",0.0,0.104,0.896,0.2732
jaeger,"I used docker cmd to start the jaeger service,",0.0,0.0,1.0,0.0
jaeger,Open the Jaeger UI on  http://localhost:16686/search,0.0,0.0,1.0,0.0
jaeger,},0.0,0.0,0.0,0.0
jaeger,},0.0,0.0,0.0,0.0
jaeger,"Image 1 
 Image 2",0.0,0.0,1.0,0.0
jaeger,Github repository with demo:  https://github.com/pavolloffay/opentracing-java-examples .,0.0,0.0,1.0,0.0
jaeger,Please help me to solve this.,0.0,0.694,0.306,0.7003
jaeger,Following Spring Cloud Sleuth's documentation I've set up a Spring Boot application with a Zipkin client:,0.0,0.0,1.0,0.0
jaeger,Gradle config:,0.0,0.0,1.0,0.0
jaeger,With this I start a Zipkin Server instance:,0.0,0.0,1.0,0.0
jaeger,And I get traces in Zipkin.,0.0,0.0,1.0,0.0
jaeger,So far so good.,0.0,0.576,0.424,0.6213
jaeger,Then I switch to a Jaeger server:,0.0,0.0,1.0,0.0
jaeger,And without any change to my application I can see those traces in Jaeger.,0.0,0.0,1.0,0.0
jaeger,Great.,0.0,1.0,0.0,0.6249
jaeger,Sleuth docs states:,0.0,0.0,1.0,0.0
jaeger,15.1.,0.0,0.0,1.0,0.0
jaeger,OpenTracing,0.0,0.0,1.0,0.0
jaeger,Spring Cloud Sleuth is compatible with OpenTracing.,0.0,0.0,1.0,0.0
jaeger,"If you have
  OpenTracing on the classpath, we automatically register the
  OpenTracing Tracer bean.",0.0,0.0,1.0,0.0
jaeger,"If you wish to disable this, set
  spring.sleuth.opentracing.enabled to false",0.0,0.231,0.769,0.4019
jaeger,Following this I added the Open Tracing dependency:,0.0,0.0,1.0,0.0
jaeger,During startup the  OpentracingAutoConfiguration  creates a  BraveTracer .,0.0,0.296,0.704,0.2732
jaeger,"The point is I've placed a breakpoint in methods such as  scopeManager() ,  activeSpan() ,  activateSpan() ,  buildSpan()  from that  BraveTracer  and none of them is invoked during the execution of the application; the traces keep showing up in Jaeger though.",0.0,0.0,1.0,0.0
jaeger,What am I missing here?,0.423,0.0,0.577,-0.296
jaeger,I am trying to implement Jaeger in the node js project.,0.0,0.0,1.0,0.0
jaeger,I have deployed this node js project(using docker image) and Jaegaer in k8s (kubectl create -f  https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/all-in-one/jaeger-all-in-one-template.yml ),0.0,0.123,0.877,0.2732
jaeger,Both are working individually but traces are not visible in the service,0.0,0.0,1.0,0.0
jaeger,I am trying to connect to jaeger collector which uses Kafka as intermediate buffer.,0.0,0.0,1.0,0.0
jaeger,Here are my doubts could any one please point to some docs .,0.152,0.159,0.69,0.0258
jaeger,"QUESTION
  1.",0.0,0.0,1.0,0.0
jaeger,"How to connect to collector by skipping agent and use kafka as intermediate buffer.Please provide me command or configuration
  2.",0.0,0.0,1.0,0.0
jaeger,"Whats the configuration for kafka to connect to particular host.When I tried to use below command its still pointing to localhost and failing
     docker run  -e SPAN_STORAGE_TYPE=kafka jaegertracing/jaeger-collector:1.17",0.113,0.0,0.887,-0.5106
jaeger,I have Springboot Webflux main application that talks to other services using RSocket and I want to configure the stack so I want to see the traces looking like this:,0.0,0.175,0.825,0.4767
jaeger,As today I need to check each app to check the flow,0.0,0.0,1.0,0.0
jaeger,I'm using implementation  'io.opentracing.contrib:opentracing-spring-jaeger-cloud-starter'  dependency,0.0,0.0,1.0,0.0
jaeger,How does one integrate opentracing (jaeger) with RSocket?,0.0,0.0,1.0,0.0
jaeger,"I saw that for WebClient, you have autoconfiguration but found little or no instructions on how to do it besides  Tracking-Zipkin  page which I don't really know how to procced from that.",0.076,0.0,0.924,-0.3362
jaeger,so I'm playing aroung with Jaeger and OpenTracing to trace the requests between my Spring Boot microservices.,0.0,0.115,0.885,0.2682
jaeger,I have setup all necessary configurations and added the dependency:,0.0,0.0,1.0,0.0
jaeger,So far all works fine.,0.0,0.31,0.69,0.2023
jaeger,"I see all the traces and spans in my Jaeger UI, no problem.",0.329,0.0,0.671,-0.5994
jaeger,"But now I have the challenge to add new spans to a specific trace, that is already finished.",0.0,0.088,0.912,0.1154
jaeger,Think of it like this.,0.0,0.385,0.615,0.3612
jaeger,A client calls one of the services and the tracing starts.,0.0,0.0,1.0,0.0
jaeger,"After the work is done, I see the trace in my Jaeger UI.",0.0,0.0,1.0,0.0
jaeger,But now the invoking clients wants to add some additional tracing data to the specific trace.,0.0,0.0,1.0,0.0
jaeger,"Like tracing information from other service, that are not within the scope of my microservices.",0.0,0.152,0.848,0.3612
jaeger,I've added a filter so I can extract the trace id and send it to the client.,0.0,0.0,1.0,0.0
jaeger,Now the client does a request containing the additional trace information and the trace id.,0.0,0.0,1.0,0.0
jaeger,These informationa should then be added as an additional span in the already finished trace.,0.0,0.0,1.0,0.0
jaeger,Now to my question.,0.0,0.0,1.0,0.0
jaeger,Is there a way to create a span and add it to a trace with only having the trace id as a String?,0.0,0.104,0.896,0.2732
jaeger,I've tried Zipkin and I could just do:,0.0,0.0,1.0,0.0
jaeger,That span could then be added by doing a POST request to my zipkin server on port 9411 which did the magic of adding this span to the trace with the given id.,0.0,0.0,1.0,0.0
jaeger,Using OpenTracing I can do:,0.0,0.0,1.0,0.0
jaeger,Unfortunately this approach needs the trace in form of a span to create the new span as a child of that trace.,0.107,0.093,0.8,-0.0772
jaeger,"Given the fact that I can only provide the trace id, I don't know how to get the needed span of that trace.",0.0,0.0,1.0,0.0
jaeger,Do I really need to make a call to my the Jaeger query to get the trace span needed or is there another approach I haven't been thinking of?,0.0,0.0,1.0,0.0
jaeger,Would really like to get some help on this.,0.0,0.44,0.56,0.6697
jaeger,Cheers!,0.0,1.0,0.0,0.5255
jaeger,I have a working Ambassador and a working Istio and I use the default Jaeger tracer in Istio which works fine.,0.0,0.101,0.899,0.2023
jaeger,Now I would like to make Ambassador report trace data to Istio's Jaeger.,0.0,0.185,0.815,0.3612
jaeger,"Ambassador documentation suggests that Jaeger is supported with the Zipkin driver, but gives example only for usage with Zipkin.",0.0,0.084,0.916,0.1655
jaeger,https://www.getambassador.io/user-guide/with-istio/#tracing-integration,0.0,0.0,1.0,0.0
jaeger,"So I checked the ports of jaeger-collector service, and picked the http: jaeger-collector-http  14268/TCP",0.0,0.0,1.0,0.0
jaeger,And modified the TracingService shown in the Ambassador docs:,0.0,0.0,1.0,0.0
jaeger,But I cannot see trace data from Ambassador in Jaeger.,0.0,0.0,1.0,0.0
jaeger,Does anyone have any experience on this topic?,0.0,0.0,1.0,0.0
jaeger,I am trying to deploy the Jaeger Helm chart with Azure Cosmos DB acting as the Cassandra Storage Backend.,0.0,0.0,1.0,0.0
jaeger,I have set up the CosmosDB and created a values file as below:,0.0,0.343,0.657,0.5719
jaeger,The command that I am using to deploy jaeger with the values file is:,0.0,0.184,0.816,0.4019
jaeger,"However, on checking the pods, the collector and the query are in CrashLoopBackOff",0.0,0.0,1.0,0.0
jaeger,Both the containers on running the describe command complain:,0.238,0.0,0.762,-0.3612
jaeger,I am not sure what am I missing here?,0.454,0.0,0.546,-0.4874
jaeger,"I'm currently playing around with  Jaeger Query  and trying to access its content through the  API , which uses gRPC.",0.0,0.091,0.909,0.2023
jaeger,"I'm not familiar with gRPC, but my understanding is that I need to use the Python gRPC compiler (grpcio_tools.protoc) on the relevant proto file to get useful Python definitions.",0.0,0.125,0.875,0.5927
jaeger,"What I'm trying to do is find out ways to access Jaeger Query by API, without the frontend UI.",0.0,0.0,1.0,0.0
jaeger,"Currently, I'm very stuck on compiling the proto files.",0.223,0.0,0.777,-0.3167
jaeger,"Every time I try, I get dependency issues ( Import ""fileNameHere"" was not found or has errors.",0.156,0.0,0.844,-0.34
jaeger,).,0.0,0.0,1.0,0.0
jaeger,The Jaeger  query.proto  file contains import references to files outside the repo.,0.0,0.0,1.0,0.0
jaeger,"Whilst I can find these and manually collect them, they also have dependencies.",0.0,0.0,1.0,0.0
jaeger,I get the impression that following through and collecting each of these one by one is not how this was intended to be done.,0.0,0.079,0.921,0.2263
jaeger,Am I doing something wrong here?,0.437,0.0,0.563,-0.4767
jaeger,The direct documentation through Jaeger is limited for this.,0.192,0.0,0.808,-0.2263
jaeger,"The below is my basic terminal session, before including any manually found files (which themselves have dependencies I would have to go and find the files for).",0.0,0.0,1.0,0.0
jaeger,Thanks for any help.,0.0,0.737,0.263,0.6808
jaeger,"I actually was trying to sample only the error traces in my application but i already have a probabilistic sampler parameter set in my application which samples the span at the beginning itself and the rest span follow the same pattern after then, i tried using force sampling option in jaeger but it doesnt seem to override the original decision made by the initial span of getting sampled or not.",0.027,0.043,0.93,0.2732
jaeger,Kindly help me out here.,0.0,0.663,0.337,0.7096
jaeger,I’ve started with instrumenting my gRPC service using go-gRPC-middleware.,0.0,0.0,1.0,0.0
jaeger,I’ve got logs working using zap and metrics exposed for Prometheus.,0.115,0.0,0.885,-0.0772
jaeger,Now that I’m trying to configure tracing using jaeger go client it requires me to add wrapper around metrics storage and logger.,0.0,0.0,1.0,0.0
jaeger,I’m not sure I understand why those wrappers are required,0.197,0.0,0.803,-0.2411
jaeger,https://github.com/jaegertracing/jaeger/blob/bf64373d1e690594fd8c279720faf32722cf1494/examples/hotrod/pkg/tracing/init.go#L46,0.0,0.0,1.0,0.0
jaeger,In our application a Node.js front end talks to a Java Spring backend.,0.0,0.0,1.0,0.0
jaeger,Everything is containerized and running in Kubernetes.,0.0,0.0,1.0,0.0
jaeger,Some time ago we added support for Jaeger distribtued tracing across the front end and back end services.,0.0,0.137,0.863,0.4019
jaeger,Jaeger has been running fine until recently.,0.0,0.231,0.769,0.2023
jaeger,Our Elasticsearch cluster was out of date so we upgraded.,0.0,0.0,1.0,0.0
jaeger,That mandated an upgrade of Jaeger--we ended up with the following bits:,0.0,0.0,1.0,0.0
jaeger,Both of the opentracing libraries have a dependency on the version 0.35.1 of the Jaeger Java client.,0.0,0.0,1.0,0.0
jaeger,"Since upgrading, traces that are created on one side or the other seem to be fine.",0.0,0.213,0.787,0.4215
jaeger,But traces that span the boundary (i.e.,0.0,0.0,1.0,0.0
jaeger,start on the Node.js front end and complete on the Java backend) generate errors in the jaeger-agent pod like this:,0.105,0.109,0.786,0.0258
jaeger,"For these traces, the Jaeger UI shows us the spans that were created by the front end before invoking the backend API, but the child backend spans do not show up as you would expect.",0.0,0.042,0.958,0.128
jaeger,What might cause this sort of processor error?,0.3,0.0,0.7,-0.4576
jaeger,Language:  Java,0.0,0.0,1.0,0.0
jaeger,Framework:  Spring boot,0.0,0.0,1.0,0.0
jaeger,Tools:  Jaeger,0.0,0.0,1.0,0.0
jaeger,I have done the following configuration for put whole trace on logs.,0.0,0.0,1.0,0.0
jaeger,But at controller level log not shown a trace.,0.0,0.0,1.0,0.0
jaeger,when hibernate query executed than after trace is put on logs(on service and repository level logs),0.0,0.0,1.0,0.0
jaeger,application.log,0.0,0.0,1.0,0.0
jaeger,Reporter class,0.0,0.0,1.0,0.0
jaeger,Appender class,0.0,0.0,1.0,0.0
jaeger,Main spring boot class,0.0,0.0,1.0,0.0
jaeger,Application.properties,0.0,0.0,1.0,0.0
jaeger,pom.xml,0.0,0.0,1.0,0.0
jaeger,Is there any way to put the std logs provided by the application and the errors to a span?,0.124,0.0,0.876,-0.34
jaeger,I know that I can send some logs with  span.LogKV()  or  span.LogFields()  but it makes code look bad while there are same logs with both application logger and span logger.,0.15,0.0,0.85,-0.6956
jaeger,I'm looking for an automated way to put all logs to the corresponding span.,0.0,0.0,1.0,0.0
jaeger,I am using a spring Cloud openFeign for making request from service#1 to service#2,0.0,0.0,1.0,0.0
jaeger,When I use restTemplate I can correctly see 2 requests in jaeger tracing.,0.0,0.0,1.0,0.0
jaeger,But when using openFeign I see only 1 request.,0.0,0.0,1.0,0.0
jaeger,Is there any way of integrating jaeger and openFeign?,0.0,0.0,1.0,0.0
jaeger,"I found this:
 https://www.baeldung.com/spring-cloud-openfeign 
 https://github.com/OpenFeign/feign-opentracing",0.0,0.0,1.0,0.0
jaeger,I've a simple Java application that I wanted to test tracing with Jaeger but encountered error.,0.215,0.0,0.785,-0.5499
jaeger,maven dependency -,0.0,0.0,1.0,0.0
jaeger,jaeger all-in-one -,0.0,0.0,1.0,0.0
jaeger,Here is the code -,0.0,0.0,1.0,0.0
jaeger,and I'm getting error -,0.474,0.0,0.526,-0.4019
jaeger,Appreciate any help !,0.0,0.851,0.149,0.69
jaeger,What are the advatages of the jaeger tracing with istio and without istio?,0.0,0.0,1.0,0.0
jaeger,For example with istio it will reduce the latency for collecting the more traces,0.0,0.0,1.0,0.0
jaeger,I am trying to get mongo logs in jaeger.,0.0,0.0,1.0,0.0
jaeger,Basically I want my jaeger to show my mongo application errors.,0.205,0.111,0.684,-0.2732
jaeger,What is the best method to do this?,0.0,0.375,0.625,0.6369
jaeger,I have tried using the maven repo- opentracing-mongo-driver (version 0.1.4),0.0,0.0,1.0,0.0
jaeger,and in my code I have configured it using -,0.0,0.0,1.0,0.0
jaeger,But I am getting this error-,0.511,0.0,0.489,-0.6355
jaeger,What is it that I am doing wrong?,0.341,0.0,0.659,-0.4767
jaeger,How do I enable  Jaeger jdbc  tracing in  Quarkus ?,0.0,0.0,1.0,0.0
jaeger,I've followed the  Quarkus  guides for  Opentracing  and didn't see any info about this.,0.0,0.0,1.0,0.0
jaeger,I'm using  Quarkus  v0.21.2 with the following extensions:,0.0,0.0,1.0,0.0
jaeger,And my code is just a basic Rest endpoint which calls my entity's Panache CRUD operation.,0.0,0.0,1.0,0.0
jaeger,Any help is appreciated.,0.0,0.75,0.25,0.7184
jaeger,I've tried the following and it didn't work:,0.0,0.0,1.0,0.0
jaeger,"What I expect in  Jaeger  is, 2 spans for 1 trace, one for the REST call and another one for the  JDBC  call.",0.0,0.0,1.0,0.0
jaeger,But what I see is just 1 span for the REST call.,0.0,0.0,1.0,0.0
jaeger,I have developed a camel route with  Spring boot .,0.0,0.0,1.0,0.0
jaeger,Now I want to trace the route using  jaeger .,0.0,0.157,0.843,0.0772
jaeger,"I tried  this example  to trace the route using  camel-opentracing  component, but I am unable to get the traces to  jaeger .",0.0,0.0,1.0,0.0
jaeger,I can only see it in the console.,0.0,0.0,1.0,0.0
jaeger,One thing i am not clear is where to add the  jaeger  url?,0.166,0.0,0.834,-0.2924
jaeger,Any working example will be helpful.,0.0,0.359,0.641,0.4215
jaeger,I'm currently trying to trace two Spring Boot (2.1.1) applications with Jaeger using  https://github.com/opentracing-contrib/java-spring-web,0.0,0.0,1.0,0.0
jaeger,also tryed with no success,0.247,0.416,0.337,0.3612
jaeger,"The tracing of the Spans for every single service / app works fine, but not over REST requests on a global level.",0.0,0.069,0.931,0.1027
jaeger,There is no dependency shown between the services like you can see in the image.,0.124,0.141,0.734,0.0772
jaeger,Shouldn't this work out of the box through the library?,0.0,0.0,1.0,0.0
jaeger,"Or do I have to implement some interceptors and request filters by my own and if so, how?",0.0,0.0,1.0,0.0
jaeger,You can CHECKOUT a minimalistic project containing the problem   here,0.252,0.0,0.748,-0.4019
jaeger,Btw: Jaeger runs as all-in-one via docker and works as expected,0.0,0.0,1.0,0.0
jaeger,According to  https://quarkus.io/guides/opentracing-guide  all Jeager configuration is via JVM args (-DJAEGER_ENDPOINT...) but I'd like to use either  application.properties  or  microprofile-config.properties  to configure tracing.,0.0,0.129,0.871,0.5023
jaeger,I've tried the following but the only config that seems to be picked up by Quarkus is the service-name all other properties are ignored.,0.114,0.0,0.886,-0.4497
jaeger,"So, question is if it is possible to configure via config-files or this is not currently supported?",0.109,0.0,0.891,-0.2411
jaeger,"I have a kubernetes cluster on google cloud platform, and on it, I have a jaeger deployment via development setup of  jaeger-kubernetes templates  
because my purpose is setup  elasticsearch  like backend storage, due to this, I follow the jaeger-kubernetes github documentation with the following actions",0.0,0.06,0.94,0.3612
jaeger,Here are configured the URLs to access to  elasticsearch  server and username and password and ports,0.0,0.0,1.0,0.0
jaeger,"And here, there are configured the download of docker images of the elasticsearch service and their volume mounts.",0.0,0.0,1.0,0.0
jaeger,"And then, at this moment we have a elasticsearch service running over 9200 and 9300 ports",0.0,0.0,1.0,0.0
jaeger,"According to the  Jaeger architecture , the  jaeger-collector  and  jaeger-query  services require access to backend storage.",0.0,0.0,1.0,0.0
jaeger,"And so, these are my services running on my kubernetes cluster:",0.0,0.0,1.0,0.0
jaeger,I execute it:,0.0,0.0,1.0,0.0
jaeger,And I get the following edit entry:,0.0,0.0,1.0,0.0
jaeger,"Here ... do I need setup our own URLs to collector and query services, which will be connect wiht elasticsearch backend service?",0.0,0.0,1.0,0.0
jaeger,How to can I setup the elasticsearch IP address or URLs here?,0.0,0.0,1.0,0.0
jaeger,"In the jaeger components, the query and collector need access to storage, but I don't know what is the elastic endpoint ...",0.0,0.0,1.0,0.0
jaeger,Is this  server-urls: http://elasticsearch:9200  a correct endpoint?,0.0,0.0,1.0,0.0
jaeger,"I am starting in the kubernetes and DevOps world, and I appreciate if someone can help me in the concepts and point me in the right address in order to setup jaeger and elasticsearch as a backend storage.",0.0,0.141,0.859,0.6597
jaeger,I am having problems pointing a jaeger agent to a collector running in openshift.,0.213,0.0,0.787,-0.4019
jaeger,I am able to browse my OCP collector endpoint doing this:,0.0,0.0,1.0,0.0
jaeger,My jaeger agent Dockerfile currently looks like this,0.0,0.263,0.737,0.3612
jaeger,I get the expected result when i point my agent to a collector running locally per the first commented line.,0.0,0.0,1.0,0.0
jaeger,I get the following error using the second uncommented CMD flag.,0.231,0.0,0.769,-0.4019
jaeger,"When i attempt the agent to the collector running on openshift, i get the error below",0.172,0.0,0.828,-0.4019
jaeger,I am able to successfully curl the collector endpoint by doing this,0.0,0.242,0.758,0.4939
jaeger,I get the following error when i attempt to curl the endpoint this way:,0.197,0.0,0.803,-0.4019
jaeger,I need help setting up a proper  --collector.host-port  flag that will connect to a collector running remotely behind an HTTPS protocol.,0.0,0.137,0.863,0.4019
jaeger,I'm struggling with setting up OpenTracing/Jaeger for a Spring Boot 2.0.2 application.,0.219,0.0,0.781,-0.4215
jaeger,Starting from a working but very sample for Spring Boot 1.5.3 I moved on to Spring Boot 2.0.2 -- which properly sent the traces.,0.0,0.0,1.0,0.0
jaeger,"But the dependencies used there were ridiculously old (like 0.0.4 for opentracing-spring-web-autoconfigure, which is now available in 0.3.2).",0.154,0.0,0.846,-0.4767
jaeger,So I migrated the application to the latest dependencies which resulted in no traces appearing anymore in Jaeger.,0.121,0.0,0.879,-0.296
jaeger,I've upload my tests to  https://gitlab.com/ceedee_/opentracing-spring-boot .,0.0,0.0,1.0,0.0
jaeger,The branches are as follows:,0.0,0.0,1.0,0.0
jaeger,Differences from 2. to 3. are as follows:,0.0,0.0,1.0,0.0
jaeger,Does anyone have a clue what I'm doing wrong in order to properly put traces into Jaeger?,0.171,0.0,0.829,-0.4767
jaeger,Hints on debugging OpenTracing/Jaeger are appreciated as well!,0.0,0.487,0.513,0.69
jaeger,"Best regards,
cd_",0.0,0.677,0.323,0.6369
jaeger,I have started Jaeger standalone binary on a Linux box and am trying to run Jaeger agent binary on Mac that tries to connect to the Jaeger collector of the standalone process.,0.0,0.0,1.0,0.0
jaeger,"However it keeps failing with ""error"":""tchannel error ErrCodeTimeout: timeout"".",0.462,0.0,0.538,-0.7184
jaeger,The problem is not with different OS versions as I get the same error when trying from another Linux box.,0.241,0.0,0.759,-0.6597
jaeger,I used telnet to confirm that the collector port was open for connection.,0.0,0.0,1.0,0.0
jaeger,"The stack trace is below-
./cmd/agent/agent- --collector.host-port=172.xx.2.4:14267
{""level"":""info"",""ts"":1542954225.5485492,""caller"":""tchannel/builder.go:94"",""msg"":""Enabling service discovery"",""service"":""jaeger-collector""}
{""level"":""info"",""ts"":1542954225.5489438,""caller"":""peerlistmgr/peer_list_mgr.go:111"",""msg"":""Registering active peer"",""peer"":""172.xx.2.4:14267""}
{""level"":""info"",""ts"":1542954225.5502574,""caller"":""agent/main.go:62"",""msg"":""Starting agent""}
{""level"":""info"",""ts"":1542954226.5518098,""caller"":""peerlistmgr/peer_list_mgr.go:157"",""msg"":""Not enough connected peers"",""connected"":0,""required"":1}
{""level"":""info"",""ts"":1542954226.552439,""caller"":""peerlistmgr/peer_list_mgr.go:166"",""msg"":""Trying to connect to peer"",""host:port"":""172.xx.2.4:14267""}
{""level"":""error"",""ts"":1542954226.8054206,""caller"":""peerlistmgr/peer_list_mgr.go:171"",""msg"":""Unable to connect"",""host:port"":""172.xx.2.4:14267"",""connCheckTimeout"":0.25,""error"":""tchannel error ErrCodeTimeout: timeout"",""stacktrace"":""github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr.(*PeerListManager).ensureConnections\n\t/Users/swarnim/go/src/github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr/peer_list_mgr.go:171\ngithub.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr.",0.081,0.081,0.838,0.0
jaeger,"(*PeerListManager).maintainConnections\n\t/Users/swarnim/go/src/github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr/peer_list_mgr.go:101""}",0.0,0.0,1.0,0.0
jaeger,I'm trying setting up a spring application which use Jaeger/Prometheus.,0.0,0.0,1.0,0.0
jaeger,"I already configured Prometheus successfully by prometheus.yaml file, but I haven't understood how configure Jaeger target endpoint.",0.0,0.13,0.87,0.2732
jaeger,Must I create a new yaml file and specify into it the configuration?,0.0,0.174,0.826,0.2732
jaeger,"If yes, with which syntax?",0.0,0.403,0.597,0.4019
jaeger,"I am using  Zuul  as an api-gateway in a spring-cloud micro-service app, so that every access  to  api-gateway/some-service/a_route  is redirected to  /a_route  in a generic way (the discovery is backed by consul).",0.0,0.038,0.962,0.0258
jaeger,I am trying to use  Jaeger  to instrument this system.,0.0,0.0,1.0,0.0
jaeger,"And at this point I am using   opentracing-spring-web-autoconfigure , because I cannot upgrade my spring boot/cloud version easily (I am using1.4.5.RELEASE Camden.SR7).",0.0,0.118,0.882,0.34
jaeger,"So I just added the dependency, created the Jaeger tracer and redirect it to the docker all in one collector.",0.0,0.1,0.9,0.25
jaeger,"I have begin by instrumenting the gateway and It somewhat works =  It generate span on the gateway, but all the route are marked :",0.0,0.0,1.0,0.0
jaeger,apigateway-service: GET,0.0,0.0,1.0,0.0
jaeger,"and there is no information concerning the forwarded route at this level, the full route itself is store in a tag : http.url 
"" http://localhost:8080/collection-service/collections/projects/ """,0.095,0.0,0.905,-0.296
jaeger,To be useful I would prefer to have span named :,0.0,0.266,0.734,0.4404
jaeger,apigateway-service: GET collection-service/collections/projects/,0.0,0.0,1.0,0.0
jaeger,Can this be configured somewhere ?,0.0,0.0,1.0,0.0
jaeger,"Our application consist of angular 6 for the UI and .netcore 2.0 for the back end, looking to implement tracing to it and so far opentracing seems the most prominent but I can't seem to find any good help documentations for .netcore 2.0 apps.",0.0,0.191,0.809,0.848
jaeger,My java code:,0.0,0.0,1.0,0.0
jaeger,My gradle dependencies:,0.0,0.0,1.0,0.0
jaeger,This code works in localhost.,0.0,0.0,1.0,0.0
jaeger,I have already passed the  JAEGER_AGENT_HOST  and  JAEGER_AGENT_PORT  env to the container.,0.0,0.0,1.0,0.0
jaeger,And I can see the Jaeger Initialized log in remote:,0.0,0.0,1.0,0.0
jaeger,Using the UDP Sender to send spans to the agent.,0.0,0.0,1.0,0.0
jaeger,"Using sender UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@27e16046, receiveBuf=null, receiveOffSet=-1, receiveLength=0))
  Using sender UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@27e16046, receiveBuf=null, receiveOffSet=-1, receiveLength=0))
  2018-08-16 13:24:32.809  INFO 1 --- [http-nio-8080-exec-1] io.jaegertracing.Configuration           : Initialized tracer=JaegerTracer(version=Java-0.30.4, serviceName=",0.0,0.0,1.0,0.0
jaeger,But I can see it in Jaeger UI.,0.0,0.0,1.0,0.0
jaeger,"And I tried to use  tcpdump , I cannot find the udp package.",0.0,0.0,1.0,0.0
jaeger,I am using Jaeger UI to display traces from my application.,0.0,0.0,1.0,0.0
jaeger,It's work fine for me if both application an Jaeger are running on same server.,0.0,0.114,0.886,0.2023
jaeger,But I need to run my Jaeger collector on a different server.,0.0,0.0,1.0,0.0
jaeger,"I tried out with JAEGER_ENDPOINT, JAEGER_AGENT_HOST and JAEGER_AGENT_PORT, but it failed.",0.331,0.0,0.669,-0.6652
jaeger,"I don't know, whether my values setting for these variables is wrong or not.",0.185,0.161,0.655,-0.1027
jaeger,Whether it required any configuration settings inside application code?,0.0,0.0,1.0,0.0
jaeger,Can you provide me any documentation for this problem?,0.281,0.0,0.719,-0.481
jaeger,I am trying to setup Jaeger tracing for my micro service that is written in Node.js using Express.js.,0.0,0.0,1.0,0.0
jaeger,"I have added a simple get request handler in my express app and when I hit the endpoint via curl, I can see that a span is generated in logs, but I do not see the name of my service in Jaeger UI.",0.0,0.0,1.0,0.0
jaeger,// server.js,0.0,0.0,1.0,0.0
jaeger,// tracing.js,0.0,0.0,1.0,0.0
jaeger,I see in logs:,0.0,0.0,1.0,0.0
jaeger,"
2018-03-08T01:03:34.519134479Z INFO  Reporting span 9b88812951bcd52f:9b88812951bcd52f:0:1",0.0,0.0,1.0,0.0
jaeger,I am new to jaeger and I am facing issues with finding the services list in the jaeger UI.,0.0,0.0,1.0,0.0
jaeger,Below are the .yaml configurations I prepared to run jaeger with my spring boot app on Kubernetes using minikube locally.,0.0,0.095,0.905,0.2263
jaeger,kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/production-elasticsearch/elasticsearch.yml --namespace=kube-system,0.0,0.344,0.656,0.2732
jaeger,kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/jaeger-production-template.yml --namespace=kube-system,0.0,0.344,0.656,0.2732
jaeger,Created deployment for my spring boot app and jaeger agent to run on the same container,0.0,0.118,0.882,0.25
jaeger,And the spring boot app service yaml,0.0,0.0,1.0,0.0
jaeger,I am getting,0.0,0.0,1.0,0.0
jaeger,No service dependencies found,0.423,0.0,0.577,-0.296
jaeger,Cannot find any information if Jaeger can be executed without docker?,0.0,0.0,1.0,0.0
jaeger,"Does a standalone jar exist, or will there be a release in the future for Jaeger like Zipkin has ?",0.0,0.135,0.865,0.3612
jaeger,Planning to use Jaeger for distributed tracing of our Application.,0.0,0.0,1.0,0.0
jaeger,"Need to use elasticsearch as db backend, rather than cassandra for Jaeger.",0.0,0.0,1.0,0.0
jaeger,I have a very general question about jaeger (opentracing).,0.0,0.0,1.0,0.0
jaeger,"I set up Jaeger, and can find the spans and traces - where each originiated from, where it ends up, and so on.",0.0,0.0,1.0,0.0
jaeger,"However, I am curious how to use Jaeger 'well'.",0.0,0.247,0.753,0.3182
jaeger,"I think Jaeger itself doesn't give much information, except for the fact I can check which server or api is the bottleneck.",0.0,0.0,1.0,0.0
jaeger,"The scenario I have in mind, is to get an alert for an error or warning from the logging system (probably it will be ElasticSearch), and get the trace id from it, and check the whole trace from Jaeger.",0.121,0.052,0.827,-0.4404
jaeger,Any suggestions on how to use Jaeger 'well'?,0.0,0.0,1.0,0.0
jaeger,i have been having some problem.,0.403,0.0,0.597,-0.4019
jaeger,i have a k8s cluster up and running and wanted it to connect to jaeger.,0.0,0.0,1.0,0.0
jaeger,followed this page(ReadMe.md)  https://github.com/jaegertracing/jaeger-operator?utm_source=thenewstack&amp;utm_medium=website&amp;utm_campaign=platform,0.0,0.0,1.0,0.0
jaeger,i have the jaeger ui now but it not able to recognize the services.,0.0,0.0,1.0,0.0
jaeger,can someone give any suggestions??,0.0,0.0,1.0,0.0
jaeger,"In the above link i didnt understand what this line means, &quot; You probably also want to download and customize the operator.yaml, setting the env var WATCH_NAMESPACE to have an empty value, so that it can watch for instances across all namespaces.",0.041,0.085,0.874,0.2263
jaeger,&quot;,0.0,0.0,1.0,0.0
jaeger,kubectl logs -n observability deployment/jaeger-operator,0.0,0.0,1.0,0.0
jaeger,"time=&quot;2021-01-07T06:48:47Z&quot; level=info msg=Versions arch=amd64 identity=observability.jaeger-operator jaeger=1.21.0 jaeger-operator=v1.21.2 operator-sdk=v0.18.2 os=linux version=go1.14.12
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Consider running the operator in a cluster-wide scope for extra features&quot;
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Auto-detected the platform&quot; platform=kubernetes
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Auto-detected ingress api&quot; ingress-api=networking
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Automatically adjusted the 'es-provision' flag&quot; es-provision=no
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Automatically adjusted the 'kafka-provision' flag&quot; kafka-provision=no
time=&quot;2021-01-07T06:48:50Z&quot; level=info msg=&quot;Install prometheus-operator in your cluster to create ServiceMonitor objects&quot; error=&quot;no ServiceMonitor registered with the API&quot;",0.0,0.031,0.969,0.2732
jaeger,"I am using below example of OpenTracing, Jaeger
 https://github.com/yurishkuro/opentracing-tutorial/tree/master/csharp/src/lesson01",0.0,0.0,1.0,0.0
jaeger,I have my Jaeger UI running for which I used the below command.,0.0,0.0,1.0,0.0
jaeger,Below is my code which I tried from above github link,0.0,0.0,1.0,0.0
jaeger,Above is the Class Library project.,0.0,0.0,1.0,0.0
jaeger,Below is my console app where I am calling the TriggerTrace Method.,0.0,0.0,1.0,0.0
jaeger,"When that function is executed I would expect some traces in the Jaeger UI,",0.0,0.0,1.0,0.0
jaeger,"Below is the error I see, I do not see any traces in the UI,
Am I missing any configuration ?",0.246,0.0,0.754,-0.5994
jaeger,I have a simple Spring Boot 2.x RestController with an endpoint performing certain remote calls as well as controller is also calling an Async method that in turn makes several remote HTTP calls.,0.0,0.127,0.873,0.4939
jaeger,I'm having opentracing-spring-jaeger-web-starter in classpath with tracing enabled.,0.0,0.0,1.0,0.0
jaeger,"If i invoke my REST endpoint, It creates a span for the endpoint call as well as remote calls that the controller is making synchronously.",0.0,0.167,0.833,0.4939
jaeger,However the remote calls made by Async method is getting reported in its own span.,0.0,0.0,1.0,0.0
jaeger,Is this by design or is there a way to propagate some context information to the Async method to better group/relate the spans ?,0.0,0.121,0.879,0.4404
jaeger,I recently deployed the jaeger agent as a daemonset on my k8s cluster alongside a collector.,0.0,0.0,1.0,0.0
jaeger,When trying to send spans to the agent using:,0.0,0.0,1.0,0.0
jaeger,When looking at the application logs I see:,0.0,0.0,1.0,0.0
jaeger,"All nodes can access each other as the security group does not block ports between them, when using a sidecar agent the spans are sent without issue.",0.0,0.167,0.833,0.5867
jaeger,Replicate:,0.0,0.0,1.0,0.0
jaeger,Deploy agent using:,0.0,0.0,1.0,0.0
jaeger,Then deploy hotrod application:,0.0,0.0,1.0,0.0
jaeger,I'm following this tutorial:  https://github.com/yurishkuro/opentracing-tutorial/tree/master/java/src/main/java/lesson03 .,0.0,0.0,1.0,0.0
jaeger,What need to be set so that services running in different hosts can send the data to the same backend?,0.0,0.0,1.0,0.0
jaeger,I am playing with quarkus and jaeger by opentracing integration.,0.0,0.184,0.816,0.2023
jaeger,After run the jaeger server and the  https://github.com/quarkusio/quarkus-quickstarts/tree/master/opentracing-quickstart  repo I found the traces at http://localhost:16686/search.,0.0,0.0,1.0,0.0
jaeger,"But I only found the Resource class, arguments, and Process name , but the &quot;Logs&quot; is not shown on trace detail expand.",0.145,0.12,0.735,-0.1531
jaeger,The steps are easy:,0.0,0.492,0.508,0.4404
jaeger,1.Run jaeger server  docker run --rm=true --name erp_jaeger_server -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 -p 5775:5775/udp -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14268:14268 -p 9411:9411 jaegertracing/all-in-one:latest,0.0,0.0,1.0,0.0
jaeger,"clone the example repo and run it
  https://github.com/quarkusio/quarkus-quickstarts/tree/master/opentracing-quickstart 
(no further configuration)",0.0,0.0,1.0,0.0
jaeger,run-&gt;  mvn quarkus:dev,0.0,0.0,1.0,0.0
jaeger,visit  http://localhost:8080/hello/,0.0,0.0,1.0,0.0
jaeger,5.Explore on jaeger ui 'http://localhost:16686/',0.0,0.0,1.0,0.0
jaeger,"6.Found the traces Tags, and Process Details but detailes content Log.info('hello') is not shown",0.0,0.0,1.0,0.0
jaeger,I was trying with @Slfj but i got the same result,0.0,0.0,1.0,0.0
jaeger,Thanks in advance.,0.0,0.592,0.408,0.4404
jaeger,"I have two endpoints, the first one uses Feign implementation:",0.0,0.0,1.0,0.0
jaeger,Controller:,0.0,0.0,1.0,0.0
jaeger,Feign Client:,0.0,0.0,1.0,0.0
jaeger,"the second, do the same thing, but uses RetroFit implementation:",0.0,0.0,1.0,0.0
jaeger,Controller:,0.0,0.0,1.0,0.0
jaeger,Retrofit Client:,0.0,0.0,1.0,0.0
jaeger,Bean:,0.0,0.0,1.0,0.0
jaeger,"With Feign implementation, Jaeger UI shows me two steps: Controller Request and the Http Request [GET] to  http://viacep.com.br .",0.0,0.0,1.0,0.0
jaeger,"With Retrofit, Jaeger does not show the request to  http://viacep.com.br .",0.0,0.0,1.0,0.0
jaeger,Only the controller step.,0.0,0.0,1.0,0.0
jaeger,Why?,0.0,0.0,1.0,0.0
jaeger,Is there some configuration that I can do at okHttpClient builder?,0.0,0.0,1.0,0.0
jaeger,I can`t use feign in this project.,0.0,0.0,1.0,0.0
jaeger,I am trying to instrument my program with jaeger-tracing (c++).,0.0,0.0,1.0,0.0
jaeger,"I was able to view my traces when I compliled the program with yaml-cpp version 0.5.3, but when I changed my yaml-cpp version to 0.6.x, I am unable to view my traces.",0.0,0.0,1.0,0.0
jaeger,Dont know why its happening.,0.0,0.0,1.0,0.0
jaeger,JaegerProgram Source code;,0.0,0.0,1.0,0.0
jaeger,compiling command -  g++ -std=c++1z test.cpp -L /usr/local/lib/libyaml-cpp.a -ljaegertracing -lyaml-cpp,0.0,0.0,1.0,0.0
jaeger,Yaml file,0.0,0.0,1.0,0.0
jaeger,"OS : ubuntu 18.04
jaegerTracing : master branch version",0.0,0.0,1.0,0.0
jaeger,UPDATE,0.0,0.0,1.0,0.0
jaeger,"after little digging I found some fact, When I parse the above mention config file try to print the result I get the same value as written in config file, but when I parse same file using  yaml-cpp-0.6.x  the sampler.type is showing 'remote' and sampler.param to be '0.001' and when I manually change this change these values to be same as in  config.yaml  it has started showing traces.",0.0,0.079,0.921,0.6428
jaeger,The error is present in parsing the yaml file as I could clearly see different values is loaded as configuration.,0.112,0.224,0.664,0.4019
jaeger,Following  this  guide I I don't really see how to implement the operator with elasticsearch.,0.0,0.0,1.0,0.0
jaeger,"Ok, so I install the operator and after that follwing the  example which is with :",0.0,0.145,0.855,0.296
jaeger,which is not supported by openshift as an api.,0.197,0.0,0.803,-0.2411
jaeger,"I just need to deploy jeager operator for with 1 elasticsearch, but this guide is quite confusing.",0.166,0.0,0.834,-0.4194
jaeger,Does anyone know a quick and easy guide on how to do it?,0.0,0.209,0.791,0.4404
jaeger,I've found an example:  https://medium.com/velotio-perspectives/a-comprehensive-tutorial-to-implementing-opentracing-with-jaeger-a01752e1a8ce,0.0,0.0,1.0,0.0
jaeger,I have a pretty large codebase and I really don't want to modify every function by adding a line like ' with tracer.start_span('booking') as span:'.,0.062,0.236,0.702,0.6369
jaeger,Is there any way to do it?,0.0,0.0,1.0,0.0
jaeger,Thanks in advance.,0.0,0.592,0.408,0.4404
jaeger,I use Jaeger with Elasticsearch and I want to remove old indices.,0.0,0.126,0.874,0.0772
jaeger,"I tried  jaeger-es-index-cleaner , see  Remove old data :",0.0,0.0,1.0,0.0
jaeger,Remove old data,0.0,0.0,1.0,0.0
jaeger,The historical data can be removed with the  jaeger-es-index-cleaner  that is also used for daily indices.,0.0,0.0,1.0,0.0
jaeger,&lt;1&gt; Remove indices older than 14 days.,0.0,0.0,1.0,0.0
jaeger,Log,0.0,0.0,1.0,0.0
jaeger,"I tried to delete all indices older than 2 days, but no indice was deleted:",0.189,0.0,0.811,-0.4215
jaeger,Indices,0.0,0.0,1.0,0.0
jaeger,"If I list all indices with  http://localhost:9200/_cat/indices , I still see old indices:",0.0,0.0,1.0,0.0
jaeger,Question,0.0,0.0,1.0,0.0
jaeger,How to delete old indices of Jaeger from Elasticsearch?,0.0,0.0,1.0,0.0
jaeger,I have configuration as  documentation  says,0.0,0.0,1.0,0.0
jaeger,Collector produces error.,0.574,0.0,0.426,-0.4019
jaeger,How I can configure collector to balance exporter for sending requests in different backends?,0.0,0.0,1.0,0.0
jaeger,info    exporterhelper/queued_retry.go:276      Exporting failed.,0.524,0.0,0.476,-0.5106
jaeger,Will retry the request after interval.,0.0,0.0,1.0,0.0
jaeger,"{&quot;component_kind&quot;: &quot;exporter&quot;, &quot;component_type&quot;: &quot;jaeger&quot;, &quot;component_name&quot;: &quot;jaeger&quot;, &quot;error&quot;: &quot;failed to push trace data via Jaeger exporter: rpc error: code = Unavailable desc = last connection error: connection error: desc = &quot;transport: Error while dialing dial tcp: address ipv4:firstHost:14250,secondHost:14250: too many colons in address&quot;&quot;, &quot;interval&quot;: &quot;30.456378855s&quot;}",0.226,0.0,0.774,-0.8689
jaeger,"I have been reading the documentation from Spring Cloud Sleuth and Zipkin, and I did not locate anything about how to show in Jaeger the logs came from Zipkin.",0.0,0.0,1.0,0.0
jaeger,This is an example using the jaeger stack:,0.0,0.0,1.0,0.0
jaeger,"And this is an example using the same stack, but exporting to Jaeger using the Zipkin collector (port 9411 on Jaeger)",0.0,0.0,1.0,0.0
jaeger,There is no  Logs  table.,0.355,0.0,0.645,-0.296
jaeger,"Does anyone knows if would be possible show those logs there, like Jaeger implementation does?",0.0,0.152,0.848,0.3612
jaeger,I have a spring boot application with several microservices.,0.0,0.0,1.0,0.0
jaeger,There are about 100+ different events.,0.0,0.0,1.0,0.0
jaeger,And I wanted to see in convenient UI to see sequence of them.,0.0,0.0,1.0,0.0
jaeger,"I googled about Jaeger UI, ran it via docker container and everything works almost fine, except one important thing, events are not grouped, I just see multiple independent events.",0.0,0.117,0.883,0.3197
jaeger,"I give 2 examples,
First: how I want it to see
Second: how I see.",0.0,0.115,0.885,0.0772
jaeger,Thanks for any suggestions.,0.0,0.492,0.508,0.4404
jaeger,"networks:
axonnet:
driver: bridge",0.0,0.0,1.0,0.0
jaeger,"I am new to Jaeger and Kafka,",0.0,0.0,1.0,0.0
jaeger,I am trying to use Kafka as intermediate buffer.,0.0,0.0,1.0,0.0
jaeger,I am using OpenTelemetry to send data to Jaeger-Collector directly using  -Dotel.exporter.jaeger.endpoint .,0.0,0.0,1.0,0.0
jaeger,Jaeger-Collector is deployed on Kubernetes and the Kafka is on another network but is accessible.,0.0,0.0,1.0,0.0
jaeger,I can confirm that the traces are sent to Jaeger-collector.,0.0,0.0,1.0,0.0
jaeger,On hitting the /metrics of collector and output tells me that spans were written successfully to Kafka.,0.0,0.167,0.833,0.4939
jaeger,jaeger_kafka_spans_written_total{status=&quot;success&quot;} 21,0.0,0.0,1.0,0.0
jaeger,The Collector logs indicate what topic I am writing to,0.0,0.0,1.0,0.0
jaeger,"{&quot;Brokers&quot;:[&quot;myKafkaBroker......&quot;}},&quot;topic&quot;:&quot;tp6&quot;}",0.0,0.0,1.0,0.0
jaeger,I want to get this (Span) data from Kafka Queue to ElasticSearch.,0.0,0.115,0.885,0.0772
jaeger,To do this I am starting the Jaeger Ingester as follows,0.0,0.0,1.0,0.0
jaeger,docker run  -e &quot;SPAN_STORAGE_TYPE=elasticsearch&quot;  jaegertracing/jaeger-ingester:1.22 --kafka.consumer.topic=tp6 --kafka.consumer.brokers='myKafkaBroker'   --es.tls.skip-host-verify,0.0,0.0,1.0,0.0
jaeger,But the container is stopped after fatal error,0.681,0.0,0.319,-0.8922
jaeger,The elasticsearch and ingester are being run on the same machine using docker.,0.0,0.0,1.0,0.0
jaeger,The elasticsearch is running on docker using,0.0,0.0,1.0,0.0
jaeger,docker run -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot;ocker.elastic.co/elasticsearch/elasticsearch:7.11.2,0.0,0.0,1.0,0.0
jaeger,I have disabled TLS so that shouldn't be a problem.,0.0,0.244,0.756,0.3089
jaeger,I am unable to get this to work.,0.0,0.0,1.0,0.0
jaeger,　　I am trying to learn kubernetes recently.,0.0,0.0,1.0,0.0
jaeger,"I have already deployed jaeger (all-in-one) by istio on kubernetes, and everying works well.",0.0,0.149,0.851,0.2732
jaeger,"Although I can see the trace information on the jaeger UI, I don't know how to extract these trace data by python.",0.0,0.0,1.0,0.0
jaeger,I want to use these data to do root causes location of microservices.,0.0,0.106,0.894,0.0772
jaeger,I think there must be some API to access these data dicectly by python but I don't find it.,0.0,0.0,1.0,0.0
jaeger,or can I access the cassandra using python to get these data?.,0.0,0.0,1.0,0.0
jaeger,I am searching for a long time on net.,0.0,0.0,1.0,0.0
jaeger,But no use.,0.583,0.0,0.417,-0.4215
jaeger,Please help or try to give some ideas how to achieve this.,0.0,0.333,0.667,0.6124
jaeger,Getting this error when i startup jaeger allinone docker latest.,0.252,0.0,0.748,-0.4019
jaeger,Not sure why this is - can anyone help here?,0.168,0.232,0.6,0.1872
jaeger,"I am running this on Windows, Docker for desktop.",0.0,0.0,1.0,0.0
jaeger,"This is behind a corp proxy, if that's helpful.",0.0,0.286,0.714,0.4215
jaeger,This is the command i am using to startup,0.0,0.0,1.0,0.0
jaeger,I'm new to Clickhouse.,0.0,0.0,1.0,0.0
jaeger,I'm trying to read Jaeger logs from Kafka into Clickhouse db.,0.0,0.0,1.0,0.0
jaeger,I have following Kafka messages format:,0.0,0.0,1.0,0.0
jaeger,"I was able to input traceID, spanID and Operation into Clickhouse using the following table:",0.0,0.0,1.0,0.0
jaeger,But I failed to input tags.,0.527,0.0,0.473,-0.6652
jaeger,Any idea which Clickhouse data type I should use for it?,0.0,0.0,1.0,0.0
jaeger,I have a tornado application in which I use jaeger for tracing.,0.0,0.0,1.0,0.0
jaeger,"My problem is that jaeger keeps logging at the INFO level, tons of entries like this :",0.141,0.13,0.729,-0.0516
jaeger,"I've tried a bunch of configuration to try to remove these entries, but so far without luck.",0.194,0.0,0.806,-0.5597
jaeger,My current jaeger logger configurations is:,0.0,0.0,1.0,0.0
jaeger,"How can I turn off, INFO logging for jaeger ?",0.0,0.0,1.0,0.0
jaeger,Using jaeger to instrutment our HTTP API (nestJS application).,0.0,0.0,1.0,0.0
jaeger,I would like to put an alert if span duration exceeds a threshold.,0.0,0.343,0.657,0.5719
jaeger,"We are using elasticsearch as backend, so we could setup elasticsearch watcher, but I am wondering if jaeger eco-system bring a better solution?",0.0,0.264,0.736,0.7783
jaeger,I was trying to use Jaeger to trace some DAG execution (with some long tasks execution time) and I have some pretty good results in term of visualization &quot;post-execution&quot;.,0.0,0.196,0.804,0.7269
jaeger,I know this is not a common usage of Jaeger (by extension OpenTracing) but this is doing  almost  what I was looking for.,0.0,0.0,1.0,0.0
jaeger,I am saying  almost  because I thought that a span would be displayed in the UI timeline as soon as it was &quot;started&quot; in the code.,0.0,0.0,1.0,0.0
jaeger,"But, as far as I understand, to be displayed in a trace, spans need to be complete and Jaeger is not yet able to store incomplete spans (I have seen some open PRs in GitHub).",0.0,0.0,1.0,0.0
jaeger,"My need is that I would like to do some real-time monitoring to know where are my bottlenecks on the DAG that I need to execute, and viewing those tasks execution in a single timeline as soon as they are started would have been awesome.",0.0,0.142,0.858,0.765
jaeger,Do you know if such a behavior is possible with Jaeger or Zipkin to do some real-time traces rendering ?,0.0,0.0,1.0,0.0
jaeger,Or if there is an open-source tool capable of doing that ?,0.0,0.206,0.794,0.3818
jaeger,Cheers !,0.0,1.0,0.0,0.5255
jaeger,I have a question regarding the Jaeger Operator which I hope someone could help me with.,0.0,0.337,0.663,0.6808
jaeger,I am deploying the Jaeger operator on k3s by simply adding the below template to my helm chart (templates/jaeger.yaml):,0.0,0.0,1.0,0.0
jaeger,I have imported the Jaeger-Operator dependency in my Chart.yaml as below:,0.0,0.0,1.0,0.0
jaeger,The operator deploys it's own ingress controller.,0.0,0.0,1.0,0.0
jaeger,What changes do I need to make to my templates so that I can disable the deployment of ingress from Jaeger Operator and have it done through the ingress.yaml that I will define ?,0.0,0.0,1.0,0.0
jaeger,B3 headers can be propagated using  zipkin.NewZipkinB3HTTPHeaderPropagator(),0.0,0.0,1.0,0.0
jaeger,as explained here,0.0,0.0,1.0,0.0
jaeger,Can uber-trace-id also be propagated along with this ?,0.0,0.0,1.0,0.0
jaeger,uber-trace-id is the default format in jaeger but I need both uber-trace-id as well as Zipkin B3 headers,0.0,0.142,0.858,0.3919
jaeger,Specifically can we add more injectors and extractors like this,0.0,0.217,0.783,0.3612
jaeger,I'm trying to use Jaeger to manage tracing system.,0.0,0.0,1.0,0.0
jaeger,Docker is running locally &quot;all-in-one&quot; image with application (on the same host) without any issues.,0.0,0.0,1.0,0.0
jaeger,My question is how to configure jaeger agent on host1 that would send traces jaeger collector on another host2.,0.0,0.0,1.0,0.0
jaeger,Host2 is configured with &quot;all-in-one&quot;.,0.0,0.0,1.0,0.0
jaeger,I can see Jaeger UI on host2 but it doesn't seem getting any traces from host1.,0.0,0.0,1.0,0.0
jaeger,Configure tracer:,0.0,0.0,1.0,0.0
jaeger,Added environment variables in yaml file on host1:,0.0,0.0,1.0,0.0
jaeger,Added jaeger image in yaml file on host2:,0.0,0.0,1.0,0.0
jaeger,Any suggestions will be appreciated.,0.0,0.452,0.548,0.5106
jaeger,I'm new to the world of Opentelemetry and would like to send the  Spring-petclinic  instrumentation data to Jaeger which is running on my remote cloud system,0.0,0.091,0.909,0.3612
jaeger,"Here is the bat file:
 java -javaagent:opentelemetry-javaagent-all.jar -Dotel.exporter=jaeger -Dotel.exporter.jaeger.endpoint=50.18.XXX.XX:14250 -Dotel.otlp.span.timeout=4000 -Dotel.jaeger.service.name=otel-ui -jar target/spring-petclinic-2.4.0.BUILD-SNAPSHOT.jar",0.0,0.0,1.0,0.0
jaeger,"When I run the bat file, I'm abe to open the petclinic app in browser (http://localhost:8080), I get the following error in the console:
 [opentelemetry.auto.trace 2021-01-06 17:22:21:008 +0530] [grpc-default-executor-1] WARN io.opentelemetry.exporter.otlp.OtlpGrpcSpanExporter - Failed to export spans.",0.213,0.0,0.787,-0.7983
jaeger,Error message: UNAVAILABLE: io exception,0.403,0.0,0.597,-0.4019
jaeger,How to resolve this issue?,0.0,0.394,0.606,0.3818
jaeger,Are there any other dependencies to be the added to the petclinic pom.xml or to the code?,0.0,0.0,1.0,0.0
jaeger,Is there any way to load jaeger embeded ui with https instead of http.,0.0,0.0,1.0,0.0
jaeger,"from this : http://jaegerip/search?&amp;uiEmbed=v0
to this : https://jaegerip/search?&amp;uiEmbed=v0",0.0,0.0,1.0,0.0
jaeger,I am trying to incorporate Jaeger and OpenTracing into my Class Library project in .Net core.,0.0,0.0,1.0,0.0
jaeger,Most of the Jaeger documentation shows how to configure Jaeger for Web API's in Startup.cs file.,0.0,0.0,1.0,0.0
jaeger,The below configuration works for .Net core API project,0.0,0.0,1.0,0.0
jaeger,but I would like to configure the same for my Class LIbrary project.,0.0,0.228,0.772,0.5023
jaeger,Is it possible ?,0.0,0.0,1.0,0.0
jaeger,I mean for example someone else uses my class library in their App I should be able to see the tracing.,0.0,0.0,1.0,0.0
jaeger,The Jaeger configuration should be made in Class LIbarary.,0.0,0.0,1.0,0.0
jaeger,I am getting the following exception while creating a span.,0.0,0.239,0.761,0.296
jaeger,It works fine for Logging Exporter but for jaeger gives the following,0.0,0.113,0.887,0.1027
jaeger,At App startup I do,0.0,0.0,1.0,0.0
jaeger,And at runtime,0.0,0.0,1.0,0.0
jaeger,I'm running my Camel Quarkus service on Openshift where Jaeger is also installed.,0.0,0.0,1.0,0.0
jaeger,The Jaeger agent is running as a daemon set.,0.0,0.0,1.0,0.0
jaeger,I'm not getting any traces from my Camel service in the Jaeger UI using following properties:,0.0,0.0,1.0,0.0
jaeger,I also have some Spring Boot services running in the same namespace and they work as they should.,0.0,0.0,1.0,0.0
jaeger,"Therefore, I think the Camel service is configured incorrectly.",0.0,0.0,1.0,0.0
jaeger,Please help.,0.0,1.0,0.0,0.6124
jaeger,What am I missing?,0.524,0.0,0.476,-0.296
jaeger,Jaeger traces to spring-boot application are not able to capture traces for the DB calls made using spring-data.,0.0,0.0,1.0,0.0
jaeger,All other calls like RESTTemplate are able to have the traces captured.,0.0,0.185,0.815,0.3612
jaeger,"Using springboot version 2.2.2.RELEASE and added below jaeger dependencies,",0.0,0.0,1.0,0.0
jaeger,Any additional dependencies are missing here?,0.306,0.0,0.694,-0.296
jaeger,I am deploying Jaeger using the  Jaeger Operator  and it seems to be working fine.,0.0,0.122,0.878,0.2023
jaeger,"However, now I am trying to set up Prometheus metrics scraping (using the  Prometheus Operator ) but I am not seeing a  Service  in my cluster that exposes the metrics ports for the Jaeger Collector (port 14269) or Query services (port 16687) ( port number reference from the Jeager Monitoring documentation ).",0.036,0.03,0.934,-0.0772
jaeger,The only relevant  Service  I see is  jaeger-operator-metrics :,0.0,0.0,1.0,0.0
jaeger,I am able to set up a Prometheus  ServiceMonitor  to scrape metrics from this service but I am not sure if this includes the metrics that are normally gathered by the Collector and Query microservices or not...,0.069,0.0,0.931,-0.3491
jaeger,I am guessing not as that would seem to violate the premise of microservices.,0.211,0.0,0.789,-0.4939
jaeger,Is there some setting in the Jaeger Operator spec that I missed for exposing those metrics endpoints in the other components?,0.193,0.0,0.807,-0.5106
jaeger,I have the problem that I cannot seem to get Grafana Tempo working with a Jaeger client.,0.172,0.0,0.828,-0.4019
jaeger,Following their official docker-compose example everything should be straight forward:  https://github.com/grafana/tempo/tree/master/example/docker-compose,0.0,0.16,0.84,0.2263
jaeger,I've basically just adapted the official Jaeger python client example.,0.0,0.0,1.0,0.0
jaeger,Since Grafana Tempo is running on the same machine the reporting_host is set to localhost and since the synthetic-load-generator uses port 14268 in the JAEGER_COLLECTOR_URL I'm using this as well.,0.0,0.068,0.932,0.2732
jaeger,I can query for traces generated by the synthetic-load-generator without a problem but I cannot seem to get it working with my script.,0.0,0.079,0.921,0.1603
jaeger,If I use e.g.,0.0,0.0,1.0,0.0
jaeger,the all-in-one Jaeger container I can query for my traces.,0.0,0.0,1.0,0.0
jaeger,As suggested here  https://grafana.com/docs/tempo/latest/getting-started/  I've also tried to use the &quot;Jaeger - Thrift Compact&quot; Protocol on port 6831 but it doesn't seem to work either.,0.0,0.0,1.0,0.0
jaeger,Can somebody point me into the right direction what I might be doing wrong?,0.205,0.0,0.795,-0.4767
jaeger,Thanks in advance!,0.0,0.615,0.385,0.4926
jaeger,!,0.0,0.0,0.0,0.0
jaeger,"Appreciate your support for the below issue as I built my demo as the below steps,",0.0,0.293,0.707,0.6597
jaeger,"I built two microservices one by Django and the another by Go,
Django send HTTP request to Go service,",0.0,0.0,1.0,0.0
jaeger,"Jaeger tool is configured for UI tracing,",0.0,0.0,1.0,0.0
jaeger,"Django tracing and Go tracing are separated in Jaeger tool and I do not know the reason although I received Django parent trace id in the request header and it is normal to be all Django request tracing including Go tracing as one request tracing in Jaeger,",0.0,0.0,1.0,0.0
jaeger,My Git repo:  https://github.com/OmarEltamasehy/django-gotracing-example,0.0,0.0,1.0,0.0
jaeger,The below is Django code for calling Golang service,0.0,0.0,1.0,0.0
jaeger,Golang service code,0.0,0.0,1.0,0.0
jaeger,Jaeger UI output,0.0,0.0,1.0,0.0
jaeger,this image refer to my inquiry why my request show Django tracing only without Go tracing for the same request,0.0,0.0,1.0,0.0
jaeger,"As the doc  https://www.jaegertracing.io/docs/1.19/troubleshooting/  says &quot;The logging reporter follows the sampling decision made by the sampler, meaning that if the span is logged, it should also reach the agent or collector.&quot;",0.0,0.035,0.965,0.0258
jaeger,"But thats not happening, my gRPC server is logging the span -  &quot;{&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;Reporting span 1559209397c51d88:3217766e13b74f76:1559209397c51d88:1&quot;,&quot;time&quot;:&quot;2020-11-11T17:31:47Z&quot;}&quot;  , but I am not able to see the same span on jaeger-all-in-one UI.",0.0,0.0,1.0,0.0
jaeger,The REST client span is projected onto the UI and is the parent of the trace.,0.0,0.0,1.0,0.0
jaeger,Please help me to be able to project the grpc spans onto UI.,0.0,0.312,0.688,0.6124
jaeger,"Referring to the link ( https://github.com/burkaa01/jaeger-tracing-kafka/tree/master/stream-app ), created stream pipeline with Jaeger enabled.",0.0,0.154,0.846,0.25
jaeger,It is a springboot application but bean configurations are defined in spring xml file.,0.0,0.0,1.0,0.0
jaeger,"As part of stream topology, in transformation, while getting processorContext.headers(), i am getting error.",0.184,0.0,0.816,-0.4019
jaeger,Stream pipeline works if jaeger is disabled.,0.0,0.0,1.0,0.0
jaeger,Also it works if the beans are defined in annotations .,0.0,0.0,1.0,0.0
jaeger,"Referred JIRA,  https://issues.apache.org/jira/browse/KAFKA-4344 
 
Clueless on the issue, here is the error stack for reference",0.302,0.0,0.698,-0.6369
jaeger,I have used the following configuration to setup the Istio,0.0,0.0,1.0,0.0
jaeger,"I want to access the services like grafana, prometheus, jaeger, kiali &amp; envoy externally - eg:  https://grafana.mycompany.com , how can I do it?",0.0,0.183,0.817,0.4215
jaeger,"Update: 
I have tried below however it doesn't work",0.0,0.0,1.0,0.0
jaeger,I'm trying to use the  Jaeger  package to send traces to Jaeger from a C# app.,0.0,0.0,1.0,0.0
jaeger,"There are no minimal examples in the jaeger-client-csharp documentation, but from what I read, I think this should work.",0.091,0.0,0.909,-0.1531
jaeger,I have jaeger-all-in-one.exe running but when I run this code there's no sign of any new traces.,0.167,0.0,0.833,-0.4215
jaeger,"I've tried manually configuring samplers, senders, reporters, etc.",0.0,0.0,1.0,0.0
jaeger,but nothing I tried worked.,0.0,0.0,1.0,0.0
jaeger,What do I need to add to get my traces to appear in Jaeger?,0.0,0.0,1.0,0.0
jaeger,"I have installed Gitlab in AKS cluster, and I also installed Jaeger.",0.0,0.0,1.0,0.0
jaeger,Both the applications are up and running.,0.0,0.0,1.0,0.0
jaeger,I want to integrate GitLab with jaeger.,0.0,0.206,0.794,0.0772
jaeger,"I searched for any documentation on how to do in AKS , but didnt find any.",0.0,0.0,1.0,0.0
jaeger,Any suggestion is welcome.,0.0,0.5,0.5,0.4588
jaeger,I have question regarding  global.tracer.zipkin.address  while deploying istio.,0.0,0.0,1.0,0.0
jaeger,"I am using Jaeger, and have Jaeger Agents deployed in DaemonSet.",0.0,0.0,1.0,0.0
jaeger,"As I have each Jaeger Agents (on each nodes), Jaeger Collector, and Jaeger Query, I believe global.tracer.zipkin.address should be configured as Jaeger Agent host.",0.0,0.0,1.0,0.0
jaeger,"However, Agents are on each nodes, and I have hard time specifying the host.",0.104,0.0,0.896,-0.1027
jaeger,How should I specify it?,0.0,0.0,1.0,0.0
jaeger,Thanks in advance.,0.0,0.592,0.408,0.4404
jaeger,"FYI) If I understood correctly, Jaeger Client will send the data to the Jaeger Agent via Envoy, and then to Jaeger Collector.",0.0,0.0,1.0,0.0
jaeger,I am working with  Vert.X 3.9  and  Java 8  and I'm trying to implement  Opentrace  with  Jaeger .,0.0,0.0,1.0,0.0
jaeger,I have an issue on sending method for the  Spans .,0.0,0.0,1.0,0.0
jaeger,"There is a  jaeger-collector  already working for another services (Not using Vert.X), but for some reason it is not receiving traces from the Vert.X app.",0.0,0.0,1.0,0.0
jaeger,So far I noticed that the Sender is setted as  UdpSender  instead of  httpSender  althought I already set the env variable  JAEGER_ENDPOINT .,0.0,0.0,1.0,0.0
jaeger,This is the Java code:,0.0,0.0,1.0,0.0
jaeger,This code creates some logs that show the Tracer and Spam creation:,0.159,0.268,0.573,0.1779
jaeger,"Regarding docs the sender should be httpSender, but here is udp",0.0,0.0,1.0,0.0
jaeger,"sender=UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@55c10031, receiveBuf=null, receiveOffSet=-1, receiveLength=0)), closeEnqueueTimeout=1000)",0.0,0.0,1.0,0.0
jaeger,These are the dependencies I am using:,0.0,0.0,1.0,0.0
jaeger,I have a simple Node.js app the uses the Open Telemetry Jaeger exporter to send trace information into Jaeger.,0.0,0.0,1.0,0.0
jaeger,It runs fine when I fire up the Jaeger as a Docker container then run the code from my machine's command line against  localhost .,0.099,0.074,0.826,-0.1531
jaeger,"However, when I try to run both the app and jaeger under Docker Compose within a Docker Compose network, the service registers and is apparent in the Jaeger UI, but the trace/span information never gets received into Jaeger.",0.0,0.0,1.0,0.0
jaeger,Here is the code:  https://github.com/reselbob/simpletracing/tree/releases/v1.0,0.0,0.0,1.0,0.0
jaeger,I attached a screenshot to demonstrate that the service seems to be registering but the spans are not getting through.,0.0,0.0,1.0,0.0
jaeger,.,0.0,0.0,0.0,0.0
jaeger,"I am manually instrumenting code using jaeger, and have a question on how to instrument code that is generated automatically for me?",0.0,0.0,1.0,0.0
jaeger,An example is when I try to instrument code that uses Spring's CrudRepository and MongoRepository.,0.0,0.0,1.0,0.0
jaeger,Anyone have any ideas?,0.0,0.0,1.0,0.0
jaeger,"When I use opentelemetry's auto instrumentation javaagent jar located here,  https://github.com/open-telemetry/opentelemetry-java-instrumentation  it is able to trace the MongoRepository method that is a generated.",0.0,0.0,1.0,0.0
jaeger,I am trying to set up Jaeger to collect traces from a spring boot application.,0.0,0.0,1.0,0.0
jaeger,"When my app starts up, I am getting this warning message",0.234,0.0,0.766,-0.4118
jaeger,warn io.jaegertracing.internal.senders.SenderResolver - No sender factories available.,0.474,0.0,0.526,-0.3818
jaeger,"Using NoopSender, meaning that data will not be sent anywhere!",0.0,0.0,1.0,0.0
jaeger,I use this method to get the jaeger tracer,0.0,0.0,1.0,0.0
jaeger,"I have manually instrumented the code, but no traces show up in the jaeger UI.",0.177,0.0,0.823,-0.4215
jaeger,I have been stuck on this problem for a few days now and would appreciate any help given!,0.21,0.254,0.536,0.2481
jaeger,"In my pom file, I have dependencies on jaeger-core and opentracing-api",0.0,0.0,1.0,0.0
jaeger,I am required to run a jaeger-agent on a bare-metal server that doesn't have support for docker.,0.148,0.0,0.852,-0.3089
jaeger,I have downloaded a jaeger-binary on it and am able to successfully accept binary traces on the udp port I specified in the config.,0.0,0.234,0.766,0.7003
jaeger,"But I have the use-case where I'm only required to accept traces in binary format, and this means that I do NOT want to open the port for accepting compact thrifts.",0.039,0.199,0.762,0.7556
jaeger,"Could anyone help me in achieving this (essentially, I should be able to open a minimal set of ports to run my agent).",0.0,0.119,0.881,0.4019
jaeger,I have a microservices-based application Running on Kubernetes.,0.0,0.0,1.0,0.0
jaeger,The microservices are built using dropwizard framework.,0.0,0.0,1.0,0.0
jaeger,I would like to enable tracing in order to track the requests and have a solution that can help debug stuff.,0.0,0.319,0.681,0.7579
jaeger,"Basically, I know the implementation using Spring boot which is pretty straightforward.",0.0,0.242,0.758,0.4939
jaeger,but I'm wondering how it could be in dropwizard based application?,0.0,0.0,1.0,0.0
jaeger,"actually, Is this is possible?",0.0,0.0,1.0,0.0
jaeger,Can someone share his experience with this topic?,0.0,0.239,0.761,0.296
jaeger,And provide me with resources or examples of how I can do that?,0.0,0.0,1.0,0.0
jaeger,Please make sure that I'm not using a service mesh.,0.0,0.397,0.603,0.5574
jaeger,I'm not able to get any tracing on Jaeger.,0.0,0.0,1.0,0.0
jaeger,I did this configuration:,0.0,0.0,1.0,0.0
jaeger,Should I keep the double quotes in the hostname and port ?,0.0,0.0,1.0,0.0
jaeger,What is the correct port to use ?,0.0,0.0,1.0,0.0
jaeger,"I am using Spring Boot as Microservice, I am using Jaeger  for Monitor and troubleshoot transactions in complex distributed systems.",0.0,0.096,0.904,0.2023
jaeger,I could see all microservices Span with respect to given trace id/ call.,0.0,0.22,0.78,0.4767
jaeger,"But When I am calling OSB layer service from Spring Boot Service, I could not OSB layer as a SPAN.",0.0,0.0,1.0,0.0
jaeger,what could will be possible solution for this.,0.0,0.247,0.753,0.3182
jaeger,Thank You,0.0,0.714,0.286,0.3612
jaeger,I am trying to trace logs of my spring-boot-application with jaeger .,0.0,0.0,1.0,0.0
jaeger,Both spring-boot microservice and jaeger are running on kubernetes ( local set-up on docker-desktop ) .,0.0,0.0,1.0,0.0
jaeger,My services traces are not visible in jaeger UI .,0.0,0.0,1.0,0.0
jaeger,The same spring boot microservice and Jaeger local set up (without kuberntes ) is working fine .,0.0,0.114,0.886,0.2023
jaeger,Below is the configuration in my application.properties to interact with jaeger-agent in kubernetes .,0.0,0.0,1.0,0.0
jaeger,Below is my code :,0.0,0.0,1.0,0.0
jaeger,https://github.com/anuragk3334/Spring-boot-and-Jaeger/tree/master/HelloWorld,0.0,0.0,1.0,0.0
jaeger,Jaeger configuration is :,0.0,0.0,1.0,0.0
jaeger,https://github.com/anuragk3334/Spring-boot-and-Jaeger/blob/master/HelloWorld/k8s/jaeger.yaml,0.0,0.0,1.0,0.0
jaeger,I created an application with  spring-cloud-bus  (for the auto-refresh from spring-cloud-config-server) and  opentracing-jaeger .,0.0,0.154,0.846,0.25
jaeger,"Without  spring-cloud-bus , jaeger shows the application logs in traces.",0.0,0.0,1.0,0.0
jaeger,"But with  spring-cloud-bus , the logs are missing.",0.318,0.0,0.682,-0.4215
jaeger,"On debugging, the following details were found.",0.0,0.0,1.0,0.0
jaeger,Please find a  sample application  with pre-configured settings here.,0.0,0.247,0.753,0.3182
jaeger,"Can someone guide me, how to work around this issue?",0.0,0.0,1.0,0.0
jaeger,Most of the integrations I have come across uses the java-agent to push the traces to a central collector and in turn one can view traces in Jaeger.,0.0,0.0,1.0,0.0
jaeger,"However in my case I can't use the java agent, hence I decided to go with the custom tracing api which seems fine and there are many examples for this.",0.0,0.062,0.938,0.2023
jaeger,"By design my low latency application limits me from making any connections to external components/ports hence I am also trying to avoid pushing the traces/spans to the local Jaeger agent or Collector endpoint, rather have the traces logged via the LogReporter.",0.102,0.0,0.898,-0.5106
jaeger,Beyond this I am wondering how to build a pipeline for pushing the trace logs in to Jaeger.,0.0,0.0,1.0,0.0
jaeger,The logs themselves are in AWS cloudwatch as streams so I am thinking if I use a Serveless Lambda to subscribe and parse these trace log events then I could ship them myself to Jaeger using may be the HTTP /api/traces endpoint (not much details but read somewhere that this exists in some form).,0.0,0.0,1.0,0.0
jaeger,At this point my question is if this is the right way or there is a better mechanism to achieve this.,0.0,0.132,0.868,0.4404
jaeger,As I have no idea if the traces themselves can be replayed in this fashion to the Collector.,0.121,0.0,0.879,-0.296
jaeger,Also not sure what format the endpoint accepts as I don't see much documentation or example around this.,0.102,0.119,0.779,0.0869
jaeger,The objective is for my application to &quot;not&quot; connect to any external monitoring infrastructure via push events so if there is any better way for Jaeger integration I would love to hear.,0.0,0.197,0.803,0.7964
jaeger,"Also I am okay if any other API in the form of OpenTracing, OpenCensus or even the latest OpenTelemetry can help with this.",0.0,0.187,0.813,0.5574
jaeger,Thanks,0.0,1.0,0.0,0.4404
jaeger,I'm trying out OpenTracing Jaeger and have the following file  test.cpp :,0.0,0.0,1.0,0.0
jaeger,And consider  config.yml  to be:,0.0,0.0,1.0,0.0
jaeger,"Now if I compile  test.cpp  with  g++ test.cpp -lopentracing -ljaegertracing -lyaml-cpp  and run  ./a.out config.yml , I get a",0.0,0.0,1.0,0.0
jaeger,"While if I compile with  g++ test.cpp -L /usr/local/lib -lopentracing -ljaegertracing -lyaml-cpp , I get a good",0.0,0.195,0.805,0.4404
jaeger,The contents of my  /usr/local/lib  are:,0.0,0.0,1.0,0.0
jaeger,"I'm using Jaeger in conjunction with a larger project involving ROS and get the same error even if I  add_compile_options(-L /usr/local/lib)  for  CMakeLists.txt  of the appropriate package; so, I wanted to better understand what the exact cause of the above error is, so hopefully that helps me to fix the one involved in ROS.",0.089,0.149,0.761,0.5604
jaeger,Thanks!,0.0,1.0,0.0,0.4926
jaeger,My query from the Jaeger ElasticSearc returns the following entry,0.0,0.0,1.0,0.0
jaeger,My goal is to find entries which have tag[&quot;internal.span&quot;] &quot;zipkin&quot;.,0.0,0.0,1.0,0.0
jaeger,Field &quot;tags&quot; is &quot;nested&quot;.,0.0,0.0,1.0,0.0
jaeger,I am trying a query like the one below.,0.0,0.294,0.706,0.3612
jaeger,I do not get hits.,0.0,0.0,1.0,0.0
jaeger,What do I miss?,0.444,0.0,0.556,-0.1531
jaeger,"In  jaeger-client-cpp  when I connect my  Tracer  variable to jaeger backend (I m using  jaeger-all-in-one  server) then upon successful connection  LOG INFO  message is shown telling me the connection is successful, but when connection is unsuccessful is just shows  LOG ERROR  message telling me that connection with server not successful.",0.13,0.164,0.706,0.2733
jaeger,So is there any way to check this programatically about the status of connection of  Tracer  with server.,0.0,0.0,1.0,0.0
jaeger,"OS-ubuntu 18.04 
 jaeger-client-cpp-v0.5.0",0.0,0.0,1.0,0.0
jaeger,The  JaegerGrpcSpanExporter  of the Java OpenTelemetry API implements the export method with a grpc blocking stub ( CollectorServiceGrpc.CollectorServiceBlockingStub ).,0.14,0.0,0.86,-0.3818
jaeger,"In case of high latence or slowness on the collector side, isn't it dangerous to block the thread?",0.135,0.119,0.746,-0.089
jaeger,How to add method name to the log section in Jaeger.,0.0,0.0,1.0,0.0
jaeger,I just included opentracing-spring-jaeger-cloud-starter in my spring boot application and it amazingly contextualized logs.,0.0,0.0,1.0,0.0
jaeger,"I can also see method name, but only in tags.",0.0,0.0,1.0,0.0
jaeger,"The problem is that when there are multiple methods invoked and if the logging is not explicitly referring method name, it is difficult (a lil bit) to trace it.",0.161,0.0,0.839,-0.6369
jaeger,"Currently level, logger, message and thread are present in the log section.",0.0,0.0,1.0,0.0
jaeger,I want to add method name as well.,0.0,0.405,0.595,0.34
jaeger,Does anyone know how to do it?,0.0,0.0,1.0,0.0
jaeger,"In case you want to see a sample application, this is one  https://github.com/winster/spring-tracer",0.0,0.106,0.894,0.0772
jaeger,"Note that, I have already set a console pattern with method name.",0.0,0.0,1.0,0.0
jaeger,But the log fields always remains the same.,0.0,0.0,1.0,0.0
jaeger,We are using Multi cluster single plane Istio on GKE.,0.0,0.0,1.0,0.0
jaeger,We are trying to get traces of all the remote cluster services into the main cluster tracing pod.,0.0,0.0,1.0,0.0
jaeger,We have exposed the tracing pod via ILB in main cluster.,0.115,0.0,0.885,-0.0772
jaeger,And created a headless zipkin service on remote cluster with ILB IP as endpoint.,0.0,0.143,0.857,0.25
jaeger,As of now it's not working.,0.0,0.0,1.0,0.0
jaeger,We have tried setting up &quot;remoteTelemetryAddress&quot; flag as well on remote clusters but still no result.,0.153,0.084,0.763,-0.3071
jaeger,"Istio Version: 1.5.5
GKE version: 1.16.8-gke.15
Network: Single VPC",0.0,0.0,1.0,0.0
jaeger,I have a node.js application using RPC messages over thrift API to communicate with another server application.,0.0,0.0,1.0,0.0
jaeger,"I want to integrate distributed tracing, such as Jaeger or Zipkin, which traces the requests through these applications.",0.0,0.075,0.925,0.0772
jaeger,"The problem is that every approach and example I found to this topic is given with HTTP requests, which I don't use.",0.124,0.0,0.876,-0.4019
jaeger,The span context is added to the HTTP header with an inject function of the tracer.,0.0,0.0,1.0,0.0
jaeger,"On server-side, the extract function can be used to get these information, which was added to the header before.",0.0,0.0,1.0,0.0
jaeger,"Generally, this basic approach for this inter-process context propagation using RPC messages over thrift API should be the same, but I simply don't know how to start, because I cannot find any examples, recommended ways or best practises.",0.0,0.19,0.81,0.8402
jaeger,"The only advice that this should be possible, I found on Zipkin's website (sub-item Thrift Tracing):
 https://zipkin.io/pages/instrumenting.html",0.0,0.0,1.0,0.0
jaeger,I'm a newbie on Jaeger and would like to know if I could trace an end-to-end transaction with a parent span &amp; child ones like the one described below with polling from child components (no direct invocation from parent to child) and callback from childs to the parent component.,0.0,0.102,0.898,0.6124
jaeger,First let's describe a simplified view of what I'd like to do.,0.0,0.2,0.8,0.3612
jaeger,A solution made of several components exposes a REST API to submit transactions.,0.117,0.18,0.703,0.2023
jaeger,It synchronously returns a transaction Id after invocation and will callback the invoker upon completion or failure of the transaction.,0.155,0.0,0.845,-0.5106
jaeger,So what I want to trace is the overall transaction from 1 to 3:,0.0,0.125,0.875,0.1477
jaeger,Under the hood:,0.0,0.0,1.0,0.0
jaeger,"I'm assuming that it will be possible to trace the entire transaction with Jaeger, but here are my questions:
Question:",0.0,0.0,1.0,0.0
jaeger,Any hints &amp; tips will be welcome.,0.0,0.333,0.667,0.4588
jaeger,Thx.,0.0,1.0,0.0,0.3612
jaeger,Using the helm chart for Jaeger I see that it makes use of the cassandra subchart.,0.0,0.0,1.0,0.0
jaeger,Looking at the documentation and config files it looks like by setting the provisionDataStore.cassandra override to false that the cassandra subchart shouldn't be getting installed.,0.0,0.094,0.906,0.3612
jaeger,"However, when the override is set I can still see the cassandra service being installed on my cluster.",0.0,0.0,1.0,0.0
jaeger,Anybody know why and how I can prevent cassandra service from being deployed to my cluster?,0.0,0.073,0.927,0.0258
jaeger,I was expecting that when I set the provisionDataStore.cassandra=false that I shouldn't see any cassandra services being deployed to my cluster.,0.0,0.0,1.0,0.0
jaeger,"This is what the requirements.yaml file looks like for the Jaeger helm chart:
dependencies:
  - name: cassandra
    version: ^0.13.1
    repository:  https://kubernetes-charts-incubator.storage.googleapis.com/ 
    condition: provisionDataStore.cassandra
  - name: elasticsearch
    version: ^7.5.1
    repository:  https://helm.elastic.co 
    condition: provisionDataStore.elasticsearch",0.0,0.079,0.921,0.3612
jaeger,I am using opentracing-spring-jaeger-web-starter in my spring boot project.,0.0,0.0,1.0,0.0
jaeger,It create auto spans for all rest call and do tag with standard tags.,0.0,0.139,0.861,0.2732
jaeger,How can i add custom tags for rest call?,0.0,0.0,1.0,0.0
jaeger,I have created a Helidon Microprofile quickstart project from helidon.io get started while configuring with Jaeger I am unable to find the Trace in Jaeger UI below are the steps which I have followed:,0.0,0.065,0.935,0.25
jaeger,Created project using,0.0,0.5,0.5,0.25
jaeger,Updated  pom.xml  with Jaeger dependencies,0.0,0.0,1.0,0.0
jaeger,Updated GreetApplication,0.0,0.0,1.0,0.0
jaeger,Updated /helidon-quickstart-mp/src/main/resources/META-INF/microprofile-config.properties,0.0,0.0,1.0,0.0
jaeger,Executed mvn package and then  target&gt;java -jar helidon-quickstart-mp.jar,0.0,0.0,1.0,0.0
jaeger,Now in my Jaeger UI I am unable to trace the running Service:,0.0,0.0,1.0,0.0
jaeger,So how can I configure Jaeger UI to my helidon Microprofile project?,0.0,0.0,1.0,0.0
jaeger,I'm trying to play around with Jaeger and open-tracing in my local k8s node (Docker for Mac) and having some trouble see traces in the UI.,0.093,0.082,0.825,-0.0772
jaeger,I'm using the Jaeger operator and deployment annotations to inject the jaeger sidecar.,0.0,0.0,1.0,0.0
jaeger,The Jaeger cr is configured to sample constantly every request.,0.0,0.0,1.0,0.0
jaeger,"Up until this point, everything seems to be fine but when I send some HTTP traffic to my pods (Through nginx-ingress) I can see it coming but can't find any traces in Jaeger UI.",0.0,0.043,0.957,0.1027
jaeger,"From reading the documentation, these steps should've implicitly collect and send the traces.",0.0,0.0,1.0,0.0
jaeger,Am I missing something?,0.524,0.0,0.476,-0.296
jaeger,I am trying to setup jaeger-all-in-one on one server.,0.0,0.0,1.0,0.0
jaeger,"If I run the exe jaeger-all-in-one, everything works as expected (using in-memory).",0.0,0.0,1.0,0.0
jaeger,"In order to see the options available with ES, I am not able to run a help command.",0.0,0.153,0.847,0.4019
jaeger,"Now, my requirement is to specify an elastic search URL.",0.0,0.0,1.0,0.0
jaeger,"I have set up the environment variables  SPAN_STORAGE_TYPES  and  ES_SERVER_URLS , but couldn't find how to run jaeger-all-in-one.exe by asking it to take in these environment variables.",0.0,0.0,1.0,0.0
jaeger,I have built a sample app to understand the trace and span using OpenTelemetry.,0.0,0.0,1.0,0.0
jaeger,I want to see them in Jaeger UI.,0.0,0.178,0.822,0.0772
jaeger,How to set up Jaeger with my application which uses OpenTelemetry for tracing?,0.0,0.0,1.0,0.0
jaeger,I have gone through Jaeger Documentation.,0.0,0.0,1.0,0.0
jaeger,They have specified that how will Jaeger will work the HTTP request kind of scenario but if I want to get traces of Nservicebus's to publish/subscribe method then How will I get using Jaeger?,0.0,0.045,0.955,0.1154
jaeger,Is it possible?,0.0,0.0,1.0,0.0
jaeger,Or Jaeger only works with HTTP requests?,0.0,0.0,1.0,0.0
jaeger,Was trying to connect to jaeger using HTTP request using nodejs but the spans are not reaching the jaeger endpoint.,0.09,0.0,0.91,-0.2235
jaeger,"please help with this code snippet.,",0.0,0.556,0.444,0.6124
jaeger,Any help would be much appreciated!,0.0,0.611,0.389,0.7424
jaeger,I'm playing with JaegerTracing in Django using tutorial  https://github.com/contino/jaeger-django-docker-tutorial .,0.0,0.184,0.816,0.2023
jaeger,Now I don't know how to take out traceId from response headers because it's not there.,0.0,0.0,1.0,0.0
jaeger,When finding traces in Jaeger UI it returns response with data (see also screenshot below):,0.0,0.0,1.0,0.0
jaeger,I suspected it in response headers but it is not.,0.153,0.0,0.847,-0.1154
jaeger,How can I do this?,0.0,0.0,1.0,0.0
jaeger,Based on  this  and  this,0.0,0.0,1.0,0.0
jaeger,How would I enable tracing for  reactive-sql-clients  ?,0.0,0.0,1.0,0.0
jaeger,"Now use  %dev.quarkus.datasource.url=vertx-reactive:postgresql://dev-db-server:5432/mydb  - it works, but no tracing support though.",0.195,0.247,0.557,0.1901
jaeger,I can see racing for my rest calls but not the db.,0.0,0.0,1.0,0.0
jaeger,Tried to use  %dev.quarkus.datasource.url=vertx-reactive:tracing:postgresql://dev-db-server:5432/mydb,0.0,0.0,1.0,0.0
jaeger,my deps:,0.0,0.0,1.0,0.0
jaeger,I am planning to use  Jaeger  tracing in on my  Golang  server.,0.0,0.0,1.0,0.0
jaeger,Everything is ok but I haven't found a way to handle  Jaeger  errors.,0.226,0.117,0.657,-0.3612
jaeger,"I want to catch, for example, connection error to  Jaeger  backend while sending trace and write it to  loggly .",0.135,0.065,0.8,-0.34
jaeger,Code example:,0.0,0.0,1.0,0.0
jaeger,I am trying to deploy Istio Jaeger UI for distributed tracing.,0.0,0.0,1.0,0.0
jaeger,Currently I am using kubectl port forwarding using the command  kubectl port-forward -n monitoring prometheus-prometheus-operator-prometheus-0 9090 .,0.0,0.0,1.0,0.0
jaeger,But it runs on  http://localhost:port  So how can I do it in production?,0.0,0.0,1.0,0.0
jaeger,Is there any other way to deploy in production.,0.0,0.0,1.0,0.0
jaeger,And also how can I make it run on  https ?,0.0,0.0,1.0,0.0
jaeger,We are able to get latency metrics of multiple microservices using Jaeger.,0.0,0.0,1.0,0.0
jaeger,Currently Jaeger stores application metrics in elasticsearch.,0.0,0.0,1.0,0.0
jaeger,My usecase is to get the latency of application from elasticsearch to prometheus.,0.0,0.0,1.0,0.0
jaeger,Is there anyway to read the elasticsearch metrics of Jaeger?,0.0,0.0,1.0,0.0
jaeger,I already used elasticsearch-prometheus-exporter which only exports cluster details of ES.,0.0,0.0,1.0,0.0
jaeger,I am analysing at a very hight level how much effort would it be for jaeger integration in nodejs microservices.,0.0,0.0,1.0,0.0
jaeger,Does it require code changes or only deployment.,0.0,0.0,1.0,0.0
jaeger,"and if code changes is required, is code changes needed in first service (i.e.",0.0,0.0,1.0,0.0
jaeger,api-gateway) or all the services need to have code changes.,0.0,0.0,1.0,0.0
jaeger,I would really appreciate if someone can give a rough idea of tasks and effort.,0.0,0.2,0.8,0.4576
jaeger,I have created a microservice based architecture using Spring Boot and deployed the application on Kubernetes/Istio platform.,0.0,0.125,0.875,0.25
jaeger,The different microservices communicate with each other using either JMS (ActiveMQ) or REST API.,0.0,0.0,1.0,0.0
jaeger,I am getting the tracing of REST communication on Istio's Jaeger but the JMS based communication is missing in Jaeger.,0.135,0.0,0.865,-0.4215
jaeger,I am using ElasticSearch to store my application logs.,0.0,0.0,1.0,0.0
jaeger,Is it possible to use the same ElasticSearch as a backend(DB) of Jaeger?,0.0,0.0,1.0,0.0
jaeger,If yes then I will store tracing specific logs in ElasticSearch and query them  on Jaeger UI.,0.0,0.153,0.847,0.4019
jaeger,I'm setting up a proof of concept featuring two ASP.NET Core applications that are both instrumented with  Jaeger  to demonstrate how it can propagate a trace between services over the wire.,0.0,0.0,1.0,0.0
jaeger,Both applications are being deployed to Azure App Services.,0.0,0.0,1.0,0.0
jaeger,I'm using the  OpenTracing Contrib  package to automatically inject the Jaeger trace context into my inter-service traffic in the form of HTTP Headers (the package is hardcoded to use that form of transmission).,0.0,0.0,1.0,0.0
jaeger,"But it appears that those headers are going missing along the way, as the receiving application is unable to resume the tracing context.",0.113,0.0,0.887,-0.4215
jaeger,"Before deploying to Azure, I'm testing the applications locally with Docker Compose, and  with that setup the context propagation works fine .",0.0,0.083,0.917,0.2023
jaeger,It's only once the apps are in Azure that things break.,0.0,0.0,1.0,0.0
jaeger,"The applications communicate over HTTPS and I've disabled HSTS and HTTPS redirection in case that might be causing Azure to drop the headers, based on the answer in  this previous thread .",0.065,0.0,0.935,-0.2732
jaeger,"I've also tried running both applications in Azure Container Instances, and that seems to be a non-starter - it doesn't fix the context propagation and seems to introduce more bugs around span relationships.",0.0,0.0,1.0,0.0
jaeger,"The two applications are nearly identical in their setup, and differ only in the API endpoints they serve.",0.0,0.0,1.0,0.0
jaeger,My CreateWebHostBuild from program.cs:,0.0,0.0,1.0,0.0
jaeger,The contents of the AddJaeger extension method which is largely borrowed from  the Contrib sample :,0.0,0.0,1.0,0.0
jaeger,My startup.cs configure method to show I'm not doing anything weird with the headers (the metrics extensions are for prometheus-net),0.0,0.074,0.926,0.1326
jaeger,I expect any calls from one application to the other to propagate the active Jaeger trace context.,0.0,0.153,0.847,0.4019
jaeger,"Instead, the two applications log their traces separately and no link can be discerned between them in the Jaeger UI.",0.104,0.0,0.896,-0.296
jaeger,"Here's a screenshot of a trace that should have spanned both services, but instead only shows spans from the first service:",0.0,0.0,1.0,0.0
jaeger,"I have three services A, B, and C that communicate like so",0.0,0.217,0.783,0.3612
jaeger,I'm using OpenTracing and Jaeger for distributed tracing.,0.0,0.0,1.0,0.0
jaeger,"The problem is these services are in different languages, but I'm still trying to propagate the information that A is the parent span so that the span tree looks like this.",0.056,0.098,0.846,0.34
jaeger,"Right now, A, B, and C are being reported as individual traces with no causality relations.",0.136,0.0,0.864,-0.296
jaeger,All the examples I've seen involved propagating causality between different microservices in the same language and in the same project build.,0.0,0.0,1.0,0.0
jaeger,None involved entirely separate services.,0.0,0.0,1.0,0.0
jaeger,"Unfortunately, I'm not able to use PyInstaller with  jaeger .",0.231,0.0,0.769,-0.34
jaeger,The problem is some sort of a thrift error between PyInstaller and  jaeger .,0.363,0.0,0.637,-0.6901
jaeger,Like discussed  here .,0.0,0.556,0.444,0.3612
jaeger,Are they any workarounds or fixes?,0.0,0.0,1.0,0.0
jaeger,I have tried it with python 3.6 and the newest jaeger-client.,0.0,0.0,1.0,0.0
jaeger,There I get an Errno 2 -  Even I don't even use a Config file,0.0,0.0,1.0,0.0
jaeger,The script runs as expected -  Spans are created and web server starts.,0.0,0.154,0.846,0.25
jaeger,"Only in the executable, it does not run.",0.0,0.0,1.0,0.0
jaeger,And shows the following error:,0.403,0.0,0.597,-0.4019
jaeger,The  Istio (version 1.0.6)  official document says:,0.0,0.0,1.0,0.0
jaeger,We can access the Jaeger UI by the following action:,0.0,0.0,1.0,0.0
jaeger,Kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath=’{.items[0].metadata.name}’) 16686:16686 &amp;,0.0,0.0,1.0,0.0
jaeger,Then we can use  http://localhost:16686 .,0.0,0.0,1.0,0.0
jaeger,"But the localhost is a Linux machine, it doesn't have a browser.",0.0,0.0,1.0,0.0
jaeger,I must open the browser on a remote machine.,0.0,0.0,1.0,0.0
jaeger,How can I do this?,0.0,0.0,1.0,0.0
jaeger,Thanks.,0.0,1.0,0.0,0.4404
jaeger,I've set up Jaeger with Opentracing in a Java environment and it works nicely with logging messages with spans and tracing.,0.0,0.132,0.868,0.4404
jaeger,But I am a bit stuck when it comes to catching and logging exceptions.,0.185,0.0,0.815,-0.3612
jaeger,But this way does not format error logging in a good readable way.,0.0,0.403,0.597,0.7742
jaeger,I have looked around for information about this as it feels pretty obvious there should be as this is one of its components for logging.,0.0,0.122,0.878,0.4939
jaeger,But I have somehow never seen anything about this.,0.0,0.0,1.0,0.0
jaeger,It is mostly about building and structuring spans.,0.0,0.0,1.0,0.0
jaeger,Hope anyone can help me with this when it comes to capturing and logging exceptions.,0.0,0.301,0.699,0.6808
jaeger,I have two basic Springboot microservices and I am using Jaeger.,0.0,0.0,1.0,0.0
jaeger,Lets say two services are  foo  and  bar .,0.0,0.0,1.0,0.0
jaeger,I am able to send  User-Agent  header from foo to bar service using Tracing Baggage property.,0.0,0.0,1.0,0.0
jaeger,"From  foo  service, I will be calling  bar  service using  localhost:port  as of now.",0.0,0.0,1.0,0.0
jaeger,The users will also send an  x-api-key  header in the request.,0.0,0.0,1.0,0.0
jaeger,This header is not being forward from  foo  to  bar  service.,0.0,0.0,1.0,0.0
jaeger,"This is my code snippet,",0.0,0.0,1.0,0.0
jaeger,"On the logs of my  bar  service, it is receiving these headers,
 uberctx-user-agent  and  uberctx-x-api-key",0.0,0.0,1.0,0.0
jaeger,"I am not sure why  uber-ctx-*  is appended, I only want  x-api-key  header to be forwarded.",0.129,0.085,0.786,-0.1685
jaeger,I run  ./jaeger-all-in-one --es.tags-as-fields.all=true --es.index-prefix=myteam.jaeger --es.server-urls=http://ip-server:9200,0.0,0.0,1.0,0.0
jaeger,"How add the variables  --es.tags-as-fields.all ,  --es.index-prefix  and  --es.server-urls  to the YAML config ?",0.0,0.0,1.0,0.0
jaeger,Thanks!,0.0,1.0,0.0,0.4926
jaeger,I'm new to using Jaeger tracing system and have been trying to implement it for a flask based microservices architecture.,0.0,0.0,1.0,0.0
jaeger,Below is my jaeger client config implemented in python:,0.0,0.0,1.0,0.0
jaeger,I read somewhere that Sampling strategy is being used to sample the number of traces especially for the trace which doesn't have any metadata.,0.0,0.056,0.944,0.0772
jaeger,"So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?",0.0,0.0,1.0,0.0
jaeger,"Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes.",0.0,0.095,0.905,0.2732
jaeger,I would like to understand this configuration spec more but not able to.,0.0,0.137,0.863,0.1901
jaeger,"Describe the bug 
We have a container running with envoy sidecar proxy with service/deployment for port 443 using Istio's own example: sample/https/nginx.",0.0,0.0,1.0,0.0
jaeger,We can curl the container to get nginx page just fine but see absolutely no traces in Jaeger for https calls.,0.137,0.059,0.804,-0.429
jaeger,We see HTTP calls in Jaeger as soon as we switch the port to 80 in deployment/service,0.0,0.0,1.0,0.0
jaeger,"Expected behavior 
We should see traces for both HTTP/HTTPS calls to the container.",0.0,0.0,1.0,0.0
jaeger,Steps to reproduce the bug:,0.0,0.0,1.0,0.0
jaeger,create nginx config:,0.0,0.512,0.488,0.2732
jaeger,Create nginx deployment :,0.0,0.512,0.488,0.2732
jaeger,curl -kv https://service-ip  gives 200,0.0,0.0,1.0,0.0
jaeger,Version,0.0,0.0,1.0,0.0
jaeger,Installation,0.0,0.0,1.0,0.0
jaeger,"Environment 
- Running this within AWS EKS",0.0,0.0,1.0,0.0
jaeger,"Cluster state 
- Attached 
 archite.tar.gz",0.0,0.0,1.0,0.0
jaeger,Edit 1,0.0,0.0,1.0,0.0
jaeger,yaml for service -  jaeger-query  :,0.0,0.0,1.0,0.0
jaeger,"
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-10-02T02:32:23Z
  labels:
    app: jaeger
    chart: tracing-1.0.1
    heritage: Tiller
    jaeger-infra: jaeger-service
    release: istio
  name: jaeger-query
  namespace: istio-system
  resourceVersion: ""5259733""
  selfLink: /api/v1/namespaces/istio-system/services/jaeger-query
  uid: 6513eded-c5eb-11e8-860c-12504ba0df7c
spec:
  clusterIP: 172.20.14.251
  ports:
  - name: query-http
    port: 16686
    protocol: TCP
    targetPort: 16686
  selector:
    app: jaeger
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}",0.0,0.065,0.935,0.5267
jaeger,Deployment :  istio-tracing  :,0.0,0.0,1.0,0.0
jaeger,"
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""1""
  creationTimestamp: 2018-10-02T02:32:23Z
  generation: 1
  labels:
    app: istio-tracing
    chart: tracing-1.0.1
    heritage: Tiller
    release: istio
  name: istio-tracing
  namespace: istio-system
  resourceVersion: ""5259783""
  selfLink: /apis/extensions/v1beta1/namespaces/istio-system/deployments/istio-tracing
  uid: 65056099-c5eb-11e8-860c-12504ba0df7c
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: jaeger
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: """"
        sidecar.istio.io/inject: ""false""
      creationTimestamp: null
      labels:
        app: jaeger
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - ppc64le
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - s390x
            weight: 2
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
                - ppc64le
                - s390x
      containers:
      - env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: COLLECTOR_ZIPKIN_HTTP_PORT
          value: ""9411""
        - name: MEMORY_MAX_TRACES
          value: ""50000""
        image: docker.io/jaegertracing/all-in-one:1.5
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 16686
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: jaeger
        ports:
        - containerPort: 9411
          protocol: TCP
        - containerPort: 16686
          protocol: TCP
        - containerPort: 5775
          protocol: UDP
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 16686
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 10m
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: 2018-10-02T02:32:23Z
    lastUpdateTime: 2018-10-02T02:32:23Z
    message: Deployment has minimum availability.",0.0,0.09,0.91,0.9517
jaeger,"reason: MinimumReplicasAvailable
    status: ""True""
    type: Available
  - lastTransitionTime: 2018-10-02T02:32:23Z
    lastUpdateTime: 2018-10-02T02:32:27Z
    message: ReplicaSet ""istio-tracing-ff94688bb"" has successfully progressed.",0.0,0.176,0.824,0.4939
jaeger,"reason: NewReplicaSetAvailable
    status: ""True""
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1",0.0,0.0,1.0,0.0
jaeger,does jaeger provide a way of querying the trace data without using the UI provided.,0.0,0.0,1.0,0.0
jaeger,I'm aware that zipkin provides an API to directly access the trace data etc.,0.0,0.0,1.0,0.0
jaeger,Use-case: i'm trying to use the trace data to pull together a custom report for internal purposes.,0.0,0.0,1.0,0.0
jaeger,I could scrape the data from the UI but wondered if there was an easier way.,0.0,0.209,0.791,0.5719
jaeger,I was developing a Spring Boot application in which the loging is done by logback and Jaeger is integrated for instrumentation.,0.0,0.0,1.0,0.0
jaeger,Myservice.java,0.0,0.0,1.0,0.0
jaeger,logback.xml,0.0,0.0,1.0,0.0
jaeger,pom.xml,0.0,0.0,1.0,0.0
jaeger,The Jeager is properly connected to server and its getting the traces.,0.0,0.0,1.0,0.0
jaeger,The problem is with the logback logs.,0.31,0.0,0.69,-0.4019
jaeger,The traceId and spanId are not getting printed in the logs.,0.0,0.0,1.0,0.0
jaeger,But I myself found a solution for that.,0.0,0.371,0.629,0.4497
jaeger,I added Spring Cloud Sleuth with my Spring Boot application.,0.0,0.0,1.0,0.0
jaeger,Now all the trace information was available in the logback log.,0.0,0.0,1.0,0.0
jaeger,But the problem is that Jaeger stopped registering traces to Jaeger server.,0.371,0.0,0.629,-0.7096
jaeger,"I tried Zipkin instead of Jaeger, but the same thing happened.",0.0,0.0,1.0,0.0
jaeger,What's wrong with my application?,0.437,0.0,0.563,-0.4767
jaeger,Is something wrong with the dependencies?,0.383,0.0,0.617,-0.4767
jaeger,I have certain applications that run jaeger-client when I enable OpenTracing and start them.,0.0,0.16,0.84,0.2732
jaeger,"First I start Jaeger collector using the command-
docker run -d -e   COLLECTOR_ZIPKIN_HTTP_PORT=9411   -p 5775:5775/udp   -p 6831:6831/udp   -p 6832:6832/udp   -p 5778:5778   -p 16686:16686   -p 14268:14268   -p 9411:9411   jaegertracing/all-in-one:latest",0.0,0.0,1.0,0.0
jaeger,"Then I start the applications like user-
start.sh user -apiserver=localhost:9900 -configfile=conf/configuration.json -traceroption enabled=true 
following which they become visible as enabled services  http://localhost:16686/api/services",0.0,0.111,0.889,0.3612
jaeger,"The problem is that if I kill the Docker running the jaeger collector- 
systemctl stop docker
and later restart docker and jaegertracing/all-in-one, the services are no longer up at  http://localhost:16686/api/services",0.321,0.0,0.679,-0.8957
jaeger,Does the jaeger client die on its own in absence of a Jaeger collector?,0.245,0.0,0.755,-0.5994
jaeger,Does the Jaeger collector needs to be running before starting the Jaeger clients?,0.0,0.0,1.0,0.0
jaeger,"If so, how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?",0.0,0.0,1.0,0.0
jaeger,I wasn't able to find any clear API in RegisterRoutes method of  https://github.com/jaegertracing/jaeger/blob/master/cmd/query/app/handler.go,0.0,0.191,0.809,0.3818
jaeger,I'm running a Spring-Boot application inside a docker container and want to instrument it with OpenTracing using the Jaeger client from Uber.,0.0,0.064,0.936,0.0772
jaeger,For the instrumentation I'm using the  OpenTracing Spring Web  library in combination with the  Jaeger  client.,0.0,0.0,1.0,0.0
jaeger,The following code snippet configures the tracer in the application:,0.0,0.0,1.0,0.0
jaeger,I can see the traces when I run the application (not inside a Docker container) and start Jaeger with the following command:,0.0,0.0,1.0,0.0
jaeger,But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces.,0.0,0.0,1.0,0.0
jaeger,After that I tried to declare the Jaeger docker container in the same docker-compose file and added a link from the  demo  service to the  jaeger  service:,0.0,0.0,1.0,0.0
jaeger,But I still can't see any traces in the Jaeger client.,0.0,0.0,1.0,0.0
jaeger,"For hours I have tried different approaches but didn't make any progress so far, if somebody could help me out I would greatly appreciate it!",0.097,0.254,0.649,0.7035
jaeger,You can find my demo project on  GitHub .,0.0,0.0,1.0,0.0
jaeger,I am hosting my application on GCP and I want to use stackdriver as my backend storage for trace spans with jaeger collectors.,0.0,0.061,0.939,0.0772
jaeger,I can't seem to find anything related to that.,0.0,0.0,1.0,0.0
jaeger,In GCP I can find clearly that they support zipkin.,0.0,0.435,0.565,0.6597
jaeger,I am not sure what to do here.,0.246,0.0,0.754,-0.2411
jaeger,Should I create some translation layer to push the data to stackdriver ?,0.0,0.174,0.826,0.2732
jaeger,Or is it supported somehow by the current zipkin connector ?,0.0,0.204,0.796,0.3182
jaeger,I truly wouldn't want to host my full tracing solution to avoid having to maintain it.,0.174,0.265,0.561,0.4172
jaeger,Can I run the Jaeger collector and somehow pass it to stackdriver ?,0.0,0.0,1.0,0.0
jaeger,Thanks,0.0,1.0,0.0,0.4404
jaeger,From react application (App.js ) imported jaeger-client.,0.0,0.0,1.0,0.0
jaeger,import jaegerClient from 'jaeger-client',0.0,0.0,1.0,0.0
jaeger,"Got exception 'TypeError: _fs2.default.readFileSync is not a function' from following line of /node_modules/jaeger-client/dist/src/thrift.js:168
 
source: _fs2.default.readFileSync(_path2.default.join(__dirname, './jaeger-idl/thrift/jaeger.thrift'), 'ascii')",0.0,0.0,1.0,0.0
jaeger,Trying to solve it.,0.0,0.375,0.625,0.2023
jaeger,Thanks for any help.,0.0,0.737,0.263,0.6808
jaeger,"Complete package.json is like below
 
{
  ""name"": ""calculator"",
  ""version"": ""0.1.0"",
  ""private"": true,
  ""homepage"": ""http://ahfarmer.github.io/calculator"",
  ""devDependencies"": {
    ""gh-pages"": ""^1.1.0"",
    ""react-scripts"": ""^1.0.17""
  },
  ""dependencies"": {
    ""ajv"": ""^6.4.0"",
    ""ajv-keywords"": ""^3.1.0"",
    ""big.js"": ""^5.0.3"",
    ""bufferutil"": ""^3.0.3"",
    ""fs"": ""0.0.1-security"",
    ""github-fork-ribbon-css"": ""^0.2.1"",
    ""hexer"": ""^1.5.0"",
    ""jaeger-client"": ""^3.10.0"",
    ""react"": ""^16.2.0"",
    ""react-dom"": ""^16.2.0"",
    ""react-tracing"": ""^0.1.5"",
    ""thrift"": ""^0.11.0""
  },
  ""scripts"": {
    ""start"": ""react-scripts start"",
    ""build"": ""react-scripts build"",
    ""test"": ""react-scripts test --env=jsdom"",
    ""eject"": ""react-scripts eject"",
     ""deploy"": ""gh-pages -d build""
    },
    ""eslintConfig"": {
        ""extends"": ""./node_modules/react-scripts/config/eslint.js""
    }
}",0.0,0.075,0.925,0.6486
jaeger,Forked from  https://github.com/ahfarmer/calculator  and I am trying to trace every user action ( button press ).,0.0,0.0,1.0,0.0
jaeger,To test tracing from react.js application.,0.0,0.0,1.0,0.0
jaeger,I'm currently looking into different openTracing Tracer-Implementations.,0.0,0.0,1.0,0.0
jaeger,I want to use  uber/jaeger-client-node  but the backend won't receive my traces.,0.0,0.103,0.897,0.0387
jaeger,"Here is what I did:
I started the all-in-one docker image:
 docker run -d -p5775:5775/udp -p16686:16686 jaegertracing/all-in-one:latest",0.0,0.0,1.0,0.0
jaeger,"Next, i wrote a simple example application:
 Gist",0.0,0.0,1.0,0.0
jaeger,"But when I go to Jaeger UI, nothing is shown about the example service.",0.0,0.0,1.0,0.0
jaeger,What did I do wrong?,0.508,0.0,0.492,-0.4767
jaeger,Thanks,0.0,1.0,0.0,0.4404
jaeger,I have not found a way to implement a Jaeger Open-Tracing framework implementation on the IBM Websphere Server platform.,0.0,0.0,1.0,0.0
jaeger,All the examples I've seen point to environment variables to be set to specify  where  to communicate to a Jaeger collection endpoint.,0.0,0.0,1.0,0.0
jaeger,I wanted to ask the community if anyone had experience with this.,0.0,0.0,1.0,0.0
jaeger,I am trying to setup Jaeger using a CentOS base image instead of Alpine.,0.0,0.0,1.0,0.0
jaeger,"The agent, collector, and Cassandra containers all work fine except for the query container.",0.0,0.122,0.878,0.2023
jaeger,The Jaeger repository is  here .,0.0,0.0,1.0,0.0
jaeger,"After changing the base image to CentOS 7, commenting out the sections that apply to copying  ca-certificates.crt  and running  docker-compose , I get the following nil pointer error message when tailing the query container",0.08,0.0,0.92,-0.4019
jaeger,panic: runtime error: invalid memory address or nil pointer dereference,0.429,0.0,0.571,-0.7184
jaeger,I ran the makefile with the necessary flags to compile the code in the app directory.,0.0,0.0,1.0,0.0
jaeger,Has anyone ever setup Jaeger using CentOS as a base image?,0.0,0.0,1.0,0.0
jaeger,Below is the full stack error from the container,0.252,0.0,0.748,-0.4019
jaeger,I'm trying to use  OpenTracing.Contrib.NetCore  with Serilog.,0.0,0.0,1.0,0.0
jaeger,I need to send to Jaeger my custom logs.,0.0,0.0,1.0,0.0
jaeger,"Now, it works only when I use default logger factory  Microsoft.Extensions.Logging.ILoggerFactory",0.0,0.0,1.0,0.0
jaeger,My Startup:,0.0,0.0,1.0,0.0
jaeger,and somewhere in controller:,0.0,0.0,1.0,0.0
jaeger,in a result I will able to see that log in Jaeger UI,0.0,0.0,1.0,0.0
jaeger,"But when I use Serilog, there are no any custom logs.",0.237,0.0,0.763,-0.4215
jaeger,"I've added  UseSerilog()  to  WebHostBuilder , and all custom logs I can see in console but not in Jaeger.",0.0,0.0,1.0,0.0
jaeger,There is open issue in  github .,0.0,0.0,1.0,0.0
jaeger,Could you please suggest how I can use Serilog with OpenTracing?,0.0,0.204,0.796,0.3182
jaeger,"I have a Django web app served from Apache2 with mod_wsgi in docker containers running on a Kubernetes cluster in Google Cloud Platform, protected by Identity-Aware Proxy.",0.0,0.112,0.888,0.4404
jaeger,"Everything is working great, but I want to send GCP Stackdriver traces for all requests without writing one for each view in my project.",0.0,0.16,0.84,0.4588
jaeger,"I found middleware to handle this, using Opencensus.",0.0,0.0,1.0,0.0
jaeger,"I went through  this documentation , and was able to manually generate traces that exported to Stackdriver Trace in my project by specifying the  StackdriverExporter  and passing the  project_id  parameter as the Google Cloud Platform  Project Number  for my project.",0.0,0.034,0.966,0.0772
jaeger,"Now to make this automatic for ALL requests, I followed the instructions to set up the middleware.",0.0,0.0,1.0,0.0
jaeger,"In settings.py, I added the module to  INSTALLED_APPS ,  MIDDLEWARE , and set up the  OPENCENSUS_TRACE  dictionary of options.",0.0,0.0,1.0,0.0
jaeger,I also added the  OPENCENSUS_TRACE_PARAMS .,0.0,0.0,1.0,0.0
jaeger,"This works great with the default exporter 'opencensus.trace.exporters.print_exporter.PrintExporter', as I can see the Trace and Span information, including Trace ID and all details in my Apache2 web server logs.",0.0,0.132,0.868,0.6249
jaeger,"However, I want to send these to my Stackdriver Trace processor for analysis.",0.0,0.106,0.894,0.0772
jaeger,"I tried setting the  EXPORTER  parameter to  opencensus.trace.exporters.stackdriver_exporter.StackdriverExporter , which works when run manually from the shell, as long as you supply the project number.",0.0,0.056,0.944,0.0772
jaeger,"When it is set up to use  StackdriverExporter , the web page will not respond load, the health check starts to fail, and ultimately the web page comes back with a 502 error, stating I should try again in 30 seconds (I believe the Identity-Aware Proxy is generating this error, once it detects the failed health check), but the server generates no errors, and there are no logs in access or errors for Apache2.",0.24,0.0,0.76,-0.9509
jaeger,"There is another dictionary in settings.py named  OPENCENSUS_TRACE_PARAMS , which I presume is needed to determine which project number the exporter should be using.",0.0,0.058,0.942,0.0772
jaeger,"The example has  GCP_EXPORTER_PROJECT  set as  None , and  SERVICE_NAME  set as  'my_service' .",0.0,0.0,1.0,0.0
jaeger,What options do I need to set to get the exporter to send back to Stackdriver instead of printing to logs?,0.0,0.0,1.0,0.0
jaeger,Do you have any idea about how I can set this up?,0.0,0.0,1.0,0.0
jaeger,settings.py,0.0,0.0,1.0,0.0
jaeger,Here's an example (I prettified the format for readability) of the Apache2 log when it is set to use the  PrintExporter :,0.0,0.0,1.0,0.0
jaeger,"Thanks in advance for any tips, assistance, or troubleshooting advice!",0.0,0.379,0.621,0.5983
jaeger,Edit 2019-02-08 6:56 PM UTC:,0.0,0.0,1.0,0.0
jaeger,I found this in the middleware:,0.0,0.0,1.0,0.0
jaeger,"The exporter is now named  StackdriverExporter , instead of  GoogleCloudExporter .",0.0,0.0,1.0,0.0
jaeger,"I set up a class in my app named  GoogleCloudExporter  that inherits  StackdriverExporter , and updated my settings.py to use  GoogleCloudExporter , but it didn't seem to work, I wonder if there is other code referencing these old naming schemes, possibly for the transport.",0.0,0.0,1.0,0.0
jaeger,I'm searching the source code for clues...,0.0,0.0,1.0,0.0
jaeger,"This at least tells me I can get rid of the ZIPKIN and JAEGER param options, as this is determined on the  EXPORTER  param.",0.0,0.098,0.902,0.34
jaeger,Edit 2019-02-08 11:58 PM UTC:,0.0,0.0,1.0,0.0
jaeger,"I scrapped Apache2 to isolate the problem and just set my docker image to use Django's built in webserver  CMD [""python"", ""/path/to/manage.py"", ""runserver"", ""0.0.0.0:80""]  and it works!",0.166,0.0,0.834,-0.5848
jaeger,"When I go to the site, it writes traces to Stackdriver Trace for each request, the Span name is the module and method being executed.",0.0,0.0,1.0,0.0
jaeger,"Somehow Apache2 is not being allowed to send these, but I can do so from the shell when running as root.",0.0,0.0,1.0,0.0
jaeger,"I'm adding Apache2 and mod-wsgi tags to the question, because I have a funny feeling this has to do with forking child processes in Apache2 and mod-WSGI.",0.0,0.161,0.839,0.5267
jaeger,"Would it be the child process being unable to be created as apache2's child process is sandboxed, or could this be a permissions thing?",0.0,0.083,0.917,0.25
jaeger,"It seems strange, because it is just calling python modules, no external system OS binaries, that I am aware of.",0.19,0.0,0.81,-0.4588
jaeger,Any other ideas would be greatly appreciated!,0.0,0.393,0.607,0.5974
jaeger,"I am building a JSON validator from scratch, but I am quite stuck with the string part.",0.184,0.0,0.816,-0.4478
jaeger,My hope was building a regex which would match the following sequence found on JSON.org:,0.0,0.182,0.818,0.4404
jaeger,My regex so far is:,0.0,0.0,1.0,0.0
jaeger,It does match the criteria with a backslash following by a character and an empty string.,0.122,0.0,0.878,-0.2023
jaeger,But I'm not sure how to use the UNICODE part.,0.213,0.0,0.787,-0.3491
jaeger,"Is there a regex to match any UNICODE character expert "" or \ or control character?",0.0,0.0,1.0,0.0
jaeger,And will it match a newline or horizontal tab?,0.0,0.0,1.0,0.0
jaeger,"The last question is because the regex match the string ""\t"", but not ""    "" (four spaces, but the idea is to be a tab).",0.0,0.0,1.0,0.0
jaeger,"Otherwise I will need to expand the regex with it, which is not a problem, but my guess is the horizontal tab is a UNICODE character.",0.0,0.135,0.865,0.3136
jaeger,"Thanks to Jaeger Kor, I now have the following regex:",0.0,0.266,0.734,0.4404
jaeger,"It appears to be correct, but is there any way to check for control characters or is this unneeded as they appear on the non-printable characters on regular-expressions.info?",0.0,0.0,1.0,0.0
jaeger,The input to validate is always text from a textarea.,0.0,0.238,0.762,0.3612
jaeger,Update: the regex is as following in case anyone needs it:,0.0,0.0,1.0,0.0
jaeger,Is there way to configure opentracing-spring-jaeger-cloud-starter to handle any other header than Uber-Trace-Id?,0.0,0.0,1.0,0.0
jaeger,I have Traefik as an ingress in my kubernetes cluster.,0.0,0.0,1.0,0.0
jaeger,Traefik can be configured to change traceContextHeaderName.,0.0,0.0,1.0,0.0
jaeger,"Default value is ""uber-trace-id"".",0.0,0.444,0.556,0.34
jaeger,"When I change it to some custom, there is no connection (I mean span connection) between services.",0.128,0.0,0.872,-0.296
jaeger,I believe that opentracing works only with Uber-Trace-Id.,0.0,0.0,1.0,0.0
jaeger,Is there way to configure that?,0.0,0.0,1.0,0.0
jaeger,I test this in minikube with Traefik as an ingress.,0.0,0.0,1.0,0.0
jaeger,Then all requests go to spring-cloud-gateway and are propagate to services.,0.0,0.0,1.0,0.0
jaeger,Thanks for help!,0.0,0.855,0.145,0.7088
jaeger,Is there a way to use Spring Cloud Sleuth with OpenTracing?,0.0,0.0,1.0,0.0
jaeger,I want to connect Spring clients with Jaeger,0.0,0.178,0.822,0.0772
jaeger,I got the video to work on Android 4.0.3 (API 15) and up using  Jaeger 25's Html5Video plugin .,0.0,0.0,1.0,0.0
jaeger,"However, With the addition of Android 4.4 and its revised Chromium-based WebView, this plugin no longer suffices,  as mentioned by its awesome developer .",0.081,0.15,0.769,0.4404
jaeger,"Now, I would much rather play these video's without any plugins anyway (which works perfectly on iOS...), so I went back to trying that.",0.0,0.248,0.752,0.765
jaeger,"Is was hoping that this Chromium-based webview would be friendlier with basic HTML5 playback, but instead, I just get the exact same error as before:  MediaPlayer(30579): Error (1,-2147483648) .",0.209,0.115,0.676,-0.6369
jaeger,"I've spent hours trying several different approaches, all to no avail.",0.18,0.0,0.82,-0.296
jaeger,I'll list a few things that I've tried below.,0.0,0.0,1.0,0.0
jaeger,"Please, does anyone have any clues to point me in the right direction?",0.0,0.161,0.839,0.3182
jaeger,I'm out of ideas...,0.0,0.0,1.0,0.0
jaeger,Normal file reference using  file:///,0.0,0.0,1.0,0.0
jaeger,Code:,0.0,0.0,1.0,0.0
jaeger,Results in:,0.0,0.0,1.0,0.0
jaeger,File reference using Phonegap's Filesystem API,0.0,0.0,1.0,0.0
jaeger,Code:,0.0,0.0,1.0,0.0
jaeger,"Results in (contains that same MediaPlayer (1, -2147483648) error):",0.0,0.0,1.0,0.0
jaeger,"In a project I'm working on, I have a textbox where the user has to input his name.",0.0,0.0,1.0,0.0
jaeger,To avoid the user from entering numbers I used the     jquery.limitkeypress.js  library written by  Brian Jaeger  and every thing was working perfectly until I tested the website in Internet Explorer 10.,0.066,0.126,0.808,0.4588
jaeger,"In IE10, you can input all the letters you want, and you can not input number or weird symbols just as I wanted but when I type a space and then a letter, I see the letter print right to the space and then the space disappearing and the latter shifting to the left.",0.05,0.023,0.927,-0.08
jaeger,The weird thing is that if I wait like 30 seconds after typing the space to type the letter it works fine.,0.071,0.179,0.75,0.3818
jaeger,I have Kubernetes 1.17.5 and Istio 1.6.8 installed with demo profile.,0.0,0.0,1.0,0.0
jaeger,And here is my test setup [nginx-ingress-controller] -&gt; [proxy&lt;-&gt;ServiceA] -&gt; [proxy&lt;-&gt;ServiceB],0.0,0.0,1.0,0.0
jaeger,When I'm sending requests to ingress controller I can see that ServiceA receives all required tracing headers from the proxy,0.0,0.0,1.0,0.0
jaeger,Problem is  x-b3-sampled  is always set to 0 and no spans/traces are getting pushed to Jaeger,0.274,0.0,0.726,-0.5994
jaeger,Few things I've tried,0.0,0.0,1.0,0.0
jaeger,Here is the config I've tried to use,0.0,0.0,1.0,0.0
jaeger,I am trying to make OpenTelemetry exporter to work with OpenTelemetry collector.,0.0,0.0,1.0,0.0
jaeger,I found this  OpenTelemetry collector demo .,0.0,0.0,1.0,0.0
jaeger,So I copied these four config files,0.0,0.0,1.0,0.0
jaeger,to my app.,0.0,0.0,1.0,0.0
jaeger,Also based on these two demos in open-telemetry/opentelemetry-js repo:,0.0,0.0,1.0,0.0
jaeger,"I came up with my version (sorry for a bit long, really hard to set up a minimum working version due to the lack of docs):",0.16,0.0,0.84,-0.4576
jaeger,.env,0.0,0.0,1.0,0.0
jaeger,docker-compose.yml,0.0,0.0,1.0,0.0
jaeger,otel-agent-config.yaml,0.0,0.0,1.0,0.0
jaeger,otel-collector-config.yaml,0.0,0.0,1.0,0.0
jaeger,"After running  docker-compose up -d , I can open Jaeger (http://localhost:16686) and Zipkin UI (http://localhost:9411).",0.0,0.0,1.0,0.0
jaeger,And my  ConsoleSpanExporter  works in both web client and Express.js server.,0.0,0.0,1.0,0.0
jaeger,"However, I tried this OpenTelemetry exporter code in both client and server, I am still having issue to connect OpenTelemetry collector.",0.0,0.0,1.0,0.0
jaeger,Please see my comment about URL inside of the code,0.0,0.204,0.796,0.3182
jaeger,Any idea?,0.0,0.0,1.0,0.0
jaeger,Thanks,0.0,1.0,0.0,0.4404
jaeger,"Input:
GCP, Kubernetes, java 11 spring boot 2 application",0.0,0.0,1.0,0.0
jaeger,Container is started with memory limit 1.6GB.,0.0,0.0,1.0,0.0
jaeger,Java application is limiting memory as well -XX:MaxRAMPercentage=80.0.,0.0,0.231,0.769,0.2732
jaeger,"Under a ""heavy"" (not really) load - about 1 http request per 100 ms during about 4 hours application is killed by OOMKiller.",0.2,0.0,0.8,-0.6705
jaeger,Internal diagnostic tools is showing that memory is far from limit:,0.0,0.0,1.0,0.0
jaeger,However GCP tools is showing the following:,0.0,0.0,1.0,0.0
jaeger,There is a suspicious that GCP is measuring something else?,0.238,0.0,0.762,-0.3612
jaeger,POD contains only java app (+jaeger agent).,0.0,0.0,1.0,0.0
jaeger,The odd thing that after restart GCP shows almost maximum memory usage instead of slowly growing if it was memory leak.,0.193,0.07,0.738,-0.4588
jaeger,EDIT:,0.0,0.0,1.0,0.0
jaeger,Docker file:,0.0,0.0,1.0,0.0
jaeger,and run it with Kubernetes (extra details are ommited):,0.0,0.0,1.0,0.0
jaeger,"UPDATE 
according top command memory limit is also far from limit however CPU utilization became more then 100% before container is OOMKilled.",0.0,0.079,0.921,0.2023
jaeger,Is it possible that Kubernetes kills container that is trying to get more CPU then allowed?,0.189,0.0,0.811,-0.5423
jaeger,UPDATE2,0.0,0.0,1.0,0.0
jaeger,this pmap was called not far before OOMKilled.,0.0,0.0,1.0,0.0
jaeger,5Gb?,0.0,0.0,1.0,0.0
jaeger,Why top is not showing this?,0.0,0.265,0.735,0.2023
jaeger,Also not sure how to interpretate pmap command result,0.197,0.0,0.803,-0.2411
jaeger,We are building a web-app using Micronaut ( v1.2.0 ) which will be deployed in a Kubernetes cluster (we are using Istio as the service-mesh).,0.0,0.0,1.0,0.0
jaeger,We would like to instrument the critical method calls so that they can generate their own spans within a HTTP request span context.,0.093,0.101,0.806,0.0516
jaeger,For this we are using the Micronaut OpenTracing support and Jaeger integration.,0.0,0.197,0.803,0.4019
jaeger,The following dependencies are included in the  pom.xml,0.0,0.0,1.0,0.0
jaeger,Have implemented Filter method with  @ContinueSpan  (also tried the same with  @NewSpan ) as shown below,0.0,0.0,1.0,0.0
jaeger,The following is maintained in the  application-k8s.yml  (also have an  application.yml  with the same settings),0.0,0.0,1.0,0.0
jaeger,However we only see the trace entries that are generated by Istio (Envoy proxies) but we don't see the details of the method calls itself.,0.0,0.0,1.0,0.0
jaeger,Any ideas as to what could be going wrong here?,0.256,0.0,0.744,-0.4767
jaeger,docker pull jaegertracing/jaeger-agent:latest,0.0,0.0,1.0,0.0
jaeger,Jaeger is just for illustration.,0.0,0.0,1.0,0.0
jaeger,But my question is more generic.,0.0,0.0,1.0,0.0
jaeger,The above command pulls the  latest  version of the  jaeger-agent  from docker-hub.,0.0,0.0,1.0,0.0
jaeger,The docker-hub page for this is :  https://hub.docker.com/r/jaegertracing/jaeger-agent,0.0,0.0,1.0,0.0
jaeger,My question is how do I find the actual version of  latest  ?,0.0,0.0,1.0,0.0
jaeger,"I looked in to the tags here, but there is not much info :
 https://hub.docker.com/r/jaegertracing/jaeger-agent/tags",0.0,0.0,1.0,0.0
jaeger,"Also I tried doing an  inspect  after pulling the image, but could not get necessary details.",0.0,0.0,1.0,0.0
jaeger,docker image inspect jaegertracing/jaeger-agent:latest,0.0,0.0,1.0,0.0
jaeger,Where can we get this information from ?,0.0,0.0,1.0,0.0
jaeger,I am trying to integrate Jaeger tracing into K-Streams.,0.0,0.0,1.0,0.0
jaeger,I was planning to add tracing to few of my most important pipelines and was wondering what would be a good way to pass traceid from one piepline to another?,0.0,0.161,0.839,0.6115
jaeger,"Here is what I have so far - At the start of stream processing pipeline, I start a server span and save the traceid into a state store.",0.0,0.127,0.873,0.4939
jaeger,"Later on, in a transform pipeline, I access the statestore and capture the  trace from the transform() method.",0.0,0.0,1.0,0.0
jaeger,Is this a good way to handle tracing in stream processing?,0.0,0.244,0.756,0.4404
jaeger,I am trying to integrate a legacy system with microservice hosted on Red Hat OpenShift platform.,0.0,0.0,1.0,0.0
jaeger,The service is a java app behind ingress gateway.,0.0,0.0,1.0,0.0
jaeger,The legacy app passes unique operation identifier as a custom header  uniqueId .,0.0,0.0,1.0,0.0
jaeger,The microservice leverages openshift service mesh support for Jaeger so I can pass tracing headers such as  x-b3-traceid  and see the request trace in Jaeger UI.,0.0,0.101,0.899,0.4019
jaeger,"Unfortunately, the legacy app cannot be modified and won't send jaeger headers but  uniqueId  conforms jaeger rules and seems ok to be used for tracing.",0.062,0.102,0.836,0.2732
jaeger,I am trying to transform  uniqueId  into  x-b3-traceid  on an envoy filter.,0.0,0.0,1.0,0.0
jaeger,"The problem is that I can copy it to any other header, but cannot modify  x-b3-*  headers.",0.11,0.0,0.89,-0.2144
jaeger,Istio keeps generating new set of  x-b3-*  headers no matter what I do in envoy filter.,0.135,0.067,0.798,-0.2732
jaeger,See filter code below.,0.0,0.0,1.0,0.0
jaeger,"I tried different filter positions (on ingress gateway, on pod sidecar, before envoy.router, etc).",0.0,0.0,1.0,0.0
jaeger,Seems nothing works.,0.0,0.0,1.0,0.0
jaeger,Can anyone recommend how can I pass custom header as a traceId for service mesh's jaeger?,0.0,0.161,0.839,0.3612
jaeger,I can create a custom proxy service transforming one header with another but it looks redundant.,0.0,0.107,0.893,0.1406
jaeger,Is it possible to achieve that with service mesh only?,0.0,0.0,1.0,0.0
jaeger,I've been using Spring Boot for a long time.,0.0,0.0,1.0,0.0
jaeger,I'm working on Micronaut now.,0.0,0.0,1.0,0.0
jaeger,I'm used to using Sleuth to print trace and span IDs automatically on logs.,0.0,0.0,1.0,0.0
jaeger,What is the sleuth equivalent in Micronaut?,0.0,0.0,1.0,0.0
jaeger,"If there is no equivalent, how to print the trace and span IDs in Micronaut using Jaeger?",0.121,0.0,0.879,-0.296
jaeger,I want to inject secret values to Kubernetes crd.,0.0,0.4,0.6,0.4588
jaeger,"For example, suppose I have Jaeger crd yaml file, and as the Elasticsearch server-url, password are secret values, I want them to be injected using Vault.",0.0,0.154,0.846,0.4588
jaeger,"When using Deployment, I can inject the secrets using Vault secret, by first creating secrets and loading them from envs in container.",0.0,0.099,0.901,0.296
jaeger,"However, as crd cannot be done that way, I don't know how to inject the values from outside securely in code.",0.0,0.221,0.779,0.6249
jaeger,Any ideas?,0.0,0.0,1.0,0.0
jaeger,I have installed istio using the official reference as on  Getting Started  page.,0.0,0.0,1.0,0.0
jaeger,"Below are the commands i used: 
 $ curl -L https://istio.io/downloadIstio | sh -",0.0,0.0,1.0,0.0
jaeger,$ istioctl install --set profile=demo,0.0,0.0,1.0,0.0
jaeger,$ kubectl label namespace default istio-injection=enabled,0.0,0.0,1.0,0.0
jaeger,I ended up with below version of istio:,0.0,0.0,1.0,0.0
jaeger,and my kubernetes version is:,0.0,0.0,1.0,0.0
jaeger,Every thing seems fine until i verify the objects installed in  istio-system  namespace,0.0,0.141,0.859,0.2023
jaeger,"As, you can see there are few missing components -
There are few  pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc.",0.188,0.0,0.812,-0.5267
jaeger,These components were  available in 1.4.2.,0.0,0.0,1.0,0.0
jaeger,"In 1.4.2 installation I could see  grafana, jaeger, kiali, prometheus, zipkin dashboards.",0.0,0.0,1.0,0.0
jaeger,But these are now  missing .,0.412,0.0,0.588,-0.4215
jaeger,Example:,0.0,0.0,1.0,0.0
jaeger,Is this expected behaviour in 1.7.2 or is my installation broken.,0.237,0.0,0.763,-0.4767
jaeger,"If the installation is broken, how else can i fix it.",0.256,0.0,0.744,-0.4767
jaeger,After all I followed the instruction from the Starter Guide.,0.0,0.0,1.0,0.0
jaeger,Im having some issues here with Opentracing and Jaegertracing when it comes to C#.,0.0,0.0,1.0,0.0
jaeger,"I have had this working before, but with Java projects.",0.0,0.0,1.0,0.0
jaeger,So I start to wonder what Im missing when it comes to C# .NET Core web service.,0.128,0.0,0.872,-0.296
jaeger,This is my class to start my tracer to be used,0.0,0.0,1.0,0.0
jaeger,Controller code that should report to the Jaeger agent and collector for show in the UI.,0.0,0.0,1.0,0.0
jaeger,Startup.cs,0.0,0.0,1.0,0.0
jaeger,But this is not working at all.,0.0,0.0,1.0,0.0
jaeger,What am I missing here to get it work with a remote server?,0.18,0.0,0.82,-0.296
jaeger,I am using Spring Boot 2 Microservices with Spring Cloud Sleuth with the Dependency Management and Spring Cloud Version Greenwich.SR2.,0.0,0.0,1.0,0.0
jaeger,My service is running in an Istio service mesh.,0.0,0.0,1.0,0.0
jaeger,Sample policy of istio is set to 100 (pilot.traceSampling: 100.0).,0.0,0.0,1.0,0.0
jaeger,"To use distributed tracing in the mesh, the applications needs to forward HTTP headers like the X-B3-TraceId and X-B3-SpanID.",0.0,0.122,0.878,0.3612
jaeger,This is achieved by simply adding Sleuth.,0.0,0.0,1.0,0.0
jaeger,All my HTTP request are are traced correctly.,0.0,0.0,1.0,0.0
jaeger,The sidecar proxies of Istio (Envoy) send the traces to the Jaeger backend.,0.0,0.0,1.0,0.0
jaeger,Sleuth is also supposed to work with Spring WebSocket.,0.0,0.0,1.0,0.0
jaeger,"But my incoming websocket requests do not get any trace or span id by sleuth; Logs look like [-,,,].",0.0,0.153,0.847,0.5023
jaeger,1.,0.0,0.0,1.0,0.0
jaeger,Question: Why is Sleuth not working for websocket?,0.0,0.0,1.0,0.0
jaeger,My WS-Config:,0.0,0.0,1.0,0.0
jaeger,My clients are able to connect to my Service via Websocket.,0.0,0.0,1.0,0.0
jaeger,I am implementing WebSocketHandler interface to handle WS messages.,0.0,0.0,1.0,0.0
jaeger,"To achieve that my WS connections are logged by Sleuth, I annotate the method that handles my connection with @NewSpan:",0.0,0.0,1.0,0.0
jaeger,"With this, Sleuth creates trace and spanId and also propagates them to the other Services, which are called via the restTemplate in this method.",0.0,0.084,0.916,0.2732
jaeger,But HTTP calls are not send to Jaeger.,0.0,0.0,1.0,0.0
jaeger,The x-B3-Sampled Header is always set to 0 by the sidcar.,0.0,0.0,1.0,0.0
jaeger,2 Question: Why are those traces not send to the tracing backend?,0.0,0.0,1.0,0.0
jaeger,Thank you in advance!,0.0,0.482,0.518,0.4199
jaeger,"I am using rabbitmq as one of the microservice and for that I want trace the rabbitmq spans,
I have used following dependencies for tracing the rabbitmq spans through opentracing,",0.0,0.048,0.952,0.0772
jaeger,I am getting only producer side spans for this microservice.,0.0,0.0,1.0,0.0
jaeger,producer,0.0,0.0,1.0,0.0
jaeger,I want to get end to end tracing for the request which passed through multiple micro services and one of them is  rabbitmq,0.0,0.058,0.942,0.0772
jaeger,like  microservice1==&gt;rabbitMQ(Producer)==&gt;Microservice2==&gt;rabbitMQ(Consumer)==&gt;Response Service,0.0,0.556,0.444,0.3612
jaeger,How can I achieve this kind of tracing in jaeger UI?,0.0,0.0,1.0,0.0
jaeger,I have Kubernetes cluster with multiple Java services deployed to AWS with Istio.,0.0,0.0,1.0,0.0
jaeger,I built a ServerInterceptor where I had the Istio needed B3 headers to the gRPC context.,0.0,0.0,1.0,0.0
jaeger,I then implemented a ClientInterceptor where I parse those headers from the gRPC context and insert them into the outgoing headers.,0.0,0.115,0.885,0.296
jaeger,I also implemented OpenTracing Server and Client Tracing Interceptors per:  https://github.com/opentracing-contrib/java-grpc,0.0,0.0,1.0,0.0
jaeger,"When I view the traces in the Jaeger UI, all I am seeing is spans reported by the sidecar.",0.0,0.0,1.0,0.0
jaeger,"They are properly linked as it gets called from service to service, but they are always of type ""client.""",0.0,0.0,1.0,0.0
jaeger,I don't see any of the spans from when I call the database.,0.0,0.0,1.0,0.0
jaeger,It seems the context is not transferred from the side car to my actual app.,0.0,0.0,1.0,0.0
jaeger,"Only sidecar to sidecar to sidecar, etc.",0.0,0.0,1.0,0.0
jaeger,"When I run the stack locally in a simple docker compose (without kubernetes and istio) configuration, I see the server spans being created and reported.",0.0,0.087,0.913,0.25
jaeger,How do I get my Java gRPC server to expand on the spans created by the Istio side cars?,0.0,0.212,0.788,0.5106
jaeger,I am trying to set up distributed event tracing throughout out microservice architecture.,0.0,0.0,1.0,0.0
jaeger,Here is some preamble about our architecture:,0.0,0.0,1.0,0.0
jaeger,Traefik load balancer that forwards request to the appropriate backend service based on the route pathname.,0.0,0.0,1.0,0.0
jaeger,"Frontend application on a ""catchall"" route that is served whenever a route is not caught by another microservice.",0.0,0.0,1.0,0.0
jaeger,Various backend services in node/dotnetcore listening on  /api/&lt;serviceName&gt;,0.0,0.0,1.0,0.0
jaeger,"traefik is setup with the  traceContextHeaderName  set to  ""trace-id"" .",0.0,0.0,1.0,0.0
jaeger,"How I imagine this would work is that the frontend application receives a header ""trace-id"" from the load balancer with a value that can be used to ""link"" the spans together for requests that are related.",0.0,0.07,0.93,0.34
jaeger,Example scenario:,0.0,0.0,1.0,0.0
jaeger,"When a customer loads attempts to sign in, they make a request for the web application, receive and render the HTML/CSS/JS, then the subsequent requests to  /api/auth/login  can be POSTed with the login data and the value of the  ""trace-id""  header supplied by traefik.",0.0,0.055,0.945,0.34
jaeger,"The backend service that handles the  /api/auth/login  endpoint can capture this  ""trace-id""  header value and publish some spans to jaeger related to the work that it is doing to validate the user.",0.0,0.14,0.86,0.5994
jaeger,What is happening:,0.0,0.0,1.0,0.0
jaeger,"When the request is made for the frontend HTML, no  ""trace-id""  header is received so any subsequent spans that are published are all considered individual traces and are not linked together.",0.068,0.0,0.932,-0.296
jaeger,traefik.toml:,0.0,0.0,1.0,0.0
jaeger,"I understand that StackOverflow is not a ""code it for me"" service.",0.0,0.0,1.0,0.0
jaeger,I am looking for guidance on what could possibly be going wrong as I am new to distributed event tracing.,0.154,0.0,0.846,-0.4767
jaeger,I have tried googling and searching for answers but I have come to a dead end.,0.331,0.0,0.669,-0.7876
jaeger,Any help/suggestions on where to look would be greatly appreciated.,0.0,0.285,0.715,0.5563
jaeger,"Please let me know if I am barking up the wrong tree, approaching this incorrectly, or if my understanding of how the  traceContextHeaderName  should work is incorrect.",0.105,0.078,0.816,-0.2023
jaeger,Using MassTransit.RabbitMQ v5.3.2 and OpenTracing.Contrib.NetCore v0.5.0.,0.0,0.0,1.0,0.0
jaeger,"I'm able publish and consume events to RabbitMQ using MassTransit and I've got OpenTracing working with Jaeger, but I haven't managed to get my OpenTracing TraceIds propogated from my message publisher to my message consumer - The publisher and consumer traces have different TraceIds.",0.0,0.0,1.0,0.0
jaeger,I've configured MassTransit with the following filter:,0.0,0.0,1.0,0.0
jaeger,"I'm not actually sure what the listener name should be, hence ""test"".",0.151,0.0,0.849,-0.2411
jaeger,The  documentation  doesn't have an example for OpenTracing.,0.0,0.0,1.0,0.0
jaeger,"Anyways, this adds a 'Publishing Message' span to the active trace on the publish side, and automatically sets up a 'Consuming Message' trace on the consumer side; however they're separate traces.",0.0,0.088,0.912,0.4019
jaeger,How would I go about consolidating this into a single trace?,0.0,0.0,1.0,0.0
jaeger,I could set a TraceId header using:,0.0,0.0,1.0,0.0
jaeger,but then how would I configure my message consumer so that this is the root TraceId?,0.0,0.0,1.0,0.0
jaeger,"Interested to see how I might do this, or if there's a different approach...",0.0,0.197,0.803,0.4019
jaeger,Thanks!,0.0,1.0,0.0,0.4926
jaeger,"I am testing Istio 1.1, but the collection of metrics is not working correctly.",0.0,0.0,1.0,0.0
jaeger,I can not find what the problem is.,0.31,0.0,0.69,-0.4019
jaeger,I followed  this tutorial  and I was able to verify all the steps without problems.,0.0,0.158,0.842,0.3089
jaeger,If I access prometheus I can see the log of some requests.,0.0,0.0,1.0,0.0
jaeger,"On the other hand, if I access Jaeger, I can not see any service (only 1 from Istio)",0.0,0.186,0.814,0.4939
jaeger,"Grafana is also having some strange behavior, most of the graphs do not show data.",0.114,0.0,0.886,-0.2023
jaeger,On  https://opentracing.io/  they state that opentracing API is:,0.0,0.0,1.0,0.0
jaeger,A Vendor-neutral APIs and instrumentation for distributed tracing,0.0,0.0,1.0,0.0
jaeger,Okay great but what does that actually mean in the context of an actual application?,0.0,0.235,0.765,0.4588
jaeger,"What parts does this Opentracing API actually consist of, what is its purpose and how does it interact with other logging related systems like ""zipkin"" and ""jaeger""",0.0,0.088,0.912,0.3612
jaeger,"Is using  Opentracing API for Java   a requirement to be able to claim ""My App supports"" opentracing?",0.0,0.143,0.857,0.3612
jaeger,Is there one Opentracing protocol (e.g data send over the wire) or are they just saying opentracing is a middle layer which allows multiple other tracing frameworks to interoperate with each other?,0.0,0.0,1.0,0.0
jaeger,Especially  this diagram  makes me think that.,0.0,0.0,1.0,0.0
jaeger,I want to implement a jaeger installation with persistent storage using elasticsearch like backend on my Kubernetes cluster on Google cloud platform.,0.0,0.174,0.826,0.4215
jaeger,I am using the jaeger kubernetes templates and I am starting with elasticsearch  production setup .,0.0,0.0,1.0,0.0
jaeger,I've downloaded and modified the  configmap.yml  file in order to change the password field value and the  elasticsearch.yml  file in order to fix the password value which I've changed.,0.0,0.151,0.849,0.5859
jaeger,My customized  .yml  files has stayed of this way:,0.0,0.0,1.0,0.0
jaeger,configmap.yml,0.0,0.0,1.0,0.0
jaeger,elasticsearch.yml,0.0,0.0,1.0,0.0
jaeger,"And then, I've created the kubernetes cluster configuration with the new password value from my machine to my KGE via  kubectl  command",0.0,0.18,0.82,0.5267
jaeger,And I've created the elasticsearch service via StatefulSet specialized pod  (also with the new password value) from my machine to my KGE via  kubectl  command,0.0,0.077,0.923,0.25
jaeger,I can see that I have the elasticsearch service created on my GKE cluster,0.0,0.154,0.846,0.25
jaeger,And I have the  elasticsearch-0  pod which have the docker container of elasticsearch service,0.0,0.0,1.0,0.0
jaeger,"But when I can detail my pod on KGE, I see that my pod have some warnings and is not healthy ...",0.24,0.0,0.76,-0.6895
jaeger,I get the pod description detail and I get this warning,0.256,0.0,0.744,-0.4118
jaeger,"Here, some part of my entire output to  describe  command",0.0,0.0,1.0,0.0
jaeger,I go to the container log section on GCP and I get the following:,0.0,0.0,1.0,0.0
jaeger,And in the audit log section I can see something like this:,0.0,0.2,0.8,0.3612
jaeger,If I try with the original files and I change the password via KGE on GCP I get this error:,0.153,0.113,0.734,-0.2083
jaeger,"After that I've create a pod, is not possible update or perform some changes?",0.0,0.149,0.851,0.2732
jaeger,kubectl apply -f .....   ?,0.0,0.0,1.0,0.0
jaeger,...,0.0,0.0,1.0,0.0
jaeger,I suposse,0.0,0.0,1.0,0.0
jaeger,How to can I change the elasticsearch password?,0.0,0.0,1.0,0.0
jaeger,"If I want configure a persistent volume claim on this pod, can I perform this before the  kubectl create -f command and my volume and mountPath will be created on container and KGE?",0.0,0.167,0.833,0.5267
jaeger,"If somebody can point me in the correct address, their support will be highly appreciated.",0.0,0.326,0.674,0.7425
jaeger,I have made a trivial 3 tier services similar to the bookinfo app on the Istio site.,0.078,0.0,0.922,-0.0258
jaeger,"Everything seems to work fine, except for the tracing with zipkin or jaeger.",0.0,0.13,0.87,0.2023
jaeger,"To clarify, I have 3 services S1, S2, S3, all pretty similar and trivial passing requests downstream and doing some work.",0.052,0.15,0.798,0.4767
jaeger,"I can see S1 and S2 in the trace, but not S3.",0.0,0.0,1.0,0.0
jaeger,"I have narrowed this down a bit further, when i use Istio version 0.5.0, I can see S3 in the trace as well, but only after some time, however, with Istio version 0.5.1, I can only see S1 and S2 in the trace, even though the services are working properly and the calls are propagating down all the way to S3.",0.0,0.027,0.973,0.1406
jaeger,"The only difference that I can see, which I am not sure if this is even an issue or not, is this output in istio-proxy for S3 using istio version 0.5.0, but not in 0.5.1",0.044,0.0,0.956,-0.1232
jaeger,"""GET /readiness HTTP/1.1"" 200 - 0 39 1 1 ""-"" ""kube-probe/1.9+"" ""0969a5a3-f6c0-9f8e-a449-d8617c3a5f9f"" ""10.X.X.18:8080"" ""127.0.0.1:8080""",0.0,0.0,1.0,0.0
jaeger,I can add the exact yaml files if need.,0.0,0.0,1.0,0.0
jaeger,"Also, I am not sure if the tracing is supposed to be coming from istio-proxy as it shows in the istio docs, but in my case, I do not see istio-proxy but rather istio-ingress only.",0.044,0.0,0.956,-0.1232
jaeger,So I'm pretty new to golang and i'm struggling to get a working example going of encrypting some text with openpgp and decrypting it again.,0.099,0.123,0.778,0.1725
jaeger,Here is what I have so far: ( https://gist.github.com/93750a142d3de4e8fdd2.git ),0.0,0.0,1.0,0.0
jaeger,This is based off of  https://github.com/jyap808/jaeger,0.0,0.0,1.0,0.0
jaeger,"When I run it, it seems to partially work, but only outputs some of the characters of the original string... Changing the original string causes some very weird issues.",0.075,0.177,0.749,0.5284
jaeger,"Clearly there is something I'm not understanding, so would appreciate any assistance given.",0.0,0.34,0.66,0.6887
jaeger,I am using opentelemetry api and sdk version 1.0.0 in python and Jaeger to see traces.,0.0,0.0,1.0,0.0
jaeger,I have two services that communicates between each other and I can see traces for each service individually on Jaeger but spans are not nested (while they should).,0.0,0.0,1.0,0.0
jaeger,This snippet show you what I do to propagate the trace between the services.,0.0,0.0,1.0,0.0
jaeger,"In previous opentelemetry versions (0.7b1), I could use directly  ctx_parent  without using  set_span_in_context  and it was working fine (I visualized nested spans on Jaeger), but unfortunately they removed the packages from pypi so I can not build anymore my project...",0.077,0.035,0.889,-0.4019
jaeger,Thanks for any help !,0.0,0.747,0.253,0.7088
jaeger,I am trying to query the traces Cassandra table which is part of the Jaeger architecture.,0.0,0.0,1.0,0.0
jaeger,As you can see the refs field is a list:,0.0,0.0,1.0,0.0
jaeger,from the python code:,0.0,0.0,1.0,0.0
jaeger,"I am working on adding opentracing in our micro services, using Jaeger.",0.0,0.0,1.0,0.0
jaeger,I have two GRPC server and one REST server.,0.0,0.0,1.0,0.0
jaeger,The default opentracing with perfectly fine with both GRPC server and all the rest-grpc request are tracked under one parent span.,0.0,0.24,0.76,0.7184
jaeger,With Java GRPC I am able to add custom child spans and it appears in perfect hierarchy in the Jaeger UI.,0.0,0.163,0.837,0.5719
jaeger,"But When I am trying to add same custom child in Go Lang, it is not added to the parent Rest Service span which has called the GRPC service.",0.0,0.0,1.0,0.0
jaeger,Below is the golang code,0.0,0.0,1.0,0.0
jaeger,"I do not want to do the whole http headers extraction, as that is already taken care by GRPC library.",0.057,0.149,0.794,0.4548
jaeger,Even with java GRPC I do not do any extraction.,0.0,0.0,1.0,0.0
jaeger,The scope manager that I use with opentracing is not available with go lang opentracing.,0.0,0.0,1.0,0.0
jaeger,Thanks in advance!!,0.0,0.635,0.365,0.5399
jaeger,!,0.0,0.0,0.0,0.0
jaeger,Cheers.,0.0,1.0,0.0,0.4767
jaeger,"OS : ubuntu 18.04
ceph : octopus
jaeger : master",0.0,0.0,1.0,0.0
jaeger,"When I implement jaegertracer in the function that is responsibe for writing file to ceph via RGW, I am unable to upload my file Im getting this error",0.111,0.0,0.889,-0.481
jaeger,But when I remove my tracer from the code it uploads the file successfully,0.0,0.264,0.736,0.6486
jaeger,source code,0.0,0.0,1.0,0.0
jaeger,When I remove the tracer it compiles fine again,0.0,0.205,0.795,0.2023
jaeger,I have activated  opentracing  on my Spring Boot micro-service application using  Jaeger  as a collector and it all works fine.,0.0,0.096,0.904,0.2023
jaeger,"I manage to get a full trace of my calls from different components, it is very useful to understand the calls to the application.",0.0,0.132,0.868,0.4927
jaeger,"Now, in the scope of performance testing, I need to generate statistics from the different readings.",0.0,0.0,1.0,0.0
jaeger,That is e.g.,0.0,0.0,1.0,0.0
jaeger,average time of traces during a time period or number of occurrences of a specific span.,0.0,0.091,0.909,0.0772
jaeger,Is there any tool to achieve that?,0.0,0.0,1.0,0.0
jaeger,Is there a standard query language/api/tool to allow to extract big numbers of opentracing metrics?,0.0,0.128,0.872,0.2263
jaeger,I'm using the Open Tracing Python library for GRPC and am trying to build off of the example script here:  https://github.com/opentracing-contrib/python-grpc/blob/master/examples/trivial/trivial_client.py .,0.0,0.0,1.0,0.0
jaeger,"Once I have sent a request through the intercepted channel, how do I find the trace-id value for the request?",0.0,0.13,0.87,0.34
jaeger,I want to use this to look at the traced data in the Jaeger UI.,0.0,0.091,0.909,0.0772
jaeger,I'm working with a few services in a kubernetes cluster.,0.0,0.0,1.0,0.0
jaeger,"I'm trying to implement telepresence to allow local debugging of code in the cluster, or potential changes in pull requests.",0.0,0.091,0.909,0.2263
jaeger,The services in the cluster are running SpringBoot REST services.,0.0,0.0,1.0,0.0
jaeger,I have a simple test case of using curl to reach a REST endpoint running in the cluster.,0.0,0.073,0.927,0.0258
jaeger,I can reach it successfully without telepresence.,0.0,0.518,0.482,0.5106
jaeger,"I'm on a Win7 laptop, running an Ubuntu VM with NAT networking.",0.0,0.0,1.0,0.0
jaeger,"I can run the telepresence command line, whose ""--run"" section runs ""mvn spring-boot:run"".",0.0,0.0,1.0,0.0
jaeger,"This defaults to proxy method ""vpn-tcp"".",0.0,0.0,1.0,0.0
jaeger,The service appears to start up fine.,0.0,0.231,0.769,0.2023
jaeger,"I can hit the service endpoint with ""localhost:8080"" successfully.",0.0,0.314,0.686,0.4939
jaeger,"However, if I rerun the test case to reach the service in the cluster, it fails with a 502 (Bad Gateway).",0.134,0.053,0.813,-0.4019
jaeger,"When I run telepresence, I can watch it replace two pods running the springboot image with a single pod running the telepresence image.",0.0,0.0,1.0,0.0
jaeger,"I've looked at the detailed properties of the service and pods both before and after running telepresence, and I don't see any obvious issues in the minor differences.",0.0,0.0,1.0,0.0
jaeger,"If I then kill the telepresence process, it eventually restores the original pods and my test case works again.",0.194,0.186,0.62,-0.296
jaeger,Note that I'm currently doing this testing while connected to our corp network with VPN.,0.0,0.0,1.0,0.0
jaeger,"The instructions in the telepresence docs say to not mix ""vpn-tcp"" with another VPN.",0.0,0.0,1.0,0.0
jaeger,I'm not sure if this is relevant.,0.246,0.0,0.754,-0.2411
jaeger,"The first time I tried this test, I was in the office, not on VPN, and I saw the same results.",0.0,0.0,1.0,0.0
jaeger,"I also tried changing the proxy method to ""inject-tcp"".",0.0,0.0,1.0,0.0
jaeger,"This resulted in the SpringBoot service failing to start, referring to a Jaeger client that couldn't connect to a server.",0.163,0.0,0.837,-0.5106
jaeger,"If it matters, here is the telepresence command that I'm executing (reverting inject-tcp change) and some initial output:",0.0,0.061,0.939,0.0258
jaeger,I'm looking for ideas to move forward from this.,0.0,0.0,1.0,0.0
jaeger,I have a Java application built using Play 2.5x.,0.0,0.286,0.714,0.34
jaeger,"I am using AspectJ with Kamon to profile methods in my selected packages, and am reporting the execution details in Jaeger.",0.0,0.0,1.0,0.0
jaeger,It works fine with synchronous methods but fails when it comes to reporting asynchronous ones.,0.204,0.077,0.718,-0.5106
jaeger,"For example, if I have a thenApply block which is executed after a future completes, then I cannot profile the lambda inside the thenApply in the current scheme of things.",0.104,0.0,0.896,-0.4404
jaeger,I do not want to add any extra code to my Play app and want the aspect to take care of the profiling.,0.094,0.215,0.691,0.6317
jaeger,Any help will be greatly appreciated :),0.0,0.705,0.295,0.8615
jaeger,I'm trying to crawl a specific page of a website ( https://www.johnlewis.com/jaeger-wool-check-knit-shift-dress-navy-check/p3767291 ) to get used to Scrapy and its features.,0.0,0.0,1.0,0.0
jaeger,"However, I can't get Scrapy to see the 'li' that contains the thumbnail images on the carousel.",0.0,0.0,1.0,0.0
jaeger,My  parse  Function currently looks as follows:,0.0,0.0,1.0,0.0
jaeger,"No matter what Scrapy isn't ""seeing"" the li.",0.237,0.118,0.645,-0.2732
jaeger,I've tried viewing the page in a scrapy shell to check Scrapy could see the images and they are showing up in the response for that (so I'm assuming Scrapy can definitely see the list/images in the list).,0.0,0.07,0.93,0.4019
jaeger,I've tried alternative lists and I've got a different list to work (as per the comment in the code).,0.0,0.0,1.0,0.0
jaeger,My only thoughts are that the carousel may be loaded with JavaScript / AJAX but I can't be too sure.,0.126,0.0,0.874,-0.3491
jaeger,"I do know that the list class will change if it is the selected image from ""li.thumbnail-slide"" to ""li.thumbnail-slide thumbnail-slide-active"" however, I've tried the following in my script to no avail:",0.071,0.0,0.929,-0.296
jaeger,Nothing works.,0.0,0.0,1.0,0.0
jaeger,Does anyone have any suggestions on what I may be doing wrong?,0.237,0.0,0.763,-0.4767
jaeger,Or suggest any further reading that may help?,0.0,0.278,0.722,0.4019
jaeger,Thanks in advance!,0.0,0.615,0.385,0.4926
jaeger,EDIT:  I have edited in the output of the program.,0.0,0.0,1.0,0.0
jaeger,The program calls for estimating a given value mu.,0.0,0.255,0.745,0.34
jaeger,"User gives a value of mu, and also provides four different numbers not equal to 1 (call them w, x, y, z).",0.0,0.112,0.888,0.34
jaeger,The program then attempts to find an estimate of the mu value by using the de Jaeger formula.,0.0,0.124,0.876,0.34
jaeger,"If I enter values of 238,900 for mu, and w=14, x=102329, y=1936, z=13
then the value of estimate should be 239,103, and the error about .08%.",0.091,0.171,0.738,0.34
jaeger,My code with the for loops works perfectly fine:,0.0,0.462,0.538,0.7184
jaeger,Output:,0.0,0.0,1.0,0.0
jaeger,"However, with the while loops, I am unable to replicate this.",0.0,0.0,1.0,0.0
jaeger,Output:,0.0,0.0,1.0,0.0
jaeger,I am creating an Excel sheet that will show when customers have paid and when they are late.,0.0,0.257,0.743,0.6369
jaeger,"With the help of a friend, I was able to come up with a formula that will tell us if a customer is past their due date by highlighting the cell red.",0.0,0.185,0.815,0.7096
jaeger,"As far as I've seen, there's no way to get conditional formatting to change the blank cell to say ""LATE"", so I was suggested to try out VBA.",0.078,0.0,0.922,-0.296
jaeger,"I have been able to change the colors and add the words I need, but how to get VBA to check the dates like the conditional formatting formula?",0.0,0.115,0.885,0.5023
jaeger,Here is the formula and what I have in my VBA sheet:,0.0,0.0,1.0,0.0
jaeger,Formula,0.0,0.0,1.0,0.0
jaeger,VBA,0.0,0.0,1.0,0.0
jaeger,"I used some code I found online (thank you Rolf Jaeger from nullskull.com) for this so I'm not 100% if I need everything there, but it seems to be working fine, so I left it for now until I learn more about VBA.",0.0,0.056,0.944,0.296
jaeger,I just started using OpenTracing with the Jaeger Cloud starter.,0.0,0.0,1.0,0.0
jaeger,For this I added the folllowing dependency to my project:,0.0,0.0,1.0,0.0
jaeger,The  io.opentracing.contrib.spring.integration.messaging.OpenTracingChannelInterceptor  of the  opentracing-spring-messaging  project adds the Scope (ThreadLocalScope) to the message header.,0.0,0.0,1.0,0.0
jaeger,The Solace binder output message handler only supports header values that are instances of Serializable.,0.0,0.286,0.714,0.6369
jaeger,So it doesn't work out of the box.,0.0,0.0,1.0,0.0
jaeger,I implemented an aspect around this message handler that looks like this (just parts of it):,0.0,0.152,0.848,0.3612
jaeger,With this hack it works.,0.0,0.0,1.0,0.0
jaeger,What would be a proper way to avoid this hack?,0.216,0.0,0.784,-0.296
jaeger,Another implementation of TextMap?,0.0,0.0,1.0,0.0
jaeger,Which one?,0.0,0.0,1.0,0.0
jaeger,Update:,0.0,0.0,1.0,0.0
jaeger,"I found a way, but I'm not sure this is the right way:",0.196,0.0,0.804,-0.3491
jaeger,},0.0,0.0,0.0,0.0
jaeger,"After going through different blogs/docs, we came to know that we can skip agent and send spans to jaeger collector from client.",0.0,0.0,1.0,0.0
jaeger,But I am thinking of skipping collector and trying to send spans from jaeger agent to UDP server like fluentD.,0.0,0.153,0.847,0.5023
jaeger,Is it possible ?,0.0,0.0,1.0,0.0
jaeger,ANy pointers will be great help,0.0,0.63,0.37,0.7783
jaeger,I want to trace the whole project with Opencensus and Jaeger.,0.0,0.126,0.874,0.0772
jaeger,I added HTTP trace in entry services and add   stratspan  in middleware surrounded whol my services and this two-span called and show on Jaeger.,0.0,0.0,1.0,0.0
jaeger,My problem is each service contain a lot of function and I want see a trace of all my functions but in this way not show overall service not shown each function.,0.062,0.038,0.9,-0.1779
jaeger,I don't like add per function add one  stratspan .,0.232,0.0,0.768,-0.2755
jaeger,I use  ctx context.Context  entry all my function but not different!,0.0,0.0,1.0,0.0
jaeger,https://www.eclipse.org/che/docs/che-7/tracing-che/  has a number of  environment variables to set to enable tracing.,0.0,0.115,0.885,0.0772
jaeger,Is it possible to set these when working on OpenShift?,0.0,0.0,1.0,0.0
jaeger,If so; where would the correct place be; and where will the Jaeger interface be available?,0.0,0.0,1.0,0.0
jaeger,Thanks,0.0,1.0,0.0,0.4404
jaeger,I am getting the following error while creating a gateway for the sample bookinfo application,0.17,0.138,0.692,-0.128
jaeger,"Internal error occurred: failed calling admission webhook
  ""pilot.validation.istio.io"": Post
   https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s :
  Address is not allowed",0.333,0.0,0.667,-0.7184
jaeger,"I have created a EKS poc cluster using two node-groups (each with two instances), one with t2.medium and another one is with t2.large type of instances in my dev AWS account using two subnets with /26 subnet with default VPC-CNI provided by EKS",0.0,0.048,0.952,0.25
jaeger,"But as the cluster is growing with multiple services running, I started facing issues of IPs not available (as per docs default vpc-cni driver treat pods as an EC2 instance)",0.0,0.172,0.828,0.6808
jaeger,to avoid same I followed following post to change networking from default to weave,0.155,0.0,0.845,-0.296
jaeger,https://medium.com/codeops/installing-weave-cni-on-aws-eks-51c2e6b7abc8,0.0,0.0,1.0,0.0
jaeger,"because of same I have resolved IPs unavailability issue,",0.0,0.195,0.805,0.1779
jaeger,Now after network reconfiguration from vpc-cni to weave,0.0,0.0,1.0,0.0
jaeger,I am started getting above issue as per subject line for my service mesh configured using Istio,0.0,0.0,1.0,0.0
jaeger,"There are a couple of services running inside the mesh and also integrated kiali, prometheus, jaeger with the same.",0.0,0.0,1.0,0.0
jaeger,"I tried to have a look at Github ( https://github.com/istio/istio/issues/9998 ) and docs
( https://istio.io/docs/ops/setup/validation/ ), but could not get a proper valid answer.",0.0,0.0,1.0,0.0
jaeger,Let me if anyone face this issue and have partial/full solution on this.,0.0,0.161,0.839,0.3182
jaeger,My application (hosted in a Kubernetes cluster with Istio installed) does NOT propagate distributed tracing headers (as described  here ).,0.0,0.0,1.0,0.0
jaeger,"My expectation is that istio-proxy should still generate a trace (consisting of a single call) that would be visible in Jaeger, even though of course the entire chain of calls would not be stitched together.",0.0,0.0,1.0,0.0
jaeger,"However, that doesn't appear to be the case, as I'm not seeing any calls to my application in Jaeger.",0.0,0.0,1.0,0.0
jaeger,In attempt to troubleshoot I have tried the following:,0.0,0.205,0.795,0.2023
jaeger,"I have enabled tracing in Mixer's configuration, and I can now see Mixer's activity in Jaeger UI (but no traces of calls to my application still).",0.087,0.0,0.913,-0.296
jaeger,"I'm new to Istio, and it appears I have run out of option.",0.0,0.0,1.0,0.0
jaeger,"First off, is my expectation correct?",0.0,0.0,1.0,0.0
jaeger,Am I supposed to be seeing traces - each consisting of a single call - in Jaeger UI when the application doesn't propagate distributed tracing headers?,0.0,0.0,1.0,0.0
jaeger,"If my expectation is correct, how can I troubleshoot further?",0.0,0.184,0.816,0.2023
jaeger,Can I somehow verify Envoy configuration and check that it's indeed tracing data to Mixer?,0.0,0.0,1.0,0.0
jaeger,"If my expectation is incorrect, can Istio's behavior be overridden so that I get what I need?",0.0,0.0,1.0,0.0
jaeger,Thank you.,0.0,0.714,0.286,0.3612
jaeger,I log application tracing informations using Jaeger.,0.0,0.0,1.0,0.0
jaeger,Do I need to use other log package again?,0.0,0.0,1.0,0.0
jaeger,"I've created an elasticsearch service to apply it like backend to jaeger tracing, using this  guide , all over Kubernetes GCP cluster.",0.0,0.191,0.809,0.5423
jaeger,I have the elasticsearch service:,0.0,0.0,1.0,0.0
jaeger,And their respective pod called elasticsearch-0,0.0,0.359,0.641,0.4215
jaeger,"I've enter to my pod configuration on GCP, and I can see that my elasticsearch-0 pod have limited resources:",0.101,0.0,0.899,-0.2263
jaeger,"And then, I want to assign it specific CPU request and CPU limit  according to the documentation , and then, I proceed to modufy the pod manifest, adding the following directives:",0.0,0.046,0.954,0.0772
jaeger,"- cpu ""2""  in the  args  section:",0.0,0.0,1.0,0.0
jaeger,"And I am including a  resources:requests  field in the container resource, in order to specify a request of 0.5 CPU and  I've include a  resources:limits  in order to specify a CPU limit of this way:",0.0,0.0,1.0,0.0
jaeger,"My complete pod manifest is this (See numerals 1,2,3,4 and 5 numerals commented with # symbol):",0.0,0.0,1.0,0.0
jaeger,"But when I apply my pod manifest file, I get the following output:",0.0,0.0,1.0,0.0
jaeger,The complete output of my  kubectl apply  command is this:,0.0,0.0,1.0,0.0
jaeger,How to can I modify my pod yaml file in order to assign it more resources and solve the  kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu request for container elasticsearch'  message?,0.0,0.071,0.929,0.2648
jaeger,What is the difference between  span.kind=server  and  span.kind=client in terms of OpenTracing?,0.0,0.0,1.0,0.0
jaeger,How do I know which one to pick?,0.0,0.0,1.0,0.0
jaeger,What does exactly it mean?,0.0,0.0,1.0,0.0
jaeger,E.g.,0.0,0.0,1.0,0.0
jaeger,there is my service Foo which is initially called by an external service Bar.,0.0,0.0,1.0,0.0
jaeger,I start tracing on my Foo side and the logic is to call another service Buzz in my system to continue the flow chain.,0.0,0.0,1.0,0.0
jaeger,"I would rather assume that external service Bar is a  client , but I can't start my trace from there.",0.0,0.0,1.0,0.0
jaeger,What would you suggest - start tracing as  client  in my service Foo and then just consider all following services as servers?,0.0,0.0,1.0,0.0
jaeger,"Setup: k8s microservices, jaeger metrics.",0.0,0.0,1.0,0.0
jaeger,I already have some services in my k8s cluster and want to mantain them separately.,0.0,0.091,0.909,0.0772
jaeger,Examples:,0.0,0.0,1.0,0.0
jaeger,Is it possible to use existing instances instead of creating istio-specific ones?,0.0,0.167,0.833,0.296
jaeger,Can istio communicate with them or it's hardcoded?,0.0,0.0,1.0,0.0
jaeger,"I have deployed ingress-nginx helm chart ( 3.20.1 ,  https://github.com/kubernetes/ingress-nginx/tree/master/charts/ingress-nginx  ) into k8s cluster.",0.0,0.0,1.0,0.0
jaeger,Some of the Ingresses configured for applications use basic auth - which works as expected.,0.0,0.0,1.0,0.0
jaeger,I.e.,0.0,0.0,1.0,0.0
jaeger,"I can access some applications in the cluster without basic auth ( as configured )
and some applications require dedicated basic auth credentials ( as configured ).",0.0,0.12,0.88,0.4588
jaeger,But trying an unknown/unsupported URL/Path is handled awkward by default-backend.,0.174,0.0,0.826,-0.2263
jaeger,Somehow the default backend is now requiring basic auth too ( as one of the ingress with basic-auth ) ?,0.0,0.0,1.0,0.0
jaeger,How can I find out why/where this basic auth kicks in when it shouldn't ?,0.0,0.0,1.0,0.0
jaeger,I do want default-backend to serve error pages for 404 without basic auth !,0.209,0.091,0.7,-0.4003
jaeger,Currently deployed Ingresses:,0.0,0.0,1.0,0.0
jaeger,E.g.,0.0,0.0,1.0,0.0
jaeger,trying a simple curl on an unknown path ( where I would expect default backend to given 404 response ) returns data that is coming from my Prometheus endpoint being basic auth protected :,0.0,0.094,0.906,0.4404
jaeger,(The real hostname/ip has been obfuscated by me !),0.0,0.0,1.0,0.0
jaeger,or similar we trying on the other host I'll get 401 from something that looks like my Jaeger endpoint instead of default-backend 404:,0.0,0.102,0.898,0.3612
jaeger,I have a web application APPA that calls a webservice  master  that calls another webservice  slave .,0.0,0.0,1.0,0.0
jaeger,"To corelate the traces, I'm using opentelemetry.",0.0,0.0,1.0,0.0
jaeger,"When  master  calls  slave , I can see the traces in Jaeger, but when the web application calls  master  I see that only the web application is called and no traces are corelated with the webservice and I can see separatly the traces from the 2 webservies correlated:",0.062,0.0,0.938,-0.4215
jaeger,"but when I cann the web application, I see only the traces from the web application and not all the 3 components:",0.0,0.0,1.0,0.0
jaeger,Here is what I am doing in the web application:,0.0,0.0,1.0,0.0
jaeger,What am I doing wrong in corellating all 3 components?,0.307,0.0,0.693,-0.4767
jaeger,"I have a Apache Camel application with an  Aggregate EIP , which uses a  JdbcAggregationRepository  to persist the exchanges.",0.0,0.0,1.0,0.0
jaeger,I also use the  OpenTracing component  with Jaeger to trace my application.,0.0,0.0,1.0,0.0
jaeger,But after aggregation the parent span of the following span is wrong.,0.274,0.0,0.726,-0.631
jaeger,Jaeger UI,0.0,0.0,1.0,0.0
jaeger,The span of route with direct  output  should be a child of one of the spans of route with direct  aggregate .,0.0,0.0,1.0,0.0
jaeger,Spring Boot application,0.0,0.0,1.0,0.0
jaeger,Logs,0.0,0.0,1.0,0.0
jaeger,See also  Trace/Span Identity .,0.0,0.0,1.0,0.0
jaeger,Research,0.0,0.0,1.0,0.0
jaeger,"If I remove  .to(file(&quot;d:/tmp/backup&quot;))  from my route, I also lose the trace ID.",0.213,0.0,0.787,-0.4019
jaeger,The span for the route with direct  output  uses a new trace ID.,0.0,0.0,1.0,0.0
jaeger,"Therefore, it is seperated from the other spans.",0.0,0.0,1.0,0.0
jaeger,Question,0.0,0.0,1.0,0.0
jaeger,How can I preserve the parent span after an aggregation?,0.0,0.0,1.0,0.0
jaeger,in a single application i can easily create (nested) spans but i am trying to trace http requests throughout multiple services and nothing i try with context propagation is working.,0.0,0.119,0.881,0.3071
jaeger,so maybe my setup is wrong.,0.383,0.0,0.617,-0.4767
jaeger,can someone please explain the exact requirements for this in python?,0.0,0.187,0.813,0.3182
jaeger,"so as far as i understand, in each microservice i have to setup my span exporter, the collector and trace providers.",0.0,0.0,1.0,0.0
jaeger,"after that is done in the classes that i want to instrument, i would just navigate to the block of code in my service that i wanna trace:",0.107,0.048,0.846,-0.3818
jaeger,when i start my service the traces work perfectly and i can see them on my Jaeger UI but the spans are not aggregated under a single trace for that one individual http request.,0.0,0.08,0.92,0.3818
jaeger,"I know there should be some context propagation in there to achieve this, but i can't find any way to get this done in python.",0.0,0.0,1.0,0.0
jaeger,I'm using the W3C ContextTrace but i can't get it to work and i'm not really sure if i should setup a trace provider in every service or what is wrong exactly.,0.204,0.0,0.796,-0.7856
jaeger,I've read a lot of documentation but i still don't know how to get this to work.,0.0,0.0,1.0,0.0
jaeger,I'm attempting to configure the open telemetry collector in Kubernetes.,0.0,0.0,1.0,0.0
jaeger,I took the jaeger all in one deployment which is here:  https://www.jaegertracing.io/docs/1.22/opentelemetry/  and ported it to kubernete running on my minikube.,0.0,0.0,1.0,0.0
jaeger,The problem is I can't seem to get the open telemetry collector to receive the jaeger traces and send it to my proxy container.,0.109,0.0,0.891,-0.4019
jaeger,My jaeger all in one app seems to be working in my minikube instance.,0.0,0.0,1.0,0.0
jaeger,Traces are being sent through the hot rap app and I can view the traces in the jaeger UI.,0.0,0.0,1.0,0.0
jaeger,My open telemetry collector looks like the following:,0.0,0.263,0.737,0.3612
jaeger,It doesn't seem that the open-tel collector is even receiving the jaeger traces.,0.0,0.0,1.0,0.0
jaeger,The logs from the container are below..,0.0,0.0,1.0,0.0
jaeger,Even when I send a ton of jaeger traces nothing ever seems to be received by the collector.,0.0,0.0,1.0,0.0
jaeger,Is there a way to debug further or a configuration I'm missing?,0.196,0.0,0.804,-0.296
jaeger,Any help would be greatly appreciated.,0.0,0.611,0.389,0.7425
jaeger,I am trying to implement a solution to the heat equation (in 1-D) utilizing Python's Fast Fourier Transform (FFT).,0.0,0.126,0.874,0.3182
jaeger,The objective is an efficient numerical scheme to account for a time-dependent temperature specified at the boundary of a semi-infinite domain.,0.0,0.135,0.865,0.4215
jaeger,"This is known in the heat-transfer literature as Duhamel's problem (see, e.g., Carslaw and Jaeger, 1959, sec.",0.144,0.0,0.856,-0.4019
jaeger,2.5).,0.0,0.0,1.0,0.0
jaeger,An exact solution is known for the forcing at the boundary specified as a function of time.,0.0,0.133,0.867,0.3182
jaeger,"In the present case, I seek the response to a boundary history given as discrete data collected at a regular interval.",0.0,0.0,1.0,0.0
jaeger,"Taking the Fourier transform of the heat equation and the appropriate boundary conditions
( !",0.0,0.0,1.0,0.0
jaeger,[equation] https://latex.codecogs.com/gif.download?T%280%2Ct%29%20%3D%20f%28t%29%3B%20%5Clim_%7Bx%20%5Crightarrow%20%5Cinfty%7D%20T%28x%2Ct%29%20%3D%200  ) and initial condition ( !,0.0,0.0,1.0,0.0
jaeger,"[equation] https://latex.codecogs.com/gif.download?T%28x%2C0%29%20%3D%200  ) with respect to time, the problem has a solution given by !",0.155,0.327,0.517,0.4574
jaeger,"[equation] https://latex.codecogs.com/gif.download?%5Ctilde%7BT%7D%20%3D%20%5Ctilde%7Bf%7D%28%5Calpha%29%20%5Cexp%20%5B-%28i%20%5Calpha%20/%20%5Ckappa%29%5E%7B1/2%7D%20x%5D  , where the tilde indicates the Fourier transform of the function, alpha is the transform variable (frequency), and kappa is the thermal diffusivity.",0.0,0.0,1.0,0.0
jaeger,"So, the scheme is to take the FFT of the boundary data, multiply by
!",0.0,0.0,1.0,0.0
jaeger,"[equation] https://latex.codecogs.com/gif.download?%5Cexp%20%5B%20-%20%28i%20%5Calpha%20/%20%5Ckappa%29%5E%7B1/2%7D%20x%5D  , and invert (using IFFT) to obtain T.",0.0,0.0,1.0,0.0
jaeger,I have constructed a benchmark problem with which to validate my coding.,0.205,0.189,0.606,-0.0516
jaeger,"In particular, I consider the forcing function !",0.0,0.0,1.0,0.0
jaeger,"[equation] https://latex.codecogs.com/gif.download?f%28t%29%20%3D%20a%20%5Cexp%20%5B%20-%20b%5E2%20%28t-t_0%29%5E2%5D  , where a, b, and t_0 are fixed constants.",0.0,0.0,1.0,0.0
jaeger,This function can be put into the classical Duhamel solution and the result can be evaluated by numerical integration.,0.0,0.113,0.887,0.3182
jaeger,"In addition, the FT of this function has a simple, closed form, which can be substituted into the solution given in the foregoing paragraph, and the FT of the solution can be inverted by direct numerical integration.",0.0,0.119,0.881,0.5574
jaeger,"Results of these two independent evaluations are identical, verifying the FT solution given above.",0.0,0.15,0.85,0.3182
jaeger,I have attempted to code this problem for the above forcing function given by discrete data sampled at a regular interval of time.,0.135,0.0,0.865,-0.481
jaeger,"I call Python's FFT routine to generate
the FT of the forcing function from the data.",0.0,0.0,1.0,0.0
jaeger,"Then, I multiply by the decaying exponential in x, and finally call IFFT to invert for the temperature history at a given fixed value of x.",0.1,0.089,0.812,-0.0772
jaeger,"The code returns values that exhibit some of the right qualitative behavior (e.g., delayed arrival of the peak temperature;  temperature tailing off at late time), but is way off both quantitatively (e.g., temperature much too high) and qualitatively (e.g., temperature at early time is not asymptotically zero).",0.03,0.038,0.932,0.1027
jaeger,"I am clearly doing something wrong in my coding of this problem, but I have been unable to identify the issue.",0.187,0.084,0.729,-0.3099
jaeger,A possible error may be found in the way I specify the frequency in the decaying exponential.,0.293,0.0,0.707,-0.6597
jaeger,I would greatly appreciate any help that someone more familiar with the use of Python's FFT and IFFT can offer.,0.0,0.259,0.741,0.7146
jaeger,My code follows:,0.0,0.0,1.0,0.0
jaeger,We have a Nodejs based microservices running in our on-prem kubernetes v1.19 with Istio v1.8.0.,0.0,0.0,1.0,0.0
jaeger,What I would like to achieve is trace or display the external API calls in Kiali where we have Jaeger clients for each microservices and able to trace internal traffics.,0.0,0.082,0.918,0.3612
jaeger,But so far I could not able to trace any external API calls hits from any microservices.The only thing that I can see the traffic for proxy in Kiali's graph overview.,0.0,0.0,1.0,0.0
jaeger,"We have a cooperate proxy, and each container have env proxies set for both http_proxy, https_proxy.Any external service accessible via a cooperate proxy thus traffics should go through the our cooperate proxy first.",0.0,0.0,1.0,0.0
jaeger,We have a secured gateway with TLS and we do not have egressgateway where only have istio-ingressgateway.,0.0,0.153,0.847,0.4019
jaeger,So is there anyway to trace external traffics likewise the internal traffics inside cluster?If yes what might be the missing thing?,0.091,0.126,0.783,0.2168
jaeger,Here are the ServiceEntries and VirtualServices that I created where I would like to use the retry feature as well the calls for proxy and externalAPI,0.0,0.239,0.761,0.6808
jaeger,I am trying to use OpenCensus and Linkerd.,0.0,0.0,1.0,0.0
jaeger,"Though Linkerd has an option to automatically provision OpenCensus and jaeger in its namespace, I don't want to use them.",0.064,0.0,0.936,-0.0572
jaeger,"Instead, I deployed them independently by myself under the namespace named 'ops'.",0.0,0.0,1.0,0.0
jaeger,"At the end (exactly 4th line from the last) of the the official  docs , it says,",0.0,0.0,1.0,0.0
jaeger,Ensure the OpenCensus collector is injected with the Linkerd proxy.,0.0,0.224,0.776,0.3818
jaeger,What does this mean?,0.0,0.0,1.0,0.0
jaeger,Should I inject linkerd sidecar into OpenCensus collector pod?,0.0,0.0,1.0,0.0
jaeger,"If so, why?",0.0,0.0,1.0,0.0
jaeger,"For example, let's say I've configured the default namespace like this.",0.0,0.2,0.8,0.3612
jaeger,"my-opencensus-collector  is in  ops  namespace, so I put  .ops  at the end of its service name, resulting  my-opencensus-collector.ops:12345 .",0.0,0.0,1.0,0.0
jaeger,"And the dedicated service account for the OpenCensus collector exists in  ops  namespace, too.",0.0,0.188,0.812,0.4588
jaeger,"In this case, should I put the namespace name at the end of service account name as well?",0.0,0.116,0.884,0.2732
jaeger,Which one would be right?,0.0,0.0,1.0,0.0
jaeger,or,0.0,0.0,1.0,0.0
jaeger,Thanks!,0.0,1.0,0.0,0.4926
jaeger,"I am trying to send 2 different microservice data to my web service using open telemetry-javaagent ,one with jaeger exporter and the other with otlp, it looks like all the traces of jaeger are sent successfully and otlp are dropped as my web service only support HTTP protocol And javaagent has otlp grpc exporter.",0.0,0.146,0.854,0.8126
jaeger,I am not able to find any other agent which would exporter data using otlp in HTTP format.,0.0,0.0,1.0,0.0
jaeger,are there any references through which I can get such an agent,0.0,0.0,1.0,0.0
jaeger,"Everyone,",0.0,0.0,1.0,0.0
jaeger,I have the following inquiry:,0.0,0.0,1.0,0.0
jaeger,"Java 11 is used along with Spring-boot, Cassandra 3.4, JRPC and Jaeger.",0.0,0.0,1.0,0.0
jaeger,"All queries to the database are made solely by Spring, with annotation.",0.0,0.0,1.0,0.0
jaeger,"Thus, there are no further queries made, clusters or sessions in the code.",0.155,0.0,0.845,-0.296
jaeger,How to track Cassandra with Jaeger?,0.0,0.0,1.0,0.0
jaeger,very new to .net and this seems like a simple questions.,0.0,0.217,0.783,0.3612
jaeger,Trying to get this code to work from the Jaeger website:  https://ocelot.readthedocs.io/en/latest/features/tracing.html,0.0,0.0,1.0,0.0
jaeger,in the startup.cs file under the ConfigureServices.,0.0,0.0,1.0,0.0
jaeger,"I have added the proper references, but am getting an error with the:",0.244,0.0,0.756,-0.5499
jaeger,Visual studio keeps complaining:,0.375,0.0,0.625,-0.2023
jaeger,'Configuration' is a namespace but is used like a type,0.0,0.317,0.683,0.5023
jaeger,Any ideas on how to fix.,0.0,0.0,1.0,0.0
jaeger,Here is what I have in startup so far:,0.0,0.0,1.0,0.0
jaeger,I have a  Jaeger  running in a docker container in my local machine.,0.0,0.0,1.0,0.0
jaeger,I've created a sample app which sends trace data to Jaeger.,0.0,0.182,0.818,0.25
jaeger,"When running from the IDE, the data is sent perfectly.",0.0,0.318,0.682,0.6369
jaeger,"I've containerized my app, and now I'm deploying it as a container, but the communication only works when I use  --link jaeger  to link both containers (expected).",0.0,0.0,1.0,0.0
jaeger,My question is:,0.0,0.0,1.0,0.0
jaeger,"Is there a way of adding the  --link  parameter within my Dockerfile, so then I don't need to specify it when running the  docker run  command?",0.0,0.0,1.0,0.0
jaeger,I'm looking for the best strategy to deploy Jaegertracing on AWS.,0.0,0.296,0.704,0.6369
jaeger,"My goal is to trace my Lambda functions (round about 10), which send the spans directly to the Collector over HTTP.",0.0,0.0,1.0,0.0
jaeger,Therefore I don't need the Jaeger-Agent.,0.0,0.0,1.0,0.0
jaeger,My question is now how I should deploy the jaeger-query and collector to AWS.,0.0,0.0,1.0,0.0
jaeger,Should I rather deploy them via ECS or via an EKS-Cluster (Jaeger Operator) or is there another solution I didn't have mentioned yet?,0.0,0.103,0.897,0.3182
jaeger,What is the advantage of Kubernetes over ECS in this case?,0.0,0.167,0.833,0.25
jaeger,Thanks in advance for your feedback!,0.0,0.39,0.61,0.4926
jaeger,"I want to use  static target  for jaeger instead of linking to a dynamic target,
static target compiles fine but when I use it in my codebase, I see  undefined reference errors :
tried:",0.096,0.134,0.77,-0.1901
jaeger,"on  using **nm -uC libjaegertracing.a**  gives  hint for missing definitions , but I don't know why it is not getting included in the static file, while when I link to static target for example code(standalone) not with code I want to link it with, everything works fine.",0.065,0.049,0.886,0.0688
jaeger,I want to know how can I know and include missing library/definition so that I don't get these errors while trying to use a manually created target with libjaegertracing.a,0.077,0.187,0.736,0.2815
jaeger,"also to verify whether I had the dependency library compiled to build static library, I used:",0.0,0.0,1.0,0.0
jaeger,which gave the complied files as expected:,0.0,0.0,1.0,0.0
jaeger,"target that builds fine: [works]
for adding dependency:",0.0,0.205,0.795,0.2023
jaeger,for adding source files:,0.0,0.0,1.0,0.0
jaeger,linking with example code:,0.0,0.0,1.0,0.0
jaeger,with source code  [ where it doesn't work],0.0,0.0,1.0,0.0
jaeger,then:,0.0,0.0,1.0,0.0
jaeger,"TLDR : How can we ignore sending traces to jaeger (or zipkin) where the header is ""get /**"", and if that is not possible ignore sending traces where  ""mvc.controller.class"" is ""RequestHttpRequestHandler""",0.152,0.0,0.848,-0.6124
jaeger,"We have a app which uses spring sleuth, in this app we have some requests sent to jaeger which result in get the header as ""get /**"" and the controller for these cases are ""RequestHttpRequestHandler""",0.0,0.0,1.0,0.0
jaeger,"However it seems that we cant use a simply ""**"" path filter and after debugging the spring code a bit each specific pattern comes up in the skip section as follows",0.0,0.0,1.0,0.0
jaeger,In my Rails app I'm trying to set up auto-instrumentation with SignalFx using  signalfx-ruby-tracing  gem which is using  jaeger-client-ruby .,0.0,0.0,1.0,0.0
jaeger,In the output of  rails sever  I'm getting this error every 30 seconds or so:,0.182,0.0,0.818,-0.481
jaeger,"ERROR -- : Failure while sending a batch of spans: ""\x81"" from
  ASCII-8BIT to UTF-8",0.38,0.0,0.62,-0.7739
jaeger,which comes from  this part of code .,0.0,0.0,1.0,0.0
jaeger,The traces are sent but I'd like to know why does this error happen because it might impact the quality of the monitoring.,0.147,0.114,0.738,-0.2353
jaeger,Thanks!,0.0,1.0,0.0,0.4926
jaeger,My pods in my kubernetes cluster crashing after startup.,0.0,0.0,1.0,0.0
jaeger,"They are in a separate namespace (not default)
I am using microk8s and tilt",0.0,0.0,1.0,0.0
jaeger,"*2020-04-21 22:38:48.766  INFO 8 --- [           main] o.s.web.context.ContextLoader 
        : Root WebApplicationContext: initialization completed in 29903 ms",0.0,0.0,1.0,0.0
jaeger,xargs: java: terminated by signal 9,0.0,0.0,1.0,0.0
jaeger,[K8s EVENT: Pod com-config-6867587f48-9zsxs (ns: common-dev)] Back-off restarting failed container,0.268,0.0,0.732,-0.5106
jaeger,[K8s EVENT: Pod com-config-6867587f48-9zsxs (ns: common-dev)] Back-off restarting failed container,0.268,0.0,0.732,-0.5106
jaeger,warte auf debugger: n,0.0,0.0,1.0,0.0
jaeger,Listening for transport dt_socket at address: 5005*,0.0,0.0,1.0,0.0
jaeger,My microk8s.status,0.0,0.0,1.0,0.0
jaeger,"dashboard: enabled
dns: enabled
ingress: enabled
registry: enabled
storage: enabled
cilium: disabled
fluentd: disabled
gpu: disabled
helm: disabled
helm3: disabled
istio: disabled
jaeger: disabled
knative: disabled
kubeflow: disabled
linkerd: disabled
metallb: disabled
metrics-server: disabled
prometheus: disabled
rbac: disabled",0.0,0.0,1.0,0.0
jaeger,Any suggestions why my pods don´t get a signal and Signal 9 comes up?,0.0,0.0,1.0,0.0
jaeger,"(Port-Forwarding is set, Firewall rules are set)",0.0,0.0,1.0,0.0
jaeger,I have a c++ program which using Jaeger for tracing,0.0,0.0,1.0,0.0
jaeger,"I compile it using 
 g++ -std=c++1z test.cpp -I /usr/local/lib/ -ljaegertracing -lyaml-cpp  where
 /usr/local/lib/libyaml-cpp.a  is the installation path.",0.0,0.0,1.0,0.0
jaeger,Error message -,0.73,0.0,0.27,-0.4019
jaeger,"I have installed  yaml-cpp-0.6.0  by downloading source code  .tar  version did  mkdir build , cd build , sudo make , sudo make install",0.0,0.0,1.0,0.0
jaeger,I dont know why my compilation is failing.,0.355,0.0,0.645,-0.5106
jaeger,I have  libyaml-cppd.so.0.6  in  yaml-cpp/build  directory and tried this path to compile but still it is failing.,0.229,0.0,0.771,-0.6652
jaeger,OS - ubuntu 18.04,0.0,0.0,1.0,0.0
jaeger,I'm trying to install the prometheus operator and inject using the sidecar.,0.0,0.0,1.0,0.0
jaeger,Mutual TLS is turned on and works okay for Jaeger.,0.0,0.174,0.826,0.2263
jaeger,For the operator though we get a failure on the oper-admission job (see image).,0.216,0.0,0.784,-0.5106
jaeger,"I believe Istio is causing this as if I release prometheus-operator prior to istio or without istio it works okay, but then it isn't injected.",0.0,0.062,0.938,0.1154
jaeger,I've tried setting the following in the istio operator sidecar settings:,0.0,0.0,1.0,0.0
jaeger,I've also tried to extend the readinessInitialDelaySeconds to 10s but still get the error.,0.21,0.08,0.71,-0.4939
jaeger,Does anyone else have any ideas?,0.0,0.0,1.0,0.0
jaeger,I am trying to use instrumentation in Go with Jaeger.,0.0,0.0,1.0,0.0
jaeger,I am running the Jaeger backend with docker like this (as explained in  https://www.jaegertracing.io/docs/1.15/getting-started/ ):,0.162,0.145,0.694,-0.0772
jaeger,"However, after running the following Go code, I am not able to see spans in the Jaeger UI at  http://localhost:16686  and I am not sure what's wrong with this code?",0.064,0.084,0.852,0.1511
jaeger,I started from a similar piece of Python code and that is able to publish spans on the Jaeger UI.,0.0,0.0,1.0,0.0
jaeger,I am digging in the docs here  https://godoc.org/github.com/uber/jaeger-client-go  and the ones for the open tracing project here  https://godoc.org/github.com/opentracing/opentracing-go  but I am a bit confused by the jargon and the library functions/methods.,0.098,0.0,0.902,-0.4497
jaeger,I'm trying out the  Jaeger/OpenTracing tutorial  and finding that none of my changes to the HotROD application code have any effect.,0.0,0.0,1.0,0.0
jaeger,The project structure is something like (abridged):,0.0,0.294,0.706,0.3612
jaeger,I start the application by running  go run main.go all .,0.0,0.0,1.0,0.0
jaeger,"It behaves as expected, the traces on Jaeger all match the screenshots on Medium.",0.0,0.0,1.0,0.0
jaeger,I edit  services/config/config.go  to change the RouteWorkerPoolSize and MySQLGetDelay variables as directed.,0.0,0.0,1.0,0.0
jaeger,Then stop the server and start it again with  go run main.go all,0.155,0.0,0.845,-0.296
jaeger,"I'd expect these changes to be reflected in the newly running server, but they aren't.",0.0,0.0,1.0,0.0
jaeger,The behaviour is the exact same as before.,0.0,0.0,1.0,0.0
jaeger,It's like go is running the old code.,0.0,0.263,0.737,0.3612
jaeger,Am I misunderstanding something about  go run ?,0.359,0.0,0.641,-0.4215
jaeger,Environment variables:,0.0,0.0,1.0,0.0
jaeger,working directory:,0.0,0.0,1.0,0.0
jaeger,Go version 1.12.6 running on Kubuntu 18.04,0.0,0.0,1.0,0.0
jaeger,I am using Jaeger opentracing in an instrumented standalone non-spring java app.,0.0,0.0,1.0,0.0
jaeger,Does opentacing/Jaeger expose any config or api or any other mechanism to disable it globally?,0.103,0.0,0.897,-0.1531
jaeger,Which mechanism are you using to enable/disable opentracing if you are in the same boat?,0.0,0.0,1.0,0.0
jaeger,I successfully added Apache Camel's  OpenTracing  component to my application.,0.0,0.286,0.714,0.4939
jaeger,I can see traces in Jaeger UI.,0.0,0.0,1.0,0.0
jaeger,But the traces for the  RabbitMQ  component show only the exchange name without the routing key as operation name.,0.0,0.0,1.0,0.0
jaeger,"Because of my application uses only one exchange with different routing keys, I need to see the routing key as operation name in my traces.",0.0,0.0,1.0,0.0
jaeger,Research,0.0,0.0,1.0,0.0
jaeger,"With  OpenTracing Spring RabbitMQ  I could expose another customized  RabbitMqSpanDecorator , see  Span decorator :",0.127,0.0,0.873,-0.1531
jaeger,Note: you can customize your spans by declaring an overridden  RabbitMqSpanDecorator  bean.,0.0,0.0,1.0,0.0
jaeger,"(However, I coulnd't change the operation name with the  RabbitMqSpanDecorator  at all, because the operation name is hard coded to  producer  or  consumer .)",0.06,0.0,0.94,-0.1027
jaeger,Unfortunately Apache Camel uses its own different implementation of a  RabbitmqSpanDecorator  to decorate spans.,0.167,0.0,0.833,-0.34
jaeger,"I wrote a custom class by overiding Apache Camel's  RabbitmqSpanDecorator , but my custom class wasn't used.",0.0,0.0,1.0,0.0
jaeger,Question,0.0,0.0,1.0,0.0
jaeger,How can I change the operation name of a span with Apache Camel OpenTracing component for Apache Camel RabbitMQ component?,0.0,0.0,1.0,0.0
jaeger,I have an application which is made up of multiple services built in proprietary language.,0.0,0.0,1.0,0.0
jaeger,I want to collect traces and ingest into Jaeger or APM solution.,0.0,0.286,0.714,0.3818
jaeger,There is no instrumentation library.,0.355,0.0,0.645,-0.296
jaeger,"However, these services produce traces in a proprietary format.",0.0,0.0,1.0,0.0
jaeger,I want to convert these traces to OpenTelemetry traces and ingest into Jaeger or APM solution.,0.0,0.217,0.783,0.3818
jaeger,I started using OpenTelemetry-Java SDK.,0.0,0.0,1.0,0.0
jaeger,Configured appropriate exporter.,0.0,0.0,1.0,0.0
jaeger,I can see the traces in Jeager as expected.,0.0,0.0,1.0,0.0
jaeger,BUT it shows only one service.,0.0,0.0,1.0,0.0
jaeger,Whereas I want to depict two different services.,0.0,0.178,0.822,0.0772
jaeger,Is that possible using OpenTelemetry?,0.0,0.0,1.0,0.0
jaeger,I am implementing service to service integration that using spring webflux.,0.0,0.0,1.0,0.0
jaeger,Each microservice is isolated and running a different port.,0.247,0.0,0.753,-0.3182
jaeger,I would like to see an end to end trace using jaeger.,0.0,0.2,0.8,0.3612
jaeger,The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.,0.089,0.0,0.911,-0.2144
jaeger,Preference-service  is receiving the request and forwarding  customer-service .,0.0,0.0,1.0,0.0
jaeger,Using sleuth and zipkin for collecting trace,0.0,0.0,1.0,0.0
jaeger,Preference service code part,0.0,0.0,1.0,0.0
jaeger,Preference service prop file,0.0,0.0,1.0,0.0
jaeger,Customer-service code part,0.0,0.0,1.0,0.0
jaeger,Customer service prop file,0.0,0.0,1.0,0.0
jaeger,Jaeger is running in the docker-compose file,0.0,0.0,1.0,0.0
jaeger,"Preference service trace in Jaeger  
 Customer service trace in Jaeger",0.0,0.0,1.0,0.0
jaeger,"Tracing makes finding parts in code, worthwhile a developers time and attention, much easier.",0.0,0.321,0.679,0.6369
jaeger,"For that reason, I attached  Jaeger  as tracer to a set of microservices inside Docker containers.",0.0,0.0,1.0,0.0
jaeger,I use  Traefik  as ingress controller/ service-mesh to route and proxy requests.,0.0,0.0,1.0,0.0
jaeger,"The problem I am facing is, that something's wrong with the  tracing  config in Traefik.",0.326,0.0,0.674,-0.7003
jaeger,Jaeger can not find the span context to connect the single/ service-dependend spans to a whole trace.,0.0,0.0,1.0,0.0
jaeger,The following line appears in the logs:,0.0,0.0,1.0,0.0
jaeger,The following snippets describe the  Docker Compose  setup.,0.0,0.0,1.0,0.0
jaeger,This is a stripped down version of the traefik Container.,0.0,0.0,1.0,0.0
jaeger,Traefik is set up using the  file provider  as base and Docker Compose labels on top of it:,0.0,0.096,0.904,0.2023
jaeger,We would like to use  OpenTelemetry  to collect information about our running servers.,0.0,0.172,0.828,0.3612
jaeger,"But our primry focus is not so much about metrics per se, but actually more about the metadata.",0.0,0.0,1.0,0.0
jaeger,"So we would like to know which version of an application is installed (on QS, Prod, ...), which .Net assembly versions are used, which RabbitMQ messages are provided and which are consumed, REST Endpoints, gRPC-Endpoints, ...",0.0,0.073,0.927,0.4144
jaeger,I assume that all of the above are valid sources for OpenTelemetry metrics?,0.0,0.0,1.0,0.0
jaeger,Can I use OpenTelemetry to create this sort of infrastructure registry?,0.0,0.189,0.811,0.2732
jaeger,Actual distributed traces and metrics are as of now actually of second concern.,0.0,0.0,1.0,0.0
jaeger,What would be my GUI?,0.0,0.0,1.0,0.0
jaeger,"Zipkin, Jaeger, another tool?",0.0,0.0,1.0,0.0
jaeger,"My application extensively uses  CompletableFuture.supplyAsync(() -&gt; someService(context, args));  &amp; we rely on supplyAsync to use  ForkJoinPool.commonPool()  thread pool to get the service run in its own thread.",0.0,0.0,1.0,0.0
jaeger,Is there a way to instrument someService call in open tracing without passing in a custom  Executor  as argument to supplyAsync() ?,0.122,0.0,0.878,-0.3612
jaeger,I'm using spring and jaeger and have the below dependency,0.0,0.0,1.0,0.0
jaeger,I have recently changed my Quarkus application from RestEasy to  Reactive Routes  to implement my HTTP endpoints.,0.0,0.0,1.0,0.0
jaeger,My Quarkus app had OpenTracing enabled and it was working fine.,0.0,0.153,0.847,0.2023
jaeger,After changing the HTTP resource layer I can not see any trace in Jaeger.,0.0,0.0,1.0,0.0
jaeger,After setting log level in DEBUG I can see my application is registered in Jaeger but I don't see any traceId or spanId in logs neither traces in Jaeger:,0.0,0.0,1.0,0.0
jaeger,I'm using the latest version of Quarkus which is 1.9.2.Final.,0.0,0.0,1.0,0.0
jaeger,Is it enabled OpenTracing when I'm using Reactive Routes?,0.0,0.0,1.0,0.0
jaeger,Thanks for building this marvelous library and sample app.,0.0,0.518,0.482,0.8188
jaeger,I struggled with a lot of libraries but ended up using this one.,0.145,0.0,0.855,-0.1779
jaeger,I have already started spreading the word in my circle!,0.0,0.0,1.0,0.0
jaeger,I want to instrument Oatpp CRUD service using  Opentracing CPP  (with  Jaeger tracer  in place).,0.0,0.091,0.909,0.0772
jaeger,I have already  integrated  these into CRUD service and that is working well.,0.0,0.16,0.84,0.2732
jaeger,"However, I want to call CRUD service from a Java app and extract the TextMapWriter so that the the traces from Java app and CRUD service could be correlated.",0.0,0.048,0.952,0.0772
jaeger,Do you mind suggesting some sample code to achieve this?,0.0,0.0,1.0,0.0
jaeger,I know how to pass in required params from Java side.,0.0,0.0,1.0,0.0
jaeger,Here's the sample Java code to inject the required headers in GET request to CRUD service.,0.0,0.0,1.0,0.0
jaeger,This is only an abstract to give you an idea of how the injection is done.,0.0,0.0,1.0,0.0
jaeger,I have put in certain &quot;imports&quot; to the class names so that you know where each reference is coming from.,0.0,0.104,0.896,0.2732
jaeger,},0.0,0.0,0.0,0.0
jaeger,I'm trying to set up a Jaeger datasource in grafana cloud.,0.0,0.0,1.0,0.0
jaeger,"In Jaeger datasource page - URL field is empty, my question is: Where can I find the required URL?",0.101,0.0,0.899,-0.2023
jaeger,"I have tried to write an imaginary URL, for test purpose, and when I clicked on 'Save &amp; Test' I've got no error feedback, but when I tried to pick Jaeger datasource in explore page I've got 'Failed to load services from Jaeger.",0.172,0.046,0.783,-0.7003
jaeger,Failed to fetch'.,0.623,0.0,0.377,-0.5106
jaeger,"So I'm confused, can someone help me understand which URL should I use and where can I find it?",0.127,0.133,0.74,0.0314
jaeger,(I'm using grafana cloud),0.0,0.0,1.0,0.0
jaeger,Setting up jaeger tracing...So previously I had a binary executable (jaeger-agent) running on a Linux CentOS 8 box alongside a server side application.,0.0,0.0,1.0,0.0
jaeger,The tracing spans were being sent to the jaeger-collector service (setup by kubernetes) on port 14250 and everything was working.,0.0,0.0,1.0,0.0
jaeger,Then recently we had to reboot the jaeger tracing service due to a system crash.,0.172,0.0,0.828,-0.4019
jaeger,Now things have stopped working and from the logs there's a &quot;504 Gateway Timeout&quot; and the agent can no longer communicate with the collector.,0.163,0.0,0.837,-0.4767
jaeger,"In AWS, we were running a kubernetes service &quot;jaeger-collector&quot; that conformed to the service here
 https://github.com/jaegertracing/jaeger-kubernetes/blob/master/jaeger-production-template.yml  with the only difference is that I'm using version 1.16.",0.0,0.0,1.0,0.0
jaeger,There's no external IP with the service.,0.268,0.0,0.732,-0.296
jaeger,How do I use  curl  to test the communication with the jaeger-collector service?,0.0,0.0,1.0,0.0
jaeger,Or do I need an external IP and perhaps that's the reason for the Gateway Timeout?,0.0,0.0,1.0,0.0
jaeger,I tried using  curl  with the ClusterIP and that didn't seem to work.,0.0,0.0,1.0,0.0
jaeger,I enabled sleuth in spring reactor:,0.0,0.0,1.0,0.0
jaeger,In spring boot sample app declared and run my program:,0.0,0.0,1.0,0.0
jaeger,Then I run jaeger and see traces coming from my app.,0.0,0.0,1.0,0.0
jaeger,"There are only traces with name &quot;async&quot;, and there are no tags with the key &quot;sample-key&quot;.",0.128,0.0,0.872,-0.296
jaeger,"Help me please, how to assign tags to span in my flux?",0.0,0.333,0.667,0.6124
jaeger,I wanna integrate open telemetry to my node.js and I have a few questions about this project.,0.0,0.0,1.0,0.0
jaeger,I am particularly interested in metrics and tracing Is it worth it to go for open telemetry or just get a Prometheus exporter and Zipkin/jaeger?,0.0,0.189,0.811,0.5984
jaeger,"Also, I am a little bit confused about metrics in open telemetry for js.",0.155,0.0,0.845,-0.2551
jaeger,There arent any default basic metrics that I can use?,0.0,0.0,1.0,0.0
jaeger,I have created a custom exporter by Implementing the SpanExporter class of OpenTelemetry.Trace.Export in .NET Core.,0.0,0.133,0.867,0.25
jaeger,I need to configure the opentelemetry traces to use this as an exporter.,0.0,0.0,1.0,0.0
jaeger,I am not using any collector here I want to directly use this exporter in-process.,0.0,0.098,0.902,0.0772
jaeger,Earlier we were using Jaeger exporter as follows:,0.0,0.0,1.0,0.0
jaeger,"I need to use the custom exporter inplace of JaegerExporter now, how to configure this?",0.0,0.0,1.0,0.0
jaeger,I am using Istio with tracing.,0.0,0.0,1.0,0.0
jaeger,I have two REST services A1 which calls A2.,0.0,0.0,1.0,0.0
jaeger,The A1 service will forward the headers received from the client to REST service A2.,0.0,0.0,1.0,0.0
jaeger,"However, I am not able to see the tracing in my Jaeger UI.",0.0,0.0,1.0,0.0
jaeger,I am sending some random values in the header using curl to my service A1:,0.0,0.172,0.828,0.4019
jaeger,Another header parameters I tried also is:,0.0,0.0,1.0,0.0
jaeger,"However, I don't see any trace on Jaeger ui.",0.0,0.0,1.0,0.0
jaeger,If just call a service A3 without passing any headers then I see that traced in the JaegerUI.,0.0,0.0,1.0,0.0
jaeger,thanks,0.0,1.0,0.0,0.4404
jaeger,I'm using the Python opentracing module with an RPC (over HTTP) client.,0.0,0.0,1.0,0.0
jaeger,At the moment I'm not interested in sending the tracing logs to an application like Jaeger - I just want to examine the span (and child spans) in the client when the RPC call returns.,0.063,0.105,0.832,0.1386
jaeger,So far I have this:,0.0,0.0,1.0,0.0
jaeger,I found I had to use  MockTracer()  to get anything at all.,0.0,0.0,1.0,0.0
jaeger,"The base  Tracer()  class didn't seem to make any of the basic information ( start_time ,  finish_time ,  tags  etc) of the spans publicly accessible.",0.0,0.0,1.0,0.0
jaeger,I can't currently figure out how to retrieve the updated span (in order to read any tags the server might have added) and any child spans created by the server from the results of the request.,0.0,0.056,0.944,0.25
jaeger,(I'm also a bit puzzled about how the server will know what kind of child spans to create - obviously they need to be the same kind as the span that is passed in through the headers.),0.046,0.057,0.897,0.1027
jaeger,"In a nutshell, while reporting traces to a central server like Jaeger is useful, my purpose here is to have the RPC client print out all the server's tracing information.",0.0,0.172,0.828,0.6597
jaeger,(Not to say I don't want the traces in Jaeger as well but I'll deal with that once the client trace reporting is working.),0.047,0.066,0.888,0.1126
jaeger,Based on the  Documentation  envoy is capable of generating and propagating the traces to the Jaeger service cluster.,0.0,0.133,0.867,0.3818
jaeger,It also states that,0.0,0.0,1.0,0.0
jaeger,"in order to fully take advantage of tracing, the application has to propagate trace headers that Envoy generates while making calls to other services.",0.0,0.09,0.91,0.3134
jaeger,"So assuming if a client calls -&gt; service A -&gt; calls Service B, service A being proxied behind envoy.",0.0,0.0,1.0,0.0
jaeger,"If service A calls service B, this call from A to B would also have to go through envoy right.",0.0,0.0,1.0,0.0
jaeger,"So the traced Id that was originally generated by envoy when the client called Service A, wouldn't this be propagated to Service B.",0.0,0.0,1.0,0.0
jaeger,Why does the application (Service A) need to forward these headers?,0.0,0.0,1.0,0.0
jaeger,I have a project with microservices in kubernetes connected though rest swagger clients.,0.0,0.0,1.0,0.0
jaeger,I want to make logging of all request and response payloads.,0.0,0.126,0.874,0.0772
jaeger,So that for each request there is an id and information about where it came from with full payload.,0.0,0.0,1.0,0.0
jaeger,And for each service to service call as well.,0.0,0.208,0.792,0.2732
jaeger,Is it possible to do that with Istio?,0.0,0.0,1.0,0.0
jaeger,"There is distributed tracing tools: zipkin, jaeger.",0.0,0.0,1.0,0.0
jaeger,But looks like they log only time.,0.0,0.351,0.649,0.5023
jaeger,Or better to handle it each app code internally?,0.0,0.266,0.734,0.4404
jaeger,I am using istio with version 1.3.5.,0.0,0.0,1.0,0.0
jaeger,Is there any configuration to be set to allow istio-proxy to log traceId?,0.0,0.137,0.863,0.2263
jaeger,I am using jaeger tracing (wit zipkin protocol) being enabled.,0.0,0.0,1.0,0.0
jaeger,"There is one thing I want to accomplish by having traceId logging:
- log correlation in multiple services upstream.",0.0,0.215,0.785,0.4767
jaeger,Basically I can filter all logs by certain traceId.,0.0,0.231,0.769,0.2732
jaeger,My question about Istio in Kubernetes.,0.0,0.0,1.0,0.0
jaeger,I have Istio sample rate of 1% and I have error which is not included in 1%.,0.162,0.0,0.838,-0.4019
jaeger,Would I see in Jaeger trace for this error?,0.309,0.0,0.691,-0.481
jaeger,I kind of new to Kubernetes and Istio.,0.0,0.0,1.0,0.0
jaeger,That's why can't tested on my own.,0.0,0.0,1.0,0.0
jaeger,I have been playing with Istio's example of  Book Application  and I wonder would I see trace with error which not included in 1% of sample rate.,0.102,0.068,0.83,-0.2263
jaeger,Configure Istio when installing with:,0.0,0.0,1.0,0.0
jaeger,As result want to know can I see error which not included in sample rate.,0.169,0.081,0.75,-0.34
jaeger,"If no, how I configure Istio to see it if possible?",0.196,0.0,0.804,-0.296
jaeger,I have installed Istio using the helm chart with the following settings:,0.0,0.0,1.0,0.0
jaeger,When I check the services running in the cluster under the  istio-system  namespace I see multiple services around tracing.,0.0,0.0,1.0,0.0
jaeger,"Since Jaeger is the default setting, I was expecting to see only the  jaeger-collector .",0.0,0.0,1.0,0.0
jaeger,"It is not clear as to what the role of  jaeger-agent ,  tracing  and  zipkin  are, any ideas ?",0.12,0.0,0.88,-0.2924
jaeger,",",0.0,0.0,0.0,0.0
jaeger,I am facing difficulty in working with jaeger and Istio.,0.231,0.0,0.769,-0.34
jaeger,Can anyone please describe the steps that are to be followed in configuring jaeger and istio for any demo application.,0.0,0.108,0.892,0.3182
jaeger,"I have tried a few blogs and sites but unfortunately, nothing worked for me.",0.22,0.0,0.78,-0.4767
jaeger,if anyone could help me in this that would be great.,0.0,0.43,0.57,0.7783
jaeger,I have these containers running on my localhost,0.0,0.0,1.0,0.0
jaeger,"openzipkin/zipkin                   |   0.0.0.0:9410- 9410/tcp, 0.0.0.0:9412- 9411/tcp",0.0,0.0,1.0,0.0
jaeger,"omnition/opencensus-collector:0.1.9 |         0.0.0.0:1777- 1777/tcp, 0.0.0.0:8888- 8888/tcp, 0.0.0.0:9411- 9411/tcp, 0.0.0.0:32776- 55678/tcp, 0.0.0.0:55680- 55679/tcp   |",0.0,0.0,1.0,0.0
jaeger,Trying to directly use the opencensus collector and my collector configuration looks like this,0.0,0.161,0.839,0.3612
jaeger,"When I run this sample, collector logs have lots of errors",0.211,0.0,0.789,-0.34
jaeger,I am unable to use the collector to sends the traces from opencensus collector to the zipkin backend.,0.0,0.0,1.0,0.0
jaeger,"Also tried to use the jaeger backend to post the traces from collector, but I still see the same errors.",0.147,0.0,0.853,-0.4767
jaeger,"I'm studying the Opentracing Standard and reading the docs I didn't found the API default Endpoints that should be used by Tracer Providers (Jaeger, LightStep...).",0.0,0.0,1.0,0.0
jaeger,"Today I'm using Spring Cloud Sleuth to send metrics do Zipkin, and now I have the option to use Opentracing (brave), but How Spring Cloud Sleuth will know the correct API URL if Opentracing docs don't have a API URL standard.",0.0,0.0,1.0,0.0
jaeger,i.e:  Jaeger and LightStep (both Opentracing providers) have different API URL.,0.0,0.0,1.0,0.0
jaeger,I installed Istio with,0.0,0.0,1.0,0.0
jaeger,"I have a service that consumes external services, so I define the following egress rule.",0.0,0.0,1.0,0.0
jaeger,"But using Jaeger I can not see the traffic to the external service, and thus be able to detect problems in the network.",0.145,0.0,0.855,-0.5499
jaeger,"I'm forwarding the appropriate headers to the external service (x-request-id, x-b3-traceid, x-b3-spanid, b3-parentspanid, x-b3-sampled, x-b3-flags, x-ot-span-context)",0.0,0.0,1.0,0.0
jaeger,Is this the correct behavior?,0.0,0.0,1.0,0.0
jaeger,what is happening?,0.0,0.0,1.0,0.0
jaeger,Can I only have statistics of internal calls?,0.0,0.0,1.0,0.0
jaeger,How can I have statistics for egress traffic?,0.0,0.0,1.0,0.0
jaeger,"I have a Spring Boot app using OpenTracing and I would like to push its data to Prometheus, so I can query all metrics via Grafana (like in this tutorial  https://www.hawkular.org/blog/2017/06/26/opentracing-appmetrics.html ).",0.0,0.085,0.915,0.3612
jaeger,"The problem is, I haven't found any consistent solution to do this, all the examples that I have found so far are outdated, deprecated or lacks documentation.",0.096,0.082,0.821,-0.1027
jaeger,"Ideally, I am looking for some solution which returns an instance of io.opentracing.Tracer, similar to what Jaeger does:",0.0,0.254,0.746,0.6249
jaeger,Best,0.0,1.0,0.0,0.6369
jaeger,"I’m struggling with the last step of a configuration using MetalLB, Kubernetes, Istio on a bare-metal instance, and that is to have a web page returned from a service to the outside world via an Istio VirtualService route.",0.078,0.0,0.922,-0.4215
jaeger,I’ve just updated the instance to,0.0,0.0,1.0,0.0
jaeger,I’ll start with what does work.,0.0,0.0,1.0,0.0
jaeger,All complementary services have been deployed and most are working:,0.0,0.0,1.0,0.0
jaeger,I say most because since the upgrade to Istio 1.0.3 I've lost the telemetry from istio-ingressgateway in the Jaeger dashboard and I'm not sure how to bring it back.,0.141,0.0,0.859,-0.5043
jaeger,I've dropped the pod and re-created to no-avail.,0.0,0.0,1.0,0.0
jaeger,"Outside of that, MetalLB and K8S appear to be working fine and the load-balancer is configured correctly (using ARP).",0.0,0.091,0.909,0.2023
jaeger,I can expose my deployment using:,0.286,0.0,0.714,-0.1531
jaeger,it all works perfectly fine and I can hit the webpage from the external load balanced IP address (I deleted the exposed service after this).,0.046,0.212,0.742,0.6908
jaeger,If I create a K8S Service in the default namespace (I've tried multiple),0.0,0.174,0.826,0.2732
jaeger,"followed by a gateway and a route (VirtualService), the only response I get is a 404 outside of the mesh.",0.0,0.0,1.0,0.0
jaeger,You'll see in the gateway I'm using the reserved word mesh but I've tried both that and naming the specific gateway.,0.0,0.0,1.0,0.0
jaeger,I've also tried different match prefixes for specific URI and the port you can see below.,0.0,0.0,1.0,0.0
jaeger,Gateway,0.0,0.0,1.0,0.0
jaeger,VirtualService,0.0,0.0,1.0,0.0
jaeger,I've double checked it's not the DNS playing up because I can go into the shell of the ingress-gateway either via busybox or using the K8S dashboard,0.06,0.0,0.94,-0.1511
jaeger,http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/shell/istio-system/istio-ingressgateway-6bbdd58f8c-glzvx/?namespace=istio-system,0.0,0.0,1.0,0.0
jaeger,and do both an,0.0,0.0,1.0,0.0
jaeger,and,0.0,0.0,1.0,0.0
jaeger,"and both work successfully, so I know the ingress-gateway pod can see those.",0.0,0.225,0.775,0.4939
jaeger,The sidecars are set for auto-injection in both the default namespace and the istio-system namespace.,0.0,0.0,1.0,0.0
jaeger,The logs for the ingress-gateway show the 404:,0.0,0.0,1.0,0.0
jaeger,192.168.224.168:80 is the IP address of the gateway.,0.0,0.0,1.0,0.0
jaeger,192.168.1.90:53960 is the IP address of my external client.,0.0,0.0,1.0,0.0
jaeger,"Any suggestions, I've tried hitting this from multiple angles for a couple of days now and I feel I'm just missing something simple.",0.099,0.0,0.901,-0.296
jaeger,Suggested logs to look at perhaps?,0.0,0.0,1.0,0.0
jaeger,I am using Kubernetes for my project and I was using Helm to install Jaeger and Kafka in a simple way.,0.0,0.0,1.0,0.0
jaeger,"The problem is that regarding kafka, the zookeeper pods started correctly I have one pod that is ""Pending"" since the beginning.",0.124,0.0,0.876,-0.4019
jaeger,I used this command  kubectl --namespace=default describe pod my-kafka-kafka-0  and got this information:,0.0,0.0,1.0,0.0
jaeger,Warning  FailedScheduling  55m (x398 over 2h)   default-scheduler  pod has unbound PersistentVolumeClaims (repeated 2 times),0.167,0.0,0.833,-0.34
jaeger,"I have no idea how to solve this issue since I am new to Kubernetes and i thought that using Helm, deploying apps with it would be straightforward.",0.081,0.067,0.852,-0.1027
jaeger,I am sorry if my question is not done in the correct way but it is also my first time here :),0.048,0.166,0.787,0.5927
jaeger,Thank you in advance for your time and I would be really happy if someone could help me!,0.0,0.404,0.596,0.8585
jaeger,I'm trying the code for the game SquareChase shown in XNA 4.0 Game programming by Example by Jaeger.,0.0,0.0,1.0,0.0
jaeger,The program is running through the VS2010 ide but the texture it is displaying is not the texture i drew.,0.0,0.0,1.0,0.0
jaeger,I've checked the Content directory and that holds the correct texture so i dont know where the incorrect texture is coming from.,0.0,0.0,1.0,0.0
jaeger,Can someone please help with this weird error?,0.328,0.373,0.299,0.1531
jaeger,I have got this code to work okay on another computer so i don't know what the problem is.,0.138,0.097,0.765,-0.2023
jaeger,Thanks....,0.0,0.0,1.0,0.0
jaeger,I'm in need of a rewrite rule for a site of mine.,0.0,0.0,1.0,0.0
jaeger,I have the following type of URLS:,0.0,0.0,1.0,0.0
jaeger,But I would like that to read:,0.0,0.394,0.606,0.5023
jaeger,This is what I think should work:,0.0,0.0,1.0,0.0
jaeger,(it doesn't),0.0,0.0,1.0,0.0
jaeger,"But then in addition, the  ?brand  parameter could actually appear as an  &amp;parameter  - e.g.",0.0,0.0,1.0,0.0
jaeger,?word=clutch&amp;brand=TedBaker .,0.0,0.0,1.0,0.0
jaeger,"To summarise - I would like the brand parameter to be made into the final part of the static URL, preceded by the word 'by'.",0.0,0.102,0.898,0.3612
jaeger,I would like this to work regardless of where the brand parameter appears in the URL string.,0.0,0.143,0.857,0.3612
jaeger,Here is the current .htaccess file:,0.0,0.0,1.0,0.0
jaeger,And here are all the URLs this needs to apply to:,0.0,0.0,1.0,0.0
jaeger,Above need to be converted to /store/category/black-bags-by-jaeger/ or /store/category/black-bags-by-jaeger/?word=Cross%20Body (for 2nd and 3rd options).,0.0,0.0,1.0,0.0
jaeger,"I also have this:
 http://bag-saver.com/store/search/?brand=Jaeger  which needs to be converted to /store/search/bags-by-jaeger/.",0.0,0.0,1.0,0.0
jaeger,I am exploring the various Tracing Systems.,0.0,0.0,1.0,0.0
jaeger,I was looking into Light Step recently.,0.0,0.0,1.0,0.0
jaeger,I have integrated my application for OpenTracing where I use the tracer from Light Step.,0.0,0.0,1.0,0.0
jaeger,Now how can view the traces I am generating.,0.0,0.0,1.0,0.0
jaeger,For example in Jaeger they had a ready to use docker image which can be used for quick demo.,0.0,0.128,0.872,0.3612
jaeger,Can somebody please help me here ?,0.0,0.556,0.444,0.6124
jaeger,I have parsed texts from several scientific pdf files.,0.0,0.0,1.0,0.0
jaeger,"All of these files contain a reference list at the end, where the authors and their publications are listed, + when and where they were released.",0.0,0.0,1.0,0.0
jaeger,"Also, there are cross-references in the text.",0.0,0.0,1.0,0.0
jaeger,For example:,0.0,0.0,1.0,0.0
jaeger,"('1', ' I.  Altintas, C.  Berkley, E.  Jaeger, M.  Jones, B.  Lud-scher, and S.  Mock.",0.177,0.0,0.823,-0.4215
jaeger,Kepler: An extensible system fordesign and execution of scientiﬁc workﬂows.,0.0,0.0,1.0,0.0
jaeger,"In In SS-DBM, pages 21–23, 2004. ')",0.0,0.0,1.0,0.0
jaeger,and a different one from another text:,0.0,0.0,1.0,0.0
jaeger,"('1', ' G.  Antoniol, G.  Canfora, G.  Casazza, A.  DeLucia, and E.  Merlo,“Recovering Traceability Links between Code and Documentation,” IEEETrans.",0.0,0.0,1.0,0.0
jaeger,Software Eng.,0.0,0.0,1.0,0.0
jaeger,", vol.",0.0,0.0,1.0,0.0
jaeger,"28, no.",0.688,0.0,0.312,-0.296
jaeger,"10, pp.",0.0,0.0,1.0,0.0
jaeger,"970-983, Oct.",0.0,0.0,1.0,0.0
jaeger,2002.'),0.0,0.0,1.0,0.0
jaeger,"I was able to recognize both with the regex, which gives me 2 capturing groups besides the full-match:",0.0,0.0,1.0,0.0
jaeger,"The  first group  I can use to get the  number of the reference , to match with the cross-references in text",0.0,0.067,0.933,0.0772
jaeger,"The  second  is the  everything else remaining , and i'd like to recognize the  author  and the  title of the publication  from it, regardless of the parsing format, if that's possible.",0.0,0.079,0.921,0.3612
jaeger,"Later, I'd like to use these values to write separate  .txt  files with the author + title.txt name, and to append their cross-references found in the text to each file.",0.0,0.161,0.839,0.6369
jaeger,For this is what I have now for this:,0.0,0.0,1.0,0.0
jaeger,"It was giving UnicodeEncodeError when I was trying to create a file with mode =""a+"", for a suggestion i changed it to bytes.",0.0,0.209,0.791,0.5423
jaeger,"It's not giving me an UnicodeEncodeError, it's giving me another now:",0.312,0.0,0.688,-0.4717
jaeger,"f = open(author+"".txt"", ""ab+"")",0.0,0.0,1.0,0.0
jaeger,"OSError: [Errno 22] Invalid argument:  "" A.  Yun chung Liu, “The Effect of Oversampling and Undersampling onClassifying Imbalanced Text Datasets,” master’s thesis,  http://www .",0.106,0.0,0.894,-0.3612
jaeger,lans.,0.0,0.0,1.0,0.0
jaeger,ece.,0.0,0.0,1.0,0.0
jaeger,utexas.,0.0,0.0,1.0,0.0
jaeger,edu/aliu/papers/aliu_ masters_thesis.,0.0,0.0,1.0,0.0
jaeger,"pdf, 2004.",0.0,0.0,1.0,0.0
jaeger,"J.  Cleland-Huang, R.  Settimi, X.  Zou, and P.  Solc, “The Detection andClassification of Non-Functional Requirements with Application to EarlyAspects,” Proc.",0.0,0.0,1.0,0.0
jaeger,Requirements Eng.,0.0,0.0,1.0,0.0
jaeger,Conf.,0.0,0.0,1.0,0.0
jaeger,"(RE ’06), pp.",0.0,0.0,1.0,0.0
jaeger,"36-45, 2006.",0.0,0.0,1.0,0.0
jaeger,"'.txt""",0.0,0.0,1.0,0.0
jaeger,"Currently the publication has everything else than the number from the references,  I'd like to switch that to author + title.txt , and I hope that would solve the error above as well.",0.072,0.284,0.643,0.7096
jaeger,I'd appreciate every suggestion for improvement!,0.0,0.6,0.4,0.7177
inspectit,I have configured InspectIT and a sample springboot application .,0.0,0.0,1.0,0.0
inspectit,there is a request mapping which was configured to leak some memory .. trouble is InspectIT does not dig deep into the method calls .,0.195,0.0,0.805,-0.6249
inspectit,the only level it goes down is doFilter-  service and then no method calls after that .. is this normal for inspectit ?,0.095,0.0,0.905,-0.296
inspectit,... somehow i would have expected it to dig deep down into the method calls,0.0,0.0,1.0,0.0
inspectit,any help is appreciated,0.0,0.75,0.25,0.7184
inspectit,I'm looking for defining the regular expression on the HTTP Sensor so that as per the documentation &quot;Apply sensor regular expression on URI&quot; option is visible on the URI Aggregation view.,0.0,0.0,1.0,0.0
inspectit,"However, apart from the method definition i cannot see any field where i can define the regular expression for URI Transformation in UI of HTTP Sensor configuration (as shown in attached ss).",0.0,0.0,1.0,0.0
inspectit,Also there is no &quot;http.cfg&quot; file present in the downloaded folders where we can do this out-of-the-box-instrumentation.,0.121,0.0,0.879,-0.296
inspectit,I am using InspectIT 1.9.3.107.,0.0,0.0,1.0,0.0
inspectit,Could I get some more information or suggestions on this?,0.0,0.0,1.0,0.0
inspectit,since i have already tried playing with http sensors and other trial n errors.,0.169,0.127,0.704,-0.1531
inspectit,"I am trying to install inspectIT in my docker container, however it seems as if they only package an installer that I must go through.",0.0,0.0,1.0,0.0
inspectit,This seems rather difficult but I was thinking: is it possible to send commands to a console application?,0.104,0.0,0.896,-0.1901
inspectit,for example if I start the installer it will get to a point like this,0.0,0.172,0.828,0.3612
inspectit,Can I send a command to it or script it in some way in order to install it on docker?,0.0,0.0,1.0,0.0
instana,I have set up a Kafka &amp; Zookeeper cluster in Production.,0.0,0.0,1.0,0.0
instana,I need to set up INSTANA APM Tool to monitor and manage the Production Kafka &amp; Zookeeper Cluster.,0.0,0.0,1.0,0.0
instana,Did anyone  has ever used INSTANA to monitor Kafka .,0.0,0.0,1.0,0.0
instana,Kindly share your thoughts on this.,0.0,0.574,0.426,0.6597
instana,I'm trying to integrate Instana into an application.,0.0,0.0,1.0,0.0
instana,More specifically I'm trying to send errors from my Angular app to Instana.,0.167,0.0,0.833,-0.34
instana,I have my code 'working' so this is kind of a question about best practice.,0.0,0.259,0.741,0.6369
instana,Instana's  Backend Correlation documentation  defines functions in 'window' from what I understand.,0.0,0.0,1.0,0.0
instana,I've set something similar to this in my index.html.,0.0,0.0,1.0,0.0
instana,"The issue I have is when I try to follow Instana's guide for  Angular 2+ Integration  regarding error tracking, where they call one of the methods that I can access from window.",0.088,0.0,0.912,-0.4019
instana,The guide just straight up calls the function  ineum(...)  by itself.,0.0,0.16,0.84,0.2263
instana,"When I try to do this, my project wouldn't compile.",0.0,0.0,1.0,0.0
instana,"My current fix is:  (&lt;any&gt;window).ineum('reportError', errorContext);  But I was looking at  another stack overflow question  where they accessed window differently in their javascript.",0.0,0.0,1.0,0.0
instana,Sorry for my confusion as this may not be an actual issue because my code is 'working' but I just want clarification on this.,0.114,0.06,0.826,-0.0772
instana,I'm not sure if I just didn't follow Instana's guides correctly but this was the best of what I could find.,0.061,0.239,0.7,0.7445
instana,I tried reaching out via their contact page but I haven't received a response quite yet.,0.0,0.104,0.896,0.1027
instana,The source map for CRA is enabled by default.,0.0,0.0,1.0,0.0
instana,"I have given Instana the permission to download source map from my application in production, but the errors reported are still compressed and uglified.",0.232,0.0,0.768,-0.7469
instana,I guess the configuration has no effect.,0.306,0.0,0.694,-0.296
instana,Referring to  this doc .,0.0,0.0,1.0,0.0
instana,"When I do a curl for the source map from the terminal, it works.",0.0,0.0,1.0,0.0
instana,"My site is on HTTPS, but the doc says it makes an HTTP request.",0.0,0.0,1.0,0.0
instana,Is that the root cause?,0.0,0.0,1.0,0.0
instana,How to fix it?,0.0,0.0,1.0,0.0
instana,Most APM solutions which analyze and store traces will sample traces to manage the volume of traces stored/analyzed (millions per day!,0.0,0.102,0.898,0.3117
instana,).,0.0,0.0,1.0,0.0
instana,Instana claims to store and analyze all traces without sampling.,0.0,0.0,1.0,0.0
instana,"I have been doing some research around Instana, but do not see any whitepaper/blog post that hints at how they accomplish this.",0.0,0.156,0.844,0.5719
instana,Wondering if there is a whitepaper / seminar I have missed that may have hinted at this.,0.145,0.0,0.855,-0.296
instana,"For example, Lightstep has a similar claim and does so through its satellite architecture - they provide a high level explanation of how they try and capture 100% of traces (temporarily).",0.0,0.0,1.0,0.0
instana,Does the community here have any good hypothesis - high level - how Instana approaches the challenge?,0.0,0.244,0.756,0.4939
instana,I'm having some trouble monitoring my GitLab installation with Instana.,0.231,0.0,0.769,-0.4019
instana,"GitLab and the nginx shipped with it are running fine, only monitoring does not work.",0.0,0.114,0.886,0.2023
instana,Instana recognises the nginx but cannot get any information because it cannot pick up data from the /nginx_status location.,0.0,0.0,1.0,0.0
instana,I added my additional configuration for /nginx_status to /etc/gitlab/gitlab.rb and installed it using gitlab-ctl reconfigure (just as the gitlab docs say).,0.0,0.0,1.0,0.0
instana,And basically gitlab works fine and exposes the status page  http://git-test:9999/nginx_status .,0.122,0.146,0.732,0.0772
instana,My config file /var/opt/gitlab/nginx/conf/nginx.conf now has an additional include for /var/opt/gitlab/nginx/conf/nginx-status.conf at its end.,0.0,0.0,1.0,0.0
instana,The file contents seem fine as well.,0.0,0.438,0.562,0.4404
instana,My problem now is that instana is not picking up the information exposed via /nginx_status stating that nginx needs configuration.,0.182,0.0,0.818,-0.4588
instana,Status URL not found.,0.0,0.0,1.0,0.0
instana,"The nginx config file was parsed and no stub_status
  direction could be found.",0.155,0.0,0.845,-0.296
instana,"This directive needs to be
  configured in order to gather nginx metrics.",0.0,0.0,1.0,0.0
instana,"The
  following snippet shows how to configure stub_status
  within an nginx config file.",0.0,0.0,1.0,0.0
instana,To me it seems that instana is not following the include and hence not pickung up the configuration from /var/opt/gitlab/nginx/conf/nginx-status.conf.,0.0,0.0,1.0,0.0
instana,So basically instana has no clue about the information it asks for.,0.167,0.0,0.833,-0.296
instana,Does anyone of you know how I can feed these status information to instana?,0.0,0.0,1.0,0.0
instana,Thanks in advance guys and best regards.,0.0,0.587,0.413,0.7964
instana,Sebastian,0.0,0.0,1.0,0.0
instana,trying to get receive informations via the Instana REST API.,0.0,0.0,1.0,0.0
instana,Looks like that:,0.0,0.556,0.444,0.3612
instana,Getting back that error:,0.474,0.0,0.526,-0.4019
instana,The matching Curl script (which I can't use) looks like that and works:,0.161,0.0,0.839,-0.2755
instana,Any idea?,0.0,0.0,1.0,0.0
instana,I'm trying to monitor a springboot microservice with instana on a docker swarm cluster.,0.0,0.0,1.0,0.0
instana,the Microservice has 2 replicas per node on a 3 nodes cluster.,0.0,0.0,1.0,0.0
instana,it is possible?,0.0,0.0,1.0,0.0
instana,do i need to run instana agent image with docker run?,0.0,0.0,1.0,0.0
instana,docker service?,0.0,0.0,1.0,0.0
instana,any help will be appreciate.,0.0,0.643,0.357,0.6597
instana,Thanks,0.0,1.0,0.0,0.4404
instana,I am trying to integrate instana( https://www.instana.com ) with react native webview.,0.0,0.0,1.0,0.0
instana,My app is a webapp which is being rendered inside webview.,0.0,0.0,1.0,0.0
instana,"My approach is to inject their javascript agent( https://www.instana.com/docs/website_monitoring/api ) to webview, but that doesn't seems to be working.",0.0,0.0,1.0,0.0
instana,Any thoughts on this will be extremely helpful.,0.0,0.306,0.694,0.4754
instana,I tried installing instana agent using docker command and it works but I need it be installed using one liner command and when tried it gives error as below:,0.12,0.0,0.88,-0.5499
instana,I tried on private wifi (with no proxy) but still the same.,0.138,0.0,0.862,-0.1531
instana,Can anyone help on this error?,0.287,0.287,0.426,0.0
instana,Thank you!,0.0,0.736,0.264,0.4199
instana,"I'd like to start tracing my sequelize SQL calls using opentracing, but I'm having a hard time figuring out how.",0.079,0.086,0.835,0.0387
instana,I'd like to adapt this code to be more flexible so I can drop it into any sequelize project:  https://github.com/instana/nodejs-sensor/blob/626ab3c8258d4e91d42a61d79603532a921b35b4/packages/core/src/tracing/instrumentation/database/pg.js,0.103,0.203,0.694,0.3214
instana,"I'm using Lightstep as a tracer, but I am using a raw tracer (not one that auto instruments) because I like the control.",0.0,0.153,0.847,0.5023
instana,Do you have suggestions on how I could add tracing to sequelize/postgres(pg)?,0.0,0.0,1.0,0.0
instana,We are running a set of Java applications in docker containers on OpenShift.,0.0,0.0,1.0,0.0
instana,On a regular basis we experience oom kills for our containers.,0.28,0.0,0.72,-0.5423
instana,To analyse this issue we set up Instana and Grafana for monitoring.,0.0,0.0,1.0,0.0
instana,In Grafana we have graphs for each of our containers showing memory metrics e.g.,0.0,0.0,1.0,0.0
instana,"JVM heap, memory.usage and memory.total_rss.",0.0,0.0,1.0,0.0
instana,From these graphs we know that the heap as well as the memory.total_rss of our containers is pretty stable on a certain level over a week.,0.0,0.324,0.676,0.8225
instana,So we assume that we do not have a memory leak in our Java application.,0.0,0.135,0.865,0.2584
instana,"However, the memeory.total is constantly increasing over the time and after a couple of days it goes beyond the configured memory limit of the docker container.",0.0,0.0,1.0,0.0
instana,As far as we can see this doesn't cause Openshift to kill the container immediately but sooner or later it happens.,0.125,0.0,0.875,-0.431
instana,We increased the memory limit of all our containers and this seems to help since Openshift is not killing our containers that often anymore.,0.0,0.284,0.716,0.8082
instana,However we still see in Grafana that the memeory.total is exceeding the configures memory limit of our containers significantly after a couple of days (rss memory is fine).,0.0,0.0,1.0,0.0
instana,"To better understand Openshifts OOM killer, does anybody know which memory metric Openshift takes into account to decide if a container has to be killed or not?",0.254,0.084,0.663,-0.7845
instana,Is the configured container memory limit related to the memory.usage or the memory.total_rss or something completely different?,0.0,0.0,1.0,0.0
instana,Thanks for help in advance.,0.0,0.651,0.349,0.6808
instana,I am referring to this page:,0.0,0.0,1.0,0.0
instana,https://www.instana.com/docs/setup_and_manage/host_agent/updates/#update-interval,0.0,0.0,1.0,0.0
instana,Is there a way to pass mode and time from outside as environment variables or any other way beside logging into the pod and manually changing the files inside etc/instana/com.instana.agent.main.config.UpdateManager.cfg file?,0.0,0.0,1.0,0.0
instana,"Given a server APP that serves an Angular App to a web browser, an Apollo GraphQL server and a third-party tracking script by Instana.",0.0,0.0,1.0,0.0
instana,"If the tracking script is run in the browser, where do I need to set the Timing-Allow-Origin header in the response?",0.0,0.0,1.0,0.0
instana,Does this need to point to the protocol + host of the third-party script?,0.0,0.0,1.0,0.0
instana,I wanted to check the address that a pod uses to connect to a FTP server.,0.0,0.0,1.0,0.0
instana,I wanted to test it by running  kubectl   -n cdol exec -it pod-namer  -- curl ipinfo.io/ip  but the connection is blocked by  an engress policy.,0.103,0.0,0.897,-0.3919
instana,I know the CLUSTER-IP of the pod.,0.0,0.0,1.0,0.0
instana,It has no EXTERNAL-IP.,0.423,0.0,0.577,-0.296
instana,I know the CLUSTER-IP of the service that pod uses.,0.0,0.0,1.0,0.0
instana,I know the node the pod is running so I am able to check the network interfaces in Instana monitoring tool.,0.0,0.0,1.0,0.0
instana,"But all the IPs are private IPs, how can I know what IP other services sees when a pod is connecting to them?",0.0,0.0,1.0,0.0
instana,I am running a Instana Agent on minishift.,0.0,0.0,1.0,0.0
instana,I see these logs:,0.0,0.0,1.0,0.0
instana,2020-05-26T03:10:58.763+00:00 | WARN  | nstana-sensor-scheduler-thread-1 | Kubernetes       | com.instana.sensor-kubernetes - 1.2.106 | Instana agent does not have permission to watch for statefulsets changes.,0.118,0.0,0.882,-0.2808
instana,Kubernetes sensor will not work properly without this permission.,0.0,0.0,1.0,0.0
instana,Please ensure proper permissions for  statefulsets  resource.,0.0,0.495,0.505,0.5994
instana,How to set proper permissions for statefulsets resource in Kubernete?,0.0,0.0,1.0,0.0
instana,edit,0.0,0.0,1.0,0.0
instana,I used 2 yaml files.,0.0,0.0,1.0,0.0
instana,I used this yaml file to install operator ditto as is without any change:  https://github.com/instana/instana-agent-operator/releases/latest/download/instana-agent-operator.yaml,0.0,0.0,1.0,0.0
instana,And I used this custom resource yaml file  https://github.com/instana/instana-agent-operator/blob/master/deploy/instana-agent.customresource.yaml  with just two changes: I added  size: 1  under spec key and replaced  replace-me  with proper agent key.,0.0,0.0,1.0,0.0
instana,edit2,0.0,0.0,1.0,0.0
instana,I used  oc api-resources  as that's how I created the project and deployed definition files.,0.0,0.143,0.857,0.25
instana,Result can be found here -   https://pastebin.com/A6zRRReS,0.0,0.0,1.0,0.0
instana,I have a  Spring Batch  application with  JpaPagingItemReader  (i modified it a bit) and 4 Jpa repositories to enrich  Model  which comes from JpaPagingItemReader.,0.0,0.0,1.0,0.0
instana,My flow is:,0.0,0.0,1.0,0.0
instana,"All works great, but today i tried to run flow with big amount of data from database.",0.0,0.145,0.855,0.3716
instana,I generated 20 files ( 2.2 GB ).,0.0,0.0,1.0,0.0
instana,"But sometimes i got  OutOfMemory Java Heap  (I had 1Gb XMS, XSS), then i up it to 2 GB and all works good, but in Instana i see, that  Old gen Java memory  is always  900  in use after GC.",0.0,0.099,0.901,0.5927
instana,It is about 1.3-1.7Gb in use.,0.0,0.0,1.0,0.0
instana,"So i start to think, how can i optimize GC of Spring Data Jpa objects.",0.0,0.211,0.789,0.4939
instana,I think they are much time in memory.,0.0,0.0,1.0,0.0
instana,"When i select Model with  JpaPagingItemReader  i detach every Model (with  entityManager.detach ), but when i enrich  Model  with custom  Spring Data Jpa  requests i am not detaching results.",0.0,0.0,1.0,0.0
instana,Maybe the problem in this and i should detach them?,0.252,0.0,0.748,-0.4019
instana,"I do not need to insert data to database, i need just to read it.",0.0,0.0,1.0,0.0
instana,Or do i need to make page size less and select about  4000  per request?,0.0,0.0,1.0,0.0
instana,I need to process  370 000  records from database and enrich them.,0.0,0.0,1.0,0.0
instana,Why Cocoapods command gives this error in terminal?,0.309,0.0,0.691,-0.481
instana,Either it is the problem with my settings or I am using VMware?,0.197,0.0,0.803,-0.4019
instana,Awaiting for your ideas &amp; tricks.,0.231,0.0,0.769,-0.128
instana,"ERROR:  Could not find a valid gem 'pod' ( = 0) in any repository 
  ERROR:  Could not find a valid gem 'install' ( = 0) in any repository 
  ERROR:  Possible alternatives: installr, instant, instana, instacli, instapi",0.276,0.0,0.724,-0.8833
lightstep,Background,0.0,0.0,1.0,0.0
lightstep,I have a java server that is making an RPC call to a go server.,0.0,0.0,1.0,0.0
lightstep,The java rpc client and go rpc server are instrumented with lightstep.,0.0,0.0,1.0,0.0
lightstep,"Everything about the trace looks normal except for where in the lightstep UI, the go rpc server span is placed.",0.0,0.0,1.0,0.0
lightstep,The java span has ts 1493929521325 which is right before the request is sent to the go server.,0.0,0.0,1.0,0.0
lightstep,"The go rpc server has 2 timestamps: 1493929521326 is when it received the request and started the span, 1493929521336 is after it responded and finished the span.",0.0,0.0,1.0,0.0
lightstep,Problem,1.0,0.0,0.0,-0.4019
lightstep,I would expect the UI to have the go span horizontally to the immediate right of the java span.,0.0,0.0,1.0,0.0
lightstep,"Instead, it is far to the right.",0.0,0.0,1.0,0.0
lightstep,The only possible cause I can think of is an incompatibility between v0.10.1 which java code is using and v0.9.1 which go is using.,0.0,0.0,1.0,0.0
lightstep,Is this a possibility?,0.0,0.0,1.0,0.0
lightstep,Do you have any thoughts on a possible cause?,0.0,0.0,1.0,0.0
lightstep,The go code is essentially:,0.0,0.0,1.0,0.0
lightstep,"Before I was using  lightstep/opentelemetry-exporter-js , I can use my own exporters and Lightstep exporter at same time.",0.0,0.0,1.0,0.0
lightstep,"However, just saw lightstep/opentelemetry-exporter-js is deprecated and replaced by  lightstep/otel-launcher-node .",0.0,0.0,1.0,0.0
lightstep,"I checked the source code of it and the demo, it looks like it is a &quot;framework&quot; on top of OpenTelemetry.",0.0,0.202,0.798,0.5106
lightstep,Is it possible to simply use it as one of OpenTelemetry exporters?,0.0,0.0,1.0,0.0
lightstep,"I've inherited a legacy Spring3 application, and I'm trying to add Lightstep instrumentation to it.",0.0,0.0,1.0,0.0
lightstep,I'm having trouble converting the instructions for manually configuration found here.,0.213,0.0,0.787,-0.4019
lightstep,https://github.com/opentracing-contrib/java-spring-web,0.0,0.0,1.0,0.0
lightstep,"In short, I need to convert the code block below to the xml equivalent.",0.195,0.0,0.805,-0.4404
lightstep,I've successfully created my Lightstep Tracer bean using the following dependencies.,0.0,0.366,0.634,0.6369
lightstep,I am trying to use  LightStep OpenTelemetry Launcher for Node.js  (which uses  OpenTelemetry Node SDK ) and  OpenTelemetry GraphQL Instrumentation  together.,0.0,0.0,1.0,0.0
lightstep,"However, I cannot see how to change the setup instruction for the GraphQL instrumentation so that they will work with the Node SDK.",0.0,0.0,1.0,0.0
lightstep,The Node SDK does not expose a  NodeTracerProvider  instance I can pass to the  GraphQLInstrumentation  instance.,0.0,0.1,0.9,0.1139
lightstep,Has anyone gotten these to play together?,0.0,0.286,0.714,0.34
lightstep,I'm using the latest milestone of spring-cloud-sleuth and I can't seem to get traces emitted through opentracing.,0.0,0.0,1.0,0.0
lightstep,"I have a  Tracer  bean defined and spring boot seems to acknowledge that, but no traces are being emitted.",0.149,0.0,0.851,-0.4215
lightstep,Is there a way to check if spring-cloud-sleuth is aware of the Tracer bean?,0.0,0.0,1.0,0.0
lightstep,update,0.0,0.0,1.0,0.0
lightstep,"I did see the merged documentation and have a  Tracer  instance on the bean, as defined below:",0.0,0.0,1.0,0.0
lightstep,"I'm not explicitly importing the OpenTracing APIs, because the LightStep tracer pulls that in transitively, but I can try doing that.",0.0,0.0,1.0,0.0
lightstep,I've also explicitly enabled OpenTracing support in my application.yml file.,0.0,0.231,0.769,0.4019
skywalking,Couldn't see in  skywalking ui  data from  istio  metrics.,0.0,0.0,1.0,0.0
skywalking,"Using below guide to installed  istio  and  skywalking  backend to  Kubernetes :
 https://github.com/apache/skywalking/tree/master/docs/en/setup/istio .",0.0,0.0,1.0,0.0
skywalking,Made all steps in the guide and deployed app.,0.0,0.0,1.0,0.0
skywalking,Deployed app is example  book app  from  istio .,0.0,0.0,1.0,0.0
skywalking,"Could see traces in  Jaeger  and works good, but can't see  istio  metrics in  skywalking ui .",0.0,0.115,0.885,0.2382
skywalking,How I can provide metrics from  istio  to  skywalking ui  or what I need to configure to make metrics from  istio  come to  skywalking ui .,0.0,0.0,1.0,0.0
skywalking,Mixer logs:,0.0,0.0,1.0,0.0
skywalking,Istio policy:,0.0,0.0,1.0,0.0
skywalking,"somehow I have failed to connect to my  skywalking  backend, problem was with name and namespace but still getting error but now  skywalking  backend getting data but ui no updates.",0.293,0.0,0.707,-0.8537
skywalking,Istio  telemetry:,0.0,0.0,1.0,0.0
skywalking,rpc error: code = Unavailable desc = upstream connect error or disconnect/reset before headers.,0.351,0.0,0.649,-0.6597
skywalking,reset reason: connection failure,0.524,0.0,0.476,-0.5106
skywalking,"I am using apache skywalking(7.0.0) in Kubernetes(v1.16.0) cluster to be my APM tool,but now I could not get service name in dashboard.",0.0,0.0,1.0,0.0
skywalking,This is my collector config in Dockerfile:,0.0,0.0,1.0,0.0
skywalking,and this is my dashboard UI:,0.0,0.0,1.0,0.0
skywalking,Is something I am missing?,0.423,0.0,0.577,-0.296
skywalking,all data collected(Endpoint\Cache\Database\MQ) except the service.What should I do to make service data collect?,0.0,0.0,1.0,0.0
skywalking,When I see the log output:,0.0,0.0,1.0,0.0
skywalking,I am follow this  docs  to install skywalking using helm 3.2.1 :,0.0,0.0,1.0,0.0
skywalking,but when I execute the second command:,0.0,0.0,1.0,0.0
skywalking,and I create skywalking directory:,0.0,0.412,0.588,0.2732
skywalking,so what should I do to make it work?,0.0,0.0,1.0,0.0
skywalking,This is I am trying follow:,0.0,0.0,1.0,0.0
skywalking,"I am now using skywalking as my apm, and now I  am configuring the address of my skywalking agent like this:",0.0,0.122,0.878,0.3612
skywalking,but it tells me this address is not correct.,0.0,0.0,1.0,0.0
skywalking,Is skywalking agent having docker image?,0.0,0.0,1.0,0.0
skywalking,What is the docker image address to use in kubernetes v1.16.0 cluster?,0.0,0.0,1.0,0.0
skywalking,I am searching from internet and only find a skywalking  base image .,0.0,0.0,1.0,0.0
skywalking,"I am install skywalking in kubernetes cluster v1.16.8 using this command, follow  this :",0.0,0.0,1.0,0.0
skywalking,"all component runs fine, and I configurate the skywalking agent as sidecar in my pod like this:",0.0,0.235,0.765,0.5106
skywalking,"but now the skywalking UI has no data, am I missing something to be configurated?",0.318,0.0,0.682,-0.6808
skywalking,"I am using skywalking 6.5.0 to monitor my apps in kubernetes cluster, this is my skywalking ui yaml config:",0.0,0.0,1.0,0.0
skywalking,"when the pod start, the log output like this:",0.0,0.238,0.762,0.3612
skywalking,"I read the skywalking official issue and tell me because the mysql jdbc was GPL licence and SkyWalking is Apache license,so I must add the jdbc driver by myself, but how to add the jdbc driver jar into the image file?",0.0,0.0,1.0,0.0
skywalking,I have no ideas.,0.524,0.0,0.476,-0.296
skywalking,I am having trouble finding any information about this in documentation.,0.231,0.0,0.769,-0.4019
skywalking,In the config/application.yml file under  storage.elasticsearch7  I see various configuration options.,0.0,0.0,1.0,0.0
skywalking,Is there a way to ensure that the indexes that get created are created using a given index template or ILM policy?,0.0,0.28,0.72,0.6808
skywalking,I am running the helm chart for the ELK stack and ES version 8.0.0-SNAPSHOT.,0.0,0.0,1.0,0.0
skywalking,My goal is to just delete indexes from SW after 2 weeks so that my cluster doesn't run out of shards.,0.0,0.0,1.0,0.0
skywalking,I have recently started exploring skywalking as APM tool.,0.0,0.0,1.0,0.0
skywalking,I am interested in looking at the time spent by methods/functions at application layer.,0.0,0.184,0.816,0.4019
skywalking,Basically a instrumentation sort of thing for the JAVA application.,0.0,0.0,1.0,0.0
skywalking,With Skywalking I just get 3 spans(methods) that have one root function and two DB execute functions.,0.0,0.0,1.0,0.0
skywalking,I tried adding the property,0.0,0.0,1.0,0.0
skywalking,But this dint work.,0.0,0.0,1.0,0.0
skywalking,I could still see only 3 spans in the dashboard for the API being hit.,0.0,0.0,1.0,0.0
skywalking,Under Profile feature I can get the thread stack.,0.0,0.0,1.0,0.0
skywalking,But i am only interested with Hotspot methods.,0.0,0.372,0.628,0.5499
skywalking,Am i missing something in configuration?,0.355,0.0,0.645,-0.296
skywalking,I want classes starting with particular pattern to be instrumented and captured in trace.,0.0,0.098,0.902,0.0772
skywalking,How can I achieve this?,0.0,0.0,1.0,0.0
skywalking,Or is there any other open source APM tool I can start with?,0.0,0.0,1.0,0.0
skywalking,"I am using kubernetes(v1.15.2) to manage my skywalking-ui(v6.5.0) apps,recently I found some app's not accessable but the pod is still running, I am not sure the app is works fine,there is no error output in pod's logs.But the pod status icon give tips:  the pod is in pending state .",0.17,0.0,0.83,-0.8313
skywalking,Why the status not same in different places?The service is down now.How to avoid this situation or make the service recover automatic?,0.109,0.0,0.891,-0.3736
skywalking,This is pod info:,0.0,0.0,1.0,0.0
skywalking,"I am using a inital container(k8s version:v1.15.2) to initial skywalking(6.5.0) jar file before container startup.But I could not found the file and directory the intial container create,this is my initial container define:",0.0,0.0,1.0,0.0
skywalking,"now the initial container execute success,I am check the log output like this:",0.0,0.172,0.828,0.3612
skywalking,now something I am confusing is where the directory locate?,0.192,0.0,0.808,-0.2263
skywalking,where is the file I am copy?,0.0,0.0,1.0,0.0
skywalking,I am login my container and do not find the jar file:,0.0,0.0,1.0,0.0
skywalking,now I am starting my app to collection metrics data like this:,0.0,0.2,0.8,0.3612
skywalking,obviously it tell me could not fond the jar file error:,0.362,0.0,0.638,-0.6256
skywalking,so what should I do to fix this?,0.0,0.0,1.0,0.0
skywalking,I already searching from internet bu found no useful way to solve my situation.,0.13,0.278,0.592,0.3612
skywalking,jstat[option vmid[interval[s|ms][count]]],0.0,0.0,1.0,0.0
skywalking,i input jstat -gc 12285 get 12285 not found,0.0,0.0,1.0,0.0
skywalking,"so, why?",0.0,0.0,1.0,0.0
skywalking,and how to fix it?,0.0,0.0,1.0,0.0
skywalking,"so, the vmid is 12285",0.0,0.0,1.0,0.0
skywalking,"2.but jstat -gc 12285 get ""12285 not found""",0.0,0.0,1.0,0.0
skywalking,"I expect the output of the java JVM's heap status, including Eden, survivor, Old, Permian's status, and GCtime information",0.0,0.128,0.872,0.3612
stagemonitor,"I am new to Stagemonitor, and I found that it is one of the best open source tool to get the performace metrics.",0.0,0.174,0.826,0.6369
stagemonitor,"So, I tried to implement this for my tomcat server.",0.0,0.0,1.0,0.0
stagemonitor,But I cannot get a clear documentation to implement and setup this.,0.236,0.0,0.764,-0.4168
stagemonitor,The wiki provided in the github page is not enough to setup the application.,0.0,0.0,1.0,0.0
stagemonitor,Can anyone guide me how to install this tool and make it a go.!,0.0,0.0,1.0,0.0
stagemonitor,!,0.0,0.0,0.0,0.0
stagemonitor,How does  Stagemonitor  compare over simple JMX metrics?,0.0,0.0,1.0,0.0
stagemonitor,"Unlike java-native JMX MBeans, Stagemonitor includes an agent that sits in your Java application, sending metrics and request traces at the central database, which can be Elasticsearch.",0.0,0.0,1.0,0.0
stagemonitor,Since both ways can serve as input for an ELK Monitoring Stack (JMX see this blog  post ) what are the benefits of Stagemonitor?,0.0,0.106,0.894,0.3818
stagemonitor,I am trying to using stagemonitor for get metrics for different methods.,0.0,0.0,1.0,0.0
stagemonitor,I used sample PetClinic application locally for get a idea.,0.0,0.0,1.0,0.0
stagemonitor,"I want to get metrics only what i need , not all of them is any possible to do that.",0.0,0.075,0.925,0.0772
stagemonitor,I change some code for my testing purpose,0.0,0.0,1.0,0.0
stagemonitor,"when i click test button in html it will go to this method and i can get the metrics form stagemonitor.If i not need to see this method metrics ,how to stop showing that in stagemonitor",0.064,0.0,0.936,-0.296
stagemonitor,@Felix,0.0,0.0,1.0,0.0
stagemonitor,I trying to use browser-widget in example and i change code for testing.,0.0,0.0,1.0,0.0
stagemonitor,"This output I'm getting i don't want to see details about Testing.I only want to see some method i need, even other methods used i don't want to see about them.",0.128,0.0,0.872,-0.1695
stagemonitor,is it possible  ?,0.0,0.0,1.0,0.0
stagemonitor,?,0.0,0.0,0.0,0.0
stagemonitor,I am trying stagemonitor with standalone applicaion.,0.0,0.0,1.0,0.0
stagemonitor,And i get metrics for jmx reporter correctly.,0.0,0.0,1.0,0.0
stagemonitor,I want to get that in browser-widget.,0.0,0.206,0.794,0.0772
stagemonitor,I saw config  stagemonitor.web.widget.enabled = true  will enable this.,0.0,0.318,0.682,0.4215
stagemonitor,But how to get dashbord.,0.0,0.0,1.0,0.0
stagemonitor,Do we have to use grafana for this ?,0.0,0.0,1.0,0.0
stagemonitor,I'm trying to get  www.stagemonitor.org  working with Grails.,0.0,0.0,1.0,0.0
stagemonitor,I've created a sample project here:  https://github.com/jbwiv/teststagemonitor,0.0,0.286,0.714,0.25
stagemonitor,I've added stagemonitor to grails-app/conf/BuildConfig.groovy as both a compile and runtime dependency.,0.0,0.0,1.0,0.0
stagemonitor,"It indeed gets installed to my maven directory after calling ""grails refresh-dependencies"":",0.0,0.0,1.0,0.0
stagemonitor,"I've also placed a stagemonitor.properties file in src/java, which at runtime gets moved to target/work/resources/stagemonitor.properties.",0.0,0.0,1.0,0.0
stagemonitor,I believe I have that properties file configured properly.,0.0,0.0,1.0,0.0
stagemonitor,I've installed the templates Grails uses for web.xml and modified to insure metadata-complete=true is not present in the generated web.xml.,0.0,0.0,1.0,0.0
stagemonitor,"However, after  grails run-app  and navigating to  http://localhost:8080/main/index , I get my index page as expected, but no stagemonitor icon to click and it appears no stagemonitor assets are included.",0.172,0.063,0.766,-0.5499
stagemonitor,I would like to disable the browser widget in production release.,0.0,0.217,0.783,0.3612
stagemonitor,I would like to define the disabling the browser widget in  stagemonitor.properties .,0.212,0.171,0.616,-0.1531
stagemonitor,"Is there any java system properties like as "" stagemonitor.browserwidget.activate=false """,0.0,0.238,0.762,0.3612
stagemonitor,Using a multi-tenant server is it possible to use these tools to get stats regarding specific clients performance usage.,0.0,0.0,1.0,0.0
stagemonitor,The setup would be a MySQL DB which holds users belonging to organisations.,0.0,0.0,1.0,0.0
stagemonitor,When the Java application is running all actions will be carried out by a User collection which has the Organisation ID variable.,0.0,0.0,1.0,0.0
stagemonitor,"Could this data then be used to work out how much CPU, memory, heap, processes etc are being used per Organisation?",0.0,0.0,1.0,0.0
stagemonitor,Thanks,0.0,1.0,0.0,0.4404
stagemonitor,I am using stagemonitor for get metrics for different methods.,0.0,0.0,1.0,0.0
stagemonitor,"I'm using PetClinic application locally for get a idea and using browser-widget for testing.I find that we can track metrics using @Timed , @Metered annotations.",0.0,0.0,1.0,0.0
stagemonitor,Is it possible to use them in the petclinic application and view them from browser-widget.,0.0,0.0,1.0,0.0
stagemonitor,I have a Spring MVC web application running on tomcat.,0.0,0.0,1.0,0.0
stagemonitor,"I need to monitor my application for performance, log the time taken by each method call along with the values of the parameters.",0.0,0.114,0.886,0.4019
stagemonitor,"I would need this logging for all the methods in all the controllers, services, util classes inside the application.",0.0,0.0,1.0,0.0
stagemonitor,I have seen the question posted earlier here :  How to log the time taken by methods in Springframework?,0.0,0.0,1.0,0.0
stagemonitor,"As for the solutions proposed for that question., I have the following concerns in my case.",0.0,0.108,0.892,0.1779
stagemonitor,1) Using Spring AOP for logging - Closely matches the requirements but as far as I know it requires adding annotations to each and every method - would prefer to avoid changing current application.,0.085,0.0,0.915,-0.4215
stagemonitor,2) Stagemonitor - Could not follow the installation instructions - it requires installation of docker which I could not because of OS limitation.,0.104,0.0,0.896,-0.296
stagemonitor,I am working on openSUSE 11.3 where as docker is available for openSUSE 12.3+,0.0,0.0,1.0,0.0
stagemonitor,3) SpringInsight - It is a great tool and exactly matches my requirements.,0.0,0.291,0.709,0.6249
stagemonitor,"But the problem is, it runs is vfabric-tc-server instance.",0.307,0.0,0.693,-0.5499
stagemonitor,I tried setting up it on tomcat 7 using the steps mentioned by Daniel in  Using Spring Insight with Tomcat 6  but it did not workout as none of the jars in insight application has the class com.springsource.insight.collection.tcserver.ltw.TomcatWeavingInsightClassLoader which was supposed to be referred from server.xml.,0.0,0.0,1.0,0.0
stagemonitor,Tried adding external jar but it did not work.,0.0,0.0,1.0,0.0
stagemonitor,I'm wondering if there are any other tools which,0.0,0.0,1.0,0.0
stagemonitor,-- will not require changing existing application MUCH - simple configuration should be acceptable.,0.0,0.161,0.839,0.3182
stagemonitor,-- will give method level performance monitoring .,0.0,0.0,1.0,0.0
stagemonitor,-- strictly should not need to migrate the existing applications to other server.,0.0,0.0,1.0,0.0
stagemonitor,Thanks in advance :),0.0,0.747,0.253,0.7096
tanzu,I am working on Springboot REST microservice.,0.0,0.0,1.0,0.0
tanzu,Facing issue while doing clear region on partition region type.,0.0,0.224,0.776,0.3818
tanzu,How to configure Gemfire cache so that my client should be able to add region to server while startup.,0.0,0.0,1.0,0.0
tanzu,Also I am thinking of exposing an API to evict cache explicitly.,0.174,0.0,0.826,-0.2732
tanzu,Is there a way we can do it?,0.0,0.0,1.0,0.0
tanzu,We had Redis cache implemented now but we want to switch to gemfire as it offers nice GUI to track regions and data using PULSE.,0.0,0.183,0.817,0.631
tanzu,Questions:,0.0,0.0,1.0,0.0
tanzu,Code:,0.0,0.0,1.0,0.0
tanzu,"Could deploy Bosh and small footprint tanzu application service(tas) in Azure, without using the domains.All Vms are running.Can i access the ccapi and apps manager with the IP address instead of the api.SYSTEMDOMAIN?",0.0,0.0,1.0,0.0
tanzu,This is a follow-up question of  How to implement HTTP request/reply when the response comes from a rabbitMQ reply queue using Spring Integration DSL?,0.0,0.0,1.0,0.0
tanzu,.,0.0,0.0,0.0,0.0
tanzu,We were able to build the Spring Integration application and the SCDF stream successfully locally.,0.0,0.186,0.814,0.4939
tanzu,We could send a http request to the rabbitMQ request queue which was bound to the SCDF stream rabbit source.,0.0,0.0,1.0,0.0
tanzu,We could also receive the response back from the rabbitMQ response queue which was bound to the SCDF stream rabbit sink.,0.0,0.0,1.0,0.0
tanzu,We have deployed the SCDF stream into PCF environment which had a binding of an internal rabbitMQ broker.,0.0,0.0,1.0,0.0
tanzu,"Now we need to specify the spring rabbitMQ connection information in the Spring Integration application properties - currently it's using the default localhost@5762, which is no longer valid.",0.078,0.0,0.922,-0.296
tanzu,Does anyone know how to get this rabbitMQ configuration properties?,0.0,0.0,1.0,0.0
tanzu,We already checked the SCDF stream rabbit source/sink log files but couldn't find the information.,0.0,0.0,1.0,0.0
tanzu,"I know we probably need to check internally whoever set up the SCDF/rabbitMQ in PCF environment, but so far we haven't heard the answers from them.",0.0,0.0,1.0,0.0
tanzu,"Also, it appears we can have a different approach that binds both the SCDF stream and the integration application to a separate rabbitMQ instance (instead of using the existing one bundled with the SCDF configuration).",0.0,0.0,1.0,0.0
tanzu,Is it a recommended solution?,0.0,0.672,0.328,0.4767
tanzu,"Thanks,",0.0,1.0,0.0,0.4404
tanzu,I am autoscaling my application based on the HTTP throughput.,0.0,0.0,1.0,0.0
tanzu,My question here is when it reaches min threshold it tries to reduce the instance created.,0.0,0.186,0.814,0.296
tanzu,But during reducing the instance count if my instance is running or it is processing prev HTTP request.,0.0,0.0,1.0,0.0
tanzu,"In this case, it will wait till the processing completes or it forcibly reduces the instance count when reached threshold.",0.0,0.069,0.931,0.1027
tanzu,"I'm trying to run E2E Kubernetes tests using Sonobuoy, but if one of the nodes has custom taints (NoSchedule) I'm getting",0.0,0.0,1.0,0.0
tanzu,Kubernetes version is 1.12.,0.0,0.0,1.0,0.0
tanzu,If I remove taints in my test cluster E2E tests passed successfully.,0.0,0.242,0.758,0.4939
tanzu,I've found that it was fixed in 1.17 version (see:  https://github.com/vmware-tanzu/sonobuoy/issues/599   https://github.com/kubernetes/kubernetes/issues/74282   https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.17.md  ),0.0,0.0,1.0,0.0
tanzu,Is there a workaround to run e2e tests on production 1.12 Kubernetes cluster having node taints?,0.0,0.0,1.0,0.0
tanzu,Except for waiting for 1.17 version and cluster upgrade of course.,0.0,0.0,1.0,0.0
tanzu,"Today I created a PCF Account (for testing purposes) and it will not let me do anything , it is telling me this:",0.0,0.095,0.905,0.25
tanzu,"&quot;We will no longer be accepting any new PWS account sign-ups after September 17, 2020.",0.124,0.146,0.73,0.1027
tanzu,Please do not hesitate to contact us with any questions.,0.0,0.34,0.66,0.4791
tanzu,"If you are interested in an enterprise-grade service for hosting applications, Tanzu Application Service (TAS) offers this capability.",0.0,0.137,0.863,0.4019
tanzu,The VMware Team&quot;,0.0,0.0,1.0,0.0
tanzu,Does this mean that I need to move my production applications from PCF to VMware Tanzu ?,0.0,0.0,1.0,0.0
tanzu,Is PCF Dead?,0.683,0.0,0.317,-0.6486
tanzu,"So, I have followed the documentation in:",0.0,0.0,1.0,0.0
tanzu,"And I am still at a loss of how do I install Velero using its helm chart at  https://github.com/vmware-tanzu/helm-charts , because I cannot reconcile the helm chart based installation with what is documented in
 https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure",0.073,0.0,0.927,-0.3182
tanzu,Here is what I have done so far:,0.0,0.0,1.0,0.0
tanzu,"Now I need to update the  values.yaml  file, but I am stuck at the credentials section.",0.161,0.0,0.839,-0.3612
tanzu,I am unable to reconcile the  velero install  instructions given on  https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure  with what I see in the values.yaml file.,0.0,0.0,1.0,0.0
tanzu,E.g.,0.0,0.0,1.0,0.0
tanzu,I have created the  credentials-velero  file:,0.0,0.333,0.667,0.25
tanzu,"Of course, it does not contain the MSI credentials and the MSI name would be associated using the dedicated label.",0.0,0.136,0.864,0.4588
tanzu,The  doc  says clearly:,0.0,0.474,0.526,0.4019
tanzu,"If you're using AAD Pod Identity, you now need to add the
aadpodidbinding=$IDENTITY_NAME label to the Velero pod(s), preferably
through the Deployment's pod template.",0.0,0.0,1.0,0.0
tanzu,But how do I do it when I install Velero using a helm chart?,0.0,0.0,1.0,0.0
zipkin,I have client application instrumented with Zipkin library with configuration in spring application.properties .,0.0,0.0,1.0,0.0
zipkin,Maven dependency,0.0,0.0,1.0,0.0
zipkin,The hawkular apm server console is reachable from the local machine.,0.0,0.0,1.0,0.0
zipkin,"However, when the rest api exposed in the client application is invoked, the zipkin trace is logged but they are not collected at the hawkular apm server.",0.042,0.0,0.958,-0.0387
zipkin,"I am not sure if its a configuration issue at client application, as the Hawkular APM UI is opening properly.",0.103,0.0,0.897,-0.2411
zipkin,"As per my understanding, Zipkin client can be integrated with Hawkular apm by simple replacing the hawkular url in place of zipkin server, but this does not seem to work.",0.0,0.0,1.0,0.0
zipkin,"Any suggestions on this, unfortunately I could not find any examples too.",0.216,0.0,0.784,-0.4118
zipkin,"Sleuth is not sending the trace information to Zipkin, even though Zipkin is running fine.",0.0,0.114,0.886,0.2023
zipkin,"I am using Spring 1.5.8.RELEASE, spring cloud Dalston.SR4 and I have added the below dependencies in my microservices:",0.0,0.0,1.0,0.0
zipkin,"My Logs are always coming false:
[FOOMS,2e740f33c26e286d,2e740f33c26e286d,false]",0.0,0.0,1.0,0.0
zipkin,My Zipkin dependencies are:,0.0,0.0,1.0,0.0
zipkin,Why am I getting false instead of true in my slueth statements?,0.0,0.219,0.781,0.4215
zipkin,The traceId and SpanId are properly generated for all the calls though.,0.0,0.0,1.0,0.0
zipkin,My Zipkin is running in port 9411,0.0,0.0,1.0,0.0
zipkin,Does anybody know where the zipking examples are located ?,0.0,0.0,1.0,0.0
zipkin,"https://twitter.github.io/zipkin/Quickstart.html#super-quickstart 
 In the following I can read:",0.0,0.0,1.0,0.0
zipkin,I have found the zipking-example on Maven Central only.,0.0,0.0,1.0,0.0
zipkin,Not on Github.,0.0,0.0,1.0,0.0
zipkin,"1.2.1-rc24 
 It is still a bug in the documentation?",0.0,0.0,1.0,0.0
zipkin,Documentation mentions zipkin-example which doesn't exist,0.0,0.0,1.0,0.0
zipkin,I wanted to update my project Zipkin setup to Spring Boot 2.2.2.RELEASE and Spring Cloud Hoxton.RELEASE but it looks like simple jars update is not enough.,0.0,0.119,0.881,0.5023
zipkin,I thought the old setup (it was working fine for Spring Boot 2.1.5.RELEASE and Greenwich.SR2) would also work for Boot 2.2.2.RELEASE and Hoxton.RELEASE but it appears I still miss something here.,0.063,0.046,0.891,-0.128
zipkin,"I'm getting the following exception ( java.lang.NoClassDefFoundError: zipkin2
/internal/Buffer$Writer ):",0.259,0.0,0.741,-0.4215
zipkin,Can't see Zipkin Server when using Spring Initializer .,0.0,0.0,1.0,0.0
zipkin,Has it been removed?,0.0,0.0,1.0,0.0
zipkin,What is the alternative?,0.0,0.0,1.0,0.0
zipkin,I have a Spring Boot app that I'm updating to 1.5.1.,0.0,0.0,1.0,0.0
zipkin,"It works great, until I add Sleuth and Zipkin to classpath",0.0,0.313,0.687,0.6249
zipkin,"when these lines are present, I get",0.0,0.0,1.0,0.0
zipkin,This is my dep.,0.0,0.0,1.0,0.0
zipkin,management,0.0,0.0,1.0,0.0
zipkin,I tried change to Dalston,0.0,0.0,1.0,0.0
zipkin,but the errors get even stranger,0.383,0.0,0.617,-0.4767
zipkin,and,0.0,0.0,1.0,0.0
zipkin,am I missing something I haven't noticed yet?,0.306,0.0,0.694,-0.296
zipkin,Which exact dependencies and  application.yml  configuration are required for Spring Boot/Cloud Zipkin server (potentially Zipkin Stream server) to persist the tracing data using MySQL?,0.0,0.0,1.0,0.0
zipkin,When I tried to integrate zipkin.,0.0,0.0,1.0,0.0
zipkin,It threw this error,0.51,0.0,0.49,-0.481
zipkin,version:,0.0,0.0,1.0,0.0
zipkin,"Prerequisites: 
 Node.js  application 
 Opencensus  library 
 Zipkin Exporter  and local Zipkin service",0.0,0.0,1.0,0.0
zipkin,app.js :,0.0,0.0,1.0,0.0
zipkin,package.json :,0.0,0.0,1.0,0.0
zipkin,Zipkin  server started locally with command:,0.0,0.0,1.0,0.0
zipkin,"after triggering  /service1  Zipkin Ui displays 2 spans for 2 different requests: 
first  /service1  incoming request that is configured in Node.js routers 
second  /external_service_2  is subsequent call to external service",0.0,0.0,1.0,0.0
zipkin,Problem,1.0,0.0,0.0,-0.4019
zipkin,"The problem is that after triggering  /service1 : 
  1.",0.278,0.0,0.722,-0.4019
zipkin,"Zipkin UI displays 2 spans with same name  MyApplication (see image),  but expected 2 different span names",0.0,0.0,1.0,0.0
zipkin,2.,0.0,0.0,1.0,0.0
zipkin,"As far Zipkin UI displays 2 spans with same name,  service dependencies page contains one Service only(see image)",0.0,0.0,1.0,0.0
zipkin,We have been playing around with Brave(Java implementation of Zipkin) and successfully added tracing for REST and database calls.,0.0,0.227,0.773,0.6124
zipkin,We would like to also add RabbitMQ to the tracing and would like some thoughts from anyone who may have had similar experiences that they could share.,0.0,0.231,0.769,0.7351
zipkin,We have tried to find some stuff online but can't seem to find an interceptor we could add to our rabbit implementation.,0.0,0.0,1.0,0.0
zipkin,Can you recommend anything?,0.0,0.455,0.545,0.3612
zipkin,Thanks in advance.,0.0,0.592,0.408,0.4404
zipkin,"I read about zipkin, but from my understanding, zipkin is suitable for tracking history of network requests and time (via Finagle).",0.0,0.0,1.0,0.0
zipkin,"However, is it possible for me to use zipkin to track java method invocation time and location?",0.0,0.0,1.0,0.0
zipkin,"For example, I want to track how long it takes for  foobar()  to execute, and what are other methods internally called by  foobar()  and its execution time and so on.",0.0,0.044,0.956,0.0772
zipkin,I have multiservices application which is using Spring Cloud OpenFeign.,0.0,0.0,1.0,0.0
zipkin,Now I have to use zipkin with that app.,0.0,0.0,1.0,0.0
zipkin,I remember that when i had app without Feign I just added Sleuth and Zipkin starters dependencies and run zipkin server on port 9411.,0.0,0.0,1.0,0.0
zipkin,After that Zipkin worked well..,0.0,0.0,1.0,0.0
zipkin,"But now, when i try same in my app with Feign i get error 500  ""original request is required"" .",0.165,0.137,0.698,-0.1531
zipkin,I guess that Feign has some problems with headers when Sleuth add traces informations.,0.184,0.0,0.816,-0.4019
zipkin,Can you help me fix this?,0.0,0.351,0.649,0.4019
zipkin,I'm new to zipkin and brave api for distribute tracing.,0.0,0.274,0.726,0.5267
zipkin,I've setup a zipkin server on my localhost listening on port 9411.,0.0,0.0,1.0,0.0
zipkin,I've executed below function but there is no trace data show in my zipkin server.,0.167,0.0,0.833,-0.4215
zipkin,Could someone point out what I'm missing?,0.268,0.0,0.732,-0.296
zipkin,I want to use zipkin to profile the internals of a traditional program.,0.0,0.115,0.885,0.0772
zipkin,"I use the term ""traditional"", since AFAIK zipkin is for tracing in a microservice environment where one request gets computed by N sub-requests.",0.0,0.0,1.0,0.0
zipkin,I would like to analyse the performance of my python program.,0.0,0.217,0.783,0.3612
zipkin,I would like to trace all python method calls and all linux syscalls which gets done.,0.0,0.152,0.848,0.3612
zipkin,How to trace the python method calls and linux syscalls to get the spans into zipkin?,0.0,0.0,1.0,0.0
zipkin,"Even if it is not feasible, I am interesting how this could be done.",0.158,0.0,0.842,-0.3089
zipkin,I would like to learn how zipkin works.,0.0,0.294,0.706,0.3612
zipkin,"So, we are using kafka queues internally for some microservices' communication, also zipkin for distributed tracing.",0.0,0.0,1.0,0.0
zipkin,Would you suggest how to bring in kafka traces in zipkin server for debugability.,0.0,0.0,1.0,0.0
zipkin,"I came across the  brave-kafka-interceptor , but could not understand it with with kafka from the minimal example provided.",0.0,0.0,1.0,0.0
zipkin,"Is there any other example around, or something altogether different library is used.",0.0,0.0,1.0,0.0
zipkin,I am using py_zipkin in my code.,0.0,0.0,1.0,0.0
zipkin,And I can see the tracing result on the Zipkin UI.,0.0,0.0,1.0,0.0
zipkin,"But I don't know how to output the tracing results to a file with specified format, like a log file.",0.0,0.169,0.831,0.5023
zipkin,Here is a example of my code:,0.0,0.0,1.0,0.0
zipkin,Small question about the possibility to integrate Zipkin with Prometheus.,0.0,0.0,1.0,0.0
zipkin,"Currently, we have a working Zipkin instance fully ready, with its web UI.",0.0,0.202,0.798,0.4201
zipkin,"Zipkin is super cool, everything is fine.",0.0,0.667,0.333,0.7906
zipkin,"We are able to have all micro services sending traces to Zipkin, and having Zipkin aggregating them.",0.0,0.0,1.0,0.0
zipkin,"We can also search the traces in the UI, etc, super cool.",0.0,0.383,0.617,0.7351
zipkin,"On the other hand, we also have a very mature battle tested Prometheus Grafana, where container level metrics, application level metrics, and many other observations are already present in it.",0.082,0.179,0.739,0.5291
zipkin,"Hence, currently, we have two places where we have to look at for production.",0.0,0.0,1.0,0.0
zipkin,"Our everything in one place Prometheus, and this super cool Zipkin.",0.0,0.435,0.565,0.7861
zipkin,"I was wondering, would it be possible to have Prometheus as the back end, or some kind of Prometheus consuming Zipkin data to display in Grafana, so we truly have all in one place please?",0.0,0.146,0.854,0.6682
zipkin,Thank you,0.0,0.714,0.286,0.3612
zipkin,I am trying to add tracing on a Wildfly server (specifically Keycloak Docker image),0.0,0.0,1.0,0.0
zipkin,Following this document  https://docs.wildfly.org/19/Admin_Guide.html#MicroProfile_OpenTracing_SmallRye,0.0,0.0,1.0,0.0
zipkin,I got as far as,0.0,0.0,1.0,0.0
zipkin,But I can't get the next parts working to set it to point to zipkin:9411,0.0,0.0,1.0,0.0
zipkin,The next command in the instructions failed,0.355,0.0,0.645,-0.5106
zipkin,"However, doing it using  /opt/jboss/startup-scripts/  also fails",0.318,0.0,0.682,-0.4215
zipkin,Using @ehsavoie answer I got a bit further,0.0,0.0,1.0,0.0
zipkin,but still does not log to zipkin which uses B3.,0.0,0.0,1.0,0.0
zipkin,I also tried,0.0,0.0,1.0,0.0
zipkin,When i run my spring boot application locally i always have trace information with exportable information = true,0.0,0.167,0.833,0.4215
zipkin,but when app running on AWS ECS in docker container i have always exportable false for all logs,0.0,0.0,1.0,0.0
zipkin,except classpath  org.hibernate .,0.0,0.0,1.0,0.0
zipkin,On this classpath exportable = true,0.0,0.412,0.588,0.4215
zipkin,I use below dependencies,0.0,0.0,1.0,0.0
zipkin,And below properties :,0.0,0.0,1.0,0.0
zipkin,Which may be the reason for different actions ?,0.0,0.0,1.0,0.0
zipkin,Maybe log levels ?,0.0,0.0,1.0,0.0
zipkin,base-url:  http://zipkin-server   must be reachable ?,0.0,0.0,1.0,0.0
zipkin,"i am running zipkin via docker with this command docker  run -d -p 9411:9411 openzipkin/zipkin  and accessing its server at  http://localhost:9411/zipkin/  
I am using playframework-2.4 i am not getting the service name in zipkin ui also trace data is now showing up it shows  0 of 0 services",0.0,0.0,1.0,0.0
zipkin,"here is my code 
application.conf",0.0,0.0,1.0,0.0
zipkin,build.sbt,0.0,0.0,1.0,0.0
zipkin,"i am accessing  http://localhost:9000/direct-user/test1  it first then  http://localhost:9411  but trace data is not showing up 
is there any thing missing ?please help",0.103,0.238,0.659,0.5719
zipkin,I need to send spans through RabbitMQ to Zipkin.,0.0,0.0,1.0,0.0
zipkin,I'm Using Spring-Cloud-Sleuth Edgware-SR5 version and SpringBoot 1.5.3.RELEASE versions.,0.0,0.0,1.0,0.0
zipkin,With older Spring-cloud sleuth version (spring-cloud-stream-binder-rabbit -  v1.1.4.RELEASE) it was working fine.,0.0,0.153,0.847,0.2023
zipkin,"When I try to start the service, I'm getting ""  ""AsyncReporter{RabbitMQSender{addresses=[localhost:5672], queue=zipkin}}.",0.0,0.0,1.0,0.0
zipkin,"Unable to establish connection to RabbitMQ server"" error.",0.278,0.0,0.722,-0.4019
zipkin,"I have gone through the documentations, but I could't able to resolve this issue.",0.0,0.236,0.764,0.5267
zipkin,Gradle Configuration:,0.0,0.0,1.0,0.0
zipkin,Application.yml:,0.0,0.0,1.0,0.0
zipkin,Exception StackTrace:,0.0,0.0,1.0,0.0
zipkin,"Thanks and Regards
Suresh",0.0,0.492,0.508,0.4404
zipkin,"I want to retrieve the TraceId of Zipkin, is there any method to get it ?",0.0,0.091,0.909,0.0772
zipkin,first i had a small issue with this class   brave.sampler.Sampler,0.0,0.0,1.0,0.0
zipkin,"could not import this class,  only imported when i added this dependency",0.0,0.0,1.0,0.0
zipkin,"and my big problem is, when i tried to use zipkin  for disturbed tracing, i added the required dependency but whenever i start the applications, it through an exception in start.",0.123,0.0,0.877,-0.3919
zipkin,and this is the stack trace.,0.0,0.0,1.0,0.0
zipkin,my pom.xml,0.0,0.0,1.0,0.0
zipkin,"i would someone to help me fix those issues, also i want to understand why this exception comes, and why the sampler class does not imported only when i add it's dependency, but i see in other projects codes there are no needs for the dependency.",0.062,0.067,0.871,-0.2023
zipkin,"I have enabled the distributed tracing using Zipkin tracer as mentioned in 
 https://github.com/openzipkin/brave  for the microservices.",0.0,0.0,1.0,0.0
zipkin,I could see the service call info and time taken in the Zipkin server running in local machine.,0.0,0.0,1.0,0.0
zipkin,"I have usecase to capture the trace logs of every http requests alongside the JSON messages (with the Trace ,span and parent IDs) received by to Zipkin server from the client.",0.0,0.0,1.0,0.0
zipkin,"Zipkin server is started in debug mode to enable the logging, however the http requests are not logged in the Zipkin server logs.",0.0,0.0,1.0,0.0
zipkin,Can someone throw some light please?,0.0,0.315,0.685,0.3182
zipkin,"Trying to create zipkin server with the dependencies added in gradle as below,",0.0,0.149,0.851,0.2732
zipkin,"Also,",0.0,0.0,1.0,0.0
zipkin,"i have added properties in both application.properties and bootstrap.properties files like,",0.0,0.217,0.783,0.3612
zipkin,application.properties,0.0,0.0,1.0,0.0
zipkin,bootstrap.properties,0.0,0.0,1.0,0.0
zipkin,"Once i start the server and load the UI page i am getting error in UI as,",0.162,0.0,0.838,-0.4019
zipkin,Can istio send tracing information to an external zipkin instance?,0.0,0.0,1.0,0.0
zipkin,It seems like istio has hard coded zipkin address several places to expect it in the istio-system namespace(by referencing it without a namespace).,0.059,0.105,0.837,0.2732
zipkin,By default Spring Sleuth only sends 10% of requests to Zipkin.,0.0,0.0,1.0,0.0
zipkin,By setting  spring.sleuth.sampler.percentage  you can increase the percentage.,0.0,0.247,0.753,0.3182
zipkin,Unfortunately it is stuck at 10% regardless of what value I set it to.,0.262,0.143,0.595,-0.25
zipkin,"I have tried 1.0, 0.5, 1, 100.",0.0,0.0,1.0,0.0
zipkin,Output from  /env,0.0,0.0,1.0,0.0
zipkin,"Regardless of the value, when I make multiple requests, only 10% make it to Zipkin.",0.0,0.156,0.844,0.34
zipkin,We are using version Finchley.M8 of Spring Cloud and 2.0.0.RELEASE of Spring Boot.,0.0,0.0,1.0,0.0
zipkin,Below are relevant POM settings.,0.0,0.0,1.0,0.0
zipkin,Could this be a bug?,0.0,0.0,1.0,0.0
zipkin,I want to load my Spring Cloud zipkin-server with elasticsearch.,0.0,0.14,0.86,0.0772
zipkin,"I think, I tried almost everything I could.",0.0,0.0,1.0,0.0
zipkin,"but, It still running with in-memory.",0.0,0.0,1.0,0.0
zipkin,"(when I restart zipkin-server, all data is lost.)",0.0,0.0,1.0,0.0
zipkin,I want to set up zipkin with elasticsearch.,0.0,0.178,0.822,0.0772
zipkin,Please tell me which exact  dependencies  and  applicartion.yml  or any other things needed.,0.0,0.161,0.839,0.3182
zipkin,"I've enabled zipkin on my application and it works fine, I see the traces.",0.0,0.13,0.87,0.2023
zipkin,"My application is using Consul service discovery, and I see a lot of traffic being traced in Zipkin.",0.0,0.0,1.0,0.0
zipkin,"Traces are like have names like ""catalog-services_watch"" and contain things like :",0.0,0.484,0.516,0.7579
zipkin,How can I disable these traces ?,0.0,0.0,1.0,0.0
zipkin,"I've tried the spring.sleuth.instrument.web.skipPattern parameter, but it's not working.",0.0,0.0,1.0,0.0
zipkin,"We have a socket based web app we currently developing using feathersJS, and we are currently leaning on using zipkin for performance tracking, but it seems that there's no instrumentation yet for socket based app, anyone have implemented Zipkin on socket based webapps?",0.064,0.0,0.936,-0.4215
zipkin,or any alternatives  you recommend?,0.0,0.385,0.615,0.3612
zipkin,Thanks you so much.,0.0,0.492,0.508,0.4404
zipkin,I'm using these dependencies:,0.0,0.0,1.0,0.0
zipkin,Is there a possibility to add the current active profile(s) to each log line?,0.0,0.184,0.816,0.4019
zipkin,This would make it possible to filter logs based on the profiles in Splunk/ELK/...,0.0,0.0,1.0,0.0
zipkin,So instead of,0.0,0.0,1.0,0.0
zipkin,it should log,0.0,0.0,1.0,0.0
zipkin,"EDIT: 
Based on Marcin's answer, I implemented it as follows:",0.0,0.0,1.0,0.0
zipkin,application.yml,0.0,0.0,1.0,0.0
zipkin,ProfileLogger.java,0.0,0.0,1.0,0.0
zipkin,LogConfig.java,0.0,0.0,1.0,0.0
zipkin,This prints logs like the following:,0.0,0.333,0.667,0.3612
zipkin,This is already good but not completely what I'm looking for yet.,0.0,0.151,0.849,0.2382
zipkin,"I'd like to add the profile from the beginning -  even the ""Started Application"" should contain the profile - if possible.",0.0,0.122,0.878,0.3612
zipkin,"Secondly, I'd like to move the  profiles  between  INFO  and  22481 .",0.0,0.2,0.8,0.3612
zipkin,One more question came up during implementation: In the linked implementation there is this statement:,0.0,0.0,1.0,0.0
zipkin,does that mean you only send traces if log-level is set to TRACE?,0.0,0.0,1.0,0.0
zipkin,"If so, how could I improve logging to stdout with that approach (given a log-level of debug/info/warn)?",0.0,0.184,0.816,0.4877
zipkin,"I think the log-pattern is overriden by Sleuth/Zipkin upon importing the dependencies and thus, local logging looks the same as tracing.",0.0,0.0,1.0,0.0
zipkin,Eventually I'm interested in having the profile displayed in local stdout as well as in Zipkin.,0.0,0.255,0.745,0.5859
zipkin,"EDIT 2:  With the help of Marcin, I have changed the pattern by introducing a  resources/logback-spring.xml  file containing these lines:",0.0,0.137,0.863,0.4019
zipkin,Note that you have to add a  bootstrap.yml  file too in order to have the application name correctly displayed.,0.0,0.0,1.0,0.0
zipkin,"Without a  bootstrap.yml  file, the above log-pattern just prints ""bootstrap"" as application name.",0.0,0.0,1.0,0.0
zipkin,The  bootstrap.yml  just contains,0.0,0.0,1.0,0.0
zipkin,in my case.,0.0,0.0,1.0,0.0
zipkin,Everything else is configured in application-[profile].yml,0.0,0.0,1.0,0.0
zipkin,Now everything works as desired:,0.0,0.344,0.656,0.2732
zipkin,I want to monitor some Windows Services with either perfino or Zipkin.,0.0,0.115,0.885,0.0772
zipkin,Does anyone know if that is possible?,0.0,0.0,1.0,0.0
zipkin,Cheers.,0.0,1.0,0.0,0.4767
zipkin,Has anyone else encountered the following problem with using Zipkin &amp; Spring Cloud Sleuth?,0.172,0.0,0.828,-0.4019
zipkin,Seems to be a problem posting out data to my localhost Zipkin server.,0.197,0.0,0.803,-0.4019
zipkin,Is there any need to configure proxy settings on Zipkin?,0.0,0.0,1.0,0.0
zipkin,I am using zipkin to track the requests across microservices.,0.0,0.0,1.0,0.0
zipkin,One of my service is running jobs using a thread pool.,0.0,0.0,1.0,0.0
zipkin,How do I transfer the zipkin header values to the threads?,0.0,0.231,0.769,0.4019
zipkin,is there a Zipkin wrapped thread pool/executor available?,0.0,0.0,1.0,0.0
zipkin,I would like to implement tracing in my microservices architecture.,0.0,0.238,0.762,0.3612
zipkin,I am using Apache Kafka as message broker and I am not using Spring Framework.,0.0,0.0,1.0,0.0
zipkin,Tracing is a new concept for me.,0.0,0.0,1.0,0.0
zipkin,"At first I wanted to create my own implementation, but now I would like to use existing libraries.",0.0,0.255,0.745,0.5859
zipkin,Brave looks like the one I will want to use.,0.0,0.545,0.455,0.7351
zipkin,"I would like to know if there are some guides, examples or docs on how to do this.",0.0,0.135,0.865,0.3612
zipkin,"Documentation on Github page is minimal, and I find it hard to start using Brave.",0.083,0.202,0.714,0.4588
zipkin,"Or maybe there is better library with proper documentation, that is easier to use.",0.0,0.322,0.678,0.6908
zipkin,I will be looking at Apache HTrace because it looks promising.,0.0,0.231,0.769,0.4019
zipkin,Some getting started guides will be nice.,0.0,0.318,0.682,0.4215
zipkin,Did saw  this ...,0.0,0.0,1.0,0.0
zipkin,"But I'm not able to make it run, whatever I have tried, I get either still on localhost, either an exception on armeria bind ( I have stuff running on :8080) and server crash...",0.0,0.0,1.0,0.0
zipkin,"In short what I have tried (Windows Server 2016, so no Linux Docker containers ) with no avail.",0.291,0.0,0.709,-0.6939
zipkin,Variation on these batch file commands:,0.0,0.0,1.0,0.0
zipkin,"SET &quot;SERVER_ADDRESS=xx.xx.xx.xx&quot; 
SET &quot;ZIPKIN_HOST=xx.xx.xx.xx&quot; 
java -jar zipkin-server-2.23.2-exec.jar --armeria.ports[0].port=9411 --&gt; armeria.ports[0].protocols[0]=http",0.0,0.0,1.0,0.0
zipkin,"It should be simple to run Zipkin on another ip, but I'm fighting.",0.213,0.0,0.787,-0.5023
zipkin,"Could you help me, maybe I'm missing something absolutely obvious...",0.171,0.209,0.62,0.128
zipkin,"But is a pretty common scenario, and is not that well documented.",0.143,0.277,0.58,0.473
zipkin,"We have around  10 Spring boot microservices , which communicate with each other via kafka.",0.0,0.0,1.0,0.0
zipkin,"The logs of each microservice are sent to Kibana, and in case of any errors, we have to sift through Kibana logs.",0.103,0.0,0.897,-0.34
zipkin,"The good thing is:  at the start of any flow, a message-id is generated by one of our microservices, and that is propagated to all the others as part of the message transfer (which happens through kafka), so we can search for the message-id in the logs, and we can see the footprint of that flow across all our microservices.",0.0,0.048,0.952,0.4404
zipkin,The bad part:  having to sift through tons of logs to get a basic idea of where things broke and why.,0.259,0.0,0.741,-0.743
zipkin,"So I was wondering if we can have some distributed tracing implemented, maybe through Zipkin (or some other open-tracing framework) that can work with the message-id that our ecosystem already produces, instead of generating a new one ?",0.0,0.0,1.0,0.0
zipkin,Thank you for your time :),0.0,0.579,0.421,0.6705
zipkin,Traces that should have been sent by dapr runtime to zipkin server somehow fails to reach it.,0.148,0.058,0.794,-0.4019
zipkin,The situation is the following:,0.0,0.0,1.0,0.0
zipkin,I'm using Docker Desktop on my Windows PC.,0.0,0.0,1.0,0.0
zipkin,I have downloaded the sample from dapr repository ( https://github.com/dapr/samples/tree/master/hello-docker-compose ) which runs perfectly out of the box with  docker-compose up .,0.0,0.198,0.802,0.6369
zipkin,Then I've added Zipkin support as per dapr documentation:,0.0,0.252,0.748,0.4019
zipkin,"When application runs, it should send traces to the server, but nothing is found in zipkin UI and logs.",0.0,0.0,1.0,0.0
zipkin,Strange thing start to appear in the logs from  nodeapp-dapr_1  service:  error while reading spiffe id from client cert,0.209,0.0,0.791,-0.5423
zipkin,Additional info - current dapr version used is 1.0.1.,0.0,0.0,1.0,0.0
zipkin,I made sure that security (mtls) is disabled in config file.,0.0,0.37,0.63,0.5719
zipkin,I am very new to using OpenTelemetry and have just tried configuring it to send traces to my Zipkin server.,0.0,0.0,1.0,0.0
zipkin,"Unfortunately , after configuring the agent by specifying zipkin exporter details , I could see an exception in the console.",0.13,0.0,0.87,-0.34
zipkin,I used Petclic as sample spring boot and have followed the documentation here  https://github.com/open-telemetry/opentelemetry-java-instrumentation,0.0,0.0,1.0,0.0
zipkin,Here is the command that I used to start spring-boot app(I have my zipkin server running at localhost:9411):,0.0,0.0,1.0,0.0
zipkin,java -javaagent:opentelemetry-javaagent-all.jar -Dotel.exporter=zipkin -Dotel.exporter.zipkin.endpoint=localhost:9411 -jar spring-petclinic-2.4.2.jar,0.0,0.0,1.0,0.0
zipkin,Exception in the console (It is trying to connect to gRpc exporter instead of Zipkins):,0.0,0.0,1.0,0.0
zipkin,Please can you let me know what is wrong with this.,0.215,0.16,0.625,-0.2023
zipkin,"EDIT : 
I already tried passing -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin but was not successful.",0.272,0.0,0.728,-0.6259
zipkin,I am just trying to start my spring boot app through eclipse by passing these params as jvm arguments.,0.137,0.0,0.863,-0.4019
zipkin,Every tutorial on OpenTelemetery seems to be using Docker setup.,0.0,0.0,1.0,0.0
zipkin,Please could someone help,0.0,0.714,0.286,0.6124
zipkin,I have a simple spring boot hello world application.,0.0,0.0,1.0,0.0
zipkin,Trying to send data to the Zipkin collector.,0.0,0.0,1.0,0.0
zipkin,"But as per logs, it's trying to use  OtlpGrpcSpanExporter .",0.0,0.0,1.0,0.0
zipkin,My application exposes a simple post rest API.,0.2,0.0,0.8,-0.128
zipkin,"Following Opentelemetry
docs  https://opentelemetry.io/docs/java/getting_started/",0.0,0.0,1.0,0.0
zipkin,[opentelemetry.auto.trace 2021-02-20 01:48:44:490 +0530] [grpc-default-executor-1] WARN io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter - Failed to export spans.,0.376,0.0,0.624,-0.6633
zipkin,"Error message: UNAVAILABLE: io exception
[opentelemetry.auto.trace 2021-02-20 01:49:14:106 +0530] [grpc-default-executor-2] WARN io.opentelemetry.exporter.otlp.metrics.OtlpGrpcMetricExporter - Failed to export metrics
io.grpc.StatusRuntimeException: UNAVAILABLE: io exception",0.324,0.0,0.676,-0.7983
zipkin,Please let me know if I have to change anything.,0.0,0.223,0.777,0.3182
zipkin,"I'm using istio with ingress gateway, and added zipkin tracing.",0.0,0.0,1.0,0.0
zipkin,All my apps are using spring boot with sleuth zipkin.,0.0,0.0,1.0,0.0
zipkin,I've deployed 2 zipkin for testing,0.0,0.0,1.0,0.0
zipkin,"the spring boot configuration are pointing to the zipkin namespace, with always sampled configuration.",0.0,0.0,1.0,0.0
zipkin,"Problem is when I'm using ingress gateway, the trace id looks like request id and it does propagate to my sub systems.",0.107,0.099,0.794,-0.0516
zipkin,"But when I query to zipkin (deployed both in istio-system from istio documentation, and manually deployed to another namespace) the trace id are not present.",0.0,0.0,1.0,0.0
zipkin,"Interestingly when I do port-forward of my outer most system, and hit the spring boot with grpc, the trace id are being propagated to the sub systems, and it does show in the zipkin dashboard.",0.0,0.076,0.924,0.4019
zipkin,although the trace id are different when using ingressgateway and port-forward direct grpc call :,0.0,0.0,1.0,0.0
zipkin,"ingressgateway : 0672471566b9305f7dcaadecaf1a8c71
direct call : cdc337ec90b8c085",0.0,0.0,1.0,0.0
zipkin,Thanks!,0.0,1.0,0.0,0.4926
zipkin,I am looking into distribution tracing tools.,0.0,0.0,1.0,0.0
zipkin,Found there two very popular.,0.0,0.436,0.564,0.4754
zipkin,What are key differences between them ?,0.0,0.0,1.0,0.0
zipkin,Which one would you recommend ?,0.0,0.385,0.615,0.3612
zipkin,Will you recommend other open source distributed tracking tool ?,0.0,0.238,0.762,0.3612
zipkin,Spring Cloud Sleuth is used for creating traceIds (Unique to request across services) and spanId (Same for one unit for work).,0.0,0.099,0.901,0.296
zipkin,My idea is that Zipkin server is used to get collective visualization of these logs across service.,0.0,0.0,1.0,0.0
zipkin,But I know and have used ELK stack which does necessarily the same function.,0.0,0.0,1.0,0.0
zipkin,"I mean we can group requests with the same traceId for visualising, using ELK stack.",0.0,0.0,1.0,0.0
zipkin,"But I do see people trying to implement distributed tracing with Sleuth, ELK along with Zipkin, as in these examples ( Link1 , Link2 ).",0.0,0.0,1.0,0.0
zipkin,But why do we need Zipkin if there is already ELK for log collection and visualising?,0.0,0.0,1.0,0.0
zipkin,Where I am missing?,0.524,0.0,0.476,-0.296
zipkin,"I can't find a below  exportable  Span in Zipkin neither by it's traceId nor by spanId (some other spans appear, so Zipkin server seems to work)",0.0,0.0,1.0,0.0
zipkin,I also can't find it's parent &quot;parentId&quot;:&quot;37eca1021fd5241c&quot; in Zipkin.,0.0,0.0,1.0,0.0
zipkin,Where can be a problem?,0.474,0.0,0.526,-0.4019
zipkin,How can I bite/debug it?,0.0,0.0,1.0,0.0
zipkin,"Possibly this span is in a flow, that was triggered by a rabbit message, not a rest request.",0.0,0.0,1.0,0.0
zipkin,Spans from a trace that were triggered by http rest request are correctly visible in Zipkin.,0.0,0.0,1.0,0.0
zipkin,But I can't find traces from flows triggered by rabbit message.,0.0,0.0,1.0,0.0
zipkin,What problem could be here?,0.403,0.0,0.597,-0.4019
zipkin,We have  slueth  in other microservices and we wants to send data to zipkin server for consolidated logging.I am trying to start my zipkin server.I am getting the following error:,0.085,0.0,0.915,-0.4019
zipkin,"I tried to use the sleuth and zipkin older versions, but getting conflicts and spring application is failing to start
We are not able to see Zipkin UI.",0.239,0.0,0.761,-0.8338
zipkin,And  pom.xml  is like:,0.0,0.455,0.545,0.3612
zipkin,Any suggestions will be helpful.,0.0,0.412,0.588,0.4215
zipkin,I am trying to link three HTTP service hops in NodeJS together into a single Zipkin trace.,0.0,0.0,1.0,0.0
zipkin,I have three services,0.0,0.0,1.0,0.0
zipkin,"The service  service-main  calls  service-hello , and  service-hello  needs to call  service-goodbye  to complete.",0.0,0.0,1.0,0.0
zipkin,"Zipkin can see these calls, but links them together as two separate traces.",0.0,0.0,1.0,0.0
zipkin,"( service-main  calling  service-hello , and  service-hello  calling  service-goodbye .",0.0,0.0,1.0,0.0
zipkin,"The services are implemented in  express , and the fetching happens via  node-fetch .",0.0,0.0,1.0,0.0
zipkin,I create an instrumented service fetcher with code that looks like this,0.0,0.338,0.662,0.5574
zipkin,and I instrument express with code that looks like this,0.0,0.238,0.762,0.3612
zipkin,"and finally, I create my tracer with code that looks like this",0.0,0.338,0.662,0.5574
zipkin,You can see the above code in context in  the following github repository .,0.0,0.0,1.0,0.0
zipkin,"I've done all this by cargo-culting the code samples from the zipkin github repositories, and I don't know enough about zipkin's implementation to diagnose this further.",0.0,0.0,1.0,0.0
zipkin,How to I get zipkin to see the  service-main  -   service-hello  -   service-goodbye  call chain as a single trace?,0.0,0.0,1.0,0.0
zipkin,I am working on micro service where micro services are communicating with each other.,0.0,0.0,1.0,0.0
zipkin,I am using Zipkin with Sleuth and Apache Kafka as a message broker and running micro service and kafka using docker-compose.,0.0,0.0,1.0,0.0
zipkin,"I am using Spring Boot (2.2.6.RELEASE), Spring Cloud (Hoxton.SR3), Kafka Image(wurstmeister/kafka:2.12-2.4.1) and latest image of Zipkin.",0.0,0.0,1.0,0.0
zipkin,"When i try to spin the container, micro service is giving below error:",0.179,0.159,0.662,-0.0772
zipkin,APPLICATION FAILED TO START,0.524,0.0,0.476,-0.5106
zipkin,Description:,0.0,0.0,1.0,0.0
zipkin,Parameter 2 of method reporter in org.springframework.cloud.sleuth.zipkin2.ZipkinAutoConfiguration &gt; required a bean of type 'zipkin2.reporter.Sender' that could not be found.,0.0,0.0,1.0,0.0
zipkin,I have microservice running in AWS ECS and listens to AWS SQS messages.,0.0,0.0,1.0,0.0
zipkin,I am using  zipkin-aws  to sent the traces to AWS Kinesis and collected in S3.,0.0,0.0,1.0,0.0
zipkin,"When there is any REST Operation, the traces are sent and collected in S3 perfectly.",0.0,0.231,0.769,0.6369
zipkin,But it doesnt capture the traces when the Microservice listens or send message to AWS queue.,0.0,0.0,1.0,0.0
zipkin,Could anyone help in configuring zipkin to listen to SQS messages.,0.0,0.213,0.787,0.4019
zipkin,I have an application that is divided into a few microservices (using Spring Eureka project).,0.0,0.0,1.0,0.0
zipkin,"All the services are registered to Eureka Server - so that the communication between the services can be realized through a ""Gateway API"" (Eureka Server)",0.0,0.0,1.0,0.0
zipkin,The logs produced by the services are reported to a Zipkin server that runs as a separate service.,0.0,0.0,1.0,0.0
zipkin,All works as expected but when I go to the Eureka dashboard I am not able to see my Zipkin service since it is not registered with Eureka.,0.0,0.0,1.0,0.0
zipkin,Question: Is it possible to register Zipkin with Eureka Server?,0.0,0.0,1.0,0.0
zipkin,Here is my docker-compose file:,0.0,0.0,1.0,0.0
zipkin,I am using the  Zipkin 5.6.11,0.0,0.0,1.0,0.0
zipkin,I append  mysql  url:,0.0,0.0,1.0,0.0
zipkin,"but when I debug the project using spring boot application, it throws an Exception caused by not found the class TracingStatementInterceptor:",0.0,0.0,1.0,0.0
zipkin,"java.sql.SQLException: Unable to load statement interceptor
  'brave.mysql.TracingStatementInterceptor'.",0.0,0.0,1.0,0.0
zipkin,Caused by: java.lang.ClassNotFoundException: brave.mysql.TracingStatementInterceptor,0.0,0.0,1.0,0.0
zipkin,The dependency is,0.0,0.0,1.0,0.0
zipkin,I have some distributed micro services written in Spring Boot and I am using RabbitMQ.,0.0,0.0,1.0,0.0
zipkin,I want to track my requests.,0.0,0.245,0.755,0.0772
zipkin,Is there a possible way of tracking without using Spring Cloud or Sleuth,0.0,0.0,1.0,0.0
zipkin,I want to send my spring boot log traces using Kafka to Zipkin server,0.0,0.098,0.902,0.0772
zipkin,I started my Kafka and Zipkin servers and I started a Kafka consumer which is listening to the topic Zipkin and I am able to see my logs here but when I open my Zipkin dashboard I can't find any traces,0.0,0.0,1.0,0.0
zipkin,"Spring boot version  :- 2.1.7.RELEASE 
 Spring Cloud version  :- Greenwich.SR2",0.0,0.0,1.0,0.0
zipkin,"when i make  spring.zipkin.sender.type=web  the traces are being shown in Zipkin dashboard 
Is there anything I'm missing here.",0.121,0.0,0.879,-0.296
zipkin,I have some services.,0.0,0.0,1.0,0.0
zipkin,I want to trace those services using zipkin-go.,0.0,0.178,0.822,0.0772
zipkin,"In every service, I am calling some of my other internal services or db calls.",0.0,0.0,1.0,0.0
zipkin,I want to trace every activity like how much time it has taken to call internal services or db.,0.0,0.192,0.808,0.4215
zipkin,I have implemented using available tutorials on internet.,0.0,0.0,1.0,0.0
zipkin,Below is my code:,0.0,0.0,1.0,0.0
zipkin,I am getting my request traced but I am not able to trace what is happening inside the  uploadimage  controller.,0.0,0.0,1.0,0.0
zipkin,Below is the screenshot of my zipkin UI:,0.0,0.0,1.0,0.0
zipkin,I want to trace all the activities happening inside uploadimage controller.,0.0,0.126,0.874,0.0772
zipkin,What should I need to pass so that I can trace all.,0.0,0.0,1.0,0.0
zipkin,"I may have missed it, but how do save query parameters in zipkin?",0.101,0.27,0.629,0.5719
zipkin,My service is running:,0.0,0.0,1.0,0.0
zipkin,"I'm missing host too, but I suspect that is because I'm running services in docker containers.",0.253,0.0,0.747,-0.5267
zipkin,update 8/13/2019,0.0,0.0,1.0,0.0
zipkin,I make a call to  https://test-service.mydomain.com/api/conn/parallel2?repetitions=20&amp;delay=10000&amp;bypassTokenCache=true&amp;overrideTokenRefreshSeconds=10,0.0,0.0,1.0,0.0
zipkin,Notice in the trace I'm missing host and query parameters.,0.196,0.0,0.804,-0.296
zipkin,Is there any way to record those values in zipkin?,0.0,0.231,0.769,0.4019
zipkin,I need to implement Zipkin tracing in one java-based service which is using Project Reactor Kafka for reactive streams and non-blocking IO operations.,0.0,0.0,1.0,0.0
zipkin,I could not find any brave instrumentation library which supports reactive-Kafka.,0.209,0.188,0.603,-0.0711
zipkin,The standard Kafka-client brave instrumentation:,0.0,0.459,0.541,0.5267
zipkin,https://github.com/openzipkin/brave/tree/master/instrumentation/kafka-clients,0.0,0.0,1.0,0.0
zipkin,has no support for Reactive-Kafka.,0.278,0.342,0.38,0.128
zipkin,Is there a library or repo which can help me with Zipkin tracing for reactive-Kafka in java?,0.0,0.153,0.847,0.4019
zipkin,"I am creating a client sdk with retrofit calls to a service, packaged as a separate jar.",0.0,0.155,0.845,0.296
zipkin,"I have to include zipkin tracer/tracing in this jar so that any application using this jar to communicate with the service, have a separate span created automatically for every call to the service.",0.0,0.062,0.938,0.25
zipkin,Is there a feasible solution to my problem?,0.27,0.23,0.5,-0.1027
zipkin,"I have been trying to solve the problem using the ""io.zipkin.brave:brave-instrumentation-okhttp3"" library.",0.2,0.133,0.667,-0.2263
zipkin,"I have also added the ""org.springframework.cloud:spring-cloud-starter-sleuth"" dependency so that the tracer-id is by-default generated.",0.0,0.0,1.0,0.0
zipkin,"But adding this jar to a project which uses kafka-streams and the ""io.zipkin.brave:brave-instrumentation-kafka-streams"" dependency, doesn't automatically initialses a new span.",0.0,0.0,1.0,0.0
zipkin,What I expect is that applications using this jar have by-default a separate span for every retrofit call they make through this jar.,0.0,0.0,1.0,0.0
zipkin,I'm trying to configure my spring boot app to log into a zipkin server.,0.0,0.0,1.0,0.0
zipkin,The problem is that this server is protected by a proxy (with basic auth) and I cannot find any documentation describing how to configure authorization with spring-sleuth.,0.094,0.101,0.804,0.0516
zipkin,I have tried to use that kind of configuration :,0.0,0.0,1.0,0.0
zipkin,"But without success, logs indicating :",0.5,0.0,0.5,-0.612
zipkin,I have tried with curl and it works.,0.0,0.0,1.0,0.0
zipkin,Has someone already succeed to configure authentication with spring-sleuth ?,0.0,0.286,0.714,0.4939
zipkin,I'm working on a spring boot application version 2.1.2 with below dependency,0.0,0.0,1.0,0.0
zipkin,The application is not able to send the traces to Zipkin server which is running on Spring boot 1.5.,0.0,0.0,1.0,0.0
zipkin,When I tried downgrading my application to Spring 1.5 it started sending traces to the Zipkin server.,0.0,0.0,1.0,0.0
zipkin,Can someone please assist.,0.0,0.434,0.566,0.3182
zipkin,Am I missing any configuration for Spring boot 2.1?,0.239,0.0,0.761,-0.296
zipkin,Below are the dependency &amp; configuration for Spring cloud Sleuth and Starter Zipkin,0.0,0.0,1.0,0.0
zipkin,Is there any CURL to get the latest available zipkin version and another CURL to get the currently running version in my local environment ?,0.0,0.0,1.0,0.0
zipkin,I have to do a version comparison on currently running Zipkin version with related to the latest available.,0.0,0.0,1.0,0.0
zipkin,I even walked through all the issues raised in github and couldn't find a solution.,0.141,0.0,0.859,-0.2411
zipkin,I have Java application that is sending requests to Spring Boot applications.,0.0,0.0,1.0,0.0
zipkin,I have implemented zipkin+sleuth on the Spring Boot applications and get traces.,0.0,0.0,1.0,0.0
zipkin,Now I want to implement zipkin on the java application and to collect the traces.,0.0,0.091,0.909,0.0772
zipkin,How to implement that?,0.0,0.0,1.0,0.0
zipkin,I need to use brave?,0.0,0.531,0.469,0.5267
zipkin,I have started zipkin-server and I can see the dashboard.,0.0,0.0,1.0,0.0
zipkin,I have tested it with simple projects and it is okay.,0.0,0.174,0.826,0.2263
zipkin,But when I test it with my app I have a problem.,0.307,0.0,0.693,-0.5499
zipkin,I have Spring Boot project that produce to kafka if the property for kafka is set on true in application.properties.,0.0,0.135,0.865,0.4215
zipkin,In my case it is always set to false and it is working correctly.,0.0,0.0,1.0,0.0
zipkin,But when I added zipkin dependency it start to send to kafka.,0.0,0.0,1.0,0.0
zipkin,And also I can not see my client app in the zipkin dashboard.,0.0,0.0,1.0,0.0
zipkin,I am using Spring Boot 1.5.6.RELEASE version,0.0,0.0,1.0,0.0
zipkin,This are my dependencies:,0.0,0.0,1.0,0.0
zipkin,And this are my properties for zipkin and sleuth.,0.0,0.0,1.0,0.0
zipkin,"By adding the first 3 properties the application is not sending requests on the beggining, but it start after I send a request to my application.",0.0,0.0,1.0,0.0
zipkin,"With dependency spring-cloud-starter-zipkin, App should connect to zipkin server when sleuth triggered.",0.0,0.0,1.0,0.0
zipkin,"I did not started zipkin server, so it should throw a connection exception.",0.0,0.0,1.0,0.0
zipkin,But nothing happened.,0.0,0.0,1.0,0.0
zipkin,"And when I start zipkin server, it can not receive anything.",0.0,0.0,1.0,0.0
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,App.java,0.0,0.0,1.0,0.0
zipkin,application.properties,0.0,0.0,1.0,0.0
zipkin,And logs,0.0,0.0,1.0,0.0
zipkin,I want to implement Zipkin and Sleuth on vert.x application.,0.0,0.14,0.86,0.0772
zipkin,I have Zipkin server application with Spring Boot and Spring Boot Services and its okay.,0.0,0.128,0.872,0.2263
zipkin,I have added dependencies and setup them in application.properties and its working good.,0.0,0.209,0.791,0.4404
zipkin,But I have one vert.x application and I have found it difficult to implement the Zipkin there.,0.188,0.0,0.812,-0.5023
zipkin,I have searched but I did not found some good example on how to implement it on vert.x application.,0.163,0.0,0.837,-0.4782
zipkin,"So, what I need to do to implement the Zipkin in the vert.x application?",0.0,0.0,1.0,0.0
zipkin,How can I disable Istio to send spans to zipkin?,0.0,0.0,1.0,0.0
zipkin,If I'm not wrong this is not a mixer adapter right?,0.0,0.221,0.779,0.3724
zipkin,It is something directly done from the pilot.,0.0,0.0,1.0,0.0
zipkin,How can I disable it?,0.0,0.0,1.0,0.0
zipkin,I'm adding Spring Cloud Sleuth with Zipkin to existing code to collect trace information and eventually log arbitrary messages.,0.0,0.0,1.0,0.0
zipkin,Regular request spans are correctly sent to Zipkin:,0.0,0.0,1.0,0.0
zipkin,"However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans).",0.0,0.122,0.878,0.3612
zipkin,"If I use  org.slf4j.Logger  to simply  LOG.info(""something"") , I see the  INFO  message in console output, with the  exportable  flag set to true:",0.0,0.128,0.872,0.4215
zipkin,"Checking the traces in Zipkin, the span is correctly found, but the message used in the  LOG.info()  line is nowhere to be seen -- which suggests me that I'm doing something wrong here, or maybe it's just not supposed to work this way.",0.09,0.0,0.91,-0.631
zipkin,My sampling percentage is set to 100%.,0.0,0.0,1.0,0.0
zipkin,Using the slf4j interface would be convenient because the existing code is already instrumented that way.,0.0,0.0,1.0,0.0
zipkin,Is it possible?,0.0,0.0,1.0,0.0
zipkin,"If so, what could be a good way to implement it?",0.0,0.244,0.756,0.4404
zipkin,I am using spring boot and zipkin.,0.0,0.0,1.0,0.0
zipkin,And using spring slueth to generate trace Id.,0.0,0.0,1.0,0.0
zipkin,Is there a way I can generate this trace Id on my own?,0.0,0.0,1.0,0.0
zipkin,"Also I want to log only specific requests say with 500 error or response time   threshold, how to do this?",0.129,0.062,0.81,-0.34
zipkin,I have a system where we have 2 modules.,0.0,0.0,1.0,0.0
zipkin,"1) Module 1 is a webapp with multiple endpoints, deployed on Tomcat.",0.0,0.0,1.0,0.0
zipkin,"2) Module 2 is an executable jar,(not a web-app) which spins up 2 Kafka consumers (K1 and K2) listening to topic1 and topic2 respectively.",0.0,0.107,0.893,0.34
zipkin,The web-app (Module 1) pushes messages to topic1.,0.0,0.0,1.0,0.0
zipkin,"K1 listens to topic1.It receives messages, processes them and pushes the processed messages to topic2.",0.0,0.0,1.0,0.0
zipkin,K2 listens to topic2.,0.0,0.0,1.0,0.0
zipkin,The messages are fully processed by K2 and do not propagate further.,0.0,0.0,1.0,0.0
zipkin,There are multiple points where errors can occur in this flow.,0.194,0.0,0.806,-0.34
zipkin,"I wanted to use Zipkin/ Jaegar to trace the entire flow, and also link the logs to the trace id, so that any issue can be easily and quickly investigated.",0.0,0.079,0.921,0.34
zipkin,Can anyone suggest me the way to go forward?,0.0,0.0,1.0,0.0
zipkin,I want to use ActiveMQ as a zipkin collector.,0.0,0.178,0.822,0.0772
zipkin,"I have already used rabbitmq as collector, but my client is specific to using ActiveMQ.",0.0,0.0,1.0,0.0
zipkin,Please let know if Zipkin supports ActiveMQ as collector and what are the configurations needed for that?,0.0,0.242,0.758,0.5859
zipkin,I have the microservice architecture of E-commerce website with all kinds of service containers and its corresponding database containers.,0.0,0.0,1.0,0.0
zipkin,"I failed to find a library to purely trace database running time for each request in like mongodb or mysql, so I used tcpdump to check the HTTP request roundtime at the port of service container.",0.09,0.068,0.842,-0.2023
zipkin,The total communication time is always shorter than Zipkin's log time for that HTTP request.,0.0,0.0,1.0,0.0
zipkin,"So I am assuming that Zipkin include some service container process time in the tracing, which is not what I desire to have.",0.101,0.0,0.899,-0.3089
zipkin,"I plan to use tcpdump on database container and decode the package for tracing id, which could be a lot of hassle.",0.0,0.0,1.0,0.0
zipkin,"Why can't zipkin track this activity, or there might be some tools I can use?",0.0,0.0,1.0,0.0
zipkin,"I am trying to run zipkin with rabbitmq collector enabled, like this:",0.0,0.2,0.8,0.3612
zipkin,I can resolve tracing to an IP address and port 5672 is open.,0.0,0.191,0.809,0.3818
zipkin,A queue called zipkin has been created in RabbitMQ.,0.0,0.222,0.778,0.25
zipkin,Here is the exception thrown:,0.0,0.0,1.0,0.0
zipkin,Does anybody have any idea what I am doing wrong?,0.279,0.0,0.721,-0.4767
zipkin,I am trying to integrate the Brave MySql Instrumentation into my Spring Boot 2.x service to automatically let its interceptor enrich my traces with spans concerning MySql-Queries.,0.0,0.12,0.88,0.5267
zipkin,The current Gradle-Dependencies are the following,0.0,0.0,1.0,0.0
zipkin,I already configured Sleuth successfully to send traces concerning HTTP-Request to my Zipkin-Server and now I wanted to add some spans for each MySql-Query the service does.,0.0,0.118,0.882,0.4939
zipkin,The TracingConfiguration it this:,0.0,0.0,1.0,0.0
zipkin,"The Query-Interceptor works properly, but my problem now is that the spans are not added to the existing trace but each are added to a new one.",0.124,0.0,0.876,-0.5499
zipkin,"I guess its because of the creation of a new sender/reporter in the configuration, but I have not been able to reuse the existing one created by the Spring Boot Autoconfiguration.",0.0,0.135,0.865,0.4678
zipkin,That would moreover remove the necessity to redundantly define the Zipkin-Url (because it is already defined for Zipkin in my application.yml).,0.0,0.0,1.0,0.0
zipkin,"I already tried autowiring the Zipkin-Reporter to my Bean, but all I got is a  SpanReporter  - but the Brave-Tracer-Builder requries a  Reporter&lt;Span&gt;",0.0,0.0,1.0,0.0
zipkin,Do you have any advice for me how to properly wire things up?,0.0,0.0,1.0,0.0
zipkin,I am thinking of integrating WSO2 ESB and Zipkin following Open Tracing using a  Customize Statics Publisher implemented  by me.,0.0,0.0,1.0,0.0
zipkin,The idea is enable statics and trace in WSO2 ESB.,0.0,0.0,1.0,0.0
zipkin,Is there any other approach to to achieve this?,0.0,0.0,1.0,0.0
zipkin,Is this approach correct?,0.0,0.0,1.0,0.0
zipkin,"The Collector Sampler today, samples the spans as received by the Zipkins Collector based on the rate.",0.0,0.0,1.0,0.0
zipkin,"But for a micro-services architecture where a single transaction traverses through multiple applications and spans, having a sampler logic to sample based on just the number of spans might cause us to loose a whole picture of a single transaction.",0.079,0.039,0.882,-0.3612
zipkin,So we are looking for an ideal solution where the Zipkin (not the app's sleuth implementation) has options to sample the transactions based on trace(or a whole transaction with all its spans) and not based on individual spans.,0.0,0.14,0.86,0.6908
zipkin,And we expect this to be more asynchronous sampler technique.,0.0,0.0,1.0,0.0
zipkin,Looking forward for your thoughts...,0.0,0.0,1.0,0.0
zipkin,"I'm looking for better graphs for the traces stored in zipkin.io database, wondering if anyone developed anything better than out of box zipkin UI..are there any alternate GUIs for Zipkin?",0.0,0.172,0.828,0.7003
zipkin,"I have mircroservice environment based on spring-boot, where i am using zipkin server and discovery-server(eureka) and config-server.",0.0,0.0,1.0,0.0
zipkin,Now i have a rest-microservice which sends logs to zipkin server and this microservice is required to resolve where is zipkin server using discovery-server.,0.0,0.11,0.89,0.3818
zipkin,following is zipkin configuration i have in my rest-microservice's application.properties(pulled from config-server).,0.0,0.0,1.0,0.0
zipkin,here MTD-ZIPKIN-SERVER is zipkin-server name in discovery-server.,0.0,0.0,1.0,0.0
zipkin,discovery-server dashboard.,0.0,0.0,1.0,0.0
zipkin,"but it does not try to resolve zipkin from discovery-server, instead it tries to connect directly using spring.zipkin.baseUrl, and i get below exception.",0.117,0.0,0.883,-0.4168
zipkin,"Dropped 1 spans due to ResourceAccessException(I/O error on POST request for "" http://MTD-ZIPKIN-SERVER/api/v1/spans "":
  MTD-ZIPKIN-SERVER; nested exception is java.net.UnknownHostException:
  MTD-ZIPKIN-SERVER)",0.137,0.0,0.863,-0.4019
zipkin,"org.springframework.web.client.ResourceAccessException: I/O error on
  POST request for "" http://MTD-ZIPKIN-SERVER/api/v1/spans "":
  MTD-ZIPKIN-SERVER; nested exception is java.net.UnknownHostException:
  MTD-ZIPKIN-SERVER     at
  org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:666)
    at
  org.springframework.web.client.RestTemplate.execute(RestTemplate.java:628)
    at
  org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:590)
    at
  org.springframework.cloud.sleuth.zipkin.RestTemplateSender.post(RestTemplateSender.java:73)
    at
  org.springframework.cloud.sleuth.zipkin.RestTemplateSender.sendSpans(RestTemplateSender.java:46)
    at
  zipkin.reporter.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:245)
    at
  zipkin.reporter.AsyncReporter$Builder.lambda$build$0(AsyncReporter.java:166)
    at zipkin.reporter.AsyncReporter$Builder$$Lambda$1.run(Unknown
  Source)   at java.lang.Thread.run(Thread.java:745) Caused by:
  java.net.UnknownHostException: MTD-ZIPKIN-SERVER  at
  java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)",0.062,0.0,0.938,-0.4019
zipkin,if i provide exact zipkin url in property spring.zipkin.baseUrl like below,0.0,0.217,0.783,0.3612
zipkin,then my rest-microservice is able to connect to zipkin-server.,0.0,0.0,1.0,0.0
zipkin,My goal here is to read zipkin-server location from discovery-srever.,0.0,0.0,1.0,0.0
zipkin,What am i doing wrong?,0.508,0.0,0.492,-0.4767
zipkin,Do i need to add some zipkin enabling annotation on my spring-boot rest-microservice?,0.0,0.0,1.0,0.0
zipkin,For some reason Zipkin is using the Consul discovery name instead of the base spring.application.name property.,0.0,0.0,1.0,0.0
zipkin,But I want it to use the non-randomized application name (so myservice instead of myservice-67gg8d368).,0.0,0.1,0.9,0.1154
zipkin,If I set the Zipkin property  zipkin.service.name  then Consul throws errors saying it cannot find the service.,0.138,0.0,0.862,-0.34
zipkin,I'm unsure why the two are even sharing properties and not just adhering to their own.,0.106,0.149,0.745,0.2023
zipkin,"I'd like the service to use it's base application name because otherwise Zipkin is hard to use, as it lists every new container as a completely new service, making it very difficult to see over time how code changes have changed timing.",0.094,0.056,0.85,-0.1761
zipkin,"UPDATE:
This is the error I get in my logs if I set the zipkin.service.name",0.184,0.0,0.816,-0.4019
zipkin,I am having problem persisting the zipkin data.,0.31,0.0,0.69,-0.4019
zipkin,I did not get any error message.,0.0,0.311,0.689,0.3089
zipkin,So I am sharing configuration to get a help.,0.0,0.536,0.464,0.6983
zipkin,"I can see my logs in zipkin UI, but not able to persist in elastic search.",0.0,0.0,1.0,0.0
zipkin,My zipkin-service pom file shared below.,0.0,0.324,0.676,0.34
zipkin,And my properties looks like this,0.0,0.333,0.667,0.3612
zipkin,I am currently using zipkin and I am trying to build a python script to extract the data saved and process them differently.,0.0,0.128,0.872,0.4215
zipkin,zipkin logs and UI are working fine.,0.0,0.231,0.769,0.2023
zipkin,I have done this :,0.0,0.0,1.0,0.0
zipkin,I have used wget and request and both are giving me the result below:,0.0,0.167,0.833,0.34
zipkin,but If I copy paste the URL used in the request or wget in a web browser the result is shown,0.0,0.0,1.0,0.0
zipkin,any idea how to extract the data in JSON or any other format for a Zipkin server?,0.0,0.0,1.0,0.0
zipkin,Thanks,0.0,1.0,0.0,0.4404
zipkin,So I'm having zipkin gathering my data inside kubernetes from other services.,0.0,0.0,1.0,0.0
zipkin,I'm having nginx ingress controller defined to expose my services and all works nice.,0.098,0.171,0.732,0.296
zipkin,As zipkin is admin thing I'd love to have it behind some security ie.,0.0,0.355,0.645,0.765
zipkin,basic auth.,0.0,0.0,1.0,0.0
zipkin,"If I add 3 lines marked as ""#problematic lines - start"" and ""#problematic lines - stop"" below my zipkin front is no longer visible and I get 503.",0.173,0.0,0.827,-0.5267
zipkin,"It's created with  https://github.com/kubernetes/ingress/tree/master/examples/auth/basic/nginx 
and no difficult things here.",0.37,0.157,0.472,-0.4019
zipkin,I'm not sure if it's not about possible infulence but I used  https://github.com/kubernetes/ingress/blob/master/controllers/nginx/rootfs/etc/nginx/nginx.conf  file as template for my nginx ingress controller as I needed to modify some CORS rules.,0.054,0.0,0.946,-0.1232
zipkin,I see there part:,0.0,0.0,1.0,0.0
zipkin,but I don't see result in:  kubectl exec nginx-ingress-controller-lalala-lalala -n kube-system cat /etc/nginx/nginx.conf | grep auth .,0.0,0.0,1.0,0.0
zipkin,Due to this my guess is that I need to add some annotation to make this  {{ if $location.BasicDigestAuth.Secured }}  part work.,0.0,0.0,1.0,0.0
zipkin,Unfortunately I cannot find anything about it.,0.324,0.0,0.676,-0.34
zipkin,"I saw the services had send the trace to zipkin server, and also found the trace result list in zipkin ui, when I clicked one of them to see the trace detail and timeline diagram, but there had no diagram just some duration as the title and json data is ok.",0.053,0.053,0.894,0.0
zipkin,Is there something I can do ?,0.0,0.0,1.0,0.0
zipkin,zipkin,0.0,0.0,1.0,0.0
zipkin,"On the official Zipkin repository  README  I see how to configure Zipkin with either Cassandra, MySql or Elastic Search.",0.0,0.0,1.0,0.0
zipkin,"However in my current job, we'd like to make use of Zipkin, but we are limited to Microsoft SQL Server as the only supported database.",0.081,0.162,0.757,0.3291
zipkin,"AFAIK Zipkin works with MySql via standard JDBC driver, therefore I think it would be possible to make it work with SqlServer as well.",0.0,0.087,0.913,0.2732
zipkin,Am I right?,0.0,0.0,1.0,0.0
zipkin,"If so, how can I configure it to work over it?",0.0,0.0,1.0,0.0
zipkin,"I recently upgraded my project from Spring Boot 1.4.1, Spring Cloud Sleuth 1.1.0, Spring Cloud Zipkin 1.1.0 to Spring Boot 1.5.3, Spring Cloud Sleuth 1.2.0, Spring Cloud Zipkin 1.2.0.",0.0,0.0,1.0,0.0
zipkin,"Read that with the latest version of Spring Cloud Sleuth, they had added ""error"" tags which will get reported to Zipkin automatically in case of any exceptions.",0.0,0.0,1.0,0.0
zipkin,I have a @ControllerAdvice class extending ResponseEntityExceptionHandler for custom exception handling.,0.0,0.0,1.0,0.0
zipkin,"I was able to report errors to the Tracer and visualize the same in Zipkin when using the old versions (Spring Boot 1.4.1, Spring Cloud Sleuth 1.1.0, Spring Cloud Zipkin 1.1.0) using the below method:",0.068,0.0,0.932,-0.34
zipkin,"After I upgraded, this doesn't seem to work and spring cloud sleuth's default error reporting was also not happening.",0.137,0.0,0.863,-0.4019
zipkin,"Only after commenting out the @ControllerAdvice and letting Spring Boot's default ErrorController to handle the exceptions, I was able to visualize the errors in Zipkin.",0.094,0.0,0.906,-0.34
zipkin,"However, we need the custom exception handling to format the error response in a standard way with error codes across all our PaaS services.",0.205,0.0,0.795,-0.6597
zipkin,Is there a way to do this?,0.0,0.0,1.0,0.0
zipkin,Should I use any other Sleuth objects to achieve this?,0.0,0.0,1.0,0.0
zipkin,"I am not able to start the Spring Cloud ZipKin Server, It is giving below mentioned exception.",0.0,0.138,0.862,0.34
zipkin,"BeanCreationException: Cannot create binder factory, no  META-INF/spring.binders  resources found on the classpath",0.286,0.0,0.714,-0.4614
zipkin,Below are my maven dependencies -,0.0,0.0,1.0,0.0
zipkin,Also my application startup class looks as below.,0.0,0.0,1.0,0.0
zipkin,Any help is highly appreciated.,0.0,0.677,0.323,0.7425
zipkin,"I have a distributed system, where a client needs information from multiple sources.",0.0,0.0,1.0,0.0
zipkin,Is there any support for marking parallel processed spans for the same trace in Brave (Java implementation of Zipkin framework)?,0.0,0.253,0.747,0.7269
zipkin,"Currently, before sending a message I call clientRequestInterceptor.handle(...) and after receiving response clientResponseInterceptor.handle(...), but there is only one instance, so only one span is recorded.",0.0,0.0,1.0,0.0
zipkin,P.S.,0.0,0.0,1.0,0.0
zipkin,I found the following project on GitHub that specified that Brave only supports one level of nested client call:  https://github.com/leigu/brave-tracer-example .,0.0,0.258,0.742,0.7096
zipkin,Maybe the same is valid for parallel client calls.,0.0,0.0,1.0,0.0
zipkin,The infrastructure of system used by my company consists of different application running on a  ActiveMQ .,0.0,0.0,1.0,0.0
zipkin,I want to know how would I setup  Zipkin  to trace the communication between different applications under  ActiveMQ ?,0.0,0.08,0.92,0.0772
zipkin,Thanks,0.0,1.0,0.0,0.4404
zipkin,"Please,  notice  that this question can seem a duplicate of  this one , but it's not the case.",0.0,0.099,0.901,0.1655
zipkin,Below I include my rational,0.0,0.0,1.0,0.0
zipkin,I'm trying to add Sleuth/Zipkin trace to my project.,0.0,0.0,1.0,0.0
zipkin,For this I have followed this  tutorial .,0.0,0.0,1.0,0.0
zipkin,My project is already using RabbitMQ for communication among the different microservices working fine.,0.0,0.122,0.878,0.2023
zipkin,"My problem is that I'm able to get the traces fine when I use the web connection, but I get an unable to connect error when I try to communicate using RabbitMQ.",0.165,0.043,0.793,-0.6124
zipkin,"As commented in the first line, my problem does not seem to be related to rabbitmq host itself, because it is up and running, and providing service to my microservices.",0.085,0.0,0.915,-0.4019
zipkin,"Sure that I missing something in the configuration, but I cannot find it (I have also checked  this post ,  this  and  this .",0.075,0.078,0.847,0.0129
zipkin,My files (I have removed not relevant part):,0.0,0.0,1.0,0.0
zipkin,"Notice that is started with profile 'docker', so host for rabbit is rabbitmq",0.0,0.0,1.0,0.0
zipkin,I have been searching if there is a way to instrument winstonJS with zipkin-js related data for each log.,0.0,0.0,1.0,0.0
zipkin,"However, I have not been able to find something.",0.0,0.0,1.0,0.0
zipkin,I saw some example where it mentioned to configure winston as follows,0.0,0.0,1.0,0.0
zipkin,"However, the formatter is not available in the latest version of winston.",0.0,0.0,1.0,0.0
zipkin,I was trying out to configure winston as,0.0,0.0,1.0,0.0
zipkin,But the fields are never received.,0.0,0.0,1.0,0.0
zipkin,"Also, when using express and configuring the winstonjs with",0.0,0.0,1.0,0.0
zipkin,My implementation is available at:  https://github.com/vtapadia/sample-node-app/blob/zipkin-tracing/src/config/logger.ts,0.0,0.0,1.0,0.0
zipkin,I just want my normal logging to have the tracing information auto injected.,0.0,0.106,0.894,0.0772
zipkin,Is it possible to achieve and can someone point to an example for this.,0.0,0.0,1.0,0.0
zipkin,Sending traces from an existing instrumented Spring Boot application to  honeycomb-opentracing-proxy  is failing with the following error in the proxy console:,0.24,0.0,0.76,-0.7184
zipkin,"Spring Boot Version: 2.1.3.RELEASE
Spring Cloud Sleuth Version: 2.1.1.RELEASE",0.0,0.0,1.0,0.0
zipkin,Running the open tracing proxy with the following docker command:,0.0,0.0,1.0,0.0
zipkin,From reading the documentation  here  the honeycomb-opentracing-proxy only supports v1 of the JSON API so I have explicitly set that in spring cloud config as this appears to default to v2.,0.0,0.079,0.921,0.3612
zipkin,application.properties,0.0,0.0,1.0,0.0
zipkin,Any help would be greatly appreciated,0.0,0.611,0.389,0.7425
zipkin,I am using Zipkin for distributed tracing.,0.0,0.0,1.0,0.0
zipkin,I have added zipkin-storage-mysql dependency in order to save the traces in MySQL DB.,0.0,0.211,0.789,0.4939
zipkin,"When I query ZIPKIN_SPANS table, I don't find the 16 char trace id in TRACE_ID Colum that I use in order to load the trace on zipkin UI.",0.0,0.0,1.0,0.0
zipkin,for ex:  localhost:9411/traces/4bcdd0bd5d2f70c0,0.0,0.0,1.0,0.0
zipkin,Please help me understand how can I figure it out.,0.0,0.417,0.583,0.6124
zipkin,"Also, How can I add a new column to the table for associating an application-specific id with it",0.0,0.0,1.0,0.0
zipkin,Spring boot application with below configuration :,0.0,0.0,1.0,0.0
zipkin,Required zipkin tracing feature.,0.0,0.0,1.0,0.0
zipkin,But my application not listing on zipkin server used proper jar also as below,0.0,0.0,1.0,0.0
zipkin,Spring Doc says,0.0,0.0,1.0,0.0
zipkin,Spring Cloud Sleuth is compatible with OpenTracing.,0.0,0.0,1.0,0.0
zipkin,"If you have OpenTracing on the classpath, we automatically register the OpenTracing Tracer bean .",0.0,0.0,1.0,0.0
zipkin,"If you wish to disable this, set spring.sleuth.opentracing.enabled to false",0.0,0.231,0.769,0.4019
zipkin,I have the below dependency in my POM.,0.0,0.0,1.0,0.0
zipkin,"But, I get the following print out it the logs when I try to print the trace and span information :  tracer: NoopTracer",0.0,0.0,1.0,0.0
zipkin,Why am I getting a NopTracer?,0.0,0.0,1.0,0.0
zipkin,Why isn't Brave being registered automatically as promised?,0.246,0.222,0.532,-0.0711
zipkin,Am I doing something wrong?,0.508,0.0,0.492,-0.4767
zipkin,I am using,0.0,0.0,1.0,0.0
zipkin,Finchley.SR2,0.0,0.0,1.0,0.0
zipkin,My Sampler looks like that:,0.0,0.385,0.615,0.3612
zipkin,In Eureka everything seems well:,0.0,0.344,0.656,0.2732
zipkin,But my problem is that in Zipkin I can't see services at all.,0.244,0.0,0.756,-0.5499
zipkin,"I found only debug logs, I have no errors:",0.479,0.0,0.521,-0.5574
zipkin,Question is simple.,0.0,0.0,1.0,0.0
zipkin,Why can't I see anything in Zipkin?,0.0,0.0,1.0,0.0
zipkin,This samples are written by  Josh Long.,0.0,0.0,1.0,0.0
zipkin,I was working on an Ecommerce microservice-based application.,0.0,0.0,1.0,0.0
zipkin,HTTP request structure: frontend -  order -  shipping -  rabbitmq -  queue-master.,0.0,0.0,1.0,0.0
zipkin,The shipping container get the trace info from order but it doesn't pass along to rabbitmq.,0.0,0.0,1.0,0.0
zipkin,Is there any configuration I should do?,0.0,0.0,1.0,0.0
zipkin,"I thought using ""spring-cloud-starter-zipkin"" will help me handle it effortlessly.",0.0,0.252,0.748,0.4019
zipkin,(tcpdump) shipping get post request with trace info:,0.0,0.0,1.0,0.0
zipkin,shipping send out to rabbitmq missing tracing data.,0.239,0.0,0.761,-0.296
zipkin,"EDITED:
Some code about post shipping request: (shipment is just an object with id and name)",0.0,0.0,1.0,0.0
zipkin,"I found that the bottleneck of zipkin is collector and API, are these two components stateless so i can deploy multi collectors and multi API?",0.0,0.0,1.0,0.0
zipkin,I want to deploy zipkin in kubernetes.,0.0,0.206,0.794,0.0772
zipkin,I have zipkin server running as spring boot app.,0.0,0.0,1.0,0.0
zipkin,I have exported jar to docker container.,0.0,0.0,1.0,0.0
zipkin,My dockerfile looks like:,0.0,0.455,0.545,0.3612
zipkin,I have one mysql container.,0.0,0.0,1.0,0.0
zipkin,Got this from official docker hub.,0.0,0.0,1.0,0.0
zipkin,when I link my zipkin container to mysql using this command :,0.0,0.0,1.0,0.0
zipkin,I am getting refused connection exception,0.355,0.0,0.645,-0.296
zipkin,Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure,0.398,0.0,0.602,-0.5106
zipkin,Although if I run my jar from host OS then it is able to connect mysql container and I can see spans stored in mysql db,0.0,0.0,1.0,0.0
zipkin,My application.yml is :,0.0,0.0,1.0,0.0
zipkin,How can I connect java docker container to mysql docker container?,0.0,0.0,1.0,0.0
zipkin,I have already explored this  link.,0.0,0.0,1.0,0.0
zipkin,I am getting this error when I try to run the unitTest in my spring boot application.,0.182,0.0,0.818,-0.481
zipkin,I notice that I only get this error when I use this version for  spring-cloud-dependencies :,0.221,0.0,0.779,-0.481
zipkin,but if I use this previous version:,0.0,0.0,1.0,0.0
zipkin,everything works as I expect,0.0,0.0,1.0,0.0
zipkin,What can I do in order to avoid this error with the last version?,0.326,0.0,0.674,-0.6514
zipkin,The unit tests are extended this class in order to mock the SpanAccessor,0.189,0.0,0.811,-0.4215
zipkin,},0.0,0.0,0.0,0.0
zipkin,"I'm trying to integrate sleuth into a Spring Boot application so that it will talk to a zipkin server for tracing, but I'm not having much luck.",0.118,0.0,0.882,-0.4973
zipkin,"I've followed a few tutorials ( link to tutorial ) and have no problems getting them to talk to zipkin, but it's not translating well to my application, and I'm not sure where to look.",0.225,0.0,0.775,-0.7281
zipkin,"essentially, in the build.gradle file, to the dependencies section, I added:",0.0,0.0,1.0,0.0
zipkin,"In the controller, I added these two beans:",0.0,0.0,1.0,0.0
zipkin,"and, I added these to the application.properties file:",0.0,0.0,1.0,0.0
zipkin,"When I do all that in the demos, they send traces to my Zipkin host at localhost:4911 just fine (For the time being, I'm just running the quickstart jar file).",0.0,0.06,0.94,0.2023
zipkin,"When I do all that in my application, I see that I have sleuth log entries with strings like:",0.0,0.143,0.857,0.3612
zipkin,"so, I know that Sleuth is working.",0.0,0.0,1.0,0.0
zipkin,"When I run a demo app with the zipkin server application shut off, the application looks like it's working fine, but, reasonably enough, the log files show a big old ConnectionRefused stack trace.",0.0,0.101,0.899,0.2846
zipkin,"When I try the same experiment with my application, I see no stack trace in my application logs, and the application also ran just fine.",0.088,0.072,0.84,-0.1027
zipkin,"Outside of my larger application, I can't reproduce my problem, and I don't know what else to share with you.",0.0,0.218,0.782,0.5358
zipkin,Anyone have any suggestions about where to start?,0.0,0.0,1.0,0.0
zipkin,"TL;DR: 
I want to persist data in ElasticSearch, how i can do this?",0.0,0.115,0.885,0.0772
zipkin,I have zipkin and Kafka and ElasticSearch.,0.0,0.0,1.0,0.0
zipkin,Kafka as transport for traces.,0.0,0.0,1.0,0.0
zipkin,"When i send trace to Kafka, i got it in zipkin UI, it is persistent in memory.",0.0,0.0,1.0,0.0
zipkin,I want to persist all traces in ES 5.0 for some time and when zipkin starts or when i search traces i want to search in ES or load trace from ES at start time of zipkin.,0.0,0.075,0.925,0.1531
zipkin,I start zipkin like this integrated with Kafka:,0.0,0.294,0.706,0.3612
zipkin,Here is  description of zipkin-storage/elasticsearch-http :,0.0,0.0,1.0,0.0
zipkin,This is output:,0.0,0.0,1.0,0.0
zipkin,But in ES in index zipkin there are no data.,0.237,0.0,0.763,-0.4215
zipkin,Springboot app.,0.0,0.0,1.0,0.0
zipkin,I'm using Brave v4 and trying not to use the brave-core module for when it is deprecated in the future.,0.0,0.152,0.848,0.5267
zipkin,With Brave v3 it was easy to pass around the current span as it was kept within the thread and handled by the Brave class.,0.0,0.306,0.694,0.8658
zipkin,I'm using the async-http-client client and I've created request and response filters which propagate to and from the header as well as starting and submitting spans.,0.0,0.146,0.854,0.4767
zipkin,This all works as expected.,0.0,0.0,1.0,0.0
zipkin,"The async-http-client pool is wired up at startup with the listeners attached, the listeners receive a TracingImpl which is just a wrapper for the Tracer class so that the listeners can do the submitting etc (well the spans inside can start/finish themselves)",0.0,0.0,1.0,0.0
zipkin,"My problem is for example: a request comes into the controller, I extract the Span from the request and now I want to use the async-http-client to make another request which would be a child of the one coming in.",0.071,0.034,0.895,-0.34
zipkin,I'm unsure how I should get the Span object I now have in my controller to the async-http-client object to it.,0.1,0.0,0.9,-0.25
zipkin,Any ideas or help would be greatly appreciated.,0.0,0.512,0.488,0.7425
zipkin,Thanks,0.0,1.0,0.0,0.4404
zipkin,I am working on implementing distributed tracing for a micro-services application.,0.0,0.0,1.0,0.0
zipkin,In zipkin UI I could not able to get child URL paths.,0.0,0.0,1.0,0.0
zipkin,For parent call whole URL is being captured like &quot;post/cart/create&quot;.,0.0,0.217,0.783,0.3612
zipkin,But for child calls only &quot;get&quot; and &quot;post&quot;.,0.0,0.0,1.0,0.0
zipkin,Not showing entire URL for child calls.,0.0,0.0,1.0,0.0
zipkin,How to fix this?,0.0,0.0,1.0,0.0
zipkin,Need help.,0.0,0.73,0.27,0.4019
zipkin,The data in Zipkin_spans table is as shown above.,0.0,0.0,1.0,0.0
zipkin,My calling service Code:,0.0,0.0,1.0,0.0
zipkin,Dependecies that i am using are,0.0,0.0,1.0,0.0
zipkin,This problem seems to be inconsistent.,0.351,0.0,0.649,-0.4019
zipkin,Sometimes it shows entire URL and sometimes not.,0.0,0.0,1.0,0.0
zipkin,Only after testing multiple times I came to know about this.,0.0,0.0,1.0,0.0
zipkin,I have even tested with spring-cloud- starter -zipkin.,0.0,0.0,1.0,0.0
zipkin,But it is also not showing any promising result.,0.265,0.0,0.735,-0.438
zipkin,So if entire URL is necessary then shouldn't I be using zipkin at all?,0.0,0.0,1.0,0.0
zipkin,suggest any other tracing mechanisms for microservices using spring.,0.0,0.0,1.0,0.0
zipkin,Small question on how to pass around the original trace id  traceId  (and not X-B3-TraceId ) please.,0.107,0.126,0.767,0.0869
zipkin,The entire flow is a call between a client ClientA to three other micro services (4 in total).,0.0,0.0,1.0,0.0
zipkin,ClientA (team A) -&gt; Service B1 (team B) -&gt; Service B2 (team B) -&gt; Service C (team C),0.0,0.0,1.0,0.0
zipkin,"Team A owns the client, the originator of the call, team C owns the last service.",0.0,0.0,1.0,0.0
zipkin,"My group, team B owns two out of the four micro services.",0.0,0.0,1.0,0.0
zipkin,"All subsequent micro services just perform computation and aggregate the response of each other back to the caller, ClientA.",0.0,0.0,1.0,0.0
zipkin,"Our team, B, had two micro services, hence we were the first out of the four to use Spring Cloud Sleuth and Zipkin.",0.0,0.0,1.0,0.0
zipkin,"Very happy, we could see out of the four services, in our services only, the traceId.",0.0,0.21,0.79,0.6115
zipkin,"This helped us in debugging, timing calls, etc, very happy.",0.0,0.307,0.693,0.6115
zipkin,"Since everyone saw benefits, now ClientA and ServiceC (the two others we do not own) also integrated with Sleuth-Zipkin.",0.0,0.126,0.874,0.3818
zipkin,"Unfortunately, it seems we are not able to &quot;chain everything&quot;.",0.211,0.0,0.789,-0.34
zipkin,🤯,0.0,0.0,0.0,0.0
zipkin,"We looked at each others traceId, and saw something like:",0.0,0.217,0.783,0.3612
zipkin,"Hence, we are now all very puzzled.",0.249,0.0,0.751,-0.2484
zipkin,As we thought we would see something like:,0.0,0.263,0.737,0.3612
zipkin,"Since for the two servicesB1 and B2, we did not do anything in particular, out of the box, we could see same traceId between the two, we thought, out of the box, we would be able to see between the four services.",0.0,0.0,1.0,0.0
zipkin,"May I ask, is my understanding not correct?",0.0,0.0,1.0,0.0
zipkin,What did I miss please?,0.271,0.39,0.339,0.1779
zipkin,"Do I have to do anything special, if yes, what, in order to ask ClientA to send me its traceId?",0.0,0.241,0.759,0.6597
zipkin,"Or maybe ClientA is already sending it, I am not getting it properly?",0.0,0.0,1.0,0.0
zipkin,(same applies to ServiceC),0.0,0.0,1.0,0.0
zipkin,Thank you for your help.,0.0,0.634,0.366,0.6369
zipkin,spring-boot 2.3.4-RELEASE,0.0,0.0,1.0,0.0
zipkin,Error is shown after adding the zipkin dependencies to a project and running with,0.184,0.0,0.816,-0.4019
zipkin,Cannot figure out the error causing it.,0.31,0.0,0.69,-0.4019
zipkin,Other spring-boot apps in the project are running fine with the same zipkin configs and application.yaml.,0.0,0.107,0.893,0.2023
zipkin,Any help to figure out how to debug this would be helpful.,0.0,0.355,0.645,0.6705
zipkin,"java.lang.IllegalStateException: Error processing condition on org.springframework.cloud.sleuth.zipkin2.ZipkinBackwardsCompatibilityAutoConfiguration$BackwardsCompatibilityConfiguration
at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:60) ~[spring-boot-autoconfigure-2.3.4.RELEASE.jar:2.3.4.RELEASE]",0.252,0.0,0.748,-0.4019
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,application.yaml,0.0,0.0,1.0,0.0
zipkin,I am using zipkin tracing in node.js application and I am getting  this  error.,0.221,0.0,0.779,-0.481
zipkin,How to resolve this error?,0.358,0.298,0.344,-0.1343
zipkin,Thanks in advance...,0.0,0.592,0.408,0.4404
zipkin,Small question as I am not able to see  X-Span-Export  in my logs please.,0.0,0.161,0.839,0.3182
zipkin,"I have an app, which is Java based, with Springboot 2.4.2, with Sleuth and Zipkin.",0.0,0.0,1.0,0.0
zipkin,"In my log4j2.xml, I configured such:",0.0,0.0,1.0,0.0
zipkin,"However, in my logs, I am only able to see:",0.0,0.0,1.0,0.0
zipkin,"I was expecting to see a &quot;true&quot; or &quot;false&quot;, like  [myservice,336a9463f46cf034,18bffe81e7500eae,true]  however, it is not here.",0.0,0.161,0.839,0.3612
zipkin,May I know what is the issue please?,0.0,0.277,0.723,0.3182
zipkin,Thank you,0.0,0.714,0.286,0.3612
zipkin,I'm running a Spring Boot app using:,0.0,0.0,1.0,0.0
zipkin,"I've declared the  spring-cloud-starter-zipkin  and  spring-cloud-starter-openfeign  dependencies, and have configured my app to point to a Zipkin server.",0.0,0.0,1.0,0.0
zipkin,Its a pretty vanilla setup and configuration (I also declare the  spring-cloud-starter-netflix-ribbon  and  spring-cloud-starter-kubernetes-all  dependencies o allow Spring Feign to use k8s service discovery).,0.0,0.203,0.797,0.6249
zipkin,My app declares a  @SpringFeign  annotated interface with a method to call to a remote service  S .,0.0,0.0,1.0,0.0
zipkin,So generally zipkin is getting spans from my app (for e.g.,0.0,0.0,1.0,0.0
zipkin,incoming REST calls) and B3 headers are being propagated via HTTP to the service  S  being called through feign.,0.0,0.0,1.0,0.0
zipkin,"But zipkin does not report a span from my app representing the Feign call to S.
Is that something that should &quot;just happen&quot;, or am I missing a piece of the puzzle?",0.091,0.0,0.909,-0.4215
zipkin,I can e.g.,0.0,0.0,1.0,0.0
zipkin,"add  @NewSpan  to the feign interface method, but that doesn't give me HTTP details for the request/response as span tags.",0.0,0.0,1.0,0.0
zipkin,And I rather not do that if this is supposed to work out of the box.,0.0,0.0,1.0,0.0
zipkin,In my Spring Boot application I use spring-cloud-starter-sleuth (Version Hoxton.SR10) for tracing.,0.0,0.0,1.0,0.0
zipkin,It is (still) a monolithic application so I widely use the  @NewSpan  annotation for creating new spans.,0.0,0.136,0.864,0.296
zipkin,"In my development environment I also use spring-cloud-starter-zipkin, which works great.",0.0,0.313,0.687,0.6249
zipkin,But on the servers of our customers I don't have access to any Zipkin server nor am allowed to install one.,0.0,0.0,1.0,0.0
zipkin,Is there any possibility to save the data Spring is sending to Zipkin and import that to my local Zipkin server?,0.0,0.138,0.862,0.4939
zipkin,Solution thanks to Marcin's inspiration:,0.0,0.811,0.189,0.8225
zipkin,"Implement  convertByteArrayToList  and  saveToFile  your own, because my solution depends on custom libraries.",0.0,0.161,0.839,0.3182
zipkin,i am new to  opencensus  and i am trying this  quick-start-example  here i want to export the traces to zipkin for that i added the exporter code and zipkin is running on docker,0.0,0.044,0.956,0.0772
zipkin,traces are not exporting to zipkin here is my example code,0.0,0.0,1.0,0.0
zipkin,i have tried to search by trace id as well but it did not show up,0.0,0.1,0.9,0.1406
zipkin,"I am using the Cassandra 3 storage type for Zipkin, but I can't find the option for it to clear the older data.",0.0,0.152,0.848,0.5267
zipkin,I am looking for something like  MEM_MAX_SPANS=1000000  as specified in the  README.md .,0.0,0.2,0.8,0.3612
zipkin,"In ElasticSearch I can set up a rule to clean up older records, maybe there is something like that in Cassandra?",0.0,0.234,0.766,0.6369
zipkin,"Question regarding Spring and Zipkin/Open tracing, in particularly, traces where an app in the middle does not have Zipkin/Open Tracing please.",0.0,0.103,0.897,0.3182
zipkin,Setup:,0.0,0.0,1.0,0.0
zipkin,"Step1: A client, front end app, without Zipkin is making a call to my-micro-service-A-with-zipkin.",0.0,0.0,1.0,0.0
zipkin,Step2: my-micro-service-A-with-zipkin will then call my-micro-service-B-with-zipkin,0.0,0.0,1.0,0.0
zipkin,Step3: my-micro-service-B-with-zipkin will then call  third-party-service-C-without-zipkin,0.0,0.0,1.0,0.0
zipkin,Step4:  third-party-service-C-without-zipkin  will then call my-micro-service-D-with-zipkin,0.0,0.0,1.0,0.0
zipkin,"Step5: After all those calls, my-micro-service-A-with-zipkin will return the response to the client.",0.0,0.0,1.0,0.0
zipkin,"A bit surprised, when searching for the trace, I could only find the call my-micro-service-A-with-zipkin -&gt; my-micro-service-B-with-zipkin, and  none of the other  calls.",0.0,0.087,0.913,0.2263
zipkin,"Basically, just one step our of the 5.",0.0,0.0,1.0,0.0
zipkin,"Question:
Is this expected please?",0.0,0.365,0.635,0.3182
zipkin,"I understand third-party-service-C-without-zipkin does not have Zipkin, but the for me, this is kinda defeating the purpose.",0.165,0.0,0.835,-0.4516
zipkin,"In this example scenario, I will never be able to see further traces other than A to B (Step2) ?",0.0,0.0,1.0,0.0
zipkin,"Also, is there a way to remediate to this?",0.0,0.0,1.0,0.0
zipkin,"The third party is a third party API, out of our control, we cannot just go and say &quot;please install Zipkin/open tracing, it will be helpful to everyone&quot;.",0.0,0.255,0.745,0.802
zipkin,Really wondering how to achieve the correct traces in this described scenario.,0.0,0.0,1.0,0.0
zipkin,Thank you,0.0,0.714,0.286,0.3612
zipkin,"Question regarding Spring Webflux 2.4.2+ project, with Spring Cloud Ilford 2020.0.0 please.",0.0,0.173,0.827,0.3182
zipkin,"In my small Spring Webflux app, I am currently using",0.0,0.0,1.0,0.0
zipkin,"I see the traces fine in m Zipkin server, very happy.",0.0,0.453,0.547,0.6997
zipkin,"But I was also expecting to be able to get some metrics, such as",0.0,0.0,1.0,0.0
zipkin,"However, while I am seeing the traces in Zipkin server, and &quot;some&quot; zipkin metrics, such as  zipkin_reporter_spans_total  I am not getting Zipkin related metrics at all.",0.0,0.0,1.0,0.0
zipkin,May I ask how to get above mentioned metrics please?,0.0,0.223,0.777,0.3182
zipkin,Thank you,0.0,0.714,0.286,0.3612
zipkin,Can I query a json file with the dependencies suing zipkin API?,0.189,0.0,0.811,-0.2732
zipkin,like with traces:,0.0,0.556,0.444,0.3612
zipkin,Seems like tags aren't working with vert.x zipkin integration.,0.0,0.238,0.762,0.3612
zipkin,Below is the sample code snippet,0.0,0.0,1.0,0.0
zipkin,We use vert.x application as api gateway which takes requests and forward the requests to other services.,0.0,0.0,1.0,0.0
zipkin,"On the server side, Tracing.currentTracer().currentSpan() always gives a null object.",0.0,0.0,1.0,0.0
zipkin,Why is the span going out of context here before the request is forwarded down to other service?,0.0,0.0,1.0,0.0
zipkin,Is there any bug in the vert.z zipkin code?,0.0,0.0,1.0,0.0
zipkin,Use Case :,0.0,0.0,1.0,0.0
zipkin,Using vert.x application as an API gateway for all incoming traffic.,0.0,0.0,1.0,0.0
zipkin,Want to sample incoming requests through vert.x,0.0,0.178,0.822,0.0772
zipkin,"Sampling option inside ZipkinTracingOptions is hard coded to Sampler.ALWAYS_SAMPLE
I feel, we should provide the application an ability to supply Sampler from outside so that all sampling options supported by Brave can be used in ZipkinTracingOptions",0.035,0.198,0.767,0.765
zipkin,Does vert.x supports any other Sampling Option or does it intend to do that in future?,0.0,0.143,0.857,0.3612
zipkin,I see active work going on for vert.x integration with zipkin.,0.0,0.231,0.769,0.4019
zipkin,https://github.com/eclipse-vertx/vertx-tracing,0.0,0.0,1.0,0.0
zipkin,Will it be backward compatible with vert.x 3.8.x version?,0.0,0.0,1.0,0.0
zipkin,"If yes, can you share tentative timelines for the same?",0.0,0.38,0.62,0.5994
zipkin,I have set up a docker image of  Zipkin  to enable distributed tracing in my organization.,0.0,0.0,1.0,0.0
zipkin,"The information needs to be available for specific users, Can I add some user authentication in front of the UI, without protecting the current flow of span information?",0.0,0.0,1.0,0.0
zipkin,"Can anyone tell me, is there any way to  integrate Zipkin (if possible Sleuth also) for non-spring boot projects ?",0.0,0.0,1.0,0.0
zipkin,I am trying to integrate Zipkin for my traditional spring application.,0.0,0.0,1.0,0.0
zipkin,It is not a microservice.,0.0,0.0,1.0,0.0
zipkin,How could I do this?,0.0,0.0,1.0,0.0
zipkin,"Any suggestions, please?",0.0,0.535,0.465,0.3182
zipkin,Thanks.,0.0,1.0,0.0,0.4404
zipkin,I am new to distributed logging and I need help around the propagation of extra fields across Http Request and Messaging Request.,0.0,0.124,0.876,0.4019
zipkin,"Currently, I am able to propagate the traceId and spanId, but I need to pass  correlationId  to be propagated across all the microservices.",0.0,0.0,1.0,0.0
zipkin,logback.xml,0.0,0.0,1.0,0.0
zipkin,I am a bit curious about how to pass the correlation id to other services.,0.0,0.161,0.839,0.3182
zipkin,I have integrated spring cloud sleuth (2.2.5.RELEASE) and Zipkin in my spring boot application.,0.0,0.0,1.0,0.0
zipkin,I am trying to exclude certain url pattern from being traced and show up in Zipkin.,0.112,0.124,0.765,0.0516
zipkin,I have this in my spring boot application.properties file,0.0,0.0,1.0,0.0
zipkin,But still I see that traceId is being generated for url /api/v1/hello and I also can visualize this request in Zipkin.,0.0,0.0,1.0,0.0
zipkin,I have also tried spring.sleuth.web.additional-skip-pattern to achieve this but that also doesn't help.,0.208,0.0,0.792,-0.438
zipkin,How  can I achieve this?,0.0,0.0,1.0,0.0
zipkin,Thanks in advance.,0.0,0.592,0.408,0.4404
zipkin,I have to configure two separate Kafka brokers (as on example below) and this is working just fine - I have my writes and reads on different Kafkas.,0.0,0.07,0.93,0.2023
zipkin,But also I need to configure zipkin+sleuth in application - and only way I can do this is via adding:,0.0,0.0,1.0,0.0
zipkin,"As soon, as  spring.kafka.bootstrap-servers  added, it started to override  kafka2.environment.spring.cloud.stream.kafka.binder.brokers  - so application just trying to write to kafka on dev1-stage.dub, instead of dev2-stage.dub.",0.0,0.0,1.0,0.0
zipkin,How can I prevent this overriding?,0.0,0.216,0.784,0.0258
zipkin,Or how I should rework configuration to support both sets of kafka brokers along with zipkin?,0.0,0.162,0.838,0.4019
zipkin,I am trying to deploy zipkin within k8s.,0.0,0.0,1.0,0.0
zipkin,I am using elasticsearch (version 6.8.8) as storage.,0.0,0.0,1.0,0.0
zipkin,The deployment works fine and the server starts.,0.0,0.205,0.795,0.2023
zipkin,"However, I only can access the server with a port-forward.",0.0,0.0,1.0,0.0
zipkin,$ kubectl -n ns-zipkin port-forward zipkin-bdcf7f78b-shd9p 8888:9411,0.0,0.0,1.0,0.0
zipkin,After that I can access  http://localhost:8888/zipkin/,0.0,0.0,1.0,0.0
zipkin,What could be the reason?,0.0,0.0,1.0,0.0
zipkin,Already the deployment of the service does not get an endpoint (see  in output below) which I would expect.,0.0,0.0,1.0,0.0
zipkin,deployment.yaml,0.0,0.0,1.0,0.0
zipkin,$ kubectl -n ns-zipkin describe deployment zipkin,0.0,0.0,1.0,0.0
zipkin,service.yaml,0.0,0.0,1.0,0.0
zipkin,$ kubectl -n ns-zipkin describe service zipkin,0.0,0.0,1.0,0.0
zipkin,ingress.yaml,0.0,0.0,1.0,0.0
zipkin,$ kubectl -n ns-zipkin describe ingress,0.0,0.0,1.0,0.0
zipkin,"I'm using openzipkin/zipkin:2.21.5 run on okd openshift with some basic default conf (possibly docker run -d -p 9411:9411 openzipkin/zipkin - but I'm not sure, it was installed by admin).",0.083,0.0,0.917,-0.3491
zipkin,After about one minute I can't go into details of trace from search page to which I could go before moment.,0.0,0.0,1.0,0.0
zipkin,Maybe Zipkin server is running out of memory and deletes old spans - are there any statistics about it?,0.0,0.0,1.0,0.0
zipkin,"Maybe it has some default number of detained traces in some circular buffer, if so, how can I increase it?",0.12,0.171,0.709,0.0422
zipkin,How can it be checked and configured?,0.0,0.0,1.0,0.0
zipkin,Where can I find any documentation about Zipkin server configuration?,0.0,0.0,1.0,0.0
zipkin,"https://hub.docker.com/r/openzipkin/zipkin  states mysql is deprecated,  elasticsearch  link is broken, in-memory is not for prod, where can I find a doc for production configuration?",0.134,0.0,0.866,-0.4767
zipkin,How can I force a Zipkin span to be exportable?,0.0,0.0,1.0,0.0
zipkin,"In below code spans are sometimes exportable, sometimes not in a non repeatable manner.",0.0,0.0,1.0,0.0
zipkin,"It seems to me that if I comment first scopedSpan, than second manually created spanInScope is exportable, but how can first scopedSpan prevent second spanInScope from being exportable?",0.0,0.096,0.904,0.1655
zipkin,How do they interfere?,0.0,0.0,1.0,0.0
zipkin,We have  spring-cloud-starter-sleuth  and  spring-cloud-starter-zipkin  dependencies in in other microservices and we are able to see the traces on the  zipkin  dashboard.,0.0,0.0,1.0,0.0
zipkin,We also want to show the custom messages on zipkin i.e whatever log messages we are logging in our controller methods.,0.0,0.061,0.939,0.0772
zipkin,we want to show additional log data on zipkin.,0.0,0.14,0.86,0.0772
zipkin,Is there any way to show these  logger.info  messages on the zipkin dashboard?,0.0,0.0,1.0,0.0
zipkin,"Also, is there any way to customize the format of messages being sent to zipkin?",0.0,0.0,1.0,0.0
zipkin,"we want to show message in something like  %d{yyyy-MMM-dd HH:mm:ss.SSS} %5p [%X{X-B3-TraceId:-},%X{X-B3-SpanId:-}] [%thread] %logger{15} - %replace(%msg %ex){'[\\r\\n]+', '\\n'}%nopex%n  format.",0.0,0.192,0.808,0.4215
zipkin,"I tried in Windows 10 machine to coonect RabbitMQ (3.6.11 version installed with Erlang 20) to ZipKin, but I got the following error:",0.151,0.0,0.849,-0.5499
zipkin,"org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Unsatisfied dependency expressed through method 'armeriaServer' parameter 4; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'serverConfigurator' defined in zipkin2.server.internal.ZipkinHttpConfiguration: Unsatisfied dependency expressed through method 'serverConfigurator' parameter 2; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:797) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:538) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:893) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:879) ~[spring-context-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:551) ~[spring-context-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758) ~[spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:140) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at zipkin.server.ZipkinServer.main(ZipkinServer.java:54) [classes!/:?]",0.154,0.053,0.793,-0.9499
zipkin,at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?,0.0,0.0,1.0,0.0
zipkin,":1.8.0_251]
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?",0.0,0.0,1.0,0.0
zipkin,":1.8.0_251]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?",0.0,0.0,1.0,0.0
zipkin,":1.8.0_251]
at java.lang.reflect.Method.invoke(Unknown Source) ~[?",0.0,0.0,1.0,0.0
zipkin,":1.8.0_251]
at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49) [zipkin-server-2.21.5-exec.jar:?]",0.0,0.0,1.0,0.0
zipkin,at org.springframework.boot.loader.Launcher.launch(Launcher.java:109) [zipkin-server-2.21.5-exec.jar:?],0.0,0.0,1.0,0.0
zipkin,at org.springframework.boot.loader.Launcher.launch(Launcher.java:58) [zipkin-server-2.21.5-exec.jar:?],0.0,0.0,1.0,0.0
zipkin,at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:88) [zipkin-server-2.21.5-exec.jar:?],0.0,0.0,1.0,0.0
zipkin,Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'serverConfigurator' defined in zipkin2.server.internal.ZipkinHttpConfiguration: Unsatisfied dependency expressed through method 'serverConfigurator' parameter 2; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect,0.225,0.078,0.697,-0.9081
zipkin,Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect,0.231,0.075,0.695,-0.8481
zipkin,Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect,0.219,0.059,0.722,-0.7184
zipkin,Caused by: java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect,0.155,0.0,0.845,-0.296
zipkin,Caused by: java.net.ConnectException: Connection refused: connect,0.306,0.0,0.694,-0.296
zipkin,I’m trying to implement an async one way span in zipkin.,0.0,0.0,1.0,0.0
zipkin,Effectively 2 different processes share a span and need to capture between them.,0.0,0.362,0.638,0.6249
zipkin,"Per the documentation , I am using the ‘cs’ and ‘sr’ annotations on the span, however don’t see the duration being captured accurately.",0.0,0.0,1.0,0.0
zipkin,"Brave version: io.zipkin.brave:brave:5.12.3
Zipkin server version:2.21.4
Using it as an in memory storage currently.",0.0,0.207,0.793,0.5267
zipkin,"This is what I’m currently doing:
Client Side:
Tracer.tospan(spancontext).",0.0,0.0,1.0,0.0
zipkin,Annotate(cs).start().flush(),0.0,0.0,1.0,0.0
zipkin,"Server Side:
Tracer.joinSpan(spancontext).",0.0,0.0,1.0,0.0
zipkin,Annotate(sr).start().flush(),0.0,0.0,1.0,0.0
zipkin,The spans Json shows 2 spans of kind Client and server.,0.0,0.274,0.726,0.5267
zipkin,Their time stamps are accurate and they have no duration.,0.196,0.0,0.804,-0.296
zipkin,In the trace graph I expect to see the span having duration as the difference between the timestamp of the client and server spans.,0.0,0.0,1.0,0.0
zipkin,However I don’t.,0.0,0.0,1.0,0.0
zipkin,I do see the expected  duration under the ‘server start’ annotation on the right pane as the ‘relative time’.,0.0,0.0,1.0,0.0
zipkin,Is there anyway to see this as a duration on the span itself?,0.0,0.0,1.0,0.0
zipkin,Is this expected behaviour or am I missing anything?,0.239,0.0,0.761,-0.296
zipkin,"Trying to integrate zipkin and brave in my cxf application, application is having spring xml configuration.",0.0,0.185,0.815,0.5267
zipkin,reference page  https://cxf.apache.org/docs/using-openzipkin-brave.html  is quite outdated and does not include new implementation of different libraries.,0.0,0.0,1.0,0.0
zipkin,Would appreciate any doc or reference for integrating same.,0.0,0.252,0.748,0.4019
zipkin,cxf-version,0.0,0.0,1.0,0.0
zipkin,I am following the tutorial for creating tracing application  zipkin  and  sleuth  but I am having some trouble.,0.185,0.084,0.731,-0.4497
zipkin,I cannot create a span.,0.476,0.0,0.524,-0.2057
zipkin,The problem is that the method does not exist.,0.252,0.0,0.748,-0.4019
zipkin,Also I cannot find the import for the tracer.,0.0,0.0,1.0,0.0
zipkin,This is what I am trying to do:,0.0,0.0,1.0,0.0
zipkin,Why is the implementation above not working?,0.0,0.0,1.0,0.0
zipkin,I want to integrate spring sleuth zipkin server with elastic server.,0.0,0.126,0.874,0.0772
zipkin,Can anyone please help how to integrate?,0.0,0.5,0.5,0.6124
zipkin,This is specifically for Zipkin's Elastic Search storage connector.,0.0,0.0,1.0,0.0
zipkin,Which does not do the index that you can use Curator.,0.0,0.0,1.0,0.0
zipkin,Is there a way of  automatically  removing old traces and have that as part of the ElasticSearch configuration (rather than building yet another service or cron job)  Since I am using it for a development server I just need it wiped every hour or so.,0.0,0.0,1.0,0.0
zipkin,I am new to zipkin server.,0.0,0.0,1.0,0.0
zipkin,I am trying to run a zipkin-server-2.12.9-exec on linux server facing the below exception.,0.0,0.0,1.0,0.0
zipkin,"2020-03-09 15:36:28.796  WARN 1685 --- [           main] s.c.a.AnnotationConfigApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.linecorp.armeria.server.Server]: Factory method 'armeriaServer' threw exception; nested exception is java.util.concurrent.CompletionException: java.lang.IllegalStateException: Armeria server failed to start
2020-03-09 15:36:28.805  INFO 1685 --- [           main] ConditionEvaluationReportLoggingListener :
Error starting ApplicationContext.",0.248,0.028,0.723,-0.9436
zipkin,To display the conditions report re-run your application with 'debug' enabled.,0.0,0.0,1.0,0.0
zipkin,"2020-03-09 15:36:28.806 ERROR 1685 --- [           main] o.s.b.SpringApplication                  : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.linecorp.armeria.server.Server]: Factory method 'armeriaServer' threw exception; nested exception is java.util.concurrent.CompletionException: java.lang.IllegalStateException: Armeria server failed to start",0.304,0.035,0.661,-0.9526
zipkin,I want to use stackdriver trace as a back end for distributed tracing in istio.,0.0,0.098,0.902,0.0772
zipkin,"I installed Docker on the VM of GCP, and run the image of zipkin-gcp.",0.0,0.0,1.0,0.0
zipkin,"Then, according to the official documentation, I configured istio to send spans to this VM.",0.0,0.0,1.0,0.0
zipkin,"However, no trace was displayed in the stackdriver trace.",0.216,0.0,0.784,-0.296
zipkin,"To isolate the problem, I stopped zipkin-gcp and checked if packets were being sent with tcpdump.",0.348,0.0,0.652,-0.6597
zipkin,"As a result, it was found that nothing was sent.",0.0,0.0,1.0,0.0
zipkin,I have confirmed port 9411 connectivity from the kubernetes cluster to the VM.,0.0,0.0,1.0,0.0
zipkin,How do I send a trace to an external zipkin server with istio?,0.0,0.0,1.0,0.0
zipkin,"I am trying to configure Istio control Plane to use zipkin as tracing backend, but I can't.",0.0,0.0,1.0,0.0
zipkin,"In their docs, they state that in order to do this, I just have to pass the following parameters when installing Istio:",0.0,0.0,1.0,0.0
zipkin,--set values.tracing.enabled=true  and  --set values.tracing.provider=zipkin .,0.0,0.0,1.0,0.0
zipkin,My problem is that I have installed Istio manually.,0.278,0.0,0.722,-0.4019
zipkin,"I found the parameter  provider: jaeger  in the  Configmap   istio-sidecar-injector , and made the change, then killed the control plane so it would be re-deployed with zipkin, but didn't work.",0.092,0.0,0.908,-0.4118
zipkin,Does anyone know what object/s should I manipulate to get zipkin?,0.0,0.0,1.0,0.0
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,class,0.0,0.0,1.0,0.0
zipkin,Please tell me how to solve the issue?,0.0,0.406,0.594,0.4767
zipkin,I have implemented different microservice where internal communication is taking place by camel and rabbitMQ with different exchange and queue.,0.0,0.0,1.0,0.0
zipkin,"I'm using  camel-zipkin  for tracing, and  log4j2  for logging.",0.0,0.0,1.0,0.0
zipkin,Ex:,0.0,0.0,1.0,0.0
zipkin,I am using Springboot  1.5.12.RELEASE  and  Camel  2.21.5 version   with the following dependencies (of camel):,0.0,0.0,1.0,0.0
zipkin,I need suggestion for my two problems,0.351,0.0,0.649,-0.4019
zipkin,My Questions are :,0.0,0.0,1.0,0.0
zipkin,Sample Code :,0.0,0.0,1.0,0.0
zipkin,Pom.xml,0.0,0.0,1.0,0.0
zipkin,SpringBoot Application,0.0,0.0,1.0,0.0
zipkin,RabbitMQConfig,0.0,0.0,1.0,0.0
zipkin,Controller adding message to queue,0.0,0.0,1.0,0.0
zipkin,RouteBuilder,0.0,0.0,1.0,0.0
zipkin,MessageActivity,0.0,0.0,1.0,0.0
zipkin,SmsActivity,0.0,0.0,1.0,0.0
zipkin,Output,0.0,0.0,1.0,0.0
zipkin,"While creating a Zipkin Server with Spring Boot(v2), I am facing Whitelabel Error Page. 
        """,0.181,0.148,0.671,-0.128
zipkin,"Whitelabel Error Page 
        This application has no explicit mapping for /error, so you are seeing this as a fallback.",0.234,0.0,0.766,-0.5994
zipkin,"Wed Oct 30 11:21:35 IST 2019
        There was an unexpected error (type=Not Found, status=404).",0.172,0.0,0.828,-0.4019
zipkin,"No message available 
        "" 
And also while i run the application in spring boot, i get: 
        ""  Cannot find template location: classpath:/templates/ (please add some templates or check your 
        Thymeleaf configuration) """,0.078,0.0,0.922,-0.296
zipkin,Please help me to resolve this,0.0,0.717,0.283,0.765
zipkin,"In the logs, Zipkin status is coming as true but I can not see it in the Zipkin UI.",0.0,0.101,0.899,0.2263
zipkin,The same things are working for the zuul but not for the other microservices.,0.0,0.0,1.0,0.0
zipkin,dependencies,0.0,0.0,1.0,0.0
zipkin,properties:,0.0,0.0,1.0,0.0
zipkin,"The only difference between zuul and this microservice is, it is using the spring cloud stream as well.",0.0,0.11,0.89,0.2732
zipkin,Can it be a reason?,0.0,0.0,1.0,0.0
zipkin,"I try to setup zipkin, elasticsearch, prometheus and grafana with docker-compose.yml 
When I run dockers, see in the log:",0.0,0.0,1.0,0.0
zipkin,dependencies_zipkin | 19/09/30 14:37:09 ERROR NetworkClient: Node [172.28.0.2:9200] failed (java.net.ConnectException: Connection refused (Connection refused)); no other nodes left - aborting...,0.443,0.0,0.557,-0.8788
zipkin,I'm on MacOS X with docker 2.1.0.3,0.0,0.0,1.0,0.0
zipkin,the content of my docker-compose.yml is this one:,0.0,0.0,1.0,0.0
zipkin,"When I connect to localhost:9200, I see that elasticsearch is working fine and on port 9411, zipkin is deployed but I have the error:",0.148,0.058,0.793,-0.4854
zipkin,ERROR: cannot load service names: server error (Service Unavailable)(due to the network error,0.469,0.0,0.531,-0.8331
zipkin,"In the log, I have this information:",0.0,0.0,1.0,0.0
zipkin,105 ^[[35mdependencies_zipkin |^[[0m 19/09/30 14:45:20 ERROR NetworkClient: Node [172.28.0.2:9200] failed (java.net.ConnectException: Connection refused (Connection refused)); no     other nodes left - aborting...,0.41,0.0,0.59,-0.8788
zipkin,and this one,0.0,0.0,1.0,0.0
zipkin,"^[[31mzipkin          |^[[0m java.lang.IllegalStateException: couldn't connect any of [Endpoint{storage:80, ipAddr=172.28.0.2, weight=1000}]",0.0,0.0,1.0,0.0
zipkin,Any idea?,0.0,0.0,1.0,0.0
zipkin,"UPDATE
by using mysql it is working fine, so the problem is at the level of elastic search.",0.143,0.087,0.77,-0.2911
zipkin,I tried alsoo by using,0.0,0.0,1.0,0.0
zipkin,"""STORAGE_PORT_9200_TCP_ADDR=127.0.0.1""",0.0,0.0,1.0,0.0
zipkin,but the issue still occurs.,0.0,0.0,1.0,0.0
zipkin,"UPDATE
As mention is the solution gave by Brian, I have to use:",0.0,0.173,0.827,0.3182
zipkin,ES_HOSTS= http://storage:9300,0.0,0.0,1.0,0.0
zipkin,"the key is on port, I was using the port 9200",0.0,0.0,1.0,0.0
zipkin,The error disappear between zipkin and es but still occurs between es and zipkin-dependencies.,0.216,0.0,0.784,-0.3182
zipkin,I use zipking for testing with curl post.,0.0,0.0,1.0,0.0
zipkin,Examples for post  https://zipkin.io/zipkin-api/#/default/post_spans,0.0,0.0,1.0,0.0
zipkin,503 Service Unavailable,0.0,0.0,1.0,0.0
zipkin,"zipkin in docker, logs in container:",0.0,0.0,1.0,0.0
zipkin,"2019-07-24 07:05:42.383 WARN 1 --- [orker-epoll-2-5]
  z.s.i.BodyIsExceptionMessage : Unexpected error handling request.",0.377,0.0,0.623,-0.5904
zipkin,com.linecorp.armeria.common.stream.AbortedStreamException: null,0.0,0.0,1.0,0.0
zipkin,I also tried example:,0.0,0.0,1.0,0.0
zipkin,but I do not sees in web ui.,0.0,0.0,1.0,0.0
zipkin,What is the corresponding command for “STORAGE_TYPE=mysql MYSQL_USER=root java -jar zipkin.jar” under cmd?,0.0,0.0,1.0,0.0
zipkin,"I succeeded in Ubuntu, but the cmd in Windows has never been successful.",0.257,0.119,0.625,-0.4953
zipkin,My spring boot application has some problem.,0.31,0.0,0.69,-0.4019
zipkin,Zipkin and jdbc can not coexist together.,0.0,0.0,1.0,0.0
zipkin,It is normal to have only one zipkin or jdbc.,0.0,0.0,1.0,0.0
zipkin,Maven dependency:,0.0,0.0,1.0,0.0
zipkin,Exception:,0.0,0.0,1.0,0.0
zipkin,Nested exception is:,0.0,0.0,1.0,0.0
zipkin,org.springframework.beans.factory.BeanCreationNotAllowedException: Error creating bean with name 'eurekaClientConfigBean': Singleton bean creation not allowed while singletons of this factory are in destruction (Do not request a bean from a BeanFactory in a destroy method implementation!),0.258,0.109,0.633,-0.784
zipkin,Trying to add Zipkin mysql tracing instrumentation by including the following library in a Spring Boot Application,0.0,0.0,1.0,0.0
zipkin,And appending the following to my jdbc connection string:,0.0,0.0,1.0,0.0
zipkin,Existing DB connection is using a c3po connection pool configured as follows:,0.0,0.0,1.0,0.0
zipkin,but running into issue starting service when attempting to acquire connections from the pool:,0.0,0.0,1.0,0.0
zipkin,We are in process to keep all monitoring and logging stuff outside of AKS.,0.0,0.0,1.0,0.0
zipkin,we got some success with Azure log analytics as well.,0.0,0.42,0.58,0.7003
zipkin,I am checking if Azure log analytics provide any feature similar to zipkin.,0.0,0.0,1.0,0.0
zipkin,i.e.,0.0,0.0,1.0,0.0
zipkin,providing trace of REST API.,0.0,0.0,1.0,0.0
zipkin,I hope you can help me.,0.0,0.651,0.349,0.6808
zipkin,Let's use for example this very simple code,0.0,0.0,1.0,0.0
zipkin,Like you can see this microservice only forwards messages from one kafka-topic to another.,0.0,0.161,0.839,0.3612
zipkin,I also want to send this data to zipkin to see the duration of the messages or something like that.,0.0,0.183,0.817,0.4215
zipkin,Maybe I've seen the solution and don't get it but I realy have looked for a solution but didn't find one.,0.0,0.213,0.787,0.5574
zipkin,You are my last hope.,0.0,0.42,0.58,0.4404
zipkin,I have seen the brave api but I don't realy understand how to use it for kafka.,0.0,0.136,0.864,0.296
zipkin,I,0.0,0.0,0.0,0.0
zipkin,I am new to sleuth and zipkin.,0.0,0.0,1.0,0.0
zipkin,I have logged some messages and sleuth is appending trace id and space id for those messages.,0.0,0.0,1.0,0.0
zipkin,I am using zipkin to visualize it.,0.0,0.0,1.0,0.0
zipkin,I am able to see timings at different microservices.,0.0,0.0,1.0,0.0
zipkin,Can we see logger messages(we put at different microservices) in zipkin UI by trace id?,0.0,0.0,1.0,0.0
zipkin,I am new to distributed tracing and trying to use the example explained in the video  https://www.youtube.com/watch?v=CFLZJSwbYI0,0.0,0.0,1.0,0.0
zipkin,In short this has following,0.0,0.0,1.0,0.0
zipkin,Now when I run zipkin server using command,0.0,0.0,1.0,0.0
zipkin,java -jar zipkin.jar,0.0,0.0,1.0,0.0
zipkin,"and accesing zipkin at url
     http://localhost:9411/zipkin",0.0,0.0,1.0,0.0
zipkin,So far everything works fine.,0.0,0.31,0.69,0.2023
zipkin,Now I am starting zipkin-client/service which is running at port 8081/8082,0.0,0.0,1.0,0.0
zipkin,After this i accessed zipkin url ( http://localhost:9411/zipkin ) but it's broken now.,0.316,0.0,0.684,-0.631
zipkin,I am wondering why starting other service on port 8081/8081 is causing zipkin server to stop responding.,0.128,0.0,0.872,-0.296
zipkin,Any kind of help is much appreciated!!,0.0,0.557,0.443,0.7423
zipkin,!,0.0,0.0,0.0,0.0
zipkin,"I am upgrading an application to Spring Boot 2.1.3 (from 1.5.x), and I am facing an issue at startup time.",0.0,0.0,1.0,0.0
zipkin,Below block can't be bound properly :,0.367,0.0,0.633,-0.4404
zipkin,I am getting this error :,0.51,0.0,0.49,-0.481
zipkin,A bit before I am getting a WARN log announcing the issue :,0.211,0.0,0.789,-0.2808
zipkin,"I am trying to follow in debug, and I end up pretty deep in Spring Boot internals 
 in  org.springframework.boot.context.properties.bind.Binder  .",0.0,0.167,0.833,0.4939
zipkin,I have a similar app with more or less same version for which it works just fine.,0.0,0.114,0.886,0.2023
zipkin,"I am trying to find a difference, compare the execution flows, but not finding anything obvious.",0.0,0.0,1.0,0.0
zipkin,"In IntelliJ, I get the auto-completion so I know my yaml is formatted properly : the ""web"" value is proposed to me.",0.0,0.118,0.882,0.34
zipkin,Any idea of how to investigate this kind of issue ?,0.0,0.0,1.0,0.0
zipkin,"I found there is an old issue  Sleuth/Zipkin tracing with @ControllerAdvice , but I meet the same problem with the latest version(spring-cloud-starter-zipkin:2.1.0.RELEASE), I debug it and find that the error is null, so zipkin just guess with statuscode.",0.182,0.0,0.818,-0.7964
zipkin,I have to throw the exception again to make zipkin notify the exception,0.0,0.0,1.0,0.0
zipkin,error is null,0.574,0.0,0.426,-0.4019
zipkin,zipkin result,0.0,0.0,1.0,0.0
zipkin,ControllerAdvice,0.0,0.0,1.0,0.0
zipkin,"throw the exception again, it works",0.0,0.0,1.0,0.0
zipkin,"In the micro-services based architecture , I have a services which helps me to fetch order details .",0.0,0.167,0.833,0.3818
zipkin,"Internally order details fetches - customer details , delivery details , product details .",0.0,0.0,1.0,0.0
zipkin,We have all services developed and the architecture is established.,0.0,0.0,1.0,0.0
zipkin,No we export everything to zipkin with the sampler rate of 100%.,0.167,0.0,0.833,-0.296
zipkin,"So it includes - INFO level logs and error level logs also , currently it is useful but we already have separate mechanism for ERROR level logs.",0.213,0.064,0.723,-0.6756
zipkin,"so , i just want to skip the zipkin logging for ERROR level log and  send only INFO level logs to zipkin",0.149,0.069,0.782,-0.4319
zipkin,"I tried searching through, but could not get any help, i am newbie to micro services",0.182,0.0,0.818,-0.438
zipkin,"any help is appreciated , thank you",0.0,0.739,0.261,0.8176
zipkin,I created a spring boot 2.1.2 basic web app using the initializr tool.,0.0,0.167,0.833,0.25
zipkin,The app starts fine and responds to a hello world kinda request.,0.0,0.153,0.847,0.2023
zipkin,"When I then attempted to add zipkin and sleuth, I now get an error.",0.197,0.0,0.803,-0.4019
zipkin,Process finished with exit code 0,0.0,0.0,1.0,0.0
zipkin,Dependencies look like this,0.0,0.455,0.545,0.3612
zipkin,"I tried to go down to spring boot version 2.0.4, which is the next down available through maven, however then the Jersey package started bucking.",0.0,0.0,1.0,0.0
zipkin,Is there a way to get zipkin and sleuth to work with spring boot 2.1.2?,0.0,0.0,1.0,0.0
zipkin,Swagger seem to have a similar issue.,0.0,0.0,1.0,0.0
zipkin,I want to write the zipkin trace data from the recorder to a file using NodeJS in a format which zipkin ui supports so that i can import the file into the zipkin ui later and do analysis.,0.0,0.106,0.894,0.4215
zipkin,Actually I have a microservice architecture as follows,0.0,0.0,1.0,0.0
zipkin,So I have 4 microservices and for every microservice I send a notification to zipkin when starts and finish it's objective.,0.0,0.0,1.0,0.0
zipkin,I have to monitor my product to make sure all requested checkouts will have,0.0,0.161,0.839,0.3182
zipkin,"zipkin as a tracking system already own all this information cause it keep the checkout track from the very beginning until the end, I'm wondering how I can query at zipkin all checkouts that have been processed by  JAVA REST API  microservice and didn't be processed by at least one of the others ( PAYMENT GATEWAY ,  SALE CREATOR  and  EMAIL NOTIFIER",0.0,0.0,1.0,0.0
zipkin,How can I query on zipkin which checkouts haven't been processed by all others microservices after  REST API ?,0.0,0.0,1.0,0.0
zipkin,Hi everyone!,0.0,0.0,1.0,0.0
zipkin,"I have ""ELK"" (6.4.2) working perfectly with filebeat, metricbeat, packetbeat and winlogbeat in CentOS 7 x86_64 (Kernel 3.10.0-862.11.6.el7.x86_64).",0.0,0.219,0.781,0.6369
zipkin,"I'm trying to integrate zipkin + elk (see  https://logz.io/blog/zipkin-elk/ ), but Elasticsearch does not create indices with Kibana.",0.122,0.0,0.878,-0.3007
zipkin,"When trying to create the indices in Kibana, the process does not end.",0.0,0.149,0.851,0.2732
zipkin,(Follow logs below).,0.0,0.0,1.0,0.0
zipkin,I suspect the zipkin connection drivers are not compatible with elk 6.4.2.,0.18,0.0,0.82,-0.296
zipkin,"Has anyone had the same problem and has a ""light at the end of the tunnel""?",0.162,0.0,0.838,-0.4019
zipkin,Tks for all!,0.0,0.0,1.0,0.0
zipkin,Java version:,0.0,0.0,1.0,0.0
zipkin,Zipkin startup:,0.0,0.0,1.0,0.0
zipkin,Error log in Elasticsearch:,0.474,0.0,0.526,-0.4019
zipkin,Requirement is to export traces for requests that matches url pattern to zipkin from apps.,0.0,0.0,1.0,0.0
zipkin,I got to know that there are options in sleuth properties to exclude traces from exporting.,0.119,0.0,0.881,-0.2263
zipkin,But my case is the opposite of it.,0.0,0.0,1.0,0.0
zipkin,Include traces for exporting for only specified url patterns.,0.0,0.0,1.0,0.0
zipkin,I was trying to have a custom httpSampler and mentioned my logic to export the trace based on url patterns.,0.0,0.0,1.0,0.0
zipkin,But it did not work as expected.,0.0,0.0,1.0,0.0
zipkin,"Any samples available on the same, would really be helpful?",0.0,0.255,0.745,0.4728
zipkin,Thanks much.,0.0,0.744,0.256,0.4404
zipkin,I have the following problem: i need to send traces to Zipkin via Kafka using Sleuth.,0.172,0.0,0.828,-0.4019
zipkin,Based on what i read in the documentation( https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka ) all i need to do is to add  spring-kafka  dependency:,0.0,0.0,1.0,0.0
zipkin,But i can't really see there to which kafka bootstrap server should i send or to which topic(is there some default topic that i should know about?,0.0,0.0,1.0,0.0
zipkin,).,0.0,0.0,1.0,0.0
zipkin,I have the following problem: i have added numerous services in Zipkin but now i want to remove some of them.,0.096,0.075,0.829,-0.1027
zipkin,I keep the data in memory so no persistency involved there.,0.242,0.0,0.758,-0.4341
zipkin,Is there any way to delete a service name from Zipkin's service name dropdown list?,0.0,0.0,1.0,0.0
zipkin,I have two Microservices (Spring boot application) .,0.0,0.0,1.0,0.0
zipkin,For tracing I am using  &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;  along with zipkin.,0.0,0.0,1.0,0.0
zipkin,Service A is producer and  send message using RabbitMQ broker.,0.0,0.0,1.0,0.0
zipkin,"On other hand Service B is the consumer, their is   @RabbitListener  .",0.0,0.262,0.738,0.4939
zipkin,"I want to exchange the traceId(with span details) from service A to Service B.
I have seen the  example  (using brave) but unable to integrate zipkin with rabbitMQ and trace propogation.",0.0,0.041,0.959,0.0387
zipkin,Can Any One please help me how to acheive this ?Any complete step-by-step and simple example?,0.0,0.277,0.723,0.6553
zipkin,"In spring initializer i couldn't find following dependencies  zipkin ui ,  zipkin stream ,  stream rabbit .I know it was available but i don't know why they've deprecated those dependencies.",0.0,0.0,1.0,0.0
zipkin,Are there any other alternatives dependencies spring initializer provide?,0.0,0.0,1.0,0.0
zipkin,"We have a spring-boot application (spring-boot-starter-parent-2.0.0.RELEASE) using spring-cloud-starter-zipkin for writing ""spans"" to zipkin.",0.0,0.0,1.0,0.0
zipkin,We use spring-integration too (through spring-boot-starter-integration) and we have added an integration flow with a PollableChannel to be used within a poller:,0.0,0.0,1.0,0.0
zipkin,"Since adding this configuration, we are having an ""asynch"" span every second.",0.0,0.0,1.0,0.0
zipkin,"It seems this span comes from the @Poller, checking whether there are items in the queue.",0.0,0.0,1.0,0.0
zipkin,I'd like to know how to control this span.,0.0,0.238,0.762,0.3612
zipkin,Is it possible to disable?,0.0,0.0,1.0,0.0
zipkin,Specially if there are no items.,0.306,0.0,0.694,-0.296
zipkin,Thanks in advance!,0.0,0.615,0.385,0.4926
zipkin,I am looking for a Tracing tool for my spring web-mvc application and i ended up with using Brave-zipkin[ https://github.com/openzipkin/brave-webmvc-example/tree/master/webmvc3] .,0.0,0.0,1.0,0.0
zipkin,"Everything looks fine for me except that, in the given example jetty server deploys the application twice; one for FrontEnd and another for Backend(using two profiles).",0.0,0.067,0.933,0.2023
zipkin,Whereas my project uses tomcat-server.,0.0,0.0,1.0,0.0
zipkin,Can anyone help me how to use this same tool for deploying in Tomcat-server and start the application without using profiles?,0.0,0.119,0.881,0.4019
zipkin,or please suggest any other open source tool for tracing simple monolithic spring-web-mvc application (not spring-boot) and i should be able to see the spans and dependency (eg controllerClass- serviceClass- repositoryClass just like we see under dependency tab of openzipkin web page:  http://localhost:9411/zipkin/dependency/ ),0.0,0.107,0.893,0.5859
zipkin,I am using zipkin to do distributed tracing of my microservice architecture.,0.0,0.0,1.0,0.0
zipkin,I have created zipkin server by adding zipkin server dependency and @EnableZipkinServer Annotation.,0.0,0.154,0.846,0.25
zipkin,Now is there a way I can add password protection to my zipkin web interface?,0.0,0.0,1.0,0.0
zipkin,I am using Zipkin with Spring Sleuth to display traces.,0.0,0.0,1.0,0.0
zipkin,"When I use it locally,  http://localhost:9411/zipkin/dependency/  displays a nicely created graph of dependencies within the eco-system.",0.0,0.29,0.71,0.5994
zipkin,"Sometimes, backends from outside that eco-system get called and those are not displayed in that graph.",0.0,0.0,1.0,0.0
zipkin,Is it possible to annotate a call (let's assume RestTemplate and Feign clients) to such an external system so Zipkin would actually draw that dependency?,0.0,0.0,1.0,0.0
zipkin,"If it's possible, what do I have to do?",0.0,0.0,1.0,0.0
zipkin,This would be my baseline of code:,0.0,0.0,1.0,0.0
zipkin,Somewhere I would like to type  httpbin  so this call gets drawn in the dependency-graph of Zipkin.,0.0,0.143,0.857,0.3612
zipkin,Thank you!,0.0,0.736,0.264,0.4199
zipkin,"//  Edit  based on current solution
I'm using Spring Cloud Finchley and added the following line before restTemplate's call:",0.0,0.113,0.887,0.3182
zipkin,I simply inject  SpanCustomizer  in this class.,0.0,0.0,1.0,0.0
zipkin,The Span is sent to Zipkin and I see the tag is set:,0.0,0.0,1.0,0.0
zipkin,"Unfortunately, it is not drawn in the dependencies-view.",0.255,0.0,0.745,-0.34
zipkin,"Is there anything else I need to configure, maybe in Zipkin rather than in Sleuth?",0.0,0.0,1.0,0.0
zipkin,I have the following setup :,0.0,0.0,1.0,0.0
zipkin,There used to be a  StreamEnvironmentPostProcessor  that did the job of adding the trace headers to the kafka bindings when I included the  spring-cloud-sleuth-stream  dependendy in the past.,0.0,0.0,1.0,0.0
zipkin,But the doc clearly states now  :,0.0,0.415,0.585,0.5499
zipkin,Note: spring-cloud-sleuth-stream is deprecated and incompatible with these destinations,0.0,0.0,1.0,0.0
zipkin,What should I do to make this work properly now ?,0.0,0.0,1.0,0.0
zipkin,Add the headers to the bindings configuration myself ?,0.0,0.0,1.0,0.0
zipkin,Or is there something I'm missing ?,0.306,0.0,0.694,-0.296
zipkin,in the documentation it is explained that your services have to resend a set of headers to enable pilot/Zipkin to correlate the information.,0.0,0.0,1.0,0.0
zipkin,But who generates the first headers and set its values?,0.0,0.283,0.717,0.5499
zipkin,The Istio Ingress controller?,0.0,0.0,1.0,0.0
zipkin,"How can I configure it, enable/disable it?",0.0,0.0,1.0,0.0
zipkin,Thank you.,0.0,0.714,0.286,0.3612
zipkin,I have done all the possible matches and mix-up of dependency and still not able to record traces in zipkin ans store it in MYSQL using RabbitMQ.,0.0,0.0,1.0,0.0
zipkin,Still i can see the trace and span id's  in console and nothing beyond this.,0.0,0.0,1.0,0.0
zipkin,Someone please take a look at the code in github from below location.,0.0,0.173,0.827,0.3182
zipkin,Github code:  https://github.com/javayp/distributed-tracing-1,0.0,0.0,1.0,0.0
zipkin,I'm testing zipkin to spring boot integration but im facing error like below.,0.199,0.183,0.618,-0.0772
zipkin,The error seems to happen when it tries to send message to zipkin server,0.172,0.0,0.828,-0.4019
zipkin,here is my pom.xml file.,0.0,0.0,1.0,0.0
zipkin,it may be have a problem in version.,0.31,0.0,0.69,-0.4019
zipkin,"and this is my application.yml file
im running zipkin server in same machine with different port",0.0,0.0,1.0,0.0
zipkin,any guide or information are welcomed!,0.0,0.35,0.65,0.4003
zipkin,"In our cluster, we have set up a Zipkin collector for Stackdriver Trace ( like this ) so we can trace our apps.",0.0,0.116,0.884,0.3612
zipkin,I am running the simple  JavaScript web example  that is offered.,0.0,0.0,1.0,0.0
zipkin,It works correctly when I configure the app to send the traces to the collector that is running in the cluster (in  recorder.js ).,0.0,0.0,1.0,0.0
zipkin,"However, when I want to inspect the traces in Stackdriver Trace, something seems to be going wrong:",0.168,0.071,0.761,-0.4215
zipkin,"The  HTTP Method  column is empty, and the  URI  column seems to show the HTTP method.",0.107,0.0,0.893,-0.2023
zipkin,How can I make these columns display the correct information?,0.0,0.0,1.0,0.0
zipkin,Let me know if I need to add more information.,0.0,0.0,1.0,0.0
zipkin,"I have a heterogeneous(Java, php, python, C#.Net) micro-service system which was written by several teams.",0.0,0.0,1.0,0.0
zipkin,All communication happens over HTTP connections.,0.0,0.0,1.0,0.0
zipkin,"I objective is to use Zipkin to trace the path of execution and identify the slowest services and start profiling them using (VisualVM, dotTrace).",0.0,0.0,1.0,0.0
zipkin,I've heard that Zipkin supports HTTP connectivity for tracing.,0.0,0.238,0.762,0.3612
zipkin,How would I go about doing this?,0.0,0.0,1.0,0.0
zipkin,Is Zipkin even the right approach?,0.0,0.0,1.0,0.0
zipkin,I'm looking for direction and some Java examples to get started.,0.0,0.0,1.0,0.0
zipkin,"Is there a http format I could use or do I need to use multiple (Wingtips, ZipkinTracerModule,Brave) libraries?",0.0,0.0,1.0,0.0
zipkin,Thanks.,0.0,1.0,0.0,0.4404
zipkin,"In the zipkin web ui, when the request url is  http://10.19.138.169:9411/zipkin/api/v1/trace/ae60bd175a61e820",0.0,0.0,1.0,0.0
zipkin,"I find the return response is 
[
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""ae60bd175a61e820"",
        ""name"": ""client"",
        ""timestamp"": 1511858133224433,
        ""duration"": 508444,
        ""binaryAnnotations"": [
            {
                ""key"": ""lc"",
                ""value"": """",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-http-c"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""19d69c3e93bc9040"",
        ""name"": ""post"",
        ""parentId"": ""ae60bd175a61e820"",
        ""timestamp"": 1511858133239803,
        ""duration"": 490921,
        ""annotations"": [
            {
                ""timestamp"": 1511858133239803,
                ""value"": ""cs"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-http-c"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133383290,
                ""value"": ""sr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133609368,
                ""value"": ""ss"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133730724,
                ""value"": ""cr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-http-c"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ],
        ""binaryAnnotations"": [
            {
                ""key"": ""ca"",
                ""value"": true,
                ""endpoint"": {
                    ""serviceName"": """",
                    ""ipv4"": ""127.0.0.1"",
                    ""port"": 43928
                }
            },
            {
                ""key"": ""http.path"",
                ""value"": ""/security/gateway"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""key"": ""http.path"",
                ""value"": ""/security/gateway"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-http-c"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""key"": ""sa"",
                ""value"": true,
                ""endpoint"": {
                    ""serviceName"": """",
                    ""ipv4"": ""127.0.0.1"",
                    ""port"": 8090
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""16eefe087852af41"",
        ""name"": ""ennmonitorsecuritygatewayserver/put"",
        ""parentId"": ""19d69c3e93bc9040"",
        ""timestamp"": 1511858133393425,
        ""duration"": 212916,
        ""annotations"": [
            {
                ""timestamp"": 1511858133393425,
                ""value"": ""cs"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133588237,
                ""value"": ""sr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133593907,
                ""value"": ""ss"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133606341,
                ""value"": ""cr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""8ef78f0edefe3a4b"",
        ""name"": ""data enqueue"",
        ""parentId"": ""16eefe087852af41"",
        ""timestamp"": 1511858133592958,
        ""duration"": 129,
        ""binaryAnnotations"": [
            {
                ""key"": ""lc"",
                ""value"": """",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""97c637bcc891b86a"",
        ""name"": ""data dequeue, send to kafka"",
        ""parentId"": ""16eefe087852af41"",
        ""timestamp"": 1511858133593147,
        ""duration"": 2416,
        ""binaryAnnotations"": [
            {
                ""key"": ""lc"",
                ""value"": """",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""f193c7f4193f2879"",
        ""name"": """",
        ""parentId"": ""16eefe087852af41"",
        ""timestamp"": 1511858133594113,
        ""duration"": 7575,
        ""annotations"": [
            {
                ""timestamp"": 1511858133594113,
                ""value"": ""ms"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133601688,
                ""value"": ""ws"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ],
        ""binaryAnnotations"": [
            {
                ""key"": ""kafka.topic"",
                ""value"": ""rdkafka"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""54a3f6268df0aaee"",
        ""name"": """",
        ""parentId"": ""f193c7f4193f2879"",
        ""timestamp"": 1511858133600067,
        ""duration"": 5,
        ""annotations"": [
            {
                ""timestamp"": 1511858133600067,
                ""value"": ""wr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-kafka-consumer"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133600072,
                ""value"": ""mr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-kafka-consumer"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ],
        ""binaryAnnotations"": [
            {
                ""key"": ""kafka.topic"",
                ""value"": ""rdkafka"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-kafka-consumer"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    }
]",0.0,0.018,0.982,0.6808
zipkin,It can easy to find that there are 8 spans.,0.0,0.266,0.734,0.4404
zipkin,"When I use the api to get the trace with the same traceId
        ElasticsearchStorage storage = ElasticsearchStorage.newBuilder()
                .hosts(Arrays.asList("" http://10.19.138.169:9200 "")).build();",0.0,0.0,1.0,0.0
zipkin,"I get 
[
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""97c637bcc891b86a"",
    ""name"": ""data dequeue, send to kafka"",
    ""timestamp"": 1511858133593147,
    ""duration"": 2416,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""id"": ""ae60bd175a61e820"",
    ""name"": ""client"",
    ""timestamp"": 1511858133224433,
    ""duration"": 508444,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-http-c"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""f193c7f4193f2879"",
    ""id"": ""54a3f6268df0aaee"",
    ""kind"": ""CONSUMER"",
    ""timestamp"": 1511858133600067,
    ""duration"": 5,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-kafka-consumer"",
      ""ipv4"": ""10.19.138.169""
    },
    ""tags"": {
      ""kafka.topic"": ""rdkafka""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""ae60bd175a61e820"",
    ""id"": ""19d69c3e93bc9040"",
    ""kind"": ""SERVER"",
    ""name"": ""post"",
    ""timestamp"": 1511858133383290,
    ""duration"": 226078,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-web"",
      ""ipv4"": ""10.19.138.169""
    },
    ""remoteEndpoint"": {
      ""ipv4"": ""127.0.0.1"",
      ""port"": 43928
    },
    ""tags"": {
      ""http.path"": ""/security/gateway""
    },
    ""shared"": true
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""19d69c3e93bc9040"",
    ""id"": ""16eefe087852af41"",
    ""kind"": ""CLIENT"",
    ""name"": ""ennmonitorsecuritygatewayserver/put"",
    ""timestamp"": 1511858133393425,
    ""duration"": 212916,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-web"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""19d69c3e93bc9040"",
    ""id"": ""16eefe087852af41"",
    ""kind"": ""SERVER"",
    ""name"": ""ennmonitorsecuritygatewayserver/put"",
    ""timestamp"": 1511858133588237,
    ""duration"": 5670,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    },
    ""shared"": true
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""f193c7f4193f2879"",
    ""kind"": ""PRODUCER"",
    ""timestamp"": 1511858133594113,
    ""duration"": 7575,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    },
    ""tags"": {
      ""kafka.topic"": ""rdkafka""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""ae60bd175a61e820"",
    ""id"": ""19d69c3e93bc9040"",
    ""kind"": ""CLIENT"",
    ""name"": ""post"",
    ""timestamp"": 1511858133239803,
    ""duration"": 490921,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-http-c"",
      ""ipv4"": ""10.19.138.169""
    },
    ""remoteEndpoint"": {
      ""ipv4"": ""127.0.0.1"",
      ""port"": 8090
    },
    ""tags"": {
      ""http.path"": ""/security/gateway""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""ae60bd175a61e820"",
    ""id"": ""19d69c3e93bc9040"",
    ""kind"": ""SERVER"",
    ""name"": ""post"",
    ""timestamp"": 1511858133383290,
    ""duration"": 226078,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-web"",
      ""ipv4"": ""10.19.138.169""
    },
    ""remoteEndpoint"": {
      ""ipv4"": ""127.0.0.1"",
      ""port"": 43928
    },
    ""tags"": {
      ""http.path"": ""/security/gateway""
    },
    ""shared"": true
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""19d69c3e93bc9040"",
    ""id"": ""16eefe087852af41"",
    ""kind"": ""SERVER"",
    ""name"": ""ennmonitorsecuritygatewayserver/put"",
    ""timestamp"": 1511858133588237,
    ""duration"": 5670,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    },
    ""shared"": true
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""97c637bcc891b86a"",
    ""name"": ""data dequeue, send to kafka"",
    ""timestamp"": 1511858133593147,
    ""duration"": 2416,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""f193c7f4193f2879"",
    ""kind"": ""PRODUCER"",
    ""timestamp"": 1511858133594113,
    ""duration"": 7575,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    },
    ""tags"": {
      ""kafka.topic"": ""rdkafka""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""19d69c3e93bc9040"",
    ""id"": ""16eefe087852af41"",
    ""kind"": ""CLIENT"",
    ""name"": ""ennmonitorsecuritygatewayserver/put"",
    ""timestamp"": 1511858133393425,
    ""duration"": 212916,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-web"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""ae60bd175a61e820"",
    ""id"": ""19d69c3e93bc9040"",
    ""kind"": ""CLIENT"",
    ""name"": ""post"",
    ""timestamp"": 1511858133239803,
    ""duration"": 490921,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-http-c"",
      ""ipv4"": ""10.19.138.169""
    },
    ""remoteEndpoint"": {
      ""ipv4"": ""127.0.0.1"",
      ""port"": 8090
    },
    ""tags"": {
      ""http.path"": ""/security/gateway""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""id"": ""ae60bd175a61e820"",
    ""name"": ""client"",
    ""timestamp"": 1511858133224433,
    ""duration"": 508444,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-http-c"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""f193c7f4193f2879"",
    ""id"": ""54a3f6268df0aaee"",
    ""kind"": ""CONSUMER"",
    ""timestamp"": 1511858133600067,
    ""duration"": 5,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-kafka-consumer"",
      ""ipv4"": ""10.19.138.169""
    },
    ""tags"": {
      ""kafka.topic"": ""rdkafka""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""8ef78f0edefe3a4b"",
    ""name"": ""data enqueue"",
    ""timestamp"": 1511858133592958,
    ""duration"": 129,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""8ef78f0edefe3a4b"",
    ""name"": ""data enqueue"",
    ""timestamp"": 1511858133592958,
    ""duration"": 129,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    }
  }
]",0.0,0.027,0.973,0.8807
zipkin,It is easy to find that there are 18 spans.,0.0,0.244,0.756,0.4404
zipkin,"It seems that some of spans are merged in the web request, I want to know to where is the cource code deal this.",0.0,0.056,0.944,0.0772
zipkin,Thanks!,0.0,1.0,0.0,0.4926
zipkin,I'm using the camel-zipkin component to trace a request that flows between two different services:,0.0,0.0,1.0,0.0
zipkin,"service-a: Camel application running on Spring Boot, acting as a simple HTTP proxy (for the purposes of this proof of concept).",0.0,0.0,1.0,0.0
zipkin,Zipkin support provided by the camel-zipkin module.,0.0,0.31,0.69,0.4019
zipkin,Route:,0.0,0.0,1.0,0.0
zipkin,service-b: Spring Boot application with a REST controller.,0.0,0.0,1.0,0.0
zipkin,Zipkin support provided by the spring-cloud-starter-kubernetes-zipkin module from Spring Cloud.,0.0,0.231,0.769,0.4019
zipkin,"When I make a request to service-a, I see part of the trace in Zipkin: I see the client request from service-a, and I see the server request in service-b, as well as the spans I've added there to instrument various parts of the request-path.",0.0,0.051,0.949,0.2732
zipkin,"However, I don't see the server request from the Camel portion, including the additional two seconds caused by the delay I've put in the route.",0.091,0.0,0.909,-0.3182
zipkin,"Tracing the camel-zipkin code, I've realized that the server request will only be traced if there is already a trace ID header, due to this line:
 https://github.com/apache/camel/blob/c6c02ff92a536e78f7ed1b9dd550d6531e852cee/components/camel-zipkin/src/main/java/org/apache/camel/zipkin/ZipkinTracer.java#L753",0.0,0.0,1.0,0.0
zipkin,"With this knowledge, I am able to get the entire trace as expected if I manually provide my own tracing headers (X-B3-TraceId, X-B3-Sampled, and X-B3-SpanId).",0.0,0.0,1.0,0.0
zipkin,"However, I would like to be able to start a trace even if the client doesn't specify one.",0.0,0.143,0.857,0.3612
zipkin,"Based on my reading of the camel-zipkin code, I think I can create a PR that will induce my desired behavior.",0.0,0.208,0.792,0.4939
zipkin,"Before I do that, though, I want to verify a couple of things:",0.0,0.126,0.874,0.0772
zipkin,Thanks!,0.0,1.0,0.0,0.4926
zipkin,I have a client application with multiple channels as SOURCE/SINK.,0.0,0.0,1.0,0.0
zipkin,I want to send logs to Zipkin server.,0.0,0.178,0.822,0.0772
zipkin,"According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP.",0.0,0.0,1.0,0.0
zipkin,At client side:,0.0,0.0,1.0,0.0
zipkin,Q1.,0.0,0.0,1.0,0.0
zipkin,Is there an automatic configuration for zipkin rabbit binding in such scenario?,0.0,0.0,1.0,0.0
zipkin,"If not, what is default channel name of zipkin SOURCE channel?",0.0,0.0,1.0,0.0
zipkin,Q2.,0.0,0.0,1.0,0.0
zipkin,Do I need to configure defaultSampler to AlwaysSampler()?,0.0,0.0,1.0,0.0
zipkin,At Server side:,0.0,0.0,1.0,0.0
zipkin,Q1.,0.0,0.0,1.0,0.0
zipkin,"Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using:
 wget -O zipkin.jar 'https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec' 
...as stated on  https://zipkin.io/pages/quickstart.html  ?",0.0,0.081,0.919,0.3527
zipkin,Q2.,0.0,0.0,1.0,0.0
zipkin,How do I configure zipkin SINK channel to destination?,0.0,0.0,1.0,0.0
zipkin,"Spring boot version: 1.5.9.RELEASE
Spring cloud version: Edgware.RELEASE",0.0,0.0,1.0,0.0
zipkin,"Following the link  Istio/Distributed tracing , I can get the tracing working with zipkin.",0.0,0.0,1.0,0.0
zipkin,"Currently in order for the client/caller to know about the x-request-id (in case no id is sent, zipkin creates one), he 
needs to send it as a part of the request.",0.068,0.065,0.867,-0.0258
zipkin,This gives him the ability of trace the request.,0.0,0.223,0.777,0.3182
zipkin,All works well.,0.0,0.512,0.488,0.2732
zipkin,"However, I am thinking maybe it is not a good idea for the client to send the x-request-id to avoid issues of constraints/duplication.",0.195,0.0,0.805,-0.5583
zipkin,"It would be good if it is possible that at the istio level, one should be able to modify the response headers and send the x-request-id back.",0.0,0.1,0.9,0.4404
zipkin,I am not finding such capabilities for istio at present.,0.0,0.0,1.0,0.0
zipkin,"If there is a way to achieve this, please let me know.",0.0,0.208,0.792,0.3869
zipkin,When I use:,0.0,0.0,1.0,0.0
zipkin,The error log is:,0.474,0.0,0.526,-0.4019
zipkin,"But when I delete the dependency, it runs normally.",0.0,0.0,1.0,0.0
zipkin,"I can't find the reason, I don't know why it's going to be a problem.",0.197,0.0,0.803,-0.4019
zipkin,this is my fegin interface,0.0,0.0,1.0,0.0
zipkin,I'm using the feign interface  like this,0.0,0.294,0.706,0.3612
zipkin,this is my parent pom.xml config,0.0,0.0,1.0,0.0
zipkin,"I tried to modify the POM file, like:",0.0,0.294,0.706,0.3612
zipkin,still error.,0.73,0.0,0.27,-0.4019
zipkin,How can I solve this?,0.0,0.375,0.625,0.2023
zipkin,I can see spans being recorded in the Zipkin UI from the following Spring Boot controller code:,0.0,0.0,1.0,0.0
zipkin,and the log output looks like:,0.0,0.333,0.667,0.3612
zipkin,but the traces are independent in the UI.,0.0,0.0,1.0,0.0
zipkin,I would expect the two calls to Google and Facebook urls to be nested concurrently under the call to the  /concurrent1  endpoint.,0.0,0.0,1.0,0.0
zipkin,I suspect that it's due to the thread that coroutines are executed on being different to the one in which the spring application is started but I have no idea how to move forward with Spring Sleuth at this point!,0.115,0.0,0.885,-0.5707
zipkin,I am using Spring cloud Zipkin to trace calls with sample percentage 0.4.,0.0,0.0,1.0,0.0
zipkin,I am not using any persistent storage like MySQL or Cassandra.,0.0,0.217,0.783,0.3612
zipkin,Could you please let me know how to set data retention period in Zipkin server e.g.,0.0,0.133,0.867,0.3182
zipkin,I want to check only 6 hours/1 day data.,0.0,0.178,0.822,0.0772
zipkin,Or if I can set max span count,0.0,0.0,1.0,0.0
zipkin,everyone,0.0,0.0,1.0,0.0
zipkin,I am trying to use Zipkin to trace services in OpenStack.,0.0,0.0,1.0,0.0
zipkin,I know it is a huge project for me.,0.0,0.277,0.723,0.3182
zipkin,So I wonder if there is an open source library for Zipkin tracing OpenStack.,0.0,0.0,1.0,0.0
zipkin,"I think I searched it before and if my mind does not cheat me, there is one presentation (only slices) for this.",0.0,0.115,0.885,0.357
zipkin,"However, I can not find it.",0.0,0.0,1.0,0.0
zipkin,Can someone help with it?,0.0,0.403,0.597,0.4019
zipkin,"I know there is the library, osprofiler, for tracing OpenStack, while the example of API seems unclear to me.",0.105,0.0,0.895,-0.25
zipkin,"Could you please give me a more detailed or even a complete example, maybe like Zipkin  https://github.com/openzipkin/pyramid_zipkin-example",0.0,0.27,0.73,0.5859
zipkin,I do not mean it is not helpful.,0.28,0.0,0.72,-0.3252
zipkin,"It seems I still have to find the RESTful request point in OpenStack, for example creating an instance may trigger one service to request neutron for networking, and I may have to locate the front end code and add a tracing code.",0.0,0.113,0.887,0.5719
zipkin,"If using py_zipkin, I can add decorator @zipkin_span(some params) before it.",0.0,0.0,1.0,0.0
zipkin,"The problem is it is tough for me to find the front end of these services like Nova, neutron, cinder and so on.",0.157,0.094,0.749,-0.1779
zipkin,It seems osprofiler does the same thing.,0.0,0.0,1.0,0.0
zipkin,"My understanding is highly likely wrong, and I appreciate who can help with it.",0.18,0.288,0.533,0.2551
zipkin,"By the way, I do not intend to trace a big project like OpenStack.",0.0,0.185,0.815,0.3612
zipkin,I intend to trace a RESTful-like or RPC system with Zipkin to collect the information to analyze.,0.0,0.0,1.0,0.0
zipkin,"Unfortunately, I have found a middle-size open source project.",0.286,0.0,0.714,-0.34
zipkin,So I choose OpenStack.,0.0,0.0,1.0,0.0
zipkin,"If you could provide me something else, that will be very helpful.",0.0,0.219,0.781,0.4754
zipkin,:),0.0,1.0,0.0,0.4588
zipkin,Thank you very much.,0.0,0.455,0.545,0.3612
zipkin,I've been roaming the depths of the internet but I find myself unsatisfied by the examples I've found so far.,0.165,0.0,0.835,-0.5499
zipkin,"Can someone point me or, show me, a good starting point to integrate zipkin tracing with jaxrs clients and amqp clients?",0.0,0.132,0.868,0.4404
zipkin,My scenario is quite simple and I'd expect this task to be trivial tbh.,0.078,0.0,0.922,-0.0258
zipkin,We have a micro services based architecture and it's time we start tracing our requests and have global perspective of our inter service dependencies and what the requests actually look like (we do have metrics but I want more!),0.0,0.091,0.909,0.3595
zipkin,.,0.0,0.0,0.0,0.0
zipkin,The communication is done via jax-rs auto generated clients and we use rabbit template for messaging.,0.0,0.0,1.0,0.0
zipkin,I've seen brave integrations with jaxrs but they are a bit simplistic.,0.0,0.18,0.82,0.296
zipkin,"My zipkin server is a spring boot mini app using stream-rabbit, so zipkin data is sent using rabbitmq.",0.0,0.0,1.0,0.0
zipkin,Thanks in advance.,0.0,0.592,0.408,0.4404
zipkin,I am raising this query as a result of seeing another query with no satisfactory answer (and reading the advice to not ask another question in an answer or comment).,0.072,0.081,0.847,0.0772
zipkin,That reference is  Enabling Sleuth slows requests down (a lot),0.0,0.0,1.0,0.0
zipkin,My issue is similar.,0.0,0.0,1.0,0.0
zipkin,I am not using Feign.,0.0,0.0,1.0,0.0
zipkin,I am using the following:,0.0,0.0,1.0,0.0
zipkin,"I am using spring-cloud-sleuth, logback and zipkin.",0.0,0.0,1.0,0.0
zipkin,When i remove the zipkin pom reference,0.0,0.0,1.0,0.0
zipkin,the performance is very quick.,0.0,0.0,1.0,0.0
zipkin,"But when I put it back, the performance is very poor.",0.338,0.0,0.662,-0.6798
zipkin,Changing my log level from  INFO  to  DEBUG  in the logging changes the non zipkin calls from 701ms to 1051ms.,0.0,0.0,1.0,0.0
zipkin,Adding zipkin http changes that timing from 1051ms to around 53 seconds.,0.0,0.0,1.0,0.0
zipkin,"In the code, i make one call to my service which in turn makes 303 calls to an arango database (via its REST interface).",0.0,0.0,1.0,0.0
zipkin,I am using spring.cloud.starter-sleuth and logstash-logback-encoder.,0.0,0.0,1.0,0.0
zipkin,"By themselves there is no performance issue, it is only when i add the spring-cloud-starter-zipkin that the performance degrades.",0.249,0.0,0.751,-0.6486
zipkin,"I am running zipkin on my local pc in a docker instance, and running my service out of eclipse as a standalone app (spring boot).",0.0,0.0,1.0,0.0
zipkin,Arango is on my local pc running as a service.,0.0,0.0,1.0,0.0
zipkin,"Unit test methods of my controller (using mockito to mock the mvc calls) that usually take 0.3 to 0.5 seconds each without zipkin, end up taking ~16seconds each when zipkin  is  enabled.",0.083,0.0,0.917,-0.4215
zipkin,The Zipkin UI reports all the calls and the sum of the calls is the roughly the full time of the call as reported by Postman.,0.0,0.0,1.0,0.0
zipkin,The Application class looks like this at the top:,0.0,0.381,0.619,0.5106
zipkin,The end of the log file looks like the following without the zipkin reference:,0.0,0.161,0.839,0.3612
zipkin,And with the zipkin reference it looks like this:,0.0,0.238,0.762,0.3612
zipkin,some logs removed to fit in the 30k character limit,0.0,0.217,0.783,0.3612
zipkin,I have zipkin server running and when i hit the zipkin client rest api end point i am getting the error below,0.13,0.0,0.87,-0.4019
zipkin,"2017-06-10 23:16:08.782  INFO [product,d650504f4f922a51,d650504f4f922a51,true] 5676 --- [ix-ProductAPI-3] com.accenture.api.ProductAPI             : List all Product
Hibernate: select product0_.prod_id as prod_id1_0_, product0_.prod_description as prod_des2_0_ from product product0_
2017-06-10 23:16:09.801  WARN [product,,,] 5676 --- [ender@698c7814)] z.r.AsyncReporter$BoundedAsyncReporter   : Dropped 1 spans due to HttpClientErrorException(404 null)",0.057,0.0,0.943,-0.2808
zipkin,"org.springframework.web.client.HttpClientErrorException: 404 null
    at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:63) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:700) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:653) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:628) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:590) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.cloud.sleuth.zipkin.RestTemplateSender.post(RestTemplateSender.java:73) ~[spring-cloud-sleuth-zipkin-1.2.0.RELEASE.jar:1.2.0.RELEASE]
    at org.springframework.cloud.sleuth.zipkin.RestTemplateSender.sendSpans(RestTemplateSender.java:46) ~[spring-cloud-sleuth-zipkin-1.2.0.RELEASE.jar:1.2.0.RELEASE]
    at zipkin.reporter.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:228) [zipkin-reporter-0.6.12.jar:na]
    at zipkin.reporter.AsyncReporter$Builder.lambda$build$0(AsyncReporter.java:153) [zipkin-reporter-0.6.12.jar:na]
    at zipkin.reporter.AsyncReporter$Builder$$Lambda$1.run(Unknown Source) [zipkin-reporter-0.6.12.jar:na]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]",0.0,0.0,1.0,0.0
zipkin,"we are using elasticsearch as the storage of zipkin, the dependency are as follows:",0.0,0.0,1.0,0.0
zipkin,"now, trace log was storaged in elasticsearch, and we can see trance spans in the zipkin ui,but cannot to see span details or find trace via traceid,with the exception of :",0.0,0.0,1.0,0.0
zipkin,"I am trying to create a tracer, then a span from the tracer.",0.0,0.189,0.811,0.2732
zipkin,Performing work.,0.0,0.0,1.0,0.0
zipkin,Then closing the span.,0.0,0.0,1.0,0.0
zipkin,"I would expect that the close action will call spanReporter.report, which should post the data to an available Zipkin server (default localhost)",0.0,0.0,1.0,0.0
zipkin,"However, that is not happening.",0.0,0.0,1.0,0.0
zipkin,Code below.,0.0,0.0,1.0,0.0
zipkin,I noticed that I was using NeverSampler.,0.0,0.0,1.0,0.0
zipkin,Changed it to AlwaysSampler (for now).,0.0,0.0,1.0,0.0
zipkin,I was also using NoOpSpanReporter as the default SpanReporter (which does nothing).,0.0,0.0,1.0,0.0
zipkin,I want to change it to a ZipkinSpanReporter.,0.0,0.206,0.794,0.0772
zipkin,(Or something else).,0.0,0.0,1.0,0.0
zipkin,This is where I am stuck.,0.333,0.0,0.667,-0.25
zipkin,Questions:,0.0,0.0,1.0,0.0
zipkin,Now the code snippet which uses the above:,0.0,0.0,1.0,0.0
zipkin,"Note: 
This particular project does not have any http calls in any of its services.",0.0,0.0,1.0,0.0
zipkin,Its a data processing project.,0.0,0.0,1.0,0.0
zipkin,@SpringBootApplication has not been used.,0.0,0.0,1.0,0.0
zipkin,(All zipkin examples use it though).,0.0,0.0,1.0,0.0
zipkin,Is it necessary?,0.0,0.0,1.0,0.0
zipkin,Hi  Hoping somebody can help am trying to get a very basic implementation of zipkin working to get to grips with distributed tracing.,0.0,0.216,0.784,0.6705
zipkin,I am using the spring boot to do this but cannot seem to get it to work.,0.0,0.0,1.0,0.0
zipkin,Nothing appears in the zipkin UI when I try to find traces for a my service.,0.0,0.0,1.0,0.0
zipkin,I have got 2 deployments as follows:,0.0,0.0,1.0,0.0
zipkin,My spring boot app which I am wanting to log:,0.0,0.0,1.0,0.0
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,With these dependencies I do get a connect error because of the rabbit mq dependency.,0.184,0.0,0.816,-0.4019
zipkin,I had to include this becuase I got a META-INF/spring binders error.,0.252,0.0,0.748,-0.4019
zipkin,Wasnt really sure how to get around this other than putting the dependency in.,0.144,0.0,0.856,-0.2912
zipkin,My application.class,0.0,0.0,1.0,0.0
zipkin,When I run this aplication and call my endpoint I can see through the logging that it should be sending it to zipkin.,0.0,0.0,1.0,0.0
zipkin,This are my full logs.,0.0,0.0,1.0,0.0
zipkin,Again I get the rabbitmq exception but not sure why i actually need this.,0.182,0.0,0.818,-0.3491
zipkin,Can zipkin not work without it,0.0,0.0,1.0,0.0
zipkin,My full logs:,0.0,0.0,1.0,0.0
zipkin,The 2nd application I deployed is my zipkin client / UI,0.0,0.0,1.0,0.0
zipkin,Pom .xml,0.0,0.0,1.0,0.0
zipkin,My application.class,0.0,0.0,1.0,0.0
zipkin,application.properties,0.0,0.0,1.0,0.0
zipkin,Running the zipkin server it starts up ok but nothing is shown in the trace logs excpet for an error,0.153,0.069,0.778,-0.4497
zipkin,When I go to the client I do get this error though,0.258,0.0,0.742,-0.481
zipkin,Any help is appreciated here,0.0,0.667,0.333,0.7184
zipkin,Thanks in advance,0.0,0.592,0.408,0.4404
zipkin,I am using the Kubernetes to deploy and trace data from application using zipkin.,0.0,0.0,1.0,0.0
zipkin,I am facing issue in replacing MySQL with Elasticsearch since I am not able to get the idea.,0.0,0.0,1.0,0.0
zipkin,"Even the replacement is done on command line basis, using STORAGE_TYPE=""Elasticsearch"" but how that can be done through kubernetes?",0.0,0.0,1.0,0.0
zipkin,I am able to run the container from docker imgaes but is there any way to replace through deployment?,0.0,0.0,1.0,0.0
zipkin,I have several services.,0.0,0.0,1.0,0.0
zipkin,I am instrumenting them using Zipkin.,0.0,0.0,1.0,0.0
zipkin,"In each module, in build.gradle is added a dependency to Zipkin:",0.0,0.0,1.0,0.0
zipkin,"In each module, in application.properties file are following settings:",0.0,0.0,1.0,0.0
zipkin,"I call a specific endpoint that use other 3 modules, in total are 4 modules.",0.0,0.0,1.0,0.0
zipkin,Entire setup is on my laptop.,0.0,0.0,1.0,0.0
zipkin,I realized that Zipkin introduces a lot of overhead.,0.0,0.0,1.0,0.0
zipkin,I used Mozilla to compare the results.,0.0,0.0,1.0,0.0
zipkin,The small values are when Zipkin does not record the requests and the big value is when Zipkin records.,0.0,0.231,0.769,0.6249
zipkin,"
Do you have any idea why there are so much overhead?",0.0,0.0,1.0,0.0
zipkin,Thank you.,0.0,0.714,0.286,0.3612
zipkin,We're looking to implement Zipkin in our stack.,0.0,0.0,1.0,0.0
zipkin,As I look into Zipkin it makes sense to me to extend the Zipkin system to handle generic flags as well.,0.0,0.174,0.826,0.4215
zipkin,Observations:,0.0,0.0,1.0,0.0
zipkin,Conclusion:,0.0,0.0,1.0,0.0
zipkin,I have a question regarding Zipkin with elasticsearch storage.,0.0,0.0,1.0,0.0
zipkin,After updating spring-cloud-sleuth to  1.1.1.RELEASE (because we updated spring boot from 1.3.8 to 1.4.4 and spring cloud from Brixton.SR6 to Camden.SR4) we also updated  zipkin-storage-elasticsearch  and  zipkin-autoconfigure-storage-elasticsearch  to version  1.16.2  in the pom.xml of our zipkin service.,0.0,0.0,1.0,0.0
zipkin,We are using elasticsearch version  2.4.1 .,0.0,0.0,1.0,0.0
zipkin,"When we boot up the service we get an 
 error on zipkin's ui  and a stacktrace:",0.162,0.0,0.838,-0.4019
zipkin,"Zipkin was working with spring-cloud-sleuth  1.0.0.RELEASE , zipkin-storage-elasticsearch, zipkin-autoconfigure-storage-elasticsearch  1.7.0  and elasticsearch  2.3.5 .",0.0,0.0,1.0,0.0
zipkin,What am I missing here?,0.423,0.0,0.577,-0.296
zipkin,Which versions should work together?,0.0,0.0,1.0,0.0
zipkin,Can be Zipkin used to instrument a console/classic application?,0.0,0.0,1.0,0.0
zipkin,I mean I have a method foo() and I want to know how much time it took.,0.0,0.098,0.902,0.0772
zipkin,Does Zipkin can be used just for applications that communicates over http protocol?,0.0,0.0,1.0,0.0
zipkin,Thanks,0.0,1.0,0.0,0.4404
zipkin,I am facing an issue where in the ZipKin UI is failing to load in the traces from MySQL.,0.163,0.0,0.837,-0.5106
zipkin,It is giving me below mentioned error on UI -,0.223,0.198,0.579,-0.0772
zipkin,Error executing query: SQL [select distinct  zipkin_spans .,0.31,0.0,0.69,-0.4019
zipkin,"trace_id 
  from  zipkin_spans  join  zipkin_annotations  on
  ( zipkin_spans .",0.0,0.268,0.732,0.296
zipkin,trace_id  =  zipkin_annotations .,0.0,0.0,1.0,0.0
zipkin,"trace_id  and
   zipkin_spans .",0.0,0.0,1.0,0.0
zipkin,id  =  zipkin_annotations .,0.0,0.0,1.0,0.0
zipkin,"span_id ) where
  ( zipkin_spans .",0.0,0.0,1.0,0.0
zipkin,start_ts  between ?,0.0,0.0,1.0,0.0
zipkin,and ?,0.0,0.0,1.0,0.0
zipkin,"and
   zipkin_annotations .",0.0,0.0,1.0,0.0
zipkin,endpoint_service_name  = ?),0.0,0.0,1.0,0.0
zipkin,"order by
   zipkin_spans .",0.0,0.0,1.0,0.0
zipkin,start_ts  desc limit ?,0.0,0.0,1.0,0.0
zipkin,"]; Expression #1 of ORDER BY
  clause is not in SELECT list, references column
  'zipkin.zipkin_spans.start_ts' which is not in SELECT list; this is
  incompatible with DISTINCT",0.0,0.0,1.0,0.0
zipkin,I see below exception on ZipKin Server -,0.0,0.0,1.0,0.0
zipkin,My ZipKin Server Configuration is a below -,0.0,0.0,1.0,0.0
zipkin,"When I query the MySQL schema, I can see the record being populated in ""zipkin.zipkin_spans and zipkin.zipkin_annotations"" table.",0.0,0.0,1.0,0.0
zipkin,"But when I try to load the Zipkin UI, it give me above error on UI.",0.202,0.0,0.798,-0.5499
zipkin,Any help is highly appreciated.,0.0,0.677,0.323,0.7425
zipkin,I have created a spring-boot application which publishes zipkin logs to a zipkin consumer.,0.0,0.167,0.833,0.25
zipkin,But the Zipkin consumer(another spring boot application) is behind some authentication filters which check for several parameters/headers in the request before allowing.,0.0,0.0,1.0,0.0
zipkin,How to I use my own custom HttpClient to publish my messages from the producer in this case?,0.0,0.0,1.0,0.0
zipkin,"I would like to create my own basic, minimalistic library used for distributed tracing with Zipkin.",0.0,0.261,0.739,0.5574
zipkin,I will be sending traces via HTTP and nothing more fancy.,0.0,0.0,1.0,0.0
zipkin,My question is if there is any more information about this topic than in the Zipkin docs and the source code of Zipkin and Brave?,0.0,0.124,0.876,0.5267
zipkin,I would like not to rely on Spring Framework.,0.0,0.263,0.737,0.3612
zipkin,"Spring Cloud Sleuth works very well, but my services are not built using Spring.",0.0,0.115,0.885,0.177
zipkin,Do you know any resources?,0.0,0.0,1.0,0.0
zipkin,Or do you have any ideas where to start doing this?,0.0,0.0,1.0,0.0
zipkin,We are using finagle stack and taught of adding zipkin for tracing our micro-services.,0.0,0.0,1.0,0.0
zipkin,I am able to see our tracing happening but parent finishes before the child.,0.0,0.0,1.0,0.0
zipkin,"I have already opened an issue here: 
 https://github.com/openzipkin/docker-zipkin/issues/100",0.0,0.0,1.0,0.0
zipkin,Any help would be really appreciated.,0.0,0.611,0.389,0.7425
zipkin,"I do have several services interacting with each other, and all of them, sending traces to openzipkin (  https://github.com/openzipkin/docker-zipkin  ).",0.0,0.0,1.0,0.0
zipkin,"While i can see the system behaviour in detail , looks like the 'dependencies' tab does not display anything at all.",0.0,0.122,0.878,0.3612
zipkin,"The trace i check has 6 services, 21 spans and 43 spans, and i believe something should appear.",0.0,0.0,1.0,0.0
zipkin,"I'm using latest ( 1.40.1 ) docker-zipkin, with cassandra as storage, and 
just connecting to the cassandra instance, can see there's no entry in the dependencies 'table'.",0.084,0.0,0.916,-0.296
zipkin,why ?,0.0,0.0,1.0,0.0
zipkin,Thanks,0.0,1.0,0.0,0.4404
zipkin,"I am trying to install  zipkin , after following the steps given ( https://github.com/twitter/zipkin/blob/master/doc/install.md ), when I access  http://localhost:8080/  on the web browser, instead of the zipkin UI, it gives,",0.0,0.0,1.0,0.0
zipkin,and gives out an error on the query terminal.,0.252,0.0,0.748,-0.4019
zipkin,Can please anyone help me how to resolve this ?,0.0,0.559,0.441,0.765
zipkin,?,0.0,0.0,0.0,0.0
zipkin,I am working in the server side of an application and I am getting the tracer id in the form of X-b3 headers from the client side in the request object.,0.0,0.0,1.0,0.0
zipkin,Now how can I create a child span for that tracer using these x-b3 headers?,0.0,0.149,0.851,0.2732
zipkin,"I have some tools running in my kubernetes cluster (ELK, zipkin,..) and i want to know in which namespace to place them, for example i have fluentd which is a daemonset running in kube-system namespace so should i place elasticsearch in the same namespace or put them together in a custom namespace so they can reach each other, i just want to know what is the best practice to do it",0.0,0.12,0.88,0.7322
zipkin,"Im trying to implement distributed tracing using Spring, RabbitMQ, Sleuth and Zipkin.",0.0,0.0,1.0,0.0
zipkin,So I added the dependencies:,0.0,0.0,1.0,0.0
zipkin,And configured  sleuth  and  zipkin  in my  bootstrap.yml :,0.0,0.0,1.0,0.0
zipkin,So starting my services and making some rest calls I get this in the logs:,0.0,0.0,1.0,0.0
zipkin,For now it looks good.,0.0,0.42,0.58,0.4404
zipkin,Sleuth added the tracing ID's.,0.0,0.0,1.0,0.0
zipkin,Calling the Zipkin UI I can see that the service names where added:,0.0,0.0,1.0,0.0
zipkin,But there are no tracing informations at all:,0.286,0.0,0.714,-0.4215
zipkin,So im wondering what im missing in my configuration.,0.216,0.0,0.784,-0.296
zipkin,EDIT,0.0,0.0,1.0,0.0
zipkin,Turned out there are tracing informations arriving in zipkin.,0.0,0.0,1.0,0.0
zipkin,I can use the search bar in the top right corner to search for tracing id's directly:,0.0,0.107,0.893,0.2023
zipkin,I will then get:,0.0,0.0,1.0,0.0
zipkin,So the question is why is there nothing in the overview or queryable via the trace lookup?,0.0,0.0,1.0,0.0
zipkin,We are using Istio/Zipkin as a tracing system on our server to add the dynamic headers through Istio sidecar proxy for tracing the request later on using Zipkin.,0.0,0.091,0.909,0.3818
zipkin,Is there anyway we could disable istio for a certain request.,0.0,0.189,0.811,0.2732
zipkin,"Problem is, we are working with JMS queues, and when JMS listener tries to listen to a certain queue, it sees the headers like x-request-id added by the istio dynamically and it gives an error (as it'll accept header keys only in camelCase or with underScore or $).",0.096,0.173,0.731,0.5106
zipkin,"We can change header keys added by istio, so we want either istio to not to add headers in some specific requests (the one queue is making).",0.0,0.057,0.943,0.1477
zipkin,I've searched google but couldn't find anything about it.,0.0,0.0,1.0,0.0
zipkin,Following is the error message we are getting:,0.278,0.0,0.722,-0.4019
zipkin,I am using spring Boot Version 1.5.14.RELEASE with spring cloud sleuth zipkin.,0.0,0.0,1.0,0.0
zipkin,If I return a ResponseEntity setting its HttpStatus as BAD_REQUEST then I see the trace highlighted in Blue color.,0.0,0.0,1.0,0.0
zipkin,Is there a way to highlight the trace in Red color for a bad request with ResponseEntity object?,0.176,0.121,0.704,-0.2732
zipkin,I explicitly threw a custom Exception for bad requests and saw the zipkin trace highlighted in Red color in Zipkin UI.,0.163,0.0,0.837,-0.5423
zipkin,But I don't want to do this as I am returning a body in ResponseEntity.,0.108,0.0,0.892,-0.0857
zipkin,I expect the Zipkin trace to be highlighted in Red color as it is a bad request but the actual color is Blue.,0.101,0.0,0.899,-0.3071
zipkin,Actual Trace,0.0,0.0,1.0,0.0
zipkin,Expected Trace,0.0,0.0,1.0,0.0
zipkin,I'm using spring-cloud's sleuth with zipkin with kafka.,0.0,0.0,1.0,0.0
zipkin,here is my pom.xml configure.,0.0,0.0,1.0,0.0
zipkin,"and ,the blew is my application.yaml:",0.0,0.0,1.0,0.0
zipkin,"and then, I start the spring boot application,try to access by browsers,but I can't find the spring cloud send sleuth  to kafka.",0.0,0.0,1.0,0.0
zipkin,"So I try to debug the zipkin2.reporter.AsyncReporter, and find that every time the [flush] method return when go to line 265 which is ""if (!bundler.isReady() &amp;&amp; !closed.get()) return;""  the code show as below.",0.0,0.0,1.0,0.0
zipkin,I know it means bufferFull is false.but why every time it is not bufferFull even if I try invoke the URL frequently?,0.0,0.0,1.0,0.0
zipkin,thank you for attention.,0.0,0.455,0.545,0.3612
zipkin,i know this is a very general question but i simply don't know how to start.,0.0,0.0,1.0,0.0
zipkin,I have spring-boot applications which serve a thrift API via HTTP.,0.0,0.0,1.0,0.0
zipkin,The same or another spring-boot app is using the thrift-client of another application to communicate.,0.0,0.0,1.0,0.0
zipkin,my goal is to trace the communication path with zipkin.,0.0,0.0,1.0,0.0
zipkin,"i could imagine, i need to somehow intercept incoming and outcoming http-calls with the application-type x-thrift but simply have no idea how to do this and properly integrate with zipkin libraries.",0.091,0.0,0.909,-0.4215
zipkin,"any hint how to start on this is highly appreciated, thanks a lot in advance",0.0,0.361,0.639,0.7764
zipkin,"I am new in spring-cloud.My projects are about spring-cloud-config, spring-cloud-eureka, spring-cloud-zipkin,I am running projects locally and it is normal.When I put my projects in Ubuntu, the project 'spring-cloud-zipkin' runs incorrectly.",0.0,0.0,1.0,0.0
zipkin,"The error is about 'Exception in thread ""main"" java.lang.ClassNotFoundException', thanks for your answer very much.",0.145,0.156,0.699,0.0516
zipkin,enter image description here,0.0,0.0,1.0,0.0
zipkin,and my code:,0.0,0.0,1.0,0.0
zipkin,enter image description here,0.0,0.0,1.0,0.0
zipkin,enter image description here,0.0,0.0,1.0,0.0
zipkin,"in the pom.xml:
 enter image description here",0.0,0.0,1.0,0.0
zipkin,I need to use Zipkn Serve to trace my spring boot application.Here is my configurations of application.yml,0.0,0.0,1.0,0.0
zipkin,But the spans not being created in Zipkin.I have added all the required dependencies to my service's pom file.,0.105,0.0,0.895,-0.2755
zipkin,and the zipkin service's pom file.,0.0,0.0,1.0,0.0
zipkin,I am using Zipkin Slueth with Spring boot.,0.0,0.0,1.0,0.0
zipkin,"Now my zipkin is working fine in normal case but when I spawn 3 new threads from main thread, it generate different traces and not 1 trace.",0.0,0.057,0.943,0.1027
zipkin,So i am unable to see the complete request.,0.0,0.0,1.0,0.0
zipkin,Same starts working if I do everything in main thread?,0.0,0.0,1.0,0.0
zipkin,My Pom for including dpendencies,0.0,0.0,1.0,0.0
zipkin,Properties,0.0,0.0,1.0,0.0
zipkin,My spring cloud version is Dalton.SR5,0.0,0.0,1.0,0.0
zipkin,So slueth sends traces to zipkin auto.,0.0,0.0,1.0,0.0
zipkin,This is all I have configured for zipkin.,0.0,0.0,1.0,0.0
zipkin,Can i use rxjava schedules hook?,0.0,0.0,1.0,0.0
zipkin,How?,0.0,0.0,1.0,0.0
zipkin,I am unable to use it?,0.0,0.0,1.0,0.0
zipkin,"SpringCloud version:Dalston.SR1,
rabbitMQ version:3.6.10,ElasticSearch version:6.2.4",0.0,0.0,1.0,0.0
zipkin,There was nothing unusual when I use MySQL as a storage.,0.0,0.0,1.0,0.0
zipkin,Now I use ElasticSearch.I can't find any services.,0.0,0.0,1.0,0.0
zipkin,I lost something?,0.697,0.0,0.303,-0.3182
zipkin,here is the picture:,0.0,0.0,1.0,0.0
zipkin,application.properties,0.0,0.0,1.0,0.0
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,zipkin is a tool for tracing request as well as tracking the span of time a service took to process the request useful in multi-service projects it doesnt require much effort for setting up u just have to add zipkin dependency in your services and define a sampler bean.,0.0,0.104,0.896,0.6124
zipkin,add the following dependency in project,0.0,0.0,1.0,0.0
zipkin,"compile group: 'org.springframework.cloud', name: 'spring-cloud-starter-zipkin', version: '1.3.2.RELEASE'",0.0,0.0,1.0,0.0
zipkin,add sampler bean inside ur project,0.0,0.0,1.0,0.0
zipkin,`,0.0,0.0,0.0,0.0
zipkin,add above bean when u want only fraction of ur requests traces to send to zipkin else define a bean,0.0,0.071,0.929,0.0772
zipkin,"add 
 spring.zipkin.base-url=localhost:9411  in ur properties file and host the zipkin server on the same port defined above.",0.0,0.0,1.0,0.0
zipkin,but if u r using api-gateway for accessing zipkin (in case of deplyment in cloud ) or inside proxy u may face the issue of broken ui elements when accessing thru gateway in this case im  using zuul with propertis as:,0.103,0.0,0.897,-0.631
zipkin,"zuul.routes.zipkin.path=/zipkin/*
zuul.routes.zipkin.url=http://localhost:9411",0.0,0.0,1.0,0.0
zipkin,First I want to integrate  zipkin  +  rabbitmq  into my project.,0.0,0.14,0.86,0.0772
zipkin,So my  pom.xml  is below:,0.0,0.0,1.0,0.0
zipkin,So after I add this.,0.0,0.0,1.0,0.0
zipkin,I can't not invoke my controller.,0.0,0.0,1.0,0.0
zipkin,"But if the controller
in the same package with the Application, can the controller be invoked?",0.0,0.0,1.0,0.0
zipkin,"I'm following  this  guide, with Zipkin.",0.0,0.0,1.0,0.0
zipkin,"I have 3 microservices involed,  A -&gt; B -&gt; C , I'm propagating headers from A to B and from B to C.
But in the Zipkin dashboard I only see entries for  A -&gt; B  and  B -&gt; C , not  A -&gt; B -&gt; C .",0.0,0.0,1.0,0.0
zipkin,Those are the headers:,0.0,0.0,1.0,0.0
zipkin,"I can see that in B  x-b3-parentspanid  is null and I guess that's wrong, but the other are working I think...how is it possible?",0.097,0.0,0.903,-0.2617
zipkin,"EDIT:
added code snippets to show headers propagation",0.0,0.0,1.0,0.0
zipkin,A -&gt; B  propagation:,0.0,0.0,1.0,0.0
zipkin,...,0.0,0.0,1.0,0.0
zipkin,B -&gt; C  propagation:,0.0,0.0,1.0,0.0
zipkin,...,0.0,0.0,1.0,0.0
zipkin,I'm working with JMS and queues (Azure queues) for the first time.,0.0,0.0,1.0,0.0
zipkin,I'm required to make a queue where Rubi server would write some data and Java would read it from queue and will do further executions.,0.0,0.0,1.0,0.0
zipkin,This process is working fine locally on my machine.,0.0,0.184,0.816,0.2023
zipkin,"I've created a REST endpoint which is writing data in the queue and once data is written in the queue, the listener would take over and read the data and execute.",0.0,0.065,0.935,0.25
zipkin,When we deploy it to Azure the error I can see in logs which is not letting the Queues start is,0.124,0.0,0.876,-0.4019
zipkin,Zipkin is also present on the Azure server as a distributed tracing system and I guess this  x-request-id  is related to Zipkin which is creating the problem.,0.097,0.079,0.824,-0.128
zipkin,I've searched Google for the issue but couldn't understand why its happening.,0.0,0.0,1.0,0.0
zipkin,Following is detailed error message:,0.403,0.0,0.597,-0.4019
zipkin,"Spring Boot Cloud Disovery Question,  Problem with Eureka hostname after docker upgrade on windows 10.",0.162,0.0,0.838,-0.4019
zipkin,"(Note: docker is not hosting spring services, just mariadb, rabbitmq, and zipkin)",0.0,0.0,1.0,0.0
zipkin,Summary,0.0,0.0,1.0,0.0
zipkin,"Everything worked fine until the docker update today, after the docker upgrade",0.0,0.141,0.859,0.2023
zipkin,"Eureka returns  ""host.docker.internal""    as the hostname for my development box (machine hosting the spring boot cloud services)",0.0,0.0,1.0,0.0
zipkin,This has worked fine until the docker updgrade on windows 10 today.,0.0,0.141,0.859,0.2023
zipkin,Any guidance on this one?,0.0,0.0,1.0,0.0
zipkin,------------------------------ Details ----------------------------,0.0,0.0,1.0,0.0
zipkin,"""",0.0,0.0,0.0,0.0
zipkin,---------------- Versions of spring ----------,0.0,0.0,1.0,0.0
zipkin,I am using windows 10 enterprise for java development.,0.0,0.0,1.0,0.0
zipkin,"I use docker-compose to host mariadb, zipkin, and rabbitmq in my dev env on my windows 10 box",0.0,0.0,1.0,0.0
zipkin,I have a multi-project gradle build with 8 spring boot cloud services,0.0,0.0,1.0,0.0
zipkin,One of the services is a spring cloud discovery service hosting Eureke,0.0,0.0,1.0,0.0
zipkin,The other spring cloud services are eureka clients.,0.0,0.0,1.0,0.0
zipkin,"Until today, everything worked
1) Eureka spring boot cloud services are started first
2) Other spring boot cloud services that are clients of the eclipse startup, register and query the spring cloud discovery client code to obtain the URL of the other services",0.0,0.0,1.0,0.0
zipkin,"Today,  The latest docker for windows 10 was pushed out, and I installed it (I have been developing this app through several other docker updates).",0.0,0.0,1.0,0.0
zipkin,"I updated docker, did a reboot.",0.0,0.0,1.0,0.0
zipkin,"After the reboot,  The Eureka server is returning  ""host.docker.internal""  as the hostname in the URL  instead of  http:/mymachinename:8080",0.0,0.0,1.0,0.0
zipkin,"I don't have the network data before the upgrade, but now it is",0.0,0.0,1.0,0.0
zipkin,My client application.properties file is:,0.0,0.0,1.0,0.0
zipkin,Server application.property file,0.0,0.0,1.0,0.0
zipkin,We are trying to implement Spring cloud sleuth with Zipkin in our project and wanted to know if Spring cloud sleuth will support DB calls with Spring data JPA.,0.0,0.088,0.912,0.4019
zipkin,I want to trace the time taken for DB calls just like service calls,0.0,0.257,0.743,0.4215
zipkin,"When I make a service call with RestTemplate, that gets sent to zipkin and I am able to see that on the dashboard",0.0,0.0,1.0,0.0
zipkin,But DB interactions with Spring data jpa is not getting displayed in Zipkin,0.0,0.0,1.0,0.0
zipkin,"While Zipkin sdk is available for Node.js, I'm looking for auto-instrumentation like Spring Cloud Sleuth in Node.js app.",0.0,0.128,0.872,0.3612
zipkin,Is there a module or framework for it in Node.js?,0.0,0.0,1.0,0.0
zipkin,What I mean by auto-instrumentation above is that in Java I don't have to write code to instrument servlets/filters/rest clients with Zipkin.,0.0,0.0,1.0,0.0
zipkin,Sleuth automatically does that.,0.0,0.0,1.0,0.0
zipkin,While Zipkin instrumentation seems manual in Node.js.,0.0,0.0,1.0,0.0
zipkin,i am using python flask in my application.,0.0,0.0,1.0,0.0
zipkin,I want to change the header before each request in order to add information for zipkin distributed tracing.,0.0,0.075,0.925,0.0772
zipkin,My current code looks like:,0.0,0.385,0.615,0.3612
zipkin,"Unfortunately, this is not working as the method  append  does not exist.",0.179,0.0,0.821,-0.34
zipkin,What is the right approach?,0.0,0.0,1.0,0.0
zipkin,"Best regards
Martin",0.0,0.677,0.323,0.6369
zipkin,I have a Spring Boot 2.0.0 REST service where I'm trying to enable Sleuth and Zipkin to send traces to my localhost Zipkin server.,0.0,0.0,1.0,0.0
zipkin,The app worked fine unti I add the two dependencies  spring-cloud-starter-sleuth  and  spring-cloud-sleuth-zipkin  to my pom.xml.,0.0,0.114,0.886,0.2023
zipkin,"Once I did that, I'm now getting a compilation error:",0.278,0.0,0.722,-0.4019
zipkin,Project build error: Non-resolvable import POM: Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT,0.213,0.0,0.787,-0.4019
zipkin,I've ensured it's not a corrupt Maven package issue by deleting my .m2 folder and updating (twice).,0.0,0.0,1.0,0.0
zipkin,Why am I getting this error and how can I fix it?,0.258,0.0,0.742,-0.481
zipkin,This is my pom.xml:,0.0,0.0,1.0,0.0
zipkin,"I have developed several micro services using spring cloud Netflix stack (Eureka, Zuul, Zipkin, config server etc.)",0.0,0.0,1.0,0.0
zipkin,Is there any open source / free solution available to spin up/down microservices instances.,0.0,0.337,0.663,0.6808
zipkin,e.g.,0.0,0.0,1.0,0.0
zipkin,if CPU usage   90% for 3 consecutive checking it will add one more instance.,0.0,0.0,1.0,0.0
zipkin,I try to add a distributed tracing in my microservices (under Kubernetes in Azure).,0.0,0.0,1.0,0.0
zipkin,I added the dependencies in the parent pom.xml :,0.0,0.0,1.0,0.0
zipkin,I use 1.4.1 and CAMDEN.SR4 because fabric8 kubeflix doesn't support newer versions.,0.184,0.0,0.816,-0.3089
zipkin,I forced 1.1.3.RELEASE to try newest sleuth version to see if it was a bug in older version of sleuth.,0.15,0.0,0.85,-0.4588
zipkin,I use this configuration of logback-spring.xml :,0.0,0.0,1.0,0.0
zipkin,And here is my application.yml :,0.0,0.0,1.0,0.0
zipkin,The zipkin URL is a Kubernetes services exposing the Zipkin server (Spring boot app with @EnableZipkinServer),0.13,0.0,0.87,-0.2732
zipkin,I then call a first service (services-1) with this code :,0.0,0.0,1.0,0.0
zipkin,which produces these logs :,0.0,0.0,1.0,0.0
zipkin,"As you can see it calls the services-i18n-2 service with a RestTemplate, which produces these logs :",0.0,0.0,1.0,0.0
zipkin,As you can see the traceId in service-2 (e0c6495a0a598cff) is different from the service-1 (eaf3dbcb2f92091b).,0.0,0.0,1.0,0.0
zipkin,And in service-2 the traceId is the same as the spanId.,0.0,0.0,1.0,0.0
zipkin,Questions :,0.0,0.0,1.0,0.0
zipkin,"FYI, I have Hystrix in the dependencies and I have removed the @HystrixCommand to be sure it was not a problem with Hystrix creating a new traceId at each HTTP call.",0.0,0.288,0.712,0.8069
zipkin,I have implemented a Distributed Transaction Logging library with Tree like Structure as mention in Google Dapper( http://research.google.com/pubs/pub36356.html ) and eBay CAL Transaction Logging Framework( http://devopsdotcom.files.wordpress.com/2012/11/screen-shot-2012-11-11-at-10-06-39-am.png ).,0.0,0.098,0.902,0.3612
zipkin,Log Format,0.0,0.0,1.0,0.0
zipkin,GUID HEX NUMBER FORMAT,0.0,0.302,0.698,0.0772
zipkin,What I would like to do is to integrate this format with Kibana UI and when user want to search and click on on TRACE_GUID it will show something similar to Distributed CALL graph which show where the time was spent.,0.0,0.091,0.909,0.4215
zipkin,Here is UI  http://twitter.github.io/zipkin/ .,0.0,0.0,1.0,0.0
zipkin,This will be great.,0.0,0.577,0.423,0.6249
zipkin,I am not UI developer if some can point me how to do this that will be great.,0.0,0.204,0.796,0.6249
zipkin,"Also I would like to know how I can index elastic search payload data so user specify some expression like in payload (duration   1000) then, Elastic Search will bring all the loglines that satisfy condition.",0.0,0.211,0.789,0.7906
zipkin,"Also, I would like to index Payload as Name=Value pair so user can query  (key3=value2 or key4 =  exception ) some sort of regular expression.",0.0,0.106,0.894,0.3612
zipkin,Please let me know if this can be achieved.,0.0,0.223,0.777,0.3182
zipkin,Any help pointer would be great..,0.0,0.351,0.649,0.4019
zipkin,"Thanks,
Bhavesh",0.0,0.744,0.256,0.4404
zipkin,I'm going to design distributed system with Scala and Akka.,0.0,0.0,1.0,0.0
zipkin,I want to aggregate tracing messages from a cluster and have possibility to view them in some kind of UI.,0.0,0.071,0.929,0.0772
zipkin,"Is Zipkin the best solution, or Flume(+some wrapper?",0.0,0.52,0.48,0.7579
zipkin,"), or something else?",0.0,0.0,1.0,0.0
zipkin,We are setting up microservice framework.,0.0,0.0,1.0,0.0
zipkin,We use following stack for distributed tracing.,0.0,0.0,1.0,0.0
zipkin,Following is how the configuration is done,0.0,0.0,1.0,0.0
zipkin,In  gradle.build  (or pom.xml) following starter dependencies added,0.0,0.0,1.0,0.0
zipkin,Add one AlwaysSampler bean,0.0,0.0,1.0,0.0
zipkin,"If we have  kafka  running, things work automatically.",0.0,0.0,1.0,0.0
zipkin,"But if kafka is not running, server does not start - this is mostly the case for development environment.",0.0,0.0,1.0,0.0
zipkin,"If I want to stop this, I have to comment out all the code mentioned here (as we use starter dependency with spring boot, it automatically configures as I understand).",0.077,0.046,0.877,-0.2263
zipkin,Can we just make some changes in properties (or yaml) files so that I don't need to go and comment out all these code?,0.0,0.0,1.0,0.0
zipkin,"Or probably another way to disable this without doing some commenting, etc.",0.0,0.0,1.0,0.0
zipkin,I am using spring cloud slueth with zipkin in spring boot to trace the services calls.,0.0,0.0,1.0,0.0
zipkin,My spring cloud version is Edgware.RELEASE,0.0,0.0,1.0,0.0
zipkin,"Now I when I tried to trace my facade layer which uses rxjava, it creates 12 traces for a single request?",0.0,0.11,0.89,0.2732
zipkin,What should I do ?,0.0,0.0,1.0,0.0
zipkin,I mean i want just 1 trace should be generated for 1 request of facade layer(facade layer do parallel calls using rxjava).,0.0,0.071,0.929,0.0772
zipkin,I am using slueth with http and have not change any property if properties file,0.0,0.0,1.0,0.0
zipkin,I have added these 2 dependencies:,0.0,0.0,1.0,0.0
zipkin,This I have added in properties file:,0.0,0.0,1.0,0.0
zipkin,i am trying to evaluate zipkin to enable distributed tracing capability for all our micro-service.,0.0,0.0,1.0,0.0
zipkin,Below are versions in my setup.,0.0,0.0,1.0,0.0
zipkin,Spring-boot version:  1.5.7.RELEASE,0.0,0.0,1.0,0.0
zipkin,"spring-cloud version:
 Camden.SR6",0.0,0.0,1.0,0.0
zipkin,zipkin version :  2.2.1,0.0,0.0,1.0,0.0
zipkin,Configuration for seluth in  application.properties,0.0,0.0,1.0,0.0
zipkin,spring.sleuth.sampler.percentage=1.0,0.0,0.0,1.0,0.0
zipkin,spring.sleuth.web.skipPattern=(^cleanup.,0.0,0.0,1.0,0.0
zipkin,|.+favicon. ),0.0,0.0,1.0,0.0
zipkin,And i created the ZipkinSpanReporter bean as below.,0.0,0.25,0.75,0.25
zipkin,Note that I have setup the Eureka server as all micro services and even zipkin server registerred with Eureka server so that the Zipkin client can resolve zipkin server via eureka,0.0,0.082,0.918,0.3818
zipkin,What I have observered is that the zipkin client (book) is not reporting all spans back to zipkin server when I checked the zipkin.,0.0,0.0,1.0,0.0
zipkin,"Some are reported, almost of spans are dropped",0.0,0.0,1.0,0.0
zipkin,I have enabled the logging for,0.0,0.0,1.0,0.0
zipkin,below are logging info:,0.0,0.0,1.0,0.0
zipkin,But I could not be able to find the traceId which is logged in  book.log  file from zipkin console,0.0,0.0,1.0,0.0
zipkin,Could you please explain why many spans are not reported to zipkin server?,0.0,0.161,0.839,0.3182
zipkin,Thanks in advance.,0.0,0.592,0.408,0.4404
zipkin,"My application is a spring-rabbitmq based application(neither spring-cloud nor spring-boot), requests were received from one queue and sent responses to another queue.",0.0,0.0,1.0,0.0
zipkin,I want to use brave to trace the system by injecting Zipkin headers before sending messages and extracting Zipkin headers right after receiving messages.,0.0,0.183,0.817,0.5719
zipkin,"The problem is  In step3 of the following scenario, how can I get span1 before sending message?",0.153,0.0,0.847,-0.4019
zipkin,Scenario:,0.0,0.0,1.0,0.0
zipkin,Code snippet before sending message:,0.0,0.0,1.0,0.0
zipkin,"In the above code,  Span currentSpan = tracer.currentSpan();  , the  currentSpan  is always  null .",0.0,0.0,1.0,0.0
zipkin,Code snippet after receiving message:,0.0,0.0,1.0,0.0
zipkin,Brave configuration code:,0.0,0.63,0.37,0.5267
zipkin,Following are my references:,0.0,0.0,1.0,0.0
zipkin,https://github.com/openzipkin/brave/tree/master/brave#one-way-tracing,0.0,0.0,1.0,0.0
zipkin,https://github.com/openzipkin/brave/blob/master/brave/src/test/java/brave/features/async/OneWaySpanTest.java,0.0,0.0,1.0,0.0
zipkin,https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30,0.0,0.0,1.0,0.0
zipkin,"I’m working on a system of microservices, implemented in Scala with Finagle and Thrift as the platform.",0.0,0.0,1.0,0.0
zipkin,"As there are a few services that nobody touched for a while, I need to find out if they are used at all anymore (or rather, which parts are not used anymore).",0.0,0.0,1.0,0.0
zipkin,"For that, IMHO a simple invocation count for each method would suffice (since the service was started, or possibly in the last 24h).",0.0,0.0,1.0,0.0
zipkin,"As far as I see, the Finagle/Thrift integration does not bring something like this built-in, at least not exposed in the admin panel.",0.09,0.052,0.857,-0.2235
zipkin,So what would be the most clever way to do this?,0.0,0.248,0.752,0.5095
zipkin,Just add a filter that counts the invocations and exposes them via the admin interface?,0.103,0.0,0.897,-0.128
zipkin,Or would Zipkin (possibly with custom code) help here?,0.0,0.252,0.748,0.4019
zipkin,I have a web service written in scala and built on top of twitter finagle RPC system.,0.0,0.114,0.886,0.2023
zipkin,Now we are hitting some performance issues.,0.0,0.0,1.0,0.0
zipkin,We have external API components and database layer.,0.0,0.0,1.0,0.0
zipkin,I am planning of installing Zipkin in order to have a service level tracing system.,0.0,0.0,1.0,0.0
zipkin,This will allow me to know where the bottleneck is at the service level.,0.0,0.128,0.872,0.2263
zipkin,I am wondering though if there are framework out there to monitor the performance inside my application layer.,0.0,0.0,1.0,0.0
zipkin,The application is a suite of filters that are applied consecutively to my data and I would like to know which filter take time to compute.,0.0,0.098,0.902,0.3612
zipkin,I heard about JVM profiling but it seems a little overkill for what I want to do.,0.0,0.1,0.9,0.1154
zipkin,What would you recommend ?,0.0,0.455,0.545,0.3612
zipkin,Thanks for your help.,0.0,0.737,0.263,0.6808
zipkin,I'm making a sequential request using Feign Builder.,0.0,0.0,1.0,0.0
zipkin,"There are no x-b3-traceid,x-b3-spanid .. in the title of the request.",0.18,0.0,0.82,-0.296
zipkin,That's why the log my last client appears on the zipkin.,0.0,0.0,1.0,0.0
zipkin,"I use spring  boot 2.4.2 , spring cloud 2020.0.0 , feign-core 10.10.1 , feign-okhttp 10.10.1.",0.0,0.0,1.0,0.0
zipkin,I have tried spring-cloud-openfeign and i achieved wanted result.,0.0,0.0,1.0,0.0
zipkin,But i don't want to use this lib.,0.182,0.0,0.818,-0.0857
zipkin,There are requests  when using Feign Builder and Rest Template in here.,0.0,0.0,1.0,0.0
zipkin,I dont' see same log at zipkin.,0.0,0.0,1.0,0.0
zipkin,My Client1 App.,0.0,0.0,1.0,0.0
zipkin,I am sending request http://localhost:8082/,0.0,0.0,1.0,0.0
zipkin,This is yml of my client1 app.,0.0,0.0,1.0,0.0
zipkin,I use same yml conf on other client apps which client2 and clint3.,0.0,0.0,1.0,0.0
zipkin,Only changes port and app name.,0.0,0.0,1.0,0.0
zipkin,This is my Feign at Client2 app.,0.0,0.0,1.0,0.0
zipkin,Here impl of ClientFeign2.,0.0,0.0,1.0,0.0
zipkin,This is my Feign at Client3 app.,0.0,0.0,1.0,0.0
zipkin,Here impl of Client3 Feign.,0.0,0.0,1.0,0.0
zipkin,pom.xml from client3 and i use  client3 at client2/pom and the same as client1.,0.0,0.0,1.0,0.0
zipkin,diff-feign-rest-image,0.0,0.0,1.0,0.0
zipkin,feign-zipkin-img,0.0,0.0,1.0,0.0
zipkin,feign-request-img,0.0,0.0,1.0,0.0
zipkin,rstlet-rest-tmplte-img,0.0,0.0,1.0,0.0
zipkin,rest-zipkin-img,0.0,0.0,1.0,0.0
zipkin,Trying to read version from properties file but getting below exception,0.0,0.0,1.0,0.0
zipkin,Below is the  pom.xml  and  properties  file,0.0,0.0,1.0,0.0
zipkin,I'm comparing different tracing backend using OpenCensus.,0.0,0.0,1.0,0.0
zipkin,I already have the simple OpenCensus.io python samples running fine using Zipkin and Azure Monitor.,0.0,0.122,0.878,0.2023
zipkin,Now I'm trying to test using GCP's Stackdriver...,0.0,0.0,1.0,0.0
zipkin,"I have set up the test code from Opencensus
 https://opencensus.io/exporters/supported-exporters/python/stackdriver/  as follows:",0.0,0.0,1.0,0.0
zipkin,I have set the environment variable for  GCP_PROJECT_ID  and also have my key file path for my service account JSON file set in  GOOGLE_APPLICATION_CREDENTIALS .,0.0,0.0,1.0,0.0
zipkin,The service account has the  &quot;Cloud trace agent&quot;  role.,0.0,0.0,1.0,0.0
zipkin,My code runs through with no errors but I can't see any info appearing in the GCP console under traces or in the monitoring dashboard.,0.13,0.0,0.87,-0.3182
zipkin,Am I missing something?,0.524,0.0,0.476,-0.296
zipkin,"Environment notes:
I'm testing this from my local Windows machine using Python 3.7.2",0.0,0.0,1.0,0.0
zipkin,We have problem with propagation of  traceId  in requests which are called by spring oauth2 module.,0.153,0.0,0.847,-0.4019
zipkin,For instance consider authorization and resource server.,0.0,0.0,1.0,0.0
zipkin,In resource server we have spring security configuration to ensure get rsa public key from authorization server with following property:,0.0,0.217,0.783,0.6124
zipkin,"When I call controller of resource server with jwt token, I can see in zipkin traces from resource server and authorization server as well, but there is no traceId propagation from resource server to authorization server.",0.077,0.043,0.88,-0.3071
zipkin,"First record is calling rest api to get resources, and second record is produced call to authorization server to find out public RSA key.",0.0,0.0,1.0,0.0
zipkin,We use istio to use distributed tracing.,0.0,0.0,1.0,0.0
zipkin,"Our microservices sometimes need to hit external APIs, which usually communicate over https.",0.0,0.0,1.0,0.0
zipkin,"To measure the exact performance of the whole system, we want to trace the communication when hitting an external API.",0.0,0.064,0.936,0.0772
zipkin,"However, distributed tracing requires access to the header of the request, but https does not allow access because the header is encrypted.",0.087,0.0,0.913,-0.2498
zipkin,"For confirmation, I deployed bookinfo on GKE with istio enabled, entered the productpage container of the productpage pod, and executed the following command.",0.0,0.0,1.0,0.0
zipkin,Only http communication was displayed on zipkin.,0.0,0.0,1.0,0.0
zipkin,"Is it possible to get a series of traces, including APIs that use external https?",0.0,0.0,1.0,0.0
zipkin,"I'm using Kubernetes on Azure cloud, and I have installed zipkin.",0.0,0.0,1.0,0.0
zipkin,"I already install nginx ingress, and if I use the following host rule, it works fine:",0.0,0.122,0.878,0.2023
zipkin,But this is not what I want.,0.21,0.0,0.79,-0.0857
zipkin,What I want is something like hostname.com/zipkin.,0.0,0.487,0.513,0.4215
zipkin,"I tried with this, but I got a 404 error:",0.372,0.0,0.628,-0.5499
zipkin,What do I have to do?,0.0,0.0,1.0,0.0
zipkin,"Edit:
I tried to add the host and after doing a describe command i get this",0.0,0.0,1.0,0.0
zipkin,"EDIT:
I solved my issue adding a rewrite rule annotation",0.0,0.231,0.769,0.2732
zipkin,"I have two services(S1, S2) in a chain.",0.0,0.0,1.0,0.0
zipkin,"I call with CURL(or Postman) S1, and S1 sends request to S2.",0.0,0.0,1.0,0.0
zipkin,"S1 has -  spring.sleuth.sampler.probability: 0.1 
S2 has -   spring.sleuth.sampler.probability: 0.5",0.0,0.0,1.0,0.0
zipkin,I don't understand how the system will behave.,0.0,0.0,1.0,0.0
zipkin,If i send 100 requests:,0.0,0.0,1.0,0.0
zipkin,or,0.0,0.0,1.0,0.0
zipkin,or,0.0,0.0,1.0,0.0
zipkin,I'm trying to generate zipkin trace id from nginx in order to be able to trace from nginx to applications.,0.0,0.0,1.0,0.0
zipkin,"To achieve this, I want to find out how to generate 16 random bytes to be used for X-B3-SpanId since $request_id generates 32 bytes (which can be used for X-B3-TraceId).",0.0,0.047,0.953,0.0964
zipkin,"I am trying to deploy to ibm-cloud a node web application with Angular 2, node.js,express and mongo.",0.0,0.0,1.0,0.0
zipkin,My app works fine without adding the socket.io require line:,0.0,0.167,0.833,0.2023
zipkin,"However, every time i try to add the socket.io and execute the container, the console logs (for each request):",0.0,0.0,1.0,0.0
zipkin,Error: Can't set headers after they are sent.,0.278,0.0,0.722,-0.4019
zipkin,"at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
    at SendStream.send (/app/node_modules/send/index.js:618:10)
    at onstat (/app/node_modules/send/index.js:730:10)
    at FSReqWrap.oncomplete (fs.js:153:5)
  Error: Can't set headers after they are sent.",0.124,0.0,0.876,-0.4019
zipkin,"at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
     at SendStream.send (/app/node_modules/send/index.js:618:10)
     at onstat (/app/node_modules/send/index.js:730:10)
     at FSReqWrap.oncomplete (fs.js:153:5)
  Error: Can't set headers after they are sent.",0.124,0.0,0.876,-0.4019
zipkin,"at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
     at SendStream.send (/app/node_modules/send/index.js:618:10)
     at onstat (/app/node_modules/send/index.js:730:10)
     at FSReqWrap.oncomplete (fs.js:153:5)",0.0,0.0,1.0,0.0
zipkin,"This is my code
        // Uncomment following to enable zipkin tracing, tailor to fit your network configuration:
        // var appzip = require('appmetrics-zipkin')({
        //     host: 'localhost',
        //     port: 9411,
        //     serviceName:'frontend'
        // });",0.0,0.077,0.923,0.3612
zipkin,Thank you very much!.,0.0,0.482,0.518,0.4199
zipkin,I have a bunch of micro-services hosted on AWS.,0.0,0.0,1.0,0.0
zipkin,"I am using StatsD, Graphite and Grafana to monitor them.",0.0,0.0,1.0,0.0
zipkin,Now I want to expand it to monitor the queues (SQS) through which these micro-services are talking to each other.,0.0,0.175,0.825,0.3818
zipkin,How can I leverage Graphite/ Grafana to do this?,0.0,0.0,1.0,0.0
zipkin,Or a better approach if there aint any support/ plugin for the same.,0.0,0.209,0.791,0.4404
zipkin,Thanks :),0.0,1.0,0.0,0.7096
zipkin,"PS : If it's gotta be Zipkin, please tell me they can co-exist or is there a catch to using multiple tracers.",0.0,0.108,0.892,0.3182
zipkin,"I have a spring boot microservice: Zuul-api-gateway-server, and I am trying to implement a Zipkin server listening to rabbitmq for logging messages within the microservice.",0.0,0.0,1.0,0.0
zipkin,I have added the following dependencies to this microservice:,0.0,0.0,1.0,0.0
zipkin,I have started the Zipkin server using the following commands:,0.0,0.0,1.0,0.0
zipkin,"SET RABBIT_URI=amqp://localhost
java -jar zipkin.jar",0.0,0.0,1.0,0.0
zipkin,I then try to start up the microservice however I get the following error:,0.197,0.0,0.803,-0.4019
zipkin,"org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitListenerContainerFactory' defined in class path resource [org/springframework/boot/autoconfigure/amqp/RabbitAnnotationDrivenConfiguration.class]: Initialization of bean failed; nested exception is java.lang.NullPointerException
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:584) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:846) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:863) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at com.shopping.sandbox.netflixzuulapigatewayserver.NetflixZuulApiGatewayServerApplication.main(NetflixZuulApiGatewayServerApplication.java:16) [classes/:na]
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_171]
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_171]
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_171]
      at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_171]
      at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) [spring-boot-devtools-2.1.1.RELEASE.jar:2.1.1.RELEASE]
  Caused by: java.lang.NullPointerException: null
      at org.springframework.amqp.rabbit.config.AbstractRabbitListenerContainerFactory.getAdviceChain(AbstractRabbitListenerContainerFactory.java:198) ~[spring-rabbit-2.1.2.RELEASE.jar:2.1.2.RELEASE]
      at brave.spring.rabbit.SpringRabbitTracing.decorateSimpleRabbitListenerContainerFactory(SpringRabbitTracing.java:170) ~[brave-instrumentation-spring-rabbit-5.4.4.jar:na]
      at org.springframework.cloud.sleuth.instrument.messaging.SleuthRabbitBeanPostProcessor.postProcessBeforeInitialization(TraceMessagingAutoConfiguration.java:186) ~[spring-cloud-sleuth-core-2.1.0.M2.jar:2.1.0.M2]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:419) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1737) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      ... 20 common frames omitted",0.051,0.019,0.93,-0.5859
zipkin,We have an application and where we are tracing the whole logs using a custom correlation header i.e.,0.0,0.0,1.0,0.0
zipkin,"X-CID with a value as for example, 615b7eea-6d4c-4efb-9431-fcbba084ea3f.",0.0,0.286,0.714,0.34
zipkin,Recently we have integrated the Spring Sleuth with the train version as Greenwich.BUILD-SNAPSHOT in our application and using brave's Span and tracer to create trace and span ad sending it to Zipkin.,0.0,0.063,0.937,0.2732
zipkin,What we are looking for a mechanism where we can use or set the CID value as a traceId rather than using the auto generated one from Sleuth.,0.0,0.088,0.912,0.34
zipkin,"The rational behind is, we would like to make it uniform CID value to use or search in Zipkin UI rather than two different values i.e.",0.0,0.248,0.752,0.765
zipkin,CID value and Spring Sleuth traceId to trace the complete API calls.,0.0,0.179,0.821,0.34
zipkin,"Please note that, we would like to reuse the X-CID from request which is already supplied in the request header.",0.0,0.211,0.789,0.5859
zipkin,Are there any APIs to override this behavior or any alternate way to achieve this ?,0.0,0.0,1.0,0.0
zipkin,TLDR :,0.0,0.0,1.0,0.0
zipkin,More details :,0.0,0.0,1.0,0.0
zipkin,I'm trying to create a custom Sleuth instrumenting service that can also work when no tracing is available.,0.114,0.109,0.777,-0.0258
zipkin,Here is a really simplified version of the service interface :,0.0,0.0,1.0,0.0
zipkin,"I then create an AutoConfiguration (as this code is in a separate module, used by many projects) :",0.0,0.13,0.87,0.2732
zipkin,The idea is really to have a kind of Noop version of the service when no Tracer is available.,0.115,0.0,0.885,-0.296
zipkin,I then declare the the AutoConfiguration in a spring.factories file as usual.,0.0,0.0,1.0,0.0
zipkin,This is the application I want to run :,0.0,0.178,0.822,0.0772
zipkin,"For all of this I want to use Zipkin, with a KafkaSpanReporter.",0.0,0.133,0.867,0.0964
zipkin,So my application.properties look like this :,0.0,0.333,0.667,0.3612
zipkin,and my (truncated) pom.xml like this :,0.0,0.333,0.667,0.3612
zipkin,When I try to run this code I get an error :,0.252,0.0,0.748,-0.4019
zipkin,If I look at the configuration report I see :,0.0,0.0,1.0,0.0
zipkin,"This is weird, because there is a  @EnableConfigurationProperties(KafkaProperties.class)  on  KafkaAutoConfiguration  and the configuration report clearly shows :",0.098,0.155,0.747,0.25
zipkin,"What is even weirder is that if I remove the  @AutoConfigureAfter(name = ""org.springframework.cloud.sleuth.autoconfig.TraceAutoConfiguration"")  in my AutoConfiguration, the service starts all right =  but I get the  NoTracedConfiguration  flavour of my bean, so  Tracer  is probably not configured yet.",0.036,0.0,0.964,-0.0644
zipkin,What can I do to fix this problem ?,0.342,0.0,0.658,-0.481
zipkin,"I'm working on the microservices project that is the dockerized Spring Cloud Netflix project and contains 3 microservices except for some Netflix services which are turbine,zipkin,discovery,configserver etc , yet.",0.0,0.0,1.0,0.0
zipkin,(Its just working on locally now..),0.0,0.0,1.0,0.0
zipkin,"Soon, I decided to deploy my project to a cloud provider with an orchestration tool.",0.0,0.0,1.0,0.0
zipkin,"After some researches, I decided to use Kuberenetes.",0.0,0.0,1.0,0.0
zipkin,"But, both of Spring Cloud Netflix and Kubernetes have some solutions for distributed systems: service discovery, load balancing, fault-tolerance, etc..",0.0,0.097,0.903,0.2617
zipkin,"In that case, using Netflix libs.",0.0,0.0,1.0,0.0
zipkin,seem unnecessary with Kubernetes.,0.0,0.0,1.0,0.0
zipkin,I read  this  and  this .,0.0,0.0,1.0,0.0
zipkin,I think Spring Cloud Kubernetes looks like a workaround solution.,0.0,0.444,0.556,0.5859
zipkin,So my questions are :,0.0,0.0,1.0,0.0
zipkin,I'm experiencing issues scaling my app with multiple requests.,0.0,0.0,1.0,0.0
zipkin,"Each request sends an ask to an actor, which then spawns other actors.",0.0,0.0,1.0,0.0
zipkin,"This is fine, however, under load(5+ asks at once), the  ask  takes a massive amount of time to deliver the message to the target actor.",0.0,0.073,0.927,0.2023
zipkin,"The original design was to bulkhead requests evenly, but this is causing a bottleneck.",0.0,0.121,0.879,0.1655
zipkin,Example:,0.0,0.0,1.0,0.0
zipkin,"In this picture, the  ask  is sent right after the query plan resolver.",0.0,0.124,0.876,0.1779
zipkin,"However, there is a multi-second gap when the Actor receives this message.",0.0,0.0,1.0,0.0
zipkin,This is only experienced under load(5+ requests/sec).,0.0,0.0,1.0,0.0
zipkin,I first thought this was a starvation issue.,0.0,0.0,1.0,0.0
zipkin,"Design:
Each planner-executor is a seperate instance for each request.",0.0,0.0,1.0,0.0
zipkin,It spawns a new 'Request Acceptor' actor each time(it logs 'requesting score' when it receives a message).,0.0,0.0,1.0,0.0
zipkin,I'm a bit stumped by this.,0.0,0.0,1.0,0.0
zipkin,From these tests it does not look like a thread starvation issue.,0.174,0.0,0.826,-0.2755
zipkin,"Back at square one, I have no idea why the message takes longer and longer to deliver the more concurrent requests I make.",0.099,0.0,0.901,-0.296
zipkin,The Zipkin trace before reaching this point does not degrade with more requests until it reaches the  ask  here.,0.0,0.253,0.747,0.5277
zipkin,"Before then, the server is able to handle multiple steps to e.g veify the request, talk to the db, and then finally go inside the planner-executor.",0.0,0.0,1.0,0.0
zipkin,So I doubt the application itself is running out of cpu time.,0.218,0.0,0.782,-0.4201
zipkin,I have a Kubernetes's and spring boot's env variables conflict error.,0.417,0.0,0.583,-0.6124
zipkin,Details is as follows:,0.0,0.0,1.0,0.0
zipkin,"When creating my zipkin server pod, I need to set env variable  RABBITMQ_HOST=http://172.16.100.83，RABBITMQ_PORT=5672 .",0.0,0.167,0.833,0.296
zipkin,Initially I define zipkin_pod.yaml as follows:,0.0,0.0,1.0,0.0
zipkin,"With this configuration, when I do command",0.0,0.0,1.0,0.0
zipkin,The console throws error:,0.474,0.0,0.526,-0.4019
zipkin,so I modified the last line of zipkin_pod.yaml file as follows: Or use brutal force to make port number as int.,0.175,0.056,0.769,-0.5859
zipkin,"Then pod is successfully created, but spring getProperties throws exception.",0.0,0.31,0.69,0.3818
zipkin,When I check logs:,0.0,0.0,1.0,0.0
zipkin,"My question is how to let kubernetes understand the port number as int, while not breaking spring boot convert rule from string to int?",0.0,0.053,0.947,0.0772
zipkin,because spring boot could not convert  !,0.0,0.0,1.0,0.0
zipkin,!31503 to int 31503 .,0.0,0.0,1.0,0.0
zipkin,I am using Spring Cloud for Creating Microservice Architecture.,0.0,0.239,0.761,0.296
zipkin,I was using the below feature from the Spring Cloud,0.0,0.0,1.0,0.0
zipkin,"Now Lets say if I have 100 microservices, then we need 100 servers to maintain each microservices.",0.0,0.0,1.0,0.0
zipkin,"So I thought of using Kubernetes to solve this issue by  deploying each microservices in a separate docker container, so now since Kubernetes takes care of microserivice health check, autoscaling, load-balancing so do I need to again use Ribbon, Eureka and Zuul.",0.0,0.119,0.881,0.6124
zipkin,Can anyone please help me on this,0.0,0.5,0.5,0.6124
zipkin,I got a  Spring Boot  application making use of  Spring Sleuth  for tracing inter-service calls.,0.0,0.0,1.0,0.0
zipkin,Within that application a  ScheduledExecutorService  exists that performs http requests in a loop (pseudo-code below):,0.0,0.0,1.0,0.0
zipkin,If I now have a look at the traces produced by Sleuth and stored in  Zipkin  I can see that all http calls are associated to a single Trace.,0.0,0.0,1.0,0.0
zipkin,Most likely because the trace context is handed over during the call to  ScheduledExecutorService::submit .,0.0,0.0,1.0,0.0
zipkin,How can I clear the current trace before starting the next iteration so that each http call will result in a new detached/orphaned trace?,0.0,0.11,0.89,0.3818
zipkin,I should say I'm really impressed with the simplicity and usefulness of spring-cloud-sleuth and zipkin.,0.0,0.318,0.682,0.6801
zipkin,"However, I'm working on a POC for which I'm considering reactive toolkits.",0.0,0.0,1.0,0.0
zipkin,Vertx 3 is the first item in my list to try (with spring cloud ecosystem).,0.0,0.0,1.0,0.0
zipkin,I'm wondering if Sleuth log tracing would work in a reactive context as I guess it relies on ThreadLocals to pass around the context?,0.0,0.0,1.0,0.0
zipkin,Keen to understand where Sleuth would stand in a reactive environment.,0.0,0.217,0.783,0.3612
zipkin,I use Spring Cloud Feign and Sleuth with a Zipkin server.,0.0,0.0,1.0,0.0
zipkin,"My problem is that when I enable Sleuth, then any simple request takes at least 600ms.",0.162,0.0,0.838,-0.4019
zipkin,"Note that for tests purposes, I've set the sampler percentage of Sleuth at 1.",0.0,0.0,1.0,0.0
zipkin,Can I do something to improve that?,0.0,0.367,0.633,0.4404
zipkin,Here some log of a request which takes 25ms without Sleuth and 700ms with Sleuth.,0.0,0.0,1.0,0.0
zipkin,(user calls /teams which calls /cities):,0.0,0.0,1.0,0.0
zipkin,"I'm trying to get an openzipkin server running in a k8s cluster, starting with testing in a minikube.",0.0,0.0,1.0,0.0
zipkin,"I'm beginner with k8s config, but here's what I've done so far:",0.0,0.0,1.0,0.0
zipkin,"What I think I'm doing is starting a new pod and deploying the zipkin image, then exposing the Web UI at port 9411 via zipkin-http.",0.087,0.0,0.913,-0.2732
zipkin,After doing this:,0.0,0.0,1.0,0.0
zipkin,Then I run the kubectl proxy so I can access the Web UI from my browser:,0.0,0.0,1.0,0.0
zipkin,Now if I browse to  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json  I get the config file contents:,0.0,0.0,1.0,0.0
zipkin,But if I browse to the root at  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/  I receive an error:,0.262,0.0,0.738,-0.5499
zipkin,The config.json it's attempting to load is the one at :9411/config.json.,0.0,0.0,1.0,0.0
zipkin,The request to load /config.json comes from a JS file that was loaded by the html in the root page.,0.0,0.0,1.0,0.0
zipkin,"Since it looks like I can get to the json file directly from both inside and outside the cluster, I'm confused as to why the JS file isn't able to load it.",0.068,0.074,0.858,0.0516
zipkin,What am I doing wrong here?,0.437,0.0,0.563,-0.4767
zipkin,Thanks!,0.0,1.0,0.0,0.4926
zipkin,"I am using  zipkin-go-opentracing , which is an implementation of the  opentracing  API for zipkin in go.",0.0,0.0,1.0,0.0
zipkin,For (reasons) I need to get the traceId from a span.,0.0,0.0,1.0,0.0
zipkin,"So the question:
given a opentracing.Span, how do I get the TraceId?",0.0,0.0,1.0,0.0
zipkin,Everything I've tried has given me some kind of type assertion error.,0.197,0.0,0.803,-0.4019
zipkin,"Thanks,",0.0,1.0,0.0,0.4404
zipkin,I am running mysql and zipkin in kubernetes.,0.0,0.0,1.0,0.0
zipkin,Zipkin is failing to connect to mysql database.,0.32,0.0,0.68,-0.5106
zipkin,I've checked environment variables and all variables are set up properly.,0.0,0.0,1.0,0.0
zipkin,Exception,0.0,0.0,1.0,0.0
zipkin,mysql-deployment.yaml,0.0,0.0,1.0,0.0
zipkin,mysql-service.yaml,0.0,0.0,1.0,0.0
zipkin,zipkin-deployment.yaml,0.0,0.0,1.0,0.0
zipkin,zipkin-service.yaml,0.0,0.0,1.0,0.0
zipkin,secret.yaml,0.0,0.0,1.0,0.0
zipkin,"I think, I need to change Charset in zipkin.",0.0,0.0,1.0,0.0
zipkin,"But, I have no way to change it other than build another image myself.",0.189,0.0,0.811,-0.4215
zipkin,I have recently started exploring sping boot 2.,0.0,0.0,1.0,0.0
zipkin,I am uisng logaback for logging purpose.,0.0,0.0,1.0,0.0
zipkin,"For distributed logging tracing, I wanted to use sping boot sleuth starter.",0.0,0.0,1.0,0.0
zipkin,But with below dependency in pom.xml without zipkin integration its not adding traceid in logs.Also added spring.application.name property in application.properties file.,0.0,0.0,1.0,0.0
zipkin,Am I missing anything here?,0.423,0.0,0.577,-0.296
zipkin,"I am using Spring Cloud Sleuth and Zipkin (via HTTP), by adding spring-cloud-starter-zipkin version 2.0.0.M6 to my dependencies (based on Spring Boot 2.0.0.RC1 and Spring Cloud Finchley M6).",0.0,0.0,1.0,0.0
zipkin,I am using @Newspan annotation to mark a child span around some (expensive) operation.,0.0,0.0,1.0,0.0
zipkin,"When the span information is sent to Zipkin, I notice that the timestamp and duration of the child span are missing.",0.104,0.0,0.896,-0.296
zipkin,This leads to a strange rendering on Zipking side.,0.205,0.0,0.795,-0.2023
zipkin,"However, when I create the child span by calling tracer#newChild, it works as expected.",0.0,0.149,0.851,0.2732
zipkin,Am I missing something?,0.524,0.0,0.476,-0.296
zipkin,Would this be an issue with Sleuth 2.0.0.M6?,0.0,0.0,1.0,0.0
zipkin,"When I run the same code using Spring Boot 1.5.9 and Spring Cloud Edgware SR2, it behaves as expected.",0.0,0.0,1.0,0.0
zipkin,Here's the JSON received on Zipkin side.,0.0,0.0,1.0,0.0
zipkin,"The span named ""child-span-with-annotation"" is the one created using @NewSpan, whereas the span ""childspanwithnewchild"" is created using tracer#newChild.",0.0,0.2,0.8,0.4588
zipkin,"
 
 [
  {
    ""traceId"": ""b1c2636366c919be"",
    ""id"": ""b1c2636366c919be"",
    ""name"": ""get"",
    ""timestamp"": 1518495271073166,
    ""duration"": 862032,
    ""annotations"": [
      {
        ""timestamp"": 1518495271073166,
        ""value"": ""sr"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      },
      {
        ""timestamp"": 1518495271935198,
        ""value"": ""ss"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      }
    ],
    ""binaryAnnotations"": [
      {
        ""key"": ""ca"",
        ""value"": true,
        ""endpoint"": {
          ""serviceName"": """",
          ""ipv6"": ""::1"",
          ""port"": 51982
        }
      },
      {
        ""key"": ""http.path"",
        ""value"": ""/hello"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      },
      {
        ""key"": ""mvc.controller.class"",
        ""value"": ""MyRestController"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      },
      {
        ""key"": ""mvc.controller.method"",
        ""value"": ""sayHello"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      }
    ]
  },
  {
    ""traceId"": ""b1c2636366c919be"",
    ""id"": ""14be7ac6eafb0e01"",
    ""name"": ""child-span-with-annotation"",
    ""parentId"": ""b1c2636366c919be"",
    ""binaryAnnotations"": [
      {
        ""key"": ""class"",
        ""value"": ""MyService"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      },
      {
        ""key"": ""method"",
        ""value"": ""expensiveOperation1"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      }
    ]
  },
  {
    ""traceId"": ""b1c2636366c919be"",
    ""id"": ""b34a4f910f27fdb4"",
    ""name"": ""childspanwithnewchild"",
    ""parentId"": ""b1c2636366c919be"",
    ""timestamp"": 1518495271479040,
    ""duration"": 453747,
    ""binaryAnnotations"": [
      {
        ""key"": ""lc"",
        ""value"": """",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      }
    ]
  }
]",0.0,0.022,0.978,0.4215
zipkin,Can someone provide a delete query to delete older records(older than 5 days)for the below mysql tables with its reference tables?,0.0,0.0,1.0,0.0
zipkin,(All reference tables records also need to deleted otherwise crashes my application),0.0,0.0,1.0,0.0
zipkin,https://github.com/openzipkin/zipkin/blob/master/zipkin-storage/mysql/src/main/resources/mysql.sql,0.0,0.0,1.0,0.0
zipkin,I have no clue to write a coordinated delete query (deleting multiple tables in same query),0.145,0.0,0.855,-0.296
zipkin,Small question regarding some actuator endpoints returning 404 please.,0.0,0.223,0.777,0.3182
zipkin,"I have a web app, based on Webflux 2.4.2, and for testing this issue only, I am using",0.0,0.0,1.0,0.0
zipkin,"Actuator is working, because a curl will get the response for /health /metrics and other endpoints.",0.0,0.0,1.0,0.0
zipkin,"However, for those endpoints  /auditevents /httptrace /integrationgraph /sessions , I am not able to get anything, besides a http 404.",0.0,0.0,1.0,0.0
zipkin,[05/Feb/2021:13:00:18 +0000] &quot;OPTIONS /auditevents HTTP/1.1&quot; 404 141 55 ms,0.0,0.0,1.0,0.0
zipkin,"Those are really the only endpoints returning 404, still do not know why.",0.0,0.0,1.0,0.0
zipkin,Don't want to spam with one question same question per endpoint.,0.099,0.171,0.73,0.2235
zipkin,All other actuator endpoints are fine.,0.0,0.265,0.735,0.2023
zipkin,Thank you,0.0,0.714,0.286,0.3612
zipkin,I have a reactive application using Spring Webflux.,0.0,0.0,1.0,0.0
zipkin,"I have used sleuth annotations like  @NewSpan  to create spans, but I am getting warning like",0.158,0.333,0.509,0.3506
zipkin,"I know  Flux.subscribe  is a final method so proxies are not generated correctly, but I can still see those spans on Zipkin.",0.0,0.0,1.0,0.0
zipkin,I need to know what are the implications of this warning.,0.234,0.0,0.766,-0.4118
zipkin,And how can I avoid this?,0.355,0.0,0.645,-0.296
zipkin,"Related to  https://github.com/openzipkin/zipkin/pull/3239  , we came across some (maybe) odd behaviour and i wanted to know if below test works as expected or not:",0.099,0.0,0.901,-0.3182
zipkin,"Basically we wanted to disable all  TRACE  requests, and set  pathPrefix(&quot;/&quot;)  to achieve this.",0.0,0.0,1.0,0.0
zipkin,But for some reason the  OPTIONS  call to  /something  gets trapped in the same path.,0.247,0.0,0.753,-0.6808
zipkin,If i remove the route decorator things work as expected.,0.0,0.0,1.0,0.0
zipkin,I am new to Microservices.,0.0,0.0,1.0,0.0
zipkin,(Learning phase).,0.0,0.0,1.0,0.0
zipkin,I have a question.,0.0,0.0,1.0,0.0
zipkin,We deploy microservices at cloud.,0.0,0.0,1.0,0.0
zipkin,(e.g.,0.0,0.0,1.0,0.0
zipkin,AWS).,0.0,0.0,1.0,0.0
zipkin,Cloud already provide load balancing and logs.,0.0,0.0,1.0,0.0
zipkin,And We also implement Load Balancing(Ribbon) and logs(Rabbit MQ and Zipkin) in Spring Boot.,0.0,0.0,1.0,0.0
zipkin,What is the difference in these two implementation?,0.0,0.0,1.0,0.0
zipkin,Do we need both?,0.0,0.0,1.0,0.0
zipkin,Can some answer these questions.,0.0,0.0,1.0,0.0
zipkin,Thanks in advance.,0.0,0.592,0.408,0.4404
zipkin,I deploy zipkin in docker (zipkin-server-2.21.7-exec.jar) and I connect with rabbit in docker.,0.0,0.0,1.0,0.0
zipkin,I'm using Eureka in docker to register microservices.,0.0,0.0,1.0,0.0
zipkin,When I run one this microservices this error compare,0.309,0.0,0.691,-0.481
zipkin,APPLICATION FAILED TO START,0.524,0.0,0.476,-0.5106
zipkin,Description:,0.0,0.0,1.0,0.0
zipkin,Parameter 2 of method reporter in org.springframework.cloud.sleuth.zipkin2.ZipkinAutoConfiguration required a bean of type 'zipkin2.reporter.Sender' that could not be found.,0.0,0.0,1.0,0.0
zipkin,The following candidates were found but could not be injected:,0.0,0.0,1.0,0.0
zipkin,Bean method 'rabbitSender' in 'ZipkinRabbitSenderConfiguration' not loaded because @ConditionalOnBean (types: org.springframework.amqp.rabbit.connection.CachingConnectionFactory; SearchStrategy: all) did not find any beans of type org.springframework.amqp.rabbit.connection.CachingConnectionFactory,0.0,0.0,1.0,0.0
zipkin,Bean method 'restTemplateSender' in 'ZipkinRestTemplateSenderConfiguration' not loaded because ZipkinSender org.springframework.cloud.sleuth.zipkin2.sender.ZipkinRestTemplateSenderConfiguration rabbit sender type,0.0,0.0,1.0,0.0
zipkin,Action:,0.0,0.0,1.0,0.0
zipkin,Consider revisiting the entries above or defining a bean of type 'zipkin2.reporter.Sender' in your configuration.,0.0,0.0,1.0,0.0
zipkin,"I use this properties
spring.zipkin.sender.type=rabbit
spring.zipkin.base-url=http://zipkin-server:9411/",0.0,0.0,1.0,0.0
zipkin,We have an EKS cluster in AWS and i am using istio as service mesh in my cluster.,0.0,0.0,1.0,0.0
zipkin,We are using istio only for injecting the sidecar into applications and to trace the application traffic through zipkin.,0.0,0.0,1.0,0.0
zipkin,To access the application from outside we are not using istio-ingressgateway instead we are using ALB &amp; ELBs,0.0,0.0,1.0,0.0
zipkin,So my problem is I am not getting any traces to zipkin / kiali when i am accessing my application through AWS LBs.,0.136,0.0,0.864,-0.4549
zipkin,Do i have to use istio-ingressgateway to record the traces in zipkin and view in kiali or is there a way to get traces using ALB/ELB as a loadbalancer?,0.0,0.0,1.0,0.0
zipkin,Running a deployment with three pod.,0.0,0.0,1.0,0.0
zipkin,The only one pod container I can connect is zipkin with  kubectl exec -it my-api-XXX -- /bin/bash .,0.0,0.0,1.0,0.0
zipkin,If I want to access the my-api container using  kubectl exec -it my-api-XXX -c &lt;my-api container ID&gt; -- /bin/bash .,0.0,0.071,0.929,0.0772
zipkin,It report a error show the container is not in that pod.,0.213,0.0,0.787,-0.4019
zipkin,Error from server (BadRequest): container my-api_containerID is not valid for pod my-api-XXX,0.197,0.0,0.803,-0.4019
zipkin,I am new to K8s and I am trying to migrate my service (which currently utilizes docker-compose.yml) to k8s.,0.0,0.0,1.0,0.0
zipkin,My service,0.0,0.0,1.0,0.0
zipkin,deploys zipkin and elasticsearch,0.0,0.0,1.0,0.0
zipkin,and these can be accessed at  'localhost:9411'  and  'localhost:9200'  respectively.,0.0,0.211,0.789,0.34
zipkin,"The most commonly used solution I found online was 'kompose' and I tried to run,",0.0,0.176,0.824,0.3744
zipkin,2.,0.0,0.0,1.0,0.0
zipkin,"Once I finish this, I run kubectl get pods and I can see my deployments, but elasticsearch and zipkin are no more responsive on their respective localhost ports.",0.087,0.23,0.683,0.6798
zipkin,Ouput of  'kubectl get pods',0.0,0.0,1.0,0.0
zipkin,Output of  'docker ps',0.0,0.0,1.0,0.0
zipkin,Output of  curl http://localhost:9200,0.0,0.0,1.0,0.0
zipkin,Can someone tell me why this is happening and how to debug?,0.0,0.0,1.0,0.0
zipkin,We are building event-driven microservices using Spring Cloud Stream(with Kafka binder) and looking at options for tracing Micorservices that are not exposed as http end point.,0.0,0.047,0.953,0.0572
zipkin,Please suggest.,0.0,0.697,0.303,0.3182
zipkin,I understand that using Sleuth will automatically add trace and span id to logs if it is over http.,0.0,0.0,1.0,0.0
zipkin,Documentation is not clear for using it with Spring Cloud Stream -  https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth#_messaging,0.166,0.0,0.834,-0.2924
zipkin,Found an example but not sure whether it is the right approach -  https://github.com/bjedrzejewski/food-order-publisher/blob/zipkin-example/src/main/java/com/e4developer/foodorderpublisher/FoodOrderController.java,0.169,0.0,0.831,-0.3491
zipkin,Thanks,0.0,1.0,0.0,0.4404
zipkin,"I want to send RabbitMQ messages tracking event to Zipkin with using spring cloud sleuth, After many research I found some configuration added recently to spring in order to manage it you can find in  here , But unfortunately there is not any  documentation that explains how can we configure it, I tried many ways but I couldn't send tracking events to Zipkin.",0.051,0.019,0.929,-0.4497
zipkin,Please advice,0.0,0.697,0.303,0.3182
zipkin,I need to set the traceId with an existing Id (we have created some kind of correlation-id from the main origin app) into brave tracer.,0.0,0.197,0.803,0.6597
zipkin,I don't want to use the Spring Sleuth/brave created one as  I want to make it consistent throughout my different micro-services.,0.12,0.098,0.783,0.1421
zipkin,I am able to create traces and span and able to send all details into Zipkin.,0.0,0.13,0.87,0.2732
zipkin,My sample snippet:,0.0,0.0,1.0,0.0
zipkin,I am using: Spring Cloud 'Greenwich.BUILD-SNAPSHOT' and brave.,0.0,0.362,0.638,0.5267
zipkin,The whole purpose is to search using correlationId rather than traceId in zipkin ui.,0.0,0.0,1.0,0.0
zipkin,"In our Spring Boot application (2.0.4.RELEASE), we use Zipkin to integrate distributed tracing.",0.0,0.0,1.0,0.0
zipkin,"When creating the integration manually with a 10% sampling rate, meaning with a  @Configuration  like this:",0.0,0.281,0.719,0.5719
zipkin,our application has a 50 percentile performance of about 19ms and a 99.9 percentile of about 90ms at around 10 requests per second.,0.0,0.0,1.0,0.0
zipkin,When integrating Sleuth 2.0.2.RELEASE instead like this in gradle:,0.0,0.238,0.762,0.3612
zipkin,the performance drops massively to a p50 of 49ms and a p999 of 120ms.,0.0,0.0,1.0,0.0
zipkin,"I tried disabling the different parts of the Sleuth integration ( spring.sleuth.async.enabled ,  spring.sleuth.reactor.enabled , etc.",0.22,0.0,0.78,-0.4767
zipkin,).,0.0,0.0,1.0,0.0
zipkin,"Disabling all these integrations brings the performance to p50: 25ms, p999: 103 ms.  Just having Sleuth adds about 15-25% of overhead.",0.134,0.0,0.866,-0.4767
zipkin,It turns out that the one thing with the significant impact is setting  spring.sleuth.log.slf4j.enabled  to  false .,0.0,0.107,0.893,0.2023
zipkin,"If all other integrations are enabled, but this is disabled, the performance stays within the Sleuth overhead mentioned above, although nothing is logged.",0.0,0.0,1.0,0.0
zipkin,"So my question is:
Is there a way to avoid the overhead by Sleuth (compared to ""manual"" tracing) and especially the one done by the SLF4J integration?",0.081,0.0,0.919,-0.296
zipkin,we are trying to make parallel calls to different recipient using scatter-gather and it works fine.,0.0,0.107,0.893,0.2023
zipkin,But the second recipient flow is not starting unless the first one is complete(traced in Zipkin).,0.0,0.0,1.0,0.0
zipkin,is there is a way to make all recipients async.. very similar to split-aggregate with executor channel.,0.0,0.0,1.0,0.0
zipkin,"flow2(),flow3(),flow4() methods are methods with  InterationFlow  as return type.",0.0,0.0,1.0,0.0
zipkin,sample code  flow2()  :,0.0,0.0,1.0,0.0
zipkin,I'm learning about microservices and I need spring cloud cli for this test project.,0.0,0.0,1.0,0.0
zipkin,I have installed spring boot cli (extracted and added to path) version Spring CLI v2.0.3.RELEASE.,0.0,0.0,1.0,0.0
zipkin,"I have installed the spring cloud cli plugin (spring-cloud-cli:1.3.1.RELEASE), and verify it with checking its version.",0.0,0.0,1.0,0.0
zipkin,"I would like to execute [spring cloud eureka configserver zipkin]
but I'm getting:",0.0,0.137,0.863,0.1901
zipkin,File ........\.m2\repository\org\springframework\cloud\launcher\spring-cloud-launcher-deployer\1.3.3.BUILD-SNAPSHOT\spring-cloud-launcher-deployer-1.3.3.BUILD-SNAPSHOT.jar must exist,0.0,0.0,1.0,0.0
zipkin,I am using rxjava 2 with spring boot 2 and spring cloud finchley.rc2.,0.0,0.0,1.0,0.0
zipkin,Now i am tracing the requests with sleuth / zipkin.,0.0,0.0,1.0,0.0
zipkin,But i am getting one issue that IO threads are reusing the olds requests traceids.,0.0,0.0,1.0,0.0
zipkin,"When I am doing this, traceids generated are fine(same for all parallel calls in same request )",0.0,0.0,1.0,0.0
zipkin,"But when I am doing this, it is not working properly.",0.0,0.0,1.0,0.0
zipkin,"I am assuming since io() threads are reused, thats why they are using old data since they are old threads?",0.0,0.0,1.0,0.0
zipkin,But how can i force them to use the trace id for current main thread?,0.0,0.0,1.0,0.0
zipkin,How can i fix this?,0.0,0.0,1.0,0.0
zipkin,I cant move the webflux as i have lot of code writtenin rxjava?,0.0,0.0,1.0,0.0
zipkin,"Currently, I am working on my company's microservices solution which uses  Spring Cloud Edgware.SR1 .",0.0,0.161,0.839,0.3182
zipkin,"This solution includes the following main flavors:
 api-gateway(Zuul) ,  service-discovery(Eureka) ,  uaa ,  zipkin-server  and  business logic  services.",0.0,0.141,0.859,0.3182
zipkin,I am trying to provide a good tracing for all requests in our system.,0.0,0.209,0.791,0.4404
zipkin,"In Zipkin UI I can see a trace for the request that starts in api-gateway, going through uaa to our business logic service.",0.0,0.0,1.0,0.0
zipkin,But requests to the Eureka are missing in the trace.,0.237,0.0,0.763,-0.4215
zipkin,"Instead, there is a separate trace with a single span for  service-discovery  endpoint  http://eureka/apps/** .",0.0,0.0,1.0,0.0
zipkin,I had the same issue with  uaa  request to  http://user/  endpoint and solved it by adding  TraceRestTemplateInterceptor  to our Oauth2 client.,0.0,0.1,0.9,0.2732
zipkin,"However, I found it difficult to override  EurekaHttpClient  and add the mentioned interceptor.",0.185,0.0,0.815,-0.3612
zipkin,Is there any other way to propagate trace id to Eureka Clients?,0.0,0.0,1.0,0.0
zipkin,I have created a spring boot application with spring cloud sleuth.,0.0,0.2,0.8,0.25
zipkin,"For POC purposes, I used zipkin on 
my local machine and I am able to instrument a external service which is not instrumented by creating 
manual span.",0.076,0.0,0.924,-0.2235
zipkin,I reffered below link.,0.0,0.0,1.0,0.0
zipkin,https://cloud.spring.io/spring-cloud-sleuth/1.2.x/multi/multi__customizations.html,0.0,0.0,1.0,0.0
zipkin,"Now, When I move to PCF environment, then I am unable to collect proper custom spans.",0.0,0.0,1.0,0.0
zipkin,PCF metrics always shows parent span and service with total time taken.,0.0,0.0,1.0,0.0
zipkin,Could anyone please let me know where I am going wrong.,0.231,0.172,0.597,-0.2023
zipkin,Zipkin Output:-,0.0,0.0,1.0,0.0
zipkin,PCF Metrics:-,0.0,0.0,1.0,0.0
zipkin,"UPDATE 
screen shot for Zipkin with @NewSpan.",0.0,0.0,1.0,0.0
zipkin,PCF metrics screen shot without call hierachy,0.0,0.0,1.0,0.0
zipkin,"I have been trying to get Spring boot 2.0 and Spring Cloud Slueth 2.x (POM= Finchley.M6) working, but no avail.",0.135,0.0,0.865,-0.4215
zipkin,I have a  service1  calls  service2  and  service3 .,0.0,0.0,1.0,0.0
zipkin,I see that a new  traceId  is created whenever a request is received in  service1  but not passed to  Service2  and  Service3  instead a new  traceid  is being created every time on  Service2  and  Service3 .,0.0,0.121,0.879,0.4588
zipkin,Is this anyhow related to  this defect  ?,0.314,0.0,0.686,-0.4118
zipkin,NOTE: I don't need zipkin support and I need sleuth for distributed tracing and will be using Splunk as log aggregater.,0.111,0.0,0.889,-0.3089
zipkin,Source Code :  https://github.com/trmsmy/springboot-cloud-examples/tree/springboot2,0.0,0.0,1.0,0.0
zipkin,I am currently running Spring Cloud Edgware.SR2.,0.0,0.0,1.0,0.0
zipkin,"I am migrating services from RabbitMQ to Kafka, and at this point I don't see any Zipkin traces when I run kafka-console-consumer.sh on the zipkin topic (i.e.,  kafka-console-consumer.sh --new-consumer --bootstrap-server localhost:9092 --topic zipkin --from-beginning ).",0.0,0.0,1.0,0.0
zipkin,"As a result, I of course don't see any trace information in the Zipkin UI.",0.0,0.0,1.0,0.0
zipkin,Following are the dependencies I have as part of the producer service:,0.0,0.0,1.0,0.0
zipkin,These are the dependency overrides I had to make after pulling in the  spring-cloud-stream-binder-kafka11  dependency per the instructions  at the bottom of the Spring Cloud Stream project page .,0.0,0.0,1.0,0.0
zipkin,"I also took a look at the instructions for  Sleuth with Zipkin via RabbitMQ or Kafka , and I think I have that part correct.",0.0,0.0,1.0,0.0
zipkin,The documentation states  If you want Sleuth over RabbitMQ add the spring-cloud-starter-zipkin and spring-rabbit dependencies.,0.0,0.085,0.915,0.0772
zipkin,"It specifically mentions  spring-cloud-starter-zipkin  is needed for RabbitMQ, but I added it even though I'm using Kafka since it didn't work without this dependency either.",0.0,0.0,1.0,0.0
zipkin,Any ideas on what I'm missing or have configured incorrectly to capture Sleuth traces and send them to the Zipkin server using Kafka?,0.091,0.0,0.909,-0.296
zipkin,"I'm trying to trace services in Openstack Mitaka using osprofiler, but i'm having some issues.",0.0,0.0,1.0,0.0
zipkin,It seems it's not possible to trace nova service in Mitaka using osprofiler (correct me if i'm wrong).,0.0,0.0,1.0,0.0
zipkin,So i was thinking of using Zipkin.,0.0,0.0,1.0,0.0
zipkin,Can anyone tell me if Zipkin integrates with openstack mitaka?,0.0,0.0,1.0,0.0
zipkin,I have a Java Spring-Cloud-based microservice integrated with RabbitMQ using Spring Boot Starter AMQP (extract from a  pom.xml  below):,0.0,0.0,1.0,0.0
zipkin,Now I would like to connect this service to the Zipkin monitoring using Sleuth.,0.0,0.172,0.828,0.3612
zipkin,"According to the  documentation , when AMQP support is enabled, Sleuth sends all its data through a RabbitMQ queue.",0.0,0.144,0.856,0.4019
zipkin,For some reason I would like to disable this default behaviour and send data via HTTP.,0.0,0.152,0.848,0.3612
zipkin,Probably there is one magic property which I cannot find.,0.0,0.0,1.0,0.0
zipkin,Do you know how I can force my application to send Sleuth-related data via HTTP to a Zipkin Server (also a Spring Boot application with  @EnableZipkinServer  annotation)?,0.0,0.0,1.0,0.0
zipkin,"In addition I would like to mention that after removing the AMQP support everything works fine, i.e.",0.0,0.35,0.65,0.7184
zipkin,Zipkin receives tracing data via HTTP.,0.0,0.0,1.0,0.0
zipkin,"Moreover, setting both  spring.zipkin.collector.http.enabled: true  and  spring.zipkin.collector.amqp.enabled: false  (and  spring.zipkin.collector.rabbitmq.enabled: false ) does not help.",0.132,0.164,0.703,0.1386
zipkin,I'm using  Spring Cloud Stream  binder from the Edgware release to send Kafka messages.,0.0,0.0,1.0,0.0
zipkin,I'm also using  Spring Sleuth  with  Zipkin .,0.0,0.0,1.0,0.0
zipkin,Spring embeds headers into the Kafka message using a custom class  EmbeddedHeaderUtils .,0.0,0.0,1.0,0.0
zipkin,This causes a problem for some non-Spring consumers of the message who would have to deal with this custom decoding.,0.13,0.0,0.87,-0.4019
zipkin,My Question:  Is there a way to configure Spring with a custom encoder/decoder for message headers (e.g.,0.0,0.0,1.0,0.0
zipkin,plain JSON)?,0.0,0.0,1.0,0.0
zipkin,Or possibly use Kafka Headers?,0.0,0.0,1.0,0.0
zipkin,Ideally any custom implementation needs to work with Spring Sleuth and Zipkin.,0.0,0.203,0.797,0.4215
zipkin,I've been having a look at the latest Finchley release to see if Kafka headers will be supported but not sure about that.,0.101,0.068,0.83,-0.2006
zipkin,Does ELK stack provide micro service and network latency monitoring in kibana?,0.0,0.0,1.0,0.0
zipkin,Zipkin  provides details bout service request and service response duration.,0.0,0.0,1.0,0.0
zipkin,In behind ELS stack should trace span events:,0.0,0.0,1.0,0.0
zipkin,cs - Client Sent,0.0,0.0,1.0,0.0
zipkin,sr - Server Received,0.0,0.0,1.0,0.0
zipkin,ss - Server Sent,0.0,0.0,1.0,0.0
zipkin,cr - Client Received,0.0,0.0,1.0,0.0
zipkin,I'm trying to create a Zipkin 1.31.1 server using Spring Boot 1.3.5.RELEASE to build a fat executable JAR with with Tomcat 8.0.33 embedded in it.,0.0,0.087,0.913,0.2732
zipkin,This is failing with the following error message:,0.5,0.0,0.5,-0.7184
zipkin,as described in  Spring Boot Enable Async Supported Like in web.xml  even with the suggested fix.,0.0,0.255,0.745,0.5859
zipkin,"After setting breakpoints in the debugger, I found that the problem is the same as described in",0.153,0.0,0.847,-0.4019
zipkin,How to Make LogbackValve async Supported,0.0,0.315,0.685,0.3182
zipkin,which wasn't answered and ultimately had the following improvement request created:,0.0,0.357,0.643,0.6124
zipkin,ch.qos.logback.access.tomcat.LogbackValve is not async-supported,0.0,0.0,1.0,0.0
zipkin,Does anyone have any recommendations how I can workaround this issue?,0.0,0.0,1.0,0.0
zipkin,I need help either:,0.0,0.574,0.426,0.4019
zipkin,OR,0.0,0.0,1.0,0.0
zipkin,Any help you can provide would be much appreciated.,0.0,0.462,0.538,0.7184
zipkin,Thanks!,0.0,1.0,0.0,0.4926
zipkin,I'm debugging calls made from my express app to another micro-service on my network.,0.0,0.0,1.0,0.0
zipkin,I'm receiving 401 errors and I need to get full raw http logs to give to my security team for analysis.,0.105,0.105,0.789,0.0
zipkin,I'm looking for some advice on tracking HTTP calls from a micro-service I have deployed on Pivotal Cloud Foundry.,0.0,0.0,1.0,0.0
zipkin,I've been doing some research and ran across tools like Zipkin and OpenTracing etc.. but those appear to be more about debugging latency and probably do not show HTTP logs.,0.0,0.057,0.943,0.1901
zipkin,I've also tried using Morgan/Winston modules but they do not track internal calls.,0.0,0.0,1.0,0.0
zipkin,"Morgan is currently what I'm using to log out the basic HTTP codes but it doesn't pick up on my calls from inside my app either, just the ones made to the app itself from the browser.",0.0,0.0,1.0,0.0
zipkin,I need to get the full raw HTTP request to assist the security team.,0.0,0.167,0.833,0.34
zipkin,I'm using the default logging output with morgan (STDOUT).,0.0,0.0,1.0,0.0
zipkin,I've console logged the headers to see the headers but would like to get them out in a slightly more readable format.,0.0,0.14,0.86,0.5023
zipkin,Does Spring cloud sleuth support WebserviceTemplate?,0.0,0.351,0.649,0.4019
zipkin,I mean - I have a service which makes 2 service calls - One using RestTemplate and another using Webservicetemplate.,0.0,0.0,1.0,0.0
zipkin,The Rest call is getting displayed in Zipkin and the Soap call using Webservicetemplate is not.,0.0,0.0,1.0,0.0
zipkin,Do I have to add @NewSpan to all my soap calls ?,0.0,0.0,1.0,0.0
zipkin,Is it not automatically done like Resttemplate?,0.26,0.0,0.74,-0.2755
zipkin,I am using spring-amqp via spring-integration.,0.0,0.0,1.0,0.0
zipkin,"By the way, when using spring-cloud-sleuth-zipkin, the following error occurred.",0.231,0.0,0.769,-0.4019
zipkin,"spring-cloud-starter-zipkin:1.0.9 
spring-integration-core:4.2.9 
spring-amqp:1.5.6",0.0,0.0,1.0,0.0
zipkin,"In Brave (zipkin tracer), we attach state read by interceptors by controlling the Dispatcher's ExecutorService.",0.0,0.195,0.805,0.5267
zipkin,It works like this..,0.0,0.455,0.545,0.3612
zipkin,This works for synchronous requests (since they don't use the dispatcher thread anyway) and normal asynchronous requests.,0.0,0.0,1.0,0.0
zipkin,"It doesn't work when there are more in-flight connections than permitted, because asynchronous requests are pushed into a ready queue before they are executed.",0.0,0.102,0.898,0.3612
zipkin,"The ready queue is not processed by the calling thread of the request, so the re-attach won't work.",0.0,0.128,0.872,0.3612
zipkin,"When an interceptor runs on a delayed request, it cannot see its calling span which breaks tracing.",0.112,0.0,0.888,-0.2263
zipkin,I was thinking that maybe the application interceptor might not have this problem.,0.221,0.0,0.779,-0.481
zipkin,"If the host/connection limit was enforced (via the mentioned queue) at the network level, I would be able to coordinate the state by dual-registering an interceptor.",0.0,0.0,1.0,0.0
zipkin,This interceptor could re-attach the state without relying on thread locals by mapping application/network level requests.,0.0,0.0,1.0,0.0
zipkin,"Sadly, this doesn't work because the host/connection limit is enforced (via the mentioned queue) at the application level, so I'm stumped.",0.123,0.0,0.877,-0.4215
zipkin,I would like to be able to trace requests especially when they are backlogged.,0.0,0.172,0.828,0.3612
zipkin,Any ideas?,0.0,0.0,1.0,0.0
zipkin,"Hats off to brianm for finding this problem, btw",0.281,0.0,0.719,-0.481
zipkin,"I've successfully used Zipkin with Hadoop Htrace in 2.6.0 x32, on Ubuntu 14.04.",0.0,0.211,0.789,0.4939
zipkin,"Now I want to use it with Hadoop 2.7.3., but I can't even enable Htrace tracing with this hadoop version.",0.0,0.063,0.937,0.0387
zipkin,"The setup for HTrace in 2.6.0 is different from 2.7.3, as it can be seen here- 2.6.0  and here- 2.7.3 .",0.0,0.0,1.0,0.0
zipkin,In 2.6.0 I'd have this line in the namenode log file :,0.0,0.0,1.0,0.0
zipkin,I have nothing like that in 2.7.3 Namenode log file.,0.209,0.0,0.791,-0.2755
zipkin,"Because of not having success with Zipkin, I tried to use the LocalFileSpanReceiver as described in the online tutorial:",0.15,0.0,0.85,-0.4585
zipkin,"The /var/log/hadoop/ exists, with 777 rights on it, but nothing...",0.0,0.0,1.0,0.0
zipkin,The TracingFsShell example compiles and runs with the following modification:,0.0,0.0,1.0,0.0
zipkin,As it can be found in the source code of hadoop in  hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/SpanReceiverHost.java  although the online tutorial does not use that method signature.,0.0,0.0,1.0,0.0
zipkin,(Source  diff ),0.0,0.0,1.0,0.0
zipkin,"The environment is the same for both Hadoop versions, java 1.7.",0.0,0.0,1.0,0.0
zipkin,"Also, hadoop is compiled from source, as the Ubuntu 14.04 is x32 bit.",0.0,0.0,1.0,0.0
zipkin,"Hadoop is deployed in fully-distributed mode, using lxc containers.",0.0,0.0,1.0,0.0
zipkin,core-site.xml  for Zipkin ( Zipkin params  here ):,0.318,0.0,0.682,-0.4215
zipkin,In a microservice environment I see two main benefits from tracing requests through all microservice instances over an entire business process.,0.0,0.126,0.874,0.3818
zipkin,"With  Zipkin  there is a tool, which addresses the first issue.",0.0,0.0,1.0,0.0
zipkin,But how can tracing be used to unveil failures in your microservice landscape?,0.25,0.0,0.75,-0.6124
zipkin,"I definitely want to trace all error afflicted spans, but not each request, where nothing went wrong.",0.172,0.302,0.526,0.408
zipkin,As mentioned  here  a custom Sampler could be used.,0.0,0.0,1.0,0.0
zipkin,"Alternatively, you may register your own Sampler bean definition and programmatically make the decision which requests should be sampled.",0.0,0.0,1.0,0.0
zipkin,"You can make more intelligent choices about which things to trace, for example, by ignoring successful requests, perhaps checking whether some component is in an error state, or really anything else.",0.137,0.18,0.684,0.4005
zipkin,"So I tried to implement that, but it doesn't work or I used it wrong.",0.257,0.0,0.743,-0.631
zipkin,"So, as the blog post suggested I registered my own Sampler:",0.0,0.0,1.0,0.0
zipkin,"And in my controller I create a new Span, which is being tagged as an error if an exception raises",0.13,0.101,0.769,-0.1531
zipkin,"Now, this doesn't work.",0.0,0.0,1.0,0.0
zipkin,If I request /calc/a the method Sampler.isSampled(Span) is being called before the Controller method throws a NumberFormatException.,0.0,0.0,1.0,0.0
zipkin,"This means, when isSampled() checks the Span, it has no tags yet.",0.167,0.0,0.833,-0.296
zipkin,And the Sampler method is not being called again later in the process.,0.0,0.0,1.0,0.0
zipkin,"Only if I open the Sampler and allow every span to be sampled, I see my tagged error-span later on in Zipkin.",0.0,0.091,0.909,0.2263
zipkin,In this case Sampler.isSampled(Span) was called only 1 time but HttpZipkinSpanReporter.report(Span) was executed 3 times.,0.0,0.0,1.0,0.0
zipkin,"So what would the use case look like, to transmit only traces, which have error spans ?",0.141,0.13,0.729,-0.0516
zipkin,"Is this even a correct way to tag a span with an arbitrary ""error_"" tag ?",0.0,0.0,1.0,0.0
zipkin,Update : I have pushed the code to  my repo  so people can take a look there to see what may be going wrong.,0.134,0.0,0.866,-0.4767
zipkin,"Edit : I'm almost sure it's the client code NOT POSTing any stats to the server, but neither guides below explain how should this be enabled: is there a configuration setting that I am missing?",0.082,0.044,0.875,-0.3174
zipkin,I have been following the quick starts on both  OpenZipkin  and  Spring Sleuth : I have a running Zipkin server from  docker-zipkin  using the  docker-compose  and Cassandra as the backend:,0.0,0.0,1.0,0.0
zipkin,I have created and run the  Spring Sleuth sample app  and it seems to be configured correctly to trace calls:,0.0,0.1,0.9,0.25
zipkin,The logs seem to show that the traces ought to be logged:,0.0,0.0,1.0,0.0
zipkin,"However, the UI does not show any traces at all, no matter what I do.",0.144,0.072,0.784,-0.2732
zipkin,The weird thing is that the  localhost:9411/trace  does show a bunch of traces (they seem to be mostly from Zipkin itself) but there are none from the  zipkin-demo  app.,0.048,0.0,0.952,-0.09
zipkin,"I believe this due to the demo app not sending the traces to the host, but I'm just using  Spring's example app , so what can I be doing wrong?",0.138,0.0,0.862,-0.631
zipkin,I would like to store the spring cloud sleuth messages that come in my zipkin server (ala @EnableZipkinStreamServer) into a NoSql store.,0.0,0.116,0.884,0.3612
zipkin,I know that the original zipkin impl used cassandra which would work for me but I am curious about say MongoDb or CouchBase.,0.0,0.195,0.805,0.5574
zipkin,I looked in the documents ( http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_zipkin_consumer ) and saw that you can use spring-boot-starter-jdbc for this and it will write into the MySQL DB instance that you define.,0.0,0.0,1.0,0.0
zipkin,I don't see an API/SPI for putting this into anything else.,0.0,0.0,1.0,0.0
zipkin,Is there one and what is it.,0.0,0.0,1.0,0.0
zipkin,I can implement the NoSql portion myself.,0.0,0.0,1.0,0.0
zipkin,"This is a continuation of the  shopping cart sample , where we have an external API that allows checkout from a shopping cart.",0.0,0.0,1.0,0.0
zipkin,"To recap, we have a flow where we create an empty shopping, add line item(s) and finally checkout.",0.095,0.111,0.794,0.0772
zipkin,"All the operations above, happen as enrichments through HTTP calls to an external service.",0.0,0.0,1.0,0.0
zipkin,We would like to add line items concurrently (as part of the add line items) call.,0.0,0.143,0.857,0.3612
zipkin,Our current configuration looks like this:,0.0,0.333,0.667,0.3612
zipkin,"To add line items in parallel, we are using an executor channel.",0.0,0.0,1.0,0.0
zipkin,"However, they still seem to be getting processed sequentially when seen in zipkin:",0.0,0.0,1.0,0.0
zipkin,What are we doing wrong?,0.437,0.0,0.563,-0.4767
zipkin,The source for the whole project is on  github  for reference.,0.0,0.0,1.0,0.0
zipkin,Thanks!,0.0,1.0,0.0,0.4926
zipkin,I've had a search over the internet but can't seem to find any straightforward instructions on how to use the Thrift protocol from behind a proxy.,0.0,0.0,1.0,0.0
zipkin,To give you a bit of background - we have a Zipkin instance setup ( https://github.com/twitter/zipkin ) that uses a Cassandra instance ( http://cassandra.apache.org/ ) to store Zipkin traces.,0.0,0.0,1.0,0.0
zipkin,Our intention is to negotiate over the thrift protocol to a collector that is then responsible for writing traces to Cassandra.,0.0,0.108,0.892,0.3182
zipkin,What conditions have to be in place for us to negotiate successfully via our corporate proxy?,0.0,0.176,0.824,0.4939
zipkin,Do we just have to set certain proxy properties when trying to negotiate or do we have to set something else up that allows this negotiation to happen?,0.0,0.072,0.928,0.2732
zipkin,Any help people can give in this direction with regards to resources and/or an answer would be greatly appreciated.,0.0,0.27,0.73,0.7425
zipkin,I am trying to install Zipkin on CentOS.,0.0,0.0,1.0,0.0
zipkin,"When I try to run  bin/collector , I get the following errors:",0.231,0.0,0.769,-0.34
zipkin,I have installed Java 7 and Scala.,0.0,0.0,1.0,0.0
zipkin,Note: These errors are from a second run of bin/collector.,0.231,0.0,0.769,-0.34
zipkin,"The first run downloaded libraries, compiled the scala files and then displayed the erorrs, however they were the same errors.",0.112,0.0,0.888,-0.34
zipkin,While the  apollo-opentracing  node library allows Apollo Server to create trace/span information for forwarding to e.g.,0.0,0.123,0.877,0.2732
zipkin,"Zipkin, it only seems to work for GraphQL mutations and queries.",0.0,0.0,1.0,0.0
zipkin,"I would like to trace Graphql subscriptions, which includes:",0.0,0.263,0.737,0.3612
zipkin,I'm having difficulty identifying suitable callbacks (or other mechanisms) to help achieve all of these things; if anyone has any advice or pointers that can offer me that would be most appreciated.,0.064,0.167,0.769,0.5984
zipkin,"Can anyone explain how to troubleshoot whether the tracingservice we configured for ambassador-gateway is working or not, we actually created the tracingService for our ambassador-gateway service as mentioned here  https://www.getambassador.io/docs/latest/topics/running/services/tracing-service/  .",0.055,0.057,0.888,0.0155
zipkin,"Later we restarted the ambassador-gateway pod and checked the logs, we found these",0.0,0.0,1.0,0.0
zipkin,"we haven't found any other info regarding tracing service apart from the above two logs, we had sent some requests to ambassador-gateway ,the related traces were not showing up in the zipkin ui we configured , can some please help me how to troubleshoot this issue?",0.0,0.139,0.861,0.7003
zipkin,TracingServiceConfiguration:,0.0,0.0,1.0,0.0
zipkin,Let's say I have a NodeJS program that has  two  separate instances of an express server running.,0.0,0.0,1.0,0.0
zipkin,"I've been able to instrument a program like this via  open telemetry , and have my spans sent/exported successfully to Zipkin.",0.0,0.251,0.749,0.6908
zipkin,All I needed to do is/was add code like the following to the start of my program.,0.0,0.143,0.857,0.3612
zipkin,and make sure that the express and http plugins were/are installed,0.0,0.187,0.813,0.3182
zipkin,This all works great -- except for  one thing .,0.0,0.339,0.661,0.6249
zipkin,Open Telemetry sees both my express services running as the same  service-main  service.,0.0,0.0,1.0,0.0
zipkin,When I instrumented these services  directly  with Zipkin -- I would add the Zipkin middleware to each running express server,0.0,0.0,1.0,0.0
zipkin,"Each tracer could be instantiated with its own service name, which allowed each service to have its individual name and show up as a different service in Zipkin.",0.0,0.0,1.0,0.0
zipkin,"
( /main ,  /hello , and  /goobye  are all service via a different express service in the above URL)",0.0,0.0,1.0,0.0
zipkin,Is this sort of thing (instrumenting two services in one program) possible with Open Telemetry?,0.0,0.0,1.0,0.0
zipkin,Or would I need to separate these two services out into separate programs in order to have each services have an individual name?,0.0,0.0,1.0,0.0
zipkin,"This question is less about solving a particular problem, and more about understanding the semantics of Open Telemetry.",0.136,0.107,0.757,-0.1477
zipkin,I'm trying to run Zipkin on Kubernetes cluster.,0.0,0.0,1.0,0.0
zipkin,This is my Deployment file:,0.0,0.0,1.0,0.0
zipkin,"when I call  /health endpoint from other deployment  inside of my cluster with pod's ip, I can access to it and this is my output:",0.0,0.0,1.0,0.0
zipkin,"but when I try to access with service name, I can't.",0.0,0.0,1.0,0.0
zipkin,this is the output of  kubectl describe svc zipkin,0.0,0.0,1.0,0.0
zipkin,"and:
    kubectl logs zipkin-9898fcf7f-2nmkx",0.0,0.0,1.0,0.0
zipkin,can anybody help me?,0.0,0.474,0.526,0.4019
zipkin,I have a very big problem I'm struggling with for 3 days.,0.452,0.0,0.548,-0.6983
zipkin,I use docker swarm on the remote server.,0.0,0.0,1.0,0.0
zipkin,20 microservices are in the same network NetA and stack StackA.,0.0,0.0,1.0,0.0
zipkin,Now I want to add Zipkin and Sleuth to my microservices to trace all requests.,0.0,0.091,0.909,0.0772
zipkin,All microservices are made by docker-compose file that looks like:,0.0,0.217,0.783,0.3612
zipkin,Now the question is - HOW to ADD Zipkin Server?,0.0,0.0,1.0,0.0
zipkin,I've added Zipkin server from Docker Hub Image.,0.0,0.0,1.0,0.0
zipkin,"NOW my ZIpkin Service is:
- in a separate network ZIPN
- in a separate stack ZIPST",0.0,0.0,1.0,0.0
zipkin,What should I do to send data do Zipkin by all my microservices?,0.0,0.0,1.0,0.0
zipkin,"What URL should i send in properties file:
spring.zipkin.base-url= http://zipkinserver_network_zipkin_server:9411/",0.0,0.0,1.0,0.0
zipkin,"Should it be maybe:
-container name (like my_zipkin_server) - but I use swarm so container name changes dynamically?",0.0,0.178,0.822,0.5023
zipkin,- network name?,0.0,0.0,1.0,0.0
zipkin,I added an additional network NetA to my Zipkin container but it didn't solved my problem - there are no traces in my Zipkin UI.,0.18,0.103,0.717,-0.281
zipkin,"Please help me, I spent 4 days with this problem without any success.",0.338,0.276,0.386,-0.2785
zipkin,Zipkin server should be in a separate Stack because will be used by different applications.,0.0,0.0,1.0,0.0
zipkin,There is only ONE case when Zipkin works: when I set Zipkin container name:,0.0,0.0,1.0,0.0
zipkin,I am using spring boot in a project and currently exploring the logging behaviour.,0.0,0.0,1.0,0.0
zipkin,For that I'm using the zipkin service.,0.0,0.0,1.0,0.0
zipkin,I have exported my logs to a json file using proper logback.xml:,0.0,0.0,1.0,0.0
zipkin,Is there a way so that I could insert a jsonObject in my message part of the log.,0.0,0.0,1.0,0.0
zipkin,Something like:,0.0,0.714,0.286,0.3612
zipkin,I have tried searching a way extensively but to no avail.,0.259,0.0,0.741,-0.4215
zipkin,Is it even possible?,0.0,0.0,1.0,0.0
zipkin,"I am wondering if I can use BAM and CEP to monitor requests from client, and even find the bottleneck of the service.",0.0,0.0,1.0,0.0
zipkin,"I found zipkin, a project that could do this, but the base of my application is WSO2, I don't want to get other projects from scratch.",0.057,0.0,0.943,-0.0857
zipkin,We are going to have a high load on telemetry service.,0.0,0.0,1.0,0.0
zipkin,"I'm looking for solutions, which be able to scale collector and backend(zipkin)",0.0,0.134,0.866,0.1779
zipkin,There is   solution for scaling zipkin.,0.0,0.315,0.685,0.3182
zipkin,Seems simple - just use internal balancing,0.0,0.0,1.0,0.0
zipkin,"But, I can't find examples for using multiple openTelemetry collectors.",0.0,0.0,1.0,0.0
zipkin,"There is no troubles to run several instances of collector, but how can I say &quot;myApp&quot; to balance beetwin them?",0.175,0.0,0.825,-0.3818
zipkin,There is no such option in exptorters.,0.268,0.0,0.732,-0.296
zipkin,What is the right way to scale such system?,0.0,0.0,1.0,0.0
zipkin,i want to implement distributed tracing(using sleuth-otel) in microservice and it should export the trace details to Google cloud tracing Api(not zipkin).,0.0,0.061,0.939,0.0772
zipkin,"application structure is like this,
service1 -&gt; pubsub -&gt; service2 -&gt; pubsub -&gt; service3
i tried the following",0.0,0.135,0.865,0.3612
zipkin,"A) In service1,",0.0,0.0,1.0,0.0
zipkin,"my dobut in this is,",0.0,0.0,1.0,0.0
zipkin,=&gt; why i did't see the application name that i mentioned in application.properties.,0.0,0.0,1.0,0.0
zipkin,Is there any other thing i missed?,0.306,0.0,0.694,-0.296
zipkin,=&gt; i also want to change this default traceId with my own value.,0.0,0.27,0.73,0.4019
zipkin,is it possible?,0.0,0.0,1.0,0.0
zipkin,if yes what should i do for that?,0.0,0.31,0.69,0.4019
zipkin,(sample code pls),0.0,0.0,1.0,0.0
zipkin,B) in service2,0.0,0.0,1.0,0.0
zipkin,doubt,1.0,0.0,0.0,-0.3612
zipkin,=&gt; why i did't get the same traceId logged in service1 here?,0.0,0.0,1.0,0.0
zipkin,(it doesn't log spanId aswell),0.0,0.0,1.0,0.0
zipkin,=&gt; How to get the same traceId in service2?,0.0,0.0,1.0,0.0
zipkin,"=&gt; To export the traces to google cloud trace API, this dependency is enough or do i need to add any config or anything?",0.0,0.0,1.0,0.0
zipkin,Micronaut has a zipkin tracing library where you can easily override the zipkin server http endpoint like this:,0.0,0.246,0.754,0.5994
zipkin,"Now by default it adds  DEFAULT_PATH = &quot;/api/v2/spans&quot;  to it, but for new relic the entire path should be like:",0.0,0.153,0.847,0.5023
zipkin,"I tried with Bean replacement and factories, but I just cannot find a proper clean solution.",0.171,0.175,0.653,0.0163
zipkin,The only solution I have found is by copying the entire  public final class HttpClientSender extends Sender  class  as  public final class NewRelicSender extends Sender  and just modifying the constructor:,0.0,0.169,0.831,0.5106
zipkin,"It does work, but I feel like there is a better way to do this.",0.0,0.392,0.608,0.7964
zipkin,I am copying almost 300 lines of code to replace 1.,0.0,0.0,1.0,0.0
zipkin,What would be the Micronaut way of doing this?,0.0,0.0,1.0,0.0
zipkin,"All,
I am trying to integrate OpenTelemetry collector with Prometheus in a .Net application.",0.0,0.0,1.0,0.0
zipkin,I am using OTLP Exporter in the application and running OpenTelemetry Collector as a windows service on same system.,0.0,0.0,1.0,0.0
zipkin,Please find the below 'config.yaml' I am using for collector,0.0,0.223,0.777,0.3182
zipkin,Here is the 'prometheus.yml',0.0,0.0,1.0,0.0
zipkin,I don't see the metrics exported to Prometheus.,0.0,0.0,1.0,0.0
zipkin,I am creating some counters programatically in the application and looking for the same in Prometheus.,0.0,0.136,0.864,0.296
zipkin,"Please note, I tried to export traces to Zipkin and it worked fine.",0.0,0.291,0.709,0.4767
zipkin,Any help is greatly appreciated.,0.0,0.677,0.323,0.7425
zipkin,Prometheus UI target health is showing 'UP' as below.,0.0,0.0,1.0,0.0
zipkin,"I have 4 spring-boot applications (A, B, C and D).",0.0,0.0,1.0,0.0
zipkin,The lifecycle of a transaction is as follows :,0.0,0.0,1.0,0.0
zipkin,"Now for a single transaction I would expect a single trace across all four application, but instead I get",0.0,0.0,1.0,0.0
zipkin,"I would have uploaded the pictures to show the zipkin spans, but for some reason I am not able to do so.",0.0,0.0,1.0,0.0
zipkin,All the above applications are Spring boot applications and they utilize spring-cloud-sleuth for producing transactions traces.,0.0,0.0,1.0,0.0
zipkin,I am relying on spring boot's autoconfiguration and these are the properties that I have set in all the applications:,0.0,0.0,1.0,0.0
zipkin,I am not able to understand what's exactly happening here.,0.0,0.0,1.0,0.0
zipkin,Why the spans are scattered across 2 traces and not one?,0.0,0.0,1.0,0.0
zipkin,I am using spring-boot 2.3.3 and spring-cloud-dependencies Hoxton.SR8.,0.0,0.0,1.0,0.0
zipkin,I have a beginner question with Docker Compose.,0.0,0.0,1.0,0.0
zipkin,I am trying to extend the  docker-compose-slim.yml  example file  from  Zipkin GitHub repository .,0.0,0.134,0.866,0.1779
zipkin,I need to change it so that it can include a simple FastAPI app that I have written.,0.0,0.0,1.0,0.0
zipkin,"Unfortunately, I cannot make them connect to each other.",0.255,0.0,0.745,-0.34
zipkin,"FastAPI gets rejected when it attempts to send a POST request to the Zipkin container, even though they are both connected to the same network with explicit links and port mapping defined in the YAML file.",0.088,0.0,0.912,-0.5106
zipkin,"However, I am able to connect to both of them from the host, however.",0.0,0.0,1.0,0.0
zipkin,Could you please tell me what I have done wrong?,0.25,0.185,0.565,-0.2023
zipkin,Here is the error message:,0.403,0.0,0.597,-0.4019
zipkin,Here is the Docker Compose YAML file:,0.0,0.0,1.0,0.0
zipkin,Here is the Dockerfile:,0.0,0.0,1.0,0.0
zipkin,"First, we've thought that problem is in the incompatible version of Brave and Zipkin.",0.149,0.188,0.663,0.1779
zipkin,"We have fixed it and now it's exactly like in the snippet above, but the problem remain.",0.175,0.086,0.739,-0.4215
zipkin,While debugging we've found out that the problem only appears when in Braves class  ThreadLocalCurrentTraceContext  is creating a new Span in method,0.105,0.198,0.698,0.34
zipkin,with  local.get()  which return not null value.,0.253,0.0,0.747,-0.2584
zipkin,local  is  final ThreadLocal&lt;TraceContext&gt;  variable and it only works when get method returns null value.,0.0,0.146,0.854,0.34
zipkin,Any idea how could we solve this problem or other possibilities where can be problem?,0.312,0.09,0.599,-0.6652
zipkin,Thanks.,0.0,1.0,0.0,0.4404
zipkin,my spring boot microservices run currently with Spring-Boot 2.2.9.RELEASE,0.0,0.0,1.0,0.0
zipkin,I also still use,0.0,0.0,1.0,0.0
zipkin,dependencies.,0.0,0.0,1.0,0.0
zipkin,Now I wanted switch to Spring Boot 2.3.2.RELEASE.,0.0,0.0,1.0,0.0
zipkin,I can compile the code without any errors.,0.0,0.253,0.747,0.2584
zipkin,After start the service I see this,0.0,0.0,1.0,0.0
zipkin,INFO: HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform],0.0,0.0,1.0,0.0
zipkin,log message.,0.0,0.0,1.0,0.0
zipkin,There are now errors.,0.444,0.0,0.556,-0.34
zipkin,And then the deployment process stopped.,0.275,0.0,0.725,-0.2263
zipkin,Nothing happens.,0.0,0.0,1.0,0.0
zipkin,At the moment I have no idea why it's not running.,0.196,0.0,0.804,-0.296
zipkin,Has someone some tip?,0.0,0.0,1.0,0.0
zipkin,"I am trying to use Spring Cloud CLI to start eureka, configserver and zipkin with this command:",0.0,0.0,1.0,0.0
zipkin,but I get this message:,0.0,0.0,1.0,0.0
zipkin,It seems like the CLI can't find some packages in the maven repos.,0.0,0.172,0.828,0.3612
zipkin,Is there a solution for this?,0.0,0.365,0.635,0.3182
zipkin,"I have 3 spring boot applications ,  spring-cloud-gateway  ,  spring-boot-resource-server  and  spring-boot-authentication-server .",0.0,0.0,1.0,0.0
zipkin,"My spring-boot-resource-server is protected via Oauth.So whenever an api is requested , it is first authenticated by authentication server using ( check_token )  enpoint.",0.0,0.127,0.873,0.4404
zipkin,"So now,i can see the request flow from gateway to resource server in zipkin as single flow",0.0,0.0,1.0,0.0
zipkin,Request-&gt; Gateway -&gt; Resource Server,0.0,0.0,1.0,0.0
zipkin,but the ( check_token ) request shows up as a separate request.,0.0,0.0,1.0,0.0
zipkin,Resource Server -&gt; Auth Server,0.0,0.0,1.0,0.0
zipkin,If my understanding is right it should be a part of the previous request,0.0,0.0,1.0,0.0
zipkin,Request-&gt; Gateway -&gt; Resource Server -&gt;  Auth Server,0.0,0.0,1.0,0.0
zipkin,Am i missing something ?,0.524,0.0,0.476,-0.296
zipkin,I searched on the web and found 2 results :,0.0,0.0,1.0,0.0
zipkin,"From the above link i think i need to register TraceFilter before my OauthFilter, but i don't seem to find any Trace filter in current release.",0.0,0.0,1.0,0.0
zipkin,I am using  Greenwich.SR3  in all 3 applications.,0.0,0.0,1.0,0.0
zipkin,Update,0.0,0.0,1.0,0.0
zipkin,I have printed filter chain if it helps.,0.0,0.302,0.698,0.3818
zipkin,update 2 :,0.0,0.0,1.0,0.0
zipkin,Updated Spring Cloud to Hoxton.SR4,0.0,0.0,1.0,0.0
zipkin,Sample Application,0.0,0.0,1.0,0.0
zipkin,"I am creating a zipkin server, and i have seen tutorial which annotated the boostrap class with @EnableZipkinServer and others with @EnableZipkinStreamServer.",0.0,0.109,0.891,0.296
zipkin,I am having a hard time understand what is difference between these two.,0.123,0.0,0.877,-0.1027
zipkin,Are they interchangeable?,0.0,0.0,1.0,0.0
zipkin,I have Spring cloud sleuth(and also zipkin) configured in my application.,0.0,0.0,1.0,0.0
zipkin,"I have a controller which calls service, which in turn calls repository and finally database.",0.0,0.0,1.0,0.0
zipkin,"The setup works fine, Sleuth is generating span id's and It is visible in zipkin as well.",0.0,0.206,0.794,0.4404
zipkin,I wanted to try creating span's across multiple internal beans and methods.,0.0,0.18,0.82,0.296
zipkin,I came across  Managing Spans with Annotations .,0.0,0.0,1.0,0.0
zipkin,This does not seem to work.,0.0,0.0,1.0,0.0
zipkin,"When I use any of the annotations mentioned here like  @NewSpan  or  @ContinueSpan , Autowiring stops working.",0.094,0.146,0.76,0.2263
zipkin,My service class which is autowired in Controller is  null .,0.0,0.0,1.0,0.0
zipkin,If I remove these annotations everything works again.,0.0,0.0,1.0,0.0
zipkin,I am using.,0.0,0.0,1.0,0.0
zipkin,spring-boot 2.2.5.RELEASE,0.0,0.0,1.0,0.0
zipkin,"spring-cloud.version Hoxton.SR3 
I have these dependencies in my pom",0.0,0.0,1.0,0.0
zipkin,Here is a sample code,0.0,0.0,1.0,0.0
zipkin,And my Service class is like,0.0,0.333,0.667,0.3612
zipkin,"My guess is, Spring-Aop has something to do with it.",0.0,0.0,1.0,0.0
zipkin,Any idea?,0.0,0.0,1.0,0.0
zipkin,"I have been working on spring cloud sleuth to push traces to zipkin, We are pushing traces to Kafka with the help of  spring-cloud-starter-stream-kafka  and  spring-cloud-sleuth-stream",0.0,0.101,0.899,0.4019
zipkin,Below are the dependencies I have added to my application,0.0,0.0,1.0,0.0
zipkin,And my application property just have kafka broker which is by default pushing traces to  sleuth  topic  spring.cloud.stream.kafka.binder.brokers=locahost:9092,0.0,0.0,1.0,0.0
zipkin,Everything is working fine as of now  but I am not able to figure out if we can start the application if Kafka is down ?,0.0,0.057,0.943,0.1027
zipkin,"From the logs I see application starts, but since it will be not able to connect to kafka we will be not able to access any of the endpoints (Added some bits of logs below).",0.0,0.0,1.0,0.0
zipkin,Looking for this solution because pushing traces is not the primary function of my application.,0.0,0.158,0.842,0.3869
zipkin,We use a lot of Finagle  Filter s and  Service s in our code.,0.0,0.0,1.0,0.0
zipkin,"However, we don't support Zipkin in our infrastructure.",0.244,0.0,0.756,-0.3089
zipkin,"At times there's a need to  trace  an incoming request across the chain of Filters/Services, especially in the face of concurrent requests.",0.0,0.0,1.0,0.0
zipkin,What's the most non-intrusive way to get such a functionality?,0.0,0.0,1.0,0.0
zipkin,"Goal: 
Would be great if Finagle itself provided an additional  apply  method like so:",0.0,0.355,0.645,0.765
zipkin,Then the subclasses could opt-in to using this method instead.,0.0,0.0,1.0,0.0
zipkin,"The last argument being the identifier that every Filter/Service in the chain could ""append to"".",0.152,0.0,0.848,-0.3612
zipkin,Example:,0.0,0.0,1.0,0.0
zipkin,The goal is to have something that can uniquely trace/log the series of filter/services effectively.,0.0,0.172,0.828,0.4404
zipkin,How may I go about by extending Finagle via traits/monkey-patching?,0.0,0.0,1.0,0.0
zipkin,Or is this not doable?,0.0,0.0,1.0,0.0
zipkin,Added dependency for cloud sleuth and zipkin dependency of version below in pom.xml,0.0,0.0,1.0,0.0
zipkin,Simple spring boot class.,0.0,0.0,1.0,0.0
zipkin,Should i add filter to set new fields?,0.0,0.0,1.0,0.0
zipkin,resultant LOG is no showing the newly added property :,0.216,0.0,0.784,-0.296
zipkin,"
does something i am missing ..?",0.355,0.0,0.645,-0.296
zipkin,I have a system that distributing tasks with queues.,0.0,0.0,1.0,0.0
zipkin,I have N services that communicate with each other with N queues.,0.0,0.0,1.0,0.0
zipkin,My system diagram looks like this:,0.0,0.333,0.667,0.3612
zipkin,"As you can see, the outcome results performance depends on multiple services.",0.0,0.0,1.0,0.0
zipkin,"So, diagnose failures (or performance issues) is hard task - it's very hard to find which part of the chain was the problem.",0.328,0.0,0.672,-0.7769
zipkin,I was looking for a tool that will provide me a map of all the processing traces.,0.0,0.0,1.0,0.0
zipkin,Something like this:,0.0,0.556,0.444,0.3612
zipkin,"With a visual diagram like this, I can find which part of the chain was the problem.",0.148,0.137,0.714,-0.0516
zipkin,Easily.,0.0,1.0,0.0,0.34
zipkin,"So, I found a few solutions that aim to solve this problem:",0.225,0.271,0.504,-0.0929
zipkin,"Unfortunately, it looks very difficult to:",0.565,0.0,0.435,-0.6361
zipkin,Am I wondering if I can have an easier solution for this simple problem?,0.161,0.304,0.536,0.34
zipkin,I am using spring-cloud-sleuth-zipkin and spring-cloud-starter-sleuth as dependency.,0.0,0.0,1.0,0.0
zipkin,When i call a netflix FeignClient call TraceId changes...,0.0,0.0,1.0,0.0
zipkin,When i call TesterClient the traceId changes?,0.0,0.0,1.0,0.0
zipkin,How could i preserve the same traceId?,0.0,0.0,1.0,0.0
zipkin,I want to pass the span across multiple threads and want to get trace.,0.0,0.191,0.809,0.1531
zipkin,I have different worker threads.,0.0,0.0,1.0,0.0
zipkin,while running I am getting  You may have forgotten to close or detach null,0.137,0.0,0.863,-0.2263
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,WorkerSpan1.java,0.0,0.0,1.0,0.0
zipkin,WorkerSpan2.java,0.0,0.0,1.0,0.0
zipkin,here I am passing span because tracer.getCurrentSpan() is returning null.,0.0,0.0,1.0,0.0
zipkin,and my controller,0.0,0.0,1.0,0.0
zipkin,TestController.java,0.0,0.0,1.0,0.0
zipkin,While running I am not getting trace in zipkin and getting warning:,0.194,0.0,0.806,-0.34
zipkin,I also want to know how to pass trace information across different threads.,0.0,0.106,0.894,0.0772
zipkin,in my microservices architecture application I'd like to add Sleuth and Zipkin server (image from Docker Hub).,0.0,0.135,0.865,0.3612
zipkin,Everything works fine locally - each microservice sends data to Zipkin server.,0.0,0.153,0.847,0.2023
zipkin,The problem is more complicated when I deployed all microservices on the server  -Zipkin Web UI is empty - no traces.,0.295,0.0,0.705,-0.6908
zipkin,"In application.properties we can explicitly set the url to the Zipkin server:
spring.zipkin.base-url:  http://10.0.44.1:9411/",0.0,0.0,1.0,0.0
zipkin,I thought that all containers can communicate each other in different stacks/networks but it's not true.,0.176,0.0,0.824,-0.4585
zipkin,What should I do to publish my Zipkin server/container to be availabe for all containers from all stacks/networks?,0.0,0.0,1.0,0.0
zipkin,Is there a possibility to do this using Portainer?,0.0,0.0,1.0,0.0
zipkin,Thank you in advance,0.0,0.455,0.545,0.3612
zipkin,I and my team is stuck in a very silly case of HttpClientErrorException let me first give overview of my work scenario.,0.093,0.065,0.841,-0.1548
zipkin,I have a micro-service stack with following things:,0.0,0.0,1.0,0.0
zipkin,In scheduler I am using spring Scheduler with cron,0.0,0.0,1.0,0.0
zipkin,The purpose of code in prepareDataForSync is to obtain data about each hotel and check current state and if any change is deducted pass it on to third party.,0.0,0.088,0.912,0.4019
zipkin,Now here comes the real problem:,0.351,0.0,0.649,-0.4019
zipkin,I make a rest service call to obtain list of hotels from my scheduler:,0.0,0.0,1.0,0.0
zipkin,All these services are running in docker environment with each service having own container and communicating via docker networking.,0.0,0.0,1.0,0.0
zipkin,"Now when I start the services including Hotel and Scheduler, everything work fine for few hours but then I get following exception in my logs and service is no more syncing with third party.",0.076,0.135,0.789,0.2846
zipkin,At line 1159 I have a service call only:,0.0,0.0,1.0,0.0
zipkin,"I checked logs in corresponding Hotel service, logs there displays the service request was received and data was collected and written to response stream but as the exception shows I never received any response and get this Exception.",0.0,0.0,1.0,0.0
zipkin,Logs from Hotel service:,0.0,0.0,1.0,0.0
zipkin,"If I restart the Scheduler service it starts working again, but again in few hours i have same issue.",0.0,0.0,1.0,0.0
zipkin,"As a workaround currently I have set up a cron on the server to restart the service every 2 hours but it is really a bad workaround, I can't rely on this in the production and need to get to the root of problem.",0.195,0.0,0.805,-0.867
zipkin,"I have have googled and tried to go through any HttpClientErrorException based question, but nothing made sense to me.",0.0,0.0,1.0,0.0
zipkin,Please let me know if more information is required from my end.,0.0,0.173,0.827,0.3182
zipkin,EDIT:,0.0,0.0,1.0,0.0
zipkin,Docker Stats Output:,0.0,0.0,1.0,0.0
zipkin,0b7d20c5a566        schedular            0.12%               1.365GiB / 31.41GiB   4.34%               0B / 0B             3.27MB / 0B         64,0.0,0.0,1.0,0.0
zipkin,TOP Output inside container,0.0,0.458,0.542,0.368
zipkin,i have s basic spring boot/cloud application based on,0.0,0.0,1.0,0.0
zipkin,But i need spring-cloud-sleuth-zipkin with at least 1.3.x.,0.0,0.0,1.0,0.0
zipkin,By importing 1.3.5.RELEASE i get a strange error.,0.529,0.0,0.471,-0.5423
zipkin,It seems like the same dependency creates a convergence.,0.0,0.434,0.566,0.5574
zipkin,Is that easily solavble?,0.0,0.444,0.556,0.34
zipkin,I have Spring Cloud Sleuth (2.0.2.RELEASE) working within a (partially) reactive class in some web based request/response system.,0.0,0.0,1.0,0.0
zipkin,The code is something like this:,0.0,0.333,0.667,0.3612
zipkin,"As there are quite many responses coming in all the time, this method is being called several hundreds of times for one single request.",0.0,0.0,1.0,0.0
zipkin,I suspect that this call with the  .publishOn  leads to hundreds and thousands of  async  spans in Zipkin (see attached screenshot).,0.104,0.0,0.896,-0.296
zipkin,At least I assume that the spans are from that because it is what I understand from  the documentation,0.0,0.0,1.0,0.0
zipkin,"So my first question would be:
How can I associate a name for such async threads?",0.0,0.0,1.0,0.0
zipkin,I don't have a place to put  @SpanName  here.,0.0,0.0,1.0,0.0
zipkin,"As a follow up, is there any way to NOT collect these spans?",0.0,0.0,1.0,0.0
zipkin,"I don't need them, they fill up our Zipkin storage, but I also don't want to disable reactive or Sleuth in general since it is needed in other places ...",0.047,0.0,0.953,-0.0857
zipkin,Screenshot from Zipkin,0.0,0.0,1.0,0.0
zipkin,I'm learning how to track my distributed processes through all the microservices.,0.0,0.0,1.0,0.0
zipkin,"I've been playing with Sleuth, Zipkin and different microservices, and it works fantastic!",0.0,0.341,0.659,0.69
zipkin,But when I try to do the same in a project interacting between the different dependencies I can not create the same behavior.,0.105,0.0,0.895,-0.3007
zipkin,This image show how currently is working different microservices.,0.0,0.0,1.0,0.0
zipkin,This is the diagram of microservices:,0.0,0.0,1.0,0.0
zipkin,And this image show how works an application with dependencies.,0.0,0.0,1.0,0.0
zipkin,This is the diagram of application with dependencies:,0.0,0.0,1.0,0.0
zipkin,"I wonder, is it possible to create the same behavior using dependencies as with microservices?",0.0,0.139,0.861,0.2732
zipkin,"I am developing Microservices specifically  ""zipkin-service"" .",0.0,0.0,1.0,0.0
zipkin,I have taken a reference from  link:  https://www.baeldung.com/tracing-services-with-zipkin .,0.0,0.0,1.0,0.0
zipkin,Could anyone please guide what's the issue ?,0.0,0.277,0.723,0.3182
zipkin,"When I added below dependencies, then I get the",0.0,0.0,1.0,0.0
zipkin,Error:,1.0,0.0,0.0,-0.4019
zipkin,Project build error: 'dependencies.dependency.version' for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing.,0.45,0.0,0.55,-0.5994
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,ZipkinServiceApplication.java,0.0,0.0,1.0,0.0
zipkin,I am running  open-zipkin  with docker-compose for testing purposes.,0.0,0.0,1.0,0.0
zipkin,The end goal is to have open-zipkin running so that I can successfully do a  curl localhost:9411/prometheus  on the zipkin container itself and view prometheus metrics.,0.0,0.131,0.869,0.5367
zipkin,How can I expose the metrics like this?,0.176,0.275,0.549,0.2263
zipkin,For reference the zipkin portion of the  docker-compose file looks like this:,0.0,0.185,0.815,0.3612
zipkin,I am using spring boot in a project and currently exploring the logging behaviour.,0.0,0.0,1.0,0.0
zipkin,For that I'm using the zipkin service.,0.0,0.0,1.0,0.0
zipkin,My  logback.xml  is as follows:,0.0,0.0,1.0,0.0
zipkin,"Now for what I have understood, to log in json, you need a custom logger implementation which I have done as follows:",0.0,0.0,1.0,0.0
zipkin,My message class is:,0.0,0.0,1.0,0.0
zipkin,Now in my controller class I'm using my custom logger as:,0.0,0.0,1.0,0.0
zipkin,My logs end up in kibana as:,0.0,0.0,1.0,0.0
zipkin,Now instead of this I would want my  message  part of the logs as a  json  entry.,0.0,0.085,0.915,0.0772
zipkin,Where am I going wrong?,0.508,0.0,0.492,-0.4767
zipkin,I have tried searching a way extensively but to no avail.,0.259,0.0,0.741,-0.4215
zipkin,Is it even possible?,0.0,0.0,1.0,0.0
zipkin,I am using Docker Toolbox 1803 under Windows 10 64bit.,0.0,0.0,1.0,0.0
zipkin,"I am trying to run  docker zipkin  in my local system, and got failure when ran the following command in Toolbox GuickStart Terminal.",0.136,0.0,0.864,-0.5106
zipkin,docker-compose -f docker-compose.yml  -f docker-compose-cassandra.yml up,0.0,0.0,1.0,0.0
zipkin,The following line of docker-compose.yml caused the problem.,0.278,0.0,0.722,-0.4019
zipkin,The failure info is like following:,0.337,0.255,0.408,-0.2023
zipkin,"UPDATE : I have set env variable  COMPOSE_CONVERT_WINDOWS_PATHS=1 , it still does not work.",0.0,0.0,1.0,0.0
zipkin,Update : Switched to Docker for Windows and WSL2.,0.0,0.0,1.0,0.0
zipkin,"The experience is great enough, but sadly I have to give up my VirutalBox.",0.214,0.148,0.638,-0.2846
zipkin,I am using spring cloud finchley.rc2 with spring boot version 2 along with sleuth and zipkin.,0.0,0.0,1.0,0.0
zipkin,I have a facade layer which uses reactor project.,0.0,0.0,1.0,0.0
zipkin,Facade calls services parallel and each service store some tracing info in rabbit mq.,0.0,0.0,1.0,0.0
zipkin,Issue is I am seeing in zipkin some spans like,0.0,0.238,0.762,0.3612
zipkin,How can i stop such traces from being captured,0.239,0.0,0.761,-0.296
zipkin,"I have a virtual machine centos(ver 7.4) on win10 machine, I do not use AWS, Google cloud service, nor Azure.",0.0,0.0,1.0,0.0
zipkin,I put master and node in one machine.,0.0,0.0,1.0,0.0
zipkin,"My original problem domain have 5 components, I configure them as ClusterIP, so they could communicate with each other(eureka, config,api,uaa,zipkin).",0.129,0.11,0.762,-0.1027
zipkin,Now I only need api talk outside.,0.0,0.0,1.0,0.0
zipkin,"But for short, I make two components for convenience (api and eureka).",0.0,0.0,1.0,0.0
zipkin,"But now, api needs to receive from outside of cluster.",0.0,0.0,1.0,0.0
zipkin,So that I configure ingress.,0.0,0.0,1.0,0.0
zipkin,"When ingress, I need to configure rbac.",0.0,0.0,1.0,0.0
zipkin,I put my yaml file here with error message.,0.278,0.0,0.722,-0.4019
zipkin,eureka_pod.yaml,0.0,0.0,1.0,0.0
zipkin,eureka_svc.yaml,0.0,0.0,1.0,0.0
zipkin,api_pod.yaml:,0.0,0.0,1.0,0.0
zipkin,api_svc.yaml:,0.0,0.0,1.0,0.0
zipkin,ingress_nginx_role_rb.yaml:,0.0,0.0,1.0,0.0
zipkin,nginx_default_backend.yaml:,0.0,0.0,1.0,0.0
zipkin,ingress_nginx_ctl.yaml:,0.0,0.0,1.0,0.0
zipkin,ingress_nginx_res.yaml:,0.0,0.0,1.0,0.0
zipkin,"when I try  172.16.100.88:31080/uaa/login , (my virtual machine current IP is  172.16.100.88 ) it says following connection problme:",0.0,0.0,1.0,0.0
zipkin,"I check ingress-nginx pod, it seems request has not yet reached nginx.",0.115,0.0,0.885,-0.0762
zipkin,"When I login in to pod gearbox-rack-api-gateway, I could see clearly it redirects to the page I expected.",0.0,0.162,0.838,0.4019
zipkin,so there must be some configuration wrong in my yaml files.,0.237,0.0,0.763,-0.4767
zipkin,=================================================================,0.0,0.0,1.0,0.0
zipkin,"In my virtual machine, I type  telnet localhost 31080 , rejected.",0.292,0.0,0.708,-0.5106
zipkin,but  telnet -6 localhost 31080  succeed.,0.0,0.462,0.538,0.6486
zipkin,And  netstat -anp | less  find 31080 binding kube-proxy.,0.0,0.0,1.0,0.0
zipkin,"I put  sysctl -q -w net.ipv6.conf.all.disable_ipv6=1  and  sysctl -w net.ipv6.conf.default.disable_ipv6=1  in my starting script, but got same result.",0.0,0.0,1.0,0.0
zipkin,==============================================================,0.0,0.0,1.0,0.0
zipkin,Yesterday question about Ipv6 is stupid.,0.405,0.0,0.595,-0.5267
zipkin,I misconfigured /etc/hosts.,0.0,0.0,1.0,0.0
zipkin,"Now I have telnet localhost 31080 work, but when I do curl  http://localhost:31080/uaa/login , it hangs there for long time.",0.0,0.0,1.0,0.0
zipkin,So pod is listening.,0.0,0.0,1.0,0.0
zipkin,"When I issue the command curl  http://localhost:31080/uaa/login , at the same time, I check several pods' log.",0.0,0.0,1.0,0.0
zipkin,Log has shown no error and has no log to say the port 31080 has been sent request.,0.321,0.0,0.679,-0.7269
zipkin,I checked ingress-nginx pod logs: I paste some here.,0.0,0.0,1.0,0.0
zipkin,"My own wild guess is that ingress nginx pod's namespace is kube-system, nginx service's name space is default, my-ingress's namespace is default.",0.0,0.0,1.0,0.0
zipkin,nginx-default-backend's name space is kube-system.,0.0,0.0,1.0,0.0
zipkin,Whether cross namespace traffic is forbidden.,0.359,0.0,0.641,-0.4215
zipkin,Experts what kind of logs do you need?,0.0,0.0,1.0,0.0
zipkin,=======================================================,0.0,0.0,1.0,0.0
zipkin,"After define all ingress controller, ingress resources as default namespace, now I did get move further.",0.0,0.0,1.0,0.0
zipkin,Now nginx redirect my  http request to https:  request.,0.0,0.0,1.0,0.0
zipkin,How to disable this feature will make my ingress working wholly.,0.0,0.0,1.0,0.0
zipkin,"I notice ingress-nginx receive the request, shown as below:",0.0,0.0,1.0,0.0
zipkin,Is there a way to customize the Span inject and extractor for spring cloud sleuth 2?,0.0,0.0,1.0,0.0
zipkin,In the documentation of the version 1.2 i found a way that is not available on the new version(2).,0.0,0.0,1.0,0.0
zipkin,"I think is because now its use Zipkin brave to take care of Span, right?",0.0,0.355,0.645,0.765
zipkin,https://cloud.spring.io/spring-cloud-sleuth/1.2.x/multi/multi__customizations.html#_example,0.0,0.0,1.0,0.0
zipkin,"I tried to back to use the stable version(1.3.3) of spring cloud sleuth, but when i use the bom for the project its make conflict in the spring boot version that i am using(2.0).",0.088,0.048,0.864,-0.3291
zipkin,Its compactible with the spring boot version 2?,0.0,0.0,1.0,0.0
zipkin,"I am using the spring cloud sleuth to make trace of services on my company, but i have a version of tracing on others services that is not compactible with the opentracing headers, so i want to change the headers of http messages to make the new services compactibles with the current tracing headers that i have in the others components.",0.0,0.037,0.963,0.2759
zipkin,Thanks,0.0,1.0,0.0,0.4404
zipkin,I have setup a demo project using spring-cloud-stream with RabbitMQ-binders and spring-cloud-sleuth.,0.0,0.0,1.0,0.0
zipkin,I have a scheduled spring-cloud-stream source:,0.0,0.0,1.0,0.0
zipkin,"and then a middle tier, similar to the final sink layer looking like:",0.0,0.185,0.815,0.3612
zipkin,I nicely see the TraceId and SpanId automagically passed through the RabbitMQ queues across all processes down to the sink process in the logfiles:,0.0,0.116,0.884,0.4404
zipkin,"so at this point the final sink tier wants to (explicitly) signal, that this trace (instance of my business process) is  finish ed here.",0.0,0.0,1.0,0.0
zipkin,How do I signal that the whole Trace has finished?,0.0,0.0,1.0,0.0
zipkin,"I only found  sleuthTracer.currentSpan().finish();  but this only finishes the Span ... not explicitly signalling, that the whole trace is finalized here.",0.0,0.0,1.0,0.0
zipkin,Did I miss something?,0.444,0.0,0.556,-0.1531
zipkin,"(quite new to zipkin, brave and sleuth)",0.0,0.362,0.638,0.5267
zipkin,"there's a microservice  with spring-boot 1.5  which uses the  Feign  to communicate with others services, also there's  spring-cloud-starter-zipkin  which wrapped all calls through the Feign and sends tracing to zipkin server.",0.0,0.0,1.0,0.0
zipkin,"The thing is i don't wanna wrap all calls and trace them, there're only several most important to do that.",0.0,0.104,0.896,0.2716
zipkin,How can i exclude some calls(methods) with Feign from tracing or exlude some whole Feign client(interface)?,0.119,0.0,0.881,-0.2263
zipkin,I am trying to use feature from  Spring Cloud  (ex: Feign or Zipkin Client) in a  Spring Boot  micro-service.,0.0,0.0,1.0,0.0
zipkin,Whenever I introduce the Spring Cloud dependencies into the pom.xml I get the following error at startup:,0.162,0.0,0.838,-0.4019
zipkin,Below is a sample pom.xml causing this.,0.0,0.0,1.0,0.0
zipkin,I am currently on  Spring Boot 2.0.0.RELEASE  and  Spring Cloud Finchley.M8 .,0.0,0.0,1.0,0.0
zipkin,What am I doing wrong?,0.508,0.0,0.492,-0.4767
zipkin,Should I switch to another version of Spring Cloud?,0.0,0.0,1.0,0.0
zipkin,"UPDATE : It's not me who's doing it wrong, even  Spring Initializr  projects demonstrate this issue.",0.181,0.0,0.819,-0.4767
zipkin,To repro:,0.0,0.0,1.0,0.0
zipkin,pom.xml:,0.0,0.0,1.0,0.0
zipkin,I also want to trace network latency from  App -&gt; Service1 -&gt; App -&gt; Service2 .,0.0,0.091,0.909,0.0772
zipkin,"Spring sleuth works perferct to find latency between Sleuth aware services say  services1-&gt;service2  as I can see CS,SR tags in Zipkin.",0.0,0.0,1.0,0.0
zipkin,Now I also want to track network Latency between devices and other area which we are hopping but those are not Sleuth aware services.,0.0,0.05,0.95,0.0387
zipkin,How can I do that.,0.0,0.0,1.0,0.0
zipkin,Any pointers would be appreciated .,0.0,0.452,0.548,0.5106
zipkin,"From the Docs,",0.0,0.0,1.0,0.0
zipkin,"That means that the current span has Trace-Id set to X, Span-Id set to
D. It also has emitted  Client Sent event .",0.0,0.0,1.0,0.0
zipkin,Are there any specific headers which needs to be sent from App to Server?,0.0,0.0,1.0,0.0
zipkin,I know Sleuth does it out of the box using Rest template.,0.0,0.0,1.0,0.0
zipkin,How can I do the same thing from Apps or other non sleuth services.,0.0,0.0,1.0,0.0
zipkin,I am trying to integrate my Application with Spring sleuth.,0.0,0.0,1.0,0.0
zipkin,I am able to do a successfull integration and I can see spans getting exported to Zipkin.,0.0,0.0,1.0,0.0
zipkin,I am exporting zipkin over http.,0.0,0.0,1.0,0.0
zipkin,"Spring boot version - 1.5.10.RELEASE 
Sleuth - 1.3.2.RELEASE 
Cloud- Edgware.SR2",0.0,0.0,1.0,0.0
zipkin,But now I need to do this in a more controlled way as application is already running in production and people are scared about the overhead which sleuth can have by adding @NewSpan on the methods.,0.104,0.0,0.896,-0.5927
zipkin,I need to decide on runtime wether the Trace should be added or not (Not talking about exporting).,0.0,0.0,1.0,0.0
zipkin,Like for actuator trace is not getting added at all.,0.0,0.217,0.783,0.3612
zipkin,I assume this will have no overhead on the application.,0.216,0.0,0.784,-0.296
zipkin,Putting  X-B3-Sampled = 0  is not exporting but adding tracing information.,0.0,0.0,1.0,0.0
zipkin,Something like skipPattern property but at runtime.,0.0,0.226,0.774,0.1901
zipkin,Always export the trace if service exceeds a certain threshold or in case of Exception.,0.0,0.139,0.861,0.2732
zipkin,If I am not exporting Spans to zipkin then will there be any overhead by tracing information?,0.0,0.0,1.0,0.0
zipkin,What about this solution ?,0.0,0.467,0.533,0.3869
zipkin,I guess this will work in sampling specific request at runtime.,0.0,0.0,1.0,0.0
zipkin,We are trying to add tracing to micro services so it can viewed in the google Stackdriver UI.,0.0,0.0,1.0,0.0
zipkin,"We are using Java Springboot apps deployed into Kubernetes containers, each microservices communicates over http.",0.0,0.0,1.0,0.0
zipkin,We’ve seen that there is Sleuth and Zipkin which if we move our RestTemplate to a bean will work.,0.0,0.0,1.0,0.0
zipkin,However we don’t really want to have to deploy a zipkin pod in each of our containers or create new zipkin collector pods.,0.0,0.156,0.844,0.4005
zipkin,Ideally we would like to get this working using just the google cloud tracing sdk with using sleuth/zipkin.,0.0,0.249,0.751,0.6486
zipkin,Playing around with the sdk we are able to get data into Stackdriver using the google cloud grpc library which just sends the data directly from the application into Stackdriver.,0.0,0.058,0.942,0.2023
zipkin,"The problem we have now is that we can send the trace id to a downstream micro service but we cannot seem to find a way to create a new span on the same trace id, it always creates a new one.",0.044,0.126,0.83,0.5346
zipkin,I can’t seem to find any documentation on how to do this.,0.0,0.0,1.0,0.0
zipkin,Surely what we are doing is what this library was build for?,0.0,0.209,0.791,0.4404
zipkin,Any pointers help on this would be great.,0.0,0.531,0.469,0.7783
zipkin,Adding a bit more info......,0.0,0.0,1.0,0.0
zipkin,"I cannot supply actual code because this is my problem, I can't actually find what I want to do.",0.15,0.072,0.778,-0.34
zipkin,Let me try to explain with a bit of code/pseudo code.,0.0,0.0,1.0,0.0
zipkin,"So lets assume this scenario, I have 3 microservices, A, B and C.",0.0,0.0,1.0,0.0
zipkin,Hope this makes sense in what I'm trying to do.,0.0,0.244,0.756,0.4404
zipkin,I want to check latencies of RPC every day about CakePHP Application each endpoints running in GKE cluster.,0.0,0.075,0.925,0.0772
zipkin,"I found it is possible using  php google client  or  zipkin server  by reading documents , but I don't know how easy to introduce to our app though both seem tough for me.",0.148,0.0,0.852,-0.5939
zipkin,"In addition, I'm concerned about GKE cluster configuration has StackDriver Trace option though our cluster it sets disabled.Can we trace span if it sets enable?",0.0,0.0,1.0,0.0
zipkin,Could you give some advices?,0.0,0.0,1.0,0.0
zipkin,im trying to connect my Spring Boot app to a Cassandra 2.2.8 cluster on EC2 instances (2 nodes).,0.0,0.0,1.0,0.0
zipkin,my use is tracing with Sleuth and Zipkin.,0.0,0.0,1.0,0.0
zipkin,"when the tracing start, the driver always point to localhost :",0.0,0.0,1.0,0.0
zipkin,com.datastax.driver.core.Cluster : New Cassandra host localhost/127.0.0.1:9042 added,0.0,0.0,1.0,0.0
zipkin,this is my application.properties,0.0,0.0,1.0,0.0
zipkin,and this is my pom.xml :,0.0,0.0,1.0,0.0
zipkin,I have got problem running my js application in browser.,0.252,0.0,0.748,-0.4019
zipkin,"I have created client-server application, and I am using  Zipkin  to trace communications between them.",0.0,0.143,0.857,0.25
zipkin,This is the client that uses Node.js  require() :,0.0,0.0,1.0,0.0
zipkin,I am using Browserify to bundle dependencies for use in the browser.,0.0,0.0,1.0,0.0
zipkin,I run  browserify CujojsClient.js -o bundle.js  to create bundle for the browser.,0.0,0.174,0.826,0.2732
zipkin,"If I run browserify with  --node --ignore-missing  options everything works well in Node.js, but when I run the bundle in the browser (Firefox 45.3.0 on windows) I only got:",0.0,0.058,0.942,0.1406
zipkin,This is the problematic part:,0.42,0.0,0.58,-0.4404
zipkin,My  index.html :,0.0,0.0,1.0,0.0
zipkin,I have added sleuth/zipkin into my project.,0.0,0.0,1.0,0.0
zipkin,"I'm using logback, and by default I get very well formatted logs in my console and files as well.",0.0,0.23,0.77,0.584
zipkin,"I'm also using a logstash appender, and when I look at how kibana presents those logs - I'm not satisfied at all.",0.115,0.0,0.885,-0.3252
zipkin,Here are details:,0.0,0.0,1.0,0.0
zipkin,pom.xml:,0.0,0.0,1.0,0.0
zipkin,My logstash appender:,0.0,0.0,1.0,0.0
zipkin,And this is what I see in kibana:,0.0,0.0,1.0,0.0
zipkin,The only thing that is resolved by the log pattern is the application name.,0.0,0.116,0.884,0.1779
zipkin,Am I missing some configuration?,0.423,0.0,0.577,-0.296
zipkin,Or maybe there's something wrong in my logstash appender?,0.279,0.0,0.721,-0.4767
zipkin,Tracing information do not propagate over kafka messages due to the method SleuthKafkaAspect.wrapProducerFactory() is not triggered.,0.0,0.0,1.0,0.0
zipkin,"On the producer side, the message is correctly sent and the tracing information is correctly logged.",0.0,0.0,1.0,0.0
zipkin,"On consumer side, instead a new traceId and spanId is created.",0.0,0.182,0.818,0.25
zipkin,"The following two logging lines show different values for traceId,spanId (and parentId):",0.0,0.197,0.803,0.4019
zipkin,"In first instance, using Krafdrop and also debugging, I verified that the message header doesn't contains any tracing information.",0.0,0.0,1.0,0.0
zipkin,"After that, I figured out that the method SleuthKafkaAspect.wrapProducerFactory() is never triggered, instead on consumer side the method SleuthKafkaAspect.anyConsumerFactory() is.",0.0,0.0,1.0,0.0
zipkin,The libraries versions used are the following:,0.0,0.0,1.0,0.0
zipkin,The kakfa client library version is 2.4.1 is due to a version downgrade related to production bug on 2.5.1 version of kafka client that increase the cpu usage.,0.0,0.081,0.919,0.3182
zipkin,I also tried to use the following libraries versions combination with no success:,0.138,0.233,0.629,0.3612
zipkin,"We migrated our project to a different spring boot version, from 2.3.0.RELEASE to 2.3.7.RELEASE.",0.0,0.0,1.0,0.0
zipkin,Before everthing was working correctly.,0.0,0.0,1.0,0.0
zipkin,Below the old libraries versions:,0.0,0.0,1.0,0.0
zipkin,We also introduced a log42/log4j (before it was slf4j with logback).,0.0,0.0,1.0,0.0
zipkin,Below the related libraries:,0.0,0.0,1.0,0.0
zipkin,The properties configured are the following:,0.0,0.0,1.0,0.0
zipkin,The configuration class for the ProducerFactory creation is the following:,0.0,0.189,0.811,0.2732
zipkin,My spring boot application class:,0.0,0.0,1.0,0.0
zipkin,"Here you can find the output of command &quot;mvn dependency:tree&quot;
 mvn_dependency_tree.txt",0.0,0.0,1.0,0.0
zipkin,Within next few months 6th edition of Spring in Action is going to be published.,0.0,0.0,1.0,0.0
zipkin,"It is said, it will not contain 3 chapters from 5th edition i.e.",0.0,0.0,1.0,0.0
zipkin,"Circut Breaker,
Eureka Service-Client Discovery, Eureka Server-Client Configuration.",0.0,0.0,1.0,0.0
zipkin,"Instead of this, it will include changes made in Spring Boot 2.4.",0.0,0.0,1.0,0.0
zipkin,"I have
alread heard that Circut Breaker (Hystrix) is outdated, but I wonder what about
rest, especially omitted chapters ?",0.0,0.0,1.0,0.0
zipkin,"I noticed that I can not choose ribbon in newest(2.4.3) Spring Boot version, zipkin also differs from earlies ones.",0.0,0.0,1.0,0.0
zipkin,What is alternative for ribbon in newest version?,0.0,0.0,1.0,0.0
zipkin,I am using  spring-cloud-starter-sleuth:3.0.1  and  spring-cloud-sleuth-zipkin:3.0.1  to generate  traceId  and  spanId  in log file.,0.0,0.0,1.0,0.0
zipkin,I was able to get those in logs using  2.2.7.RELEASE  version.,0.0,0.0,1.0,0.0
zipkin,I have tried using  logback  but not able to have with  3.0.1  version.,0.0,0.0,1.0,0.0
zipkin,"As per 3.0.1 documentation, they have removed Legacy MDC entries but brave  spanId  &amp;  traceId  are there.",0.0,0.223,0.777,0.6808
zipkin,Dependency hierarchy:,0.0,0.0,1.0,0.0
zipkin,traceId &amp; spanId are not generated in log:,0.0,0.0,1.0,0.0
zipkin,I have tried to see this request's tracing on zipkin and able to see it with traceid and spanid:,0.0,0.0,1.0,0.0
zipkin,Can anyone help me to get traceid and spanid in log file using logback/log4j?,0.0,0.172,0.828,0.4019
zipkin,I have a microservices based software architecture.,0.0,0.0,1.0,0.0
zipkin,There is a php application which orchestrates the communication among microservices and the application's whole logic.,0.0,0.0,1.0,0.0
zipkin,I need to simulate the communication between microservices as a graph.,0.0,0.0,1.0,0.0
zipkin,"There will be edges with weights , which will represent the affinities between microservices.",0.0,0.0,1.0,0.0
zipkin,I am searching for a tool in order to collect all messages and their size.,0.0,0.0,1.0,0.0
zipkin,"I have read that there are distibuted tracing systems like Zipkin which i have already deployed, and could accomplish this task.",0.0,0.238,0.762,0.6486
zipkin,"But, i cannot find how to collect the messages i want.",0.0,0.153,0.847,0.1154
zipkin,This is the php library i used for the instrumentation of my app,0.0,0.0,1.0,0.0
zipkin,[https://github.com/openzipkin/zipkin-php],0.0,0.0,1.0,0.0
zipkin,Any ideas about other tools or how to use Zipkin differently to achieve my goal?,0.0,0.0,1.0,0.0
zipkin,I am using Spring Cloud Sleuth to send spans to zipkin when a Spring Boot application sends a message (to RabbitMQ).,0.0,0.0,1.0,0.0
zipkin,I would like to customize the information sent to zipkin to include some extra tags that are populated from certain headers of the outgoing Message e.g.,0.0,0.236,0.764,0.7003
zipkin,the  myCustomTag  below.,0.0,0.0,1.0,0.0
zipkin,Is it possible to do this using Sleuth/brave?,0.0,0.0,1.0,0.0
zipkin,It feels like a messaging equivalent of registering (e.g.),0.0,0.263,0.737,0.3612
zipkin,a bean of type  brave.http.HttpRequestParser  but I couldn't see an obvious way forward.,0.0,0.0,1.0,0.0
zipkin,I use the latest Spring Boot 2.4.0.m3 to setup Spring Sleuth.,0.0,0.0,1.0,0.0
zipkin,"On the Zipkin, the data outflow from a source is directed to &quot;broker&quot;.",0.0,0.0,1.0,0.0
zipkin,The following is the code snap:,0.0,0.0,1.0,0.0
zipkin,where fooService is org.springframework.cloud.stream.messaging.Source data type.,0.0,0.0,1.0,0.0
zipkin,The sink is connected to &quot;Kafka&quot;.,0.0,0.0,1.0,0.0
zipkin,And there isn't a connection between the &quot;broker&quot; and &quot;Kafka&quot;.,0.0,0.0,1.0,0.0
zipkin,"Also, the sink has a data outflow to &quot;Kafka&quot;.",0.0,0.0,1.0,0.0
zipkin,And the following is a code snap:,0.0,0.0,1.0,0.0
zipkin,"To my eyes, there are two errors in the picture.",0.211,0.0,0.789,-0.34
zipkin,One is the label of &quot;broker&quot;.,0.0,0.0,1.0,0.0
zipkin,It should be &quot;Kafka&quot; as well.,0.0,0.296,0.704,0.2732
zipkin,And the sink should have inflow data from Kafka instead of outflow.,0.0,0.0,1.0,0.0
zipkin,Is some additional configuration needed to resolve those two errors?,0.185,0.2,0.615,0.0516
zipkin,"We are working on an IOT platform, which ingests many device parameter
values (time series) every second from may devices.",0.0,0.124,0.876,0.4019
zipkin,"Once ingested the
each JSON (batch of multiple parameter values captured at a particular
instance) What is the best way to track the JSON as it flows through
many microservices down stream in an event driven way?",0.0,0.169,0.831,0.7845
zipkin,"We use spring boot technology predominantly and all the services are
containerised.",0.0,0.0,1.0,0.0
zipkin,"Eg: Option 1 - Is associating UUID to each object and then updating
the states idempotently in Redis as each microservice processes it
ideal?",0.0,0.139,0.861,0.5267
zipkin,"Problem is each microservice will be tied to Redis now and we
have seen performance of Redis going down as number api calls to Redis
increase as it is single threaded (We can scale this out though).",0.067,0.089,0.844,-0.0258
zipkin,Option 2 - Zipkin?,0.0,0.0,1.0,0.0
zipkin,"Note: We use Kafka/RabbitMQ to process the messages in a distributed
way as you mentioned here.",0.0,0.0,1.0,0.0
zipkin,"My question is about a strategy to track
each of this message and its status (to enable replay if needed to
attain only once delivery).",0.0,0.0,1.0,0.0
zipkin,"Let's say a message1 is being by processed
by Service A, Service B, Service C. Now we are having issues to track
if the message failed getting processed at Service B or Service C as
we get a lot of messages",0.084,0.0,0.916,-0.5106
zipkin,"I have the zipkin deployment and service below as you can see zipkin is located under monitoring namespace the, i have an env variable called  ZIPKIN_URL  in each of my pods which are running under default namespace, this varibale takes this URL  http://zipkin:9411/api/v2/spans  but since zipkin is running in another namespace i tried this :",0.0,0.0,1.0,0.0
zipkin,http://zipkin.monitoring.svc.cluster.local:9411/api/v2/spans,0.0,0.0,1.0,0.0
zipkin,i also tried this format :,0.0,0.0,1.0,0.0
zipkin,http://zipkin.monitoring:9411/api/v2/spans,0.0,0.0,1.0,0.0
zipkin,"but when i check the logs of my pods, i see  connection refused exception",0.203,0.0,0.797,-0.4215
zipkin,when i exec into one of my pods and try curl  http://zipkin.tools.svc.cluster.local:9411/api/v2/spans,0.0,0.0,1.0,0.0
zipkin,its shows me  Mandatory parameter is missing: serviceNameroot,0.232,0.137,0.632,-0.2263
zipkin,Here is zipkin resource :,0.0,0.0,1.0,0.0
zipkin,I have adapted the sample from Micronaut Users Guide V1.2.10 chapter Tracing Annotations.,0.0,0.0,1.0,0.0
zipkin,My code looks that like:,0.0,0.385,0.615,0.3612
zipkin,"The question is, why the call for method doubleName is not logged to zipkin (at least it does not show up in the zipkin GUI).",0.0,0.0,1.0,0.0
zipkin,The REST call to address() is logged.,0.0,0.0,1.0,0.0
zipkin,Do only REST calls get logged and no local method calls?,0.18,0.0,0.82,-0.296
zipkin,Actually I don't think that's that the case because the Users Guides sample tells that this should work.,0.0,0.0,1.0,0.0
zipkin,Any ideas?,0.0,0.0,1.0,0.0
zipkin,I am using  Sleuth 2.1.3.,0.0,0.0,1.0,0.0
zipkin,"I want  to add  a custom ""trace  ID"" as ""correlation id"" with alpha numeric  value and want to spit in logs  with  spanid  and  parent id.",0.0,0.192,0.808,0.4588
zipkin,If i use below implementation for creating new custom trace id.,0.0,0.196,0.804,0.296
zipkin,does it  get  printed in logs ?,0.0,0.0,1.0,0.0
zipkin,"I tried  below implementation   but   does not see any custom trace in log 
 https://github.com/openzipkin/zipkin-aws/blob/release-0.11.2/brave-propagation-aws/src/main/java/brave/propagation/aws/AWSPropagation.java",0.0,0.0,1.0,0.0
zipkin,I tried with above code  from  https://cloud.spring.io/spring-cloud-sleuth/reference/html/#propagation  but didnt see any custom trace id in log,0.0,0.0,1.0,0.0
zipkin,I have 2 services which are exchanging events via Kafka.,0.0,0.0,1.0,0.0
zipkin,"First service packs necessary tracing info (headers which are set by brave: traceId, spanId and so on) right in message payload.",0.0,0.145,0.855,0.5267
zipkin,Consumer of the second service retrieve this information and create appropriate consumer span.,0.0,0.149,0.851,0.2732
zipkin,I can see the whole tracing including both services in Zipkin.,0.0,0.0,1.0,0.0
zipkin,But I can't see appropriate tracing information in logs on consumer side.,0.0,0.0,1.0,0.0
zipkin,On the producer side all is ok.,0.0,0.268,0.732,0.296
zipkin,"The code of consumer side (as the tracings are sending to zipkin I think the most of configuration is ok, but the tracing information just don't propagate to log context...):",0.0,0.054,0.946,0.1531
zipkin,What can you advice?,0.0,0.0,1.0,0.0
zipkin,Maybe I should attach something else but it seems like it is pretty much the only code I have around sleuth and brave.,0.0,0.403,0.597,0.9209
zipkin,"I'm using Spring cloud stream binder kafka, Edgware.SR4 release.",0.0,0.0,1.0,0.0
zipkin,I have set custom headers to a message payload and published it but i can't see those headers in consumer end.,0.0,0.0,1.0,0.0
zipkin,I have used Message object to bind payload and headers.,0.0,0.0,1.0,0.0
zipkin,I have tried adding the property spring.cloud.stream.kafka.binder.headers but it did not work,0.0,0.0,1.0,0.0
zipkin,Producer:,0.0,0.0,1.0,0.0
zipkin,Application.yml,0.0,0.0,1.0,0.0
zipkin,MessageChannelConstants.java,0.0,0.0,1.0,0.0
zipkin,SampleMessageChannels.java,0.0,0.0,1.0,0.0
zipkin,SampleEventPublisher.java,0.0,0.0,1.0,0.0
zipkin,Consumer:,0.0,0.0,1.0,0.0
zipkin,application.yml,0.0,0.0,1.0,0.0
zipkin,MessageChannelConstants.java,0.0,0.0,1.0,0.0
zipkin,SampleMessageChannels.java,0.0,0.0,1.0,0.0
zipkin,SampleEventListener.java,0.0,0.0,1.0,0.0
zipkin,"Below is the Exception I got,",0.0,0.0,1.0,0.0
zipkin,Note: I am using spring cloud sleuth and zipkin dependency as well.,0.0,0.174,0.826,0.2732
zipkin,We are running a java trading application and have around 50 orders per second.,0.0,0.0,1.0,0.0
zipkin,"When an order comes in, it jumps between services and we want to measure latency inside every service and between services by an external service which should gather all data with timestamps and produce distributions with percentiles.",0.0,0.035,0.965,0.0772
zipkin,We want to measure latency for every order in order to find issues and explain them to our members if they have latency-related questions.,0.0,0.053,0.947,0.0772
zipkin,The issue we are facing is a framework to choose to propagate orders from every service to another service with timestamps attached to calculate and produce latencies.,0.0,0.0,1.0,0.0
zipkin,"Given the flow of orders we have, what will be the most promising approach for us?",0.0,0.166,0.834,0.4576
zipkin,"We looked into Zipkin, it also supports gRPC - does it fit our use-case?",0.0,0.312,0.688,0.6124
zipkin,Any other recommendations?,0.0,0.0,1.0,0.0
zipkin,p.s.,0.0,0.0,1.0,0.0
zipkin,we cannot use the transport we use for the business logic as we are going to get rid of it soon.,0.0,0.0,1.0,0.0
zipkin,I am new to node js and was trying to integrate zipkins with my node APi using appmetrics-zipkin npm package.,0.0,0.0,1.0,0.0
zipkin,"Zipkin works fine except when there are multiple http calls in async  parallel method , it gives trace of only the first http call which was finished...I need trace for all the API calls in async parallel......Please help",0.0,0.114,0.886,0.5423
zipkin,I have a dependency  https://mvnrepository.com/artifact/io.zipkin.reporter2/zipkin-sender-okhttp3/2.7.14  declared as,0.0,0.0,1.0,0.0
zipkin,This dependency in it's pom has parent dependency with pom file that declares dependency like this:,0.0,0.143,0.857,0.3612
zipkin,project.groupid equals to io.zipkin.reporter2 and project.version equals to 2.7.14.,0.0,0.0,1.0,0.0
zipkin,So maven should import dependency  &lt;artifactId&gt;zipkin-reporter&lt;/artifactId&gt;  with version equals to 2.7.14.,0.0,0.0,1.0,0.0
zipkin,BUT  it imports versions 2.2.0.,0.0,0.0,1.0,0.0
zipkin,I don't declare this dependency anywhere else and other dependencies don't have this dependency as transative.,0.0,0.0,1.0,0.0
zipkin,"I tried reinstall maven, reclone project from git, invalidate cache and restart IDEA, delete .m2 folder - nothing worked.",0.0,0.0,1.0,0.0
zipkin,The weird thing is that I have other project that is using the very same dependency ( &lt;artifactId&gt;zipkin-sender-okhttp3&lt;/artifactId&gt; ) and all versions are in order how they should be.,0.064,0.0,0.936,-0.1779
zipkin,Any ideas how I can fix it?,0.0,0.0,1.0,0.0
zipkin,Edit: mvn dependency:tree output (I edited out sensitive info): `,0.0,0.0,1.0,0.0
zipkin,"`
All modules have version 2.7.7 as they should be but one module with name  ...-deployment has versions 2.2.0.",0.0,0.0,1.0,0.0
zipkin,It doesn't set explicitly there.,0.0,0.0,1.0,0.0
zipkin,"I have a distributed process which runs across two different servers (A and B) and I get two different log files A.log and B.log, I need this merged into a single 
file.",0.0,0.0,1.0,0.0
zipkin,I have referred to following links but I am unable to get a merged file from the same:,0.0,0.0,1.0,0.0
zipkin,Is there something that I am missing?,0.306,0.0,0.694,-0.296
zipkin,Edit: I need the logs in something along these lines,0.0,0.0,1.0,0.0
zipkin,I am using spring-cloud-sleuth:2.0.1.RELEASE with Spring Webflux.,0.0,0.0,1.0,0.0
zipkin,"The doc talks about logging trace, span, etc using MDC.",0.0,0.0,1.0,0.0
zipkin,It also talks about sending traces to Zipkin via HTTP.,0.0,0.0,1.0,0.0
zipkin,I am interested in logging the trace information in more elaborate way.,0.0,0.213,0.787,0.4019
zipkin,"With every log statement, I want to emit the zipkin traces in the JSON format - very close to what's depicted here:  https://zipkin.io/pages/data_model.html",0.0,0.061,0.939,0.0772
zipkin,What is the best way to accomplish this in sleuth?,0.0,0.467,0.533,0.7906
zipkin,I'm using  spring-cloud-sleuth  and zipkin.,0.0,0.0,1.0,0.0
zipkin,"In the producer, it worked.I can see message in kafka topic   see image .",0.0,0.0,1.0,0.0
zipkin,"but in the consumer,some exception occur.",0.0,0.0,1.0,0.0
zipkin,solve by this issue .,0.0,0.375,0.625,0.2023
zipkin,but next exception cannot solve.,0.321,0.0,0.679,-0.2235
zipkin,project infomation,0.0,0.0,1.0,0.0
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,application.yml,0.0,0.0,1.0,0.0
zipkin,main class,0.0,0.0,1.0,0.0
zipkin,I am facing some errors in a spring boot project where I am using spring integration to connect to RabbitMQ.,0.13,0.0,0.87,-0.34
zipkin,I am doing the configuration for RabbitMQ in XML files like this:,0.0,0.2,0.8,0.3612
zipkin,But I am creating two of each component.,0.0,0.318,0.682,0.4215
zipkin,How to set the primaries ones?,0.0,0.0,1.0,0.0
zipkin,"Now the problem comes here, I was using this version for spring cloud:",0.197,0.0,0.803,-0.4019
zipkin,"And everything was working fine, but if I update the version to:",0.0,0.123,0.877,0.1027
zipkin,This error is coming:,0.474,0.0,0.526,-0.4019
zipkin,And the error comes because of this dependency:,0.278,0.0,0.722,-0.4019
zipkin,If I remove this dependency the error is not coming.,0.252,0.0,0.748,-0.4019
zipkin,You can find an example project to reproduce this scenario.,0.0,0.0,1.0,0.0
zipkin,In the pom file you'll see this:,0.0,0.0,1.0,0.0
zipkin,https://github.com/fjmpaez911/spring-integration-zipkin-cloud,0.0,0.0,1.0,0.0
zipkin,So I need to know how to set a primary configuration for RabbitMQ and in addition I think that could be an issue because this error only comes if I use this version  Edgware.RELEASE,0.097,0.0,0.903,-0.481
zipkin,Am I missing something?,0.524,0.0,0.476,-0.296
zipkin,"I have multiple services, some of which use Hystrix's HystrixObservableCommand to call other services and others use HystrixCommand.",0.0,0.0,1.0,0.0
zipkin,How do I pass on traceIds from the calling service to the Observables in HystrixObservableCommand and also have them be passed on if the fallback is called?,0.0,0.0,1.0,0.0
zipkin,All services are using grpc-java.,0.0,0.0,1.0,0.0
zipkin,Sample code that I have:,0.0,0.0,1.0,0.0
zipkin,WorldCommand.java,0.0,0.0,1.0,0.0
zipkin,I am using Zipkin grpc tracing and MDCCurrentTraceContext to print the traceId and spanId in the logs.,0.0,0.0,1.0,0.0
zipkin,"Both the log entries in the WorldCommand do not print out the trace and span ids, they are called on RxIoScheduler thread.",0.0,0.0,1.0,0.0
zipkin,EDIT,0.0,0.0,1.0,0.0
zipkin,Added ConcurrencyStrategy as suggested by Mike.,0.0,0.0,1.0,0.0
zipkin,HelloService calls two services World and Team.,0.0,0.0,1.0,0.0
zipkin,"The WorldCommand is a HystrixObservableCommand, the TeamCommand is a HystrixCommand.",0.0,0.0,1.0,0.0
zipkin,PreservableContext class,0.0,0.0,1.0,0.0
zipkin,The log in PreservableContexts and CustomHystrixConcurrencyStrategy never get printed.,0.0,0.0,1.0,0.0
zipkin,I am registering the startegy when I start the HelloServer.,0.0,0.0,1.0,0.0
zipkin,EDIT 2,0.0,0.0,1.0,0.0
zipkin,Updated how the Observables are set up:,0.0,0.0,1.0,0.0
zipkin,"I have a weird problem now, the calls to TeamCommand and WorldCommand doesn't complete as in this code is never executed:",0.206,0.0,0.794,-0.5267
zipkin,"Also, if there is a fallback, the hystrix-timer threads doesn't have the MDC anymore.",0.0,0.0,1.0,0.0
zipkin,I have 2 very simple spring-cloud-stream applications.,0.0,0.0,1.0,0.0
zipkin,"Service3, the message producer, sends messages to Service4, the consumer, through the binder-kafka.",0.0,0.0,1.0,0.0
zipkin,And I use spring-cloud-sleuth to trace the spans among them.,0.0,0.0,1.0,0.0
zipkin,But only the spans in Service3 are available in zipkin server.,0.0,0.0,1.0,0.0
zipkin,No span shows for Service4.,0.355,0.0,0.645,-0.296
zipkin,Service3,0.0,0.0,1.0,0.0
zipkin,Service4,0.0,0.0,1.0,0.0
zipkin,Servic4 (message consumer) is not traced,0.0,0.0,1.0,0.0
zipkin,What did I miss?,0.444,0.0,0.556,-0.1531
zipkin,I am trying to trace HTTP calls made through  Async RestTemplate  from a Spring Boot Application.,0.0,0.0,1.0,0.0
zipkin,I have a ZipKin instance running locally to which the microservices in question point to.,0.0,0.0,1.0,0.0
zipkin,"I could see spans recorded at every service in ZipKin UI, however I am not able to see the trace covering all the spans.",0.0,0.0,1.0,0.0
zipkin,With  RestTemplate  the trace is recorded as normal.,0.0,0.0,1.0,0.0
zipkin,i.e.,0.0,0.0,1.0,0.0
zipkin,I am able to see end-to-end via the UI.,0.0,0.0,1.0,0.0
zipkin,"Any pointers will help,
Thanks in advance.",0.0,0.528,0.472,0.6808
zipkin,I would like to know if it is possible to modify  iron-ajax  somehow to use  cujojs-rest  to perform all the requests.,0.0,0.116,0.884,0.3612
zipkin,I would like to use cujojs-rest zipkin instrumentation for tracing in my app.,0.0,0.185,0.815,0.3612
zipkin,Here is an example app using cujojs-rest zipkin instrumentation to generate trace data for Zipkin:  wingtips-cujojs-spark-example,0.0,0.0,1.0,0.0
zipkin,So let's say I have got code like this:,0.0,0.263,0.737,0.3612
zipkin,I would like to achieve the same in  iron-ajax,0.0,0.263,0.737,0.3612
zipkin,I am using  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_adding_to_the_project  for adding Spring Cloud Sleuth without the Zipkin integration,0.0,0.0,1.0,0.0
zipkin,"But in one of the microserives out of three,  does not show the spanid, and token id in logs  after adding dependency for all services (all are http request services, though there are couple of more services which require JMS - on which I need to work)",0.0,0.0,1.0,0.0
zipkin,Service1,0.0,0.0,1.0,0.0
zipkin,Service2,0.0,0.0,1.0,0.0
zipkin,Service3,0.0,0.0,1.0,0.0
zipkin,"Experts, Please suggest what can be done to see the effect in Service2",0.0,0.161,0.839,0.3182
zipkin,I am using Spring Cloud Sleuth + Zipkin to have an insight of the service timing and behaviour.,0.0,0.0,1.0,0.0
zipkin,"The only downside I have found is: when there are several instances of every microservice I haven't found a way to determine which instance Zipkin information is referring to, since it identifies them all by its service name (which is the same for all).",0.048,0.0,0.952,-0.25
zipkin,Is there a way to configure Sleuth to add service-instance dinstintion in Zipkin?,0.0,0.0,1.0,0.0
zipkin,"I am using a script from the following paper (Zipkin, E.F., Royle, J.A., Dawson, D.K., Bates, S., 2010.",0.0,0.0,1.0,0.0
zipkin,Multi-species occurrence models to evaluate the effects of conservation and management actions.,0.0,0.0,1.0,0.0
zipkin,"Biological Conservation 143, 479-484) to estimate bird species occupancy.",0.0,0.344,0.656,0.6369
zipkin,"One of my variables in the detection estimate (the K loop in the code below) is Wind, which is a categorical variable, with levels 1-6.",0.0,0.0,1.0,0.0
zipkin,"I have attempted to use the  dcat  function in OpenBUGS which what I hope is an uniformative prior (beta(1,1)), but OpenBUGS fails with error:",0.257,0.069,0.674,-0.743
zipkin,"Which, when I remove the line  b3[i] ~ dcat(p[i,])#WIND Data  does not happen.",0.0,0.0,1.0,0.0
zipkin,"Any advise on how to specify dcat properly, or how to code categorical variables for WinBUGS/OpenBUGS would be greatly appreciated!",0.0,0.17,0.83,0.5974
zipkin,what are the best practices in tracing of spring boot 2 microservice applications?,0.0,0.276,0.724,0.6369
zipkin,I found some 2 years old tutorials where tracing server was as another spring boot application with following dependencies:,0.0,0.0,1.0,0.0
zipkin,and push traces with following configuration:,0.0,0.0,1.0,0.0
zipkin,and,0.0,0.0,1.0,0.0
zipkin,Is this solution still actual and suitable for production or should we configure standalone docker image of zipkin instead of spring boot app and connect it to ELK stack with logs?,0.0,0.071,0.929,0.3182
zipkin,What do you recommended?,0.0,0.375,0.625,0.2023
zipkin,It will be great if you can provide some example what is recommended approach to handle it in.,0.0,0.269,0.731,0.7096
zipkin,Thank you in advice.,0.0,0.455,0.545,0.3612
zipkin,i use zipkin with kafka,0.0,0.0,1.0,0.0
zipkin,and work log:,0.0,0.0,1.0,0.0
zipkin,"and loop log...
Can anyone tell me what is the reason?",0.0,0.0,1.0,0.0
zipkin,"kafka-server: 0.11.0
kafka-client: 1.0.1
zipkin: 2.10.4",0.0,0.0,1.0,0.0
zipkin,I'm developing event-driven Microservices which I use Java and Scala.,0.0,0.0,1.0,0.0
zipkin,"I used Spring Sleuth and Zipkin for request tracing with Java services, can I use Spring Sleuth with Scala?",0.0,0.0,1.0,0.0
zipkin,if not how can I generate trace id and span id in Scala to be sent to Zipkin.,0.0,0.0,1.0,0.0
zipkin,"In spring and spring-boot there is a lot of ""magic"" that happens just by annotating methods and classes.",0.0,0.0,1.0,0.0
zipkin,"For learning purposes and do-it-yourself stuff I would be interested to have a look at them and so wondering how to find the ""magic code"" that an annotation ""causes"" ...",0.0,0.091,0.909,0.4019
zipkin,"is there a ""cook-book"" on how to find the implementing code of Annotations inside spring jars?",0.0,0.0,1.0,0.0
zipkin,We have integrated NiFi within our product suits.,0.0,0.0,1.0,0.0
zipkin,"We would like to track a user request by ""Trace Id"", which spans across different components.",0.0,0.152,0.848,0.3612
zipkin,Do let me know whether NiFi have some capability to support something similar to ZipKin.,0.0,0.162,0.838,0.4019
zipkin,Thanks,0.0,1.0,0.0,0.4404
zipkin,Senthil.,0.0,0.0,1.0,0.0
appdynamics,"I suggest to look into Gartners magic quadrant and get dynaTrace since it has negligible overhead , less than 1% in production under load.",0.0,0.0,1.0,0.0
appdynamics,Full Disclosure: I currently work for AppDynamics.,0.0,0.0,1.0,0.0
appdynamics,AppDynamics was designed from the ground up for high volume production environments but works equally well in both prod and non-prod.,0.0,0.117,0.883,0.3919
appdynamics,"It's currently running in production in some of the worlds largest mission critical application environments at Netflix, Exact Target, Edmunds, and many others.",0.095,0.0,0.905,-0.3182
appdynamics,Here are a few quotes from existing customers…,0.0,0.0,1.0,0.0
appdynamics,"""It's like a profiler that you can run in production"" -- Leonid Igolnik, Taleo",0.0,0.172,0.828,0.3612
appdynamics,"""We found that the overhead was negligible"" -- Jacob Marcus, Care.com",0.0,0.0,1.0,0.0
appdynamics,"""We wanted a monitoring solution that wouldn't impact our production runway"" -- John Martin, Edmunds",0.0,0.15,0.85,0.3182
appdynamics,AppDynamics overhead is extremely low but I suggest you test it and see for yourself.,0.115,0.0,0.885,-0.177
appdynamics,You can download and use it for free from the AppDynamics website.,0.0,0.231,0.769,0.5106
appdynamics,Good luck in your search for the right APM tool.,0.0,0.424,0.576,0.7096
appdynamics,Yes it will if the application is sensitive to extra GC cycles caused by call stack sampling.,0.0,0.144,0.856,0.4019
appdynamics,The impact will depend on the number of threads and typical call stack depth.,0.0,0.091,0.909,0.0772
appdynamics,"This is not specific to AppDynamics, other call stack sampling solutions such as NewRelic and VisualVM Sampler will have a similar impact.",0.0,0.078,0.922,0.1779
appdynamics,http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_Good_APM_vs_AppDynamics_Bad_APM.pdf,0.0,0.0,1.0,0.0
appdynamics,http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_vs_AppDynamics.pdf,0.0,0.0,1.0,0.0
appdynamics,There are a number of assumptions made by a vendor but the following are most common:,0.0,0.081,0.919,0.0387
appdynamics,TRANSLATION: We assume you have a slow performing database backend.,0.0,0.0,1.0,0.0
appdynamics,TRANSLATION: We assume that you already know your performance hotspots.,0.0,0.0,1.0,0.0
appdynamics,TRANSLATION: We assume that you will not notice tricks used to hide our overhead.,0.113,0.091,0.796,-0.0849
appdynamics,TRANSLATION: We assume that you know little about performance engineering.,0.0,0.0,1.0,0.0
appdynamics,And my favorite (5) is the restriction within a vendors software license on the publication of benchmark results.,0.104,0.149,0.746,0.2263
appdynamics,TRANSLATION: We assume that you blindly accept our claims – unquestionably.,0.0,0.224,0.776,0.3818
appdynamics,"Appdynamics will not slow down your system significant, I was on a usermeeting and they said that they always try to be under 2% cpu usage, thats nothing compared to what you get from them.",0.0,0.053,0.947,0.2023
appdynamics,"They are working with samples per time, so if you have 10 requests per second or 100, they will still take the same amout of your cpu / bandwith / whatever.",0.0,0.0,1.0,0.0
appdynamics,The way that these products generally work is by doing bytecode injection / function interposition / monkey-patching on commonly used libraries and methods.,0.0,0.0,1.0,0.0
appdynamics,"For instance, you might hook into JDBC query methods, servlet base classes, and HTTP client libraries.",0.0,0.0,1.0,0.0
appdynamics,"When a request enters the application, track all the important methods/calls it makes, and log them in some way.",0.0,0.096,0.904,0.2023
appdynamics,"Take the data and crunch it into analytics, charts, and alerts.",0.0,0.0,1.0,0.0
appdynamics,"On top of that, you can start to add in statistical profiling or other options.",0.0,0.114,0.886,0.2023
appdynamics,The tricky things are tracking requests across process boundaries and dealing with the volume of performance data you'll gather.,0.082,0.0,0.918,-0.1531
appdynamics,(I work on this problem at AppNeta),0.342,0.0,0.658,-0.481
appdynamics,"One thing to check out is Twitter Zipkin ( https://github.com/twitter/zipkin ), doesn't support much and pretty early-stage but interesting project.",0.07,0.243,0.687,0.615
appdynamics,Both AppDynamics and New Relic use Standard BCI to monitor the common interfaces (entry and exit points) developers use to build applications (e.g.,0.0,0.0,1.0,0.0
appdynamics,"Servlet, struts, SOAP, JMS, JDBC, ...).",0.0,0.0,1.0,0.0
appdynamics,This provides a basic skeleton of code execution (call graphs) with timing information which represents less than 5% of code that is executed.,0.0,0.0,1.0,0.0
appdynamics,The secret is to then uncover the timing of the remaining 95% code execution during slowdowns without incurring too much overhead in a production JVM.,0.0,0.0,1.0,0.0
appdynamics,AppDynamics uses a combination of in-memory agent analytics and Java API calls to then extract the remaining code execution in real-time.,0.0,0.0,1.0,0.0
appdynamics,This means no custom instrumentation is required or explicit declaration of what classes/methods you want the monitoring solution to instrument.,0.096,0.158,0.746,0.1027
appdynamics,AppDynamics data collection is very different to that of New Relic.,0.0,0.0,1.0,0.0
appdynamics,"For example, with AppDynamics you can get a complete distributed call graph across multiple JVMs for a specific user request, rather than say an aggregate of requests.",0.0,0.0,1.0,0.0
appdynamics,"BCI is a commodity these days, the difference is in the analytics and algorithms used by vendors that trigger diagnostics/call graph information so you end up with the right visibility at the right time to solve problems.",0.07,0.047,0.883,-0.2263
appdynamics,Steve.,0.0,0.0,1.0,0.0
appdynamics,That error means that gradle is unable to resolve the dependency on  com.appdynamics:appdynamics-runtime .,0.166,0.16,0.675,-0.0258
appdynamics,The easiest way to fix this problem is to use the AppDynamics libraries from maven central rather than the  adeum-maven-repo  directory.,0.125,0.112,0.762,-0.0836
appdynamics,You can do that by editing your top level gradle file to look like this:,0.0,0.249,0.751,0.5106
appdynamics,Then your project-level gradle file would look like:,0.0,0.263,0.737,0.3612
appdynamics,"Note that I have removed the references to  adeum-maven-repo , and changed the version numbers on the AppDynamics artifacts to refer to them as they exist in maven central.",0.0,0.0,1.0,0.0
appdynamics,"Once you've done this, you no longer need  adeum-maven-repo  in your project, since gradle is now downloading these dependencies automatically.",0.104,0.0,0.896,-0.296
appdynamics,"JAVA_OPTS=""$JAVA_OPTS -Djboss.modules.system.pkgs=org.jboss.byteman,com.singularity,org""
If you do not initialize the JVM, the installation throws a ""class not found"" exception.",0.0,0.0,1.0,0.0
appdynamics,"-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/p:
/jboss-logmanager-.jar",0.0,0.0,1.0,0.0
appdynamics,"JDK9 and above, -Xbootclasspath/p option has been removed; use -Xbootclasspath/a instead.",0.0,0.0,1.0,0.0
appdynamics,"-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/a:
/jboss-logmanager-.jar",0.0,0.0,1.0,0.0
appdynamics,https://docs.appdynamics.com/display/PRO45/JBoss+and+Wildfly+Startup+Settings#JBossandWildflyStartupSettings-InitializetheJVM,0.0,0.0,1.0,0.0
appdynamics,A great place to ask this question would be on the AppDynamics discussion forums so that AppDynamics support can answer you directly...  http://appsphere.appdynamics.com/t5/Discussions/ct-p/Discussions,0.0,0.261,0.739,0.7943
appdynamics,I'm guessing there is a permissions issue somewhere.,0.0,0.0,1.0,0.0
appdynamics,"You should copy all files to run the agent, not just javaagent.jar.",0.0,0.0,1.0,0.0
appdynamics,This is a thread about it.,0.0,0.0,1.0,0.0
appdynamics,http://appsphere.appdynamics.com/t5/Lite-for-Java/Keep-getting-a-quot-Invalid-Agent-Installation-Directory-quot-on/td-p/1151,0.0,0.0,1.0,0.0
appdynamics,The definition can be found in AppDynamics docs:  Slow and Stalled Transactions,0.141,0.0,0.859,-0.2023
appdynamics,"By default AppDynamics considers a slow transaction one that lasts longer than 3 times the standard deviation for the last two hours, a very slow transaction 4 times the standard deviation for the last two hours, and a stalled transaction one that lasts longer than 300 deviations above the average for the last two hours.",0.035,0.0,0.965,-0.2023
appdynamics,"Of course, you can modify the default rules by providing your own:  Configure Thresholds",0.0,0.0,1.0,0.0
appdynamics,"Even it is not the desired behaviour, if filtering by URL and specifying only createNewActivity worked for me (as long as there are no other REST URLs matching this).",0.129,0.0,0.871,-0.4614
appdynamics,"Generally, monitoring tools cannot record method-level data continuously, because they have to operate at a much lower level of overhead compared to profiling tools.",0.091,0.0,0.909,-0.296
appdynamics,"They focus on ""business transactions"" that show you high-level performance measurements with associated semantic information, such as the processing of an order in your web shop.",0.0,0.0,1.0,0.0
appdynamics,Method level data only comes in when these business transactions are too slow.,0.0,0.0,1.0,0.0
appdynamics,The monitoring tool will then start sampling the executing thread and show you a call tree or hot spots.,0.0,0.0,1.0,0.0
appdynamics,"However, you will not get this information for the entire VM for a continuous interval like you're used to from a profiler.",0.0,0.116,0.884,0.3612
appdynamics,"You mentioned JProfiler, so if you are already familiar with that tool, you might be interested in  perfino  as a monitoring solution.",0.0,0.208,0.792,0.6124
appdynamics,It shows you samples on the method level and has cross-over functionality into profiling with the native JVMTI interface.,0.0,0.0,1.0,0.0
appdynamics,It allows you to do  full sampling of the entire JVM  for a selected amount of time and look at the results in the JProfiler GUI.,0.0,0.0,1.0,0.0
appdynamics,Disclaimer: My company develops JProfiler and perfino.,0.0,0.0,1.0,0.0
appdynamics,Information I got from another website.,0.0,0.0,1.0,0.0
appdynamics,Application Insights (AI) is a very simplistic APM tool today.,0.0,0.0,1.0,0.0
appdynamics,"It does do transaction tracing, it has very limited code and runtime diagnostics, and it doesn’t go very deep into analyzing the browser performance.",0.087,0.0,0.913,-0.2944
appdynamics,Application Insights is much more like Google Analytics than like a typical APM tool.,0.0,0.337,0.663,0.6794
appdynamics,We could install the AppDynamics extension use Azure portal or Kudu tool( https://functionAppname.scm.azurewebsites.net/ ).,0.0,0.0,1.0,0.0
appdynamics,Azure Portal:,0.0,0.0,1.0,0.0
appdynamics,Kudu UI,0.0,0.0,1.0,0.0
appdynamics,After installation,0.0,0.0,1.0,0.0
appdynamics,I have used Crashlytics and Google Analytics together without any issue.,0.0,0.0,1.0,0.0
appdynamics,All the logging is done in a background process so I don't think you will notice any speed degradation but the app is technically doing more work so there is some sort of performance hit.,0.064,0.0,0.936,-0.296
appdynamics,I haven't seen any issues with the crashlog.,0.0,0.0,1.0,0.0
appdynamics,Analytics libraries just write the crashlogs to a file and then send them the next time the user opens your app.,0.0,0.0,1.0,0.0
appdynamics,They are not affecting how the actually crashes are handled by the operating system so there shouldn't be any issue with them conflicting.,0.109,0.0,0.891,-0.4019
appdynamics,First  : I would strongly suggest removing the unused one (or the one you don't prefer) from your code.,0.0,0.116,0.884,0.2732
appdynamics,"For reasons, like : 
1.",0.0,0.455,0.545,0.3612
appdynamics,It will increase project size which in turn will increase your bundle size.,0.0,0.295,0.705,0.5574
appdynamics,2.,0.0,0.0,1.0,0.0
appdynamics,Messy code.,0.714,0.0,0.286,-0.3612
appdynamics,3.,0.0,0.0,1.0,0.0
appdynamics,There is no point checking two different analytics.,0.239,0.0,0.761,-0.296
appdynamics,4.,0.0,0.0,1.0,0.0
appdynamics,"While third person is understanding the code, he would waste his time in understanding which will lead to confusion.",0.227,0.0,0.773,-0.6124
appdynamics,I might be missing other reasons.,0.355,0.0,0.645,-0.296
appdynamics,"Second  : To answer your question, it should work fine.",0.0,0.184,0.816,0.2023
appdynamics,"I did the same in one of my projects, where initially I was using  Hockey Crash reporting .",0.162,0.0,0.838,-0.4019
appdynamics,But then client asked to use  Crashlytics .,0.0,0.0,1.0,0.0
appdynamics,I didn't remove Hockey SDK immediately.,0.0,0.0,1.0,0.0
appdynamics,"Though this worked fine and both reported the issues, but soon I removed Hockey SDK from the code.",0.0,0.08,0.92,0.1027
appdynamics,There are integration tools available between influxdb/AppDynamics and grafana/AppDynamics.,0.0,0.0,1.0,0.0
appdynamics,https://github.com/Appdynamics/MetricMover,0.0,0.0,1.0,0.0
appdynamics,https://grafana.com/plugins/dlopes7-appdynamics-datasource/installation ).,0.0,0.0,1.0,0.0
appdynamics,There's nothing that integrates between Prometheus and AppDynamics at the moment,0.0,0.0,1.0,0.0
appdynamics,"I'm not sure there will be one going forward, seeing how they are competing in the same space from different vantage points (Open Source vs Enterprise)",0.073,0.0,0.927,-0.2411
appdynamics,"If you want to monitor only your worker processes of IIS and Standalone Service, You can use CLR Crash event on your policy configuration.",0.104,0.05,0.846,-0.34
appdynamics,AppDynamics automaticaly creates CLR Crash Events If your IIS or Standalone Service are crashed.,0.161,0.125,0.714,-0.1531
appdynamics,"You can find the details of CLR Crash Events:
 https://docs.appdynamics.com/display/PRO45/Monitor+CLR+Crashes",0.231,0.0,0.769,-0.4019
appdynamics,"Also, Sample policy configuration: 
 Policy Configuration Screen",0.0,0.0,1.0,0.0
appdynamics,AppDynamics have a fork of contrib project  here  where there is an exporter but it isn't clear if it has been finished,0.122,0.0,0.878,-0.4168
appdynamics,there is a Beta program now running for using AppDynamics with the OpenTelemetry Collector.,0.0,0.0,1.0,0.0
appdynamics,More details are available in the public docs:  https://docs.appdynamics.com/display/PRO21/Ingest+OpenTelemetry+Trace+Data,0.0,0.0,1.0,0.0
appdynamics,"Looks like you create the metric, then edit a dashboard, then click on a widget -  add metric -  (browse, but choose ""Individual Nodes"" instead of JMX, then select your metric.",0.0,0.117,0.883,0.3182
appdynamics,voila.,0.0,0.0,1.0,0.0
appdynamics,The best place for you to get this question answered would be on the AppDynamics community discussion boards.,0.0,0.198,0.802,0.6369
appdynamics,Here's a link for you...  http://community.appdynamics.com/t5/Forums-Community-AppDynamics/ct-p/Discussions,0.0,0.0,1.0,0.0
appdynamics,The AppDynamics documentation site is also a great resource and you don't even need a login to access them...  http://docs.appdynamics.com/,0.0,0.194,0.806,0.6249
appdynamics,"Installation instructions for the controller can be found in:
 http://docs.appdynamics.com/display/PRO14S/Install+the+Controller 
Assuming you are using App Server Agent for Java, installation instructions for that and the machine agent can be found in the above site on the Left Nav table of contents.",0.0,0.0,1.0,0.0
appdynamics,"The controller is the management server, the app server agent monitors the JVM and the machine agent collects system metrics (such as CPU, Memory, Disk &amp; Network).",0.0,0.0,1.0,0.0
appdynamics,Very minimal configuration is required.,0.0,0.0,1.0,0.0
appdynamics,"From what I understand from their documentation, AppD do not have a way to capture heap dumps.",0.162,0.0,0.838,-0.4019
appdynamics,They suggest using Memory Leak detection feature in such scenarios.,0.211,0.0,0.789,-0.34
appdynamics,"On a different note, I know we can get thread dumps which maybe helpful in some cases (Agents -  Request Agent Log Files)",0.115,0.119,0.766,0.0258
appdynamics,Heap dumps in appdynamics can be taken for JRockit JVM by the below method(Note : This does not work for IBM JVM),0.119,0.0,0.881,-0.4019
appdynamics,p0 – Path to generate heap dump(/path/dump.hprof),0.0,0.0,1.0,0.0
appdynamics,p1 -  True – GC before Heap dump ; False - No GC before heap dump,0.407,0.154,0.44,-0.5574
appdynamics,Note : If you want heap dump to be generated in the case of out of memory give,0.138,0.069,0.794,-0.3182
appdynamics,p0 : HeapDumpOnOutOfMemoryError,0.0,0.0,1.0,0.0
appdynamics,Also note that these values will be lost on JVM restart.,0.164,0.193,0.643,0.1027
appdynamics,"Data retrieval by APM tools is done in several ways, each one with its pros and cons",0.0,0.0,1.0,0.0
appdynamics,"Bytecode injection  (for both Java and .NET) is one technique, which is somewhat intrusive but allows you to get data from places the application owner (or even 3rd party frameworks) did not intend to allow.",0.052,0.092,0.856,0.3718
appdynamics,"Native function interception  is similar to bytecode injection, but allows you to intercept unmanaged code",0.0,0.0,1.0,0.0
appdynamics,Application plugins  - some applications (e.g.,0.0,0.0,1.0,0.0
appdynamics,"Apache, IIS) give access to monitoring and application information via a well-documented APIs and plugin architecture",0.0,0.0,1.0,0.0
appdynamics,Network sniffing  allows you to see all the communication to/from the monitored machine,0.0,0.0,1.0,0.0
appdynamics,"OS specific un/documented APIs  - just like application plugins, but for the Windows/*nix",0.0,0.137,0.863,0.1901
appdynamics,"Disclaimer : I work for Correlsense, provider of APM software SharePath, which uses all of the above methods to give you complete end-to-end transaction visibility.",0.0,0.0,1.0,0.0
appdynamics,I've tried add multi dex without giving any minimum or maximum of method count per dex file wise.I've tried with simply just adding multidex and able to build.And Yes!!,0.063,0.102,0.835,0.3067
appdynamics,I am able to build app too.,0.0,0.0,1.0,0.0
appdynamics,major change is in  afterEvaluate  &amp;  incremental true  in  dexoption .,0.0,0.237,0.763,0.4215
appdynamics,build.gradle,0.0,0.0,1.0,0.0
appdynamics,application's parent gradle,0.0,0.0,1.0,0.0
appdynamics,If above thing have still issue just check your dependecies hierarchy if any other extra dependecies are added (Based on your  build.gradle    packagingOptions  there should be some other dependecies there).Not sure but it may possible because of internal library conflicts its not proceeding further to create dexfile or build.,0.063,0.08,0.857,-0.0258
appdynamics,Let me know if anything,0.0,0.0,1.0,0.0
appdynamics,You can use the REST API of the Controller described here  https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs,0.0,0.0,1.0,0.0
appdynamics,"To access the REST API Browser, in a Web browser, go to:",0.0,0.0,1.0,0.0
appdynamics,this will give you a nice swagger UI description of the available resources.,0.0,0.203,0.797,0.4215
appdynamics,Alternatively you can create reports directly out of the box in AppDynamics via the  Dashboards &amp; Reports  section.,0.0,0.11,0.89,0.2732
appdynamics,"First you need to setup the AppDynamics Controller (should be hosted on a separate  machine), then you need java agents on the machine with the application.",0.0,0.0,1.0,0.0
appdynamics,"You need to wire the application with the Java agent, e.g.",0.0,0.0,1.0,0.0
appdynamics,like this,0.0,0.714,0.286,0.3612
appdynamics,Now the Java agent sends application information to the controller.,0.0,0.0,1.0,0.0
appdynamics,Take a look at the documentation.,0.0,0.0,1.0,0.0
appdynamics,https://docs.appdynamics.com/display/PRO43/Getting+Started,0.0,0.0,1.0,0.0
appdynamics,The application will automatically be available in AppDynamics and you can see the dashboard.,0.0,0.0,1.0,0.0
appdynamics,"You don't need to host the controller, you can get a SAAS account here.",0.0,0.0,1.0,0.0
appdynamics,https://www.appdynamics.com/free-trial/  .,0.0,0.0,1.0,0.0
appdynamics,Once you get a SAAS account lookup the account name and account key in the welcome email and use the instructions in the email to add JVM args to your app as shown.,0.0,0.088,0.912,0.4588
appdynamics,Don't forget to set the account name and account access key args: -Dappdynamics.agent.accountName -Dappdynamics.agent.accountAccessKey .,0.0,0.114,0.886,0.1695
appdynamics,See  4.3.x Documentation⇒POJO Entry Points⇒Monitor Java Interface Static and Default Methods :,0.0,0.0,1.0,0.0
appdynamics,"Note that another Java language feature introduced in Java 8, lambda method interfaces, are not supported by the AppDynamics Java Agent.",0.089,0.0,0.911,-0.2411
appdynamics,It’s possible that this is due to technical difficulties with  JDK-8145964  as you suspect.,0.268,0.0,0.732,-0.5267
appdynamics,But I’d also point out that this kind of Instrumentation would be questionable.,0.189,0.0,0.811,-0.4215
appdynamics,"It’s not this JRE generated class that implements any specific behavior, it’s the invoked target method.",0.0,0.0,1.0,0.0
appdynamics,Support for Lambda expressions has been in the product since 4.1 which shipped in 2015 I believe.,0.0,0.153,0.847,0.4019
appdynamics,That being said we are always enhancing support.,0.0,0.278,0.722,0.4019
appdynamics,These do have some limitations after initialization of the classes using them (dynamic instrumentation limits).,0.0,0.0,1.0,0.0
appdynamics,"The product should support them, we have added some additional capabilities and features for Lambda expressions in our next major release.",0.0,0.119,0.881,0.4019
appdynamics,Have you tried to contact help @ appdynamics.com,0.0,0.31,0.69,0.4019
appdynamics,"Looks like it was not mentioned in release notes, but instead  Support Advisory 56039  was raised.",0.0,0.275,0.725,0.6486
appdynamics,They indeed mention JDK-8145964 as a reason for removing the support.,0.0,0.231,0.769,0.4019
appdynamics,I got this too...,0.0,0.0,1.0,0.0
appdynamics,I was running from command-line as a non-root user:,0.0,0.0,1.0,0.0
appdynamics,I added the shell expand(-x) switch and log to the command(s) like so:,0.0,0.185,0.815,0.3612
appdynamics,"If we tail the last bit of that log you get, this response in debug mode:",0.0,0.0,1.0,0.0
appdynamics,and the script checkLibaio.sh isn't left there... so you cannot figure it out easily.,0.0,0.156,0.844,0.34
appdynamics,I also have a RedHat variant with the packages installed:,0.0,0.0,1.0,0.0
appdynamics,"Strangely enough I have one VM from the same image that will install the distribution just fine, and one that will not, so on the broken install (where I really want to install this).",0.151,0.092,0.758,-0.4889
appdynamics,"I ran another command from the expanded view of the install.log, which was a really long JVM command line.",0.0,0.0,1.0,0.0
appdynamics,Anyways I got it to work and then made a looping script to retrieve the file (Because AppD for some reason removes the check script before you can look at it).,0.0,0.0,1.0,0.0
appdynamics,The script is as follows:,0.0,0.0,1.0,0.0
appdynamics,I you run this script like me on the faulty platform what you will discover is that your version of Linux has both:,0.093,0.101,0.806,0.0516
appdynamics,and,0.0,0.0,1.0,0.0
appdynamics,installed.,0.0,0.0,1.0,0.0
appdynamics,To work around this you should temporarily make a name change to one of these two package manager executables so it cannot be found (by your shell environment).,0.0,0.0,1.0,0.0
appdynamics,Most common here will be that you are running a RedHat variant where someone chose to install dpkg (For who knows what reason).,0.0,0.0,1.0,0.0
appdynamics,If so desired remove that package and the install should be successful.,0.0,0.382,0.618,0.7346
appdynamics,As stated on the AppD docs regarding  Kubernetes and AppDynamics APM,0.0,0.0,1.0,0.0
appdynamics,Install a Standalone Machine Agent (1) in a Kubernetes node.,0.0,0.0,1.0,0.0
appdynamics,Install an APM Agent (2) inside each container in a pod you want to monitor.,0.0,0.091,0.909,0.0772
appdynamics,"The Standalone Machine Agent then collects hardware metrics for each monitored container, as well as Machine and Server metrics for the host (3), and forwards the metrics to the Controller.",0.0,0.068,0.932,0.2732
appdynamics,ContainerID and UniqueHostID can be taken from  /proc/self/cgroup,0.0,0.0,1.0,0.0
appdynamics,ContainerID  cat /proc/self/cgroup | awk -F '/' '{print $NF}'  | head -n 1,0.0,0.0,1.0,0.0
appdynamics,UniqueHostID  sed -rn '1s#.,0.0,0.0,1.0,0.0
appdynamics,*/##; 1s/(.{12}).,0.0,0.0,1.0,0.0
appdynamics,*/\1/p' /proc/self/cgroup,0.0,0.0,1.0,0.0
appdynamics,Thanks for the reply to my question.,0.0,0.326,0.674,0.4404
appdynamics,The .NET agent in most APM tools works the same way which leverages the profiler APIs in the .NET SDK and allows for data collection and also callbacks and other interception.,0.0,0.0,1.0,0.0
appdynamics,"Most of the tools also use performance counter data, and other sources aside from inside the .NET runtime.",0.0,0.0,1.0,0.0
appdynamics,This allows you to do several things similar to Java in terms of data collection.,0.0,0.0,1.0,0.0
appdynamics,ref:  https://docs.microsoft.com/en-us/visualstudio/profiling/walkthrough-using-profiler-apis?view=vs-2017,0.0,0.0,1.0,0.0
appdynamics,http://www.blong.com/conferences/dcon2003/internals/profiling.htm,0.0,0.0,1.0,0.0
appdynamics,"The solution was adding ""appdynamics"" to the ""externals"" in the Webpack configuration:  https://webpack.js.org/configuration/externals/",0.0,0.161,0.839,0.3182
appdynamics,This allows AppDynamics to use the default Node.js require import.,0.0,0.0,1.0,0.0
appdynamics,The most important thing to understand about this error is the meaning of this line:,0.172,0.115,0.714,-0.2575
appdynamics,Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target,0.0,0.0,1.0,0.0
appdynamics,"SSL certificates work by establishing a certificate chain, or a hierarchy of trust.",0.0,0.248,0.752,0.5106
appdynamics,"For example, if I go to  https://www.google.com  and look at their cert, this is what I see:",0.0,0.0,1.0,0.0
appdynamics,"There is the google cert, which sits on their servers/CDN, then an intermediate cert which also sits on their servers/CDN, then a trusted root CA cert which is in the  client  keystore and is implicitly trusted.",0.0,0.158,0.842,0.7351
appdynamics,"So when someone browses to google, b/c they have the root CA cert and have trusted it, the browser (client) will trust that the server is actually who they say they are and will establish a secure connection to the site.",0.0,0.192,0.808,0.8316
appdynamics,"So getting back to your error, whatever CA issued the server cert being used by RabbitMQ, the monitor is not recognizing it as trusted.",0.097,0.112,0.791,0.1027
appdynamics,"To troubleshoot this error, here are the things to do:",0.242,0.139,0.619,-0.3237
appdynamics,There are a couple other gotchas to watch out for:,0.0,0.0,1.0,0.0
appdynamics,This issue was solved here:  https://community.appdynamics.com/t5/Java-Java-Agent-Installation-JVM/monitor-URL-page-availability/td-p/40904,0.0,0.296,0.704,0.2732
appdynamics,Summary:,0.0,0.0,1.0,0.0
appdynamics,"Turns out if you go to configure -  instrumentation -  JMX tab then voila, you can now delete metrics and modify/edit them.",0.0,0.0,1.0,0.0
appdynamics,But nowhere else.,0.0,0.0,1.0,0.0
appdynamics,Odd.,1.0,0.0,0.0,-0.3182
appdynamics,I've instrumented  org.apache.ode.bpel.engine.BpelProcess.handleJobDetails and it is showing me transactions going through Carbon out to the backends.,0.0,0.0,1.0,0.0
appdynamics,It is a bit hard to completely answer your question and solve the issue with the provided information.,0.077,0.099,0.824,0.1027
appdynamics,"However, I hope my questions below help you to get on the right track.",0.0,0.337,0.663,0.6808
appdynamics,1.),0.0,0.0,1.0,0.0
appdynamics,"After making the configuration change, did you also restart the AppDynamics.AgentCoordinator_WindowsService?",0.0,0.0,1.0,0.0
appdynamics,Without restarting it the new configuration will not be applied to the agent itself.,0.0,0.0,1.0,0.0
appdynamics,2.),0.0,0.0,1.0,0.0
appdynamics,"Also important is your windows service hosting any OOTB support entry point like WCF, ASP.NET MVC4 WebAPI, web services etc.?",0.0,0.292,0.708,0.7184
appdynamics,"If not, you need to setup a custom entry point.",0.0,0.0,1.0,0.0
appdynamics,If you  check out the AppDynamics documentation and search for'POCO Entry Points' you should get onto the right track,0.0,0.0,1.0,0.0
appdynamics,3.),0.0,0.0,1.0,0.0
appdynamics,"In case No.1 &amp; No.2 did not do the trick, could you please attach the config.xml file for review?",0.0,0.169,0.831,0.3502
appdynamics,Or directly reach out to the AppDynamics customer success team.,0.0,0.375,0.625,0.5859
appdynamics,"Kind regards,
Theo",0.0,0.63,0.37,0.5267
appdynamics,Disclaimer: I work for AppDynamics as part of the Customer Success team.,0.0,0.27,0.73,0.5719
appdynamics,Just to add to you answer and to clarify a little.,0.0,0.0,1.0,0.0
appdynamics,"Until release 3.7.7 the .NET agent from AppDynamics used the web.config (for IIS based application), App.config for standalone and windows services or the global.config plus some environment variables to configure the agent.",0.0,0.0,1.0,0.0
appdynamics,With the 3.7.8 or later release we replaced this with a cleaner truly singly configuration file approach.,0.0,0.247,0.753,0.5574
appdynamics,The configuration file is called config.xml and located in the %ProgramData%\AppDynamics... directory.,0.0,0.0,1.0,0.0
appdynamics,For any version after 3.7.8 all settings have to be in the config.xml.,0.0,0.0,1.0,0.0
appdynamics,You really should take this up with AppDynamics support by filing a ticket or posting in the support forums...  http://www.appdynamics.com/support/#helptab,0.0,0.241,0.759,0.6597
appdynamics,There are many things you should analyse.,0.0,0.0,1.0,0.0
appdynamics,It depends on what your LR portal uses most.,0.0,0.0,1.0,0.0
appdynamics,"You may want to analyse web content , user, groups, calnder, theme related services.",0.0,0.098,0.902,0.0772
appdynamics,I would ask support on this if you can't get it to work.,0.0,0.197,0.803,0.4019
appdynamics,Here is the configuration for custom exit and entry points in the product:  https://docs.appdynamics.com/display/PRO14S/Configure+Custom+Exit+Points,0.0,0.0,1.0,0.0
appdynamics,"Love to help you out here, but there are no details about the error, please email help@appdynamics.com for assistance.",0.229,0.267,0.505,0.0129
appdynamics,"I assume this is C#, but wouldn't know based on this.",0.0,0.0,1.0,0.0
appdynamics,AppDynamics supports many languages and technologies.,0.0,0.333,0.667,0.3612
appdynamics,For whom it might be of interest I have found a workaround and more details about this issue.,0.0,0.167,0.833,0.4588
appdynamics,This occurs only in the following scenario:,0.0,0.0,1.0,0.0
appdynamics,If you use the MsBuild 14 that is installed along Microsoft Visual Studio 2015 RC then this issue does not occur anymore.,0.0,0.0,1.0,0.0
appdynamics,From my first findings there is an issue in ServiceStack's way of caching the endpoints and wrapping the execute method using Linq but I don't understand why this happens only when AppDynamics agent is installed.,0.0,0.0,1.0,0.0
appdynamics,@mythz if you want to dig deeper into this issue I'm available to help but with the above solution everything is ok.,0.0,0.327,0.673,0.775
appdynamics,Hope this helps,0.0,0.846,0.154,0.6705
appdynamics,"There are lots of APIs available out of the box, you can find the docs at  https://docs.appdynamics.com/  not everything has an API.",0.0,0.0,1.0,0.0
appdynamics,You should find the user management as part of the configuration API here :  https://docs.appdynamics.com/display/PRO42/Configuration+API,0.0,0.0,1.0,0.0
appdynamics,They do different things.,0.0,0.0,1.0,0.0
appdynamics,ELK will give you log aggregation that you can add in other functionality.,0.0,0.0,1.0,0.0
appdynamics,Appdynamics is great for real time monitoring and profiling.,0.0,0.339,0.661,0.6249
appdynamics,I think it depends on what you're going for.,0.0,0.0,1.0,0.0
appdynamics,Logging a distributed system and capturing error messages in one place might be very helpful with ELK.,0.136,0.156,0.707,0.101
appdynamics,"Not just that, but ELK can be used in a number of other ways.",0.0,0.108,0.892,0.1154
appdynamics,Elasticsearch can be used stand alone as a search engine or data cache.,0.154,0.0,0.846,-0.25
appdynamics,TL;DR  It depends on what you're doing.,0.0,0.0,1.0,0.0
appdynamics,Maybe yes...maybe no...,0.0,0.0,1.0,0.0
appdynamics,"On that particular server, we have the .NET agent running as well.",0.0,0.16,0.84,0.2732
appdynamics,"After shutting off the .NET agent, we were no longer having this issue with 1-2 hour gaps in the metric browser.",0.099,0.0,0.901,-0.296
appdynamics,"Apparently, there is some conflict in having multiple machine agents installed on the same server.",0.141,0.0,0.859,-0.3182
appdynamics,You need both unit tests and integration tests.,0.0,0.0,1.0,0.0
appdynamics,"Unit tests should not use in database or File, ect.",0.0,0.0,1.0,0.0
appdynamics,I like to use Spring profiles for my tests.,0.0,0.263,0.737,0.3612
appdynamics,"For instance, if I have a profile called integeration_test.",0.0,0.0,1.0,0.0
appdynamics,"(I'm using xml) then in your context do something like:  &lt;beans profile=""test""&gt;TODO &lt;/beans&gt;  And configure your data-source in there.",0.0,0.122,0.878,0.3612
appdynamics,"I know there are ways to rollback all your transactions after running a test, but I like this better.",0.0,0.358,0.642,0.8322
appdynamics,"Just don't delete everything in your real database haha, could even put some safety code in the clearDatabase to make sure that doesn't happen.",0.0,0.278,0.722,0.7964
appdynamics,"For performance testing you will really need to figure out what you want to achieve, and what is meaningful to display.",0.0,0.159,0.841,0.3818
appdynamics,"If you have a specific question about performance testing you can ask that, otherwise it is too broader topic.",0.0,0.0,1.0,0.0
appdynamics,Maybe you can make a mini-webapp which does performance testing for you and has the results exposed as URL requests for displaying in HTML.,0.056,0.0,0.944,-0.0772
appdynamics,Really just depends on how much effort you are willing to spend on it and what you want to test.,0.0,0.064,0.936,0.0772
appdynamics,Once the agent is attached you can use the  AppDynamics Database Queries Window,0.0,0.0,1.0,0.0
appdynamics,"Reflection (inherently  TypeVisitor  and  TypeToken  classes) is always costly in Java, try not using it.",0.091,0.0,0.909,-0.1027
appdynamics,Rendering time seems OK.,0.0,0.494,0.506,0.4466
appdynamics,"There can be thousand reasons for high latency in an application, but you only gave this much information so that's about the best answer you can get.",0.0,0.182,0.818,0.7783
appdynamics,That's the beauty of APM is you don't need to deal with logging to get performance data.,0.0,0.192,0.808,0.5859
appdynamics,"APM tools instrument the applications themselves regardless of what the code does (logging, generating metrics, etc).",0.0,0.0,1.0,0.0
appdynamics,"AppDynamics can collect log data, and provide similar capabilities to what a log analytics tool can do, but it's of less value than the transaction tracing and instrumentation you get.",0.0,0.087,0.913,0.394
appdynamics,"Your application has to be built in a supported language (Java, .NET, PHP, Python, C++, Node.js) and if it's web or mobile based you can also collect client side performance data and unify between both frontend and backend.",0.0,0.06,0.94,0.3182
appdynamics,If you have questions just reach out and I can answer them for you.,0.0,0.084,0.916,0.0258
appdynamics,Good luck!,0.0,1.0,0.0,0.7345
appdynamics,You basically need the AppDynamics Controller and a AppDynamics Machine-Agent which will be installed on the machine to monitor.,0.0,0.0,1.0,0.0
appdynamics,In the Machine-Agent configuration you set the URI of the controller and the agent starts to report machine metrics to the controller.,0.0,0.0,1.0,0.0
appdynamics,"Then you can configure alarms, see metrics, create dashboards, etc.",0.172,0.172,0.656,0.0
appdynamics,"I can deliver you more information if you want, but as Jonah Kowall said, take a look at the documentation as well  AppDynamics Machine Agent Doc",0.0,0.147,0.853,0.4215
appdynamics,"Typically, you don't need to change anything in your code to capture JMX metrics except your Java Beans have to fulfill the Management Beans (MBeans) requirements you have to enable JMX monitoring in each Java process that should be monitored and the monitored system runs on Java 1.5 or later.",0.0,0.056,0.944,0.4404
appdynamics,See also  here  and  here .,0.0,0.0,1.0,0.0
appdynamics,Then you can navigate to  Tiers &amp; Nodes -&gt; Select a tier -&gt; JMX tab,0.0,0.0,1.0,0.0
appdynamics,"I am not exactly sure what the problem is, but you can share a Dashboard and open this shared Dashboard URL and make the Browser fullscreen.",0.112,0.216,0.672,0.6242
appdynamics,Thats how we do it and it works perfectly (on AppDynamics controller version 4.2).,0.0,0.244,0.756,0.6369
appdynamics,We use the 'revolver tab' add-on for the browser to switch between desktops.,0.0,0.0,1.0,0.0
appdynamics,Try adding the line below in the crx-quickstart/conf/sling.properties:,0.0,0.0,1.0,0.0
appdynamics,"org.osgi.framework.bootdelegation=com.singularity.*,com.yourkit.",0.0,0.0,1.0,0.0
appdynamics,"*, ${org.apache.sling.launcher.bootdelegation}",0.0,0.0,1.0,0.0
appdynamics,You can build multiple baselines within AppDynamics if you'd like to.,0.0,0.2,0.8,0.3612
appdynamics,The thresholds should be auto calculated off deviation from baseline.,0.0,0.0,1.0,0.0
appdynamics,This makes it so you don't need to configure them manually.,0.0,0.0,1.0,0.0
appdynamics,"If you want to do SLA tracking, Business iQ (analytics) can do this very well.",0.0,0.221,0.779,0.4005
appdynamics,We also are building additional features around SLA use cases we can share.,0.0,0.155,0.845,0.296
appdynamics,Feel free to email me or support for a hand.,0.0,0.605,0.395,0.8481
appdynamics,"I think everything you need is described  here  
Basically you need to do the following:",0.0,0.0,1.0,0.0
appdynamics,probably in your case it is,0.0,0.0,1.0,0.0
appdynamics,via CURL it looks like this,0.0,0.333,0.667,0.3612
appdynamics,You can find more information about the AppDynamics API  here,0.0,0.0,1.0,0.0
appdynamics,"Inside the widget settings make sure you select the ""Stack Areas or Columns"" checkbox.",0.0,0.15,0.85,0.3182
appdynamics,This works in every version of AppD I've used including 4.3.0.2.,0.0,0.0,1.0,0.0
appdynamics,"SQL 2005 is supported, but this was a bug which was introduced in version 4.3.0.",0.0,0.113,0.887,0.1655
appdynamics,There is currently a diagnostic patch for this issue for supported customers.,0.0,0.187,0.813,0.3182
appdynamics,The fix should be in the next patch level once we isolate the issue.,0.122,0.0,0.878,-0.2023
appdynamics,If you'd like support just email help@appdynamics.com and they can assist.,0.0,0.366,0.634,0.6369
appdynamics,Thanks.,0.0,1.0,0.0,0.4404
appdynamics,In order to capture this data follow these steps:,0.0,0.0,1.0,0.0
appdynamics,"Pointer, you can also adjust the time periods in the metric browser to adjust the time.",0.0,0.0,1.0,0.0
appdynamics,Hope this is helpful.,0.0,0.74,0.26,0.6908
appdynamics,Couple of things:,0.0,0.0,1.0,0.0
appdynamics,I think the general URL format for app dynamics applications are (notice the '#'):,0.0,0.149,0.851,0.2732
appdynamics,"Also, I think the requests.get method needs an additional parameter for the 'account'.",0.0,0.0,1.0,0.0
appdynamics,"For instance, my auth format looks like:",0.0,0.294,0.706,0.3612
appdynamics,I am able to get a right response code back with this config.,0.0,0.0,1.0,0.0
appdynamics,Let me know if this works for you.,0.0,0.0,1.0,0.0
appdynamics,You could also use native python code for more control:,0.0,0.0,1.0,0.0
appdynamics,example:,0.0,0.0,1.0,0.0
appdynamics,If you prefer it in JSON simply specify it in the request.,0.0,0.0,1.0,0.0
appdynamics,"You can get severity information by appending &amp;severities=INFO,WARN,ERROR to your URL.",0.0,0.0,1.0,0.0
appdynamics,"So your url must be like :  http://:/controller/rest/applications//business-transactions?output=JSON&amp;severities=INFO,WARN,ERROR",0.0,0.294,0.706,0.3612
appdynamics,Severity is associated with specific events/entities in AppDynamics.,0.0,0.0,1.0,0.0
appdynamics,Based on your API call I can see that you are trying to retrieve information about Business Transactions (BTs).,0.0,0.0,1.0,0.0
appdynamics,Severity param is not associated with BTs.,0.0,0.0,1.0,0.0
appdynamics,e.g.,0.0,0.0,1.0,0.0
appdynamics,"You can pull Severity for Health rule violations in AppDynamics by making the following API call:
http:///controller/rest/applications//problems/healthrule-violations
Result:",0.167,0.0,0.833,-0.5267
appdynamics,You can find further information about using AppD controller API in the following documentation pages:,0.0,0.0,1.0,0.0
appdynamics,https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs,0.0,0.0,1.0,0.0
appdynamics,https://docs.appdynamics.com/display/PRO42/Alert+and+Respond+API,0.0,0.0,1.0,0.0
appdynamics,The Issue is resolved.,0.0,0.362,0.638,0.1779
appdynamics,"The controller-info of Machine Agent need not to have any Application,Node and Tier name.",0.0,0.0,1.0,0.0
appdynamics,It should include unique host id which should be same as controller-info of JavaAgent.,0.0,0.0,1.0,0.0
appdynamics,It looks like you can use environment variables to configure the python appdynamics agent as well.,0.0,0.247,0.753,0.5574
appdynamics,Open up your repl,0.0,0.0,1.0,0.0
appdynamics,"For the usual configuration values (APP_NAME, TIER_NAME, NODE_NAME, etc) you can configure them via the environment variables.",0.0,0.144,0.856,0.4019
appdynamics,You just need to prefix them with 'APPD_'.,0.0,0.0,1.0,0.0
appdynamics,For for APP_NAME it would be:,0.0,0.0,1.0,0.0
appdynamics,You can configure the python agent in your code like so:,0.0,0.2,0.8,0.3612
appdynamics,"Alternatively, you can pass in the location of your appdynamics.cfg file.",0.0,0.0,1.0,0.0
appdynamics,"That is to say, setting environment variables is not enough.",0.0,0.0,1.0,0.0
appdynamics,"Then you need to manually start the proxy (after you  appd.init ) by running
 pyagent proxy start",0.0,0.0,1.0,0.0
appdynamics,The agent configuration from your code will be automatically used by the proxy.,0.0,0.0,1.0,0.0
appdynamics,For a full list of config keys see the  setting docs,0.0,0.0,1.0,0.0
appdynamics,I managed to define just environment variables without changing application code.,0.0,0.0,1.0,0.0
appdynamics,Note that variable name for controller host is APPD_CONTROLLER_HOST.,0.0,0.0,1.0,0.0
appdynamics,You can also pass command line parameters to the process.,0.0,0.0,1.0,0.0
appdynamics,"Yes, it does transaction tracking for every intra-component call across languages.",0.0,0.213,0.787,0.4019
appdynamics,Without code changes.,0.0,0.0,1.0,0.0
appdynamics,"When there is a slow transaction detail down to code level will show up in snapshots, this is what you'd use for diagnostics.",0.0,0.0,1.0,0.0
appdynamics,Depending on your language you can also configure data collectors which do runtime instrumentation of custom data from the code.,0.0,0.0,1.0,0.0
appdynamics,"These show up on every call, and you can turn them into metrics too.",0.0,0.0,1.0,0.0
appdynamics,Once again no code changes.,0.355,0.0,0.645,-0.296
appdynamics,Zipkin only does tracing.,0.0,0.0,1.0,0.0
appdynamics,"APM tools like Appdynamics do other monitoring (browser, mobile, database, server, network).",0.0,0.185,0.815,0.3612
appdynamics,Code-level diagnostics with automated overhead controls and limiters.,0.0,0.0,1.0,0.0
appdynamics,Don't forget log analytics and transaction analytics.,0.0,0.217,0.783,0.1695
appdynamics,It also collects metrics.,0.0,0.0,1.0,0.0
appdynamics,"There is a lot more to APM than just tracing, which is what Zipkin does.",0.0,0.0,1.0,0.0
appdynamics,"You could do this with a stack of 20 open source tools, but you'd have to deal with disjointed UIs and data models not to mention the work associated with keeping them all working.",0.084,0.0,0.916,-0.4497
appdynamics,"It's actually more the other way around - Crittercism does crash logging, network monitoring, and performance timing whereas AppDynamics is more focused on server monitoring.",0.098,0.105,0.797,0.0498
appdynamics,"Both products essentially do the same thing, if anything AppDynamics has advanced beyond Crittercism (now called Apteligent).",0.0,0.111,0.889,0.25
appdynamics,"They had the lead when the mAPM market started, but now there is not much innovating happening, especially after they sold the company to VMware earlier this year.",0.0,0.0,1.0,0.0
appdynamics,"Here are the mRUM docs for AppD:  https://docs.appdynamics.com/display/PRO44/Mobile+Real+User+Monitoring  some of the more advanced features in AppD you will not find in Appteligent would be things like screen recording, breadcrumb/navigation for every click and interaction.",0.0,0.13,0.87,0.5849
appdynamics,"The most valuable feature, of course, is the measurement outside of a straight mobile use case.",0.0,0.289,0.711,0.6478
appdynamics,"AppD does measure and ties together the mobile transaction with the backend, server (and network), Docker containers, AWS or other cloud providers.",0.0,0.0,1.0,0.0
appdynamics,"Additionally, it monitors performance and usage of browsers.",0.0,0.0,1.0,0.0
appdynamics,"Disclosure: I used to be the lead for these products at Gartner, and have been working at AppDynamics for the last 3 years.",0.0,0.0,1.0,0.0
appdynamics,For Apple platforms I recommend you to avoid any third party crash log frameworks and use  Xcode crashes organizer.,0.203,0.216,0.581,0.0772
appdynamics,It seems that you schould be able to define your custom  Asynchronous Transaction Demarcator  as described in:   https://docs.appdynamics.com/display/PRO44/Asynchronous+Transaction+Demarcators,0.0,0.0,1.0,0.0
appdynamics,which will point to the last method of Runnable that you passes to the Executor.,0.0,0.0,1.0,0.0
appdynamics,Then according to the documentation all you need is to attach the Demarcator to your Business Transaction and it will collect the asynchronous call.,0.0,0.0,1.0,0.0
appdynamics,The first step is to add a username and password in the etc/users.properties file.,0.0,0.0,1.0,0.0
appdynamics,"For most purposes, it is ok to just 
use the default settings provided out of the box.",0.0,0.121,0.879,0.296
appdynamics,"For this, just uncomment the following line:",0.0,0.0,1.0,0.0
appdynamics,"Then, you must bypass credential checks on BrokeViewMBean by adding it to the whitelist ACL configuration.",0.0,0.0,1.0,0.0
appdynamics,You can do so by replacing this line:,0.0,0.0,1.0,0.0
appdynamics,with this:,0.0,0.0,1.0,0.0
appdynamics,"In addition to being the correct way, it also enables several different configuration options (eg: port, listen address, etc) by just changing the file org.apache.karaf.management.cfg on broker's etc directory.",0.0,0.0,1.0,0.0
appdynamics,"Please keep in mind that JMX access is made through a different JMX connector root in this case: it uses  karaf-root  instead of  jmxrmi , which was previously used in the older method.",0.0,0.071,0.929,0.3182
appdynamics,"It also uses port 1099 by default, instead of 1616.",0.0,0.0,1.0,0.0
appdynamics,"Therefore, the uri should be",0.0,0.0,1.0,0.0
appdynamics,"From my current knowledge: no, AppDynamics doesn't support OpenTracing yet.",0.358,0.0,0.642,-0.5358
appdynamics,"Usually, APM vendors have their own OpenTracing tracers build off the official specification and then get them listed at  http://opentracing.io .",0.0,0.0,1.0,0.0
appdynamics,But as of this writing there is no mention of any AppDynamics Tracers at  https://opentracing.io/docs/supported-tracers/  nor  https://github.com/opentracing-contrib/meta .,0.149,0.0,0.851,-0.4215
appdynamics,"Full disclosure: I work for Instana, a competitor that does support OpenTracing.",0.0,0.231,0.769,0.4019
appdynamics,No AppD doesn't support OpenTracing at this time.,0.426,0.0,0.574,-0.5358
appdynamics,The question is why do you want this functionality when you can already extract custom data from transactions dynamically with most AppDynamics agents?,0.0,0.153,0.847,0.4215
appdynamics,Do you really want to hardcode your APM tool into your software application?,0.0,0.117,0.883,0.1513
appdynamics,"AppDynamics is building a unique way to support OpenTracing, which is currently in testing, but the approach is not by hardcoding libraries into the code.",0.0,0.074,0.926,0.2144
appdynamics,"If you'd like to learn more please reach out to support, or you can contact me directly as I work for AppDynamics.",0.0,0.35,0.65,0.8004
appdynamics,Thanks.,0.0,1.0,0.0,0.4404
appdynamics,You need to add:,0.0,0.0,1.0,0.0
appdynamics,into   wrapper.conf  file.,0.0,0.0,1.0,0.0
appdynamics,https://docs.appdynamics.com/display/PRO45/Tanuki+Service+Wrapper+Settings,0.0,0.0,1.0,0.0
appdynamics,You can export the dashboard and you will get the json version of your dashboard.,0.0,0.0,1.0,0.0
appdynamics,(The export button is located top of the your dashboard page),0.0,0.153,0.847,0.2023
appdynamics,This json file can be easily editable with any editor and you can replace the Application and/or other properties which contain on of your dashboards.,0.0,0.091,0.909,0.34
appdynamics,https://docs.appdynamics.com/display/PRO45/Import+and+Export+Custom+Dashboards+and+Templates+Using+the+UI,0.0,0.0,1.0,0.0
appdynamics,Due to its proprietary nature i don't think the internals are available for anyone to view or discuss.,0.0,0.0,1.0,0.0
appdynamics,Below link might give an idea of how the product works at a high level.,0.0,0.0,1.0,0.0
appdynamics,https://www.appdynamics.com/product/how-it-works/,0.0,0.0,1.0,0.0
appdynamics,"You are on the right track, but you are not saying that you are having errors or showing the.",0.147,0.0,0.853,-0.4767
appdynamics,"Yet, based on your experience to date, I am sure you already know that PowerShell provides cmdlets for working with XML.",0.0,0.108,0.892,0.3182
appdynamics,See them using ---,0.0,0.0,1.0,0.0
appdynamics,or get other XML modules from the MS PowerShellGallery.com using ---,0.0,0.0,1.0,0.0
appdynamics,--- and install the one(s) that fit your needed goals.,0.0,0.217,0.783,0.3612
appdynamics,And of course there are lot's of examples and videos on the topic.,0.0,0.0,1.0,0.0
appdynamics,"Searching for 'PowerShell working with XML', gives you plenty of hits.",0.0,0.0,1.0,0.0
appdynamics,PowerShell Data Basics: XML,0.0,0.0,1.0,0.0
appdynamics,"For the most part, what you will find is very similar to what you already have posted.",0.0,0.0,1.0,0.0
appdynamics,"Yet, you say you want add nodes / elements, but that is not in your post, so, something like the below should help.",0.0,0.306,0.694,0.8109
appdynamics,Or even just using the  WebAdministration module  on the IIS server directly.,0.0,0.0,1.0,0.0
appdynamics,You should check out the AppDynamics agent installation PowerShell Extension:  https://www.appdynamics.com/community/exchange/extension/dotnet-agent-installation-with-remote-management-powershell-extension/,0.0,0.0,1.0,0.0
appdynamics,"It should be able to handle what you are trying to do without having to generate the xml manually, check the pdf for advanced options and read about the Add-IISApplicationMonitoring cmdlet",0.0,0.062,0.938,0.25
appdynamics,What it can’t do is the newer stuff like “multiple business application” and “custom node name” configuration.,0.0,0.135,0.865,0.3612
appdynamics,(Which you can’t do in the install wizard either),0.0,0.0,1.0,0.0
appdynamics,"AppDynamics itself powerful enough to provide all sort of information, However if you still want actuator data to be captured and displayed then you may need to use AppD extension.",0.0,0.128,0.872,0.4767
appdynamics,Please refer below official link to AppDyanmics Exchange.,0.0,0.247,0.753,0.3182
appdynamics,https://www.appdynamics.com/community/exchange/,0.0,0.0,1.0,0.0
appdynamics,if the relevant is not available you may need write your own.,0.0,0.0,1.0,0.0
appdynamics,I found this solution  https://github.com/Appdynamics/flowmap-builder,0.0,0.434,0.566,0.3182
appdynamics,Seems to be working so far.,0.0,0.0,1.0,0.0
appdynamics,Doesn't rely (directly) on APIs but it works!,0.0,0.0,1.0,0.0
appdynamics,For anyone who is looking for an answer here.,0.0,0.0,1.0,0.0
appdynamics,This is resolved by unchecking the Extended  tracking option for kafka in AppDynamics Console.,0.0,0.116,0.884,0.1779
appdynamics,The option will be there in Automatic Backend Discovery.,0.0,0.0,1.0,0.0
appdynamics,Hope this helps.,0.0,0.846,0.154,0.6705
appdynamics,"Disable default Kafka configuration on ""Backend Detection"" page",0.0,0.0,1.0,0.0
appdynamics,"Define a new ""Custom Exit Point"" with the last class and method which you see on the call graphs before Kafka exit call",0.0,0.0,1.0,0.0
appdynamics,"Don't forget to click on ""Is High Volume"" checkbox.",0.0,0.172,0.828,0.1695
appdynamics,If we take the example of an ECommerce Application:,0.0,0.0,1.0,0.0
appdynamics,"Business Transactions  are Checkout, Landing Page, Add to Cart etc.",0.0,0.0,1.0,0.0
appdynamics,which are known by every end user of the application.,0.0,0.0,1.0,0.0
appdynamics,"These business transactions cover all the method executions, database calls, web service calls etc.",0.0,0.0,1.0,0.0
appdynamics,Service End Points  are the sub calls(method call or web service call) execute inside of the Business Transactions.,0.0,0.0,1.0,0.0
appdynamics,"Such as ""Check Inventory"" service which is executed in Checkout and Add to Cart transactions as well.",0.0,0.116,0.884,0.2732
appdynamics,"Information Points  are the key business or technical metric counts, such as Checkout amount, Add to Cart item count.",0.0,0.0,1.0,0.0
appdynamics,"Service End Points and Information Points only give you performance metrics but Business Transactions also give you full code visibility with ""call graphs""",0.0,0.0,1.0,0.0
appdynamics,"Also, there are some limitations like max 200 Business Transaction on the default but you can change these rules.",0.0,0.089,0.911,0.1901
appdynamics,"While configuring the BTs &amp; SEs, you must focus on the needs of AppDynamics users.",0.0,0.0,1.0,0.0
appdynamics,"If you configure AppDynamics for mostly business teams, I can use BTs like I described above.",0.0,0.161,0.839,0.3612
appdynamics,"But If you target the Dev and Ops teams, you can configure your BTs based on method or service calls.",0.0,0.0,1.0,0.0
appdynamics,There is no only single approach on BT &amp; SE configuration.,0.18,0.0,0.82,-0.296
appdynamics,You must shape that with the needs of your AppDynamics users.,0.0,0.0,1.0,0.0
appdynamics,Configuration- Instrumentation- Transaction Detection- Add,0.0,0.0,1.0,0.0
appdynamics,"On the ""Split Transactions Using Request Data"" section you must choose "" Specific URI Segments ""
Segment Numbers: 1,2,4",0.0,0.0,1.0,0.0
appdynamics,"In your case transaction name will be ""/data/scenario/job""",0.0,0.0,1.0,0.0
appdynamics,Sample Configuration:,0.0,0.0,1.0,0.0
appdynamics,"On analyzing the heap dump taken during system idle state, we only see various WebAppClassLoaders holding instances of different library classes.",0.115,0.0,0.885,-0.3818
appdynamics,This pattern is also explained in official blogs of APM experts like  Plumbr  and  Datadog  as a sign of healthy JVM where regular GC activity is occurring and they explain that it means none of the objects will stay in memory forever.,0.0,0.118,0.882,0.6369
appdynamics,From Plumbr blog:,0.0,0.0,1.0,0.0
appdynamics,Seeing the following pattern is a confirmation that the JVM at question is definitely not leaking memory.,0.0,0.153,0.847,0.4019
appdynamics,The reason for the double-sawtooth pattern is that the JVM needs to allocate memory on the heap as new objects are created as a part of the normal program execution.,0.0,0.067,0.933,0.25
appdynamics,Most of these objects are short-lived and quickly become garbage.,0.0,0.0,1.0,0.0
appdynamics,These short-lived objects are collected by a collector called “Minor GC” and represent the small drops on the sawteeth.,0.0,0.0,1.0,0.0
appdynamics,The reason you don't see the browser name in HTTP protocol is because there is no browser.,0.121,0.0,0.879,-0.296
appdynamics,The protocol is a transport level protocol which means it simulates the traffic of the browser without running the actual browser.,0.0,0.0,1.0,0.0
appdynamics,This allows the protocol to simulate many more virtual users than a client level protocol such as TruClient.,0.0,0.0,1.0,0.0
appdynamics,EDIT: There is no dedicated API and you must use the user agent.,0.136,0.185,0.679,0.2023
appdynamics,Please refer to this article for more details:  https://www.appdynamics.com/blog/engineering/how-to-use-appdynamics-with-loadrunner-for-load-testing/,0.0,0.223,0.777,0.3182
appdynamics,I'd try setting &quot;Use data from last&quot; to  60 minutes .,0.0,0.0,1.0,0.0
appdynamics,In Criteria-tab use &quot;Single Metric&quot; with  Sum  of &quot;Calls per Minute&quot; and define your threshold.,0.0,0.0,1.0,0.0
appdynamics,The Drop-off rate is the percentage of user sessions which reach the specific page and then end their session (they do not reach any further pages as part of the session and therefore &quot;drop-off&quot; the map).,0.0,0.061,0.939,0.0516
appdynamics,This can be seen in the example (image) in the  documentation,0.0,0.0,1.0,0.0
appdynamics,Yes - you can do this using ADQL / Analytics searches (assuming you have this licensed).,0.0,0.172,0.828,0.4019
appdynamics,Without specifics I can only give you a general guide:,0.0,0.0,1.0,0.0
appdynamics,Generally the workflow for sending reports from AppDynamics is as follows:,0.0,0.0,1.0,0.0
appdynamics,Note: There is a sample custom dashboard .json available here to get you started:  https://community.appdynamics.com/t5/Knowledge-Base/Sample-Custom-Dashboard-for-Business-Transaction-Report/ta-p/21264,0.0,0.0,1.0,0.0
appdynamics,Note: There are already many &quot;Standard Reports&quot; which could make things easier depending on use case.,0.0,0.157,0.843,0.4215
appdynamics,"Note: If you instead want to export data and then analyse with your own tooling, then see the docs on Public API's available here:  https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs",0.0,0.051,0.949,0.0772
appdynamics,All currently available AppDynamics APIs are documented here:  https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs,0.0,0.0,1.0,0.0
appdynamics,The main page includes a summary of what is available.,0.0,0.0,1.0,0.0
appdynamics,Business transactions should accomplish this.,0.0,0.412,0.588,0.4215
appdynamics,If you want to report on each web service you can build a report or custom dashboard.,0.0,0.08,0.92,0.0772
appdynamics,If you need more assistance just email help@appdynamics.com,0.0,0.0,1.0,0.0
appdynamics,AFAIK the critical point for AppDynamics (or profilers like that) it is essential to find an entry point.,0.111,0.12,0.769,0.0516
appdynamics,"Usually the prefered way is to have an Servlet ""Endpoint"" that starts a threat and can be followed.",0.175,0.0,0.825,-0.5267
appdynamics,"For the scenario you are describing this wouldn't work as it's missing the ""trigger"" to start the following.",0.115,0.0,0.885,-0.296
appdynamics,Most likely you'll need to build your own app-dynamics monitoring extension for it.,0.0,0.0,1.0,0.0
appdynamics,By default much of the Apache stuff is excluded.,0.231,0.0,0.769,-0.34
appdynamics,"Try adding Call Graph Settings (Configure    Instrumentation    Call Graph Settings), to include specific transports, like org.apache.camel.component.file.",0.0,0.143,0.857,0.3612
appdynamics,* in the Specific sub-packages / classes from the Excluded Packages to be included in call graphs section.,0.138,0.0,0.862,-0.34
appdynamics,Do not include org.apache.camel.,0.0,0.0,1.0,0.0
appdynamics,* as it will instrument all the camel code which is very expensive.,0.0,0.0,1.0,0.0
appdynamics,"You may want to do it at first to detect what you want to watch, but make sure to change it back.",0.0,0.216,0.784,0.5023
appdynamics,Edit AppServerAgent\conf\app-agent-config.xml:,0.0,0.0,1.0,0.0
appdynamics,From the Controller web site:,0.0,0.0,1.0,0.0
appdynamics,"Configure    Instrumentation    Call Graph Settings
Add Always Shown Package/Class: org.apache.camel.",0.0,0.0,1.0,0.0
appdynamics,*,0.0,0.0,0.0,0.0
appdynamics,"Servers    App Servers    {tiername}    {nodename}    Agents
App Server Agent
Configure
Use Custom Configuration
find-entry-points: true",0.0,0.167,0.833,0.4215
appdynamics,Can I ask if your trial is beyond 15 days?,0.0,0.0,1.0,0.0
appdynamics,According to their site the trial only includes monitoring for one agent beyond that time - that would explain why only your first agent delivers data.,0.0,0.0,1.0,0.0
appdynamics,"Other tools in that space, e.g: Dynatrace free trial - gives you a bit more time if that's what you need and also allow you to extend the free trial if you need more time",0.0,0.26,0.74,0.8481
appdynamics,You have gone beyond the 15 day pro trial and you are using the free product.,0.0,0.18,0.82,0.5106
appdynamics,"If you want to buy the product you can get an extension on the trial, if you do not want to buy the product it will not trace across JVMs.",0.0,0.085,0.915,0.1531
appdynamics,"Not sure if you already have resolved this, but: SNI is SQL Server Network Interface, and the mentioned method exists in most ADO.NET full call stacks that wait for data from SQL Server.",0.044,0.04,0.916,-0.0338
appdynamics,"This is regardless of whether the higher-level implementation is EF, raw ADO.NET or whatever.",0.0,0.0,1.0,0.0
appdynamics,"I'm not sure which metric or signal AppDynamics uses to capture the completion of a stored procedure execution, but you could be seeing this kind of behavior if your stored procedure completes relatively fast, but transmitting the query result from the server to your client takes a while.",0.032,0.0,0.968,-0.1232
appdynamics,"Without knowing more about your infrastructure, it is very hard to help further.",0.108,0.189,0.703,0.3117
appdynamics,"If the problem still persists, I would recommend running the same query in SQL Server Management studio with SET STATISTICS TIME ON and ""Include Client Statistics"" switched to on.",0.087,0.08,0.833,-0.0516
appdynamics,Perhaps those numbers would give you an idea on whether data transfer is actually the problem.,0.153,0.0,0.847,-0.4019
appdynamics,"In my case it was indeed, as Jouni mentions, very slow transmitting of the query results.",0.0,0.0,1.0,0.0
appdynamics,I use Automapper to prepare data for sending to client.,0.0,0.0,1.0,0.0
appdynamics,"So, it's unclear what exact property caused the load but to be sure I've cut all compound ones I don't need to show on client side.",0.147,0.101,0.752,-0.0872
appdynamics,(I originally needed a collection to show in grid on client side.),0.0,0.0,1.0,0.0
appdynamics,The execution became very fast.,0.0,0.0,1.0,0.0
appdynamics,I've came across similar issue - it turns out that SqlDataReader.Dispose can get stuck for very lengthy time if you break early from large select.,0.08,0.0,0.92,-0.25
appdynamics,see:  https://github.com/dotnet/corefx/issues/29181,0.0,0.0,1.0,0.0
appdynamics,"please see my reply in 
 G1GC long pause with initial-mark",0.0,0.204,0.796,0.3182
appdynamics,your every setting should has a solid reason to be here...and unfortunately some of them don't have e.g.,0.126,0.084,0.789,-0.2023
appdynamics,"-XX:+UseBiasedLocking (used for combination of tenured and young generation GCs, but G1GC is capable to handle both) -XX:+DisableExplicitGC (its unpredictable, in my experience its never restrict explicit gc calls)",0.0,0.186,0.814,0.7332
appdynamics,"please use/tweak accordingly below mentioned settings to get optimum results, I'm giving you baseline to move forward, hope so these settings will work for you:
 Java G1 garbage collection in production",0.0,0.284,0.716,0.8668
appdynamics,"We recorded this bug on 1.7._u06 and upgraded to  1.7.0_21-b11  just a couple of days ago and we haven't seen any Full GC's since upgrade, so it seems like this bug was fixed in JVM.",0.0,0.077,0.923,0.4144
appdynamics,The code cache memory profiles look much nicer now too in the profiler.,0.0,0.195,0.805,0.4404
appdynamics,"In the past, this problem used to be a daily one, one to more times per day.",0.172,0.0,0.828,-0.481
appdynamics,"If the situation changes, I will report back.",0.0,0.0,1.0,0.0
appdynamics,"Until then, I consider this problem solved with the upgrade.",0.256,0.172,0.573,-0.2558
appdynamics,"Your query profile shows that the ""Query end"" time is very large.",0.0,0.0,1.0,0.0
appdynamics,This may be caused by a very (too) large  query cache .,0.0,0.0,1.0,0.0
appdynamics,"Every time you perform an update statement (INSERT, DELETE, UPDATE), the query cache must be updated (every query that reads from the updated tables is invalidated).",0.0,0.0,1.0,0.0
appdynamics,I got in touch with RDS engineers from amazon and they gave me the solution.,0.0,0.25,0.75,0.4588
appdynamics,Such a high latency was due to a very low performing storage type.,0.193,0.0,0.807,-0.3384
appdynamics,"Indeed, I was using the default 5GB SSD (which they call GP2) which gives 3 IOPS per GB of storage, resulting in 15 IOPS when my application required about 50 IOPS or even more.",0.0,0.0,1.0,0.0
appdynamics,"Therefore, they suggested me to change the storage type to  Magnetic  which provides 100 IOPS as baseline.",0.0,0.0,1.0,0.0
appdynamics,"Moreover, I've also been able to decrease the instance type because the bottleneck was only the disk.",0.0,0.0,1.0,0.0
appdynamics,The migration took about 3h due to the very low performance of the source disk (GP2).,0.138,0.0,0.862,-0.3384
appdynamics,Hope it may help someone out there!,0.0,0.541,0.459,0.7088
appdynamics,We never were able to properly solve the issue but at some point it vanished.,0.0,0.091,0.909,0.1027
appdynamics,If I'm not wrong one the client change the appdynamics configuration / removed it which seemed to have solved the issue.,0.0,0.205,0.795,0.5653
appdynamics,did you take a look to this google group ticket issue?,0.0,0.0,1.0,0.0
appdynamics,https://groups.google.com/forum/#!topic/hazelcast/ivk6hzk2YwA,0.0,0.0,1.0,0.0
appdynamics,Here in particular looks the reason of the issue.,0.0,0.0,1.0,0.0
appdynamics,https://github.com/hazelcast/hazelcast/issues/553,0.0,0.0,1.0,0.0
appdynamics,Have you tried to Minify your code?,0.0,0.0,1.0,0.0
appdynamics,Minifying unneccesary characters from your code without removing any functionality.,0.0,0.0,1.0,0.0
appdynamics,This could help speed up the download times.,0.0,0.278,0.722,0.4019
appdynamics,Take a look at the following link:  http://www.htmlgoodies.com/beyond/reference/7-tips-to-make-your-websites-load-faster.html,0.0,0.0,1.0,0.0
appdynamics,Ill take a look at serving assets from webroot,0.267,0.162,0.571,-0.2732
appdynamics,From the book  (emphasis added):,0.0,0.0,1.0,0.0
appdynamics,It’s a well known fact that  serving assets through PHP is guaranteed to be slower than serving those assets without invoking PHP .,0.0,0.234,0.766,0.5423
appdynamics,"And while the core team has taken steps to make plugin and theme asset serving as fast as possible, there may be situations where more performance is required.",0.0,0.085,0.915,0.3612
appdynamics,In these situations it’s recommended that you either symlink or copy out plugin/theme assets to directories in app/webroot with paths matching those used by CakePHP.,0.0,0.132,0.868,0.3612
appdynamics,"Depending on many factors, &quot;slower&quot; can be anywhere between barely-noticeable to barely-usable.",0.0,0.0,1.0,0.0
appdynamics,"This advice is not version specific, and pretty much always applies.",0.0,0.242,0.758,0.4939
appdynamics,"To make assets load faster, let the webserver take care of them for you.",0.0,0.29,0.71,0.5994
appdynamics,"Yes, it is possible for the behaviour of GCs to change over time due to JIT optimisation.",0.0,0.261,0.739,0.6486
appdynamics,One example is 'Escape Analysis' which has been enabled by default since Java 6u23.,0.0,0.116,0.884,0.1779
appdynamics,This type of optimisation can prevent some objects from being created in the heap and therefore not require garbage collection at all.,0.0,0.231,0.769,0.5719
appdynamics,For more information see  Java 7's HotSpot Performance Enhancements .,0.0,0.0,1.0,0.0
appdynamics,It is really difficult to find the exact cause of your problem without more information.,0.297,0.0,0.703,-0.6697
appdynamics,"But I can try to answer to your question : 
 Can the OS block the garbage collection ?",0.216,0.0,0.784,-0.5927
appdynamics,It is very unlikely than your OS blocks the thread garbage collector and let the other threads run.,0.101,0.0,0.899,-0.2263
appdynamics,You should not investigate that way.,0.0,0.0,1.0,0.0
appdynamics,Can the OS block the JVM ?,0.367,0.0,0.633,-0.4404
appdynamics,"Yes it perflecty can and it do it a lot, but so fast than you think that the processes are all running at the same time.",0.0,0.072,0.928,0.2144
appdynamics,jvm is a process like the other and his under the control of the OS.,0.0,0.161,0.839,0.3612
appdynamics,You have to check the cpu used by the application when it hangs (with monitoring on the server not in the jvm).,0.0,0.0,1.0,0.0
appdynamics,If it is very low then I see 2 causes (but there are more) :,0.179,0.0,0.821,-0.3384
appdynamics,"In theory, YES, it can.",0.0,0.462,0.538,0.5319
appdynamics,"But it practice, it never should.",0.0,0.0,1.0,0.0
appdynamics,"In most Java virtual machines, application threads are not the only threads that are running.",0.0,0.0,1.0,0.0
appdynamics,"Apart from the application threads, there are compilation threads, finalizer threads, garbage collection threads, and some more.",0.0,0.0,1.0,0.0
appdynamics,"Scheduling decisions for allocating CPU cores to these threads and other threads from other programs running on the machine are based on many parameters (thread priorities, their last execution time, etc), which try be fair to all threads.",0.0,0.059,0.941,0.3182
appdynamics,"So, in practice no thread in the system should be waiting for CPU allocation for an unreasonably long time and the operating system should not block any thread for an unlimited amount of time.",0.067,0.065,0.868,-0.0149
appdynamics,There is minimal activity that the garbage collection threads (and other VM threads) need to do.,0.0,0.0,1.0,0.0
appdynamics,They need to check periodically to see if a garbage collection is needed.,0.0,0.0,1.0,0.0
appdynamics,"Even if the application threads are all suspended, there could be other VM threads, such as the JIT compiler thread or the finalizer thread, that do work and ,hence, allocate objects and trigger garbage collection.",0.084,0.0,0.916,-0.4767
appdynamics,This is particularly true for meta-circular JVM that implement VM threads in Java and not in a C/C++;,0.0,0.162,0.838,0.4754
appdynamics,"Moreover, most modern JVM use a generational garbage collector (A garbage collector that partitions the heap into separate spaces and puts objects with different ages in different parts of the heap) This means as objects get older and older, they need to be moved to other older spaces.",0.0,0.0,1.0,0.0
appdynamics,"Hence, even if there is no need to collect objects, a generational garbage collector may move objects from one space to another.",0.099,0.0,0.901,-0.296
appdynamics,Of course the details of each garbage collector in different from JVM to JVM.,0.0,0.0,1.0,0.0
appdynamics,"To put more salt on the injury, some JVMs support more than one type of garbage collector.",0.137,0.132,0.732,-0.0258
appdynamics,But seeing a minimal garbage collection activity in an idle application is no surprise.,0.17,0.161,0.669,-0.0387
appdynamics,Does your OS have swapping enabled.,0.0,0.0,1.0,0.0
appdynamics,"I've noticed HUGE problems with Java once it fills up all the ram on an OS with swapping enabled--it will actually devistate windows systems, effictevly locking them up and causing a reboot.",0.078,0.087,0.835,0.0857
appdynamics,My theory is this:,0.0,0.0,1.0,0.0
appdynamics,"At first it doesn't effect the system much, but if you try to launch an app that wants a bunch of memory it can take a really long time, and your system just keeps degrading.",0.14,0.0,0.86,-0.7351
appdynamics,"Multiple large VMs can make this worse, I run 3 or 4 huge ones and my system now starts to sieze when I get over 60-70% RAM usage.",0.13,0.082,0.788,-0.3237
appdynamics,This is conjecture but it describes the behavior I've seen after days of testing.,0.0,0.0,1.0,0.0
appdynamics,"The effect is that all the swapping seems to ""Prevent"" gc.",0.0,0.0,1.0,0.0
appdynamics,More accurately the OS is spending most of the GC time swapping which makes it look like it's hanging doing nothing during GC.,0.0,0.102,0.898,0.3612
appdynamics,"A fix--set -Xmx to a lower value, drop it until you allow enough room to avoid swapping.",0.313,0.207,0.481,-0.296
appdynamics,"This has always fixed my problem, if it doesn't fix yours then I'm wrong about the cause of your problem :)",0.298,0.105,0.596,-0.6705
appdynamics,One major GC per minute doesn't at all seem troublesome.,0.268,0.0,0.732,-0.5106
appdynamics,"Usually it takes about half a second, so that's 1/120th of your overall CPU usage.",0.0,0.0,1.0,0.0
appdynamics,It is also quite natural that heavy application load results in more memory allocation.,0.0,0.177,0.823,0.4201
appdynamics,Apparently you are allocating some objects that live on for a while (could be caching).,0.0,0.0,1.0,0.0
appdynamics,My conclusion: the demonstrated GC behavior is not a proof that there is something wrong with your application's memory allocation.,0.147,0.0,0.853,-0.4767
appdynamics,I have looked more carefully at your diagrams (unfortunately they are quite difficult to read).,0.168,0.108,0.724,-0.25
appdynamics,"You don't have one GC per min; you have 60  seconds  of major GC per minute, which would mean it's happening all the time.",0.0,0.0,1.0,0.0
appdynamics,That  does  look like major trouble; in fact in those conditions you usually get an OOME due to &quot;GC time percentage threshold crossed&quot;.,0.103,0.095,0.802,-0.0516
appdynamics,Note that the CMS collector which you are using is actually slower than the default one; its advantage is only that it doesn't &quot;stop the world&quot; as much.,0.0,0.069,0.931,0.25
appdynamics,It may not be the best choice for you.,0.296,0.0,0.704,-0.5216
appdynamics,"But you do look to either have a memory leak, or a general issue in program design.",0.181,0.0,0.819,-0.4767
appdynamics,Are you always waiting for GC to take care of removing unused references?,0.0,0.211,0.789,0.4939
appdynamics,"Is there some places in your application that you know a reference to a heavy weight object won't be used anymore from that point, but it is not manually nulled?",0.0,0.0,1.0,0.0
appdynamics,Perhaps setting such heavy weight objects to null manually at the right places could prevent them growing till they hit the end of the heap....,0.0,0.109,0.891,0.2023
appdynamics,"When JVM reaches heap maximum size, GC is called more frequently to prevent OOM exception.",0.0,0.165,0.835,0.144
appdynamics,Normal program execution should not happen at such circumstances.,0.0,0.0,1.0,0.0
appdynamics,"When a new object is allocated and JVM cant get enough of free space, GC is called.",0.0,0.18,0.82,0.5106
appdynamics,This might postpone object allocation process and thus slowdown overall performance.,0.16,0.0,0.84,-0.2263
appdynamics,Garbage collection happens not concurrently in this case and you do not benefit from CMS collector.,0.142,0.0,0.858,-0.357
appdynamics,"Try to play with  CMSInitiatingOccupancyFraction , its default value is about 90%.",0.0,0.348,0.652,0.5859
appdynamics,"Setting this parameter to lower values, will force garbage collection before application reaches heap maximum.",0.122,0.215,0.663,0.1779
appdynamics,Thus GC will work in parallel with application not clashing with it.,0.0,0.0,1.0,0.0
appdynamics,Have a look at article  Starting a Concurrent Collection Cycle .,0.0,0.0,1.0,0.0
appdynamics,"Looking at the  source code  of  UTF8JsonGenerator._flushBuffer() , there is no indication of  LockSupport.parkNanos() .",0.155,0.0,0.845,-0.296
appdynamics,So it has probably been inlined by the JIT compiler from  OutputStream.write() .,0.0,0.0,1.0,0.0
appdynamics,My guess is it's the place where – for your application – Tomcat typically waits until the client has accepted all the output (expect for the last piece that fits into the typical connection buffer size) before it can close the connection.,0.0,0.051,0.949,0.2732
appdynamics,We have had bad experience with slow clients in the past.,0.259,0.0,0.741,-0.5423
appdynamics,"Until they have retrieved all the output, they block a thread in Tomcat.",0.209,0.0,0.791,-0.4404
appdynamics,And blocking a few dozens threads in Tomcat seriously reduces the throughput of a busy web app.,0.249,0.0,0.751,-0.5106
appdynamics,Increasing the number of threads isn't the best option as the blocked threads also occupy a considerable amount of memory.,0.24,0.057,0.703,-0.6331
appdynamics,So what you want is that Tomcat can handle a request as quickly as possible and then move on to the next request.,0.0,0.069,0.931,0.144
appdynamics,"We have solved the problem by configuring our reverse proxy, which we always had in front of Tomcat, to immediately consume all output from Tomcat and deliver it to the client at the client's speed.",0.071,0.056,0.873,-0.1531
appdynamics,The reverse proxy is very efficient at handling slow clients.,0.0,0.256,0.744,0.4754
appdynamics,"In our case, we have used  nginx .",0.0,0.0,1.0,0.0
appdynamics,We also looked at  Apache httpd .,0.0,0.0,1.0,0.0
appdynamics,"But at the time, it wasn't capable of doing it.",0.236,0.0,0.764,-0.4168
appdynamics,Additional Note,0.0,0.0,1.0,0.0
appdynamics,Clients that unexpectedly disconnect also look like slow clients to the server as it takes some time until it has been fully established that the connection is broken.,0.098,0.079,0.823,-0.1531
appdynamics,"You could use Javamelody, but you have to:",0.0,0.0,1.0,0.0
appdynamics,Generate war file from your play framework web,0.317,0.195,0.488,-0.3612
appdynamics,Edit web.xml in war file (http://code.google.com/p/javamelody/wiki/UserGuide?tm=6),0.438,0.0,0.562,-0.5994
appdynamics,You need to set Multi-Release to true in the jar's MANIFEST.MF.,0.0,0.219,0.781,0.4215
appdynamics,In the assembly plugin you should be able to do that by adding,0.0,0.0,1.0,0.0
appdynamics,to the configuration section of your assembly configuration.,0.0,0.0,1.0,0.0
appdynamics,You could also use the jar plugin to create your jar.,0.0,0.174,0.826,0.2732
appdynamics,For that you would do,0.0,0.0,1.0,0.0
appdynamics,Use Stackify Prefix for this kind of things:,0.0,0.0,1.0,0.0
appdynamics,https://stackify.com/prefix/,0.0,0.0,1.0,0.0
appdynamics,"View SQL queries: Including SQL parameters, affected records and how long it took to download the result set.",0.086,0.0,0.914,-0.1531
appdynamics,"use tracer plugin where u can import the information in csv/html/xml.This is for jvisualvm
 http://visualvm.java.net/plugins.html",0.0,0.0,1.0,0.0
appdynamics,Start the application and run jconsole on the PID.,0.0,0.0,1.0,0.0
appdynamics,While its running look at the heap in the console.,0.0,0.0,1.0,0.0
appdynamics,When it near maxes get a heap dump.,0.302,0.0,0.698,-0.3818
appdynamics,Download Eclipse MAT and parse the heap dump.,0.271,0.0,0.729,-0.3818
appdynamics,If you notice the retained heap size is vastly less then the actual binary file parse the heap dump with -keep_unreachable_objects being set.,0.105,0.045,0.85,-0.3612
appdynamics,If the latter is true and you are doing a full GC often you probably have some kind of leak going on.,0.11,0.114,0.776,0.0276
appdynamics,"Keep in mind when I say leak I don't mean a leak where the GC cannot retain memory, rather some how you are building large objects and making them unreachable often enough to cause the GC to consume a lot of CPU time.",0.115,0.0,0.885,-0.5859
appdynamics,If you were seeing true memory leaks you would see GC Over head reached errors,0.129,0.226,0.645,0.2023
appdynamics,You can get  POD_NAME  and  POD_NAMESPACE  passing them as environment variables via  fieldRef .,0.0,0.0,1.0,0.0
appdynamics,EDIT :  Added example env  REFERENCE_EXAMPLE  to show how to reference variables.,0.0,0.0,1.0,0.0
appdynamics,Thanks to  this  answer for pointing out the  $()  interpolation.,0.0,0.244,0.756,0.4404
appdynamics,"You can reference  supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP  as mentioned in the documentation  here .",0.0,0.128,0.872,0.3612
appdynamics,"However,  CLUSTERNAME  is not a standard property available.",0.0,0.0,1.0,0.0
appdynamics,"According to this  PR #22043 , the  CLUSTERNAME  should be injected to the  .metadata  field if using GCE.",0.0,0.0,1.0,0.0
appdynamics,"Otherwise, you'll have to specific the  CLUSTERNAME  manually in the  .metadata  field  and then use  fieldRef  to inject it as an environment variable.",0.0,0.0,1.0,0.0
appdynamics,"Below format helped me, suggested by ewok2030 and Praveen.",0.0,0.0,1.0,0.0
appdynamics,Only one thing to make sure that the variable should be declared before they are used as JAVA_OPTS.,0.0,0.119,0.881,0.3182
appdynamics,containers:,0.0,0.0,1.0,0.0
appdynamics,Are you sure you're not getting the FileNotFound when trying to do getInputStream on the connection later (or in your real code in case this is a simplified example)?,0.0,0.078,0.922,0.3182
appdynamics,Since HttpUrlConnection implements http protocol and 4xx codes are error codes trying to call getInputStream generates FNF.,0.144,0.0,0.856,-0.4019
appdynamics,Instead you should call getErrorStream.,0.0,0.0,1.0,0.0
appdynamics,I ran your code (without the auth part) and I don't get any FilenotFoundException testing on url's that return 404.,0.0,0.0,1.0,0.0
appdynamics,So in this case HttpUrlConnection correctly implements the http protocol and appdynamics correctly catches the error.,0.153,0.0,0.847,-0.4019
appdynamics,"I'm facing the same issue with jersey wrapping the FnF in a UniformResourceException but after some analyzing it's actually either jersey that should provide ways of checking the status code before returning output or correctly use httpurlconnection, and in our case - the webservice should not return 404 for requests that yields no found results but rather an empty collection.",0.082,0.0,0.918,-0.6124
appdynamics,"Not sure whether it would be the cause of memory leaks, but your function definitely has a ton of unnecessary cruft that could be cleaned up with Bluebird's  Promise.mapSeries()  and other Bluebird helpers.",0.04,0.169,0.791,0.6926
appdynamics,Doing so may very well help cut down on memory leaks.,0.148,0.352,0.5,0.5059
appdynamics,doSomething  function reduced to 8 lines:,0.0,0.0,1.0,0.0
appdynamics,After this operations,0.0,0.0,1.0,0.0
appdynamics,will be,0.0,0.0,1.0,0.0
appdynamics,and,0.0,0.0,1.0,0.0
appdynamics,will be,0.0,0.0,1.0,0.0
appdynamics,"Since you are a Spring Framework user, consider using  Spring AMQP .",0.0,0.0,1.0,0.0
appdynamics,The  RabbitTemplate  uses cached channels over a single connection with the channel being checked out of the cache (and returned) for each operation.,0.0,0.0,1.0,0.0
appdynamics,"The default cache size is 1, so it generally needs to be configured for an environment like yours.",0.0,0.128,0.872,0.3612
appdynamics,I'm leaning towards it being an issue with the number of channels and the channel cache size.,0.0,0.075,0.925,0.0772
appdynamics,Does anyone know if there's a limit on the number of channels on a queue?,0.0,0.098,0.902,0.0772
appdynamics,It seems like specifying connections rather than channels might help here.,0.0,0.366,0.634,0.6369
appdynamics,"If anyone has any information it would really help,  getting stuck for time :)",0.118,0.311,0.571,0.576
appdynamics,I think you should use  @PersistenceContext  annotation to obtain  EntityManager  from Spring context and  @Transactional  annotation to drive your transactions.,0.0,0.0,1.0,0.0
appdynamics,"This way you won't need to worry about closing Hibernate sessions manually, and number of connections will not increase until there are  too many connections .",0.071,0.134,0.795,0.1887
appdynamics,"The problem did not lay in the sessions but in the connection pool, sessions where being managed correctly, but the connection pool in the JDBC layer wasn't closing the connections.",0.06,0.0,0.94,-0.2144
appdynamics,These are the things that I did to fix them.,0.0,0.0,1.0,0.0
appdynamics,JDBC context configuration,0.0,0.0,1.0,0.0
appdynamics,1.- Changed the JDBC connection factory from tomcat's old BasicDataSourceFactory to tomcat's new DataSourceFactory,0.0,0.0,1.0,0.0
appdynamics,"2.-  Tuned the JDBC settings based on this article: 
 http://www.tomcatexpert.com/blog/2010/04/01/configuring-jdbc-pool-high-concurrency",0.0,0.0,1.0,0.0
appdynamics,Session factory xml configuration,0.0,0.0,1.0,0.0
appdynamics,3.- Deleted this line from the session factory configuration:,0.0,0.0,1.0,0.0
appdynamics,This is how my configuration ended up:,0.0,0.0,1.0,0.0
appdynamics,JDBC context configuration,0.0,0.0,1.0,0.0
appdynamics,Session factory xml configuration,0.0,0.0,1.0,0.0
appdynamics,"Hey there, Just crawl in to this URL.",0.0,0.0,1.0,0.0
appdynamics,https://help.ubuntu.com/community/EnvironmentVariables,0.0,0.0,1.0,0.0
appdynamics,"It will better help you.The above URL gives all informations about
  Environment Variable Ubuntu.",0.0,0.318,0.682,0.6808
appdynamics,"ANd the above POST is updated in 3 Jan
  2014.",0.0,0.0,1.0,0.0
appdynamics,Put the environment variables into the global /etc/environment file:,0.0,0.0,1.0,0.0
appdynamics,...,0.0,0.0,1.0,0.0
appdynamics,...,0.0,0.0,1.0,0.0
appdynamics,"Execute ""source /etc/environment"" in every shell where you want the variables to be updated:",0.0,0.091,0.909,0.0772
appdynamics,Check that it works:,0.0,0.0,1.0,0.0
appdynamics,Here is a other link from mkyong,0.0,0.0,1.0,0.0
appdynamics,"Here is some info on environment variables, setting your path and where to install things that I hope is useful for you to get your environment setup.",0.0,0.195,0.805,0.7003
appdynamics,.bashrc : is specific for the  bash  shell.,0.0,0.0,1.0,0.0
appdynamics,".profile : is used by several shells, and was originally used by the bourne shell (from memory).",0.0,0.0,1.0,0.0
appdynamics,.profile  might not be loaded by bash if there is a  .bashrc  present.,0.0,0.0,1.0,0.0
appdynamics,Some shells read it only if there is no shell specific configuration present.,0.155,0.0,0.845,-0.296
appdynamics,"If you happen to be using a different shell, you will need to look at how best to configure environment variables for that shell.",0.0,0.16,0.84,0.6369
appdynamics,"Note that adding to the above files only effect the user that you set them for, since they live in  /home/username/ .",0.0,0.0,1.0,0.0
appdynamics,"Also remember to source the file again, or reload the shell so that your settings take effect.",0.0,0.0,1.0,0.0
appdynamics,You can achieve this with something like  source .bashrc  after you edit it at the command line to avoid having to restart or reopen terminal.,0.079,0.09,0.83,0.0772
appdynamics,"If you would like to set system wide variables, you can do that in  /etc/environment .",0.0,0.152,0.848,0.3612
appdynamics,"If you would like to execute java / ant / maven, etc.",0.0,0.217,0.783,0.3612
appdynamics,"from the command line, or enable applications that require the  PATH  environment variable to be set correctly to work, you will also need to add the  ./bin  directories to your PATH.",0.0,0.0,1.0,0.0
appdynamics,Depending on your preference regarding system wide or user specific path settings:,0.0,0.0,1.0,0.0
appdynamics,etc.,0.0,0.0,1.0,0.0
appdynamics,in the relevant file.,0.0,0.0,1.0,0.0
appdynamics,"A side point and entirely optional, the correct place to install java, ant, maven, etc if not from .deb's would be in  /opt , according to the  FilesystemHierarchyStandard",0.0,0.0,1.0,0.0
appdynamics,You might check out Spring Insight.,0.0,0.0,1.0,0.0
appdynamics,"Spring-insight source, design and alternatives",0.0,0.0,1.0,0.0
appdynamics,http://www.springsource.org/insight,0.0,0.0,1.0,0.0
appdynamics,Java Melody  might be relevant for you.,0.0,0.0,1.0,0.0
appdynamics,This is an issue with the AppDynamics plugin and gradle plugin tools 3.6.x.,0.0,0.0,1.0,0.0
appdynamics,You can see this link:  https://community.appdynamics.com/t5/End-User-Monitoring-EUM/AppDynamic-EUM-setup-for-Android-Cordova-project/td-p/38864,0.0,0.0,1.0,0.0
appdynamics,I have downgrade gradle plugin tools to 3.5.x and solve it,0.0,0.167,0.833,0.2023
appdynamics,"You can create a cron job to run from Monday to Saturday, here for each hour:",0.0,0.13,0.87,0.2732
appdynamics,"And another to cover the interval you want on Sunday, here one by one hour from 10:00 AM to 03:00 PM:",0.0,0.061,0.939,0.0772
appdynamics,Looks like httprequest plugin does not support uploading zip file.,0.177,0.196,0.627,0.0624
appdynamics,This is my observation.,0.0,0.0,1.0,0.0
appdynamics,I think upload will use  Content-Type: multipart/form-data .,0.0,0.0,1.0,0.0
appdynamics,But httpRequest plugin is not supporting this type.,0.308,0.0,0.692,-0.4782
appdynamics,However it supports  APPLICATION_OCTETSTREAM(ContentType.APPLICATION_OCTET_STREAM),0.0,0.455,0.545,0.3612
appdynamics,Could you post output from your curl?,0.0,0.0,1.0,0.0
appdynamics,Following Code worked for me :,0.0,0.0,1.0,0.0
appdynamics,It turns out the code I was given was completely wrong for angular 2 implementation.,0.22,0.0,0.78,-0.5256
appdynamics,The code they gave me is for running on the web server's side with node js.,0.0,0.0,1.0,0.0
appdynamics,"Since angular 2 is an SPA that runs on the browser, it would never work.",0.0,0.0,1.0,0.0
appdynamics,I did some research and found this example application that I added a few tweaks to:  https://github.com/derrekyoung/appd-sampleapp-angular2,0.0,0.0,1.0,0.0
appdynamics,"in your appdynamics.cfg, change",0.0,0.0,1.0,0.0
appdynamics,to,0.0,0.0,1.0,0.0
appdynamics,"I am not exactly sure what the problem is, but I just tried it with AppDynamics Controller Version 4.2 to share a custom dashboard and open the shared Link in a different browser and I didn't need to put in any credentials.",0.079,0.14,0.781,0.5528
appdynamics,Maybe AppDynamics Version 3.9 had a bug (assuming you used version 3.9 according to the docs link you posted),0.0,0.0,1.0,0.0
appdynamics,Here is what I did:,0.0,0.0,1.0,0.0
appdynamics,"CloudForms is not trying to substitute OpenShift UI, but complement it, giving you more information about the environment and providing additional capabilities on top of OPenShift.",0.0,0.181,0.819,0.6486
appdynamics,You can see information about what is being done and demos in videos:,0.0,0.0,1.0,0.0
appdynamics,https://www.youtube.com/watch?v=FVLo9Nc_10E&amp;list=PLQAAGwo9CYO-4tQsnC6oWgzhPNOykOOMd&amp;index=15,0.0,0.0,1.0,0.0
appdynamics,"And you can find the presentations here
 http://www.slideshare.net/ManageIQ/presentations",0.0,0.0,1.0,0.0
appdynamics,Just follow this istructions:,0.0,0.0,1.0,0.0
appdynamics,"Clone a local copy of this project with
git clone  https://github.com/Appdynamics/ECommerce-Android",0.0,0.0,1.0,0.0
appdynamics,Open Android Studio and Import Project (select app/build.gradle),0.0,0.0,1.0,0.0
appdynamics,Android Studio will ask you to build the project with gradle.,0.0,0.0,1.0,0.0
appdynamics,Gradle will use the  build.gradle  inside the project.,0.0,0.0,1.0,0.0
appdynamics,The version of gradle is inside the  gradle/wrapper/gradle-wrapper.properties  file:,0.0,0.0,1.0,0.0
appdynamics,Check for example the file inside the  app :,0.0,0.0,1.0,0.0
appdynamics,"You need put a directory called ""adeum-maven-repo"" in your project setup .",0.0,0.0,1.0,0.0
appdynamics,https://docs.appdynamics.com/display/PRO39/Instrument+an+Android+Application#InstrumentanAndroidApplication-SetupforGradle,0.0,0.0,1.0,0.0
appdynamics,Whether this is an issue or not is really up to you to decide.,0.0,0.0,1.0,0.0
appdynamics,"On the one hand, a full GC that runs for one second every 10 minutes is not a significant issue for throughput.",0.07,0.14,0.79,0.3834
appdynamics,"On the other hand, the full GC is probably going to dramatically reduce response times during that those one second windows.",0.0,0.138,0.862,0.4939
appdynamics,But that may not be important or even relevant to your application.,0.146,0.0,0.854,-0.2235
appdynamics,The thing I would be worried about is whether your load testing is a realistic test.,0.145,0.0,0.855,-0.296
appdynamics,"The application appears to need 4Gb of heap space under test, but is it also going to need that in a real-world use?",0.0,0.0,1.0,0.0
appdynamics,I'd be worried that a memory leak might show up when it is deployed into production.,0.261,0.0,0.739,-0.5574
appdynamics,Or that the load in your load testing is causing the application's in-memory caching to reach a steady state in that won't be reproduced in production.,0.0,0.044,0.956,0.0258
appdynamics,"As a general rule, it is a bad thing for the heap to be running close to full, so an increase in the heap is probably advisable.",0.12,0.089,0.791,-0.2315
appdynamics,"Your application's performance doesn't appear to be suffering, but you could be ""on the cusp"".",0.128,0.0,0.872,-0.2617
appdynamics,"I thought that once old gen utilization is above certain level, GC will try to free/compact old gen area and if nothing is freed it would either fail with OOM and start crazy full GC cycles just before it.",0.184,0.047,0.768,-0.7234
appdynamics,I suspect that the monitoring reports might be misleading you.,0.412,0.0,0.588,-0.5994
appdynamics,"If the full GC cycles were  really  not reclaiming anything, I'd expect the behaviour to be different.",0.0,0.0,1.0,0.0
appdynamics,"Try turning on JVM GC log message, and see what they tell you about the amount of memory that the full GC cycles are managing to reclaim.",0.0,0.0,1.0,0.0
appdynamics,My answer with links got deleted by another SO user so I'm listing the steps here.,0.0,0.0,1.0,0.0
appdynamics,First uninstall using this,0.0,0.0,1.0,0.0
appdynamics,"$ npm uninstall -g strong-cli
$ npm uninstall -g loopback-sdk-angular-cli",0.0,0.0,1.0,0.0
appdynamics,and then install,0.0,0.0,1.0,0.0
appdynamics,npm install -g strongloop,0.0,0.0,1.0,0.0
appdynamics,You can now run slc strongops,0.0,0.0,1.0,0.0
appdynamics,and let us know how it goes.,0.0,0.0,1.0,0.0
appdynamics,"I created a Linux init.d Daemon script which you can use to run your app with slc as service:
 https://gist.github.com/gurdotan/23311a236fc65dc212da",0.0,0.105,0.895,0.25
appdynamics,Might be useful to some of you.,0.0,0.326,0.674,0.4404
appdynamics,You should try Compuware's dynaTrace,0.0,0.0,1.0,0.0
appdynamics,"i don't think that you can do full-featured profiling of distributed request across number of JVM's - AppDynamics from what i can remember understands the EE stuff - like calling DB, EJB, RMI, or remote webservice - however it still works in scope of JVM.",0.0,0.091,0.909,0.4215
appdynamics,"Isn't it suffient in your case just to use java profiler (like yourkit, jprofiler)?",0.0,0.0,1.0,0.0
appdynamics,you can try 24x7monitoring,0.0,0.0,1.0,0.0
appdynamics,https://code.google.com/p/monitor-24x7/,0.0,0.0,1.0,0.0
appdynamics,"it provides method level monitoring, SQL queries, business transactions...",0.0,0.0,1.0,0.0
appdynamics,Did you try the free version of AppDynamics.,0.0,0.32,0.68,0.5106
appdynamics,It's called AppDynamics LITE.,0.0,0.0,1.0,0.0
appdynamics,You can take a look also to EXTRAHOP free version.,0.0,0.292,0.708,0.5106
appdynamics,Maybe it is good enough for your needs.,0.0,0.293,0.707,0.4404
appdynamics,Also you can try using a SaaS solutions such as NewRelic or Boundary.,0.0,0.134,0.866,0.1779
appdynamics,They have free accounts that could also be good enough for your needs.,0.0,0.36,0.64,0.7351
appdynamics,"Finally if you want to monitor the performance of any specific JAVA application, you can use  http://www.moskito.org/ .",0.0,0.075,0.925,0.0772
appdynamics,It's totaly FREE.,0.0,0.668,0.332,0.6166
appdynamics,can you tell me the methodology for deriving those stats?,0.0,0.0,1.0,0.0
appdynamics,"In most cases, this behavior comes down to poor code implementation in a custom portlet or a JVM configuration issue and more likely the latter.",0.124,0.0,0.876,-0.4767
appdynamics,Take a look at the  Ehcache Garbage Collection Tuning Documentation .,0.0,0.0,1.0,0.0
appdynamics,"When you run  jstat , how do the garbage collection times look?",0.0,0.0,1.0,0.0
appdynamics,"I realize it's been a while, but I'll contribute anyway as it may help other people too.",0.0,0.202,0.798,0.5499
appdynamics,"In my case, importing Instrumentation in iOS caused this error; it seems to be a problem in the latest version of @appdynamics/react-native-agent (version 20.7.0 as of writing).",0.195,0.0,0.805,-0.7027
appdynamics,"I instead initialized AppDynamics in native code (in the AppDelegate.m file), as follows:",0.0,0.0,1.0,0.0
appdynamics,"For more info, check the iOS guide:
 https://docs.appdynamics.com/display/PRO45/Instrument+an+iOS+Application",0.0,0.0,1.0,0.0
appdynamics,"Additionally, I avoided importing AppDynamics in javascript by requiring it in runtime, in Android only.",0.156,0.0,0.844,-0.34
appdynamics,"There's a lot of different questions here, and some of them don't have answers without more information about your specific setup, but I'll try to give you a good overview.",0.0,0.125,0.875,0.5927
appdynamics,Why Tracing?,0.0,0.0,1.0,0.0
appdynamics,You've already intuited that there are a lot of similarities between &quot;APM&quot; and &quot;tracing&quot; - the differences are fairly minimal.,0.0,0.0,1.0,0.0
appdynamics,"Distributed Tracing is a superset of capabilities marketed as APM (application performance monitoring) and RUM (real user monitoring), as it allows you to capture performance information about the work being done in your services to handle a single, logical request both at a per-service level, and at the level of an entire request (or transaction) from client to DB and back.",0.0,0.0,1.0,0.0
appdynamics,"Trace data, like other forms of telemetry, can be aggregated and analyzed in different ways - for example, unsampled trace data can be used to generate RED (rate, error, duration) metrics for a given API endpoint or function call.",0.067,0.062,0.871,-0.0516
appdynamics,"Conventionally, trace data is annotated (tagged) with properties about a request or the underlying infrastructure handling a request (things like a customer identifier, or the host name of the server handling a request, or the DB partition being accessed for a given query) that allows for powerful exploratory queries in a tool like Jaeger or a commercial tracing tool.",0.0,0.137,0.863,0.7783
appdynamics,Sampling,0.0,0.0,1.0,0.0
appdynamics,The overall performance impact of generating traces varies.,0.0,0.0,1.0,0.0
appdynamics,"In general, tracing libraries are designed to be fairly lightweight - although there are a lot of factors that influence this overhead, such as the amount of attributes on a span, the log events attached to it, and the request rate of a service.",0.0,0.0,1.0,0.0
appdynamics,"Companies like Google will aggressively sample due to their scale, but to be honest, sampling is more beneficial to consider from a long-term storage perspective rather than an up-front overhead perspective.",0.043,0.275,0.682,0.8702
appdynamics,"While the additional overhead per-request to create a span and transmit it to your tracing backend might be small, the cost to store trace data over time can quickly become prohibitive.",0.0,0.068,0.932,0.2732
appdynamics,"In addition, most traces from most systems aren't terribly interesting.",0.168,0.237,0.595,0.2334
appdynamics,This is why dynamic and tail-based sampling approaches have become more popular.,0.0,0.363,0.637,0.6901
appdynamics,"These systems move the sampling decision from an individual service layer to some external process, such as the  OpenTelemetry Collector , which can analyze an entire trace and determine if it should be sampled in or out based on user-defined criteria.",0.0,0.0,1.0,0.0
appdynamics,"You could, for example, ensure that any trace where an error occurred is sampled in, while 'baseline' traces are sampled at a rate of 1%, in order to preserve important error information while giving you an idea of steady-state performance.",0.117,0.147,0.736,0.1027
appdynamics,Proprietary APM vs. OSS,0.0,0.0,1.0,0.0
appdynamics,One important distinction between something like AppDynamics or New Relic and tools like Jaeger is that Jaeger does not rely on proprietary instrumentation agents in order to generate trace data.,0.0,0.201,0.799,0.7003
appdynamics,"Jaeger supports OpenTelemetry, allowing you to use open source tools like the  OpenTelemetry Java Automatic Instrumentation  libraries, which will automatically generate spans for many popular Java frameworks and libraries, such as Spring.",0.0,0.212,0.788,0.7783
appdynamics,"In addition, since OpenTelemetry is available in multiple languages with a shared data format and trace context format, you can guarantee that your traces will work properly in a polyglot environment (so, if you have Node.JS or Golang services in addition to your Java services, you could use OpenTelemetry for each language, and trace context propagation would work seamlessly between all of them).",0.0,0.069,0.931,0.5267
appdynamics,"Even more advantageous, though, is that your instrumentation is decoupled from a specific vendor or tool.",0.0,0.166,0.834,0.4201
appdynamics,"You can instrument your service with OpenTelemetry and then send data to one - or more - analysis tools, both commercial and open source.",0.0,0.0,1.0,0.0
appdynamics,"This frees you from vendor lock-in, and allows you to select the best tool for the job.",0.0,0.299,0.701,0.7506
appdynamics,"If you'd like to learn more about OpenTelemetry, observability, and other topics I wrote a longer series that you can find  here  (look for the other 'OpenTelemetry 101' posts).",0.0,0.088,0.912,0.3612
appdynamics,Windows 10 64-bit.,0.0,0.0,1.0,0.0
appdynamics,Powershell 5.,0.0,0.0,1.0,0.0
appdynamics,Does not require admin privileges.,0.353,0.0,0.647,-0.2924
appdynamics,How to quickly and efficiently extract text from a large logfile from input1 to input2 using powershell 5?,0.0,0.144,0.856,0.4019
appdynamics,Sample logfile below.,0.0,0.0,1.0,0.0
appdynamics,For testing purposes copy your logfile to the desktop and name it logfile.txt,0.0,0.0,1.0,0.0
appdynamics,What is the default text editor in Windows Server 2012 R2 Standard 64-bit?,0.0,0.0,1.0,0.0
appdynamics,See line 42.,0.0,0.0,1.0,0.0
appdynamics,What program do you have associated with .txt files?,0.0,0.0,1.0,0.0
appdynamics,See line 42.,0.0,0.0,1.0,0.0
appdynamics,logfile.txt:,0.0,0.0,1.0,0.0
appdynamics,Results of running script on logfile.txt:,0.0,0.0,1.0,0.0
appdynamics,Powershell in four hours at Youtube,0.0,0.0,1.0,0.0
appdynamics,Parse and extract text with powershell at Bing,0.0,0.0,1.0,0.0
appdynamics,How to quickly and efficiently extract text from a large logfile from line x to line y using powershell 5?,0.0,0.144,0.856,0.4019
appdynamics,How to quickly and efficiently extract text from a large logfile from date1 to date2 using powershell 5?,0.0,0.144,0.856,0.4019
appdynamics,curl's  --user  command line option sets up HTTP authentication for the request ( username:password ).,0.0,0.0,1.0,0.0
appdynamics,"In the case of AppDynamics' Configuration API (which is a subset of their Controller API),  user1@customer1:secret  are your account credentials in the format documented  here :",0.0,0.0,1.0,0.0
appdynamics,"You should not be retrieving any embedded resources in your load test, you need to limit the scope of the embedded resources scanning to  your application under test domain only",0.0,0.0,1.0,0.0
appdynamics,"Any external domains like  googleapis  or   appdynamics  must be excluded via  URLs must match  input (lives at ""Advanced"" tab of the  HTTP Request  sampler, or even better  HTTP Request Defaults )",0.069,0.155,0.776,0.4588
appdynamics,More information:  Excluding Domains from the Load Test,0.0,0.0,1.0,0.0
appdynamics,The problem with this approach is you'll have to manually do that each time.,0.172,0.0,0.828,-0.4019
appdynamics,I would highly recommend just configuring your app server to automatically load the AppDynamics agent.,0.0,0.177,0.823,0.4201
appdynamics,"Another option is using the universal agent, which does auto-attach:  https://docs.appdynamics.com/display/PRO43/Install+the+Universal+Agent  Doing this one off attach is never really a good idea, as you'll have to get the PID each time.",0.083,0.0,0.917,-0.3865
appdynamics,"The error indicates that you are probably not running the attach as the same user the JVM is running under, but it could also be permissions or something else as well, hence I would use the methods that work all the time :)",0.039,0.14,0.821,0.7003
appdynamics,Use Dependency Injection instead of creating the actual WCF endpoints and passing them around.,0.0,0.145,0.855,0.296
appdynamics,Then mocking them up is trivial.,0.487,0.0,0.513,-0.4215
appdynamics,You would then use the interface and let DI take care of the rest!,0.0,0.212,0.788,0.5411
appdynamics,The best way to logging the requests to an external logging provider.,0.0,0.276,0.724,0.6369
appdynamics,Check out  http://docs.cloudfoundry.org/devguide/services/log-management-thirdparty-svc.html .,0.0,0.0,1.0,0.0
appdynamics,You can actually log to any http endpoint that supports POST.,0.0,0.2,0.8,0.3612
appdynamics,You can use Splunk to calculate your response time and requests per second.,0.0,0.0,1.0,0.0
appdynamics,The logs that come out of that are in real time and are streamed to your logging endpoint.,0.0,0.0,1.0,0.0
appdynamics,It contains information about the requests as well as log messages from your app.,0.0,0.139,0.861,0.2732
appdynamics,ex.,0.0,0.0,1.0,0.0
appdynamics,There are basic stats available when you run  cf app &lt;app-name&gt; .,0.0,0.0,1.0,0.0
appdynamics,"These include the memory, cpu and disk utilization of your app.",0.0,0.0,1.0,0.0
appdynamics,"You can also access these via the REST api, documented here.",0.0,0.0,1.0,0.0
appdynamics,https://s3.amazonaws.com/cc-api-docs/41397913/apps/get_detailed_stats_for_a_started_app.html,0.0,0.0,1.0,0.0
appdynamics,That's not going to help with requests per second or response time though.,0.158,0.0,0.842,-0.3089
appdynamics,"@jsloyer's solution would work for that, or you could use an APM like NewRelic, which will give you a wealth of data for virtually nothing.",0.0,0.276,0.724,0.7906
appdynamics,Yes this is pretty easy to use.,0.0,0.688,0.312,0.8316
appdynamics,"You can use Logstash as the ingesting engine, you just need the correct parser.",0.0,0.0,1.0,0.0
appdynamics,Check out  http://scottfrederick.cfapps.io/blog/2014/02/20/cloud-foundry-and-logstash  for the parser and config for ingesting cloud foundry logs.,0.0,0.0,1.0,0.0
appdynamics,I was playing around with this before and it worked quite well.,0.0,0.318,0.682,0.4927
appdynamics,Let me know if you have any issues.,0.0,0.0,1.0,0.0
appdynamics,From checking out the code for Istio -  https://github.com/istio  - this appears to be an application with components written in Go and C++ which are deployed to containers using Kubernetes.,0.0,0.0,1.0,0.0
appdynamics,This would mean that for monitoring you should be looking at:,0.0,0.0,1.0,0.0
appdynamics,You have you work cut out for you here - both SDK's would require changes to the code being ran to get visibility.,0.091,0.0,0.909,-0.2732
appdynamics,It's hard to say what's wrong without seeing your Jenkins job and JMeter  Thread Group  configuration.,0.243,0.0,0.757,-0.5423
appdynamics,"In order to apply external settings in JMeter you need to define threads, ramp-up and the number of iterations using JMeter Properties via  __P() function  like:",0.0,0.137,0.863,0.4215
appdynamics,"Once done you will be able to  override the values of these properties using  -J  command line argument , for example in Jenkins:",0.099,0.107,0.794,0.0516
appdynamics,This way you will be able to pass whatever number of virtual users/iterations without having to change your script.,0.0,0.067,0.933,0.0772
appdynamics,More information:  Apache JMeter Properties Customization Guide,0.0,0.0,1.0,0.0
appdynamics,"just to make it clear, a  live connection  is (in the world of Power BI 😉) a connection to either a Power BI dataset or a SSAS tabular model.",0.0,0.098,0.902,0.3818
appdynamics,"I think what you are looking for is  DirectQuery , but it is currently not supported for URL GET commands.",0.126,0.0,0.874,-0.3491
appdynamics,MS docs - Power BI data sources,0.0,0.0,1.0,0.0
appdynamics,To get realtime data you need to use one of the supported sources.,0.0,0.161,0.839,0.3182
appdynamics,maybe AppDynamics supports direct DB access.,0.0,0.333,0.667,0.3612
appdynamics,DirectQuery is supported by SQL databases.,0.0,0.315,0.685,0.3182
appdynamics,Another way is to offload the data to a supported source eg.,0.0,0.187,0.813,0.3182
appdynamics,SQL-db or CDS-service and then connect your pbi to that source.,0.0,0.0,1.0,0.0
appdynamics,The correct and hard way is to amend the SELinux boolean on Tomcat directories to allow Tomcat amend files created by other users.,0.055,0.154,0.791,0.3612
appdynamics,Read  here .,0.0,0.0,1.0,0.0
appdynamics,The easy and dirty solution is to  start-up process includes a step for renaming yesterdays AppDynamics logs  as user  sysXYZ  .,0.12,0.216,0.664,0.3182
appdynamics,And that way you avoid the problem.,0.495,0.0,0.505,-0.5994
appdynamics,Use  su - sysXYZ &lt;script&gt;  command or  sudo -iu sysXYZ &lt;script&gt;  command or  sudo -t &lt;Tomcat role&gt; &lt;script&gt;,0.0,0.0,1.0,0.0
appdynamics,Good luck.,0.0,1.0,0.0,0.7096
appdynamics,Add  CheckedParameter=false  in the connection string to fix the issue.,0.0,0.0,1.0,0.0
appdynamics,I found my answer on the Flow Force online help ( https://manual.altova.com/flowforceserver/flowforceserver/ ),0.0,0.231,0.769,0.4019
appdynamics,"The Flow Force is deployed as two servers, which in a window env, can be started and stopped as windows services (can be found via ""Control Panel"" ""Administrative Tools"" Services).",0.064,0.0,0.936,-0.2263
appdynamics,"With this information, I can monitor them via NAGIOS.",0.0,0.0,1.0,0.0
appdynamics,"One possibility is that you've got a cached execution plan which works fine for most parameter values, or combination of parameter values, but which fails badly for certain values/combinations.",0.207,0.212,0.581,-0.4256
appdynamics,You can try adding a non-filtering predicate such as  1 = 1  to your WHERE clause.,0.0,0.0,1.0,0.0
appdynamics,"I've read  but haven't tested that this can be used to force a hard parse, but it may be that you need to change the value (e.g.",0.056,0.108,0.836,0.3612
appdynamics,"1 = 1 ,  2 = 2 ,  3 = 3 , etc) for each execution of your query.",0.0,0.0,1.0,0.0
appdynamics,"Here's a recursive solution that yields key ""paths"".",0.0,0.277,0.723,0.3182
appdynamics,"The  (*keys, k)  syntax is available in Python versions  = 3.5, you can also use  keys + (k,)",0.0,0.0,1.0,0.0
appdynamics,"As a little modifcation to @Jon Clements code, this is what gives me what I need.",0.0,0.0,1.0,0.0
appdynamics,Try this:-,0.0,0.0,1.0,0.0
appdynamics,Application Performance Management(APM),0.0,0.0,1.0,0.0
appdynamics,In simpler terms:,0.0,0.0,1.0,0.0
appdynamics,"APM monitors the speed at which transactions are performed both by
  end-users and by the systems and network infrastructure that support a
  software application, providing an end-to-end overview of potential
  bottlenecks and service interruptions.",0.074,0.074,0.852,0.0
appdynamics,"In pragmatic terms, this typically involves the use of a suite of software tools—or a single integrated SaaS or on-premises tool—to view and diagnose an application’s speed, reliability, and other performance metrics in order to maintain an optimal level of service.",0.0,0.062,0.938,0.3612
appdynamics,"Here is
 Wiki description.",0.0,0.0,1.0,0.0
appdynamics,You don't need a machine agent extension (and additional custom metrics) to capture if a JVM goes down.,0.0,0.0,1.0,0.0
appdynamics,The machine agent delivers these kind of information out of the box.,0.0,0.0,1.0,0.0
appdynamics,"You have to start the java process with the app server agent and associate the application, that is instrumented with the machine agent.",0.0,0.0,1.0,0.0
appdynamics,Now you can go to the  application dashboard -&gt; Events tab  and check for the  JVM Crash Event .,0.137,0.0,0.863,-0.4019
appdynamics,See also  here  and  here .,0.0,0.0,1.0,0.0
appdynamics,I'm not familiar with the AppDynamics output.,0.0,0.0,1.0,0.0
appdynamics,I assume that's a cumulative view of Threads and their sleep times.,0.0,0.0,1.0,0.0
appdynamics,So Threads get reused and so the sleep times add up.,0.0,0.0,1.0,0.0
appdynamics,"In some cases, a Thread gets a connection directly, without any waiting and in another call the Thread has to wait until the connection can be provided.",0.0,0.0,1.0,0.0
appdynamics,"The wait duration depends on when a connection becomes available, or the wait limit is hit.",0.0,0.0,1.0,0.0
appdynamics,"Your screenshot shows a Thread, which waited  172ms .",0.0,0.0,1.0,0.0
appdynamics,"Assuming the  sleep  is only called within the Jedis/Redis invocation path, the Thread waited  172ms  in total to get a connection.",0.0,0.0,1.0,0.0
appdynamics,"Another Thread, which waited  530ms  looks to me as if the first attempt to get a connection wasn't successful (which explains the first  500ms ) and on a second attempt, it had to wait for  30ms .",0.088,0.0,0.912,-0.4717
appdynamics,It could also be that it waited 2x for  265ms .,0.0,0.0,1.0,0.0
appdynamics,Sidenote:,0.0,0.0,1.0,0.0
appdynamics,1000+ connections could severely limit scalability.,0.375,0.0,0.625,-0.4588
appdynamics,Spring Data Redis also supports other drivers which don't require pooling but work with fewer connections (see  Spring Data Redis Connectors  and  here ).,0.0,0.071,0.929,0.1901
appdynamics,You are getting this error as you have not provided the  values required by app to work.,0.264,0.0,0.736,-0.6579
appdynamics,You need to add the values in  PreferenceConstants  .,0.0,0.278,0.722,0.4019
appdynamics,Did you get it working?,0.0,0.0,1.0,0.0
appdynamics,"Actually, you know.",0.0,0.0,1.0,0.0
appdynamics,You are trying to connect to server and app key is the one generated at server and has to be implemented at end application.,0.0,0.0,1.0,0.0
appdynamics,"Since this is a template, there is no server allocated by the code writer to test and so you don't have app key.",0.095,0.0,0.905,-0.296
appdynamics,"Try exploring IoT frameworks by IBM any other or even you can try open source, Kaa.",0.0,0.0,1.0,0.0
appdynamics,It will be worth if want explore this kind of cloud dependent apps.,0.0,0.225,0.775,0.296
appdynamics,This sounds like you open a transaction and get a database connection always and only after check cache content.,0.0,0.135,0.865,0.3612
appdynamics,"But since you have opened the transactional context, it will be closed, issuing the commit, as no exceptions happened.",0.124,0.124,0.752,0.0
appdynamics,You most likely need to move the checking of the cache outside of the transactional context.,0.0,0.0,1.0,0.0
appdynamics,"Of course, confirming this depends on your application code not included in the question.",0.0,0.0,1.0,0.0
appdynamics,The transaction demarcation related invokes on the  Connection  will happen nevertheless.,0.0,0.0,1.0,0.0
appdynamics,You could use something like Spring's  LazyConnectionDataSourceProxy  (doc'ed  here ) to avoid having these sent when they are not required.,0.101,0.115,0.783,0.0772
appdynamics,The  asadmin  manual page says the following:,0.0,0.0,1.0,0.0
appdynamics,"For the Windows operating system in single mode, a backslash is
  required to escape the colon and the backslash characters.",0.0,0.086,0.914,0.1779
appdynamics,So try the following:,0.0,0.0,1.0,0.0
appdynamics,See also:,0.0,0.0,1.0,0.0
appdynamics,"In theory, yes: if Resource Manager has been enabled it could be the case that different Resource Manager plans have such an impact but experience shows that this feature is seldom used.",0.0,0.056,0.944,0.2144
appdynamics,In practive this kind of difference can have many cause:-,0.0,0.0,1.0,0.0
appdynamics,The first thing to look at database level is something similar to Statspack report (or AWR if licensing allows) to compare database configuration and activity.,0.0,0.0,1.0,0.0
appdynamics,"And don't forget that application performance is not only database performance it depends also on application server, network and front-end.",0.0,0.081,0.919,0.1695
appdynamics,I think this should do what you want.,0.0,0.178,0.822,0.0772
appdynamics,"I'm afraid it's not tested, but the principle is this:",0.0,0.0,1.0,0.0
appdynamics,"1) if the Set-Cookie header starts with ADRUM, then set the env variable ADRUM_cookie.",0.0,0.0,1.0,0.0
appdynamics,"2) if the ADRUM_cookie env variable is /not/ set, edit the Set-Cookie header.",0.0,0.0,1.0,0.0
appdynamics,"If we are talking about ""run tool-get result"" the best option -  Java Mission Control .",0.0,0.244,0.756,0.6369
appdynamics,It's free in test environment.,0.0,0.452,0.548,0.5106
appdynamics,You need to pay only for some features in production.,0.135,0.0,0.865,-0.1027
appdynamics,It's much better than old VisualVM.,0.0,0.367,0.633,0.4404
appdynamics,You can write a data to file using  Flight Recorder .,0.0,0.0,1.0,0.0
appdynamics,You can setup start point and duration.,0.0,0.0,1.0,0.0
appdynamics,You just need to start your application like this:,0.0,0.238,0.762,0.3612
appdynamics,"-XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:StartFlightRecording=duration=60s,filename=myrecording.jfr",0.0,0.0,1.0,0.0
appdynamics,I think what you are wanting is info around instrumentation of the front end of an Angular SPA.,0.0,0.0,1.0,0.0
appdynamics,Please see documentation here:  https://docs.appdynamics.com/display/PRO21/Monitor+Single-Page+Applications  - Angular is mentioned as being available using SPA2.,0.0,0.161,0.839,0.3182
appdynamics,"(Note for the front end we use Javascript BRUM for monitoring, general docs are here:  https://docs.appdynamics.com/display/PRO21/Browser+Monitoring  - this is a seperate part of the product than the APM used to monitor backends)",0.0,0.0,1.0,0.0
appdynamics,"Since some packages are not working with different OS configurations, you need to setup a new build agent.",0.0,0.0,1.0,0.0
appdynamics,Deploy an agent on Windows,0.0,0.0,1.0,0.0
datadog,It's possible to solve this by to changing the query by using zero interpolation.,0.0,0.122,0.878,0.2023
datadog,"You can put "".fill(zero)"" behind your query in the json or choose the option from the UI.",0.0,0.0,1.0,0.0
datadog,EDIT:,0.0,0.0,1.0,0.0
datadog,"You're right, the interpolation is not working when no data is available.",0.0,0.146,0.854,0.2235
datadog,I had the same problem in the end.,0.31,0.0,0.69,-0.4019
datadog,Support of Datadog said it isn't possible to show zero when there is no data for a metric.,0.111,0.136,0.754,0.128
datadog,Now there is a feature request made for it.,0.0,0.0,1.0,0.0
datadog,"It would be nice if more people will request for this feature, so it will be prioritized.",0.0,0.149,0.851,0.4215
datadog,"Nonetheless I have tried to create a workaround by adding a second metric that always has data as a second query and added a formula ((b - b) + a) that negates the second query, but when there is data in the intended query it's show in the graph.",0.0,0.036,0.964,0.1406
datadog,This will result in a zero line when there is no data available.,0.167,0.0,0.833,-0.296
datadog,Scenario without data:,0.0,0.0,1.0,0.0
datadog,"The only problem is that when you have a data in the intended query, it looks ugly and the zero line is gone.",0.231,0.0,0.769,-0.7184
datadog,As you see in the following screenshot.,0.0,0.0,1.0,0.0
datadog,Scenario with data:,0.0,0.0,1.0,0.0
datadog,"Conclusion: 
The workaround is not perfect, but it will work for some situation.",0.143,0.0,0.857,-0.2498
datadog,"For example, filling up query values with zero instead of (no data).",0.0,0.197,0.803,0.4019
datadog,I hope this is a bit better answer to the problem.,0.186,0.4,0.414,0.4767
datadog,"how about  the ""default"" function ?",0.0,0.0,1.0,0.0
datadog,"so  default(sum:foo.bar{hello:world} by {baz}, 0)  or some such?",0.0,0.0,1.0,0.0
datadog,There is now a  default_zero()  function that can be used in Datadog by modifying through JSON directly.,0.0,0.0,1.0,0.0
datadog,The  default_zero()  function does what you're looking for.,0.0,0.0,1.0,0.0
datadog,"You can type it in manually, as  stephenlechner suggests .",0.0,0.0,1.0,0.0
datadog,There's another way I found:,0.0,0.0,1.0,0.0
datadog,"When you save the graph and reopen, you'll see that things have been reshuffled a little, and you'll see a ""default 0"" section tagged onto the end of the metric's definition.",0.0,0.103,0.897,0.4939
datadog,Metrics queries now support wildcards.,0.0,0.403,0.597,0.4019
datadog,"Example 1: Getting all the requests with a status tag starting with  2 :
 http.server.requests.count{status:2*}",0.0,0.0,1.0,0.0
datadog,"Example 1: Getting all the requests with a service tag ending with  mongo :
 http.server.requests.count{service:*mongo}",0.0,0.0,1.0,0.0
datadog,"Example 3 (advanced): Getting all the requests with a service tag starting with  blob  and ending with  postgres :
 http.server.requests.count{service:blob*,service:*postgres} 
 (this will match  service:blob-foo-postgres  and  service:blob_bar_postgres  but not  service:my_name_postgres )",0.0,0.0,1.0,0.0
datadog,I've finally found a dropwizzard module that integrates this library with datadog:  metrics-datadog,0.0,0.0,1.0,0.0
datadog,I've created a Spring configuration class that creates and initializes this Reporter using properties of my YAML.,0.0,0.227,0.773,0.4767
datadog,Just insert this dependency in your pom:,0.0,0.0,1.0,0.0
datadog,Add this configuration to your YAML:,0.0,0.0,1.0,0.0
datadog,and add this configuration class to your project:,0.0,0.0,1.0,0.0
datadog,"If JMX is an option for you, you may use the  JMX dropwizrd reporter  combined with  java datalog integration",0.0,0.0,1.0,0.0
datadog,It seems that Spring Boot 2.x added several monitoring system into its metrics.,0.0,0.0,1.0,0.0
datadog,DataDog is one of them supported by  micrometer.io .,0.0,0.247,0.753,0.3182
datadog,See reference documentation:  https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-newrelic,0.0,0.0,1.0,0.0
datadog,"For Spring Boot 1.x, you can use back-ported package:",0.0,0.0,1.0,0.0
datadog,compile 'io.micrometer:micrometer-spring-legacy:latest.release',0.0,0.0,1.0,0.0
datadog,Confirmed on IRC (#datadog on freenode) that:,0.0,0.0,1.0,0.0
datadog,Datadog doesn't support multiple Y-axis at this time.,0.244,0.0,0.756,-0.3089
datadog,"If the two axis have the same units but different degrees (10 vs 10 million), then using a non-linear scale such as  log  might provide what you need:",0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/203038729-Is-it-possible-to-adjust-the-y-axis-for-my-graphs-,0.0,0.0,1.0,0.0
datadog,They do allow dual y-axis now,0.0,0.275,0.725,0.2263
datadog,https://docs.datadoghq.com/dashboards/widgets/timeseries/#y-axis-controls,0.0,0.0,1.0,0.0
datadog,Introducing dual Y-axis for time series widgets.,0.0,0.0,1.0,0.0
datadog,"The time series widget on dashboards now support dual y-axis, making it easier than ever to compare two sets of data on a single graph.",0.0,0.2,0.8,0.6705
datadog,"By removing the need to create separate graphs, your dashboards can show even more valuable information viewable at a glance.",0.0,0.244,0.756,0.6697
datadog,If  dd-agent  listens on  localhost  it can receive data only from localhost (127.0.0.1).,0.0,0.0,1.0,0.0
datadog,Try to change the  dd-agent  host to  0.0.0.0  instead of  localhost .,0.0,0.0,1.0,0.0
datadog,We are using  docker-dd-agent  and it works OOTB.,0.0,0.0,1.0,0.0
datadog,You will need to set  non_local_traffic: yes  in your  /etc/dd-agent/datadog.conf  file.,0.0,0.213,0.787,0.4019
datadog,Otherwise the agent will reject metrics from containers.,0.278,0.0,0.722,-0.4019
datadog,"After setting, you will need to restart the agent for the change to take effect:  sudo /etc/init.d/datadog-agent restart  or  sudo service datadog-agent restart",0.0,0.0,1.0,0.0
datadog,The  docker-dd-agent  image enables  non_local_traffic: yes  by default.,0.0,0.278,0.722,0.4019
datadog,You don't actually want to use the IP of the host in this case.,0.086,0.0,0.914,-0.0572
datadog,"If you're running the docker dd-agent, there are two environment variables you can tap into:",0.0,0.0,1.0,0.0
datadog,"statsd.connect(DOGSTATSD_PORT_8125_UDP_ADDR, DOGSTATSD_PORT_8125_UDP_PORT)",0.0,0.0,1.0,0.0
datadog,That should do the trick.,0.231,0.0,0.769,-0.0516
datadog,"If not, you should be able to find the relevant info to your problem in  this section of the Datadog docs .",0.119,0.0,0.881,-0.4019
datadog,"Also, I should point out that the only Python library that Datadog shows in their docs is  datadogpy .",0.0,0.0,1.0,0.0
datadog,"I asked DataDog support, and apparently as of January 2020 this is not possible, but is a feature request in their backlog.",0.0,0.089,0.911,0.2144
datadog,"I know this is not a great answer to the question but if I hear that this changes, I will update my answer.",0.107,0.0,0.893,-0.284
datadog,There doesn't appear to be a Crashalytics direct integration yet.,0.0,0.0,1.0,0.0
datadog,Datadog maintains mobile SDKs that can be included in mobile clients to produce metrics and logs to the platform.,0.0,0.0,1.0,0.0
datadog,"For iOS Logs, see here:  https://docs.datadoghq.com/logs/log_collection/ios/",0.0,0.0,1.0,0.0
datadog,For Android Logs:  https://docs.datadoghq.com/logs/log_collection/android/,0.0,0.0,1.0,0.0
datadog,"There's also a public Android SDK for Real User Monitoring, which can be read here:  https://docs.datadoghq.com/real_user_monitoring/android/",0.0,0.0,1.0,0.0
datadog,"And the announcement, with a link for the private beta for iOS signup here:  https://www.datadoghq.com/blog/datadog-mobile-rum/",0.0,0.0,1.0,0.0
datadog,"After speaking to the support team at DataDog, I managed to find out the following information relating to what the no_pod pods were.",0.0,0.114,0.886,0.4019
datadog,"Our Kubernetes check is getting the list of containers from the Kubernetes API, which exposes aggregated data.",0.086,0.0,0.914,-0.128
datadog,"In the metric explorer configuration here, you can see a couple of containers named /docker and / that are getting picked up along with the other containers.",0.0,0.0,1.0,0.0
datadog,Metrics with pod_name:no_pod that come from container_name:/ and container_name:/docker  are just metrics aggregated across multiple containers.,0.0,0.0,1.0,0.0
datadog,(So it makes sense that these are the highest values in your graphs.),0.0,0.184,0.816,0.4019
datadog,"If you don't want your graphs to show these aggregated container metrics though, you can clone the dashboard and then exclude these pods from the query.",0.115,0.0,0.885,-0.2783
datadog,"To do so, on the cloned dashboard, just edit the query in the JSON tab, and in the tag scope, add !pod_name:no_pod.",0.0,0.0,1.0,0.0
datadog,So it appears that these pods are the docker and root level containers running outside of the cluster and will always display unless you want to filter them out specifically which I now do.,0.0,0.039,0.961,0.0772
datadog,Many thanks to the support guys at DataDog for looking into the issue for me and giving me a great explanation as to what the pods were and essentially confirming that I can just safely filter these out and not worry about them.,0.0,0.336,0.664,0.9494
datadog,The free text editor you have in the screenshot is for metric queries.,0.0,0.216,0.784,0.5106
datadog,Events in graphs are added as overlay to show when events happened over time.,0.0,0.0,1.0,0.0
datadog,"There is no widget, as of now, that shows a single value for the number of times an event occurred.",0.1,0.169,0.731,0.128
datadog,"But you can use the event timeline widget, which will show a timeline of events grouped by status and bucketed over a defined period of time.",0.0,0.0,1.0,0.0
datadog,See below:,0.0,0.0,1.0,0.0
datadog,"Well I realized that the  query value  only works with metrics, so to create a counter we can emit metrics with  value: 1  and then count them with the  rollup(sum, 60)  function.",0.0,0.271,0.729,0.8062
datadog,"dog.emit_point('some.event.name', 1)",0.0,0.0,1.0,0.0
datadog,"sum:some.event.name{*}.rollup(sum, 60)",0.0,0.0,1.0,0.0
datadog,The main thing to understand here is that DataDog does not retrieve all the points for a given timeframe.,0.0,0.0,1.0,0.0
datadog,"Actually as  McCloud  says,  for a given time range we do not return more than 350 points , which is very important to have in mind when you create a counter.",0.0,0.139,0.861,0.4927
datadog,"When you query value from a metric in a timeframe, DataDog return a group of points that represents the real stored points, not all the points; the level of how those points are represented (as I understand) is called granurallity, and what you do with this  rollup  function is to define how those points are going to represent the real points, which in this case is going to be using the  sum  function.",0.0,0.034,0.966,0.34
datadog,"I hope this helps somebody, I'm still learning about it.",0.0,0.44,0.56,0.6705
datadog,Regards,0.0,0.0,1.0,0.0
datadog,"In Ruby on the client, I use:",0.0,0.0,1.0,0.0
datadog,"I then have a dashboard with a
 Query Value 
widget, set to &quot;take the [Sum] value from the displayed timeframe&quot;.",0.0,0.242,0.758,0.5859
datadog,I tested this and it seems to give the right numbers.,0.0,0.0,1.0,0.0
datadog,"The
 .as_count() 
seems key.",0.0,0.0,1.0,0.0
datadog,Answering just in case someone will spot this question via Google.,0.0,0.0,1.0,0.0
datadog,You cannot.,0.0,0.0,1.0,0.0
datadog,"StatsD protocol do not define tags or comments at all, so there is no possibility for that.",0.133,0.0,0.867,-0.3535
datadog,You need to use different library like  Statix  for that.,0.0,0.217,0.783,0.3612
datadog,Yes it is possible to emit metrics to DataDog from a AWS Lambda function.,0.0,0.184,0.816,0.4019
datadog,If you were using node.js you could use  https://www.npmjs.com/package/datadog-metrics  to emit metrics to the API.,0.0,0.0,1.0,0.0
datadog,"It supports counters, gauges and histograms.",0.0,0.333,0.667,0.3612
datadog,You just need to pass in your app/api key as environment variables.,0.0,0.0,1.0,0.0
datadog,Matt,0.0,0.0,1.0,0.0
datadog,The easier way is using this library:  https://github.com/marceloboeira/aws-lambda-datadog,0.0,0.286,0.714,0.4215
datadog,"It has runtime no dependencies, doesn't require authentication and reports everything to cloud-watch too.",0.145,0.0,0.855,-0.296
datadog,You can read more about it here:  https://www.datadoghq.com/blog/how-to-monitor-lambda-functions/,0.0,0.0,1.0,0.0
datadog,"In most cases, the datadog agent retrieves metrics from an integration by connecting to a URL endpoint.",0.0,0.0,1.0,0.0
datadog,"This would be the case for services such as nginx, mysql etc.",0.0,0.0,1.0,0.0
datadog,"This means that you can run just one datadog agent on the host, and configure it to listen to URL endpoints of services exposed from each container.",0.048,0.0,0.952,-0.0772
datadog,"For example, assuming a mysql docker container is run with the following command:",0.0,0.0,1.0,0.0
datadog,You can instruct the agent running on the host to connect to the container IP in the  mysql.yaml  agent configuration:,0.0,0.0,1.0,0.0
datadog,Varnish is slightly different as the agent retrieves metrics using the  varnishstat  binary.,0.0,0.0,1.0,0.0
datadog,According to the example template:,0.0,0.0,1.0,0.0
datadog,In order to support monitoring a Varnish instance which is running as a Docker container we need to wrap commands (varnishstat) with scripts which perform a docker exec on the running container.,0.0,0.088,0.912,0.4019
datadog,"To do this, on the host, create a wrapper script for the container:",0.0,0.16,0.84,0.2732
datadog,Then specify the script location in the  varnish.yaml  agent configuration:,0.0,0.0,1.0,0.0
datadog,There are 2 relevant options in  /etc/dd-agent/conf.d/docker_daemon.yaml :,0.0,0.0,1.0,0.0
datadog,"collect_disk_stats 
If you use devicemapper-backed storage (which is default in ECS but not in vanilla Docker or Kubernetes), docker.data.",0.0,0.0,1.0,0.0
datadog,* and docker.metadata.,0.0,0.0,1.0,0.0
datadog,* statistics should do what you are looking for.,0.0,0.0,1.0,0.0
datadog,"collect_container_size 
A generic way, using the docker API but virtually running df in every container.",0.0,0.0,1.0,0.0
datadog,This enables the docker.container.,0.0,0.0,1.0,0.0
datadog,* metrics.,0.0,0.0,1.0,0.0
datadog,"See more here:
 https://help.datadoghq.com/hc/en-us/articles/115001786703-How-to-report-host-disk-metrics-when-dd-agent-runs-in-a-docker-container-",0.0,0.0,1.0,0.0
datadog,"and here:
 https://github.com/DataDog/docker-dd-agent/blob/master/conf.d/docker_daemon.yaml#L46",0.0,0.0,1.0,0.0
datadog,"In your question, I looked for your purpose in terms of using the port 8125 or 8126 ports.",0.0,0.0,1.0,0.0
datadog,"8125 port is used for stasd metrics, and 8126 is used for APM (trace) data.",0.0,0.0,1.0,0.0
datadog,So if you want to use 8125 the important thing to do is having  non_local_traffic : yes .,0.0,0.318,0.682,0.6204
datadog,So there must be another problem which I don't know yet.,0.231,0.0,0.769,-0.4019
datadog,But if your purpose is using APM/trace port: 8126 is only bound to localhost by default.,0.0,0.0,1.0,0.0
datadog,You should make it listen to any network interface by the  bind_host: 0.0.0.0  configuration.,0.0,0.0,1.0,0.0
datadog,"Currently, it will refuse the requests from your containers since they are not coming from localhost.",0.128,0.0,0.872,-0.296
datadog,I had a similar problem and this page helped me:  https://github.com/DataDog/ansible-datadog/issues/149,0.252,0.0,0.748,-0.4019
datadog,UPDATED ANSWER:,0.0,0.0,1.0,0.0
datadog,Still yes.,0.0,0.73,0.27,0.4019
datadog,Docs for new Dashboard endpoint  here .,0.0,0.0,1.0,0.0
datadog,ORIGINAL ANSWER:,0.0,0.697,0.303,0.3182
datadog,Yes.,0.0,1.0,0.0,0.4019
datadog,Docs for screenboards  here .,0.0,0.0,1.0,0.0
datadog,Docs for timeboards  here .,0.0,0.0,1.0,0.0
datadog,"I doubt the Datadog agent will ever work on App Services web app as you do not have access to the running host, it was designed for VMs.",0.088,0.0,0.912,-0.3612
datadog,Have you tried this  https://www.datadoghq.com/blog/azure-monitoring-enhancements/  ?,0.0,0.0,1.0,0.0
datadog,They say they support AppServices,0.0,0.403,0.597,0.4019
datadog,"To respond to your comment on wanting custom metrics, this is still possible without the agent at the same location.",0.0,0.0,1.0,0.0
datadog,After installing the nuget package of datadog called statsdclient you can then configure it to send the custom metrics to an agent located elsewhere.,0.0,0.0,1.0,0.0
datadog,Example below:,0.0,0.0,1.0,0.0
datadog,I have written a app service extension for sending Datadog APM metrics with .NET core and provided instructions for how to set it up here:  https://github.com/payscale/datadog-app-service-extension,0.0,0.0,1.0,0.0
datadog,Let me know if you have any questions or if this doesn't apply to your situation.,0.0,0.0,1.0,0.0
datadog,Logs from App Services can also be sent to Blob storage and forwarded from there via an Azure Function.,0.0,0.0,1.0,0.0
datadog,"Unlike traces and custom metrics from App Services, this does not require a VM running the agent.",0.0,0.0,1.0,0.0
datadog,Docs and code for the Function are available here:,0.0,0.0,1.0,0.0
datadog,https://github.com/DataDog/datadog-serverless-functions/tree/master/azure/blobs_logs_monitoring,0.0,0.0,1.0,0.0
datadog,If you want to use DataDog for logging from Azure Function of App Service you can use Serilog and DataDog Sink to the log files:,0.0,0.051,0.949,0.0772
datadog,Full source code and required NuGet packages  are here:,0.0,0.0,1.0,0.0
datadog,You can deploy the Datadog agent in a container / instance that you manage and the configure it according to  these instructions  to gather metrics from the remote ElasticSearch cluster that is hosted on Elastic Cloud.,0.0,0.0,1.0,0.0
datadog,"You need to create a  conf.yaml  file in the  elastic.d/  directory and provide the required information (Elasticsearch endpoint/URL, username, password, port, etc) for the agent to be able to connect to the cluster.",0.0,0.063,0.937,0.2732
datadog,You may find a sample configuration file  here .,0.0,0.0,1.0,0.0
datadog,"As George Tseres mentioned above, the way I had to get this working was to set up collection on a separate instance (through docker) and then to configure it to read the specific Elastic Cloud instances.",0.0,0.0,1.0,0.0
datadog,"I ended up making this:  https://github.com/crwang/datadog-elasticsearch , building that docker image, and then pushing it up to AWS ECR.",0.0,0.0,1.0,0.0
datadog,"Then, I spun up a Fargate service / task to run the container.",0.0,0.0,1.0,0.0
datadog,I also set it to run locally with  docker-compose  as a test.,0.0,0.0,1.0,0.0
datadog,These system.io metrics are reported from a  system agent check  that uses  iostat  under the hood.,0.0,0.0,1.0,0.0
datadog,According to the  iostat manpage  one of the metrics  %util  (reported as  system.io.util  within Datadog) seems to do the job:,0.0,0.0,1.0,0.0
datadog,%util: Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device).,0.0,0.0,1.0,0.0
datadog,Device saturation occurs when this value is close to 100%.,0.0,0.234,0.766,0.4118
datadog,"You can create a monitor, as a multi alert on host/device, when this metric is over 90 on the last 30 minutes on average, here is a current screenshot of such an example:",0.0,0.133,0.867,0.5106
datadog,Of course one can also monitor other iostat metrics to identify other I/O performance failure modes.,0.18,0.0,0.82,-0.5106
datadog,This can be achieved in two ways.,0.0,0.0,1.0,0.0
datadog,"If you only want this logic to applied on this one graph, you can divide metric either using the UI editor and clicking advanced or using the JSON editor:",0.0,0.109,0.891,0.3182
datadog,UI Editor:  http://cl.ly/1c0K2O3P1E2K,0.0,0.0,1.0,0.0
datadog,Or JSON editor:  https://help.datadoghq.com/hc/en-us/articles/203764925-How-do-I-use-arithmetic-for-my-time-series-data-,0.0,0.0,1.0,0.0
datadog,"Alternatively, you can use the Metric Summary page to edit this metric's metadata and alter this metric's unit throughout the application as seen here:  http://cl.ly/2x0Z290w2I3V",0.0,0.0,1.0,0.0
datadog,https://app.datadoghq.com/metric/summary,0.0,0.0,1.0,0.0
datadog,Hope this helps.,0.0,0.846,0.154,0.6705
datadog,"Also, you can reach out to support@datadoghq.com if you run into any other issues in the future.",0.0,0.064,0.936,0.0258
datadog,"So, while trying to debug this I deleted the deployment + dameonset and service and recreated it.",0.0,0.0,1.0,0.0
datadog,Afterwards it worked....,0.0,0.0,1.0,0.0
datadog,Have you seen the  Discovering Services  docs?,0.0,0.0,1.0,0.0
datadog,I'd recommend using DNS for service discovery rather than environment variables since environment variables require services to come up in a particular order.,0.0,0.106,0.894,0.3612
datadog,There are two error in your code:,0.31,0.0,0.69,-0.4019
datadog,"Once done, your code will run flawlessly.",0.0,0.231,0.769,0.2023
datadog,"So from that log line, it appears as though  this  try  is excepting  in the library's  hostname.py .",0.0,0.0,1.0,0.0
datadog,So either...,0.0,0.0,1.0,0.0
datadog,"(A) The  hostname line  is where it's excepting, and (weirdly) the
library requires that a  hostname  option be set in your
 datadog.conf  file.",0.0,0.0,1.0,0.0
datadog,"Maybe worth setting a hostname there if you
haven't already.",0.0,0.192,0.808,0.2263
datadog,"Or,",0.0,0.0,1.0,0.0
datadog,"(B) The  get_config() line  is where it's excepting, and so the
library isn't able to correctly identify the configuration file
location (or access it, possibly related to permissions).",0.0,0.0,1.0,0.0
datadog,"Based on
the directory structure in your question, I think you're working on
an OSX / mac environment, which means the library will be using the
function  _mac_config_path()  in  config.py  to try to identify the
configuration path, which from  this line in the function  would
make it  seem  as though the library were looking for the
configuration file in  ~/.datadog-agent/agent/datadog.conf  instead
of the appropriate  ~/.datadog-agent/datadog.conf .",0.0,0.0,1.0,0.0
datadog,"Which might be a
legitimate bug...",0.0,0.0,1.0,0.0
datadog,"So if I were you, and if all this seemed right, I'd try adding a  hostname in the  datadog.conf  to see if that helped, and if it didn't, then I'd try making a  ~/.datadog-agent/agent/  directory and copying your  datadog.conf  file there as well, just to see if that got things working.",0.0,0.043,0.957,0.2732
datadog,"This answer assumes you are working in an OSX / mac environment, and will likely not be correct otherwise.",0.0,0.0,1.0,0.0
datadog,"If either (A) or (B) are the case, then that's a problem with the library and should be updated--it would be kind of you to open an issue on  the library itself  to bring this up so the Datadog team that supports that library can be made aware.",0.054,0.05,0.896,-0.0516
datadog,"I suspect not many people end up this library in OSX / mac environments to begin with, so that could explain all this.",0.099,0.0,0.901,-0.296
datadog,"You installed the Datadog agent &amp; trace agent on a Mac, listening on localhost.",0.0,0.0,1.0,0.0
datadog,You installed the flask application and ddtrace library in a docker container on a linux vm sending traffic to localhost.,0.0,0.0,1.0,0.0
datadog,Those two localhosts are describing two different machines.,0.0,0.0,1.0,0.0
datadog,"The easiest option is going to be running both the Agent &amp; flask app on the Mac, or running both in docker.",0.0,0.118,0.882,0.4215
datadog,The latter is most similar to an eventual production deployment.,0.0,0.0,1.0,0.0
datadog,Do that.,0.0,0.0,1.0,0.0
datadog,"According to the documentation, this can be achieved using following properties in telegraf.conf:",0.0,0.0,1.0,0.0
datadog,https://docs.influxdata.com/telegraf/v1.12/administration/configuration/#measurement-filtering,0.0,0.0,1.0,0.0
datadog,where  namepass  defines pattern list of points which will be emitted.,0.0,0.0,1.0,0.0
datadog,Datadog has two agents.,0.0,0.0,1.0,0.0
datadog,And yes for DogStatsD the node agents need to be deployed as Daemonset.,0.0,0.184,0.816,0.4019
datadog,Here is the the deployment manifest for  cluster agent  and  node agent .,0.0,0.0,1.0,0.0
datadog,"Yes, the following should work:",0.0,0.403,0.597,0.4019
datadog,tag_one:(A OR B),0.0,0.0,1.0,0.0
datadog,"Unfortunately the query syntax is slightly different in different contexts, I find, so I don't know if that will solve your particular problem.",0.205,0.072,0.723,-0.5106
datadog,"If you want to remove the warning, you can try adding  none  and  shm  to the  excluded_filesystems  in disk.yaml.",0.116,0.063,0.821,-0.2732
datadog,This file should exist or be created in the Agent's conf.d directory.,0.0,0.154,0.846,0.25
datadog,"Otherwise, you'll find more options  here .",0.0,0.0,1.0,0.0
datadog,If you are looking to exclude the logs from the agent within the platform you can look at excluding the agent ( doc ),0.083,0.0,0.917,-0.2263
datadog,Found the answer here:  https://github.com/DataDog/datadog-agent/issues/3329,0.0,0.0,1.0,0.0
datadog,The field is  mount_point_blacklist,0.0,0.0,1.0,0.0
datadog,"In Datadog Logs, there's a difference between the Tags associated with the execution environment, and Attributes set on Log entry content.",0.0,0.0,1.0,0.0
datadog,From  this section in the docs :,0.0,0.0,1.0,0.0
datadog,Context  refers to the infrastructure and application context in which the log has been generated.,0.0,0.0,1.0,0.0
datadog,"Information is gathered from tags—whether automatically attached (host name, container name, log file name, serverless function name, etc.",0.0,0.0,1.0,0.0
datadog,")—or added through custom tags (team in charge, environment, application version, etc.)",0.0,0.0,1.0,0.0
datadog,on the log by the Datadog Agent or Log Forwarder.,0.0,0.0,1.0,0.0
datadog,"And looking into the  source for the browser SDK , we can see:",0.0,0.0,1.0,0.0
datadog,"This shows us that the  tags  query string parameter being submitted is being calculated based on configuration, and only provides a small amount of user-configurable entries, like  env ,  service  - these were released very recently in version 1.11.5 -  here's the change  introducing them.",0.0,0.059,0.941,0.3612
datadog,"So you may not be able to set  tags  for a specific log entry - rather you can set  attributes  per log entry, like in the example you shared, which is setting  Attributes  for the logger instance as a whole.",0.0,0.123,0.877,0.5994
datadog,Attributes are part of the log  Content  - which will be viewable in the body of the log entry.,0.0,0.0,1.0,0.0
datadog,"Yes, this is confusing since the function used is named  addContext / setContext  - and these don't set the same thing as the documentation's ""Context"" - rather they modify the Attributes that are associated with the log entry.",0.051,0.072,0.878,0.2023
datadog,"In that event, you may want to have either custom logger instances that provide specific attributes for that logger, or add context inline to the log entry, like this:",0.0,0.123,0.877,0.4215
datadog,Here's the docs  on this approach which show what other default attributes are being set per log entry.,0.0,0.0,1.0,0.0
datadog,"The Monitor section of Datadog now includes a ""Monitor status"" page for each of the monitor you define (for example the URL monitoring you already have).",0.0,0.0,1.0,0.0
datadog,"On this page, you can see by group/scope the monitor history and it shows you the uptime for that monitor.",0.0,0.0,1.0,0.0
datadog,More to read about this new feature  here,0.0,0.0,1.0,0.0
datadog,"It's not yet available as a ""report"" but that's a good idea!",0.0,0.315,0.685,0.63
datadog,The exception you are seeing is related to the IO system metrics collection and has nothing to do with your custom dogstream parser.,0.0,0.0,1.0,0.0
datadog,If you look at the stack trace it says that it wasn't able to apply the  _parse_linux2  function.,0.0,0.0,1.0,0.0
datadog,To troubleshoot that further you should take a look at the output of,0.0,0.141,0.859,0.2023
datadog,which is the command launched by the agent.,0.0,0.176,0.824,0.128
datadog,Feel free to open a bug on the agent GitHub repository.,0.0,0.268,0.732,0.5106
datadog,References:,0.0,0.0,1.0,0.0
datadog,Just a few items to note to get this working:,0.0,0.0,1.0,0.0
datadog,"
dogstatsd = Statsd.new('MY_API_KEY')",0.0,0.0,1.0,0.0
datadog,"This line of code is trying to use your API key to establish a statsD connection, but this should actually be trying to establish the statsD connection via the statsD port currently configured on your Agent as seen here:",0.0,0.0,1.0,0.0
datadog,"
Create a stats instance.",0.0,0.512,0.488,0.2732
datadog,"statsd = Statsd.new('localhost', 8125)",0.0,0.0,1.0,0.0
datadog,"The easiest way to get your custom metrics into Datadog is to send them to DogStatsD, a metrics aggregation server bundled with the Datadog Agent (in versions 3.0 and above).",0.0,0.091,0.909,0.4215
datadog,"DogStatsD implements the StatsD protocol, along with a few extensions for special Datadog features.",0.0,0.184,0.816,0.4019
datadog,http://docs.datadoghq.com/guides/dogstatsd/,0.0,0.0,1.0,0.0
datadog,"If you would not like to deploy an Agent on the host running the RoR application, you can utilize DogAPI gem:",0.095,0.0,0.905,-0.2755
datadog,https://github.com/DataDog/dogapi-rb,0.0,0.0,1.0,0.0
datadog,Which has additional documentation to get this custom metrics submitted:,0.0,0.0,1.0,0.0
datadog,"If you have additional questions, please reach out to support@datadoghq.com",0.0,0.298,0.702,0.34
datadog,This Datadog blog should guide you on how to build a monitor.,0.0,0.0,1.0,0.0
datadog,https://www.datadoghq.com/blog/monitoring-cassandra-with-datadog/,0.0,0.0,1.0,0.0
datadog,There is indeed a  template Cassandra dashboard  in Datadog (where I work) that should appear as soon as you enable the integration.,0.0,0.0,1.0,0.0
datadog,This dash has a mix of Cassandra-specific metrics (e.g.,0.0,0.0,1.0,0.0
datadog,"cache hit rates), plus metrics from the host (e.g.",0.0,0.0,1.0,0.0
datadog,CPU).,0.0,0.0,1.0,0.0
datadog,"You can select a particular host or subset of hosts to make those host-level metrics more meaningful by  changing the scope of the dashboard , and the graphs will re-render on the fly.",0.0,0.08,0.92,0.3804
datadog,You can also clone and modify the dashboard as you wish by clicking the gear icon in the upper right.,0.0,0.124,0.876,0.4019
datadog,"This dash should provide a good starting point for monitoring Cassandra, but we have an even better template dashboard in the works.",0.0,0.234,0.766,0.7003
datadog,I'll update this answer as soon as it's released.,0.0,0.0,1.0,0.0
datadog,"In the meantime, the blog post shared by John KVS should help you to identify key metrics that you might want to monitor.",0.0,0.242,0.758,0.6597
datadog,Duplicate the definition of  event.sent  in  event.failed .,0.0,0.0,1.0,0.0
datadog,"As soon as you restart the agent, any  sent  event will be seen as  sent   and   failed .",0.171,0.0,0.829,-0.5106
datadog,"After a minute or so you revert the definition of  failed  to the proper definition, but you now have a few (admittedly bogus) failed event in datadog, allowing you to use the metric.",0.185,0.0,0.815,-0.765
datadog,"On your datadog dashboard, edit the relevant metric, and instead of using the graphical interface (tab  edit ) got to the  JSON  tab, where you can manually enter your metric name (probably a slightly updated cut &amp; paste of your  sent  metric), no matter if an event exists or not.",0.08,0.022,0.898,-0.4445
datadog,"Datadog monitors evaluate every minute, I think.",0.0,0.0,1.0,0.0
datadog,"So in your  sum(last_30m){X}  example, every minute, the monitor would sum the values of  {X}  over the last 30 minutes, and if that value was above whatever threshold you set, then it would trigger an alert.",0.0,0.181,0.819,0.743
datadog,"Same thing for  sum(last_1h){X} , but every minute it would evaluate the sum over the last 1 hour.",0.0,0.0,1.0,0.0
datadog,"The first step will be to make sure you have the  datadog agent running , and that the APM component of it is running and ready to receive trace data from your applications ( this option in your datadog.conf , which must be set to ""true"").",0.0,0.105,0.895,0.5859
datadog,"Second, you'll want to install the appropriate library(ies) for the languages your applications are written in.",0.0,0.08,0.92,0.0772
datadog,You can find them all listed in your datadog account on this page:  https://app.datadoghq.com/apm/docs,0.0,0.0,1.0,0.0
datadog,"Third, once the trace libraries are installed, you'll want to add trace integrations for the tools you're interested in collecting APM data on, and the instructions for those will be found in each library's docs.",0.0,0.108,0.892,0.4588
datadog,"(E.g,  Python ,  Ruby , and  Go )",0.0,0.0,1.0,0.0
datadog,"The integraitons will be a fairly quick way to get pretty granular spans on where your applications have higher latency, errors, etc.",0.098,0.13,0.772,0.2023
datadog,"If from there you'd like to go even further, each library's docs also have instructions on how you can write your own custom tracing functions to expose more info on your custom applications--that's a little more work, but is fairly straight-forward.",0.032,0.043,0.926,0.1154
datadog,You'll probably want to add those bit-by-bit as you go.,0.0,0.126,0.874,0.0772
datadog,"Then you'd be all set, I think.",0.0,0.0,1.0,0.0
datadog,"You'll be tracing services, resources to get the latency, request-count, and error-count of your application requests, and you can drill down to the flame-graphs to further understand what requests spend the most time where in your applications.",0.0,0.0,1.0,0.0
datadog,"Looking back now, seems like they made some recent changes to the setup process that makes it even easier to get the web framework and database integrations added if you're using Python.",0.0,0.15,0.85,0.6486
datadog,They've even got a command line tool in their  get-started section now .,0.0,0.0,1.0,0.0
datadog,Hope this helps!,0.0,0.853,0.147,0.6996
datadog,And reach out to their support team (support@datadoghq.com) if you run into issues along the way--they're always happy to lend a hand.,0.0,0.386,0.614,0.8658
datadog,For now I see only two possibilities:,0.0,0.0,1.0,0.0
datadog,"Use GCP custom metrics
 https://cloud.google.com/monitoring/custom-metrics/creating-metrics 
and datadog integration with GCP
 https://www.datadoghq.com/product/integrations/#cat-google-cloud",0.0,0.0,1.0,0.0
datadog,"Use datadog statsd client, java one -
 https://github.com/DataDog/java-dogstatsd-client  so you can deploy
datadog agent on GCP and connect through it.",0.0,0.0,1.0,0.0
datadog,Sample with kubernetes.,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/tracing/setup/kubernetes/#deploy-agent-daemonset,0.0,0.0,1.0,0.0
datadog,datadog deployment.yaml for kubernetes,0.0,0.0,1.0,0.0
datadog,"Currently I'm investigating this so I'm not sure how to do this, yet.",0.156,0.0,0.844,-0.3017
datadog,It has support to CURL means you can make REST API calls.,0.0,0.197,0.803,0.4019
datadog,Try using some Http libraries like  HttpURLConnection  in java to make those POST requests.,0.0,0.161,0.839,0.3612
datadog,I don't think we need a java based SDK for that as you can frame your own SDK on top of REST api's.,0.0,0.083,0.917,0.2023
datadog,There are two ways to access  dd-trace agent  on host from a container:,0.0,0.0,1.0,0.0
datadog,1.,0.0,0.0,1.0,0.0
datadog,"Only on  &lt;HOST_IP&gt;:8126 , if docker container is started in a bridge network:",0.0,0.0,1.0,0.0
datadog,dd-trace agent  should be bound to  &lt;HOST_IP&gt;  or  0.0.0.0  (which includes  &lt;HOST_IP&gt; ).,0.0,0.0,1.0,0.0
datadog,2.,0.0,0.0,1.0,0.0
datadog,"On  &lt;HOST_IP&gt;:8126  (if  dd-trace agent  is bound to  &lt;HOST_IP&gt;  or  0.0.0.0 ) and  localhost:8126 , if docker container is started in the host network:",0.0,0.0,1.0,0.0
datadog,"As you already try to reach  dd-trace agent  on  localhost:8126 , so the second way is the best solution.",0.0,0.336,0.664,0.765
datadog,First step is to install the DataDog agent on the server in which you are running your application:,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/agent/,0.0,0.0,1.0,0.0
datadog,You then need to enable the  DogStatsD  service in the DataDog agent:,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/developers/dogstatsd/,0.0,0.0,1.0,0.0
datadog,"After that, you can send metrics to the  statsd  agent using any Go library that connects to  statsd .",0.0,0.0,1.0,0.0
datadog,For example:,0.0,0.0,1.0,0.0
datadog,https://github.com/DataDog/datadog-go,0.0,0.0,1.0,0.0
datadog,https://github.com/go-kit/kit/tree/master/metrics/statsd,0.0,0.0,1.0,0.0
datadog,Here's an example program sending some counts using the first library:,0.0,0.0,1.0,0.0
datadog,Here is a convenience wrapper for DD.,0.0,0.0,1.0,0.0
datadog,"It uses ENV vars to configure it, but it's a nice utility to package out common DD calls once you have the agent running in the background.",0.0,0.129,0.871,0.5719
datadog,I figured out how to do this using the datadog api  https://docs.datadoghq.com/api/?lang=python#post-timeseries-points .,0.0,0.0,1.0,0.0
datadog,"The following python script takes in the jtl file (jmeter results) and posts the transaction name, response time, and status (pass/fail) to datadog.",0.0,0.0,1.0,0.0
datadog,You can actually just use:,0.0,0.0,1.0,0.0
datadog,Datadog's monitor templating feature will fill in the blank and forward to the relevant channel as long as you whitelisted it in the integrations tile for Slack.,0.0,0.0,1.0,0.0
datadog,"Starts to get messy, but you could nest two ""does not"" conditional variables, like this:",0.254,0.0,0.746,-0.5291
datadog,AFAIK it is not possible at the Moment to use micrometer to send events to datadog.,0.0,0.0,1.0,0.0
datadog,"Micrometer states that  ""Micrometer is not a distributed tracing system or an event logger. """,0.0,0.0,1.0,0.0
datadog,"on its section ""1.",0.0,0.0,1.0,0.0
datadog,"Purpose"" on its  concepts page .",0.0,0.0,1.0,0.0
datadog,This doc will show you a comprehensive list  of all integrations that involve log collection.,0.0,0.133,0.867,0.25
datadog,"Some of these include other common log shippers, which can also be used to forward logs to Datadog.",0.0,0.0,1.0,0.0
datadog,Among these you'd find...,0.0,0.0,1.0,0.0
datadog,"That said, you  can still just use the Datadog agent to collect logs only  (they want you to collect everything with their agent, that's why they warn you against collecting just their logs).",0.042,0.039,0.92,-0.0258
datadog,"If you want to collect logs from docker containers, the Datadog agent is an easy way to do that, and it has the benefit of adding lots of relevant docker-metadata as tags to your logs.",0.0,0.184,0.816,0.7351
datadog,( Docker log collection instructions here .),0.0,0.0,1.0,0.0
datadog,"If you don't want to do that, I'd look at Fluentd first on the list above -- it has a good reputation for containerized log collection, promotes JSON log formatting (for easier processing), and scales reasonably well.",0.029,0.24,0.731,0.8393
datadog,You need to tell Datadog to pull custom metric.,0.0,0.0,1.0,0.0
datadog,Go to AWS integrations configuration page (Integrations side menu -  Integrations -  Amazon Web Services).,0.0,0.124,0.876,0.1779
datadog,"You will see a list of services to integrate with, custom metrics is the last option on list.",0.0,0.0,1.0,0.0
datadog,Make sure it's ticked.,0.394,0.324,0.282,-0.128
datadog,Takes a while for Datadog to actually start pulling the metric.,0.0,0.0,1.0,0.0
datadog,Do you have the ability to add some parameters in the logs sent.,0.0,0.161,0.839,0.3182
datadog,From  the documentation  you should be able to inject the trace id into your logs in a way that Datadog will interpret them.,0.0,0.0,1.0,0.0
datadog,You can also look at a parser to extract the trace id and span id from the raw log.,0.0,0.0,1.0,0.0
datadog,This documentation  should help you out on that.,0.0,0.278,0.722,0.4019
datadog,"From the documentation, if you don't have JSON logs, you need to include  dd.trace_id  and  dd.span_id  in your formatter:",0.0,0.0,1.0,0.0
datadog,"If your logs are raw formatted, update your formatter to include
   dd.trace_id  and  dd.span_id  in your logger configuration:",0.0,0.0,1.0,0.0
datadog,"So if you add  %X{dd.trace_id:-0} %X{ dd.span_id:-0} , it should work.",0.0,0.0,1.0,0.0
datadog,"A better way would be to use a sidecar container with a logging agent, it won't increase the load on the API server.",0.086,0.127,0.787,0.2354
datadog,Reference:  https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent,0.0,0.0,1.0,0.0
datadog,Datadog agent looks like doesn't support /suggest running as a sidecar ( https://github.com/DataDog/datadog-agent/issues/2203#issuecomment-416180642 ),0.164,0.182,0.654,0.0624
datadog,I suggest looking at using other logging agent and pointing the backend to datadog.,0.0,0.0,1.0,0.0
datadog,Some options are:,0.0,0.0,1.0,0.0
datadog,Datadog supports them,0.0,0.556,0.444,0.3612
datadog,Is that sample text formatted properly?,0.0,0.0,1.0,0.0
datadog,The final entity object is missing a  ]  from the end.,0.216,0.0,0.784,-0.296
datadog,"entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }",0.0,0.0,1.0,0.0
datadog,should be,0.0,0.0,1.0,0.0
datadog,"entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }]",0.0,0.0,1.0,0.0
datadog,I'm going to continue these instructions assuming that was a typo and the entity field actually ends with  ] .,0.0,0.0,1.0,0.0
datadog,"If it doesn't, I think you need to fix the underlying log to be formatted properly and close out the bracket.",0.0,0.0,1.0,0.0
datadog,"Instead of just skipping the entire log and only parsing out that json bit, I decided to parse the entire thing and show what would look good as a final result.",0.0,0.094,0.906,0.4404
datadog,So the first thing we need to do is pull out that set of key/value pairs after the request object:,0.0,0.0,1.0,0.0
datadog,"Example Input:  thread-191555 app.main - [cid: 2cacd6f9-546d-41ew-a7ce-d5d41b39eb8f, uid: e6ffc3b0-2f39-44f7-85b6-1abf5f9ad970] Request: protocol=[HTTP/1.0] method=[POST] path=[/metrics] headers=[Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache] entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }]",0.0,0.086,0.914,0.6249
datadog,"Grok parser rule:  app_log thread-%{integer:thread} %{notSpace:file} - \[%{data::keyvalue("": "")}\] Request: %{data:request:keyvalue(""="","""",""[]"")}",0.0,0.0,1.0,0.0
datadog,Result:,0.0,0.0,1.0,0.0
datadog,"Notice how we use the keyvalue parser with a quoting string of  [] , that allows us to easily pull out everything from the request object.",0.0,0.094,0.906,0.34
datadog,Now the goal is to pull out the details from that entity field inside of the request object.,0.0,0.0,1.0,0.0
datadog,With Grok parsers you can specify a specific attribute to parse further.,0.0,0.0,1.0,0.0
datadog,"So in that same pipeline we'll add another grok parser processor, right after our first",0.0,0.0,1.0,0.0
datadog,"And then configure the advanced options section to run on  request.entity , since that is what we called the attribute",0.0,0.1,0.9,0.25
datadog,"Example Input:  HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }",0.0,0.0,1.0,0.0
datadog,Grok Parser Rule:  entity_rule %{notSpace:request.entity.class} %{notSpace:request.entity.media_type} %{data:request.entity.json:json},0.0,0.0,1.0,0.0
datadog,Result:,0.0,0.0,1.0,0.0
datadog,Now when we look at the final parsed log it has everything we need broken out:,0.171,0.0,0.829,-0.4767
datadog,"Also just because it was really simple, I threw in a third grok processor for the headers chunk as well (the advanced settings are set to parse from  request.headers ):",0.088,0.129,0.784,0.0772
datadog,"Example Input:  Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache",0.0,0.121,0.879,0.6249
datadog,"Grok Parser Rule:  headers_rule %{data:request.headers:keyvalue("": "", ""/)(; :"")}",0.0,0.0,1.0,0.0
datadog,Result:,0.0,0.0,1.0,0.0
datadog,The only tricky bit here is that I had to define a characterWhiteList of  /)(; : .,0.118,0.0,0.882,-0.1531
datadog,Mostly to handle all those special characters are in the  User-Agent  field.,0.0,0.197,0.803,0.4019
datadog,References :,0.0,0.0,1.0,0.0
datadog,Just the documentation and some guess &amp; checking in my personal Datadog account.,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/logs/processing/parsing/?tab=matcher#key-value-or-logfmt,0.0,0.0,1.0,0.0
datadog,"When installing Datadog in your K8s Cluster, you install a  Node Logging Agent  as a Daemonset with various volume mounts on the hosting nodes.",0.0,0.0,1.0,0.0
datadog,"Among other things, this gives Datadog access to the Pod logs at /var/log/pods and the container logs at /var/lib/docker/containers.",0.0,0.0,1.0,0.0
datadog,Kubernetes and the underlying Docker engine will only include output from stdout and stderror in those two locations (see  here  for more information).,0.0,0.0,1.0,0.0
datadog,"Everything that is written by containers to log files residing inside the containers, will be invisible to K8s, unless more configuration is applied to extract that data, e.g.",0.0,0.0,1.0,0.0
datadog,by applying the  side care container pattern .,0.0,0.348,0.652,0.4939
datadog,"So, to get things working in your setup,  configure logback to log to stdout rather than /var/app/logs/myapp.log",0.0,0.0,1.0,0.0
datadog,"Also, if you don't use APM there is no need to instrument your code with the datadog.jar and do all that tracing setup (setting up ports etc).",0.078,0.0,0.922,-0.296
datadog,You need to tell Datadog that you're interested in that content by creating a facet from the field.,0.0,0.246,0.754,0.5994
datadog,"Click a log message, mouse over the attribute name, click the gear on the left, then Create facet for @...",0.0,0.104,0.896,0.2732
datadog,"For logs indexed after you create the facet, you can search with  @fieldName:text* , where  fieldName  is the name of your field.",0.0,0.095,0.905,0.2732
datadog,You'll need to re-hydrate (reprocess) earlier logs to make them searchable.,0.0,0.0,1.0,0.0
datadog,You won't need to create a facet if you use fields from the  standard attributes list .,0.115,0.0,0.885,-0.2057
datadog,The error message itself is not a good fit to be defined as a facet.,0.419,0.0,0.581,-0.7364
datadog,If you are using JSON and want the main message (say from a  msg  json field) to be searchable in the Datadog  content  field.,0.0,0.056,0.944,0.0772
datadog,"Instead of making
facet for  msg , you can define a &quot;Message Remapper&quot; in the log configuration to map it to the  Content .",0.0,0.0,1.0,0.0
datadog,And then you can do wildcard searches.,0.0,0.0,1.0,0.0
datadog,log config screenshot,0.0,0.0,1.0,0.0
datadog,"After reading this documentation,  https://docs.datadoghq.com/tracing/setup_overview/setup/ruby/#resque , did you try making the options a hash with curly braces surrounding?",0.0,0.0,1.0,0.0
datadog,Options is specified as being a hash.,0.0,0.0,1.0,0.0
datadog,"So everything after  c.use :resque,   should be a  hash .",0.0,0.0,1.0,0.0
datadog,You can use the Agent's info command to see if the check is reporting correctly:,0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/203764635,0.0,0.0,1.0,0.0
datadog,If the Agent Status shows the check is reporting correctly but your metrics are still not reporting you can contact support@datadoghq.com and they can troubleshoot this issue further.,0.0,0.075,0.925,0.296
datadog,Hope this helps!,0.0,0.853,0.147,0.6996
datadog,This is not correct graph to detect correct resource limit.,0.0,0.0,1.0,0.0
datadog,"You graph shows CPU usage of your app in the cluster, but resource limit is per pod (container).",0.0,0.0,1.0,0.0
datadog,We (and you as well) don't know from the graph how many containers were up and running.,0.0,0.0,1.0,0.0
datadog,You can determinate right CPU limit from the container CPU usage graph(s).,0.0,0.141,0.859,0.2023
datadog,You will need Datadog-Docker integration:,0.0,0.0,1.0,0.0
datadog,"Please be aware that Kubernetes relies on Heapster to report metrics,
  rather than the cgroup file directly.",0.0,0.126,0.874,0.3182
datadog,"The collection interval for
  Heapster is unknown which can lead to innacurate time-related data,
  such as CPU usage.",0.0,0.0,1.0,0.0
datadog,"If you require more precise metrics, we recommend
  using the Datadog-Docker Integration.",0.0,0.185,0.815,0.3612
datadog,Then it will depends how Datadog measure CPU utilization per container.,0.0,0.0,1.0,0.0
datadog,"If container CPU utilization has max 100%, then 100% CPU container utilization ~ 1000m ~ 1.",0.0,0.0,1.0,0.0
datadog,I recommend you to read how and when cgroup limits CPU -  https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html,0.0,0.2,0.8,0.3612
datadog,You will need a deep knowledge to set proper CPU limits.,0.0,0.0,1.0,0.0
datadog,"If you don't need to prioritize any container, then IMHO the best practice is to set 1 ( resources.requests.cpu ) for all your containers - they will have always equal CPU times.",0.0,0.135,0.865,0.6369
datadog,"I am not familiar with the products and libraries that you are using, but there is an open source library  MgntUtils  that can extract full or filtered stacktrace from exception as a String.",0.0,0.0,1.0,0.0
datadog,Since you mentioned that you can pass the text (i.e.,0.0,0.0,1.0,0.0
datadog,String) this library may help you.,0.0,0.351,0.649,0.4019
datadog,Here are the links to  MgntUtils  library:,0.0,0.0,1.0,0.0
datadog,I hope this helps,0.0,0.846,0.154,0.6705
datadog,"You need to pass  Content-Type  as a header with the request, as shown  in the docs",0.0,0.0,1.0,0.0
datadog,Response:,0.0,0.0,1.0,0.0
datadog,"Your data is also not formatted according to the docs (there should be no  dash  field at the top level, for starters).",0.092,0.075,0.833,-0.1027
datadog,The format you're using to send the data does not comply with the  documentation  and your call fails to complete.,0.128,0.0,0.872,-0.4215
datadog,The call would work if you change your  $data  to:,0.0,0.0,1.0,0.0
datadog,"Agreed that the error returned by the API is not really helpful, filed an issue in Datadog to correct that  behavior and return the proper error code and not a 500 (it's actually a 500 and you can see it by printing  curl_getinfo($ch)  after you executed your curl session).",0.15,0.04,0.811,-0.7049
datadog,Why not use dogstatsd instead of threadstats?,0.0,0.0,1.0,0.0
datadog,"If you're already running the dd-agent on your node in a way that's reachable by your containers, you can use the  datadog.statsd.increment()  method instead to send the metric over statsd to the agent, and from there it would get forwarded to your datadog account.",0.0,0.0,1.0,0.0
datadog,"Dogstatsd has the benefits of being more straight-forward and of being somewhat easier to troublehsoot, at least with debug-level logging.",0.0,0.221,0.779,0.6258
datadog,"Threadstats sometimes has the benefit of not requiring a dd-agent, but it does very little (if any) error logging, so makes it difficult to troubleshoot cases like these.",0.188,0.206,0.607,-0.09
datadog,"If you went the dogstatsd route, you'd use the following code:",0.0,0.0,1.0,0.0
datadog,"And from there you'd find your metric metadata with the ""rate"" type and with an interval of ""10"", and you could use the ""as_count"" function to translate the values to counts.",0.0,0.083,0.917,0.4019
datadog,In the python script I was initializing with an api key:,0.0,0.0,1.0,0.0
datadog,And sending some events,0.0,0.0,1.0,0.0
datadog,"When I changed it to initialize like this, it started working with the dd-agent:",0.0,0.172,0.828,0.3612
datadog,I didn't need the link command (--link dogstatsd:dogstastd).,0.0,0.0,1.0,0.0
datadog,"With that setup it now works in the staging environment, but not in production.",0.0,0.0,1.0,0.0
datadog,:/,1.0,0.0,0.0,-0.34
datadog,Maybe the  Datadog events-post api endpoint ?,0.0,0.0,1.0,0.0
datadog,Or their  metrics-post endpoint ?,0.0,0.0,1.0,0.0
datadog,"Although, it's not altogether clear to me what is meant by ""send this record only to Datadog"".",0.12,0.0,0.88,-0.2924
datadog,"If you use the log setup for Lambda log parsing that is described in  Datadog's AWS Lambda integration tile , that should be another perfectly good way to get the data collected into Datadog, but I think it may only collect those logs as metrics instead of events.",0.0,0.094,0.906,0.5499
datadog,I found the answer thanks to  @tqr_aupa_atleti  and the support team from Datadog.,0.0,0.359,0.641,0.6808
datadog,"On the Datadog dashboard panel, I had to click Metrics -  Summary and look for my metric.",0.0,0.0,1.0,0.0
datadog,I looked at the tags and I could figure out it was a custom metric form my company that uses data from Amplitude.,0.0,0.0,1.0,0.0
datadog,"However if you look at this metric carefully it appears to be
  calculating these percentiles on a short range (not sure what) and for
  each tuple of the tags that exist.",0.0,0.119,0.881,0.4215
datadog,The short range that you have noticed is actually the flush interval which defaults to 10 seconds.,0.0,0.0,1.0,0.0
datadog,"As per  this  article on histogram metric by datadog,",0.0,0.0,1.0,0.0
datadog,"It aggregates the values that are sent during the flush interval
  (usually defaults to 10 seconds).",0.0,0.153,0.847,0.4019
datadog,"So if you send 20 values for a
  metric during the flush interval, it'll give you the aggregation of
  those values for the flush interval",0.0,0.197,0.803,0.6597
datadog,For your query -,0.0,0.0,1.0,0.0
datadog,"Ideally what I'd like to get is a 95th percentile of the ResponseTime
  metric over all the tags (maybe I filter it down by 1 or 2 and have a
  couple of different graphs) but over the last week or so.",0.0,0.097,0.903,0.3919
datadog,"Is there an
  easy way to do this?",0.0,0.293,0.707,0.4404
datadog,"as per my reading of the datadog docs, there isn't a way to get this done at the moment.",0.0,0.0,1.0,0.0
datadog,It might be a good idea to check with datadog support regarding this.,0.0,0.359,0.641,0.6808
datadog,More details  here .,0.0,0.0,1.0,0.0
datadog,"I think what you want is to add the  &quot;exact_match: false&quot;  option, like so:",0.0,0.257,0.743,0.4215
datadog,This should match on any process whose path+name  include  the search string you provide.,0.0,0.0,1.0,0.0
datadog,"Alternatively, if you only want it to match on the name of the process, you'll want to set the search_string to be the exact name of the process that's running (so whatever is given as the name when you run a  ps | grep &quot;ecommerce-order&quot; , which in your case seems to be  ecommerce-order-0.0.1-SNAPSHOT.jar )",0.0,0.05,0.95,0.1531
datadog,I am on the same boat.,0.0,0.0,1.0,0.0
datadog,I found this link:  datadog instrumentation .,0.0,0.0,1.0,0.0
datadog,"So, currently (20.11.2017) there are not agents for C#.",0.0,0.0,1.0,0.0
datadog,"Only Go, python and ruby are available.",0.0,0.0,1.0,0.0
datadog,(Disclosure: I'm a software developer at Datadog.),0.0,0.0,1.0,0.0
datadog,Datadog's open-source .NET Tracer is currently (2019-02-11) in open beta.,0.0,0.0,1.0,0.0
datadog,It supports manual and automatic instrumentation and  OpenTracing .,0.0,0.263,0.737,0.3612
datadog,"For a list of currently supported languages/frameworks, see the  updated documentation .",0.0,0.204,0.796,0.3182
datadog,Happy tracing!,0.0,0.8,0.2,0.6114
datadog,Looks like I found the problem -  https://github.com/DataDog/jenkins-datadog-plugin/issues/101,0.293,0.272,0.435,-0.0516
datadog,"current Datadog version 0.6.1. has a bug , after change the Jenkins main config ( any change , not related to Datadog configuration) it stop works.",0.095,0.0,0.905,-0.296
datadog,I downgrade it to 0.5.7 and it works OK,0.0,0.295,0.705,0.4466
datadog,"Not possible today, but that is in Datadog's plans for development.",0.0,0.0,1.0,0.0
datadog,"What you can do as a workaround, though, is add a link to your logs explorer with the query that triggered the monitor alert, so you can get a quick reference to what were the logs that triggered it.",0.0,0.059,0.941,0.296
datadog,"This link, for example, would quickly scope you to the error logs over the last 15 minutes:  https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc",0.137,0.0,0.863,-0.4019
datadog,"And markdown is supported, so you can keep your monitor messages prettier without long links in the messages.",0.0,0.252,0.748,0.6597
datadog,"Like so:
 [Check the error logs here](https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc)",0.283,0.239,0.478,-0.1189
datadog,It looks to me that these are Datadog specific configuration parameters.,0.0,0.0,1.0,0.0
datadog,"So you first need to install the Datadog app to your Slack workspace, which you find on the Slack App Directory.",0.0,0.0,1.0,0.0
datadog,Here is how the process is described in the official documentation:,0.0,0.0,1.0,0.0
datadog,Installation,0.0,0.0,1.0,0.0
datadog,"The Slack integration is installed via its integration tile in the
  Datadog application.",0.0,0.0,1.0,0.0
datadog,Configuration,0.0,0.0,1.0,0.0
datadog,Source:  official documentation  from Datadog,0.0,0.0,1.0,0.0
datadog,This might be a problem that  other people are running into too .,0.213,0.0,0.787,-0.4019
datadog,"kubelet is no longer listening on the ReadOnlyPort in newer Kubernetes versions, and the port is being deprecated.",0.115,0.0,0.885,-0.296
datadog,Samuel Cormier-Iijima reports that the issue can be solved by adding adding  KUBELET_EXTRA_ARGS=--read-only-port=10255  in  /etc/default/kubelet  on the node host.,0.0,0.104,0.896,0.2732
datadog,It is in fact possible to send an alert if a metric shows the same value for a fix period of time.,0.0,0.204,0.796,0.5574
datadog,You can do this by using the diff() function to your query to produce delta values from consecutive delta points and then apply the abs() function to take absolute values of these deltas.,0.0,0.148,0.852,0.6597
datadog,"To do this we use the Arithmetic functions available, which can be applied using the '+' button to your query in UI.",0.0,0.0,1.0,0.0
datadog,"For alert conditions in the metric monitor itself, configure as follows:
Select threshold alert
Set the “Trigger when the metric is…” dropdown selector to below or equal to
Set the “Alert Threshold” field to 0 (zero)",0.0,0.118,0.882,0.5267
datadog,This configuration will trigger an alert event when no change in value has been registered over the selected timeframe.,0.096,0.202,0.702,0.34
datadog,Here is a link to datadog article:  https://docs.datadoghq.com/monitors/faq/how-can-i-configure-a-metric-monitor-to-alert-on-no-change-in-value/,0.0,0.0,1.0,0.0
datadog,By introducing space in datadog.json.j2 template definition .i.e.,0.0,0.0,1.0,0.0
datadog,and running deployment again I got the working config as below,0.0,0.0,1.0,0.0
datadog,However I am not able to understand the behaviour if anyone could help me understand it,0.0,0.162,0.838,0.4019
datadog,it appears to me like you're missing a couple environment vars in your docker-compose  datadog  service configuration.,0.118,0.134,0.749,0.0772
datadog,And also the volume that adds the registry for tailing the logs from the docker socket.,0.0,0.0,1.0,0.0
datadog,Maybe try something like this if you haven't?,0.0,0.263,0.737,0.3612
datadog,"from there, if you still end up with trouble, you may want to reach out to support@datadoghq.com to ask for help.",0.109,0.206,0.685,0.1027
datadog,They're pretty quick to reply.,0.0,0.444,0.556,0.4939
datadog,"Installed it as ""Run with Admin rights"" and it fixed the issue.",0.0,0.0,1.0,0.0
datadog,"For me, I had to manually give the  ddagentuser  account read access to the file",0.0,0.0,1.0,0.0
datadog,C:\ProgramData\Datadog\auth_token,0.0,0.0,1.0,0.0
datadog,Have you tried specifying dependencies with  go mod  yet?,0.0,0.0,1.0,0.0
datadog,"I faced the same issue and finally can solved by generating  go.mod  file with these command,",0.0,0.13,0.87,0.2732
datadog,Here  is the details explanation.,0.0,0.0,1.0,0.0
datadog,"I think the disconnect here is that the pattern needs to be in the Jboss log manager, and then they're encoded to JSON.",0.0,0.0,1.0,0.0
datadog,Have you tried puttinng  %X{dd.trace_id:-0} %X{dd.span_id:-0}  into your Jboss logging pattern?,0.0,0.0,1.0,0.0
datadog,"If not, I'd also recommend opening a ticket at support@datadoghq.com and we can work this through with you.",0.117,0.0,0.883,-0.2755
datadog,"I’m not sure if it is a recent addition, but the Datadog public API supports configuring Log Archives:  https://docs.datadoghq.com/api/latest/logs-archives/",0.071,0.157,0.772,0.4155
datadog,You can also use tools like Terraform to configure them:  https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/logs_archive  (it uses the Datadog API internally),0.0,0.135,0.865,0.3612
datadog,"If you are trying to connect to an HTTPS URL for Datadog ( https://app.datadoghq.com  in your example), then you will need to set the  https.proxyHost  system property for it to have effect -  http.proxyHost  is for HTTP URLs[1].",0.0,0.0,1.0,0.0
datadog,These are system-wide settings that will be used by the default  HttpSender  ( HttpUrlConnectionSender ) if a  Proxy  is not passed to its constructor.,0.0,0.0,1.0,0.0
datadog,The micrometer doc says,0.0,0.0,1.0,0.0
datadog,But I dont understand what it means?,0.0,0.0,1.0,0.0
datadog,"should I replace this url with my
  proxy url or is there any specific uri pattern with the proxy?",0.0,0.0,1.0,0.0
datadog,"This is referring to a different kind of proxy that you would configure to receive your Datadog traffic on your internal network, and it would forward it to Datadog outside of your network.",0.0,0.0,1.0,0.0
datadog,If you are using an HTTP proxy then you should use the system properties or an  HttpSender  with your HTTP proxy configured (e.g.,0.0,0.0,1.0,0.0
datadog,an  HttpUrlConnectionSender  and passing a  Proxy  to its constructor).,0.0,0.0,1.0,0.0
datadog,You can configure a custom  HttpSender  with a  DatadogMeterRegistry  using its  Builder .,0.0,0.0,1.0,0.0
datadog,"If you expose this as a  Bean  in a  @Configuration  class, Spring Boot will use it in its auto-configuration.",0.091,0.0,0.909,-0.1531
datadog,For example:,0.0,0.0,1.0,0.0
datadog,[1]  https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html,0.0,0.0,1.0,0.0
datadog,That's all handled in the  message  attribute with  conditional logic variables .,0.0,0.0,1.0,0.0
datadog,"If, for example, you define your  message  value to be this...",0.0,0.194,0.806,0.34
datadog,... then ...,0.0,0.0,1.0,0.0
datadog,got this resolved with below,0.0,0.298,0.702,0.1779
datadog,"rule1  %{ipv4:network.client.ip}\s+-\s+%{word:user}\s+\[%{date(&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;):date}\]\s+\&quot;%{word:http.module}\s+\/v1\/resources\/+%{word:onemds.module}\?+%{data:onemds.params:keyvalue(&quot;=&quot;,&quot;/:&quot;,&quot;&quot;,&quot;:&amp;&quot;)}",0.0,0.0,1.0,0.0
datadog,was never expecting the delimiter that can take 2 characters above  :&amp;,0.0,0.0,1.0,0.0
datadog,this helped to remove  rs:  for all except the first one.,0.0,0.0,1.0,0.0
datadog,not an elegant approach but it worked for my use case.,0.151,0.0,0.849,-0.1967
datadog,Found the answer:,0.0,0.0,1.0,0.0
datadog,The metric  kubernetes.kubelet.volume.stats.used_bytes  will allow you to get the disk usage on PersistentVolumes.,0.0,0.137,0.863,0.2263
datadog,This can be achieved using the tag  persistentvolumeclaim  on this metric.,0.0,0.0,1.0,0.0
datadog,You can view this metric and tag combination in your account here:  https://app.datadoghq.com/metric/summary?filter=kubernetes.kubelet.volume.stats.used_bytes&amp;metric=kubernetes.kubelet.volume.stats.used_bytes,0.0,0.0,1.0,0.0
datadog,Documentation on this metric can be found here:  https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubelet,0.0,0.0,1.0,0.0
datadog,"This answer doesn't solve the problem, because postgres is not running in the cluster, it's running in Azure.",0.08,0.114,0.806,0.1695
datadog,"I'll leave it up since it might be interesting, but I posted another answer for the actually environment setup.",0.058,0.098,0.844,0.1901
datadog,"For containerized setups, it's not usually recommended to set up a configmap or try giving the agent a yaml file.",0.08,0.12,0.8,0.2042
datadog,Instead the recommended configuration is to put annotations on the postgres pod:  https://docs.datadoghq.com/integrations/postgres/?tab=containerized#containerized .,0.0,0.13,0.87,0.2023
datadog,"This concept of placing the config on the application pod, not with the datadog agent, is called autodiscovery.",0.0,0.0,1.0,0.0
datadog,This blog post does a good job explaining the benefits of this solution:  https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/#autodiscovery,0.0,0.438,0.562,0.7783
datadog,Here is a picture diagram showing how the agent goes out to the pods on the same node and would pull the configuration from them:,0.0,0.0,1.0,0.0
datadog,"To configure this, you'd take each of the sections of the yaml config, convert them to json, and set them as annotations on the postgres manifest.",0.0,0.0,1.0,0.0
datadog,"An example of how to set up pod annotations is provided for redis, apache, and http here:  https://docs.datadoghq.com/agent/kubernetes/integrations/?tab=kubernetes#examples",0.0,0.0,1.0,0.0
datadog,For your scenario I would do something like:,0.0,0.294,0.706,0.3612
datadog,"notice how the folder name  postgres.d/conf.yaml  maps to the  check_names  annotation, the  init_configs  section maps to  init_configs  annotation, etc.",0.0,0.0,1.0,0.0
datadog,"For the section on custom metrics, since I personally am more familiar with the yaml config, and it's easier to just fill out, I'll usually go to a yaml to json converter, and copy the json from there",0.0,0.074,0.926,0.4215
datadog,A key thing to notice for all those configs is that I never set the hostname.,0.0,0.0,1.0,0.0
datadog,That is automatically discovered by the agent as it scans through containers.,0.0,0.0,1.0,0.0
datadog,"However you may have set  my-postgres-host.com  because this postgres instance is not actually running in your kubernetes cluster, and is instead living on its own, and not in a container.",0.0,0.0,1.0,0.0
datadog,"If that is the case, I would recommend trying to just put the agent on the postgres node directly, all the yaml stuff you've done would work just fine, if that db and the agent are both directly on the vm.",0.0,0.102,0.898,0.5106
datadog,It looks like the question has been updated to say that this postgres db you are trying to monitor is not actually running the the cluster.,0.0,0.091,0.909,0.3612
datadog,"And you are not able to put an agent directly on the postgres server since it's a managed service in Azure, so you don't have access to the underlying host.",0.0,0.0,1.0,0.0
datadog,"In those situations it is common to have a random datadog agent on some other host set up the postgres integration anyway, but instead of having  host: localhost  in the yaml config, put the hostname you would put to access the db externally.",0.0,0.0,1.0,0.0
datadog,In your example it was  host: my-postgres-host.com .,0.0,0.0,1.0,0.0
datadog,This provides all the same benefits of the normal integration (except you won't have cpu/disk/resource metrics available obviously),0.0,0.133,0.867,0.3818
datadog,"This is all fine and makes sense, but what if all of the agents you have installed are the agents in the kubernetes daemonset you created?",0.0,0.14,0.86,0.4404
datadog,You don't have any hosts directly on VMs to run this check.,0.0,0.0,1.0,0.0
datadog,But we definitely don't recommend configuring the daemonset to run this check directly.,0.155,0.206,0.639,0.2228
datadog,"If you did, that would mean you are collecting duplicate metrics from that one postgres db in every single node in your cluster.",0.0,0.0,1.0,0.0
datadog,"Since every agent is a copy, they'd each be running the same check on the same db you define.",0.0,0.0,1.0,0.0
datadog,Luckily I notice that you are running the Datadog Cluster Agent.,0.0,0.268,0.732,0.5106
datadog,"This is a separate Datadog tool that is deployed as a single service once per cluster, instead of a daemonset running once per node.",0.0,0.0,1.0,0.0
datadog,It is possible to have the cluster agent configured to run 'cluster level' checks.,0.0,0.0,1.0,0.0
datadog,"Perfect for things like databases, message queues, or http checks.",0.0,0.437,0.563,0.7351
datadog,The basic idea is that (in addition to it's other jobs) the cluster agent will also schedule checks.,0.0,0.0,1.0,0.0
datadog,"the DCA (datadog cluster agent) will choose one agent from the daemonset to run the check, and if that node agent pod dies, the DCA will find a new one to run the cluster check.",0.0,0.0,1.0,0.0
datadog,Here are the docs on how to set up the DCA to run cluster checks:  https://docs.datadoghq.com/agent/cluster_agent/clusterchecks/#how-it-works,0.0,0.0,1.0,0.0
datadog,"To configure it you would enable some flags, and give the DCA the yaml file you created with a config map, or just mounting the file directly.",0.0,0.074,0.926,0.25
datadog,The DCA will pass along that config to whichever node agent it chooses to run the check.,0.0,0.0,1.0,0.0
datadog,Answering my own question.,0.0,0.0,1.0,0.0
datadog,The DataDog logging page has a Configuration section.,0.0,0.0,1.0,0.0
datadog,On that page the &quot;Pre processing for JSON logs&quot; section allows you to specify alternate property names for a few of the major log message properties.,0.0,0.0,1.0,0.0
datadog,If you add @m to the Message attributes section and @l to the Status attributes section you will correctly ingest JSON messages from the  RenderedCompactJsonFormatter  formatter.,0.0,0.0,1.0,0.0
datadog,If you add RenderedMessage and Level respectively you will correctly ingest  JsonFormatter(renderMessage: true)  formatter.,0.0,0.156,0.844,0.34
datadog,"You can specify multiple attributes in each section, so you can simultaneously support both formats.",0.0,0.162,0.838,0.4019
datadog,"After few days of research and follow up with datadog support team, I am able to get the APM logs on datadog portal.",0.0,0.114,0.886,0.4019
datadog,"Below is my  docker-compose.yml  file configuration,  I believe it helps someone in future",0.0,0.191,0.809,0.3818
datadog,The  Dockerfile  for my python long running application,0.0,0.0,1.0,0.0
datadog,"Please note, on the requirements.txt file I have   ddtrace  package listed",0.0,0.204,0.796,0.3182
datadog,"In the  instructions  it refers to http://ccloudexporter_ccloud_exporter_1:2112/metrics in the open metrics file, but in my setup docker-compose gave the ccloud exporter container the name ccloud_ccloud_exporter_1.",0.0,0.0,1.0,0.0
datadog,To prevent this I added &quot;container_name: ccloud_ccloud_exporter_1&quot; in the docker compose file and used &quot;http://ccloud_ccloud_exporter_1:2112/metrics&quot; as prometheus url in the openmetrics file.,0.0,0.052,0.948,0.0258
datadog,Tracing for requests coming from the browser are handled by RUM and not APM.,0.0,0.0,1.0,0.0
datadog,I can't find the documentation for it but there is a configuration option on the  browser SDK  to allow tracing to specific endpoints.,0.0,0.105,0.895,0.3291
datadog,"The tracing context will automatically be propagated to APM if enabled on the server side, and both the browser and server spans will appear in the same trace.",0.0,0.0,1.0,0.0
datadog,Usually metrics exposed to  /actuator/metrics  are sent to the metrics system like datadog.,0.088,0.169,0.743,0.296
datadog,You can try to check what exactly gets sent to datadog by examining the source code of  DatadogMeterRegistry,0.0,0.0,1.0,0.0
datadog,"Put a breakpoint in the publish method and see what gets sent, or, alternatively set the logger of the class to &quot;trace&quot; so that it will print the information that gets sent to the datadog (line 131 in the linked source code).",0.0,0.0,1.0,0.0
datadog,Another possible direction to check is usage of filters (see  MeterFilter ) that can filter out some metrics.,0.0,0.0,1.0,0.0
datadog,This did the trick : Thanks to @MarkBramnik,0.132,0.319,0.549,0.4019
datadog,Try this,0.0,0.0,1.0,0.0
datadog,You might be able to get this if you...,0.0,0.0,1.0,0.0
datadog,"If you start having trouble as you start setting it up, you may want to reach out to support@datadoghq.com to get their assistance.",0.108,0.096,0.797,-0.3182
datadog,"this seem to be working:
 -@userId:*?",0.0,0.0,1.0,0.0
datadog,*  do not forget the minus at the start.,0.0,0.192,0.808,0.1695
datadog,Either of the  count_not_null()  or  count_nonzero()  functions should get you where you want.,0.0,0.098,0.902,0.0772
datadog,"If graph your metric, grouped by your tag, and then apply one of those functions, it should return the count of unique tag values under that tag key.",0.0,0.091,0.909,0.4019
datadog,So in your case:,0.0,0.0,1.0,0.0
datadog,count_not_null(sum:your.metric.name{*} by {file_name}),0.0,0.0,1.0,0.0
datadog,"And it works with multiple group-by tags too, so if you had separate tags for  file_name  and  directory  then you could use this same approach to graph the count of unique  combinations  of these tag values, or the count of unique combinations of  directory + file_name :",0.0,0.058,0.942,0.4019
datadog,"count_not_null(your.metric.name{*} by {file_name,directory})",0.0,0.0,1.0,0.0
datadog,"Yes, it is possible.",0.0,0.474,0.526,0.4019
datadog,"You can do that in a  processing pipeline  with a grok parser, but you'll want to configure which attribute the grok parser applies to in the advanced settings ( docs here ).",0.0,0.128,0.872,0.4497
datadog,"(By default grok parsers apply to the ""message"" attribute, but you can configure them to parse any attribute.)",0.0,0.0,1.0,0.0
datadog,"In this case, you'd set the  Extract From  field to  requestUri .",0.0,0.0,1.0,0.0
datadog,The  Helper Rules  section is not necessary for this.,0.0,0.231,0.769,0.34
datadog,"And then in the main  Define Parsing Rules  section, you'll plug in a rule similar to this:",0.0,0.0,1.0,0.0
datadog,or even further,0.0,0.0,1.0,0.0
datadog,What about using the template variables  doc ?,0.0,0.0,1.0,0.0
datadog,You could select:,0.0,0.0,1.0,0.0
datadog,Then you'll be able to replace your  {name:$flavor-db-master}  with  {$Name},0.0,0.0,1.0,0.0
datadog,"Otherwise, if you actually wants the value of the template variable you have to use  $flavor.value .",0.0,0.138,0.862,0.34
datadog,I advise to use a not widget to check the actual behavior.,0.0,0.0,1.0,0.0
datadog,EDIT:,0.0,0.0,1.0,0.0
datadog,This kind of setup is not the recommended.,0.185,0.0,0.815,-0.1511
datadog,It would be better to set two tags on your database:,0.0,0.225,0.775,0.4404
datadog,"You would then have a unique selection of tags,  env:dev,dbname:db1-master .",0.0,0.0,1.0,0.0
datadog,It would then be easy to have a query such as:,0.0,0.244,0.756,0.4404
datadog,You can do this in a processing pipeline with 2 steps:,0.0,0.0,1.0,0.0
datadog,"If there are other queries/patterns you want to use to determine the log level/status, you can add multiple rules to the  Category Processor  in (1), and you can map the  level  value to  info/warn/error  and any other relevant status value.",0.0,0.142,0.858,0.6249
datadog,You mostly have to wait for it all to fill in over time.,0.0,0.0,1.0,0.0
datadog,Metric timestamps cannot be more than 10 minutes in the future or more than 1 hour in the past.,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/developers/metrics/,0.0,0.0,1.0,0.0
datadog,I got the answer to the second question.,0.0,0.0,1.0,0.0
datadog,"Now, I can get all tables from one database that I specified.",0.0,0.0,1.0,0.0
datadog,All I needed to do; relation_regex: '.,0.0,0.0,1.0,0.0
datadog,*' and disabled relation_name.,0.0,0.0,1.0,0.0
datadog,Answer to the first question I got from datadog is that there is no way to monitor all the DBs without listing them individually.,0.091,0.0,0.909,-0.296
datadog,"They may change this in future, but for now we have to add blocks for each and every database that we want to monitor",0.091,0.056,0.853,-0.2263
datadog,This works for me:,0.0,0.0,1.0,0.0
datadog,"What you are doing is correct only, however, the common mistake is not following the below.",0.138,0.0,0.862,-0.34
datadog,"This library MUST be imported and initialized before any instrumented
  module.",0.0,0.0,1.0,0.0
datadog,"When using a transpiler, you MUST import and initialize the
  tracer library in an external file and then import that file as a
  whole when building your application.",0.0,0.0,1.0,0.0
datadog,"This prevents hoisting and
  ensures that the tracer library gets imported and initialized before
  importing any other instrumented module.",0.0,0.067,0.933,0.0772
datadog,"Basically, you cannot have  require(any instrumented lib)  (e.g.",0.0,0.0,1.0,0.0
datadog,"http, express, etc) before calling init() tracing function.",0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/tracing/setup/nodejs/,0.0,0.0,1.0,0.0
datadog,"It's probably too late, but it may be useful to others.",0.0,0.278,0.722,0.5927
datadog,You can set tags by using the environment variables of the application container (not the agent container) by using DD_TAGS.,0.0,0.0,1.0,0.0
datadog,Apparently there is a datadog/agent:latest-jmx that should be used that contains the java image...,0.0,0.0,1.0,0.0
datadog,I just missed it in the docs.,0.306,0.0,0.694,-0.296
datadog,"I discussed this with Datadog support, and they confirmed that the  awslogs  logging driver prevents the Datadog agent container from accessing a container's logs.",0.0,0.167,0.833,0.4588
datadog,"Since  awslogs  is currently the only logging driver available to tasks using the Fargate launch type, getting logs into Datadog will require another method.",0.0,0.0,1.0,0.0
datadog,"Since the  awslogs  logging driver emits logs to CloudWatch, one method that I have used is to create a subscription to stream those log groups to Datadog's Lambda function as configured  here .",0.0,0.068,0.932,0.2732
datadog,"You can do that from the  Lambda side  using CloudWatch logs as the trigger, or from the CloudWatch Logs side, by clicking  Actions    Stream to AWS Lambda .",0.0,0.0,1.0,0.0
datadog,I chose the Lambda option because it was quick and easy and required no code changes to our applications (since we are still in the evaluation stage).,0.076,0.1,0.825,0.1779
datadog,Datadog support advised me that it was necessary to modify the Lambda function in order to attribute logs to the corresponding service:,0.0,0.114,0.886,0.4019
datadog,"In  this block , modify it to something like:",0.254,0.219,0.526,-0.1027
datadog,According to Datadog support:,0.0,0.474,0.526,0.4019
datadog,I had to make further modifications to set the value of the  syslog.,0.0,0.179,0.821,0.34
datadog,"*  keys in a way that made sense for our applications, but it works great.",0.0,0.32,0.68,0.7684
datadog,I found the solution: the user  datadog  didnt have permission to read connections that wasnt form him.,0.0,0.133,0.867,0.3182
datadog,So it was just getting a single row.,0.0,0.0,1.0,0.0
datadog,I gave permissions for that user to read  pg_stat_activity,0.0,0.0,1.0,0.0
datadog,"It looks like you created  some  policy, but not the policy of required type.",0.0,0.213,0.787,0.3071
datadog,"When you create the role for Datadog, you have to choose a very specific role type:",0.0,0.13,0.87,0.2732
datadog,Select Another AWS account for the Role Type.,0.0,0.0,1.0,0.0
datadog,and then create a policy for that role.,0.0,0.259,0.741,0.2732
datadog,"Also, don't forget to",0.0,0.357,0.643,0.1695
datadog,Check off Require external ID,0.0,0.0,1.0,0.0
datadog,You shouldn't have any problems as long as you follow the guideline step by step:  https://docs.datadoghq.com/integrations/amazon_web_services/,0.0,0.131,0.869,0.3089
datadog,"I had this problem, when I tried to both use the role-assumption role as an assumption role on the  assume_role_policy , as well as trying to attach it.",0.097,0.076,0.827,-0.1531
datadog,"Once I got rid of the aws_iam_policy that I created with the role-assumption policy doc as well as the role-policy attachment, it worked.",0.0,0.259,0.741,0.6486
datadog,Hope this helps.,0.0,0.846,0.154,0.6705
datadog,One of Dockers main features is portability and it makes sense to bind datadog into that environment.,0.0,0.0,1.0,0.0
datadog,That way they are packaged and deployed together and you don't have the overhead of installing datadog manually everywhere you choose to deploy.,0.0,0.0,1.0,0.0
datadog,"What they are also implying is that you should use  docker-compose  and turn your application / docker container into an multi-container Docker application, running your image(s) alongside the docker agent.",0.0,0.0,1.0,0.0
datadog,"Thus you will not need to write/build/run/manage a container via Dockerfile, but rather add the agent image to your  docker-compose.yml  along with its configuration.",0.0,0.0,1.0,0.0
datadog,Starting your multi-container application will still be easy via:,0.0,0.266,0.734,0.4404
datadog,Its really convenient and gives you additional features like their  autodiscovery  service.,0.0,0.185,0.815,0.3612
datadog,Just use a find me Twimlet.,0.0,0.0,1.0,0.0
datadog,Enter up to 10 numbers and a timeout between moving on to the next number.,0.0,0.091,0.909,0.0772
datadog,Twilio will do the rest.,0.0,0.0,1.0,0.0
datadog,https://www.twilio.com/labs/twimlets/findme,0.0,0.0,1.0,0.0
datadog,If you are looking for a more full featured paid solution I'd recommend PagerDuty.,0.0,0.304,0.696,0.5859
datadog,DataDog has an integration for PagerDuty.,0.0,0.0,1.0,0.0
datadog,Any monitor that gets triggered that mentions  @pagerduty-myteamname (as example) in the monitor message will cause PagerDuty to page the on call person.,0.0,0.0,1.0,0.0
datadog,If that person does not acknowledge the page you can configure it to go through list of people to contact next until it is acknowledged by someone.,0.0,0.0,1.0,0.0
datadog,The role arn:aws:iam::xxxxxxxxxx:role/DatadogAWSIntegrationRole also has to have permission to assume the role on the other account.,0.0,0.0,1.0,0.0
datadog,You'll have to update the DatadogAWSIntegrationRole on the primary account to include:,0.0,0.0,1.0,0.0
datadog,I suspect you're probably looking to query the event stream which is where all alerts from monitors can be found.,0.109,0.0,0.891,-0.296
datadog,The docs at  https://docs.datadoghq.com/api/#events-get-all  are a pretty good starting place.,0.0,0.466,0.534,0.7269
datadog,"You'll want to query this endpoint with the proper source and tags, but this should be a starting point.",0.0,0.063,0.937,0.0387
datadog,"If this doesn't quite work, I'd recommend looking at pulling the details from the monitor as shown here:   https://docs.datadoghq.com/api/#monitor-get-details .",0.0,0.133,0.867,0.4144
datadog,This may be a second option if you're unable to get the information you're looking for from the event stream.,0.0,0.0,1.0,0.0
datadog,"The ""ReadTimeout: HTTPConnectionPool"" error can be corrected by adding a timeout parameter under instances in the elasticsearch.yaml",0.153,0.0,0.847,-0.4019
datadog,"Frank,",0.0,0.0,1.0,0.0
datadog,"Your use case follows the standard ""custom metric"" submission that is common within Datadog.",0.0,0.0,1.0,0.0
datadog,Using one of the supported libraries:,0.0,0.315,0.685,0.3182
datadog,http://docs.datadoghq.com/libraries/#java,0.0,0.0,1.0,0.0
datadog,You can leverage the statsD port of an Agent running on your host to submit these custom metrics:,0.0,0.0,1.0,0.0
datadog,http://docs.datadoghq.com/guides/dogstatsd/,0.0,0.0,1.0,0.0
datadog,You will want to install the Agent on either the host running this function or point your statsD connection towards an accepting host:,0.0,0.157,0.843,0.4404
datadog,http://docs.datadoghq.com/guides/basic_agent_usage/,0.0,0.0,1.0,0.0
datadog,There are additional docs found here that should help you understand how custom metrics work in Datadog:,0.0,0.144,0.856,0.4019
datadog,https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-,0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/203765485-How-do-I-submit-custom-metrics-What-s-their-overhead-,0.0,0.0,1.0,0.0
datadog,"Usually when troubleshooting custom metric submissions, we try to implement some form of local printing/logging to ensure the statsD connection is being made and that the custom function is being called and submitted.",0.0,0.122,0.878,0.5106
datadog,"Once you can confirm the metric is being sent to the Agent, use the Metric Summary page to check for the custom metric:",0.0,0.0,1.0,0.0
datadog,https://app.datadoghq.com/metric/summary,0.0,0.0,1.0,0.0
datadog,"If all else fails, reach out to Datadog at support@datadoghq.com",0.235,0.092,0.672,-0.4019
datadog,"It is very much possible, just use the alias property of the  attribute  filter:",0.0,0.0,1.0,0.0
datadog,Verify if the datadog package is installed in your environment.,0.0,0.0,1.0,0.0
datadog,You can do this with this command:,0.0,0.0,1.0,0.0
datadog,"If it's not installed, you can install it with this command:",0.0,0.0,1.0,0.0
datadog,You could tag your metrics with the name or ID of the agent it is collecting metrics from (if you aren't already).,0.0,0.0,1.0,0.0
datadog,Then in Datadog you could write a query that groups by the agent ID and applies a count_not_null function:  https://docs.datadoghq.com/dashboards/functions/count/,0.0,0.0,1.0,0.0
datadog,This basically hijacks a random metric to extract the unique count of agents reporting that metric to assume the total count of agents.,0.0,0.0,1.0,0.0
datadog,"You wouldn't be able to easily group by queue though, so idk if it would be a good solution to your use case.",0.068,0.277,0.655,0.694
datadog,Your idea around using gauges sounds good to me.,0.0,0.266,0.734,0.4404
datadog,You can send a new metric called something like  myagent.running  which sends a value of 1 for each of your agents and does a sum of all gauges in order to get a count.,0.0,0.154,0.846,0.5994
datadog,That is actually how the metric  datadog.agent.running  is implemented:  https://docs.datadoghq.com/integrations/agent_metrics/#metrics,0.0,0.0,1.0,0.0
datadog,After testing different queries I found that running this query groups the results by query statements and will return the count of each.,0.0,0.0,1.0,0.0
datadog,In Datadogs mysql configuration I added tags for the query statement and database name.,0.0,0.0,1.0,0.0
datadog,"Now since they are grouped, I can see information per different statement in datadog.",0.0,0.0,1.0,0.0
datadog,Datadog’s IIS integration queries the Web Service performance counters automatically and sends the results to Datadog.,0.0,0.0,1.0,0.0
datadog,The Web Service performance counter class collects information from the World Wide Web Publishing Service.,0.0,0.0,1.0,0.0
datadog,You can enable the IIS integration by creating a configuration file either manually or through the Datadog Agent GUI.,0.0,0.115,0.885,0.296
datadog,"To create a configuration file through the GUI, navigate to the “Checks” tab, choose “Manage Checks,” and select the iis check from the “Add a Check” menu.",0.0,0.08,0.92,0.2732
datadog,You can also manually create a conf.yaml file in C:\ProgramData\Datadog\conf.d\iis.d.,0.0,0.208,0.792,0.2732
datadog,There is a sites attribute in the conf.yaml file.,0.0,0.0,1.0,0.0
datadog,This attribute represents the IIS site you want to monitor.,0.0,0.126,0.874,0.0772
datadog,You only need to delete the sites you want to exclude.,0.156,0.107,0.738,-0.1531
datadog,More information you can refer to this link:  IIS monitoring with Datadog .,0.0,0.0,1.0,0.0
datadog,"If you are using Datadog's .NET Tracer, you can set  DD_TRACE_ENABLED=false  in the  appSettings  section of the  web.config  file ( docs ).",0.0,0.0,1.0,0.0
datadog,For example:,0.0,0.0,1.0,0.0
datadog,Another option is to deploy a  datadog.json  file ( docs ) in the root of your app that contains:,0.0,0.0,1.0,0.0
datadog,(Disclaimer: I work at Datadog),0.0,0.0,1.0,0.0
datadog,Update: I found this bit of code in the tracing client repo:,0.0,0.0,1.0,0.0
datadog,https://github.com/DataDog/dd-trace-js/issues/1249,0.0,0.0,1.0,0.0
datadog,maybe it would help,0.0,0.474,0.526,0.4019
datadog,Old message:,0.0,0.0,1.0,0.0
datadog,Never mind.,0.0,0.0,1.0,0.0
datadog,"seems like my solution is only for express, graphql doesn't support that property",0.132,0.281,0.586,0.3699
datadog,You probably want to just modify the validateStatus property in the http module:,0.0,0.098,0.902,0.0772
datadog,Callback function to determine if there was an error.,0.252,0.0,0.748,-0.4019
datadog,It should take a status code as its only parameter and return true for success or false for errors,0.1,0.272,0.628,0.6249
datadog,https://datadoghq.dev/dd-trace-js/interfaces/plugins.http.html#validatestatus,0.0,0.0,1.0,0.0
datadog,As an example you should be able to mark 403s as not be errors with something like this:,0.0,0.221,0.779,0.5478
datadog,"Unfortunately, that doesn't seem to be a setting you can directly control at this time.",0.156,0.0,0.844,-0.34
datadog,"The reasoning is that a given timeseries could be split by tag, so setting a single color for a timeseries split by tag would amount to multiple entires of the same color, and that wouldn't make sense.",0.0,0.0,1.0,0.0
datadog,"To support the semantic meaning, I've often used the following settings:",0.0,0.213,0.787,0.4019
datadog,"I'm not sure I know what distinction there is of Error vs Critical in your definition, but using these palettes has proven useful for my team.",0.167,0.129,0.704,0.2189
datadog,"If you're looking for a specific widget to change color based on value - so if the number exceeds a threshold - take a look at the  Query Value Widget , as that can be customized to change color based on the current value.",0.0,0.205,0.795,0.7759
datadog,"Alternately, if you have a Monitor already set for the timeseries, use the  Alert Value Widget  to show the current status, with less configuration, since the thresholds are managed in the Monitor's definition.",0.0,0.133,0.867,0.5574
datadog,"You can't set a color per line, but you can set the color per query.",0.0,0.0,1.0,0.0
datadog,"If you edit the graph using json, that field  requests.style.palette  is exposed and you can just try typing in whatever color you want there.",0.053,0.053,0.894,0.0
datadog,https://imgur.com/VrZZl72,0.0,0.0,1.0,0.0
datadog,"If you want to have one time series that is green for hits, and one that is red for errors, you just make two metric queries, and then color one green and one red.",0.067,0.036,0.896,-0.2732
datadog,https://imgur.com/AHGi1Hk,0.0,0.0,1.0,0.0
datadog,"That endpoint is used to send events to Datadog, if your one account is sending them to the logs product, it is most likely because that account has a special flag that converts all your incoming events into logs.",0.0,0.068,0.932,0.4019
datadog,This is common for users that use the Security Monitoring product.,0.0,0.194,0.806,0.34
datadog,I would recommend reaching out to support@datadoghq.com to see if this flag is enabled in that one account.,0.0,0.223,0.777,0.5106
datadog,"And if you want to replicate that behavior in another account, you can ask them to enable that feature.",0.0,0.067,0.933,0.0772
datadog,"In Datadog, create an API key in Integrations, APIs.",0.0,0.208,0.792,0.2732
datadog,Give the API key a name.,0.0,0.0,1.0,0.0
datadog,In NLog.config create a target.,0.0,0.412,0.588,0.2732
datadog,The url is either datadoghq.com or datadoghq.eu (for europe).,0.0,0.0,1.0,0.0
datadog,Now create a rule to write to the target and you are done!,0.0,0.179,0.821,0.3382
datadog,"All of the parameters can be configured to become columns in Datadog, and/or facets to select on.",0.0,0.0,1.0,0.0
datadog,"I am using a date parameter so that the date matches other logs, rather than displaying the built in date.",0.0,0.0,1.0,0.0
datadog,The datadog agent you deployed has no power to run scripts or take action.,0.145,0.0,0.855,-0.296
datadog,It is purely a monitoring/data collection tool.,0.0,0.0,1.0,0.0
datadog,However one of the things your monitors in the Datadog application can do is trigger events when they go into an alert state.,0.0,0.091,0.909,0.296
datadog,"There are  lots of integrations : creating a ticket in Jira, posting a message to Slack, triggering an SNS topic.",0.0,0.121,0.879,0.296
datadog,"What I recommend you try to do, is to create some kind of job or script that can be triggered externally, like a lambda function, or a jenkins job, or anything really.",0.0,0.215,0.785,0.7269
datadog,When the monitor goes off you can use a  webhook  to trigger that script to do whatever you define.,0.0,0.0,1.0,0.0
datadog,Here is a blog post showing  how twilio sent out a text message by connecting their api to a webhook .,0.0,0.0,1.0,0.0
datadog,"In these kinds of checks, the response order matters, since the columns returned from the DB are going to be mapped back to the names specified in the YAML.",0.0,0.038,0.962,0.0258
datadog,Reading the error message:,0.474,0.0,0.526,-0.4019
datadog,Error: postgres:953578488181a512 | (postgres.py:398) | non-numeric value  cldtx  for metric column  active_connections  of metric_prefix  postgresql,0.168,0.149,0.683,-0.0772
datadog,"We can see that the value of  cldtx  is being returned for the  active_connections  column, which in the YAML is declared as a gauge, and this is a string.",0.0,0.085,0.915,0.34
datadog,"The fix should be straightforward, by reordering the YAML, like so:",0.0,0.2,0.8,0.3612
datadog,"Alternately, if you want to keep the YAML ordered, change the query to:",0.0,0.098,0.902,0.0772
datadog,So I ended up just looking at the tests in the datadog terraform provider and noticing the query format they are testing.,0.0,0.0,1.0,0.0
datadog,It seems you need to specify a time range and also add in a comparison threshold that matches your critical alert threshold.,0.102,0.098,0.8,-0.0258
datadog,That was what was missing.,0.355,0.0,0.645,-0.296
datadog,It is not possible no.,0.0,0.321,0.679,0.2235
datadog,Confirmed with DD support.,0.0,0.474,0.526,0.4019
datadog,"Actually it is possible, but you need to put every json log into quotes (some prefix before each log will also work), so that Datadog agent will consider this as a 'text'.",0.0,0.0,1.0,0.0
datadog,I.e.,0.0,0.0,1.0,0.0
datadog,log.json  file should contain quoted logs:,0.0,0.0,1.0,0.0
datadog,"After that, in Datadog Logs Configuration you need to add a pipeline with Grok parser filter  json  (see filter tab in  Matcher and Filter ):",0.109,0.0,0.891,-0.4215
datadog,This allowed me to perform full text search thru all fields in my json logs and automatically parse all json fields as attributes.,0.0,0.0,1.0,0.0
datadog,P.S.,0.0,0.0,1.0,0.0
datadog,This solution was provided by Datadog support 2 years ago.,0.0,0.417,0.583,0.6124
datadog,And seems they are working on solution to allow full text search for JSON logs.,0.0,0.244,0.756,0.4939
datadog,"It looks from the above snippets that the  combined  Morgan format is sent directly sent to Winston, and then parsed within a log pipeline in Datadog.",0.0,0.0,1.0,0.0
datadog,"Since the  combined  format doesn't include the body and there is no built-in token for it, you would have to use a custom format with your own tokens and then update your pipeline accordingly.",0.064,0.0,0.936,-0.296
datadog,"For example, to create a custom format in Morgan that includes the status code and the body:",0.0,0.123,0.877,0.2732
datadog,You can also create a token to achieve the same result with a simpler format definition:,0.0,0.139,0.861,0.2732
datadog,"You can find the documentation for custom Morgan formats  here , creating tokens  here , and Datadog log pipeline parsing  here .",0.0,0.109,0.891,0.296
datadog,Hope this helps!,0.0,0.853,0.147,0.6996
datadog,"Maybe you can ask them to add it by opening a feature request:
 https://github.com/DataDog/documentation/issues/new/choose",0.0,0.0,1.0,0.0
datadog,but it looks like I'm missing an important part here.,0.184,0.357,0.459,0.3919
datadog,That was the point.,0.0,0.0,1.0,0.0
datadog,The lambda itself has not much todo with particular  statusCodes .,0.0,0.0,1.0,0.0
datadog,So I either may log each status code and let datadog parse it accordingly.,0.0,0.0,1.0,0.0
datadog,"Or, that's the solution I went for, I can leverage API-Gateway for monitoring status codes per lambda.",0.0,0.141,0.859,0.3182
datadog,"You can use most any of the common open source log shippers to send server logs to Datadog without using the Datadog agent, for example  fluentd .",0.0,0.0,1.0,0.0
datadog,"But there can be several benefits to using the Datadog agent to collect server logs, such as:",0.0,0.175,0.825,0.5267
datadog,"There are other ways to collect logs in Datadog, among those is the  HTTP API .",0.0,0.0,1.0,0.0
datadog,"Since this API uses a POST method, I bet you could configure Datadog's  webhook integration  to generate log events from Datadog events and alerts.",0.0,0.0,1.0,0.0
datadog,"That said, before you go through the trouble of doing this, if you have a use-case or reason you're interested in doing this, you may want to reach out to Datadog support to see if they have some features coming / in beta that would get you what you want without the extra work on your end.",0.044,0.15,0.806,0.5267
datadog,(What  is  your use-case?,0.0,0.0,1.0,0.0
datadog,I'm curious),0.0,0.0,1.0,0.0
datadog,"Sounds like some of your  http.server.requests.count  metric values do not have any  status  tag, so when you group by the  status  tag, those are being aggregaed with a value of  n/a .",0.0,0.22,0.78,0.765
datadog,"If it is intentional/expected that this metric would have values without the  status  tag and you just want to ignored those metric values, then you can use the  exclude_null()  function to remove that tag grouping from your graph (docs  here ).",0.05,0.146,0.804,0.5267
datadog,"If it is not intentional/expected that this metric would have values without the  status  tag, then you probably want to reach out to support@datadoghq.com to get that looked into.",0.0,0.164,0.836,0.4767
datadog,Just add this in the startup.cs.,0.0,0.0,1.0,0.0
datadog,I'm very sorry for this late answer.,0.21,0.0,0.79,-0.1513
datadog,"After exchanging a bunch of emails with the Datadog support guys, it turned out that the solution is straightforward:",0.0,0.238,0.762,0.6124
datadog,The  Dispose()  method force the sinks to gracefully close up and send the logs stored in cache.,0.0,0.175,0.825,0.5267
datadog,Please note that the logs do not appear instantly in the Datadog console: allow a few seconds (to some minutes) for their systems to process the logs you send.,0.0,0.139,0.861,0.4939
datadog,"You might want to use datadog service-check which is also can be sent by the StatsDClient, and then add it to your monitor/dashboard page.",0.0,0.053,0.947,0.0772
datadog,https://docs.datadoghq.com/developers/service_checks/,0.0,0.0,1.0,0.0
datadog,"Another option is to use  Datadog's Java / JMX integration  to capture the health data that's exposed over JMX -- this can probably give you up/down health data, and can certainly give you a lot more granular health metrics too.",0.033,0.06,0.907,0.2732
datadog,"Well, apparently you can  -@facet:*",0.0,0.344,0.656,0.2732
datadog,"Didn't specify it in my question because it was not important, but what I really needed was a way to either  filter by a specific facet value, or get logs without said facet",0.04,0.096,0.864,0.4222
datadog,The following works for me:,0.0,0.0,1.0,0.0
datadog,Spring is scanning your classpath that seems incomplete.,0.0,0.0,1.0,0.0
datadog,Maybe it is related to Spring's class loading mechanisms.,0.0,0.0,1.0,0.0
datadog,The  class in question  exists and seems to be part of the agent.,0.0,0.0,1.0,0.0
datadog,"Possibly, you are using an outdated version of the agent.",0.0,0.0,1.0,0.0
datadog,I'm not particularly familiar with this Datadog JSON format but the general pattern I would propose here has multiple steps:,0.0,0.0,1.0,0.0
datadog,Here's a basic example of that to show the general principle.,0.0,0.0,1.0,0.0
datadog,It will need more work to capture all of the details you included in your initial example.,0.0,0.0,1.0,0.0
datadog,"The separate normalization step to build  local.screenboard  here isn't strictly necessary: you could instead put the same sort of normalization expressions (using  try  to set defaults for things that aren't set) directly inside the  resource ""datadog_screenboard""  block arguments if you wanted.",0.126,0.0,0.874,-0.6808
datadog,"I prefer to treat normalization as a separate step because then this leaves a clear definition in the configuration for what we're expecting to find in the JSON and what default values we'll use for optional items, separate from defining how that result is then mapped onto the physical  datadog_screenboard  resource.",0.0,0.151,0.849,0.7906
datadog,I wasn't able to test the example above because I don't have a Datadog account.,0.0,0.0,1.0,0.0
datadog,I'm sorry if there are minor typos/mistakes in it that lead to errors.,0.252,0.0,0.748,-0.4019
datadog,"My hope was to show the general principle of mapping from a serialized data file to a resource rather than to give a ready-to-use solution, so I hope the above includes enough examples of different situations that you can see how to extend it for the remaining Datadog JSON features you want to support in this module.",0.0,0.227,0.773,0.8957
datadog,"If this JSON format is a interchange format formally documented by Datadog, it could make sense for Terraform's Datadog provider to have the option of accepting a single JSON string in this format as configuration, for easier exporting.",0.0,0.137,0.863,0.6597
datadog,"That may require changes to the Datadog provider itself, which is beyond what I can answer here but might be worth raising in the GitHub issues for that provider to streamline this use-case.",0.0,0.07,0.93,0.3291
datadog,I solved this by adding 'squashfs' to the list of filesystem types to be ignored by the datadog agent.,0.113,0.103,0.784,-0.0516
datadog,Create a file  /etc/datadog-agent/conf.d/disk.d/conf.yaml :,0.0,0.512,0.488,0.2732
datadog,Restart datadog agent ( systemctl restart datadog-agent ).,0.0,0.0,1.0,0.0
datadog,"If you have an ever increasing counter, you can use the a function called  rate .",0.0,0.0,1.0,0.0
datadog,You'll be able to select it with the  +  on the query line.,0.0,0.0,1.0,0.0
datadog,"With that you'll be able to have a rate of increase per seconds, minutes or hours.",0.0,0.141,0.859,0.3182
datadog,"If you are looking to get a difference between the same metric but at another point in the past, you have a function called  timeshift  that could also help.",0.0,0.12,0.88,0.5499
datadog,This is also accessible with the small  +  on the right of the query line.,0.0,0.0,1.0,0.0
datadog,"Finally, if you are looking at comparing two different metrics, you  have a button called  Advanced  that will enable you to write more complex queries such as a difference between two metrics.",0.0,0.065,0.935,0.25
datadog,"I believe you are looking for  clusterName :
 https://github.com/helm/charts/blob/master/stable/datadog/values.yaml#L75",0.0,0.0,1.0,0.0
datadog,You can add it in your  values.yaml  under the  datadog  section like this:,0.0,0.172,0.828,0.3612
datadog,refer below command,0.0,0.0,1.0,0.0
datadog,"I am not a fan of helm, but you can accomplish this in 2 ways:",0.098,0.244,0.659,0.4971
datadog,via env vars: make use of  DD_AC_EXCLUDE  variable to exclude the Redis containers: eg  DD_AC_EXCLUDE=name:prefix-redis,0.119,0.0,0.881,-0.2263
datadog,"via a config map: mount an empty config map in  /etc/datadog-agent/conf.d/redisdb.d/ , below is an example where I renamed the  auto_conf.yaml  to  auto_conf.yaml.example .",0.087,0.0,0.913,-0.2023
datadog,alter the daemonset/deployment object:,0.0,0.0,1.0,0.0
datadog,"On the bottom of the infrastructure list, you should see a link called ""JSON API permalink"".",0.0,0.0,1.0,0.0
datadog,"If you  query it , this should give you a JSON of all your hosts with their agent version.",0.0,0.0,1.0,0.0
datadog,You can then query it with a quick Python script.,0.0,0.0,1.0,0.0
datadog,"Your metrics should have a common prefix like  myapp.metric1 ,  myapp.metric2 , etc.",0.0,0.217,0.783,0.3612
datadog,Then you can disable all metrics and enable explicitly all  myapp.,0.0,0.0,1.0,0.0
datadog,*  metrics like so:,0.0,0.556,0.444,0.3612
datadog,application.properties:,0.0,0.0,1.0,0.0
datadog,the  management.metrics.enable.&lt;your_custom_prefix&gt;  will enable all  &lt;your_custome_prefix&gt;.,0.0,0.0,1.0,0.0
datadog,*  metrics.,0.0,0.0,1.0,0.0
datadog,"If you want to enable some of the built-in core metrics again, for example reenabling  jvm.",0.0,0.08,0.92,0.0772
datadog,"* , you can do:",0.0,0.0,1.0,0.0
datadog,"I've created a sample project  in github  that disables core metrics, enables custom metrics, and  jvm.",0.0,0.125,0.875,0.25
datadog,*  metrics and sends to Datadog.,0.0,0.0,1.0,0.0
datadog,Not sure I fully grasp the issue.,0.282,0.0,0.718,-0.2411
datadog,Here are some steps to collect your traces:,0.0,0.0,1.0,0.0
datadog,"Just in case, more info on Open Tracing  here",0.0,0.0,1.0,0.0
datadog,Seems like there is a typo in your command.,0.0,0.263,0.737,0.3612
datadog,DD_DOGSTATD_NON_LOCAL_TRAFFIC   is used instead of  DD_DOGSTATSD_NON_LOCAL_TRAFFIC,0.0,0.0,1.0,0.0
datadog,I usually used the below command for testing with Datadog:,0.0,0.0,1.0,0.0
datadog,Have you tried  composite monitors ?,0.0,0.0,1.0,0.0
datadog,You should be able to combine your low CPU Credit monitor with another monitor that looks at events from RDS.,0.093,0.115,0.793,0.128
datadog,Two monitors such as:,0.0,0.0,1.0,0.0
datadog,A composite monitor: A &amp;&amp; !B,0.0,0.0,1.0,0.0
datadog,(I hope my example makes sense),0.0,0.367,0.633,0.4404
datadog,"so, I had to install a Datadog agent on ec2 instance and configure it to be able to access all mysql dbs and collect the mysql performance schema metrics.",0.0,0.0,1.0,0.0
datadog,And surely with correct security groups for the ec2.,0.0,0.431,0.569,0.6486
datadog,I followed this  docs  and  this  and contacted the support.,0.0,0.252,0.748,0.4019
datadog,"If the Windows OS is D drive, the setting is installed in  D:\ProgramData\Datadog .",0.0,0.0,1.0,0.0
datadog,"Copying it to  C:\ProgramData\Datadog  will work, but I submitted an improvement request to Datadog Support.",0.0,0.386,0.614,0.8201
datadog,Yes it does have this functionality in the Audit Association entity.,0.0,0.213,0.787,0.4019
datadog,The entity stored with the blame_id in the Audit Log entity contains information regarding the user.,0.0,0.0,1.0,0.0
datadog,The one with source_id contains information regarding the entity itself and thus the ID of the entity in the  fk  field.,0.0,0.0,1.0,0.0
datadog,"Once the datadog is properly installed on your server, you can use the custom metric feature to let datadog read your query result into a custom metric and then use that metric to create a dashboard.",0.0,0.06,0.94,0.2732
datadog,You can find more on custom metric on datadog  here,0.0,0.0,1.0,0.0
datadog,They work with yaml file so be cautious with the formatting of the yaml file that will hold your custom metric.,0.077,0.0,0.923,-0.1725
datadog,It looks like Datadog uses zstd compression in order to compress its data before sending it:  https://github.com/DataDog/datadog-agent/blob/972c4caf3e6bc7fa877c4a761122aef88e748b48/pkg/util/compression/zlib.go,0.0,0.135,0.865,0.3612
datadog,This is what I've got so far.,0.0,0.0,1.0,0.0
datadog,Everything but the  source .,0.0,0.0,1.0,0.0
datadog,Need to use category-processor  https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor,0.0,0.0,1.0,0.0
datadog,Example:,0.0,0.0,1.0,0.0
datadog,"I didn't do enough research before posting this question, but the answer for anyone else looking.",0.0,0.0,1.0,0.0
datadog,"You are using the DataDog configuration for the commercial  k6 Cloud service  ( k6 cloud ), not locally run k6 tests ( k6 run ).",0.0,0.0,1.0,0.0
datadog,"test_run_id  is a concept in the cloud service, though it's also easy to emulate locally as a way to distinguish between test runs.",0.0,0.127,0.873,0.4404
datadog,"For local tests, you should enable the DataDog output by running k6 with  k6 run --out datadog script.js .",0.0,0.0,1.0,0.0
datadog,"I assume you did that, otherwise you wouldn't see any metrics in DataDog.",0.0,0.0,1.0,0.0
datadog,"Then, you can use the  tags  option  to inject a unique extra tag for all metrics generated by a particular k6 run, so you can differentiate them in DataDog.",0.0,0.0,1.0,0.0
datadog,For example:,0.0,0.0,1.0,0.0
datadog,"Of course, you can choose any  key=value  combination, you are not restricted to  test_run_id .",0.0,0.144,0.856,0.2924
datadog,Nothing is wrong there.,0.0,0.46,0.54,0.3724
datadog,DataDog conceals that the Kafka integration uses Dogstatsd under the hood.,0.0,0.0,1.0,0.0
datadog,"When  use_dogstatsd: 'true  within /etc/datadog-agent/datadog.yaml is set, metrics do appear in DataDog webUI.",0.0,0.189,0.811,0.4215
datadog,If that option is not set the default Broker data is available via JMXFetch using  sudo -u dd-agent datadog-agent status  as also via  sudo -u dd-agent datadog-agent check kafka  but not in the webUI.,0.0,0.0,1.0,0.0
datadog,Based on the  doc  you can decide which one is good for your use case.,0.0,0.172,0.828,0.4404
datadog,"Stackdriver is detailed as &quot;Monitoring, logging, and diagnostics for applications on Google Cloud Platform and AWS&quot;.",0.0,0.0,1.0,0.0
datadog,"Google Stackdriver provides powerful monitoring, logging, and diagnostics.",0.0,0.286,0.714,0.4215
datadog,"It equips you with insight into the health, performance, and availability of cloud-powered applications, enabling you to find and fix issues faster.",0.0,0.0,1.0,0.0
datadog,"Grafana can be classified as a tool in the &quot;Monitoring Tools&quot; category, while Stackdriver is grouped under &quot;Cloud Monitoring&quot;.",0.0,0.0,1.0,0.0
datadog,Answering my own question this might be helpful for others,0.0,0.237,0.763,0.4215
datadog,I had to set,0.0,0.0,1.0,0.0
datadog,inside  /etc/datadog-agent/datadog.yaml  then I've created  python.d/conf.yaml  with the following configs,0.0,0.182,0.818,0.25
datadog,Restart the agent with,0.0,0.0,1.0,0.0
datadog,You can see your logs in the dashboard logs panel,0.0,0.0,1.0,0.0
datadog,Window option available... Go to integration and  click on agents..,0.0,0.0,1.0,0.0
datadog,There is an option available windows ( left side ).. Click on windows then u get link..... Just copy the link and paste it on ur windows Server.... Then datadog starts monitoring...,0.0,0.0,1.0,0.0
datadog,It takes 5 minutes to monitor,0.0,0.0,1.0,0.0
datadog,The MeterRegistry already has implemented how to send custom metrics (posting to DataDog) See the code  https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java#L133,0.0,0.0,1.0,0.0
datadog,It appears like you are trying to add common tags to each metric.,0.0,0.172,0.828,0.3612
datadog,"Perhaps you shouldn't implement your own DataDog registry, and instead use the provided one to send the metrics and set the common tags via config:",0.0,0.0,1.0,0.0
datadog,I don't believe there is any way to graph the historical behavior of the SLI from an SLO.,0.0,0.0,1.0,0.0
datadog,"The closest you could get would be to measure the underlying metric, so if you had  good events / bad events  you could display that percentage.",0.119,0.099,0.782,-0.1531
datadog,But the calculation of how often that percentage is above or below a certain threshold would not be possible.,0.0,0.135,0.865,0.3919
datadog,I recommend reaching out to support@datadoghq.com to let them know it's a feature you're interested in.,0.0,0.389,0.611,0.7184
datadog,They might be able to provide some updates.,0.0,0.0,1.0,0.0
datadog,Not sure if this will work but you can give it a try..:,0.119,0.0,0.881,-0.1232
datadog,{{^is_exact_match a.value b.value }},0.0,0.0,1.0,0.0
datadog,"@my@mail.com
Alert 2 hosts has passed the threshold",0.0,0.268,0.732,0.296
datadog,{{/is_exact_match}},0.0,0.0,1.0,0.0
datadog,same value - ignore - do nothing,0.316,0.304,0.38,-0.0258
datadog,The problem is that you probably might get 2 alerts at the same time...,0.184,0.0,0.816,-0.4019
datadog,It turns out that trace id can be set via HTTP endpoint  https://docs.datadoghq.com/api/v1/tracing/#send-traces .,0.0,0.0,1.0,0.0
datadog,There doesn't seem to be an option for sending traces to the agent directly.,0.0,0.0,1.0,0.0
datadog,"This can still be useful if the performance penalty of making HTTP calls is not a concern, i.e., if you are not working on a real-time system.",0.104,0.1,0.796,-0.0258
datadog,"Datadog keeps the logs for a period of time according to the billing plan you've selected:  https://www.datadoghq.com/pricing/#section-log  If you choose the 7 day plan, logs will be dropped from Datadog after 7 days.",0.0,0.0,1.0,0.0
datadog,"The default plan seems to be 15 days, but there are other options between 3-60 days.",0.0,0.0,1.0,0.0
datadog,I've solved it now by verifying the status via code and by adding tags to the metrics:,0.0,0.116,0.884,0.2732
datadog,This way I can filter in my dashboard for  occurrence:first  only.,0.0,0.0,1.0,0.0
datadog,"To make sure things are clear, you have a metric called  myService.errorType  with a tag  entity .",0.0,0.29,0.71,0.5994
datadog,This metric is a counter that will increase every time an entity is in error.,0.159,0.135,0.706,-0.1027
datadog,You will then use this metric query:,0.0,0.0,1.0,0.0
datadog,"When you speak about UUID, it seems that the cardinality is small (here you show 3).",0.0,0.0,1.0,0.0
datadog,Which means that every hour you will have small amount of UUID available.,0.0,0.0,1.0,0.0
datadog,"In that case, adding UUID to the metric tags is not as critical as user ID, timestamp, etc.",0.0,0.103,0.897,0.2411
datadog,which have a limitless number of options.,0.0,0.206,0.794,0.0772
datadog,"I would invite you to add this uuid tag, and check the cardinality in the  metric summary page  to ensure it works.",0.0,0.181,0.819,0.4939
datadog,"Then to get the number of UUID concerned by errors, you can use something like:",0.132,0.209,0.659,0.1027
datadog,"Finally, as an alternative, if the cardinality of UUID can go through the roof, I would invite you to work with logs or work with Christopher's solution which seems to limit the cardinality increase as well.",0.0,0.211,0.789,0.743
datadog,You may need to set the environment variable  DD_APM_NON_LOCAL_TRAFFIC=true  in your datadog agent container.,0.0,0.0,1.0,0.0
datadog,Ref:  https://docs.datadoghq.com/agent/docker/apm/?tab=linux#docker-apm-agent-environment-variables,0.0,0.0,1.0,0.0
datadog,Allowing 'triggered' (but not 'recovery') monitor notifications is not a configurable option for the integration on either Opsgenie or Datadog.,0.0,0.0,1.0,0.0
datadog,"You  can , however, control this within the Datadog Monitor message body where you reference opsgenie",0.0,0.0,1.0,0.0
datadog,Lorem ipsum dolor sit amet @opsgenie-oncall @slack-somechannel,0.0,0.0,1.0,0.0
datadog,"You can wrap the opsgenie reference within the message body with conditional tags (datadog actually calls them variables) documented here:
 https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables 
Then the message body might look something like this:",0.0,0.079,0.921,0.3612
datadog,Lorem ipsum dolor sit amet {{#is_alert}}@opsgenie-oncall{{/is_alert}} @slack-somechannel,0.0,0.0,1.0,0.0
datadog,Now the alert to opsgenie will only occur on the 'trigger' of the monitor but not on 'recovery',0.0,0.086,0.914,0.1531
datadog,"If you are sending metrics to an actual StatsD server, then tags are not supported by the protocol.",0.103,0.0,0.897,-0.2411
datadog,You would need to instead send the metrics to the Datadog agent's DogStatsD endpoint which extends StatsD with additional features such as tags.,0.0,0.064,0.936,0.128
datadog,You can find more information about DogStatsD  here .,0.0,0.0,1.0,0.0
datadog,"If you are already using the DogStatsD endpoint, then I would suspect an incompatibility with the  node-statsd  library.",0.121,0.0,0.879,-0.296
datadog,"The library has not been updated for 6 years and it's possible that something changed since then, causing it to no longer work.",0.095,0.0,0.905,-0.296
datadog,In that case I would recommend switching to a more recent DogStatsD client that is still maintained such as  hot-shots .,0.0,0.128,0.872,0.3612
datadog,Hope this helps!,0.0,0.853,0.147,0.6996
datadog,There's a 2-click path from Slack that should already do this for you out-of-the-box.,0.0,0.0,1.0,0.0
datadog,"The slack notification gives you a link to the alert event in your Datadog account (click-1), and from the alert event, towards the bottom you'll find a series of links to other relevant places, one of those is ""Related Logs"" (click-2).",0.0,0.106,0.894,0.5267
datadog,"That brings you to the Log Explorer scoped to the relevant time period of the alert, and scoped to the tags of whatever it was that was alerted on (so presumably the logs you're looking for).",0.0,0.059,0.941,0.296
datadog,"If you want to add a link of this sort as something you can configure in the alert message, that sounds like something you should reach out to support@datadoghq.com for to ask Datadog to implement it.",0.0,0.186,0.814,0.6249
datadog,In the end we solved the problem dynamically building the url for the logs:,0.148,0.251,0.601,0.2263
datadog,There's a Prometheus endpoint for Tibco EMS:,0.0,0.0,1.0,0.0
datadog,https://community.tibco.com/wiki/statistics-logger-tibco-enterprise-message-service#toc-15,0.0,0.0,1.0,0.0
datadog,"I think you can then add the prometheus integration to your datadog agent to send the data do datadog, and build your own dashboard for that:",0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/integrations/prometheus/#data-collected,0.0,0.0,1.0,0.0
datadog,"I had something similar issue and chose the first option, but i won't say it is from terraform perspective (since i also had lack in experience in terraform).",0.109,0.0,0.891,-0.4497
datadog,"The first hierarchy was more reasonable in segregation aspects, plus would be easier to add/remove/update organizations by demand.",0.074,0.138,0.788,0.3182
datadog,Answer from Datadog Support to this:,0.0,0.351,0.649,0.4019
datadog,Thanks again for reaching out to Datadog!,0.0,0.5,0.5,0.6114
datadog,"From looking further into this, there does not seem to be a way we can package the JDBC  driver with the Datadog Agent.",0.0,0.0,1.0,0.0
datadog,I understand that this is not desirable as you would prefer to use a standard image but I believe the best way to have these bundled together would be to have a custom image for your deployment.,0.039,0.152,0.81,0.7445
datadog,Apologies for any inconveniences that this may cause.,0.0,0.0,1.0,0.0
datadog,"Yes, you can configure widgets to exclude results by tags.",0.151,0.214,0.635,0.2023
datadog,You can do this by applying a tag prepended with a  !,0.0,0.0,1.0,0.0
datadog,"to signify ""not"".",0.0,0.0,1.0,0.0
datadog,"So in your case, you can set up your widget scoped over  importance:ignore  and then hit the little  &lt;/&gt;  button on the right to expose the underlying query, and sneak a  !",0.061,0.0,0.939,-0.2244
datadog,in front to make it  !importance:ignore .,0.0,0.0,1.0,0.0
datadog,"This doc has a nice example  (although it's for notebooks, it works the same in dashboards as well).",0.0,0.149,0.851,0.4215
datadog,"After talking with Datadog support, it seems like this is a known issue.",0.0,0.342,0.658,0.6369
datadog,Thanks for your patience while we looked into this issue.,0.0,0.244,0.756,0.4404
datadog,We we're currently investigating this along with PoolExecutors and will reach out with updates.,0.0,0.078,0.922,0.0258
datadog,"Right now it looks like those child spans within the async call lose context, so they appear disconnected.",0.127,0.118,0.755,-0.0516
datadog,The workaround for now is to pass in the parent's context.,0.0,0.0,1.0,0.0
datadog,Add this line just before calling the thread pool executor.,0.0,0.0,1.0,0.0
datadog,Then pass that context to the function that gets run in the threadpool:,0.0,0.0,1.0,0.0
datadog,And use it to create a span inside the function like this:,0.0,0.338,0.662,0.5574
datadog,The complete example looks like this:,0.0,0.333,0.667,0.3612
datadog,This will produce a result that looks like this:,0.0,0.263,0.737,0.3612
datadog,First  you'll want to make sure your logs are well structured (which you can control in  Datadog's processing pipelines ).,0.0,0.251,0.749,0.5719
datadog,"Effectively you'll want to parse out the ""code"" values into some ""error code"" attribute.",0.138,0.352,0.51,0.4939
datadog,If your log events are in this format...,0.0,0.0,1.0,0.0
datadog,"...Then all you need is a fairly simple grok parser rule, thanks to the ""json"" filter function.",0.0,0.162,0.838,0.4404
datadog,"Something like this would get you where you want (note the  %{data::json}  part, that's what parses the in-log JSON).",0.0,0.183,0.817,0.4215
datadog,"Once you've configured this, your logs will also have an attribute called ""error.code"" with a value of  2001  or  1001  or whatever.",0.0,0.107,0.893,0.34
datadog,Second  you'll want to  create a facet  for that new  error.code  attribute so that you can  make toplist / timeseries / etc.,0.0,0.167,0.833,0.34
datadog,"graphs grouped out by your ""error code"" facet .",0.278,0.0,0.722,-0.4019
datadog,"No, you cannot install the Datadog agent on a Snowflake host.",0.196,0.0,0.804,-0.296
datadog,We use our separate job scheduling system to monitor Snowflake by running queries (e.g.,0.0,0.0,1.0,0.0
datadog,"checks on SYSTEM$CLUSTERING_DEPTH, aggregate queries against the QUERY_HISTORY for timing, etc) via the JDBC connector then relaying the results to our monitoring stack (similar to how Datadog agent would work.)",0.0,0.0,1.0,0.0
datadog,Point both of those at a service like  httpbin  to see how they differ.,0.0,0.172,0.828,0.3612
datadog,"Requests'  data  option for POST requests  generates form-encoded data  by default, while  curl  passes the JSON string through directly.",0.0,0.0,1.0,0.0
datadog,You can manually encode your payload as a JSON string:,0.0,0.0,1.0,0.0
datadog,or if you have Requests version 2.4.2 or later you can use the  json  parameter to have your  dict  converted to JSON automatically:,0.0,0.0,1.0,0.0
datadog,One solution would be to setup in  logs &gt; configuration &gt; pipelines  a  category processor  to add a new attribute that could be made searchable.,0.0,0.095,0.905,0.3182
datadog,edit: 19th Nov 2019,0.0,0.0,1.0,0.0
datadog,Step 1:,0.0,0.0,1.0,0.0
datadog,Add grok parser to extract sign:,0.0,0.0,1.0,0.0
datadog,"rule:  detect_dollar .*%{regex(""[$]+""):dollarSign}.",0.0,0.0,1.0,0.0
datadog,*,0.0,0.0,0.0,0.0
datadog,Step 2:,0.0,0.0,1.0,0.0
datadog,Then you can setup a category processor as indicated above.,0.0,0.0,1.0,0.0
datadog,This could look for the attribute  @dollarSign:$  and set the attribute  hasDollarSign  to True and set it to false otherwise.,0.0,0.128,0.872,0.4215
datadog,Step 3:,0.0,0.0,1.0,0.0
datadog,Create a facet on  dollarSign  attribute.,0.0,0.344,0.656,0.2732
datadog,Following logs can then be searched for.,0.0,0.0,1.0,0.0
datadog,For logs with no  $,0.423,0.0,0.577,-0.296
datadog,For logs with  $,0.0,0.0,1.0,0.0
datadog,You can do the same with the  hasDollarSign  attribute and set it as a facet.,0.0,0.0,1.0,0.0
datadog,I've tried with this query which is similar to yours:,0.0,0.0,1.0,0.0
datadog,And this seems to give me a count by  dbinstanceidentifier  results.,0.0,0.0,1.0,0.0
datadog,Do you have more information to provide?,0.0,0.0,1.0,0.0
datadog,Maybe an event list and a monitor result screenshot?,0.0,0.0,1.0,0.0
datadog,Side note : I also use this for Kafka (just as a reference) but it should not be required in your case:,0.0,0.0,1.0,0.0
datadog,The main issue is that you would have to mount the volume in both the app container and the agent container in order to make it available.,0.0,0.0,1.0,0.0
datadog,It also means you have to find a place to store the log file before it gets picked up by the agent.,0.0,0.0,1.0,0.0
datadog,Doing this for every container could become difficult to maintain and time consuming.,0.172,0.0,0.828,-0.3612
datadog,An alternative approach would be to instead send the logs to  stdout  and let the agent collect them with the Docker integration.,0.0,0.0,1.0,0.0
datadog,"Since you configured  logsConfigContainerCollectAll  to  true , the agent is already configured to collect the logs from every container output, so configuring Winston to output to  stdout  should just work.",0.0,0.091,0.909,0.4215
datadog,See:  https://docs.datadoghq.com/agent/docker/log/,0.0,0.0,1.0,0.0
datadog,"To support rochdev comment, here are a few code snippets to help out (if you do not opt in for the STDOUT method which should be simpler).",0.0,0.184,0.816,0.6597
datadog,This is only to mount the right volume inside the container agent.,0.0,0.0,1.0,0.0
datadog,"On your app deployment, add:",0.0,0.0,1.0,0.0
datadog,And on your agent daemonset:,0.0,0.0,1.0,0.0
datadog,"At first glance, this seems to be a use case more suitable for the APM part of Datadog which would measure the execution time and could be  instrumented  to measure the execution time of the smaller functions (if it does not picked up this data automatically).",0.0,0.0,1.0,0.0
datadog,You can then create some nice charts with  Trace Search and Analytics .,0.0,0.329,0.671,0.5994
datadog,You could also use a custom metric with tags such as  opsize:large  and  opsize:small  which would represent the execution time (a gauge).,0.0,0.0,1.0,0.0
datadog,You can find more details  here .,0.0,0.0,1.0,0.0
datadog,"At the moment, the log module of Datadog does not seem to support the calculation you expect to see.",0.111,0.0,0.889,-0.3089
datadog,"However, the two solutions above and the related logs can be made visible in a dashboard side by side.",0.0,0.091,0.909,0.1779
datadog,Have you tried using the  start  and  end  tags with a 24 hour window?,0.0,0.0,1.0,0.0
datadog,"As I posted the issue on GitHub, they give an answer the issue was in source code for dd-trace-php and they will fix and new release.",0.0,0.0,1.0,0.0
datadog,https://github.com/DataDog/dd-trace-php/issues/334,0.0,0.0,1.0,0.0
datadog,Below response of DatDog in github:,0.0,0.0,1.0,0.0
datadog,"Ah now this is much more clear, thanks for sharing.",0.0,0.559,0.441,0.8347
datadog,This is a known problem that we are currently and actively working on.,0.18,0.153,0.667,-0.1027
datadog,"As I cannot commit to that, the fix will be probably come out with the next release.",0.112,0.0,0.888,-0.2235
datadog,"At an higher level, the cause is an issue we have in some specific cases while invoking private/protected methods and parent::* invocations.",0.0,0.0,1.0,0.0
datadog,"In the meantime, if you are still interested in testing/using the other integrations, the only thing I can recommend is to disable the pdo integration:  fastcgi_param DD_INTEGRATIONS_DISABLED pdo .",0.0,0.172,0.828,0.6369
datadog,"Again, the fix to this is currently in development and will be released very soon.",0.0,0.0,1.0,0.0
datadog,Use the Query Value widget.,0.0,0.375,0.625,0.34
datadog,"It can only show a single value, which is the average for the current time window that has been chosen.",0.0,0.118,0.882,0.34
datadog,Maybe you could mod the process check to also tag the process number metric by PID ( this is probly where you'd change that ).,0.0,0.053,0.947,0.0772
datadog,That way you could group your monitor by your pid tag and the no-data alerts would tell you when the pid switched.,0.0,0.0,1.0,0.0
datadog,"But this would also alert on expected pid changes, so maybe you'd have to schedule downtimes too aggressively for this to be a good idea?",0.096,0.217,0.686,0.5719
datadog,Maybe monitoring some crash logs with  their Log Management tool  would be a better approach?,0.153,0.165,0.682,0.0516
datadog,Lambda is serverless.,0.0,0.0,1.0,0.0
datadog,Datadog agent is for the host.,0.0,0.0,1.0,0.0
datadog,While running lambda you have absolutely no control over the host as you are not managing it.,0.135,0.0,0.865,-0.3597
datadog,"Hence, You can monitor application running on lambda using datadog integration of lambda for the different application.",0.0,0.0,1.0,0.0
datadog,You may follow below link for AWS Integration of datadog.,0.0,0.0,1.0,0.0
datadog,Ref:  https://docs.datadoghq.com/integrations/amazon_lambda/,0.0,0.0,1.0,0.0
datadog,You can monitor a database from a different host as long as the host the agent is running on has access.,0.0,0.0,1.0,0.0
datadog,So for this section in the config file:,0.0,0.0,1.0,0.0
datadog,https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/data/conf.yaml.example#L4,0.0,0.0,1.0,0.0
datadog,instead of using  localhost  you can set the IP.,0.0,0.0,1.0,0.0
datadog,This will allow you to monitor your database without adding a new agent.,0.0,0.147,0.853,0.2263
datadog,Then you would just follow the normal  postrgres setup  and you will have access to all the metrics and  service checks  including  postgres.can_connect  which is probably what you care about.,0.0,0.099,0.901,0.4939
datadog,"It seems to be an intentional ""feature""  https://github.com/kamon-io/kamon-datadog/issues/19  introduced in 1.x.",0.0,0.0,1.0,0.0
datadog,They have chosen approach to put service name in tag instead.,0.0,0.0,1.0,0.0
datadog,"Even though datadog is being run from the same machine, it is setting up a separate server on your machine.",0.0,0.0,1.0,0.0
datadog,"Because of that, it sounds like the datadog agent doesn't have access to your z:/ driver.",0.0,0.143,0.857,0.3612
datadog,"Try to put the ""TaskResults"" folder in your root directory (when running from datadog - where the mycheck.yaml file is) and change the path accordingly.",0.0,0.0,1.0,0.0
datadog,"If this works and you still want to have a common drive to be able to share files from your computer to datadog's agent, you have to find a way to mount a drive\folder to the agent.",0.0,0.099,0.901,0.3612
datadog,They probably have a way to do that in the  documentation,0.0,0.0,1.0,0.0
datadog,The solution to this is to create a file share on the network drive and use that path instead of the full network drive path.,0.0,0.239,0.761,0.6808
datadog,May be obvious to some but it didn't occur to me right away since the normal Python code worked without any issue outside of Datadog.,0.0,0.0,1.0,0.0
datadog,So instead of:,0.0,0.0,1.0,0.0
datadog,use,0.0,0.0,1.0,0.0
datadog,Have a look at the documentation for  Dogstream .,0.0,0.0,1.0,0.0
datadog,It allows you to send metrics to datadog from log files (including summarised metrics).,0.0,0.0,1.0,0.0
datadog,You may need to write a custom parser for any data that is not in the datadog canonical format in order for datadog to recognize the data.,0.0,0.0,1.0,0.0
datadog,Checkout the example  here .,0.0,0.0,1.0,0.0
datadog,This sounds like a bug.,0.0,0.455,0.545,0.3612
datadog,It is possible the Datadog exporter is running in a non-daemon thread.,0.0,0.0,1.0,0.0
datadog,The JVM views non-daemon threads as application critical work.,0.223,0.0,0.777,-0.3182
datadog,So essentially the JVM thinks it shouldn't shutdown until the non-daemon thread finishes.,0.0,0.0,1.0,0.0
datadog,"In the case of the Datadog exporter thread, that probably won't happen.",0.0,0.0,1.0,0.0
datadog,"To verify there are non-daemon threads, use  jstack  to generate a thread dump.",0.191,0.0,0.809,-0.3818
datadog,(command:  jstack &lt;pid&gt; ) or dump all threads in your  close  method:,0.206,0.0,0.794,-0.3818
datadog,An example thread dump output is below.,0.302,0.0,0.698,-0.3818
datadog,Notice the word 'daemon' on the first line:,0.0,0.0,1.0,0.0
datadog,Gauge metric  types will do the job here given that your query does not run more than once within 10 seconds.,0.0,0.0,1.0,0.0
datadog,"If that is not the case, go for  count metric",0.0,0.0,1.0,0.0
datadog,"The flush interval in datadog by default is 10 seconds, if you use a  gauge metric  and the metric is reported more than once in a flush interval, datadog agent only sends the last value ignoring the previous ones.",0.067,0.06,0.873,-0.0772
datadog,"For  count metric  in contrast, the agent sums up all the values reported in the flush interval.",0.0,0.144,0.856,0.4019
datadog,More details about flush interval  here .,0.0,0.0,1.0,0.0
datadog,The best metric type would be a  histogram  metric.,0.0,0.375,0.625,0.6369
datadog,"This will take multiple values, and pre-aggregate them within a flush window, so you will be able to get things like min/max/sum/avg and various percentiles.",0.0,0.191,0.809,0.6369
datadog,If you run multiple times within a flush window:,0.0,0.0,1.0,0.0
datadog,"As I have mentioned in the comment, you are affected by  two pass model .",0.118,0.0,0.882,-0.1531
datadog,You should remove the keys in the resource added to the end of the chef run or triggered by the DD cookbook resources invoked as the last one in the run.,0.0,0.0,1.0,0.0
datadog,"However, it may not work with all versions of DD cookbook.",0.0,0.0,1.0,0.0
datadog,"From few DD cookbook versions, it is possible to store keys in node's run state which is not written to the Chef server.",0.0,0.0,1.0,0.0
datadog,The above example is preferred solution to your issue.,0.0,0.223,0.777,0.3182
datadog,I noticed that the API key of Datadog changes everytime whenever in close the site and open a new instance.,0.0,0.0,1.0,0.0
datadog,"so after entering new API key , the issue was solved",0.0,0.189,0.811,0.2732
datadog,Two approaches that may work:,0.0,0.0,1.0,0.0
datadog,"It looks like flink has an  HTTP connector  to send metrics to Datadog, which at first glance looks to send over the Datadog metrics API instead of dogstatsd.",0.0,0.085,0.915,0.3612
datadog,"Dogstatsd is not very different from statsd otherwise, so it's often easy to modify statsd libraries to work for dogstatsd.",0.0,0.143,0.857,0.4877
datadog,"This project on GitHub  seems to be such a project, and may come in handy.",0.0,0.0,1.0,0.0
datadog,"Well, you  could  use the Datadog API to script the creation of 20 unique dashboards that all share the same content, but with different hosts.",0.0,0.176,0.824,0.4019
datadog,This is the part of the API docs that would help (with examples!),0.0,0.2,0.8,0.4574
datadog,"for Timeboards , and this one  for Screenboards .",0.0,0.0,1.0,0.0
datadog,"That said, I'd personally find 20 dashboards a bit cluttered / unwieldy in my own Datadog account.",0.0,0.0,1.0,0.0
datadog,"Instead, if it was me, I'd try to (A) find clever uses of dashboard template variables (on e.g, cluster tags, host tags, etc.",0.0,0.12,0.88,0.4588
datadog,"), or (B) group out by each host tag and apply  the ""top()"" function  in some way so that I'd be able to see just the most extreme-value hosts.",0.0,0.0,1.0,0.0
datadog,But that's certainly up to you :),0.0,0.587,0.413,0.7964
datadog,Your answer appears to be there in the text -- you're missing a Python package.,0.145,0.0,0.855,-0.296
datadog,"Try running  sudo pip install psutil , then restarting the agent.",0.0,0.0,1.0,0.0
datadog,"Can you add your agent version, OS and version, and how you installed the agent to your text as well?",0.0,0.1,0.9,0.2732
datadog,It looks like you're also using a  very  old version of the agent (it's up to 5.17.,0.0,0.143,0.857,0.3612
datadog,* for a number of OS's) so there may be better package bundling or critical updates since v. 4.4.0.,0.112,0.205,0.683,0.2263
datadog,Try installing a newer version as well.,0.0,0.296,0.704,0.2732
datadog,Please find the required,0.0,0.434,0.566,0.3182
datadog,In the code you posted:,0.0,0.0,1.0,0.0
datadog,This seems to be creating an empty client object.,0.164,0.2,0.636,0.1027
datadog,"Shouldn't you be creating a client with your keys using  datadog.NewClient(""..."", ""..."")  as in the first code snippet you posted?",0.095,0.0,0.905,-0.2235
datadog,"Also, you should check the error returned as that will give you more hints to troubleshoot the issue:",0.13,0.099,0.771,-0.1621
datadog,`,0.0,0.0,0.0,0.0
datadog,The solution:,0.0,0.697,0.303,0.3182
datadog,docker container 0,0.0,0.0,1.0,0.0
datadog,"#!/bin/sh 
    java -Djava.util.logging.config.file=logging.properties -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=9998 -Dcom.sun.management.jmxremote.port=9998 -Djava.rmi.server.hostname=$HOSTNAME -Dcom.sun.management.jmxremote.host=$HOSTNAME -Dcom.sun.management.jmxremote.local.only=false -jar /app/my-streams.jar",0.0,0.0,1.0,0.0
datadog,docker container 1,0.0,0.0,1.0,0.0
datadog,way more stuff was done that is available from from stack overflow posts.,0.0,0.0,1.0,0.0
datadog,but the above fixes the metrics finding error from datadog-agent.,0.283,0.0,0.717,-0.5499
datadog,Here is how to run each component:,0.0,0.0,1.0,0.0
datadog,"docker container 0 
* my-streams 
* spin up dependent services in tab 
** mvn clean package docker:build 
** docker-compose up",0.0,0.144,0.856,0.4019
datadog,"docker container 1 
* docker build -t dd-agent-my-streams .",0.0,0.0,1.0,0.0
datadog,* docker run -v /var/run/docker.sock:/var/run/docker.sock:ro   -v /proc/:/host/proc/:ro   -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro -e LOG_LEVEL=DEBUG -e SD_BACKEND=docker --network=mystreams_default,0.0,0.0,1.0,0.0
datadog,"ssh into docker container 1 to verify if metrics work 
* docker ps // to find the name of the container to log into 
* docker exec -it  /bin/bash 
root@904e6561cc97:/# service datadog-agent configcheck 
root@904e6561cc97:/# service datadog-agent jmx list_everything 
root@904e6561cc97:/# service datadog-agent jmx collect",0.0,0.0,1.0,0.0
datadog,I think what you actually want is the metrics-query API endpoint?,0.0,0.126,0.874,0.0772
datadog,http://docs.datadoghq.com/api/#metrics-query,0.0,0.0,1.0,0.0
datadog,There are also a few Node.JS libraries that may be able to handle this kind of metric querying for you:  http://docs.datadoghq.com/libraries/#community-node,0.0,0.0,1.0,0.0
datadog,The recommendation of:,0.0,0.0,1.0,0.0
datadog,"""Please don't include endlessly growing tags in your metrics, like timestamps or user ids.",0.088,0.277,0.635,0.5076
datadog,"Please limit each metric to 1000 tags.""",0.0,0.277,0.723,0.3182
datadog,Is more of a warning against using infinitely expanding values as they can drastically increase your custom metric usage.,0.118,0.22,0.661,0.323
datadog,As mentioned in the following article:,0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-,0.0,0.0,1.0,0.0
datadog,"""By default customers are allotted 100 custom metrics per host across their entire infrastructure rather than on a per-host basis.",0.0,0.0,1.0,0.0
datadog,"For example if you were licensed for 3 hosts, you would have 300 custom metrics by default - these 300 metrics may be divided equally amongst each individual host, or all 300 metrics could be sent from a single host.""",0.0,0.0,1.0,0.0
datadog,You will want to keep in mind when configuring your metrics/tags of your current allotment of custom metrics and any billing implications that may have.,0.0,0.051,0.949,0.0772
datadog,"That said, if having these tags is important to your team please reach out to support@datadoghq.com and we can sync up with the Sales Team to determine what is best for your team and use case.",0.0,0.227,0.773,0.8126
datadog,"There is a preview feature that allows you to graph your SNAT port usage and allocation, see:",0.0,0.0,1.0,0.0
datadog,https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics#how-do-i-check-my-snat-port-usage-and-allocation,0.0,0.0,1.0,0.0
datadog,You can look in to other datadog kube metrics like kubernetes.replicas.available / total to alert if no of available - total &lt; 0.,0.088,0.189,0.723,0.3612
datadog,Same can be done or for daemonset pods also there is a specific metric exposed.,0.091,0.0,0.909,-0.0772
datadog,"[Datadog docs-kube metrics][1]
[1]: https://docs.datadoghq.com/agent/kubernetes/data_collected/",0.0,0.0,1.0,0.0
datadog,"An issue with IE11 is fixed in v1.26.1
See the fix here:  [RUMF-791] prevent IE11 performance entry error #633",0.13,0.053,0.817,-0.3818
datadog,Datadog's Ruby library keeps this info on the struct  Datadog.tracer.active_correlation .,0.0,0.0,1.0,0.0
datadog,You can call  Datadog.tracer.active_correlation.trace_id  to grab the trace ID.,0.0,0.0,1.0,0.0
datadog,"You probably want to be using the MySQL integration, and configure the 'custom queries' option:  https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries",0.0,0.08,0.92,0.0772
datadog,You can follow those instructions after you configure the base integration  https://docs.datadoghq.com/integrations/mysql/#pagetitle  (This will give you a lot of use metrics in addition to the custom queries you want to run),0.0,0.043,0.957,0.0772
datadog,"As you mentioned, DogStatsD is a library you can import to whatever script or application in order to submit metrics.",0.0,0.0,1.0,0.0
datadog,But it really isn't a common practice in the slightest to modify the underlying code of your database.,0.0,0.0,1.0,0.0
datadog,"So instead it makes more sense to externally run a query on the database, take those results, and send them to datadog.",0.0,0.0,1.0,0.0
datadog,You could totally write a python script or something to do this.,0.0,0.0,1.0,0.0
datadog,"However the Datadog agent already has this capability built in, so it's probably easier to just use that.",0.0,0.153,0.847,0.4703
datadog,"I am also just assuming SQL refers to MySQL, there are other integration for things like SQL Server, and PostgreSQL, and pretty much every implementation of sql.",0.0,0.192,0.808,0.6908
datadog,"And the same pattern applies where you would configure the integration, and then add an extra line to the config file where you have the check run your queries.",0.0,0.0,1.0,0.0
datadog,"There is  dogapi  which wraps the entire Datadog API and should be able to do the above use case, probably using a mix of  metric.query ,  infrastructure.search ,  search.query  and  monitor.getAll .",0.0,0.0,1.0,0.0
datadog,"For example, to get the list of monitors, it would look something like this:",0.0,0.161,0.839,0.3612
datadog,Please keep in mind that I didn't test the above code.,0.0,0.204,0.796,0.3182
datadog,"If you need something that is not in the library, it should also be fairly easy to wrap the API directly since it's a simple HTTP endpoint.",0.0,0.104,0.896,0.4404
datadog,I hope this helps!,0.0,0.853,0.147,0.6996
datadog,"For each web application that you want to configure with a different  Datadog APM service name , you need to set the environment variable  DD_SERVICE_NAME .",0.0,0.056,0.944,0.0772
datadog,"If they're all running under the same IIS process, that's not possible.",0.0,0.0,1.0,0.0
datadog,"In IIS there's a feature named  Application Pool , which can be used to isolate multiple web applications by running them under different processes.",0.079,0.0,0.921,-0.2023
datadog,The first thing you need to do is to create a separate application pool for each web application.,0.0,0.116,0.884,0.2732
datadog,"Once you're done with that, you can set a different  DD_SERVICE_NAME  for each application pool.",0.0,0.0,1.0,0.0
datadog,The  command  to set an environment variable scoped to a specific application pool is,0.0,0.0,1.0,0.0
datadog,"where  MyAppPool  is the name of the application pool, and  my-service  is the service name that you want to use for the Datadog APM.",0.0,0.053,0.947,0.0772
datadog,"After running the above command, you have to restart IIS for the changes to take effect:",0.0,0.0,1.0,0.0
datadog,"Starting with version 1.0 of Datadog's .NET Tracer, you can set most settings in your application's  app.config / web.config  file.",0.0,0.0,1.0,0.0
datadog,"For example, to set  DD_SERVICE_NAME :",0.0,0.0,1.0,0.0
datadog,[Disclaimer: I am a Datadog employee],0.0,0.0,1.0,0.0
datadog,Maybe you could use the Datadog respective dashboard apis to update your metric names in all dashboards?,0.0,0.149,0.851,0.4215
datadog,Should theoretically be not too complicated to script out.,0.0,0.0,1.0,0.0
datadog,"https://docs.datadoghq.com/api/?lang=python#screenboards 
 https://docs.datadoghq.com/api/?lang=python#timeboards",0.0,0.0,1.0,0.0
datadog,Use the  requests  library its a lot simpler,0.0,0.0,1.0,0.0
datadog,Generate a request header like this,0.0,0.385,0.615,0.3612
datadog,Send the request like this,0.0,0.385,0.615,0.3612
datadog,"While searching through this  other issue , I found that all that is needed to fix this issue is to specify the API key and the application key within the URL.",0.0,0.0,1.0,0.0
datadog,Consider the following.,0.0,0.0,1.0,0.0
datadog,"Yes, kind of.",0.0,0.574,0.426,0.4019
datadog,"It's possible to show single value on a dashboard (just use ""Query Value"" visualization), but it must be based on some metric reported to Datadog.",0.0,0.134,0.866,0.34
datadog,This is how it looks like:,0.0,0.333,0.667,0.3612
datadog,It only applies to produce requests.,0.0,0.0,1.0,0.0
datadog,First you may need to download the MSI file:,0.0,0.0,1.0,0.0
datadog,The actual powershell command for installation (with extra optional arguments included as arguments):,0.184,0.0,0.816,-0.4019
datadog,It's been a while since i've done this (8 months or so?,0.0,0.298,0.702,0.6428
datadog,"), so it could be outdated, but it used to work :).",0.0,0.0,1.0,0.0
datadog,"Note, if you're running this from a remote provisioning script, you'll probly have to schedule this to be executed not-remotely so that the installation command can be run with heightened permissions, which i believe is required.",0.0,0.0,1.0,0.0
datadog,"And you  may  need to make sure the computer is plugged into the power source (i remember hitting some infuriating issue where that was an arbitrary requirement for Windows scheduled tasks to run, and Windows didn't allow me to configure around that).",0.109,0.05,0.841,-0.4149
datadog,is your  activemq_58.yaml  all in one line like that?,0.0,0.238,0.762,0.3612
datadog,You probably want it to be more like this:,0.0,0.369,0.631,0.4754
datadog,There are a variety of issues here.,0.0,0.0,1.0,0.0
datadog,1.,0.0,0.0,1.0,0.0
datadog,You've misconfigured the scope formats.,0.0,0.0,1.0,0.0
datadog,(metrics.scope.operator),0.0,0.0,1.0,0.0
datadog,"For one the configuration doesn't make sense since you specify ""metrics.scope.operator"" multiple times; only the last config entry is honored.",0.0,0.167,0.833,0.5859
datadog,"Second, and more importantly, you have misunderstood for scope formats are used for.",0.15,0.162,0.688,0.0498
datadog,Scope formats configure which context information (like the ID of the task) is included in the reported metric's name.,0.0,0.0,1.0,0.0
datadog,"By setting it to a constant (""latency"") you've told Flink to not include anything.",0.0,0.0,1.0,0.0
datadog,"As a result, the numRecordsIn metrics for every operator is reported as ""latency.numRecordsIn"".",0.0,0.0,1.0,0.0
datadog,I suggest to just remove your scope configuration.,0.0,0.0,1.0,0.0
datadog,2.,0.0,0.0,1.0,0.0
datadog,You've misconfigured the Datadog Tags,0.0,0.0,1.0,0.0
datadog,I do not understand what you were trying to do with your tags configuration.,0.0,0.0,1.0,0.0
datadog,"The tags configuration option can only be used to provide  global  tags, i.e.",0.0,0.0,1.0,0.0
datadog,"tags that are attached to every single metrics, like ""Flink"".",0.0,0.217,0.783,0.3612
datadog,By  default  every metric that the Datadog reports has tags attached to it for every available scope variable available.,0.0,0.0,1.0,0.0
datadog,"So, if you have an operator name A, then the numRecordsIn metric will be reported with a tag ""operator_name:A"".",0.0,0.0,1.0,0.0
datadog,"Again, I would suggest to just remove your configuration.",0.0,0.0,1.0,0.0
datadog,"For the container agent, you'll want to run  sudo docker exec -it dd-agent /etc/init.d/datadog-agent status  from your unix based box.",0.0,0.064,0.936,0.0772
datadog,"If, however, you are using the alpine image the command is:  docker exec -it dd-agent /opt/datadog-agent/bin/agent status  (different path).",0.0,0.0,1.0,0.0
datadog,More here in this KB from Datadog:  https://help.datadoghq.com/hc/en-us/articles/203764635-Agent-Status-and-Information,0.0,0.0,1.0,0.0
datadog,I had issues with it too.,0.0,0.0,1.0,0.0
datadog,"The agent is most likely sending a value for 100 incorrectly as 10000 instead, as that's the highest I've seen it go.",0.0,0.118,0.882,0.3947
datadog,"To make it usable, ensure the graph type is 'line', click ""Advanced"" on the metric, and make the equation  a / 100 .",0.0,0.12,0.88,0.3818
datadog,"I didn't find any guidance on this either in my search, but this seems to go along with what I've seen logged locally into a Windows box and watching the performance counters locally.",0.0,0.0,1.0,0.0
datadog,Just should set hostname for  org.coursera.metrics.datadog.DatadogReporter.Builder :,0.0,0.0,1.0,0.0
datadog,Set is almost never the right custom metric type to use.,0.0,0.0,1.0,0.0
datadog,It will send a count of the number of unique items per a given tag.,0.0,0.098,0.902,0.0772
datadog,"The underlying items details will be stripped from the metric, meaning that from one time slice to the next, you will have no idea that actual true number of items over time.",0.062,0.116,0.822,0.2263
datadog,For example,0.0,0.0,1.0,0.0
datadog,"Your time series to datadog will report  3 , and then  2 .",0.0,0.0,1.0,0.0
datadog,But because the underlying device info is stripped you have no idea how to combine that 2 and 3 if you to zoom out in time and roll up the numbers to show 1 data point per minute.,0.076,0.0,0.924,-0.4215
datadog,"It could be any number from 3 to 5, but the Datadog backend has no idea.",0.165,0.068,0.767,-0.3919
datadog,(even though we know that across those 30 seconds there were 4 unique values total),0.0,0.172,0.828,0.4019
datadog,"Plus even if it was accurate somehow, you can't create an alert of it or notify anyone, because you won't know which device is having issues if you see a spike of devices in the 60 second bucket.",0.096,0.0,0.904,-0.4023
datadog,So let's go through other metric options.,0.0,0.0,1.0,0.0
datadog,"The only metric types that are ever worth using are usually  distributions  or  gauges , or [counts].",0.0,0.112,0.888,0.2263
datadog,"A gauge metric is just a measurement of the latency at a point in time, it's usually good for things like CPU or Memory of a computer, or temperature in a room.",0.0,0.178,0.822,0.6597
datadog,"Numbers that are impossible to actually collect all dat a points for so you just take measurements every 10 seconds, or every minute, or however often you never to get an idea of the behavior.",0.0,0.0,1.0,0.0
datadog,"A count metric is more exact, it's the number of things that happened.",0.0,0.106,0.894,0.0772
datadog,"Usually good for number of requests to a server, or number of files processed.",0.0,0.355,0.645,0.5423
datadog,"Even something like the amount of bytes flowing through something, although that usually is treated like a gauge by most people.",0.0,0.217,0.783,0.6124
datadog,"Distributions are good for when you want to create a gauge metric, but you need detailed measurements for every single event that happens.",0.0,0.197,0.803,0.3919
datadog,For example a web server is handling hundreds of requests per second and we need to know the latency metrics of that server.,0.0,0.0,1.0,0.0
datadog,It's not possible to send a latency metric for every request as a gauge.,0.0,0.0,1.0,0.0
datadog,Gauges have a built in limit of 1 data point per second (in Datadog).,0.0,0.0,1.0,0.0
datadog,Anything more sent in a 1 second interval gets dropped.,0.0,0.0,1.0,0.0
datadog,"But we need stats for every request, so a distribution will summarize the data, it keep a running count, min, max, average, and optionally several percentiles (p50, p75, p99).",0.0,0.0,1.0,0.0
datadog,I haven't seen many good use cases for metric types outside of those 3.,0.167,0.0,0.833,-0.3412
datadog,"For your scenario, it seems like you would want to be sending a distribution metric for that device interval.",0.0,0.192,0.808,0.4215
datadog,So device 1 sends a value of 10.14 and device 3 sends a value of 2.3 and so on.,0.0,0.291,0.709,0.6517
datadog,Then you can use a  distribution widget  in a dashboard to show the number of devices for each interval bucket.,0.0,0.071,0.929,0.0772
datadog,Of course make sure you tag each metric by the device that is generating the metric.,0.0,0.133,0.867,0.3182
datadog,"I was able to do that by using this api call:  https://docs.datadoghq.com/api/?lang=python#get-a-screenboard 
and then get it as a son file, which can be passed to cloud formation later.",0.0,0.0,1.0,0.0
datadog,Actually what you need to do is create a generic method.,0.0,0.189,0.811,0.2732
datadog,Now when you are hitting the different different end points just call the updateCounter method will will capture your metric with the specific name of your route.,0.0,0.0,1.0,0.0
datadog,"For example you have route like add and subtract
Then call the update counter method with metric name add and subtract.",0.0,0.111,0.889,0.3612
datadog,You can leverage Datadog's Agent to collect metrics via a JMX connection.,0.0,0.0,1.0,0.0
datadog,There is documentation found here:,0.0,0.0,1.0,0.0
datadog,http://docs.datadoghq.com/integrations/java/,0.0,0.0,1.0,0.0
datadog,https://www.datadoghq.com/blog/monitoring-jmx-metrics-with-datadog/,0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/204501525-Custom-JMX-Integration-s-,0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/207525586-View-jmx-data-in-jConsole-and-set-up-your-jmx-yaml-to-collect-them,0.0,0.0,1.0,0.0
datadog,That should help you get setup and collecting the necessary metrics exposed by your JMX port.,0.072,0.15,0.778,0.34
datadog,"That said, if you encounter any issues reach out to support@datadoghq.com and they can assist you.",0.0,0.068,0.932,0.0258
datadog,What you have is a nil pointer dereference.,0.0,0.0,1.0,0.0
datadog,"(Unless you are using package  unsafe , which you probably shouldn't touch, so I'm assuming you're not.)",0.0,0.0,1.0,0.0
datadog,It looks like the  e  argument to  func (c *Client) Event(e *Event) error  is  nil  when called from  github.com/some/path/server/http.go:86 .,0.229,0.11,0.661,-0.4019
datadog,"Thanks to a comment from @twotwotwo, I think I figured this out.",0.0,0.266,0.734,0.4404
datadog,In this line,0.0,0.0,1.0,0.0
datadog,I wrote the following program to demonstrate to myself how different function signatures appear in a stack trace:,0.0,0.0,1.0,0.0
datadog,"When  bam  is called, which acts on  *Y  but has no arguments or return value, the output contains:",0.26,0.127,0.613,-0.5023
datadog,"When  foo  is called, which acts on  *Y  and takes a  *X  as argument, but has no return value, the output contains:",0.177,0.121,0.702,-0.1154
datadog,"When  bar  is called, which acts on  *Y , takes a  *X  as argument, and returns a  *Y , the output contains:",0.128,0.0,0.872,-0.3612
datadog,"When  baz  is called, which acts on  *Y , takes  *X  as argument, and returns an  error  (which is an interface), the output contains:",0.198,0.0,0.802,-0.6369
datadog,Think differently :),0.0,0.6,0.4,0.4588
datadog,Do bind a nginx-server (vhost) on port 10080  in addition  - that server does offer the status location and what you need.,0.0,0.0,1.0,0.0
datadog,Server on 80/443 is also there and ONLY that one is bound/exposed to host ( exposed to the outer world ).,0.064,0.0,0.936,-0.0772
datadog,"Since datadog is part of your docker-network / service network, it can still access 10080 in the internal network, but nobody else from the outer network.",0.0,0.0,1.0,0.0
datadog,"Bulletproof, easy - no strings attached.",0.272,0.358,0.37,0.1779
datadog,Since we are running the service through  docker-compose  and our issue being we don't know the IP of the agent.,0.0,0.0,1.0,0.0
datadog,So the simple solution is to know the IP before starting.,0.0,0.204,0.796,0.3744
datadog,And that means assigning our agent a specific IP,0.0,0.0,1.0,0.0
datadog,Here is a update  docker-compose  to do that,0.0,0.0,1.0,0.0
datadog,Now you can do two possible things,0.0,0.0,1.0,0.0
datadog,You can listen only on  172.25.0.101  which is accessible only container running on agent network.,0.0,0.0,1.0,0.0
datadog,Also you can add  allow 172.25.0.100  to only allow the agent container to be able to access this.,0.0,0.192,0.808,0.4215
datadog,There are two (easier) ways to go about it.,0.0,0.0,1.0,0.0
datadog,"First one is  docker-compose  but since I already have a setup running since 2 years which doesn't use docker-compose, I went for the 2nd way.",0.0,0.0,1.0,0.0
datadog,Second way is  Allow  Directive with a range of IPs.,0.0,0.192,0.808,0.2263
datadog,Eg:,0.0,0.0,1.0,0.0
datadog,"I am not security expert, but mostly  192.168.",0.202,0.0,0.798,-0.1326
datadog,"*  IP range is for local networks, not sure about  172.18.",0.179,0.0,0.821,-0.2411
datadog,*  range though.,0.0,0.0,1.0,0.0
datadog,"To get more idea about this IP range thing and CIDR stuff, refer below links
 http://nginx.org/en/docs/http/ngx_http_access_module.html",0.0,0.0,1.0,0.0
datadog,https://www.ripe.net/about-us/press-centre/understanding-ip-addressing,0.0,0.0,1.0,0.0
datadog,"As the err info said dh key is too small, a larger one might help.",0.0,0.172,0.828,0.4019
datadog,Replace the default dh512.pem file with dh4096.pem,0.0,0.0,1.0,0.0
datadog,"sudo wget ""https://git.openssl.org/gitweb/?p=openssl.git;a=blob_plain;f=apps/dh4096.pem"" -O dh4096.pem",0.0,0.0,1.0,0.0
datadog,Ref:  http://www.alexrhino.net/jekyll/update/2015/07/14/dh-params-test-fail.html,0.0,0.0,1.0,0.0
datadog,This is actually a lot harder than it seems.,0.0,0.0,1.0,0.0
datadog,"Representing big integers in JavaScript can be done using the  BigInt  data type (by suffixing the number with  n ), which is fairly widely supported at this point.",0.0,0.126,0.874,0.3818
datadog,This would make your object look like this:,0.0,0.263,0.737,0.3612
datadog,"The problem presents itself in the JSON serialization, as there is currently no support for the serialization of  BigInt  objects.",0.199,0.11,0.691,-0.296
datadog,"And when it comes to JSON serialization, your options for customization are very limited:",0.144,0.0,0.856,-0.2944
datadog,So the only option that I can find is to (at least partially) implement your own JSON serialization mechanism.,0.0,0.0,1.0,0.0
datadog,"This is a  very  poor man's implementation that calls  toString()  for object properties that are of type  BigInt , and delegates to  JSON.stringify()  otherwise:",0.139,0.0,0.861,-0.5256
datadog,"
 
 const o = {
  ""span_id"": 16956440953342013954n,
  ""trace_id"": 13756071592735822010n
};

const stringify = (o) =&gt; '{'
  + Object.entries(o).reduce((a, [k, v]) =&gt; ([
      ...a, 
      `""${k}"": ${typeof v === 'bigint' ?",0.0,0.0,1.0,0.0
datadog,"v.toString() : JSON.stringify(v)}`
    ])).join(', ')
  + '}';

console.log(stringify(o));",0.0,0.0,1.0,0.0
datadog,"Note that the above will not work correctly in a number of cases, most prominently nested objects and arrays.",0.0,0.071,0.929,0.0772
datadog,"If I were to do this for real-world usage, I would probably base myself on  Douglas Crockford's JSON implementation .",0.0,0.0,1.0,0.0
datadog,It should be sufficient to add an additional case around  this line :,0.0,0.0,1.0,0.0
datadog,You could try to use  clinic  in order to debug and profile the app.,0.0,0.0,1.0,0.0
datadog,pretty good tool for nodeJS.,0.0,0.67,0.33,0.7269
datadog,You could user  node-memwatch  to detect where is memory leak.,0.211,0.0,0.789,-0.34
datadog,"It also might be a known issue, here is the  link  with a similar issue.",0.0,0.0,1.0,0.0
datadog,You are on the right path.,0.0,0.0,1.0,0.0
datadog,The guide I'm about to link to begins by following a similar approach to the one you've taken.,0.0,0.0,1.0,0.0
datadog,"I'll link to the section that talks about monitoring memory in real time, which is available when you  Record allocation timeline  in chrome://inspect",0.0,0.0,1.0,0.0
datadog,https://marmelab.com/blog/2018/04/03/how-to-track-and-fix-memory-leak-with-nodejs.html#watching-memory-allocation-in-real-time,0.0,0.0,1.0,0.0
datadog,"This question is quite old, however, still might be useful for new users of Google Cloud.",0.0,0.162,0.838,0.4404
datadog,In 'Metrics Explorer' in Google Cloud Console there is an option to write a query with MQL (click  Query Editor  button).,0.0,0.0,1.0,0.0
datadog,MQL supports expressions which are described in detail  here .,0.0,0.238,0.762,0.3612
datadog,The simplest example for dividing one metric by another would look like this:,0.0,0.172,0.828,0.3612
datadog,"It is probably caused by a regression between .NET Core 2.2 and .NET Core 3.0
Apparently it will be fixed in version 3.1.7",0.0,0.0,1.0,0.0
datadog,"Just starting the process causes the memory leak on linux, because of a non released handle",0.146,0.0,0.854,-0.34
datadog,Issue has been tracked here  https://github.com/dotnet/runtime/issues/36661,0.0,0.0,1.0,0.0
datadog,The problem is that the advice class will be loaded on the system class loader as a part of the agent whereas the actual application code is loaded on a sub-class loader that is not visible to the system class loader.,0.066,0.0,0.934,-0.4019
datadog,This situation does not change if you load your agent on the boot loader either.,0.0,0.0,1.0,0.0
datadog,"Therefore, the agent cannot load the  HttpServletRequest  class which is part of the uber-jar.",0.0,0.0,1.0,0.0
datadog,This is a typical problem with agents and Byte Buddy has a standard way to circumvent it by using a  Transformer.ForAdvice  instance instead of using the  Advice  class directly.,0.097,0.0,0.903,-0.4019
datadog,Byte Buddy then creates a virtual class loader hierarchy that considers classes represented by both class loaders.,0.0,0.123,0.877,0.2732
datadog,Update : The problem is that you are calling down to your interceptor that is defined in the system class loader where the class in question is not available.,0.091,0.0,0.909,-0.4019
datadog,The annotated code will be inlined but the invoked method will not.,0.0,0.0,1.0,0.0
datadog,"If you copy-pasted the code into the annotated method, the behavior is as you'd expect it.",0.0,0.0,1.0,0.0
datadog,Byte Buddy uses the annotated code as template and reuses a lot of information emitted by javac to guarantee a speedy conversion.,0.0,0.095,0.905,0.25
datadog,"Therefore, the library cannot simply copy the method and should rather feed the entire method body to javac.",0.0,0.0,1.0,0.0
datadog,The reason for this error is because apache is listening to port 80 on IPv4 &amp; IPv6.,0.163,0.0,0.837,-0.481
datadog,This will explicitly tell apache to listen to IPv4.,0.0,0.0,1.0,0.0
datadog,In apache config change:,0.0,0.0,1.0,0.0
datadog,Listen 80,0.0,0.0,1.0,0.0
datadog,to,0.0,0.0,1.0,0.0
datadog,Listen 0.0.0.0:80,0.0,0.0,1.0,0.0
datadog,Make sure the file is being copied in to your docker container and being used in apache.,0.0,0.126,0.874,0.3182
datadog,Or add an extra step in the Dockerfile:,0.0,0.0,1.0,0.0
datadog,&amp;&amp; sed -i 's/^Listen 80$/Listen 0.0.0.0:80/' /etc/apache2/httpd.conf,0.0,0.0,1.0,0.0
datadog,"Containers are about isolation so in container ""localhost"" means inside container  so ddtrace-test cannot find ddagent inside his container.",0.13,0.0,0.87,-0.4019
datadog,You have 2 ways to fix that:,0.0,0.0,1.0,0.0
datadog,"I'm guessing you're talking about ""metrics"" instead of matrix!",0.0,0.0,1.0,0.0
datadog,"On the Producer, you have  kafka.producer:type=producer-metrics,client-id=""{client-id}"" .",0.0,0.0,1.0,0.0
datadog,That metric has 2 interesting attributes:,0.0,0.403,0.597,0.4019
datadog,request-latency-avg: The average request latency in ms,0.0,0.0,1.0,0.0
datadog,request-latency-max: The maximum request latency in ms,0.0,0.0,1.0,0.0
datadog,"On the broker side, there are a few metrics you want to check to investigate your issue:",0.0,0.08,0.92,0.0772
datadog,Request total time: Total time Kafka took to process the request.,0.0,0.0,1.0,0.0
datadog,"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce",0.0,0.0,1.0,0.0
datadog,"In case this is high, you can check the break down metrics:",0.0,0.0,1.0,0.0
datadog,These are all listed in the metrics recommended to monitor list in the Kafka documentation:  http://kafka.apache.org/documentation/#monitoring,0.0,0.107,0.893,0.2023
datadog,Some preliminary Google searching lands me on  https://github.com/kubernetes/kubernetes/pull/42717  by way of  https://github.com/kubernetes/kubernetes/issues/24657 .,0.0,0.0,1.0,0.0
datadog,It looks like the pull request was merged in time to be in Kubernetes 1.7.,0.0,0.152,0.848,0.3612
datadog,This should mean that you can use the Downward API to expose  status.hostIP  as an environment variable ( https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ ) or a file in a volume ( https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/ ).,0.065,0.0,0.935,-0.1531
datadog,Your application would then need to read the environment variable or file to get the value of the actual host IP address.,0.0,0.103,0.897,0.34
datadog,"If you agent is written by yourself, you can open and listen on a Unix domain socket and let the other pod send data through it.",0.0,0.0,1.0,0.0
datadog,"If not, you can write a small data proxy that listens on a Unix socket for data.",0.0,0.0,1.0,0.0
datadog,"On the other end, by sharing a pod with the daemon, you can easily send data to the local container",0.0,0.234,0.766,0.6369
datadog,"I use the exact same setup,  dd-agent  running as a DaemonSet in my kubernetes cluster.",0.0,0.0,1.0,0.0
datadog,"Using the same port mapping you commented  here , you can just send metrics to the hostname of the node an application is running on.",0.0,0.0,1.0,0.0
datadog,You can add the node name to the pods environment using the downward api in your pod spec:,0.0,0.0,1.0,0.0
datadog,"Then, you can just open an UDP connection to  ${NODE_NAME}:8125  to connect to the datadog agent.",0.0,0.0,1.0,0.0
datadog,There are a number of different ways you could handle this:,0.0,0.126,0.874,0.0772
datadog,These are just some of your options.,0.0,0.0,1.0,0.0
datadog,Since the Airflow webserver is just a Flask app you can really expose metrics in whatever way you see fit.,0.088,0.117,0.795,0.1548
datadog,"As I understand, you can monitor running tasks in DAGs using DataDog, refer the integration with Airflow  docs",0.0,0.0,1.0,0.0
datadog,You may refer metrics via DogStatD  docs .,0.0,0.0,1.0,0.0
datadog,"Also, look at this  page  would be useful to understand what to monitor.",0.0,0.195,0.805,0.4404
datadog,"E.g., the metrics as below:",0.0,0.0,1.0,0.0
datadog,HikariCP is a connection pool and JDBC is the API for managing a connection.,0.0,0.0,1.0,0.0
datadog,So it can be thought that Spring thinks about separating connection-pool-manager metrics from connection metrics.,0.0,0.0,1.0,0.0
datadog,You need to set the  aws_ecs_task_definition 's  network_mode  to  awsvpc  if you are defining the  network_configuration  of the service that uses that task definition.,0.0,0.0,1.0,0.0
datadog,This is mentioned in the  documentation for the  network_configuration  parameter of the  aws_ecs_service  resource :,0.0,0.0,1.0,0.0
datadog,"network_configuration  - (Optional) The network configuration for the
  service.",0.0,0.0,1.0,0.0
datadog,"This parameter is required for task definitions that use the
   awsvpc  network mode to receive their own Elastic Network Interface,
  and it is not supported for other network modes.",0.065,0.0,0.935,-0.2411
datadog,In your case you've added the  network_mode  parameter to the  container  definition instead of the  task  definition (a task is a collection of n containers and are grouped together to share some resources).,0.0,0.068,0.932,0.296
datadog,The  container definition schema  doesn't allow for a  network_mode  parameter.,0.172,0.0,0.828,-0.1695
datadog,You can use a conditional with count to override if a resource is to be created.,0.0,0.133,0.867,0.25
datadog,The example below will only create the resource when the variable environment is not = production.,0.0,0.13,0.87,0.2732
datadog,"If Count = 0 then the resource won't be created,",0.199,0.0,0.801,-0.1877
datadog,"Regards,",0.0,0.0,1.0,0.0
datadog,I don't think there is something that does this for you automatically.,0.0,0.0,1.0,0.0
datadog,You have to reset the counter yourself at each reporting interval.,0.0,0.0,1.0,0.0
datadog,Something like this should work:,0.0,0.385,0.615,0.3612
datadog,"Actually, It is quite simple.",0.0,0.0,1.0,0.0
datadog,This is called  Packaging namespace packages .,0.0,0.0,1.0,0.0
datadog,https://packaging.python.org/guides/packaging-namespace-packages/,0.0,0.0,1.0,0.0
datadog,All you need is to separate all packages to sub - packages and after install it with a namespace.,0.0,0.0,1.0,0.0
datadog,2 questions that'll be helpful:,0.0,0.483,0.517,0.4215
datadog,Now some clarification on where I think you're going wrong here:,0.256,0.0,0.744,-0.4767
datadog,I suggest tracing the problematic query to see what cassandra was doing.,0.225,0.0,0.775,-0.4404
datadog,https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tracing_r.html,0.0,0.0,1.0,0.0
datadog,"Open cql shell, type  TRACING ON  and execute your query.",0.0,0.0,1.0,0.0
datadog,"If everything seems fine, there is a chance that this problem happens occasionally, in which case I'd suggest tracing the queries using nodetool settraceprobablilty for some time, until you manage to catch the problem.",0.141,0.099,0.759,-0.3818
datadog,You enable it on each node separately using  nodetool settraceprobability &lt;param&gt;  where param is the probability (between 0 and 1) that the query will get traced.,0.0,0.0,1.0,0.0
datadog,"Careful: this WILL cause increased load, so start with a very low number and go up.",0.128,0.283,0.589,0.2228
datadog,"If this problem is occasional there is a chance that this might be caused by long garbage collections, in which case you need to analyse the GC logs.",0.091,0.067,0.842,-0.1779
datadog,Check how long your GC's are.,0.0,0.0,1.0,0.0
datadog,"edit: just to be clear, if this problem is caused by GC's you will NOT see it with tracing.",0.121,0.117,0.762,-0.0258
datadog,"So first check your GC's, and if its not the problem then move on to tracing.",0.0,0.131,0.869,0.3089
datadog,Here you can find how to add a new webhook to your mandrill account:  https://mandrillapp.com/api/docs/webhooks.php.html#method=add,0.0,0.0,1.0,0.0
datadog,"tha main thing here is this:
 $url = 'http://example/webhook-url'; 
this is your webhook URL what will process the data sent by mandrill and forward the information to Datadog.",0.0,0.0,1.0,0.0
datadog,and this is a description about what mandrill will send to your webhook URL:  http://help.mandrill.com/entries/21738186-Introduction-to-Webhooks,0.0,0.0,1.0,0.0
datadog,a listener for webhooks is nothing else then a website/app which triggers an action if a request comes in.,0.0,0.0,1.0,0.0
datadog,Usually you keep it secret or secure it with (http basic) authentication.,0.0,0.179,0.821,0.34
datadog,E.g.,0.0,0.0,1.0,0.0
datadog,create a website called  http://yourdomain.com/hooklistener.php .,0.0,0.412,0.588,0.2732
datadog,You can then call it with HTTP POST or GET and pass some data like hooklistener.php?event=triggerDataDog or with POST and send data along with the body.,0.0,0.091,0.909,0.3612
datadog,You then run a script or anything you want to process that event.,0.0,0.106,0.894,0.0772
datadog,"A ""listener"" is just any URL that you host where you can receive data that is posted to it.",0.0,0.0,1.0,0.0
datadog,"Keep in mind, since you mentioned Zapier, you can set up a trigger that receives the webhook data - in this case the listener URL is provided by Zapier, and you can then send that data into any application (or even post to another webhook).",0.0,0.0,1.0,0.0
datadog,Using Zapier is nice because it doesn't require you to write the listener code that receives the hook data and does something with it.,0.0,0.109,0.891,0.4215
datadog,I believe that Amazon actually offers a service that would accomplish your goal -  CloudWatch   (pricing) .,0.0,0.29,0.71,0.5423
datadog,I'm going to take your points one by one.,0.0,0.0,1.0,0.0
datadog,"Note that I haven't actually  used  it before, but the documentation is fairly clear.",0.0,0.221,0.779,0.5267
datadog,One server had high CPU for over 5 mins when the alert should be triggered after 1 minute,0.0,0.128,0.872,0.296
datadog,It looks like CloudWatch can be configured to send an alert (which I'll get to) after one minute of a condition being met:,0.0,0.19,0.81,0.5719
datadog,"One can actually set conditions for many other metrics as well - this is what I see on one of my instances, and I think that detailed monitoring (I use free), might have even more:",0.0,0.063,0.937,0.2732
datadog,What else is out there that can do the same job and  will also integrate with Pager Duty?,0.0,0.0,1.0,0.0
datadog,I'm assuming you're talking about  this .,0.0,0.0,1.0,0.0
datadog,It turns out the Pager Duty has a  helpful guide  just for integrating CloudWatch.,0.0,0.189,0.811,0.4215
datadog,How nice!,0.0,0.756,0.244,0.4753
datadog,"Here's the pricing page , as you would probably like to parse it instead of me telling you.",0.0,0.135,0.865,0.3612
datadog,"I'll give a brief overview, though:",0.0,0.0,1.0,0.0
datadog,"You don't want basic monitoring, as it only gives you metrics once per five minutes (which you've indicated is unacceptable.)",0.06,0.0,0.94,-0.0572
datadog,"Instead, you want detailed monitoring (once every minute).",0.0,0.157,0.843,0.0772
datadog,"For an EC2 instance, the price for detailed monitoring is $3.50 per instance  per month .",0.0,0.0,1.0,0.0
datadog,"Additionally, every alarm you make is $0.10 per month.",0.231,0.0,0.769,-0.34
datadog,This is actually very cheap if compared to  CopperEgg's pricing  - $70/mo versus  maybe  $30 per month for 9 instances and copious amounts of alarms.,0.087,0.0,0.913,-0.2732
datadog,"In reality, you'll probably be paying more like $10/mo.",0.0,0.259,0.741,0.4201
datadog,"Pager Duty's tutorial suggests you use SNS, which is another cost.",0.0,0.0,1.0,0.0
datadog,The good thing:  it's dirt cheap .,0.258,0.312,0.43,0.128
datadog,$0.60 per million notifications.,0.0,0.0,1.0,0.0
datadog,"If you ever get above a dollar in a year for SNS, you need to perform some serious reliability improvements on your servers.",0.058,0.102,0.841,0.25
datadog,You're not just limited to Amazon's pre-packaged metrics!,0.0,0.219,0.781,0.2401
datadog,"You can actually send custom metrics (time it took to complete a cronjob, whatever) to Cloudwatch via a PUT request.",0.0,0.0,1.0,0.0
datadog,Quite handy.,0.0,0.0,1.0,0.0
datadog,Submit Custom Metrics generated by your own applications (or by AWS resources not mentioned above) and have them monitored by Amazon CloudWatch.,0.0,0.075,0.925,0.1779
datadog,You can submit these metrics to Amazon CloudWatch via a simple Put API request.,0.0,0.124,0.876,0.1779
datadog,(from  here ),0.0,0.0,1.0,0.0
datadog,"So all in all: CloudWatch is quite cheap, can do 1-minute frequency stats, and will integrate with Pager Duty.",0.0,0.0,1.0,0.0
datadog,In short Server Density is a monitoring tool that will monitor all the relevant server metrics.,0.0,0.0,1.0,0.0
datadog,You can take a look at this page  where it’s all described .,0.0,0.0,1.0,0.0
datadog,One server had high CPU for over 5 mins when the alert should be triggered after 1 minute,0.0,0.128,0.872,0.296
datadog,Server Density’s open source agent collects and posts the data to their server every minute and you can decide yourself when that alert should be triggered.,0.0,0.081,0.919,0.296
datadog,In the alert below you can see that the alert will alert 1 person after 1 minute and then repeatedly alert every 5 minutes.,0.0,0.341,0.659,0.7783
datadog,There is a lot of other metrics that you can alert on too.,0.0,0.167,0.833,0.296
datadog,What else is out there that can do the same job and will also integrate with Pager Duty?,0.0,0.0,1.0,0.0
datadog,Server Density also integrates with PagerDuty.,0.0,0.0,1.0,0.0
datadog,The only thing you need to do is to  generate an api key at PagerDuty  and then provide that in the settings.,0.0,0.0,1.0,0.0
datadog,Just provide the API key in the settings and you can then in check pagerduty as one of the alert recipients.,0.0,0.099,0.901,0.296
datadog,You can find the  pricing page here .,0.0,0.0,1.0,0.0
datadog,I’ll give you a brief overview of it.,0.0,0.0,1.0,0.0
datadog,The pricing starts at $10 for one server plus one web check and then get’s cheaper per server the more servers you add.,0.0,0.0,1.0,0.0
datadog,"Everything will be monitored once every minute and there is no fees added for the amount of alerts added or triggered, even if that is an SMS to your phone number.",0.068,0.04,0.892,-0.2263
datadog,"The cost is slightly more expensive than the Cloudwatch example, but the support is good.",0.0,0.363,0.637,0.8126
datadog,If you used copperegg before they have a  migration tool  too.,0.0,0.0,1.0,0.0
datadog,Server Density allows you to monitor all the things!,0.0,0.0,1.0,0.0
datadog,Then only thing you need to do is to send us custom metrics which you can do with a plugin written by yourself or by someone else.,0.0,0.0,1.0,0.0
datadog,I have to say that the graphs that Server Density provides is somewhat akin to eye candy too.,0.0,0.0,1.0,0.0
datadog,Most other monitoring solutions I’ve seen out there have quite dull dashboards.,0.2,0.131,0.669,-0.2568
datadog,It will do the job for you.,0.0,0.0,1.0,0.0
datadog,"Not as cheap as CloudWatch, but doesn’t lock you in into AWS.",0.0,0.0,1.0,0.0
datadog,It’ll give you 1 minute frequency metrics and integrate with pagerduty + a lot more stuff.,0.0,0.0,1.0,0.0
datadog,Reference,0.0,0.0,1.0,0.0
datadog,https://people.apache.org/~dkulp/camel/eventnotifier-to-log-details-about-all-sent-exchanges.html,0.0,0.0,1.0,0.0
datadog,Update,0.0,0.0,1.0,0.0
datadog,"The  Exchange.CREATED_TIMESTAMP  is no longer stored as exchange property, but you should use the  getCreated  method on Exchange.",0.086,0.0,0.914,-0.1531
datadog,"If you put in ECS Task Definition (sample from json version, but in UI also possible to setup), you should be able to configure container logs:",0.0,0.0,1.0,0.0
datadog,Since your question says  is there a way to inspect inside/after each task completes  - I'm assuming you haven't tried this celery-result-backend stuff.,0.0,0.0,1.0,0.0
datadog,So you could check out this feature which is provided by Celery itself :  Celery-Result-Backend / Task-result-Backend  .,0.0,0.0,1.0,0.0
datadog,It is very useful for storing results of your celery tasks.,0.0,0.242,0.758,0.4927
datadog,Read through this =   https://docs.celeryproject.org/en/stable/userguide/configuration.html#task-result-backend-settings,0.0,0.0,1.0,0.0
datadog,"Once you get an idea of how to setup this result-backend, Search for  result_extended  key (in the same link) to be able to add  queue-names  in your task return values.",0.0,0.085,0.915,0.4019
datadog,Number of options are available - Like you can setup these results to go to any of these :,0.0,0.202,0.798,0.4215
datadog,I have made use of this  Result-Backend  feature with  Elasticsearch  and this how my task results are stored :,0.0,0.0,1.0,0.0
datadog,It is just a matter of adding few configurations in  settings.py  file as per your requirements.,0.0,0.073,0.927,0.0258
datadog,Worked really well for my application.,0.0,0.324,0.676,0.3384
datadog,And I have a weekly cron that clears only  successful results  of tasks - since we don't need the results anymore - and I can see only  failed results   (like the one in image).,0.096,0.148,0.756,0.2023
datadog,These were main keys for my requirement :  task_track_started  and  task_acks_late  along with  result_backend,0.0,0.0,1.0,0.0
datadog,Try  recover  to catch all panics and log them.,0.266,0.0,0.734,-0.4404
datadog,"Without that, it'll write the panic msg to stderr:",0.292,0.0,0.708,-0.5106
datadog,You can override the default  TracingInstrumentation  with your own implementation.,0.0,0.0,1.0,0.0
datadog,It will be picked automatically due to the @ConditionalOnMissingBean annotation in the  GraphQLInstrumentationAutoConfiguration  class.,0.0,0.0,1.0,0.0
datadog,Here is a simple example that adds two custom metrics:  graphql.counter.query.success  and  graphql.counter.query.error :,0.0,0.0,1.0,0.0
datadog,"My application.yaml, just in case:",0.0,0.0,1.0,0.0
datadog,"I'm using spring-boot-starter-parent:2.2.2.RELEASE, graphql-spring-boot-starter:6.0.0",0.0,0.0,1.0,0.0
datadog,I hope it helps.,0.0,0.846,0.154,0.6705
datadog,Avg CPU usage may not give better view.,0.256,0.0,0.744,-0.3412
datadog,Check if max CPU utilization is getting around 100%.,0.0,0.0,1.0,0.0
datadog,"If so, you may need to optimize on ES side.",0.0,0.262,0.738,0.4939
datadog,You should understand how cluster autoscaler works.,0.0,0.0,1.0,0.0
datadog,It is responsible  only  for adding or removing nodes.,0.0,0.223,0.777,0.3182
datadog,It is not responsible for creating or destroying pods.,0.554,0.0,0.446,-0.7543
datadog,So in your case cluster autoscaler is not doing anything because it's useless.,0.189,0.0,0.811,-0.4215
datadog,Even if you add one more node - there will be still a requirement to run DaemonSet pods on nodes where is not enough CPU.,0.0,0.0,1.0,0.0
datadog,That's why it is not adding nodes.,0.0,0.0,1.0,0.0
datadog,What you should do is to manually remove some pods from occupied nodes.,0.0,0.0,1.0,0.0
datadog,Then it will be able to schedule DaemonSet pods.,0.0,0.0,1.0,0.0
datadog,"Alternatively you can reduce CPU requests of Datadog to, for example, 100m or 50m.",0.0,0.0,1.0,0.0
datadog,This should be enough to start those pods.,0.0,0.0,1.0,0.0
datadog,You can add priorityClassName to point to a high priority PriorityClass to your DaemonSet.,0.0,0.0,1.0,0.0
datadog,Kubernetes will then remove other pods in order to run the DaemonSet's pods.,0.0,0.0,1.0,0.0
datadog,"If that results in unschedulable pods, cluster-autoscaler should add a node to schedule them on.",0.0,0.0,1.0,0.0
datadog,"See  the docs  (Most examples based on that) (For some pre-1.14 versions, the apiVersion is likely a beta (1.11-1.13) or alpha version (1.8 - 1.10) instead)",0.0,0.0,1.0,0.0
datadog,Apply it to your workload,0.0,0.0,1.0,0.0
datadog,Here are two ways that work:,0.0,0.0,1.0,0.0
datadog,1.,0.0,0.0,1.0,0.0
datadog,"(Drill down to  .services , remember it as  $services  for later use, get the list of keys, and select the ones such that the corresponding value in  $services  has a  build  key).",0.0,0.076,0.924,0.34
datadog,2.,0.0,0.0,1.0,0.0
datadog,"(Drill down to  .services , convert to a list of  {""key"": ..., ""value"": ...}  objects, select the ones where the  .value  has a  build  key, and return the  .key  for each).",0.0,0.082,0.918,0.34
datadog,"The second is probably more idiomatic jq, but the first provides an interesting way to think about the problem as well.",0.128,0.223,0.649,0.3919
datadog,"Here's a third approach, notable for being oblivious to the upper reaches:",0.0,0.107,0.893,0.0516
datadog,Have you tried to mock the  datalog  module inside your function  test ?,0.203,0.0,0.797,-0.4215
datadog,"As long as your other scripts are not running concurrently with your test, this may work.",0.0,0.0,1.0,0.0
datadog,"That way the mock itself will be set only when the function is called, instead of being set in your script scope.",0.118,0.0,0.882,-0.4215
datadog,You could use  unittest.mock.patch .,0.0,0.0,1.0,0.0
datadog,If you are using pytest you can do the same with the  monkeypatch  fixture.,0.0,0.0,1.0,0.0
datadog,I have used many of solutions you mentioned.,0.0,0.221,0.779,0.1779
datadog,Splunk is good but it becomes really expensive if you have huge amount of data.,0.0,0.274,0.726,0.5994
datadog,You could have always used Cloudwatch Logs but it doesn't give you so much on visual part..,0.0,0.0,1.0,0.0
datadog,"I will recommend ELK (ElasticSearch, Logstash, Kibana) stack.",0.0,0.294,0.706,0.3612
datadog,It is a very standard solution; in which logs are stored in Elastic Search.,0.0,0.177,0.823,0.3774
datadog,Kibana is used for visualization of logs.,0.0,0.0,1.0,0.0
datadog,This works in almost real time.,0.0,0.0,1.0,0.0
datadog,If you have very specific dashboards; then you can always create custom dashboards using some front end technologies like AngularJS etc.,0.0,0.195,0.805,0.5574
datadog,but if visual part is really huge and very flexible then I feel ELK is better.,0.0,0.455,0.545,0.8758
datadog,"ELK (ElasticSearch, Logstash, Kibana) stack is a really good solution for what you are looking for, but in some cases ELK is not going to be able to get some metrics, in this case you have some solutions like create your own  beat  program to get the information or use another program to gather this metrics like Apache NiFi.",0.0,0.225,0.775,0.9199
datadog,"You can use AWS CloudWatch, create a log stream for each of your application or service.",0.0,0.13,0.87,0.2732
datadog,"Define your custom metrics, create a dashboard and alert.",0.0,0.417,0.583,0.5106
datadog,It's not limited to AWS things; you can use CloudWatch log agent for On-premises services or software on your local network.,0.0,0.077,0.923,0.1695
datadog,For more information read the following article by Jeff Barr,0.0,0.0,1.0,0.0
datadog,https://aws.amazon.com/blogs/aws/cloudwatch-log-service/,0.0,0.0,1.0,0.0
datadog,and,0.0,0.0,1.0,0.0
datadog,https://aws.amazon.com/blogs/aws/improvements-to-cloudwatch-logs-dashboards/,0.0,0.0,1.0,0.0
datadog,"FYI: we already monitor a lot of application and service inside and outside of AWS by CloudWatch, and it works like a charm.",0.0,0.301,0.699,0.7739
datadog,It seems normal because  Ansible  does not refer to Python virtual environment in your case:,0.0,0.0,1.0,0.0
datadog,In  virtualenv  non-installed packages are initialized from real system environment.,0.0,0.0,1.0,0.0
datadog,So you can achieve it by setting up  Ansible  within  virtualenv,0.0,0.0,1.0,0.0
datadog,Have a look at this example:,0.0,0.0,1.0,0.0
datadog,After installation of  Ansible  in  virtualenv,0.0,0.0,1.0,0.0
datadog,Ansible  refers to the paths of Python virtual environment:,0.0,0.0,1.0,0.0
datadog,ps: Need to deactivate and activate again the  virtualenv  once to load the  Ansible  from virtual environment after the installation.,0.0,0.0,1.0,0.0
datadog,To get you started: Create a timelion expression:,0.0,0.259,0.741,0.2732
datadog,What you are looking for is achievable using Visual Builder visualization,0.0,0.187,0.813,0.3182
datadog,See  https://www.elastic.co/guide/en/kibana/6.1/time-series-visualizations.html,0.0,0.0,1.0,0.0
datadog,Aggregate  Max  or  Avg  on  system.cpu.total.pct,0.0,0.0,1.0,0.0
datadog,Group By  Terms  By  beat.hostname.keyword,0.0,0.0,1.0,0.0
datadog,The visualization will show CPU usage in % for all hosts sending metrics to your cluster.,0.0,0.0,1.0,0.0
datadog,If you add more hosts those will show up too!,0.0,0.0,1.0,0.0
datadog,This is just to illustrate @ben5556's answer with an image.,0.0,0.0,1.0,0.0
datadog,"NOTE:  The ""Term"" is  beat.hostname .",0.0,0.0,1.0,0.0
datadog,sorry for the delay.,0.643,0.0,0.357,-0.3818
datadog,From your error log I can't see any issues on recovery being thrown but I either don't see any connection attempts.,0.093,0.0,0.907,-0.2144
datadog,I wonder if you have some issues with data in the group replication relay logs...,0.0,0.0,1.0,0.0
datadog,I suggest you open a bug if the problem still persists.,0.252,0.0,0.748,-0.4019
datadog,"As a workaround you can try to reset the applier channel before ""START GROUP_REPLICATION""",0.0,0.0,1.0,0.0
datadog,"RESET SLAVE ALL FOR CHANNEL ""group_replication_applier"";",0.0,0.0,1.0,0.0
datadog,The telegraf/influxdb/grafana stack can monitor space left on disc.,0.0,0.0,1.0,0.0
datadog,Kapacitor can also be added if you want alerts.,0.0,0.14,0.86,0.0772
datadog,"If you want to specify a limit, you have to use a dedicated partition / mount point or a btrfs subvolume with quotas.",0.0,0.202,0.798,0.5106
datadog,"Another option is to make cron job to clean up unused docker images, unused docker volume, and exited docker container.",0.0,0.124,0.876,0.4019
datadog,I use this method myself.,0.0,0.0,1.0,0.0
datadog,A better approach than using DaemonSets to run your application would be to use a Deployment so that you don't tie your application to the number of nodes in your cluster.,0.0,0.135,0.865,0.4939
datadog,You can then deploy the datadog agent image as a DaemonSet with a set  spec.template.spec.affinity  that selects nodes with a pod of your application running.,0.0,0.0,1.0,0.0
datadog,This will make sure you have a datadog agent in every node where your application runs.,0.0,0.141,0.859,0.3182
datadog,Another option is to deploy the datadog agent container in the same pod as your application container.,0.0,0.0,1.0,0.0
datadog,"In this case you can reach the agent through localhost and scale together, but might end up with more than an agent per node, hence my preference for a DaemonSet with an affinity.",0.0,0.033,0.967,0.0129
datadog,"My team ran it as a daemon set for the purposes of collecting node metrics, but only exposed it as a normal cluster IP service for the purposes of programmatically sending it data from other apps in the cluster.",0.039,0.0,0.961,-0.1154
datadog,You don't need to expose it on a node port unless you need to access it from outside the cluster and don't have a service-aware load balancer like an ingress controller.,0.0,0.127,0.873,0.4486
datadog,"(That would be quite a strange use case, so chances are you don't need to expose it on a node port.)",0.096,0.174,0.73,0.1821
datadog,is the name of the default queue.,0.0,0.0,1.0,0.0
datadog,"As you state, it is ""queue:$NAME"" but namespaces (if you use them (please don't)) will also prefix the key.",0.0,0.0,1.0,0.0
datadog,You can not set a name on the instances of docker that manages amazon.,0.0,0.124,0.876,0.1779
datadog,The namespaces it uses are to be able to handle the scaling of the service.,0.0,0.0,1.0,0.0
datadog,"Think that if you write the name and then the service you ask for more than one instance of your application, amazon could not instantiate it on the same node.",0.0,0.055,0.945,0.1779
datadog,I hope the explanation has served.,0.0,0.42,0.58,0.4404
datadog,"No, there is not a way to control the name used for the container in Amazon ECS.",0.123,0.095,0.782,-0.128
datadog,ECS picks a random name designed to avoid conflicts (since names must be unique in Docker; you can't have two containers with the same name) and you can see the code  here .,0.142,0.0,0.858,-0.5859
datadog,"However, ECS does give you a few things that might be able to help you.",0.0,0.172,0.828,0.4019
datadog,"There are automatically-assigned Docker labels for the task ARN, the container name in your task definition, the task definition family, the task definition revision, and the cluster; see  here .",0.0,0.0,1.0,0.0
datadog,"Additionally, you can assign your own custom Docker labels through the task definition.",0.0,0.0,1.0,0.0
datadog,I just had some difficulties to determine what is exactly your second separator.,0.167,0.0,0.833,-0.296
datadog,"you text example shows '·', but when I checked what is just after 'Elberg"" and before '2nd...', I found 4 characters : code 32 (space), code 194 (¬), code 183 (∑), code 32 (space).",0.0,0.0,1.0,0.0
datadog,"In the script bellow, I have used the code 194. it works when I cut/paste your text example into a file.",0.0,0.0,1.0,0.0
datadog,Here is the script :,0.0,0.0,1.0,0.0
datadog,"Note : if the text does not contain ""Job posted by "", then myAuthor is ''.",0.0,0.0,1.0,0.0
datadog,"You had the right idea to use  AppleScript's text item delimiters , but the way you tried to extract the name was giving you trouble.",0.124,0.108,0.768,-0.1154
datadog,"First, though, I'll go through some things you can do to improve your script:",0.0,0.182,0.818,0.4404
datadog,"There's no need to break the file contents into lines; AppleScript can operate on entire paragraphs or more, if desired.",0.097,0.105,0.797,0.046
datadog,Removing these unnecessary steps (and adding new ones to make it work on the entire file) shrinks the script considerably:,0.0,0.0,1.0,0.0
datadog,This right here is what's giving you wrong output:,0.248,0.192,0.56,-0.1779
datadog,This is incorrect.,0.0,0.0,1.0,0.0
datadog,It's not the  last  word you want; that's the last word of the file!,0.0,0.109,0.891,0.1511
datadog,"To extract the poster of the job listing, change it to the following:",0.0,0.0,1.0,0.0
datadog,"Due to AppleScript's weird Unicode handling, for whatever reason the dot (·) that separates the name from the other text is converted to ""¬∑"" when run though the script.",0.057,0.0,0.943,-0.1779
datadog,"So, we look for ""¬"" instead.",0.0,0.0,1.0,0.0
datadog,Some last code fixes:,0.0,0.0,1.0,0.0
datadog,"Some of your variable names use  the_snake_case , while others use  theCamelCase .",0.0,0.0,1.0,0.0
datadog,"It's generally a good idea to use one convention or another, so I fixed that, too.",0.0,0.182,0.818,0.4404
datadog,"I assumed you wanted that dollar sign in the output for whatever reason, so I kept it in.",0.0,0.0,1.0,0.0
datadog,"If you don't want it, just replace  set output to ""$ ""  with  set output to """" .",0.075,0.0,0.925,-0.0572
datadog,"So, your final, working script looks like this:",0.0,0.263,0.737,0.3612
datadog,"I agree it's hard to find, the closest one I can find is this",0.101,0.18,0.719,0.2732
datadog,"Return the number of instances that are set for the given module
  version.",0.0,0.098,0.902,0.0772
datadog,"This is only valid for fixed modules, an error will be raised for automatically-scaled modules.",0.162,0.0,0.838,-0.4019
datadog,Support for automatically-scaled modules may be supported in the future.,0.0,0.385,0.615,0.6124
datadog,https://cloud.google.com/appengine/docs/python/refdocs/google.appengine.api.modules.modules,0.0,0.0,1.0,0.0
datadog,Btw you can also have monitoring from the StackDriver which has metric for total instance,0.0,0.0,1.0,0.0
datadog,You should also be able to use the recently GA'd App Engine Admin API to figure this out.,0.0,0.0,1.0,0.0
datadog,"The nice thing about the admin API is that it's going to work for both standard and flexible:
 https://cloud.google.com/appengine/docs/admin-api/",0.0,0.217,0.783,0.5719
datadog,Here's the endpoint that returns all of the instances for a given service/version:,0.0,0.0,1.0,0.0
datadog,https://cloud.google.com/appengine/docs/admin-api/reference/rest/v1/apps.services.versions.instances/list,0.0,0.0,1.0,0.0
datadog,"Depending on the language you're using, there's usually a nice wrapper in the form of a ""Google API client"" + language library.",0.0,0.135,0.865,0.4215
datadog,Hope this helps!,0.0,0.853,0.147,0.6996
datadog,"If you're trying to collect stats, you might want to use the  Stackdriver Monitoring API  to collect the timeseries values that Google has already aggregated.",0.0,0.148,0.852,0.4588
datadog,"In particular, the list of  App Engine Metrics is here .",0.0,0.0,1.0,0.0
datadog,"For example,  system/instance_count  is the metric indicating the number of instances App Engine is running.",0.0,0.085,0.915,0.0772
datadog,"I hate it when SO questions only end up with partial answers, so here's a complete, working example.",0.198,0.0,0.802,-0.5719
datadog,"If you paste it into your interactive console, it should work for you.",0.0,0.0,1.0,0.0
datadog,(Don't forget to set the  versionsId  to whatever your default app version is.,0.0,0.122,0.878,0.1695
datadog,"If you know how I can get it to use the default version, please post a comment.",0.0,0.141,0.859,0.3182
datadog,"'default', '*', 'any', etc.",0.0,0.0,1.0,0.0
datadog,all no da workie.),0.423,0.0,0.577,-0.296
datadog,Strictly achieved by trial and error:,0.351,0.0,0.649,-0.4019
datadog,The easiest way to proceed is to create a custom check.,0.0,0.38,0.62,0.5994
datadog,You can read up on this here:  http://docs.datadoghq.com/guides/agent_checks/ .,0.0,0.0,1.0,0.0
datadog,"There isn't a way to take a pre-existing Nagios or Sensu plugin and have it work as is with Datadog, but looking at one of the delayed_job plugins on Github, looks like it should be pretty easy to convert to a Datadog check.",0.0,0.236,0.764,0.9081
datadog,"If you have any issues, reach out to support either via email or #datadog on IRC.",0.0,0.213,0.787,0.4215
datadog,This was a issue with deployed DataDog daemonset for me:,0.0,0.0,1.0,0.0
datadog,What I did to resolve:,0.0,0.464,0.536,0.3818
datadog,Check daemonset if it exists or not:,0.0,0.0,1.0,0.0
datadog,Edit the datadog daemonset:,0.0,0.0,1.0,0.0
datadog,"In the opened yaml, add",0.0,0.0,1.0,0.0
datadog,Add this in  env:  tag for all places.,0.0,0.0,1.0,0.0
datadog,For me there were 4 places which are having DD tags in the yaml.,0.0,0.0,1.0,0.0
datadog,Save and close it.,0.0,0.516,0.484,0.4939
datadog,The daemonset will restart.,0.0,0.0,1.0,0.0
datadog,And the application will start getting traced.,0.0,0.0,1.0,0.0
datadog,"If you are using the Helm chart, you can overwrite on the values:",0.0,0.184,0.816,0.4019
datadog,hey!,0.0,0.0,1.0,0.0
datadog,"Sorry in advance if my answer isn't correct because  I'm a complete newby  in kuber and helm and I can't make sure that it will help, but maybe it helps.",0.083,0.165,0.753,0.5602
datadog,"So, the problem, as I can understand, in the resulting  ConfigMap  configuration.",0.229,0.0,0.771,-0.4549
datadog,"From my expirience, I faced the same with the following config:",0.0,0.0,1.0,0.0
datadog,And I could solve it only by surrounding with quotes all the values:,0.0,0.31,0.69,0.5423
datadog,"Per Yuri's suggestion, I found the culprit, and this is how (thanks to Google Support for walking me through this):",0.0,0.13,0.87,0.4019
datadog,"This showed me a graph making it clear just about all of my requests were coming from a credential named  datadog-metrics-collection , a service account I'd set up previously to collect GCP metrics and emit to Datadog.",0.0,0.075,0.925,0.3818
datadog,"Considering the answer posted and question, If we think we do not need Stackdriver monitoring, we can disable stackdriver monitoring API using bellow steps:",0.0,0.0,1.0,0.0
datadog,In addition you can view Stackdriver usage by billing account and also can estimate cost using Stackdriver pricing calculator [a] [b].,0.0,0.0,1.0,0.0
datadog,View Stackdriver usage by billing account:,0.0,0.0,1.0,0.0
datadog,4.Select Group By   SKU.,0.0,0.0,1.0,0.0
datadog,"This menu might be hidden; you can access it by clicking Show 
  Filters.",0.0,0.0,1.0,0.0
datadog,You can also select just one or some of these SKUs if you don't want to group your usage data.,0.06,0.0,0.94,-0.0572
datadog,"Note: If your usage of any of these SKUs is 0, they don't appear in the Group By   SKU pull-down menu.",0.0,0.0,1.0,0.0
datadog,"For example, who use only the Cloud console might never generate API requests, so Monitoring API Requests doesn't appear in the list.",0.0,0.0,1.0,0.0
datadog,Use the Stackdriver pricing calculator [b]:,0.0,0.0,1.0,0.0
datadog,[a]  https://cloud.google.com/stackdriver/estimating-bills#billing-acct-usage,0.0,0.0,1.0,0.0
datadog,[b]  https://cloud.google.com/products/calculator/#tab=google-stackdriver,0.0,0.0,1.0,0.0
datadog,You need to  start  the publishing.,0.0,0.0,1.0,0.0
datadog,Compare with the  LoggingMeterRegistry,0.0,0.0,1.0,0.0
datadog,In your constructor something like:,0.0,0.385,0.615,0.3612
datadog,Why not try?,0.0,0.0,1.0,0.0
datadog,It will return the docker container's hostname.,0.0,0.0,1.0,0.0
datadog,"If you haven't set a hostname explicitly, using something like  docker run -h hostname image command  then it will return the docker host's hostname.",0.0,0.102,0.898,0.3612
datadog,"Alternatively, you could do this using a deployment tool like puppet, ansible, etc.",0.0,0.185,0.815,0.3612
datadog,and template the file when you deploy the container.,0.0,0.0,1.0,0.0
datadog,See  HTTP response status codes,0.0,0.0,1.0,0.0
datadog,The categories are generally:,0.0,0.0,1.0,0.0
datadog,"So 4XX errors are errors, but they indicate the client is likely at fault.",0.397,0.0,0.603,-0.7374
datadog,E.g.,0.0,0.0,1.0,0.0
datadog,The user went to a page or user agent made a request to a page that does not exist.,0.0,0.0,1.0,0.0
datadog,"The server responds with 404 because &quot;Everything on my end is fine, but that page isn't real.&quot;",0.0,0.08,0.92,0.1027
datadog,Is it an error?,0.474,0.0,0.526,-0.4019
datadog,Sure.,0.0,1.0,0.0,0.3182
datadog,Could you potentially identify issues (e.g.,0.0,0.0,1.0,0.0
datadog,"typos in links, missing pages, misspellings, malformed API requests, etc..) by routing these to your logs?",0.128,0.0,0.872,-0.296
datadog,Sure.,0.0,1.0,0.0,0.3182
datadog,Are you obligated to take action on it?,0.0,0.0,1.0,0.0
datadog,Not if you don't want to.,0.196,0.0,0.804,-0.0572
datadog,You're probably best to determine why you feel they are not actionable.,0.0,0.276,0.724,0.6369
datadog,Most likely are actionable.,0.0,0.0,1.0,0.0
datadog,Micrometer uses  MeterFilter s registered with a  MeterRegistry  to modified the meters that are registered.,0.0,0.0,1.0,0.0
datadog,The modifications include the ability to map a meter's ID to something different.,0.0,0.173,0.827,0.3182
datadog,"In Spring Boot, you can use a  MeterRegistryCustomizer  bean to add a  MeterFilter  to a registry.",0.0,0.0,1.0,0.0
datadog,"You can use generics to work with a registry of a specific type, for example  MeterRegistryCustomizer&lt;DatadogMeterRegistry&gt;  for a customizer that is only interested in customizing the Datadog registry.",0.0,0.101,0.899,0.4019
datadog,"Putting this together, you can map the ID of the  http.server.request  meter to  i.want.to.be.different  using the following bean:",0.0,0.0,1.0,0.0
datadog,There are some options you should consider:,0.0,0.0,1.0,0.0
datadog,don't update anything and just stick to Kubernetes 1.15 (not recommended as it is 4 main versions behind the latest one),0.0,0.087,0.913,0.2023
datadog,git clone  your repo and change  apiVersion  to  apps/v1  in all your resources,0.0,0.0,1.0,0.0
datadog,"use  kubectl convert  in order to change the  apiVersion , for example:  kubectl convert -f deployment.yaml --output-version apps/v1",0.0,0.0,1.0,0.0
datadog,It is worth to mention that stuff gets deprecated for a reason and it is strongly not recommended to stick to old ways if they are not supported anymore.,0.113,0.127,0.761,0.1144
datadog,"If you are doing the setup in an organisation, datadog or prometheus is probably the way to go.",0.0,0.0,1.0,0.0
datadog,You can capture other Kafka related metrics as well.,0.0,0.208,0.792,0.2732
datadog,These agents also have integrations with many other tools beside Kafka and will be a good common choice for monitoring.,0.0,0.139,0.861,0.4404
datadog,"If you are just doing it for personal POC type of a project and you just want to  view  the lag, I find CMAK very useful ( https://github.com/yahoo/CMAK ).",0.08,0.15,0.769,0.2716
datadog,"This does  not  have historical data, but provides a good  current  visual state of Kafka cluster including lag.",0.141,0.175,0.683,0.1901
datadog,For cluster wide metrics you can use kafka_exporter ( https://github.com/danielqsj/kafka_exporter ) which exposes some very useful cluster metrics(including consumer lag) and is easy to integrate with prometheus and visualize using grafana.,0.045,0.181,0.774,0.6801
datadog,Burrow is extremely effective and specialised in monitoring consumer lag.Burrow is good at caliberating consumer offset and more importantly validate if the lag is malicious or not.,0.067,0.323,0.61,0.8506
datadog,It has integrations with pagerduty so that the alerts are pushed to the necessary parties.,0.0,0.162,0.838,0.4019
datadog,https://community.cloudera.com/t5/Community-Articles/Monitoring-Kafka-with-Burrow-Part-1/ta-p/245987,0.0,0.0,1.0,0.0
datadog,What burrow has:,0.0,0.0,1.0,0.0
datadog,If you are looking for quick solution you can deploy burrow followed by the burrow front end  https://github.com/GeneralMills/BurrowUI,0.0,0.119,0.881,0.3182
datadog,You could use the Java Admin Kafka API quite easily to expose this over command line or as an HTTP call quite quickly with Spring Boot (could avoid any metrics for each app individually since Admin API could do it for any group).,0.087,0.058,0.855,-0.0953
datadog,See link for an example:  https://gquintana.github.io/2020/01/16/Retrieving-Kafka-lag.html,0.0,0.0,1.0,0.0
datadog,Code taken from example above.,0.0,0.0,1.0,0.0
datadog,Get all groups:,0.0,0.0,1.0,0.0
datadog,Get highest offset in a set of partitions:,0.0,0.0,1.0,0.0
datadog,High level on connecting them together to get lag:,0.231,0.0,0.769,-0.34
datadog,"Use the supplied jconsole.sh script in bin, don't try and build up the classpath by hand.",0.0,0.176,0.824,0.4939
datadog,You also need to use the custom service url.,0.0,0.0,1.0,0.0
datadog,See the docs for details,0.0,0.0,1.0,0.0
datadog,You can run datadog tracing on AWS Elastic Beanstalk with Flask by configure tracing manually as defined  here :,0.0,0.0,1.0,0.0
datadog,"Custom metrics are on the roadmap for the APM agent, but we're still working on the exact schedule.",0.0,0.0,1.0,0.0
datadog,In the meantime you could either use the  JMX config options  of the agent with custom JMX key properties.,0.0,0.0,1.0,0.0
datadog,Or use the Elasticsearch output of Micrometer.,0.0,0.0,1.0,0.0
datadog,Maybe just change the Micrometer output as an interim solution and potentially switch to custom APM metrics once they are available?,0.0,0.103,0.897,0.3182
datadog,"There's also the option to get metrics with  Metricbeat from JMX / Jolokia , but that sounds like an even bigger change and not really a long-term upside.",0.0,0.119,0.881,0.5023
datadog,One straight forward way I can think of in this case is to use the  .env  file for your docker-compose.,0.0,0.095,0.905,0.2263
datadog,docker-compose.yaml  file will look something like this,0.0,0.294,0.706,0.3612
datadog,.env  file for each stack will look something like this,0.0,0.217,0.783,0.3612
datadog,and,0.0,0.0,1.0,0.0
datadog,for different projects.,0.0,0.0,1.0,0.0
datadog,Note:,0.0,0.0,1.0,0.0
datadog,You're making things harder than they have to be.,0.0,0.0,1.0,0.0
datadog,Your app is containerized- use a container system.,0.0,0.0,1.0,0.0
datadog,ECS is  very  easy to get going with.,0.0,0.313,0.687,0.4927
datadog,"It's a json file that defines your deployment- basically analogous to docker-compose (they actually supported compose files at some point, not sure if that feature stayed around).",0.069,0.081,0.849,0.0869
datadog,You can deploy an arbitrary number of services with different container images.,0.0,0.106,0.894,0.0772
datadog,"We like to use a terraform module with the image tag as a parameter, but easy enough to write a shell script or whatever.",0.0,0.228,0.772,0.6808
datadog,"Since you're trying to save money, create a single application load balancer.",0.0,0.371,0.629,0.6486
datadog,"each app gets a hostname, and each container gets a subpath.",0.0,0.0,1.0,0.0
datadog,"For short lived feature branch deployments, you can even deploy on Fargate and not have an ongoing server cost.",0.0,0.0,1.0,0.0
datadog,It turns out the solution involved capabilities from docker-compose.,0.0,0.223,0.777,0.3182
datadog,In docker docs the concept is called  Multiple Isolated environments on a single host,0.161,0.0,0.839,-0.3182
datadog,to achieve this:,0.0,0.0,1.0,0.0
datadog,I used an .env file with so many env vars.,0.0,0.0,1.0,0.0
datadog,The main one is  CONTAINER_IMAGE_TAG  that defines the git branch ID to identify the stack.,0.0,0.0,1.0,0.0
datadog,"A separate docker-compose-dev file defines ports, image tags, extra metadata that is dev related",0.0,0.0,1.0,0.0
datadog,Finally the use of  --project-name  in the docker-compose command allows to have different stacks.,0.0,0.0,1.0,0.0
datadog,an example docker-compose Bash function that uses the docker-compose command,0.0,0.0,1.0,0.0
datadog,"The separation should be done in the image tags, container names, network names, volume names and project name.",0.0,0.0,1.0,0.0
datadog,The issue is that your library depends on  gcc  to run.,0.0,0.0,1.0,0.0
datadog,"If you are running in a container, you can try two options:",0.0,0.0,1.0,0.0
datadog,"You could also need  musl-dev  package, but you should try without it first.",0.0,0.0,1.0,0.0
datadog,"Since MacOS and most Linux distros come with GCC, I guess you could be using Windows.",0.0,0.0,1.0,0.0
datadog,"In this case, you need to install  MinGW .",0.0,0.0,1.0,0.0
datadog,"I know this is old but I ran into this problem too, About Alexey answer, on windows, you should install MinGW and add the path to win environment.",0.125,0.156,0.719,0.2529
datadog,You should follow  this .,0.0,0.0,1.0,0.0
datadog,"In case MinGW did not work, you can install  this  one which worked perfectly for me on windows.",0.0,0.198,0.802,0.6369
datadog,I had the same error and installing the .NET Framework 4.6.1 SDK ( https://dotnet.microsoft.com/download/visual-studio-sdks ) and restarting the Datadog Agent solved the problem,0.22,0.086,0.694,-0.5106
datadog,Use  context.Request.Path  conditionally if your  routeData  is null.,0.0,0.0,1.0,0.0
datadog,It is the closest I can think of since Identity Server 4 middleware has internal routing logic for the standard OAuth protocol routes.,0.0,0.0,1.0,0.0
datadog,"After a few more days of research, discovered that:",0.0,0.0,1.0,0.0
datadog,"The errors you are getting are coming from the remote computer, that is, the Heroku dyno.",0.138,0.0,0.862,-0.34
datadog,You can't follow the instructions in the warning (to update bundler) as you can't run arbitrary instructions on their servers.,0.112,0.0,0.888,-0.34
datadog,"Heroku only support limited "" carefully curated "" versions of bundler.",0.157,0.347,0.496,0.3182
datadog,"Normally when the bundler versions don't match it just gives a warning, not an error, so you can  potentially  just ignore it.",0.195,0.09,0.715,-0.3903
datadog,Personally I like to eliminate warnings (or supress them if elimination isn't possible) so that when new warnings pop up I am more likely to notice them and deal with them.,0.134,0.076,0.79,-0.2263
datadog,"That being said, I was not able to ""downgrade"" my Gemfile.lock from 2.0.1 to 1.15.2.",0.0,0.0,1.0,0.0
datadog,I had to first delete Gemfile.lock and then recreate it (presumably there are potentially breaking changes across these major versions).,0.0,0.0,1.0,0.0
datadog,I suspect this is the second problem you encountered.,0.45,0.0,0.55,-0.5994
datadog,The best way around these warnings/errors is to match your local version of Bundler to Heroku's carefully curated version.,0.0,0.251,0.749,0.6908
datadog,"That page above links to another page with the currently supported versions: 
 https://devcenter.heroku.com/articles/ruby-support#libraries",0.0,0.161,0.839,0.3182
datadog,As of today that's version 2.0.1 for Gemfile.locks bundled with 2.x and 1.15.2 for everything else.,0.0,0.0,1.0,0.0
datadog,Could be merge conflicts in the Gemfile.lock.,0.302,0.0,0.698,-0.3818
datadog,Try running  bundle install  locally and see if it works before committing and pushing to heroku.,0.0,0.08,0.92,0.0772
datadog,add bash script as userparameters in zabbix-agent.,0.0,0.0,1.0,0.0
datadog,"Since it requires admin permissions, we can not give out UAA clients for the firehose.",0.0,0.0,1.0,0.0
datadog,"However, there are different ways to get metrics in context of a user.",0.0,0.0,1.0,0.0
datadog,CF API,0.0,0.0,1.0,0.0
datadog,"You can obtain basic metrics of a specific app by polling the CF API:
 https://apidocs.cloudfoundry.org/5.0.0/apps/get_detailed_stats_for_a_started_app.html",0.0,0.0,1.0,0.0
datadog,"However, since you have to poll (and for each app), it's not the recommended way.",0.102,0.0,0.898,-0.1511
datadog,Metrics in syslog drain,0.0,0.0,1.0,0.0
datadog,"CF allows devs to forward their logs to syslog drains; in more recent versions, CF also sends metrics to this syslog drain (see  https://docs.cloudfoundry.org/devguide/deploy-apps/streaming-logs.html#container-metrics ).",0.0,0.0,1.0,0.0
datadog,"For example, you could use Swisscom's Elasticsearch service to store these metrics and then analyze it using Kibana.",0.0,0.0,1.0,0.0
datadog,Metrics using loggregator (firehose),0.0,0.0,1.0,0.0
datadog,"The firehose allows streaming logs to clients for two types of roles:
Streaming  all  logs to admins (which requires a UAA client with admin permissions) and streaming  app  logs and metrics to devs with permissions in the app's space.",0.0,0.0,1.0,0.0
datadog,This is also what the  cf logs  command uses.,0.0,0.0,1.0,0.0
datadog,cf top  also works this way  (it enumerates all apps and streams the logs of each app).,0.0,0.101,0.899,0.2023
datadog,"However, you will find out that most open source tools that leverage the firehose only work in admin mode, since they're written for the platform operator.",0.0,0.0,1.0,0.0
datadog,"Of course you also have the possibility to monitor your app by instrumenting it (white box approach), for example by configuring Spring actuator in a Spring boot app or by including an agent of your favourite APM vendor (Dynatrace, AppDynamics, ...)",0.0,0.0,1.0,0.0
datadog,I guess this is the most common approach; we've seen a lot of teams having success by instrumenting their applications.,0.0,0.179,0.821,0.5719
datadog,Especially since advanced monitoring anyway requires you to create your own metrics as the firehose provided cpu/memory metrics are not that powerful in a microservice world.,0.081,0.153,0.766,0.2608
datadog,"However, option 2. would be worth a try as well, especially since the ELK's stack metric support is getting better and better.",0.0,0.439,0.561,0.8885
datadog,How are you spinning up ecs-agent container?,0.0,0.0,1.0,0.0
datadog,What is docker run command?.,0.0,0.0,1.0,0.0
datadog,Did you try like below?.,0.0,0.385,0.615,0.3612
datadog,"Yes, you can pass a script to the instance that will be executed on the first boot (but not thereafter).",0.0,0.13,0.87,0.4019
datadog,It is often referred to as a  User Data script .,0.0,0.0,1.0,0.0
datadog,See:,0.0,0.0,1.0,0.0
datadog,"If you wish to install  after  the instance has started, use the  AWS Systems Manager Run Command .",0.0,0.144,0.856,0.4019
datadog,The simplest way to get access to data in Cloudyn is to configure a report and schedule data to be pushed to a storage account.,0.0,0.0,1.0,0.0
datadog,"From there, you can use standard storage account APIs to access the data.",0.0,0.0,1.0,0.0
datadog,"Instead of using Cloudyn APIs, however, I would recommend using the  Cost Management Query API  for aggregated cost/usage data or  UsageDetails API  for raw usage.",0.0,0.098,0.902,0.3612
datadog,"If you want to secure access to docker socket,  this docker documents  is a good start.",0.0,0.355,0.645,0.6808
datadog,Spoke with Datadog support.,0.0,0.474,0.526,0.4019
datadog,Very helpful but the short answer is that there is currently no option to add additional tags to specify the specific proc_name in the individual  gunicorn.yaml  file.,0.094,0.069,0.838,-0.191
datadog,As a workaround to enable grouping we enabled unique prefixes for each application but the trade-off is that the metrics are no longer sharing the same namespace.,0.092,0.121,0.787,0.2263
datadog,I've submitted a new feature request on the Github project which will hopefully be considered.,0.0,0.172,0.828,0.4019
datadog,Response time is in  time  field.,0.0,0.0,1.0,0.0
datadog,There is an additional metric  latency  which provides time to first byte.,0.0,0.0,1.0,0.0
datadog,See:,0.0,0.0,1.0,0.0
datadog,You might also want to read :,0.0,0.206,0.794,0.0772
datadog,Shared buffers are used for postgres memory cache (at a lower level closer to postgres as compared to OS cache).,0.102,0.111,0.787,0.0516
datadog,Setting it to 7gb means that pg will cache to 7gb of data.,0.0,0.0,1.0,0.0
datadog,So if you are doing a lot of full table scans or (recursive) CTEs that may improve performance.,0.0,0.153,0.847,0.4404
datadog,"Note that  postgres  master process will allocate this entire amount at database startup, which is why you are seeing your OS use 10GB of ram now.",0.0,0.0,1.0,0.0
datadog,work_mem  is memory used for sorts and  each  concurrent sort allocates a bucket of this size.,0.0,0.0,1.0,0.0
datadog,"Therefore this is only bounded by  max_connections  * concurrent sorts, so effectively it is  only  bounded by the sort complexity of your queries, so increasing this poses the most risk to system stability.",0.066,0.104,0.83,0.3288
datadog,"(That is, if you have a single query that the query planner executes with 8 merge sorts, you will use 8* work_mem  every time the query is executed).",0.0,0.0,1.0,0.0
datadog,maintenance_work_mem  is the memory used by  VACUUM  and friends (including  ALTER TABLE ADD FOREIGN KEY !,0.0,0.195,0.805,0.5255
datadog,Increasing this may increase VACUUM speed.,0.0,0.315,0.685,0.3182
datadog,"wal_buffers  has no benefit beyond 16MB, which is the largest WAL chunk the server will write at one time.",0.099,0.135,0.766,0.2023
datadog,This can help with slow write i/o.,0.0,0.31,0.69,0.4019
datadog,See also:  https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server,0.0,0.0,1.0,0.0
datadog,It depends.,0.0,0.0,1.0,0.0
datadog,"Also, your image doesn't display as of my answer.",0.0,0.0,1.0,0.0
datadog,"If your machines are super memory hungry and you are an individual without unlimited income, I think your approach would be fine.",0.0,0.231,0.769,0.6908
datadog,"I would recommend a slightly higher arbitrary percentage to start with, such as 50%, to provide a bit of wiggle room.",0.0,0.128,0.872,0.3612
datadog,Continue to analyze the memory usage and adjust your maximum accordingly.,0.0,0.0,1.0,0.0
datadog,I don't see any reason to set memory usage below default.,0.0,0.0,1.0,0.0
datadog,"Otherwise, you can be much more gratuitous and provide 100-200% extra memory, in case your application experiences sudden heavy load.",0.0,0.0,1.0,0.0
datadog,"As you can see  here , Ansible provides role dependecies.",0.0,0.0,1.0,0.0
datadog,You may create in  Datadog.datadog  role new directory named meta with main.yml file.,0.0,0.149,0.851,0.2732
datadog,In  meta/main.yml  write,0.0,0.0,1.0,0.0
datadog,"After that, when you call  Datadog.datadog  role, Ansible will run  role1  automatically before  Datadog.datadog  role.",0.0,0.0,1.0,0.0
datadog,"If you create another role named  Datadog.datadog1  with the same  meta/main.yml  file and call roles  Datadog.datadog  and  Datadog.datadog1 , then Ansible will run  role1  only once, before running Datadogs roles.",0.0,0.07,0.93,0.2732
datadog,"For your data model, I would suggest adding   time   as a clustering column:",0.0,0.0,1.0,0.0
datadog,Use descending order to keep the latest metrics first.,0.0,0.0,1.0,0.0
datadog,You can then query using the LIMIT clause to get the most recent hour:,0.0,0.0,1.0,0.0
datadog,Or day:,0.0,0.0,1.0,0.0
datadog,"Depending upon how long you plan to keep the data, you may want to add a  column for year, month, or days to the table to limit your partition size.",0.0,0.044,0.956,0.0772
datadog,"For example, if you wish to keep data for 3 months,  a  month  column can be added to partition your keys by id and month:",0.0,0.109,0.891,0.4019
datadog,"If you keep data for several years, use year + month or a date value.",0.0,0.167,0.833,0.34
datadog,"Regarding your final question, about separate tables or a single table.",0.0,0.0,1.0,0.0
datadog,"Cassandra supports sparse columns, so you can make multiple inserts in a common table for each metric without updating any data.",0.0,0.116,0.884,0.3612
datadog,"However, it's always faster to write just once per row.",0.0,0.0,1.0,0.0
datadog,You may need separate tables if you have to query for different metrics by an alternative key.,0.0,0.0,1.0,0.0
datadog,"For example, query for disk usage by id and disk name.",0.0,0.0,1.0,0.0
datadog,You'd need a separate table or a materialized view to support that query pattern.,0.0,0.197,0.803,0.4019
datadog,"Finally, your schema defines an  assetid , but this isn't defined in your primary key so with your current schema you can't query using assetid.",0.0,0.0,1.0,0.0
datadog,Try,0.0,0.0,1.0,0.0
datadog,"Finally, found the solution by examining services logs.",0.0,0.247,0.753,0.3182
datadog,w32time service was configured as &quot;manual start&quot; instead of delay-auto or auto.,0.0,0.0,1.0,0.0
datadog,"And though &quot;Set time automatically&quot; was &quot;On&quot;, clock synchronization only happened after starting the service and resyncing.",0.0,0.0,1.0,0.0
datadog,So I've changed the Startup type of the Windows time service from Manual to Automatic (Delayed Start) using the following command:,0.0,0.0,1.0,0.0
datadog,&amp;sc.exe config w32time start= delayed-auto,0.0,0.0,1.0,0.0
datadog,Disabled Time Synchronization (service task) in Task Scheduler.,0.0,0.0,1.0,0.0
datadog,Disable-ScheduledTask -TaskName &quot;SynchronizeTime&quot; -TaskPath &quot;\Microsoft\Windows\Time Synchronization&quot;,0.0,0.0,1.0,0.0
datadog,Time Skews are being fixed automatically and occur less often.,0.0,0.0,1.0,0.0
datadog,Typical work flow will look like this (there are other methods),0.0,0.2,0.8,0.3612
datadog,"This can be achieved in multiple ways, easiest is to create a template with index pattern , alias and mapping.",0.0,0.234,0.766,0.5994
datadog,Example: Any new index created matching the pattern  staff-*  will be assigned with given mapping and attached to alias  staff   and we can query  staff  instead of individual indexes and setup alerts.,0.0,0.061,0.939,0.25
datadog,We can use cwl--aws-containerinsights-eks-cluster-for-test-host to run queries.,0.0,0.0,1.0,0.0
datadog,"Note: If unsure of mapping, we can remove mapping section.",0.182,0.0,0.818,-0.25
datadog,FetchFollower acting like &quot;long poll&quot; request.,0.0,0.333,0.667,0.3612
datadog,It waits till it gets  replica.fetch.min.bytes  data for replication or  replica.fetch.wait.max.ms  timeout (which is by default 500ms).,0.0,0.0,1.0,0.0
datadog,"So it's basically ok, it's just means that most of FetchFollower requests are waiting for data",0.0,0.141,0.859,0.3535
datadog,This is likely related to this  issue in the portmap plugin .,0.0,0.0,1.0,0.0
datadog,"The current working theory is that a conntrack entry is created when the client pod reaches out for the UDP host port, and that entry becomes stale when the server pod is deleted, but it's not deleted, so clients keep hitting it, essentially blackholing the traffic.",0.0,0.057,0.943,0.1531
datadog,You can try removing the conntrack entry with something like  conntrack -D -p udp --dport 8125  on one of the impacted host.,0.0,0.106,0.894,0.3612
datadog,If that solves the issue then that was the root cause of your problem.,0.161,0.125,0.714,-0.1531
datadog,This workaround described in the GitHub issue should mitigate the issue until a fix is merged:,0.0,0.0,1.0,0.0
datadog,You can add an initContainer to the server's pod to run the conntrack command when it starts:,0.0,0.0,1.0,0.0
datadog,Some things I'd consider indicative of the health of the cluster are as follows:,0.0,0.0,1.0,0.0
datadog,Offline/Under Replicated Partitions : This is a good indicator as to whether all the nodes in a cluster are even online.,0.0,0.146,0.854,0.4404
datadog,"If one goes offline, you will almost certainly see some under-replication, and if several are offline, you might even see some offline partitions.",0.176,0.082,0.742,-0.101
datadog,"Active Controller : If this keeps changing, then it means that the cluster is potentially unstable.",0.137,0.148,0.714,0.0516
datadog,"The controller should not change regularly; if it does, then something is wrong with your cluster.",0.171,0.0,0.829,-0.4767
datadog,Bytes In/Out : These show that your cluster is able to send and receive data.,0.0,0.0,1.0,0.0
datadog,"If these are lower than you'd expect, then it might imply that the cluster is undergoing some sort of network issue which would possibly impact the cluster health.",0.075,0.0,0.925,-0.296
datadog,Hope this helps!,0.0,0.853,0.147,0.6996
datadog,.strip()  for removing whitespace characters.,0.0,0.0,1.0,0.0
datadog,".replace('offset=', '')  for removing that string.",0.0,0.0,1.0,0.0
datadog,You should be able to chain them too.,0.0,0.0,1.0,0.0
datadog,How to extract the numeric value appeared after  offset= ?,0.0,0.231,0.769,0.34
datadog,Why i prefer regular expression?,0.0,0.0,1.0,0.0
datadog,"Because even if the string contains other keywords, regular expression will extract the numeric value which appeared after the  offset=  expression.",0.0,0.107,0.893,0.34
datadog,"For example, check for the following cases with my given example.",0.0,0.0,1.0,0.0
datadog,How to remove leading and trailing whitespace characters?,0.0,0.0,1.0,0.0
datadog,"will remove all the leading and trailing whitespace characters such as \n, \r, \t, \f, space.",0.0,0.0,1.0,0.0
datadog,For more flexibility use the following,0.0,0.35,0.65,0.4005
datadog,Reference: see this SO  answer .,0.0,0.0,1.0,0.0
datadog,The straightforward way is:,0.0,0.0,1.0,0.0
datadog,For example:,0.0,0.0,1.0,0.0
datadog,Example:,0.0,0.0,1.0,0.0
datadog,Up to you if you need to convert the output.,0.0,0.0,1.0,0.0
datadog,"Since it looks like  StatsDClient  is an interface of some kind, it would make your testing effort easier to simply inject this dependency into your object.",0.0,0.274,0.726,0.8271
datadog,"Even if you're not using an IoC container like Spring or Guice, you can still somewhat control this simply by passing an instance of it in through the constructor.",0.0,0.082,0.918,0.3612
datadog,This will make your testing simpler since all you realistically need to do is mock the object passed in during test.,0.123,0.0,0.877,-0.4215
datadog,"Right now, the reason it's failing is because you're  new ing up the instance, and Mockito (in this current configuration) isn't equipped to mock the newed instance.",0.108,0.076,0.816,-0.2425
datadog,"In all honesty, this set up will make testing simpler to conduct, and you should only need your client configured in one area.",0.0,0.127,0.873,0.4939
datadog,You are getting things wrong here.,0.383,0.0,0.617,-0.4767
datadog,"You don't use a  mocking  framework to test your ""class under test"".",0.0,0.184,0.816,0.3089
datadog,"You use the mocking framework to create  mocked  objects; which you then pass to your ""class under test"" within a test case.",0.199,0.084,0.717,-0.4404
datadog,"Then your ""code under test"" calls methods on the mocked object; and by controlling returned values (or by verifying what happens to your mock); that is how you write your testcases.",0.068,0.079,0.853,0.1027
datadog,"So, your testcase for a MetricRecorder doesn't mock a MetricRecorder; it should mock the StatsDClient class; and as Makoto suggests; use  dependency  injection to put an object of that class into MetricRecorder.",0.0,0.143,0.857,0.5667
datadog,"Besides: basically writing ""test-able"" code is something that needs to be practiced.",0.0,0.0,1.0,0.0
datadog,I wholeheartedly recommend you to watch these  videos  if you are serious about getting in this business.,0.073,0.14,0.787,0.296
datadog,All of them; really (worth each second!,0.0,0.0,1.0,0.0
datadog,).,0.0,0.0,1.0,0.0
datadog,Kamon was being built for Java 1.7 by default.,0.0,0.0,1.0,0.0
datadog,"Now, it will support 1.6",0.0,0.403,0.597,0.4019
datadog,"As told by @TRW in the comments, using this should do the trick:",0.091,0.0,0.909,-0.0516
datadog,I had to open a ticket asking the Heroku CS team to apply the &quot;pg_monitor&quot; role to my user.,0.0,0.0,1.0,0.0
datadog,They've granted the role and now everything is working fine,0.0,0.322,0.678,0.4215
datadog,Maybe it's a complicated idea but I think you can make your own cache store wrapper that decides which cache store to use if I understand your question correctly.,0.0,0.0,1.0,0.0
datadog,"When calling the cache method, it eventually calls  read_fragment  and  write_fragment  on your controller  https://apidock.com/rails/AbstractController/Caching/Fragments/read_fragment  and those methods call  cache_store.read  and  cache_store.write .",0.0,0.0,1.0,0.0
datadog,"Then you could have a custom cache store class with custom  read  and  write  method that, depending on an option, delegates the read and write to real cache stores.",0.0,0.0,1.0,0.0
datadog,Then you use it like...,0.0,0.0,1.0,0.0
datadog,I'm not sure if that's what you are asking sorry.,0.29,0.0,0.71,-0.3098
datadog,Easy solution is to fetch this library directly and do  add_subdirectory .,0.0,0.366,0.634,0.6369
datadog,But this requires cmake &gt;= 3.11.,0.0,0.0,1.0,0.0
datadog,Create dir  cmake  and file  cmake/cpp-datadogstatsd.cmake,0.0,0.296,0.704,0.2732
datadog,cpp-datadogstatsd.cmake :,0.0,0.0,1.0,0.0
datadog,"Then, include this cmake file, and link  DataDogStatsD_static  to your lib/exe:",0.0,0.0,1.0,0.0
datadog,"As you have limited requirements, you could achieve this without a bot.",0.16,0.0,0.84,-0.2263
datadog,MS Teams has income and outgoing webhooks.,0.0,0.268,0.732,0.296
datadog,You could create a  Incoming webhook  inside a Teams channel.,0.0,0.231,0.769,0.2732
datadog,It provides an URL which you could use inside the monitoring remote server and POST the message in JSON format to the webhook url.,0.0,0.0,1.0,0.0
datadog,It will be posted in teams channel like below,0.0,0.238,0.762,0.3612
datadog,For sending message back to the server you need to configure the  Outgoing webhook  in the channel.,0.0,0.121,0.879,0.296
datadog,Spring Cloud Data Flow and Skipper servers are  Spring Boot  applications and hence you can configure/customize logging system based on your requirements.,0.0,0.0,1.0,0.0
datadog,Here are some of the references to configure logging system for a Spring Boot app:,0.0,0.0,1.0,0.0
datadog,docker logs  and similar just collect the stdout and stderr streams from the main process running inside the container.,0.0,0.0,1.0,0.0
datadog,"There's not a ""log level"" associated with that, though some systems might treat or highlight the two streams differently.",0.0,0.242,0.758,0.6249
datadog,"As a basic example, you could run",0.0,0.0,1.0,0.0
datadog,"The resulting file listing isn't especially ""error"" or ""debug"" level.",0.0,0.0,1.0,0.0
datadog,"The production-oriented setups I'm used to include the log level in log messages (in a Node context, I've used the  Winston  logging library), and then use a tool like  fluentd  to collect and parse those messages.",0.0,0.07,0.93,0.3612
datadog,Make sure  CreatedDate  is indexed.,0.0,0.365,0.635,0.3182
datadog,Make sure  CreatedDate  is using the  date  column type .,0.0,0.223,0.777,0.3182
datadog,"This will be more efficient on storage (just 4 bytes), performance, and you can use all the built in  date formatting  and  functions .",0.0,0.128,0.872,0.4754
datadog,Avoid  select *  and only select the columns you need.,0.216,0.0,0.784,-0.296
datadog,Use  YYYY-MM-DD  ISO 8601 format .,0.0,0.0,1.0,0.0
datadog,"This has nothing to do with performance, but it will avoid a lot of ambiguity.",0.177,0.0,0.823,-0.4215
datadog,The real problem is likely that you have thousands of tables with which you regularly make unions of hundreds of tables.,0.119,0.0,0.881,-0.4019
datadog,This indicates a need to redesign your schema to simplify your queries and get better performance.,0.0,0.172,0.828,0.4404
datadog,Unions and date change checks suggest a lot of redundancy.,0.0,0.0,1.0,0.0
datadog,Perhaps you've partitioned your tables by date.,0.0,0.0,1.0,0.0
datadog,Postgres has its own built in  table partitioning  which might help.,0.0,0.213,0.787,0.4019
datadog,Without more detail that's all I can say.,0.0,0.0,1.0,0.0
datadog,Perhaps ask another question about your schema.,0.0,0.0,1.0,0.0
datadog,"Without seeing  EXPLAIN (ANALYZE, BUFFERS) , all we can do is speculate.",0.0,0.0,1.0,0.0
datadog,But we can do some pretty good speculation.,0.0,0.576,0.424,0.8462
datadog,Cluster the tables on the index on CreatedDate.,0.0,0.0,1.0,0.0
datadog,"This will allow the data to be accessed more sequentially, allowing more read-ahead (but this might not help much for some kinds of storage).",0.086,0.073,0.841,-0.092
datadog,"If the tables have high write load, they may not stay clustered and so you would have recluster them occasionally.",0.0,0.0,1.0,0.0
datadog,"If they are static, this could be a one-time event.",0.0,0.0,1.0,0.0
datadog,Get more RAM.,0.0,0.0,1.0,0.0
datadog,"If you want to perform as if all the data was in memory, then get all the data into memory.",0.0,0.064,0.936,0.0772
datadog,"Get faster storage, like top-notch SSD.",0.0,0.333,0.667,0.3612
datadog,"It isn't as fast as RAM, but much faster than HDD.",0.0,0.0,1.0,0.0
datadog,The answer to the question is found in the comments to it.,0.0,0.0,1.0,0.0
datadog,"Hence, this question should not go unanswered.",0.0,0.0,1.0,0.0
datadog,"The code from the question works as expected, however, the path where the named pipe resides is a special path and this is the reason why the data that is being sent to it never reaches the script.",0.03,0.07,0.901,0.372
datadog,The corresponding special casing in Bash for instance can be found in  redir.c .,0.0,0.184,0.816,0.4019
datadog,The solution to the problem is to use a real UDP server on that port:,0.159,0.135,0.706,-0.1027
datadog,It turns out that someone had turned on a scheduled job that was sending a super expensive query that was supposed to be a singleton onto the query queue every 5 min.,0.0,0.126,0.874,0.5994
datadog,"The query takes 20 min to run, so eventually the system bogs down and falls over.",0.0,0.0,1.0,0.0
datadog,"Apparently this was / is an issue with  rpy2 , which was a dependency of our project.",0.0,0.0,1.0,0.0
datadog,It was being imported by a utility module that was imported on startup.,0.0,0.0,1.0,0.0
datadog,This caused it to be called on every single request to our REST API endpoints.,0.0,0.0,1.0,0.0
datadog,Putting the import inside the actual function that was using it fixed this issue.,0.0,0.0,1.0,0.0
datadog,"What you want is either the  is_match  or  is_exact_match  conditional variable, which are  documented here  (with examples).",0.0,0.075,0.925,0.0772
datadog,"The idea is that you can nest your messages  and notifications  in conditional logic arguments so that only when the monitor alerts/warns/resolves, or only when the evaluated tag scope matches certain conditions, will certain messages or notification channels be part of the alert.",0.056,0.133,0.811,0.4019
datadog,So in your case you want your message to include something like this:,0.0,0.257,0.743,0.4215
datadog,"{{#is_exact_match ""environment.name"" ""prod""}}",0.0,0.0,1.0,0.0
datadog,Add special prod message here,0.0,0.403,0.597,0.4019
datadog,and @pagerduty or @pagerduty-foo,0.0,0.0,1.0,0.0
datadog,{{/is_exact_match}},0.0,0.0,1.0,0.0
datadog,Add message that should always show up here,0.0,0.0,1.0,0.0
datadog,and @slack-bar,0.0,0.0,1.0,0.0
datadog,"In this case, only when the ""environment"" tag's value is ""prod"" will the bracketed content be included (which includes the pagerduty notification).",0.0,0.103,0.897,0.34
datadog,The non-bracketed part will always be included (which includes the slack notification).,0.0,0.0,1.0,0.0
datadog,(?,0.0,0.0,1.0,0.0
datadog,":\/private\/toolbox\/)(.+)  ought to match your route path, capturing the wildcard as the first group:",0.0,0.0,1.0,0.0
datadog,"I cannot speak to that RegExp's performance, however.",0.0,0.0,1.0,0.0
datadog,"I can't speak specifically for the Java implementation, but in the CSharp client, the ability to send this data to Datadog is done to 127.0.0.1 via UDP port 8125.",0.0,0.098,0.902,0.4497
datadog,It's on the same thread as your executing code and not asynchronous.,0.0,0.0,1.0,0.0
datadog,The whole effort by your process is finished once the UDP message is sent - it's fired and immediately forgotten.,0.244,0.0,0.756,-0.6705
datadog,"The thread overhead you mention occurs in the separate Datadog agent process which is listening on the other end of UDP 8125, and has it's own thread pool and ability to buffer some data before sending up to Datadog's servers.",0.0,0.056,0.944,0.3182
datadog,Do you have additional information that shows this behavior?,0.0,0.0,1.0,0.0
datadog,"Based on what I know, this doesn't sound like a side effect of the Datadog/StatsD stuff.",0.14,0.0,0.86,-0.2755
datadog,"I found the answer on Datadog's help forum:  ""How to graph percentiles in Datadog"" .",0.0,0.184,0.816,0.4019
datadog,"So the gist is that the latency itself didn't go up, but aggregating over multiple streams (where each stream corresponds to each custom tag) caused the graph to display a different shape.",0.0,0.0,1.0,0.0
datadog,"Just use  NLog.MappedDiagnosticsLogicalContext.Set(""userid"", ""someValue"")  together with  ${mdlc:item=userid}  where needed.",0.0,0.0,1.0,0.0
datadog,See also  http://github.com/NLog/NLog/wiki/MDLC-Layout-Renderer,0.0,0.0,1.0,0.0
datadog,MappedDiagnosticsLogicalContext  uses  CallContext  (and  AsyncLocal  on NetCore) which are thread-safe.,0.0,0.0,1.0,0.0
datadog,"Settings will also support async Task and follow to the chained tasks, but if scheduling a Time-callback based on a user-request, then the Timer-callback will not see the userid.",0.0,0.066,0.934,0.2144
datadog,"You should avoid changing  LogManager.Configuration.Variable  at runtime, they are global for all concurrent requests, and might get lost during configuration-reload (If autoreload configured).",0.176,0.0,0.824,-0.5423
datadog,"If anyone will need the answer, this is how I did this.",0.0,0.0,1.0,0.0
datadog,"It shows the biggest tables that were not last vacuumed in that past 2 weeks, but limits the list for 20 results.",0.0,0.0,1.0,0.0
datadog,If you want to you can change LIMIT 20 or delete it for shorter/longer list.,0.0,0.085,0.915,0.0772
datadog,Also change pg.last_autovacuum for analyze or anything else from pg.stat table you want to check and also 2 week can be changed to whatever time period you want.,0.0,0.094,0.906,0.1531
datadog,In datadog under postgres.yaml I added this:,0.0,0.0,1.0,0.0
datadog,Then I added this as a top-list inside a dashboard.,0.0,0.0,1.0,0.0
datadog,"You can also make it as an alert and sum up the counts inside the metrics and decide what's your limit that about it you want to start vacuuming, although it's just recommended to run it regularly and not just when it's just too big, that's why we use this only as a list in our dashboard.",0.0,0.091,0.909,0.5106
datadog,To get eyes only.,0.0,0.0,1.0,0.0
datadog,"Datadog, like OMS and other monitoring software uses the Azure VM agent to steam the information.",0.0,0.143,0.857,0.3612
datadog,Once this agent is installed on the system we are able to gather the info needed.,0.0,0.0,1.0,0.0
datadog,The VM agent is not something that goes out over the internet like other connections.,0.0,0.152,0.848,0.3612
datadog,"Hence, you should still see the reporting available.",0.0,0.0,1.0,0.0
datadog,"Rather, it should be a direct connection from the Hyper-V manager and the VM itself.",0.0,0.0,1.0,0.0
datadog,"This therefore, bypassing any NSG rules you would have in place.",0.0,0.0,1.0,0.0
datadog,I have installed data dog agent on one of my virtual machines,0.0,0.0,1.0,0.0
datadog,Datadog agent will collect system metrics and  forward  to Datadog.,0.0,0.0,1.0,0.0
datadog,Datadog agent works like this:,0.0,0.385,0.615,0.3612
datadog,"Also you can try to perform a network capture on your Azure VM, then we are able to find the detailed of the agent behavior.",0.0,0.0,1.0,0.0
datadog,Here is the network capture in my test VM:,0.0,0.0,1.0,0.0
datadog,We can find that  Datadog agent forward over HTTPS(443) to Datadog HQ .,0.0,0.0,1.0,0.0
datadog,"After you deny port 443 in NSG outbound rules, the datadog will not get your metrics:",0.138,0.0,0.862,-0.34
datadog,"More information about datadog agent, please refer to this official  article .",0.0,0.187,0.813,0.3182
datadog,Anything printed to STDOUT will be sent to your logging addons like SumoLogic.,0.0,0.172,0.828,0.3612
datadog,The options you've shown should take care of that.,0.0,0.286,0.714,0.4939
datadog,"The mechanism that addons like SumoLogic use is call  Log Drains , and you can tap into that your self to get your log stream over HTTP.",0.0,0.091,0.909,0.3612
datadog,"hmm, so you're trying to use autodiscovery to find which container the dd-agent should be running the etcd check on?",0.0,0.0,1.0,0.0
datadog,and you're using the auto_conf files approach?,0.0,0.0,1.0,0.0
datadog,"And there, you're wondering how to apply the  %%host%%  template variable?",0.0,0.0,1.0,0.0
datadog,"If that's what you're interested in, I think you'll want to add it into your  etcd.yaml  on the  url  line, as shown in  the example file  like so:",0.0,0.213,0.787,0.6705
datadog,When submitting histograms via dogstatsD you should be automatically creating 5 metrics as shown here:,0.0,0.145,0.855,0.296
datadog,dog.histogram(...),0.0,0.0,1.0,0.0
datadog,Usage: Used to track the statistical distribution of a set of values over a statsd flush period.,0.0,0.162,0.838,0.4019
datadog,Actually submits as multiple metrics:,0.0,0.0,1.0,0.0
datadog,Additional details on metric types and their submission sources can be found here:,0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/206955236-Metric-types-in-Datadog,0.0,0.0,1.0,0.0
datadog,It appears for your use case  metric.count  would be the closest match for calculating the total length of your word.,0.0,0.0,1.0,0.0
datadog,"Once selected, you can make use of the  as_count()  modifier which will calculate the total count rather than the average over the flushing period.",0.0,0.0,1.0,0.0
datadog,More information on this use case can be found here:,0.0,0.0,1.0,0.0
datadog,https://help.datadoghq.com/hc/en-us/articles/204271195-Why-is-a-counter-metric-being-displayed-as-a-decimal-value-,0.0,0.0,1.0,0.0
datadog,If you find yourself still running into any issues with this submission feel free to reach out to support@datadoghq.com,0.0,0.206,0.794,0.5267
datadog,You can modify the following according to your needs.,0.0,0.0,1.0,0.0
datadog,"What this basically does, is that it prevents  print()  from writing the default end character ( end='' ) and at the same time, it write a carriage return ( '\r' ) before anything else.",0.0,0.044,0.956,0.0772
datadog,"In simple terms, you are overwriting the previous  print()  statement.",0.0,0.0,1.0,0.0
datadog,"the naive solution would be to just use the total amount of rows in your dataset and the index your are at, then calculate the progress:",0.07,0.169,0.762,0.4588
datadog,This will only be somewhat reliable if every row takes around the same time to complete.,0.0,0.0,1.0,0.0
datadog,"Because you have a large dataset, it might average out over time, but if some rows take a millisecond, and another takes 10 minutes, the percentage will be garbage.",0.0,0.0,1.0,0.0
datadog,Also consider rounding the percentage to one decimal:,0.0,0.0,1.0,0.0
datadog,Printing for every row might slow your task down significantly so consider this improvement:,0.0,0.228,0.772,0.5899
datadog,"There are, of course, also modules for this:",0.0,0.0,1.0,0.0
datadog,progressbar,0.0,0.0,1.0,0.0
datadog,progress,0.0,1.0,0.0,0.4215
datadog,You could use Datadog's Outlier detection to identify instances which exhibit behavior outside the normal for it's peer set.,0.0,0.0,1.0,0.0
datadog,"As an example, you could create an outlier detection monitor:",0.0,0.189,0.811,0.2732
datadog,http://docs.datadoghq.com/guides/outliers/#alerts,0.0,0.0,1.0,0.0
datadog,Which would be scoped to a system metric like  aws.ec2.cpuutilization  and be alerted if any host spiked abnormally or had very low utilization in comparison to its group.,0.08,0.084,0.836,0.0276
datadog,There are some additional blog posts which discuss the use of the algorithms that can be found here:,0.0,0.0,1.0,0.0
datadog,https://www.datadoghq.com/blog/introducing-outlier-detection-in-datadog/,0.0,0.0,1.0,0.0
datadog,https://www.datadoghq.com/blog/outlier-detection-algorithms-at-datadog/,0.0,0.0,1.0,0.0
datadog,https://www.datadoghq.com/blog/scaling-outlier-algorithms/,0.0,0.0,1.0,0.0
datadog,"That said, if you find yourself needing additional assistance with outlier detection you can always reach out to the Support team at support@datadoghq.com or by using the internal support features found here:",0.0,0.183,0.817,0.6705
datadog,https://app.datadoghq.com/help,0.0,0.0,1.0,0.0
datadog,Hope this helps!,0.0,0.853,0.147,0.6996
datadog,I have found that  Blackfire  is doing the trick.,0.146,0.0,0.854,-0.0516
datadog,Seems to be relatively easy to install and can run it free locally.,0.0,0.36,0.64,0.7351
datadog,"Datadog Tags are generally strings, but also support &quot;key:value&quot; strings, which is most useful, since then the  key  can act as a dimension.",0.0,0.282,0.718,0.8334
datadog,"There's no support that I know of that allows for a single key with multiple values, so I don't think Datadog will support the syntax you're attempting.",0.073,0.267,0.66,0.7096
datadog,You  may  want to try:,0.0,0.245,0.755,0.0772
datadog,in your config.,0.0,0.0,1.0,0.0
datadog,General reference here:  https://docs.datadoghq.com/getting_started/tagging/,0.0,0.0,1.0,0.0
datadog,"You're welcome to go look through the source code yourself , but generally my comment is correct.",0.0,0.118,0.882,0.25
datadog,"Nest binds all route handlers and enhancers (guards, interceptors, pipes, and filters) as a large anonymous function, in a very abstract way (does the same thing for Fastify as far as I can tell).",0.0,0.0,1.0,0.0
datadog,You can use Fluentd as a  daemonset  on your cluster.,0.0,0.0,1.0,0.0
datadog,see this repo and docker images -&gt;  fluent/fluentd-docker-image,0.0,0.0,1.0,0.0
datadog,and use this filter to add  Kubernetes metadata  to every log collected by Fluentd and then use a grep filter to exclude logs that are not in your namespaces.,0.066,0.0,0.934,-0.2263
datadog,something like this:,0.0,0.556,0.444,0.3612
datadog,did u try bulk publish?,0.0,0.0,1.0,0.0
datadog,"publish(topic,[]Message{1,2,3,4,.....})",0.0,0.0,1.0,0.0
datadog,I believe your requirement can be accomplished using cmdlet  Invoke-AzVMRunCommand  /  Invoke-AzureRmVMRunCommand  or  Set-AzVMCustomScriptExtension  /  Set-AzureRmVMCustomScriptExtension .,0.0,0.195,0.805,0.4404
datadog,Related scripts can be found  here  and  here .,0.0,0.0,1.0,0.0
datadog,"Just FYI,  this  and  this  are actual references for the above information.",0.0,0.187,0.813,0.368
datadog,Hope this update helps!,0.0,0.743,0.257,0.6996
datadog,This is possible by adding the annotation below to nginx ingress:,0.0,0.0,1.0,0.0
datadog,See full answer at  https://github.com/DataDog/dd-opentracing-cpp/issues/118,0.0,0.0,1.0,0.0
datadog,"Turn on the ""general log"" and have it write to a file.",0.0,0.0,1.0,0.0
datadog,Wait a finite amount of time.,0.0,0.0,1.0,0.0
datadog,Then use  pt-query-digest  to summarize the results.,0.0,0.0,1.0,0.0
datadog,Turn off the general log before it fills up disk.,0.0,0.0,1.0,0.0
datadog,The slowlog (with a small value in  long_query_time ) is more useful for finding naughty queries.,0.0,0.318,0.682,0.6801
datadog,The issue was the size of the HTTPRequest was to large the higher the parallelism which makes sense.,0.0,0.0,1.0,0.0
datadog,"I was getting back ""Request Entity Too Large"" however the exception wasn't logging out correctly so I missed it.",0.152,0.0,0.848,-0.4341
datadog,It seems that the Flink  DatadogHttpReporter  does not take the size of the request into consideration when building it.,0.0,0.0,1.0,0.0
datadog,I modified the Reporter to limit the number of metrics per request to 1000.,0.0,0.098,0.902,0.0772
datadog,Now the metrics are showing up just fine.,0.0,0.205,0.795,0.2023
datadog,"This becomes pretty easy with Datadog's Log Management product -- you can measure lots of things by endpoint, including hits, unique client-ip count, latency (if you add response time to your nginx logs).",0.0,0.164,0.836,0.7269
datadog,More info on the setup and these use cases  in this blogpost .,0.0,0.0,1.0,0.0
datadog,Documentation on the logs part of the nginx integration  here .,0.0,0.0,1.0,0.0
datadog,"This is definitely possible, but you will want to change your tag setup a little.",0.0,0.216,0.784,0.3182
datadog,"You want to take advantage of  key:value  syntax with your tags, so that you can group out the tags by their common  key .",0.0,0.136,0.864,0.3182
datadog,"So in your case, instead of tagging by  entity.count.payment , you would want to tag by  entity.count:payment  or better yet  entity:payment .",0.0,0.189,0.811,0.4939
datadog,That way you can write one query of your metric and use the  group by  functionality on the shared  entity  tag key to see it's values for all the different  entity  tags.,0.0,0.145,0.855,0.6249
datadog,"From there, you can use the  top  function to always see just the top n values, whether that be  payment  or  cart  or  visit  etc.",0.0,0.231,0.769,0.6486
datadog,This doc here about tags  is definitely worth a read!,0.0,0.411,0.589,0.5983
datadog,Tags can make graphing and monitoring much easier and more scalable.,0.0,0.219,0.781,0.4215
datadog,It is necessary to  add AspectJ weaver as Java Agent  when you're starting your Akka aplication:  -javaagent:aspectjweaver.jar,0.0,0.0,1.0,0.0
datadog,You can add the following settings in your project SBT configuration:,0.0,0.0,1.0,0.0
datadog,So AspectJ weaver JAR will be copied to  ./lib_managed/jars/org.aspectj/aspectjweaver/aspectjweaver-[aspectJWeaverV].jar  in your project root.,0.0,0.0,1.0,0.0
datadog,Then you can refer this JAR in your Dockerfile:,0.0,0.0,1.0,0.0
datadog,Few things to debug,0.0,0.0,1.0,0.0
datadog,You can easily get all needed data via querying dmv and other resources inside SQL Server.,0.0,0.138,0.862,0.34
datadog,Good start is  here .,0.0,0.492,0.508,0.4404
datadog,I have 35 Cassandra nodes (different clusters) monitored without any problems with graphite + carbon + whisper + grafana.,0.0,0.139,0.861,0.3089
datadog,But i have to tell that re-configuring collection and aggregations windows with whisper is a pain.,0.255,0.0,0.745,-0.6652
datadog,"There's many alternatives today for this job, you can use influxdb (+ telegraf) stack for example.",0.0,0.0,1.0,0.0
datadog,"Also with datadog you don't need grafana, they're also a visualizing platform.",0.0,0.0,1.0,0.0
datadog,"I've worked with it some time ago, but they have some misleading names for some metrics in their plugin, and some metrics were just missing.",0.216,0.0,0.784,-0.7469
datadog,"As a pros for this platform, it's really easy to install and use.",0.0,0.225,0.775,0.4927
datadog,"We have a cassandra cluster of 36 nodes in production right now (we had 51 but migrated the instance type since then so we need less C* servers now), monitored using a single graphite server.",0.0,0.0,1.0,0.0
datadog,We are also saving data for 30 days but in a 60s resolution.,0.0,0.0,1.0,0.0
datadog,We excluded the internode metrics (e.g.,0.324,0.0,0.676,-0.34
datadog,"open connections from a to b) because of the scaling of the metric count, but keep all other.",0.0,0.0,1.0,0.0
datadog,"This totals to ~510k metrics, each whisper file being ~500kb in size =  ~250GB.",0.0,0.0,1.0,0.0
datadog,"iostat tells me, that we have write peaks to ~70k writes/s.",0.0,0.0,1.0,0.0
datadog,This all is done on a single AWS i3.2xlarge instance which include 1.9TB nvme instance storage and 61GB of RAM.,0.0,0.0,1.0,0.0
datadog,To fully utilize the power of the this disk type we increased the number of carbon caches.,0.0,0.185,0.815,0.34
datadog,The cpu usage is very low (&lt;20%) and so is the iowait (&lt;1%).,0.166,0.0,0.834,-0.3384
datadog,"I guess we could get away with a less beefy machine, but this gives us a lot of headroom for growing the cluster and we are constantly adding new servers.",0.0,0.073,0.927,0.2617
datadog,"For the monitoring: Be prepared that AWS will terminate these machines more often than others, so backup and restore are more likely a regular operation.",0.0,0.166,0.834,0.5209
datadog,I hope this little insight helped you.,0.0,0.367,0.633,0.4404
datadog,It looks like that you have not set your JMX_PORT for kafka from where your datadog agent can listen information abouot the metrics.,0.0,0.102,0.898,0.3612
datadog,"Restart your Kafka with the following additional key/value pair parameter:
'JMX_PORT=9999'",0.0,0.0,1.0,0.0
datadog,$ JMX_PORT=9999 ./kafka-server-start.sh ../config/server.properties,0.0,0.0,1.0,0.0
datadog,This error essentially means that the Datadog Agent is unable to connect to the Kafka instance to retrieve metrics from the exposed mBeans over the RMI protocol.,0.138,0.0,0.862,-0.4588
datadog,"This error can be resolved by including the following JVM (Java Virtual Machine) arguments when starting the Kafka instance (required for Producer, Consumer, and Broker as they are all separate Java instances)
please",0.141,0.104,0.755,-0.34
datadog,Please read this article,0.0,0.434,0.566,0.3182
datadog,"Nexus 3.0.1 exposes authenticated access to metrics using  http://metrics.dropwizard.io/3.1.0/manual/servlets/ 
You have these endpoints available for different purposes:
 
        {host:port}/service/metrics/healthcheck
        {host:port}/service/metrics/data
        {host:port}/service/metrics/ping
        {host:port}/service/metrics/threads",0.07,0.0,0.93,-0.128
datadog,"I recall the default behavior being that each gear can handle 16 concurrent connections, then auto-scaling would kick in and you would get a new gear.",0.0,0.0,1.0,0.0
datadog,Therefore I would think it makes sense to start by testing that a gear works well with 16 users at once.,0.0,0.104,0.896,0.2732
datadog,"If not, then you can  change the scaling policy  to what works best for you application.",0.0,0.219,0.781,0.6369
datadog,BlazeMeter  is a tool that could probably help with creating the connections.,0.0,0.353,0.647,0.5994
datadog,"They mention 100,000 concurrent users on that main page so I don't think you have to worry about getting banned for this sort of test.",0.211,0.0,0.789,-0.7096
datadog,spring.sleuth.baggage.correlation-fields  automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.,0.0,0.144,0.856,0.4019
datadog,I suppose you use Sleuth out of the box (uses Brave):,0.0,0.0,1.0,0.0
datadog,The  spring.sleuth.baggage.correlation-fields  property automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.,0.0,0.13,0.87,0.4019
datadog,"Also, using  MDCScopeDecorator , you can set the baggage values to Slf4j’s MDC programmatically, you can see how to do it in  Sleuth docs :",0.0,0.109,0.891,0.4019
datadog,"We figured this out in the comments, I'm posting an answer that summarizes it all up: it seems the root cause was using different versions of different spring-boot modules.",0.0,0.0,1.0,0.0
datadog,"It is a good rule of thumb to not define the versions yourself but use BOMs and let them define the versions for you, e.g.",0.0,0.078,0.922,0.2382
datadog,see:  spring-boot-dependencies .,0.0,0.0,1.0,0.0
datadog,This way you will use the compatible (and tested) versions.,0.0,0.0,1.0,0.0
datadog,management.metrics.tags.your-tag  is the way to add tags to all of your metrics.,0.0,0.0,1.0,0.0
datadog,A good way to check this is looking at  /actuator/metrics .,0.0,0.266,0.734,0.4404
datadog,"I ended up going with the  sbt-javaagent  plugin to avoid extra code to exclude the agent jar from the classpath, which the plugin handles automatically.",0.157,0.0,0.843,-0.4767
datadog,"The trick/hack was to filter out the default  addJava -javaagent  line the  sbt-javaagent  plugin adds automatically , and then appending a new script snippent to only enable the javaagent when a certain env.",0.0,0.068,0.932,0.2732
datadog,variable is set.,0.0,0.0,1.0,0.0
datadog,You can use simple  jcmd  command line tool,0.0,0.0,1.0,0.0
datadog,As an example of running this on my simple Clojure application:,0.0,0.0,1.0,0.0
datadog,"As shown in the documentation in  your link , WHL files are also supported.",0.0,0.161,0.839,0.3182
datadog,It says:,0.0,0.0,1.0,0.0
datadog,You might already have one or more Python libraries packaged as an .egg or a .whl file.,0.0,0.0,1.0,0.0
datadog,There is a .whl file available for the DataDog python library here:  https://pypi.org/project/datadog/#files .,0.0,0.0,1.0,0.0
datadog,"You might try downloading that file, uploading it to your S3 bucket, and using that as your Python library for your Glue job.",0.0,0.0,1.0,0.0
datadog,You might be more successful using that than trying to build your own .egg file.,0.0,0.226,0.774,0.624
datadog,The AWS API calls to start a task are:,0.0,0.0,1.0,0.0
datadog,StartTask :,0.0,0.0,1.0,0.0
datadog,Starts a new task from the specified task definition on the specified container instance or instances.,0.0,0.0,1.0,0.0
datadog,RunTask :,0.0,0.0,1.0,0.0
datadog,Starts a new task using the specified task definition.,0.0,0.0,1.0,0.0
datadog,"You can allow Amazon ECS to place tasks for you, or you can customize how Amazon ECS places tasks using placement constraints and placement strategies.",0.0,0.194,0.806,0.5106
datadog,"Since this is AWS API calls, there are equivalent calls in CLI and SDK.",0.0,0.0,1.0,0.0
datadog,"A colleaque of mine informed me that, since we're using docker, we can by-pass supervisor and just run the horizon artisan command directly as the entrypoint of the container.",0.0,0.0,1.0,0.0
datadog,"So, I removed all things related to supervisor and my service yaml is simplified to the following and the logs are coming into datadog:",0.0,0.0,1.0,0.0
datadog,"you can open it by navigating to the directory it is in, and then typing",0.0,0.0,1.0,0.0
datadog,You need root permissions to view the file as far as I know.,0.0,0.0,1.0,0.0
datadog,Replace  key  and  value  to what you want to use.,0.0,0.316,0.684,0.4019
datadog,"In my case  key  is ""testKey"" and  value  is ""testValue""",0.0,0.211,0.789,0.34
datadog,it it my full sample code and xml configuration info.,0.0,0.0,1.0,0.0
datadog,code,0.0,0.0,1.0,0.0
datadog,log4j2.xml,0.0,0.0,1.0,0.0
datadog,output,0.0,0.0,1.0,0.0
datadog,You are configuring the filter on a new  StatsdMeterRegistry .,0.0,0.0,1.0,0.0
datadog,When using a  MeterRegistryCustomizer  you need to operate on the registry that was passed in.,0.0,0.0,1.0,0.0
datadog,"Since the customizer will be used against all registries, you also would need to add an if statement to only filter against the registry you want filtered.",0.0,0.048,0.952,0.0772
datadog,It doesn't have Grafana support yet (coming in a week or 2) but Questdb supports traditional SQL on a Time Series database and might be able to do what you want.,0.05,0.145,0.804,0.4715
datadog,https://questdb.io  or  https://github.com/questdb  on GitHub.,0.0,0.0,1.0,0.0
datadog,Datadog can process logs through their pipeline fitering feature,0.0,0.0,1.0,0.0
datadog,https://docs.datadoghq.com/logs/processing/pipelines/,0.0,0.0,1.0,0.0
datadog,"If you already have an attribute that contains the url, a really easy way to do this would be to use the  processing pipelines  and add a processor of the "" url parser "" type to these logs.",0.0,0.088,0.912,0.4927
datadog,"You just plug in the attribute that contains the url and an attribute path that you'd like to contain all the outputs from it (usually  http.url_details ), and then all new logs will get the extra url parsing applied.",0.0,0.062,0.938,0.3612
datadog,"If your logs have the ""source:nginx"" applied to them (configured in the log shipper), then you'll already have an out-of-the-box Nginx processing pipeline that Datadog has for structuring standard Nginx syntax logs.",0.0,0.0,1.0,0.0
datadog,You can clone that and then just add your new url parser there.,0.0,0.0,1.0,0.0
datadog,"Or, if your syntax is similar to the standard syntax, you can just modify their default suggested parsers (in the cloned pipeline).",0.0,0.0,1.0,0.0
datadog,"In any case, it'd be worth looking at that default pipeline for inspiration for other valuable things to do beyond url parsing.",0.0,0.307,0.693,0.8126
datadog,Assuming you'd like the output to look like the following:,0.0,0.385,0.615,0.6124
datadog,"you need to escape the  {  and and use  \""  instead of  \' :",0.0,0.134,0.866,0.1779
datadog,Unfortuanetly I can not propose an exact solution/workaround to you but you might have a look at the following documentations/API's:,0.0,0.0,1.0,0.0
datadog,Indices Stats API,0.0,0.0,1.0,0.0
datadog,Cluster Stats API,0.0,0.0,1.0,0.0
datadog,Nodes Stats API,0.0,0.0,1.0,0.0
datadog,The cpu usage is not included in the exported fields but maybe you can derive a high cpu usage behaviour from the other fields.,0.0,0.0,1.0,0.0
datadog,I hope I could help you in some way.,0.0,0.528,0.472,0.6808
datadog,You can reference  this doc  to find where the default logging path is for Jenkins depending on your OS.,0.0,0.0,1.0,0.0
datadog,"(For linux, it's  /var/log/jenkins/jenkins.log  if you don't configure it to be something else.",0.0,0.0,1.0,0.0
datadog,Then as long as your  Datadog agent  is v6+ you can use the Datadog agent to tail your jenkins.log file by following  this doc .,0.0,0.0,1.0,0.0
datadog,"Specifically, you'd add this line to your  dadatod.yaml :",0.0,0.0,1.0,0.0
datadog,"and add this content to any old  conf.yaml  file nested in your  conf.d/  directory, such as  conf.d/jenkins.d/conf.yaml :",0.0,0.0,1.0,0.0
datadog,"Then the agent will tail your log file as it's written to, and will forward it to your Datadog account so you can query, graph, and monitor on your log data there.",0.0,0.0,1.0,0.0
datadog,"Once you have the logs coming in, you may want to write a  processing pipeline  to get the critical attributes parsed out, but that would be material for a new question :) .",0.049,0.152,0.799,0.5423
datadog,This command will only take a split second.,0.0,0.0,1.0,0.0
datadog,You must have spent 35 minutes waitung for the  ACCESS EXCLUSIVE  lock on the table to be granted (all the while blocking any transaction unfortunate enough to be queued behind you).,0.152,0.115,0.733,-0.3328
datadog,You probably have a problem with long transactions.,0.31,0.0,0.69,-0.4019
datadog,"Normally they should be as short as possible, otherwise they hold locks for a long time and also keep  VACUUM  from cleaning up dead row versions.",0.152,0.0,0.848,-0.6486
datadog,"The lock is necessary, but is should not pose a problem with a well behaved database workload.",0.0,0.299,0.701,0.6744
datadog,You can create one PowerShell script to execute your Batch scripts remotely.,0.0,0.16,0.84,0.2732
datadog,And Even you can schedule your PowerShell script using Windows Task Scheduler which will run as per your settings.,0.0,0.0,1.0,0.0
datadog,Rubber Duck.,0.0,0.0,1.0,0.0
datadog,"Turns out because we changed  .set  to  .default , we lost the ability to have the variables properly set during the first run.",0.093,0.093,0.813,0.0
datadog,.normal  will do it for us.,0.0,0.0,1.0,0.0
datadog,"Lots of threads are in WAITING state, and it's absolutely ok for them.",0.0,0.172,0.828,0.3597
datadog,"For example, there are thread which have the following stack trace:",0.0,0.0,1.0,0.0
datadog,This only means threads are waiting for any tasks to do.,0.0,0.0,1.0,0.0
datadog,"However, other stacks do not look good.",0.286,0.0,0.714,-0.3412
datadog,Those threads are waiting for connection to be free in the pool.,0.0,0.231,0.769,0.5106
datadog,C3P0 is a pool of database connections.,0.0,0.0,1.0,0.0
datadog,"Instead of creating a new connection every time, they are cached in the pool.",0.0,0.155,0.845,0.296
datadog,"Upon closing, the connection itself is not closed, but only returned to the pool.",0.0,0.0,1.0,0.0
datadog,"So, if hibernate for some reason (or other user) do not close connection after releasing it, then pool can get exhausted.",0.111,0.0,0.889,-0.3612
datadog,"In order to resolve an issue, you have to find out why some connections are not closed after using.",0.0,0.126,0.874,0.3818
datadog,Try to look at your code to do this.,0.0,0.0,1.0,0.0
datadog,The other option is to temporarily go without C3P0 (pooling).,0.0,0.0,1.0,0.0
datadog,"This is not forever, but at least you can check whether this guess is right.",0.0,0.0,1.0,0.0
datadog,"In Grails 3, You should put the below code to  grails-app/conf/spring/resources.groovy :",0.0,0.0,1.0,0.0
datadog,"Neither, you want something like this I think:",0.14,0.287,0.573,0.3134
datadog,Also putting the key into node attributes like that is very unsafe and kind of defeats the point of encrypted bags since node attributes are all written back to the Chef Server and so the key will be sent unencrypted.,0.06,0.058,0.882,-0.024
datadog,"I still don't know why it makes a difference, but adding the  -4  option made it work",0.0,0.0,1.0,0.0
datadog,Here's the man page on the option:,0.0,0.0,1.0,0.0
datadog,-4      Forces nc to use IPv4 addresses only.,0.0,0.0,1.0,0.0
datadog,Problem in this case is not running scripts via JMeter GUI.,0.213,0.0,0.787,-0.4019
datadog,Instead it is related to network.,0.0,0.0,1.0,0.0
datadog,I had a similar distributed setup in EC2-environment and I successfully executed heavy load tests in GUI mode.,0.0,0.186,0.814,0.4939
datadog,"In my case, all my JMeter (master/slaves) were running on EC2 instances (windows environment).",0.0,0.0,1.0,0.0
datadog,"So, I will recommend you to setup your  JMeter   (Master)  on EC2 and run scripts via GUI mode.",0.0,0.148,0.852,0.4173
datadog,If you still want to run in command line mode then you simply need to pass command to create jtl file while the script runs on command line.,0.0,0.116,0.884,0.34
datadog,Later on you can use this JTL to generate any JMeter report as per requirement.,0.0,0.0,1.0,0.0
datadog,For more details check..,0.0,0.0,1.0,0.0
datadog,Jmeter - Run .jmx file through command line and get the summary report in a excel,0.0,0.188,0.812,0.4588
datadog,jmeter -n -t /path/to/your/test.jmx  -l /path/to/results/file.jtl,0.0,0.0,1.0,0.0
datadog,Please refer to Dmitri answer in following question to reduce JTL size.,0.0,0.173,0.827,0.3182
datadog,How can we control size of JTL file while running test from Non GUI Mode,0.0,0.0,1.0,0.0
datadog,"Before implementing the code, you need to look around in Widows ""registry"" using ""regedit"" and find the exact registry key value for the software.",0.0,0.094,0.906,0.34
datadog,"Below example shows, how to fetch the version number of ""internet explorer"".",0.0,0.106,0.894,0.0772
datadog,"Also recommended to have basic knowledge on Ruby array and hash, to understand the code",0.0,0.114,0.886,0.2023
datadog,I've used registry_key_XXXXX Chef methods.,0.0,0.0,1.0,0.0
datadog,Note: Registry key entry may differ for Windows 32bit and 64bit,0.0,0.0,1.0,0.0
datadog,The problem is your  sendData()  function.,0.351,0.0,0.649,-0.4019
datadog,This function is called in your for loop and has the following line:,0.0,0.0,1.0,0.0
datadog,"This line will create a new DataDog client, which uses a Unix socket.",0.0,0.174,0.826,0.2732
datadog,This explains your error message.,0.403,0.0,0.597,-0.4019
datadog,"With every iteration of your loop, a new socket is &quot;allocated&quot;.",0.0,0.0,1.0,0.0
datadog,"After a sufficient amount of loops no sockets can be opened, resulting in:",0.167,0.0,0.833,-0.296
datadog,socket: too many open files,0.0,0.0,1.0,0.0
datadog,To fix this you should create the client only once and pass it to your method as parameter.,0.0,0.11,0.89,0.2732
datadog,The variable  $LASTEXITCODE  will give you the exit code of the last native command (executable) that was run.,0.0,0.0,1.0,0.0
datadog,"I'm not familiar with the program, but I've had to solve a problem like this before.",0.169,0.26,0.571,0.2263
datadog,"I'm making the assumption that you're always going to start the output you want with a line 'Dogstatsd', and always end with several equals signs.",0.0,0.053,0.947,0.0772
datadog,"Based on that, you could script out your output like this:",0.0,0.2,0.8,0.3612
datadog,"We get the values defining the length of the file, the length until we hit the first line you want, the length where the output ends, and trim accordingly.",0.0,0.129,0.871,0.4588
datadog,I would strongly suggest to setup a pre-production environment and run load tests (with tools like  JMeter ) in conjunction with server-side monitoring.,0.0,0.204,0.796,0.5574
datadog,Tomcat backends can be monitored using the JMX protocol.,0.0,0.0,1.0,0.0
datadog,You have 2 solutions :,0.0,0.459,0.541,0.1779
datadog,"Like always, free software costs nothing but your time, and paid software gets you straight to the issue in exchanges for some pennies.",0.0,0.238,0.762,0.6428
elasticapm,As it is reported on the official site it could be released in the future.,0.0,0.0,1.0,0.0
elasticapm,"Some of the features we are very excited to provide in the near future
  include distributed tracing and providing framework-specific
  information (e.g., route change times) for some of the frontend
  frameworks such as React, Angular, Vue.js, etc.",0.0,0.07,0.93,0.4005
elasticapm,https://www.elastic.co/blog/elastic-apm-rum-js-agent-is-generally-available,0.0,0.0,1.0,0.0
elasticapm,For now you can rely on Elastic APM RUM JS Agent using JS tags:,0.0,0.0,1.0,0.0
elasticapm,https://www.elastic.co/guide/en/apm/agent/js-base/3.x/getting-started.html#using-script-tags,0.0,0.0,1.0,0.0
elasticapm,You can try to increase:,0.0,0.365,0.635,0.3182
elasticapm,Please take a look on documentation:  Tune APM Server,0.0,0.247,0.753,0.3182
elasticapm,You can attach your ElasticApmAttacher.attach() in the Spring Application main class,0.0,0.0,1.0,0.0
elasticapm,"For a SpringBootApplication packaged as a war file, and deployed to Tomcat server, this can be added to the configure method",0.178,0.0,0.822,-0.5994
elasticapm,Below code might help:,0.0,0.474,0.526,0.4019
elasticapm,unfortunately oracle is not supported by elastic apm agent.,0.384,0.0,0.616,-0.5207
elasticapm,you should wrap your  oracleQueryRunner  in order to start and end agent spans manually.,0.0,0.0,1.0,0.0
elasticapm,put this code in your  main.ts  file:,0.0,0.0,1.0,0.0
elasticapm,"It's true, there isn't an Elixir agent for Elastic APM - you can upvote  this issue  to get the topic more attention.",0.0,0.123,0.877,0.4215
elasticapm,"As you discovered, you can use the OpenTelemetry in the meantime.",0.0,0.0,1.0,0.0
elasticapm,"To do that, run the OpenTelemetry contrib collector (otel collector) and configure it to export to  elastic  - there is a full explanation  in the docs  along with this sample configuration:",0.0,0.0,1.0,0.0
elasticapm,"In your application, configure the tracer use the  opentelemetry exporter .",0.0,0.0,1.0,0.0
elasticapm,At that point you'll have a tracer in your application sending traces to the otel collector.,0.0,0.0,1.0,0.0
elasticapm,"From there, traces will be exported to the Elastic Stack via APM Server.",0.0,0.0,1.0,0.0
elasticapm,In summary:  your app -&gt; otel collector -&gt; apm-server -&gt; elasticsearch,0.0,0.0,1.0,0.0
elasticapm,The  Erlang/Elixir Agent Docs  have sample code for starting and decorating spans.,0.0,0.0,1.0,0.0
elasticapm,transaction.duration.us  should indeed be what you're looking for.,0.0,0.0,1.0,0.0
elasticapm,It's the duration in microseconds as an integer.,0.0,0.0,1.0,0.0
elasticapm,"Divide it by 1000 to get milliseconds, or by 1'000'000 to get seconds.",0.0,0.0,1.0,0.0
elasticapm,https://www.elastic.co/guide/en/apm/server/7.9/exported-fields-apm-transaction.html#_duration_2,0.0,0.0,1.0,0.0
elasticapm,"When you bring up containers using compose, each container has its own networking stack (so they can each talk to themselves on  localhost , but they need an ip address or dns name to talk to a different container!",0.0,0.0,1.0,0.0
elasticapm,).,0.0,0.0,1.0,0.0
elasticapm,Compose by default connects each of the containers to a default network and gives each a dns name with the name of the service.,0.0,0.0,1.0,0.0
elasticapm,If your compose file looks like,0.0,0.333,0.667,0.3612
elasticapm,A process in the  apm  container could access elasticsearch at  http://elasticsearch:9200,0.0,0.0,1.0,0.0
elasticapm,"If you are starting all the services with single docker compose file, the app-server.yaml should have the value like this
 
output:
  elasticsearch:
    hosts: elasticsearch:9200
 
The ""hosts: elasticsearch:9200"" should be service name of the elasticsearch you mentioned in the docker-compose.",0.0,0.117,0.883,0.5994
elasticapm,Like in the followiing,0.0,0.455,0.545,0.3612
elasticapm,"
    version: '2'
    services:
      elasticsearch:
          image: elasticsearch:latest",0.0,0.0,1.0,0.0
elasticapm,Answer to the Errors - I have custom resole.extensions in the  webpack.config.js :,0.211,0.0,0.789,-0.34
elasticapm,That was missing the default  .json :,0.306,0.0,0.694,-0.296
elasticapm,Now only the warnings are left:,0.306,0.0,0.694,-0.296
elasticapm,I addressed them to the developer:  https://github.com/elastic/apm-agent-nodejs/issues/1154,0.0,0.0,1.0,0.0
elasticapm,To listen on  0.0.0.0  try:,0.0,0.0,1.0,0.0
elasticapm,Try using a configuration:,0.0,0.0,1.0,0.0
elasticapm,where the file apm-server/config/apm-server.yml has your config content:,0.0,0.0,1.0,0.0
elasticapm,Note the rum.allow_origins option that you can configure to resolve the CORS issue.,0.0,0.178,0.822,0.3818
elasticapm,https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html,0.0,0.0,1.0,0.0
elasticapm,you have to modify your elastic APM python agent code.,0.0,0.0,1.0,0.0
elasticapm,"In general, you can add labels to your span example",0.0,0.0,1.0,0.0
elasticapm,"also, you can add directly to span object.",0.0,0.0,1.0,0.0
elasticapm,You will get the ip from request object flask,0.0,0.0,1.0,0.0
elasticapm,request.remote_addr   set this to the desired key.,0.0,0.259,0.741,0.2732
elasticapm,more details of APIs on elastic APM python agent can be found here -  https://www.elastic.co/guide/en/apm/agent/python/current/api.html,0.0,0.0,1.0,0.0
elasticapm,Thanks,0.0,1.0,0.0,0.4404
elasticapm,These metrics come directly from  java.lang.management.GarbageCollectorMXBean .,0.0,0.0,1.0,0.0
elasticapm,"The value of the  jvm.gc.time  metric is taken from  GarbageCollectorMXBean.getCollectionTime , which is indeed accumulating since the process started.",0.0,0.124,0.876,0.34
elasticapm,"Assuming you're looking at metrics from a single JVM, there are a couple of possible reasons for why the value would appear to have gone backwards:",0.0,0.094,0.906,0.34
elasticapm,"If the process had restarted (which I expect you would know about anyway), the metrics documents in Elasticsearch would have different values for the field  agent.ephemeral_id .",0.0,0.101,0.899,0.4019
elasticapm,"The more likely answer is that you're seeing values for two different memory managers/GC generations, in which case the metrics documents in Elasticsearch would have different values for the field  labels.name .",0.0,0.157,0.843,0.6597
elasticapm,You don't end  trans1  and  trans2 .,0.0,0.0,1.0,0.0
elasticapm,"Just put these 2 lines to the point where these end, and everything should show up fine:",0.0,0.107,0.893,0.2023
elasticapm,"There is the  CaptureTransaction , which is a convenient method that can wrap you any code and makes sure the transaction is ended and all exceptions are captured - so you use that method and it does ""everything"" for you.",0.0,0.06,0.94,0.3182
elasticapm,"Then there is the  StartTransaction  method - this is the one you use in your code -, which starts the transaction and does not do anything else.",0.0,0.0,1.0,0.0
elasticapm,The advantage here is that you get an  ITransaction  instance which you can use wherever and whenever you want.,0.0,0.163,0.837,0.3182
elasticapm,But in this case you need to call  .End()  on it manually once the transaction (aka the code you want to capture) is executed.,0.0,0.059,0.941,0.1154
elasticapm,Same with  CaptureSpan  and  StartSpan .,0.0,0.0,1.0,0.0
elasticapm,"So you used  CaptureSpan  for your spans, so those where ended automatically when the lambda with  Task.Delay  finished, on the other hand you started your transactions with  StartTransaction  but only called  .End()  on  trans3  and not on the 2 other transactions.",0.0,0.051,0.949,0.2732
elasticapm,There is some explanation with a demo  here  - sample code of that demo is  here .,0.0,0.0,1.0,0.0
elasticapm,Currently background services are not captured out of the box.,0.0,0.0,1.0,0.0
elasticapm,What you can do is to use the  Public Agent API  and with a little bit of an additional code you can capture those also as transactions.,0.0,0.0,1.0,0.0
elasticapm,Something like this in the background service:,0.0,0.294,0.706,0.3612
elasticapm,"I run via docker-compose elasticsearch, apm, kibana and tomcat application in docker.",0.0,0.0,1.0,0.0
elasticapm,In apm- -transaction-  index exist this meta information:  container.id .,0.0,0.0,1.0,0.0
elasticapm,And in apm- -metrics-  index this information is also stored.,0.0,0.0,1.0,0.0
elasticapm,"Try to look at json structure at Discover tab by index pattern ""apm-*""",0.0,0.0,1.0,0.0
elasticapm,enter image description here,0.0,0.0,1.0,0.0
elasticapm,did you try to give permissions to folder /opt/elastic ?,0.0,0.0,1.0,0.0
elasticapm,"It's hard to say without knowing exactly how you're using NestJS and GraphQL together, and without knowing which parts of Apollo Server that NestJS itself uses.",0.053,0.0,0.947,-0.1027
elasticapm,Seeing a small sample of what  I am using the graphql feature of nestjs  means would be useful.,0.0,0.162,0.838,0.4404
elasticapm,Here's a few datapoints that might help you narrow things down.,0.0,0.231,0.769,0.4019
elasticapm,"Also, opening  an issue  in the Agent repository or  a question in their forums  might get more of the right eyes on this.",0.0,0.0,1.0,0.0
elasticapm,"The Elastic APM instrumentation for Apollo Server works by wrapping the  runHttpQuery  function of the  apollo-server-core  module, and  marking the transaction with  trans._graphqlRoute .",0.0,0.0,1.0,0.0
elasticapm,"When the agent  sees this  _graphqlRoute  property , it runs some code that will set a default name for the transaction",0.0,0.0,1.0,0.0
elasticapm,"In your application, either the  _graphqlRoute  property isn't getting set, or the renaming code above is doing something weird, or something about NestJS comes along and renames the transaction after it's been named with the above code.",0.045,0.0,0.955,-0.1779
elasticapm,Knowing more specifically  what  you're doing would help folks narrow in on your problems.,0.155,0.155,0.69,0.0
elasticapm,The metrics shown in Kibana are sent by the APM agent that like you said has limited access to your environment.,0.081,0.107,0.812,0.1531
elasticapm,It basically says anything that is collected by the JVM running your JAR.,0.0,0.0,1.0,0.0
elasticapm,If you want to get further visibility into the CPU details of your local environment then you must augment your setup using  Elastic MetricBeats  that ships O.S level details about your machine that sees beyond what the JVM can see.,0.0,0.032,0.968,0.0772
elasticapm,"In the presentation below I show how to configure logs, metrics, and APM altogether.",0.0,0.0,1.0,0.0
elasticapm,https://www.youtube.com/watch?v=aXbg9pZCjpk,0.0,0.0,1.0,0.0
elasticapm,"currently,we run apm-agent for java application, but after 1 day, server cpu has over 100% and java application killed by system",0.248,0.0,0.752,-0.8047
elasticapm,"I am not familiar with what is Elastic APM, but if it says it supports Spring Boot, then it means it supports any spring-boot-based framework which Spring Cloud Stream is.",0.0,0.194,0.806,0.7579
elasticapm,"The  JVM GC metrics tracked  right now are  jvm.gc.alloc ,  jvm.gc.time , and  jvm.gc.count .",0.0,0.0,1.0,0.0
elasticapm,"If you are looking for additional ones, which ones would those be?",0.0,0.0,1.0,0.0
elasticapm,And could you  open an issue with the details .,0.0,0.0,1.0,0.0
elasticapm,Please import from saved objects option -  https://github.com/elastic/apm-contrib/blob/master/apm-agent-java/dashboards/java_metrics_dashboard_7.x.json,0.0,0.505,0.495,0.6249
elasticapm,To include spans into the transactions you should start the spans from the transaction object,0.0,0.0,1.0,0.0
elasticapm,"
 
 ...
var span = transaction.startSpan('My custom span')
...",0.0,0.0,1.0,0.0
elasticapm,And ending the parent transaction object all the nested spans will be also ended in cascade,0.0,0.0,1.0,0.0
elasticapm,https://www.elastic.co/guide/en/apm/agent/js-base/4.x/transaction-api.html#transaction-start-span,0.0,0.0,1.0,0.0
elasticapm,You can start your application with argument  active=false .,0.263,0.0,0.737,-0.3612
elasticapm,C:\Users\dsm\Documents&gt;java -jar apm-agent-attach-1.9.0-standalone.jar --pid 16832 --args 'service_name=test;server_urls=http://localhost:8200;active=false',0.0,0.0,1.0,0.0
elasticapm,You simply need to change your query to this:,0.0,0.0,1.0,0.0
elasticapm,"The accepted answer no longer works, you can use the following",0.165,0.158,0.677,-0.0258
elasticapm,The key features of APM agents is normally in their framework integrations.,0.0,0.0,1.0,0.0
elasticapm,The Java APM agent is mostly focussed on web frameworks — see the list of  supported technologies .,0.0,0.133,0.867,0.3182
elasticapm,"But you already mentioned the  public API  — if you manually instrument your code with that, you will still be able to use it.",0.0,0.0,1.0,0.0
elasticapm,It just doesn't automatically understand the framework and you need to help it with that.,0.0,0.162,0.838,0.4019
elasticapm,"Alternatively, if your tool supports OpenTracing then you could use the  OpenTracing bridge  for that.",0.0,0.152,0.848,0.3612
elasticapm,When routing to a different subpage you have to set the Route Name manually.,0.0,0.0,1.0,0.0
elasticapm,You can achieve this via a filter on the 'change-route' type.,0.0,0.0,1.0,0.0
elasticapm,"See  apm.addFilter()  
docs:  https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#apm-add-filter",0.0,0.0,1.0,0.0
elasticapm,Something like this should work:,0.0,0.385,0.615,0.3612
elasticapm,Another way to do it would be to observe for transaction events which is fired whenever transaction starts and ends.,0.159,0.0,0.841,-0.5574
elasticapm,API docs -  https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#observe,0.0,0.0,1.0,0.0
elasticapm,"The agent also supports frameworks such as Vue.js, React and Angular out of the box, so the above code should not be necessary.",0.0,0.102,0.898,0.3612
elasticapm,Vue docs -  https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html,0.0,0.0,1.0,0.0
elasticapm,"Logging is separate from APM / tracing, but can be integrated.",0.0,0.0,1.0,0.0
elasticapm,"https://github.com/elastic/ecs-logging-java  is a curated logging library that will also correlate the trace IDs, so you can tie both together.",0.0,0.0,1.0,0.0
elasticapm,Keep using SLF4J and just add the right logging backend.,0.0,0.0,1.0,0.0
elasticapm,The output can then be picked up by Filebeat (as described in the repository) and you're ready to go.,0.0,0.122,0.878,0.3612
elasticapm,"Elastic-apm-agent-java automatically capture exceptions when you use slf4j implementation  Logger#error(""message"", Throwable) .",0.0,0.0,1.0,0.0
elasticapm,More information you can find  here,0.0,0.0,1.0,0.0
elasticapm,Try  this  official docker-compose set up:,0.0,0.0,1.0,0.0
elasticapm,"The Elastic RUM agent has support for  click user interactions , therefore you shouldn't need to manually start these type of transactions.",0.0,0.119,0.881,0.4019
elasticapm,Regarding the failure in your code the correct API call is  getCurrentTransaction  and not  currentTransaction .,0.191,0.0,0.809,-0.5106
elasticapm,Hope this helps.,0.0,0.846,0.154,0.6705
elasticapm,"So, I'm guessing that the handler wrappers are dropping the buffalo.Context information.",0.0,0.0,1.0,0.0
elasticapm,That's correct.,0.0,0.0,1.0,0.0
elasticapm,The problem is that  buffalo.WrapHandler  ( Source ) throws away all of the context other than the underlying  http.Request / http.Response :,0.137,0.0,0.863,-0.4019
elasticapm,"So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?",0.0,0.0,1.0,0.0
elasticapm,I can see two options:,0.0,0.0,1.0,0.0
elasticapm,There's an open issue in the Elastic APM agent for the latter option:  elastic/apm#39 .,0.0,0.0,1.0,0.0
elasticapm,There are basically 2 ways the agent captures things:,0.0,0.0,1.0,0.0
elasticapm,"In a typical ASP.NET Classic MVC application the agent has auto instrumentation for outgoing HTTP calls with  HttpClient , Database calls with EF6 (Make sure to  add the interceptor ) ( SqlClient  support is already work-in-progress, hopefully released soon).",0.0,0.242,0.758,0.836
elasticapm,"So unless you have any of these within those requests, the agent won't capture things out of the box.",0.0,0.0,1.0,0.0
elasticapm,"If you want to capture more things, currently the way to go is to place some agent specific code - so basically manual code instrumentation - into your application and use the public agent API.",0.0,0.039,0.961,0.0772
elasticapm,Finally noticed issue.,0.0,0.0,1.0,0.0
elasticapm,application API is returning service name as Bootstrap because.,0.0,0.0,1.0,0.0
elasticapm,Id is not set so it is using default value,0.0,0.211,0.789,0.34
elasticapm,The Id can be set like this,0.0,0.294,0.706,0.3612
elasticapm,"You can try the method described here  disscuss-elastic , via ElasticApmAttacher#attach(map of properties).",0.0,0.0,1.0,0.0
elasticapm,"Is this not yet included in the .NET agent, or is there additional configuration necessary to get this working?",0.0,0.0,1.0,0.0
elasticapm,This is not yet included in the .NET Agent unfortunately.,0.211,0.0,0.789,-0.34
elasticapm,WebFlux can run on Servlet containers with support for the Servlet 3.1 Non-Blocking IO API as well as on other async runtimes such as  Netty  and Undertow.,0.0,0.161,0.839,0.5859
elasticapm,Each Spring Boot web application includes an embedded web server.,0.0,0.0,1.0,0.0
elasticapm,"For reactive stack applications, the  spring-boot-starter-webflux  includes Reactor Netty by default I guess.",0.0,0.0,1.0,0.0
elasticapm,"And it does not include  Servlet API  (Netty is non-Servlet runtime), but it looks like your  Elastic APM  expects this API to be present.",0.0,0.124,0.876,0.5023
elasticapm,Try to use  spring-boot-starter-tomcat  instead of Netty.,0.0,0.0,1.0,0.0
elasticapm,"When  switching to a different HTTP server , you need to exclude the default dependencies in addition to including the one you need.",0.087,0.0,0.913,-0.2263
elasticapm,Here is an example:,0.0,0.0,1.0,0.0
elasticapm,Tomcat dependency brings Servlet API.,0.0,0.0,1.0,0.0
elasticapm,Perhaps it will resolve your issue.,0.0,0.342,0.658,0.3818
elasticapm,Looks like there is no support from elastic for WebFlux yet,0.143,0.338,0.519,0.4588
elasticapm,Check here  https://github.com/elastic/apm-agent-java/issues/60,0.0,0.0,1.0,0.0
elasticapm,"They are currently working on it, but there is not a date to be ready yet",0.0,0.188,0.812,0.5023
elasticapm,I got the answer after posting the same on  Elastic Support Forum .,0.0,0.213,0.787,0.4019
elasticapm,It was a very prompt response.,0.0,0.0,1.0,0.0
elasticapm,"This was not a problem from Elastic APM side, and was more of a silly problem from my side.",0.0,0.296,0.704,0.5986
elasticapm,Refer the  discussion  to find the problem and solution.,0.225,0.192,0.583,-0.1027
elasticapm,"This question was cross-posted to discuss.elastic.co, and you can see the answer that was provided there:  https://discuss.elastic.co/t/elastic-apm-python-system-metrics-dont-show-process-related-metrics-like-memory-on-kibana/240531/2?u=basepi",0.0,0.0,1.0,0.0
elasticapm,"In your ES Cloud console, you need to Edit the cluster configuration, scroll to the APM section and then click &quot;User override settings&quot;.",0.0,0.0,1.0,0.0
elasticapm,In there you can override the target index by adding the following property:,0.0,0.0,1.0,0.0
elasticapm,"Note that if you change this setting, you also need to modify the corresponding index template to match the new index name.",0.0,0.0,1.0,0.0
elasticapm,currently elastic-apm-agent support natively Quartz framework(since 1.8).,0.0,0.31,0.69,0.4019
elasticapm,"If you use it, instrumentation should work.",0.0,0.0,1.0,0.0
elasticapm,But you should add your packages to  application_packages .,0.0,0.0,1.0,0.0
elasticapm,It would be good if you can share mini-demo project.,0.0,0.389,0.611,0.6249
elasticapm,And I can reproduce your problem locally.,0.351,0.0,0.649,-0.4019
elasticapm,Information from  supported-technologies-details,0.0,0.0,1.0,0.0
elasticapm,"All of the Elastic APM agents, with the exception of the RUM JavaScript agent, have a  verify_server_cert  configuration variable.",0.0,0.0,1.0,0.0
elasticapm,You can set this to  false  to disable server TLS certificate verification.,0.0,0.0,1.0,0.0
elasticapm,I am using docker-compose,0.0,0.0,1.0,0.0
elasticapm,and the following option doesn't work.,0.0,0.0,1.0,0.0
elasticapm,I think its a bug for apm..,0.0,0.0,1.0,0.0
elasticapm,When I use same option in apm-server.yml it works fine.,0.0,0.184,0.816,0.2023
elasticapm,One thing you can use is the  Filter API  for this.,0.0,0.0,1.0,0.0
elasticapm,With that you have access to all transactions and spans before they are sent to the APM Server.,0.0,0.0,1.0,0.0
elasticapm,"You can't run through all the spans on a given transaction, so you need some tweaking - for this I use a  Dictionary  in my sample.",0.0,0.0,1.0,0.0
elasticapm,Couple of thing here:,0.0,0.0,1.0,0.0
elasticapm,Looks like it can be done using drop_event processor in api-server.yml.,0.0,0.2,0.8,0.3612
elasticapm,and in code set custom context:,0.0,0.0,1.0,0.0
elasticapm,Commenting out  'SERVER_URL': '127.0.0.1:8200'  solved the problem.,0.276,0.214,0.51,-0.1531
elasticapm,I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.,0.0,0.129,0.871,0.34
elasticapm,"Yes, this is possible and there is an API for it.",0.0,0.213,0.787,0.4019
elasticapm,This part of the documentation  explains it.,0.0,0.0,1.0,0.0
elasticapm,So you'll need to do this when you start your transaction - I imagine in your scenario this will be when you read a message from RabbitMQ.,0.0,0.0,1.0,0.0
elasticapm,"When you  start the transaction  there is an optional parameter called  distributedTracingData  - if you pass it, then the transaction will reuse the traceid which you passed through RabbitMQ, and this way the new transaction will be part of the whole trace.",0.0,0.0,1.0,0.0
elasticapm,"If you don't pass this parameter, a new traceid will be generated and a new trace will be started.",0.0,0.0,1.0,0.0
elasticapm,Another comment that may help: you pass the trace id into the method where you start the transaction and each span will inherit this trace id within a transaction - so you control this on the transaction level and accordingly you don't pass it into a span.,0.0,0.059,0.941,0.4019
elasticapm,Here is a small code snippet on how this would look:,0.0,0.0,1.0,0.0
elasticapm,"@gregkalapos, again thank you for the information.",0.0,0.294,0.706,0.3612
elasticapm,I checked how to acquire the neccessary trace information as in  node.js agent documentation  and when I debugged noticed that it was the trace id.,0.0,0.0,1.0,0.0
elasticapm,Next in the C# consumer end I placed a code snippet as mentioned in the  .Net agent  and gave it a run.,0.0,0.0,1.0,0.0
elasticapm,Kibana displayed the transactions from two different services in a single trace as I hoped it would.,0.0,0.157,0.843,0.3818
elasticapm,Initially asked this questions because Visual Studio did not show the source as expected in the editor.,0.0,0.0,1.0,0.0
elasticapm,So what I did was clicked Build after right clicking on the .sln (solution) then noticed that necessary version of .Net SDK was not there and version of MSBuild related to it.,0.0,0.0,1.0,0.0
elasticapm,Once I updated Visual Studio the sources were visible.,0.0,0.0,1.0,0.0
elasticapm,Hope this would help if someone faced a similar situation.,0.0,0.444,0.556,0.6808
elasticapm,Using labels should be the best way to add custom details to the transaction/span but you can also use the  addCustomContext()  method:,0.0,0.11,0.89,0.3818
elasticapm,https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-add-custom-context,0.0,0.0,1.0,0.0
elasticapm,That error indicates the agent can't connect to apm-server.,0.252,0.0,0.748,-0.4019
elasticapm,SERVER_URL  should be  ELASTIC_APM_SERVER_URL  in the apm-agent-container env.,0.0,0.0,1.0,0.0
elasticapm,"Thanks for the reply, I'm able to connect apm-server with the agent, but in kibana dashboard, I'm getting &quot; No data has been received from agents yet&quot; .",0.094,0.066,0.84,-0.2144
elasticapm,My application is running fine,0.0,0.31,0.69,0.2023
elasticapm,Good job so far.,0.0,0.492,0.508,0.4404
elasticapm,"Your pipeline is almost good, however, the grok pattern needs some fixing and you have some orphan curly braces.",0.0,0.127,0.873,0.3832
elasticapm,Here is a working example:,0.0,0.0,1.0,0.0
elasticapm,Response:,0.0,0.0,1.0,0.0
elasticapm,Just note that the exact date is missing so the @timestamp field resolve to January 1st this year.,0.106,0.125,0.769,0.1027
elasticapm,Try to specify the  config_file  using the following notation:,0.0,0.0,1.0,0.0
elasticapm,-Delastic.apm.config_file=elasticapm.properties,0.0,0.0,1.0,0.0
elasticapm,The attacher can create the log file depending on the settings configured during startup.,0.0,0.139,0.861,0.2732
elasticapm,See the [1] current code for a better understanding.,0.0,0.293,0.707,0.4404
elasticapm,[1]  https://github.com/elastic/apm-agent-java/blob/0465d479430172c3e745afd2ef5b62a3da6b60aa/apm-agent-attach-cli/src/main/java/co/elastic/apm/attach/AgentAttacher.java#L79,0.0,0.0,1.0,0.0
elasticapm,"Do you mean that you need new ""Transaction type""?",0.0,0.0,1.0,0.0
elasticapm,"If yes, so you should set  type  annotation parameter.",0.0,0.252,0.748,0.4019
elasticapm,But @CaptureTranscation annotation work  in case :,0.0,0.0,1.0,0.0
elasticapm,"I worked with the Elastic APM team, who had just rolled out this package:  https://www.npmjs.com/package/elastic-apm-node",0.0,0.0,1.0,0.0
elasticapm,"The directions are pretty self-explanatory, works like a charm.",0.0,0.627,0.373,0.8126
elasticapm,Hard to tell without debugging but since some connections are getting dropped when you add more load + concurrency it's likely that you need more replicas on your  Kubernetes deployments  and possibly adjusts the  Resources  on your container pod specs.,0.031,0.0,0.969,-0.0516
elasticapm,If this turns out to be the case you can also configure an  HPA  (Horizontal Pod Autoscaler) to handle your load.,0.0,0.0,1.0,0.0
elasticapm,Are you not building a custom Dockerfile and you could just add it there (using wget or curl probably)?,0.0,0.0,1.0,0.0
elasticapm,"If you really want a build dependency,  https://search.maven.org/artifact/co.elastic.apm/elastic-apm-agent/1.7.0/jar  should be what you want.",0.0,0.242,0.758,0.2928
elasticapm,"PS: IMO it's a feature that this is only a runtime dependency and you can just add, remove, change it independently of your application; unless you want to do some custom instrumentation.",0.0,0.043,0.957,0.0772
elasticapm,If the objective is to attach labels to a transaction over multiple spans then using the  public APIs from the Elastic APM for Java  is a better choice instead of instrumenting the JVM with ByteBuddy.,0.0,0.083,0.917,0.4404
elasticapm,You will have much more freedom to do what you want to do without relying on a hacking.,0.0,0.279,0.721,0.6997
elasticapm,"FYI, the Elastic APM agent for Java already instrument the JVM with additional bytecode so what you are doing may get even more confusing because of this.",0.074,0.085,0.841,0.0875
elasticapm,"Alternatively, you can also use the  OpenTracing Bridge  to set labels in a transaction.",0.0,0.0,1.0,0.0
elasticapm,Disclaimer: This answer is a stub for now.,0.0,0.0,1.0,0.0
elasticapm,You should first try to explore a more canonical way of doing things like Ricardo suggested.,0.0,0.152,0.848,0.3612
elasticapm,"If for some reason that does not work, then we could explore ways to instrument your agent class - not so much because I think it is a good idea but because it is technically interesting.",0.0,0.151,0.849,0.6705
elasticapm,"Basically, we would have to find out if maybe the class you want to instrument was already loaded before your ByteBuddy agent gets active.",0.0,0.154,0.846,0.4588
elasticapm,Then you would have to use class retransformation rather than redefinition.,0.0,0.0,1.0,0.0
elasticapm,You would have to make sure the advice you apply can do its job without the need to change the class structure with regard to method signatures and fields.,0.0,0.076,0.924,0.3182
elasticapm,"You would also need to make sure that the advice and ByteBuddy are visible to the other agent's classloader, e.g.",0.0,0.108,0.892,0.3182
elasticapm,by putting both on the boot class path.,0.0,0.0,1.0,0.0
elasticapm,But let's not get ahead of ourselves.,0.0,0.0,1.0,0.0
elasticapm,"Explore Ricardo's ideas first, please.",0.0,0.365,0.635,0.3182
elasticapm,Based on what I've seen it looks like there isn't a &quot;right&quot; way to do this with the stock  nuxt  command line application.,0.0,0.106,0.894,0.3612
elasticapm,"The problem seems to be that while  nuxt.config.js  is the first time a user has a chance to add some javascript, that the  nuxt  command line application bootstraps the Node's HTTP frameworks before this config file is  required .",0.07,0.052,0.879,-0.1779
elasticapm,This means the elastic agent (or any APM agent) doesn't have a chance to hook into the modules.,0.098,0.0,0.902,-0.1877
elasticapm,The  current recommendations  from the Nuxt team appears to be,0.0,0.0,1.0,0.0
elasticapm,Invoke  nuxt  manually via  -r,0.0,0.0,1.0,0.0
elasticapm,Skip  nuxt  and  use NuxtJS programmatically  as a middleware in your framework of choice,0.0,0.0,1.0,0.0
elasticapm,Based on Alan Storm answer (from Nuxt team) I made it work but with a little modification:,0.0,0.0,1.0,0.0
elasticapm,"actually there are many reasons why your app not starting depending on how you setup and configured your ELK stack , but for me I did the following and it's working fine :",0.0,0.071,0.929,0.296
elasticapm,create image from this Dockerfile:,0.0,0.344,0.656,0.2732
elasticapm,run the created image :,0.0,0.4,0.6,0.25
elasticapm,The exception comes from the constructur of the  Kernel32  class which is a class of the Maven coordinate  net.java.dev.jna:jna-platform  which itself depends on  net.java.dev.jna:jna .,0.0,0.0,1.0,0.0
elasticapm,It seems to me like you have to incompatible versions of those dependencies on the class path.,0.0,0.135,0.865,0.3612
elasticapm,I assume that you use version 4 of JNA core and version 5 of JNA platform.,0.0,0.0,1.0,0.0
elasticapm,Upgrade the first or downgrade the latter and the error should disappear.,0.315,0.0,0.685,-0.5574
elasticapm,"Well, as an option, you can use something like that",0.0,0.365,0.635,0.5574
elasticapm,It seems that you are using the oss distribution of elasticsearch but the defaut version of apm.,0.0,0.0,1.0,0.0
elasticapm,upgrade the elasticsearch cluster to the default disto or use this oss apm docker image: docker.elastic.co/apm/apm-server-oss:7.0.1,0.0,0.0,1.0,0.0
elasticapm,OpenTracing   is a set of standard APIs that consistently model and describe the behavior of distributed systems ),0.0,0.0,1.0,0.0
elasticapm,"OpenTracing did not describe how to collect, report, store or represent the data of interrelated traces and spans.",0.0,0.0,1.0,0.0
elasticapm,It is implementation details (such as  jaeger  or  wavefront ).,0.0,0.0,1.0,0.0
elasticapm,jaeger-client-csharp is very jaeger-specific.,0.0,0.0,1.0,0.0
elasticapm,"But there is one exception, called  zipkin  which in turns is not fully OpenTracing compliant, even it has similar terms.",0.0,0.0,1.0,0.0
elasticapm,"If you are OK with  opentracing-contrib/csharp-netcore  (hope you are using this library) then if you want to achieve ""no code change"" (in target microservice) in order to configure tracing subsystem, you should use some plug-in model.",0.056,0.107,0.837,0.2577
elasticapm,"Good news that aspnetcore has concept of  hosted startup assemblies , which allow you to configure tracing system.",0.0,0.242,0.758,0.5859
elasticapm,"So, you can have some library called  JaegerStartup  where you will implement IHostedStartup like follows:",0.0,0.152,0.848,0.3612
elasticapm,"When you decide to switch the tracing system - you need to create another library, which can be loaded automatically, and target microservice code will not be touched.",0.0,0.075,0.925,0.2732
elasticapm,The best way to troubleshoot what is going on is to check if the events from Heartbeat are being collected.,0.0,0.25,0.75,0.7184
elasticapm,"The Uptime application only displays events from Heartbeat, and therefore — this is the Beat that you need to check.",0.0,0.0,1.0,0.0
elasticapm,"First, check the connectivity of Heartbeat and the configured output:",0.0,0.0,1.0,0.0
elasticapm,"Secondly, check if the events are being generated.",0.0,0.0,1.0,0.0
elasticapm,You can check this by commenting out your existing output (Likely Elasticsearc/Elastic Cloud) and enabling either the  Console  output or the  File  output.,0.0,0.0,1.0,0.0
elasticapm,Then start your Metricbeat and check if events are being generated.,0.0,0.0,1.0,0.0
elasticapm,"If they are, then it might be something with the backend side of things; maybe Elasticsearch is rejecting the documents sent and refusing to index them.",0.192,0.0,0.808,-0.6908
elasticapm,"Apropos, Elastic is implementing a native  Jenkins  plugin that allows you to observe your CI pipeline using OpenTelemetry compatible backends such as  Elastic APM .",0.0,0.0,1.0,0.0
elasticapm,You can learn more about this plugin  here .,0.0,0.0,1.0,0.0
elasticapm,"The only way to get configuration is to check apm-server.yml in your instance, but if you want to check your agent configuration you can use Agent Configuration API, for more information check  https://www.elastic.co/guide/en/apm/server/current/agent-configuration-api.html .",0.0,0.043,0.957,0.1154
elasticapm,I can by pass it when I comment entityframework-&gt;Interceptor node in web.config file,0.0,0.0,1.0,0.0
elasticapm,And I can continue after uncomment it,0.0,0.0,1.0,0.0
elasticapm,About the possibility of using some kind of Proxy between your gke cluster and elastic apm.,0.0,0.0,1.0,0.0
elasticapm,"You can check the following link [1], to see if it can fit your necessities.",0.0,0.152,0.848,0.3612
elasticapm,[1]  https://cloud.google.com/vpc/docs/special-configurations#proxyvm,0.0,0.0,1.0,0.0
elasticapm,Since you didn't mention it above: did you instrument a Go application?,0.0,0.0,1.0,0.0
elasticapm,The Elastic APM Go &quot;Agent&quot; is a package which you use to instrument your application source code.,0.0,0.0,1.0,0.0
elasticapm,"It is not an independent process, but runs within your application.",0.0,0.0,1.0,0.0
elasticapm,"So, first (if you haven't already) instrument your application.",0.0,0.0,1.0,0.0
elasticapm,See  https://www.elastic.co/guide/en/apm/agent/go/current/getting-started.html#instrumenting-source,0.0,0.0,1.0,0.0
elasticapm,"Here's an example web server using  Echo , and the  apmechov4  instrumentation module:",0.0,0.0,1.0,0.0
elasticapm,"If you run that and send some requests to  http://localhost:8080/hello/world , you should soon see requests in the APM app in Kibana.",0.0,0.0,1.0,0.0
elasticapm,"If you still don't see anything in Kibana, you can follow  https://www.elastic.co/guide/en/apm/agent/go/current/troubleshooting.html#agent-logging  to enable logging.",0.0,0.0,1.0,0.0
elasticapm,Here's what you can expect to see if the agent is able to successfully send data to the server:,0.0,0.151,0.849,0.4939
elasticapm,"If on the other hand the server is inaccessible, you would see something like this:",0.0,0.305,0.695,0.6908
elasticapm,"Sorry my bad, the server urls weren't correctly passed to docker.",0.348,0.0,0.652,-0.5859
elasticapm,"This is by design in Django, and it is intentionally designed in this way.",0.0,0.0,1.0,0.0
elasticapm,this is a parametrized way.,0.0,0.0,1.0,0.0
elasticapm,suppose someone has a column name with spaces like  test column name  then think what would happen.,0.0,0.143,0.857,0.3612
elasticapm,"it will lead to some unwanted errors, so don't change the underlying logic of the framework.",0.235,0.0,0.765,-0.5106
elasticapm,"Thanks @BjarniRagnarsson, The upper case letters was making this behavior of framework as @Sanjay mentioned.",0.0,0.172,0.828,0.4404
elasticapm,Solution:,0.0,1.0,0.0,0.3182
elasticapm,"For a high-level overview type of information, have a look at  Elastic Stack Monitoring .",0.0,0.0,1.0,0.0
elasticapm,"If you want to look at any monitoring in more detail, have a look at the  monitoring APIs themselves .",0.0,0.071,0.929,0.0772
elasticapm,"If you want to log this sort of information, you should set thresholds  for your Elasticsearch slow log .",0.0,0.071,0.929,0.0772
elasticapm,"If you want to index and then view data from the slow log,  you can always use Filebeat to ingest that slow log data back into Elasticsearch .",0.0,0.048,0.952,0.0772
elasticapm,You're calling one method from another.,0.0,0.0,1.0,0.0
elasticapm,Spring is creating a proxy around your method.,0.0,0.268,0.732,0.296
elasticapm,If you call one method from another from the same class then you're not going through the proxy.,0.0,0.0,1.0,0.0
elasticapm,Extract the method annotated with new span to a separate class and it will work fine.,0.0,0.114,0.886,0.2023
jaeger,"After searching a solution for some time, I found a docker-compose.yml file which had the Jaeger Query,Agent,collector and Elasticsearch configurations.",0.0,0.126,0.874,0.3182
jaeger,docker-compose.yml,0.0,0.0,1.0,0.0
jaeger,"The docker-compose.yml file installs the elasticsearch, Jaeger collector,query and agent.",0.0,0.0,1.0,0.0
jaeger,"Install docker and docker compose first
 https://docs.docker.com/compose/install/#install-compose",0.0,0.0,1.0,0.0
jaeger,"Then, execute these commands in order",0.0,0.0,1.0,0.0
jaeger,"start all the docker containers - Jaeger agent,collector,query and elasticsearch.",0.0,0.0,1.0,0.0
jaeger,sudo docker start container-id,0.0,0.0,1.0,0.0
jaeger,access -   http://localhost:16686/,0.0,0.0,1.0,0.0
jaeger,"If Jaeger needs to be set up in Kubernetes cluster as a helm chart, one can use this:  https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger 
It can delploy either Elasticsearch or Cassandara as a storage backend.",0.0,0.0,1.0,0.0
jaeger,Which is just a matter of right value being passed in to the chart:,0.0,0.241,0.759,0.3612
jaeger,"This section shows the helm command as an example:
 https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger#installing-the-chart-using-a-new-elasticsearch-cluster",0.0,0.0,1.0,0.0
jaeger,If you would like to deploy the Jaeger with Elasticsearch and Kibana to quickly validate and check the stack e.g.,0.0,0.217,0.783,0.6124
jaeger,"in kind or Minikube, the following snippet may help you.",0.0,0.433,0.567,0.7269
jaeger,"For people who are using OpenTelemetry, Jaeger, and Elasticsearch, here is the way.",0.0,0.0,1.0,0.0
jaeger,Note the image being used are  jaegertracing/jaeger-opentelemetry-collector  and  jaegertracing/jaeger-opentelemetry-agent .,0.0,0.0,1.0,0.0
jaeger,Then just need,0.0,0.0,1.0,0.0
jaeger,Reference:  https://github.com/jaegertracing/jaeger/blob/master/crossdock/jaeger-opentelemetry-docker-compose.yml,0.0,0.0,1.0,0.0
jaeger,"As I mentioned in my comment on the OP's first answer above, I was getting an error when running the docker-compose exactly as given:",0.114,0.0,0.886,-0.4019
jaeger,Error: unknown flag: --collector.host-port,0.474,0.0,0.526,-0.4019
jaeger,I think this CLI flag has been deprecated by the Jaeger folks since that answer was written.,0.0,0.0,1.0,0.0
jaeger,So I poked around in the jaeger-agent documentation a bit:,0.0,0.0,1.0,0.0
jaeger,And I got this to work with a couple of small modifications:,0.0,0.0,1.0,0.0
jaeger,The updated docker-compose.yaml:,0.0,0.0,1.0,0.0
jaeger,https://github.com/opentracing-contrib/java-spring-cloud  project automatically sends standard logging to the active span.,0.0,0.231,0.769,0.4019
jaeger,Just add the following dependency to your pom.xml,0.0,0.0,1.0,0.0
jaeger,Or use this  https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core  starter if you want only logging integration.,0.0,0.115,0.885,0.0772
jaeger,Then I use  opentracing-spring-jaeger-cloud-starter,0.0,0.0,1.0,0.0
jaeger,"I got just one line in console with current trace and span
i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - my_method",0.0,0.0,1.0,0.0
jaeger,Then I use  spring-cloud-starter-sleuth,0.0,0.0,1.0,0.0
jaeger,"I got the Trace and Spans like [my-service,90e1114e35c897d6,90e1114e35c897d6,false] in each line and it's helpfull for filebeat in ELK",0.0,0.135,0.865,0.3612
jaeger,How could I get the same log in console using opentracing-spring-jaeger-cloud-starter ?,0.0,0.0,1.0,0.0
jaeger,my opentracing config,0.0,0.0,1.0,0.0
jaeger,Here is what I did to make jdbc related logs from Logback (Slf4j) write into Jaeger server:,0.0,0.0,1.0,0.0
jaeger,Beginning with Logback config (logback-spring.xml):,0.0,0.0,1.0,0.0
jaeger,Here is my appender:,0.0,0.0,1.0,0.0
jaeger,I was facing similar issue.,0.0,0.0,1.0,0.0
jaeger,ConnectionInfo was getting traced but not the SQL statements.,0.0,0.0,1.0,0.0
jaeger,"In my case, I had to enable traceWithActiveSpanOnly=true.",0.0,0.0,1.0,0.0
jaeger,"For e.g, : jdbc:tracing:h2:mem:test?traceWithActiveSpanOnly=true",0.0,0.0,1.0,0.0
jaeger,After that the statements started getting traced.,0.0,0.0,1.0,0.0
jaeger,Check the documentation of opentracing java-jdbc module here,0.0,0.0,1.0,0.0
jaeger,"Based on my experience and reading online, I found this interesting line in Istio  mixer faq",0.0,0.182,0.818,0.481
jaeger,"Mixer trace generation is controlled by command-line flags: trace_zipkin_url, trace_jaeger_url, and trace_log_spans.",0.0,0.0,1.0,0.0
jaeger,"If any of those flag values are set, trace data will be written directly to those locations.",0.0,0.144,0.856,0.4019
jaeger,"If no tracing options are provided, Mixer will not generate any application-level trace information.",0.145,0.0,0.855,-0.296
jaeger,"Also, if you go deep into mixer  helm chart , you will find traces of Zipkin and Jaeger signifying that it’s mixer that is passing trace info to Jaeger.",0.0,0.0,1.0,0.0
jaeger,I also got confused which reading this line in one of the articles,0.173,0.0,0.827,-0.3182
jaeger,Istio injects a sidecar proxy (Envoy) in the pod in which your application container is running.,0.0,0.0,1.0,0.0
jaeger,This sidecar proxy transparently intercepts (iptables magic) all network traffic going in and out of your application.,0.0,0.0,1.0,0.0
jaeger,"Because of this interception, the sidecar proxy is in a unique position to automatically trace all network requests (HTTP/1.1, HTTP/2.0 &amp; gRPC).",0.0,0.0,1.0,0.0
jaeger,"On Istio mixer documentation, The Envoy sidecar logically calls Mixer before each request to perform precondition checks, and after each request to report telemetry.",0.0,0.0,1.0,0.0
jaeger,The sidecar has local caching such that a large percentage of precondition checks can be performed from cache.,0.0,0.0,1.0,0.0
jaeger,"Additionally, the sidecar buffers outgoing telemetry such that it only calls Mixer infrequently.",0.0,0.155,0.845,0.296
jaeger,Update:  You can enable tracing to understand what happens to a request in Istio and also the role of mixer and envoy.,0.0,0.0,1.0,0.0
jaeger,Read more information  here,0.0,0.0,1.0,0.0
jaeger,"I found the solution to my problem, in case anybody is facing similar issues.",0.169,0.144,0.688,-0.1027
jaeger,"I was missing the environment variable  JAEGER_SAMPLER_MANAGER_HOST_PORT , which is necessary if the (default) remote controlled sampler is used for tracing.",0.109,0.0,0.891,-0.296
jaeger,This is the working docker-compose file:,0.0,0.0,1.0,0.0
jaeger,For anyone finding themselves on this page looking at a simialr issue for jaeger opentracing in .net core below is a working docker compose snippet and working code in Startup.cs.,0.0,0.0,1.0,0.0
jaeger,The piece I was missing was getting jaeger to read the environment variables from docker-compose as by default it was trying to send the traces over udp on localhost.,0.075,0.0,0.925,-0.296
jaeger,Add following nuget packages to your api's.,0.0,0.0,1.0,0.0
jaeger,"Then inside your Startup.cs Configure method, you will need to configure jaeger and opentracing as below.",0.0,0.0,1.0,0.0
jaeger,The jaeger-agent service should look like,0.0,0.333,0.667,0.3612
jaeger,"Don't use IP, use FQDN.",0.0,0.0,1.0,0.0
jaeger,"First, try to hardcode value for  jaegerHost",0.0,0.286,0.714,0.34
jaeger,"where  jaeger-agent  - service name,  jaeger  - namespace of service",0.0,0.0,1.0,0.0
jaeger,"Also, you should create  jaeger-collector  service",0.0,0.296,0.704,0.2732
jaeger,Are you closing the tracer and the scope?,0.0,0.0,1.0,0.0
jaeger,"If you are using a version before 0.32.0, you should manually call  tracer.close()  before your process terminates, otherwise the spans in the buffer might not get dispatched.",0.0,0.0,1.0,0.0
jaeger,"As for the scope, it's common to wrap it in a try-with-resources statement:",0.0,0.0,1.0,0.0
jaeger,You might also want to check the OpenTracing tutorial at  https://github.com/yurishkuro/opentracing-tutorial  or the Katacoda-based version at  https://www.katacoda.com/courses/opentracing,0.0,0.075,0.925,0.0772
jaeger,-- EDIT,0.0,0.0,1.0,0.0
jaeger,and is deployed on a different hostname and port,0.0,0.0,1.0,0.0
jaeger,Then you do need to tell the tracer where to send the traces.,0.0,0.0,1.0,0.0
jaeger,"Either export the  JAEGER_ENDPOINT  environment variable, pointing to a collector endpoint, or set  JAEGER_AGENT_HOST / JAEGER_AGENT_PORT , with the location of the agent.",0.0,0.0,1.0,0.0
jaeger,You can check the available environment variables for your client on the following URL:  https://www.jaegertracing.io/docs/1.7/client-features/,0.0,0.0,1.0,0.0
jaeger,You need to add some more properties to your config options.,0.0,0.0,1.0,0.0
jaeger,For reporter deployed on localhost and local sampler strategy :,0.0,0.0,1.0,0.0
jaeger,Replace  localhost  by server or route name to target another host for Jeager runtime.,0.0,0.0,1.0,0.0
jaeger,This link ( https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/ ) provides the details of how to enable jaeger traces.,0.0,0.0,1.0,0.0
jaeger,The simplest way to enable jaeger to spring-boot application is add the dependency and the required properties.,0.0,0.0,1.0,0.0
jaeger,Dependency:,0.0,0.0,1.0,0.0
jaeger,Example Properties,0.0,0.0,1.0,0.0
jaeger,Answering to your question about dependencies it is explained here in Dependencies section ( https://github.com/opentracing-contrib/java-spring-jaeger ):,0.167,0.0,0.833,-0.4215
jaeger,"The opentracing-spring-jaeger-web-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-web-starter This means that by including it, simple web Spring Boot microservices include all the necessary dependencies to instrument Web requests / responses and send traces to Jaeger.",0.0,0.0,1.0,0.0
jaeger,"The opentracing-spring-jaeger-cloud-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-cloud-starter This means that by including it, all parts of the Spring Cloud stack supported by Opentracing will be instrumented",0.0,0.071,0.929,0.3182
jaeger,And by the way:,0.0,0.0,1.0,0.0
jaeger,same as:,0.0,0.0,1.0,0.0
jaeger,You can use  Jaeger Operator  to deploy Jaeger on kubernetes.The Jaeger Operator is an implementation of a Kubernetes Operator.,0.0,0.0,1.0,0.0
jaeger,Operators are pieces of software that ease the operational complexity of running another piece of software.,0.0,0.143,0.857,0.3612
jaeger,"More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application",0.0,0.0,1.0,0.0
jaeger,Follow this link for steps to deploy JAEGER on kubernetes .,0.0,0.0,1.0,0.0
jaeger,https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/,0.0,0.0,1.0,0.0
jaeger,make following changes in application.properties,0.0,0.0,1.0,0.0
jaeger,You can use this link for a better understanding -  https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/,0.0,0.266,0.734,0.4404
jaeger,You add  opentracing-spring-jaeger-starter   library  into the project which simply contains the code needed to provide a Jaeger implementation of the OpenTracing's  io.opentracing.Tracer  interface.,0.0,0.0,1.0,0.0
jaeger,Since you have the jaeger deployed in Kubernetes and exposed it via a loadbalancer service you can use the loadbalancer IP and port to connect to it from outside the Kubernetes cluster.,0.042,0.0,0.958,-0.0772
jaeger,"So the solution that works for me is -
I have made the following changes in my application.properties file of application",0.0,0.125,0.875,0.3774
jaeger,You should config:,0.0,0.0,1.0,0.0
jaeger,"This is why tracing is a useful tool, it often reveals issues like this that you wouldn't suspect otherwise.",0.0,0.327,0.673,0.7421
jaeger,"If your application is using async framework, these gaps may indicate execution waiting on available threads.",0.0,0.0,1.0,0.0
jaeger,Or your application may be CPU throttled during and between the spans.,0.0,0.0,1.0,0.0
jaeger,"You cannot really explain the gaps from the trace itself, but you surely do have them.",0.0,0.204,0.796,0.5927
jaeger,Time to whip out the profiler.,0.0,0.0,1.0,0.0
jaeger,"Turns out that  Feign  clients are  currently not supported  or to be precise, the spring startes do not configure the Feign clients accordingly.",0.082,0.0,0.918,-0.2411
jaeger,"If you want to use Jaeger with your Feign clients, you have to provide an integration of your own.",0.0,0.067,0.933,0.0772
jaeger,"In my experience so far, the jaeger community is a lesser supportive one, thus you have to gain such knowledge on your own, which in my opinion is a big downside and you should probably consider, using an alternative.",0.048,0.135,0.817,0.5574
jaeger,"In some cases it might be necessary to explicitly expose the  Feign Client  in the Spring configuration, in order to get the traceId propagated.",0.065,0.0,0.935,-0.1531
jaeger,This can be done easily by adding the following into one of your configuration classes,0.0,0.146,0.854,0.34
jaeger,The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT!,0.0,0.219,0.781,0.6696
jaeger,Jaegar has the ability to collect Zipkin spans:,0.0,0.247,0.753,0.3182
jaeger,https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin,0.0,0.0,1.0,0.0
jaeger,You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.,0.079,0.084,0.837,0.0258
jaeger,The above will send Zipkin spans to  http://localhost:9411  by default.,0.0,0.0,1.0,0.0
jaeger,You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.,0.0,0.103,0.897,0.34
jaeger,spring.zipkin.base-url= http://your-jaegar-server:9411,0.0,0.0,1.0,0.0
jaeger,Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.,0.0,0.0,1.0,0.0
jaeger,"In the log4j2.xml file, all you have to mention is",0.0,0.0,1.0,0.0
jaeger,[%X],0.0,0.0,1.0,0.0
jaeger,You can find the sample code here:,0.0,0.0,1.0,0.0
jaeger,https://github.com/anoophp777/spring-webflux-jaegar-log4j2,0.0,0.0,1.0,0.0
jaeger,Some web frameworks return empty string if a non-existent header is queried.,0.153,0.0,0.847,-0.2023
jaeger,I have seen this in Spring Boot and KoaJS.,0.0,0.0,1.0,0.0
jaeger,"If any of the tracing headers is not sent by Istio, this header logic causes us to send empty string for those non-existent headers which breaks tracing.",0.065,0.0,0.935,-0.2023
jaeger,My suggestion is after getting the values for headers filter out the ones with empty string as their values and propogate the remaining ones.,0.064,0.191,0.745,0.5574
jaeger,"Also, do you get an Error Message?",0.31,0.0,0.69,-0.4019
jaeger,"If so, please post it.",0.0,0.393,0.607,0.3804
jaeger,BINGO!,0.0,0.0,1.0,0.0
jaeger,We need to setup the ReporterConfigurations as below.,0.0,0.0,1.0,0.0
jaeger,previously my ones were default ones that's why it always connected to local.,0.0,0.0,1.0,0.0
jaeger,"Even better, you can create the Configuration from Environment as below providing the environment variables as below",0.0,0.25,0.75,0.6124
jaeger,You can provide this when you run the docker container,0.0,0.0,1.0,0.0
jaeger,"-e JAVA_OPTS=""",0.0,0.0,1.0,0.0
jaeger,""" ....",0.0,0.0,1.0,0.0
jaeger,Step 1 : First we need to configure remote host address and port.,0.0,0.0,1.0,0.0
jaeger,"Step 2 : configure sender configuration and pass the remote host and port in                           withAgentHost, withAgentPort.",0.0,0.0,1.0,0.0
jaeger,"SenderConfiguration senderConfig = Configuration.SenderConfiguration.fromEnv()
                    .withAgentHost(JAEGER_HOST)
                    .withAgentPort(JAEGER_PORT);",0.0,0.0,1.0,0.0
jaeger,Step 3 : Pass sender configuration in reporter configuration,0.0,0.0,1.0,0.0
jaeger,"Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv()
                .withLogSpans(true)
                .withSender(senderConfig);",0.0,0.0,1.0,0.0
jaeger,This seems a bit old and it's a bit hard to tell what's wrong.,0.31,0.0,0.69,-0.5423
jaeger,"My first guess would be the sampling strategy, as Jaeger samples one trace in one thousand, but looks like you did set it.",0.0,0.129,0.871,0.5023
jaeger,"
JAEGER_SAMPLER_TYPE=const
JAEGER_SAMPLER_PARAM=1",0.0,0.0,1.0,0.0
jaeger,I would recommend you start by using a simple  Configuration.fromEnv().getTracer()  to get your tracer.,0.0,0.185,0.815,0.3612
jaeger,"Then, control it via env vars, probably setting  JAEGER_REPORTER_LOG_SPANS  to  true .",0.0,0.219,0.781,0.4215
jaeger,"With this option, you should be able to see in the logs whenever Jaeger emits a span.",0.0,0.0,1.0,0.0
jaeger,"You can also set the  --log-level=debug  option to the agent and collector (or all-in-one, in your case) to see when these components receive a span from a client.",0.0,0.0,1.0,0.0
jaeger,You can see that jaeger-query configuration includes: SPAN_STORAGE_TYPE: &quot;kafka&quot;,0.0,0.0,1.0,0.0
jaeger,"The error indicates that a kafka client used by jaeger-query to store spans in Kafka cannot in fact reach Kafka, and therefore the jaeger storage factory fails to initialize.",0.208,0.0,0.792,-0.6782
jaeger,This can be either because Kafka failed to start (did you check)?,0.231,0.0,0.769,-0.5106
jaeger,Or a misconfig of the network in your docker.,0.0,0.0,1.0,0.0
jaeger,I was missing a lot of information.,0.355,0.0,0.645,-0.296
jaeger,I managed to get it working:,0.0,0.0,1.0,0.0
jaeger,Update :,0.0,0.0,1.0,0.0
jaeger,I got the same exception ( Tracer bean is not configured!.. ),0.0,0.0,1.0,0.0
jaeger,when I use your version of spring cloud jaeger dependencies.,0.0,0.0,1.0,0.0
jaeger,This is irrespective of the  RxJava  package.,0.0,0.0,1.0,0.0
jaeger,I think you can directly use  opentracing-spring-jaeger-cloud-starter  which is a combination of  opentracing-spring-cloud-starter  and  opentracing-spring-jaeger-starter .,0.0,0.0,1.0,0.0
jaeger,Read  this details  for java spring jaeger.,0.0,0.0,1.0,0.0
jaeger,"The opentracing-spring-jaeger-cloud-starter starter is convenience
starter that includes both opentracing-spring-jaeger-starter and
opentracing-spring-cloud-starter This means that by including it, all
parts of the Spring Cloud stack supported by Opentracing will be
instrumented",0.0,0.071,0.929,0.3182
jaeger,Note :,0.0,0.0,1.0,0.0
jaeger,Maybe RxJava tracing won't work without registering the tracer using the decorators provided by  opentracing-contrib .,0.0,0.0,1.0,0.0
jaeger,Please see the working app  here .,0.0,0.315,0.685,0.3182
jaeger,I have followed  this spring guide  for Reactive Restful webservice and  jaeger  worked with below  pom.xml  without any  Tracer bean  -,0.0,0.122,0.878,0.3612
jaeger,"Thing is, you should use opentelemetry collector if you use opentelemetry exporter.",0.0,0.0,1.0,0.0
jaeger,Pls see schema in attachment,0.0,0.538,0.462,0.3612
jaeger,"Also I created a gist, which will help you to setup
pls see",0.0,0.429,0.571,0.6124
jaeger,https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f,0.0,0.0,1.0,0.0
jaeger,"(just tune the values, export to jaeger-all-in-one instead of separate + cassandra, etc)",0.0,0.197,0.803,0.4019
jaeger,From the official FAQ ( https://www.jaegertracing.io/docs/latest/faq/#do-i-need-to-run-jaeger-agent ):,0.359,0.0,0.641,-0.4215
jaeger,jaeger-agent  is not always necessary.,0.0,0.0,1.0,0.0
jaeger,Jaeger client libraries can be configured to export trace data directly to  jaeger-collector .,0.0,0.0,1.0,0.0
jaeger,"However, the following are the reasons why running  jaeger-agent  is recommended:",0.0,0.153,0.847,0.2023
jaeger,If you check the your  base image  it from scratch.,0.0,0.0,1.0,0.0
jaeger,"So there is no  Bash, ash  as the image is from scratch so it will only cotnain  hotrod-linux .",0.127,0.0,0.873,-0.3535
jaeger,"To get sh or bash in such cases you need to use multi-stage Dockerfile, you can use the base image in Dockerfile and then copy the binaries from the base image in multi-stage in Dockerfile.",0.0,0.0,1.0,0.0
jaeger,Here you go,0.0,0.0,1.0,0.0
jaeger,"so now you can build and test and you will able to run command inside container using docker exec, here is the example",0.0,0.0,1.0,0.0
jaeger,https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/Dockerfile,0.0,0.0,1.0,0.0
jaeger,As I can see hotrod image was built from scratch image.,0.0,0.0,1.0,0.0
jaeger,And from the docker hub:,0.0,0.0,1.0,0.0
jaeger,"""an explicitly empty image, especially for building images ""FROM
  scratch""...",0.167,0.0,0.833,-0.2023
jaeger,"""This image is most useful in the context of building base images
  (such as debian and busybox) or super minimal images (that contain
  only a single binary and whatever it requires, such as hello-world).""",0.0,0.186,0.814,0.796
jaeger,https://hub.docker.com/_/scratch,0.0,0.0,1.0,0.0
jaeger,"So, I think there is not bash inside this image",0.0,0.0,1.0,0.0
jaeger,"'reporting_host' must be not 'localhost', but 'jaeger', just as service in docker-compose.yml called.",0.0,0.0,1.0,0.0
jaeger,"'reporting_host' =&gt; 'jaeger',",0.0,0.0,1.0,0.0
jaeger,"Also, I needed to add  $tracer-&gt;flush();  after all, it closes all the entities and does sending via UDP behind the scenes.",0.0,0.0,1.0,0.0
jaeger,"So, answering my own question.",0.0,0.0,1.0,0.0
jaeger,Jaeger does not support cross system spans.,0.273,0.0,0.727,-0.3089
jaeger,Every sub-system is responsible for its own span in the whole system.,0.0,0.173,0.827,0.3182
jaeger,"For reference, check this answer  https://github.com/opentracing/specification/issues/143",0.0,0.0,1.0,0.0
jaeger,"If anyone else would like to set up Jaeger in spring project, here's what I did:",0.0,0.152,0.848,0.3612
jaeger,Add dependencies to pom:,0.0,0.0,1.0,0.0
jaeger,Set up you web.xml to register new tracing filter  tracingFilter  to intercept REST API:,0.0,0.0,1.0,0.0
jaeger,Register jaeger tracer in spring mvc:,0.0,0.0,1.0,0.0
jaeger,Set up the  tracingFilter  bean we described in web.xml:,0.0,0.0,1.0,0.0
jaeger,Finally define jaeger tracer spring configuration:,0.0,0.0,1.0,0.0
jaeger,"I have got following gradle dependencies working,",0.0,0.0,1.0,0.0
jaeger,"Following tracer bean configuration,",0.0,0.0,1.0,0.0
jaeger,And then the spans can be recorded as,0.0,0.0,1.0,0.0
jaeger,Here is the working example you can refer  https://github.com/krushnatkhawale/jaeger-with-spring-boot-web-app,0.0,0.0,1.0,0.0
jaeger,So did you install direct  or created a yaml from the templates ?,0.0,0.167,0.833,0.25
jaeger,"I would run the command you used to install but with template function and then add the options for jaeger,Kiali and grafana.",0.0,0.0,1.0,0.0
jaeger,Here is  howto : from official repository.,0.0,0.0,1.0,0.0
jaeger,you need to update  values.yaml .,0.0,0.0,1.0,0.0
jaeger,"and turn on  grafana,  kiali and jaeger.",0.0,0.0,1.0,0.0
jaeger,For example with kiali change:,0.0,0.0,1.0,0.0
jaeger,to,0.0,0.0,1.0,0.0
jaeger,than rebuild the Helm dependencies:,0.0,0.0,1.0,0.0
jaeger,than upgrade your istio inside kubernetes:,0.0,0.0,1.0,0.0
jaeger,"that's it, hope it was helpful",0.0,0.588,0.412,0.6908
jaeger,Scalabilty is dependent on sampling frequency and volumes.,0.0,0.0,1.0,0.0
jaeger,The agent supports adaptive sampling which is a feedback loop from the collector to your instrumented app.,0.0,0.143,0.857,0.3612
jaeger,You can statically define this up front in your instrumentation but you lose the adaptive features.,0.191,0.0,0.809,-0.5499
jaeger,It is possible to bypass agent all together and send metrics directly to collector.,0.0,0.0,1.0,0.0
jaeger,Just define variable JAEGER_ENDPOINT in your app running environment.,0.0,0.0,1.0,0.0
jaeger,This behaviour is documented but buried down in the Jager git repo:,0.0,0.0,1.0,0.0
jaeger,https://github.com/jaegertracing/jaeger-client-java/blob/master/jaeger-core/README.md,0.0,0.0,1.0,0.0
jaeger,Would a single agent colocated with a single collector be possible in a Jaeger deployment?,0.0,0.0,1.0,0.0
jaeger,"It's possible, and that's how the  ""all-in-one""  image works.",0.0,0.0,1.0,0.0
jaeger,Would it be advisable?,0.0,0.0,1.0,0.0
jaeger,Depends on your architecture.,0.0,0.0,1.0,0.0
jaeger,"If you don't expect your Jaeger infra to grow, using the all-in-one is easier from the maintenance perspective.",0.0,0.141,0.859,0.4215
jaeger,"If you need your Jaeger infra to be highly available, then you probably want to place your agents closer to your instrumented applications than to your collector and scale the collectors separately.",0.0,0.04,0.96,0.0772
jaeger,More about the Jaeger Agent is discussed in the following blog posts:,0.0,0.0,1.0,0.0
jaeger,"Running Jaeger Agent on bare metal 
 Deployment strategies for the Jaeger Agent",0.0,0.0,1.0,0.0
jaeger,Is it possible to skip the agent altogether and submit spans directly to the collector over HTTP?,0.0,0.0,1.0,0.0
jaeger,"For some clients (Java, NodeJS and C#), yes.",0.0,0.278,0.722,0.4019
jaeger,Look for the  JAEGER_ENDPOINT  option .,0.0,0.0,1.0,0.0
jaeger,"You can turn on the debugging in the client by setting the option  JAEGER_REPORTER_LOG_SPANS  to true (or use the related option in the  ReporterConfiguration , as it seems that's how you are using it).",0.0,0.08,0.92,0.4215
jaeger,https://www.jaegertracing.io/docs/1.8/client-features/,0.0,0.0,1.0,0.0
jaeger,"Once you confirm the traces are being generated and sent to the agent, set the log-level in the agent to  debug :",0.0,0.0,1.0,0.0
jaeger,"
docker run -p... jaegertracing/jaeger-agent:1.8 --log-level=debug",0.0,0.0,1.0,0.0
jaeger,"If you don't see anything in the logs there indicating that the agent received a span (or span batch), then you might need to configure your client with the Agent's address ( JAEGER_AGENT_HOST  and  JAEGER_AGENT_PORT , or related options in the  Configuration  object).",0.0,0.0,1.0,0.0
jaeger,"You mentioned that you are deploying in Azure AKS, so, I guess that the agent isn't available at  localhost , which is the default location where the client sends the spans.",0.0,0.0,1.0,0.0
jaeger,"Typically, the agent would be deployed as a sidecar in such a scenario:",0.0,0.0,1.0,0.0
jaeger,https://github.com/jaegertracing/jaeger-kubernetes#deploying-the-agent-as-sidecar,0.0,0.0,1.0,0.0
jaeger,The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT!,0.0,0.219,0.781,0.6696
jaeger,Jaegar has the ability to collect Zipkin spans.,0.0,0.247,0.753,0.3182
jaeger,https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin,0.0,0.0,1.0,0.0
jaeger,You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.,0.079,0.084,0.837,0.0258
jaeger,The above will send Zipkin spans to http://localhost:9411 by default.,0.0,0.0,1.0,0.0
jaeger,You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.,0.0,0.103,0.897,0.34
jaeger,Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.,0.0,0.0,1.0,0.0
jaeger,"In the  log4j2.xml  file, all you have to mention is",0.0,0.0,1.0,0.0
jaeger,I'll be uploading a working example of this approach into my GitHub and sharing the link.,0.0,0.167,0.833,0.4215
jaeger,EDIT 1:,0.0,0.0,1.0,0.0
jaeger,You can find the sample code here:,0.0,0.0,1.0,0.0
jaeger,https://github.com/anoophp777/spring-webflux-jaegar-log4j2,0.0,0.0,1.0,0.0
jaeger,The Jaeger helm chart is now available  here .,0.0,0.0,1.0,0.0
jaeger,You need to add the helm repo first using the following:,0.0,0.0,1.0,0.0
jaeger,This can be installed with:,0.0,0.0,1.0,0.0
jaeger,"Yes you can, and I have shown that numerous times during my presentations ( https://toomuchcoding.com/talks ) and we describe it extensively in the documentation ( https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/ ).",0.0,0.109,0.891,0.4019
jaeger,Sleuth will set up your logging pattern which you can then parse and visualize using the ELK stack.,0.0,0.0,1.0,0.0
jaeger,Sleuth takes care of tracing context propagation and can send the spans to a span storage (e.g.,0.0,0.176,0.824,0.4939
jaeger,Zipkin or Jaeger).,0.0,0.0,1.0,0.0
jaeger,Sleuth does take care of updating the MDC for you.,0.0,0.262,0.738,0.4939
jaeger,Please always read the documentation and the project page before filing a question,0.0,0.173,0.827,0.3182
jaeger,The solution for this one is to simply increase the memory size in the  istio-config.yaml  file.,0.0,0.247,0.753,0.5574
jaeger,"in my case, I'm updating the PVC and it looks like it's already filled with data and decreasing it wasn't an option for istio, so I increased it in the config file instead:",0.0,0.149,0.851,0.6418
jaeger,Based on following example from  jaeger docs :,0.0,0.0,1.0,0.0
jaeger,and on example  cli falgs :,0.0,0.0,1.0,0.0
jaeger,I infere that you should be able to do the following:,0.0,0.0,1.0,0.0
jaeger,Notice that I split the cli options with the dot and added it as a nested fields in yaml.,0.0,0.0,1.0,0.0
jaeger,Do the same to other parameters by analogy.,0.0,0.0,1.0,0.0
jaeger,"See the answer on this  jaeger issue , you will need to query elastic search or the source where the data is stored.",0.0,0.0,1.0,0.0
jaeger,"Alternatively, you should raise an issue on  jaeger-ui  detailing your case.",0.0,0.0,1.0,0.0
jaeger,"When you open an individual trace in the Jaeger UI, there is a View dropdown in the top right corner.",0.0,0.091,0.909,0.2023
jaeger,One of the options is to view/download the given trace as a JSON file.,0.0,0.0,1.0,0.0
jaeger,"You can also programmatically query the Jaeger query service via JSON/Protobuf API, but those endpoints will not result in a data format that you can load back into the UI.",0.0,0.0,1.0,0.0
jaeger,https://www.jaegertracing.io/docs/latest/apis/#trace-retrieval-apis,0.0,0.0,1.0,0.0
jaeger,"OK, I figured out the issue here which may be obvious to those with more expertise.",0.0,0.173,0.827,0.4466
jaeger,The guide I linked to above that describes how to make an Ingress spec for gRPC  is specific to NGINX.,0.0,0.0,1.0,0.0
jaeger,"Meanwhile, I am using K3S which came out of the box with Traefik as the Ingress Controller.",0.0,0.0,1.0,0.0
jaeger,"Therefore, the annotations I used in my Ingress spec had no affect:",0.18,0.0,0.82,-0.296
jaeger,So I found  another Stack Overflow post discussing Traefik and gRPC  and modified my original Ingress spec above a bit to include the annotations mentioned there:,0.0,0.091,0.909,0.3182
jaeger,These are the changes I made:,0.0,0.0,1.0,0.0
jaeger,Hopefully this helps someone else running into this same confusion.,0.152,0.366,0.483,0.4767
jaeger,"It's not clear in the documentation, but I managed to get it working by providing the  SPAN_STORAGE_TYPE  and the respective connection details to allow the jaeger components to talk to the storage running outside of the all-in-one container.",0.038,0.145,0.816,0.666
jaeger,"For instance, I'm running elasticsearch on my Mac, so I used the following command to run all-in-one:",0.0,0.0,1.0,0.0
jaeger,According to istio  documentation,0.0,0.0,1.0,0.0
jaeger,"To see trace data, you must send requests to your service.",0.0,0.0,1.0,0.0
jaeger,The number of requests depends on Istio’s sampling rate.,0.0,0.14,0.86,0.0772
jaeger,You set this rate when you install Istio.,0.0,0.0,1.0,0.0
jaeger,The default sampling rate is 1%.,0.0,0.0,1.0,0.0
jaeger,You need to send at least 100 requests before the first trace is visible.,0.0,0.0,1.0,0.0
jaeger,Could you try to send at least 100 requests and check if it works?,0.0,0.0,1.0,0.0
jaeger,If you wan't to change the default sampling rate then there is istio  documentation  about that.,0.0,0.0,1.0,0.0
jaeger,Customizing Trace sampling,0.0,0.0,1.0,0.0
jaeger,The sampling rate option can be used to control what percentage of requests get reported to your tracing system.,0.0,0.0,1.0,0.0
jaeger,This should be configured depending upon your traffic in the mesh and the amount of tracing data you want to collect.,0.0,0.061,0.939,0.0772
jaeger,The default rate is 1%.,0.0,0.0,1.0,0.0
jaeger,"To modify the default random sampling to 50, add the following option to your tracing.yaml file.",0.0,0.0,1.0,0.0
jaeger,The sampling rate should be in the range of 0.0 to 100.0 with a precision of 0.01.,0.0,0.0,1.0,0.0
jaeger,"For example, to trace 5 requests out of every 10000, use 0.05 as the value here.",0.0,0.146,0.854,0.34
jaeger,After digging around in the OpenTracing C# .NET Core source ( https://github.com/opentracing-contrib/csharp-netcore ) I figured out how to override the top level Span.OperationName.,0.0,0.087,0.913,0.2023
jaeger,I had to update my  Startup.ConfigureServices()  call to  services.AddOpenTracing()  to the following:,0.0,0.0,1.0,0.0
jaeger,I have resolved it after configuring port as 14250 as JaegerGrpcSpanExporter internally uses grpc port which has been configured to 14250 for jaeger-collector,0.0,0.075,0.925,0.1779
jaeger,"I was studying Opentracing and Jeager and I've used this tutorial to get familiar with the basic possibilities:
 https://github.com/yurishkuro/opentracing-tutorial/tree/master/java",0.0,0.0,1.0,0.0
jaeger,"If you take a look in the case 1 (Hello World), it explains how to &quot; Annotate the Trace with Tags and Logs &quot;.",0.0,0.0,1.0,0.0
jaeger,"That would answer your questions 1, 2 and 3, as with that you can add all the info that you would like within spans and logs.",0.0,0.094,0.906,0.3612
jaeger,"Here is a snippet from the repository (but I'd recommend checking there, as it has a more detailed explanation):",0.0,0.135,0.865,0.3612
jaeger,"In this case  helloTo  is a variable containing a name, to whom the app will say hello.",0.0,0.0,1.0,0.0
jaeger,It would create a span tag called hello-to with the value that is coming from the execution.,0.0,0.243,0.757,0.5423
jaeger,"Below we have an example for the logs case, where the whole  helloStr  message is added to the logs:",0.0,0.0,1.0,0.0
jaeger,"Regarding the last question, that would be easier, you can use the Jaeger UI to search for the trace you would like, there is a field for that on the top left corner:",0.0,0.197,0.803,0.7269
jaeger,There you go.,0.0,0.0,1.0,0.0
jaeger,There are various overloaded methods as follows,0.0,0.0,1.0,0.0
jaeger,I want to add some fields to span tags so that it's easy to search in JaegerUI.,0.0,0.242,0.758,0.5367
jaeger,"Jaeger API provides  log  method to log multiple fields that needs to be added to a map, the method signature is as follows,",0.0,0.0,1.0,0.0
jaeger,"Span log(Map&lt;String, ?&gt; fields);",0.0,0.0,1.0,0.0
jaeger,eg:,0.0,0.0,1.0,0.0
jaeger,"spanId and traceId are stored in JaegerSpanContext class, which can be obtained from context() method of Span class.",0.0,0.0,1.0,0.0
jaeger,There is a search box in the navigation bar of Jaeger UI where you can search traces by trace ID.,0.0,0.0,1.0,0.0
jaeger,"What you did is for http 1.x, and it doesn't work for http2/grpc.",0.0,0.0,1.0,0.0
jaeger,Please dive into grpc impl in springboot doc.,0.0,0.247,0.753,0.3182
jaeger,Thanks Yuri.,0.0,0.744,0.256,0.4404
jaeger,Yes it was a clock issue.,0.0,0.403,0.597,0.4019
jaeger,"Although the host machine (VM) updated its clock on every unpause, docker for windows did not.",0.0,0.0,1.0,0.0
jaeger,The timezones were correct for all containers but the times were all out by the exact same amount.,0.0,0.0,1.0,0.0
jaeger,This must be the docker internal clock that seems to only get updated once on launch and not at the launch of every new container.,0.0,0.0,1.0,0.0
jaeger,"Although all container clocks were out by the same amount, the windows host machine was correct.",0.0,0.0,1.0,0.0
jaeger,The messages were arriving but the times/dates were outside the time frame window the UI was displaying.,0.0,0.0,1.0,0.0
jaeger,If I set a custom date range I'm sure they would appear.,0.0,0.204,0.796,0.3182
jaeger,The containers must have been out by a few days with the continual stopping and starting and not by just a few hours.,0.074,0.0,0.926,-0.1531
jaeger,Time drift in docker is a known issue on Mac and Windows OS.,0.0,0.0,1.0,0.0
jaeger,Check the date/time in a docker container using this (apologies in advance),0.0,0.0,1.0,0.0
jaeger,or calculate the drift...,0.0,0.0,1.0,0.0
jaeger,Unfortunately there doesnt seem to be a better solution than occasionally restarting the container.,0.136,0.295,0.568,0.4215
jaeger,The problem is that your Jaeger collector is not accessible from outside docker network host as you specified in your docker command.,0.114,0.0,0.886,-0.4019
jaeger,This would only work if your spring boot application is deployed on the host network too.,0.0,0.0,1.0,0.0
jaeger,Try to run Jaeger as follows:,0.0,0.0,1.0,0.0
jaeger,It should trigger.,0.0,0.0,1.0,0.0
jaeger,Have you tried looking at the logs being generated by your pods?,0.0,0.0,1.0,0.0
jaeger,In my case I got the following,0.0,0.0,1.0,0.0
jaeger,"ERROR Failed to flush spans in reporter: error sending spans over UDP:
  Error: getaddrinfo ENOTFOUND  http://jaeger-agent , packet size: 984,
  bytes sent: undefined",0.403,0.0,0.597,-0.9029
jaeger,Changing it to jaeger-agent worked for me.,0.0,0.0,1.0,0.0
jaeger,Also if it helps I have declared this under my jaeger image in docker-compose.yml:,0.0,0.178,0.822,0.3818
jaeger,The answer here is to install istio with  --set values.global.tracer.zipkin.address  as provided in  istio documentation,0.0,0.0,1.0,0.0
jaeger,And,0.0,0.0,1.0,0.0
jaeger,"Use the original TracingService  setting: service: ""zipkin.istio-system:9411""  as Donato Szilagyi confirmed in comments.",0.0,0.161,0.839,0.3182
jaeger,Great!,0.0,1.0,0.0,0.6588
jaeger,It works.,0.0,0.0,1.0,0.0
jaeger,"And this time I used the original TracingService setting: service: ""zipkin.istio-system:9411"" – Donato Szilagy",0.0,0.173,0.827,0.3182
jaeger,"Not sure, but it seems you miss setting the TLS for Cassandra Storage in Azure Cosmos DB.",0.184,0.0,0.816,-0.3359
jaeger,"When you use the Cassandra client to connect the Cassandra Storage in Azure Cosmos DB, it will give out the time out error, but if you enable the SSL, the connection works well.",0.052,0.075,0.873,0.2023
jaeger,So I think you can try to enable the TLS for Cassandra in your values.yaml following the steps in the Github which provided.,0.0,0.0,1.0,0.0
jaeger,A colleague of mine provided the answer...,0.0,0.0,1.0,0.0
jaeger,"It was hidden in the Makefile, which hadn't worked for me as I don't use Golang (and it had been more complex than just installing Golang and running it, but I digress...).",0.0,0.0,1.0,0.0
jaeger,The following .sh will do the trick.,0.167,0.0,0.833,-0.0516
jaeger,"This assumes the query.proto file is a subdirectory from the same location as the script below, under model/proto/api_v2/ (as it appears in the main Jaeger repo).",0.0,0.0,1.0,0.0
jaeger,"This will definitely generate the needed Python file, but it will still be missing dependencies.",0.159,0.105,0.737,-0.2382
jaeger,"Jaeger clients implement so-called  head-based sampling , where a sampling decision is made at the root of the call tree and propagated down the tree along with the trace context.",0.0,0.0,1.0,0.0
jaeger,"This is done to guarantee consistent sampling of all spans of a given trace (or none of them), because we don't want to make the coin flip at every node and end up with partial/broken traces.",0.034,0.055,0.911,0.1969
jaeger,Implementing on-error sampling in the head-based sampling system is not really possible.,0.0,0.0,1.0,0.0
jaeger,"Imaging that your service is calling service A, which returns successfully, and then service B, which returns an error.",0.118,0.14,0.742,0.128
jaeger,Let's assume the root of the trace was not sampled (because otherwise you'd catch the error normally).,0.144,0.0,0.856,-0.4019
jaeger,"That means by the time you know of an error from B, the whole sub-tree at A has been already executed and all spans discarded because of the earlier decision not to sample.",0.145,0.0,0.855,-0.6249
jaeger,The sub-tree at B has also finished executing.,0.0,0.0,1.0,0.0
jaeger,The only thing you can sample at this point is the spans in the current service.,0.0,0.0,1.0,0.0
jaeger,You could also implement a reverse propagation of the sampling decision via response to your caller.,0.0,0.0,1.0,0.0
jaeger,"So in the best case you could end up with a sub-branch of the whole trace sampled, and possible future branches if the trace continues from above (e.g.",0.0,0.147,0.853,0.6666
jaeger,via retries).,0.0,0.0,1.0,0.0
jaeger,"But you can never capture the full trace, and sometimes the reason B failed was because A (successfully) returned some data that caused the error later.",0.267,0.0,0.733,-0.8402
jaeger,"Note that reverse propagation is not supported by the OpenTracing or OpenTelemetry today, but it has been discussed in the last meetings of the W3C Trace Context working group.",0.05,0.0,0.95,-0.1232
jaeger,"The alternative way to implement sampling is with  tail-based sampling , a technique employed by some of the commercial vendors today, such as Lightstep, DataDog.",0.0,0.0,1.0,0.0
jaeger,It is also on the roadmap for Jaeger (we're working on it right now at Uber).,0.0,0.0,1.0,0.0
jaeger,"With tail-based sampling 100% of spans are captured from the application, but only stored in memory in a collection tier, until the full trace is gathered and a sampling decision is made.",0.0,0.0,1.0,0.0
jaeger,"The decision making code has a lot more information now, including errors, unusual latencies, etc.",0.156,0.0,0.844,-0.34
jaeger,"If we decide to sample the trace, only then it goes to disk storage, otherwise we evict it from memory, so that we only need to keep spans in memory for a few seconds on average.",0.0,0.0,1.0,0.0
jaeger,Tail-based sampling imposes heavier performance penalty on the traced applications because 100% of traffic needs to be profiled by tracing instrumentation.,0.188,0.0,0.812,-0.5267
jaeger,"You can read more about head-based and tail-based sampling either in Chapter 3 of my book ( https://www.shkuro.com/books/2019-mastering-distributed-tracing/ ) or in the awesome paper  ""So, you want to trace your distributed system?",0.0,0.167,0.833,0.6597
jaeger,"Key design insights from years of practical experience""  by Raja R. Sambasivan, Rodrigo Fonseca, Ilari Shafer, Gregory R. Ganger ( http://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf ).",0.0,0.0,1.0,0.0
jaeger,"You can bind it to metrics and logging frameworks, but you don't have to.",0.0,0.0,1.0,0.0
jaeger,"You can simply just call  cfg.NewTracer() , like in this example:",0.0,0.217,0.783,0.3612
jaeger,Source:  https://github.com/jaegertracing/jaeger-client-go/blob/3585cc566102e0ea2225177423e3fcc3d2e5fd7a/config/example_test.go#L88-L105,0.0,0.0,1.0,0.0
jaeger,Check the Jaeger Go Client readme for more information on the metrics/logging integration:  https://github.com/jaegertracing/jaeger-client-go,0.0,0.0,1.0,0.0
jaeger,Jaeger clients are designed to have a minimum set of dependencies.,0.0,0.0,1.0,0.0
jaeger,We don't know if your application is using Prometheus metrics or Zap logger.,0.0,0.0,1.0,0.0
jaeger,This is why  jaeger-client-go  (as well as many other Jaeger clients in other languages) provide two lightweight interfaces for a Logger and MetricsFactory that can be implemented for a specific logs/metrics backend that your application is using.,0.0,0.058,0.942,0.2732
jaeger,"Of course, the bindings for Prometheus and Zap are already implemented in the  jaeger-lib  and can be included optionally.",0.0,0.0,1.0,0.0
jaeger,It looks like you have different versions of opentracing.,0.0,0.238,0.762,0.3612
jaeger,"The spring-starter-jaeger version 2.x upgrade the version of opentracing, so you might have introduced this breaking changes when you upgraded the dependency version.",0.0,0.0,1.0,0.0
jaeger,"Unfortunatelly, the reporter interface is used to report FINISHED spans, it is invoked on JaegerSpan.finish.",0.0,0.0,1.0,0.0
jaeger,I presume this is why it does not appear in logs.,0.0,0.0,1.0,0.0
jaeger,"If you are using spring boot with auto configuration, the logs printed using log4j will be instrumented and sent automatically in the span.",0.0,0.0,1.0,0.0
jaeger,"In Go this is not very straightforward, and largely depends on the logging library you use and the interface it provides.",0.0,0.0,1.0,0.0
jaeger,One example is implemented in the  HotROD demo  in the Jaeger repository and it is described in the  blog post accompanying the demo .,0.0,0.0,1.0,0.0
jaeger,It uses  go.uber.org/zap  logging library underneath and allows to write log statements accepting the Context argument:,0.131,0.136,0.733,0.0258
jaeger,"Behind the scenes  logger.For(ctx)  captures the current tracing spans and adds all log statements to that span, in addition to sending then to  stdout  as usual.",0.0,0.0,1.0,0.0
jaeger,"To my knowledge,  go.uber.org/zap  does not yet support this mode of logging natively, and therefore requires a wrapper.",0.124,0.0,0.876,-0.3089
jaeger,"If you use another logging library, than on the high level this is what needs to happen:",0.0,0.0,1.0,0.0
jaeger,spring-cloud-openfeign  since is from the spring-cloud family should be instrumented automatically once you add  opentracing-spring-jaeger-cloud-starter  the  as stated  here .,0.0,0.0,1.0,0.0
jaeger,"But sometimes (depending on how you create your feign client bean) you need to explicitly expose the bean to the spring context, so that the autoconfiguration can instrument your Feign Client.",0.057,0.079,0.864,0.1901
jaeger,Something like this:,0.0,0.556,0.444,0.3612
jaeger,it is kotlin but you can adapt.,0.0,0.0,1.0,0.0
jaeger,This was due to the helidon dependency.,0.0,0.0,1.0,0.0
jaeger,https://helidon.io/docs/latest/#/guides/03_quickstart-mp,0.0,0.0,1.0,0.0
jaeger,Also I had to upgrade  opentracing-api  version to  0.33.0,0.0,0.0,1.0,0.0
jaeger,"If you are already using Istio in the deployment, then enabling tracing in it will provide more complete picture of request processing, such as accounting for the time spent in the network between the proxies.",0.0,0.0,1.0,0.0
jaeger,"You also don't need to have full tracing instrumentation in your services as long as they pass through certain headers, then Istio can still provide a pretty accurate picture of the traces (but you cannot capture any business specific data in the traces).",0.0,0.117,0.883,0.6486
jaeger,"Traces generated by Istio will have standardized span names that you can use to reason about the SLAs across the whole infrastructure, whereas explicit tracing instrumentation inside the services can often use different naming schemes, especially when services are written in different languages and using different frameworks.",0.0,0.0,1.0,0.0
jaeger,"For the best of both worlds, I would recommend adding instrumentation inside the services for full fidelity, and also enabling tracing in Istio to capture full picture of request execution (and all network latencies).",0.0,0.178,0.822,0.7717
jaeger,You most likely have a mismatch with your OpenTracing libraries.,0.0,0.0,1.0,0.0
jaeger,It looks like your Servlet Filter integration (io.opentracing.contrib.web.servlet.filter.TracingFilter) is making use of a method that doesn't exist.,0.0,0.143,0.857,0.3612
jaeger,You can use opentracing  java-jdbc  extension  it will works in Quarkus (I didn't test the native mode).,0.0,0.0,1.0,0.0
jaeger,You need to use the version 0.0.12 as the latest one is based on Opentracing 0.33 but Quarkus use the version 0.31.,0.0,0.0,1.0,0.0
jaeger,Add the dependency to your pom.xml:,0.0,0.0,1.0,0.0
jaeger,"Update your application.properties to use the opentracing-jdbc driver, the following are for a Postgres database:",0.0,0.0,1.0,0.0
jaeger,You will then saw the SQL queries in Jaeger as spans.,0.0,0.0,1.0,0.0
jaeger,What I eventually did is create a JaegerTraces and annotated with Bean,0.0,0.189,0.811,0.2732
jaeger,"Apache Camel doesn't provide an implementation of  OpenTracing , so you have to add also an implementation to your dependencies.",0.0,0.0,1.0,0.0
jaeger,For example  Jaeger .,0.0,0.0,1.0,0.0
jaeger,Maven POM:,0.0,0.0,1.0,0.0
jaeger,"Also you have to enable OpenTracing for Apache Camel on your Spring Boot application class, see  Spring Boot :",0.0,0.0,1.0,0.0
jaeger,"If you are using Spring Boot then you can add the  camel-opentracing-starter  dependency, and turn on OpenTracing by annotating the main class with  @CamelOpenTracing .",0.0,0.0,1.0,0.0
jaeger,"The Tracer will be implicitly obtained from the camel context’s Registry, or the ServiceLoader, unless a Tracer bean has been defined by the application.",0.0,0.0,1.0,0.0
jaeger,Spring Boot application class:,0.0,0.0,1.0,0.0
jaeger,"Could you try using a more recent version of Jaeger:  https://www.jaegertracing.io/docs/latest/getting-started/#all-in-one  - actually 1.11 is now out, so could try that.",0.0,0.0,1.0,0.0
jaeger,The problem is that you are using  RestTemplate template = new RestTemplate();  to get an instance of the  RestTemplate  to make a REST call.,0.114,0.0,0.886,-0.4019
jaeger,Doing that means that Opentracing cannot instrument the call to add necessary HTTP headers.,0.0,0.0,1.0,0.0
jaeger,Please consider using  @Autowired RestTemplate restTemplate,0.0,0.315,0.685,0.3182
jaeger,"While doing  mvnDebug quarkus:dev  (without  jvm.args ) and placing a breakpoint  here , I see that you all your params are being passed except  quarkus.jaeger.sampler.parameter  which is wrong.",0.119,0.0,0.881,-0.4767
jaeger,It should be  quarkus.jaeger.sampler.param,0.0,0.0,1.0,0.0
jaeger,When you are accessing the service from the pod in the  same namespace  you can use just the service name.,0.0,0.0,1.0,0.0
jaeger,Example:,0.0,0.0,1.0,0.0
jaeger,If you are accessing the service from the pod in the  different namespace  you should also specify the namespace.,0.0,0.0,1.0,0.0
jaeger,Example:,0.0,0.0,1.0,0.0
jaeger,"To check in what namespace the service is located, use the following command:",0.0,0.0,1.0,0.0
jaeger,Note : Changing ConfigMap does not apply it to deployment instantly.,0.0,0.0,1.0,0.0
jaeger,"Usually, you need to restart all pods in the deployment to apply new ConfigMap values.",0.0,0.162,0.838,0.4019
jaeger,"There is no rolling-restart functionality at the moment, but you can use the following command as a workaround: 
 (replace deployment name and pod name with the real ones)",0.058,0.0,0.942,-0.1531
jaeger,"I don't think it's possible at the moment and you should definitely ask for this feature in the mailing list, Gitter or GitHub issue.",0.0,0.109,0.891,0.4019
jaeger,"The current assumption is that a clear TChannel connection can be made between the agent and collector(s), all being part of the same trusted network.",0.0,0.206,0.794,0.6908
jaeger,"If you are using the Java, Node or C# client, my recommendation in your situation is to have your Jaeger Client to talk directly to the collector.",0.0,0.0,1.0,0.0
jaeger,Look for the env var  JAEGER_ENDPOINT  in the  Client Features  documentation page.,0.0,0.0,1.0,0.0
jaeger,"The problem was, that the Report instance used a NoopSender -- thus ignoring the connection settings.",0.293,0.0,0.707,-0.6597
jaeger,Using,0.0,0.0,1.0,0.0
jaeger,in your POM will provide an appropriate Sender to the SenderFactory used by Jaeger's SenderResolver::resolve method.,0.0,0.0,1.0,0.0
jaeger,This solved my problem.,0.397,0.309,0.294,-0.1531
jaeger,The Jaeger tracer (the part that runs along with your application) will send spans via UDP to an agent running on localhost by default.,0.0,0.0,1.0,0.0
jaeger,"If your agent is somewhere else, set the  JAEGER_AGENT_HOST / JAEGER_AGENT_PORT  env vars accordingly.",0.0,0.0,1.0,0.0
jaeger,"If you don't want an agent running on localhost and want to access a Jaeger Collector directly via HTTP, then set the  JAEGER_ENDPOINT  env var.",0.1,0.0,0.9,-0.1139
jaeger,"More info about these env vars can be found in the  documentation  or here:
 https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core#configuration-via-environment",0.0,0.0,1.0,0.0
jaeger,"No,  it cannot , but it wouldn't hurt to open an issue there with this suggestion.",0.088,0.201,0.712,0.4703
jaeger,There is several components which works together and can fully satisfy your requirement.,0.0,0.215,0.785,0.5095
jaeger,"Common  opentracing library , consisted of abstract layer for span, tracer, injectors and extractors, etc.",0.0,0.0,1.0,0.0
jaeger,Official  jaeger-client-csharp .,0.0,0.0,1.0,0.0
jaeger,"Full list of clients can be found  here , which implement  opentracing abstraction layer  mentioned earlier.",0.0,0.0,1.0,0.0
jaeger,"The final piece is the  OpenTracing API for .NET , which is glue between  opentracing library  and  DiagnosticSource  concept in dotnet.",0.0,0.0,1.0,0.0
jaeger,"Actually, the final library has  sample  which uses jaeger csharp implementation of ITracer and configure it as default GlobalTracer.",0.0,0.0,1.0,0.0
jaeger,"At the rest in your Startup.cs, you will end up with something like from that sample (services is IServiceCollection):",0.0,0.122,0.878,0.3612
jaeger,I resolved this.,0.0,0.63,0.37,0.1779
jaeger,It related to the sample rate.,0.0,0.0,1.0,0.0
jaeger,"After I configured the  JAEGER_SAMPLER_TYPE  and  JAEGER_SAMPLER_PARAM , I can see the data.",0.0,0.0,1.0,0.0
jaeger,"In server 2 , Install jaeger",0.0,0.0,1.0,0.0
jaeger,"In server 1, set these environment variables.",0.0,0.0,1.0,0.0
jaeger,"Change the tracer registration application code as below in server 1, so that it will get the configurations from the environment variables.",0.0,0.0,1.0,0.0
jaeger,Hope this works!,0.0,0.615,0.385,0.4926
jaeger,Check this link for integrating elasticsearch as the persistence storage backend so that the traces will not remove once the Jaeger instance is stopped.,0.076,0.0,0.924,-0.2263
jaeger,How to configure Jaeger with elasticsearch?,0.0,0.0,1.0,0.0
jaeger,I finally figured this out after trying out different combinations.,0.0,0.0,1.0,0.0
jaeger,This is happening because Jaeger agent is not receiving any UDP packets from my application.,0.0,0.0,1.0,0.0
jaeger,"You need to tell the tracer where to send UDP packets, which in this case is  docker-machine ip  
I added:",0.0,0.0,1.0,0.0
jaeger,and then I was able to see my services in Jaeger UI.,0.0,0.0,1.0,0.0
jaeger,Service graph data must be generated in Jaeger.,0.0,0.0,1.0,0.0
jaeger,Currently it's possible with via a Spark job here:  https://github.com/jaegertracing/spark-dependencies,0.0,0.192,0.808,0.2263
jaeger,"The Downloads page ( https://www.jaegertracing.io/download/ ) lists both the Docker images and the raw binaries built for various platforms (Linux, macOS, windows).",0.0,0.0,1.0,0.0
jaeger,You can also build binaries from source.,0.0,0.0,1.0,0.0
jaeger,"Just to add to Yuris answer, you can also download the source from github -  Github - Jaeger  This is useful for diagnosing issues, or just getting a better understanding of how it all works.",0.0,0.162,0.838,0.7003
jaeger,I have run both the released apps and custom versions on both windows and linux servers without issues.,0.0,0.0,1.0,0.0
jaeger,For windows I would recommend running as a service using Nssm.,0.0,0.238,0.762,0.3612
jaeger,Nssm details,0.0,0.0,1.0,0.0
jaeger,Elastic search works fine for this.,0.0,0.265,0.735,0.2023
jaeger,And Kibana allows you to build nice aggregated views of the traffic.,0.0,0.203,0.797,0.4215
jaeger,A recommendation from my experience is to use the  --es.tags-as-fields.dot-replacement  option and specify a character.,0.0,0.0,1.0,0.0
jaeger,This flattens the data structure.,0.0,0.0,1.0,0.0
jaeger,Its very useful because ElasticSearch/Kibana struggle with the tags data as an array.,0.139,0.194,0.667,0.2247
jaeger,I had the same problem.,0.474,0.0,0.526,-0.4019
jaeger,Found this page that explains how to configure Thrift sender:  https://github.com/jaegertracing/jaeger-client-csharp/blob/master/src/Senders/Jaeger.Senders.Thrift/README.md,0.0,0.0,1.0,0.0
jaeger,The C# tutorial does not mention it though ...,0.0,0.0,1.0,0.0
jaeger,And here is my InitTracer().,0.0,0.0,1.0,0.0
jaeger,Works fine with Jaeger launched from binary:,0.0,0.398,0.602,0.3182
jaeger,I solved it by using this library instead  https://github.com/opentracing-contrib/java-spring-cloud,0.0,0.231,0.769,0.2732
jaeger,It seem to have an option to enable or disable different instrumentation feature.,0.0,0.0,1.0,0.0
jaeger,Read about  opentracing.spring.cloud.async.enabled  for more info.,0.0,0.0,1.0,0.0
jaeger,"Looks like your DaemonSet misses the  hostNetwork  property, to be able to listen on the node IP.",0.098,0.129,0.773,0.1531
jaeger,You can check that article for further info:  https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677,0.0,0.0,1.0,0.0
jaeger,You have two options:,0.0,0.0,1.0,0.0
jaeger,"For (2), you can pass the environment variable to you applications:",0.0,0.0,1.0,0.0
jaeger,Additional references:,0.0,0.0,1.0,0.0
jaeger,"By default, OpenTracing doesn't log automatically into span logs, only important messages that Jaeger feels it needs to be logged and is needed for tracing would be there :).",0.0,0.06,0.94,0.2023
jaeger,"The idea is to separate responsibilities between Tracing and Log management,  Check this GitHub discussion .",0.0,0.0,1.0,0.0
jaeger,An alternative would be to use centralized log management and print traceId &amp; spanId into your logs for troubleshooting and correlating logs and tracing.,0.0,0.069,0.931,0.1779
jaeger,"As iabughosh said, the main focus on jaeger is traceability, monitoring and performance, not for logging.",0.0,0.0,1.0,0.0
jaeger,"Anyway, i found that using the @traced Bean injection you can insert a tag into the current span, that will be printed on Jaeger UI.",0.0,0.0,1.0,0.0
jaeger,this example will added the first 4 lines of an excpetion to the Tags seccion.,0.0,0.0,1.0,0.0
jaeger,(I use it on my global ExceptionHandler to add more info about the error):,0.0,0.0,1.0,0.0
jaeger,},0.0,0.0,0.0,0.0
jaeger,and you will see the little stacktrace at JaegerUI.,0.0,0.0,1.0,0.0
jaeger,Hope helps,0.0,1.0,0.0,0.6705
jaeger,Did you install the operator on openshift using the instructions listed:  https://github.com/jaegertracing/jaeger-operator#openshift  ?,0.0,0.0,1.0,0.0
jaeger,"Did the operator start up ok, if you not were there errors in the log?",0.0,0.246,0.754,0.5
jaeger,"The  Creating a new Jaeger Instance  section starts with a link to some examples, including  simple-prod.yaml , which creates a Jaeger instance that uses an Elasticsearch cluster at the specified URL.",0.0,0.147,0.853,0.5106
jaeger,You simply run:,0.0,0.0,1.0,0.0
jaeger,It doesn't work in golang grpc client.,0.0,0.0,1.0,0.0
jaeger,I used openTelemetry  load balancing  Another option - use kubernetes to balance requests to backends.,0.0,0.0,1.0,0.0
jaeger,I realized that I had got into a completely wrong direction.,0.326,0.0,0.674,-0.5256
jaeger,"I thought that I have to access the backend storage to get the trace data, which actually make the problem much more complex.",0.119,0.0,0.881,-0.4019
jaeger,I got the answer from github discussion and here is the address  https://github.com/jaegertracing/jaeger/discussions/2876#discussioncomment-477176,0.0,0.0,1.0,0.0
jaeger,Can you paste the Collector config file?,0.0,0.0,1.0,0.0
jaeger,It seems you are using the gRPC protocol and it's not supported on the system where the collector is running.,0.094,0.0,0.906,-0.2411
jaeger,https://github.com/open-telemetry/opentelemetry-collector/blob/master/exporter/otlpexporter/README.md,0.0,0.0,1.0,0.0
jaeger,gRPC port isn't enabled in your jaeger instance.,0.0,0.0,1.0,0.0
jaeger,You can try a docker-compose file like this,0.0,0.294,0.706,0.3612
jaeger,And you can connect to it without problems,0.0,0.244,0.756,0.3089
jaeger,Remove your dependencies and use the following one that will include also the instrumentation you need,0.0,0.0,1.0,0.0
jaeger,I figured it out... the Jaeger Operator doesn't create a  Service  exposing the metrics endpoints.,0.124,0.124,0.752,0.0
jaeger,These endpoints are just exposed via the pods for the Collector and Query components.,0.091,0.0,0.909,-0.0772
jaeger,An example from the Collector pod spec:,0.0,0.0,1.0,0.0
jaeger,Note the  admin-http  port there.,0.0,0.0,1.0,0.0
jaeger,"So to get the Prometheus Operator to scrape these metrics, I created a  PodMonitor  which covers both the Collector and Query components because both of them have the  labels/app: jaeger  and  admin-http  ports defined:",0.0,0.061,0.939,0.25
jaeger,referring to the documentation provided in below link helped to resolve the issue.,0.0,0.178,0.822,0.3818
jaeger,java.lang.IllegalStateException: This should not happen as headers() should only be called while a record is processed,0.0,0.0,1.0,0.0
jaeger,I got it working as mentioned below,0.0,0.0,1.0,0.0
jaeger,I would assume that it may not be the right way of exposing the services.,0.139,0.0,0.861,-0.2732
jaeger,Instead,0.0,0.0,1.0,0.0
jaeger,This is the simplest working example that I was able to find.,0.0,0.0,1.0,0.0
jaeger,Here is a more realistic example that builds the tracer from a configuration.,0.0,0.0,1.0,0.0
jaeger,"GitLab Helm charts support tracing, and you can configure it with:",0.0,0.213,0.787,0.4019
jaeger,For more details refer : https://docs.gitlab.com/charts/charts/globals.html#tracing,0.0,0.0,1.0,0.0
jaeger,According to  istio  documentation:,0.0,0.0,1.0,0.0
jaeger,"Consult the   Jaeger documentation   to
get started.",0.0,0.0,1.0,0.0
jaeger,"No special changes are needed for Jaeger to work with
Istio.",0.158,0.194,0.647,0.128
jaeger,"Once Jaeger is installed, you will need to point Istio proxies to send
traces to the deployment.",0.0,0.0,1.0,0.0
jaeger,"This can be configured with   --set values.global.tracer.zipkin.address=&lt;jaeger-collector-address&gt;:9411 
at installation time.",0.0,0.0,1.0,0.0
jaeger,"See the
 ProxyConfig.Tracing 
for advanced configuration such as TLS settings.",0.0,0.182,0.818,0.25
jaeger,Istio documentation states to use jaeger collector address in  global.tracer.zipkin.address .,0.0,0.0,1.0,0.0
jaeger,"As for the Jaeger agent host, according to  Jaeger  Operator documentation:",0.0,0.0,1.0,0.0
jaeger,"&lt;9&gt; By default, the operator assumes that agents are deployed as
sidecars within the target pods.",0.0,0.0,1.0,0.0
jaeger,"Specifying the strategy as
“DaemonSet” changes that and makes the operator deploy the agent as
DaemonSet.",0.0,0.0,1.0,0.0
jaeger,"Note that your tracer client will probably have to override
the “JAEGER_AGENT_HOST” environment variable to use the node’s IP.",0.0,0.0,1.0,0.0
jaeger,Your tracer client will then most likely need to be told where the agent is located.,0.0,0.0,1.0,0.0
jaeger,"This is usually done by setting the environment variable   JAEGER_AGENT_HOST   to the value of the Kubernetes node’s IP, for example:",0.0,0.112,0.888,0.34
jaeger,Solved by adding dependency in pom file on jaeger-thrift.,0.0,0.208,0.792,0.2732
jaeger,I enabled instrumentation on the services using those two dependencies:,0.0,0.0,1.0,0.0
jaeger,"And, I used jaeger-client to configure the tracer using environment variables:",0.0,0.0,1.0,0.0
jaeger,Getting a Tracer Instance:,0.0,0.0,1.0,0.0
jaeger,"Finally, in the dropwizard application, you have to register the tracer like so",0.0,0.172,0.828,0.3612
jaeger,You need to keep double quotes as it is.,0.0,0.0,1.0,0.0
jaeger,An issue has been identified similar to this [1] and has been fixed recently.,0.0,0.0,1.0,0.0
jaeger,Can you try to get the latest WUM updated API Manager 3.1.0 and try enabling Jaeger open tracing?,0.0,0.0,1.0,0.0
jaeger,"Alternatively, this issue will not occur when using &quot;localhost&quot; as the hostname.",0.0,0.0,1.0,0.0
jaeger,[1]  https://github.com/wso2/product-apim/issues/7940,0.0,0.0,1.0,0.0
jaeger,Run Jager using the docker image as follows.,0.0,0.0,1.0,0.0
jaeger,Then add the following config to the deployment.toml.,0.0,0.0,1.0,0.0
jaeger,Side Note: For zipkin you can use the following.,0.0,0.0,1.0,0.0
jaeger,"I'm not sure if what you're looking for exists today per se, but you could accomplish this with OpenTelemetry by writing traces through the LogReporter then using a serverless function to read the cloudwatch stream and send it to Jaeger (or to an OpenTelemetry Collector that sends them to Jaeger).",0.028,0.071,0.901,0.4971
jaeger,You could also write a custom plugin for the OpenTelemetry Collector that read a cloudwatch stream into OTLP then exported it to any endpoint supported by the collector.,0.0,0.084,0.916,0.3182
jaeger,You are missing the configuration of Jaeger address.,0.239,0.0,0.761,-0.296
jaeger,"Since you did not provided it, it is trying to connect to the default one, which is TCP protocol,  127.0.0.1  and port 5778.",0.0,0.0,1.0,0.0
jaeger,Check for details the configuration section  here .,0.0,0.0,1.0,0.0
jaeger,You just need to make use of  tags.value  instead of  value  in your match query.,0.0,0.146,0.854,0.34
jaeger,Below query should help:,0.0,0.474,0.526,0.4019
jaeger,"if it throws error then it is unable to reach host, this method is costly I agree but this is the only viable solution I find",0.11,0.207,0.683,0.4019
jaeger,so I give it a try and my answer to the question above are:,0.0,0.0,1.0,0.0
jaeger,"Q1) yes it is possible (congrats to the jaeger team, package pretty easy to grasp with a good documentation)",0.0,0.455,0.545,0.8934
jaeger,"Q2) I did struggle a bit with this one and thanks to  https://github.com/CHOMNANP/jaeger-js-text-map-demo  I implemented a solution by adding a ""textCarrier"" with a ref.",0.102,0.231,0.667,0.4404
jaeger,"to the span context formatted as ""FORMAT_TEXT_MAP"" to the message Component 1 was publishing towards Component 2.",0.0,0.0,1.0,0.0
jaeger,Code snipper in C1 on the first API invocation,0.0,0.0,1.0,0.0
jaeger,followed by this part when sending the msg on redis:,0.0,0.0,1.0,0.0
jaeger,The getTextCarrierBySpanObject function is coming from  https://github.com/CHOMNANP/jaeger-js-text-map-demo,0.0,0.0,1.0,0.0
jaeger,Code snippet in C2 receiving the msg from redis,0.0,0.0,1.0,0.0
jaeger,I tested with version 1.13,0.0,0.0,1.0,0.0
jaeger,"So all in all a pretty convincing prototyping exercise with Jaeger, will most probably pilot it now on a real project before trying it in production.",0.0,0.211,0.789,0.7096
jaeger,According to  https://helm.sh/docs/chart_template_guide/control_structures/  a string is converted to a boolean of True.,0.0,0.237,0.763,0.4215
jaeger,So even a string of false would get evaluated as a Boolean of True by Helm.,0.0,0.177,0.823,0.4215
jaeger,"I was using Spinnaker which handles all overrides as a string unless the ""Raw Overrides"" box is checked.",0.0,0.0,1.0,0.0
jaeger,If that box is checked than it converts the string to primitives where applicable.,0.0,0.0,1.0,0.0
jaeger,"My issue was that even though I was overriding with a value of false, Spinnaker would pass that as a string to Helm which would then evaluate that as True.",0.0,0.172,0.828,0.6369
jaeger,"The solution was to check the ""Raw Overrides"" box in Spinnaker.",0.0,0.187,0.813,0.3182
jaeger,You can set a tag to Span creating a new custom Span,0.0,0.196,0.804,0.296
jaeger,or retrieving the current active Span,0.0,0.351,0.649,0.4019
jaeger,Here is a working example:  https://github.com/jobinesh/cloud-native-applications/tree/master/helidon-example-mp-jaeger .,0.0,0.0,1.0,0.0
jaeger,See if that helps you.,0.0,0.394,0.606,0.3818
jaeger,"If you are interested, see the details captured here:  https://www.jobinesh.com/2020/04/tracing-api-calls-in-your-helidon.html",0.0,0.231,0.769,0.4019
jaeger,You need to enable open tracing in nginx ingress controller.,0.0,0.0,1.0,0.0
jaeger,To enable the instrumentation we must enable OpenTracing in the configuration ConfigMap:,0.0,0.0,1.0,0.0
jaeger,"To enable or disable instrumentation for a single Ingress, use the enable-opentracing annotation:",0.0,0.0,1.0,0.0
jaeger,You must also set the host to use when uploading traces:,0.0,0.0,1.0,0.0
jaeger,https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/,0.0,0.0,1.0,0.0
jaeger,Are you connecting it to only elasticsearch or stack like ELK/EFK?.,0.0,0.2,0.8,0.3612
jaeger,"I had tried but we cannot configure ELK in jeager-all-one.exe alone in windows without docker.You can do it by running Jeager-collector, Jeager agent and Jeager query individually by mentioning configurations related to ELK.",0.075,0.0,0.925,-0.3612
jaeger,In Jeager collector and Jeager query you need to set up variables  SPAN_STORAGE_TYPES  and  ES_SERVER_URLS .,0.0,0.0,1.0,0.0
jaeger,To start a jaeger container:,0.0,0.0,1.0,0.0
jaeger,Then you should by able to access to the Jaeger UI at  http://localhost:16686,0.0,0.0,1.0,0.0
jaeger,"Once you've have a Jaeger up and running, you need to configure a Jaeger exporter to forward spans to Jaeger.",0.0,0.0,1.0,0.0
jaeger,This will depends of the language used.,0.0,0.0,1.0,0.0
jaeger,Here  is the straightforward documentation to do so in python.,0.0,0.0,1.0,0.0
jaeger,"Not out of the box, you have to plug a behaviour into NSB that uses open telemetry
 https://github.com/open-telemetry/opentelemetry-dotnet 
You will have to write custom code.",0.0,0.0,1.0,0.0
jaeger,"Plus you can do push metrics as well as shown in our app insights, Prometheus and other samples.",0.0,0.11,0.89,0.2732
jaeger,Let's continue the conversation in our support channels?,0.0,0.278,0.722,0.4019
jaeger,Not sure if you are still looking for a solution for this.,0.148,0.173,0.679,0.0869
jaeger,You should be able to do this currently using the  NServiceBus.Extensions.Diagnostics.OpenTelemetry  package from  nuget .,0.0,0.0,1.0,0.0
jaeger,This is built by Jimmy Bogard and instruments NServiceBus with the required support for  Open Telemetry .,0.0,0.153,0.847,0.4019
jaeger,The source for this is available  here .,0.0,0.0,1.0,0.0
jaeger,You can connect this to any backend of your choice that supports  Open Telemetry  including but not limited to  Jaeger  and  Zipkin .,0.0,0.158,0.842,0.4116
jaeger,"Additionally, here is an  example  that shows this in action.",0.0,0.0,1.0,0.0
jaeger,Got it!,0.0,0.0,1.0,0.0
jaeger,We need to enable sampling strategy to reach the collector endpoint.,0.0,0.099,0.901,0.0258
jaeger,Found out how.,0.0,0.0,1.0,0.0
jaeger,I just added one single line of code into tracing.py of django_opentracing lib:,0.0,0.0,1.0,0.0
jaeger,And the result:,0.0,0.0,1.0,0.0
jaeger,I see..,0.0,0.0,1.0,0.0
jaeger,I thought  @Traced  will be somehow propagated to my db-services/repositories.,0.0,0.0,1.0,0.0
jaeger,"No, I have to put it explicitly:",0.306,0.0,0.694,-0.296
jaeger,That fixes the issue.,0.0,0.0,1.0,0.0
jaeger,According to the documentation  Remotely Accessing Telemetry Addons .,0.0,0.0,1.0,0.0
jaeger,There are different ways how to acces telemetry.,0.0,0.0,1.0,0.0
jaeger,The Recommended way is to create Secure acces using https instead of http.,0.0,0.387,0.613,0.6486
jaeger,Note for both methods:,0.0,0.0,1.0,0.0
jaeger,This option covers securing the transport layer only.,0.0,0.247,0.753,0.3182
jaeger,You should also configure the telemetry addons to require authentication when exposing them externally.,0.139,0.0,0.861,-0.2732
jaeger,Please note that jaeger itself doesn't support authentication methods  github  and workaround using Apache httpd server  here .,0.115,0.118,0.767,0.0108
jaeger,With your recruitments you can use Gateways (SDS)  with self-signed certificates :,0.0,0.0,1.0,0.0
jaeger,a .),0.0,0.0,1.0,0.0
jaeger,Make sure your that during istio instalation youe have enabled SDS at ingress gateway  --set gateways.istio-ingressgateway.sds.enabled=true  and  --set tracing.enabled=true  for tacing purposes.,0.0,0.099,0.901,0.3182
jaeger,b .),0.0,0.0,1.0,0.0
jaeger,Create self signed certificates for testing purposes you can use this  example and repository .,0.0,0.139,0.861,0.2732
jaeger,c .),0.0,0.0,1.0,0.0
jaeger,Please follow  Generate client and server certificates and keys   and  Configure a TLS ingress gateway using SDS .,0.0,0.133,0.867,0.3182
jaeger,Create Virtualservice and Gateway:,0.0,0.412,0.588,0.2732
jaeger,Hope this help,0.0,0.848,0.152,0.6808
jaeger,The prometheus-es-exporter provides a way to create metrics using queries.,0.0,0.208,0.792,0.2732
jaeger,For further details you can check  prometheus-es-exporter#query-metrics,0.0,0.0,1.0,0.0
jaeger,Great question and a very popular one too.,0.0,0.59,0.41,0.8016
jaeger,"In short, yes, code changes are required.",0.0,0.31,0.69,0.4019
jaeger,Not just in one service but in all the services that a request will go through.,0.0,0.0,1.0,0.0
jaeger,You need to instrument all services to get continuous traces that will be able to tell you the story of a request as it travels through the system.,0.0,0.0,1.0,0.0
jaeger,"I believe you can reuse Elasticsearch for multiple purposes - each would use a different set of indices, so separation is good.",0.0,0.149,0.851,0.4877
jaeger,from:  https://www.jaegertracing.io/docs/1.11/deployment/  :,0.0,0.0,1.0,0.0
jaeger,Collectors require a persistent storage backend.,0.0,0.0,1.0,0.0
jaeger,Cassandra and Elasticsearch are the primary supported storage backends,0.0,0.223,0.777,0.3182
jaeger,"Tying the networking all together, a docker-compose example:
 How to configure Jaeger with elasticsearch?",0.0,0.0,1.0,0.0
jaeger,"While this isn't exactly what you asked, it sounds like what you're trying to achieve is seeing tracing for your JMS calls in Jaegar.",0.0,0.098,0.902,0.3612
jaeger,"If that is the case, you could use an OpenTracing tracing solution for JMS or ActiveMQ to report tracing data directly to Jaegar.",0.0,0.095,0.905,0.3182
jaeger,Here's one potential solution I found with a quick google.,0.0,0.247,0.753,0.3182
jaeger,There may be others.,0.0,0.0,1.0,0.0
jaeger,https://github.com/opentracing-contrib/java-jms,0.0,0.0,1.0,0.0
jaeger,"Maybe you should check whether the application services which you set up in a hurry are both in the same azure resource group as the VM running the Jaeger all-in-one instance, otherwise the second application might not be able to communicate with the Jaeger instance at all.",0.0,0.0,1.0,0.0
jaeger,"It doesn't really matter in which language your individual microservices are written, you should see them all in the same trace.",0.061,0.0,0.939,-0.0749
jaeger,"Given that you are seeing three traces instead of one trace with three spans, it appears that the context propagation isn't working.",0.0,0.0,1.0,0.0
jaeger,"Check your HTTP client in your nodejs services, they should perform the  ""inject"" operation .",0.0,0.0,1.0,0.0
jaeger,"Your service ""B"" and ""C"" should then perform the ""extract"" operation.",0.0,0.0,1.0,0.0
jaeger,"If you haven't yet, check  Yuri Shkuro's OpenTracing Tutorial .",0.0,0.0,1.0,0.0
jaeger,"The lesson 3 is about the context propagation, including the inject and extract operations.",0.0,0.0,1.0,0.0
jaeger,"I'm not quite sure how it works in the NodeJS world, but in Java, it should be sufficient to have the  opentracing-contrib/java-web-servlet-filter  instrumentation library in your classpath, as it would register the necessary pieces in the right hooks and make the trace context available for each incoming HTTP request.",0.032,0.0,0.968,-0.1505
jaeger,It seems that PyInstaller can't resolve  jaeger_client  import.,0.238,0.0,0.762,-0.2924
jaeger,So an easy way is to just edit your spec file and add the whole  jaeger_client  library as a  Tree  class:,0.0,0.143,0.857,0.4902
jaeger,And generate your executable with  pyinstaller script.spec .,0.0,0.0,1.0,0.0
jaeger,You can create a  NodePort  service using the  app: jaeger  selector to expose the UI outside the cluster.,0.086,0.112,0.802,0.128
jaeger,"kubectl port-forward  command default is expose to  localhost  network only, try to add  --address 0.0.0.0",0.103,0.0,0.897,-0.1531
jaeger,see  kubectl command reference,0.0,0.0,1.0,0.0
jaeger,There are several ways of doing this.,0.0,0.0,1.0,0.0
jaeger,The  port-forward  works fine on Google Cloud Shell.,0.0,0.205,0.795,0.2023
jaeger,"If you are using GKE, then I strongly recommend using Cloud Shell, and  port-forward  as it is the easiest way.",0.0,0.316,0.684,0.7506
jaeger,"On other clouds, I don't know.",0.0,0.0,1.0,0.0
jaeger,What is suggesting Stefan would work.,0.0,0.0,1.0,0.0
jaeger,"You can edit the jaeger service with  kubectl edit svc jaeger-query , then change the type of the service from  ClusterIP  to  NodePort .",0.0,0.0,1.0,0.0
jaeger,"Finally, you can access the service with  NODE_IP:PORT  (any node).",0.0,0.0,1.0,0.0
jaeger,"If you do  kubectl get svc , you will see the new port assigned to the service.",0.0,0.0,1.0,0.0
jaeger,Note: You might need to open a firewall rule for that port.,0.0,0.0,1.0,0.0
jaeger,"You can also make the service type  LoadBalancer , if you have a control plane to set up an external IP address.",0.0,0.0,1.0,0.0
jaeger,"This would be a more expensive solution, but you would have a dedicated external IP address for your service.",0.0,0.278,0.722,0.6993
jaeger,"There are more ways, but I would say these are the appropriate ones.",0.0,0.0,1.0,0.0
jaeger,This issue looks more to have to do with Java it self then either Opentracing and Jaeger.,0.0,0.0,1.0,0.0
jaeger,as  ex.getStackTrace()  is more of the problem.,0.331,0.0,0.669,-0.4522
jaeger,As it should be more like,0.0,0.358,0.642,0.4201
jaeger,Problem solved.,0.562,0.437,0.0,-0.1531
jaeger,Setting a baggage item is  not  the same as setting an HTTP header.,0.0,0.0,1.0,0.0
jaeger,You should use your HTTP client (not shown in your example) to set the HTTP header.,0.0,0.0,1.0,0.0
jaeger,"Baggage items might or not be available as individual HTTP headers: it's a detail implementation of the underlying tracer, such as Jaeger's.",0.0,0.0,1.0,0.0
jaeger,"Each ""dot"" would be a new child node in the YAML file, like:",0.0,0.185,0.815,0.3612
jaeger,"Make sure to run the process with the env var SPAN_STORAGE_TYPE set to elasticsearch, like:",0.0,0.27,0.73,0.5859
jaeger,(as seen on  https://github.com/jaegertracing/jaeger/issues/1299 ),0.0,0.0,1.0,0.0
jaeger,"So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?",0.0,0.0,1.0,0.0
jaeger,Using the sampler type as  const  with  1  as the value means that you are sampling everything.,0.0,0.138,0.862,0.34
jaeger,"Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes.",0.0,0.095,0.905,0.2732
jaeger,I would like to understand this configuration spec more but not able to.,0.0,0.137,0.863,0.1901
jaeger,There are several things that might be happening.,0.0,0.0,1.0,0.0
jaeger,"You might not be closing spans, for instance.",0.0,0.0,1.0,0.0
jaeger,I recommend reading the following two blog posts to try to understand what might be happening:,0.0,0.152,0.848,0.3612
jaeger,Help!,0.0,1.0,0.0,0.4574
jaeger,Something is wrong with my Jaeger installation!,0.361,0.0,0.639,-0.5255
jaeger,The life of a span,0.0,0.0,1.0,0.0
jaeger,"Your best chance is to get the data from whatever storage the Jaeger collector is using (Cassandra, Elastic.)",0.0,0.279,0.721,0.7351
jaeger,( https://www.jaegertracing.io/docs/1.6/deployment/  ),0.0,0.0,1.0,0.0
jaeger,My suggestion is to store in Elastic and use Kibana to accomplish what you need.,0.0,0.167,0.833,0.4215
jaeger,Old topic but the current version of Jaeger Query UI is a single page app and has an underlying API that allows the same queries capabilities as the UI.,0.0,0.0,1.0,0.0
jaeger,You're using  Camden  release train with boot  2.0  and Sleuth  2.0 .,0.0,0.0,1.0,0.0
jaeger,That's completely incompatible.,0.0,0.0,1.0,0.0
jaeger,"Please generate a project from start.spring.io from scratch, please don't put any versions manually for spring cloud projects, and please try again.",0.0,0.277,0.723,0.7096
jaeger,Try using  Finchley  release train instead of  Camden,0.0,0.0,1.0,0.0
jaeger,"The problem is that if I kill the Docker running the jaeger collector- systemctl stop docker and later restart docker and jaegertracing/all-in-one, the services are no longer up at  http://localhost:16686/api/services",0.321,0.0,0.679,-0.8957
jaeger,That's because you are using the in-memory storage.,0.0,0.0,1.0,0.0
jaeger,"If you stop and start the container, the storage is reset, so, you'll effectively lose your data.",0.231,0.142,0.627,-0.2466
jaeger,"For production purposes, you should use a backing storage like Cassandra or Elasticsearch.",0.0,0.265,0.735,0.3818
jaeger,Does the Jaeger collector needs to be running before starting the Jaeger clients?,0.0,0.0,1.0,0.0
jaeger,"No, but spans reported by clients when the collector isn't available might get dropped.",0.11,0.0,0.89,-0.1531
jaeger,"Note that clients will send spans to the agent by default, and will not contact the collector directly.",0.0,0.0,1.0,0.0
jaeger,"So, if the agent isn't available, spans might get dropped as well.",0.0,0.16,0.84,0.2732
jaeger,how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?,0.0,0.0,1.0,0.0
jaeger,Use the configuration option  --memory.max-traces .,0.0,0.0,1.0,0.0
jaeger,"With this option, older traces will get overwritten by new ones once this limit is reached.",0.0,0.085,0.915,0.1027
jaeger,But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces,0.0,0.0,1.0,0.0
jaeger,"That's because the Jaeger client will, by default, send the spans via UDP to an agent at  localhost .",0.0,0.0,1.0,0.0
jaeger,"When your application is running in a Docker container, your  localhost  there is the container itself, so that the spans are lost.",0.103,0.0,0.897,-0.3182
jaeger,"As you are linking the Jaeger container with your application, you may want to get it solved by exporting the env var  JAEGER_AGENT_HOST  to  jaeger .",0.0,0.129,0.871,0.34
jaeger,Turns out I don't need neither Zipkin nor Jaeger to have my traces on Stackdriver.,0.0,0.0,1.0,0.0
jaeger,All that is needed is a deployment of  zipkin-collector  and a service to point to it and all my traces are now reporting as expected on GCP Stackdriver.,0.0,0.0,1.0,0.0
jaeger,"Not jaeger, able to send traces to zipkin server, using zipkin-simple.",0.0,0.0,1.0,0.0
jaeger,Related code is in repository  https://github.com/debmalya/calculator,0.0,0.0,1.0,0.0
jaeger,node-jaeger-client currently doesn't run in the browser.,0.0,0.0,1.0,0.0
jaeger,There is ongoing  work  to make jaeger-client browser friendly.,0.0,0.286,0.714,0.4939
jaeger,This issue:  readFileSync is not a function  contains relevant information to why you're seeing the error message.,0.153,0.0,0.847,-0.4019
jaeger,"Essentially, you're trying to run jaeger-client (a nodejs library) using react-scripts which doesn't contain the modules that jaeger-client needs.",0.0,0.0,1.0,0.0
jaeger,There are two issues here.,0.0,0.0,1.0,0.0
jaeger,One is that your code sets the port for Jaeger client to 5775.,0.0,0.0,1.0,0.0
jaeger,"This port expects a different data model than what Node.js client sends, you can remove the  agentHost  and  agentPort  parameters and rely on defaults.",0.0,0.0,1.0,0.0
jaeger,The second issue is that you're running the Docker image without exposing the required UDP port.,0.0,0.108,0.892,0.2057
jaeger,"The correct command is shown in the  documentation , as of today it should be this (one long line):",0.0,0.0,1.0,0.0
jaeger,Liberty does not have Open Tracing Tracer implementation for Jaeger yet.,0.0,0.254,0.746,0.5267
jaeger,We have a sample Tracer implementation for Zipkin.,0.0,0.0,1.0,0.0
jaeger,You can find it at  https://github.com/WASdev/sample.opentracing.zipkintracer .,0.0,0.0,1.0,0.0
jaeger,Jaegar claims it is backward compatible with Zipkin by accepting spans in Zipkin formats over HTTP.,0.0,0.148,0.852,0.3818
jaeger,Feel free to open a RFE at  https://developer.ibm.com/wasdev/help/submit-rfe/,0.0,0.355,0.645,0.5106
jaeger,This is most likely caused by the static assets  not  being included in the binary.,0.0,0.108,0.892,0.1779
jaeger,You can try that out by running the binary you compiled.,0.0,0.0,1.0,0.0
jaeger,"Instead of compiling on your own, a better approach would be to get the official binaries from the releases page and build your Docker container using that.",0.0,0.104,0.896,0.4404
jaeger,https://github.com/jaegertracing/jaeger/releases/latest,0.0,0.0,1.0,0.0
jaeger,"This is a limitation in the Serilog logger factory implementation; in particular, Serilog currently ignores added providers and assumes that Serilog Sinks will replace them instead.",0.158,0.0,0.842,-0.5106
jaeger,"So, the solutions is implementaion a simple  WriteTo.OpenTracing()  method to connect Serilog directly to  OpenTracing",0.0,0.132,0.868,0.2449
jaeger,I had this problem while using gunicorn with gevent as the worker class.,0.197,0.0,0.803,-0.4019
jaeger,To resolve and get cloud traces working the solution was to monkey patch grpc like so,0.0,0.363,0.637,0.7506
jaeger,See  https://github.com/grpc/grpc/issues/4629#issuecomment-376962677,0.0,0.0,1.0,0.0
jaeger,For your exact question create a character class,0.0,0.259,0.741,0.2732
jaeger,And then you can just add * on the end to get 0 or unlimited number of them or alternatively 1 or an unlimited number with +,0.0,0.11,0.89,0.1531
jaeger,or,0.0,0.0,1.0,0.0
jaeger,"Also there is this below, found at  https://regex101.com/  under the library tab when searching for json",0.0,0.0,1.0,0.0
jaeger,"This should match any valid json, you can also test it at the website above",0.0,0.0,1.0,0.0
jaeger,EDIT:,0.0,0.0,1.0,0.0
jaeger,Link to the regex,0.0,0.0,1.0,0.0
jaeger,"No, there is no out-of-the-box possibility to change the HTTP header name.",0.306,0.0,0.694,-0.5267
jaeger,"However, you can enable B3 header propagation with  opentracing.jaeger.enable-b3-propagation=true .",0.0,0.0,1.0,0.0
jaeger,"To configure Traefik to send the trace data as B3 headers, see  https://github.com/containous/traefik/blob/master/docs/content/observability/tracing/jaeger.md#propagation .",0.0,0.0,1.0,0.0
jaeger,traceContextHeaderName  should also be configured as  X-B3-TraceId  then.,0.0,0.0,1.0,0.0
jaeger,There's an ongoing discussion over here -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/599  .,0.0,0.0,1.0,0.0
jaeger,In general we don't explicitly use the OpenTracing API but we are Zipkin compatible in terms of header propagation.,0.0,0.0,1.0,0.0
jaeger,You can also manipulate the header names as you wish so if any sort of library you're using requires other header names for span / trace etc.,0.0,0.097,0.903,0.4019
jaeger,then you can set it yourself as you want to.,0.0,0.126,0.874,0.0772
jaeger,Spring Sleuth is now OpenTracing compatible.,0.0,0.0,1.0,0.0
jaeger,All you have to do is use OpenTracing Jars in your class path.,0.0,0.0,1.0,0.0
jaeger,You can then use Sleuth-Zipkin to send instrumentation data to Jaeger's Zipkin collector.,0.0,0.0,1.0,0.0
jaeger,This way you achieve everything you want with minimal configuration.,0.0,0.126,0.874,0.0772
jaeger,You can use my sample program as an example here:,0.0,0.0,1.0,0.0
jaeger,https://github.com/anoophp777/spring-webflux-jaegar-log4j2,0.0,0.0,1.0,0.0
jaeger,It looks like this is not currently possible with Cordova 3.4 when attempting to read a video file out of the application assets.,0.0,0.174,0.826,0.4939
jaeger,See  https://issues.apache.org/jira/browse/CB-6079,0.0,0.0,1.0,0.0
jaeger,"It is possible to read the file if its copied to a directory outside the application assets, or the file is stored remotely.",0.0,0.075,0.925,0.1779
jaeger,But not in the app assets folder any longer.,0.0,0.204,0.796,0.2617
jaeger,I have a similar issue - my application has a welcome screen with a short video explaining the application (~300k) which I cannot play out of the APK itself.,0.078,0.115,0.807,0.2415
jaeger,"jquery.limitkeypress.js has some issue with ie, I recommend you to use a more powerful library.",0.0,0.337,0.663,0.6801
jaeger,http://github.com/RobinHerbots/jquery.inputmask,0.0,0.0,1.0,0.0
jaeger,With this library you can use something like this:,0.0,0.238,0.762,0.3612
jaeger,It works perfectly on ie.,0.0,0.512,0.488,0.6369
jaeger,:),0.0,1.0,0.0,0.4588
jaeger,Sorry i have not updated that plugin in a few years but...,0.126,0.0,0.874,-0.0772
jaeger,jquery.limitkeypress  now works with IE9+ there was an issue with how the selection was determined.,0.0,0.146,0.854,0.34
jaeger,IE11 killed support for their document.selection but they kept the document.setSelectionRange which i was using to test what browser was being used...,0.117,0.078,0.805,-0.2263
jaeger,IE9 enabled document.selectionStart and document.selectionEnd so i now check directly what browser version of IE peoples are using...,0.0,0.0,1.0,0.0
jaeger,I added this to check for IE version:,0.0,0.0,1.0,0.0
jaeger,So my selection functions now look like this:,0.0,0.263,0.737,0.3612
jaeger,After few days of digging I've figured it out.,0.0,0.0,1.0,0.0
jaeger,Problem is in the format of the  x-request-id  header that nginx ingress controller uses.,0.172,0.0,0.828,-0.4019
jaeger,Envoy proxy expects it to be an UUID (e.g.,0.0,0.0,1.0,0.0
jaeger,x-request-id: 3e21578f-cd04-9246-aa50-67188d790051 ) but ingrex controller passes it as a non-formatted random string ( x-request-id: 60e82827a270070cfbda38c6f30f478a ).,0.0,0.0,1.0,0.0
jaeger,When I pass properly formatted x-request-id header in the request to ingress controller its getting passed down to envoy proxy and request is getting sampled as expected.,0.0,0.0,1.0,0.0
jaeger,"I also tried to remove
x-request-id header from the request from ingress controller to ServiceA with a simple EnvoyFilter.",0.0,0.0,1.0,0.0
jaeger,And it also works as expected.,0.0,0.0,1.0,0.0
jaeger,Envoy proxy generates a new x-request-id and request is getting traced.,0.0,0.0,1.0,0.0
jaeger,The demo you tried is using older configuration and opencensus which should be replaced with otlp receiver.,0.0,0.0,1.0,0.0
jaeger,"Having said that this is a working example
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node/docker 
So I'm copying the files from there:",0.0,0.0,1.0,0.0
jaeger,docker-compose.yaml,0.0,0.0,1.0,0.0
jaeger,collector-config.yaml,0.0,0.0,1.0,0.0
jaeger,prometheus.yaml,0.0,0.0,1.0,0.0
jaeger,This should work fine with opentelemetry-js ver.,0.0,0.231,0.769,0.2023
jaeger,0.10.2,0.0,0.0,1.0,0.0
jaeger,Default port for traces is 55680 and for metrics 55681,0.0,0.0,1.0,0.0
jaeger,"The link I posted previously - you will always find there the latest up to date working example:
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node 
And for web example you can use the same docker and see all working examples here:
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/tracer-web/",0.0,0.0,1.0,0.0
jaeger,Thank you sooo much for @BObecny's help!,0.0,0.523,0.477,0.6696
jaeger,This is a complement of @BObecny's answer.,0.0,0.0,1.0,0.0
jaeger,Since I am more interested in integrating with Jaeger.,0.0,0.3,0.7,0.4576
jaeger,"So here is the config to set up with all Jaeger, Zipkin, Prometheus.",0.0,0.0,1.0,0.0
jaeger,And now it works on both front end and back end.,0.0,0.0,1.0,0.0
jaeger,First both front end and back end use same exporter code:,0.0,0.0,1.0,0.0
jaeger,docker-compose.yaml,0.0,0.0,1.0,0.0
jaeger,collector-config.yaml,0.0,0.0,1.0,0.0
jaeger,prometheus.yaml,0.0,0.0,1.0,0.0
jaeger,"Per the log file, there are more than 10,000 started threads.",0.0,0.0,1.0,0.0
jaeger,That's  a lot  even if we don't look at the less that 2 CPUs/cores reserved for the container (limits.cpu = request.cpu = 1600 millicores).,0.0,0.0,1.0,0.0
jaeger,"Each thread, and its stack, is allocated in memory separate from the heap.",0.0,0.0,1.0,0.0
jaeger,It is quite possible that the large number of started threads is the cause for the OOM problem.,0.135,0.065,0.8,-0.34
jaeger,"The JVM is started with the Native Memory Tracking related options ( -XX:NativeMemoryTracking=detail, -XX:+UnlockDiagnosticVMOptions, -XX:+PrintNMTStatistics)  that could help to see the memory usage, including what's consumed by those threads.",0.0,0.091,0.909,0.4019
jaeger,This doc  could be a starting point for Java 11.,0.0,0.0,1.0,0.0
jaeger,"In any case, it would be highly recommended to  not  have that many threads started.",0.0,0.13,0.87,0.2716
jaeger,E.g.,0.0,0.0,1.0,0.0
jaeger,"use a pool, start and stop them when not needed anymore...",0.196,0.0,0.804,-0.296
jaeger,There are two reasons a container is OOM Killed: Container Quota and System Quota.,0.273,0.0,0.727,-0.6705
jaeger,OOM Killer  only  triggers with memory related issues.,0.381,0.0,0.619,-0.6486
jaeger,"If your system is far from being out of memory, there is probably a limit in your container.",0.0,0.0,1.0,0.0
jaeger,"To your process inside the pod, the pod resource limit is like the whole system being OOM.",0.0,0.135,0.865,0.3612
jaeger,"Also, it's worth checking the Resource Requests because by default they are not set.",0.0,0.128,0.872,0.2263
jaeger,Requests must be less than or equal to container limits.,0.0,0.0,1.0,0.0
jaeger,That means that containers could be overcommitted on nodes and killed by  OOMK if multiple containers are using more memory than their respective requests at the same time.,0.135,0.084,0.781,-0.4019
jaeger,In my case the issue was with debugger component that is located in CMD line of Docker file,0.0,0.0,1.0,0.0
jaeger,After removal application stopped leaking.,0.322,0.0,0.678,-0.2263
jaeger,But disappeared only native memory leak.,0.577,0.0,0.423,-0.6652
jaeger,As later investigated there also was heap memory leak induced by jaegger tracer component (luckily here we have much more tools).,0.107,0.0,0.893,-0.34
jaeger,After its removal application became stable.,0.0,0.306,0.694,0.296
jaeger,I don't know if those components were leaky by itself or with combination with other components but fact is that now it is stable.,0.0,0.113,0.887,0.4215
jaeger,"Istio have this feature called  Distributed Tracing , which enables users to track requests in mesh that is distributed across multiple services.",0.0,0.0,1.0,0.0
jaeger,"This can be used to visualize request latency, serialization and parallelism.",0.0,0.0,1.0,0.0
jaeger,For this to work Istio uses  Envoy Proxy - Tracing  feature.,0.0,0.0,1.0,0.0
jaeger,You can deploy  Bookinfo Application  and see how  Trace context propagation  works.,0.0,0.0,1.0,0.0
jaeger,"If you have the same issue explained in this ticket, you need to wait for the next release of micronaut or use the workaround mentioned by micronaut guys there.",0.0,0.0,1.0,0.0
jaeger,https://github.com/micronaut-projects/micronaut-core/issues/2209,0.0,0.0,1.0,0.0
jaeger,"latest  is just a tag like any other -- you will want  docker image inspect , which will give you information about the other tags on your image.",0.0,0.137,0.863,0.4215
jaeger,"In the case of  jaegertracing/jaeger-agent:latest , it doesn't look this image has any other tags, so it's probable that this image is tracking something like the master branch of a source control repository, i.e., it doesn't correspond to a published version at all.",0.0,0.06,0.94,0.3612
jaeger,"As @max-gasner mentioned, it's common for  latest  to be tracking the  master  branch of a git repository.",0.0,0.0,1.0,0.0
jaeger,This allows the engineers to quickly build and test images before they are released and version tagged.,0.0,0.0,1.0,0.0
jaeger,This is one of the reasons why it's not recommended to ever use  latest  tags for anything critical where you need reproducibility.,0.163,0.0,0.837,-0.4389
jaeger,"jaegertracing/jaeger-agent:latest  doesn't have any other tags so the only way to determine which ""version"" of  latest  you are using is to look at the digest.",0.0,0.0,1.0,0.0
jaeger,This uniquely identifies the image build.,0.0,0.0,1.0,0.0
jaeger,Tags actually resolve to digests.,0.0,0.394,0.606,0.3818
jaeger,"So when a new image is built with the  latest  tag, that tag will then resolve to the digest of the new image.",0.0,0.11,0.89,0.3818
jaeger,DockerHub only shows the short version.,0.0,0.0,1.0,0.0
jaeger,You can inspect the full digest like this:,0.0,0.263,0.737,0.3612
jaeger,There are similar ideas in this zipkin/brave repo by @jeqo.,0.0,0.0,1.0,0.0
jaeger,https://github.com/jeqo/brave/tree/kafka-streams-processor/instrumentation/kafka-streams,0.0,0.0,1.0,0.0
jaeger,There also seems to be something available in opentracing-contrib repo but it seems to only at trace producer/consumer level.,0.0,0.0,1.0,0.0
jaeger,https://github.com/opentracing-contrib/java-kafka-client/tree/master/opentracing-kafka-streams,0.0,0.0,1.0,0.0
jaeger,"As, you can see there are few missing components - There are few pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc.",0.188,0.0,0.812,-0.5267
jaeger,These components were available in 1.4.2.,0.0,0.0,1.0,0.0
jaeger,These components where merged with version 1.5 into one service named  istiod .,0.0,0.0,1.0,0.0
jaeger,See:  https://istio.io/latest/blog/2020/istiod/,0.0,0.0,1.0,0.0
jaeger,"In 1.4.2 installation I could see grafana, jaeger, kiali, prometheus, zipkin dashboards.",0.0,0.0,1.0,0.0
jaeger,But these are now missing.,0.412,0.0,0.588,-0.4215
jaeger,These AddonComponents must be installed manually and are not part of  istioctl  since version 1.7.,0.0,0.0,1.0,0.0
jaeger,See:  https://istio.io/latest/blog/2020/addon-rework/,0.0,0.0,1.0,0.0
jaeger,So your installation is not broken.,0.0,0.338,0.662,0.3724
jaeger,It's just a lot has changed since 1.4.,0.0,0.0,1.0,0.0
jaeger,I would suggest to go through the release announcements to read about all changes:  https://istio.io/latest/news/releases/,0.0,0.0,1.0,0.0
jaeger,Iv finally found the solution.,0.0,0.365,0.635,0.3182
jaeger,It seemed to have to do with how the reporter is started up.,0.0,0.0,1.0,0.0
jaeger,"Anyhow, I changed my tracer class to this.",0.0,0.0,1.0,0.0
jaeger,I know there are several inactive variables here right now.,0.0,0.0,1.0,0.0
jaeger,Will see if they still can be of use some how.,0.0,0.0,1.0,0.0
jaeger,But none is needed right now to get it rolling.,0.0,0.0,1.0,0.0
jaeger,Hope this might help someone else trying to get the .NET Core working properly together with a remote Jeagertracing server.,0.0,0.248,0.752,0.6808
jaeger,I use  io.opentracing.contrib:opentracing-spring-rabbitmq-starter:3.0.0 .,0.0,0.0,1.0,0.0
jaeger,You can find some documentation  here .,0.0,0.0,1.0,0.0
jaeger,With this mechanism I achieved exactly what you asked for.,0.0,0.0,1.0,0.0
jaeger,I found 2 important things to make sure:,0.0,0.506,0.494,0.4767
jaeger,Hopefully you find this helpful.,0.0,0.665,0.335,0.714
jaeger,Ugh.,1.0,0.0,0.0,-0.4215
jaeger,I am an idiot.,0.623,0.0,0.377,-0.5106
jaeger,Here is what was going wrong for anyone else who might be stuck something like this:,0.248,0.121,0.631,-0.3818
jaeger,"The frontend application  is  receiving a header, I was just looking in the wrong place.",0.205,0.0,0.795,-0.4767
jaeger,The request comes from the load balancer to the node frontend microservice which sends its response to the browser.,0.0,0.0,1.0,0.0
jaeger,"I was checking the browser for the header, but the node frontend microservice was not forwarding this header to the browser.",0.0,0.0,1.0,0.0
jaeger,If anyone is interested; I ended up solving this by creating some publish and consume MassTransit middleware to do the trace propagation via trace injection and extraction respectively.,0.0,0.297,0.703,0.8271
jaeger,I've put the solution up on GitHub -  https://github.com/yesmarket/MassTransit.OpenTracing,0.0,0.247,0.753,0.3182
jaeger,Still interested to hear if there's a better way of doing this...,0.0,0.384,0.616,0.6808
jaeger,"In istio 1.1, the default sampling rate is 1%, so you need to send at least 100 requests before the first trace is visible.",0.0,0.0,1.0,0.0
jaeger,This can be configured through the  pilot.traceSampling  option.,0.0,0.0,1.0,0.0
jaeger,OpenTracing is the API that  your  code will interact with directly.,0.0,0.0,1.0,0.0
jaeger,"Basically, your application would be ""instrumented"" using the OpenTracing API and a concrete tracer (like Jaeger or Brave/Zipkin) would capture the data and send it somewhere.",0.0,0.0,1.0,0.0
jaeger,This allows your application to use a neutral API throughout your code so that you can change from one provider to another without having to change your entire code base.,0.0,0.0,1.0,0.0
jaeger,"Another way to think about it is that OpenTracing is like SLF4J in the Java logging world, whereas Jaeger and Zipkin are the concrete implementations, such as Log4j in the Java logging world.",0.0,0.072,0.928,0.3612
jaeger,Trace context propagation might be missing.,0.306,0.0,0.694,-0.296
jaeger,https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-context-propagation,0.0,0.0,1.0,0.0
jaeger,"I cannot test your code, but the only thing i can think off, is that the order of execution is wrong.",0.187,0.0,0.813,-0.631
jaeger,"You first make a string, then you make it Base64, then you encrypt it.",0.0,0.0,1.0,0.0
jaeger,Now you undo the Base64 and afterwards you decrypt the encoded string.,0.0,0.0,1.0,0.0
jaeger,These last two must be swapped.,0.0,0.0,1.0,0.0
jaeger,"A reminder that  security is unusually treacherous territory , and if there's a way to call on other well-tested code even more of your toplevel task than just what Go's OpenPGP package is handling for you, consider it.",0.0,0.066,0.934,0.34
jaeger,It's good that at least low-level details are outsourced to  openpgp  because they're nasty and so so easy to get wrong.,0.218,0.228,0.554,0.0745
jaeger,"But tiny mistakes at any level can make crypto features worse than useless; if there's a way to write less security-critical code, that's one of the best things anyone can do for security.",0.236,0.189,0.574,-0.296
jaeger,"On the specific question: you have to  Close()  the writer to get everything flushed out (a trait OpenPGP's writer shares with, say,  compress/gzip 's).",0.0,0.087,0.913,0.296
jaeger,"Unrelated changes: the way you're printing things is a better fit  log.Println , which just lets you pass a bunch of values you want printed with spaces in between (like, say, Python  print ), rather than needing format specifiers like  ""%s""  or  ""%d"" .",0.0,0.254,0.746,0.872
jaeger,"(The ""EXTRA"" in your initial output is what Go's  Printf  emits when you pass more things than you had format specifiers for.)",0.0,0.0,1.0,0.0
jaeger,"It's also best practice to check errors (I dropped  if err != nil s where I saw a need, but inelegantly and without much thought, and I may not have gotten all the calls) and to run  go fmt  on your code.",0.042,0.071,0.887,0.2942
jaeger,"Again,  I can't testify to the seaworthiness of this code or anything like that.",0.0,0.172,0.828,0.3612
jaeger,But now it round-trips all the text.,0.0,0.0,1.0,0.0
jaeger,I wound up with:,0.0,0.0,1.0,0.0
jaeger,"You could try with  StartSpanFromContext , inside your gRPC handlers:",0.0,0.0,1.0,0.0
jaeger,As the documentation of  otgrpc.OpenTracingServerInterceptor  says:,0.0,0.0,1.0,0.0
jaeger,"[...] the server Span will be embedded in the context.Context for the
application-specific gRPC handler(s) to access.",0.0,0.0,1.0,0.0
jaeger,If we look at the function implementation:,0.0,0.0,1.0,0.0
jaeger,"
 Edit : Given the above, you probably can omit this code:",0.0,0.0,1.0,0.0
jaeger,The issue was related to  yaml  file parsing,0.0,0.0,1.0,0.0
jaeger,"Jaeger has a UI to look at your data, but no tools to create statistics.",0.16,0.152,0.688,-0.0387
jaeger,However all your data is being stored in a DB of your choice.,0.0,0.0,1.0,0.0
jaeger,Storing it in e.g.,0.0,0.0,1.0,0.0
jaeger,Elasticsearch gives you a powerful query language to look at the data as well as many other tools that integrate with it.,0.0,0.205,0.795,0.5994
jaeger,"Correct me, if I'm wrong.",0.437,0.0,0.563,-0.4767
jaeger,"If you mean how to find the trace-id on the server side, you can try to access the OpenTracing span by  get_active_span .",0.0,0.0,1.0,0.0
jaeger,"The trace-id, I suppose, should be one of the tags in it.",0.0,0.0,1.0,0.0
jaeger,I had missed a key piece of documentation.,0.306,0.0,0.694,-0.296
jaeger,"In order to get a trace ID, you have to create a span on the client side.",0.0,0.13,0.87,0.2732
jaeger,This span will have the trace ID that can be used to examine data in the Jaeger UI.,0.0,0.0,1.0,0.0
jaeger,The span has to be added into the GRPC messages via an  ActiveSpanSource  instance.,0.0,0.0,1.0,0.0
jaeger,"Of course, you could switch the ordering of the  with  statements so that the span is created after the GRPC channel.",0.0,0.091,0.909,0.25
jaeger,That part doesn't make any difference.,0.0,0.0,1.0,0.0
jaeger,"You assumption is correct, the elements are there, but not exactly where you think they are.",0.0,0.0,1.0,0.0
jaeger,To easily check if an element is part of the response html and not being loaded by javascript I normally recommend using a  browser plugin to disable javascript .,0.0,0.17,0.83,0.5994
jaeger,"If you want the images, they are still part of the html response, you can get them with:",0.0,0.071,0.929,0.0772
jaeger,the main image appears separately:,0.0,0.0,1.0,0.0
jaeger,Hope that helps you.,0.0,0.733,0.267,0.6705
jaeger,You haven't initialized the variables for the next few iterations.,0.0,0.0,1.0,0.0
jaeger,You need to reinitialize the variables used for while loop's condition check outside their respective while loops.,0.0,0.149,0.851,0.4215
jaeger,i.e,0.0,0.0,1.0,0.0
jaeger,Similarly do it for the while loops which use variables  c  &amp;  d .,0.0,0.0,1.0,0.0
jaeger,"Daniel's answer is correct  : the 
structure of the  while  loop should be:",0.0,0.0,1.0,0.0
jaeger,"It may not be possible to input text with conditional formatting, but you can change the font color.",0.0,0.0,1.0,0.0
jaeger,"A solution could be to put the word ""LATE in the specified cell(s) beforehand and set the font-color equal to the background-color, which makes the word invisable.",0.0,0.084,0.916,0.3182
jaeger,"When the condition (formula) evaluates true, the new format will change the font-color and the word LATE appears.",0.0,0.141,0.859,0.4215
jaeger,No VBA requiered.,0.524,0.0,0.476,-0.296
jaeger,On the other hand: wouldn't a simple if-formula be better?,0.0,0.466,0.534,0.7269
jaeger,Something like:,0.0,0.714,0.286,0.3612
jaeger,If you wish you can then change the background with conditional formating,0.0,0.197,0.803,0.4019
jaeger,There's really not many options for other than starting a span in each function you'd like to instrument:,0.0,0.135,0.865,0.3612
jaeger,"If your functions have a common call signature, or you can coalesce your function into a common call signature, you can write a wrapper.",0.0,0.0,1.0,0.0
jaeger,"Examples of this can be seen in http  ""middleware"" .",0.0,0.0,1.0,0.0
jaeger,"Consider the http.Handler, you could write a  decorator  for your functions that handles the span lifecycle:",0.0,0.0,1.0,0.0
jaeger,A similar pattern could be applied by  embedding  structs.,0.0,0.0,1.0,0.0
jaeger,Is it possible to set these when working on OpenShift?,0.0,0.0,1.0,0.0
jaeger,"Yes, you can configure it for the Che master of your installation.",0.0,0.197,0.803,0.4019
jaeger,OpenShift is the Saas Che offering,0.0,0.0,1.0,0.0
jaeger,As a user of che.openshift.io you can't leverage from tracing capabilities of Che at this moment.,0.0,0.0,1.0,0.0
jaeger,This 'appears' to be related to the switch from AWS CNI to weave.,0.0,0.0,1.0,0.0
jaeger,"CNI uses the IP range of your VPC while weave uses its own address range (for pods), so there may be remaining iptables rules from AWS CNI, for example.",0.0,0.0,1.0,0.0
jaeger,"Internal error occurred: failed calling admission webhook ""pilot.validation.istio.io"": Post  https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s : Address is not allowed",0.333,0.0,0.667,-0.7184
jaeger,"The message above implies that whatever address  istio-galley.istio-system.svc  resolves to, internally in your K8s cluster, is not a valid IP address.",0.0,0.082,0.918,0.1779
jaeger,So I would also try to see what that resolves to.,0.0,0.159,0.841,0.1779
jaeger,(It may be related to  coreDNS ).,0.0,0.0,1.0,0.0
jaeger,You can also try the following  these steps ;,0.0,0.0,1.0,0.0
jaeger,"Basically, (quoted)",0.0,0.0,1.0,0.0
jaeger,"Furthermore, you can try reinstalling everything from the beginning using weave.",0.0,0.0,1.0,0.0
jaeger,Hope it helps!,0.0,0.853,0.147,0.6996
jaeger,OpenTracing is a framework for Distributed Tracing.,0.0,0.0,1.0,0.0
jaeger,"As such, it is more about performance monitoring and observability than logging (what NLog is about).",0.0,0.0,1.0,0.0
jaeger,OpenTracing allows you to manually instrument your code to generate traces with relevant spans containing information about code execution in your app.,0.0,0.0,1.0,0.0
jaeger,"This includes annotating spans with errors and arbitrary keys and values, which you  could  use instead of logging.",0.114,0.128,0.758,0.0772
jaeger,"However, that's not the same as dedicated structured logging.",0.0,0.273,0.727,0.4588
jaeger,Here's an article/guide on how to work with the limit-ranger and its default values [1],0.0,0.162,0.838,0.4019
jaeger,[1] https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b,0.0,0.0,1.0,0.0
jaeger,"The  span.kind=server  tag denotes an entry span, e.g.",0.0,0.0,1.0,0.0
jaeger,a span created in the local code in response to an external request.,0.0,0.154,0.846,0.25
jaeger,"Likewise,  span.kind=client  denotes an exit span, e.g.",0.0,0.0,1.0,0.0
jaeger,a call made from the local code to another server.,0.0,0.0,1.0,0.0
jaeger,"In your example, the span generated for Foo is a  span.kind=server  and the span recording the call to Buzz is a  span.kind=client .",0.0,0.0,1.0,0.0
jaeger,Kubernetes provides quite a big variety of Networking and Load Balancing features from the box.,0.0,0.0,1.0,0.0
jaeger,"However, the idea to simplify and extend the functionality  of  Istio sidecars  is a good choice as they are used for automatic injection into the Pods in order to proxy the traffic between internal Kubernetes services.",0.0,0.122,0.878,0.5574
jaeger,You can implement  sidecars  manually or automatically.,0.0,0.0,1.0,0.0
jaeger,"If you choose the manual way, make sure to add the appropriate parameter under Pod's annotation field:",0.0,0.126,0.874,0.3182
jaeger,"Automatic  sidecar  injection requires  Mutating Webhook admission controller , available since Kubernetes version 1.9 released, therefore  sidecars  can be integrated for Pod's creation process as well.",0.0,0.154,0.846,0.4939
jaeger,Get yourself familiar with this  Article  to shed light on using different monitoring and traffic management tools in Istio.,0.0,0.0,1.0,0.0
jaeger,Yes - it is possible to use external services with istio.,0.0,0.231,0.769,0.4019
jaeger,"You can disable grafana and prometheus just by setting proper flags in values.yaml of istio helm chart (grafana.enabled=false, etc).",0.0,0.0,1.0,0.0
jaeger,"You can check  kyma-project  project to see how istio is integrated with prometheus-operator, grafana deployment with custom dashboards, and custom jaeger deployment.",0.0,0.0,1.0,0.0
jaeger,From your list only certmanager is missing.,0.268,0.0,0.732,-0.296
jaeger,I am not sure why Istio doesn't automatically trace your calls to external APIs.,0.141,0.0,0.859,-0.2411
jaeger,"Perhaps it requires an egress gateway to be used, I'm not sure.",0.151,0.0,0.849,-0.2411
jaeger,"Note also that Istio creates traces for http(s) traffic, not TCP.",0.0,0.174,0.826,0.2732
jaeger,"However, this is something you can still do programmatically.",0.0,0.0,1.0,0.0
jaeger,You can use any of the  Jaeger client libraries  to augment&quot;the traces already created by Envoy by appending your own spans.,0.0,0.091,0.909,0.25
jaeger,"To do so, you need first to extract the trace context from the HTTP headers of the incoming request (assuming that your external API calls are consecutive to an incoming request), and then create a new span as child of that previous span context.",0.0,0.048,0.952,0.2732
jaeger,A good idea would be to use  OpenTracing semantic conventions  when you tag your new span.,0.0,0.172,0.828,0.4404
jaeger,Tools like Kiali will be able to leverage some information if it follows this convention.,0.0,0.152,0.848,0.3612
jaeger,I've found this blog post that explains how to do it with the nodejs jaeger client:  https://rhonabwy.com/2019/01/06/adding-tracing-with-jaeger-to-an-express-application/,0.0,0.0,1.0,0.0
jaeger,"Yes, the OpenCensus collector should be injected with the Linkerd proxy because the proxies themselves send the span info using mTLS.",0.0,0.119,0.881,0.4019
jaeger,"With mTLS, the sending (client) and receiving (server) sides of the request must present certificates to each other in to  verify  that identities to each other in a way that validates that the identity was issued by the same trusted source.",0.0,0.126,0.874,0.6705
jaeger,The Linkerd service mesh is made up of the control plane and the data plane.,0.0,0.0,1.0,0.0
jaeger,The control plane is a set of services that run within the cluster to implement the features of the service mesh.,0.0,0.0,1.0,0.0
jaeger,Mutual TLS (mTLS) is one of those features and is implemented by the  linkerd-identity  component of the control plane.,0.0,0.0,1.0,0.0
jaeger,"The data plane is comprised of any number of the Linkerd proxies which are injected into the services in the application, like the OpenCensus collector.",0.0,0.142,0.858,0.4215
jaeger,"Whenever a proxy is started within a pod, it sends a certificate signing request to the  linkerd-identity  component and receives a certificate in return.",0.0,0.0,1.0,0.0
jaeger,"So, when the Linkerd proxies in the control plane send the spans to the collector, they authenticate themselves with those certificates, which must be verified by the proxy injected into the OpenCensus collector Pod.",0.0,0.0,1.0,0.0
jaeger,"This ensures that all traffic, even distributed traces, are sent securely within the cluster.",0.0,0.156,0.844,0.34
jaeger,"In your case, you should suffix the service account with the namespace.",0.0,0.0,1.0,0.0
jaeger,"By default, Linkerd will use the Pod namespace, so if the service account doesn't exist in the Pod namespace, then the configuration will be invalid.",0.0,0.0,1.0,0.0
jaeger,"The  logic  has a function that checks for a namespace in the annotation name and appends it, if it exists:",0.0,0.0,1.0,0.0
jaeger,"So, this one is correct:",0.0,0.0,1.0,0.0
jaeger,you are using System.Configuration namespace which causes ambiguity.,0.0,0.0,1.0,0.0
jaeger,i would suggest remove the using System.Configuration.,0.0,0.0,1.0,0.0
jaeger,And try specifying fully qualified name for the Configuration.,0.0,0.0,1.0,0.0
jaeger,visual studio would suggest possible candidates (press Ctrl .,0.0,0.0,1.0,0.0
jaeger,on the Class name you want to qualify) provided you have added all required references in project already.,0.0,0.071,0.929,0.0772
jaeger,If you have a,0.0,0.0,1.0,0.0
jaeger,then the C# compiler gets confused with  Configuration  and thinks it refers to the namespace  System.Configuration .,0.133,0.0,0.867,-0.3182
jaeger,You can solve it by using the explicit namespace  Jaeger :,0.0,0.167,0.833,0.2023
jaeger,There is no possibility of doing it in the Dockerfile if you want to keep two separate image.,0.113,0.067,0.821,-0.2263
jaeger,How should you know in advance the name/id of the container you're going to link ?,0.0,0.0,1.0,0.0
jaeger,Below are two solutions :,0.0,0.362,0.638,0.1779
jaeger,"I recommend you using  netwoking , by creating:",0.0,0.54,0.46,0.5719
jaeger,"and then run with --network=&quot;network&quot;
using docker-compose with  network  and link to each other
example:",0.0,0.0,1.0,0.0
jaeger,-I  is used for  include  paths.,0.0,0.0,1.0,0.0
jaeger,Use  -L  for library paths:,0.0,0.0,1.0,0.0
jaeger,It also looks like you've linked with a shared library  libyaml-cppd.so  - not the static library  libyaml-cpp.a .,0.0,0.274,0.726,0.5994
jaeger,I don't recognize the  d  in  libyaml-cppd.so  though.,0.0,0.0,1.0,0.0
jaeger,I'd check if that's really the library you built.,0.0,0.0,1.0,0.0
jaeger,"libyaml-cpp  will be built as a static library by default ( libyaml-cpp.a ) and on a 64 bit machine, it will probably default to being installed in  /usr/local/lib64 .",0.0,0.0,1.0,0.0
jaeger,You are only allowed to do very limited things in  namespace std .,0.166,0.0,0.834,-0.2944
jaeger,Adding new functions/classes are not allowed (unless as template specializations including user defined types) - so remove  namespace std { ... }  around your program.,0.0,0.0,1.0,0.0
jaeger,Also.,0.0,0.0,1.0,0.0
jaeger,the  main  function should be in the global namespace.,0.0,0.0,1.0,0.0
jaeger,The reason it's not found by the linker is because you put it in a namespace ( std ).,0.0,0.0,1.0,0.0
jaeger,UPDATE : The issue is resolved follow this link exaclty  https://github.com/jaegertracing/jaeger-client-cpp/issues/162#issuecomment-565892473  (use thrift version 0.11 or 0.11+),0.0,0.102,0.898,0.1779
jaeger,Fisrt of all according to istio  documentation  Prometheus is used as default observation operator in istio mesh by default:,0.0,0.0,1.0,0.0
jaeger,The  default Istio metrics  are defined by a set of configuration artifacts that ship with Istio and are exported to  Prometheus  by default.,0.0,0.0,1.0,0.0
jaeger,"Operators are free to modify the shape and content of these metrics, as well as to change their collection mechanism, to meet their individual monitoring needs.",0.0,0.184,0.816,0.6597
jaeger,So by having istio injected prometheus operator You end up with two Prometheus operators in Your istio mesh.,0.0,0.0,1.0,0.0
jaeger,"Secondly, when you enforce Mutual TLS in Your istio mesh every connection has to be secure ( TLS ).",0.0,0.124,0.876,0.34
jaeger,And as You mentioned it works when there is no istio injection.,0.167,0.0,0.833,-0.296
jaeger,So the most likely cause is that the readiness probe fails because it is using  HTTP  protocol which is insecure (plain text) and this is one of the reason why You would get  503  error.,0.201,0.048,0.751,-0.743
jaeger,"If you really need prometheus operator within istio mesh, this could be fixed by creating  DestinationRule  to  Disable  tls mode just for the readiness probe.",0.0,0.154,0.846,0.4939
jaeger,Example:,0.0,0.0,1.0,0.0
jaeger,Note: Make sure to modify it so that it matches Your namespaces and hosts.,0.0,0.15,0.85,0.3182
jaeger,Also there could be some other prometheus collisions within mesh.,0.189,0.0,0.811,-0.2732
jaeger,The other solution would be not to have prometheus istio injected in the first place.,0.0,0.141,0.859,0.3182
jaeger,You can disable istio injection in prometheus namespace by using the following commands:,0.0,0.0,1.0,0.0
jaeger,I was able to publish some spans so I could see them on  http://localhost:16686,0.0,0.0,1.0,0.0
jaeger,This is the updated main function:,0.0,0.0,1.0,0.0
jaeger,go run  compiles and runs the named main Go package.,0.0,0.0,1.0,0.0
jaeger,"Only  go build  or  go install  would compile the packages named by the import paths, along with their dependencies,",0.0,0.0,1.0,0.0
jaeger,I guess if you set sampler to 0 in the configuration then no traces will be captured.,0.136,0.0,0.864,-0.296
jaeger,https://github.com/jaegertracing/jaeger-client-java#testing,0.0,0.0,1.0,0.0
jaeger,But it's specific to Jaeger.,0.0,0.0,1.0,0.0
jaeger,Otherwise you can use NoopTracer like  Tracer tracer = NoopTracerFactory.create();   Maven,0.0,0.217,0.783,0.3612
jaeger,You can set the service name in the code as follows:,0.0,0.0,1.0,0.0
jaeger,"Based on some of the hints provided by Opentelemetry Java community, created two providers which indeed creates two services.",0.0,0.194,0.806,0.4767
jaeger,"Reusing same exporter, so that multiple connections to the backend can be avoided.",0.167,0.0,0.833,-0.34
jaeger,I will learn more about  reusing exporter to create two or more provides in the same application in coming days.,0.0,0.104,0.896,0.2732
jaeger,"Thank you to amazing Opentelemetry java community - @John Watson (jkwatson), Bogdan Drutu, Rupinder Singh, Anuraag Agrawal.",0.0,0.31,0.69,0.743
jaeger,Based on this:,0.0,0.0,1.0,0.0
jaeger,The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.,0.089,0.0,0.911,-0.2144
jaeger,it seems that the tracing information is not propagated across services.,0.0,0.0,1.0,0.0
jaeger,You can check this by looking into the HTTP headers and check the  traceId .,0.0,0.0,1.0,0.0
jaeger,In order to make this work the  traceId  should be the same across the requests.,0.0,0.0,1.0,0.0
jaeger,You should see the same  traceId  in the logs too.,0.0,0.0,1.0,0.0
jaeger,The documentation gives you some pointers how to troubleshoot this:,0.0,0.167,0.833,0.2023
jaeger,"I'm not 100% sure what the problem is you're experiencing, but here's some things to consider.",0.192,0.0,0.808,-0.325
jaeger,"According to  this post on the Traefik forums , that message you're seeing is  debug  level because it's not something you should be worried about.",0.087,0.0,0.913,-0.296
jaeger,"It's just logging that no trace context was found, so a new one will be created.",0.128,0.116,0.756,-0.0516
jaeger,"That second part is not in the message, but apparently that's what happens.",0.0,0.0,1.0,0.0
jaeger,You should check to see if you're getting data appearing in Jaeger.,0.0,0.0,1.0,0.0
jaeger,"If you are, that message is probably nothing to worry.",0.0,0.211,0.789,0.3412
jaeger,"If you are getting data in Jaeger, but it's not connected, that will be because Traefik can only only work with trace context that is already in inbound requests, but it can't add trace context to outbound requests.",0.0,0.0,1.0,0.0
jaeger,"Within your application, you'll need to implement trace propagation so that your outbound requests include the trace context that was received as part of the incoming request.",0.0,0.0,1.0,0.0
jaeger,"Without doing that, every request will be sent without trace context and will start a new trace when it is received at the next Traefik ingress point.",0.0,0.0,1.0,0.0
jaeger,The problem actually was with the  traceContextHeaderName .,0.31,0.0,0.69,-0.4019
jaeger,Sadly I can not tell exactly what the problem was as the  git diff  only shows that nothing changed around traefik and jaeger at the point where I fixed it.,0.175,0.0,0.825,-0.6705
jaeger,I assume config got &quot;stuck&quot; somehow.,0.0,0.0,1.0,0.0
jaeger,"I tracked down the  related lines in source , but as I am no Go-Dev, I can only guess if there's a bug.",0.141,0.0,0.859,-0.4215
jaeger,"What I did was to switch back to  uber-trace-id , which magically &quot;fixed&quot; it.",0.0,0.0,1.0,0.0
jaeger,"After I ran some traces and connected another service (node, npm  jaeger-client  with  process.env.TRACER_STATE_HEADER_NAME  set to an equal value), I switched back to  traefik-trace-id  and things worked.",0.0,0.0,1.0,0.0
jaeger,"To my knowledge, the design of  ForkJoinPool.commonPool()  makes it impossible to actually replace that pool programmatically with an instrumented version.",0.0,0.0,1.0,0.0
jaeger,So the only workaround is to do it via bytecode manipulation.,0.18,0.0,0.82,-0.296
jaeger,"The  OpenTelemetry Java Automatic Instrumentation  libraries perform a lot of magic to be able to take care of correctly propagating context through async/concurrency primitives, you may want to give them a try.",0.0,0.138,0.862,0.5423
jaeger,"Tracing is enabled by default for JAX-RS endpoints only, not for reactive routes at the moment.",0.0,0.0,1.0,0.0
jaeger,You can activate tracing by annotating your route with  @org.eclipse.microprofile.opentracing.Traced .,0.0,0.0,1.0,0.0
jaeger,"Yes, adding @Traced enable to activate tracing on reactive routes.",0.0,0.231,0.769,0.4019
jaeger,"Unfortunately, using both JAX-RS reactive and reactive routes bugs the tracing on event-loop threads used by JAX-RS reactive endpoint when they get executed.",0.098,0.0,0.902,-0.34
jaeger,"I only started Quarkus 2 days ago so i don't really the reason of this behavior (and whether it's normal or it's a bug), but obviously switching between two completely mess up the tracing.",0.113,0.0,0.887,-0.5704
jaeger,Here is an example to easily reproduce it:,0.0,0.255,0.745,0.34
jaeger,Here is a screenshot that show the issue,0.0,0.0,1.0,0.0
jaeger,"As you can see, as soon as the JAX-RS resource is it and executed on one of the two threads available, it &quot;corrupts&quot; it, messing the trace_id reported (i don't know if it's the generation or the reporting on logs that is broken) on logs for the next calls of the reactive route.",0.0,0.0,1.0,0.0
jaeger,"This does not happen on the JAX-RS resource, as you can notice on the screenshot as well.",0.0,0.116,0.884,0.2732
jaeger,So it seems to be related to reactive routes only.,0.0,0.0,1.0,0.0
jaeger,Another point here is the fact that JAX-RS Reactive resources are incorrectly reported on Jaeger.,0.0,0.0,1.0,0.0
jaeger,(with a mention to a missing root span) Not sure if it's related to the issue but that's also another annoying point.,0.281,0.0,0.719,-0.6839
jaeger,I'm thinking to completely remove the JAX-RS Reactive endpoint and replace them by normal reactive route to eliminate this bug.,0.0,0.0,1.0,0.0
jaeger,I would appreciate if someone with more experience than me could verify this or tell me what i did wrong :),0.125,0.23,0.645,0.3818
jaeger,"EDIT 1: I added a route filter with priority 500 to clear the MDC and the bug is still there, so definitely not coming from MDC.",0.0,0.217,0.783,0.7262
jaeger,EDIT 2: I opened a  bug report  on Quarkus,0.0,0.0,1.0,0.0
jaeger,"EDIT 3: It seems related to how both implementations works (thread locals versus context propagation in actor based context)
So, unless JAX-RS reactive resources are marked @Blocking (and get executed in a separated thread pool), JAX-RS reactive and Vertx reactive routes are incompatible when it comes to tracing (but also probably the same for MDC related informations since MDC is also thread related)",0.0,0.0,1.0,0.0
jaeger,"I tried applying your file on a Kubernetes 1.16 cluster, and there are a couple of issues with it:",0.0,0.0,1.0,0.0
jaeger,The .spec.selector field defines how the Deployment finds which Pods to manage.,0.0,0.0,1.0,0.0
jaeger,It seems like you are applying something that is super old.,0.0,0.416,0.584,0.7506
jaeger,"Kubernetes notes the below in its doc, and so I wonder if this used to work on older versions of Kubernetes where selectors were defaulted.",0.0,0.0,1.0,0.0
jaeger,"As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the .spec.template.",0.0,0.0,1.0,0.0
jaeger,The pod selector will no longer be defaulted when left empty.,0.308,0.0,0.692,-0.4588
jaeger,"It seems like you should take a new approach-- in looking around, I found a couple of good tutorials  here  and  here , and Jaeger themselves offer a similar approach  here .",0.0,0.184,0.816,0.6597
jaeger,They all make use of  Kubernetes Operators .,0.0,0.0,1.0,0.0
jaeger,"A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user",0.0,0.122,0.878,0.3818
jaeger,I don't know what you mean by  &quot;So previously I had a binary executable (jaeger-agent) running on a Linux CentOS 8 box alongside a server side application&quot;,0.0,0.0,1.0,0.0
jaeger,"The file you are applying looks like it  deploys the agent as a daemonset , which means the agent is run as a pod on each node of your cluster.",0.0,0.088,0.912,0.3612
jaeger,"If it is running in your k8's cluster, then  this is how I normally approach troubleshooting kubernetes services .",0.0,0.096,0.904,0.1779
jaeger,"If it is running outside of your cluster entirely, then you need to make sure the Service it talks to is exposed outside of the cluster probably using type LoadBalancer.",0.041,0.073,0.886,0.25
jaeger,"Metrics in OpenTelemetry are currently undergoing continued development and refinement, so they aren't necessarily available for each language yet (see  this tag in the OpenTelemetry JS repo  for an example of metric instruments that aren't up to date yet with spec), but once they are, I'd expect for metrics to be added to the existing node/web instrumentation packages.",0.0,0.0,1.0,0.0
jaeger,"That said, I would still advise you to try out OpenTelemetry for traces at this point, as it's pretty stable for tracing.",0.0,0.221,0.779,0.6597
jaeger,"You can use a Prometheus client to export metrics separately, and once OpenTelemetry metrics are fully supported in the JS library, switch over to that.",0.0,0.101,0.899,0.3804
jaeger,There are a few different reasons why You could be experiencing this issue.,0.0,0.0,1.0,0.0
jaeger,From  istio  documentation:,0.0,0.0,1.0,0.0
jaeger,"Since Istio 1.0.3, the sampling rate for tracing has been reduced to 1% in the   default    configuration profile .",0.0,0.0,1.0,0.0
jaeger,This means that only 1 out of 100 trace instances captured by Istio will be reported to the tracing backend.,0.0,0.0,1.0,0.0
jaeger,The sampling rate in the   demo   profile is still set to 100%.,0.0,0.0,1.0,0.0
jaeger,See   this section   for more information on how to set the sampling rate.,0.0,0.0,1.0,0.0
jaeger,"If you still do not see any trace data, please confirm that your ports conform to the Istio   port naming conventions   and that the appropriate container port is exposed (via pod spec, for example) to enable traffic capture by the sidecar proxy (Envoy).",0.029,0.052,0.919,0.25
jaeger,"If you only see trace data associated with the egress proxy, but not the ingress proxy, it may still be related to the Istio   port naming conventions .",0.0,0.0,1.0,0.0
jaeger,Starting with   Istio 1.3   the protocol for   outbound   traffic is automatically detected.,0.0,0.0,1.0,0.0
jaeger,Hope it helps.,0.0,0.846,0.154,0.6705
jaeger,"The short answer is ""you can't.""",0.0,0.0,1.0,0.0
jaeger,My question was based on a very fundamental misunderstanding of what opentracing does.,0.219,0.0,0.781,-0.4728
jaeger,"Tracing context is only propagated downstream, not upstream.",0.0,0.0,1.0,0.0
jaeger,From the same discussion thread:,0.0,0.0,1.0,0.0
jaeger,"On the wire propagation is only meant to carry ""span context"", which
  is a small set of ID fields and possible baggage.",0.0,0.0,1.0,0.0
jaeger,"Returning the whole
  trace as part of the request is not a use case that was considered.",0.0,0.0,1.0,0.0
jaeger,and,0.0,0.0,1.0,0.0
jaeger,The trace collection is meant to be asynchronous and out of process.,0.0,0.0,1.0,0.0
jaeger,So my understanding is now thus:,0.0,0.0,1.0,0.0
jaeger,"Each individual software component creates its own tracing data, bundles it up, and sends it off to the tracing server (e.g.",0.0,0.095,0.905,0.2732
jaeger,Jaeger).,0.0,0.0,1.0,0.0
jaeger,Each software component  must  be configured to use the same tracing provider and the same tracing server - an RPC client cannot tell an RPC server that for a particular trace it should use the Jaeger tracing provider and a Jaeger server at such-and-such an address.,0.0,0.0,1.0,0.0
jaeger,"(At least, the opentracing standard doesn't provide a way to do this.)",0.0,0.0,1.0,0.0
jaeger,The tracing information injected into a RPC request by the client allows the RPC server to embed a 'parent' ID field into the tracing information.,0.0,0.0,1.0,0.0
jaeger,It's then the responsibility of the tracer (e.g.,0.0,0.0,1.0,0.0
jaeger,Jaeger) to figure out the relationships between the various traces it has received from various software components by matching up ID codes embedded in them.,0.0,0.0,1.0,0.0
jaeger,So what I wanted to do is not a use case considered by opentracing and is not possible.,0.0,0.0,1.0,0.0
jaeger,My interpretation of this is that We need to keep in mind that communication between services needs to support forwarding/&quot;passing along&quot; the trace ID's so that the tracing works correctly.,0.0,0.085,0.915,0.4019
jaeger,So it warns us against situations where:,0.219,0.0,0.781,-0.1725
jaeger,Client calls -&gt; Service A #using http request with trace ID in header.,0.0,0.0,1.0,0.0
jaeger,Service A -&gt; Service B #using tcp request that does not support headers and the trace ID header is lost.,0.222,0.0,0.778,-0.5511
jaeger,This situation could break or limit tracing functionality.,0.0,0.0,1.0,0.0
jaeger,On the other hand If we have situation where:,0.0,0.286,0.714,0.4939
jaeger,Client calls -&gt; Service A #using http request with trace ID in header.,0.0,0.0,1.0,0.0
jaeger,Service A -&gt; Service B #using http request the trace ID is forwarded to service B.,0.0,0.0,1.0,0.0
jaeger,This situation allows for the trace ID header to be present in both connections so the tracing can be logged and then viewed in tracing service dashboard.,0.0,0.0,1.0,0.0
jaeger,Then We can explore the path taken by the request and view the latency incurred at each hop.,0.0,0.0,1.0,0.0
jaeger,Hope it helps.,0.0,0.846,0.154,0.6705
jaeger,"When doing tracing in a service mesh, behind proxies, the traceID generated upon the initial client call is propagated automatically only so long as the call goes from proxy- proxy.",0.0,0.0,1.0,0.0
jaeger,So:,0.0,0.0,1.0,0.0
jaeger,"To get around this, Microservice A just needs to know which headers represent the traceIDs, how to append into it, and some state to make sure it makes it to outgoing requests.",0.0,0.134,0.866,0.5423
jaeger,Then you'll get a full transaction chain.,0.0,0.0,1.0,0.0
jaeger,"Without the service propagating the headers, tracing basically gives you each path that ends in a microservice.",0.0,0.0,1.0,0.0
jaeger,"Still useful, but not as complete of a picture.",0.0,0.218,0.782,0.2382
jaeger,"By default, you don't have a logging system on Istio.",0.0,0.0,1.0,0.0
jaeger,"I mean, besides the native logging of Kubernetes.",0.0,0.0,1.0,0.0
jaeger,"Zipkin and Jaeger are tracing systems, meaning for latency, not for logging.",0.0,0.0,1.0,0.0
jaeger,"You can definitely get this info through Istio components, but you will have to set it up first.",0.0,0.098,0.902,0.2144
jaeger,I found  this  articles; in Istio website about how to collect logs.,0.0,0.0,1.0,0.0
jaeger,I would say  Fluentd  +  Elasticsearch  would give you something as powerful as you need.,0.0,0.189,0.811,0.4215
jaeger,Unfortunately I don't have any examples.,0.375,0.0,0.625,-0.34
jaeger,According to envoy proxy documentation for envoy  v1.12.0  used by istio  1.3 :,0.0,0.0,1.0,0.0
jaeger,Envoy provides the capability for reporting tracing information regarding communications between services in the mesh.,0.0,0.0,1.0,0.0
jaeger,"However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests.",0.0,0.063,0.937,0.2732
jaeger,"Whichever tracing provider is being used, the service should propagate the   x-request-id   to enable logging across the invoked services to be correlated.",0.0,0.0,1.0,0.0
jaeger,"The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood.",0.0,0.0,1.0,0.0
jaeger,"This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests.",0.0,0.0,1.0,0.0
jaeger,"This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace.",0.0,0.161,0.839,0.6124
jaeger,Alternatively the trace context can be manually propagated by the service:,0.0,0.0,1.0,0.0
jaeger,"When using the LightStep tracer, Envoy relies on the service to propagate the 
   x-ot-span-context 
  HTTP header while sending HTTP requests to other services.",0.0,0.0,1.0,0.0
jaeger,"When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( 
   x-b3-traceid ,
   x-b3-spanid ,
   x-b3-parentspanid ,
   x-b3-sampled ,
  and 
   x-b3-flags ).",0.0,0.0,1.0,0.0
jaeger,"The 
   x-b3-sampled 
  header can also be supplied by an external client to either enable or
  disable tracing for a particular request.",0.0,0.0,1.0,0.0
jaeger,"In addition, the single 
   b3 
  header propagation format is supported, which is a more compressed
  format.",0.0,0.141,0.859,0.3182
jaeger,"When using the Datadog tracer, Envoy relies on the service to propagate the Datadog-specific HTTP headers ( 
   x-datadog-trace-id ,
   x-datadog-parent-id ,
   x-datadog-sampling-priority ).",0.0,0.0,1.0,0.0
jaeger,TLDR: traceId headers need to be manually added to B3 HTTP headers.,0.0,0.0,1.0,0.0
jaeger,Additional information:  https://github.com/openzipkin/b3-propagation,0.0,0.0,1.0,0.0
jaeger,If you have sampling rate set to 1% then error will be seen in Jaeger once it occurs 100 times.,0.124,0.0,0.876,-0.4019
jaeger,This is mentioned at  Distributed Tracing - Jaeger :,0.0,0.0,1.0,0.0
jaeger,"To see trace data, you must send requests to your service.",0.0,0.0,1.0,0.0
jaeger,The number of requests depends on Istio’s sampling rate.,0.0,0.14,0.86,0.0772
jaeger,You set this rate when you install Istio.,0.0,0.0,1.0,0.0
jaeger,The default sampling rate is 1%.,0.0,0.0,1.0,0.0
jaeger,You need to send at least 100 requests before the first trace is visible.,0.0,0.0,1.0,0.0
jaeger,"To send a 100 requests to the   productpage   service, use the following command:",0.0,0.0,1.0,0.0
jaeger,$ for i in  `seq 1 100`;  do  curl -s -o /dev/null http://$GATEWAY_URL/productpage;  done,0.0,0.0,1.0,0.0
jaeger,"If you are not seeing the error in the current sample, I would advice make the sample higher.",0.0,0.124,0.876,0.3089
jaeger,You can read about  Tracing context propagation  which is being done by  Envoy .,0.0,0.0,1.0,0.0
jaeger,Envoy automatically sends spans to tracing collectors,0.0,0.0,1.0,0.0
jaeger,Alternatively the trace context can be manually propagated by the service:,0.0,0.0,1.0,0.0
jaeger,"Just mentioning beforehand (you might already know) that a Kubernetes Service is not a ""service"" as in a piece of code.",0.0,0.0,1.0,0.0
jaeger,"It is a way for Kubernetes components &amp; deployments to communicate with one another through an interface which always stays the same, regardless of how many pods or servers there are.",0.0,0.0,1.0,0.0
jaeger,"When Istio deploys it's tracing mechanism, it deploys modular parts so it can deploy them independently, and also scale them independently, very much like micro-services.",0.0,0.104,0.896,0.4173
jaeger,Generally a Kubernetes deployed utility will be deployed as a few parts which make up the bigger picture.,0.0,0.0,1.0,0.0
jaeger,For instance in your case:,0.0,0.0,1.0,0.0
jaeger,jaeger-agent - This is the components which will collect all the traffic and tracing from your nodes.,0.0,0.0,1.0,0.0
jaeger,"jaeger-collector - This is the place where all of the jaeger-agents will push the logs and traces they find on the node, and the collector will aggregate these as a trace may span multiple nodes.",0.0,0.0,1.0,0.0
jaeger,tracing - might be the component which injects the tracing ID's into network traffic for the agent to watch.,0.0,0.0,1.0,0.0
jaeger,"zipkin - could be the UI which allows debugging with traces, or replaying requests etc.",0.0,0.0,1.0,0.0
jaeger,"The above might not be absolutely correct, but I hope you get the idea of why multiple parts would be deployed.",0.0,0.183,0.817,0.6423
jaeger,"In the same way we deploy mysql, and our containers separately, Kubernetes projects are generally deployed as a set of deployments or pods.",0.0,0.0,1.0,0.0
jaeger,"To complement @christiaan-vermeulen's answer: the  tracing  service is Jaeger's UI (jaeger-query) so that the same URL can be used for alternative backends, whereas the Zipkin service is a convenience service, allowing applications using Zipkin tracers (like Brave) to send data to Jaeger without requiring complex changes.",0.0,0.0,1.0,0.0
jaeger,"If you look closely, the Zipkin service is backed by the jaeger-collector as well.",0.0,0.211,0.789,0.296
jaeger,I hope you have followed the official documentation of the jager with istio.,0.0,0.209,0.791,0.4404
jaeger,If you are using the helm chart make the following changes required.,0.0,0.0,1.0,0.0
jaeger,Export the dashboard via Kube port-forward or ingress.,0.0,0.0,1.0,0.0
jaeger,Official Documentation.,0.0,0.0,1.0,0.0
jaeger,https://istio.io/docs/tasks/telemetry/distributed-tracing/jaeger/,0.0,0.0,1.0,0.0
jaeger,NOTE: The important thing by default jaeger will trace something like 0.1% request i.e.,0.0,0.264,0.736,0.5106
jaeger,1 request out of 100 so put a lot of requests only then you can see a trace in UI.,0.0,0.0,1.0,0.0
jaeger,I had a wrong opencensus collector configuration.,0.437,0.0,0.563,-0.4767
jaeger,The docker container network cannot see port 9411 as it was on the host network.,0.0,0.0,1.0,0.0
jaeger,I was able to fix the issue after noticing this misconfiguration.,0.0,0.0,1.0,0.0
jaeger,"OpenTracing does not define the concrete data model, how the data should be collected, nor how it should be transported.",0.0,0.0,1.0,0.0
jaeger,"As such, there's no specification for the endpoint.",0.239,0.0,0.761,-0.296
jaeger,"This allows implementations like Jaeger to use non-HTTP transport by default when sending data from the client (tracer) to the backend, by sending UDP packets to an intermediate ""Jaeger Agent"".",0.0,0.079,0.921,0.3612
jaeger,"Given that the base model is pretty much similar among implementations, it's common to have tracing solutions to support each other's endpoints.",0.0,0.286,0.714,0.765
jaeger,"For instance, Jaeger is able to expose an endpoint with  Zipkin compatibility .",0.127,0.0,0.873,-0.1531
jaeger,"Based on your question, I think you might be interested in the OpenTelemetry project, a successor to the OpenTracing project, as the result of a merge with the OpenCensus project.",0.0,0.155,0.845,0.5574
jaeger,"OpenTelemetry provides its own tracer and is able to ""receive"" data in several formats (including Jaeger), and ""export"" to several backends.",0.0,0.0,1.0,0.0
jaeger,Assuming that your services are  defined in Istio’s internal service registry.,0.0,0.0,1.0,0.0
jaeger,If not please configure it according to instruction  service-defining .,0.197,0.0,0.803,-0.2411
jaeger,"In HTTPS all the HTTP-related information like method, URL path, response code, is encrypted so Istio  cannot  see and cannot monitor that information for HTTPS.",0.0,0.094,0.906,0.3612
jaeger,"If you need to monitor HTTP-related information in access to external HTTPS services, you may want to let your applications issue HTTP requests and configure Istio to perform TLS origination.",0.0,0.043,0.957,0.0772
jaeger,First you have to  redefine  your ServiceEntry and create VirtualService  to rewrite the HTTP request port and add a DestinationRule to perform TLS origination.,0.0,0.087,0.913,0.2732
jaeger,The VirtualService redirects HTTP requests on port 80 to port 443 where the corresponding DestinationRule then performs the TLS origination.,0.0,0.0,1.0,0.0
jaeger,"Unlike the previous ServiceEntry, this time the protocol on port 443 is HTTP, instead of HTTPS, because clients will only send HTTP requests and Istio will upgrade the connection to HTTPS.",0.0,0.0,1.0,0.0
jaeger,I hope it helps.,0.0,0.846,0.154,0.6705
jaeger,"Note that tracing data (spans) are not the same as ""metrics"", although there could be some overlap in some cases.",0.0,0.0,1.0,0.0
jaeger,"I recommend the following blog post on what is the purpose of each, including logging:",0.0,0.161,0.839,0.3612
jaeger,https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html,0.0,0.0,1.0,0.0
jaeger,"That said, there is the OpenTracing library mentioned in the blog post you linked, called  opentracing-contrib/java-metrics .",0.0,0.0,1.0,0.0
jaeger,It allows you to pick specific spans and record them as data points (metrics).,0.0,0.0,1.0,0.0
jaeger,"It works as a decorator of a concrete tracer, so, your spans would reach a concrete backend like Jaeger and, additionally, create data points based on the configured spans.",0.0,0.199,0.801,0.5719
jaeger,"The data points are then reported via  Micrometer , which can be configured to expose this data in Prometheus format.",0.082,0.0,0.918,-0.1531
jaeger,"The problem is, I haven't found any consistent solution to do this, all the examples that I have found so far are outdated, deprecated or lacks documentation.",0.096,0.082,0.821,-0.1027
jaeger,"Please, open an issue on the  java-metrics  repository with the problems you are facing.",0.159,0.135,0.706,-0.1027
jaeger,Just to close this question out for the solution to the problem in my instance.,0.15,0.128,0.722,-0.1027
jaeger,The mistake in configuration started all the way back in the Kubernetes cluster initialisation.,0.156,0.0,0.844,-0.34
jaeger,I had applied:,0.0,0.0,1.0,0.0
jaeger,the pod-network-cidr using the same address range as the local LAN on which the Kubernetes installation was deployed i.e.,0.0,0.0,1.0,0.0
jaeger,the desktop for the Ubuntu host used the same IP subnet as what I'd assigned the container network.,0.0,0.0,1.0,0.0
jaeger,"For the most part, everything operated fine as detailed above, until the Istio proxy was trying to route packets from an external load-balancer IP address to an internal IP address which happened to be on the same subnet.",0.0,0.046,0.954,0.2023
jaeger,Project Calico with Kubernetes seemed to be able to cope with it as that's effectively Layer 3/4 policy but Istio had a problem with it a L7 (even though it was sitting on Calico underneath).,0.097,0.053,0.849,-0.3818
jaeger,The solution was to tear down my entire Kubernetes deployment.,0.0,0.204,0.796,0.3182
jaeger,I was paranoid and went so far as to uninstall Kubernetes and deploy again and redeploy with a pod network in the 172 range which wasn't anything to do with my local lan.,0.062,0.0,0.938,-0.25
jaeger,I also made the same changes in the Project Calico configuration file to match pod networks.,0.0,0.0,1.0,0.0
jaeger,"After that change, everything worked as expected.",0.0,0.0,1.0,0.0
jaeger,I suspect that in a more public configuration where your cluster was directly attached to a BGP router as opposed to using MetalLB with an L2 configuration as a subset of your LAN wouldn't exhibit this issue either.,0.062,0.0,0.937,-0.296
jaeger,I've documented it more in this post:,0.0,0.0,1.0,0.0
jaeger,"Microservices: .Net, Linux, Kubernetes and Istio make a powerful combination",0.0,0.259,0.741,0.4215
jaeger,"Actually, there is no error.",0.62,0.0,0.38,-0.5994
jaeger,The code was changing the color of a yellow texture to a red tint inline.,0.0,0.0,1.0,0.0
jaeger,Try this : (if you can gives us all the variants of the url it would be better),0.0,0.0,1.0,0.0
jaeger,"Unlike Jaeger, LightStep is a commercial SaaS offering.",0.0,0.0,1.0,0.0
jaeger,"If you wanted to try out their service, you'd need to contact their sales team.",0.0,0.0,1.0,0.0
jaeger,Managed to create the desired capturing groups:,0.0,0.457,0.543,0.4939
jaeger,"Then I could write out the files, it looks correct as for these few occurences.",0.0,0.0,1.0,0.0
inspectit,"You need to add instrumentation rules for your application to ""dig deeper"".",0.0,0.0,1.0,0.0
inspectit,The  doFilter  and  service  methods are instrumented by default as part of the HTTP instrumentation profile.,0.0,0.0,1.0,0.0
inspectit,"In addition to the HTTP profile, by default inspectIT instruments SQL statement executions, other entry points into your application (like JMS), the places where external HTTP/JMS calls are done and Java Executors.",0.0,0.0,1.0,0.0
inspectit,"There are some other common instrumentation profiles (Hibernate, SQL parameters, Spring, etc), but you need to enable them by yourself.",0.0,0.0,1.0,0.0
inspectit,"Usually, you would start with the default settings and then, in addition, add instrumentation rules related to your application.",0.0,0.0,1.0,0.0
instana,Disclaimer: I work for Instana.,0.0,0.0,1.0,0.0
instana,There is not much to setup.,0.0,0.0,1.0,0.0
instana,Instana provides out-of-the-box support for Kafka and Zookeeper nodes.,0.0,0.252,0.748,0.4019
instana,So all you need to do is to install the Instana agent on the server(s) you want to monitor.,0.0,0.067,0.933,0.0772
instana,It will automatically detect your Kafka and Zookeeper installations and start reporting metrics for them to your tenant unit.,0.0,0.0,1.0,0.0
instana,"If you don't have a tenant unit yet, you can register for a free trial at  https://www.instana.com/trial/  or contact Sales.",0.0,0.163,0.837,0.5106
instana,"If you need additional help, I suggest to open a ticket at  https://instana.zendesk.com  to get dedicated support.",0.0,0.412,0.588,0.8126
instana,Instana has a  demo application  that shows to do this.,0.0,0.0,1.0,0.0
instana,To summarize the parts that you would need:,0.0,0.0,1.0,0.0
instana,The combination of these two steps will make TypeScript aware of the function.,0.0,0.0,1.0,0.0
instana,Now you can use  ineum  just like any other global.,0.0,0.217,0.783,0.3612
instana,Instana will use the same protocol to make the sourcemap request.,0.0,0.0,1.0,0.0
instana,"The documentation example uses http, but it will work with https the same way.",0.0,0.0,1.0,0.0
instana,The most likely reason for your problem is that the sourcemap is not readable from the public internet.,0.137,0.0,0.863,-0.4019
instana,"In your case, the sourcemap file requires http session authentication and redirects to a login page.",0.0,0.0,1.0,0.0
instana,"You could remove the location /nginx_status in that server, and add a new server section like this:",0.0,0.143,0.857,0.3612
instana,"That endpoint requires a POST, it appears you are using GET.",0.0,0.0,1.0,0.0
instana,Hence method not allowed.,0.0,0.0,1.0,0.0
instana,"Instana offers an agent tailored to React native, which simplifies the integration.",0.0,0.0,1.0,0.0
instana,The  React native agent  is different than the one used for website monitoring.,0.0,0.0,1.0,0.0
instana,You can get started with React native monitoring by creating a mobile app within Instana's user interface under  Websites &amp; Mobile Apps -&gt; Mobile Apps .,0.0,0.087,0.913,0.296
instana,For the React native agent you can find  dedicated documentation and installation instructions  on Instana's documentation site.,0.0,0.158,0.842,0.4588
instana,"For further questions and support, I suggest leveraging  Instana's support portal .",0.0,0.403,0.597,0.6597
instana,the Instana repository has been upgraded to support Disco Dingo as well.,0.0,0.324,0.676,0.5859
instana,To whoever removed his/her answer: It was a correct answer.,0.0,0.0,1.0,0.0
instana,I don't know why you deleted it.,0.0,0.0,1.0,0.0
instana,"Anyhow, I am posting again in case someone stumbles here.",0.0,0.0,1.0,0.0
instana,You can control frequency and time by using  INSTANA_AGENT_UPDATES_FREQUENCY  and  INSTANA_AGENT_UPDATES_TIME  environment variables.,0.0,0.0,1.0,0.0
instana,Updating  mode  via env variable is still unknown at this point.,0.0,0.0,1.0,0.0
instana,Look at this page for more info:  https://www.instana.com/docs/setup_and_manage/host_agent/on/docker/#updates-and-version-pinning,0.0,0.0,1.0,0.0
instana,"Most agent settings that one may want to change quickly are available as environment variables, see  https://www.instana.com/docs/setup_and_manage/host_agent/on/docker .",0.0,0.075,0.925,0.0772
instana,"For example, setting the mode via environment variable is supported as well with  INSTANA_AGENT_MODE , see e.g.,  https://hub.docker.com/r/instana/agent .",0.0,0.227,0.773,0.5267
instana,The valid values are:,0.0,0.474,0.526,0.4019
instana,"On Kubernetes, it is also of course possible to use a ConfigMap to override files in the agent container.",0.0,0.0,1.0,0.0
instana,Solved.,0.0,1.0,0.0,0.2732
instana,"Added flags to my run configuration, and increase XMS and XMX twice.",0.0,0.173,0.827,0.3182
instana,"pod install  is  cocoapods  command, not part of  ruby  or  gem .",0.0,0.0,1.0,0.0
instana,The error means there is no  pod  or  install  package in  ruby  package repository.,0.29,0.0,0.71,-0.5994
instana,"After writing a proper  Podfile  in your Xcode project dir, just run  pod install .",0.0,0.0,1.0,0.0
lightstep,[Disclaimer: I work at  LightStep],0.0,0.0,1.0,0.0
lightstep,Sorry you're having trouble getting Java and Go to play well together.,0.242,0.273,0.485,0.128
lightstep,I suspect this is caused by time-correction being enabled in Java but not being used in Go.,0.096,0.0,0.904,-0.1531
lightstep,"You can disable time correction in Java using the  withClockSkewCorrection(boolean clockCorrection)  
option to turn off clockCorrection when passing in options to the LightStep tracer",0.0,0.0,1.0,0.0
lightstep,Here is the updated  README  and a link to the  option code,0.0,0.0,1.0,0.0
lightstep,"If you contact us via the [Support] button in LightStep, we should be able to get you sorted out.",0.0,0.0,1.0,0.0
lightstep,Please send us a note so that we can confirm that this is solved for you.,0.0,0.253,0.747,0.5267
lightstep,We'll start monitoring SO more carefully so that we catch these things earlier.,0.0,0.187,0.813,0.4152
lightstep,Thanks and happy tracing!,0.0,0.775,0.225,0.784
lightstep,Will,0.0,0.0,1.0,0.0
lightstep,lightstep-opentelemetry-launcher-node  basically bundles the required things for you for easier configuration so this is not an exporter.,0.0,0.149,0.851,0.4215
lightstep,If you were to simply replace the &quot;LightstepExporter&quot; with &quot;OpenTelemetry Collector Exporter&quot; in your code you can simply do this,0.0,0.0,1.0,0.0
lightstep,The default  YOUR_DIGETS_URL  from  lightstep/otel-launcher-node  is  https://ingest.lightstep.com:443/api/v2/otel/trace,0.0,0.0,1.0,0.0
lightstep,"If you go to the latest snapshot documentation (or milestone) and you search for the word OpenTracing, you would get your answer.",0.0,0.0,1.0,0.0
lightstep,It's here  https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html#_opentracing,0.0,0.0,1.0,0.0
lightstep,Spring Cloud Sleuth is compatible with OpenTracing.,0.0,0.0,1.0,0.0
lightstep,"If you have OpenTracing on the classpath, we automatically register the OpenTracing Tracer bean.",0.0,0.0,1.0,0.0
lightstep,"If you wish to disable this, set  spring.sleuth.opentracing.enabled  to false",0.0,0.231,0.769,0.4019
lightstep,So it's enough to just have OpenTracing on the classpath and Sleuth will work out of the box,0.0,0.0,1.0,0.0
skywalking,"Explanation how to set up  skywalking  properly:
 https://github.com/apache/skywalking/issues/3589#issuecomment-543268029",0.0,0.0,1.0,0.0
skywalking,"It is the dashboard default time filter value problem, the time range did not contains data:",0.141,0.126,0.733,-0.0772
skywalking,change the time start and end to having collection data area.,0.0,0.0,1.0,0.0
skywalking,Finally I build the side car image by myself:,0.0,0.0,1.0,0.0
skywalking,this is the docker file:,0.0,0.0,1.0,0.0
skywalking,how to add the jdbc driver jar into the image file?,0.0,0.0,1.0,0.0
skywalking,One way would be an  initContainer:  and then artificially inject the jdbc driver via  -Xbootclasspath,0.0,0.0,1.0,0.0
skywalking,"a similar, although slightly riskier way, is to find a path that is already on the classpath of the image, and attempt to volume mount the jar path into that directory",0.07,0.0,0.93,-0.2748
skywalking,"All of this seems kind of moot given that your image looks like one that is custom built, and therefore the correct action is to update the  Dockerfile  for it to download the jar at build time",0.0,0.065,0.935,0.3612
skywalking,"I created a lifecycle that performs the delete action after a set time, and then I added this configuration to the skywalking application.yml under  storage.elasticsearch7 :",0.0,0.091,0.909,0.25
skywalking,"SW creates index templates, and now I see that this is part of the template, and indeed the indexes have this sw-policy attached.",0.0,0.091,0.909,0.2732
skywalking,So you can see this more clearly in the output.,0.0,0.25,0.75,0.4576
skywalking,The pod is Running but the Ready flag is false meaning the container is up but is failing the Readiness Probe.,0.158,0.204,0.638,0.0772
skywalking,So you should go throught this document first,0.0,0.0,1.0,0.0
skywalking,https://kubernetes.io/docs/concepts/storage/volumes/#hostpath,0.0,0.0,1.0,0.0
skywalking,use  hostPath  as sample,0.0,0.0,1.0,0.0
skywalking,You need reference it for both init container and normal container.,0.0,0.0,1.0,0.0
stagemonitor,Most of the metrics stagemonitor collects are not available via JMX.,0.0,0.0,1.0,0.0
stagemonitor,"For example, response time statistics grouped by the endpoint of your application.",0.0,0.0,1.0,0.0
stagemonitor,"Also, stagemonitor is much more than just metrics.",0.0,0.0,1.0,0.0
stagemonitor,"It is also a profiler, you can use to see which methods caused a request to be slow.",0.0,0.0,1.0,0.0
stagemonitor,"Further more, it can (soon) do distributed tracing which helps you to analyze and debug latency problems in a microservice environment by correlating related requests.",0.099,0.095,0.806,-0.0258
stagemonitor,It also offers you a Kibana dashboard you can use to drill into the requests your application serves to find out about causes of errors or latency.,0.088,0.0,0.912,-0.34
stagemonitor,Another use case is lightweight web analytics to identify which devices and operating systems your customers use to access your site.,0.0,0.0,1.0,0.0
stagemonitor,Two possible solutions.,0.0,0.459,0.541,0.1779
stagemonitor,"You can have a button 'hide', that will hide the metrics using some javascript code.",0.116,0.0,0.884,-0.1779
stagemonitor,Or in the same button you do the following:,0.0,0.0,1.0,0.0
stagemonitor,It doesn't appear to be compatible with Grails.,0.0,0.0,1.0,0.0
stagemonitor,If you enable logging,0.0,0.0,1.0,0.0
stagemonitor,you'll see a bunch of error stacktraces that appear to imply that the way they're using Javassist to wire in tracing code isn't compatible with Groovy and/or the AST transformations that Grails uses:,0.08,0.0,0.92,-0.4019
stagemonitor,Finally I found out how to disable the browser widget.,0.0,0.0,1.0,0.0
stagemonitor,Set:,0.0,0.0,1.0,0.0
stagemonitor,You can see more information about it  here .,0.0,0.0,1.0,0.0
stagemonitor,MySQL does not provide anything more than how much data each tenant has.,0.0,0.0,1.0,0.0
stagemonitor,That can be found in  information_schema .,0.0,0.0,1.0,0.0
stagemonitor,"If you need CPU/IO/etc., you need to set up multiple instances of MySQL in VMs or cgroups and have the OS / VM-manager provide the data.",0.0,0.0,1.0,0.0
stagemonitor,"This will cost extra RAM, so it may not be worth it.",0.132,0.0,0.868,-0.1695
stagemonitor,I'm afraid this is currently not possible.,0.0,0.0,1.0,0.0
stagemonitor,"However, stagemonitor offers a ""Custom Metrics"" dashboard for Grafana.",0.0,0.0,1.0,0.0
stagemonitor,"To see the metrics locally, currently the only way is to enable periodic logging of all metrics.",0.0,0.0,1.0,0.0
stagemonitor,Stagemonitor now features a in browser widget that is automatically injected in your web page.,0.0,0.0,1.0,0.0
stagemonitor,You don't need any infrastructure or docker for this and the configuration and set up is easy.,0.0,0.153,0.847,0.4404
stagemonitor,"For more information, visit  http://www.stagemonitor.org/ .",0.0,0.0,1.0,0.0
stagemonitor,This is how you enable the widget:  https://github.com/stagemonitor/stagemonitor/wiki/Step-2%3A-Log-Only-Monitoring-In-Browser-Widget#in-browser-widget .,0.0,0.0,1.0,0.0
tanzu,"The problem(s) (as noted in  GEODE-788 ,  GEODE-7665 ,  GEODE-7666 ,  GEODE-7670 ,  GEODE-7672  and  GEODE-7676 ) is, is that GemFire/Geode does not support  Region.clear()  for  PARTITION   Regions  (yet).",0.089,0.0,0.911,-0.3089
tanzu,"When you declare the  @CacheEvent(allEntries = true)  annotation/attribute on your managed application component, for example...",0.0,0.0,1.0,0.0
tanzu,This in effect calls  Region.clear()  (see  here ).,0.0,0.0,1.0,0.0
tanzu,"This behavior works for  REPLICATE  and  LOCAL   Regions , however not for  PARTITION   Regions , given the numerous GemFire/Geode problems.",0.137,0.0,0.863,-0.4019
tanzu,"It is currently a WIP, though.",0.0,0.0,1.0,0.0
tanzu,"There was (partly, still is) an intention in Spring Data for Apache Geode &amp; VMware Tanzu (Pivotal) GemFire to handle cache clear operations.",0.0,0.106,0.894,0.3818
tanzu,https://jira.spring.io/browse/DATAGEODE-265,0.0,0.0,1.0,0.0
tanzu,"However, this is on hold until the above GEODE JIRA tickets get sorted out.",0.0,0.0,1.0,0.0
tanzu,The short answer is no.,0.355,0.0,0.645,-0.296
tanzu,"You really, really want to have DNS set up properly.",0.0,0.172,0.828,0.2195
tanzu,Here's the long answer that is more nuanced.,0.0,0.0,1.0,0.0
tanzu,All requests to your foundation go through the Gorouter.,0.0,0.0,1.0,0.0
tanzu,"Gorouter will take the incoming request, look at the  Host  header and use that to determine where to send the request.",0.0,0.0,1.0,0.0
tanzu,This happens the same for system services like CAPI and UAA as it does for apps you deploy to the foundation.,0.0,0.111,0.889,0.3612
tanzu,DNS is a requirement because of the  Host  header.,0.0,0.0,1.0,0.0
tanzu,A browser trying to access CAPI or an application on your foundation is going to set the  Host  header based on the DNS entry you type into your browser's address bar.,0.0,0.0,1.0,0.0
tanzu,The cf CLI is going to do the same thing.,0.0,0.0,1.0,0.0
tanzu,There are some ways to work around this:,0.0,0.0,1.0,0.0
tanzu,If you are strictly using a client like  curl  where you can set the  Host  header to arbitrary values.,0.0,0.245,0.755,0.6369
tanzu,"In that way, you could set the host header to  api.system_domain  and at the same time connect to the IP address of your foundation.",0.0,0.0,1.0,0.0
tanzu,That's not a very elegant way to use CF though.,0.257,0.0,0.743,-0.4158
tanzu,You can manually set entries in your /etc/hosts` (or similar on Windows).,0.0,0.0,1.0,0.0
tanzu,This is basically a way to override DNS resolution and supply your own custom IP.,0.0,0.0,1.0,0.0
tanzu,"You would need to do this for  uaa.system_domain ,  login.system_domain ,  api.system_domain  and any host names you want to use for apps deployed to your foundation, like  my-super-cool-app.apps_domain .",0.0,0.137,0.863,0.4215
tanzu,These should all point to the IP of the load balancer that's in front of your pool of Gorouters.,0.0,0.0,1.0,0.0
tanzu,If you add enough entries into  /etc/hosts  you can make the cf CLI work.,0.0,0.0,1.0,0.0
tanzu,I have done this on occasion to bypass the load balancer layer for troubleshooting purposes.,0.0,0.116,0.884,0.1779
tanzu,"Where this won't work is on systems where you can't edit  /etc/hosts , like customers or external users of software running on your foundation or if you're trying to deploy apps on your foundation that talk to each other using routes on CF (because you can't edit  /etc/hosts  in the container).",0.041,0.0,0.959,-0.2755
tanzu,Like if you have  app-a.apps_domain  and  app-b.apps_domain  and  app-a  needs to talk to  app-b .,0.0,0.161,0.839,0.3612
tanzu,That won't work because you have no DNS resolution for  apps_domain .,0.18,0.0,0.82,-0.296
tanzu,You can probably make app-to-app communication work if you are able to use container-to-container networking and the  apps.internal  domain though.,0.0,0.0,1.0,0.0
tanzu,The resolution for that domain is provided by Bosh DNS.,0.0,0.0,1.0,0.0
tanzu,"You have to be aware of this difference though when deploying your apps and map routes on the  apps.internal  domain, as well as setting network policy to allow traffic to flow between the two.",0.0,0.111,0.889,0.4588
tanzu,"Anyway, there might be other hiccups.",0.0,0.0,1.0,0.0
tanzu,This is just off the top of my head.,0.0,0.184,0.816,0.2023
tanzu,You can see it's a lot better if you can set up DNS.,0.0,0.209,0.791,0.4404
tanzu,The most easy way to achieve a portable solution is a service like  xip.io  that will work out of the box.,0.0,0.333,0.667,0.7902
tanzu,"I have setup and run a lot of PoCs that way, when wildcard DNS was something that enterprise IT was still oblivious about.",0.0,0.0,1.0,0.0
tanzu,It works like this (excerpt from their site):,0.0,0.263,0.737,0.3612
tanzu,What is xip.io?,0.0,0.0,1.0,0.0
tanzu,"xip.io is a magic domain name that provides wildcard DNS
for any IP address.",0.0,0.0,1.0,0.0
tanzu,Say your LAN IP address is 10.0.0.1.,0.0,0.0,1.0,0.0
tanzu,"Using xip.io,",0.0,0.0,1.0,0.0
tanzu,"mysite.10.0.0.1.xip.io   resolves to   10.0.0.1
foo.bar.10.0.0.1.xip.io   resolves to   10.0.0.1",0.0,0.362,0.638,0.34
tanzu,...and so on.,0.0,0.0,1.0,0.0
tanzu,"You can use these domains to access virtual
hosts on your development web server from devices on your
local network, like iPads, iPhones, and other computers.",0.0,0.091,0.909,0.3612
tanzu,No configuration required!,0.555,0.0,0.445,-0.3595
tanzu,It is unclear whether you're using the SCDF tile or the SCDF OSS (via  manfest.yml ) on PCF.,0.111,0.0,0.889,-0.25
tanzu,"Suppose you're using the OSS, AFA.",0.0,0.0,1.0,0.0
tanzu,"In that case, you are providing the right RMQ service-instance configuration (that you pre-created) in the  manifest.yml , then SCDF would automatically propagate that RMQ service instance and bind it to the apps it is deploying to your ORG/Space.",0.0,0.0,1.0,0.0
tanzu,You don't need to muck around with connection credentials manually.,0.0,0.0,1.0,0.0
tanzu,"On the other hand, if you are using the SCDF Tile, the SCDF service broker will auto-create the RMQ SI and automatically bind it to the apps it deploys.",0.0,0.103,0.897,0.4939
tanzu,"In summary, there's no reason to manually pass the connection credentials or pack them as application properties inside your apps.",0.104,0.0,0.896,-0.296
tanzu,You can automate all this provided you're configuring all this correctly.,0.0,0.0,1.0,0.0
tanzu,"&quot; In this case it will wait till the processing completes or it
forcibly reduces the instance count when reached threshold.",0.0,0.065,0.935,0.1027
tanzu,&quot;,0.0,0.0,1.0,0.0
tanzu,"Answer: 
No, the App Autoscaler will not force anything, after the decision cycle, it will prepare the instance to be escalated-down (shutdown), so the intention is to avoid lose requests or data during this process.",0.182,0.0,0.818,-0.7269
tanzu,"Please, take a look into the documentation below, it will help you to understand better the App Autoscaler mechanism.",0.0,0.345,0.655,0.7845
tanzu,How App Autoscaler Determines When to Scale:,0.0,0.0,1.0,0.0
tanzu,"Every 35 seconds, App Autoscaler makes a decision about whether to
scale up, scale down, or keep the same number of instances.",0.0,0.061,0.939,0.0772
tanzu,"To make a scaling decision, App Autoscaler averages the values of a
given metric for the most recent 120 seconds.",0.0,0.137,0.863,0.4019
tanzu,The following diagram provides an example of how App Autoscaler makes scaling decisions:,0.0,0.0,1.0,0.0
tanzu,"Reference: 
 VMWare Tanzu App Autoscaler documentation",0.0,0.0,1.0,0.0
tanzu,VMWare Tanzu is the former Pivotal Cloud Foundry (PCF).,0.0,0.0,1.0,0.0
tanzu,I have the same question and as far as I understood from  App Container Lifecycle  it’s up to your app to gracefully shutdown but that might not be possible in given 10 seconds as some processes might take longer.,0.0,0.058,0.942,0.296
tanzu,"Shutdown 
CF requests a shutdown of your app instance in the following scenarios:
When a user runs  cf scale , cf stop, cf push, cf delete, or cf restart-app-instance
As a result of a system event, such as the replacement procedure during Diego Cell evacuation or when an app instance stops because of a failed health check probe
To shut down the app, CF sends the app process in the container a SIGTERM.",0.101,0.0,0.899,-0.7269
tanzu,"By default, the process has ten seconds to shut down gracefully.",0.0,0.254,0.746,0.5267
tanzu,"If the process has not exited after ten seconds, CF sends a SIGKILL.",0.0,0.0,1.0,0.0
tanzu,"By default, apps must finish their in-flight jobs within ten seconds of receiving the SIGTERM before CF terminates the app with a SIGKILL.",0.0,0.0,1.0,0.0
tanzu,"For instance, a web app must finish processing existing requests  and stop accepting new requests.",0.131,0.155,0.714,0.1027
tanzu,Note: One exception to the cases mentioned above is when monit restarts a crashed Diego Cell rep or Garden server.,0.0,0.0,1.0,0.0
tanzu,"In this case, CF immediately stops the apps that are still running using SIGKILL.",0.11,0.0,0.89,-0.1531
tanzu,I think there's a workaround for kubernetes versions prior to 1.17.,0.0,0.0,1.0,0.0
tanzu,On  kubernetes version v1.16  you can run Sonobuoy (Sonobuoy version v0.16.1 or higher) with providing the test framework flag:  --allowed-not-ready-nodes=1,0.0,0.0,1.0,0.0
tanzu,And on  kubernetes version prior to v1.16  it was more complicated.,0.0,0.0,1.0,0.0
tanzu,I haven't tested this but according to docs:,0.0,0.0,1.0,0.0
tanzu,Pivotal Web Services is not the same as Pivotal Cloud Foundry.,0.0,0.0,1.0,0.0
tanzu,"Pivotal Web Services has been sunset, yes.",0.0,0.31,0.69,0.4019
tanzu,"Tanzu Application Service is VMware's enterprise solution that is, if you want to think about it this way, a self-hosted Pivotal Web Services (this is a gross understatement, but works for this situation).",0.062,0.085,0.852,-0.0644
tanzu,Are you looking to test Cloud Foundry for its suitability?,0.0,0.0,1.0,0.0
tanzu,Add the AzureIdentity and AzureIdentityBinding roles to cluster as mentioned in docs.,0.0,0.0,1.0,0.0
tanzu,"once done, use the selector defined AzureIdentityBinding as label while deploying helm chart.",0.0,0.0,1.0,0.0
tanzu,Check for the actual syntax for podLabels using with --set in helm install command.,0.0,0.0,1.0,0.0
tanzu,Or you can clone the charts and make changes to values.yaml below and install it from local charts.,0.0,0.0,1.0,0.0
tanzu,https://github.com/vmware-tanzu/helm-charts/blob/04615803a7d976ce15a6eeb4b1bd6a1cfb9a02c5/charts/velero/values.yaml#L27,0.0,0.0,1.0,0.0
tanzu,"Just for help:
 https://medium.com/@kimvisscher/using-aad-pod-identity-in-an-aks-cluster-117c08565692",0.0,0.474,0.526,0.4019
tanzu,It seems that you are struggling with how to format the  secretContents  section of the values.yaml file.,0.149,0.0,0.851,-0.4215
tanzu,"If that is so, take a look at a recent update to the documentation.",0.0,0.0,1.0,0.0
tanzu,It lays out and documents exactly how to format it:,0.0,0.0,1.0,0.0
tanzu,https://github.com/vmware-tanzu/helm-charts/blob/1ea07c7fb9c7ba910ba52801c536a6f8dcee096d/charts/velero/values.yaml#L219-L232,0.0,0.0,1.0,0.0
zipkin,I found that I need to add a sampler percentage.,0.0,0.0,1.0,0.0
zipkin,By default zero percentage of the samples are sent and that is why the sleuth was not sending anything to zipkin.,0.0,0.0,1.0,0.0
zipkin,"when I added  spring.sleuth.sampler.percentage=1.0  in the properties files, it started working.",0.0,0.0,1.0,0.0
zipkin,"If you are exporting all the span data to Zipkin, sampler can be installed  by creating a bean definition in the Spring boot main class",0.0,0.087,0.913,0.296
zipkin,"For the latest version of cloud dependencies  &lt;version&gt;Finchley.SR2&lt;/version&gt; 
The correct property to send traces to zipkin is:  spring.sleuth.sampler.probability=1.0 
Which has changed from percentage to probability.",0.0,0.0,1.0,0.0
zipkin,"It is a very long time ago, but it looks like it was moved here:",0.0,0.2,0.8,0.5023
zipkin,http://zipkin.io/pages/quickstart,0.0,0.0,1.0,0.0
zipkin,Found multiple language examples at  github .,0.0,0.0,1.0,0.0
zipkin,"If you need basic setup steps:  https://zipkin.io/ 
Integrated zipkin with spring boot 2 and mysql 
 Steps 
 example 
Here is sample",0.0,0.0,1.0,0.0
zipkin,Lately I have been trying the same and couldn't find that option in initializer.,0.0,0.0,1.0,0.0
zipkin,I am just posting this if anyone encounters the same issues and lands on this page.,0.0,0.0,1.0,0.0
zipkin,"You can refer below sample GitHub project which is consists of four micro services ( zipkin server, client, rest service, and Eureka ) using Edgware release with latest version of sleuth.",0.0,0.0,1.0,0.0
zipkin, Sample Zipkin Server/Client,0.0,0.0,1.0,0.0
zipkin,Zipkin Server is not part of Spring initializers.,0.0,0.0,1.0,0.0
zipkin,You have to use the official release of the Zipkin server,0.0,0.0,1.0,0.0
zipkin,https://github.com/openzipkin/zipkin#quick-start,0.0,0.0,1.0,0.0
zipkin,And custom servers are not supported anymore meaning you can't use  @EnableZipkinServer  anymore since 2.7,0.123,0.0,0.877,-0.2411
zipkin,https://github.com/openzipkin/zipkin#quick-start,0.0,0.0,1.0,0.0
zipkin,"I can't explain the error that you got with Dalston.BUILD-SNAPSHOT, but the error with Camden.SR4 is because it's not compatible with Spring Boot 1.5.",0.0,0.177,0.823,0.5448
zipkin,I'd recommend upgrading to Camden.SR5  which is compatible with Spring Boot 1.5 .,0.0,0.185,0.815,0.3612
zipkin,Even I got this error while setting up my project.,0.281,0.0,0.719,-0.481
zipkin,I was using Spring boot 1.5.8 with the Brixton.SR6 release.,0.0,0.0,1.0,0.0
zipkin,"However, when I consulted the site  http://projects.spring.io/spring-cloud/  I got to know the issue and I updated my dependency to Dalston.SR4 and then the application started working.",0.0,0.0,1.0,0.0
zipkin,"The official documentation was helpful, but I think it didn't include all the dependencies explicitly (at least as of now).",0.0,0.095,0.905,0.2263
zipkin,I had to do some extra research for samples to get all the required dependencies and configuration together.,0.0,0.0,1.0,0.0
zipkin,"I wanted to share it, because I believe it could be helpful for someone else.",0.0,0.312,0.688,0.6124
zipkin,Spring Boot version:   1.4.0.RELEASE,0.0,0.0,1.0,0.0
zipkin,Spring Cloud version:   Brixton.SR4,0.0,0.0,1.0,0.0
zipkin,POM:,0.0,0.0,1.0,0.0
zipkin,Java:,0.0,0.0,1.0,0.0
zipkin,application.yml:,0.0,0.0,1.0,0.0
zipkin,References:,0.0,0.0,1.0,0.0
zipkin,https://cloud.spring.io/spring-cloud-sleuth/,0.0,0.0,1.0,0.0
zipkin,"Your application starts in Tomcat Server but Zipkin use another server but i dont know that name , i include this code to spring boot starter web dependecy for ignore tomcat server",0.104,0.0,0.896,-0.5023
zipkin,This worked for me try it,0.0,0.0,1.0,0.0
zipkin,I have tested this with the official  opencensus-node  example at github.,0.0,0.0,1.0,0.0
zipkin,"Zipkin UI displays 2 spans with same name MyApplication(see image),
  but expected 2 different span names",0.0,0.0,1.0,0.0
zipkin,"Just to be clear,  MyApplication  is the service name you set in your app.js, and the span names are those which you selected on the image  /service1 ,  /service1 ,  /external_service_2 .",0.0,0.085,0.915,0.3818
zipkin,"I think this is the intended behavior, you got one service ( MyApplication ), a root span ( /service1 ) and a child span ( /external_service_2 ).",0.0,0.0,1.0,0.0
zipkin,If you got multiple services connected to the same Zipkin server then you'll have multiple names for the services.,0.0,0.0,1.0,0.0
zipkin,From the Zipkin's  documentation :,0.0,0.0,1.0,0.0
zipkin,Span,0.0,0.0,1.0,0.0
zipkin,A set of Annotations and BinaryAnnotations that correspond to a particular RPC.,0.0,0.0,1.0,0.0
zipkin,"Spans contain identifying information such as traceId, spanId, parentId, and RPC name.",0.0,0.0,1.0,0.0
zipkin,Trace,0.0,0.0,1.0,0.0
zipkin,A set of spans that share a single root span.,0.0,0.239,0.761,0.296
zipkin,Traces are built by collecting all Spans that share a traceId.,0.0,0.196,0.804,0.296
zipkin,The spans are then arranged in a tree based on spanId and parentId thus providing an overview of the path a request takes through the system.,0.0,0.0,1.0,0.0
zipkin,"As far Zipkin UI displays 2 spans with same name, service dependencies
  page contains one Service only(see image)",0.0,0.0,1.0,0.0
zipkin,"Again, this is the intended behavior, since you got only one service and the external request you made goes through it.",0.0,0.0,1.0,0.0
zipkin,"If you mean the framed names on the first image, at the top it shows only the root span you clicked on the previous screen.",0.0,0.07,0.93,0.2023
zipkin,"However, you can write custom span names after a little change in your code.",0.0,0.0,1.0,0.0
zipkin,From the  tracing documentation  (with your code):,0.0,0.0,1.0,0.0
zipkin,"Now you can use  tracer.startRootSpan , I used it in the express sample with a request:",0.0,0.0,1.0,0.0
zipkin,A span must be closed.,0.0,0.0,1.0,0.0
zipkin,"For more information, check the  test file  of the tracer.",0.0,0.0,1.0,0.0
zipkin,The best way to ask for a feature is using github issues.,0.0,0.296,0.704,0.6369
zipkin,"To add a new transport such as RabbitMQ, you'd have to affect Brave (reporter) and Zipkin (collector)",0.0,0.185,0.815,0.5267
zipkin,"https://github.com/openzipkin/zipkin/issues 
 https://github.com/openzipkin/brave/issues",0.0,0.0,1.0,0.0
zipkin,"It's not suitable, Zipkin is about tracing in distributed systems.",0.0,0.0,1.0,0.0
zipkin,"I would say you would want something like a profiler, such as  Visual VM ,  - free and included with the JDK, or  YourKit .",0.0,0.295,0.705,0.7269
zipkin,Other profilers are available.,0.0,0.0,1.0,0.0
zipkin,"No, it  is not suitable at all .",0.268,0.0,0.732,-0.296
zipkin,Why?,0.0,0.0,1.0,0.0
zipkin,"Because the architecture of Zipkin have multiple levels of indirection: you have to send your spans into collector service and even if collector is running on your own machine there is a huge overhead -- imagine you are traveling from two neighbour cities somewhere in Europe and somebody proposes you instead of going directly from A to B, fly from A to New York, USA and back to B.",0.0,0.034,0.966,0.3182
zipkin,That is simply ridiculous.,0.455,0.0,0.545,-0.3612
zipkin,As for the tools that may suit your needs: VisualVM and Yourkit mentioned by @Jonathan are good for looking at average situation in your program -- if you need to carefully inspect low level paths in your program  InTrace  might be a better choice.,0.043,0.151,0.806,0.6369
zipkin,Zipkin is foremost intended to provide observability into a complex distributed network of services (aka Microservice Architecture).,0.0,0.0,1.0,0.0
zipkin,It wasn't intended to support just a single application.,0.244,0.0,0.756,-0.3089
zipkin,"So a profiler can often be a better way to go, particularly if you have an urgent issue to diagnose.",0.0,0.227,0.773,0.5719
zipkin,"That said, most profilers will only provide realtime data, where Zipkin persists, allowing for analysis over large datasets.",0.0,0.0,1.0,0.0
zipkin,"If Zipkin is already in your stack, it's not a bad idea to enrich higher level Zipkin spans (ie a complete REST requests) with specific method calls as child-spans, particularly those that do I/O.",0.0,0.084,0.916,0.431
zipkin,This way you can drill down within Zipkin to more quickly determine bottlenecks.,0.0,0.0,1.0,0.0
zipkin,"Otherwise you'd have to first look at the Zipkin span for high-level bottlenecks, then separately look at other logs or run a profiler, which is inefficient.",0.0,0.0,1.0,0.0
zipkin,** If it's already in your stack **,0.0,0.0,1.0,0.0
zipkin,It's hard to tell without more information.,0.189,0.0,0.811,-0.1027
zipkin,But it can be related to  incompatible libraries .,0.0,0.0,1.0,0.0
zipkin,Can you post your dependencies?,0.0,0.0,1.0,0.0
zipkin,In case you are using  older version  of okhttpclient with  latest  spring cloud:greenwich it can cause this issue.,0.0,0.0,1.0,0.0
zipkin,I'm using  Greenwich.RELEASE  with  okhttpclient:10.2.0  which works without problems,0.0,0.22,0.78,0.3089
zipkin,Use the below dependency Management for spring-boot to download the suitable versions for cloud version,0.0,0.0,1.0,0.0
zipkin,"I am using Java 10, cloud.version is  Finchley.SR2  and sprinb-boot:2.2.0 and spring-cloud-starter-openfeign :2.1.2.RELEASE.",0.0,0.0,1.0,0.0
zipkin,and this combination worked for me to fix the issue.,0.0,0.0,1.0,0.0
zipkin,Acctual problem was 10.x.x feign-core  was not working only  and io.github.openfeign:feign-core:jar:9.7.0:compile was working.,0.184,0.0,0.816,-0.4019
zipkin,"I faced this problem using java 11, springboot 2.3.0.RELEASE, and spring-cloud version Greenwich.RELEASE.",0.197,0.0,0.803,-0.4019
zipkin,Adding the following dependences saved me:,0.0,0.359,0.641,0.4215
zipkin,Hope this helps someone.,0.0,0.733,0.267,0.6705
zipkin,It seems to be a timing issue.,0.0,0.0,1.0,0.0
zipkin,"If we add some delay, for instance, between children spans execution like",0.155,0.169,0.676,0.0516
zipkin,In between,0.0,0.0,1.0,0.0
zipkin,Then we get to see spans:,0.0,0.0,1.0,0.0
zipkin,I've faced something like this before when Zipkin dropped spans I was (mistakenly) assigning wrong timestamps to.,0.158,0.128,0.714,-0.1531
zipkin,"For reference and ease of reproduction: I've setup a  project  for reproducing this issue / ""fix"".",0.0,0.161,0.839,0.3612
zipkin,"In zipkin lingo, what you are asking about is often called ""local spans"" or ""local tracing"", basically an operation that neither originated, nor resulted in a remote call.",0.0,0.0,1.0,0.0
zipkin,"I'm not aware of anything at the syscall level, but many tracers support explicit instrumentation of function calls.",0.0,0.173,0.827,0.5499
zipkin,"For example, using  py_zipkin 
 
@zipkin_span(service_name='my_service', span_name='some_function')
def some_function(a, b):
    return do_stuff(a, b)",0.0,0.0,1.0,0.0
zipkin,"Besides explicit instrumentation like this, one could also export data to zipkin.",0.0,0.185,0.815,0.3612
zipkin,"For example, one could convert trace data that is made in another tool to  zipkin's json format .",0.0,0.0,1.0,0.0
zipkin,"This probably doesn't answer your question, but I hope it helps.",0.0,0.475,0.525,0.8047
zipkin,The easiest way to get it working is to use the Micrometer library and configure the micrometer to send this data to the Zipkin server.,0.0,0.104,0.896,0.4215
zipkin,"Enabling metrics using micrometer is very simple, you need to just add a micrometer-core and spring-cloud-starter-zipkin libraries.",0.0,0.0,1.0,0.0
zipkin,See this tutorial for details about configuration and code  https://www.baeldung.com/tracing-services-with-zipkin,0.0,0.0,1.0,0.0
zipkin,Micrometer will report consumer/producer metrics to Zipkin,0.0,0.0,1.0,0.0
zipkin,If you search for zipkin grafana you'll get this as one of the first results  https://grafana.com/docs/grafana/latest/features/datasources/zipkin/,0.0,0.0,1.0,0.0
zipkin,You need to reload after adding the subsystem:,0.0,0.0,1.0,0.0
zipkin,This jboss-cli script should enable opentracing before starting the server properly.,0.0,0.0,1.0,0.0
zipkin,I'm not sure how/when you can execute that with keycloack image,0.164,0.0,0.836,-0.2411
zipkin,Connection refused: connect,0.524,0.0,0.476,-0.296
zipkin,"Simply means that RabbitMQ is not running on  localhost:5672  (which is the default if you don't provide a host/port, or addresses, for it in your  application.yml ).",0.0,0.0,1.0,0.0
zipkin,"If service 2 is getting traceId from service 1, you can take the traceId from requestHeader in your java code.",0.0,0.0,1.0,0.0
zipkin,Otherwise sleuth generate a new traceId in service 2.,0.0,0.0,1.0,0.0
zipkin,To get the trace Id In java,0.0,0.0,1.0,0.0
zipkin,Just do,0.0,0.0,1.0,0.0
zipkin,"Hello you can also get the x-b3-traceid header information from the request, I created a Util class for that -   https://gist.github.com/walterwhites/067dd635986e564aafdb5ac559073b0f",0.0,0.105,0.895,0.25
zipkin,Then in your main class you just need to call,0.0,0.0,1.0,0.0
zipkin,"after many searches, i found that there are a version conflicts between the dependencies.",0.191,0.0,0.809,-0.3818
zipkin,and thanks for  vladimir-vagaytsev,0.0,0.492,0.508,0.4404
zipkin,"so, i see that  spring-cloud-starter-sleuth  imported as a different version.",0.0,0.0,1.0,0.0
zipkin,to fix it i have added   sleuth.version  to properties  in pom.xml like so.,0.0,0.185,0.815,0.3612
zipkin,then in dependency management we need to specify the version like so,0.0,0.185,0.815,0.3612
zipkin,after then remove unused dependencies build and run.,0.0,0.0,1.0,0.0
zipkin,This class comes from zipkin-2.,0.0,0.0,1.0,0.0
zipkin,You can try adding this dependency.,0.0,0.0,1.0,0.0
zipkin,"Hello seeing your screenshot maybe you are using a spring boot 2.x version, I had the same problem with spring boot 2.0.3 with Finchley.RELEASE.",0.114,0.0,0.886,-0.4019
zipkin,"I found that Zipkin custom Server are not any more supported and deprecated for this reason it is not possible to use @EnableZipkinServer in Spring Cloud code and you have the ui but not the server side configured, api endpoint and so on.",0.037,0.0,0.963,-0.1505
zipkin,form the Zipkin base code:,0.0,0.0,1.0,0.0
zipkin,"the code is available on  official repo of Zipkin 
I solve the my problem using the official docker image with a compose",0.12,0.08,0.8,-0.2263
zipkin,How you can see i use the streaming version.,0.0,0.0,1.0,0.0
zipkin,it for me work,0.0,0.0,1.0,0.0
zipkin,I hope that is can help you,0.0,0.583,0.417,0.6808
zipkin,try with this:,0.0,0.0,1.0,0.0
zipkin,I believe you should be able to as long as you use the fully qualified domain name.,0.0,0.0,1.0,0.0
zipkin,"For instance,  zipkin.mynamespace.svc.cluster.local .",0.0,0.0,1.0,0.0
zipkin,Ok we found the problem and also a work around.,0.227,0.185,0.588,-0.128
zipkin,"It looks like all the documentation out there is wrong, at least for the version of Spring Cloud Sleuth we are using.",0.121,0.098,0.781,-0.1531
zipkin,The correct property is not  spring.sleuth.sampler.percentage .,0.0,0.0,1.0,0.0
zipkin,The correct property is  spring.sleuth.sampler.probability,0.0,0.0,1.0,0.0
zipkin,And here is a workaround we found right before noticing that the property was wrong.,0.193,0.0,0.807,-0.4767
zipkin,Here are some official documentation from Spring Cloud that contain the wrong property.,0.205,0.0,0.795,-0.4767
zipkin,https://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M5/single/spring-cloud-sleuth.html,0.0,0.0,1.0,0.0
zipkin,https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html,0.0,0.0,1.0,0.0
zipkin,Here is the source code that is being used and it is using  probability  not  percentage .,0.0,0.0,1.0,0.0
zipkin,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/sampler/SamplerProperties.java,0.0,0.0,1.0,0.0
zipkin,"Creating custom zipkin servers is an unsupported configuration, but if you must all of the configuration options are documented in the project readme:  https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md",0.073,0.063,0.864,-0.0644
zipkin,"For the dependencies part, the most important one is  zipkin-autoconfigure-storage-elasticsearch-http , here's an full maven pom.xml example:",0.0,0.122,0.878,0.2716
zipkin,"For the configuration part, you will need the following in you  application.yml :",0.0,0.0,1.0,0.0
zipkin,I configured zipkin to use ES as a data storage on top of kubernetes.,0.0,0.141,0.859,0.2023
zipkin,If it fits your requirement feel free to download and use  https://github.com/handysofts/zipkin-on-kubernetes,0.0,0.231,0.769,0.5106
zipkin,I found that these traces are actually generated by  https://github.com/spring-cloud/spring-cloud-consul/blob/master/spring-cloud-consul-discovery/src/main/java/org/springframework/cloud/consul/discovery/ConsulCatalogWatch.java,0.0,0.0,1.0,0.0
zipkin,"And since this is a class annotated with @scheduled , this Sleuth aspect applies :",0.0,0.0,1.0,0.0
zipkin,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/TraceSchedulingAspect.java,0.0,0.0,1.0,0.0
zipkin,"And therefore, the property to control the skipped regexp is not spring.sleuth.instrument.web.skipPattern , but spring.sleuth.instrument.",0.0,0.0,1.0,0.0
zipkin,scheduled .skip-pattern,0.0,0.0,1.0,0.0
zipkin,Of course - you have to just provide your own logging format (e.g.,0.0,0.0,1.0,0.0
zipkin,via   logging.pattern.level  - check  https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html  for more info).,0.0,0.0,1.0,0.0
zipkin,Then you have to register your own  SpanLogger  bean implementation where you will take care of adding the value of a spring profile via MDC  (you can take a look at this as an example  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/log/Slf4jSpanLogger.java  ),0.0,0.149,0.851,0.6808
zipkin,UPDATE:,0.0,0.0,1.0,0.0
zipkin,There's another solution for more complex approach that seems much easier than rewriting Sleuth classes.,0.0,0.282,0.718,0.6249
zipkin,You can try the  logback-spring.xml  way like here -  https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/service1/src/main/resources/logback-spring.xml#L5-L11  .,0.0,0.238,0.762,0.3612
zipkin,I'm resolving the application name there so maybe you could do the same with active profiles and won't need to write any code?,0.0,0.202,0.798,0.6486
zipkin,"As long as you have the ability to specify VM parameters, you can add the monitoring agent, regardless of the whether the JVM is started as a Windows service.",0.0,0.078,0.922,0.3182
zipkin,"For perfino, that VM parameter is",0.0,0.0,1.0,0.0
zipkin,please ensure config your zipkin sever correctly in your spring boot config file.,0.0,0.308,0.692,0.5994
zipkin,just like this:,0.0,0.556,0.444,0.3612
zipkin,And add below config in your zipkin client spring boot config file:,0.0,0.0,1.0,0.0
zipkin,We have a  LazyTraceExecutor  that you can use -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/async/LazyTraceExecutor.java  .,0.0,0.0,1.0,0.0
zipkin,"There are a bunch of ways to answer this, but I'll answer it from the ""one-way"" perspective.",0.0,0.0,1.0,0.0
zipkin,"The short answer though, is I think you have to roll your own right now!",0.0,0.0,1.0,0.0
zipkin,"While Kafka can be used in many ways, it can be used as a transport for unidirectional single producer single consumer messages.",0.0,0.0,1.0,0.0
zipkin,"This action is similar to normal one-way RPC, where you have a request, but no response.",0.167,0.0,0.833,-0.4215
zipkin,"In Zipkin, an RPC span is usually request-response.",0.0,0.0,1.0,0.0
zipkin,"For example, you see timing of the client sending to the server, and also the way back to the client.",0.0,0.0,1.0,0.0
zipkin,One-way is where you leave out the other side.,0.13,0.0,0.87,-0.0516
zipkin,"The span starts with a ""cs"" (client send) and ends with a ""sr"" (server received).",0.0,0.0,1.0,0.0
zipkin,"Mapping this to Kafka, you would mark client sent when you produce the message and server received when the consumer receives it.",0.0,0.0,1.0,0.0
zipkin,The trick to Kafka is that there is no nice place to stuff the trace context.,0.177,0.146,0.677,0.1027
zipkin,"That's because unlike a lot of messaging systems, there are no headers in a Kafka message.",0.145,0.0,0.855,-0.296
zipkin,"Without a trace context, you don't know which trace (or span for that matter) you are completing!",0.0,0.0,1.0,0.0
zipkin,"The ""hack"" approach is to stuff trace identifiers as the message key.",0.0,0.0,1.0,0.0
zipkin,A less hacky way would be to coordinate a body wrapper which you can nest the trace context into.,0.0,0.0,1.0,0.0
zipkin,Here's an example of the former:,0.0,0.0,1.0,0.0
zipkin,https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30,0.0,0.0,1.0,0.0
zipkin,"I meet the same problem too.Here is my solution, a less hacky way as above said.",0.159,0.135,0.706,-0.1027
zipkin,"you must implements MyHttpServerRequest and MyRequest.It is easy,you just return something a span need,such as uri,header,method.",0.0,0.0,1.0,0.0
zipkin,"This is a rough and ugly code example,just offer an idea.",0.268,0.0,0.732,-0.5106
zipkin,server.address=&lt;ip&gt;  does not work?,0.0,0.0,1.0,0.0
zipkin,java -jar zipkin.jar --server.address=192.168.0.7,0.0,0.0,1.0,0.0
zipkin,If it's not working you can add a property file and connects to it when the server starts:,0.0,0.0,1.0,0.0
zipkin,java -jar zipkin.jar --spring.config.location=./application.properties,0.0,0.0,1.0,0.0
zipkin,in application.properties:,0.0,0.0,1.0,0.0
zipkin,"I'm not entirely sure if that's what you mean, but you can use Jeager  https://www.jaegertracing.io/   which checks if trace-id already exist in the invocation metadata and in it generate child trace id.",0.049,0.0,0.951,-0.1505
zipkin,Based on all trace ids call diagrams are generated,0.0,0.0,1.0,0.0
zipkin,In your command please try the following -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin,0.0,0.187,0.813,0.3182
zipkin,"i had the same issue, but i solved with this jvm arguments:",0.25,0.187,0.563,-0.2263
zipkin,"-Dotel.traces.exporter=zipkin -Dotel.metrics.exporter=none -Dotel.exporter.zipkin.endpoint=http://localhost:9411/api/v2/spans
Maybe the error is on zipkin.endpoint, try to write the entire url.",0.162,0.0,0.838,-0.4019
zipkin,"Regards,
Marco",0.0,0.0,1.0,0.0
zipkin,I get the same problem and below command did the trick.,0.328,0.0,0.672,-0.4404
zipkin,I checked the source code.,0.0,0.0,1.0,0.0
zipkin,It looks the property name has been changed:,0.0,0.0,1.0,0.0
zipkin,https://github.com/open-telemetry/opentelemetry-java/blob/14ace1ec32dbb194b8990763beb3ab6935849547/sdk-extensions/autoconfigure/src/main/java/io/opentelemetry/sdk/autoconfigure/TracerProviderConfiguration.java#L43,0.0,0.0,1.0,0.0
zipkin,Getting a handle on the distributed tracing space can be a bit confusing.,0.16,0.0,0.84,-0.2263
zipkin,Here's a quick summary...,0.0,0.0,1.0,0.0
zipkin,Open Source Tracers,0.0,0.0,1.0,0.0
zipkin,"There are a number of popular open source tracers, which is where Zipkin sits:",0.0,0.272,0.728,0.4767
zipkin,Commercial Tracers,0.0,0.0,1.0,0.0
zipkin,There are also a lot of vendors offering commercial monitoring/observability tools which are either centred around or include distributed tracing:,0.0,0.0,1.0,0.0
zipkin,Standardisation Efforts,0.0,0.0,1.0,0.0
zipkin,Alongside all these products are numerous attempts at creating standards around distributed tracing.,0.0,0.155,0.845,0.296
zipkin,"These typically start by creating a standard API for the trace-recording side of the architecture, and sometimes extend to become prescriptive about the content of traces or even the wire format.",0.0,0.122,0.878,0.4404
zipkin,This is where OpenTracing fits in.,0.0,0.0,1.0,0.0
zipkin,"So it is not a tracing solution itself, but an API that can be implemented by the trace recording SDKs of multiple tracers, allowing you to swap between vendors more easily.",0.045,0.107,0.848,0.4693
zipkin,The most common standards are:,0.0,0.0,1.0,0.0
zipkin,"Note that the first two in the list have been abandoned, with their contributors joining forces to create the third one together.",0.12,0.084,0.797,-0.2263
zipkin,[1],0.0,0.0,1.0,0.0
zipkin,[1]  https://opensource.googleblog.com/2019/05/opentelemetry-merger-of-opencensus-and.html,0.0,0.0,1.0,0.0
zipkin,"I was using it with default storage which is discouraged in production use, it can handle only small amount of data and can be treated only as a demo version.",0.091,0.0,0.909,-0.4019
zipkin,What helped a little was setting,0.0,0.0,1.0,0.0
zipkin,spring.sleuth.sampler.probability: 0.01,0.0,0.0,1.0,0.0
zipkin,-- by default it logs all spans.,0.0,0.0,1.0,0.0
zipkin,You should create application.properties file and after that you should add the following,0.0,0.149,0.851,0.2732
zipkin,Your application.properties :,0.0,0.0,1.0,0.0
zipkin,Your main class :,0.0,0.0,1.0,0.0
zipkin,Your Pom.xml :,0.0,0.0,1.0,0.0
zipkin,Your zipkin port :,0.0,0.0,1.0,0.0
zipkin,"It looks like it is related to  https://github.com/openzipkin/zipkin-js/pull/498 , could you try with zipkin-context-cls@0.19.2-alpha.7 and change  ctxImpl  into  ctxImpl = new CLSContext('zipkin', true); ?",0.0,0.111,0.889,0.3612
zipkin,"The problem ended up not being on Zipkin's end, but instead in how I was instrumenting the express server.",0.098,0.0,0.902,-0.2144
zipkin,I had added the zipkin middleware  after  my call to  app.get .,0.0,0.0,1.0,0.0
zipkin,Express executes middlwares in order and makes no distinction between middleware for a named route vs. something added via  app.use .,0.109,0.0,0.891,-0.296
zipkin,Doing things like this,0.0,0.455,0.545,0.3612
zipkin,Gave me the result I was looking for.,0.0,0.0,1.0,0.0
zipkin,"The reason behind error was that i forgot to add the kafka dependency in pom.xml
After adding the dependency, error was gone.",0.221,0.0,0.779,-0.6597
zipkin,"According to Sleuth documentation, AWS SQS is &quot;natively&quot; supported only on the consumer's side:",0.0,0.15,0.85,0.3182
zipkin,https://docs.spring.io/spring-cloud-sleuth/docs/current-SNAPSHOT/reference/html/#spring-cloud-aws-messaging-sqs,0.0,0.0,1.0,0.0
zipkin,In order to add seamless tracing over AWS SQS I resorted to Brave SQS instrumentation (aka SqsMessageTracing) and had to add another dependency:,0.0,0.139,0.861,0.5267
zipkin,and have the following configuration:,0.0,0.0,1.0,0.0
zipkin,This is just because I didn't want to do the SQS producer instrumentation myself nor add the tracing headers programmatically.,0.064,0.0,0.936,-0.0572
zipkin,Little reference for the Brave instrumentation can be found here:,0.0,0.274,0.726,0.5267
zipkin,https://github.com/spring-cloud/spring-cloud-sleuth/issues/1550#issuecomment-589686583,0.0,0.0,1.0,0.0
zipkin,My SQS message producer looks like this:,0.0,0.294,0.706,0.3612
zipkin,FINAL NOTE,0.0,0.0,1.0,0.0
zipkin,Not required but I also excluded the,0.383,0.0,0.617,-0.4767
zipkin,Since it performs an AWS environment configuration scan at application startup which wasn't required for me (and also raised a lengthy error log),0.114,0.0,0.886,-0.4019
zipkin,I think you must have found your answer by now.,0.0,0.0,1.0,0.0
zipkin,But I am posting this for future reference.,0.0,0.0,1.0,0.0
zipkin,"Take a look at this  Github issue , it basically explains everything and provides a few workarounds.",0.0,0.0,1.0,0.0
zipkin,According to  this  Spring Cloud Sleuth is the only tracer that supports messaging.,0.0,0.172,0.828,0.3612
zipkin,"Brave is the library spring cloud sleuth is built on, therefore you could make it work without sleuth:  https://github.com/openzipkin/brave",0.0,0.159,0.841,0.5267
zipkin,"Just to clarify though, Sleuth doesn't force you to use any of the rest of the spring-cloud components.",0.0,0.0,1.0,0.0
zipkin,"It is  spring-cloud  because it is one of the ""cloud native"" spring technologies",0.0,0.0,1.0,0.0
zipkin,ok I finally realized whats the mistake that I have done when I was starting my zipkin server with this command,0.117,0.107,0.777,-0.0516
zipkin,but I was not specifying my Zipkin server where my Kafka is running so when I did,0.0,0.0,1.0,0.0
zipkin,it worked,0.0,0.0,1.0,0.0
zipkin,"I am new to zipkin and golang, If you want to trace internal process, then you can create span from context",0.0,0.159,0.841,0.34
zipkin,"example: say you have api called Login, inside login you might perform database operation or any other operations",0.0,0.0,1.0,0.0
zipkin,"On my project, we generated the spans manually before sending the events.",0.0,0.0,1.0,0.0
zipkin,"var span = tracing.tracer().nextSpanWithParent(req -&gt; true,
Void.class, ctx.get(Span.class).context());",0.0,0.318,0.682,0.4215
zipkin,span.name(&quot;yourSpanName&quot;).start();,0.0,0.0,1.0,0.0
zipkin,return sendEventPublisher.doOnError(span::error).doOnTerminate(span::finish);,0.0,0.0,1.0,0.0
zipkin,"This way, we also link the span to the publisher lifecycle as we had problems with webflux sharing spans between threads.",0.11,0.114,0.776,0.0258
zipkin,"Basically, we create a span and link it to the parent context created by Spring for the request (either from an incoming B3 HTTP header, or generated if absent).",0.0,0.136,0.864,0.4767
zipkin,&quot;ctx&quot; is the subscriber context here.,0.0,0.0,1.0,0.0
zipkin,This also implied to tell sleuth not to generate the spans for async operations in application.properties:,0.0,0.0,1.0,0.0
zipkin,spring.sleuth.async.enabled=false,0.0,0.0,1.0,0.0
zipkin,"For basic authentication, the username and password are required to be sent as part of the HTTP Header  Authorization .",0.0,0.0,1.0,0.0
zipkin,"The header value is computed as Base64 encoding of the string  username:password .So if the username is  abcd  and password is  1234 , the header will look something like this (Chatset used: UTF-8).",0.0,0.14,0.86,0.5994
zipkin,Authorization: Basic YWJjZDoxMjM0,0.0,0.0,1.0,0.0
zipkin,Sleuth cloud project provides  ZipkinRestTemplateCustomizer  to configure the  RestTemplate  used to communicate with the Zipkin server.,0.0,0.0,1.0,0.0
zipkin,"Refer to the documentation for the same: 
 https://cloud.spring.io/spring-cloud-sleuth/reference/html/#sending-spans-to-zipkin",0.0,0.0,1.0,0.0
zipkin,Note: Base64 encoding is reversible and hence Basic auth credentials are not secured.,0.158,0.0,0.842,-0.3089
zipkin,HTTPS communication should be used along with Basic Authentication.,0.0,0.0,1.0,0.0
zipkin,I got same problem.,0.574,0.0,0.426,-0.4019
zipkin,Seem Spring boot  2.1.2.RELEASE  not work with Zipkin.,0.0,0.0,1.0,0.0
zipkin,Please upgrade to Spring boot version &gt;  2.1.2.RELEASE .,0.0,0.247,0.753,0.3182
zipkin,"For those who could come across with a same scenario like this,",0.0,0.2,0.8,0.3612
zipkin,github  has given  APIs  to get details on the repository tag set of each project release as a json object ( https://api.github.com/repos/openzipkin/zipkin/tags  ).,0.0,0.0,1.0,0.0
zipkin,So that can be used to get the latest version of zipkin.,0.0,0.0,1.0,0.0
zipkin,"To get the currently running version of my system, zipkin has given an actuator/info end point ( http://localhost:9411/actuator/info ).",0.0,0.0,1.0,0.0
zipkin,Yes.,0.0,1.0,0.0,0.4019
zipkin,You have to use Brave.,0.0,0.459,0.541,0.5267
zipkin,"In fact, Spring cloud sleuth (V2) uses Brave under the hood.",0.0,0.254,0.746,0.5267
zipkin,Check the brave web-mvc example to get started.,0.0,0.327,0.673,0.5267
zipkin,https://github.com/openzipkin/brave-webmvc-example,0.0,0.0,1.0,0.0
zipkin,Try to change all properties with this:,0.0,0.0,1.0,0.0
zipkin,"spring.sleuth.sampler.percentage=1.0 is Edgware
so you need that",0.0,0.0,1.0,0.0
zipkin,baseUrl by default is localhost,0.0,0.0,1.0,0.0
zipkin,"When I change the project root log level to ""debug"", I saw some error report from zipkin.",0.162,0.0,0.838,-0.4019
zipkin,Then I realized that the zipkin server I used was very very old.,0.0,0.0,1.0,0.0
zipkin,And the zipkin API call returned 404.,0.0,0.0,1.0,0.0
zipkin,When I updated my zipkin server to latest version.,0.0,0.0,1.0,0.0
zipkin,It worked.,0.0,0.0,1.0,0.0
zipkin,According to  the section titled Cleanup in the Istio docs :,0.0,0.0,1.0,0.0
zipkin,"However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans).",0.0,0.122,0.878,0.3612
zipkin,"If I use org.slf4j.Logger to simply LOG.info(""something""), I see the INFO message in console output, with the exportable flag set to true:",0.0,0.128,0.872,0.4215
zipkin,You can't send logs to Zipkin.,0.0,0.0,1.0,0.0
zipkin,You can send log statements to ELK.,0.0,0.0,1.0,0.0
zipkin,"You can check out the sample  https://github.com/marcingrzejszczak/vagrant-elk-box  that has a vagrant box with ELK, uses Sleuth for log correlation and uses ELK to visualize the logs",0.0,0.0,1.0,0.0
zipkin,To log only request with particular error you can add the log in your exception mapper where you are handling those error.,0.213,0.0,0.787,-0.6597
zipkin,"To show the log for error response you can set like below,",0.178,0.164,0.658,-0.0516
zipkin,and set,0.0,0.0,1.0,0.0
zipkin,You can add the trace id in your logback.xml,0.0,0.0,1.0,0.0
zipkin,"""request_id"":
                {""trace_id"":""%X{X-B3-TraceId}"",""span_id"":""%X{X-B3-SpanId}"",""parent_span_id"":""%X{X-B3-ParentSpanId}""},",0.0,0.0,1.0,0.0
zipkin,I found the solution I think.,0.0,0.434,0.566,0.3182
zipkin,I changed it to this:,0.0,0.0,1.0,0.0
zipkin,RABBIT_URI=amqp://user:password@tracing:5672,0.0,0.0,1.0,0.0
zipkin,Please use latest snapshots.,0.0,0.434,0.566,0.3182
zipkin,Sleuth in latest snapshots uses brave internally so integration will be extremely simple.,0.0,0.221,0.779,0.5267
zipkin,This feature is available in edgware release train.,0.0,0.0,1.0,0.0
zipkin,That corresponds to version 1.3.x of sleuth,0.0,0.0,1.0,0.0
zipkin,The Zipkin UI is making an AJAX request to the API in order to retrieve the data that is displayed.,0.0,0.0,1.0,0.0
zipkin,You can find the API definition for zipkin at:,0.0,0.0,1.0,0.0
zipkin,https://github.com/openzipkin/zipkin-api,0.0,0.0,1.0,0.0
zipkin,I believe you are looking for the URL:  http://zipkin.iamplus.xyz/api/v1/traces,0.0,0.0,1.0,0.0
zipkin,From there you will get the traces matching your filter,0.0,0.0,1.0,0.0
zipkin,I have the same config running on my ingress 9.0-beta.11.,0.0,0.0,1.0,0.0
zipkin,I guess it's just a misconfiguration.,0.0,0.0,1.0,0.0
zipkin,First I'll recommend you to not change the template and use the default values and just change when the basic-auth works.,0.0,0.215,0.785,0.6369
zipkin,What the logs of ingress show to you?,0.0,0.0,1.0,0.0
zipkin,Did you create the basic-auth file in the same namespace of the ingress resource?,0.0,0.139,0.861,0.2732
zipkin,The issue got fixed -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/585  .,0.0,0.0,1.0,0.0
zipkin,In the upcoming releases 1.1.5 and 1.2.1 it should work,0.0,0.0,1.0,0.0
zipkin,Spring Cloud Zipkin Stream is using Spring Cloud Stream underneath.,0.0,0.0,1.0,0.0
zipkin,You need to provide how do you want to send the spans to Zipkin - thus you need a binder.,0.0,0.071,0.929,0.0772
zipkin,One possible binder is the RabbitMQ binder.,0.0,0.0,1.0,0.0
zipkin,Check out this:  https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/zipkin-server/build.gradle#L6,0.0,0.0,1.0,0.0
zipkin,It seems that Brave does not support this.,0.194,0.292,0.515,0.2828
zipkin,An issue has been reported on their GitHub page.,0.0,0.0,1.0,0.0
zipkin,https://github.com/openzipkin/brave/issues/166,0.0,0.0,1.0,0.0
zipkin,I don't know much about ActiveMQ but you need to pass the zipkin trace information along.,0.0,0.0,1.0,0.0
zipkin,"Review the  ActiveMQ Collector  section in this doc 
 https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md",0.0,0.0,1.0,0.0
zipkin,I just set up Zipkin tracing for a stack that includes RabbitMQ.,0.0,0.0,1.0,0.0
zipkin,"I added the parent_span_id, and span_id to the message header before the message is placed on the queue.",0.0,0.0,1.0,0.0
zipkin,Then the applications that read the messages get the trace information from the header.,0.0,0.0,1.0,0.0
zipkin,"And if you need more help, I recommend jumping on IRC #zipkin.",0.0,0.391,0.609,0.6976
zipkin,"I was not able to reproduce your issue with the  spring-cloud-sleuth-sample-zipkin  app (it worked to me), here's what I did:",0.0,0.0,1.0,0.0
zipkin,A few pointers to troubleshoot this:,0.0,0.31,0.69,0.2023
zipkin,Try to make it work using the  sample  and try to bring the working example closer to your app (by adding dependencies) and see what is the difference between the two and where will it break.,0.0,0.0,1.0,0.0
zipkin,If you can create a minimal sample app (e.g.,0.0,0.231,0.769,0.2732
zipkin,": based on the zipkin sample) that reproduces the issue, please feel free to create an issue on GH:  https://github.com/spring-cloud/spring-cloud-sleuth  and tag me ( @jonatan-ivanov ), I can take a look.",0.0,0.243,0.757,0.7717
zipkin,Finally I found it.,0.0,0.0,1.0,0.0
zipkin,I had 2 problemas,0.0,0.0,1.0,0.0
zipkin,1 - I was using zipkin-slim docker image for my zip container.,0.0,0.0,1.0,0.0
zipkin,This image doesn't contain the rabbitmq collector  rabbitmq collector .,0.0,0.0,1.0,0.0
zipkin,I have replaces by standar zipkin image,0.0,0.0,1.0,0.0
zipkin,"2 - I do not know why, but the connection from sleuth/zipkin to RabbitMQ is not retrying (I will investigate further).",0.0,0.0,1.0,0.0
zipkin,"So, if I was in a hurry and test very early (when RabbitMQ is not yet available) it fails, and not retries.",0.128,0.0,0.872,-0.4215
zipkin,My docker-compose relevant sections now are like this:,0.0,0.263,0.737,0.3612
zipkin,Thanks again to  Jonatan Ivanov  for helping me!,0.0,0.473,0.527,0.6588
zipkin,probably best to have the issue you raised in github vs cross posting.,0.0,0.259,0.741,0.6369
zipkin,it is a bug  https://github.com/honeycombio/honeycomb-opentracing-proxy/issues/37,0.0,0.0,1.0,0.0
zipkin,There are 2 entries in mysql zipkin_spans table,0.0,0.0,1.0,0.0
zipkin,Example,0.0,0.0,1.0,0.0
zipkin,32 character hex trace id  5ec92d0240cd9dee0421f4763e9f674f  displayed in zipkin ui corresponds to,0.0,0.0,1.0,0.0
zipkin,trace_id_high = 6830039797584469486 in mysql  (5EC92D0240CD9DEE -  upper 16 hex character),0.0,0.0,1.0,0.0
zipkin,id = 297787839077115727 in mysql  (421F4763E9F674F -  lower 16 hex charecter),0.216,0.0,0.784,-0.296
zipkin,After continue efforts and going throgh core api of spring boot application I got my solution:),0.0,0.0,1.0,0.0
zipkin,Root cause of my issue is below :,0.0,0.0,1.0,0.0
zipkin,"MY application using Spring boot RabitMQ integration and due that
zipkin taking 1st prefrance to RabitMQ sender and my trace are ignored
my zipkin server.",0.087,0.0,0.913,-0.3182
zipkin,"So use below configration is anyone has same issue to avoid lot painless efforts , even we are not getting in logs of server root cause of it",0.075,0.075,0.85,0.0
zipkin,*---,0.0,0.0,1.0,0.0
zipkin,I was using Finchley.SR2 train of releases.,0.0,0.0,1.0,0.0
zipkin,"Once I upgraded to the latest Spring Boot and Spring Cloud versions, the issue fixed itself.",0.0,0.0,1.0,0.0
zipkin,I removed the opentracing-spring-cloud-starter dependency and am now just using,0.0,0.0,1.0,0.0
zipkin,Check your configuration file and make sure the baseUrl is given properly here,0.0,0.161,0.839,0.3182
zipkin,OK!,0.0,1.0,0.0,0.3595
zipkin,I see now the problem!,0.499,0.0,0.501,-0.4574
zipkin,"So, you say that your HTTP request has these tracing headers:  X-B3-TraceId ,  X-B3-SpanId ,  X-B3-Sampled ,  X-Span-Name ,  X-B3-ParentSpanId .",0.0,0.0,1.0,0.0
zipkin,Then you have this code:,0.0,0.0,1.0,0.0
zipkin,And that's absolutely natural that your tracing header are not transferred to the RabbitMQ: there is just no those headers to send.,0.088,0.112,0.8,0.1513
zipkin,I believe that you can extract those headers in this  @RequestMapping  method and populate them to the AMQP message before sending.,0.0,0.0,1.0,0.0
zipkin,See  org.springframework.amqp.core.MessageBuilder .,0.0,0.0,1.0,0.0
zipkin,"I also think that Spring Cloud Sleuth should have some mechanism to obtain those headers, e.g.",0.0,0.0,1.0,0.0
zipkin,Tracer.currentSpan() :  http://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.RC1/single/spring-cloud-sleuth.html#_current_span,0.0,0.0,1.0,0.0
zipkin,"Yes, they are both stateless.",0.0,0.403,0.597,0.4019
zipkin,You can deploy them using whatever horizontal-scalability construct is available to you.,0.0,0.0,1.0,0.0
zipkin,"When connecting to the mysql container while using links, you need to use the container name as a hostname.",0.0,0.0,1.0,0.0
zipkin,Change the connection string to:,0.0,0.0,1.0,0.0
zipkin,"And when starting the zipkin container, set the env variable:",0.0,0.0,1.0,0.0
zipkin,Why are you mocking a span?,0.403,0.0,0.597,-0.4019
zipkin,This makes absolutely no sense.,0.384,0.0,0.616,-0.3597
zipkin,Also a Span is never a bean.,0.0,0.0,1.0,0.0
zipkin,You already create a normal span via a builder and you should leave that.,0.09,0.158,0.752,0.2263
zipkin,Assuming that you have set up the Boot context property and  you want to mock out  tracer  bean you should do the following,0.112,0.052,0.837,-0.3612
zipkin,"*sigh, so it turns out, that someone had turned off zipking tracing in a properties file, for no good reason.",0.1,0.131,0.769,0.1779
zipkin,*sigh,0.0,0.0,1.0,0.0
zipkin,You are trying to run 2 different applications.,0.0,0.0,1.0,0.0
zipkin,To run the  zipkin  application with with ElasticSearch and Kafka you will need to run it with both sets of environment variables:,0.0,0.0,1.0,0.0
zipkin,"Once you have the  zipkin  server running with ES, then you can use your second command to generate the data for the dependency graph view",0.0,0.0,1.0,0.0
zipkin,"At the moment, there's no replacement for the ""thread binder"" apis.",0.18,0.0,0.82,-0.296
zipkin,There will be in the coming months.,0.0,0.0,1.0,0.0
zipkin,This is indeed needed to renovate existing instrumentation.,0.0,0.0,1.0,0.0
zipkin,"Until then, you can re-use thread binders via TracerAdapter or use a different in-process propagation library.",0.0,0.0,1.0,0.0
zipkin,The following link includes a working example  https://github.com/openzipkin/brave/tree/master/brave#upgrading-from-brave-3,0.0,0.0,1.0,0.0
zipkin,"It was removed in Sleuth 3.0, though it seems the docs were not updated, I'm going to update the docs soon.",0.0,0.0,1.0,0.0
zipkin,"To fix the rest with your logs, you can check the logging config  here , the  log integration in the docs  and  this answer .",0.0,0.0,1.0,0.0
zipkin,This should be done out of the box:  https://docs.spring.io/spring-cloud-sleuth/docs/2.2.7.RELEASE/reference/html/#feign,0.0,0.0,1.0,0.0
zipkin,"You can take a look at the feign sample (you need to go back in the history, currently it is for 3.x):  https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-feign",0.0,0.0,1.0,0.0
zipkin,"In order to see if propagation works, look into the outgoing request, it should contain the tracing-related headers.",0.0,0.115,0.885,0.296
zipkin,"You could create your own  SpanHandler  bean that takes the  FinishedSpan , converts into JSON and stores it somewhere on your drive.",0.0,0.095,0.905,0.2732
zipkin,Then you could just iterate over jsons and upload them to the Zipkin server,0.0,0.0,1.0,0.0
zipkin,"No, the tracing SPI will not be backported to Vert.x 3.",0.18,0.0,0.82,-0.296
zipkin,I would recommend to check out  Migrate from Vert.x 3 to Vert.x 4 :,0.0,0.217,0.783,0.3612
zipkin,"When­ever pos­si­ble, Vert.x 4 APIs have been made avail­able in
Vert.x 3 with a dep­re­ca­tion of the old API, giv­ing the
op­por­tu­nity to im­prove a Vert.x 3 ap­pli­ca­tion with a bet­ter
API while al­low­ing the ap­pli­ca­tion to be ready for a Vert.x 4
mi­gra­tion.",0.0,0.067,0.933,0.3612
zipkin,"In other words, one of the Vert.x 4 goals was to minimize the upgrading effort.",0.0,0.0,1.0,0.0
zipkin,You should use e.g.,0.0,0.0,1.0,0.0
zipkin,openzipkin Brave project or Opentelemetry projects directly.,0.0,0.362,0.638,0.5267
zipkin,Sleuth works only with boot based projects,0.0,0.0,1.0,0.0
zipkin,"I think what you call correlationId is in fact the traceId, if you are new to distributed tracing, I highly recommend reading the docs of  spring-cloud-sleuth , the  introduction  section will give you a basic understanding while the  propagation  will tell you well, how your fields are propagated across services.",0.0,0.1,0.9,0.5984
zipkin,"I also recommend this talk:  Distributed Tracing: Latency Analysis for Your Microservices - Grzejszczak, Krishna .",0.0,0.172,0.828,0.3612
zipkin,To answer your exact questions:,0.0,0.0,1.0,0.0
zipkin,How the correlation id will be passed to Kafka messages?,0.0,0.0,1.0,0.0
zipkin,"Kafka has headers, I assume the fields are propagated through Kafka headers.",0.0,0.0,1.0,0.0
zipkin,How the correlation id will be passed to Http requests?,0.0,0.0,1.0,0.0
zipkin,Through HTTP Headers.,0.0,0.0,1.0,0.0
zipkin,Is it possible to use existing tracedId from other service?,0.0,0.0,1.0,0.0
zipkin,"Not just possible, Sleuth does this for you out of the box.",0.0,0.0,1.0,0.0
zipkin,If there is a traceId in the incoming request/message/event/etc.,0.0,0.0,1.0,0.0
zipkin,Sleuth will not create a new one but it will use it (see the docs I linked above).,0.086,0.0,0.914,-0.1045
zipkin,Your service is expecting following labels on pod:,0.0,0.0,1.0,0.0
zipkin,Although it looks like you have only one label on zipkin pods:,0.0,0.185,0.815,0.3612
zipkin,"Label selector uses logical AND (&amp;&amp;), and this means that all labels specified must be on pod to match it.",0.0,0.0,1.0,0.0
zipkin,The following worked.,0.0,0.0,1.0,0.0
zipkin,"Sorry, I cannot provide info on all details since I don't know them :( Maybe somebody else can.",0.073,0.136,0.791,0.2746
zipkin,deployment.yaml,0.0,0.0,1.0,0.0
zipkin,service.yaml,0.0,0.0,1.0,0.0
zipkin,ingress.yaml,0.0,0.0,1.0,0.0
zipkin,It's because of sampling.,0.0,0.0,1.0,0.0
zipkin,Please create a bean of sampler type whose value can be Sampler.ALWAYS or set the probability property to 1.0,0.0,0.312,0.688,0.7003
zipkin,Hi I just resolved this issue ..,0.0,0.254,0.746,0.1779
zipkin,Step1.verify C:\Programfiles\Err(version) folder including bin folder is created or not.,0.0,0.182,0.818,0.25
zipkin,Otherwise try to download and install Erlang again.,0.0,0.0,1.0,0.0
zipkin,reinstall RabbitMQ and try connecting Zipkin.,0.0,0.0,1.0,0.0
zipkin,Make sure Erlang version and RabbitMQ version is compatible.,0.0,0.223,0.777,0.3182
zipkin,Step2.Check ERLANG_HOME is set to proper location in environment variables.,0.0,0.0,1.0,0.0
zipkin,"at this point if RabbitMQ windows installer pointing to any old erlang version installed earlier  try to install RabitMQ windows manually
follow the steps mentioned in below link for manual installation",0.0,0.0,1.0,0.0
zipkin,https://www.rabbitmq.com/install-windows-manual.html,0.0,0.0,1.0,0.0
zipkin,That's an old implementation.,0.0,0.0,1.0,0.0
zipkin,Below I have modified your code to work:,0.0,0.0,1.0,0.0
zipkin,For more information check this link:  https://gist.github.com/marcingrzejszczak/d3c15a0c11dda71970e42c513c9c0e09,0.0,0.0,1.0,0.0
zipkin,From  zipkin docs :,0.0,0.0,1.0,0.0
zipkin,There is no support for TTL through this SpanStore.,0.185,0.227,0.588,0.128
zipkin,It is recommended instead to use Elastic Curator to remove indices older than the point you are interested in.,0.0,0.209,0.791,0.5423
zipkin,"I had the same problem and solved it by:
Open windows task manger and kill all java instances java.exe or javaw.exe",0.279,0.079,0.642,-0.743
zipkin,I have finally figured out what could be the cause of this issue:,0.0,0.0,1.0,0.0
zipkin,The install option:,0.0,0.0,1.0,0.0
zipkin,--set values.tracing.provider=zipkin --set values.global.tracer.zipkin.address,0.0,0.0,1.0,0.0
zipkin,requires  &lt;zipkin-collector-service&gt;.&lt;zipkin-collector-namespace&gt;:9411  according to  istio  documentation.,0.0,0.0,1.0,0.0
zipkin,While You have just IP address and port of external server.,0.0,0.0,1.0,0.0
zipkin,This most likely means that the install option requires existing name that is in istio service mesh registry.,0.0,0.0,1.0,0.0
zipkin,"So if Your zipkin collector is outside cluster We need to add  ServiceEntry ,  VirtualService  and maybe  DestinationRule  and so the external service can be used within mesh.",0.0,0.0,1.0,0.0
zipkin,You can follow  istio  documentation to see how to create these objects for external service.,0.0,0.13,0.87,0.2732
zipkin,Here  is another guide.,0.0,0.0,1.0,0.0
zipkin,After that We need to update the tracer address value with the  VirtualService  as an endpoint.,0.0,0.138,0.862,0.34
zipkin,Hope this helps.,0.0,0.846,0.154,0.6705
zipkin,By using the following  commands  I was able to generate the manifests using  istioctl  with parameters You mentioned:,0.0,0.0,1.0,0.0
zipkin,Then compared them to see differences made with those parameter modifications.,0.0,0.0,1.0,0.0
zipkin,You can try to manually modify those applied settings or apply it to Your cluster.,0.0,0.0,1.0,0.0
zipkin,Istioctl I used to generate these manifests:,0.0,0.0,1.0,0.0
zipkin,Hope it helps.,0.0,0.846,0.154,0.6705
zipkin,"Still with using 2.2.0 parent, I still face the whitelable error.",0.231,0.0,0.769,-0.4019
zipkin,I will check on this latter but by changing the pom defination the Zipkin server work,0.0,0.0,1.0,0.0
zipkin,And in zipkinserverapplication we need the @Enablezipkinserver,0.0,0.0,1.0,0.0
zipkin,Hi if your spring cloud app target is a Spring Boot 2.x base I suggest to do not try to use the @EnableZipkinServer because it is not a raccomanded way as the java doc suggest:,0.0,0.0,1.0,0.0
zipkin,form the Zipkin base code:,0.0,0.0,1.0,0.0
zipkin,I spoke for personal experience with spring boot application 2.x family.,0.0,0.0,1.0,0.0
zipkin,"The my solution,for development, was use a docker compose like below(consider that my application use spring cloud stream in order to push on zipkin the tracing information:",0.0,0.091,0.909,0.3612
zipkin,On the other hands if your target is a spring boot 1.5.x you can use the legacy embedded zipkin server like below:,0.0,0.111,0.889,0.3612
zipkin,POM:,0.0,0.0,1.0,0.0
zipkin,Application:,0.0,0.0,1.0,0.0
zipkin,package it.valeriovaudi.emarket;,0.0,0.0,1.0,0.0
zipkin,"both solution works for me, spring boot 1.5.x was the base spring boot version for my master thesis and spring boot 2.x version for a my personal distributed system project that I use every day for my persona family budget management.",0.0,0.057,0.943,0.3182
zipkin,I hope that this can help you,0.0,0.583,0.417,0.6808
zipkin,For more details can read the document  https://www.baeldung.com/spring-boot-actuators#boot-2x-actuator,0.0,0.0,1.0,0.0
zipkin,I have a working project with spring cloud stream and zipkin using the following configuration (maybe you should set the sender.type):,0.0,0.0,1.0,0.0
zipkin,Hope this can help.,0.0,0.737,0.263,0.6808
zipkin,"The problem lies in your  ES_HOSTS  variable, from the docs  here :",0.379,0.0,0.621,-0.6705
zipkin,So you will need:  ES_HOSTS=http://storage:9200,0.0,0.0,1.0,0.0
zipkin,Finally I have this file:,0.0,0.0,1.0,0.0
zipkin,Main differences are the usage of,0.0,0.0,1.0,0.0
zipkin,"""ES_HOSTS=elasticsearch:9300""",0.0,0.0,1.0,0.0
zipkin,instead of,0.0,0.0,1.0,0.0
zipkin,"""ES_HOSTS=storage:9300""",0.0,0.0,1.0,0.0
zipkin,and in the dependencies configuration I add the entrypoint in dependencies:,0.0,0.0,1.0,0.0
zipkin,"entrypoint: crond -f
  This one is really the key to not have the exception when I start docker-compose.",0.0,0.0,1.0,0.0
zipkin,"To solve this issue, I check the this project:  https://github.com/openzipkin/docker-zipkin",0.0,0.184,0.816,0.2023
zipkin,The remaining question is: why do I need to use entrypoint: crond -f,0.0,0.0,1.0,0.0
zipkin,"I found examples from:
 https://github.com/openzipkin/zipkin/tree/master/zipkin-lens/testdata",0.0,0.0,1.0,0.0
zipkin,It works well.,0.0,0.512,0.488,0.2732
zipkin,"Set the datasource setting in the application.yml file of the application as follows,",0.0,0.0,1.0,0.0
zipkin,You can add the zipkin attribute to POM.xml,0.0,0.0,1.0,0.0
zipkin,Problems can occur due to Spring's auto configuration property.,0.252,0.0,0.748,-0.4019
zipkin,"Therefore, modify the datasource setting as follows and modify the datasource configuration related source to make the application work normally.",0.0,0.0,1.0,0.0
zipkin,It looks like the trace is triggering an old-fashioned inverted-lock-order deadlock freezing threads that are attempting to acquire new Connections.,0.163,0.107,0.73,-0.0772
zipkin,The last of the three deadlocked threads is  trying to get a lock on some singleton or bean .,0.0,0.0,1.0,0.0
zipkin,It has already passed through and presumably acquired a lock on a  GenericScope .,0.0,0.0,1.0,0.0
zipkin,"The other two threads are  trying to acquire a lock on a  GenericScope , which presumably the first thread has.",0.0,0.0,1.0,0.0
zipkin,An unexpected reentrance from the  zipkin  code into spring is generating a deadlock.,0.179,0.0,0.821,-0.34
zipkin,"c3p0  has a fixed-size thread pool that notices when all its threads (just 3 here,  c3p0 's default) are persistently frozen, then (pretty correctly in this case) declares a deadlock and replaces the blocked threads in hopes of recovering.",0.112,0.069,0.819,-0.1779
zipkin,Does c3p0 recover?,0.0,0.0,1.0,0.0
zipkin,Is this a rare or frequent deadlock?,0.324,0.0,0.676,-0.34
zipkin,"There's not much you can easily do to prevent this deadlock, I think either you'll have to tolerate it or do without the instrumentation.",0.105,0.133,0.762,-0.0644
zipkin,Application Insights users would also be able to leverage the distributed tracing offered through Zipkin by instrumenting their services using existing libraries.,0.0,0.0,1.0,0.0
zipkin,"To use the Application Insights back-end store, configure your Zipkin server instance to use the Application Insights  plug-in .",0.0,0.0,1.0,0.0
zipkin,This integration makes monitoring and debugging your overall end-to-end applications much easier.,0.0,0.203,0.797,0.4215
zipkin,"Once you have the data in Application Insights, you can always perform  cross-resource log queries  between Application Insights and Log Analytics.",0.0,0.0,1.0,0.0
zipkin,Additional Documentation Reference -,0.0,0.0,1.0,0.0
zipkin,Zipkin to Application Insights Module,0.0,0.0,1.0,0.0
zipkin,Zipkin-Azure,0.0,0.0,1.0,0.0
zipkin,Send Log Data to Azure Monitor with HTTP Data Collector API (public preview),0.0,0.0,1.0,0.0
zipkin,Hope the above information helps.,0.0,0.647,0.353,0.6705
zipkin,No you can't.,0.524,0.0,0.476,-0.296
zipkin,You can use tools like Elasticsearch Logstash Kibana to visualize it.,0.0,0.2,0.8,0.3612
zipkin,"You can go to my repo  https://github.com/marcingrzejszczak/docker-elk  and run  ./   getReadyForConference.sh , it will start docker containers with the ELK stack, run the apps, curl the request to the apps so that you can then check them in ELK.",0.0,0.0,1.0,0.0
zipkin,"Alright, so after a few hours struggling, I made some progress, and now the app starts - even though the root cause of the issue is not fully clear to me at this time.",0.141,0.13,0.73,-0.1029
zipkin,Below are my findings :,0.0,0.0,1.0,0.0
zipkin,"one strange thing I noticed : if I change the  sender.type  from  web  to  rabbit , then the application starts with no error.",0.295,0.0,0.705,-0.6908
zipkin,"I also found this Spring Boot  issue report , very similar to mine, that was pointing at a JDK bug.",0.0,0.0,1.0,0.0
zipkin,"And indeed, upgrading from  jdk1.8.0_25  to  jdk1.8.0_201  .",0.0,0.0,1.0,0.0
zipkin,"Finally, I also found that if I was using  jdk1.8.0_25  and wasn't providing the  sender.type  at all, then the app was also starting with no issue.",0.087,0.0,0.913,-0.296
zipkin,"For some reason, in the other app that I have and that works, I am able to use  jdk1.8.0_25  and  sender.type: web",0.0,0.0,1.0,0.0
zipkin,"If anyone has a methodology to figure out this kind of issue quickly, don't hesitate to add it in the comment or edit this answer.",0.0,0.073,0.927,0.2057
zipkin,It makes perfect sense that it's  null .,0.0,0.381,0.619,0.5719
zipkin,That's because YOU control the way what happens with the caught exception.,0.0,0.0,1.0,0.0
zipkin,"In your case, nothing, cause you swallow that exception.",0.0,0.0,1.0,0.0
zipkin,"If you want to do sth better, just add the error tag manually via the  SpanCustomizer .",0.136,0.211,0.653,0.128
zipkin,That way you'll add the exception to the given span.,0.0,0.0,1.0,0.0
zipkin,It will then automatically get closed and reported to Zipkin (you can do sth else than  ex.toString()  of course.,0.0,0.0,1.0,0.0
zipkin,Zipkin  is a solution for distributed tracing.,0.0,0.315,0.685,0.3182
zipkin,Specifically it allows to track latency problems in distributed system.,0.231,0.0,0.769,-0.4019
zipkin,Also it's a greate tool for debugging/investigating problems in your application.,0.231,0.0,0.769,-0.4019
zipkin,So by definition it requires to collect successful and failed traces.,0.205,0.236,0.559,0.128
zipkin,However  traces  have nothing to do with logging.,0.0,0.0,1.0,0.0
zipkin,"Assuming you mean controlling the logging level of Zipkin server, then you can just set it using  --logging.level.zipkin2=INFO .",0.0,0.0,1.0,0.0
zipkin,I don't understand the problem.,0.0,0.429,0.571,0.3089
zipkin,You don't send logs to Zipkin.,0.0,0.0,1.0,0.0
zipkin,You send spans to Zipkin.,0.0,0.0,1.0,0.0
zipkin,Zipkin has nothing to do with logs.,0.0,0.0,1.0,0.0
zipkin,Seems to work once I added the Web package.,0.0,0.0,1.0,0.0
zipkin,Though I don't recall it being needed previously.,0.0,0.0,1.0,0.0
zipkin,"Zipkin currently supports four types of backend storage to store spans in-memory, MySQl, ElasticSearch, Cassandra.",0.0,0.152,0.848,0.3612
zipkin,Although for production it is recommended to use ES or Cassandra.,0.0,0.153,0.847,0.2023
zipkin,The other two can be used for learning and understanding.,0.0,0.0,1.0,0.0
zipkin,Traces stored in the in-memory is ephemeral and won't be available after the restart.,0.0,0.0,1.0,0.0
zipkin,"In the zipkin UI there is an option to see the trace and download it, which can be used to view at a later point in time.",0.0,0.0,1.0,0.0
zipkin,If you still have further questions drop in to the zipkin  gitter  channel.,0.149,0.0,0.851,-0.2732
zipkin,we also use use zipkin but can't query with zipkin as elk.,0.0,0.0,1.0,0.0
zipkin,we can just click on each services which are display on zipkin and get more info as below image.,0.0,0.0,1.0,0.0
zipkin,Zipkin is not a business transaction tracking system and it should not be used that way because it is not built for this purpose.,0.0,0.0,1.0,0.0
zipkin,There are other tools which are built specifically to cater the needs to business operations which you must consider.,0.0,0.0,1.0,0.0
zipkin,P.S.,0.0,0.0,1.0,0.0
zipkin,I am a Zipkin contributor.,0.0,0.0,1.0,0.0
zipkin,This is not an answer to how achieve this with zipkin but yes for the whole problem.,0.161,0.161,0.679,0.0
zipkin,If you have a  transaction that didn't complete it's steps then you probably have two of following issues,0.0,0.0,1.0,0.0
zipkin,Some microservice failed to deliver the event to the next one and didn't figure it out,0.18,0.0,0.82,-0.5106
zipkin,"You have to make sure delivery at least once here, using Kafka you have to wait until message get flushed to the server for example",0.0,0.087,0.913,0.3182
zipkin,The destiny microservice received the message and is not processing it,0.0,0.0,1.0,0.0
zipkin,"You have to make sure you application is processing what it's supposed to,  you can monitor the database if the transactions are there or use some tool like LinkedIn burrow to monitor your Kafka message group if you are integrating by using Kafka.",0.0,0.105,0.895,0.5859
zipkin,"Conclusion is, instead to try monitor all the thing once it looks like creating specialist monitors for every step will be more assertive and simple to develop.",0.0,0.158,0.842,0.5719
zipkin,Here is the related issue:,0.0,0.0,1.0,0.0
zipkin,https://github.com/openzipkin/zipkin/issues/1939,0.0,0.0,1.0,0.0
zipkin,"I opened a issue on the zipkin github, a theme already being treated as a bug.",0.0,0.0,1.0,0.0
zipkin,"Initial thread:
 https://github.com/openzipkin/zipkin/issues/2218#issuecomment-432876510",0.0,0.0,1.0,0.0
zipkin,"Bug track:
 https://github.com/openzipkin/zipkin/issues/2219",0.0,0.0,1.0,0.0
zipkin,Tks for all!,0.0,0.0,1.0,0.0
zipkin,You have to use  spring.sleuth.web.skipPattern,0.0,0.0,1.0,0.0
zipkin,sample you will get here  https://www.baeldung.com/tracing-services-with-zipkin,0.0,0.0,1.0,0.0
zipkin,I think to remove the service names from zipkin you have to Re-deploy the zipkin service,0.0,0.0,1.0,0.0
zipkin,You will need to remove the spring.zipkin.base-url property from the corresponding applications to remove it from zipkin server list.,0.0,0.0,1.0,0.0
zipkin,"finally got working after spring verison updated to  5.x 
It already have  Brave Instrument for zipkin trace",0.0,0.175,0.825,0.5267
zipkin,If you read the docs or any information starting from edgware you would see that we've removed that support.,0.0,0.13,0.87,0.4019
zipkin,You should use native zipkin rabbit / kafka dependencies.,0.0,0.0,1.0,0.0
zipkin,Everything is there in the docs.,0.0,0.0,1.0,0.0
zipkin,If it comes from the  @Scheduled  method then you can use  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/SleuthSchedulingProperties.java#L38  ( spring.sleuth.scheduled.skipPattern ) to find the thread and disable it.,0.0,0.0,1.0,0.0
zipkin,If you say its name is  async  then it means that it comes from a  TraceRunnable  or  TraceCallable .,0.0,0.0,1.0,0.0
zipkin,That can be problematic to get rid off.,0.293,0.0,0.707,-0.4404
zipkin,You can file an issue in Sleuth to allow  SpanAdjuster  to actually not send spans to Zipkin (by for example returning  null ).,0.0,0.079,0.921,0.2263
zipkin,You can also try to disable async at all  spring.sleuth.async.enabled .,0.0,0.0,1.0,0.0
zipkin,If you're not using any other features of async that should not interfere.,0.0,0.0,1.0,0.0
zipkin,Brave will work regardless of the server that you choose to use.,0.0,0.236,0.764,0.5267
zipkin,Remove the jetty configuration from the pom file and use the Tomcat.,0.0,0.0,1.0,0.0
zipkin,If you still have trouble or want to know more about zipkin/brave connect to the community via the gitter channel.,0.123,0.059,0.818,-0.34
zipkin,P.S.,0.0,0.0,1.0,0.0
zipkin,I contribute to OpenZipkin (Zipkin),0.0,0.0,1.0,0.0
zipkin,"The UI cannot be password protected without also password protecting the API endpoints, including the one you would send your spans to.",0.103,0.0,0.897,-0.3412
zipkin,PS: The  @EnableZipkinServer  annotation has been deprecated,0.0,0.0,1.0,0.0
zipkin,EDGWARE,0.0,0.0,1.0,0.0
zipkin,Have you read the documentation?,0.0,0.0,1.0,0.0
zipkin,If you use Spring Cloud Sleuth in Edgware version if you read the Sleuth section you would find this piece of the documentation  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_custom_sa_tag_in_zipkin,0.0,0.0,1.0,0.0
zipkin,Let me copy that for you,0.0,0.0,1.0,0.0
zipkin,"54.5 Custom SA tag in Zipkin Sometimes you want to create a manual Span that will wrap a call to an external service which is not
  instrumented.",0.0,0.129,0.871,0.34
zipkin,"What you can do is to create a span with the
  peer.service tag that will contain a value of the service that you
  want to call.",0.0,0.216,0.784,0.5859
zipkin,"Below you can see an example of a call to Redis that is
  wrapped in such a span.",0.0,0.0,1.0,0.0
zipkin,"[Important]   Important Remember not to add both peer.service tag and
  the SA tag!",0.0,0.148,0.852,0.2714
zipkin,You have to add only peer.service.,0.0,0.0,1.0,0.0
zipkin,FINCHLEY,0.0,0.0,1.0,0.0
zipkin,The  SA  tag will not work for Finchley.,0.0,0.0,1.0,0.0
zipkin,You have to do it in the following manner using the  remoteEndpoint  on the span.,0.0,0.0,1.0,0.0
zipkin,That was a bug in Spring Cloud Sleuth in Edgware.,0.0,0.0,1.0,0.0
zipkin,The Stream Kafka Binder in Edgware required explicit passing of headers that should get propagated.,0.0,0.0,1.0,0.0
zipkin,The side effect of adding  sleuth-stream  on the classpath was exactly that feature.,0.0,0.0,1.0,0.0
zipkin,By fixing the  https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005  issue we're adding back the missing feature to core.,0.155,0.0,0.845,-0.296
zipkin,This is not ported to Finchley since Stream Kafka Binder in Finchley passes all headers by default.,0.0,0.0,1.0,0.0
zipkin,The workaround for Edgware is to pass a list of headers in the following manner:,0.0,0.0,1.0,0.0
zipkin,The Istio sidecar proxy (Envoy) generates the first headers.,0.0,0.0,1.0,0.0
zipkin,According to  https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#x-request-id :,0.0,0.0,1.0,0.0
zipkin,Envoy will generate an x-request-id header for all external origin requests (the header is sanitized).,0.0,0.0,1.0,0.0
zipkin,It will also generate an x-request-id header for internal requests that do not already have one.,0.0,0.0,1.0,0.0
zipkin,You've mixed almost everything you could have mixed.,0.0,0.0,1.0,0.0
zipkin,On the app side you're using both the deprecated zipkin server and the deprecated client.,0.0,0.0,1.0,0.0
zipkin,On the server side you're using deprecated zipkin server.,0.0,0.0,1.0,0.0
zipkin,My suggestion is that you go through the documentation  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_spring_cloud_sleuth  and read that the  stream servers  are deprecated and you should use the openzipkin zipkin server with rabbitmq support ( https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq ).,0.0,0.083,0.917,0.4019
zipkin,On the consumer side use  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka  .,0.0,0.0,1.0,0.0
zipkin,It really is as simple as that.,0.0,0.0,1.0,0.0
zipkin,Also don't forget to turn on the sampling percentage to 1.0,0.0,0.143,0.857,0.1695
zipkin,"Just add below, it need to be working,",0.0,0.0,1.0,0.0
zipkin,"Yeah,you should use different libraries for different languages.",0.0,0.0,1.0,0.0
zipkin,"Brave for Java,Zipkin4net for C# and so on.",0.0,0.327,0.673,0.5267
zipkin,"For more details,you can visit Zipkin official site:  Zipkin Existing instrumentations .",0.0,0.0,1.0,0.0
zipkin,Then all you shoud do is following the librarie guide.,0.0,0.0,1.0,0.0
zipkin,Have fun!,0.0,0.782,0.218,0.5562
zipkin,The first request uses v1 of the Zipkin api while the second uses v2 (see  https://github.com/openzipkin/zipkin/issues/1499  for the v2 specification).,0.0,0.0,1.0,0.0
zipkin,"Spans are broken up by kind (SERVER and CLIENT) instead of having client receive, server receive, client send, and server send annotations (hence why there are more spans).",0.095,0.105,0.8,0.0772
zipkin,I have a client application with multiple channels as SOURCE/SINK.,0.0,0.0,1.0,0.0
zipkin,I want to send logs to Zipkin server.,0.0,0.178,0.822,0.0772
zipkin,Zipkin is not a tool to store logs,0.0,0.0,1.0,0.0
zipkin,"According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP.",0.0,0.0,1.0,0.0
zipkin,No - you need the  sleuth-stream  dependency on the client side and the  zipkin-stream  dependency on the server side (which got deprecated and you should start using the inbuilt rabbitmq support from Zipkin).,0.063,0.077,0.86,0.128
zipkin,At client side: Q1.,0.0,0.0,1.0,0.0
zipkin,Is there an automatic configuration for zipkin rabbit binding in such scenario?,0.0,0.0,1.0,0.0
zipkin,"If not, what is default channel name of zipkin SOURCE channel?",0.0,0.0,1.0,0.0
zipkin,"Yes, there is.",0.0,0.574,0.426,0.4019
zipkin,The channel is  sleuth,0.0,0.0,1.0,0.0
zipkin,Q2.,0.0,0.0,1.0,0.0
zipkin,Do I need to configure defaultSampler to AlwaysSampler()?,0.0,0.0,1.0,0.0
zipkin,"No, you have the  PercentageBasedSampler  (I'm pretty sure it's written in the docs).",0.124,0.311,0.565,0.5106
zipkin,You can tweak its values.,0.0,0.403,0.597,0.4019
zipkin,At Server side: Q1.,0.0,0.0,1.0,0.0
zipkin,Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using: wget -O zipkin.jar ' https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec ' ...as stated on  https://zipkin.io/pages/quickstart.html  ?,0.0,0.081,0.919,0.3527
zipkin,You should do the wget.,0.0,0.0,1.0,0.0
zipkin,If you want to use the legacy stream support then you should create a zipkin server yourself.,0.0,0.319,0.681,0.6249
zipkin,Q2.,0.0,0.0,1.0,0.0
zipkin,How do I configure zipkin SINK channel to destination?,0.0,0.0,1.0,0.0
zipkin,If you're using the legacy zipkin stream app then it's automatically configured to point to proper destination.,0.0,0.0,1.0,0.0
zipkin,You can tweak the destination as you please in the standard way Spring Cloud Stream supports it.,0.0,0.242,0.758,0.5859
zipkin,I'm not sure I fully understand your question but I can elaborate a bit on how istio works with respect to tracing:,0.065,0.183,0.751,0.5674
zipkin,"Tracing means identifying every span or node that is part of the original request, so typically an Id is generated by the istio-ingress and your application should  propagate it  so each istio-proxy can capture and forward that information to istio-mixer which then lets you use Zipkin or Jaeger to visualize it.",0.0,0.044,0.956,0.3182
zipkin,Istio can't know when you make outcalls from your application for which original request it was for unless you do copy the headers.,0.0,0.095,0.905,0.3182
zipkin,Does that help/makes sense ?,0.0,0.0,1.0,0.0
zipkin,"It looks a incompatibility between version in my opinion, something is overridden when you inject the spring-cloud-starter-zipkin dependency",0.0,0.0,1.0,0.0
zipkin,What i don't understand from your question is:,0.0,0.0,1.0,0.0
zipkin,"Do you need this dependency ""spring-cloud-starter-zipkin"", are you using it?",0.0,0.0,1.0,0.0
zipkin,"If no obviously just put it out of the pom, if yes, check which version are you using:",0.105,0.129,0.766,0.128
zipkin,mvn dependency:tree and try to align spring-cloud-starter-zipkin with the Spring version you are using.,0.0,0.0,1.0,0.0
zipkin,Playing a bit with the version of your artifacts you will find the solution.,0.0,0.272,0.728,0.4767
zipkin,Hope it helped.,0.0,0.592,0.408,0.4404
zipkin,I use &quot;TraceCallable&quot; class from &quot; spring-cloud-sleuth &quot; lib to solve it in my code.,0.0,0.122,0.878,0.2023
zipkin,My code example is:,0.0,0.0,1.0,0.0
zipkin,"Attention: This solution does works for logging purposes, but does not work for other Sleuth features like instrumenting RestTemplates to send the tracing headers to other services.",0.0,0.164,0.836,0.5994
zipkin,"So unfortunately, this is not a fully working solution.",0.448,0.0,0.552,-0.5942
zipkin,:(,1.0,0.0,0.0,-0.4404
zipkin,"Some time after adopting @Baca's solution, I discovered that Kotlin Coroutines offer direct integration with slf4j, which is what Spring Sleuth builds on.",0.0,0.099,0.901,0.3182
zipkin,"Sleuth adds properties  X-B3-TraceId ,  traceId ,  X-B3-SpanId , and  spanId  to the thread's MDC.",0.0,0.0,1.0,0.0
zipkin,You can retain the parent thread's MDC for a coroutine with the code shown below.,0.0,0.0,1.0,0.0
zipkin,The coroutine framework will take care of restoring the MDC context on the worker thread whenever the coroutine is executed/resumed.,0.0,0.231,0.769,0.6597
zipkin,This is the easiest solution I could discover so far.,0.0,0.421,0.579,0.6249
zipkin,:),0.0,1.0,0.0,0.4588
zipkin,The launch method takes an optional CoroutineContext and the coroutine-slf4j integration implements the MDCContext.,0.0,0.0,1.0,0.0
zipkin,This class captures the calling thread's MDC context (creates a copy) and uses that for the coroutine execution.,0.0,0.0,1.0,0.0
zipkin,Add this dependency to your build.gradle:,0.0,0.0,1.0,0.0
zipkin,"Project:  https://github.com/Kotlin/kotlinx.coroutines/tree/master/integration/kotlinx-coroutines-slf4j 
Documentation:  https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-slf4j/index.html",0.0,0.0,1.0,0.0
zipkin,With Spring boot  Dalston.SR3  (which uses open zipkin 1.28) you can achieve this by setting property  zipkin.storage.mem.max-spans=xxx  This will limit the number of spans and discard old ones.,0.068,0.044,0.887,-0.1779
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,The best way to trace OpenStack project is to use Osprofiler library.,0.0,0.276,0.724,0.6369
zipkin,If you just want to understand the workflow or just know about the types of calls being made inside OpenStack then Osprofiler is the best and easiest way to get the trace.,0.0,0.223,0.777,0.8074
zipkin,Now Osprofiler is an accepted OpenStack project and the official project to get traces for OpenStack.,0.0,0.123,0.877,0.2732
zipkin,"Instead of having to go through the whole code and adding instrumentation points near HTTP request or RPC calls, osprofiler is already integrated in all of the main projects of OpenStack (Nova, Neutron, Keystone, Glance etc..).",0.0,0.0,1.0,0.0
zipkin,You just have to enable osprofiler in the configuration files of each project in OpenStack to get a trace of that particular project.,0.0,0.0,1.0,0.0
zipkin,You can go through this link -  https://docs.openstack.org/osprofiler/latest/,0.0,0.0,1.0,0.0
zipkin,Enabling of Osprofiler in the configuration files can be done by adding these lines at the end of the configuration file (nova.conf or neutron.conf) :,0.0,0.0,1.0,0.0
zipkin,The connection_string parameter indicates the collector (where the trace information is stored).,0.0,0.0,1.0,0.0
zipkin,By default it uses Ceilometer.,0.0,0.0,1.0,0.0
zipkin,You can actually redirect the trace information to other collectors like Elasticsearch by changing the connection_string parameter in the conf file to the elasticsearch server.,0.0,0.094,0.906,0.3612
zipkin,This is by far the easiest way to get a trace in OpenStack with just minimal effort.,0.0,0.157,0.843,0.4215
zipkin,After some discussion with Marcin Grzejszczak and Adrien Cole (zipkin and sleuth creators/active developers) I ended up creating a Jersey filter that acts as bridge between sleuth and brave.,0.0,0.183,0.817,0.6808
zipkin,"Regarding AMQP integration, added a new @StreamListener with a conditional for zipkin format spans (using headers).",0.0,0.0,1.0,0.0
zipkin,Sending messages to the sleuth exchange with zipkin format will then be valid and consumed by the listener.,0.0,0.0,1.0,0.0
zipkin,"For javascript (zipkin-js), I ended up creating a new AMQP Logger that sends zipkin spans to a determined exchange.",0.0,0.247,0.753,0.5574
zipkin,"If someone ends up reading this and needs more detail, you're welcome to reach out to me.",0.0,0.225,0.775,0.5209
zipkin,Take a look at Sampling interval in the docs :,0.0,0.0,1.0,0.0
zipkin,In distributed tracing the data volumes can be very high so sampling can be important (you usually don’t need to export all spans to get a good picture of what is happening).,0.0,0.139,0.861,0.5719
zipkin,Spring Cloud Sleuth has a Sampler strategy that you can implement to take control of the sampling algorithm.,0.0,0.0,1.0,0.0
zipkin,"Samplers do not stop span (correlation) ids from being generated, but they do prevent the tags and events being attached and exported.",0.0,0.115,0.885,0.1516
zipkin,"By default you get a strategy that continues to trace if a span is already active, but new ones are always marked as non-exportable.",0.0,0.081,0.919,0.2144
zipkin,"If all your apps run with this sampler you will see traces in logs, but not in any remote store.",0.0,0.0,1.0,0.0
zipkin,"For testing the default is often enough, and it probably is all you need if you are only using the logs (e.g.",0.0,0.0,1.0,0.0
zipkin,with an ELK aggregator).,0.0,0.0,1.0,0.0
zipkin,"If you are exporting span data to Zipkin or Spring Cloud Stream, there is also an AlwaysSampler that exports everything and a PercentageBasedSampler that samples a fixed fraction of spans.",0.0,0.0,1.0,0.0
zipkin,http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling,0.0,0.0,1.0,0.0
zipkin,You are blending Zipkin autoconfigure version 1.2 with Zipkin 1.26.,0.0,0.0,1.0,0.0
zipkin,This results in a version missmatch.,0.0,0.0,1.0,0.0
zipkin,The problem is casued by two reasons.,0.31,0.0,0.69,-0.4019
zipkin,First:just as Rafael Winterhalter said the version dismatched; second: you are lacking dependency for zipkin.,0.0,0.0,1.0,0.0
zipkin,"Finally i fixed the problem by adding the whole 'io.zipkin.java:zipkin"" library.",0.231,0.0,0.769,-0.4019
zipkin,Here is my final pom file with the storage type of elasticsearch:,0.0,0.0,1.0,0.0
zipkin,Appears that a sleuth span is not the same as a Zipkin span.,0.0,0.0,1.0,0.0
zipkin,"Hence, in the above code there is no way to instantiate a default tracer with zipkin span reporter.",0.121,0.0,0.879,-0.296
zipkin,I converted the sleuth span into a zipkin span and then reported it to zipkin.,0.0,0.0,1.0,0.0
zipkin,The class to convert it is available in spring-cloud-sleuth-stream.,0.0,0.0,1.0,0.0
zipkin,I used pretty much the same class with some tweaks.,0.0,0.286,0.714,0.4939
zipkin,Here you have a very basic example of Sleuth &amp; HTTP communication.,0.0,0.0,1.0,0.0
zipkin,https://github.com/openzipkin/sleuth-webmvc-example  You can set your dependencies in a similar manner and everything should work fine.,0.0,0.122,0.878,0.2023
zipkin,In your example you've got Stream but I don't think you're using it so it's better to remove it.,0.0,0.201,0.799,0.6448
zipkin,As M.Deinum said remove  stream  and  stream-rabbit  dependencies what if you do not need some AMQP server to store the trace message.,0.0,0.0,1.0,0.0
zipkin,or,0.0,0.0,1.0,0.0
zipkin,"config the AMQP(rabbitMQ in your code) from application-configuration(both) and add  zipkin-stream  &amp;  stream-rabbit  in  zipkin-server  side, so this time your app( zipkin-client ) will not direct connect with  zipkin-server  
and it will be:",0.0,0.0,1.0,0.0
zipkin,You may define all needed params via ENV options.,0.0,0.0,1.0,0.0
zipkin,Here is a cmd for running zipkin in docker:,0.0,0.0,1.0,0.0
zipkin,All these params can be defined in Deployment (see  Expose Pod Information to Containers Through Environment Variables ),0.091,0.0,0.909,-0.1531
zipkin,1.You should check if your Zipkin Server is on.,0.0,0.0,1.0,0.0
zipkin,2.You should check if the Span transfering is async.,0.0,0.0,1.0,0.0
zipkin,"In HTTP,Zipkin uses in-band transfer,all the information carried in HTTP headers.The cost time of generating Span is about 200 nanosecond.",0.0,0.0,1.0,0.0
zipkin,"The TL;DR; is that  B3 propagation  was initially designed for fixed size data: carrying data ancillary to tracing isn't in scope, and for this reason any solution that extends B3 in such a fashion wouldn't be compatible with existing code.",0.0,0.093,0.907,0.4215
zipkin,"So, that means any solution like this will be an extension which means custom handling in the  instrumented apps  which are the things passing headers around.",0.0,0.167,0.833,0.5859
zipkin,The server won't care as it never sees these headers anyway.,0.208,0.0,0.792,-0.3875
zipkin,Ways people usually integrate other things like flags with zipkin is to add a tag aka binary annotation including its value (usually in the root span).,0.0,0.176,0.824,0.5994
zipkin,"This would allow you to query or retrieve these offline, but it doesn't address in-flight lookups from applications.",0.067,0.078,0.856,0.0516
zipkin,"Let's say that instead of using an intermediary like linkerd, or a platform-specific propagated context, we want to dispatch the responsibility to the tracing layer.",0.0,0.147,0.853,0.4215
zipkin,"Firstly, what sort of data could work alright?",0.0,0.222,0.778,0.25
zipkin,The easiest is something set-once (like zipkin's trace id).,0.0,0.259,0.741,0.4215
zipkin,Anything set and propagated without mutating it is the least mechanics.,0.0,0.0,1.0,0.0
zipkin,"Next in difficulty is appending new entries mid-stream, and most difficult is mutating/merging entries.",0.302,0.0,0.698,-0.6361
zipkin,Let's assume this is for inbound flags which never change through the request/trace tree.,0.0,0.0,1.0,0.0
zipkin,"We see a header when processing trace data, we store it and forward it downstream.",0.0,0.0,1.0,0.0
zipkin,"If this value doesn't need to be read by the tracing system, it is easiest, as it is largely a transport/propagation concern.",0.0,0.215,0.785,0.6369
zipkin,"For example, maybe other middleware read that header and it is only a ""side job"" we are adding to the tracer to remember certain things to pass along.",0.0,0.075,0.925,0.2732
zipkin,"If this was done in a single header, it would be less code than a pattern in each of the places this would be to added.",0.0,0.0,1.0,0.0
zipkin,"It would be even less code if the flags could be encoded in a number, however unrealistic that may be.",0.0,0.067,0.933,0.0772
zipkin,"There are libraries with apis to manipulate the propagated context manually, for example,  ""baggage"" from brownsys  and OpenTracing (of which some libraries support zipkin).",0.0,0.105,0.895,0.4019
zipkin,"The former aims to be a generic layer for any instrumentation (ex monitoring, chargeback, tracing etc) and the latter is specific to tracing.",0.0,0.0,1.0,0.0
zipkin,OpenTracing has defines abstract types like  injector and extractor  which could be customized to carry other fields.,0.0,0.135,0.865,0.3612
zipkin,"However, you still would need a concrete implementation (which knows your header format etc) in order to do this.",0.0,0.0,1.0,0.0
zipkin,"Unless you want applications to read this data, it would need to be a secret detail of that implementation (specifically the trace context).",0.0,0.058,0.942,0.0772
zipkin,"Certain zipkin-specific libraries like Spring Cloud Sleuth and Brave have means to  customize how headers are parsed , to support variants of B3 or new or site-specific trace formats.",0.0,0.308,0.692,0.8658
zipkin,"Not all support this at the moment, but I would expect this type of feature to become more common.",0.087,0.0,0.913,-0.1603
zipkin,This means you may need to do some surgery in order to support all platforms you may need to support.,0.0,0.231,0.769,0.6597
zipkin,"So long story short is that there are some libraries which are pluggable with regards to propagation, and those will be easiest to modify to support this use case.",0.0,0.169,0.831,0.6705
zipkin,Some code will be needed regardless as B3 doesn't currently define an expression like this.,0.0,0.152,0.848,0.3612
zipkin,In general it is better to use the zipkin's http variant of Elasticsearch as it cannot conflict with Spring Boot's elasticsearch library versions.,0.0,0.188,0.812,0.5943
zipkin,I would set everything in zipkin's group id to latest (currently 1.21.0 which is Spring Boot 1.4.x) and use zipkin-autoconfigure-storage-elasticsearch-http (plus.. the one you are using  will be dropped ),0.0,0.0,1.0,0.0
zipkin,Make sure your es hosts is specified in url syntax ex.,0.0,0.187,0.813,0.3182
zipkin,http://host1:9200,0.0,0.0,1.0,0.0
zipkin,Zipkin generates traces and communicates them back to a Zipkin server.,0.0,0.0,1.0,0.0
zipkin,The latter action is typically performed via HTTP but Zipkin is agnostic to how a span is started and ended.,0.0,0.0,1.0,0.0
zipkin,"If you want to measure the execution time of a single method, you would normally create a local span that is nested inside a server span.",0.0,0.139,0.861,0.34
zipkin,Zipkin is a distributrd tracing tool for discovering machine-to-machine interaction which is why spans often cover HTTP.,0.0,0.0,1.0,0.0
zipkin,"If you want to measure execution time of a method, a tool like metrics might be more suited.",0.0,0.213,0.787,0.4215
zipkin,This was an issue with MySQL 5.7 and more recently resolved.,0.0,0.165,0.835,0.2449
zipkin,You can try latest Zipkin.,0.0,0.0,1.0,0.0
zipkin,You'd have to implement your own ZipkinSpanReporter that would look more or less like  https://github.com/spring-cloud/spring-cloud-sleuth/blob/v1.0.8.RELEASE/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java  .,0.0,0.15,0.85,0.355
zipkin,In the next version of Sleuth you will be able to register a bean of ZipkinSpanReporter that can you a custom version of a publisher -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/1.0.x/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java,0.0,0.0,1.0,0.0
zipkin,Instrumenting a library is something that sometimes folks have to do for one reason or another.,0.0,0.0,1.0,0.0
zipkin,"There are several tracer libraries in Java but the salient points about creating a tracer are either on the website, or issues on the website.",0.0,0.199,0.801,0.6652
zipkin,"http://zipkin.io/pages/instrumenting.html 
 https://github.com/openzipkin/openzipkin.github.io/issues/11",0.0,0.0,1.0,0.0
zipkin,OpenTracing also has some nice fundamentals to look at  http://opentracing.io/,0.0,0.237,0.763,0.4215
zipkin,"This isn't a one-answer type of question, in my experience, as you'll learn you'll need to address other things that tracer libraries address out-of-box.",0.0,0.0,1.0,0.0
zipkin,"For that reason I'd highly suggest joining gitter so that you
can have a dialogue through your journey  https://gitter.im/openzipkin/zipkin",0.0,0.0,1.0,0.0
zipkin,I was recording wrong annotation i.e client instead of server.,0.279,0.0,0.721,-0.4767
zipkin,Just a simple change did the trick.,0.194,0.0,0.806,-0.0516
zipkin,"Trace.traceService(""Function1"",""Test"")",0.0,0.0,1.0,0.0
zipkin,Sample working Zipkin example:  https://gist.github.com/AkhilJ876/3e38757c28d43924f296dd2d147c0bd9#file-zipkintracing_example-L34,0.0,0.0,1.0,0.0
zipkin,"Same problem here with the docker images using Cassandra (1.40.1, 1.40.2, 1.1.4).",0.197,0.0,0.803,-0.4019
zipkin,This is a problem specific to using Cassandra as the storage tier.,0.213,0.0,0.787,-0.4019
zipkin,Mysql and the in-memory storage generate the dependency graph on-demand as expected.,0.0,0.0,1.0,0.0
zipkin,There are references to the following project to generate the Cassandra graph data for the UI to display.,0.0,0.0,1.0,0.0
zipkin,This looks to be superseded by ongoing work mentioned here,0.0,0.0,1.0,0.0
zipkin,"If storage type is other than inline storage, for zipkin dependencies graph you have to start separate cron job/scheduler which reads the storage database and builds the graph.",0.0,0.0,1.0,0.0
zipkin,Because zipkin dependencies is separate spark job .,0.0,0.241,0.759,0.2263
zipkin,For reference :  https://github.com/openzipkin/docker-zipkin-dependencies,0.0,0.0,1.0,0.0
zipkin,I have used zipkin with elastic search as storage type.,0.0,0.0,1.0,0.0
zipkin,I will share the steps for setting up the zipkin dependencies with elastic search and cron job for the same:,0.0,0.109,0.891,0.296
zipkin,"Other solution is to start a separate service and run the cron job
  using docker",0.0,0.15,0.85,0.3182
zipkin,"Steps to get the latest zipkin-dependencies jar try running given
  command on teminal",0.0,0.0,1.0,0.0
zipkin,you will get jar file at above mention directory,0.0,0.0,1.0,0.0
zipkin,Dockerfile,0.0,0.0,1.0,0.0
zipkin,entry.sh,0.0,0.0,1.0,0.0
zipkin,script.sh,0.0,0.0,1.0,0.0
zipkin,crontab.txt,0.0,0.0,1.0,0.0
zipkin,This is due to not having an instance of the query server running.,0.0,0.0,1.0,0.0
zipkin,I'm in the middle of a re-write that'll simplify all of this.,0.0,0.0,1.0,0.0
zipkin,"Until then, you need to spin up a query server.",0.0,0.0,1.0,0.0
zipkin,I would say you can have fluentd in multiple namespaces and Elasticsearch in one namespace and fluentd can discover Elasticsearch via K8s internal DNS A/AAAA record e.g.,0.0,0.0,1.0,0.0
zipkin,elasticsearch.${namespace}.svc.cluster.local .,0.0,0.0,1.0,0.0
zipkin,"I don't have any link to the best practice, but I would show you a practice I saw from the community.",0.0,0.14,0.86,0.3818
zipkin,"If you are not familiar with configuring K8s cluster, I recommend to deploy ELK by Helm.",0.0,0.152,0.848,0.3612
zipkin,It will save you a lot of time and give you enough configuration options.,0.0,0.211,0.789,0.4939
zipkin,https://github.com/helm/charts/tree/master/stable/elastic-stack .,0.0,0.0,1.0,0.0
zipkin,"Install your ELK helm release on a  separate  namespace, for example:  logging .",0.0,0.0,1.0,0.0
zipkin,Install fluentd in any namespaces in your cluster and configure elasticsearch host  https://github.com/helm/charts/tree/master/stable/fluentd-elasticsearch,0.0,0.0,1.0,0.0
zipkin,"I used an Aspect and from the returned ResponseEntity object, decided whether or not to programatically add an error tag to span.",0.119,0.0,0.881,-0.4019
zipkin,"With this tag, zipkin will identify and highlight the trace in red colour.",0.0,0.167,0.833,0.34
zipkin,Below is the code snippet to add error tag to span.,0.213,0.0,0.787,-0.4019
zipkin,i think i found a suitable way to do this.,0.0,0.0,1.0,0.0
zipkin,After further thinking my idea went into the direction of using annotations and aspects for intercepting HTTP requests from/to thrifts http client which seems to be quite some work.,0.0,0.0,1.0,0.0
zipkin,"after further search, i found this library for spring-boot serving exactly my needs:
 https://github.com/aatarasoff/spring-thrift-starter",0.0,0.0,1.0,0.0
zipkin,I dont kown can you see my pic and I put my code:,0.0,0.0,1.0,0.0
zipkin,pom.xml:,0.0,0.0,1.0,0.0
zipkin,ZipkinApplication.java:,0.0,0.0,1.0,0.0
zipkin,The error:,0.73,0.0,0.27,-0.4019
zipkin,I can say your YAML has some bad indentation and things are not in the right sections even.,0.179,0.0,0.821,-0.5423
zipkin,"Otherwise though, you are trying to run Zipkin in an unsupported configuration.",0.197,0.0,0.803,-0.4019
zipkin,Please check out our quickstart documentation:  https://zipkin.io/pages/quickstart.html,0.0,0.277,0.723,0.3182
zipkin,There are 2 approaches to this,0.0,0.0,1.0,0.0
zipkin,Looking at your yml file you have added,0.0,0.0,1.0,0.0
zipkin,which means your approach is 2.,0.0,0.0,1.0,0.0
zipkin,"But then in your pom, you have added  zipkin-server  and  zipkin-autoconfigure-ui  dependencies which is not required.",0.0,0.0,1.0,0.0
zipkin,I will try to separate both setups,0.0,0.0,1.0,0.0
zipkin,1.,0.0,0.0,1.0,0.0
zipkin,To Start Zipkin server with SpringBootApplication,0.0,0.0,1.0,0.0
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,application.properties,0.0,0.0,1.0,0.0
zipkin,Application.java,0.0,0.0,1.0,0.0
zipkin,2.,0.0,0.0,1.0,0.0
zipkin,To Start Zipkin server as a standalone and use SpringBootApplication as Zipkin Client,0.0,0.0,1.0,0.0
zipkin,Start Zipkin server,0.0,0.0,1.0,0.0
zipkin,pom.xml,0.0,0.0,1.0,0.0
zipkin,application.properties,0.0,0.0,1.0,0.0
zipkin,Edit 1:,0.0,0.0,1.0,0.0
zipkin,@EnableZipkinServer  is deprecated and unsupported as per Brian Devins's comment.,0.231,0.0,0.769,-0.4019
zipkin,"So, please go through the  doc  for more detail info.",0.0,0.224,0.776,0.3804
zipkin,You're using an ancient version of Spring CLoud.,0.0,0.0,1.0,0.0
zipkin,Please upgrade to latest Edgware.,0.0,0.365,0.635,0.3182
zipkin,The RxJava support is very basic so we suggest that you use Project Reactor.,0.0,0.172,0.828,0.4019
zipkin,To do that just migrate to Finchley and it should work out of the box with WebFlux.,0.0,0.0,1.0,0.0
zipkin,"You're using an ancient version of Sleuth, can you please upgrade?",0.0,0.187,0.813,0.3182
zipkin,Why do you provide Zipkin's version manually?,0.0,0.0,1.0,0.0
zipkin,Also as far as I see you're using the Sleuth's Zipkin server (that is deprecated in Edgware and removed in Finchley).,0.0,0.0,1.0,0.0
zipkin,My suggestion is that you stop using the Sleuth's Stream server (you can read more about this here  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka ).,0.104,0.0,0.896,-0.296
zipkin,1) In order not to pick versions by yourself it’s much better if you add the dependency management via the Spring BOM,0.0,0.121,0.879,0.4404
zipkin,2) Add the dependency to spring-cloud-starter-zipkin - that way all dependent dependencies will be downloaded,0.0,0.0,1.0,0.0
zipkin,"3) To automatically configure rabbit, simply add the spring-rabbit dependency",0.0,0.0,1.0,0.0
zipkin,"the simplest solution i have found for solving broken ui for zipkin thru gateway
is by changing following property of zipkin-server-shared.yml file inside zipkin server",0.108,0.163,0.729,0.1531
zipkin,"zipkin:
  ui:
   base-path: /zipkin
 
change above property to",0.0,0.0,1.0,0.0
zipkin,"zipkin:
  ui:
   base-path: /api/tracing/zipkin",0.0,0.0,1.0,0.0
zipkin,"and change ur zuul path to following
 zuul.routes.zipkin.path=/api/tracing/*",0.0,0.0,1.0,0.0
zipkin,and than access zipkin using follwing url,0.0,0.0,1.0,0.0
zipkin,https://gatewayhost:port/api/tracing/zipkin/,0.0,0.0,1.0,0.0
zipkin,"give attention to small details in config and dont forget to put trailing ""/"" after zipkin  in url",0.0,0.089,0.911,0.1695
zipkin,It has nothing to do with Spring Cloud Sleuth or Zipkin.,0.0,0.0,1.0,0.0
zipkin,@SpringBootApplication automatically does @ComponentScan so all @RestController classes will get registered as beans if they are in the same package as your @SpringBootApplication annotated class or if it's in the child packages.,0.0,0.0,1.0,0.0
zipkin,Please read and try to understand how Spring Boot works by reading this chapter of the docs -  https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-using-springbootapplication-annotation.html,0.0,0.119,0.881,0.3182
zipkin,Object.keys(headers).filter((key) =&gt; TRACING_HEADERS.includes(key)).map((key) =&gt; headers[key])  returns an  array .,0.0,0.0,1.0,0.0
zipkin,What you want is:,0.0,0.302,0.698,0.0772
zipkin,I'm pretty sure this isn't an istio / distributed tracing issue ;-),0.0,0.484,0.516,0.7579
zipkin,b3-propagation of x-b3-parentspanid ( https://github.com/openzipkin/b3-propagation ) can be configured in your application.yml by adding:,0.0,0.0,1.0,0.0
zipkin,Details of error (Java stack trace) would be really useful here.,0.181,0.214,0.604,0.1263
zipkin,"By error message I assume, you are using  qpid JMS client , that is performing check of message properties' names.",0.137,0.0,0.863,-0.4019
zipkin,"These names can contain only characters, that are valid  Java identifier characters .",0.0,0.0,1.0,0.0
zipkin,"In string 'queue-name' there is a '-' character, that is not Java identifier.",0.0,0.0,1.0,0.0
zipkin,"To fix, you need to change 'queue-name' into something with valid characters, for example 'queue_name' (with underscore), or 'queueName' (camel case).",0.0,0.0,1.0,0.0
zipkin,Section 3.5.1 of the JMS 2 specification states this about message properties:,0.0,0.0,1.0,0.0
zipkin,Property names must obey the rules for a message selector identifier.,0.0,0.0,1.0,0.0
zipkin,"See
  Section 3.8 “Message selection” for more information.",0.0,0.0,1.0,0.0
zipkin,"In regards to identifiers, section 3.8.1.1 states, in part:",0.0,0.0,1.0,0.0
zipkin,An identifier is an unlimited-length character sequence that must begin with a Java identifier start character; all following characters must be Java identifier part characters.,0.0,0.0,1.0,0.0
zipkin,An identifier start character is any character for which the method  Character.isJavaIdentifierStart  returns  true .,0.0,0.177,0.823,0.4215
zipkin,This includes '_' and '$'.,0.0,0.0,1.0,0.0
zipkin,An identifier part character is any character for which the method   Character.isJavaIdentifierPart  returns  true .,0.0,0.177,0.823,0.4215
zipkin,If you pass the character  -  into either  Character.isJavaIdentifierStart  or  Character.isJavaIdentifierPart  the return value is  false .,0.0,0.146,0.854,0.34
zipkin,"In other words,  the  -  character in the name of a message property violates the JMS specification  and therefore will cause an error.",0.24,0.0,0.76,-0.7184
zipkin,From the error message its obvious that you are using qpid JMS client for communication through queues.,0.144,0.0,0.856,-0.4019
zipkin,qpid client won’t allow any keys which violates java variable naming convention e.g.,0.204,0.117,0.679,-0.34
zipkin,"you won’t be able to send x-request-id in a queue’s header
which qpid jms client is consuming as it’ll throw error.",0.124,0.0,0.876,-0.4019
zipkin,You need to take care of istio/zipkin to not to add certain headers (id you don’t need them actually) with the queue when its trying to communicate on azure bus.,0.055,0.097,0.848,0.3369
zipkin,So you have to disable the istio/zipkin libraries  to intercept the request for queues so that request to/from queue can be made without headers.,0.0,0.0,1.0,0.0
zipkin,This will fix the issue.,0.0,0.0,1.0,0.0
zipkin,"It the application.properties file for each eureka client ,  I added/changed",0.0,0.0,1.0,0.0
zipkin,------------------ client,0.0,0.0,1.0,0.0
zipkin,-------------------- eureka server application.property--------------------,0.0,0.0,1.0,0.0
zipkin,I was facing a similar issue where the eureka server was registering the services at host.docker.internal instead of localhost.,0.0,0.0,1.0,0.0
zipkin,The issue in my case was an altered host file at location C:\Windows\System32\Drivers\etc\hosts.,0.0,0.0,1.0,0.0
zipkin,I deleted all the lines in the host file and saved it using npp with admin privilege.,0.0,0.275,0.725,0.6486
zipkin,Restart the server post this change.,0.0,0.0,1.0,0.0
zipkin,Looks like 'Docker Desktop' was changing the hostfile.,0.0,0.263,0.737,0.3612
zipkin,"""message"": ""Connection refused: no further information: host.docker.internal in eureka gateway error",0.47,0.0,0.53,-0.7269
zipkin,Resolution:,0.0,0.0,1.0,0.0
zipkin,"check ping host.docker.internal
response is some ip addresses apart form local host i,e 127.0.0.1
remove the C:\Windows\System32\Drivers\etc\hosts.file entries , make it empty",0.083,0.0,0.917,-0.2023
zipkin,then restart eureka and your microservice instance.,0.0,0.0,1.0,0.0
zipkin,also will find the message like below in the log this ensures you are registered in eureka,0.0,0.135,0.865,0.3612
zipkin,"DiscoveryClient_BEER-SERVICE/DESKTOP-G2AIGG1:beer-service:
splitting the above log message which denotes discovery client 
BEER-SERVICE is my service and 
DESKTOP-G2AIGG1 is my pc name
beer-service is the service registered.",0.0,0.0,1.0,0.0
zipkin,"I was also facing the same issue, when I was loadbalancing my restTemplate.",0.0,0.0,1.0,0.0
zipkin,Something like this,0.0,0.556,0.444,0.3612
zipkin,This is because of the ribbon client.,0.0,0.0,1.0,0.0
zipkin,"So, without making any changes in the host file, when i deleted this code and made use of  RestTemplateBuilder  to get restTemplate, everything was working fine.",0.0,0.07,0.93,0.2023
zipkin,Code Example:,0.0,0.0,1.0,0.0
zipkin,You can try this approach as well.,0.0,0.259,0.741,0.2732
zipkin,Thanks for the tip on the host file on windows,0.0,0.244,0.756,0.4404
zipkin,I found that docker adds aliases  in the host file for host.docker.internal and gateway.docker.internal.,0.0,0.0,1.0,0.0
zipkin,I am guessing that Eureka does a host lookup from the IP and host.docker.internal is returned.,0.0,0.0,1.0,0.0
zipkin,"I am not an expert at hosts files, but I added an alias for my actual host name to my IP (note: we use static ip's).",0.0,0.0,1.0,0.0
zipkin,"After doing this, docker did not change my host file on reboot and the reverse lookup of the ip-&gt;host now returns my machine name instead of host.docker.internal",0.0,0.0,1.0,0.0
zipkin,"Solution for Window 10:
You don't have to remove all the lines from hosts files.",0.0,0.141,0.859,0.3182
zipkin,"Just comment this if exists (#192.168.1.4 host.docker.internal) (as we use this when playing with docker)
And paste this (127.0.0.1   host.docker.internal)
It worked for me.",0.0,0.073,0.927,0.2023
zipkin,You can use the new Dalston feature of using annotations on Spring Data repositories.,0.0,0.0,1.0,0.0
zipkin,You can check out this for more info  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_managing_spans_with_annotations,0.0,0.0,1.0,0.0
zipkin,This is really strange because you are using latest relase and in the GitHub spring-cloud-sleuth depends to  &lt;brave.version&gt;4.17.2&lt;/brave.version&gt; .,0.11,0.0,0.89,-0.2716
zipkin,And I think 4.16.3-SNAPSHOT version is not exists in the maven repo.,0.0,0.0,1.0,0.0
zipkin,(just checked 2.0.0.M8 depends to this version),0.0,0.0,1.0,0.0
zipkin,If you change to  &lt;sleuth.version&gt;2.0.0.M7&lt;/sleuth.version&gt;  it does find the required dependencies.,0.0,0.0,1.0,0.0
zipkin,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/pom.xml,0.0,0.0,1.0,0.0
zipkin,The M8 for sleuth was broken.,0.303,0.306,0.391,0.0085
zipkin,That issue will be fixed in M9.,0.0,0.0,1.0,0.0
zipkin,You can use M8 but you have to explicitly change the brave version to some release one.,0.0,0.308,0.692,0.7695
zipkin,"For 1, 2, 3 it was because I was doing a new RestTemplate.",0.0,0.0,1.0,0.0
zipkin,The doc says :,0.0,0.0,1.0,0.0
zipkin,You have to register RestTemplate as a bean so that the interceptors will get injected.,0.0,0.0,1.0,0.0
zipkin,If you create a RestTemplate instance with a new keyword then the instrumentation WILL NOT work.,0.0,0.139,0.861,0.2732
zipkin,"So RTFM for myself, and this solved my 3 first problems :",0.207,0.182,0.612,-0.0836
zipkin,The first step to good searching in elasticsearch is to create fields from your data.,0.0,0.278,0.722,0.6124
zipkin,"With logs, logstash is the proper tool.",0.0,0.0,1.0,0.0
zipkin,The grok{} filter uses patterns (existing or user-defined regexps) to split the input into fields.,0.0,0.0,1.0,0.0
zipkin,You would need to make sure that it was mapped to an integer (e.g.,0.0,0.15,0.85,0.3182
zipkin,%{INT:duration:int} in your pattern).,0.0,0.0,1.0,0.0
zipkin,"You could then query elasticsearch for ""duration: 1000"" to get the results.",0.0,0.0,1.0,0.0
zipkin,"Elasticsearch uses the lucene query engine, so you can find sample queries based on that.",0.0,0.0,1.0,0.0
zipkin,Zipkin is the best solution.,0.0,0.684,0.316,0.7579
zipkin,--zipkin developer,0.0,0.0,1.0,0.0
zipkin,"EDIT  - Ok ok, here's a serious answer:",0.149,0.506,0.345,0.4767
zipkin,Zipkin is a distributed tracing system developed by Twitter because our service-oriented-architecture is so goddamned big that it's often hard to understand WTF is happening in any given request.,0.276,0.0,0.724,-0.8608
zipkin,"Seriously, here's a visualization in Zipkin of all the services dependencies at twitter:",0.134,0.0,0.866,-0.1779
zipkin,Is your platform this intense?,0.0,0.256,0.744,0.0964
zipkin,You should use zipkin.,0.0,0.0,1.0,0.0
zipkin,Did I mention it's one of the best scaling systems I've ever seen?,0.0,0.276,0.724,0.6369
zipkin,"It has zero problem keeping up with twitter-level load, and that might be important to you if you're that big.",0.12,0.08,0.8,-0.2263
zipkin,What's that you say?,0.0,0.0,1.0,0.0
zipkin,You're not as big as twitter?,0.0,0.0,1.0,0.0
zipkin,"You only have three services: a web frontend, some kind of middleware, and your database backend?",0.0,0.0,1.0,0.0
zipkin,Maybe zipkin is a bit overkill for you.,0.0,0.0,1.0,0.0
zipkin,"We've done some work to make it a bit easier to setup, but really my job isn't to make zipkin easy for you, it's to make zipkin awesome for Twitter.",0.0,0.305,0.695,0.9081
zipkin,"Still, if you plan on scaling scala, the twitter stack with Finagle etc is insanely good.",0.0,0.162,0.838,0.4404
zipkin,Don't let all the evangelists from Typesafe fool you.,0.266,0.0,0.734,-0.4404
zipkin,Their stack has some serious deficiencies when you try to deploy it in massive-scale architectures.,0.085,0.0,0.915,-0.0772
zipkin,"But again, our job isn't to tell you how good our stack is, or even help you use it.",0.0,0.303,0.697,0.8126
zipkin,It's to make our stack awesome.,0.0,0.451,0.549,0.6249
zipkin,"You can add the following setting on your properties key to disable zipkin,  source .",0.0,0.0,1.0,0.0
zipkin,"Better yet, create separate development properties (like  application-dev.properties ) to avoid changing above setting everytime you want to run in your machine:  https://stackoverflow.com/a/34846351/4504053",0.083,0.238,0.679,0.4767
zipkin,Most likely your code is broken.,0.383,0.0,0.617,-0.4767
zipkin,You can check out the  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/ZipkinAutoConfiguration.java  class where for Edgware we've added load balanced zipkin server resolution.,0.0,0.0,1.0,0.0
zipkin,Is it correct that you are using the code example from the baeldung tutorial?,0.0,0.0,1.0,0.0
zipkin,( http://www.baeldung.com/tracing-services-with-zipkin  - 3.2.,0.0,0.0,1.0,0.0
zipkin,Spring Config),0.0,0.0,1.0,0.0
zipkin,I think there is a mistake with line 34 and 35 (the closing curly brace).,0.167,0.0,0.833,-0.34
zipkin,I've fixed the problem by modifing the method like this:,0.205,0.189,0.606,-0.0516
zipkin,Maybe this helps someone or maybe someone from baeldung reads this and could verify and correct the code example.,0.0,0.126,0.874,0.3818
zipkin,;),0.0,1.0,0.0,0.2263
zipkin,Problem solved.,0.562,0.437,0.0,-0.1531
zipkin,tracer.withSpanInScope(clientSpan)  would do the work.,0.0,0.0,1.0,0.0
zipkin,"Note that,  withSpanInScope(...)  has not been called before sending messages .",0.0,0.0,1.0,0.0
zipkin,"Some people use zipkin to identify dead services, but probably metrics/stats would be the better route if you are trying to break down and report by thrift method.",0.082,0.118,0.8,0.296
zipkin,Well before starting digging into JVM stuff or setting up all the infrastructure needed by Zipkin you could simply start by measuring some application-level metrics.,0.0,0.08,0.92,0.2732
zipkin,You could try the library  metrics  via this  scala api .,0.0,0.0,1.0,0.0
zipkin,Basically you manually set up counters and gauges at specific points of your application that will help you diagnose your bottleneck problem.,0.106,0.106,0.787,0.0
zipkin,The problem might be related to the fact that you're creating the Feign builder manually via  Feign.builder()  factory method.,0.123,0.1,0.776,-0.128
zipkin,We're unable to instrument that call.,0.0,0.0,1.0,0.0
zipkin,You should create a bean (via  SleuthFeignBuilder.builder ) and inject that into your code.,0.0,0.16,0.84,0.2732
zipkin,Dependencies are resolved before plugins are executed.,0.0,0.221,0.779,0.1779
zipkin,So the properties you read with the properties-maven-plugin are not available in the  &lt;dependencies&gt;  section.,0.0,0.0,1.0,0.0
zipkin,"If you want to set a dependency version by property, this property must be set inside the POM, on the command line or in the  settings.xml .",0.0,0.051,0.949,0.0772
zipkin,"You are using an old version of the plugin ( 1.0-alpha-2 ), update it to the  latest   1.0.0 .",0.0,0.0,1.0,0.0
zipkin,Then make sure that the file  version.properties  is in the folder  C:\Workspace .,0.0,0.173,0.827,0.3182
zipkin,"Anyway, with the latest version of the plugin you should get a proper error message if it can't find the file.",0.124,0.0,0.876,-0.4019
zipkin,One more suggestion:  spring-cloud-starter-zipkin  belongs to the  org.springframework.cloud  group which follows another version.,0.0,0.0,1.0,0.0
zipkin,The suggested way to declare that dependency is like the following:,0.0,0.2,0.8,0.3612
zipkin,I was able to setup OpenCensus in GCP (in an instance on the project) by following the steps  mentioned here .,0.0,0.0,1.0,0.0
zipkin,"To set it up quickly, here are the commands I ran in a brand new Ubuntu instance",0.0,0.0,1.0,0.0
zipkin,You should use  egress-gateway .,0.0,0.0,1.0,0.0
zipkin,"When all external calls go to the gateway, istio can get the metadata and does some tracing works.",0.0,0.0,1.0,0.0
zipkin,There are many advantages when using ingress/egress gateway:,0.0,0.263,0.737,0.3612
zipkin,Based on  envoy documentation  it doesn't support https tracing.,0.22,0.0,0.78,-0.3089
zipkin,The tracing configuration specifies global settings for the HTTP tracer used by Envoy.,0.0,0.0,1.0,0.0
zipkin,The configuration is defined by the Bootstrap tracing field.,0.0,0.0,1.0,0.0
zipkin,"Envoy may support other tracers in the future, but right now the HTTP tracer is the only one supported.",0.0,0.22,0.78,0.5859
zipkin,And this post on  stackoverflow,0.0,0.0,1.0,0.0
zipkin,"HTTPS (HTTP over SSL) sends all HTTP content over a SSL tunel, so HTTP content and headers are encrypted as well.",0.0,0.1,0.9,0.2732
zipkin,"I have even tried to reproduce that, but like in your case zipkin worked only for http.",0.0,0.178,0.822,0.5023
zipkin,Based on that I would say it's not possible to use zipkin to track https.,0.0,0.0,1.0,0.0
zipkin,It's because you haven't mentioned the  host  here:,0.0,0.0,1.0,0.0
zipkin,"First, previous answer is wrong, you don't need to specify  host  it is not mandatory unless you want to set up a DNS.",0.176,0.053,0.772,-0.4628
zipkin,"Second, the backend  zipkin  requires the  /zipkin  URI to respond right?",0.0,0.0,1.0,0.0
zipkin,"If this is the case, then the rewrite annotation is removing the URI.",0.0,0.0,1.0,0.0
zipkin,So you would need to change your yaml like this to pass  /zipkin  to your backend.,0.0,0.143,0.857,0.3612
zipkin,Just to clarify the OP problem.,0.351,0.0,0.649,-0.4019
zipkin,There are different  ingress Controllers,0.0,0.0,1.0,0.0
zipkin,Note:,0.0,0.0,1.0,0.0
zipkin,"When you create an ingress, you should annotate each ingress with the appropriate ingress.class to indicate which ingress controller should be used if more than one exists within your cluster.",0.0,0.068,0.932,0.2732
zipkin,"If you do not define a class, your cloud provider may use a default ingress controller.",0.0,0.0,1.0,0.0
zipkin,"Ideally, all ingress controllers should fulfill this specification, but the various ingress controllers operate slightly differently.",0.0,0.216,0.784,0.431
zipkin,Using this annotation:,0.0,0.0,1.0,0.0
zipkin,It looks like you are using NGINX Ingress Controller provided by nginxinc.,0.0,0.185,0.815,0.3612
zipkin,You can find more information about  Rewrites Support  for NGINX Ingress Controller provided by  nginxinc here .,0.0,0.153,0.847,0.4019
zipkin,example:,0.0,0.0,1.0,0.0
zipkin,It's different from the kubernetes community at  kubernetes/ingress-nginx repo .,0.0,0.0,1.0,0.0
zipkin,Different ingress controllers have different configs and annotations.,0.0,0.0,1.0,0.0
zipkin,So for this example:,0.0,0.0,1.0,0.0
zipkin,Test it:,0.0,0.0,1.0,0.0
zipkin,If S1 sends a request to S2 it's the S1 that sets the sampler value and S2 just continues the result.,0.0,0.112,0.888,0.34
zipkin,"In other words S1 will export to zipkin 100 * 0.1 = 10 requests, and S2 will export 100 * 0.1 = 10 requests.",0.0,0.0,1.0,0.0
zipkin,S1 makes a decision and S2 will continue it.,0.0,0.0,1.0,0.0
zipkin,"FYI, this worked after generating dummy X-B3-SpanId; it works as long as X-B3-TraceId is unique.",0.0,0.153,0.847,0.368
zipkin,e.g.,0.0,0.0,1.0,0.0
zipkin,"I know this is old but I have just had exactly the same problem, and have just worked out it's being caused by the appmetrics libraries.",0.134,0.0,0.866,-0.5499
zipkin,Once I figure out how to get it working I'll update this.,0.0,0.0,1.0,0.0
zipkin,EDIT:,0.0,0.0,1.0,0.0
zipkin,OK managed to get it working with appmetrics-dash.,0.0,0.295,0.705,0.4466
zipkin,You need to use monitor() instead of attach() and move the monitor to the end of your routes as so.,0.0,0.0,1.0,0.0
zipkin,I have investigated appmetrics-prometheus and it only has an attach() at this stage so can't be used:,0.0,0.0,1.0,0.0
zipkin,"Alright, so I'm going to answer this based on what you said here:",0.0,0.143,0.857,0.25
zipkin,Or a better approach  if there aint any support/ plugin for the same.,0.0,0.209,0.791,0.4404
zipkin,"The way that I do it us through  Prometheus , in combination with  cloudwatch_exporter , and  alertmanager .",0.0,0.0,1.0,0.0
zipkin,"The configuration for  cloudwatch_exporter  to monitor SQS is going to be something like (this is only two metrics, you'll need to add more based on what you're looking to monitor):",0.0,0.079,0.921,0.3612
zipkin,"You'll then need to configure prometheus to scrape the  cloudwatch_exporter  endpoint at an interval, for ex what I do:",0.0,0.0,1.0,0.0
zipkin,You would then configure  alertmanager  to alert based on those scraped metrics; I do not alert on those metrics so I cannot give you an example.,0.0,0.167,0.833,0.5267
zipkin,"But, to give you an idea how of this architecture, a diagram is below:",0.0,0.0,1.0,0.0
zipkin,If you need to use something like  statsd  you can use  statsd_exporter .,0.0,0.185,0.815,0.3612
zipkin,"And, just in-case you were wondering, yes  Grafana supports prometheus .",0.0,0.394,0.606,0.6369
zipkin,"As there is a bug in Spring AMQP, which will be fixed in Release 2.1.3 
 Issue link",0.0,0.0,1.0,0.0
zipkin,"For a tempory fix, you can enable retry properties to create advice chain.",0.0,0.16,0.84,0.2732
zipkin,Hope this resolves your problem.,0.29,0.495,0.215,0.2263
zipkin,"I had this same issue, changing Spring Boot version to 2.1.0.RELEASE did the trick for me.",0.079,0.0,0.921,-0.0516
zipkin,You should try it too.,0.0,0.0,1.0,0.0
zipkin,There must be something wrong with  RabbitMQ in Spring Boot version 2.1.1.RELEASE.,0.22,0.0,0.78,-0.4767
zipkin,add build.gradle,0.0,0.0,1.0,0.0
zipkin,apply plugin: 'org.springframework.boot',0.0,0.0,1.0,0.0
zipkin,"springBootVersion=2.1.3.RELEASE
springCloudVersion=Greenwich.RELEASE",0.0,0.0,1.0,0.0
zipkin,"Finally got it to work removing  @AutoConfigureAfter ,  @CondtionnalOnBean  and  @ConditionnalOnMissingBean , using instead  @ConditionalOnClass ,  @ConditionnalOnMissingClass  and reproducing other  @Conditionnals  from  TraceAutoConfiguration .",0.0,0.0,1.0,0.0
zipkin,"Not great, but at least working.",0.3,0.0,0.7,-0.284
zipkin,I think that Christian Posta article you refer to is very good.,0.0,0.242,0.758,0.4927
zipkin,"As he says, you can deal with the most common use-cases with the out of the box Kubernetes solutions for discovery (kub dns), load-balancing (with Services) and edge services/gateway (Ingress).",0.0,0.055,0.945,0.1779
zipkin,"As Christian also points out, if you need to dynamically discover services by actively querying rather than knowing what you are looking for then Spring Cloud Kubernetes can be better than going directly to Kubernetes Apis.",0.0,0.189,0.811,0.7717
zipkin,If you need to refresh your app from a config change and see it update quickly without going through a rolling update (which would be needed if you were mounting the configmap as a volume) then Spring cloud Kubernetes config client could be of value.,0.0,0.055,0.945,0.34
zipkin,The ribbon integration could also be of value if you need client-side load-balancing.,0.0,0.167,0.833,0.34
zipkin,So you could start out without Spring Cloud Kubernetes and add parts of it if and when you find that it would help.,0.0,0.109,0.891,0.4019
zipkin,I think it is better to think of the project as adding extra options and conveniences rather than alternatives to Kubernetes-native solutions.,0.0,0.195,0.805,0.5574
zipkin,It is also worth noting that you can deploy a Netflix stack app to Kubernetes (including using Zuul and eureka) and there isn't necessarily anything wrong with that.,0.0,0.151,0.849,0.5352
zipkin,It has the advantage that you can work with it outside Kubernetes and it might be more convenient for your particular team if it's Java team.,0.0,0.074,0.926,0.25
zipkin,"The main downside is that the Netflix stack is very tied to Java, whereas Kubernetes is language neutral.",0.105,0.0,0.895,-0.25
zipkin,We had this very similar issue with Akka.,0.0,0.0,1.0,0.0
zipkin,We observed huge delay in ask pattern to deliver messages to the target actor on peek load.,0.117,0.117,0.765,0.0
zipkin,Most of these issues are related to heap memory consumption and not because of usages of dispatchers.,0.0,0.0,1.0,0.0
zipkin,Finally we fixed these issues by tuning some of the below configuration and changes.,0.0,0.0,1.0,0.0
zipkin,1) Make sure you stop entities/actors which are no longer required.,0.299,0.156,0.544,-0.2732
zipkin,If its a persistent actor then you can always bring it back when you need it.,0.0,0.0,1.0,0.0
zipkin,Refer :  https://doc.akka.io/docs/akka/current/cluster-sharding.html#passivation,0.0,0.0,1.0,0.0
zipkin,2) If you are using cluster sharding then check the akka.cluster.sharding.state-store-mode.,0.0,0.0,1.0,0.0
zipkin,By changing this to persistence we gained 50% more TPS.,0.0,0.224,0.776,0.3818
zipkin,3) Minimize your log entries (set it to info level).,0.0,0.0,1.0,0.0
zipkin,4) Tune your logs to publish messages frequently to your logging system.,0.0,0.0,1.0,0.0
zipkin,"Update the batch size, batch count and interval accordingly.",0.0,0.0,1.0,0.0
zipkin,So that the memory is freed.,0.0,0.351,0.649,0.4019
zipkin,In our case huge heap memory is used for buffering the log messages and send in bulk.,0.0,0.126,0.874,0.3182
zipkin,If the interval is more then you may fill your heap memory and that affects the performance (more GC activity required).,0.0,0.0,1.0,0.0
zipkin,5) Run blocking operations on a separate dispatcher.,0.302,0.0,0.698,-0.3818
zipkin,6) Use custom serializers (protobuf) and avoid JavaSerializer.,0.239,0.0,0.761,-0.296
zipkin,7) Add the below JAVA_OPTS to your jar,0.0,0.0,1.0,0.0
zipkin,"export JAVA_OPTS=""$JAVA_OPTS -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 -Djava.security.egd=file:/dev/./urandom""",0.0,0.0,1.0,0.0
zipkin,The main thing is XX:MaxRAMFraction=2 which will utilize more than 60% of available memory.,0.0,0.0,1.0,0.0
zipkin,"By default its 4 means your application will use only one fourth of the available memory, which might not be sufficient.",0.0,0.0,1.0,0.0
zipkin,Refer :  https://blog.csanchez.org/2017/05/31/running-a-jvm-in-a-container-without-getting-killed/,0.0,0.0,1.0,0.0
zipkin,"Regards,",0.0,0.0,1.0,0.0
zipkin,Vinoth,0.0,0.0,1.0,0.0
zipkin,"As @Bal Chua and @Pär Nilsson mentioned, for environmental variables you can use only string variables because Linux environmental variables can be only strings.",0.0,0.0,1.0,0.0
zipkin,"So, if you use yaml, you need to place value into quotes to force Kubernetes to use string.",0.0,0.124,0.876,0.34
zipkin,For example:,0.0,0.0,1.0,0.0
zipkin,"Even when you use Spring Cloud, 100 services do NOT mean 100 servers.",0.0,0.0,1.0,0.0
zipkin,In Spring Cloud the packaging unit is Spring Boot application and a single server may host many such Spring Boot applications.,0.0,0.0,1.0,0.0
zipkin,"If you want, you can containerize the Spring Boot applications and other Spring Cloud infrastructure support components.",0.0,0.211,0.789,0.4588
zipkin,But that is not Kubernetes.,0.0,0.0,1.0,0.0
zipkin,"If you move to Kubernetes, you don't need the infrastructure support services like Zuul, Ribbon etc.",0.0,0.271,0.729,0.6369
zipkin,"because Kubernetes has its own components for service discovery, gateway, load balancer etc.",0.0,0.0,1.0,0.0
zipkin,"In Kubernetes, the packaging unit is Docker images and one or more Docker containers can be put inside one pod which is the minimal scaling unit.",0.0,0.0,1.0,0.0
zipkin,"So, Kubernetes has a different set of components to manage the Microservices.",0.0,0.0,1.0,0.0
zipkin,Kubernetes is a different platform than Spring cloud.,0.0,0.0,1.0,0.0
zipkin,Both have the same objectives.,0.0,0.0,1.0,0.0
zipkin,"However, Kubernetes has some additional features like self healing, auto-scaling, rolling updates, compute resource management, deployments etc.",0.0,0.135,0.865,0.3612
zipkin,"Just to add to saptarshi basu's answer, you might want to look at  https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes  as it walks through the comparison and asks which responsibilities you might want to be handled by which components when using Spring cloud on kubernetes",0.0,0.066,0.934,0.1531
zipkin,If you're using Sleuth 2.0 you can call on the  Tracer  a method to create a new trace.,0.0,0.123,0.877,0.2732
zipkin,In the older version of sleuth I guess what I'd do is to use an executor that is  NOT  a bean.,0.0,0.0,1.0,0.0
zipkin,That way you would lose the trace and it would get restarted at some point (by rest template or sth like that).,0.107,0.099,0.794,-0.0516
zipkin,Thanks for the kind words!,0.0,0.687,0.313,0.7644
zipkin,In Sleuth Edgware we will support Reactor -  https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-reactor  and in Sleuth Finchley we will support reactor and webflux  https://github.com/spring-cloud/spring-cloud-sleuth/blob/2.0.x/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceWebFluxAutoConfiguration.java .,0.0,0.241,0.759,0.6597
zipkin,In other words it's already possible to use Sleuth in the reactive context.,0.0,0.0,1.0,0.0
zipkin,It seems like you are using  Sleuth with Zipkin via HTTP .,0.0,0.2,0.8,0.3612
zipkin,You can try the  Sleuth with Zipkin via Spring Cloud Stream  approach.,0.0,0.0,1.0,0.0
zipkin,"I haven't done the benchmark myself, but it should improve the performance in theory.",0.0,0.243,0.757,0.5927
zipkin,Please see the documentation at:  https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_spring_cloud_stream,0.0,0.315,0.685,0.3182
zipkin,I wonder what kind of a benchmarking method you have picked.,0.0,0.0,1.0,0.0
zipkin,Which version of Sleuth are you using?,0.0,0.0,1.0,0.0
zipkin,Also is this one single benchmark that you're doing?,0.0,0.0,1.0,0.0
zipkin,Is it on your computer?,0.0,0.0,1.0,0.0
zipkin,Has the JVM gotten heated up?,0.0,0.0,1.0,0.0
zipkin,Are there any other processes executed?,0.0,0.0,1.0,0.0
zipkin,Doing benchmarking is not that easy... You can use tools like JMH to do it better.,0.0,0.278,0.722,0.6597
zipkin,BTW try turning off the DEBUG logging level and check the results again.,0.0,0.0,1.0,0.0
zipkin,We are performing benchmark tests of Sleuth and from what we see when adding Sleuth the latency gets increased by around 20 ms. Definitely not 600 ms.,0.0,0.161,0.839,0.5859
zipkin,I belive you had problem with: org.springframework.cloud.sleuth.zipkin.ServerPropertiesEndpointLocator.,0.351,0.0,0.649,-0.4019
zipkin,It uses InetUtils.findFirstNonLoopbackAddress() to determine instance address.,0.0,0.0,1.0,0.0
zipkin,Method is called synchronously when each span is close (in ZipkinSpanListener#convert).,0.0,0.0,1.0,0.0
zipkin,The workaround is to create custom org.springframework.cloud.sleuth.zipkin.EndpointLocator.,0.0,0.259,0.741,0.2732
zipkin,You can use something like that:,0.0,0.333,0.667,0.3612
zipkin,And combine it with one of existing EndpointLocators.,0.0,0.0,1.0,0.0
zipkin,You can find them in: org.springframework.cloud.sleuth.zipkin.ZipkinAutoConfiguration.,0.0,0.0,1.0,0.0
zipkin,This issue is already fixed in sleuth 2.X.X.,0.0,0.0,1.0,0.0
zipkin,Where: org.springframework.cloud.sleuth.zipkin2.DefaultEndpointLocator caches server address:,0.0,0.0,1.0,0.0
zipkin,The web app is trying to access  config.json  at root (accessing as  /config.json  vs just  config.json  ) - that is  http://localhost:8001/config.json  .,0.0,0.0,1.0,0.0
zipkin,This would obviously be wrong as it should be  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json,0.256,0.0,0.744,-0.4767
zipkin,There is a very simple solution for this - just run:,0.0,0.244,0.756,0.3774
zipkin,Now just go to  http://localhost:9411  and the UI should be up (tried and verified.),0.0,0.0,1.0,0.0
zipkin,You can get the name of the pod by doing  kubectl get pods,0.0,0.0,1.0,0.0
zipkin,"PS:  kubectl proxy  is generally meant to access the Kubernetes API, and  kube port-forward  is the right tool in this case.",0.0,0.0,1.0,0.0
zipkin,I'm not sure this is the right way to do it but this should normally works,0.09,0.0,0.91,-0.1232
zipkin,I have seen this issue a lot.,0.0,0.0,1.0,0.0
zipkin,"From my experience the most common cause is, that the base64 string was encoded on the commandline using  echo '$mypw' | base64  which will create newlines in the encoded string.",0.0,0.07,0.93,0.2732
zipkin,You need to use the  -n  switch to echo:  echo -n '$mypw' | base64 .,0.0,0.0,1.0,0.0
zipkin,What logging framework are you using?,0.0,0.0,1.0,0.0
zipkin,I was using log4j2 in my project.,0.0,0.0,1.0,0.0
zipkin,It started working when I removed log4j2 and left it to the default logging of spring-cloud-sleuth.,0.0,0.0,1.0,0.0
zipkin,It's a bug -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/855  .,0.0,0.0,1.0,0.0
zipkin,I've fixed it ATM.,0.0,0.0,1.0,0.0
zipkin,A workaround is to start it manually either in each method that uses  @NewSpan  by calling  start()  method on current span (that doesn't scale too nicely),0.0,0.0,1.0,0.0
zipkin,You can also create a bean of  SpanCreator  (you can check the fixed version here  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/annotation/DefaultSpanCreator.java ),0.0,0.13,0.87,0.2732
zipkin,Notice the  .start()  at the end of the method.,0.0,0.0,1.0,0.0
zipkin,Try this,0.0,0.0,1.0,0.0
zipkin,I hope this will help you.,0.0,0.651,0.349,0.6808
zipkin,According to  Spring Boot Reference Docs  :,0.0,0.0,1.0,0.0
zipkin,"To enable  /httptrace  in the actuator, then you have to create a bean of   InMemoryHttpTraceRepository  class in the custom  @Configuration  class which provides the trace of the request and response.",0.0,0.07,0.93,0.2732
zipkin,"To enable  /auditevents  in the actuator, then you have to create a bean of  InMemoryAuditEventRepository  class in the custom  @Configuration  class which exposes audit events information.",0.056,0.079,0.865,0.1531
zipkin,"To enable  /integrationgraph  in actuator, you have to add  spring-integration-core dependency  in the pom.xml (as per documentation) :",0.0,0.0,1.0,0.0
zipkin,"or if you are having a spring-boot project, then add this :",0.0,0.0,1.0,0.0
zipkin,/actuator/sessions  are by-default enabled.,0.0,0.0,1.0,0.0
zipkin,But still you can add this explicitly to check the behaviour.,0.0,0.0,1.0,0.0
zipkin,Add this in application.properties.,0.0,0.0,1.0,0.0
zipkin,Thanks Jorg Heymans for the question.,0.0,0.367,0.633,0.4404
zipkin,"Yeah, it's a bug and should be fixed by  https://github.com/line/armeria/pull/3120 
Thank you!",0.0,0.357,0.643,0.6114
zipkin,Ribbon is a client side load balancer which means there is no any other hop in between your client and service.,0.104,0.0,0.896,-0.296
zipkin,Basically you keep and maintain a list of service on your client.,0.0,0.0,1.0,0.0
zipkin,In AWS load balancer case you need to make another hop in between the client and server.,0.0,0.0,1.0,0.0
zipkin,Both have advanges and disadvantages.,0.403,0.0,0.597,-0.4019
zipkin,Former has the advantage of not having any dependency to any specific external solution.,0.0,0.264,0.736,0.5106
zipkin,Basically with ribbon and service discovery like eureka you can deploy your product to any cloud provider or on-premise setup without additional effort.,0.0,0.102,0.898,0.3612
zipkin,Latter has advantage of not needing an extra component of service discovery or keeping the cache of service list on client.,0.0,0.091,0.909,0.25
zipkin,But it has that additional hop which might be an issue if you are trying to run an very high-load system.,0.0,0.0,1.0,0.0
zipkin,Although I don't have much experience with AWS CloudWatch what I know is it helps you to collect logs to a central place from different AWS components.,0.0,0.102,0.898,0.3818
zipkin,And that is what you are trying to do with your solution.,0.0,0.173,0.827,0.3182
zipkin,"kubectl exec -it ""pod-name"" -c ""container-name"" -n ""namespace""",0.0,0.0,1.0,0.0
zipkin,Here only the container name is needed.,0.0,0.0,1.0,0.0
zipkin,In your case it will be:,0.0,0.0,1.0,0.0
zipkin,kubectl exec -it my-api-XXX -c my-api  -- /bin/bash,0.0,0.0,1.0,0.0
zipkin,You can exec to Zipkin because  exec  is taking zipkin as the default container.,0.0,0.0,1.0,0.0
zipkin,It is solved now; all I had to do was port forwarding.,0.0,0.174,0.826,0.2732
zipkin,"Thanks,",0.0,1.0,0.0,0.4404
zipkin,"By default you service is exposed as  ClusterIP , in this case your service will be accessible from within your cluster.",0.064,0.0,0.936,-0.0772
zipkin,"You can use port forwarding "" With this connection in place you can use your local workstation to debug your application that is running in the pod "" as described in the answer above.",0.0,0.0,1.0,0.0
zipkin,"Another approach is to use other  ""service types""  like  NodePort .",0.0,0.217,0.783,0.3612
zipkin,You can find more information here  Publishing services (ServiceTypes),0.0,0.0,1.0,0.0
zipkin,"Sleuth will do the same for messaging by using message headers to propagate  span id, trace id  and other relevant information.",0.0,0.0,1.0,0.0
zipkin,It does so by registering special channel interceptor.,0.0,0.297,0.703,0.4522
zipkin,"The configuration you're referring to is for the instrumentation of messaging systems, not for sending traces to zipkin using a messaging system.",0.0,0.0,1.0,0.0
zipkin,"You should look at this  auto-configuration , and especially this  sender config .",0.0,0.0,1.0,0.0
zipkin,What you want to do has also been documented here:  https://cloud.spring.io/spring-cloud-sleuth/2.0.x/single/spring-cloud-sleuth.html#_sleuth_with_zipkin_over_rabbitmq_or_kafka,0.0,0.115,0.885,0.0772
zipkin,You should only need to add  spring-cloud-starter-zipkin  and  spring-rabbit  to your dependencies.,0.0,0.0,1.0,0.0
zipkin,"If you want to change the default queue (which is  zipkin ), then you'll need to add  spring.zipkin.rabbitmq.queue  to your properties.",0.0,0.061,0.939,0.0772
zipkin,You will need your own  PropagationFactory  implementation.,0.0,0.0,1.0,0.0
zipkin,Here is the default one:  https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/B3Propagation.java,0.0,0.0,1.0,0.0
zipkin,You can create a bean and sleuth should use that instead of this one.,0.0,0.149,0.851,0.2732
zipkin,More specifically you will need an implementation with a custom  TraceContext.Extractor&lt;C&gt;  implementation.,0.0,0.0,1.0,0.0
zipkin,"This can then pull the trace ID from your header, and add return the appropriate  TraceContext .",0.0,0.0,1.0,0.0
zipkin,Then it can pass it along using the normal headers.,0.0,0.0,1.0,0.0
zipkin,If you'd like to use the same correlation header when sending downstream then you will also have to implement  TraceContext.Injector&lt;C&gt; .,0.0,0.116,0.884,0.3612
zipkin,The option is to disable Slf4j integration as you mentioned.,0.0,0.0,1.0,0.0
zipkin,"When a new span / scope is created, we go through Slf4j to put data in MDC and it takes time unfortunately.",0.107,0.089,0.804,-0.1027
zipkin,Disabling that will save it.,0.333,0.344,0.323,0.0258
zipkin,This is indeed possible with the mentioned  executor channel .,0.0,0.0,1.0,0.0
zipkin,All you recipient flows must really start from the  ExecutorChannel .,0.0,0.0,1.0,0.0
zipkin,In your case you have to modify all of them to something like this:,0.0,0.161,0.839,0.3612
zipkin,Pay attention to the  IntegrationFlows.from(MessageChannels.executor(taskExexecutor())) .,0.259,0.0,0.741,-0.1027
zipkin,That's exactly how you can make each sub-flow async.,0.0,0.0,1.0,0.0
zipkin,UPDATE,0.0,0.0,1.0,0.0
zipkin,For the older Spring Integration version without  IntegrationFlow  improvement for the sub-flows we can do like this:,0.124,0.125,0.751,0.0052
zipkin,This is similar to what you show in the comment above.,0.0,0.0,1.0,0.0
zipkin,"Works for me with 1.4.0.RELEASE (2.0.0.RELEASE isn't out yet, but should be soon).",0.0,0.0,1.0,0.0
zipkin,You probably have a bad jar file in your local maven cache (e.g.,0.241,0.0,0.759,-0.5423
zipkin,the one that it complains about).,0.342,0.0,0.658,-0.3818
zipkin,You have to provide a different logging pattern to make it work with PCF Metrics AFAIR.,0.0,0.0,1.0,0.0
zipkin,You need the parent span to be present in logs.,0.0,0.0,1.0,0.0
zipkin,"Set the property  logging.pattern.level: ""%clr(%5p) %clr([${spring.application.name:-},%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-B3-ParentSpanId:-},%X{X-Span-Export:-}]){yellow}"" .",0.0,0.0,1.0,0.0
zipkin,Check this example:  https://github.com/pivotal-cf/pcf-metrics-trace-example-spring,0.0,0.0,1.0,0.0
zipkin,"PCF metrics does  not support custom spans, it only shows the respomse time distribution span that corresponds to http request routed by goRouter.",0.093,0.0,0.907,-0.3089
zipkin,It was a bug that got fixed with this commit -  https://github.com/spring-cloud/spring-cloud-sleuth/commit/d7a0747907f4ab7201f67e7d0c762a324fbe0668  .,0.0,0.217,0.783,0.3612
zipkin,Please check out the latest snapshots,0.0,0.315,0.685,0.3182
zipkin,Why are you setting the values of dependencies manually?,0.0,0.252,0.748,0.4019
zipkin,Please use the Edgware.SR2 BOM.,0.0,0.365,0.635,0.3182
zipkin,"You have to add the kafka dependency, ensure that rabbit is not on the classpath.",0.0,0.157,0.843,0.3818
zipkin,If you have both kafka and rabbit on the classpath you need to set the  spring.zipkin.sender.type=kafka,0.0,0.0,1.0,0.0
zipkin,UPDATE:,0.0,0.0,1.0,0.0
zipkin,"As we describe in the documentation, the Sleuth Stream support is deprecated in Edgware and removed in FInchley.",0.0,0.137,0.863,0.4019
zipkin,"If you've decided to go with the new approach of using native Zipkin messaging support, then you have to use the Zipkin Server with Kafka as described here  https://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10  .",0.0,0.088,0.912,0.4019
zipkin,Let me copy part of the docs here,0.0,0.0,1.0,0.0
zipkin,"The following configuration points apply apply when  KAFKA_BOOTSTRAP_SERVERS  or
 zipkin.collector.kafka.bootstrap-servers  is set.",0.0,0.0,1.0,0.0
zipkin,"They can be configured by setting an environment
variable or by setting a java system property using the  -Dproperty.name=value  command line
argument.",0.111,0.0,0.889,-0.3612
zipkin,"Some settings correspond to ""New Consumer Configs"" in
 Kafka documentation .",0.0,0.0,1.0,0.0
zipkin,Environment Variable | Property | New Consumer Config | Description,0.0,0.0,1.0,0.0
zipkin,"KAFKA_BOOTSTRAP_SERVERS  |  zipkin.collector.kafka.bootstrap-servers  | bootstrap.servers | Comma-separated list of brokers, ex.",0.0,0.0,1.0,0.0
zipkin,127.0.0.1:9092.,0.0,0.0,1.0,0.0
zipkin,No default,0.688,0.0,0.312,-0.296
zipkin,KAFKA_GROUP_ID  |  zipkin.collector.kafka.group-id  | group.id | The consumer group this process is consuming on behalf of.,0.0,0.0,1.0,0.0
zipkin,Defaults to  zipkin,0.0,0.0,1.0,0.0
zipkin,KAFKA_TOPIC  |  zipkin.collector.kafka.topic  | N/A | Comma-separated list of topics that zipkin spans will be consumed from.,0.0,0.0,1.0,0.0
zipkin,Defaults to  zipkin,0.0,0.0,1.0,0.0
zipkin,KAFKA_STREAMS  |  zipkin.collector.kafka.streams  | N/A | Count of threads consuming the topic.,0.0,0.0,1.0,0.0
zipkin,Defaults to  1,0.0,0.0,1.0,0.0
zipkin,OpenStack does not have Zipkin as an inbuilt tracer.,0.0,0.0,1.0,0.0
zipkin,Hence OSProfiler was adopted as a standard project for tracing in OpenStack.,0.0,0.0,1.0,0.0
zipkin,"As far as i can see from the documentation, Nova should have OSProfiler support for Mitaka.",0.0,0.162,0.838,0.4019
zipkin,"Although i have not used OSProfiler with Mitaka, I have worked with OSProfiler with Newton and subsequent releases.",0.0,0.0,1.0,0.0
zipkin,You can post the issue that you are facing so that it will be easier to debug.,0.0,0.149,0.851,0.4215
zipkin,"If you're using Edgware release train, just set  spring.zipkin.sender.type=web .",0.0,0.0,1.0,0.0
zipkin,That way you force the HTTP based span sending,0.0,0.0,1.0,0.0
zipkin,"Embedded headers are not pluggable, but you can disable them with  ...producer.header-mode=raw .",0.0,0.0,1.0,0.0
zipkin,"With Ditmars (1.3.x) you can use the kafka11 artifact, which supports native headers - you have to override a bunch of dependencies (kafka-clients, SK, SIK and kafka itself if you are using the  KafkaEmbedded  broker for testing.",0.0,0.068,0.932,0.3612
zipkin,See  the relesae notes ).,0.0,0.0,1.0,0.0
zipkin,There's  a discussion on Gitter  about overriding the versions.,0.0,0.0,1.0,0.0
zipkin,spring-kafka 1.3.x natively uses 0.11 but 1.3.1 and higher (1.3.2 is current) also supports the 1.0.0 client.,0.0,0.169,0.831,0.5023
zipkin,Elmhurst (2.0) - currently in milestones - uses SK 2.1.0 which natively uses 1.0.0 kafka.,0.0,0.0,1.0,0.0
zipkin,We implemented this on our microservices platform,0.0,0.0,1.0,0.0
zipkin,A lot of the logging is done by pushing requests onto a RabbitMQ queue and then getting logstash to consume that.,0.0,0.0,1.0,0.0
zipkin,Other data is obtained via filebeat transmitting the logs to logstash,0.0,0.0,1.0,0.0
zipkin,Both the logs and the RabbitMQ data has the id attached so can be correlated,0.0,0.0,1.0,0.0
zipkin,An alternative approach would be to build instrumentation into each microservice that specifically monitored latency and then record that directly into logstash,0.0,0.0,1.0,0.0
zipkin,You might like to read  https://medium.com/devopslinks/how-to-monitor-the-sre-golden-signals-1391cadc7524  for a general guide to essential monitoring that is applicable to microservices,0.0,0.135,0.865,0.3612
zipkin,I figured out how to disable the bean that was injecting LogbackAccess.,0.0,0.0,1.0,0.0
zipkin,This resolved the issue so that Zipkin is now accepting requests.,0.0,0.323,0.677,0.5106
zipkin,"To log internal HTTP request sent from a Node.js server, you can create a Proxy Node.js server and log all requests there using  Morgan .",0.0,0.091,0.909,0.2732
zipkin,"First, define 3 constants (or read from your project config file):",0.0,0.0,1.0,0.0
zipkin,"Second, When your Node.js server is launched, start the logger server at the same time if  ENABLE_LOGGER  is true.",0.0,0.202,0.798,0.5106
zipkin,The logger server only do one thing: log the request and forward it to the real API server using  request  module.,0.0,0.0,1.0,0.0
zipkin,You can use  Morgan  to provide more readable format.,0.0,0.0,1.0,0.0
zipkin,"Third, in your Node.js server, send API request to logger server when  ENABLE_LOGGER  is true, and send API directly to the real server when  ENABLE_LOGGER  is false.",0.0,0.097,0.903,0.4215
zipkin,No - we haven't added any instrumentation around Webservicetemplate.,0.239,0.0,0.761,-0.296
zipkin,You'd have to add an interceptor similar to the one we add for RestTemplate.,0.0,0.0,1.0,0.0
zipkin,You'd have to pass all the tracing headers to the request so that the other side can properly parse it.,0.0,0.0,1.0,0.0
zipkin,We have an internal OkHttpClient wrapper implementing Call.Factory which adds an initial interceptor:,0.0,0.0,1.0,0.0
zipkin,to solve this problem.,0.451,0.26,0.289,-0.3237
zipkin,"It is not transparent, however, so may not be good for Brave.",0.152,0.215,0.633,0.2486
zipkin,"It works fine, because in practice once a client is configured, you only really use the  Call.Factory  interface :-)",0.0,0.204,0.796,0.4767
zipkin,"The  Ctx  bit is just the context with stuff we want to propagate, we can do it implicit or explicit, hence the extra method to explicitly take it.",0.0,0.046,0.954,0.0772
zipkin,Thanks for trying out HTrace!,0.0,0.444,0.556,0.4926
zipkin,Sorry that the version issue is such a pain right now.,0.365,0.0,0.635,-0.5574
zipkin,It is much easier to configure HTrace with the version in cloudera's CDH5.5 distribution of Hadoop and later.,0.0,0.141,0.859,0.4215
zipkin,"There is a good description of how to do it here:  http://blog.cloudera.com/blog/2015/12/new-in-cloudera-labs-apache-htrace-incubating/   If you want to stick with an Apache release of the source code rather than a vendor release, try Hadoop 3.0.0-alpha1.",0.0,0.127,0.873,0.4939
zipkin,http://hadoop.apache.org/releases.html,0.0,0.0,1.0,0.0
zipkin,The HTrace libraries shippped in Hadoop 2.6 and 2.7 are very old... we never backported HTrace 4.x to those branches.,0.0,0.0,1.0,0.0
zipkin,"They were stability branches, so new features like tracing was out of scope.",0.0,0.187,0.813,0.4144
zipkin,"There is some functionality there, but not much.",0.0,0.0,1.0,0.0
zipkin,I recommend using the newer HTrace 4.x library which is actively developed.,0.0,0.348,0.652,0.5859
zipkin,"The HTrace 4.x branch also has a stable API, so hopefully breakage will be minimized in the future.",0.0,0.257,0.743,0.6361
zipkin,"Exactly, in the code, I see the configuration key's prefix is  dfs.htrace , not the  hadoop.htrace .",0.0,0.0,1.0,0.0
zipkin,"And in dfsclient, it's  dfs.client.htrace .",0.0,0.0,1.0,0.0
zipkin,"You can change the prefix to  dfs.htrace , then restart the cluster and it take effect.",0.0,0.0,1.0,0.0
zipkin,The code is in class  org.apache.hadoop.tracing.SpanReceiverHost .,0.0,0.0,1.0,0.0
zipkin,Hope this help!,0.0,0.855,0.145,0.7088
zipkin,The sampling decision is taken for a trace.,0.0,0.0,1.0,0.0
zipkin,That means that when the first request comes in and the span is created you have to take a decision.,0.0,0.1,0.9,0.25
zipkin,You don't have any tags / baggage at that point so you must not depend on the contents of tags to take this decision.,0.0,0.0,1.0,0.0
zipkin,That's a wrong approach.,0.608,0.0,0.392,-0.4767
zipkin,You are taking a very custom approach.,0.0,0.0,1.0,0.0
zipkin,If you want to go that way (which is not recommended) you can create a custom implementation of a  SpanReporter  -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/SpanReporter.java#L30  .,0.0,0.167,0.833,0.34
zipkin,SpanReporter  is the one that is sending spans to zipkin.,0.0,0.0,1.0,0.0
zipkin,You can create an implementation that will wrap an existing  SpanReporter  implementation and will delegate the execution to it only when some values of tags match.,0.0,0.167,0.833,0.5859
zipkin,But from my perspective it doesn't sound right.,0.0,0.0,1.0,0.0
zipkin,If I'm not mistaken (and I guess I'm not) no wonder that you're not sending the Spans to Zipkin cause you didn't add the Zipkin dependency.,0.081,0.077,0.842,-0.0232
zipkin,Check the  Sleuth with Zipkin via HTTP  section of the docs:  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html  .,0.0,0.0,1.0,0.0
zipkin,This config worked for me in 1 of my application:,0.0,0.0,1.0,0.0
zipkin,Enabling property might do the trick!,0.23,0.0,0.77,-0.126
zipkin,"First of all the main feature of Spring Integration is  MessageChannel , but it still isn't clear to me why people are missing  .channel()  operator in between endpoint definitions.",0.177,0.0,0.823,-0.6784
zipkin,I mean that for your case it must be like:,0.0,0.238,0.762,0.3612
zipkin,Now about your particular problem.,0.403,0.0,0.597,-0.4019
zipkin,"Look,  ContentEnricher  ( .enrich() ) is request-reply component:  http://docs.spring.io/spring-integration/reference/html/messaging-transformation-chapter.html#payload-enricher .",0.0,0.0,1.0,0.0
zipkin,Therefore it sends request to its  requestChannel  and waits for reply.,0.0,0.0,1.0,0.0
zipkin,And it is done independently of the  requestChannel  type.,0.0,0.0,1.0,0.0
zipkin,I raw Java we can demonstrate such a behavior with this code snippet:,0.0,0.0,1.0,0.0
zipkin,where you should see that  ADD_LINE_ITEM_CHANNEL  as an  ExecutorChannel  doesn't have much value because we are blocked within loop for the reply anyway.,0.165,0.0,0.835,-0.4829
zipkin,"A  .split()  does exactly similar loop, but since by default it is with the  DirectChannel , an iteration is done in the same thread.",0.0,0.0,1.0,0.0
zipkin,Therefore each next item waits for the reply for the previous.,0.0,0.0,1.0,0.0
zipkin,"That's why you definitely should parallel exactly as an input for the  .enrich() , just after  .split() .",0.0,0.153,0.847,0.4019
zipkin,The Apache Thrift TSocketTransport (almost certainly what you are using) uses TCP on a configurable port.,0.0,0.146,0.854,0.34
zipkin,Cassandra usually uses port 9160 for thrift.,0.0,0.0,1.0,0.0
zipkin,When using Thrift/TCP no HTTP setup is necessary.,0.239,0.0,0.761,-0.296
zipkin,Just open 9160 (and any other ports your custom thrift servers may be listening on).,0.0,0.0,1.0,0.0
zipkin,"Though you can use Thrift over HTTP, Thrift is RPC, not REST, so proxy caching will cause problems, the client needs a direct comm channel with the server.",0.094,0.0,0.906,-0.4019
zipkin,"If you do need to access a thrift service via a proxy, something like this would work:",0.0,0.152,0.848,0.3612
zipkin,https://github.com/totally/thrift_goodies/blob/master/transport.py,0.0,0.0,1.0,0.0
zipkin,You can kill the kerberos stuff if you don't need that.,0.32,0.0,0.68,-0.6908
zipkin,i get the same messages but still be able to collect messages and view them with the web service.,0.0,0.0,1.0,0.0
zipkin,I dont know why the [error] prefix is in front of it but if you read the chars behind you see INF/DEB and so on...,0.0,0.0,1.0,0.0
zipkin,It stays for INFO and DEBUG.,0.0,0.0,1.0,0.0
zipkin,Greets,0.0,1.0,0.0,0.1531
zipkin,"yes, you can have multiple express running in the same node process (thats how clustering works in node as well)",0.0,0.124,0.876,0.4019
zipkin,but you will need to have them running on different ports.,0.0,0.0,1.0,0.0
zipkin,;,0.0,0.0,0.0,0.0
zipkin,It is possible to create two separate tracer providers.,0.0,0.208,0.792,0.2732
zipkin,"Only one of them will be the global tracer provider, which the API will use if you call API methods.",0.0,0.0,1.0,0.0
zipkin,"You can't use the plugins in this configuration, which means you will have to manually instrument your application.",0.0,0.0,1.0,0.0
zipkin,"If this is a use-case which is important to you, I suggest you create an issue on the github repo.",0.0,0.196,0.804,0.4404
zipkin,"This indicates that the pod is not ready, hence the service will not add that pod's IP in the list of endpoints.",0.091,0.0,0.909,-0.2755
zipkin,Check the readiness probe of your pod by describing it and debug the issue that's making it non-ready.,0.0,0.105,0.895,0.25
zipkin,"Once this if fixed, you'll start seeing some endpoints populated when you describe the service &amp; that will enable you to access the service by DNS name.",0.0,0.0,1.0,0.0
zipkin,First specify your own overlay network (see bottom of code below) and use it for your services.,0.0,0.0,1.0,0.0
zipkin,"Then in your compose file for your other services like ZIpkin, add the  backbone  network to its list.",0.0,0.128,0.872,0.3612
zipkin,Eg:,0.0,0.0,1.0,0.0
zipkin,"Note that outside of the first compose file, you'll need to prefix the  project  name for your network.",0.0,0.0,1.0,0.0
zipkin,Unless you set the environment variable  COMPOSE_PROJECT_NAME  it will be the name of the directory that the compose file is in.,0.0,0.0,1.0,0.0
zipkin,Do a  docker network ls  to find out the full name of the network to use.,0.0,0.0,1.0,0.0
zipkin,"OK, so I solved my problem using only docker-compose files.",0.207,0.373,0.42,0.3318
zipkin,By Portainer I pasted my second docker-compose file in Stack section (I creaated new Stack):,0.0,0.0,1.0,0.0
zipkin,Now we should use 'my_name_zipkin' name to communicate with this service.,0.0,0.0,1.0,0.0
zipkin,Service name is the name we should use to communicate between containers.,0.0,0.0,1.0,0.0
zipkin,So in properties file I set:,0.0,0.0,1.0,0.0
zipkin,"The slf4j API only takes  String  as the input to the  info ,  debug ,  warn ,  error  messages.",0.227,0.0,0.773,-0.4767
zipkin,"What you could do is create your own JsonLogger wrapper, which takes a normal  Logger  (maybe wraps around it), which you could include at the top of your classes like:",0.0,0.198,0.802,0.6597
zipkin,private static final JsonLogger logger = new JsonLogger(LoggerFactory.getLogger(MyClass.class)) ;,0.0,0.0,1.0,0.0
zipkin,"You can then use Jackson, GSON or your favourite object to JSON mapper inside your JsonLogger so that you could do what you want.",0.0,0.053,0.947,0.0772
zipkin,"It can then offer the  info ,  debug ,  warn ,  error  methods like a normal logger.",0.247,0.151,0.602,-0.1531
zipkin,You can also create your own  JsonLoggerFactory  which encapsulates this for you so that the line to include in each class is more concise.,0.0,0.084,0.916,0.2732
zipkin,"Yes, you can use BAM/CEP for this.",0.0,0.31,0.69,0.4019
zipkin,If you need real time monitoring you can use CEP and you can use BAM for batch process.,0.0,0.0,1.0,0.0
zipkin,"From BAM 2.4.0 onwards, CEP features have been added inside BAM also hence you can use BAM and do real time analytics.",0.0,0.0,1.0,0.0
zipkin,What type of services are involved with your scenario?,0.0,0.0,1.0,0.0
zipkin,Depends on this you can use already existing data publisher or write new data publisher for BAM/CEP to publish your request details.,0.0,0.0,1.0,0.0
zipkin,"For example if you are having chain of axis2 webservice calls for a request from client, and you want to monitor where the bottle neck/more time consumed, then you may use the service stats publishing, and monitor the average time take to process the message which will help you to see where the actual delay has been introduced.",0.038,0.066,0.896,0.1779
zipkin,For this you can use existing service statistics publisher feature.,0.0,0.0,1.0,0.0
zipkin,"Also BAM will allow you to create your own dashboard to visualize, hence you can customize the dashboard.",0.0,0.2,0.8,0.4588
zipkin,Also with BAM 2.4.0 we have introduced notifications feature also which you can define some threshold value and configure to send notification if that cross that threshold value.,0.0,0.156,0.844,0.5859
zipkin,"I contributed to Micronaut and submitted a PR, which is now merged.",0.0,0.0,1.0,0.0
zipkin,Pull request,0.0,0.0,1.0,0.0
zipkin,So it was application B which was not passing the header along.,0.0,0.0,1.0,0.0
zipkin,Turns out that the queue uri had a property  targetClient  which was set to 1.,0.0,0.0,1.0,0.0
zipkin,The uri is something like,0.0,0.385,0.615,0.3612
zipkin,"Now I am not an IBM MQ expert by far, but the  documentation  states that setting this property to 1 means that  Messages do not contain an MQRFH2 header.",0.0,0.0,1.0,0.0
zipkin,"I toggled it to 0 and voila, all spans fall into place.",0.0,0.0,1.0,0.0
zipkin,You must tell the containers the network &quot;foo_network&quot;.,0.0,0.0,1.0,0.0
zipkin,The External flag says that the containers are not accessible from outside.,0.0,0.0,1.0,0.0
zipkin,"Of course you don't have to bet, but I thought as an example it might be quite good.",0.0,0.211,0.789,0.6474
zipkin,And because of the &quot;links&quot; function look here  Link,0.0,0.0,1.0,0.0
zipkin,I think I found the problem.,0.474,0.0,0.526,-0.4019
zipkin,Than I used the Exceutor in my code:,0.0,0.0,1.0,0.0
zipkin,With this configuration the service was not starting correctly.,0.0,0.0,1.0,0.0
zipkin,Now the @Bean(&quot;threadPoolTaskExecutor&quot;) configuration is removed and I'm using only @Async.,0.0,0.0,1.0,0.0
zipkin,But why it's not working with Spring Boot Starter 2.3.x?,0.0,0.0,1.0,0.0
zipkin,And there was no error message in  the log.,0.412,0.0,0.588,-0.5994
zipkin,"Zipkin is a Spring-Boot-based project, the @EnableZipkinServer is not a Spring Cloud annotation.",0.0,0.0,1.0,0.0
zipkin,It’s an annotation that’spart of the Zipkin project.,0.0,0.0,1.0,0.0
zipkin,"This often confuses people who are new to the Spring Cloud Sleuth and Zipkin, because the Spring Cloud team did write the @EnableZipkinStreamServer annotation as part of Spring Cloud Sleuth.",0.073,0.0,0.927,-0.3182
zipkin,The @EnableZipkinStreamServer annotation simplifies the use of Zipkin with RabbitMQ and Kafka.,0.0,0.0,1.0,0.0
zipkin,Advantages of  @EnableZipkinServer is simplicity in setup.,0.0,0.294,0.706,0.3612
zipkin,With the @EnableZipkinStream server you need to set up and configure the services being traced and the Zipkin server to publish/listen to RabbitMQ or Kafka for tracing data.The advantage of the @EnableZipkinStreamServer annotation is that you can continue to collect trace data even if the Zipkin server is unavailable.,0.0,0.04,0.96,0.25
zipkin,This is because the trace messages will accumulate the trace data on a message queue until the Zipkin server is available for processing the records.,0.0,0.0,1.0,0.0
zipkin,"If you use the @EnableZipkinServer annotation and the Zipkin server is unavailable,the trace data that would have been sent by the service(s) to Zipkin will be lost.",0.081,0.0,0.919,-0.3182
zipkin,"Please don't use field injection, use constructor injection.",0.0,0.247,0.753,0.3182
zipkin,Also new span there doesn't make sense cause you already have a new span created by the framework.,0.0,0.111,0.889,0.25
zipkin,Your versions are wrong.,0.508,0.0,0.492,-0.4767
zipkin,"Please don't set the versions by yourself, please use the Spring Cloud BOM (spring-cloud-dependencies) dependency management like presented below",0.0,0.307,0.693,0.7269
zipkin,Also - it's enough for you to add the starters.,0.0,0.0,1.0,0.0
zipkin,"You've added a starter in a given version and then you've added the core dependency in another one, that makes no sense.",0.104,0.0,0.896,-0.296
zipkin,Last thing - versions 1.x are deprecated and no longer maintained.,0.196,0.0,0.804,-0.296
zipkin,The current version is 2.2.0.RELEASE and release train version is Hoxton.RELEASE,0.0,0.0,1.0,0.0
zipkin,This can be done using Finagle's  Contexts .,0.0,0.0,1.0,0.0
zipkin,"Contexts give you access to request-scoped state, such as a request’s deadline, throughout the logical life of a request without requiring them to be explicitly passed",0.0,0.0,1.0,0.0
zipkin,Unfortunately the best answer to this issue is to upgrade to the latest version of Sleuth where we've migrated to Brave as an internal tracer and fixed a lot of issues.,0.065,0.205,0.73,0.7351
zipkin,You can connect your existing container to another network,0.0,0.0,1.0,0.0
zipkin,The error-code implies an error on the other end - 400-errors are not located on your end.,0.153,0.0,0.847,-0.4019
zipkin,Have you tried dumping the response (including headers)?,0.247,0.0,0.753,-0.3182
zipkin,"Also, did you try to re-authenticate, perhaps reset cookings, etc?",0.0,0.0,1.0,0.0
zipkin,Did you contact the other end?,0.0,0.0,1.0,0.0
zipkin,How did they respond to it?,0.0,0.0,1.0,0.0
zipkin,Add a dependencyManagement entry for io.zipkin.zipkin2:zipkin:2.7.1,0.0,0.0,1.0,0.0
zipkin,"For me or anybody finding this thread:
Solved it by upgrading from Camden to Edgware which contains 1.3.5 (and resolving everything around that switch).",0.0,0.176,0.824,0.5719
zipkin,You can create your own custom  SpanAdjuster  that will modify the span name.,0.0,0.149,0.851,0.2732
zipkin,You can also use  FinishedSpanHandler  to operate on finished spans to tweak them.,0.0,0.0,1.0,0.0
zipkin,"Yes, when you create a span you can set the service name.",0.0,0.348,0.652,0.5859
zipkin,Just call  newSpan.remoteServiceName(...),0.0,0.0,1.0,0.0
zipkin,"Taking the input of @MarcinGrzejszczak as reference, I resolved using a custom span:",0.0,0.145,0.855,0.1779
zipkin,Where  tracer  is an autowired object from  Trace :,0.0,0.0,1.0,0.0
zipkin,Both classes are in  brave  package,0.0,0.405,0.595,0.5267
zipkin,Result:,0.0,0.0,1.0,0.0
zipkin,"If you want to take a look at the implementation in more detail, here is the sample:  https://github.com/juanca87/sample-traceability-microservices",0.0,0.075,0.925,0.0772
zipkin,add this under your  section at the end of your pom.xml.,0.0,0.0,1.0,0.0
zipkin,you may need to add for all the dependencies.,0.0,0.0,1.0,0.0
zipkin,"Maybe, I didn't get your question right, but with almost your  docker-compose.yaml  file:",0.0,0.0,1.0,0.0
zipkin,prometheus  metrics are available on  localhost:9411/metrics  both inside container and on host system:,0.0,0.0,1.0,0.0
zipkin,I finally got it working.,0.0,0.0,1.0,0.0
zipkin,I just changed the logstash config file and added:,0.0,0.0,1.0,0.0
zipkin,The filter part was missing earlier.,0.306,0.0,0.694,-0.296
zipkin,"Can you follow the guidelines described here  https://stackoverflow.com/help/how-to-ask  and the next question you ask, ask it with more details?",0.0,0.0,1.0,0.0
zipkin,E.g.,0.0,0.0,1.0,0.0
zipkin,I have no idea how exactly you use Sleuth?,0.239,0.0,0.761,-0.296
zipkin,Anyways I'll try to answer...,0.0,0.0,1.0,0.0
zipkin,"You can create a  SpanAdjuster  bean, that will analyze the span information (e.g.",0.0,0.16,0.84,0.2732
zipkin,span tags) and basing on that information you will change the sampling decision so as not to send it to Zipkin.,0.0,0.0,1.0,0.0
zipkin,Another option is to wrap the default span reporter in a similar logic.,0.0,0.0,1.0,0.0
zipkin,Yet another option is to verify what kind of a thread it is that is creating this span and toggle it off (assuming that it's a  @Scheduled  method) -  https://cloud.spring.io/spring-cloud-static/Finchley.RC2/single/spring-cloud.html#__literal_scheduled_literal_annotated_methods,0.0,0.078,0.922,0.296
zipkin,name: tcp -  protocol: TCP ?,0.0,0.0,1.0,0.0
zipkin,"I tried to back to use the stable version(1.3.3) of spring cloud sleuth, but when i use the bom for the project its make conflict in the spring boot version that i am using(2.0).",0.088,0.048,0.864,-0.3291
zipkin,Its compactible with the spring boot version 2?,0.0,0.0,1.0,0.0
zipkin,You can't use the Sleuth 1.3 with Boot 2.0.,0.0,0.0,1.0,0.0
zipkin,"I am using the spring cloud sleuth to make trace of services on my company, but i have a version of tracing on others services that is not compactible with the opentracing headers, so i want to change the headers of http messages to make the new services compactibles with the current tracing headers that i have in the others components.",0.0,0.037,0.963,0.2759
zipkin,"Yeah, that's the Brave change.",0.0,0.651,0.349,0.6808
zipkin,For http you can define your own parses.,0.0,0.0,1.0,0.0
zipkin,https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceHttpAutoConfiguration.java#L57-L68  .,0.0,0.0,1.0,0.0
zipkin,"
@Autowired HttpClientParser clientParser;
    @Autowired HttpServerParser serverParser;
    @Autowired @ClientSampler HttpSampler clientSampler;
    @Autowired(required = false) @ServerSampler HttpSampler serverSampler;",0.0,0.0,1.0,0.0
zipkin,These are the samplers that you can register.,0.0,0.0,1.0,0.0
zipkin,For messaging you'd have to create your own version of the global channel interceptor - like the one we define here -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/messaging/TraceSpringIntegrationAutoConfiguration.java#L49-L53  .,0.0,0.195,0.805,0.5574
zipkin,"If that's not acceptable for you, go ahead and file an issue in Sleuth so we can discuss it there.",0.094,0.0,0.906,-0.2411
zipkin,There's no concept of a trace finishing in zipkin.,0.239,0.0,0.761,-0.296
zipkin,A span within a trace can start and finish.,0.0,0.0,1.0,0.0
zipkin,"We don't start and stop spans on different hosts, so unfinished spans probably are accidental.",0.08,0.117,0.803,0.1501
zipkin,You can chat more here if you like  https://gitter.im/spring-cloud/spring-cloud-sleuth,0.0,0.238,0.762,0.3612
zipkin,"In Sleuth  1.3.x  you can create a custom  SpanReporter  that, before sending a span to Zipkin, would analyze the URL and would not report that span.",0.0,0.084,0.916,0.2732
zipkin,In Sleuth  2.0.x  you can create a custom  HttpSampler  for the client side (with name  sleuthClientSampler ),0.0,0.13,0.87,0.2732
zipkin,Turns out the issue was a corrupt Maven package.,0.0,0.0,1.0,0.0
zipkin,Deleting my  .m2\repository  folder and running  mvn spring-boot:run  to downloaded dependencies and run my app resolved the issue.,0.0,0.091,0.909,0.1779
zipkin,Please read this page from Zipkin -  https://zipkin.io/pages/instrumenting.html  .,0.0,0.277,0.723,0.3182
zipkin,It's all written there how things should work.,0.0,0.0,1.0,0.0
zipkin,HTTP Tracing,0.0,0.0,1.0,0.0
zipkin,HTTP headers are used to pass along trace information.,0.0,0.0,1.0,0.0
zipkin,The B3 portion of the header is so named for the original name of Zipkin: BigBrotherBird.,0.0,0.133,0.867,0.3182
zipkin,Also please check the B3 specification page -  https://github.com/openzipkin/b3-propagation,0.0,0.247,0.753,0.3182
zipkin,people are scared about the overhead which sleuth can have by adding @NewSpan on the methods.,0.162,0.0,0.838,-0.4404
zipkin,Do they have any information about the overhead?,0.0,0.0,1.0,0.0
zipkin,Have they turned it on and the application started to lag significantly?,0.179,0.0,0.821,-0.34
zipkin,What are they scared of?,0.42,0.0,0.58,-0.4404
zipkin,Is this a high-frequency trading application that you're doing where every microsecond counts?,0.0,0.0,1.0,0.0
zipkin,I need to decide on runtime whether the Trace should be added or not (Not talking about exporting).,0.0,0.0,1.0,0.0
zipkin,Like for actuator trace is not getting added at all.,0.0,0.217,0.783,0.3612
zipkin,I assume this will have no overhead on the application.,0.216,0.0,0.784,-0.296
zipkin,Putting X-B3-Sampled = 0 is not exporting but adding tracing information.,0.0,0.0,1.0,0.0
zipkin,Something like skipPattern property but at runtime.,0.0,0.226,0.774,0.1901
zipkin,I don't think that's possible.,0.0,0.0,1.0,0.0
zipkin,"The instrumentation is set up by adding interceptors, aspects etc.",0.0,0.0,1.0,0.0
zipkin,They are started upon application initialization.,0.0,0.0,1.0,0.0
zipkin,Always export the trace if service exceeds a certain threshold or in case of Exception.,0.0,0.139,0.861,0.2732
zipkin,With the new Brave tracer instrumentation (Sleuth 2.0.0) you will be able to do it in a much easier way.,0.0,0.267,0.733,0.7351
zipkin,"Prior to this version you would have to implement your own version of a  SpanReporter  that verifies the tags (if it contains an  error  tag), and if that's the case send it to zipkin, otherwise not.",0.074,0.0,0.926,-0.4019
zipkin,If I am not exporting Spans to zipkin then will there be any overhead by tracing information?,0.0,0.0,1.0,0.0
zipkin,"Yes, there is cause you need to pass tracing data.",0.0,0.231,0.769,0.4019
zipkin,"However, the overhead is small.",0.0,0.0,1.0,0.0
zipkin,The code you've provided is not related to Sleuth but opentracing.,0.0,0.0,1.0,0.0
zipkin,"In Sleuth you would call  Tracer.createSpan(""name"")  and that way a child span od your current trace would be created.",0.0,0.105,0.895,0.25
zipkin,I've also managed to get it working by using just the cloud trace api by doing this before I create a span.,0.0,0.1,0.9,0.2732
zipkin,Not sure if there is a negative of doing this.,0.447,0.0,0.553,-0.687
zipkin,I succeeded to send gcp's trace api in php client via REST.,0.0,0.219,0.781,0.4215
zipkin,"It can see trace set by php client parameters , but my endpoint for trace api has stopped though I don't know why.Maybe ,it is not still supported well because the document have many ambiguous expression so, I realized watching server response by BigQuery with fluentd and DataStudio and it seem best solution because auto span can be set by table name with yyyymmdd and we can watch arbitrary metrics with custom query or calculation field.",0.084,0.104,0.812,0.577
zipkin,Zipkin's connection to cassandra is independent from the normal spring setup.,0.0,0.0,1.0,0.0
zipkin,We use some very specific setup.,0.0,0.0,1.0,0.0
zipkin,you'll want to set properties in the namespace of zipkin.storage.cassandra,0.0,0.126,0.874,0.0772
zipkin,https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/resources/zipkin-server-shared.yml#L40,0.0,0.0,1.0,0.0
zipkin,"As the documentation suggests , you need to create a  ProducerFactory  bean if you want to use your own  KafkaTemplate :",0.0,0.175,0.825,0.34
zipkin,The Spring Cloud project is moving to their own solutions.,0.0,0.159,0.841,0.1779
zipkin,"Ribbon is replaced by the Spring Cloud Load Balancer, Hysterix by the Spring Cloud Circuit Breaker, Zuul by the Spring Cloud Gateway.",0.0,0.0,1.0,0.0
zipkin,"This is a good read, including examples, about this topic:  https://piotrminkowski.com/2020/05/01/a-new-era-of-spring-cloud/",0.0,0.244,0.756,0.4404
zipkin,Sleuth does this for you by default in 3.x too:  https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-log-integration,0.0,0.0,1.0,0.0
zipkin,You can break this functionality by misconfiguring your log pattern or  logging.pattern.level  or your classpath.,0.0,0.0,1.0,0.0
zipkin,"What I would suggest is going to  https://start.spring.io , generate a new project using sleuth and web/webflux, write a single controller and check the logs (do not create any log config file, just leave everything on default).",0.089,0.0,0.911,-0.2533
zipkin,Let me add to this thread my three bits.,0.0,0.0,1.0,0.0
zipkin,"Speaking of Envoy, yes, when attached to your application it adds a lot of useful features from observability bucket, e.g.",0.0,0.248,0.752,0.6808
zipkin,network level statistics and tracing.,0.0,0.0,1.0,0.0
zipkin,"Here is the question, have you considered running your legacy apps inside service mesh, like Istio ?.",0.0,0.135,0.865,0.3612
zipkin,Istio simplifies deployment and configuration of Envoy for you.,0.0,0.0,1.0,0.0
zipkin,"It injects sidecar container (istio-proxy, in fact Envoy instance) to your Pod application, and gives you these extra features like a set of service metrics out of the box*.",0.0,0.085,0.915,0.3612
zipkin,"Example: Stats produced by Envoy in Prometheus format, like  istio_request_bytes  are visualized in Kiali Metrics dashboard for inbound traffic as  request_size  (check screenshot)",0.0,0.102,0.898,0.3612
zipkin,"*as mentioned by @David Kruk, you still needs to have Prometheus server deployed in your cluster to be able to pull these metrics to Kiali dashboards.",0.0,0.0,1.0,0.0
zipkin,You can learn more about Istio  here .,0.0,0.0,1.0,0.0
zipkin,There is also a dedicated section on how to  visualize metrics  collected by Istio (e.g.,0.0,0.188,0.812,0.4588
zipkin,request size).,0.0,0.0,1.0,0.0
zipkin,"You can use a  TagValueResolver  or a  MessageSpanCustomizer , see the docs for the details.",0.0,0.0,1.0,0.0
zipkin,Better approach will be using Kafka instead of Redis.,0.0,0.266,0.734,0.4404
zipkin,"Create a topic for every microservice &amp; keep moving the packet from
one topic to another after processing.",0.0,0.116,0.884,0.2732
zipkin,"Keep appending the results to same object and keep moving it down the line, untill every micro-service has processed it.",0.0,0.0,1.0,0.0
zipkin,"What you have is correct, your issue is likely not DNS.",0.0,0.0,1.0,0.0
zipkin,You can confirm by doing just a DNS lookup and comparing that to the IP of the Service.,0.0,0.0,1.0,0.0
zipkin,The  doubleName  method is private.,0.0,0.0,1.0,0.0
zipkin,Micronaut cannot apply AOP annotations (like  ContinueSpan  to private methods.,0.0,0.0,1.0,0.0
zipkin,"Are these methods on a bean ( Singleton , etc.)?",0.0,0.0,1.0,0.0
zipkin,I have found the span annotations only get applied on beans properly.,0.0,0.0,1.0,0.0
zipkin,I had to refactor some of my code to create beans from  Factory s or such.,0.0,0.139,0.861,0.2732
zipkin,You've passed in the  B3Propagation.FACTORY  as the implementation of the propagation factory so you're explicitly stating that you want the default B3 headers.,0.0,0.056,0.944,0.0772
zipkin,You've said that you want some other field that is alphanumeric to be also propagated.,0.0,0.085,0.915,0.0772
zipkin,"Then in a log parsing tool you can define that you want to use your custom field as the trace id, but it doesn't mean that the deafult X-B3-TraceId field will be changed.",0.0,0.036,0.964,0.0387
zipkin,"If you want to use your custom field as trace id that Sleuth understands, you need to change the logging format and implement a different propagation factory bean.",0.0,0.048,0.952,0.0772
zipkin,"One  of the  way which   worked for me is 
using ExtraFieldPropagation
and adding those   keys in sleuth  properties  under   propagation-keys
and  whitelisted-keys",0.0,0.0,1.0,0.0
zipkin,"sample code 
  '  @Autowired Tracer tracer;",0.0,0.0,1.0,0.0
zipkin,"With Edgware (SCSt Ditmars), you have to specify which headers will be transferred.",0.0,0.0,1.0,0.0
zipkin,See Kafka Binder Properties .,0.0,0.0,1.0,0.0
zipkin,This is because Edgware was based on Kafka before it supported headers natively and we encode the headers into the payload.,0.0,0.103,0.897,0.3182
zipkin,spring.cloud.stream.kafka.binder.headers,0.0,0.0,1.0,0.0
zipkin,The list of custom headers that will be transported by the binder.,0.0,0.0,1.0,0.0
zipkin,Default: empty.,0.643,0.0,0.357,-0.2023
zipkin,You should also be sure to upgrade spring-kafka to 1.3.9.RELEASE and kafka-clients to 0.11.0.2.,0.0,0.15,0.85,0.3182
zipkin,"Preferably, though, upgrade to Finchley or Greemwich.",0.0,0.0,1.0,0.0
zipkin,Those versions support headers natively.,0.0,0.403,0.597,0.4019
zipkin,"Well, without seeing any code, I could only give you a sample of how you should achieve this.",0.0,0.123,0.877,0.2732
zipkin,"So an http call, for example if you use node-fetch or axios will return a promise.",0.0,0.141,0.859,0.3182
zipkin,"To wait for promises paralelly, you can do the following:",0.0,0.224,0.776,0.3818
zipkin,"Note that I use fetch API here, provided in node by the node-fetch package.",0.0,0.0,1.0,0.0
zipkin,Fetch returns a  Promise .,0.0,0.535,0.465,0.3182
zipkin,Then I call  Promise.all(promises)  where  promises  is a  Promise  array.,0.0,0.45,0.55,0.5994
zipkin,You can then do whatever you would like to do with the 3 responses and your requests were made paralelly.,0.0,0.122,0.878,0.3612
zipkin,"Hope this helps, good luck!",0.0,0.921,0.079,0.8932
zipkin,You have,0.0,0.0,1.0,0.0
zipkin,This means that there is a  &lt;dependencyManagement&gt;  entry in your POM or your Parent POM that sets the version to  2.2.0 .,0.0,0.0,1.0,0.0
zipkin,You can have s custom span reporter that before sending spans to zipkin will dump the span as a json structure to logs.,0.115,0.0,0.885,-0.3818
zipkin,UPDATE:,0.0,0.0,1.0,0.0
zipkin,"With this PR merged  https://github.com/spring-cloud/spring-cloud-sleuth/pull/1068 , in 2.1.0 you'll have an easy way to implement your own MDC entries",0.0,0.146,0.854,0.4404
zipkin,Usage of Sleuth Stream is deprecated.,0.0,0.0,1.0,0.0
zipkin,"Please use the  zipkin  starter, add the  Kafka  dependency and set things as presented here  https://cloud.spring.io/spring-cloud-static/Edgware.SR1/multi/multi__introduction.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka",0.0,0.133,0.867,0.3182
zipkin,"fran, in Edgware.RELEASE the  &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;  will resolve Zipkin 2 dependencies try using  &lt;artifactId&gt; spring-cloud-starter-zipkin-legacy&lt;/artifactId&gt;  instead",0.0,0.167,0.833,0.3818
zipkin,"To define the primary connection factory for RabbitMQ in XML files, you can do something like this:",0.0,0.135,0.865,0.3612
zipkin,"I do not have much knowledge with hysterix, but if you are trying to pass some contextual info like trace IDs around, then io.grpc.Context is the correct class to use.",0.0,0.104,0.896,0.5023
zipkin,You would need to call  context.withValue  to create a new context with the traceID.,0.0,0.149,0.851,0.2732
zipkin,"In the places where you want the data, you need to attach the context.",0.0,0.091,0.909,0.0772
zipkin,"Also be sure to detach the context when done, which I do not see happening in your snippet.",0.0,0.126,0.874,0.3182
zipkin,You need to use ...,0.0,0.0,1.0,0.0
zipkin,HystrixPlugins.getInstance().registerConcurrencyStrategy(...),0.0,0.0,1.0,0.0
zipkin,... to register a custom  HystrixConcurrencyStrategy  that uses your own  Callable  ...,0.0,0.0,1.0,0.0
zipkin,... that applies context preservation around the circuit ...,0.0,0.0,1.0,0.0
zipkin,... via is a helper class capable of preserving the Zipkin context ...,0.0,0.333,0.667,0.6124
zipkin,... and allowing an easy method of adding other contexts you may wish to preserve e.g.,0.0,0.286,0.714,0.6808
zipkin,"MDC, SecurityContext etc ...",0.0,0.0,1.0,0.0
zipkin,This is a guess.,0.0,0.0,1.0,0.0
zipkin,Kafka has no concept of message headers (where spans are stored).,0.18,0.0,0.82,-0.296
zipkin,SCSt therefore has to embed message headers in the payload.,0.0,0.0,1.0,0.0
zipkin,The current version requires you to &quot;opt-in&quot; which headers you want transported in this way.,0.0,0.085,0.915,0.0772
zipkin,Documentation here .,0.0,0.0,1.0,0.0
zipkin,spring.cloud.stream.kafka.binder.headers,0.0,0.0,1.0,0.0
zipkin,The list of custom headers that will be transported by the binder.,0.0,0.0,1.0,0.0
zipkin,Default: empty.,0.643,0.0,0.357,-0.2023
zipkin,"Unfortunately, patterns are not currently supported, you have to list the headers individually.",0.284,0.0,0.716,-0.5207
zipkin,We are  considering adding support for patterns  and/or transporting all headers by default.,0.0,0.184,0.816,0.4019
zipkin,"Finally, I found 2 issues related with my applications.",0.0,0.0,1.0,0.0
zipkin,1.,0.0,0.0,1.0,0.0
zipkin,The application with @EnalbeZipkinStreamServer could not be traced.,0.0,0.0,1.0,0.0
zipkin,This looks like a by design.,0.0,0.385,0.615,0.3612
zipkin,2.,0.0,0.0,1.0,0.0
zipkin,"If kafka is used as the binder, the applications should specify the headers as the following:",0.0,0.0,1.0,0.0
zipkin,"I'm not sure this is what you are expecting, you can add this dependancy in  pom.xml  if you are using maven:",0.089,0.0,0.911,-0.2411
zipkin,and a  AlwaysSampler @Bean  in your  SpringBootApplication  class,0.0,0.0,1.0,0.0
zipkin,This will help you to sample your inputs in zipkin all time.,0.0,0.197,0.803,0.4019
zipkin,The best thing to do would be to show your sample project.,0.0,0.276,0.724,0.6369
zipkin,Another is to check if you don't have a custom logback.xml or any other type of logging configuration that breaks the current set up (most likely you do cause I can see that the pattern is different).,0.0,0.0,1.0,0.0
zipkin,So you have an ip and port of your app so that could give you a hint.,0.0,0.0,1.0,0.0
zipkin,"Also if you want a custom span of yours to have that information, then it's enough to add a custom tag to it.",0.0,0.061,0.939,0.0772
zipkin,"Actually you can always call the  tracer.addTag(""key"", ""value"")  to put the additional information that you need.",0.0,0.0,1.0,0.0
zipkin,"I am not sure about the  dcat  distribution itself, but your error may be because you have:",0.264,0.0,0.736,-0.6163
zipkin,"however when you specify  p  in the model, it is a three dimensional array  p[j,k,i].",0.0,0.0,1.0,0.0
zipkin,I think you need:,0.0,0.0,1.0,0.0
zipkin,"Note, the  i  is the last index for  p , and the two commas.",0.0,0.0,1.0,0.0
zipkin,Hope his helps...,0.0,0.592,0.408,0.4404
zipkin,You can use spring cloud sleuth.,0.0,0.0,1.0,0.0
zipkin,Please check the documentation for examples of using elk stack to harvest the logs.,0.0,0.15,0.85,0.3182
zipkin,"The zipkin server can be fetched as a standalone jar, you don't need to create your custom version",0.102,0.0,0.898,-0.2057
zipkin,It is indeed a cluster problem.,0.403,0.0,0.597,-0.4019
zipkin,There is a problem with the  __consumer_offsets  topic data of kafka.,0.231,0.0,0.769,-0.4019
zipkin,It is good to restart kafka after deleting.,0.0,0.293,0.707,0.4404
zipkin,I have no knowledge of it being impossible.,0.268,0.0,0.732,-0.296
zipkin,Maybe you should first try doing it and then asking a question?,0.0,0.0,1.0,0.0
zipkin,"Also, if for some reason it turns out you can't use it, then if you just google  zipkin scala  you'll see things like  https://github.com/lloydmeta/zipkin-futures  ,  https://github.com/bizreach/play-zipkin-tracing  etc.",0.0,0.091,0.909,0.3612
zipkin,You'll find all of it on GitHub.,0.0,0.0,1.0,0.0
zipkin,Spring Web annotations,0.0,0.0,1.0,0.0
zipkin,Spring framework annotations,0.0,0.0,1.0,0.0
zipkin,More spring framework annotations,0.0,0.0,1.0,0.0
zipkin,You can use Apache NiFi's built-in provenance capabilities to trace how a given flow went through the system.,0.0,0.0,1.0,0.0
zipkin,https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#data_provenance,0.0,0.0,1.0,0.0
