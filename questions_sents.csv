toolname,question_id,accepted_answer_id,answer_count,creation_date,is_answered,last_activity_date,last_edit_date,owner_id,owner_reputation,score,view_count,title,sentence
AppDynamics,13524580,13570380.0,5,"2012/11/23, 09:15:18",True,"2013/10/22, 16:10:28","2012/11/23, 09:31:33",974169.0,7456.0,6,7790,Will AppDynamics (Performance Monitoring Tool) slow down my Production Application?,Is it ok to deploy Performance Monitoring Tool - AppDynamics - in Production?
AppDynamics,13524580,13570380.0,5,"2012/11/23, 09:15:18",True,"2013/10/22, 16:10:28","2012/11/23, 09:31:33",974169.0,7456.0,6,7790,Will AppDynamics (Performance Monitoring Tool) slow down my Production Application?,The application to be monitored is a standard Java/J2EE Web Application.
AppDynamics,13524580,13570380.0,5,"2012/11/23, 09:15:18",True,"2013/10/22, 16:10:28","2012/11/23, 09:31:33",974169.0,7456.0,6,7790,Will AppDynamics (Performance Monitoring Tool) slow down my Production Application?,"I have never worked with AppDynamics, and my concern is that it may actually slow down my application."
AppDynamics,13524580,13570380.0,5,"2012/11/23, 09:15:18",True,"2013/10/22, 16:10:28","2012/11/23, 09:31:33",974169.0,7456.0,6,7790,Will AppDynamics (Performance Monitoring Tool) slow down my Production Application?,Has anyone used AppDynamics in Production?
AppDynamics,13524580,13570380.0,5,"2012/11/23, 09:15:18",True,"2013/10/22, 16:10:28","2012/11/23, 09:31:33",974169.0,7456.0,6,7790,Will AppDynamics (Performance Monitoring Tool) slow down my Production Application?,Or should it be used only in Test kind of enivornments.
AppDynamics,17412297,17416710.0,2,"2013/07/01, 22:35:38",True,"2013/07/02, 08:52:04","2013/07/02, 00:11:12",463061.0,5807.0,4,3650,AppDynamics or NewRelic kind of system - how does it work?,How do you build a AppDynamic or New Relic kind of system that collects performance metrics of your application including detailed call tree stats by merely installing a software on the servers where your application runs?
AppDynamics,17412297,17416710.0,2,"2013/07/01, 22:35:38",True,"2013/07/02, 08:52:04","2013/07/02, 00:11:12",463061.0,5807.0,4,3650,AppDynamics or NewRelic kind of system - how does it work?,Is it even possible to collect such metrics without compiling your apps with debug information?
AppDynamics,17412297,17416710.0,2,"2013/07/01, 22:35:38",True,"2013/07/02, 08:52:04","2013/07/02, 00:11:12",463061.0,5807.0,4,3650,AppDynamics or NewRelic kind of system - how does it work?,What are the performance trade offs to consider when building such a service?
AppDynamics,17412297,17416710.0,2,"2013/07/01, 22:35:38",True,"2013/07/02, 08:52:04","2013/07/02, 00:11:12",463061.0,5807.0,4,3650,AppDynamics or NewRelic kind of system - how does it work?,How do such software minimize the performance impact they themselves might be having on the application.
AppDynamics,31001196,31063347.0,1,"2015/06/23, 14:10:42",True,"2015/06/26, 04:13:54",nan,2281821.0,721.0,3,2289,Appdynamics implementation,"I'm trying to add Appdynamics into my application, I'm doing those steps:  https://docs.appdynamics.com/display/PRO40/Instrument+an+Android+Application#InstrumentanAndroidApplication-ToaddtheAppDynamicsAndroidagentrepositorytoyourproject  but after all I have error:"
AppDynamics,31001196,31063347.0,1,"2015/06/23, 14:10:42",True,"2015/06/26, 04:13:54",nan,2281821.0,721.0,3,2289,Appdynamics implementation,This is how my build.gradle (for all project) looks like:
AppDynamics,31001196,31063347.0,1,"2015/06/23, 14:10:42",True,"2015/06/26, 04:13:54",nan,2281821.0,721.0,3,2289,Appdynamics implementation,and build.gradle (from app module):
AppDynamics,31001196,31063347.0,1,"2015/06/23, 14:10:42",True,"2015/06/26, 04:13:54",nan,2281821.0,721.0,3,2289,Appdynamics implementation,and  adeum-maven-repo  paste into project.
AppDynamics,31001196,31063347.0,1,"2015/06/23, 14:10:42",True,"2015/06/26, 04:13:54",nan,2281821.0,721.0,3,2289,Appdynamics implementation,Any idea what am I doing wrong?
AppDynamics,56138542,nan,1,"2019/05/15, 00:01:10",False,"2019/11/11, 14:45:04","2019/05/15, 07:57:45",11500656.0,41.0,3,384,Appdynamics problems to install in jboss app server,"I followed the instructions on the website of appdynamic, but when I was following this step:"
AppDynamics,56138542,nan,1,"2019/05/15, 00:01:10",False,"2019/11/11, 14:45:04","2019/05/15, 07:57:45",11500656.0,41.0,3,384,Appdynamics problems to install in jboss app server,When I ran standalone.bat this error appears:
AppDynamics,16161856,nan,2,"2013/04/23, 08:46:12",True,"2013/08/21, 05:35:52",nan,2113898.0,175.0,2,2009,Problems installing appdynamics,I have installed Appdynamics lite on my server and it worked fine when I used to run my tomcat instance with root user.
AppDynamics,16161856,nan,2,"2013/04/23, 08:46:12",True,"2013/08/21, 05:35:52",nan,2113898.0,175.0,2,2009,Problems installing appdynamics,"But from the time I have created a new user ""Tomcat"" and start executing my apache tomcat with this user, I am not able to run appdynamics."
AppDynamics,16161856,nan,2,"2013/04/23, 08:46:12",True,"2013/08/21, 05:35:52",nan,2113898.0,175.0,2,2009,Problems installing appdynamics,"I have copied the javaagent at this location with all rights(read,write,execute) to tomcat ""/home/tomcat/profiler/AppServerLite""."
AppDynamics,16161856,nan,2,"2013/04/23, 08:46:12",True,"2013/08/21, 05:35:52",nan,2113898.0,175.0,2,2009,Problems installing appdynamics,It throws an exception as follows :
AppDynamics,25891322,26749167.0,1,"2014/09/17, 16:02:19",True,"2014/11/05, 05:23:23",nan,239823.0,2987.0,2,4850,What does stall mean in AppDynamics Pro?,I wonder what stall means in AppDynamics Pro.
AppDynamics,25891322,26749167.0,1,"2014/09/17, 16:02:19",True,"2014/11/05, 05:23:23",nan,239823.0,2987.0,2,4850,What does stall mean in AppDynamics Pro?,Can you give an example which Appdynamics Pro application name that situation as Stall?
AppDynamics,25891322,26749167.0,1,"2014/09/17, 16:02:19",True,"2014/11/05, 05:23:23",nan,239823.0,2987.0,2,4850,What does stall mean in AppDynamics Pro?,Thanks.
AppDynamics,29122786,nan,1,"2015/03/18, 15:00:06",False,"2015/03/18, 15:14:41",nan,4382794.0,505.0,2,699,Use wildcard to filter URL in AppDynamics,"I'm trying to filter on AppDynamics to get all the request to a particular REST URL, the REST URL is not fixed as long as in the URL"
AppDynamics,29122786,nan,1,"2015/03/18, 15:00:06",False,"2015/03/18, 15:14:41",nan,4382794.0,505.0,2,699,Use wildcard to filter URL in AppDynamics,/AppEngine/rest/evac/${id}/createNewActivity
AppDynamics,29122786,nan,1,"2015/03/18, 15:00:06",False,"2015/03/18, 15:14:41",nan,4382794.0,505.0,2,699,Use wildcard to filter URL in AppDynamics,"On the Transaction Snapshots you have the option to filter results, and in the filters you can filter by URL:"
AppDynamics,29122786,nan,1,"2015/03/18, 15:00:06",False,"2015/03/18, 15:14:41",nan,4382794.0,505.0,2,699,Use wildcard to filter URL in AppDynamics,"If I search for a concrete URL (with ${id} defined) I can search it, but I cannot find how to use a wildcar to find this URL with any ${id}."
AppDynamics,29122786,nan,1,"2015/03/18, 15:00:06",False,"2015/03/18, 15:14:41",nan,4382794.0,505.0,2,699,Use wildcard to filter URL in AppDynamics,I tried so far to use
AppDynamics,29122786,nan,1,"2015/03/18, 15:00:06",False,"2015/03/18, 15:14:41",nan,4382794.0,505.0,2,699,Use wildcard to filter URL in AppDynamics,With no results.
AppDynamics,29122786,nan,1,"2015/03/18, 15:00:06",False,"2015/03/18, 15:14:41",nan,4382794.0,505.0,2,699,Use wildcard to filter URL in AppDynamics,"The one with works just a bit is using /AppEngine/rest/evac/* which also retrieves other REST which start with the same URL, so I can export the results and filter outside AppDynamics."
AppDynamics,29122786,nan,1,"2015/03/18, 15:00:06",False,"2015/03/18, 15:14:41",nan,4382794.0,505.0,2,699,Use wildcard to filter URL in AppDynamics,But there is a way to use a wildcard so I can find the desired results in AppDynamics?
AppDynamics,33134494,33141659.0,1,"2015/10/14, 22:57:47",True,"2015/10/15, 09:52:42",nan,5409237.0,43.0,2,768,Monitoring Code/Method-level Statistics using AppDynamics,I am now working on Performance Testing of a Java Application that runs on GlassFish Server 4.1.
AppDynamics,33134494,33141659.0,1,"2015/10/14, 22:57:47",True,"2015/10/15, 09:52:42",nan,5409237.0,43.0,2,768,Monitoring Code/Method-level Statistics using AppDynamics,"After going through some statistics that I got from AppDynamics tool, I find that there is no possibility for me to drill down to code/method level issues."
AppDynamics,33134494,33141659.0,1,"2015/10/14, 22:57:47",True,"2015/10/15, 09:52:42",nan,5409237.0,43.0,2,768,Monitoring Code/Method-level Statistics using AppDynamics,"For example, I can see the time taken by each method or function using dotTrace or JProfiler but AppDynamics tool seems to skip all these features."
AppDynamics,33134494,33141659.0,1,"2015/10/14, 22:57:47",True,"2015/10/15, 09:52:42",nan,5409237.0,43.0,2,768,Monitoring Code/Method-level Statistics using AppDynamics,"I was also looking for a free solution, hence I choose AppDynamics."
AppDynamics,33134494,33141659.0,1,"2015/10/14, 22:57:47",True,"2015/10/15, 09:52:42",nan,5409237.0,43.0,2,768,Monitoring Code/Method-level Statistics using AppDynamics,Now I feel I am not on the right track.
AppDynamics,33134494,33141659.0,1,"2015/10/14, 22:57:47",True,"2015/10/15, 09:52:42",nan,5409237.0,43.0,2,768,Monitoring Code/Method-level Statistics using AppDynamics,Can someone let me know more about this tool if I am missing something or suggest any other quick and easy solution to this.
AppDynamics,33134494,33141659.0,1,"2015/10/14, 22:57:47",True,"2015/10/15, 09:52:42",nan,5409237.0,43.0,2,768,Monitoring Code/Method-level Statistics using AppDynamics,Is there a possibility that the monitors on GlassFish server 4.1 can do the same for no cost?
AppDynamics,41130233,42376943.0,1,"2016/12/13, 22:44:33",True,"2017/02/21, 22:06:15",nan,992280.0,207.0,2,3680,Comparision between AppDynamics and Application Insights,I am trying to a good comparison between AppDynamics and Application Insights in regard to Azure App Service.
AppDynamics,41130233,42376943.0,1,"2016/12/13, 22:44:33",True,"2017/02/21, 22:06:15",nan,992280.0,207.0,2,3680,Comparision between AppDynamics and Application Insights,"I tried to google around but couldn't find any good comparison, if someone can point me or summarize here."
AppDynamics,47278327,47278626.0,1,"2017/11/14, 07:30:53",True,"2017/11/14, 07:56:39",nan,1942143.0,972.0,2,568,AppDynamics with Azure Functions,Has anybody solved to monitor Azure functions using AppDynamics ?
AppDynamics,47278327,47278626.0,1,"2017/11/14, 07:30:53",True,"2017/11/14, 07:56:39",nan,1942143.0,972.0,2,568,AppDynamics with Azure Functions,I don't see any option to add a AppDynamics extension to the Azure functions app.
AppDynamics,48631700,48631903.0,2,"2018/02/05, 23:22:31",True,"2018/02/05, 23:53:27",nan,5175709.0,23597.0,2,310,Would you face any problems if you have two analytics (Crashlytics and AppDynamics) in your iOS codebase?,For a period of time we might want to have the two analytics together.
AppDynamics,48631700,48631903.0,2,"2018/02/05, 23:22:31",True,"2018/02/05, 23:53:27",nan,5175709.0,23597.0,2,310,Would you face any problems if you have two analytics (Crashlytics and AppDynamics) in your iOS codebase?,Could this be problematic?
AppDynamics,48631700,48631903.0,2,"2018/02/05, 23:22:31",True,"2018/02/05, 23:53:27",nan,5175709.0,23597.0,2,310,Would you face any problems if you have two analytics (Crashlytics and AppDynamics) in your iOS codebase?,Would it be degrade the speed?
AppDynamics,48631700,48631903.0,2,"2018/02/05, 23:22:31",True,"2018/02/05, 23:53:27",nan,5175709.0,23597.0,2,310,Would you face any problems if you have two analytics (Crashlytics and AppDynamics) in your iOS codebase?,Would there be any fight between who captures the crash log?
AppDynamics,48631700,48631903.0,2,"2018/02/05, 23:22:31",True,"2018/02/05, 23:53:27",nan,5175709.0,23597.0,2,310,Would you face any problems if you have two analytics (Crashlytics and AppDynamics) in your iOS codebase?,!
AppDynamics,52994676,53074197.0,1,"2018/10/25, 20:08:54",True,"2018/10/31, 01:18:54",nan,6317694.0,743.0,2,1089,Can AppDynamics work with a Prometheus backend?,Most popular logging and monitoring stacks like ELK stack or Time series DB-Grafana are designed to be integrated.
AppDynamics,52994676,53074197.0,1,"2018/10/25, 20:08:54",True,"2018/10/31, 01:18:54",nan,6317694.0,743.0,2,1089,Can AppDynamics work with a Prometheus backend?,"Can AppDynamics work with other samplers/DBs, in particular Prometheus?"
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,I have a problem with my health rule configuration.
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,All I want is to have health rule which will be checking if service is running or not.
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,I have two types of services:
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,IIS
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,Standalone services
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,The problem is that some services are recognized as critical due to health rule violation.
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,"For example, I have two exactly the same services on two hosts and the only difference is that one of them is in use not so often."
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,Due to lack of activity on this service appdynamics pointing me it as critical.
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,Most probably I have done something wrong.
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,Any ideas?
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,I'm struggling with it as additional task.
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,Tried appdynamics community website but nothing which could point me solution.
AppDynamics,53397807,nan,1,"2018/11/20, 18:52:01",False,"2019/11/11, 17:18:30",nan,1487752.0,364.0,2,158,AppDynamics - monitoring of services,Here's my health rule configuration :
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,I have a Spring application monitored by Appdynamics.
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,"This application has a Service Endpoint recognized by Appdynamics, named lets say:  /general/endpoint"
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,Now within the application there are multiple endpoints like:
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,Now back to Appdynamics: Within the Service Endpoints menu I can find my  /general/endpoint  and click on it.
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,"Then I see a table with the actual REST API calls, their respective execution time, the specific URL (like  /general/endpoint/do-something-1 ) and some more information."
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,If I am interested in monitoring the requests for a specific URL I can do the following:
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,"Now I see what I want, all the requests and their potential problems for a specific URL inside my application."
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,Now comes my actual question:
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,How can I achieve the same for Dashboards?
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,I was only able to create a dashboard with the  Calls per Minute  for the  /general/endpoint  but not for a specific URL like  /general/endpoint/do-something-1 .
AppDynamics,60461133,nan,0,"2020/02/29, 03:11:37",False,"2020/02/29, 03:11:37",nan,7940088.0,119.0,2,83,How to create a dashboard in Appdynamics based on specific REST URLs of one endpoint,Is there a way to apply the more fine grade filter for dashboards as well?
AppDynamics,65946794,66421315.0,2,"2021/01/29, 01:39:56",True,"2021/03/01, 13:29:48","2021/01/29, 02:44:02",2500390.0,343.0,2,92,Is there any OpenTelemetry exporter for Appdynamics?,I am doing some proof of concept to ingest traces and metrics to AppDynamics without installing Appdynamics agent.
AppDynamics,65946794,66421315.0,2,"2021/01/29, 01:39:56",True,"2021/03/01, 13:29:48","2021/01/29, 02:44:02",2500390.0,343.0,2,92,Is there any OpenTelemetry exporter for Appdynamics?,I have an application emitting prometheus metrics and traces.
AppDynamics,65946794,66421315.0,2,"2021/01/29, 01:39:56",True,"2021/03/01, 13:29:48","2021/01/29, 02:44:02",2500390.0,343.0,2,92,Is there any OpenTelemetry exporter for Appdynamics?,I could not find any Appdynamics documentation talking about opentelemetry Collector.
AppDynamics,65946794,66421315.0,2,"2021/01/29, 01:39:56",True,"2021/03/01, 13:29:48","2021/01/29, 02:44:02",2500390.0,343.0,2,92,Is there any OpenTelemetry exporter for Appdynamics?,I could not find exporter in  https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter  either.
AppDynamics,65946794,66421315.0,2,"2021/01/29, 01:39:56",True,"2021/03/01, 13:29:48","2021/01/29, 02:44:02",2500390.0,343.0,2,92,Is there any OpenTelemetry exporter for Appdynamics?,Can you please advise on how to use opencollector with Appdynamics?
AppDynamics,13182423,13182788.0,1,"2012/11/01, 19:30:48",True,"2012/11/01, 19:52:02",nan,32453.0,49920.0,1,570,how do you show a metric (graph on the dashboard) of appdynamics that shows different values &quot;per application&quot;?,"With appdynamics it's easy to setup metrics that ""aggregate"" data from different nodes."
AppDynamics,13182423,13182788.0,1,"2012/11/01, 19:30:48",True,"2012/11/01, 19:52:02",nan,32453.0,49920.0,1,570,how do you show a metric (graph on the dashboard) of appdynamics that shows different values &quot;per application&quot;?,"What if you want to aggregate some date, and some other data, show as distinct lines in the graph?"
AppDynamics,13182423,13182788.0,1,"2012/11/01, 19:30:48",True,"2012/11/01, 19:52:02",nan,32453.0,49920.0,1,570,how do you show a metric (graph on the dashboard) of appdynamics that shows different values &quot;per application&quot;?,Is that possible?
AppDynamics,24797525,nan,2,"2014/07/17, 10:36:42",False,"2020/01/01, 17:46:22",nan,3086014.0,3467.0,1,4037,How to set up Appdynamics Environment,I am trying to install Appdynamics APM tool.
AppDynamics,24797525,nan,2,"2014/07/17, 10:36:42",False,"2020/01/01, 17:46:22",nan,3086014.0,3467.0,1,4037,How to set up Appdynamics Environment,It has three components :
AppDynamics,24797525,nan,2,"2014/07/17, 10:36:42",False,"2020/01/01, 17:46:22",nan,3086014.0,3467.0,1,4037,How to set up Appdynamics Environment,I have few queries:
AppDynamics,29858961,30294344.0,2,"2015/04/25, 02:06:59",True,"2017/03/17, 10:57:47","2016/12/17, 13:09:03",4784244.0,205.0,1,5643,How to capture Heap Dumps with AppDynamics?,How can we capture heap dumps with the help of appdynamics?
AppDynamics,33210833,33780274.0,1,"2015/10/19, 12:21:37",True,"2015/11/18, 14:36:17",nan,2391950.0,149.0,1,544,How does AppDynamics (and programs alike) retrieve information,How does AppDynamics and similar problems retrieve data from apps ?
AppDynamics,33210833,33780274.0,1,"2015/10/19, 12:21:37",True,"2015/11/18, 14:36:17",nan,2391950.0,149.0,1,544,How does AppDynamics (and programs alike) retrieve information,"I read somewhere here on SO that it is based on bytecode injection, but is there some official or reliable source to this information ?"
AppDynamics,41933094,nan,1,"2017/01/30, 12:00:03",False,"2018/02/22, 11:54:27","2018/02/22, 11:54:27",726863.0,65563.0,1,457,Error while integrating AppDynamics: transformClassesWithMultidexlistForLiveDebug,I am trying to integrate apps performance monitoring tool with my Android Application by my gradle fails saying
AppDynamics,41933094,nan,1,"2017/01/30, 12:00:03",False,"2018/02/22, 11:54:27","2018/02/22, 11:54:27",726863.0,65563.0,1,457,Error while integrating AppDynamics: transformClassesWithMultidexlistForLiveDebug,Below is my gradle root gradle file
AppDynamics,41933094,nan,1,"2017/01/30, 12:00:03",False,"2018/02/22, 11:54:27","2018/02/22, 11:54:27",726863.0,65563.0,1,457,Error while integrating AppDynamics: transformClassesWithMultidexlistForLiveDebug,"And here is my App's gradle file,"
AppDynamics,41933094,nan,1,"2017/01/30, 12:00:03",False,"2018/02/22, 11:54:27","2018/02/22, 11:54:27",726863.0,65563.0,1,457,Error while integrating AppDynamics: transformClassesWithMultidexlistForLiveDebug,I am already having Multidex flag enabled still it gives me the problem while running the Application.
AppDynamics,41933094,nan,1,"2017/01/30, 12:00:03",False,"2018/02/22, 11:54:27","2018/02/22, 11:54:27",726863.0,65563.0,1,457,Error while integrating AppDynamics: transformClassesWithMultidexlistForLiveDebug,"And, also I have in my Application class"
AppDynamics,43315501,43318354.0,1,"2017/04/10, 08:23:01",True,"2017/04/10, 11:34:41",nan,3592502.0,175.0,1,668,Reports api in AppDynamics for generating custom report in csv or specific format,I would like to know which existing api in AppDynamics can we use to generate custom reports.
AppDynamics,43315501,43318354.0,1,"2017/04/10, 08:23:01",True,"2017/04/10, 11:34:41",nan,3592502.0,175.0,1,668,Reports api in AppDynamics for generating custom report in csv or specific format,"The use case is like, i am consolidating reports from multiple tools so,i would be using the API of app dynamics and doing a backend call to pull the data i need to display and putting it in a csv or excel.All of this will happen in an automated way, would like to know the api or any specific way to do the same in app dynamics."
AppDynamics,43539704,nan,0,"2017/04/21, 13:01:03",False,"2021/01/28, 19:08:28","2021/01/28, 19:08:28",2867900.0,13.0,1,160,AppDynamics TimeStamp Difference,How to find TimeStamp difference in App Dynamics Query Language (ADQL)?
AppDynamics,43539704,nan,0,"2017/04/21, 13:01:03",False,"2021/01/28, 19:08:28","2021/01/28, 19:08:28",2867900.0,13.0,1,160,AppDynamics TimeStamp Difference,I tried column1 - column2.
AppDynamics,43539704,nan,0,"2017/04/21, 13:01:03",False,"2021/01/28, 19:08:28","2021/01/28, 19:08:28",2867900.0,13.0,1,160,AppDynamics TimeStamp Difference,"Getting error message ""Operator  [SUBTRACTION]  not valid on field types  [DATETIME], [DATETIME] ."
AppDynamics,44283136,nan,2,"2017/05/31, 14:07:29",True,"2017/06/03, 01:27:31",nan,6891949.0,11.0,1,969,monitoring of java application in appdynamics,I have my java application up and running.
AppDynamics,44283136,nan,2,"2017/05/31, 14:07:29",True,"2017/06/03, 01:27:31",nan,6891949.0,11.0,1,969,monitoring of java application in appdynamics,I want to import that application in Appdynamics for monitoring.
AppDynamics,44283136,nan,2,"2017/05/31, 14:07:29",True,"2017/06/03, 01:27:31",nan,6891949.0,11.0,1,969,monitoring of java application in appdynamics,Can any one please suggest how to import java applications in appdynamics.
AppDynamics,47754411,nan,0,"2017/12/11, 15:50:34",False,"2017/12/14, 09:57:45","2017/12/14, 09:57:45",9083920.0,11.0,1,117,How to drill down the object tracking instances for standalone java applications using AppDynamics?,"Data is not loading from standalone java applications to agent controller, but only say java agent is connected successfully and waiting for data to load."
AppDynamics,47754411,nan,0,"2017/12/11, 15:50:34",False,"2017/12/14, 09:57:45","2017/12/14, 09:57:45",9083920.0,11.0,1,117,How to drill down the object tracking instances for standalone java applications using AppDynamics?,"In Autodetection and Object tracing dashboard tabs, not able to view the detail or drill down the object tracking instances for standalone Java applications."
AppDynamics,47754411,nan,0,"2017/12/11, 15:50:34",False,"2017/12/14, 09:57:45","2017/12/14, 09:57:45",9083920.0,11.0,1,117,How to drill down the object tracking instances for standalone java applications using AppDynamics?,Configured adding some of the fully qualified class names from the application.
AppDynamics,47754411,nan,0,"2017/12/11, 15:50:34",False,"2017/12/14, 09:57:45","2017/12/14, 09:57:45",9083920.0,11.0,1,117,How to drill down the object tracking instances for standalone java applications using AppDynamics?,"Please advice, do I need to do any other settings to get the drill down feature?"
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,We have cluster of instances whereas each instance has DropWizard metrics gatherer.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,We're also trying to leverage AppDynamics custom metrics and that works so that custom script hits DropWizard exposed endpoint (/metrics) and sends metrics of interest to AppDynamics Controller.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,AppDynamics has 2 cluster rollout strategies for how the metric is displayed in a whole application view (tier) - SUM and AVG.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,While this works well for stuff like counts (sum is used) and average processing times (avg is used) - we for now don't have any idea of how to aggregate each instance percentiles exposed by DropWizard - neither sum nor avg looks correct.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,Example:
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,sum  will give 1700 what of course isn't useful at all.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,avg  will give 600 - which isn't correct either - we're losing track of higher bound.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,If AppDynamics had MAX Cluster rollout - that would be more or less fair - still not correct though.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,But AppDynamics doesn't have that.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,We also understand that the only fully correct way of gathering cluster percentiles is to perform aggregation from all nodes at one place (e.g.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,"logstash, etc..) and not on each instance."
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,But for now that's what we have - just sending custom metrics periodically.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,It would be great if anyone suggests something regarding that.
AppDynamics,48622556,nan,0,"2018/02/05, 14:25:14",False,"2018/02/05, 14:25:14",nan,2401173.0,421.0,1,88,AppDynamics Custom DropWizard Percentiles Metrics Rollout,"Thanks in advance,"
AppDynamics,49004859,nan,3,"2018/02/27, 11:06:52",True,"2018/02/28, 17:45:13",nan,4647853.0,9111.0,1,91,Lambda expression support in Appdynamics 4.2 and 4.3,"I see that Appdynamics 4.2 claims to  support  Java 8 lambda instrumenting, but this support was  removed  in 4.3."
AppDynamics,49004859,nan,3,"2018/02/27, 11:06:52",True,"2018/02/28, 17:45:13",nan,4647853.0,9111.0,1,91,Lambda expression support in Appdynamics 4.2 and 4.3,I cannot find anything in  4.3 release notes  that mentions removing support for lambdas.
AppDynamics,49004859,nan,3,"2018/02/27, 11:06:52",True,"2018/02/28, 17:45:13",nan,4647853.0,9111.0,1,91,Lambda expression support in Appdynamics 4.2 and 4.3,What's happened?
AppDynamics,49004859,nan,3,"2018/02/27, 11:06:52",True,"2018/02/28, 17:45:13",nan,4647853.0,9111.0,1,91,Lambda expression support in Appdynamics 4.2 and 4.3,Is it somehow related to  JDK-8145964 ?
AppDynamics,50713335,nan,1,"2018/06/06, 09:17:15",True,"2018/06/06, 09:34:40",nan,9901471.0,19.0,1,160,Trial AppDynamics package fails to install,Getting below error during platform installation:
AppDynamics,50713335,nan,1,"2018/06/06, 09:17:15",True,"2018/06/06, 09:34:40",nan,9901471.0,19.0,1,160,Trial AppDynamics package fails to install,"""Required libaio package is not found."
AppDynamics,50713335,nan,1,"2018/06/06, 09:17:15",True,"2018/06/06, 09:34:40",nan,9901471.0,19.0,1,160,Trial AppDynamics package fails to install,"..."""
AppDynamics,50713335,nan,1,"2018/06/06, 09:17:15",True,"2018/06/06, 09:34:40",nan,9901471.0,19.0,1,160,Trial AppDynamics package fails to install,"However, above package is already installed:"
AppDynamics,50713335,nan,1,"2018/06/06, 09:17:15",True,"2018/06/06, 09:34:40",nan,9901471.0,19.0,1,160,Trial AppDynamics package fails to install,Here is output from the installation script:
AppDynamics,53537738,nan,1,"2018/11/29, 13:13:20",True,"2019/07/14, 21:58:47","2019/07/14, 21:58:47",7756971.0,1999.0,1,732,Appdynamics implementation on Kubernetes,I have read about the Appdynamics in Kubernetes but I got confused.
AppDynamics,53537738,nan,1,"2018/11/29, 13:13:20",True,"2019/07/14, 21:58:47","2019/07/14, 21:58:47",7756971.0,1999.0,1,732,Appdynamics implementation on Kubernetes,"The scenario is like I am having EC2 under which Kubernetes is running which is having POD and under 1 pod, multiple container is running."
AppDynamics,53537738,nan,1,"2018/11/29, 13:13:20",True,"2019/07/14, 21:58:47","2019/07/14, 21:58:47",7756971.0,1999.0,1,732,Appdynamics implementation on Kubernetes,Where I have to install machine-agent?
AppDynamics,53537738,nan,1,"2018/11/29, 13:13:20",True,"2019/07/14, 21:58:47","2019/07/14, 21:58:47",7756971.0,1999.0,1,732,Appdynamics implementation on Kubernetes,In EC2 or in daemon set?
AppDynamics,53537738,nan,1,"2018/11/29, 13:13:20",True,"2019/07/14, 21:58:47","2019/07/14, 21:58:47",7756971.0,1999.0,1,732,Appdynamics implementation on Kubernetes,And where I have to install app-agent?
AppDynamics,53537738,nan,1,"2018/11/29, 13:13:20",True,"2019/07/14, 21:58:47","2019/07/14, 21:58:47",7756971.0,1999.0,1,732,Appdynamics implementation on Kubernetes,do I have to add app-agent in each container Dockerfile?
AppDynamics,53537738,nan,1,"2018/11/29, 13:13:20",True,"2019/07/14, 21:58:47","2019/07/14, 21:58:47",7756971.0,1999.0,1,732,Appdynamics implementation on Kubernetes,"And lastly, what would be my hostName and uniqueHostId?"
AppDynamics,54046191,54083177.0,1,"2019/01/04, 23:18:34",True,"2019/01/08, 01:13:19",nan,4470907.0,177.0,1,289,How does AppDynamics agent instrument .NET applications internally,I explore AppDynamics and other APM solutions to choose right one for my company.
AppDynamics,54046191,54083177.0,1,"2019/01/04, 23:18:34",True,"2019/01/08, 01:13:19",nan,4470907.0,177.0,1,289,How does AppDynamics agent instrument .NET applications internally,I have created simple demo .NET application (WCF service and console client to consume it).
AppDynamics,54046191,54083177.0,1,"2019/01/04, 23:18:34",True,"2019/01/08, 01:13:19",nan,4470907.0,177.0,1,289,How does AppDynamics agent instrument .NET applications internally,Then I installed AppDynamics agent on machine and configure it for both client and service as for standalone applications:
AppDynamics,54046191,54083177.0,1,"2019/01/04, 23:18:34",True,"2019/01/08, 01:13:19",nan,4470907.0,177.0,1,289,How does AppDynamics agent instrument .NET applications internally,"When I start my client and service I see that AppD agent have ""injected"" code to my applications and write ""Running non-obfuscated client"" to the console"
AppDynamics,54046191,54083177.0,1,"2019/01/04, 23:18:34",True,"2019/01/08, 01:13:19",nan,4470907.0,177.0,1,289,How does AppDynamics agent instrument .NET applications internally,I want to understand what technics or methods AppDynamics agent use to instrument .NET applications without SDK and being just a separate process (service)?
AppDynamics,54046191,54083177.0,1,"2019/01/04, 23:18:34",True,"2019/01/08, 01:13:19",nan,4470907.0,177.0,1,289,How does AppDynamics agent instrument .NET applications internally,"How does it listen for incoming WCF calls of my service without being directly used by the service (it's not referenced as an assembly, even not mentioned in app.config)?"
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,Has anyone experienced Webpack dependency compiling issues when using the AppDynamics library?
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,And did you find a way to work around it?
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,I believe this is an issue stemming from their library.
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,"When trying to install the AppDynamics package for monitoring a Node.js/Express application, our Webpack build process is not able to import a handful of dependencies."
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,"Specifically, the errors output are:"
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,"Our project is set up with:
- Webpack v4.29.0
- Node.js v11.0.0
- Appdynamics v4.5"
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,The Appdynamics usage is at the top of our server file as:
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,And our Webpack configuration is:
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,"So far we have tried downgrading the Webpack version, downgrading the Node environment to 10.15, and using other import methods for the AppDynamics package, but this seems like an issue internal to the Appdynamics library?"
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,"The main question is, has anyone experienced Webpack dependency compiling issues when using the Appdynamics library?"
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,And did you find a way to work around it?
AppDynamics,55048208,55150379.0,1,"2019/03/07, 18:08:19",True,"2019/10/29, 18:27:37","2019/03/08, 18:13:09",2625630.0,41.0,1,705,AppDynamics (Node.js/Express) dependencies broken compiling with Webpack--can anyone confirm?,Any help or clues would be appreciated ❤️
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,I am trying to setup the rabbitmq machine agent for AppDynamics with a standalone RabbitMQ.
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,https://www.appdynamics.com/community/exchange/extension/rabbitmq-monitoring-extension/
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,itMQ Monitoring Plugin 2.0.2
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,A curl on the RabbitMQ API works fine
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,Celery Flower is talking to rabbit fine with the following config options
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,My rabbitMQ Monitoring plugin is configured like so in config.yml
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,"In my troubleshooting, I followed this guide to add /opt/ca/cacert.pem to a Java keystore."
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,https://github.com/MichalHecko/SSLPoke
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,I initialize the machine agent as follows
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,I am still getting the following error for every RabbitMQ api call by the monitor in machineagent-bundle-64bit-linux-4.5.14.2293/logs/machine-agent.log
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,What am I missing?
AppDynamics,58208802,58210199.0,1,"2019/10/02, 23:17:10",True,"2019/10/03, 01:37:31",nan,6268004.0,471.0,1,243,How to use SSL with AppDynamics RabbitMQ Monitoring Plugin,Thank you!
AppDynamics,60186796,nan,0,"2020/02/12, 12:58:02",False,"2020/02/12, 12:58:02",nan,12884772.0,11.0,1,36,Appdynamics instrumentation for Tibco - how to configure dynamically unique node name for each JVM,We are instrumenting java agents on Tibco.
AppDynamics,60186796,nan,0,"2020/02/12, 12:58:02",False,"2020/02/12, 12:58:02",nan,12884772.0,11.0,1,36,Appdynamics instrumentation for Tibco - how to configure dynamically unique node name for each JVM,There are few JVMs ob each server and we are trying to configure unique node name (since which ever JVM starts first we get data only for that JVM).
AppDynamics,60186796,nan,0,"2020/02/12, 12:58:02",False,"2020/02/12, 12:58:02",nan,12884772.0,11.0,1,36,Appdynamics instrumentation for Tibco - how to configure dynamically unique node name for each JVM,"We tried to add the following in the startup command in the tra file:
 -Dappdynamics.agent.tierName=%tibco.deployment% -Dappdynamics.agent.nodeName=$HOSTNAME.%tibco.deployment%"
AppDynamics,60186796,nan,0,"2020/02/12, 12:58:02",False,"2020/02/12, 12:58:02",nan,12884772.0,11.0,1,36,Appdynamics instrumentation for Tibco - how to configure dynamically unique node name for each JVM,which didn't work since $HOSTNAME was not translated.
AppDynamics,60186796,nan,0,"2020/02/12, 12:58:02",False,"2020/02/12, 12:58:02",nan,12884772.0,11.0,1,36,Appdynamics instrumentation for Tibco - how to configure dynamically unique node name for each JVM,We need to define this dynamically since at every tibco deployment the configuration is lost and if we indicate a specific node name we have to reconfiure every tra separately.
AppDynamics,60186796,nan,0,"2020/02/12, 12:58:02",False,"2020/02/12, 12:58:02",nan,12884772.0,11.0,1,36,Appdynamics instrumentation for Tibco - how to configure dynamically unique node name for each JVM,How can we get the hostname dynamically into the tra file so we won't have to redefine every JVM (and we have many) after every deployment ?
AppDynamics,60186796,nan,0,"2020/02/12, 12:58:02",False,"2020/02/12, 12:58:02",nan,12884772.0,11.0,1,36,Appdynamics instrumentation for Tibco - how to configure dynamically unique node name for each JVM,"Regards,
Yy"
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,Our existing Infra is hosted on private servers.
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,AppDynamics  is used for monitoring hundreds of application &amp; host performances.
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,As a move we are moving all our applications on Azure.
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,Is this possible to get away from AppDynamics &amp; use any Azure solutions  for the same purpose.
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,Possibly Azure App Insight/Monitor ?
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,?
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,We have tried Java Application Monitoring on  Azure App Insight ; Azure Monitor is useful there.
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,Also we have used LogAnalytics for creating various performance Dashboards on Azure Monitor.
AppDynamics,61316980,nan,0,"2020/04/20, 10:17:01",False,"2020/05/04, 12:37:11",nan,2955930.0,515.0,1,117,Can I replace AppDynamics with Azure App Insights,Can Application Insight support all the similar features of AppDynamics: Like Workflow Monitoring Performance Monitoring etc etc..
AppDynamics,61789932,nan,0,"2020/05/14, 08:27:54",False,"2020/05/14, 08:27:54",nan,8548631.0,111.0,1,91,Bundle React/Node.js docker container with Appdynamics using Webpack throws error,I have react app using webpack.
AppDynamics,61789932,nan,0,"2020/05/14, 08:27:54",False,"2020/05/14, 08:27:54",nan,8548631.0,111.0,1,91,Bundle React/Node.js docker container with Appdynamics using Webpack throws error,Building a docker image out of it is failing.
AppDynamics,61789932,nan,0,"2020/05/14, 08:27:54",False,"2020/05/14, 08:27:54",nan,8548631.0,111.0,1,91,Bundle React/Node.js docker container with Appdynamics using Webpack throws error,It is failing because appdynamics package.
AppDynamics,61789932,nan,0,"2020/05/14, 08:27:54",False,"2020/05/14, 08:27:54",nan,8548631.0,111.0,1,91,Bundle React/Node.js docker container with Appdynamics using Webpack throws error,I am getting the error only during docker build and npm run build seems to work fine without any issues.
AppDynamics,61789932,nan,0,"2020/05/14, 08:27:54",False,"2020/05/14, 08:27:54",nan,8548631.0,111.0,1,91,Bundle React/Node.js docker container with Appdynamics using Webpack throws error,"Environment:
React 16
Node 13
appdynamics var 4.5.20
webpack 4
Docker"
AppDynamics,61789932,nan,0,"2020/05/14, 08:27:54",False,"2020/05/14, 08:27:54",nan,8548631.0,111.0,1,91,Bundle React/Node.js docker container with Appdynamics using Webpack throws error,package.json
AppDynamics,61789932,nan,0,"2020/05/14, 08:27:54",False,"2020/05/14, 08:27:54",nan,8548631.0,111.0,1,91,Bundle React/Node.js docker container with Appdynamics using Webpack throws error,webpack.config.js
AppDynamics,61789932,nan,0,"2020/05/14, 08:27:54",False,"2020/05/14, 08:27:54",nan,8548631.0,111.0,1,91,Bundle React/Node.js docker container with Appdynamics using Webpack throws error,ERROR
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,"Im trying to monitor a page availability with Appdynamics 
I have an IIS server with one site and several applications."
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,Appdynamics .Net agent 20.4.1 installed on the monitored server
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,each application has a appName.svc web page that I can call to check if the service is up.
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,Im trying  AppDynamics Extension for URL Monitoring  and followed the installation instructions.
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,"I can see in  Metric browser  the URL monitor section, under that I see 'Metric Uploaded'."
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,where do I see indication that a URl is down/up?
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,"can I monitor multiple URLs, as i did in yml file?"
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,config.yml  file section looks like this:
AppDynamics,63647198,nan,1,"2020/08/29, 15:56:40",False,"2021/02/08, 13:51:32",nan,4205113.0,43.0,1,209,Appdynamics monitor URL/page availability,log:
AppDynamics,13198172,13198174.0,1,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",nan,32453.0,49920.0,0,1835,how do you delete a metric in appdynamics?,"With the Analyze -  Metric browser in AppDynamics, you can go in and look at all the various metrics in the tree, but there's no right-click ""edit"" option."
AppDynamics,13198172,13198174.0,1,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",nan,32453.0,49920.0,0,1835,how do you delete a metric in appdynamics?,How do you modify/delete a metric?
AppDynamics,18895293,nan,1,"2013/09/19, 15:59:03",False,"2013/09/20, 13:44:18",nan,2795488.0,3.0,0,362,Monitoring wso2 carbon using Appdynamics,HI I'm wondering if anyone has any thoughts on using  Appdynamics  to monitor WSO2.
AppDynamics,18895293,nan,1,"2013/09/19, 15:59:03",False,"2013/09/20, 13:44:18",nan,2795488.0,3.0,0,362,Monitoring wso2 carbon using Appdynamics,"Out of the box appdynamics detects the servlet request coming in and that it gets written to the database, but beyond that it loses track of the transaction."
AppDynamics,18895293,nan,1,"2013/09/19, 15:59:03",False,"2013/09/20, 13:44:18",nan,2795488.0,3.0,0,362,Monitoring wso2 carbon using Appdynamics,"so if anyone could give some help as to what other classes I should instrument, It would be a real help."
AppDynamics,18895293,nan,1,"2013/09/19, 15:59:03",False,"2013/09/20, 13:44:18",nan,2795488.0,3.0,0,362,Monitoring wso2 carbon using Appdynamics,thanks
AppDynamics,18895293,nan,1,"2013/09/19, 15:59:03",False,"2013/09/20, 13:44:18",nan,2795488.0,3.0,0,362,Monitoring wso2 carbon using Appdynamics,Sunil Vanmullem
AppDynamics,19518737,nan,3,"2013/10/22, 16:05:25",False,"2013/10/26, 00:32:59","2013/10/23, 11:13:37",2508411.0,3374.0,0,1775,Did someone manage to monitor windows services with appdynamics?,Normally newer verisons of Appdynamics should display windows services if you add them specificially into the config.xml.
AppDynamics,19518737,nan,3,"2013/10/22, 16:05:25",False,"2013/10/26, 00:32:59","2013/10/23, 11:13:37",2508411.0,3374.0,0,1775,Did someone manage to monitor windows services with appdynamics?,"I did that, restartet the services and the agent, but nothing happened."
AppDynamics,19518737,nan,3,"2013/10/22, 16:05:25",False,"2013/10/26, 00:32:59","2013/10/23, 11:13:37",2508411.0,3374.0,0,1775,Did someone manage to monitor windows services with appdynamics?,Did anyone manage to display the Services ?
AppDynamics,19518737,nan,3,"2013/10/22, 16:05:25",False,"2013/10/26, 00:32:59","2013/10/23, 11:13:37",2508411.0,3374.0,0,1775,Did someone manage to monitor windows services with appdynamics?,"If yes, where do they appear?"
AppDynamics,21576957,nan,0,"2014/02/05, 14:12:41",False,"2015/10/05, 20:46:52","2015/10/05, 20:46:52",1932034.0,149.0,0,644,AppDynamics doesn&#39;t monitor calls from Hystrix app,I develop common java web app.
AppDynamics,21576957,nan,0,"2014/02/05, 14:12:41",False,"2015/10/05, 20:46:52","2015/10/05, 20:46:52",1932034.0,149.0,0,644,AppDynamics doesn&#39;t monitor calls from Hystrix app,"I use  Hystrix  in my app, actually I have a  REST client  whose methods wrapped in  hystrix  commands."
AppDynamics,21576957,nan,0,"2014/02/05, 14:12:41",False,"2015/10/05, 20:46:52","2015/10/05, 20:46:52",1932034.0,149.0,0,644,AppDynamics doesn&#39;t monitor calls from Hystrix app,My web app uses this rest client to communicate with remote server.
AppDynamics,21576957,nan,0,"2014/02/05, 14:12:41",False,"2015/10/05, 20:46:52","2015/10/05, 20:46:52",1932034.0,149.0,0,644,AppDynamics doesn&#39;t monitor calls from Hystrix app,My web app is configured as described in  hystrix   wiki  (it's needed to calculate statistics for  hystrix  dashboard).
AppDynamics,21576957,nan,0,"2014/02/05, 14:12:41",False,"2015/10/05, 20:46:52","2015/10/05, 20:46:52",1932034.0,149.0,0,644,AppDynamics doesn&#39;t monitor calls from Hystrix app,"To monitor my web app I use  AppDynamics  tool, but after I started using  rest client  based on  Hystrix  all calls from my web app aren't being displayed in  AppDynamics ."
AppDynamics,21576957,nan,0,"2014/02/05, 14:12:41",False,"2015/10/05, 20:46:52","2015/10/05, 20:46:52",1932034.0,149.0,0,644,AppDynamics doesn&#39;t monitor calls from Hystrix app,When I switched implementation to client without Hystrix everything is working well as expected.
AppDynamics,21576957,nan,0,"2014/02/05, 14:12:41",False,"2015/10/05, 20:46:52","2015/10/05, 20:46:52",1932034.0,149.0,0,644,AppDynamics doesn&#39;t monitor calls from Hystrix app,Maybe somebody knows what is the problem?
AppDynamics,21576957,nan,0,"2014/02/05, 14:12:41",False,"2015/10/05, 20:46:52","2015/10/05, 20:46:52",1932034.0,149.0,0,644,AppDynamics doesn&#39;t monitor calls from Hystrix app,Thanks.
AppDynamics,28692916,nan,1,"2015/02/24, 12:08:04",False,"2015/06/13, 13:27:31","2015/06/13, 13:27:31",1587232.0,283.0,0,342,How to configure AppDynamics to detect Liferay properly?,I want to analyse my Liferay Server with AppDynamics.
AppDynamics,28692916,nan,1,"2015/02/24, 12:08:04",False,"2015/06/13, 13:27:31","2015/06/13, 13:27:31",1587232.0,283.0,0,342,How to configure AppDynamics to detect Liferay properly?,Is there special configuration to better analyse Liferay specific things or is the only way to check the JSP related execution and not the internal service calls from the JSPs?
AppDynamics,28692916,nan,1,"2015/02/24, 12:08:04",False,"2015/06/13, 13:27:31","2015/06/13, 13:27:31",1587232.0,283.0,0,342,How to configure AppDynamics to detect Liferay properly?,System: Liferay 6.2 on Tomcat with a MySQL Database
AppDynamics,30706344,nan,1,"2015/06/08, 13:07:33",False,"2015/06/09, 13:57:33",nan,28841.0,10847.0,0,541,Appdynamics configuration to detect Java Elasticsearch client calls,I am trying to find suitable entry point for ES client.
AppDynamics,30706344,nan,1,"2015/06/08, 13:07:33",False,"2015/06/09, 13:57:33",nan,28841.0,10847.0,0,541,Appdynamics configuration to detect Java Elasticsearch client calls,At the moment I have:
AppDynamics,30706344,nan,1,"2015/06/08, 13:07:33",False,"2015/06/09, 13:57:33",nan,28841.0,10847.0,0,541,Appdynamics configuration to detect Java Elasticsearch client calls,Class that implements an Interface which equals org.elasticsearch.client.ElasticsearchClient
AppDynamics,30706344,nan,1,"2015/06/08, 13:07:33",False,"2015/06/09, 13:57:33",nan,28841.0,10847.0,0,541,Appdynamics configuration to detect Java Elasticsearch client calls,and method name: prepareSearch
AppDynamics,30706344,nan,1,"2015/06/08, 13:07:33",False,"2015/06/09, 13:57:33",nan,28841.0,10847.0,0,541,Appdynamics configuration to detect Java Elasticsearch client calls,It seems to collect number of calls but I wonder if there is a better configuration to make ES calls to show up in Tier Flow Map.
AppDynamics,31219797,31309166.0,2,"2015/07/04, 13:57:22",True,"2015/07/09, 08:49:40",nan,2384622.0,117.0,0,188,Could not load type when using servicestack and AppDynamics monitor,"When having the AppDynamics performance monitor installed, the servicestack API fails to load with the following exception:"
AppDynamics,31219797,31309166.0,2,"2015/07/04, 13:57:22",True,"2015/07/09, 08:49:40",nan,2384622.0,117.0,0,188,Could not load type when using servicestack and AppDynamics monitor,"Could not load type 'd__38' from assembly '###, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null'."
AppDynamics,31219797,31309166.0,2,"2015/07/04, 13:57:22",True,"2015/07/09, 08:49:40",nan,2384622.0,117.0,0,188,Could not load type when using servicestack and AppDynamics monitor,"StackTrace:
   at ###.BaseService 1.&lt;Any&gt;d__38.MoveNext() in ###\Services\BaseService.cs:line 190
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder 1.Start[TStateMachine](TStateMachine&amp; stateMachine)
   at ###.BaseService 1.Any(T request)
   at ServiceStack.Host.ServiceRunner 1.Execute(IRequest request, Object instance, TRequest requestDto)"
AppDynamics,31219797,31309166.0,2,"2015/07/04, 13:57:22",True,"2015/07/09, 08:49:40",nan,2384622.0,117.0,0,188,Could not load type when using servicestack and AppDynamics monitor,Any help is greatly appreciated.
AppDynamics,31219797,31309166.0,2,"2015/07/04, 13:57:22",True,"2015/07/09, 08:49:40",nan,2384622.0,117.0,0,188,Could not load type when using servicestack and AppDynamics monitor,Thank you
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,I want to execute the below command through Java code.
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,This is to create connection to my Appdynamics Contoller
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,curl --user user@customer1:password ' http://192.168.1.11:9090/controller/rest/applications?output=JSON '
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,The java code that I have written for this is
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,I keep on getting below error
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,"HTTP/1.1 401 Unauthorized
Error report  HTTP Status 401 -    type  Status report"
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,message
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,description This request requires HTTP authentication ().
AppDynamics,36596678,nan,0,"2016/04/13, 14:15:36",False,"2016/04/13, 16:43:38","2016/04/13, 16:43:38",1999130.0,233.0,0,366,Fetch Appdynamics data in Java,Can anyone please help.
AppDynamics,38454851,nan,1,"2016/07/19, 12:41:46",False,"2016/07/19, 15:55:16",nan,6607077.0,1.0,0,55,Does Appdynamics provide any APIs for creating project/job and user,I Have tried to find if Appdynamics provides any API but I could not find any.
AppDynamics,38454851,nan,1,"2016/07/19, 12:41:46",False,"2016/07/19, 15:55:16",nan,6607077.0,1.0,0,55,Does Appdynamics provide any APIs for creating project/job and user,"So I want to know if there are any APIs available for creating users and project/job in Appdynamics, so that I can automate the process using Python scripts."
AppDynamics,39540605,nan,1,"2016/09/17, 00:53:57",True,"2016/10/06, 16:34:01",nan,514150.0,2426.0,0,1190,Any need for the ELK-stack or Graphite when using Appdynamics?,We are in the process of building a new server infrastructure and will be using Appdynamics for analytics of the Java applications.
AppDynamics,39540605,nan,1,"2016/09/17, 00:53:57",True,"2016/10/06, 16:34:01",nan,514150.0,2426.0,0,1190,Any need for the ELK-stack or Graphite when using Appdynamics?,"Appdynamics has a lot of features, so it seems that server metrics via collectd to Graphite will no longer be necessary."
AppDynamics,39540605,nan,1,"2016/09/17, 00:53:57",True,"2016/10/06, 16:34:01",nan,514150.0,2426.0,0,1190,Any need for the ELK-stack or Graphite when using Appdynamics?,Application metrics can also be fed straight into Appdynamics.
AppDynamics,39540605,nan,1,"2016/09/17, 00:53:57",True,"2016/10/06, 16:34:01",nan,514150.0,2426.0,0,1190,Any need for the ELK-stack or Graphite when using Appdynamics?,"How about Logstash, ElasticSearch and Kibana and centralised logging."
AppDynamics,39540605,nan,1,"2016/09/17, 00:53:57",True,"2016/10/06, 16:34:01",nan,514150.0,2426.0,0,1190,Any need for the ELK-stack or Graphite when using Appdynamics?,Is there still any reason to build an ELK stack for the Java developers when they can use Appdynamics?
AppDynamics,39652512,39715544.0,1,"2016/09/23, 07:13:08",True,"2016/09/27, 06:32:35",nan,5006681.0,506.0,0,410,AppDynamics URL Monitor Extension Sometimes Doesn&#39;t Send Metrics,I have installed AppDynamics's Java Machine Agent along with the URL Monitoring Extension.
AppDynamics,39652512,39715544.0,1,"2016/09/23, 07:13:08",True,"2016/09/27, 06:32:35",nan,5006681.0,506.0,0,410,AppDynamics URL Monitor Extension Sometimes Doesn&#39;t Send Metrics,"Every day, for 1 or 2 hours, the metrics are not appearing on my metric browser."
AppDynamics,39652512,39715544.0,1,"2016/09/23, 07:13:08",True,"2016/09/27, 06:32:35",nan,5006681.0,506.0,0,410,AppDynamics URL Monitor Extension Sometimes Doesn&#39;t Send Metrics,"I checked the logs corresponding to those time periods, and I see that the HTTP Requests are being made and are getting back HTTP 200 OK responses."
AppDynamics,39652512,39715544.0,1,"2016/09/23, 07:13:08",True,"2016/09/27, 06:32:35",nan,5006681.0,506.0,0,410,AppDynamics URL Monitor Extension Sometimes Doesn&#39;t Send Metrics,"My assumption is that the extension is not sending over the metrics, but I am unable to understand the cause of it."
AppDynamics,39652512,39715544.0,1,"2016/09/23, 07:13:08",True,"2016/09/27, 06:32:35",nan,5006681.0,506.0,0,410,AppDynamics URL Monitor Extension Sometimes Doesn&#39;t Send Metrics,Can anyone point me into the right direction?
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,I was not able to find any information regarding configuration of  AppDynamics  agent for  JUnit  tests.
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,I would like to test performance of Hibernate queries of  Spring  based web service backed by  PostgreSQL  database.
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,Tests must be able to rollback the data at the termination.
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,Should it be unit or integration tests?
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,What is the best way to accomplish it?
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,How to make  AppDynamics  collect and display graphs of query execution times?
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,UPDATE:
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,I was not able to set up addDynamics agent for JUnit tests inside IDEA.
AppDynamics,40452664,nan,2,"2016/11/06, 20:01:11",False,"2020/10/09, 16:13:06","2016/11/07, 11:33:16",480632.0,12009.0,0,260,How to configure AppDynamics to measure execution times of Hibernate queries?,"The VM arguments is pointing to agent  -javaagent:""C:\Tools\AppDynamicsAgent\javaagent.jar"" , the firewall is off but for some reason in appdynamics web based (SaaS) set up dialog shows that no agent able to connect:"
AppDynamics,41755184,41757511.0,1,"2017/01/20, 04:37:17",True,"2017/01/20, 08:38:17","2017/01/20, 08:20:51",4039431.0,1.0,0,165,Appdynamics : Understand and improving the performance bottleneck identified by Appdynamics(AppD),"I'm using AppD as APM for my application and in slow transaction reports it shows most of the calls, which is not our application code and we are calling open source libraries method."
AppDynamics,41755184,41757511.0,1,"2017/01/20, 04:37:17",True,"2017/01/20, 08:38:17","2017/01/20, 08:20:51",4039431.0,1.0,0,165,Appdynamics : Understand and improving the performance bottleneck identified by Appdynamics(AppD),For example :
AppDynamics,41755184,41757511.0,1,"2017/01/20, 04:37:17",True,"2017/01/20, 08:38:17","2017/01/20, 08:20:51",4039431.0,1.0,0,165,Appdynamics : Understand and improving the performance bottleneck identified by Appdynamics(AppD),com.google.common.reflect.TypeVisitor.visit  method of google library takes almost 155 ms time and  com.google.common.reflect.TypeToken.equals()  method takes almost 60 ms. and  org.apache.tapestry5.internal.services.RenderQueueImpl.render()  takes almost 50 ms.
AppDynamics,41755184,41757511.0,1,"2017/01/20, 04:37:17",True,"2017/01/20, 08:38:17","2017/01/20, 08:20:51",4039431.0,1.0,0,165,Appdynamics : Understand and improving the performance bottleneck identified by Appdynamics(AppD),I want to highlight that  I've checked and my server is not loaded and both CPU and memory usage is very low as well this time taken is for very small amount of data processing .
AppDynamics,41755184,41757511.0,1,"2017/01/20, 04:37:17",True,"2017/01/20, 08:38:17","2017/01/20, 08:20:51",4039431.0,1.0,0,165,Appdynamics : Understand and improving the performance bottleneck identified by Appdynamics(AppD),Let me know the reason behind this and how can I optimize the performance of my application.
AppDynamics,41764607,nan,2,"2017/01/20, 15:15:06",True,"2017/03/24, 13:14:03",nan,7446427.0,1.0,0,767,how to monitor server using Appdynamics?,I have an application that is generating 3 kind of log files
AppDynamics,41764607,nan,2,"2017/01/20, 15:15:06",True,"2017/03/24, 13:14:03",nan,7446427.0,1.0,0,767,how to monitor server using Appdynamics?,"and I want to analyse the performance of my server using appdynamics so what kind of data my logs should be generating to generate analytics for server health, performance, throughput, server utilization?"
AppDynamics,42274447,nan,1,"2017/02/16, 14:48:43",True,"2017/03/27, 10:58:07",nan,4242765.0,588.0,0,104,RMI VS AppDynamics,JMX used for monitoring and managing the services/components &amp; devices.
AppDynamics,42274447,nan,1,"2017/02/16, 14:48:43",True,"2017/03/27, 10:58:07",nan,4242765.0,588.0,0,104,RMI VS AppDynamics,"My question is about monitoring,for the monitoring purpose do we have to change any code if we use JMX."
AppDynamics,42274447,nan,1,"2017/02/16, 14:48:43",True,"2017/03/27, 10:58:07",nan,4242765.0,588.0,0,104,RMI VS AppDynamics,"If that is the case, App dynamics will solve this process without doing single line of code change ?"
AppDynamics,42361990,42996563.0,1,"2017/02/21, 10:19:21",True,"2017/06/07, 09:23:41",nan,7042155.0,169.0,0,293,AppDynamics Dashboards on the TV display,"I haven't found anywhere an answer, so I decided to write here."
AppDynamics,42361990,42996563.0,1,"2017/02/21, 10:19:21",True,"2017/06/07, 09:23:41",nan,7042155.0,169.0,0,293,AppDynamics Dashboards on the TV display,Is it possible to display AppDynamics Dashboards on the TV display?
AppDynamics,42361990,42996563.0,1,"2017/02/21, 10:19:21",True,"2017/06/07, 09:23:41",nan,7042155.0,169.0,0,293,AppDynamics Dashboards on the TV display,"Currently I'm using something like  GRUNT  (gruntjs.com), but nowhere can I find whether it is feasible with that?"
AppDynamics,42361990,42996563.0,1,"2017/02/21, 10:19:21",True,"2017/06/07, 09:23:41",nan,7042155.0,169.0,0,293,AppDynamics Dashboards on the TV display,"Currently I'm using GRUNT for displaying tasks from Jenkins, but I don't know how to configure it with AppDynamics."
AppDynamics,42361990,42996563.0,1,"2017/02/21, 10:19:21",True,"2017/06/07, 09:23:41",nan,7042155.0,169.0,0,293,AppDynamics Dashboards on the TV display,"Regards,
Kamil"
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,I am trying to setup the AppDynamics java agent.
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,I am facing issues in loading java agent in the JVM.
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,I try to add the below argument to the start.bat jvm options.
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,-javaagent:C:\javaagent.jar
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,"However, the aem do not start after this."
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,I have kept AppMachineAgent folder in the same drive as the AEM installation.
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,"However, javaagent.jar is not kept in the bin folder of the AEM."
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,Do I need to keep it in the bin folder?
AppDynamics,42719255,nan,1,"2017/03/10, 15:12:23",False,"2018/10/18, 01:37:57",nan,1667824.0,323.0,0,917,Setting up AppDynamics with Adobe Experience Manager AEM,Any suggested steps I am missing?
AppDynamics,42950752,42982843.0,1,"2017/03/22, 13:47:22",True,"2017/03/23, 19:21:05",nan,4296092.0,171.0,0,137,How to configure separate transaction threshold for each application in AppDynamics controller?,I am having multiple java application configured in my app-dynamics controller and they have their own java agent running and reporting the metrics.
AppDynamics,42950752,42982843.0,1,"2017/03/22, 13:47:22",True,"2017/03/23, 19:21:05",nan,4296092.0,171.0,0,137,How to configure separate transaction threshold for each application in AppDynamics controller?,My problem is that SLA for each application is different and if i change the slow transaction threshold for a single application.
AppDynamics,42950752,42982843.0,1,"2017/03/22, 13:47:22",True,"2017/03/23, 19:21:05",nan,4296092.0,171.0,0,137,How to configure separate transaction threshold for each application in AppDynamics controller?,"it changes it for other application as well, which is creating lot of trouble for me."
AppDynamics,42950752,42982843.0,1,"2017/03/22, 13:47:22",True,"2017/03/23, 19:21:05",nan,4296092.0,171.0,0,137,How to configure separate transaction threshold for each application in AppDynamics controller?,So my question is how to configure separate transaction threshold for each application in AppDynamics controller  ?
AppDynamics,43496426,nan,1,"2017/04/19, 15:56:24",False,"2017/04/21, 09:47:35",nan,4926141.0,267.0,0,183,AD authentication for AppDynamics using Rest/python SDK,I am able to authenticate and fetch details using local user account using the python SDK of App Dynamics is there a way to authenticate using AD from python API or using the REST/curl.
AppDynamics,44269450,44338324.0,1,"2017/05/30, 21:31:09",True,"2017/06/03, 01:35:15",nan,649224.0,103.0,0,163,Creating a stacked area graph in AppDynamics dashboard,Is it possible to create a stacked area graph in AppDynamics?
AppDynamics,44269450,44338324.0,1,"2017/05/30, 21:31:09",True,"2017/06/03, 01:35:15",nan,649224.0,103.0,0,163,Creating a stacked area graph in AppDynamics dashboard,I want to show the cumulative effects of API response and browser DOM ready to visualize where variance is originating.
AppDynamics,44269450,44338324.0,1,"2017/05/30, 21:31:09",True,"2017/06/03, 01:35:15",nan,649224.0,103.0,0,163,Creating a stacked area graph in AppDynamics dashboard,"I can put both of these on a single graph, but if I choose Area, they overlap."
AppDynamics,44269450,44338324.0,1,"2017/05/30, 21:31:09",True,"2017/06/03, 01:35:15",nan,649224.0,103.0,0,163,Creating a stacked area graph in AppDynamics dashboard,How do I get them to stack?
AppDynamics,44269450,44338324.0,1,"2017/05/30, 21:31:09",True,"2017/06/03, 01:35:15",nan,649224.0,103.0,0,163,Creating a stacked area graph in AppDynamics dashboard,"I'm on AppDynamics Version 4.3.1.2, build 47"
AppDynamics,44391470,44411981.0,1,"2017/06/06, 16:33:25",True,"2017/06/07, 14:55:46","2017/06/06, 17:04:34",1172579.0,545.0,0,624,AppDynamics database agent 4.3.10 error on SQL Server 2005,"I just testing AppDynamics for my database, I am able get it work on MySQL 5 and SQL Server 2014, but I got a JDBC error on SQL Server 2005."
AppDynamics,44391470,44411981.0,1,"2017/06/06, 16:33:25",True,"2017/06/07, 14:55:46","2017/06/06, 17:04:34",1172579.0,545.0,0,624,AppDynamics database agent 4.3.10 error on SQL Server 2005,Here is the error log:
AppDynamics,44391470,44411981.0,1,"2017/06/06, 16:33:25",True,"2017/06/07, 14:55:46","2017/06/06, 17:04:34",1172579.0,545.0,0,624,AppDynamics database agent 4.3.10 error on SQL Server 2005,"06 6月 2017 00:55:59,461 ERROR [AD Thread Pool-Global0] DBAgentPollingForUpdate:30 - Fatal transport error while connecting to URL [/controller/instance/DBAGENT_MACHINE_ID/db-monit
  or-config/37784]: org.apache.http.NoHttpResponseException: davinci2017060100542331.saas.appdynamics.com:443 failed to respond
  06 6月 2017 00:55:59,473  WARN [AD Thread Pool-Global0] DBAgentPollingForUpdate:62 - Invalid response for configuration request from controller/could not connect."
AppDynamics,44391470,44411981.0,1,"2017/06/06, 16:33:25",True,"2017/06/07, 14:55:46","2017/06/06, 17:04:34",1172579.0,545.0,0,624,AppDynamics database agent 4.3.10 error on SQL Server 2005,"Msg: Fatal transp
  ort error while connecting to URL [/controller/instance/DBAGENT_MACHINE_ID/db-monitor-config/37784]
  06 6月 2017 00:56:00,026  INFO [-Scheduler-3] ADBCollector:141 - DB Collector DBSERVER01 is temporarily disabled."
AppDynamics,44391470,44411981.0,1,"2017/06/06, 16:33:25",True,"2017/06/07, 14:55:46","2017/06/06, 17:04:34",1172579.0,545.0,0,624,AppDynamics database agent 4.3.10 error on SQL Server 2005,"06 6月 2017 00:56:01,026  INFO [-Scheduler-3] ARelationalDBCollector:59 - (Re)initialize the DB collector 'DBSERVER01'."
AppDynamics,44391470,44411981.0,1,"2017/06/06, 16:33:25",True,"2017/06/07, 14:55:46","2017/06/06, 17:04:34",1172579.0,545.0,0,624,AppDynamics database agent 4.3.10 error on SQL Server 2005,"06 6月 2017 00:56:01,040  INFO [-Scheduler-3] MSSqlCollector:74 - Obtained connection for url jdbc:sqlserver://192.168.1.100:1433
  06 6月 2017 00:56:01,047  INFO [-Scheduler-3] MSSqlCollector:139 - SQL Server Version = 9.00.5057.00 ( 2005.0 )
  06 6月 2017 00:57:00,025 ERROR [-Scheduler-1] ADBCollector:172 -  Error collecting data for database 'DBSERVER01'
  com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near 'sys'."
AppDynamics,44391470,44411981.0,1,"2017/06/06, 16:33:25",True,"2017/06/07, 14:55:46","2017/06/06, 17:04:34",1172579.0,545.0,0,624,AppDynamics database agent 4.3.10 error on SQL Server 2005,"at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:216)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1515)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:404)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:350)
          at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:5696)
          at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1715)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:180)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:155)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:285)
          at com.singularity.ee.agent.dbagent.collector.db.relational.mssql.AMSSqlCollectorDelegate.collectDBMSMetrics(AMSSqlCollectorDelegate.java:335)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollectorDelegate.collectPerMinute(ADBCollectorDelegate.java:88)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollector.collect(ADBCollector.java:156)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollector.run(ADBCollector.java:139)
          at com.singularity.ee.util.javaspecific.scheduler.AgentScheduledExecutorServiceImpl$SafeRunnable.run(AgentScheduledExecutorServiceImpl.java:122)
          at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
          at com.singularity.ee.util.javaspecific.scheduler.ADFutureTask$Sync.innerRunAndReset(ADFutureTask.java:335)
          at com.singularity.ee.util.javaspecific.scheduler.ADFutureTask.runAndReset(ADFutureTask.java:152)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.access$101(ADScheduledThreadPoolExecutor.java:119)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.runPeriodic(ADScheduledThreadPoolExecutor.java:206)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.run(ADScheduledThreadPoolExecutor.java:236)
          at com.singularity.ee.util.javaspecific.scheduler.ADThreadPoolExecutor$Worker.runTask(ADThreadPoolExecutor.java:694)
          at com.singularity.ee.util.javaspecific.scheduler.ADThreadPoolExecutor$Worker.run(ADThreadPoolExecutor.java:726)
          at java.lang.Thread.run(Unknown Source)"
AppDynamics,44569260,nan,1,"2017/06/15, 16:48:30",True,"2017/07/24, 17:16:35","2017/06/15, 17:09:30",6246302.0,83.0,0,182,Programmatically fetching AppDynamics Transaction Scorecard information using REST,I need to fetch the transaction scorecard data of a business application using the REST API of AppDynamics .
AppDynamics,44569260,nan,1,"2017/06/15, 16:48:30",True,"2017/07/24, 17:16:35","2017/06/15, 17:09:30",6246302.0,83.0,0,182,Programmatically fetching AppDynamics Transaction Scorecard information using REST,Following is the a sample view of AppDynamics Transaction Scorecard
AppDynamics,44569260,nan,1,"2017/06/15, 16:48:30",True,"2017/07/24, 17:16:35","2017/06/15, 17:09:30",6246302.0,83.0,0,182,Programmatically fetching AppDynamics Transaction Scorecard information using REST,"I have done through the AppDynamics REST API documentation to some extent,but not found anything so far ."
AppDynamics,44569260,nan,1,"2017/06/15, 16:48:30",True,"2017/07/24, 17:16:35","2017/06/15, 17:09:30",6246302.0,83.0,0,182,Programmatically fetching AppDynamics Transaction Scorecard information using REST,Can anybody have any idea on this ?
AppDynamics,44776999,nan,2,"2017/06/27, 12:39:49",False,"2018/05/22, 21:41:45","2017/06/27, 12:55:34",6246302.0,83.0,0,594,Issue with AppDynamics REST call with Python,I tried to call AppDynamics API using python requests but face an issue.
AppDynamics,44776999,nan,2,"2017/06/27, 12:39:49",False,"2018/05/22, 21:41:45","2017/06/27, 12:55:34",6246302.0,83.0,0,594,Issue with AppDynamics REST call with Python,I wrote a sample code using the python client as follows...
AppDynamics,44776999,nan,2,"2017/06/27, 12:39:49",False,"2018/05/22, 21:41:45","2017/06/27, 12:55:34",6246302.0,83.0,0,594,Issue with AppDynamics REST call with Python,It works fine.
AppDynamics,44776999,nan,2,"2017/06/27, 12:39:49",False,"2018/05/22, 21:41:45","2017/06/27, 12:55:34",6246302.0,83.0,0,594,Issue with AppDynamics REST call with Python,But if I do a simple call like the following
AppDynamics,44776999,nan,2,"2017/06/27, 12:39:49",False,"2018/05/22, 21:41:45","2017/06/27, 12:55:34",6246302.0,83.0,0,594,Issue with AppDynamics REST call with Python,I get the following response:
AppDynamics,44776999,nan,2,"2017/06/27, 12:39:49",False,"2018/05/22, 21:41:45","2017/06/27, 12:55:34",6246302.0,83.0,0,594,Issue with AppDynamics REST call with Python,Am I doing anything wrong here ?
AppDynamics,44805162,nan,2,"2017/06/28, 17:18:55",True,"2017/07/22, 13:24:47",nan,6246302.0,83.0,0,174,How to get severity information for Business Transaction severity info using AppDynamics REST call,When I am invoking a REST URI from the browser using an URL like the following
AppDynamics,44805162,nan,2,"2017/06/28, 17:18:55",True,"2017/07/22, 13:24:47",nan,6246302.0,83.0,0,174,How to get severity information for Business Transaction severity info using AppDynamics REST call,http://:/controller/rest/applications//business-transactions?output=JSON
AppDynamics,44805162,nan,2,"2017/06/28, 17:18:55",True,"2017/07/22, 13:24:47",nan,6246302.0,83.0,0,174,How to get severity information for Business Transaction severity info using AppDynamics REST call,and this is providing the output as
AppDynamics,44805162,nan,2,"2017/06/28, 17:18:55",True,"2017/07/22, 13:24:47",nan,6246302.0,83.0,0,174,How to get severity information for Business Transaction severity info using AppDynamics REST call,"This output is missing the indicator/field for severity information like WARNING,CRITICAL,NORMAL etc."
AppDynamics,44805162,nan,2,"2017/06/28, 17:18:55",True,"2017/07/22, 13:24:47",nan,6246302.0,83.0,0,174,How to get severity information for Business Transaction severity info using AppDynamics REST call,How to get the severity information from the AppDynamics REST call ?
AppDynamics,44939278,nan,1,"2017/07/06, 06:25:37",False,"2017/07/07, 03:51:06","2017/07/07, 03:47:58",1858160.0,7.0,0,493,AppDynamics Reuse nodename in Machine Agent Registration,We have implemented AppDynamics in Jboss application.
AppDynamics,44939278,nan,1,"2017/07/06, 06:25:37",False,"2017/07/07, 03:51:06","2017/07/07, 03:47:58",1858160.0,7.0,0,493,AppDynamics Reuse nodename in Machine Agent Registration,We have load balancer and autoscalling which means we will have node registration when new server comes up.
AppDynamics,44939278,nan,1,"2017/07/06, 06:25:37",False,"2017/07/07, 03:51:06","2017/07/07, 03:47:58",1858160.0,7.0,0,493,AppDynamics Reuse nodename in Machine Agent Registration,The problem here is Java and Machine Agent.
AppDynamics,44939278,nan,1,"2017/07/06, 06:25:37",False,"2017/07/07, 03:51:06","2017/07/07, 03:47:58",1858160.0,7.0,0,493,AppDynamics Reuse nodename in Machine Agent Registration,"Java Agent can reuse name with prefix (Appd Controlled Node Names) , but machine agent node name need to be provided at configuration level."
AppDynamics,44939278,nan,1,"2017/07/06, 06:25:37",False,"2017/07/07, 03:51:06","2017/07/07, 03:47:58",1858160.0,7.0,0,493,AppDynamics Reuse nodename in Machine Agent Registration,We are getting two separate agents listed.
AppDynamics,44939278,nan,1,"2017/07/06, 06:25:37",False,"2017/07/07, 03:51:06","2017/07/07, 03:47:58",1858160.0,7.0,0,493,AppDynamics Reuse nodename in Machine Agent Registration,One is 100% with Machine Agent and another with Java agent.
AppDynamics,44939278,nan,1,"2017/07/06, 06:25:37",False,"2017/07/07, 03:51:06","2017/07/07, 03:47:58",1858160.0,7.0,0,493,AppDynamics Reuse nodename in Machine Agent Registration,We need Machine Agent will ping at same line.
AppDynamics,44939278,nan,1,"2017/07/06, 06:25:37",False,"2017/07/07, 03:51:06","2017/07/07, 03:47:58",1858160.0,7.0,0,493,AppDynamics Reuse nodename in Machine Agent Registration,https://i.stack.imgur.com/MhpeU.png
AppDynamics,46591986,nan,3,"2017/10/05, 20:55:01",True,"2018/12/10, 13:41:43",nan,6118299.0,3372.0,0,526,Using environment variables in AppDynamics Python Agent configuration,"What is the meaning and use of environment variables in the Python Agent configuration for App Dynamics, as documented here:"
AppDynamics,46591986,nan,3,"2017/10/05, 20:55:01",True,"2018/12/10, 13:41:43",nan,6118299.0,3372.0,0,526,Using environment variables in AppDynamics Python Agent configuration,https://docs.appdynamics.com/display/PRO42/Python+Agent+Settings
AppDynamics,46591986,nan,3,"2017/10/05, 20:55:01",True,"2018/12/10, 13:41:43",nan,6118299.0,3372.0,0,526,Using environment variables in AppDynamics Python Agent configuration,More specifically:
AppDynamics,46591986,nan,3,"2017/10/05, 20:55:01",True,"2018/12/10, 13:41:43",nan,6118299.0,3372.0,0,526,Using environment variables in AppDynamics Python Agent configuration,"If a value is set in the file and the corresponding environment variable is also set, which one takes precedence?"
AppDynamics,46591986,nan,3,"2017/10/05, 20:55:01",True,"2018/12/10, 13:41:43",nan,6118299.0,3372.0,0,526,Using environment variables in AppDynamics Python Agent configuration,"If I want to use environment variables for some of these values, can they be omitted from the file?"
AppDynamics,47129599,nan,1,"2017/11/06, 06:07:25",False,"2017/11/07, 04:33:37",nan,2759366.0,17.0,0,207,Does AppDynamics log request and response,Can AppDynamics show the request or response being exchanged between different microservices systems.
AppDynamics,47129599,nan,1,"2017/11/06, 06:07:25",False,"2017/11/07, 04:33:37",nan,2759366.0,17.0,0,207,Does AppDynamics log request and response,"They show the call trace, but couldnt find the details of what is passing between the calls."
AppDynamics,47135535,nan,1,"2017/11/06, 13:07:31",True,"2017/11/07, 04:37:05",nan,2759366.0,17.0,0,1959,AppDynamics vs Zipkin as APM Tools,What are the differences in features between AppDynamics and Zipkin apart from the pricing since zipkin is opensource.
AppDynamics,47135535,nan,1,"2017/11/06, 13:07:31",True,"2017/11/07, 04:37:05",nan,2759366.0,17.0,0,1959,AppDynamics vs Zipkin as APM Tools,"Can any of them show request or response, in their console?"
AppDynamics,47378215,47398459.0,3,"2017/11/19, 16:53:00",True,"2018/06/07, 13:25:10","2017/11/20, 19:27:07",936617.0,2848.0,0,123,Appdynamics vs Crittercism which one should be used for crash logs and health check for iOS/Android apps,Is this comparison even valid?
AppDynamics,47378215,47398459.0,3,"2017/11/19, 16:53:00",True,"2018/06/07, 13:25:10","2017/11/20, 19:27:07",936617.0,2848.0,0,123,Appdynamics vs Crittercism which one should be used for crash logs and health check for iOS/Android apps,Appdynamics does a lot of other things beyond crash logs.
AppDynamics,47378215,47398459.0,3,"2017/11/19, 16:53:00",True,"2018/06/07, 13:25:10","2017/11/20, 19:27:07",936617.0,2848.0,0,123,Appdynamics vs Crittercism which one should be used for crash logs and health check for iOS/Android apps,So using Crittercism for crashlogs is a good idea or bad.
AppDynamics,49509430,nan,1,"2018/03/27, 12:40:23",False,"2018/10/04, 10:52:28",nan,889334.0,25.0,0,453,How AppDynamics 4.4 to track async transaction,Consider below code:
AppDynamics,49509430,nan,1,"2018/03/27, 12:40:23",False,"2018/10/04, 10:52:28",nan,889334.0,25.0,0,453,How AppDynamics 4.4 to track async transaction,"I could use AppDynamics ""Java POJO"" rule to create a business transaction to track all the calls to Job.process() method."
AppDynamics,49509430,nan,1,"2018/03/27, 12:40:23",False,"2018/10/04, 10:52:28",nan,889334.0,25.0,0,453,How AppDynamics 4.4 to track async transaction,But the measured response time didn't reflect real cost by the async thread started by java.util.concurrent.ExecutorService.
AppDynamics,49509430,nan,1,"2018/03/27, 12:40:23",False,"2018/10/04, 10:52:28",nan,889334.0,25.0,0,453,How AppDynamics 4.4 to track async transaction,This exact problem is also described in AppDynamics document:  End-to-End Latency Performance  that:
AppDynamics,49509430,nan,1,"2018/03/27, 12:40:23",False,"2018/10/04, 10:52:28",nan,889334.0,25.0,0,453,How AppDynamics 4.4 to track async transaction,"The return of control stops the clock on the transaction in terms of measuring response time, but meanwhile the logical processing for the transaction continues."
AppDynamics,49509430,nan,1,"2018/03/27, 12:40:23",False,"2018/10/04, 10:52:28",nan,889334.0,25.0,0,453,How AppDynamics 4.4 to track async transaction,The same AppDynamics document tries to give a solution to address this issue but the instructions it provides is not very clear to me.
AppDynamics,49509430,nan,1,"2018/03/27, 12:40:23",False,"2018/10/04, 10:52:28",nan,889334.0,25.0,0,453,How AppDynamics 4.4 to track async transaction,Could anyone give more executable guide on how to configure AppD to track async calls like the one shown above?
AppDynamics,49536659,nan,0,"2018/03/28, 17:03:03",False,"2019/01/04, 19:37:21","2019/01/04, 19:37:21",4021776.0,917.0,0,268,Host name and agentId in node name in AppDynamics,We are in the process of configuring AppDynamics for one of our applications.
AppDynamics,49536659,nan,0,"2018/03/28, 17:03:03",False,"2019/01/04, 19:37:21","2019/01/04, 19:37:21",4021776.0,917.0,0,268,Host name and agentId in node name in AppDynamics,"Since there are many instances of the application, we want to add nodename with agentId and hostName so as to identify the different instances."
AppDynamics,49536659,nan,0,"2018/03/28, 17:03:03",False,"2019/01/04, 19:37:21","2019/01/04, 19:37:21",4021776.0,917.0,0,268,Host name and agentId in node name in AppDynamics,"Below is how we are trying to do, but it does not seems to work:"
AppDynamics,49536659,nan,0,"2018/03/28, 17:03:03",False,"2019/01/04, 19:37:21","2019/01/04, 19:37:21",4021776.0,917.0,0,268,Host name and agentId in node name in AppDynamics,Once I start the JVM the node name appears as CalculationEngine_null_null.
AppDynamics,49536659,nan,0,"2018/03/28, 17:03:03",False,"2019/01/04, 19:37:21","2019/01/04, 19:37:21",4021776.0,917.0,0,268,Host name and agentId in node name in AppDynamics,I was hoping nodename to come CalculationEngine_3_a301-564.com where 3 being the agentid and a301-564 being the host name.
AppDynamics,49536659,nan,0,"2018/03/28, 17:03:03",False,"2019/01/04, 19:37:21","2019/01/04, 19:37:21",4021776.0,917.0,0,268,Host name and agentId in node name in AppDynamics,Also even if the host name parameter is not correct at least it should show CalculationEngine_3_null
AppDynamics,49536659,nan,0,"2018/03/28, 17:03:03",False,"2019/01/04, 19:37:21","2019/01/04, 19:37:21",4021776.0,917.0,0,268,Host name and agentId in node name in AppDynamics,What could be wrong here?
AppDynamics,49536659,nan,0,"2018/03/28, 17:03:03",False,"2019/01/04, 19:37:21","2019/01/04, 19:37:21",4021776.0,917.0,0,268,Host name and agentId in node name in AppDynamics,Or is it not possible?
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,I am using the ActiveMQ extension of AppDynamics.
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,It is good to start.
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,With JMXRemote(enabled in artemis.profile) it is OK.
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,"But, I want it from localhost."
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,JMX is enabled by default for localhost for AMQ.
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,AMQ management console use jmx internally and it works without JMXRemote enabled.
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,What service URL jolokia use internally to connect using JMX from localhost?
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,I have tryed with following URL:
AppDynamics,50218209,nan,1,"2018/05/07, 18:59:33",False,"2018/05/10, 13:22:05","2018/05/10, 12:09:32",4057456.0,94.0,0,499,AppDynamics monitoring with AMQ 7.0.1,"serviceUrl: ""service:jmx:rmi:///jndi/rmi://:1099/jmxrmi"""
AppDynamics,52825100,nan,2,"2018/10/16, 00:38:29",True,"2018/10/25, 15:53:20",nan,9094432.0,1.0,0,991,AppDynamics support Opentracing,I'm trying to determine if AppDyanmics support Opentracing.
AppDynamics,52825100,nan,2,"2018/10/16, 00:38:29",True,"2018/10/25, 15:53:20",nan,9094432.0,1.0,0,991,AppDynamics support Opentracing,I've looked in the app dynamics site and stack overflow but can't find a clear answer.
AppDynamics,52825100,nan,2,"2018/10/16, 00:38:29",True,"2018/10/25, 15:53:20",nan,9094432.0,1.0,0,991,AppDynamics support Opentracing,"Thanks,
Carlos"
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,We have a JBoss fuse ESB instance Running Version 6.3.0.
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,"With this, Installed as a Service using tanuki wrapper."
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,When passing the javaagent Argument The Application (hawtio)Breaks.
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,we have tried passing the argument in the following files:
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,JBOSSHOME/bin/karaf/
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,JBOSSHOME/etc/jboss-fuse-wrapper.conf
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,-javaagent:/agenthome/javaagent.jar
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,"The JVM Loads the argument when it is passing in the wrapper.conf, but as mentioned before the application is not working when the argument is loaded in the JVM."
AppDynamics,54003615,nan,1,"2019/01/02, 11:03:12",False,"2019/11/11, 15:36:01","2019/01/02, 11:11:38",10856943.0,1.0,0,102,Appdynamics Instrumentation on Jboss Fuse ESB 6.3,have anyone instrumented JBoss Fuse ESB with app dynamics before?
AppDynamics,55000073,nan,0,"2019/03/05, 11:58:54",False,"2019/03/05, 14:09:48","2019/03/05, 14:09:48",8324973.0,161.0,0,32,AppDynamics iOS block my request and response nil,"I implement AppDynamics in my iOS app, I put everything in doc 
 like initWithKey licence key, but if I write initWithKey in appDelegate my all service request return nil."
AppDynamics,55000073,nan,0,"2019/03/05, 11:58:54",False,"2019/03/05, 14:09:48","2019/03/05, 14:09:48",8324973.0,161.0,0,32,AppDynamics iOS block my request and response nil,Can anyone help me Thank you.
AppDynamics,55234879,nan,1,"2019/03/19, 08:29:09",False,"2019/11/11, 15:18:20",nan,2613958.0,105.0,0,113,How to copy existing dashboard to the new Project in Appdynamics,How to copy an existing dashboard to the new project in appdynamics.
AppDynamics,55332537,nan,1,"2019/03/25, 08:51:07",False,"2019/04/23, 15:42:30","2019/03/25, 10:20:42",11253338.0,1.0,0,166,How does AppDynamics APM works internally?,"How does AppDynamics works internally, In my current company we are planning to use AppDynamics but teams want to know how it actually works such as How it collects data, how does it communicate and how it intercepts java transactions and other related stuff."
AppDynamics,55332537,nan,1,"2019/03/25, 08:51:07",False,"2019/04/23, 15:42:30","2019/03/25, 10:20:42",11253338.0,1.0,0,166,How does AppDynamics APM works internally?,So I tried looking into AppDynamics knowledge base but did not get accurate technical answer I need.
AppDynamics,55697936,nan,2,"2019/04/16, 01:16:39",False,"2019/04/16, 17:25:33","2019/04/16, 17:25:33",11365460.0,1.0,0,240,How do I update AppDynamics .net agent via Powershell?,not new to coding but I have been working in python before and shaking the rust off my Powershell and working with xml.
AppDynamics,55697936,nan,2,"2019/04/16, 01:16:39",False,"2019/04/16, 17:25:33","2019/04/16, 17:25:33",11365460.0,1.0,0,240,How do I update AppDynamics .net agent via Powershell?,We are trying to automate the deployment of AppDynamics's .net agent and we have the deployment piece down as well as upgrade.
AppDynamics,55697936,nan,2,"2019/04/16, 01:16:39",False,"2019/04/16, 17:25:33","2019/04/16, 17:25:33",11365460.0,1.0,0,240,How do I update AppDynamics .net agent via Powershell?,Now I am trying to include specific applications and tiers running on IIS.
AppDynamics,55697936,nan,2,"2019/04/16, 01:16:39",False,"2019/04/16, 17:25:33","2019/04/16, 17:25:33",11365460.0,1.0,0,240,How do I update AppDynamics .net agent via Powershell?,So again shaking off rust here but I am trying the find a simple script to update the config.xml to add the application and update the tier from IIS.
AppDynamics,55697936,nan,2,"2019/04/16, 01:16:39",False,"2019/04/16, 17:25:33","2019/04/16, 17:25:33",11365460.0,1.0,0,240,How do I update AppDynamics .net agent via Powershell?,It's not a complex xml file but any help or direction would be helpful
AppDynamics,55697936,nan,2,"2019/04/16, 01:16:39",False,"2019/04/16, 17:25:33","2019/04/16, 17:25:33",11365460.0,1.0,0,240,How do I update AppDynamics .net agent via Powershell?,Just trying to find the right syntax to update the nodes properly and arguments
AppDynamics,55697936,nan,2,"2019/04/16, 01:16:39",False,"2019/04/16, 17:25:33","2019/04/16, 17:25:33",11365460.0,1.0,0,240,How do I update AppDynamics .net agent via Powershell?,"
 
 #what I need updated
&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;appdynamics-agent xmlns:xsd=""http://stuff.com"" xmlns:xsi=""http://stuff.com""&gt;
  &lt;controller host=""test.saas.appdynamics.com"" port=""123"" ssl=""true"" enable_tls12=""true""&gt;
  &lt;applications&gt; #need to create this
    &lt;application default=""true"" name=""app1"" /&gt; #need to add the default='true' if there is more than one app
    &lt;application name=""App2""/&gt; #this 
  &lt;/applications&gt; #this
    &lt;account name=""testacct"" password=""123456"" /&gt;
  &lt;/controller&gt;
  &lt;machine-agent /&gt;
  &lt;app-agents&gt;
    &lt;IIS&gt;
      &lt;applications&gt; #there by default with the default settings 
        &lt;application controller-application=""app1"" path=""/app1path"" site=""WebSite1""&gt; # need to add this
          &lt;tier name=""app1-1"" /&gt; #and this
        &lt;/application&gt; #this too
        &lt;application controller-application=""app2"" path=""/app2path"" site=""WebSite2""&gt; #some more
          &lt;tier name=""app2-2"" /&gt; #you guessed it
        &lt;/application&gt; #this as well
      &lt;/applications&gt; #ends with this
    &lt;/IIS&gt;
  &lt;/app-agents&gt;
&lt;/appdynamics-agent&gt;"
AppDynamics,55787843,nan,1,"2019/04/22, 03:11:42",False,"2019/04/23, 15:24:27",nan,6880017.0,1.0,0,177,Pcf appdynamics spring actuator,We have a spring boot app running in pivotal cloud foundry and we have also configured appdynamics and we can now see our app on appd controller GUI.
AppDynamics,55787843,nan,1,"2019/04/22, 03:11:42",False,"2019/04/23, 15:24:27",nan,6880017.0,1.0,0,177,Pcf appdynamics spring actuator,"I would like to build a appd dashboard and call actuator end points like info , health."
AppDynamics,55787843,nan,1,"2019/04/22, 03:11:42",False,"2019/04/23, 15:24:27",nan,6880017.0,1.0,0,177,Pcf appdynamics spring actuator,How do i do this?
AppDynamics,55787843,nan,1,"2019/04/22, 03:11:42",False,"2019/04/23, 15:24:27",nan,6880017.0,1.0,0,177,Pcf appdynamics spring actuator,"Or I am also open to other ideas on building appd dashboard for micro services
Please advise"
AppDynamics,57912322,nan,1,"2019/09/12, 21:05:52",False,"2019/11/11, 14:36:40",nan,5846608.0,21.0,0,214,How to get application flow map data via AppDynamics API?,"I want to load ""application flow map"" data (that can be seen on the web UI dashboard) from AppDynamics APIs."
AppDynamics,57912322,nan,1,"2019/09/12, 21:05:52",False,"2019/11/11, 14:36:40",nan,5846608.0,21.0,0,214,How to get application flow map data via AppDynamics API?,My goal is to upload the data in Neo4j so we can study our microservices architecture using graph algorithms.
AppDynamics,57912322,nan,1,"2019/09/12, 21:05:52",False,"2019/11/11, 14:36:40",nan,5846608.0,21.0,0,214,How to get application flow map data via AppDynamics API?,The AppDynamics Application Model API doesn't seem to provide data up to this level.
AppDynamics,57912322,nan,1,"2019/09/12, 21:05:52",False,"2019/11/11, 14:36:40",nan,5846608.0,21.0,0,214,How to get application flow map data via AppDynamics API?,"I'll build a client later but for now I use curl with requests like:
curl --user MyUserName:MyPassword  https://hostname/controller/rest/applications/OurApp/tiers  and variations of this according to the documentation"
AppDynamics,57912322,nan,1,"2019/09/12, 21:05:52",False,"2019/11/11, 14:36:40",nan,5846608.0,21.0,0,214,How to get application flow map data via AppDynamics API?,https://docs.appdynamics.com/display/PRO45/Application+Model+API
AppDynamics,57912322,nan,1,"2019/09/12, 21:05:52",False,"2019/11/11, 14:36:40",nan,5846608.0,21.0,0,214,How to get application flow map data via AppDynamics API?,Only curl requests for now.
AppDynamics,57912322,nan,1,"2019/09/12, 21:05:52",False,"2019/11/11, 14:36:40",nan,5846608.0,21.0,0,214,How to get application flow map data via AppDynamics API?,See point 2.
AppDynamics,57912322,nan,1,"2019/09/12, 21:05:52",False,"2019/11/11, 14:36:40",nan,5846608.0,21.0,0,214,How to get application flow map data via AppDynamics API?,I expect a JSON output all the tiers of OurApp with interactions between them.
AppDynamics,57958059,58010383.0,2,"2019/09/16, 16:36:46",True,"2019/11/11, 13:57:56",nan,7611518.0,41.0,0,667,Magic v1 does not support record headers - Appdynamics Adding SingularityHeader,I am facing  Magic v1 does not support record headers  while consuming the messages form Kafka.
AppDynamics,57958059,58010383.0,2,"2019/09/16, 16:36:46",True,"2019/11/11, 13:57:56",nan,7611518.0,41.0,0,667,Magic v1 does not support record headers - Appdynamics Adding SingularityHeader,I understand this comes for the older kafka client version.
AppDynamics,57958059,58010383.0,2,"2019/09/16, 16:36:46",True,"2019/11/11, 13:57:56",nan,7611518.0,41.0,0,667,Magic v1 does not support record headers - Appdynamics Adding SingularityHeader,But in my case AppDynamics is injecting a SingularityHeader as given below -
AppDynamics,57958059,58010383.0,2,"2019/09/16, 16:36:46",True,"2019/11/11, 13:57:56",nan,7611518.0,41.0,0,667,Magic v1 does not support record headers - Appdynamics Adding SingularityHeader,Kafka Client Version - 0.10.2.0.
AppDynamics,57958059,58010383.0,2,"2019/09/16, 16:36:46",True,"2019/11/11, 13:57:56",nan,7611518.0,41.0,0,667,Magic v1 does not support record headers - Appdynamics Adding SingularityHeader,I need suggestions here other than Upgrading Kafka Client version to 0.11.x from 0.10.2.0 (that is not an option).
AppDynamics,57958059,58010383.0,2,"2019/09/16, 16:36:46",True,"2019/11/11, 13:57:56",nan,7611518.0,41.0,0,667,Magic v1 does not support record headers - Appdynamics Adding SingularityHeader,Is there a way to disable this from APPD itself ?
AppDynamics,57977882,58801167.0,1,"2019/09/17, 18:50:12",True,"2019/11/11, 14:16:20",nan,4493048.0,242.0,0,161,AppDynamics - What usecase do Business Transactions fulfill?,I have a set of webservice endpoints.
AppDynamics,57977882,58801167.0,1,"2019/09/17, 18:50:12",True,"2019/11/11, 14:16:20",nan,4493048.0,242.0,0,161,AppDynamics - What usecase do Business Transactions fulfill?,I'd like to use AppDynamics to collect metrics on the performance &amp; error rate of these endpoints.
AppDynamics,57977882,58801167.0,1,"2019/09/17, 18:50:12",True,"2019/11/11, 14:16:20",nan,4493048.0,242.0,0,161,AppDynamics - What usecase do Business Transactions fulfill?,Are Business Transactions the right tool to use for this?
AppDynamics,57977882,58801167.0,1,"2019/09/17, 18:50:12",True,"2019/11/11, 14:16:20",nan,4493048.0,242.0,0,161,AppDynamics - What usecase do Business Transactions fulfill?,"If not, then what  are  Business Transactions useful for?"
AppDynamics,57977882,58801167.0,1,"2019/09/17, 18:50:12",True,"2019/11/11, 14:16:20",nan,4493048.0,242.0,0,161,AppDynamics - What usecase do Business Transactions fulfill?,(The documentation explains that Business Transactions monitor a single transaction from end-to-end.
AppDynamics,57977882,58801167.0,1,"2019/09/17, 18:50:12",True,"2019/11/11, 14:16:20",nan,4493048.0,242.0,0,161,AppDynamics - What usecase do Business Transactions fulfill?,"I should conceptualize my transactions ""from the end user's perspective"" etc."
AppDynamics,57977882,58801167.0,1,"2019/09/17, 18:50:12",True,"2019/11/11, 14:16:20",nan,4493048.0,242.0,0,161,AppDynamics - What usecase do Business Transactions fulfill?,But this doesn't answer my question - what usecase do Business Transactions fulfill that isn't better suited to Information Points or Service Endpoints etc.?)
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,I have a web application running on JBoss/Wildfly and using RESTEasy.
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,I'm monitoring it with AppDynamics.
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,I've configured my business transaction detection to use a Java Servlet.
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,"This just about works, but some of my REST paths contain UUIDs, for example:"
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,"Each time this end-point is invoked with a different UUID, AppD treats it as a different business transaction."
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,"Is there a way to make AppD recognise UUIDs within a path, and group these into a single business transaction?"
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,Something like:
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,I should be able to do it by applying a regex to the request's path info:
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,or even just
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,but I can't figure out how to escape it properly.
AppDynamics,58644710,nan,1,"2019/10/31, 15:45:55",False,"2019/11/11, 13:55:20","2019/10/31, 19:33:46",4331750.0,2736.0,0,104,AppDynamics to group REST endpoints containing UUIDs as a single business transaction,"doesn't work, and neither does"
AppDynamics,59835705,nan,0,"2020/01/21, 09:01:06",False,"2020/01/21, 10:13:22","2020/06/20, 12:12:55",12752700.0,11.0,0,188,Docker - Appdynamics - machine agent issue,I cloned the image from  https://github.com/Appdynamics/docker-machine-agent
AppDynamics,59835705,nan,0,"2020/01/21, 09:01:06",False,"2020/01/21, 10:13:22","2020/06/20, 12:12:55",12752700.0,11.0,0,188,Docker - Appdynamics - machine agent issue,and executed the  docker-compose up  command after the machine agent installation.
AppDynamics,59835705,nan,0,"2020/01/21, 09:01:06",False,"2020/01/21, 10:13:22","2020/06/20, 12:12:55",12752700.0,11.0,0,188,Docker - Appdynamics - machine agent issue,But I am getting below error while starting the machine agent.
AppDynamics,59835705,nan,0,"2020/01/21, 09:01:06",False,"2020/01/21, 10:13:22","2020/06/20, 12:12:55",12752700.0,11.0,0,188,Docker - Appdynamics - machine agent issue,docker-compose up
AppDynamics,59835705,nan,0,"2020/01/21, 09:01:06",False,"2020/01/21, 10:13:22","2020/06/20, 12:12:55",12752700.0,11.0,0,188,Docker - Appdynamics - machine agent issue,Creating docker-machine-agent ... done
AppDynamics,59835705,nan,0,"2020/01/21, 09:01:06",False,"2020/01/21, 10:13:22","2020/06/20, 12:12:55",12752700.0,11.0,0,188,Docker - Appdynamics - machine agent issue,Attaching to docker-machine-agent
AppDynamics,59835705,nan,0,"2020/01/21, 09:01:06",False,"2020/01/21, 10:13:22","2020/06/20, 12:12:55",12752700.0,11.0,0,188,Docker - Appdynamics - machine agent issue,docker-machine-agent | /bin/sh: 1: /opt/appdynamics/machine-agent//start-appdynamics: not found
AppDynamics,59835705,nan,0,"2020/01/21, 09:01:06",False,"2020/01/21, 10:13:22","2020/06/20, 12:12:55",12752700.0,11.0,0,188,Docker - Appdynamics - machine agent issue,docker-machine-agent exited with code 127
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,We are using AppDynamics and VisualVM to monitor our application heap memory usage.
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,We see similar graph as stated in these questions -  this  and  this .
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,the red boxes show idle system heap usage - peaks are seen only when system is in idle state and are even observed when no application is deployed.
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,"the green arrow points to actual application in use state - When system is in use, we see relatively very less heap usage being reported."
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,"Based on the clarifications in other SO questions, if we say it is due to garbage collection, why would GC not occur during application use?"
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,"When system is idle, we see system objects like java.land.String, byte[], int[] etc."
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,"getting reported in AppDynamics, but how to find who is responsible for creating them?"
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,"Again, in the heap dumps taken during idle state, we see only 200MB out of 500MB memory used, when the server has dedicated -Xmx4g configuration."
AppDynamics,60082614,nan,1,"2020/02/05, 21:10:58",False,"2020/02/13, 10:51:30",nan,2131858.0,11.0,0,116,Why do APM tools like AppDynamics or VisualVM show heap memory peaks during idle state?,How should we make sense of these observations?
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),I'm new in LoadRunner.
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),My problem is:
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),"When I'm running the script from LoadRunner (Http/Https protocol), I'm unable to see the browserName(like firefox, Chrome, IE etc)  in monitoring tool (AppDynamics)."
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),"But when I'm running script from LoadRunner (using truClient protocol), I'm able to see the browserName in AppDynamics."
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),My Objective is to see the browser Name in AppDynamics Monitoring tool when I'm using HTTP/HTTPS protocol.
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),**
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),**
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),"I have compared both the header when script was running from    different protocol using Fiddler, Burp-Suite."
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),"(No difference) 
User-Agent string is common in both the protocol."
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),Can someone suggest/help me on this?
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),Thanks in advance.
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),"Note:  
Version of LoadRunner is 12.60."
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),Just running with one transaction i.e launching the home page only from both the protocol.
AppDynamics,60127980,nan,1,"2020/02/08, 16:57:48",False,"2020/02/09, 10:51:22",nan,12863700.0,1.0,0,107,Unable to see browser(Firefox/chrome/IE) in Monitoring tool (AppDynamics) when load is running from Load Runner(Http/Https protocol),Please let me know if you need more information from my end.
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,"I'm using Xcode 11.3, my app is in Swift."
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,"When I add AppDynamics pod to my project and try to build, I get ""1222 duplicate symbols for architecture arm64"" error."
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,"All duplicates are in several .a static libraries that are unrelated to AppDynamics and that are all linked in ""Link Binary With Libraries"" in Build Phases (there are other .framework-s here too but they don't cause any issue)."
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,"I tried all the standard things like cleaning build folder, deleting derived data, restarting computer, building with Xcode 11.2 version, playing with -ObjC flag, but none of this helped."
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,"Since there are so many duplicates, changing those static libraries is not an option as it has been suggested in some threads."
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,Project also has other pods added that all worked fine.
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,"I see that there were other similar questions, but I could not find an answer that worked in my case, I've been stuck on this for more than a day already."
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,Does anyone have some other suggestion what I could try?
AppDynamics,60512109,nan,0,"2020/03/03, 18:57:30",False,"2020/03/03, 18:57:30",nan,6217970.0,56.0,0,44,Duplicate symbols for architecture arm64 after installing AppDynamics pod,I would like to understand why this is happening?
AppDynamics,61465215,nan,0,"2020/04/27, 20:54:02",False,"2020/04/27, 20:54:02",nan,8106849.0,40.0,0,30,How to initialize appdynamics agent for spring application,I have a jar file of java application.
AppDynamics,61465215,nan,0,"2020/04/27, 20:54:02",False,"2020/04/27, 20:54:02",nan,8106849.0,40.0,0,30,How to initialize appdynamics agent for spring application,I have appdynamics agent installed on my machine.
AppDynamics,61465215,nan,0,"2020/04/27, 20:54:02",False,"2020/04/27, 20:54:02",nan,8106849.0,40.0,0,30,How to initialize appdynamics agent for spring application,But not sure how to pass paramters so that appdynamics will monitor this java application.
AppDynamics,61730657,nan,1,"2020/05/11, 16:17:05",False,"2020/07/07, 15:55:48","2020/05/12, 06:59:58",10447534.0,1.0,0,50,AppDynamics Alerting - How to trigger alert based on number of call (trxn) per hour health rule. (example call &lt; 10/hour),My requirement is - AppD Health rule should trigger alert when call per hour &lt; x (eg: 10).
AppDynamics,61730657,nan,1,"2020/05/11, 16:17:05",False,"2020/07/07, 15:55:48","2020/05/12, 06:59:58",10447534.0,1.0,0,50,AppDynamics Alerting - How to trigger alert based on number of call (trxn) per hour health rule. (example call &lt; 10/hour),"But while setting this,  I could not find call/hour metric in AppD controller."
AppDynamics,61730657,nan,1,"2020/05/11, 16:17:05",False,"2020/07/07, 15:55:48","2020/05/12, 06:59:58",10447534.0,1.0,0,50,AppDynamics Alerting - How to trigger alert based on number of call (trxn) per hour health rule. (example call &lt; 10/hour),It has all perMin metrics.
AppDynamics,61730657,nan,1,"2020/05/11, 16:17:05",False,"2020/07/07, 15:55:48","2020/05/12, 06:59:58",10447534.0,1.0,0,50,AppDynamics Alerting - How to trigger alert based on number of call (trxn) per hour health rule. (example call &lt; 10/hour),So how to achieve this alerting criteria in AppD health rule?
AppDynamics,61788037,nan,0,"2020/05/14, 05:04:16",False,"2020/05/14, 05:04:16",nan,3850795.0,35.0,0,34,AppDynamics HTTP Listener,I'm trying to get the AppD HTTP Listener working on my Linux system.
AppDynamics,61788037,nan,0,"2020/05/14, 05:04:16",False,"2020/05/14, 05:04:16",nan,3850795.0,35.0,0,34,AppDynamics HTTP Listener,My AppDynamics java process has the required argument running: -Dmetric.http.listener=true
AppDynamics,61788037,nan,0,"2020/05/14, 05:04:16",False,"2020/05/14, 05:04:16",nan,3850795.0,35.0,0,34,AppDynamics HTTP Listener,"I ran a curl command and also SoupUI and received ""Couldn't connect to host"" error."
AppDynamics,61788037,nan,0,"2020/05/14, 05:04:16",False,"2020/05/14, 05:04:16",nan,3850795.0,35.0,0,34,AppDynamics HTTP Listener,I checked and couldn't find a port being listened too on the Linux server.
AppDynamics,61788037,nan,0,"2020/05/14, 05:04:16",False,"2020/05/14, 05:04:16",nan,3850795.0,35.0,0,34,AppDynamics HTTP Listener,AppD Reference:  https://docs.appdynamics.com/display/PRO45/Standalone+Machine+Agent+HTTP+Listener
AppDynamics,62078493,nan,0,"2020/05/29, 07:00:47",False,"2020/05/29, 07:00:47",nan,12985420.0,13.0,0,176,AppDynamics for Spring boot application,I am trying to configure AppDynamics for a Springboot project and I am getting following exception:
AppDynamics,62078493,nan,0,"2020/05/29, 07:00:47",False,"2020/05/29, 07:00:47",nan,12985420.0,13.0,0,176,AppDynamics for Spring boot application,I can provide more information if anybody is familiar with the exception
AppDynamics,62086855,nan,0,"2020/05/29, 16:14:47",False,"2020/05/29, 16:14:47",nan,9901194.0,1.0,0,10,configuring Appdynamics syntectical transaction in springboot project,I am working on spring boot and using gradle build tool.I am just wondering how to configure syntectical transaction in my project.
AppDynamics,62086855,nan,0,"2020/05/29, 16:14:47",False,"2020/05/29, 16:14:47",nan,9901194.0,1.0,0,10,configuring Appdynamics syntectical transaction in springboot project,Any response will be highly appreciated.
AppDynamics,62257195,nan,0,"2020/06/08, 10:38:43",False,"2020/06/08, 10:38:43",nan,13049332.0,76.0,0,79,How to combine multiple metrics of appdynamics in to single query in grafana,we installed grafana-appdynamics plugin in the browser.
AppDynamics,62257195,nan,0,"2020/06/08, 10:38:43",False,"2020/06/08, 10:38:43",nan,13049332.0,76.0,0,79,How to combine multiple metrics of appdynamics in to single query in grafana,appdynamics shows metrics for individual nodes within cluster (it wont sum up for eg: Active sessions)
AppDynamics,62257195,nan,0,"2020/06/08, 10:38:43",False,"2020/06/08, 10:38:43",nan,13049332.0,76.0,0,79,How to combine multiple metrics of appdynamics in to single query in grafana,I wrote different metrics/queries for each node to fetch from appdynamics and plot in grafana under single panel.. now my question is how to sum up the all the values which are output of all the nodes at a given point?
AppDynamics,62675592,nan,0,"2020/07/01, 14:15:13",False,"2020/07/01, 14:15:13",nan,4845044.0,113.0,0,32,AppDynamics: How to Remove user data in android,"As per app dynamics android documentation, we can set a userdata using below code in android"
AppDynamics,62675592,nan,0,"2020/07/01, 14:15:13",False,"2020/07/01, 14:15:13",nan,4845044.0,113.0,0,32,AppDynamics: How to Remove user data in android,But is there a way to remove user data?
AppDynamics,62675592,nan,0,"2020/07/01, 14:15:13",False,"2020/07/01, 14:15:13",nan,4845044.0,113.0,0,32,AppDynamics: How to Remove user data in android,"In ios, the App dynamics library has a method to remove."
AppDynamics,62675592,nan,0,"2020/07/01, 14:15:13",False,"2020/07/01, 14:15:13",nan,4845044.0,113.0,0,32,AppDynamics: How to Remove user data in android,below is the code
AppDynamics,62675592,nan,0,"2020/07/01, 14:15:13",False,"2020/07/01, 14:15:13",nan,4845044.0,113.0,0,32,AppDynamics: How to Remove user data in android,Is there a method to remove user data in android?
AppDynamics,62675592,nan,0,"2020/07/01, 14:15:13",False,"2020/07/01, 14:15:13",nan,4845044.0,113.0,0,32,AppDynamics: How to Remove user data in android,"My current implementation to remove userdata is by setting empty string, is that correct?"
AppDynamics,62675592,nan,0,"2020/07/01, 14:15:13",False,"2020/07/01, 14:15:13",nan,4845044.0,113.0,0,32,AppDynamics: How to Remove user data in android,or do we have similar method as that of IOS appdynamics method?
AppDynamics,62759383,nan,0,"2020/07/06, 18:31:04",False,"2020/07/06, 18:31:04",nan,13878615.0,1.0,0,25,How do I add an MSMQ to AppDynamics?,"I've logged in, Created a Tier, and I do not see the &quot;Node&quot; with the name of the server/msmq that I need to add."
AppDynamics,62759383,nan,0,"2020/07/06, 18:31:04",False,"2020/07/06, 18:31:04",nan,13878615.0,1.0,0,25,How do I add an MSMQ to AppDynamics?,What am I missing?
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,Current setting:
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,"• a collection of 30 or so microservices in Java, Node.JS and PHP running on Fargate (a very small portion of them on EC2 instances)
• AppDynamics agents installed on all of them but out of date by several months at best"
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,Desired outcome:
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,"• upgrade agents on a schedule (once a month)
• the download and unzip should happen on schedule but the deployment should be OKed once confirmed it doesn't introduce bugs"
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,What we have explored:
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,"• because of the second point above, we've dismissed the option of doing this in the Dockerfile
• creating a cronjob on an existing instance with similar function to download and unzip to EFS, EFS volume is then associated with the task definitions."
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,The problem we encountered is that EFS has a limit on associations per availability zone.
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,"• explored the option of using a Lambda written in Python
• currently exploring the option of automating it with Chef"
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,With the last two options we don't have very much expertise at all in the team.
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,Does anyone have experience with automating this?
AppDynamics,63056377,nan,0,"2020/07/23, 17:15:10",False,"2020/07/23, 17:15:10",nan,13361025.0,13.0,0,16,Automate AppDynamics agent upgrade,In your experience what would be the best and easiest way to achieve this?
AppDynamics,63346977,nan,0,"2020/08/10, 22:43:27",False,"2020/08/10, 22:43:27",nan,7748526.0,1.0,0,18,Is it possible to create AppDynamics Alert when my cache (vendor product) is down,I have a java application that writes some events to a cache (vendor product) .
AppDynamics,63346977,nan,0,"2020/08/10, 22:43:27",False,"2020/08/10, 22:43:27",nan,7748526.0,1.0,0,18,Is it possible to create AppDynamics Alert when my cache (vendor product) is down,I would like to set up a health rule violation in AppDynamics to detect when connections to the cache fails.
AppDynamics,63346977,nan,0,"2020/08/10, 22:43:27",False,"2020/08/10, 22:43:27",nan,7748526.0,1.0,0,18,Is it possible to create AppDynamics Alert when my cache (vendor product) is down,what would be the best way to do so?
AppDynamics,63517278,nan,1,"2020/08/21, 08:58:06",False,"2021/01/05, 16:39:58",nan,9242007.0,11.0,0,30,Appdynamics - Experience journey map - drop-off rate,We are using AppDynamics to monitor an iOS app.
AppDynamics,63517278,nan,1,"2020/08/21, 08:58:06",False,"2021/01/05, 16:39:58",nan,9242007.0,11.0,0,30,Appdynamics - Experience journey map - drop-off rate,In Experience Journey map in AppDynamics what does the &quot;drop-off rate&quot; mean?
AppDynamics,63517278,nan,1,"2020/08/21, 08:58:06",False,"2021/01/05, 16:39:58",nan,9242007.0,11.0,0,30,Appdynamics - Experience journey map - drop-off rate,drop-off rate
AppDynamics,63813920,nan,0,"2020/09/09, 17:35:40",False,"2020/09/09, 17:35:40",nan,4170795.0,3.0,0,178,Springboot + Spring Cache Abstraction (Lettuce) + APM (AppDynamics),"I have an app built with SpringBoot and Spring Cache Abstraction, using Redis through Lettuce."
AppDynamics,63813920,nan,0,"2020/09/09, 17:35:40",False,"2020/09/09, 17:35:40",nan,4170795.0,3.0,0,178,Springboot + Spring Cache Abstraction (Lettuce) + APM (AppDynamics),"I need to monitor via APM AppDynamics tool, but by default it only gets data from Jedis."
AppDynamics,63813920,nan,0,"2020/09/09, 17:35:40",False,"2020/09/09, 17:35:40",nan,4170795.0,3.0,0,178,Springboot + Spring Cache Abstraction (Lettuce) + APM (AppDynamics),"I can create an exit point in AppDynamics, but I need to know exactly which class and method is responsible for opening the connection and executing commands to REDIS."
AppDynamics,63813920,nan,0,"2020/09/09, 17:35:40",False,"2020/09/09, 17:35:40",nan,4170795.0,3.0,0,178,Springboot + Spring Cache Abstraction (Lettuce) + APM (AppDynamics),Can anyone help me with this issue?
AppDynamics,64915057,nan,0,"2020/11/19, 17:43:30",False,"2020/11/19, 17:43:30",nan,14670166.0,1.0,0,12,prevent duplication of dashboards in Appdynamics using API,I created python script to promote dashboard form one environment to other like from dev to test and then to prod using Custom Import and Export API ( https://docs.appdynamics.com/display/PRO45/Configuration+Import+and+Export+API )
AppDynamics,64915057,nan,0,"2020/11/19, 17:43:30",False,"2020/11/19, 17:43:30",nan,14670166.0,1.0,0,12,prevent duplication of dashboards in Appdynamics using API,"The problem that I am facing is, If I am sending a dashboard which is in DEV to TEST and if the same dashboard already exists in TEST, then ideally it should overwrite it, but is creating a duplicate for the same, which is not what I want, can you suggest something...."
AppDynamics,65134789,nan,1,"2020/12/03, 23:59:58",False,"2021/03/01, 14:02:52",nan,14759440.0,1.0,0,42,How to calculate Apdex score in AppDynamics?,I am wondering if there is a way I can use AppDynamics to calculate an Apdex score for my APIs and Apps.
AppDynamics,65134789,nan,1,"2020/12/03, 23:59:58",False,"2021/03/01, 14:02:52",nan,14759440.0,1.0,0,42,How to calculate Apdex score in AppDynamics?,"If so, what would be the best possible way to do so?"
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,We're using Appdynamics Java agent for monitoring our production applications.
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,We have noticed slow growth in memory and the application eventually stalls.
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,We ran a head dump on one of the JVMs and got the below reports.
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,"Problem Suspect 1: 
 The thread com.singularity.ee.agent.appagent.kernel.config.xml.a@ 0x1267......"
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,AD thread config Poller keeps local variable config size of 28546.79(15.89%) KB
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,"Problem Suspect 2: 
 280561 Instances of
com.singularity.ee.agent.appagent.services.transactionmonitor.com.exitcall.p loaded by com.singularity.ee.agent.appagent.kernel.classloader.d@ 0x6c000....
occupy 503413.3(28.05%) KB."
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,These instances are referenced from one instance of java.util.HashMap$Node[]...
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,We figured that these classes were from the Appdynamics APM that hooks on to the running JVM and sends monitored events to the controller.
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,"There is so much convoluted process associated with reaching out to the vendor, so I am wondering if there are any work arounds for this like we enabling our java apps with JMX and Appd getting the monitoring events from JMX rather than directly hooking on to the applications' JVM."
AppDynamics,65212248,nan,0,"2020/12/09, 09:19:41",False,"2020/12/09, 09:38:40","2020/12/09, 09:38:40",1680811.0,39.0,0,143,Appdynamics Agent connection causing memory leak in Java applications,Thanks for your suggestions.
AppDynamics,65710328,nan,1,"2021/01/13, 23:53:06",False,"2021/03/20, 18:19:18",nan,6350335.0,19.0,0,25,How Fetch AppDynamics Reports in Java,Do we have any opensource or any proven code which collects  the App Dynamics reports from App Dynamics Servers ?
AppDynamics,65942338,nan,0,"2021/01/28, 19:34:39",False,"2021/01/28, 19:34:39",nan,6144.0,57567.0,0,12,In AppDynamics ADQL is there a way to select distinct across multiple colums?,"In regular SQL, I could write a query like:"
AppDynamics,65942338,nan,0,"2021/01/28, 19:34:39",False,"2021/01/28, 19:34:39",nan,6144.0,57567.0,0,12,In AppDynamics ADQL is there a way to select distinct across multiple colums?,"However in ADQL syntax, the following query will work for a single column, but not for multiple:"
AppDynamics,65942338,nan,0,"2021/01/28, 19:34:39",False,"2021/01/28, 19:34:39",nan,6144.0,57567.0,0,12,In AppDynamics ADQL is there a way to select distinct across multiple colums?,Is there any options in ADQL ( https://docs.appdynamics.com/display/PRO21/ADQL+Reference ) to achieve the same end result?
AppDynamics,65942338,nan,0,"2021/01/28, 19:34:39",False,"2021/01/28, 19:34:39",nan,6144.0,57567.0,0,12,In AppDynamics ADQL is there a way to select distinct across multiple colums?,I tried the following &quot;hack&quot; (and with a small set of data it seems to work... but will timeout with a large set of data).
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,I'm new to APPDYNAMCS and looking for APPDYNAMICS Public Rest APIs for the below data.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,I'm able to find out a few of them but not all.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,Can someone help me with this?
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,Thanks in Advance
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,Looking for REST APIs for the below data.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,"1.Configuration Items( Business Application, servers, business service, etc) and relationship among them."
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,2.Service Map data.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,3.Raw Event.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,4.Alert data.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,5.Raw Metrics.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,6.Raw Logs.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,7.Raw Traces.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,8.SLO/SLI data.
AppDynamics,66096667,nan,1,"2021/02/08, 08:12:24",False,"2021/03/01, 13:19:27",nan,2155454.0,85.0,0,21,APPDYNAMICS Public Rest APIs info,"9.Real User Monitoring / Synthetic Monitoring data
10.User sessions data"
AppDynamics,65033383,nan,0,"2020/11/27, 09:16:55",False,"2020/11/27, 09:16:55",nan,13015756.0,129.0,0,21,Issue with Rest API calls for AppDynamics,I'm trying to get the list of existing Applications using python script.
AppDynamics,65033383,nan,0,"2020/11/27, 09:16:55",False,"2020/11/27, 09:16:55",nan,13015756.0,129.0,0,21,Issue with Rest API calls for AppDynamics,Here is my script which fails.
AppDynamics,65033383,nan,0,"2020/11/27, 09:16:55",False,"2020/11/27, 09:16:55",nan,13015756.0,129.0,0,21,Issue with Rest API calls for AppDynamics,I'm new to python scripting.
AppDynamics,65033383,nan,0,"2020/11/27, 09:16:55",False,"2020/11/27, 09:16:55",nan,13015756.0,129.0,0,21,Issue with Rest API calls for AppDynamics,Basically I want to login to the application first and list the existing applications using the python script.
AppDynamics,61329623,nan,0,"2020/04/20, 21:37:44",False,"2020/04/20, 21:37:44",nan,906048.0,1433.0,0,97,go build cannot find appdynamics go-sdk package,"go build  is unable to find the 'appdynamics' package, even though the GOPATH is properly set."
AppDynamics,61329623,nan,0,"2020/04/20, 21:37:44",False,"2020/04/20, 21:37:44",nan,906048.0,1433.0,0,97,go build cannot find appdynamics go-sdk package,"I downloaded the package, copied it onto the GOPATH:  ~/go/src/appdynamics  and ran  go install appdynamics ."
AppDynamics,61329623,nan,0,"2020/04/20, 21:37:44",False,"2020/04/20, 21:37:44",nan,906048.0,1433.0,0,97,go build cannot find appdynamics go-sdk package,I am using go v.1.10 on Ubuntu 18.4.
AppDynamics,61329623,nan,0,"2020/04/20, 21:37:44",False,"2020/04/20, 21:37:44",nan,906048.0,1433.0,0,97,go build cannot find appdynamics go-sdk package,Visual Studio Code is able see the package and code completion works within the IDE.
AppDynamics,61329623,nan,0,"2020/04/20, 21:37:44",False,"2020/04/20, 21:37:44",nan,906048.0,1433.0,0,97,go build cannot find appdynamics go-sdk package,"However, running  go get -fix -v appdynamics  produces the following error:"
AppDynamics,61329623,nan,0,"2020/04/20, 21:37:44",False,"2020/04/20, 21:37:44",nan,906048.0,1433.0,0,97,go build cannot find appdynamics go-sdk package,I have also tested this using the github.com/appdynamics namespace per the appdynamics go-sdk instructions.
AppDynamics,61329623,nan,0,"2020/04/20, 21:37:44",False,"2020/04/20, 21:37:44",nan,906048.0,1433.0,0,97,go build cannot find appdynamics go-sdk package,"Also, I am aware of all the other go build 'cannot find package'  questions  on S.O."
AppDynamics,31358408,nan,1,"2015/07/11, 17:52:28",False,"2015/07/12, 15:01:07",nan,3910376.0,1.0,0,154,Backend Service metrics in AppDynamics,My java application is connected to remote webservice application where appdynamics is not installed.
AppDynamics,31358408,nan,1,"2015/07/11, 17:52:28",False,"2015/07/12, 15:01:07",nan,3910376.0,1.0,0,154,Backend Service metrics in AppDynamics,I am seeing those services as backed services.
AppDynamics,31358408,nan,1,"2015/07/11, 17:52:28",False,"2015/07/12, 15:01:07",nan,3910376.0,1.0,0,154,Backend Service metrics in AppDynamics,The remote webservice application has multiple webservices.
AppDynamics,31358408,nan,1,"2015/07/11, 17:52:28",False,"2015/07/12, 15:01:07",nan,3910376.0,1.0,0,154,Backend Service metrics in AppDynamics,I want to track response time of each webservice separately.
AppDynamics,31358408,nan,1,"2015/07/11, 17:52:28",False,"2015/07/12, 15:01:07",nan,3910376.0,1.0,0,154,Backend Service metrics in AppDynamics,Should i create different tier for each service or resolve all services into single tier?
AppDynamics,31358408,nan,1,"2015/07/11, 17:52:28",False,"2015/07/12, 15:01:07",nan,3910376.0,1.0,0,154,Backend Service metrics in AppDynamics,Is there any other better way of doing this?
AppDynamics,29083565,29093613.0,3,"2015/03/16, 19:47:15",True,"2015/04/01, 21:48:07","2015/03/21, 22:56:44",306406.0,579.0,0,1052,How to get appdynamics to detect Apache Camel Business Transactions,has anybody gotten Appdynamics java agent to detect Apache Camel business transactions?
AppDynamics,29083565,29093613.0,3,"2015/03/16, 19:47:15",True,"2015/04/01, 21:48:07","2015/03/21, 22:56:44",306406.0,579.0,0,1052,How to get appdynamics to detect Apache Camel Business Transactions,Picking up files from a directory (polling) and then sending off to activemq.
AppDynamics,29083565,29093613.0,3,"2015/03/16, 19:47:15",True,"2015/04/01, 21:48:07","2015/03/21, 22:56:44",306406.0,579.0,0,1052,How to get appdynamics to detect Apache Camel Business Transactions,"Another case is camel deployed on apache karaf, need to track outgoing http calls using appDynamics"
AppDynamics,29083565,29093613.0,3,"2015/03/16, 19:47:15",True,"2015/04/01, 21:48:07","2015/03/21, 22:56:44",306406.0,579.0,0,1052,How to get appdynamics to detect Apache Camel Business Transactions,Best
AppDynamics,29346979,29370213.0,2,"2015/03/30, 16:04:05",True,"2015/05/31, 05:34:32",nan,2219920.0,3513.0,-1,465,how to configure appdynamics for multiple server?,"Hi I am fairly new to appdynamics and using it to configure my server for trial period, I have 3 tomcats, I followed the documentation I got to know that we need to put appagent and machineagent to through data back to controller, If I try to download appagent and machineagent jar file fromir their official site I always end up with the same version and which ever tomcat starts first I get data only for that machine
 
This is what I have used for tomcat  catalina.sh '"
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,"On our ASP.Net website, we've had some requests timeout."
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,"AppDynamics shows that the SQL procedure calls are returning in a matter of seconds, but we're spending 100+ seconds in SNIReadSyncOverAsync."
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,Does anyone know what this method is / does and why it would be taking that much time?
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,We're not using EF which is referenced in every question / post I've been able to find about it.
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,Thanks in advance
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,Update
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,"It's been a while and while we never came to a resolution as to why all of the time was being spent in SNIReadSyncOverAsync, I have a few thoughts."
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,"I think that in this case, it may have been the way that specific version of AppDynamics was reporting the time spent on the SQL calls, but I have no real data to back that up, just my guess from what I observed."
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,We eventually stopped seeing the time reported as being spent in SNIReadSyncOverAsync and it shifted to the queries themselves timing out.
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,That still didn't make a lot of since because the same queries would run instantly in SSMS on the same database.
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,"The ultimate answer ended up being related to ARITHABORT causing our application and SSMS to use two different execution plans (see  https://dba.stackexchange.com/a/9841 ), explaining why we couldn't reproduce the timeouts with SSMS."
AppDynamics,14993309,16543274.0,3,"2013/02/21, 03:53:47",True,"2018/04/18, 19:08:53","2017/04/13, 15:42:39",65560.0,1375.0,28,11363,What is SNIReadSyncOverAsync and why would it take a long time to complete?,"Once we resolved that, we were able to identify a few portions of the procedure that needed tuning and we haven't run into the unexplained timeouts or SNIReadSyncOverAsync since."
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,I am using
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,with G1 garbage collector.
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,JVM argumens are
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,"However, I am experiencing following Full GC scans without any apparent reason, how to get rid of them?"
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,GC log with some tail from preceding events:
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,Other similar ones:
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,Other similar issue reports:
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,"http://grokbase.com/t/openjdk/hotspot-gc-use/1192sy84j5/g1c-strange-full-gc-behavior 
   http://grokbase.com/p/openjdk/hotspot-gc-use/123ydf9c92/puzzling-why-is-a-full-gc-triggered-here 
   http://mail.openjdk.java.net/pipermail/hotspot-gc-use/2013-February/001484.html"
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,"I have been analyzing the issue using appdynamics profiler and I have found out that every time Full GC occurs, Code Cache (configured to its maximum) is full."
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,It seems like a bug in GC.
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,"See also the profiler image, two unnecessary Full GC:s in middle between 24/5 and 25/5."
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,"More importantly, they kill the server usability, because they last 60 seconds each:"
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,Profiler log image http://eisler.vps.kotisivut.com/logs/g1gc-code-cache-full-gc-bug-illustration.png
AppDynamics,14330329,16782833.0,2,"2013/01/15, 04:40:24",True,"2017/06/13, 09:55:40","2017/06/13, 09:55:40",1335717.0,1275.0,15,8022,Why is G1 Full GC being triggered seeminly unnecessarily?,"See also discussion about Azul's pauseless GC, they seem to have worked out this kind of issues  http://www.artima.com/lejava/articles/azul_pauseless_gc.html"
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,I'm developing a social-like application which is currently deployed using AWS services.
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,"In particular, the DB runs on RDS using MYSQL."
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,"So far, we're testing the app using a limited number of users (mostly friends) resulting in an average of 15 Write IOPS/sec."
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,"The real problem is related to the very high writing latency of the db, which is always above 100ms."
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,The RDS instance is a db.m3.xlarge which is much more than what we need.
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,"I tried to perform a load test in a separate instance (identical configuration of DB and EC2) but i've not been able to reproduce such a high latency, even if I was sending a much higher number of requests."
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,"So I thought it may be due to table fragmentation, but i've not yet run a table optimisation, because the db wouldn't be accessible during this procedure."
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Do you have any experience with this problem?
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,MORE INFO
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,The biggest table (called  Message ) has about 790k rows.
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,"Concerning this table, the following query"
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,took 11s to be executed.
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,"Even worse, the query"
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,"took 14s, but the table Comment has about 160k."
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Those two tables are generated by:
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,and
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,SOME PLOTS
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Using  AppDynamics  I've been able to extract the following plots:
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Wait States : Isn't the query end time too big?
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Page Buffer :
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Write Latency and Queue :
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Query Cache
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Thank for your help!
AppDynamics,28436858,28481902.0,2,"2015/02/10, 18:35:13",True,"2018/08/13, 11:49:47","2015/02/11, 18:44:44",3132445.0,525.0,9,11015,MySQL High Write Latency,Andrea
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,"For one installation of our application we have been seeing issues on production that were reported as ""system is getting slower"" or ""requests never returning"" by the users."
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,In the end the server had to be restarted.
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,We had several of those incidents and a nightly restart of the server seems to be working as a workaround.
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,Our application makes heavy use of dynamic classloading (.jar files stored in database as blobs) and reflection.
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,Environment details:
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,We switched to
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,but it looks like we are still facing the issue.
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,"What we are seeing in the logfiles, heapdumps, threadumps and gc logs so fare is the following"
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,What we are seeing is
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,See below for an excerpt of the threadump:
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,The situation after updating to Java 1.7.80 / G1Gc seems to be similar.
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,"Unfortunately no threadump available, just wicket warnings in the log)"
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,We are currently unable to reproduce this (still working on that).
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,But maybe somebody in the community has seen something similar and has an idea what could help us to reproduce or solve the issues.
AppDynamics,35971200,49953075.0,1,"2016/03/13, 16:03:37",True,"2018/04/21, 09:34:37",nan,812093.0,639.0,7,1651,Possible reasons for threads getting stuck in getDeclaredConstructors?,"One guess that we currently having is that this is related to native memory consumption (Because of information like  http://www.ibm.com/developerworks/java/library/j-nativememory-linux/ ) but we don't see any hints in this regards in the logs (no OutOfMemory errors, no reports from the linux administrators that the system is running out of memory)"
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,Because of a known (fixed) bug in Hazelcast 2.5 we've decided this would be the next upgrade candidate for our project.
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,But after dropping in the latest version (3.2.2) we had horrible performance.
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,The way we are using Hazelcast:
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,"Using Hazelcast 2.5 we had great performance when, instead of using  map.values() , we supplied a list of all contained keys  map.getAll(containedKeys) ."
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,The way we keep track of the containedKeys by adding an  EntryListener  to the map which stores the containedKeys in a concurrent set.
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,"This was added by a colleague and feels like a hack, but works like a charm."
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,"Now when we upgrade to Hazelcast 3.2.2 we instantly see problems with  java.io , for example look at the following snippet from AppDynamics:"
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,"This is something we haven't seen in Hazelcast 2.5, but do have in 3.2.2."
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,It grinds our application to a complete standstill.
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,Replacing the jar with 2.5 again (and renaming Entry back to MapEntry) and nothing is wrong.
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,What could be causing this?
AppDynamics,24261110,nan,1,"2014/06/17, 13:11:19",False,"2015/03/09, 09:38:07",nan,442274.0,822.0,7,860,Performance drop upgrading from Hazelcast 2.5 to 3+,Maybe it isn't using the near-cache anymore?
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,I have a website written in cakephp on linux server.
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,I have a problem with extremly slow download time of my css and js files.
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,"For example, thats the network tab in chrome when loading my homepage:"
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,"As you can see, one of my css files took 59 seconds to download!"
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,Its important to note that it is not always the same css file.
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,"Sometimes its JS file, sometimes other css but they have to be downloaded before other content of the page is displayed, therefore they block the page loading."
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,"Because of waiting for that one file to download, website is not displayed for 59 seconds."
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,"I checked my server and it has a very low load, cpu runs on 10% and there is less than 20% of ram used."
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,Its an apache server with the following prefork settings:
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,This mentioned slow download time happened with maybe 3-4 simultaneous users on the website.
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,I have my app under APM with appdynamics and nothing suspicious is shown there.
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,I checked php.ini file with server admin and everything seems to be good there as well.
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,What other software can I use to find the source of this issue?
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,There is not much info in apache logs either.
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,Any suggestions would be greatly appreciated
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,EDIT:
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,I moved all of my assets to webroot and got these results on another domain that is using that same server:
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,"As you can see, this time its jquery file that took 27 seconds to download."
AppDynamics,32717415,nan,2,"2015/09/22, 15:53:02",True,"2016/10/21, 23:32:03","2015/09/22, 17:20:34",2083691.0,421.0,6,2884,css and JS files take too long to download,It is stored in the app/webroot
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,We have a production web application running on our intranet which:
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,is configured with:
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,Each day the heap usage:
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,"at which point the heap rises to 55% in about 40 minutes and is collected back to 37%, ad infinitum until the next restart."
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,"We have AppDynamics installed on the JVM and can see that Major Garbage Collections take place roughly every minute without much of an impact on the memory (except the falls outlined above of course) until the memory reaches 37%, when the Major collections become much less frequent."
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,"There are obviously hundreds of factors external to the behaviour of a web application, but one avenue of research is the fact that Hotspot JIT information is obviously lost when the JVM is stopped."
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,Are there GC optimisations/etc which are also lost with the shutdown of the JVM?
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,Is the JVM effectively consuming more memory than it needs to because certain Hotspot optimisations haven't yet taken place?
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,Is it possible that we would get better memory performance from this application if the JVM wasn't restarted and we found another way to perform a backup of the database?
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,"(Just to reiterate, I know that there are a hundred thousand things that could influence the behaviour of an application, especially an application that hardly anyone else knows!"
AppDynamics,20975043,nan,1,"2014/01/07, 17:07:52",False,"2016/07/23, 21:57:52","2016/07/23, 21:56:59",68283.0,15063.0,6,196,Does the behaviour of the Java garbage collector evolve over time or get impacted by JIT?,I really just want to know whether there are certain things to do with the memory performance of a JVM which are lost when it is stopped)
AppDynamics,16090702,16108782.0,3,"2013/04/18, 21:40:57",True,"2013/04/19, 19:32:49",nan,nan,nan,6,840,Can the OS stop a Java process from garbage collecting?,I'm monitoring a production system with  AppDynamics  and we just had the system slow to a crawl and almost freeze up.
AppDynamics,16090702,16108782.0,3,"2013/04/18, 21:40:57",True,"2013/04/19, 19:32:49",nan,nan,nan,6,840,Can the OS stop a Java process from garbage collecting?,"Just prior to this event, AppDynamics is showing all GC activity (minor and major alike) flatline for several minutes...and then come back to life."
AppDynamics,16090702,16108782.0,3,"2013/04/18, 21:40:57",True,"2013/04/19, 19:32:49",nan,nan,nan,6,840,Can the OS stop a Java process from garbage collecting?,"Even during periods of ultra low load on the system, we still see our JVMs doing  some  GC activity."
AppDynamics,16090702,16108782.0,3,"2013/04/18, 21:40:57",True,"2013/04/19, 19:32:49",nan,nan,nan,6,840,Can the OS stop a Java process from garbage collecting?,We've never had it totally flatline and drop to 0.
AppDynamics,16090702,16108782.0,3,"2013/04/18, 21:40:57",True,"2013/04/19, 19:32:49",nan,nan,nan,6,840,Can the OS stop a Java process from garbage collecting?,Also - the network I/O flatlined at the same instance of time as the GC/memory flatline.
AppDynamics,16090702,16108782.0,3,"2013/04/18, 21:40:57",True,"2013/04/19, 19:32:49",nan,nan,nan,6,840,Can the OS stop a Java process from garbage collecting?,"So I ask: can something at the system level cause a JVM to freeze, or cause its garbage collection to hang/freeze?"
AppDynamics,16090702,16108782.0,3,"2013/04/18, 21:40:57",True,"2013/04/19, 19:32:49",nan,nan,nan,6,840,Can the OS stop a Java process from garbage collecting?,This is on a CentOS machine.
AppDynamics,41552371,nan,0,"2017/01/09, 18:21:57",False,"2017/01/09, 18:21:57",nan,146272.0,8462.0,5,72,How to handle native binaries with Yarn on multiple platforms?,"I have a nodejs project with a few modules, one of them (appdynamics) serving a native binary depending on the platform/architecture of the requester."
AppDynamics,41552371,nan,0,"2017/01/09, 18:21:57",False,"2017/01/09, 18:21:57",nan,146272.0,8462.0,5,72,How to handle native binaries with Yarn on multiple platforms?,This in combination with the fingerprinting of yarn in the lockfile creates an issue:
AppDynamics,41552371,nan,0,"2017/01/09, 18:21:57",False,"2017/01/09, 18:21:57",nan,146272.0,8462.0,5,72,How to handle native binaries with Yarn on multiple platforms?,Yarn does not allow this out of the box because of the aforementioned fingerprinting.
AppDynamics,41552371,nan,0,"2017/01/09, 18:21:57",False,"2017/01/09, 18:21:57",nan,146272.0,8462.0,5,72,How to handle native binaries with Yarn on multiple platforms?,"For now the only workaround I've found is to mount the codebase into a Docker container and run yarn commands from there, then commit the lockfile changes."
AppDynamics,41552371,nan,0,"2017/01/09, 18:21:57",False,"2017/01/09, 18:21:57",nan,146272.0,8462.0,5,72,How to handle native binaries with Yarn on multiple platforms?,Is there a better way to achieve this?
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,"We have serious application issue at peak time application get very very slow and when i check on AppDynamics matrix, my heap memory is full and GC kicked in every minute and that make it very very slow."
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,here is the configuration of my java (tomcat)
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,Java options are  -Djava.awt.headless=true -Xmx2048m -XX:MaxPermSize=256m -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+DisableExplicitGC
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,Major GC collection time spend per min (ms)
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,CMS Old Gen usage in MB
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,Par Eden space in MB
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,Any suggestion why par eden space and old gen hitting hard line?
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,"Here is the last 12 Hour picture of Heap usage and Major GC collection (in Green dots), GC was very high between  3:00AM to 7:00AM  but when i restart application around  7:30AM  everything is good and application response time was very fast, why reboot fixed everything?"
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,Major GC collection time spend per min (ms) after 4GB (Zero Major GC)
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,CMS Old Gen usage in MB after 4GB Heap
AppDynamics,18276208,18276354.0,3,"2013/08/16, 17:53:19",True,"2013/08/19, 17:45:19","2020/06/20, 12:12:55",1159538.0,13579.0,5,2517,Java application slow because of heap,Par Eden space in MB after 4GB Heap
AppDynamics,53269696,53270016.0,1,"2018/11/12, 22:34:49",True,"2018/11/13, 02:14:49","2018/11/13, 02:14:49",3589882.0,43.0,4,846,How to prevent concurrency lock on flushBuffer in Spring Boot Tomcat Jackson?,I am having issues with concurrency when writing JSON out from my Spring Boot WAR app deployed to Tomcat 8.
AppDynamics,53269696,53270016.0,1,"2018/11/12, 22:34:49",True,"2018/11/13, 02:14:49","2018/11/13, 02:14:49",3589882.0,43.0,4,846,How to prevent concurrency lock on flushBuffer in Spring Boot Tomcat Jackson?,In the screenshot from AppDynamics there seems to be a considerable wait when the jackson library is performing _flushBuffer.
AppDynamics,53269696,53270016.0,1,"2018/11/12, 22:34:49",True,"2018/11/13, 02:14:49","2018/11/13, 02:14:49",3589882.0,43.0,4,846,How to prevent concurrency lock on flushBuffer in Spring Boot Tomcat Jackson?,This issue arises under load testing for even a small amount (&lt; 10) users.
AppDynamics,53269696,53270016.0,1,"2018/11/12, 22:34:49",True,"2018/11/13, 02:14:49","2018/11/13, 02:14:49",3589882.0,43.0,4,846,How to prevent concurrency lock on flushBuffer in Spring Boot Tomcat Jackson?,I have configured the messageConverters in my configuration class.
AppDynamics,53269696,53270016.0,1,"2018/11/12, 22:34:49",True,"2018/11/13, 02:14:49","2018/11/13, 02:14:49",3589882.0,43.0,4,846,How to prevent concurrency lock on flushBuffer in Spring Boot Tomcat Jackson?,"I am using 
Spring Boot 1.5.4
Java 1.8
Jackson 2.9.7
Tomcat 8.5.33"
AppDynamics,7409112,nan,1,"2011/09/14, 01:04:28",True,"2011/09/15, 05:34:03",nan,221951.0,11721.0,4,1094,Javamelody and Play Framework,Is it possible to monitor Play Framework application performance with Javamelody?
AppDynamics,7409112,nan,1,"2011/09/14, 01:04:28",True,"2011/09/15, 05:34:03",nan,221951.0,11721.0,4,1094,Javamelody and Play Framework,I'm using Javamelody with Spring apps.
AppDynamics,7409112,nan,1,"2011/09/14, 01:04:28",True,"2011/09/15, 05:34:03",nan,221951.0,11721.0,4,1094,Javamelody and Play Framework,I find it much better than free version of AppDynamics or Dyna Trace.
AppDynamics,7409112,nan,1,"2011/09/14, 01:04:28",True,"2011/09/15, 05:34:03",nan,221951.0,11721.0,4,1094,Javamelody and Play Framework,You can't use filter for HTTP monitoring or aspect for method monitoring.
AppDynamics,7409112,nan,1,"2011/09/14, 01:04:28",True,"2011/09/15, 05:34:03",nan,221951.0,11721.0,4,1094,Javamelody and Play Framework,I think I should make something like filter or aspect.
AppDynamics,7409112,nan,1,"2011/09/14, 01:04:28",True,"2011/09/15, 05:34:03",nan,221951.0,11721.0,4,1094,Javamelody and Play Framework,I have no idea how to add performance monitoring to JDBC queries.
AppDynamics,7409112,nan,1,"2011/09/14, 01:04:28",True,"2011/09/15, 05:34:03",nan,221951.0,11721.0,4,1094,Javamelody and Play Framework,Any ideas?
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,My multi-jar app runs in Java 11 and shows a warning related to Log4j2:
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,WARNING: sun.reflect.Reflection.getCallerClass is not supported.
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,This will impact performance.
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,"It doesn't crash, but quite bothers me since the Operations team (AppDynamics monitor) has asked me about it."
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,"I read that I need to use the ""Multi-Release:true"" entry in the manifest, but I don't kow how to tell the Maven Assembly Plugin to add it."
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,I don't use any other plugin in the pom.xml.
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,Should I use the  Maven Shade Plugin  instead?
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,"Anyway, here's the Maven Assembly Plugin section of my pom.xml."
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,"The library I'm including (that I also wrote) uses Log4j 2 as a dependency, as shown below:"
AppDynamics,60741008,nan,1,"2020/03/18, 15:51:49",True,"2020/03/18, 18:46:54","2020/03/18, 18:46:54",9679725.0,622.0,3,1635,Maven Assembly Plugin - Multi-Release:true,How can I get rid of this warning?
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,We have developed set of APIs using spring boot.
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,"When performance test was run and hitting more than 5000 calls/minute, avg response time started increasing."
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,"When we investigated with the help of AppDynamics, more than 2% transactions are having slow response time (more than 1.5 seconds)."
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,But the CPU usage is still under 20%.
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,And all of them are waiting exactly at the same location i. e  org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827.
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,Below is the complete call graph.
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,What is the root cause for threads handing at this location?
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,Are there any SpringBooot properties to be updated to eliminate this?
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,"Following is the source code of RequestMappingHandlerAdapter:invokeHandlerMethod and it hangs at invocableMethod.invokeAndHandle(webRequest, mavContainer);"
AppDynamics,46387418,nan,0,"2017/09/24, 09:54:02",False,"2019/08/08, 14:52:12","2017/09/25, 09:21:57",997811.0,53.0,3,763,Spring boot threads are waiting org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827,"protected ModelAndView invokeHandlerMethod(HttpServletRequest request,
            HttpServletResponse response, HandlerMethod handlerMethod) throws Exception {"
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,"Recently, our company is focusing on performance of the application we are developing for long time."
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,"One thing we noticed during performance test, certain methods are making so many database calls (over 500 queries)"
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,"Then this brings a question like, which methods are doing so many calls and how should be prioritize which method to refactor first."
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,"When we initially try to refactor some of those methods, we observed that it is requiring a lot of effort to reduce the number of round-trips."
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,The reason is our data access layer is pretty much depending on NHibernate ORM framework and we figured out that we have totally misused Lazyloading configuration from the beginning of the development.
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,That is why number of round-trips are huge and impacting the performance a lot.
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,And just correcting Lazy Loading configuration creates a lot of regression.
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,"Thus, we somehow have to figure out a way to collect number of database call per http request."
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,I have seen some tools like Application Insight or AppDynamics provides overall result for all Dependent calls.
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,But I am just wondering is there a way to collect these traces differently than using those frameworks ?
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,"For instance, every time http request is made, can we have attribute in the controller that whenever ExecuteQuery() or SqlDataAdapter.Fill method is called within the call stack of the method, can it increase the counter."
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,I am looking for a solution something like this.
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,Any help is greatly appreciated.
AppDynamics,43771883,nan,1,"2017/05/04, 03:15:33",True,"2017/05/04, 12:49:36",nan,1493558.0,189.0,3,472,Measuring number of database calls per http request,Thank you in advance for all suggestions.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,We have a situation where we have a Grails 2.3.11 based application that uses Quartz (version 2.2.1 / Grails Quartz plugin version 1.0.2) jobs to do certain long running processes (1-5 minutes) in background so that a polling service allows the browser to fetch the progress.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,This is used primarily for import and export of data from the application.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,"For example, when the application first starts, the export for 200,000+ rows takes approx 2 minutes."
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,The following day the export takes 3+ minutes.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,The third day the export takes more than 6 minutes.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,We have narrowed the problem down to just the Quartz jobs.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,When the system is in the degraded state all other web pages respond with nearly identical response times as when the system is in optimal condition.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,It appears that the Quartz jobs tend to slowdown linearly or incrementally over the period of 2 to 3 days.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,"This may be usage related or time, for which we are uncertain."
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,We are familiar with the memory leak bug  reported by Burt Beckwith  and added the fix to our code.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,"We were experiencing the memory leak before but now memory management appears to be health, even when the job performance is 5-10x slower than"
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,The jobs use GORM for most of the queries.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,We've optimized some to use criterias with projects so they are light weight but haven't been able to change all the logic over so there are a number of Gorm objects.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,In the case of the exports we've changed the queries to be read-only.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,The logic also clears out the hibernate session appropriately to limit the number of objects in memory.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,Here are a few additional details:
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,Any suggestions would be greatly appreciated.
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,"Thanks,"
AppDynamics,37667200,nan,0,"2016/06/07, 00:30:52",False,"2016/06/07, 19:05:25","2016/06/07, 19:05:25",1522341.0,113.0,3,269,Grails Quartz job performance degrades / slows over time,John
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,The following code:
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,Has started throwing the following exception on SOME machines.
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,What could cause this?
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,EDIT: the machines that experience the error are running Windows Server 2008 R2.
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,Windows Server 2012 and desktop machines running windows 7 work fine.
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,"(this is true, but I now think a different issue is the relevant difference... see below)."
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,"EDIT: as an additional note, this occurred right after updating our codebase to Entity Framework 6.1.1.-beta1."
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,"In the above code, The IDisposable is a class which wraps an EF DbContext."
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,EDIT: why the votes to close?
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,EDIT: the stack trace of the failure ends at the  WeakReference&lt;T&gt;  constructor called in the above code:
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,EDIT: it turns out that the machines having issues with this were running  AppDynamics .
AppDynamics,24089896,nan,0,"2014/06/06, 23:09:50",False,"2014/06/12, 18:20:50","2014/06/12, 18:20:50",1142970.0,19142.0,3,133,Constructing a WeakReference&lt;T&gt; throws COMException,Uninstalling that seems to have removed the issue.
AppDynamics,19772523,22447659.0,1,"2013/11/04, 18:40:07",True,"2014/03/17, 07:40:09",nan,68283.0,15063.0,3,2745,Log output of JVisualVM,Are there any tools that can connect to a remote JVM and log to a file the information which is displayed in the Monitor tab of JVisualVM or the Overview tab of JConsole?
AppDynamics,19772523,22447659.0,1,"2013/11/04, 18:40:07",True,"2014/03/17, 07:40:09",nan,68283.0,15063.0,3,2745,Log output of JVisualVM,"I am aware of applications such as AppDynamics etc - this is for a little performance test for a machine which is already set up - even though we have AppDynamics licences, using AppDynamics isn't really an option in this scenario."
AppDynamics,6588296,6588384.0,1,"2011/07/05, 23:11:06",True,"2011/07/05, 23:36:43",nan,465179.0,24086.0,3,298,Solving intermittent Garbage Collection problem - Java,I have spring enterprise app running on JDK 1.6 under Windows 2008.
AppDynamics,6588296,6588384.0,1,"2011/07/05, 23:11:06",True,"2011/07/05, 23:36:43",nan,465179.0,24086.0,3,298,Solving intermittent Garbage Collection problem - Java,The app gets slow or unresponsive at random times.
AppDynamics,6588296,6588384.0,1,"2011/07/05, 23:11:06",True,"2011/07/05, 23:36:43",nan,465179.0,24086.0,3,298,Solving intermittent Garbage Collection problem - Java,I suspect it is memory leak and the GC is kicking into over drive.
AppDynamics,6588296,6588384.0,1,"2011/07/05, 23:11:06",True,"2011/07/05, 23:36:43",nan,465179.0,24086.0,3,298,Solving intermittent Garbage Collection problem - Java,How can I troubleshoot this without restarting JVM using java.exe -verbose:gc parameter?
AppDynamics,6588296,6588384.0,1,"2011/07/05, 23:11:06",True,"2011/07/05, 23:36:43",nan,465179.0,24086.0,3,298,Solving intermittent Garbage Collection problem - Java,I really cannot shutdown this app.
AppDynamics,6588296,6588384.0,1,"2011/07/05, 23:11:06",True,"2011/07/05, 23:36:43",nan,465179.0,24086.0,3,298,Solving intermittent Garbage Collection problem - Java,I'm planning on doing AppDynamics on it once I can restart it but for know what can I do?
AppDynamics,6588296,6588384.0,1,"2011/07/05, 23:11:06",True,"2011/07/05, 23:36:43",nan,465179.0,24086.0,3,298,Solving intermittent Garbage Collection problem - Java,What are my options?
AppDynamics,52680807,nan,2,"2018/10/06, 18:52:29",True,"2018/10/08, 13:21:09","2018/10/08, 10:37:20",6220162.0,415.0,2,5953,"How to assign cluster, namespace and pod name in kubernetes yaml file&#39;s environment variable","I have a requirement to pass  cluster, namespace and pod name to AppDynamics agent from my container deployed in Kubernetes cluster."
AppDynamics,52680807,nan,2,"2018/10/06, 18:52:29",True,"2018/10/08, 13:21:09","2018/10/08, 10:37:20",6220162.0,415.0,2,5953,"How to assign cluster, namespace and pod name in kubernetes yaml file&#39;s environment variable","I tried something as below, but that does not work."
AppDynamics,52680807,nan,2,"2018/10/06, 18:52:29",True,"2018/10/08, 13:21:09","2018/10/08, 10:37:20",6220162.0,415.0,2,5953,"How to assign cluster, namespace and pod name in kubernetes yaml file&#39;s environment variable",and
AppDynamics,52680807,nan,2,"2018/10/06, 18:52:29",True,"2018/10/08, 13:21:09","2018/10/08, 10:37:20",6220162.0,415.0,2,5953,"How to assign cluster, namespace and pod name in kubernetes yaml file&#39;s environment variable",Could anyone please help me here how to collect the detail and pass to AppD.
AppDynamics,52680807,nan,2,"2018/10/06, 18:52:29",True,"2018/10/08, 13:21:09","2018/10/08, 10:37:20",6220162.0,415.0,2,5953,"How to assign cluster, namespace and pod name in kubernetes yaml file&#39;s environment variable",Thanks in advance.
AppDynamics,50355471,nan,1,"2018/05/15, 19:50:00",False,"2020/02/27, 10:11:48","2018/05/16, 09:16:39",8979242.0,73.0,2,159,AppyDynamic reports error for FileNotFoundException,"Have a simple method for connection,"
AppDynamics,50355471,nan,1,"2018/05/15, 19:50:00",False,"2020/02/27, 10:11:48","2018/05/16, 09:16:39",8979242.0,73.0,2,159,AppyDynamic reports error for FileNotFoundException,"In case of URL path not being a valid one, FileNotFoundException is getting logged as an error in AppyDynamics."
AppDynamics,50355471,nan,1,"2018/05/15, 19:50:00",False,"2020/02/27, 10:11:48","2018/05/16, 09:16:39",8979242.0,73.0,2,159,AppyDynamic reports error for FileNotFoundException,How to prevent AppDynamics from catching these exceptions since as part of the code its handled as a boolean return but AppDynamics is flooded with FileNotFoundException.
AppDynamics,50355471,nan,1,"2018/05/15, 19:50:00",False,"2020/02/27, 10:11:48","2018/05/16, 09:16:39",8979242.0,73.0,2,159,AppyDynamic reports error for FileNotFoundException,Thanks in advance.
AppDynamics,50355471,nan,1,"2018/05/15, 19:50:00",False,"2020/02/27, 10:11:48","2018/05/16, 09:16:39",8979242.0,73.0,2,159,AppyDynamic reports error for FileNotFoundException,"Update 
As per AppDynamics documentation  https://docs.appdynamics.com/display/PRO44/Errors+and+Exceptions 
 An HTTP error response, such as a status code 404 or 500 response  get recorded as a transaction snapshot error."
AppDynamics,50355471,nan,1,"2018/05/15, 19:50:00",False,"2020/02/27, 10:11:48","2018/05/16, 09:16:39",8979242.0,73.0,2,159,AppyDynamic reports error for FileNotFoundException,As i know at this point in my code above response 404 is legitimate.
AppDynamics,50355471,nan,1,"2018/05/15, 19:50:00",False,"2020/02/27, 10:11:48","2018/05/16, 09:16:39",8979242.0,73.0,2,159,AppyDynamic reports error for FileNotFoundException,How can I modify my code to prevent AppDynamics showing it up ?
AppDynamics,50355471,nan,1,"2018/05/15, 19:50:00",False,"2020/02/27, 10:11:48","2018/05/16, 09:16:39",8979242.0,73.0,2,159,AppyDynamic reports error for FileNotFoundException,Any Suggestions will be helpful.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,I am working on the ASP.NET application which is used by 10K people approx.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,Till last week it was working fine and suddenly from past week it's performance is degraded.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,"Application is taking too long time to respond, also opening each page taking a lot of time(1-2 mins approx.)"
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,even though no complex calls or functionality is present.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,I have checked all stored procedures in database and all are working fine and their execution time is less than 5 seconds.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,"Also using SAAS AppDynamics tools I have checked all calls and request-response time in each page of the application, but it seems everything to be fine there also."
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,All calls between application server and data base server are happening in 2-3 seconds.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,I am not able to find where exactly the issue is.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,Is there any applicaiton like AppDynamics which can be helpful?
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,NOTE : I am using linked server in some stored procedures.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,Need help in identifying the issue.
AppDynamics,48503307,nan,0,"2018/01/29, 16:15:38",False,"2018/01/29, 16:15:38",nan,6300350.0,186.0,2,133,Application performance degraded suddenly - ASP.NET,Thanks in advance.
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,My node app's CPU usage is gradually increasing.
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,I have found that memory leaks are happening.
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,"Through AppDynamics, I have found that there is a significant amount of retained memory which keeps increasing over time under  processImmediate  call tree."
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,"As I drilled in, I found the problem was with  settlePromises  function."
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,I want to get your opinion on one particular usage of promises I have been using.
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,Looping of promises.
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,Below is a sample function structure of such usage.
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,The heap growth over an hour is plotted in the below picture
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,The above function has to perform a synchronous update with the objects in data array.
AppDynamics,47300483,nan,1,"2017/11/15, 08:08:35",False,"2017/11/15, 19:23:51","2017/11/15, 08:50:23",2528378.0,439.0,2,632,Memory leak - promises in a loop,Is there a chance of memory leak with this?
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,I'm very new to nodejs.
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,"In my dockerized environment, I want to provide appdynamics support to nodejs apps."
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,This mandates every app to require the following as the first line in their app.
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,I plan to do that by providing a wrapper called  appdynamics.js  around the app's entry file.
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,Details:
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,"I run a script in my nodejs docker image to replace the entry file name in the app's package.json with ""appdynamics.js"", where appdynamics.js has the above appdynamics related require statement."
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,"Ex :  {scripts { ""start"" : ""node server.js"" }}  will be replaced with 
  {scripts { ""start"" : ""node appdynamics.js""}}"
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,"Then, i ""require"" the ""server.js"" inside appdynamics.js."
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,Invoke npm start.
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,My only concern is this:
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,"If the  package.json  had something like scripts  { ""start"" : ""coffee server.coffee"" } , my script will replace it to  { ""start"" : ""coffee appdynamics.js"" } ."
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,"and then my script will invoke  npm start , which will error out."
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,What is the best way to solve this?
AppDynamics,38600280,nan,1,"2016/07/27, 00:34:56",True,"2016/07/28, 09:30:52","2017/05/23, 13:28:45",6147402.0,21.0,2,12939,Nodejs app with npm start script,This is a follow up question to  Use &quot;coffee&quot; instead of &quot;node&quot; command in production
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,I am creating a REST api to send message to RabbitMQ and was trying to understand what are the best practice for creating/closing channels.
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,I am using RabbitMQ Java client api.
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,Currently I have a class  RabbitMQPublisherConnection  where I spring inject RabbitMQ connection.
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,This class is then spring injected to another class  RabbitMQPublisherChannel .
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,This class has the following function to create a channel:
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,Now I have the third class  RabbitMQPublisher  where I spring inject  RabbitMQPublisherChannel  class.
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,My application context looks like this:
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,The class  RabbitMQPublisher  has the function to publish a message to RabbitMQ:
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,This application is run through tomcat and I noticed with AppDynamics that the closing the channel takes like 47% of the total time taken to publish message.
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,When I remove the call to close the channel then I save this 47% of time which is like 32ms but then I notice in my RabbitMQ management console that the number of channel is ever increasing for that connection.
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,So my questions are -
AppDynamics,29023156,29033676.0,1,"2015/03/13, 03:07:29",True,"2015/03/13, 15:37:34",nan,596990.0,1367.0,2,3238,RabbitMQ channel best practice,Thanks
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"So, detailed description."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,I'm working on a microservice framework that uses Rabbit as the event bus.
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"Each service runs on it's own dedicated VM inside a Tomcat container (4 cores, 4GB RAM of which 2 are available to Tomcat)."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,Each service both consumes and publishes messages back to Rabbit.
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"When I crank up the consumers, channel settings and prefetch size I can get an individual service to perform well."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"My problem comes when I try to test scalability, i.e."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,a 2nd VM instance with the service running on it.
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"Instead of the throughput doubling (or at least increasing), it can actually get slower, and I'm very confused."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"I've checked for errors and exceptions in the service, used analytics tools (AppDynamics) to check the time spent in the service and the resources used, and everything looks fine, so as far as I can tell it's my Rabbit configuration that's the problem."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"The specific settings used to achieve high performance for one service are:
-Consumers: 20."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"-Channel cache size: 200
-Prefetch: 500"
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,Using this it seems to work quite well.
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"However when I add the second service the queues aren't being consumed as fast and they start to back up quite quickly, and I'm at a loss to understand why."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,I've experimented a little with the settings above but can't seem to get anywhere.
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"I don't have any access to the Rabbit cluster to change settings so I can't do anything there, but I have full control over the VM my service runs in (Java settings, Tomcat, Rabbit settings..)"
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"The service doesn't do anything explicit with connections or ack policies, so it's possible they may need to be tweaked?"
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"A few articles mention that there should be one channel per consumer (or even 1 for consume and 1 for publish), but that makes things slower than the larger figure above.."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"I'm at a loss, so any help is appreciated, more details can be provided."
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"Java: 7
Spring Core: 3.2.2
Rabbit AMQP: 3.1.2
Spring Rabbit: 1.3.5
Spring AMQP: 1.3.5"
AppDynamics,26555066,nan,1,"2014/10/24, 22:37:17",True,"2014/10/27, 21:06:16","2014/10/24, 23:35:29",702498.0,41.0,2,187,RabbitMQ throughput falls off when a second process consumers from a queue,"EDIT: I'm using a ConnectionFactory in Spring XML config (defaults to a CachingConnectionFactory I believe) that I set the channel cache size on, and then set the factory into the listener container, dunno if that helps.."
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"I have an Spring+Hibernate+Tomcat+MySql application in production, I'm running into a problem."
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"I think the application is not closing it's jdbc connections, and when it reaches its limits (currently 200), the application stop responding, and I have to restart tomcat."
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,Do I need to close this connections somewhere ?
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,Here is my Datasource:
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"And here is an image of the appdynamics monitoring the connections, from 3 days until now"
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,Here is a excerpt of the error I get on the catalina.out log file:
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,type Exception report
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"message Request processing failed; nested exception is
  org.hibernate.exception.JDBCConnectionException: Cannot open
  connection"
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,description  The server encountered an internal error that prevented it from fulfilling this request.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,exception
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"org.springframework.web.util.NestedServletException: Request
  processing failed; nested exception is
  org.hibernate.exception.JDBCConnectionException: Cannot open
  connection
  org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:932)
  org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:816)
  javax.servlet.http."
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,root cause
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"org.hibernate.exception.JDBCConnectionException: Cannot open
  connection
  org.hibernate.exception.SQLStateConverter.convert(SQLStateConverter.java:99)
  org.hibernate.exception.JDBCExceptionHelper.convert(JDBCExceptionHelper.java:66)
  org.hibernate.exception.JDBCExceptionHelper.convert(JDBCExceptionHelper.java:52)
  org.hibernate.jdbc.ConnectionManager."
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,root cause
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException:
   Too many connections 
  sun.reflect.GeneratedConstructorAccessor67.newInstance(Unknown Source)
  sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
  java.lang.reflect.Constructor.newInstance(Constructor.java:513)
  com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
  com.mysql.jdbc.Util.getInstance(Util.java:381)
  com.mysql.jdbc.SQLError."
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,UPDATE
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,The Category domain object is mapped like this:
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"So my guess is, taking mithridas comments into account, that I´m using the hibernate  session´s manually, And I would need to close them with something like this:"
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,Or i could implement @PersistenceContext.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,Would anyone direct me to this implementations so I can evaluate which is best for us to use ?
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,Thank you.
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"UPDATE2: 
Added more info in answer to James Massey comment:"
AppDynamics,23480106,23682727.0,2,"2014/05/05, 22:20:04",True,"2014/05/15, 18:33:56","2014/05/06, 14:16:24",360903.0,962.0,2,7934,My spring+hibernate app does not closes jdbc connections,"These are my: datasource, sessionFactory, transactionManager, and categoryDAO sessionFactory assignment:"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"I am confused where to set enviroment variables in Ubuntu 12.04
Now I am giving like this {editing 2 files to set path variables }"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,export JAVA_OPTS=&quot;$JAVA_OPTS -Xms1024M -Xmx2048M -XX:MaxPermSize=1024M -XX:PermSize=128M&quot;
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,#------------------- PATH SETTINGS ------------------#
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"#-------- Ant Home  
ANT_HOME=/programs/apache-ant-1.8.0"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"#-------- Maven Home  
M2_HOME=/programs/apache-maven-3.2.1"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"# --------- JDK 1.6 Home  
JAVA_HOME=/programs/java/jdk1.6.0_37"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"# ----------JDK 1.7 Home  
#JAVA_HOME=/programs/java/jdk1.7.0_09"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"# ------------- Path Settings  
PATH=$PATH:$JAVA_HOME/bin:$ANT_HOME/bin:$M2_HOME/bin"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"#----------Enabling AppDynamics Viewer---------  
PATH=$PATH:/programs/AppDynamicsLite/LiteViewer"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"I am getting no errors as all paths are set and i can use JAVA, JAVAC, ANT &amp; MAVEN
I am not prefixing export command to set paths in .profile
Only heap settings are put in .bashrc"
AppDynamics,23099992,nan,3,"2014/04/16, 07:28:07",True,"2014/04/16, 08:06:09","2020/06/20, 12:12:55",3021508.0,31.0,2,3962,Where to set environment variables in ubuntu 12.04,"but i havenot used ANT_OPTS and MAVEN_OPTS
Hence i am confused whether they are needed or not"
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",I am using a Java web application called AR System.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","After installing the PHP-Java Bridge, I started seeing  java.lang.OutOfMemoryError: PermGen space error  in the Tomcat logs."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","(I see in Windows Task Manager that there are 6 PHP-CGI.exe processes, all similar in memory footprint, give or take 5 MB)."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","It would occur every other day or so and then shortened to every day, sometimes twice a day."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Consequently, the application hangs and I have to restart it."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",And I added a Windows Task to restart Tomcat during non-peak hours to give me some cushion.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",I suspected a memory leak and started doing some research.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Normally, Tomcat sits at around 300-350 MB."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","With the PHP-Java Bridge, memory jumped up significantly."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","In fact, the error has occurred anywhere from 450-600 MB."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","I learned that default PermGen is 64MB and PermGen should be set to 1/4, up to 1/3 of Tomcat memory (sorry, I don't recall the link)."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Tomcat is running under Windows Services at this point, and I added the following to its properties:"
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",I enforced GC on PermGen memory and increased the size from the default 64 MB size to 128-256 MB.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Memory went up all the way to 800-850 MB, slowly, but it wasn't hanging during peak hours, albeit I still had Tomcat intentionally restart during non-peak hours, via a Windows Task."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","If I take off the restart, it  MIGHT  eventually hang but I haven't tried it."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",I still suspected a memory leak.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","I installed a trial version of AppDynamics to monitor the application, its memory, and run leak detection."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Additionally, to use tools like VisualVM and Memory Analyzer (MAT), I disabled The Tomcat Windows service and ran Tomcat from the Windows Command Line, via catalina.bat."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","I appended Java Options to the file; I made sure Tomcat memory was 1024 MB, Perm Gen was 128/256 MB, and ensured PHP-Java Bridge and AppDynamics was running."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","As of right now, PermGen is holding at 163 MB used, and AppDynamic's Automatic Leak Detection did not detect any leaks with any Java Collections."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","I fired up MAT, created a heap dump and analyzed for leaks."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","When I ran it yesterday, it found three possible suspects:"
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","When I ran it today, it found 2 possible suspects:"
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","So, with MAT and AppDynamics, it appears that no memory leaks were detected for classes directly related to the PHP-Java Bridge JAR files."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","I haven't tried using Plumbr, but I can't find the free beta version."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","The free version detects leaks, but you have to pay to see it."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Again, I don't have a source link at this time, but I recall reading that Tomcat 5.x  can  have performance and memory leak issues."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Of course, that doesn't mean everybody will have those issues, just a select number."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",I know Tomcat 6 and Tomcat 7 redesigned their memory management or how they structure memory.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","I also did speak with someone from BMC, the maker of AR System, and they said the current version of AR System I'm using could suffer from performance and memory issues."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","But, again, none of this was a problem before the PHP-Java Bridge."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",It was only after I installed it that this PermGen memory issue started.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Since the tools above did not report any leaks, does that mean there are no leaks and PHP-Java Bridge just needed more than 64 MB PermGen memory?"
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","Or, is there an inherit problem with my version of Tomcat and installing the PHP-Java Bridge just broke the proverbial camel's back?"
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",Upgrading to a newer version of AR System and Tomcat is not an option.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","If there is a leak, I can uninstall the PHP-Java Bridge or continue trying to find a leak and fix it."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",Any help would be appreciated.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",Thank you.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",Update 1
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","With MAT, I looked at the thread overview and stacks and you can see below that the PHP-Java Bridge contributes about 2/3 of the total heap memory of Tomcat."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",That's a lot of memory!
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","I think there is a leak, I do."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",I can't find any information on the PHP-Java Bridge having inherit memory leak issues.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","But, to me, it appears that the problem is not that Tomcat is leaking."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",Ideas?
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","AppDynamics couldn't find any leaks, even when I manually added classes that were suspected in MAT."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",What I'm wondering is perhaps the PermGen error is a symptom of that case where the program has no leak and needs more PermGen memory allotted.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","It would be helpful to know if the PHP-Java Bridge is designed to eat a lot of memory, this much memory; maybe it's optimized for 64-bit, since the current setup is a 32-bit Java Web application."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","If I knew that this bridge needs a lot of memory, I would say OK, fine, and go from there."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",But it certainly appears as if there is a memory leak somewhere in the chain.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",Update 2
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",I've been running Plumbr now for 2 hours and almost 10 minutes.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?",I see that Tomcat memory is shooting up to 960 MB and probably will continue to climb.
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","For those familiar with the program, the Java web application has been analyzed 3 times."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","So far, no leaks have been reported."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","If it stays this way, then the two conclusions I've arrived at are a) there are no leaks or b) there is a leak and, somehow, both AppDynamics and Plumbr missed it."
AppDynamics,18472943,nan,0,"2013/08/27, 21:31:43",False,"2019/07/03, 15:02:24","2013/08/29, 21:42:57",717236.0,4799.0,2,5070,"java.lang.OutOfMemoryError: PermGen space error, possible memory leak with Tomcat or PHP-Java Bridge?","If there are truly no leaks with this set of applications working together, then it must be that the Bridge uses a lot of memory and needs more PermGen memory than Tomcat's default, 64 MB -- at the very least, for 32-bit Java web applications."
AppDynamics,11916710,nan,2,"2012/08/11, 21:44:52",False,"2012/08/29, 11:08:08","2012/08/29, 11:04:43",892029.0,49711.0,2,391,Intra-JVM Profilers?,I've been reading up on  AppDynamics  Lite all morning and absolutely love it!
AppDynamics,11916710,nan,2,"2012/08/11, 21:44:52",False,"2012/08/29, 11:08:08","2012/08/29, 11:04:43",892029.0,49711.0,2,391,Intra-JVM Profilers?,"It's pretty nice to be able to drop a JAR into your web app (WAR), deploy it and have it automatically run perf tests on your app."
AppDynamics,11916710,nan,2,"2012/08/11, 21:44:52",False,"2012/08/29, 11:08:08","2012/08/29, 11:04:43",892029.0,49711.0,2,391,Intra-JVM Profilers?,I was wondering if anything similar exist for full-fledged profiling?
AppDynamics,11916710,nan,2,"2012/08/11, 21:44:52",False,"2012/08/29, 11:08:08","2012/08/29, 11:04:43",892029.0,49711.0,2,391,Intra-JVM Profilers?,"Something similar to, say, VisualVM, but that deploys as a JAR and that can be packaged inside a WAR?"
AppDynamics,11916710,nan,2,"2012/08/11, 21:44:52",False,"2012/08/29, 11:08:08","2012/08/29, 11:04:43",892029.0,49711.0,2,391,Intra-JVM Profilers?,Online searches didn't turn up much but then again I might not be searching for the right thing.
AppDynamics,11916710,nan,2,"2012/08/11, 21:44:52",False,"2012/08/29, 11:08:08","2012/08/29, 11:04:43",892029.0,49711.0,2,391,Intra-JVM Profilers?,"I call this an ""intra-JVM profiler"" because its profiling the same Java process its running inside of (like AppDynamics)."
AppDynamics,11916710,nan,2,"2012/08/11, 21:44:52",False,"2012/08/29, 11:08:08","2012/08/29, 11:04:43",892029.0,49711.0,2,391,Intra-JVM Profilers?,I love open source but this is not a mandatory requisite.
AppDynamics,11916710,nan,2,"2012/08/11, 21:44:52",False,"2012/08/29, 11:08:08","2012/08/29, 11:04:43",892029.0,49711.0,2,391,Intra-JVM Profilers?,Thanks in advance for any pointers/recommendations!
AppDynamics,62951468,nan,0,"2020/07/17, 12:49:19",False,"2020/07/17, 12:54:57","2020/07/17, 12:54:57",6234408.0,71.0,1,23,Loopback application middleware not working - No matching layer found errors,I have integrated my loopback application with appDynamics library and all the middlewares are not firing from then on.
AppDynamics,62951468,nan,0,"2020/07/17, 12:49:19",False,"2020/07/17, 12:54:57","2020/07/17, 12:54:57",6234408.0,71.0,1,23,Loopback application middleware not working - No matching layer found errors,Used the debug command - DEBUG=* npm start to get the logs and found these logs coming up.
AppDynamics,62951468,nan,0,"2020/07/17, 12:49:19",False,"2020/07/17, 12:54:57","2020/07/17, 12:54:57",6234408.0,71.0,1,23,Loopback application middleware not working - No matching layer found errors,All the middlewares declared through middleware.json and imperatively through app.middleware() command are having this error message -  No matching layer found
AppDynamics,62951468,nan,0,"2020/07/17, 12:49:19",False,"2020/07/17, 12:54:57","2020/07/17, 12:54:57",6234408.0,71.0,1,23,Loopback application middleware not working - No matching layer found errors,Any idea what's going wrong here - Which loopback package is responsible for express routing implementation
AppDynamics,60872992,61748113.0,1,"2020/03/26, 19:33:09",True,"2020/12/11, 16:46:36",nan,6664192.0,73.0,1,1010,Session &#39;app&#39;: Installation did not succeed. The application could not be installed: INSTALL_FAILED_INVALID_APK,"Hello good morning community, I am somewhat confused, I am integrating the Cisco AppDynamics tool, when performing the integration as mentioned in the documentation and when running the project it throws the following error."
AppDynamics,60872992,61748113.0,1,"2020/03/26, 19:33:09",True,"2020/12/11, 16:46:36",nan,6664192.0,73.0,1,1010,Session &#39;app&#39;: Installation did not succeed. The application could not be installed: INSTALL_FAILED_INVALID_APK,Event Log:
AppDynamics,60872992,61748113.0,1,"2020/03/26, 19:33:09",True,"2020/12/11, 16:46:36",nan,6664192.0,73.0,1,1010,Session &#39;app&#39;: Installation did not succeed. The application could not be installed: INSTALL_FAILED_INVALID_APK,build.gradel:
AppDynamics,60872992,61748113.0,1,"2020/03/26, 19:33:09",True,"2020/12/11, 16:46:36",nan,6664192.0,73.0,1,1010,Session &#39;app&#39;: Installation did not succeed. The application could not be installed: INSTALL_FAILED_INVALID_APK,build.gradel(:app)
AppDynamics,60872992,61748113.0,1,"2020/03/26, 19:33:09",True,"2020/12/11, 16:46:36",nan,6664192.0,73.0,1,1010,Session &#39;app&#39;: Installation did not succeed. The application could not be installed: INSTALL_FAILED_INVALID_APK,dependencies {
AppDynamics,60872992,61748113.0,1,"2020/03/26, 19:33:09",True,"2020/12/11, 16:46:36",nan,6664192.0,73.0,1,1010,Session &#39;app&#39;: Installation did not succeed. The application could not be installed: INSTALL_FAILED_INVALID_APK,}
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,I am trying to build a docker image and push it to AWS ECS.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,I am currently integrating an API with appdynamics.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,I need to issue a command to pip to start a proxy before the app opens.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,When running locally I do this with  pyagent proxy start  and it works fine.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,The problem comes when using that command with docker.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,When I attempt to use that command during the build and push process I get the following error stack.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,I run the app locally from mac and install the package to a pipfile on a mac.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,I know that darwin corresponds to macOS.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,Is this an issue because docker runs Linux or another issue I missing.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,The markers the error traces says it is ignoring appear in the pipfile.lock.
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,I would like some aid in understanding what the error means and any help is appreciated!
AppDynamics,60871312,nan,0,"2020/03/26, 18:01:47",False,"2020/03/26, 18:01:47",nan,7981821.0,449.0,1,111,How to fix errors when run pip in dockerfile?,Thank you!
AppDynamics,58873513,nan,1,"2019/11/15, 11:03:10",False,"2019/12/09, 18:27:29",nan,12377328.0,11.0,1,50,Cron expression to run all the time except every Sunday 10am to 3pm,"I need to suppress AppDynamics alerts on every Sunday between 10ma to 3pm and remaining all the time, they should run."
AppDynamics,58873513,nan,1,"2019/11/15, 11:03:10",False,"2019/12/09, 18:27:29",nan,12377328.0,11.0,1,50,Cron expression to run all the time except every Sunday 10am to 3pm,"To achieve this, i need to write a croj expression to satisfy the condition of ""run all the time except every Sunday 10am to 3pm""."
AppDynamics,58873513,nan,1,"2019/11/15, 11:03:10",False,"2019/12/09, 18:27:29",nan,12377328.0,11.0,1,50,Cron expression to run all the time except every Sunday 10am to 3pm,what could be the cron expression for this ?
AppDynamics,51988007,nan,0,"2018/08/23, 17:19:46",False,"2018/08/23, 17:22:07","2018/08/23, 17:22:07",3067860.0,296.0,1,301,Why could select 1 from dual be itermittently slow on the database side?,"Servers: Weblogic 12.1.1 (being upgraded soon, yes); Database: Oracle 12c"
AppDynamics,51988007,nan,0,"2018/08/23, 17:19:46",False,"2018/08/23, 17:22:07","2018/08/23, 17:22:07",3067860.0,296.0,1,301,Why could select 1 from dual be itermittently slow on the database side?,Our (very old) JSP monitoring page does  SELECT 1 FROM DUAL  to check for the database being up and a very basic check on response time.
AppDynamics,51988007,nan,0,"2018/08/23, 17:19:46",False,"2018/08/23, 17:22:07","2018/08/23, 17:22:07",3067860.0,296.0,1,301,Why could select 1 from dual be itermittently slow on the database side?,"We also have (newer) AppDynamics monitoring on our servers, which includes monitoring the monitoring page."
AppDynamics,51988007,nan,0,"2018/08/23, 17:19:46",False,"2018/08/23, 17:22:07","2018/08/23, 17:22:07",3067860.0,296.0,1,301,Why could select 1 from dual be itermittently slow on the database side?,"Lately we have been experiencing overall application slowness, and the monitoring page has been reflecting that."
AppDynamics,51988007,nan,0,"2018/08/23, 17:19:46",False,"2018/08/23, 17:22:07","2018/08/23, 17:22:07",3067860.0,296.0,1,301,Why could select 1 from dual be itermittently slow on the database side?,"In particular  SELECT 1 FROM DUAL  query which we expect to be very consistent has been intermittently slow...as in 2+ seconds just for that query as reported by AppDynamics, when a normal response time is 50ms."
AppDynamics,51988007,nan,0,"2018/08/23, 17:19:46",False,"2018/08/23, 17:22:07","2018/08/23, 17:22:07",3067860.0,296.0,1,301,Why could select 1 from dual be itermittently slow on the database side?,"Mostly our focus has been on the network because that seems the most likely, but we haven't found anything."
AppDynamics,51988007,nan,0,"2018/08/23, 17:19:46",False,"2018/08/23, 17:22:07","2018/08/23, 17:22:07",3067860.0,296.0,1,301,Why could select 1 from dual be itermittently slow on the database side?,What conditions on the database side could cause this?
AppDynamics,51988007,nan,0,"2018/08/23, 17:19:46",False,"2018/08/23, 17:22:07","2018/08/23, 17:22:07",3067860.0,296.0,1,301,Why could select 1 from dual be itermittently slow on the database side?,"The database is run by a separate team, and they're helpful but they're responsible for a large number of applications, so we get the best effect if we can ask for specific, measurable statistics."
AppDynamics,50618877,nan,3,"2018/05/31, 10:24:52",True,"2020/01/09, 21:37:32",nan,2334391.0,1032.0,1,3724,how to sent file(zip) using httprequest plugin jenkins,"I have a requirement to upload zip file to appDynamics, i need to use the httpsrequest plugin for that from my jenkins pipeline"
AppDynamics,50618877,nan,3,"2018/05/31, 10:24:52",True,"2020/01/09, 21:37:32",nan,2334391.0,1032.0,1,3724,how to sent file(zip) using httprequest plugin jenkins,upload request for appdynamics :
AppDynamics,50618877,nan,3,"2018/05/31, 10:24:52",True,"2020/01/09, 21:37:32",nan,2334391.0,1032.0,1,3724,how to sent file(zip) using httprequest plugin jenkins,we are using a shell to execute the above request now but I am trying to find out how to sent multiple zip files using  httpsRequest plugin
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,I am trying to assist in setting up AppDynamics with an Angular 2 app that is hosted in IIS.
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,The app is already up and running.
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,"There is a part I am having trouble on, the instructions for that part say say:"
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,"1) From the root directory of your Node.js application, run this command:
    npm install appdynamics@4.3.5
   For every Node.js application you are instrumenting, insert the following call in the application source code at the first line of the main module (such as the server.js file), before any other require statements:"
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,2) Restart you application
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,"I did step 1 locally in the console, but I don't know what to do for step 2."
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,"If I add that script to the page I get ""The Reference error: require is not defined""."
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,I learned that that function is not meant to run on the browser.
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,"It's meant to be run server-side, but I do not see node js or any server.js files on our dev web server."
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,Does anyone have any suggestions on where to put that snippet.
AppDynamics,47024920,47084941.0,1,"2017/10/31, 00:04:37",True,"2017/11/02, 23:29:17",nan,550621.0,190.0,1,573,App Dynamics for Angular 2 App in IIS - Node.js Agent Installation and Configuration,Will it even work with the current setup?
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,I am using supervisor to manage gunicorn process.
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,"[program:Test app]
command = /env/bin/pyagent run -c /etc/appdynamics.cfg -- /env/bin/gunicorn app:app --bind 0.0.0.0:8000 --worker-class sanic.worker.GunicornWorker
directory = /projects/app_dir/
autorestart=true"
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,Appdynamics versions; pip freeze
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,Appdynamics.cfg
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,"When i start the procees, i can see that agent is loaded properly and proxy is started as well."
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,But the problem is i dont see any data reported to controller when i generate load to my app.
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,(using wrk to generate load)
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,Agent and proxy  Logs does not have any info about app data/metrics.
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,Would really appreciate if someone can help me find out  the issue?
AppDynamics,45650349,nan,1,"2017/08/12, 15:36:25",True,"2017/11/16, 02:37:15","2017/08/12, 16:50:06",422769.0,27.0,1,549,Need Help: Python agent does not send any data to proxy and controller,"Thanks,
Manivasagan"
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,I want to Share the  AppDynamic's Dashboards  in an external website under an &lt; iframe   so that the reports(statistics) can be visible without logging into the  AppDynamic  tool on an external website.
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,Requirements :
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,"I tried doing this  ""Sharing a Custom Dashboard"""
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,"By clicking the ""copy shared URL"" I got the AppDynamics' particular dashboard's URL"
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,"Whenever I run that URL on  Chrome , it gives the following  error  :"
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,"and when I try on  IE  (Internet Explorer) , it throws this  error  :"
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,On Other tools like  Splunk  and  Sitecatalyst  there is a concept of Sharing the reports by embedded URLs.
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,Not sure how  AppDynamic  works
AppDynamics,41357203,nan,1,"2016/12/28, 09:18:35",False,"2017/04/13, 11:20:58","2016/12/28, 12:04:24",7348631.0,19.0,1,424,AppDynamic&#39;s Dashboard sharing to external website,Thanks.
AppDynamics,38635466,nan,1,"2016/07/28, 14:47:29",True,"2016/07/28, 17:40:52","2016/07/28, 15:10:30",5220147.0,11.0,1,1071,CloudForms and Openshift,"I have a generic question here and I have just started using Openshift enterprise and Origin but I would like to know the details on Cloudforms UI, I know that CloudForms UI can do a lot of things including managing Openshift instances but I would like to know the following in terms of managing Openshift instance, can CloudForms be able to do the following :"
AppDynamics,38635466,nan,1,"2016/07/28, 14:47:29",True,"2016/07/28, 17:40:52","2016/07/28, 15:10:30",5220147.0,11.0,1,1071,CloudForms and Openshift,What I am trying to find here is to see if CloudForms can provide an end to end Openshift solution.
AppDynamics,38635466,nan,1,"2016/07/28, 14:47:29",True,"2016/07/28, 17:40:52","2016/07/28, 15:10:30",5220147.0,11.0,1,1071,CloudForms and Openshift,"The end user must only have his/her code ready, rest everything could be within the UI."
AppDynamics,38635466,nan,1,"2016/07/28, 14:47:29",True,"2016/07/28, 17:40:52","2016/07/28, 15:10:30",5220147.0,11.0,1,1071,CloudForms and Openshift,Kindly let me know what all are possible and what all are not.
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,I need advice on the API's which can be used to do end to end monitoring on the Cloud instance.
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,"To give the complete picture, I have a model(R/SQL/Scala) running on C3 instance and i want to do end to end monitoring from data fetch to relevant output from the model."
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,The end to end monitoring also needs to be displayed on a dashboard.
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,The instance i am using is Linux.
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,"When i googled for this, i cam across lots of technologies like Zabbix, AppDynamics etc."
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,which are more sort of products.
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,Is there any existing API which is platform independent and can be integrated with multiple technologies like R/Scala etc?
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,"Also,If I am choosing the existing API do i need to be dependent on the developers for implementing that in my model?"
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,Or Shall i start from the scratch with REST/SOAP web service?
AppDynamics,33494034,nan,0,"2015/11/03, 10:00:05",False,"2015/11/03, 10:06:47","2015/11/03, 10:06:47",3266257.0,255.0,1,31,End to End Monitoring Framework on C3 cloud,Any help in this regard will be very much appreciated.
AppDynamics,30763900,nan,0,"2015/06/10, 21:00:18",False,"2015/11/22, 19:30:35","2015/11/22, 19:30:35",804954.0,2808.0,1,402,Dropwizard 0.7 service hangs while writing output,I have a dropwizard 0.7.0 service.
AppDynamics,30763900,nan,0,"2015/06/10, 21:00:18",False,"2015/11/22, 19:30:35","2015/11/22, 19:30:35",804954.0,2808.0,1,402,Dropwizard 0.7 service hangs while writing output,Occasionally (1 in 5000 requests) the service will spend 60 seconds writing its response.
AppDynamics,30763900,nan,0,"2015/06/10, 21:00:18",False,"2015/11/22, 19:30:35","2015/11/22, 19:30:35",804954.0,2808.0,1,402,Dropwizard 0.7 service hangs while writing output,AppDynamics is showing that 60 seconds are being spent inside com.sun.jersey.spi.container.servlet.WebComponent$Writer:write:300.
AppDynamics,30763900,nan,0,"2015/06/10, 21:00:18",False,"2015/11/22, 19:30:35","2015/11/22, 19:30:35",804954.0,2808.0,1,402,Dropwizard 0.7 service hangs while writing output,After 60 seconds we get the following exception:
AppDynamics,30763900,nan,0,"2015/06/10, 21:00:18",False,"2015/11/22, 19:30:35","2015/11/22, 19:30:35",804954.0,2808.0,1,402,Dropwizard 0.7 service hangs while writing output,We also see the following in the logs that shows that the connection is idle for at least 30 seconds:
AppDynamics,30763900,nan,0,"2015/06/10, 21:00:18",False,"2015/11/22, 19:30:35","2015/11/22, 19:30:35",804954.0,2808.0,1,402,Dropwizard 0.7 service hangs while writing output,What could cause such behaviour?
AppDynamics,30763900,nan,0,"2015/06/10, 21:00:18",False,"2015/11/22, 19:30:35","2015/11/22, 19:30:35",804954.0,2808.0,1,402,Dropwizard 0.7 service hangs while writing output,A couple of other observations:
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,"UPDATED Q : I have tried to run this sample eCommerce app on android studio 1.2.1, build 141.1903!"
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,[enter image description here][1]..
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,https://github.com/Appdynamics/ECommerce-Android
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,"and i did what is instructed to run it
but it keeps asking to upgrade gradle to gradle 2.0 or advance."
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,(after i downloaded the new version 2.4).
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,how to upgrade or integrate gradle in android studio?
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,what other problems related to it I have to solve?
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,error I'm getting
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,Error:
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0.
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,"Searched in the following locations:
    file //ANDRO1/gradle/m2repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom"
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,file://ANDRO1/gradle/m2repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,https // repo1.maven.org/maven2/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,https // repo1.maven.org/maven2/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,file /c/ Users/Piyush/.m2/repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,file /C /Users/Piyush/.m2/repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar
AppDynamics,30191131,30225687.0,2,"2015/05/12, 15:37:55",True,"2017/07/13, 18:32:16","2017/07/13, 18:32:16",4891404.0,45.0,1,1176,Could not find com.appdynamics:appdynamics-gradle-plugin:2.0,"Required by:
        ECommerce-Android-master:app:unspecified"
AppDynamics,27415368,27420289.0,1,"2014/12/11, 06:26:32",True,"2014/12/11, 12:20:03","2014/12/11, 06:47:10",655249.0,1224.0,1,3388,100% old generation - is it an issue?,"We are doing a load test of an application, and after some time AppDynamics reports  ""PS Old gen"" at 100% in red ."
AppDynamics,27415368,27420289.0,1,"2014/12/11, 06:26:32",True,"2014/12/11, 12:20:03","2014/12/11, 06:47:10",655249.0,1224.0,1,3388,100% old generation - is it an issue?,Full GC is running every 10 minutes.
AppDynamics,27415368,27420289.0,1,"2014/12/11, 06:26:32",True,"2014/12/11, 12:20:03","2014/12/11, 06:47:10",655249.0,1224.0,1,3388,100% old generation - is it an issue?,"Memory's ""Current Utilization"" varies between  70-90% , it goes like this for hours and never fails with OOM."
AppDynamics,27415368,27420289.0,1,"2014/12/11, 06:26:32",True,"2014/12/11, 12:20:03","2014/12/11, 06:47:10",655249.0,1224.0,1,3388,100% old generation - is it an issue?,"I thought that once old gen utilization is above certain level, GC will try to free/compact old gen area and if nothing is freed it would either fail with OOM and start crazy full GC cycles just before it."
AppDynamics,27415368,27420289.0,1,"2014/12/11, 06:26:32",True,"2014/12/11, 12:20:03","2014/12/11, 06:47:10",655249.0,1224.0,1,3388,100% old generation - is it an issue?,However I don't see any of these.
AppDynamics,27415368,27420289.0,1,"2014/12/11, 06:26:32",True,"2014/12/11, 12:20:03","2014/12/11, 06:47:10",655249.0,1224.0,1,3388,100% old generation - is it an issue?,The application runs fine with 100% old gen utilization.
AppDynamics,27415368,27420289.0,1,"2014/12/11, 06:26:32",True,"2014/12/11, 12:20:03","2014/12/11, 06:47:10",655249.0,1224.0,1,3388,100% old generation - is it an issue?,"We are using Oracle Java 7u14 (64b, 4 cpu cores, 10gb RAM) and JVM is configured with"
AppDynamics,27415368,27420289.0,1,"2014/12/11, 06:26:32",True,"2014/12/11, 12:20:03","2014/12/11, 06:47:10",655249.0,1224.0,1,3388,100% old generation - is it an issue?,Thank you!
AppDynamics,25134646,25416702.0,2,"2014/08/05, 11:47:42",True,"2014/12/24, 12:38:57","2014/08/05, 11:57:02",2069087.0,160.0,1,739,Setting up StrongLoop with my node applications,I'm currently testing platforms that provide a monitoring service for nodejs application.
AppDynamics,25134646,25416702.0,2,"2014/08/05, 11:47:42",True,"2014/12/24, 12:38:57","2014/08/05, 11:57:02",2069087.0,160.0,1,739,Setting up StrongLoop with my node applications,So I found (for now) StrongLoop and AppDynamics (recently acquire nodetime).
AppDynamics,25134646,25416702.0,2,"2014/08/05, 11:47:42",True,"2014/12/24, 12:38:57","2014/08/05, 11:57:02",2069087.0,160.0,1,739,Setting up StrongLoop with my node applications,Actually I'm testing StrongLoop service.
AppDynamics,25134646,25416702.0,2,"2014/08/05, 11:47:42",True,"2014/12/24, 12:38:57","2014/08/05, 11:57:02",2069087.0,160.0,1,739,Setting up StrongLoop with my node applications,"I have followed all the steps describe in the documentation but I can't see any data on the dashboard, only the StrongLoop Demo App."
AppDynamics,25134646,25416702.0,2,"2014/08/05, 11:47:42",True,"2014/12/24, 12:38:57","2014/08/05, 11:57:02",2069087.0,160.0,1,739,Setting up StrongLoop with my node applications,Here is all the steps :
AppDynamics,25134646,25416702.0,2,"2014/08/05, 11:47:42",True,"2014/12/24, 12:38:57","2014/08/05, 11:57:02",2069087.0,160.0,1,739,Setting up StrongLoop with my node applications,Any idea ?
AppDynamics,25134646,25416702.0,2,"2014/08/05, 11:47:42",True,"2014/12/24, 12:38:57","2014/08/05, 11:57:02",2069087.0,160.0,1,739,Setting up StrongLoop with my node applications,Tank you.
AppDynamics,19191781,nan,1,"2013/10/05, 01:31:28",True,"2013/10/11, 22:56:19",nan,1120092.0,4417.0,1,273,Profiling Azure Worker Role,I am looking for something to do for Azure Worker Roles what New Relic and AppDynamics can do for Azure Web Roles.
AppDynamics,19191781,nan,1,"2013/10/05, 01:31:28",True,"2013/10/11, 22:56:19",nan,1120092.0,4417.0,1,273,Profiling Azure Worker Role,I have tried both solutions for my background workers with little success.
AppDynamics,19191781,nan,1,"2013/10/05, 01:31:28",True,"2013/10/11, 22:56:19",nan,1120092.0,4417.0,1,273,Profiling Azure Worker Role,Am I missing something in configuration for either of these services or is there another service out there that can do what I need?
AppDynamics,19191781,nan,1,"2013/10/05, 01:31:28",True,"2013/10/11, 22:56:19",nan,1120092.0,4417.0,1,273,Profiling Azure Worker Role,"I am open to hosting my own service if there is an option I run ""locally""."
AppDynamics,19191781,nan,1,"2013/10/05, 01:31:28",True,"2013/10/11, 22:56:19",nan,1120092.0,4417.0,1,273,Profiling Azure Worker Role,What I'm looking for:
AppDynamics,19191781,nan,1,"2013/10/05, 01:31:28",True,"2013/10/11, 22:56:19",nan,1120092.0,4417.0,1,273,Profiling Azure Worker Role,Nice to have:
AppDynamics,11687049,11687150.0,3,"2012/07/27, 14:41:22",True,"2014/05/02, 09:58:07",nan,1527394.0,3733.0,1,6826,java: open-source APM (application performance management),I am looking for an open-source application/tool/technology which can show times of distributed request across distributed system.
AppDynamics,11687049,11687150.0,3,"2012/07/27, 14:41:22",True,"2014/05/02, 09:58:07",nan,1527394.0,3733.0,1,6826,java: open-source APM (application performance management),"I have found some wonderful stuff like  AppDynamics , but they are all commercial."
AppDynamics,11687049,11687150.0,3,"2012/07/27, 14:41:22",True,"2014/05/02, 09:58:07",nan,1527394.0,3733.0,1,6826,java: open-source APM (application performance management),"I don't need such a wide functionality, but simple request tracking."
AppDynamics,11687049,11687150.0,3,"2012/07/27, 14:41:22",True,"2014/05/02, 09:58:07",nan,1527394.0,3733.0,1,6826,java: open-source APM (application performance management),"I have also had a look on  this list , but I have some difficulties to understand it."
AppDynamics,11687049,11687150.0,3,"2012/07/27, 14:41:22",True,"2014/05/02, 09:58:07",nan,1527394.0,3733.0,1,6826,java: open-source APM (application performance management),Could you recommend some solutions if you are experienced with APM?
AppDynamics,7077885,nan,1,"2011/08/16, 15:01:03",False,"2011/09/01, 03:19:24",nan,224844.0,7521.0,1,1054,liferay running slow,I have installed Liferay-Tomcat 6.0.6 on one of my Linux machine having 4GB of RAM and it uses MySQL installed on a different machine.
AppDynamics,7077885,nan,1,"2011/08/16, 15:01:03",False,"2011/09/01, 03:19:24",nan,224844.0,7521.0,1,1054,liferay running slow,The liferay runs really slow even for 10 concurrent users.
AppDynamics,7077885,nan,1,"2011/08/16, 15:01:03",False,"2011/09/01, 03:19:24",nan,224844.0,7521.0,1,1054,liferay running slow,I have attached the screen shot taken from the AppDynamics which shows EhCache and C3PO both are responding slow at times.
AppDynamics,7077885,nan,1,"2011/08/16, 15:01:03",False,"2011/09/01, 03:19:24",nan,224844.0,7521.0,1,1054,liferay running slow,Are there any special config required for EhCache or C3PO?
AppDynamics,7077885,nan,1,"2011/08/16, 15:01:03",False,"2011/09/01, 03:19:24",nan,224844.0,7521.0,1,1054,liferay running slow,?
AppDynamics,7077885,nan,1,"2011/08/16, 15:01:03",False,"2011/09/01, 03:19:24",nan,224844.0,7521.0,1,1054,liferay running slow,I am currently running with default configurations.
AppDynamics,63590062,nan,1,"2020/08/26, 06:29:11",False,"2020/10/14, 20:36:25",nan,5928700.0,407.0,0,82,null is not an object (evaluating &#39;InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY&#39;),As I instrument my React native application with appdynamics the react native application gets the runtime error
AppDynamics,63590062,nan,1,"2020/08/26, 06:29:11",False,"2020/10/14, 20:36:25",nan,5928700.0,407.0,0,82,null is not an object (evaluating &#39;InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY&#39;),"'null is not an object (evaluating
'InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY')'"
AppDynamics,63590062,nan,1,"2020/08/26, 06:29:11",False,"2020/10/14, 20:36:25",nan,5928700.0,407.0,0,82,null is not an object (evaluating &#39;InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY&#39;),As I integrate it remains fine with the integration but as soon as I instument the app stops running.
AppDynamics,63590062,nan,1,"2020/08/26, 06:29:11",False,"2020/10/14, 20:36:25",nan,5928700.0,407.0,0,82,null is not an object (evaluating &#39;InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY&#39;),After integration I have used
AppDynamics,63590062,nan,1,"2020/08/26, 06:29:11",False,"2020/10/14, 20:36:25",nan,5928700.0,407.0,0,82,null is not an object (evaluating &#39;InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY&#39;),and
AppDynamics,63590062,nan,1,"2020/08/26, 06:29:11",False,"2020/10/14, 20:36:25",nan,5928700.0,407.0,0,82,null is not an object (evaluating &#39;InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY&#39;),on the top of the file.
AppDynamics,63590062,nan,1,"2020/08/26, 06:29:11",False,"2020/10/14, 20:36:25",nan,5928700.0,407.0,0,82,null is not an object (evaluating &#39;InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY&#39;),Also did all the steps of manual link for android .
AppDynamics,63590062,nan,1,"2020/08/26, 06:29:11",False,"2020/10/14, 20:36:25",nan,5928700.0,407.0,0,82,null is not an object (evaluating &#39;InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY&#39;),Is there something I am missing here?
AppDynamics,63180746,nan,0,"2020/07/30, 23:16:40",False,"2020/07/30, 23:16:40",nan,14024164.0,1.0,0,8,Custom pdf / bi reports from azure app insights,Is there an option to create a pdf/bi report from the azure app insights data?
AppDynamics,63180746,nan,0,"2020/07/30, 23:16:40",False,"2020/07/30, 23:16:40",nan,14024164.0,1.0,0,8,Custom pdf / bi reports from azure app insights,"After running performance tests , going to individual tabs(failure/performance/availability etc) is time taking exercise."
AppDynamics,63180746,nan,0,"2020/07/30, 23:16:40",False,"2020/07/30, 23:16:40",nan,14024164.0,1.0,0,8,Custom pdf / bi reports from azure app insights,Would like to know if there is a plug in/custom controls available like in &quot;CA-Wily&quot; or Appdynamics where you an export all monitoring data to a pdf and share with relevant stakeholders.
AppDynamics,63110700,nan,0,"2020/07/27, 10:35:46",False,"2020/07/27, 10:35:46",nan,3914559.0,25.0,0,46,System.Threading.WaitHandle.WaitOneNative shows long duration in .NET application,Our application is built on .NET 4.0 with Oracle 12c Version 1 as database.
AppDynamics,63110700,nan,0,"2020/07/27, 10:35:46",False,"2020/07/27, 10:35:46",nan,3914559.0,25.0,0,46,System.Threading.WaitHandle.WaitOneNative shows long duration in .NET application,I was assigned to find bottlenecks in the application from the database side.
AppDynamics,63110700,nan,0,"2020/07/27, 10:35:46",False,"2020/07/27, 10:35:46",nan,3914559.0,25.0,0,46,System.Threading.WaitHandle.WaitOneNative shows long duration in .NET application,I am using Appdynamics to find slowest database calls.
AppDynamics,63110700,nan,0,"2020/07/27, 10:35:46",False,"2020/07/27, 10:35:46",nan,3914559.0,25.0,0,46,System.Threading.WaitHandle.WaitOneNative shows long duration in .NET application,I have found that in most cases database queries are executed within few seconds but System.Threading.WaitHandle.WaitOneNative took over 20 seconds.
AppDynamics,63110700,nan,0,"2020/07/27, 10:35:46",False,"2020/07/27, 10:35:46",nan,3914559.0,25.0,0,46,System.Threading.WaitHandle.WaitOneNative shows long duration in .NET application,I do not understand what can be the reason for this.
AppDynamics,63110700,nan,0,"2020/07/27, 10:35:46",False,"2020/07/27, 10:35:46",nan,3914559.0,25.0,0,46,System.Threading.WaitHandle.WaitOneNative shows long duration in .NET application,We don't have a dedicated front end engineer to look at this.
AppDynamics,63110700,nan,0,"2020/07/27, 10:35:46",False,"2020/07/27, 10:35:46",nan,3914559.0,25.0,0,46,System.Threading.WaitHandle.WaitOneNative shows long duration in .NET application,Any help will be appreciated.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,I have come to know about opentracing and is even working on a POC with Jaeger and Spring.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,We have around 25+ micro services in production.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,I have read about it but is a bit confused as how it can be really used.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,I'm thinking to use it as a troubleshooting tool to identify the root cause of a failure in the application.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,"For this, we can search for httpStatus codes, custom tags, traceIds and application logs in JaegerUI."
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,"Also, we can find areas of bottlenecks/slowness by monitoring the traces."
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,What are the other usages?
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,Jaeger has a request sampler and I think we should not sample every request in Prod as it may have adverse impact.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,Is this true?
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,"If yes, then why and what can be the impact on the application?"
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,I guess it can't be really used for troubleshooting in this case as we won't have data on every request.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,What sampling configuration is recommended for Prod?
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,"Also, how a tool like Jaeger is different from APM tools and where does it fit in?"
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,I mean you can do something similar with APM tools as well.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,"For e.g., one can drill through a service's transaction and jump to corresponding request to other service in AppDynamics."
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,Alerts can be put on slow transactions.
AppDynamics,62998835,nan,1,"2020/07/20, 18:29:52",True,"2020/07/27, 17:38:53",nan,2841947.0,537.0,0,368,Usages of Opentracing tools like Jaeger,"One can also capture request headers/body so that they can be searched upon, etc."
AppDynamics,62980726,nan,0,"2020/07/19, 16:09:50",False,"2020/07/19, 16:09:50",nan,5737572.0,336.0,0,34,SSO for web data source in Power BI,"I have a weburl (Appdynamics) which allows SSO, I want to use this website as the data source in Power BI."
AppDynamics,62980726,nan,0,"2020/07/19, 16:09:50",False,"2020/07/19, 16:09:50",nan,5737572.0,336.0,0,34,SSO for web data source in Power BI,"If I let the credentials be the default windows one, Power BI still doesn't move past the login page."
AppDynamics,62980726,nan,0,"2020/07/19, 16:09:50",False,"2020/07/19, 16:09:50",nan,5737572.0,336.0,0,34,SSO for web data source in Power BI,We generally enter the account and get the message on Appdynamics
AppDynamics,62980726,nan,0,"2020/07/19, 16:09:50",False,"2020/07/19, 16:09:50",nan,5737572.0,336.0,0,34,SSO for web data source in Power BI,and it directly logs us in.
AppDynamics,62980726,nan,0,"2020/07/19, 16:09:50",False,"2020/07/19, 16:09:50",nan,5737572.0,336.0,0,34,SSO for web data source in Power BI,How to achieve this in Power BI?
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,"In our project, we are getting AppDynamics logs(application logs) and machine logs and sometime the the size of the logs increase which eats out the disk size."
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,What I am trying to do it is to get the content between two dates like 10 Nov and 13 Nov and delete the rest.
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,"Since we are working in windows environment, this needs to be done in powershell."
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,It is easier to handle such things in linux but I am not good at powershell scripting.
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,Below is the code snippet.
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,Code Snippet with file paths
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,The ERROR i get while executing the script.
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,"The powershell and windows version:
Name: Windows PowerShell ISE Host - Version : 5.1.14409.1018
Name: Microsoft Windows Server 2012 R2 Standard 64bit"
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,your help will be highly obliged.
AppDynamics,59191709,nan,1,"2019/12/05, 11:13:16",False,"2019/12/10, 12:20:26","2019/12/10, 12:20:26",5360236.0,39.0,0,272,how to fetch logs content between two dates in windows powershell?,"Best regards,"
AppDynamics,55505087,55505955.0,1,"2019/04/04, 01:12:17",True,"2019/04/04, 02:58:03",nan,5335874.0,45.0,0,62,Problems understanding the CURL -X POST command,I'm trying to create a user using Appdynamics' Configuration API
AppDynamics,55505087,55505955.0,1,"2019/04/04, 01:12:17",True,"2019/04/04, 02:58:03",nan,5335874.0,45.0,0,62,Problems understanding the CURL -X POST command,I'm trying to use this curl command but am not sure what the parameters after --user are.
AppDynamics,55505087,55505955.0,1,"2019/04/04, 01:12:17",True,"2019/04/04, 02:58:03",nan,5335874.0,45.0,0,62,Problems understanding the CURL -X POST command,"This is AppDynamics specific, a better understanding of tenancy or which my users are, or where i can create users would be helpful."
AppDynamics,55505087,55505955.0,1,"2019/04/04, 01:12:17",True,"2019/04/04, 02:58:03",nan,5335874.0,45.0,0,62,Problems understanding the CURL -X POST command,Also should I be using the pem key to communicate with my controller host.
AppDynamics,55505087,55505955.0,1,"2019/04/04, 01:12:17",True,"2019/04/04, 02:58:03",nan,5335874.0,45.0,0,62,Problems understanding the CURL -X POST command,"Here's the link to the Documentation page i'm referring to;
 https://docs.appdynamics.com/display/PRO44/Configuration+API#ConfigurationAPI-CreateandModifyAppDynamicsUsers"
AppDynamics,55011683,nan,0,"2019/03/05, 23:13:35",False,"2019/03/05, 23:20:24","2019/03/05, 23:20:24",584569.0,155.0,0,121,Nativescript: adding a run script phase,Hi I am developing a plugin to wrap around AppDynamics.
AppDynamics,55011683,nan,0,"2019/03/05, 23:13:35",False,"2019/03/05, 23:20:24","2019/03/05, 23:20:24",584569.0,155.0,0,121,Nativescript: adding a run script phase,I need to do the following from this documentation  https://docs.appdynamics.com/display/PRO45/Upload+the+dSYM+File
AppDynamics,55011683,nan,0,"2019/03/05, 23:13:35",False,"2019/03/05, 23:20:24","2019/03/05, 23:20:24",584569.0,155.0,0,121,Nativescript: adding a run script phase,"however since the platform files are regenerated each time, can I do this through hooks?"
AppDynamics,55011683,nan,0,"2019/03/05, 23:13:35",False,"2019/03/05, 23:20:24","2019/03/05, 23:20:24",584569.0,155.0,0,121,Nativescript: adding a run script phase,The  xcode_build_dsym_upload.sh  is part of the podFile and when I try with hooks (which i think is before the xCode build) I'm getting a  no file or directory found  error.
AppDynamics,55011683,nan,0,"2019/03/05, 23:13:35",False,"2019/03/05, 23:20:24","2019/03/05, 23:20:24",584569.0,155.0,0,121,Nativescript: adding a run script phase,Any ideas?
AppDynamics,54993328,nan,1,"2019/03/05, 01:46:07",False,"2019/03/05, 14:04:18",nan,2762956.0,110.0,0,110,"How to set custom header only for main http url/sampler, not embedded resources?",I have a website that requires a custom header to access.
AppDynamics,54993328,nan,1,"2019/03/05, 01:46:07",False,"2019/03/05, 14:04:18",nan,2762956.0,110.0,0,110,"How to set custom header only for main http url/sampler, not embedded resources?","How do I configure JMeter to only send this custom header to the main site URL/http request sampler, and not send it to any embedded resources such as appdynamics or googleapis?"
AppDynamics,54993328,nan,1,"2019/03/05, 01:46:07",False,"2019/03/05, 14:04:18",nan,2762956.0,110.0,0,110,"How to set custom header only for main http url/sampler, not embedded resources?","Right now, I have several ""HTTP Request"" samplers trying to act like a browser by using HTTP Defaults and checking the ""Retrieve All Embedded Resources"" box."
AppDynamics,54993328,nan,1,"2019/03/05, 01:46:07",False,"2019/03/05, 14:04:18",nan,2762956.0,110.0,0,110,"How to set custom header only for main http url/sampler, not embedded resources?","The request URL is in the form of "" https://example.com/path/ ."""
AppDynamics,54993328,nan,1,"2019/03/05, 01:46:07",False,"2019/03/05, 14:04:18",nan,2762956.0,110.0,0,110,"How to set custom header only for main http url/sampler, not embedded resources?",This part needs the custom header to access.
AppDynamics,54993328,nan,1,"2019/03/05, 01:46:07",False,"2019/03/05, 14:04:18",nan,2762956.0,110.0,0,110,"How to set custom header only for main http url/sampler, not embedded resources?","When retrieving embedded resources (like fonts.googleapis.com), the custom header should not be sent."
AppDynamics,54993328,nan,1,"2019/03/05, 01:46:07",False,"2019/03/05, 14:04:18",nan,2762956.0,110.0,0,110,"How to set custom header only for main http url/sampler, not embedded resources?",Any ideas on how I can get this configured?
AppDynamics,44038577,nan,1,"2017/05/18, 07:26:44",False,"2019/04/06, 12:20:54","2018/06/07, 11:23:33",8028692.0,3.0,0,549,Issue while attaching app-agent javaagent.jar with existing(and running jvm),My organization asked our team to use this new tool AppDynamics for better performance testing results and reports.
AppDynamics,44038577,nan,1,"2017/05/18, 07:26:44",False,"2019/04/06, 12:20:54","2018/06/07, 11:23:33",8028692.0,3.0,0,549,Issue while attaching app-agent javaagent.jar with existing(and running jvm),"For that I have to attach javaagent with running jvm,  on their community this step"
AppDynamics,44038577,nan,1,"2017/05/18, 07:26:44",False,"2019/04/06, 12:20:54","2018/06/07, 11:23:33",8028692.0,3.0,0,549,Issue while attaching app-agent javaagent.jar with existing(and running jvm),However when I run the same I get following result on cmd (Using windows-8 64 bit)
AppDynamics,44038577,nan,1,"2017/05/18, 07:26:44",False,"2019/04/06, 12:20:54","2018/06/07, 11:23:33",8028692.0,3.0,0,549,Issue while attaching app-agent javaagent.jar with existing(and running jvm),"java.lang.reflect.InvocationTargetException
Caused by: java.io.IOException: no such process 
Exception in thread ""main"" java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
Caused by: java.lang.reflect.InvocationTargetException
Caused by: java.io.IOException: no such process"
AppDynamics,44038577,nan,1,"2017/05/18, 07:26:44",False,"2019/04/06, 12:20:54","2018/06/07, 11:23:33",8028692.0,3.0,0,549,Issue while attaching app-agent javaagent.jar with existing(and running jvm),This is  the link  of their documentation.
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,The .NET web applications we build all integrate with a third party application through a WCF service.
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,Every time a page loads a number of WCF service calls are made to retrieve data that are used to populate some user controls.
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,Through AppDynamics I can tell that there could be up to 8 WCF calls to load a given page.
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,AppDynamics tells us that the WCF calls cost up to 85% of the load time.
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,This is a serious impact on developer productivity.
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,Is there a way to intercept all the outbound WCF calls from our .NET web application and stub them with fake data so that pages will not break and load faster?
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,The pages do not need these data to run in development environment.
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,Thanks for your input!
AppDynamics,39730780,nan,1,"2016/09/27, 20:22:01",False,"2016/09/27, 20:49:55",nan,239628.0,641.0,0,202,How to Intercept Outbound WCF Service Calls from a .NET Web Application,John
AppDynamics,26778994,27018574.0,3,"2014/11/06, 14:01:21",True,"2014/11/19, 15:53:11",nan,4141104.0,9.0,0,854,"How can I obtain statistics (requests per second, response time) for an app hosted on Cloud Foundry Pivotal?","I am trying to obtain statistics for an app which is hosted on my Cloud Foundry Pivotal without using any 3rd party applications like ""AppDynamics"" (or others)."
AppDynamics,26778994,27018574.0,3,"2014/11/06, 14:01:21",True,"2014/11/19, 15:53:11",nan,4141104.0,9.0,0,854,"How can I obtain statistics (requests per second, response time) for an app hosted on Cloud Foundry Pivotal?","Specifically, I want to find out the  'Requests per second' and 'Response Time'."
AppDynamics,26778994,27018574.0,3,"2014/11/06, 14:01:21",True,"2014/11/19, 15:53:11",nan,4141104.0,9.0,0,854,"How can I obtain statistics (requests per second, response time) for an app hosted on Cloud Foundry Pivotal?","I know that it is possible to access memory, disk space and cpu utilization by an app because Pivotal provides these statistics."
AppDynamics,26778994,27018574.0,3,"2014/11/06, 14:01:21",True,"2014/11/19, 15:53:11",nan,4141104.0,9.0,0,854,"How can I obtain statistics (requests per second, response time) for an app hosted on Cloud Foundry Pivotal?",So does Pivotal also provide 'Requests per second' and 'Response Time'?
AppDynamics,19382503,nan,0,"2013/10/15, 16:29:01",False,"2013/10/15, 18:51:04","2013/10/15, 18:15:15",1129813.0,253.0,0,132,Java 1.4/Apache 4.1 webapplication application monitoring,I used some good monitoring tools like javamelody and appDynamics to montior apache-tomcat 5.25 servers performance and get some useful statistics for the deployed web application (running on java 1.5 VM).
AppDynamics,19382503,nan,0,"2013/10/15, 16:29:01",False,"2013/10/15, 18:51:04","2013/10/15, 18:15:15",1129813.0,253.0,0,132,Java 1.4/Apache 4.1 webapplication application monitoring,Actually I need to monitor apache-tomcat server version 4.1.24 running on java 1.4.2_12-b03 VM.
AppDynamics,19382503,nan,0,"2013/10/15, 16:29:01",False,"2013/10/15, 18:51:04","2013/10/15, 18:15:15",1129813.0,253.0,0,132,Java 1.4/Apache 4.1 webapplication application monitoring,I could not find some useful tools to monitor tomcat version 4 and java 4 web applications similair to javamelody or any other +java5 monitoring tools.
AppDynamics,19382503,nan,0,"2013/10/15, 16:29:01",False,"2013/10/15, 18:51:04","2013/10/15, 18:15:15",1129813.0,253.0,0,132,Java 1.4/Apache 4.1 webapplication application monitoring,Any idea abt some useful monitoring tools for apache-tomcat 4/Java 1.4?
AppDynamics,19382503,nan,0,"2013/10/15, 16:29:01",False,"2013/10/15, 18:51:04","2013/10/15, 18:15:15",1129813.0,253.0,0,132,Java 1.4/Apache 4.1 webapplication application monitoring,Thanks
AppDynamics,66872969,nan,1,"2021/03/30, 17:31:51",False,"2021/04/17, 13:46:07",nan,14914159.0,1.0,0,21,How to integrate Appdynamics in/with Istio?,I am new to Istio service mesh.
AppDynamics,66872969,nan,1,"2021/03/30, 17:31:51",False,"2021/04/17, 13:46:07",nan,14914159.0,1.0,0,21,How to integrate Appdynamics in/with Istio?,I have to integrate/configure appdynamics in istio.
AppDynamics,66872969,nan,1,"2021/03/30, 17:31:51",False,"2021/04/17, 13:46:07",nan,14914159.0,1.0,0,21,How to integrate Appdynamics in/with Istio?,I have no clue how to do that.
AppDynamics,66872969,nan,1,"2021/03/30, 17:31:51",False,"2021/04/17, 13:46:07",nan,14914159.0,1.0,0,21,How to integrate Appdynamics in/with Istio?,Anything related to this would help.
AppDynamics,66872969,nan,1,"2021/03/30, 17:31:51",False,"2021/04/17, 13:46:07",nan,14914159.0,1.0,0,21,How to integrate Appdynamics in/with Istio?,Any example or related links or video...anything.
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,I am new to jenkins.
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,I am trying to moniter performance test for my project.
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,I have my scripts in Jmeter.
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,I have created paarmeterize job in jenkins as shown.
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,"Threads: 1
RampUp: 1
Loop: 40."
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,I am using backend listner to check data in Grafana and Appdynamics.
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,"Now when i start the build the scripts run only once, but i am expecting script must run for 40 times (With build success)."
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,"But when i run it through jmeter, scripts run for 40 times succesfully."
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,(Some issue with jenkins i suppose)
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,Please suggest how can i resolve the issue in jenkins as my project requeiremnt needs it to run script from jenkins.
AppDynamics,65072887,nan,1,"2020/11/30, 13:40:54",False,"2020/11/30, 14:14:21",nan,13624537.0,31.0,0,34,Parameterize Performance Build in Jenkins,Thank you in advance!
AppDynamics,64881926,nan,1,"2020/11/17, 21:22:22",False,"2020/11/19, 12:13:48",nan,3140836.0,367.0,0,71,Is it possible to call a REST API from Power BI to create a realtime feed?,I'm trying to create a live connection from our AppDynamics data to PowerBI for reporting purposes.
AppDynamics,64881926,nan,1,"2020/11/17, 21:22:22",False,"2020/11/19, 12:13:48",nan,3140836.0,367.0,0,71,Is it possible to call a REST API from Power BI to create a realtime feed?,An example of a command I would need to run to get AppD data is below.
AppDynamics,64881926,nan,1,"2020/11/17, 21:22:22",False,"2020/11/19, 12:13:48",nan,3140836.0,367.0,0,71,Is it possible to call a REST API from Power BI to create a realtime feed?,Is it possible to run arbitrary commands like this in PowerBI to return JSON data to PowerBI?
AppDynamics,64442861,64443455.0,1,"2020/10/20, 13:05:28",True,"2020/10/22, 10:08:59","2020/10/22, 10:08:59",4374278.0,27.0,0,33,SELinux access issue. Process run as specific user cannot update files owned by same user but created by separate process,"We have a system where Chef has deployed a monitoring agent, AppDynamics, as a specific user - lets call that user sysXYZ for the sake of this post."
AppDynamics,64442861,64443455.0,1,"2020/10/20, 13:05:28",True,"2020/10/22, 10:08:59","2020/10/22, 10:08:59",4374278.0,27.0,0,33,SELinux access issue. Process run as specific user cannot update files owned by same user but created by separate process,"The AppDynamics agents create a daily log file, all with sysXYZ user ownership."
AppDynamics,64442861,64443455.0,1,"2020/10/20, 13:05:28",True,"2020/10/22, 10:08:59","2020/10/22, 10:08:59",4374278.0,27.0,0,33,SELinux access issue. Process run as specific user cannot update files owned by same user but created by separate process,"Tomcat, also being run as sysXYZ user, hosts the application that is being monitored by the AppDynamics agent."
AppDynamics,64442861,64443455.0,1,"2020/10/20, 13:05:28",True,"2020/10/22, 10:08:59","2020/10/22, 10:08:59",4374278.0,27.0,0,33,SELinux access issue. Process run as specific user cannot update files owned by same user but created by separate process,"Every day, the Tomcat instance is restarted (project has their reasons) and the start-up process includes a step for renaming yesterdays AppDynamics logs."
AppDynamics,64442861,64443455.0,1,"2020/10/20, 13:05:28",True,"2020/10/22, 10:08:59","2020/10/22, 10:08:59",4374278.0,27.0,0,33,SELinux access issue. Process run as specific user cannot update files owned by same user but created by separate process,"However, this is prevented as a permissions issue."
AppDynamics,64442861,64443455.0,1,"2020/10/20, 13:05:28",True,"2020/10/22, 10:08:59","2020/10/22, 10:08:59",4374278.0,27.0,0,33,SELinux access issue. Process run as specific user cannot update files owned by same user but created by separate process,Tomcat running as user sysXYZ cannot amend files owned by user sysXYZ but created by something that is not Tomcat.
AppDynamics,64442861,64443455.0,1,"2020/10/20, 13:05:28",True,"2020/10/22, 10:08:59","2020/10/22, 10:08:59",4374278.0,27.0,0,33,SELinux access issue. Process run as specific user cannot update files owned by same user but created by separate process,I get thet SELinux is meant to prevent unexpected access - say a malicious actor has been introduced - and I am fine with that concept.
AppDynamics,64442861,64443455.0,1,"2020/10/20, 13:05:28",True,"2020/10/22, 10:08:59","2020/10/22, 10:08:59",4374278.0,27.0,0,33,SELinux access issue. Process run as specific user cannot update files owned by same user but created by separate process,What can we do here to allow the Tomcat instance to rename the files appropriately each time it is restarted?
AppDynamics,63482204,nan,0,"2020/08/19, 10:42:06",False,"2020/08/19, 10:42:06",nan,2889673.0,977.0,0,130,Conditional &quot;X-Prompt&quot; in Angular Scehmatics,"Requirement  : To display conditional prompts, based on the previous selection ( &quot;Analytics&quot;, in this case )."
AppDynamics,63482204,nan,0,"2020/08/19, 10:42:06",False,"2020/08/19, 10:42:06",nan,2889673.0,977.0,0,130,Conditional &quot;X-Prompt&quot; in Angular Scehmatics,"Problem  : For any selection, it is still displaying the prompt to add Google Analytics's Tracking Id even if i select AppDynamics or none."
AppDynamics,63482204,nan,0,"2020/08/19, 10:42:06",False,"2020/08/19, 10:42:06",nan,2889673.0,977.0,0,130,Conditional &quot;X-Prompt&quot; in Angular Scehmatics,Reference  :  Applying Subschemas Conditionally
AppDynamics,63482204,nan,0,"2020/08/19, 10:42:06",False,"2020/08/19, 10:42:06",nan,2889673.0,977.0,0,130,Conditional &quot;X-Prompt&quot; in Angular Scehmatics,"Note : For Simplicity, i have removed &quot;allOf&quot;,&quot;anyOf&quot; etc."
AppDynamics,63482204,nan,0,"2020/08/19, 10:42:06",False,"2020/08/19, 10:42:06",nan,2889673.0,977.0,0,130,Conditional &quot;X-Prompt&quot; in Angular Scehmatics,"and code for any other analytics tool, still it was not working."
AppDynamics,63482204,nan,0,"2020/08/19, 10:42:06",False,"2020/08/19, 10:42:06",nan,2889673.0,977.0,0,130,Conditional &quot;X-Prompt&quot; in Angular Scehmatics,Code  (Schema.json):
AppDynamics,63482204,nan,0,"2020/08/19, 10:42:06",False,"2020/08/19, 10:42:06",nan,2889673.0,977.0,0,130,Conditional &quot;X-Prompt&quot; in Angular Scehmatics,Read many articles on this but still couldn't figure it out.
AppDynamics,63482204,nan,0,"2020/08/19, 10:42:06",False,"2020/08/19, 10:42:06",nan,2889673.0,977.0,0,130,Conditional &quot;X-Prompt&quot; in Angular Scehmatics,Is there any way to add these conditional prompts?
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow, The above query from Hibernate takes only few 100 milliseconds to execute.
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,But the list() call which executes the above query and returns a list takes more than a minute (sometimes   2mins) to execute.
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,"I am not sure what is causing this.I was able to track down the query timing from AppDynamics, but could not drill down into why the list() call takes more than a minute 
  The number of records returned is around 8-9k (may be more sometimes)."
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,The table has around 800k records and the c3 column referred in the query is indexed.
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,I have set the query.setMaxresults value to 50k.
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,The number of columns in the actual query is 24.
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,I reduced the column in the query here for simplicity
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,The list() method does some processing on the returned results which is time consuming.
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,Did someone face this issue before?
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,Any help is appreciated.
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,"Hibernate version: 2.0, JDK - 1.8, DB - Oracle 11.2.0.4"
AppDynamics,63284708,nan,0,"2020/08/06, 16:31:03",False,"2020/08/06, 16:31:03",nan,4542775.0,207.0,0,21,hibernate query.list() is slow,Thanks
AppDynamics,58700944,nan,0,"2019/11/04, 22:58:29",False,"2019/11/05, 07:03:19","2019/11/05, 07:03:19",5669196.0,101.0,0,67,"After updating Spring Boot(1.5.20.RELEASE), getting error in AWS environment, but working fine in local",After updating to Spring Boot (1.5.20.RELEASE) - I'm getting the following error in my latest environment (in AWS) but in local it is working fine.
AppDynamics,58700944,nan,0,"2019/11/04, 22:58:29",False,"2019/11/05, 07:03:19","2019/11/05, 07:03:19",5669196.0,101.0,0,67,"After updating Spring Boot(1.5.20.RELEASE), getting error in AWS environment, but working fine in local",ERROR StatusLogger No log4j2 configuration file found.
AppDynamics,58700944,nan,0,"2019/11/04, 22:58:29",False,"2019/11/05, 07:03:19","2019/11/05, 07:03:19",5669196.0,101.0,0,67,"After updating Spring Boot(1.5.20.RELEASE), getting error in AWS environment, but working fine in local",Using default configuration: logging only errors to the console.
AppDynamics,58700944,nan,0,"2019/11/04, 22:58:29",False,"2019/11/05, 07:03:19","2019/11/05, 07:03:19",5669196.0,101.0,0,67,"After updating Spring Boot(1.5.20.RELEASE), getting error in AWS environment, but working fine in local",pom.xml
AppDynamics,58700944,nan,0,"2019/11/04, 22:58:29",False,"2019/11/05, 07:03:19","2019/11/05, 07:03:19",5669196.0,101.0,0,67,"After updating Spring Boot(1.5.20.RELEASE), getting error in AWS environment, but working fine in local",Please suggest.
AppDynamics,58700944,nan,0,"2019/11/04, 22:58:29",False,"2019/11/05, 07:03:19","2019/11/05, 07:03:19",5669196.0,101.0,0,67,"After updating Spring Boot(1.5.20.RELEASE), getting error in AWS environment, but working fine in local",This is working fine in my local environment.
AppDynamics,58700944,nan,0,"2019/11/04, 22:58:29",False,"2019/11/05, 07:03:19","2019/11/05, 07:03:19",5669196.0,101.0,0,67,"After updating Spring Boot(1.5.20.RELEASE), getting error in AWS environment, but working fine in local","Full Agent Registration Info Resolver using node name [b196dc6a-64de-469b-b70d-78e99a12b504]
Install Directory resolved to[/opt/appdynamics]
ERROR StatusLogger No log4j2 configuration file found."
AppDynamics,58700944,nan,0,"2019/11/04, 22:58:29",False,"2019/11/05, 07:03:19","2019/11/05, 07:03:19",5669196.0,101.0,0,67,"After updating Spring Boot(1.5.20.RELEASE), getting error in AWS environment, but working fine in local",Using default configuration: logging only errors to the console.
AppDynamics,58090654,nan,0,"2019/09/25, 06:13:15",False,"2019/09/25, 23:37:20","2019/09/25, 23:37:20",12116412.0,13.0,0,729,send multiline form with ansible uri module,"I'm trying to upload a multi-line file to a appdynamics controller, using the ansible uri module."
AppDynamics,58090654,nan,0,"2019/09/25, 06:13:15",False,"2019/09/25, 23:37:20","2019/09/25, 23:37:20",12116412.0,13.0,0,729,send multiline form with ansible uri module,Any advice?
AppDynamics,58090654,nan,0,"2019/09/25, 06:13:15",False,"2019/09/25, 23:37:20","2019/09/25, 23:37:20",12116412.0,13.0,0,729,send multiline form with ansible uri module,"As you'll notice in the snippets, I'm brand new to Ansible..."
AppDynamics,58090654,nan,0,"2019/09/25, 06:13:15",False,"2019/09/25, 23:37:20","2019/09/25, 23:37:20",12116412.0,13.0,0,729,send multiline form with ansible uri module,"It works fine with shell: curl, but I can't seem to get the output correct for a more structured play."
AppDynamics,58090654,nan,0,"2019/09/25, 06:13:15",False,"2019/09/25, 23:37:20","2019/09/25, 23:37:20",12116412.0,13.0,0,729,send multiline form with ansible uri module,Expected outcome - I'm able to successfully import the health rule to AppD
AppDynamics,58090654,nan,0,"2019/09/25, 06:13:15",False,"2019/09/25, 23:37:20","2019/09/25, 23:37:20",12116412.0,13.0,0,729,send multiline form with ansible uri module,"Actual - I get a 200 response, and nothing happens."
AppDynamics,58090654,nan,0,"2019/09/25, 06:13:15",False,"2019/09/25, 23:37:20","2019/09/25, 23:37:20",12116412.0,13.0,0,729,send multiline form with ansible uri module,Response:
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,The AppDynamics java agent requires the JBoss Domain.xml and Host.xml files to be modified in order to run.
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,"To make AppDynamics work, I must add a ""property"" element with two attributes."
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,"&lt;property name=""jboss.modules.system.pkgs"" value=""com.singularity""/&gt;"
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,"I have also tried to add the element and attributes int he same code block, but received a ansible error stating the element and attributes are exclusive:"
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,The issue I am running into is this: At the same path there is a duplicate property with different attributes that must remain unchanged
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,"&lt;property name=""java.net.preferIPv4Stack"" value=""true""/&gt;"
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,"When using Ansible, I always seem to overwrite ALL the ""property"" elements."
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,I have tried adding the property element with the attributes on the same like  (as if it were the property name) but receive a python error:
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,The latest version of the Ansible code I am using is as follows:
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,I am at a loss on how to deal with duplicated element names with different attributes via ansible.
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,In the end I would need to end up with  this:
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,Any help would be great appreciated.
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,"I know the answer is staring m in the face, but I just don't see it"
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,And added point would also to ensure I don't have identical elements with identical attributes as well.
AppDynamics,56113334,nan,0,"2019/05/13, 16:25:09",False,"2019/05/13, 16:25:09",nan,4921851.0,13.0,0,217,Using ansible to modify a JBoss proprty with out changing all property tags,"Thanks, in advance."
AppDynamics,55948183,nan,1,"2019/05/02, 11:11:57",False,"2019/05/03, 08:23:29","2019/05/02, 13:51:57",192923.0,4861.0,0,24,Unwanted mysql logs to get the procedure definitions,I am using Aurora and .NET 3.5 framework in my application which is internally using .NET mysqlconnector 6.9.3 version.
AppDynamics,55948183,nan,1,"2019/05/02, 11:11:57",False,"2019/05/03, 08:23:29","2019/05/02, 13:51:57",192923.0,4861.0,0,24,Unwanted mysql logs to get the procedure definitions,"When I enable my general_log, I see below logs."
AppDynamics,55948183,nan,1,"2019/05/02, 11:11:57",False,"2019/05/03, 08:23:29","2019/05/02, 13:51:57",192923.0,4861.0,0,24,Unwanted mysql logs to get the procedure definitions,I am looking forward a way that I can disable the same or Is this how mysql connector designed.
AppDynamics,55948183,nan,1,"2019/05/02, 11:11:57",False,"2019/05/03, 08:23:29","2019/05/02, 13:51:57",192923.0,4861.0,0,24,Unwanted mysql logs to get the procedure definitions,Any thoughts around this!
AppDynamics,55948183,nan,1,"2019/05/02, 11:11:57",False,"2019/05/03, 08:23:29","2019/05/02, 13:51:57",192923.0,4861.0,0,24,Unwanted mysql logs to get the procedure definitions,"I use appdynamics to monitor the queries and I see that whenever there is a procedure call, there is a INFORMATION_SCHEMA calls as well."
AppDynamics,55948183,nan,1,"2019/05/02, 11:11:57",False,"2019/05/03, 08:23:29","2019/05/02, 13:51:57",192923.0,4861.0,0,24,Unwanted mysql logs to get the procedure definitions,Please have a look at the below logs.
AppDynamics,55948183,nan,1,"2019/05/02, 11:11:57",False,"2019/05/03, 08:23:29","2019/05/02, 13:51:57",192923.0,4861.0,0,24,Unwanted mysql logs to get the procedure definitions,"this link  suggested to set  innodb_stats_on_metadata  to  0 , but didn't help me."
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,"At my new assignment, I need to understand a mid level Java application."
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,"To understand the flow faster, I had this idea that if I could see at runtime functions are being called, which function finally responded, then I could really get the whole map in my mind."
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,I've worked with tools like AppDynamics which tells the latency/DB calls etc.
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,But what I am looking after is something which will tell me the flow at Runtime.
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,"Like,  
 Controller.getStudent() -&gt; Service.getStudent() -&gt; Repository.getStudent() -&gt; ...."
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,I am wondering if there are any tools/techniques as such.
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,Like recording a stacktrace in debug mode.
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,I can imagine a tool doing  Thread.currentThread().getStackTrace() .
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,Does anybody have some idea regarding how can do this?
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,(I'm using Springboot and Jboss)
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,"Edit: I'm not really looking after logging, debugging etc."
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,I feel there could be something which could tell what functions are being executed.
AppDynamics,55057999,nan,1,"2019/03/08, 08:41:50",True,"2019/03/08, 09:09:07","2019/03/08, 08:46:17",6935577.0,505.0,0,63,Any way to monitor a Java application&#39;s Function Flow?,And determine if there are any limitations.
AppDynamics,53271297,nan,1,"2018/11/13, 00:56:36",False,"2018/11/14, 06:16:21",nan,1394897.0,342.0,0,49,FlowForce - monitoring and alerting tool,I had to configure AppDynamics alerts in the past for Java applications I worked for.
AppDynamics,53271297,nan,1,"2018/11/13, 00:56:36",False,"2018/11/14, 06:16:21",nan,1394897.0,342.0,0,49,FlowForce - monitoring and alerting tool,"I also heard of Nagios, but I am not very sure how that works."
AppDynamics,53271297,nan,1,"2018/11/13, 00:56:36",False,"2018/11/14, 06:16:21",nan,1394897.0,342.0,0,49,FlowForce - monitoring and alerting tool,"Now, I need to configure alerts for a FlowForce Server, but I don't believe it can be integrated with AppDynamics or Nagios."
AppDynamics,53271297,nan,1,"2018/11/13, 00:56:36",False,"2018/11/14, 06:16:21",nan,1394897.0,342.0,0,49,FlowForce - monitoring and alerting tool,"I saw FlowForce allow me to send some alerts, like when a step of a job fails, but I would like to have some server alerts, like, for instance, if the license expires and, as a result, the server is automatically shut down."
AppDynamics,53271297,nan,1,"2018/11/13, 00:56:36",False,"2018/11/14, 06:16:21",nan,1394897.0,342.0,0,49,FlowForce - monitoring and alerting tool,I am wondering the best way to achieve it.
AppDynamics,53271297,nan,1,"2018/11/13, 00:56:36",False,"2018/11/14, 06:16:21",nan,1394897.0,342.0,0,49,FlowForce - monitoring and alerting tool,I am running it on a Windows environment BTW.
AppDynamics,53271297,nan,1,"2018/11/13, 00:56:36",False,"2018/11/14, 06:16:21",nan,1394897.0,342.0,0,49,FlowForce - monitoring and alerting tool,Suggestions are welcome.
AppDynamics,53271297,nan,1,"2018/11/13, 00:56:36",False,"2018/11/14, 06:16:21",nan,1394897.0,342.0,0,49,FlowForce - monitoring and alerting tool,Thank you in advance!
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"We have instrumented a .Net 4.0 application, running in IIS8.0 on Windows 2012 with an AppDynamics APM agent (v4.5.2)."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"This server also has McAfee Endpoint Protection installed, v10.6.0.542, with Threat Prevention v10.6.0.672."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"With the APM agent installed, CPU is much higher under typical load (~50-60% with agent vs 10% without, across 2 vCPUs)."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"Under heavy load, the application also starts becoming unstable (requests start queuing and timing out, response times become very high, errors begin occurring)."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"We have noticed that with McAfee enabled, it injects two DLLs into the w3wp process - EpMPApi.dll and EpMPThe.dll."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"We checked this using Process Explorer, looking at loaded DLLs for the process."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,We ran various combinations of performance test:
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"We attempted to add w3wp.exe as an exception in McAfee, however we saw that the DLLs were still loaded, and the high CPU and poor performance still occurred."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"In memory dumps, we consistently saw the application threads waiting on critical sections used by EpMPApi.dll."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,It seemed to be related to the application attempting to make socket connections (which it does frequently as all requests involves WCF calls to a downstream system).
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,"We would like to understand if/how we can configure McAfee to either exclude w3wp.exe fully, or perhaps stop whatever activity it is doing that the APM agent seems to interact badly with."
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,We are also working on the APM agent side to understand if we can do anything there to prevent or work around the behaviour.
AppDynamics,52754016,nan,0,"2018/10/11, 09:55:04",False,"2018/10/11, 09:55:04",nan,4718586.0,11.0,0,489,How can I prevent McAfee from interacting with an APM agent on a .Net Application?,Thanks!
AppDynamics,52032181,nan,0,"2018/08/27, 06:26:10",False,"2018/08/27, 06:26:10",nan,10188025.0,51.0,0,312,How to use the container name in rancher inside the application code running within the container,Having a nodejs application running inside a docker container orchestrated by Rancher.
AppDynamics,52032181,nan,0,"2018/08/27, 06:26:10",False,"2018/08/27, 06:26:10",nan,10188025.0,51.0,0,312,How to use the container name in rancher inside the application code running within the container,Using appdynamics to monitor the service.
AppDynamics,52032181,nan,0,"2018/08/27, 06:26:10",False,"2018/08/27, 06:26:10",nan,10188025.0,51.0,0,312,How to use the container name in rancher inside the application code running within the container,For that we need to add few configuration parameters in main js file.
AppDynamics,52032181,nan,0,"2018/08/27, 06:26:10",False,"2018/08/27, 06:26:10",nan,10188025.0,51.0,0,312,How to use the container name in rancher inside the application code running within the container,One of the configuration has to be unique for all the containers running for this service.
AppDynamics,52032181,nan,0,"2018/08/27, 06:26:10",False,"2018/08/27, 06:26:10",nan,10188025.0,51.0,0,312,How to use the container name in rancher inside the application code running within the container,"So, i would like to pass the rancher container name as that configuration."
AppDynamics,51468443,nan,1,"2018/07/22, 22:10:37",False,"2018/07/23, 04:42:13","2018/07/23, 04:42:13",6794293.0,1.0,0,39,Sporadic long query execution in Oracle RDS,We are experiencing sporadic long queries execution in our application.
AppDynamics,51468443,nan,1,"2018/07/22, 22:10:37",False,"2018/07/23, 04:42:13","2018/07/23, 04:42:13",6794293.0,1.0,0,39,Sporadic long query execution in Oracle RDS,The database is Oracle 12.1 RDS.
AppDynamics,51468443,nan,1,"2018/07/22, 22:10:37",False,"2018/07/23, 04:42:13","2018/07/23, 04:42:13",6794293.0,1.0,0,39,Sporadic long query execution in Oracle RDS,"I can see in AppDynamics that query was executed for 13s, I'm executing it myself in Oracle SQL Developer and it never takes longer than 0.1s."
AppDynamics,51468443,nan,1,"2018/07/22, 22:10:37",False,"2018/07/23, 04:42:13","2018/07/23, 04:42:13",6794293.0,1.0,0,39,Sporadic long query execution in Oracle RDS,I can't put query here as there are 3 of them that sporadically give execution time longer than 10s and for each of them I can't reproduce it in SQL Developer.
AppDynamics,51468443,nan,1,"2018/07/22, 22:10:37",False,"2018/07/23, 04:42:13","2018/07/23, 04:42:13",6794293.0,1.0,0,39,Sporadic long query execution in Oracle RDS,"We've started to log Execution plan for long running queries using /*+ gather_plan_statistics */ and it is the same as if query executed for 0.1s except the fact that it doesn't have such a record ""1 SQL Plan Directive used for this statement""."
AppDynamics,51468443,nan,1,"2018/07/22, 22:10:37",False,"2018/07/23, 04:42:13","2018/07/23, 04:42:13",6794293.0,1.0,0,39,Sporadic long query execution in Oracle RDS,I'm looking for any ideas that could help to identify the root cause of this behavior.
AppDynamics,48974684,nan,3,"2018/02/25, 16:39:28",True,"2018/02/26, 09:16:56",nan,5550284.0,2251.0,0,31,Search for a key for a given value in a dictionary,I have this dictionary below
AppDynamics,48974684,nan,3,"2018/02/25, 16:39:28",True,"2018/02/26, 09:16:56",nan,5550284.0,2251.0,0,31,Search for a key for a given value in a dictionary,"I am trying to get the key for the value  ""art"" ."
AppDynamics,48974684,nan,3,"2018/02/25, 16:39:28",True,"2018/02/26, 09:16:56",nan,5550284.0,2251.0,0,31,Search for a key for a given value in a dictionary,"But I am only able to search for the value  ""art""  or the list it is in."
AppDynamics,48974684,nan,3,"2018/02/25, 16:39:28",True,"2018/02/26, 09:16:56",nan,5550284.0,2251.0,0,31,Search for a key for a given value in a dictionary,Here is my code below
AppDynamics,48974684,nan,3,"2018/02/25, 16:39:28",True,"2018/02/26, 09:16:56",nan,5550284.0,2251.0,0,31,Search for a key for a given value in a dictionary,I get the list for the given value
AppDynamics,48974684,nan,3,"2018/02/25, 16:39:28",True,"2018/02/26, 09:16:56",nan,5550284.0,2251.0,0,31,Search for a key for a given value in a dictionary,"How do I get the associated key in this case  ""avgresptime""  for the given list I found for the value and in turn it's given key which is  ""Appdynamics"" ?"
AppDynamics,48974684,nan,3,"2018/02/25, 16:39:28",True,"2018/02/26, 09:16:56",nan,5550284.0,2251.0,0,31,Search for a key for a given value in a dictionary,Is there any better way to do it since my approach involves O(n^3) runnning time?
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,We have a functioning  ASP.NET MVC 5.2.2  website running on  .NET Framework 4.5.1  hosted on a web farm with numerous servers.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,The web application integrates with AppyDynamics.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,The  AppDynamics .NET Agent 4.3.2.1  is installed on each server.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,I noticed that it's an outdated version but we are unable to update yet.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,We use StructureMap for our IoC.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,"Sometimes something happens across the entire farm, perhaps by a:"
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,There is a chance that one of the web applications on an affected node will not function properly.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,It will start up but the IoC never executed resulting in the following error:
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,We can't prove it but we think that the AppDynamics Agent intercepts the IIS application just at the wrong time.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,The web application starts but the IoC is not configured hence the error above.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,My Questions
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,Is this a known issue?
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,I really did search online
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,"If you have experienced this, do you have an idea of what the possible cause(s) can be?"
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,How can we fix this annoying intermittent error for our customers?
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,Is it as simple as updating something?
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,Like StructureMap &amp; AppDynamics?
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,Edit
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,The web application is deployed to a network shared folder which feeds all the web servers in the farm.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,:(
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,The IoC setup must be working if it is working for all the other nodes.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,This problem seemed to have started since the installation of the AppDynamics Agents.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,It only happens if IIS is reset across the farm.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,One of the sites on that node will throw the error I mentioned above.
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,I am still investigating on my side
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,Application_Start()  from the  Global.asax .
AppDynamics,48742694,nan,0,"2018/02/12, 11:02:18",False,"2018/02/12, 12:27:56","2018/02/12, 12:27:56",849986.0,3062.0,0,88,Intermittent System.InvalidOperationException thrown on ASP.NET MVC web application after an IIS reset on a web farm,"For a crisp view of the image, click on it."
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?","We have an issue with our PROD application.. we are seeing random latencies in the transactions(API based, no sessions)."
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?","The transactions just freezes b/w the processing, no pattern (not pausing at a section of transaction, or a time of a day), although one pattern would be - It is more occurring when we do more transactions."
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?","We have checked storage, network, db and for other hardware related issues but couldn't find any."
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?","One issue that i could see is - 
GC (Allocation Failure) being written to catalina.out every 2secs - Is this ok?"
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",or is this an indication that there are lot of objects created
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?","Here is our tomcat/JVM and server config
Xmx is set to 4096m, xss set to 256k,connector is bio, http11protocol, maxthreads=150, acceptcount=100, compression=on, compressionminsize=2048
Hardware - 32G memory, 8cpu."
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?","cpu spikes or memory usage are all normal during 
these events."
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",Even GC collection time per min looks normal too.
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",We use appdynamics and that doesn't point to any issues as well.
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",we use log4j that writes the logging which points to this latency.
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",We use SSL/TLS as well but one way to rule this out is - these latencies are recorded b/w the transactions processing statements which happens after the SSL termination.
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",what kind of logging should we enable to better understand the delay?
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",any other recommendation?
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",Are they are any PROD ready tools that will show SSL performance?
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",(apparently APPdynamics do not show this)
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",Does anyone use compression for API based calls when the requests are less than 2kb?
AppDynamics,48410597,nan,0,"2018/01/23, 22:51:32",False,"2018/02/07, 12:07:13",nan,9258811.0,1.0,0,202,"tomcat - Performance issues with Java application in prod, minor memory leak or bug in the code? or something else?",really appreciate you guys for helping out here!
AppDynamics,47754856,47754925.0,1,"2017/12/11, 16:15:49",True,"2017/12/11, 16:20:02",nan,633961.0,26851.0,0,111,What is an &quot;APM solution&quot;?,"What is an ""APM solution""?"
AppDynamics,47754856,47754925.0,1,"2017/12/11, 16:15:49",True,"2017/12/11, 16:20:02",nan,633961.0,26851.0,0,111,What is an &quot;APM solution&quot;?,I have this term from this blog post:  http://www.myloadtest.com/new-relic-vs-appdynamics/
AppDynamics,42076632,nan,1,"2017/02/06, 22:24:18",False,"2017/03/28, 11:21:22","2017/02/09, 22:31:15",7525311.0,3.0,0,218,appdynamics_javaprocess alert whenever the process goes down,I am looking to setup a monitoring alert in AppDynamics.
AppDynamics,42076632,nan,1,"2017/02/06, 22:24:18",False,"2017/03/28, 11:21:22","2017/02/09, 22:31:15",7525311.0,3.0,0,218,appdynamics_javaprocess alert whenever the process goes down,I want to generate an email alert whenever the java process in the Centos machine goes down.
AppDynamics,42076632,nan,1,"2017/02/06, 22:24:18",False,"2017/03/28, 11:21:22","2017/02/09, 22:31:15",7525311.0,3.0,0,218,appdynamics_javaprocess alert whenever the process goes down,How do i set this up?
AppDynamics,42076632,nan,1,"2017/02/06, 22:24:18",False,"2017/03/28, 11:21:22","2017/02/09, 22:31:15",7525311.0,3.0,0,218,appdynamics_javaprocess alert whenever the process goes down,I am using AppD version 4.2.
AppDynamics,42076632,nan,1,"2017/02/06, 22:24:18",False,"2017/03/28, 11:21:22","2017/02/09, 22:31:15",7525311.0,3.0,0,218,appdynamics_javaprocess alert whenever the process goes down,"I have tried installing the process monitoring plugin, but I am still not getting any data in custom metrics."
AppDynamics,42076632,nan,1,"2017/02/06, 22:24:18",False,"2017/03/28, 11:21:22","2017/02/09, 22:31:15",7525311.0,3.0,0,218,appdynamics_javaprocess alert whenever the process goes down,https://www.appdynamics.com/community/exchange/extension/process-monitoring-extension/
AppDynamics,40737754,nan,0,"2016/11/22, 11:11:02",False,"2016/11/22, 11:42:40","2016/11/22, 11:24:05",5051834.0,83.0,0,139,No module name found,"I had created a python file which will pull the data from app dynamics for which i had imported the package from appd.request import AppDynamicsClient
it is working fine but now i want to convert it into .exe for which i am using py2exe in py2exe i am trying to include the package appdynamics but it is giving error package not found"
AppDynamics,40737754,nan,0,"2016/11/22, 11:11:02",False,"2016/11/22, 11:42:40","2016/11/22, 11:24:05",5051834.0,83.0,0,139,No module name found,"these are the packages i am importing in main file which i am trying to convert it into .exe file
setup.py file"
AppDynamics,40737754,nan,0,"2016/11/22, 11:11:02",False,"2016/11/22, 11:42:40","2016/11/22, 11:24:05",5051834.0,83.0,0,139,No module name found,)
AppDynamics,40737754,nan,0,"2016/11/22, 11:11:02",False,"2016/11/22, 11:42:40","2016/11/22, 11:24:05",5051834.0,83.0,0,139,No module name found,"ERROR MESSAGE:
ImportError: No module named AppDynamicsClient"
AppDynamics,39848912,nan,0,"2016/10/04, 12:36:05",False,"2016/10/04, 12:36:05",nan,5660885.0,11.0,0,53,One of the OSB node has high response time,We have two osb nodes in cluster.
AppDynamics,39848912,nan,0,"2016/10/04, 12:36:05",False,"2016/10/04, 12:36:05",nan,5660885.0,11.0,0,53,One of the OSB node has high response time,"One of node osb1 has less ovearall response time ( 1 sec) when measured in appdynamics,  another  node osb2 has high response(20sec)."
AppDynamics,39848912,nan,0,"2016/10/04, 12:36:05",False,"2016/10/04, 12:36:05",nan,5660885.0,11.0,0,53,One of the OSB node has high response time,We brought down each of this node and tested individually.
AppDynamics,39848912,nan,0,"2016/10/04, 12:36:05",False,"2016/10/04, 12:36:05",nan,5660885.0,11.0,0,53,One of the OSB node has high response time,We see same behavior.
AppDynamics,39848912,nan,0,"2016/10/04, 12:36:05",False,"2016/10/04, 12:36:05",nan,5660885.0,11.0,0,53,One of the OSB node has high response time,Any suggestions on what to look into to identify the issue.?
AppDynamics,39848912,nan,0,"2016/10/04, 12:36:05",False,"2016/10/04, 12:36:05",nan,5660885.0,11.0,0,53,One of the OSB node has high response time,The osb configuration across both the nodes Is identical and jvm configuration also identical.
AppDynamics,39848912,nan,0,"2016/10/04, 12:36:05",False,"2016/10/04, 12:36:05",nan,5660885.0,11.0,0,53,One of the OSB node has high response time,Heap usage is same.
AppDynamics,39848912,nan,0,"2016/10/04, 12:36:05",False,"2016/10/04, 12:36:05",nan,5660885.0,11.0,0,53,One of the OSB node has high response time,CPU bit differs.
AppDynamics,36871506,nan,1,"2016/04/26, 20:12:27",True,"2016/04/27, 11:54:56","2016/04/27, 11:54:56",4614631.0,445.0,0,1037,Jedis Get Data: JedisConnectionFailureException iterating a section of code over long period of time,So I have a code that gets value from Redis using Jedis Client.
AppDynamics,36871506,nan,1,"2016/04/26, 20:12:27",True,"2016/04/27, 11:54:56","2016/04/27, 11:54:56",4614631.0,445.0,0,1037,Jedis Get Data: JedisConnectionFailureException iterating a section of code over long period of time,"But at a time, the Redis was at maximum connection and these exceptions were getting thrown:"
AppDynamics,36871506,nan,1,"2016/04/26, 20:12:27",True,"2016/04/27, 11:54:56","2016/04/27, 11:54:56",4614631.0,445.0,0,1037,Jedis Get Data: JedisConnectionFailureException iterating a section of code over long period of time,"When I check an AppDynamics analysis of this scenario, I saw some iteration of some calls over a long period of time (1772 seconds)."
AppDynamics,36871506,nan,1,"2016/04/26, 20:12:27",True,"2016/04/27, 11:54:56","2016/04/27, 11:54:56",4614631.0,445.0,0,1037,Jedis Get Data: JedisConnectionFailureException iterating a section of code over long period of time,The calls are shown in the snips.
AppDynamics,36871506,nan,1,"2016/04/26, 20:12:27",True,"2016/04/27, 11:54:56","2016/04/27, 11:54:56",4614631.0,445.0,0,1037,Jedis Get Data: JedisConnectionFailureException iterating a section of code over long period of time,Can anyone explain what's happening here?
AppDynamics,36871506,nan,1,"2016/04/26, 20:12:27",True,"2016/04/27, 11:54:56","2016/04/27, 11:54:56",4614631.0,445.0,0,1037,Jedis Get Data: JedisConnectionFailureException iterating a section of code over long period of time,And why Jedis didn't stop after the Timeout setting (500ms)?
AppDynamics,36871506,nan,1,"2016/04/26, 20:12:27",True,"2016/04/27, 11:54:56","2016/04/27, 11:54:56",4614631.0,445.0,0,1037,Jedis Get Data: JedisConnectionFailureException iterating a section of code over long period of time,Can I prevent this from happening for long?
AppDynamics,36871506,nan,1,"2016/04/26, 20:12:27",True,"2016/04/27, 11:54:56","2016/04/27, 11:54:56",4614631.0,445.0,0,1037,Jedis Get Data: JedisConnectionFailureException iterating a section of code over long period of time,This is what my Bean definitions for the Jedis look like:
AppDynamics,36459842,nan,2,"2016/04/06, 21:48:09",False,"2016/07/06, 01:16:50","2016/04/06, 22:27:41",6144192.0,173.0,0,1138,AppKey cannot be null or empty,I am trying to develope an android ecommerce UI for demo.
AppDynamics,36459842,nan,2,"2016/04/06, 21:48:09",False,"2016/07/06, 01:16:50","2016/04/06, 22:27:41",6144192.0,173.0,0,1138,AppKey cannot be null or empty,I downloaded the template files from this source at github  https://github.com/Appdynamics/ECommerce-Android .
AppDynamics,36459842,nan,2,"2016/04/06, 21:48:09",False,"2016/07/06, 01:16:50","2016/04/06, 22:27:41",6144192.0,173.0,0,1138,AppKey cannot be null or empty,When I run the application in android studio everything builds fine but when I go to open the app on my emulator it crashes with this error message in the logcat
AppDynamics,36459842,nan,2,"2016/04/06, 21:48:09",False,"2016/07/06, 01:16:50","2016/04/06, 22:27:41",6144192.0,173.0,0,1138,AppKey cannot be null or empty,I think the problem is coming from this line of code by I'm not sure how to fix it
AppDynamics,36459842,nan,2,"2016/04/06, 21:48:09",False,"2016/07/06, 01:16:50","2016/04/06, 22:27:41",6144192.0,173.0,0,1138,AppKey cannot be null or empty,Here is the preferences.xml file
AppDynamics,36459842,nan,2,"2016/04/06, 21:48:09",False,"2016/07/06, 01:16:50","2016/04/06, 22:27:41",6144192.0,173.0,0,1138,AppKey cannot be null or empty,Any input would be greatly appreciated thanks.
AppDynamics,34103927,34137555.0,2,"2015/12/05, 12:45:38",True,"2015/12/07, 17:44:30",nan,1614862.0,2721.0,0,235,c3p0 is always calling commit even if data is coming from Ehcache,I have a web service which makes readonly call(bunch of select queries) to get data from DB.
AppDynamics,34103927,34137555.0,2,"2015/12/05, 12:45:38",True,"2015/12/07, 17:44:30",nan,1614862.0,2721.0,0,235,c3p0 is always calling commit even if data is coming from Ehcache,"But it makes call to the database for the first time only, after that, all entities involved are cached in hibernate second level cache using Ehcache."
AppDynamics,34103927,34137555.0,2,"2015/12/05, 12:45:38",True,"2015/12/07, 17:44:30",nan,1614862.0,2721.0,0,235,c3p0 is always calling commit even if data is coming from Ehcache,"The problem is even if my request is not making any DB call to get data, the application is always making a commit call to the database which is effecting my response time of the web service."
AppDynamics,34103927,34137555.0,2,"2015/12/05, 12:45:38",True,"2015/12/07, 17:44:30",nan,1614862.0,2721.0,0,235,c3p0 is always calling commit even if data is coming from Ehcache,"My web service response time is 90ms, out of which the commit call is contributing 35ms(40%) all the time."
AppDynamics,34103927,34137555.0,2,"2015/12/05, 12:45:38",True,"2015/12/07, 17:44:30",nan,1614862.0,2721.0,0,235,c3p0 is always calling commit even if data is coming from Ehcache,The datasource is configured as spring bean with data source class com.mchange.v2.c3p0.ComboPooledDataSource as shown below.
AppDynamics,34103927,34137555.0,2,"2015/12/05, 12:45:38",True,"2015/12/07, 17:44:30",nan,1614862.0,2721.0,0,235,c3p0 is always calling commit even if data is coming from Ehcache,It is making a commit call to DB for every call to the web service which I can see it in the call graph taken from appdynamics.
AppDynamics,34103927,34137555.0,2,"2015/12/05, 12:45:38",True,"2015/12/07, 17:44:30",nan,1614862.0,2721.0,0,235,c3p0 is always calling commit even if data is coming from Ehcache,The cache hit ratio is 100% for all those entries involved in the request.
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,I have my cakePHP application (hosted on centOS 7) monitored by appdynamics APM.
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,In their monitoring controller I have breakdown of transactions that take too long.
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,I also installed a simple chrome page timing plugin.
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,On one of my webpages I got the following results:
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,As you can see the page loaded after 157 seconds!
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,However in my APM the slowest Transaction recorded has 'execution time' at 2.1 seconds.
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,If my server serves the pages in under 2 seconds (and usually in around 0.5 second) where does this terrible 157 seconds come from?
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,How can I monitor the source of that load time?
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,Thats another example with firefox plugin for page load times:
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,This one took nearly 54 seconds and thats a real load time (saw it mysekf).
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,However firefox Firebug under Net tab shows that for that same page:
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,6 seconds for that same request?
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,Why are they so different and why is firebug incorrect?
AppDynamics,32231743,nan,0,"2015/08/26, 19:17:31",False,"2015/08/27, 21:13:59","2015/08/27, 21:13:59",2083691.0,421.0,0,946,very slow response time that is not indicated in the PHP profiler,I saw myself that the load took over 50 seconds
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,"I downloaded Appdynamics agent for Java, which required adding jvm option for glassfish server 3.1.2, for javaagent.jar, giving path of agent."
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,user which application server runs on has full permissions on this folder.
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,"After adding this jvm in glassfish server 3.1.2, a restart of server is required."
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,"After executing restart, server could not start givng error: error opening ZIP file or JAR manifest missing C:AppServerAgent:javaagent.rar."
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,"I noticed that the option was not added in domain.xml file, but still the option is required for starting the machine."
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,"I tried to add it manually in the domain.xml file, but still no success."
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,What can I do?
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,now the appication hosted by glassfish doesnt start because the server is down.
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,Any help?
AppDynamics,28027105,nan,1,"2015/01/19, 16:38:59",False,"2015/01/20, 10:48:34",nan,4470413.0,11.0,0,1464,jvm startup error when adding jvm-option,Thank you in advance.
AppDynamics,61509776,nan,1,"2020/04/29, 22:09:51",True,"2020/04/29, 22:29:43",nan,2344459.0,696.0,-1,61,Oracle Database performance issue in spring-boot application,We have installed 2 instance of same application in a same datacenter.
AppDynamics,61509776,nan,1,"2020/04/29, 22:09:51",True,"2020/04/29, 22:29:43",nan,2344459.0,696.0,-1,61,Oracle Database performance issue in spring-boot application,Both the app is using same oracle DB.
AppDynamics,61509776,nan,1,"2020/04/29, 22:09:51",True,"2020/04/29, 22:29:43",nan,2344459.0,696.0,-1,61,Oracle Database performance issue in spring-boot application,But we are observing performance issue in one application.
AppDynamics,61509776,nan,1,"2020/04/29, 22:09:51",True,"2020/04/29, 22:29:43",nan,2344459.0,696.0,-1,61,Oracle Database performance issue in spring-boot application,In AppDynamics we can see the response time of one application is much higher that other.
AppDynamics,61509776,nan,1,"2020/04/29, 22:09:51",True,"2020/04/29, 22:29:43",nan,2344459.0,696.0,-1,61,Oracle Database performance issue in spring-boot application,Is it possible to intentionally prioritise/configure the DB such a way.
AppDynamics,61509776,nan,1,"2020/04/29, 22:09:51",True,"2020/04/29, 22:29:43",nan,2344459.0,696.0,-1,61,Oracle Database performance issue in spring-boot application,"If yes, where should I look into the database."
AppDynamics,61509776,nan,1,"2020/04/29, 22:09:51",True,"2020/04/29, 22:29:43",nan,2344459.0,696.0,-1,61,Oracle Database performance issue in spring-boot application,Any Idea why this is happening?
AppDynamics,61509776,nan,1,"2020/04/29, 22:09:51",True,"2020/04/29, 22:29:43",nan,2344459.0,696.0,-1,61,Oracle Database performance issue in spring-boot application,I am totally clueless here.
AppDynamics,37658175,nan,1,"2016/06/06, 15:59:57",False,"2016/06/06, 17:02:01","2016/06/06, 17:02:01",6289531.0,101.0,-1,253,exclude regular expression,I have a regular expression for the HttpOnly configuration :
AppDynamics,37658175,nan,1,"2016/06/06, 15:59:57",False,"2016/06/06, 17:02:01","2016/06/06, 17:02:01",6289531.0,101.0,-1,253,exclude regular expression,Header edit Set-Cookie ^(.
AppDynamics,37658175,nan,1,"2016/06/06, 15:59:57",False,"2016/06/06, 17:02:01","2016/06/06, 17:02:01",6289531.0,101.0,-1,253,exclude regular expression,*)$ $1;HttpOnly;Secure
AppDynamics,37658175,nan,1,"2016/06/06, 15:59:57",False,"2016/06/06, 17:02:01","2016/06/06, 17:02:01",6289531.0,101.0,-1,253,exclude regular expression,"For Appdynamics EUM, i want to exclude from this regular expression everything that begin with ""ADRUM"" (without quotes)."
AppDynamics,37658175,nan,1,"2016/06/06, 15:59:57",False,"2016/06/06, 17:02:01","2016/06/06, 17:02:01",6289531.0,101.0,-1,253,exclude regular expression,How can i proceed ?
AppDynamics,37658175,nan,1,"2016/06/06, 15:59:57",False,"2016/06/06, 17:02:01","2016/06/06, 17:02:01",6289531.0,101.0,-1,253,exclude regular expression,Thanks a lot for your help
AppDynamics,37658175,nan,1,"2016/06/06, 15:59:57",False,"2016/06/06, 17:02:01","2016/06/06, 17:02:01",6289531.0,101.0,-1,253,exclude regular expression,Best regards !
AppDynamics,37658175,nan,1,"2016/06/06, 15:59:57",False,"2016/06/06, 17:02:01","2016/06/06, 17:02:01",6289531.0,101.0,-1,253,exclude regular expression,Ludo
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,My system setup is like: Application A takes requests from outside world and communicates with the backend REST apis.
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,REST api also communicates with mysql database.
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,My requirement is to have a tool from which I can just monitor the resource usage and may be the performance of the web server.
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,I want to have graphs for the resource usage which means I need historical data otherwise I would have just used the windows task manager to see the resource usage.
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,This means I do not need any load generator(that will be done by the Application A) just a resource monitor.
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,"I googled and found tools like appdynamics, Nagios, munin but not sure if they are what I need."
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,I haven't done performance testing earlier so there's lot of confusion.
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,Just looking for some guidance.
AppDynamics,32768813,32768918.0,1,"2015/09/24, 21:53:29",True,"2016/02/03, 19:13:19",nan,3275095.0,1405.0,-2,82,A Simple utility to monitor web application performance,Thanks
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,I am new to the AppDynamics.
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,We want to integrate AppDynamics in our Angular application (It is intranet Single Page Application).
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,I saw this page but this is about AngularJS not Angular.
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,https://www.appdynamics.com/supported-technologies/java/angularjs-monitoring
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,We are using Cloud Foundry to host our application.
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,There is no issues at backend service.
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,Since it comes with property files where we added AppDynamic entries and then when we push our application it will be integrated with AppDynamics.
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,But where as Angular doesn't have that configuration.
AppDynamics,66992681,nan,1,"2021/04/07, 22:11:36",False,"2021/04/15, 14:03:49",nan,912111.0,3105.0,-3,36,Integrate app dynamics in the Angular application,So any suggestion about how to integrate AppDynamics to an Angular Application.
AppDynamics,50682773,nan,1,"2018/06/04, 17:21:25",False,"2018/06/05, 10:03:15","2018/06/04, 17:45:24",9879815.0,169.0,-3,33,Getting error deployment due to two different OS configurations in VSTS,We are facing the below issue on the bundle which is created by auto build.
AppDynamics,50682773,nan,1,"2018/06/04, 17:21:25",False,"2018/06/05, 10:03:15","2018/06/04, 17:45:24",9879815.0,169.0,-3,33,Getting error deployment due to two different OS configurations in VSTS,The build agent is configured with 32 bit server and our app server is with 64 bit server.
AppDynamics,50682773,nan,1,"2018/06/04, 17:21:25",False,"2018/06/05, 10:03:15","2018/06/04, 17:45:24",9879815.0,169.0,-3,33,Getting error deployment due to two different OS configurations in VSTS,Some of our node modules such as Appdynamics are not working with two different OS configurations.
Datadog,49671175,nan,4,"2018/04/05, 14:24:59",True,"2020/08/28, 05:49:42",nan,2894711.0,1372.0,13,4468,Replace no data by zeros in datadog graphs,There does not seem to be a way to replace no data by zeros when using formulas in datadog.
Datadog,49671175,nan,4,"2018/04/05, 14:24:59",True,"2020/08/28, 05:49:42",nan,2894711.0,1372.0,13,4468,Replace no data by zeros in datadog graphs,"I've tried fill zero but it doesn't seem to work
I would simply like my dd agent monitor to display 0 instead of no data when it is down"
Datadog,55848522,nan,1,"2019/04/25, 14:42:46",False,"2021/03/29, 16:43:11",nan,672798.0,3398.0,12,3074,query metrics on tag value with regex in datadog,I want to filter metrics on tag value with a regex.
Datadog,55848522,nan,1,"2019/04/25, 14:42:46",False,"2021/03/29, 16:43:11",nan,672798.0,3398.0,12,3074,query metrics on tag value with regex in datadog,I can do it in Prometheus but I could not find an equivalent way in Datadog.
Datadog,55848522,nan,1,"2019/04/25, 14:42:46",False,"2021/03/29, 16:43:11",nan,672798.0,3398.0,12,3074,query metrics on tag value with regex in datadog,"For example, to select the following metric whose  status  tag value starts with  2 , I can use the query  http.server.requests.count{status=~""^2..$""}"
Datadog,55848522,nan,1,"2019/04/25, 14:42:46",False,"2021/03/29, 16:43:11",nan,672798.0,3398.0,12,3074,query metrics on tag value with regex in datadog,"I have the same metric with the same tags in Datadog too, but couldn't find a way to have the same query."
Datadog,34398692,34400454.0,3,"2015/12/21, 17:04:52",True,"2018/03/23, 12:50:05","2015/12/21, 20:17:25",750117.0,7984.0,11,13993,Spring boot metrics + datadog,Does anyone know how to integrate Spring boot metrics with datadog?
Datadog,34398692,34400454.0,3,"2015/12/21, 17:04:52",True,"2018/03/23, 12:50:05","2015/12/21, 20:17:25",750117.0,7984.0,11,13993,Spring boot metrics + datadog,Datadog  is a cloud-scale monitoring service for IT.
Datadog,34398692,34400454.0,3,"2015/12/21, 17:04:52",True,"2018/03/23, 12:50:05","2015/12/21, 20:17:25",750117.0,7984.0,11,13993,Spring boot metrics + datadog,It allows users to easily visualice their data using a lot of charts and graphs.
Datadog,34398692,34400454.0,3,"2015/12/21, 17:04:52",True,"2018/03/23, 12:50:05","2015/12/21, 20:17:25",750117.0,7984.0,11,13993,Spring boot metrics + datadog,I have a spring boot application that is using  dropwizard  metrics to populate a lot of information about all methods I annotated with  @Timed .
Datadog,34398692,34400454.0,3,"2015/12/21, 17:04:52",True,"2018/03/23, 12:50:05","2015/12/21, 20:17:25",750117.0,7984.0,11,13993,Spring boot metrics + datadog,On the other hand I'm deploying my application in heroku so I can't install a Datadog agent.
Datadog,34398692,34400454.0,3,"2015/12/21, 17:04:52",True,"2018/03/23, 12:50:05","2015/12/21, 20:17:25",750117.0,7984.0,11,13993,Spring boot metrics + datadog,I want to know if there is a way to automatically integrate spring boot metric system reporting with datadog.
Datadog,37010163,37013010.0,3,"2016/05/03, 20:04:03",True,"2020/10/01, 10:13:05",nan,496289.0,12342.0,11,4427,Does Datadog support graphs with 2 Y-axis with different scales?,Like this one:
Datadog,37010163,37013010.0,3,"2016/05/03, 20:04:03",True,"2020/10/01, 10:13:05",nan,496289.0,12342.0,11,4427,Does Datadog support graphs with 2 Y-axis with different scales?,[
Datadog,37010163,37013010.0,3,"2016/05/03, 20:04:03",True,"2020/10/01, 10:13:05",nan,496289.0,12342.0,11,4427,Does Datadog support graphs with 2 Y-axis with different scales?,"If yes, how do I create one?"
Datadog,37010163,37013010.0,3,"2016/05/03, 20:04:03",True,"2020/10/01, 10:13:05",nan,496289.0,12342.0,11,4427,Does Datadog support graphs with 2 Y-axis with different scales?,"From all documentation I've read so far, it doesn't seem to support it."
Datadog,37010163,37013010.0,3,"2016/05/03, 20:04:03",True,"2020/10/01, 10:13:05",nan,496289.0,12342.0,11,4427,Does Datadog support graphs with 2 Y-axis with different scales?,But I don't see anyone confirming that it's not supported anywhere.
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,I installed dd-agent on Amazon linux ec2.
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,"If I run my python script directly on the host machine (I used the SDK named ""dogstatsd-python""), all the metrics can be sent to datadog (I logged in to datadoghq.com and saw the metrics there)."
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,the script is something like:
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,"However, I launched a docker container and run the same script from inside the container:"
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,"'172.14.0.1' is the IP of the host, which was extracted with command"
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,No metrics were sent to datadog at all.....
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,"I'm guessing that maybe it's due to some configuration issue like ""address binding""."
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,Maybe the dd-agent I installed on the host can only receive metrics from 'localhost'.
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,Hope someone could help me.
Datadog,35111147,nan,3,"2016/01/31, 08:44:06",True,"2016/09/22, 15:43:05","2016/01/31, 08:49:21",4386638.0,171.0,10,4401,datadog agent not reachable from inside docker container,Thank you in advance.
Datadog,57918254,59791963.0,1,"2019/09/13, 09:20:07",True,"2020/01/17, 19:12:16",nan,1787315.0,1110.0,7,834,Datadog: Use a tag value in an alias,"I have a timeseries graph in a time board that displays data for one metric that has multiple tags called ""page""."
Datadog,57918254,59791963.0,1,"2019/09/13, 09:20:07",True,"2020/01/17, 19:12:16",nan,1787315.0,1110.0,7,834,Datadog: Use a tag value in an alias,"The graph has one line for each tag and I'm running functions on the values, so the query for my data is ""ewma_5(avg:client.load_time{env:prod}) by {page}""."
Datadog,57918254,59791963.0,1,"2019/09/13, 09:20:07",True,"2020/01/17, 19:12:16",nan,1787315.0,1110.0,7,834,Datadog: Use a tag value in an alias,"This query means the tooltip values when I hover on the graph are things like ""ewma_5(avg:client.load_time{env:prod})""."
Datadog,57918254,59791963.0,1,"2019/09/13, 09:20:07",True,"2020/01/17, 19:12:16",nan,1787315.0,1110.0,7,834,Datadog: Use a tag value in an alias,"I want to know if there is anyway to use the alias function with the tag value in it, so something like ""alias"": ""{page}""?"
Datadog,35608127,nan,1,"2016/02/24, 18:48:07",False,"2020/10/25, 04:57:48",nan,135336.0,3637.0,6,482,Is there any way to get log events from crashlytics / fabric into ELK or a SaaS metrics platform like datadog?,"If you use a stack like ELK or datadog for collecting server-side logs and events, how do you integrate mobile-side metrics?"
Datadog,35608127,nan,1,"2016/02/24, 18:48:07",False,"2020/10/25, 04:57:48",nan,135336.0,3637.0,6,482,Is there any way to get log events from crashlytics / fabric into ELK or a SaaS metrics platform like datadog?,"Is there any way to get these out of crashlytics directly, or does this log aggregation need to be implemented separately?"
Datadog,40741803,42023919.0,1,"2016/11/22, 14:24:20",True,"2017/03/14, 23:26:05","2016/11/22, 14:38:00",2525626.0,1199.0,6,659,Why is my DataDog instance reporting a Kubernetes &quot;no_pod&quot;?,We are running a Kubernetes Cluster in AWS and we are collecting the metrics in DataDog using the dd-agent DaemonSet.
Datadog,40741803,42023919.0,1,"2016/11/22, 14:24:20",True,"2017/03/14, 23:26:05","2016/11/22, 14:38:00",2525626.0,1199.0,6,659,Why is my DataDog instance reporting a Kubernetes &quot;no_pod&quot;?,"We have a Pod being displayed in our metrics tagged as ""no_pod"" and it is using a lot of resources, Memory/CPU/NetworkTx/NetworkRX."
Datadog,40741803,42023919.0,1,"2016/11/22, 14:24:20",True,"2017/03/14, 23:26:05","2016/11/22, 14:38:00",2525626.0,1199.0,6,659,Why is my DataDog instance reporting a Kubernetes &quot;no_pod&quot;?,"Is there any explanation to what this pod is, how I can find it, kill it, restart it etc?"
Datadog,40741803,42023919.0,1,"2016/11/22, 14:24:20",True,"2017/03/14, 23:26:05","2016/11/22, 14:38:00",2525626.0,1199.0,6,659,Why is my DataDog instance reporting a Kubernetes &quot;no_pod&quot;?,"I have found the dd-agent  source code  which seems to define the ""no_pod"" label but I can't make much sense of why it is there, where it is coming from and how I can find it through kubectl etc."
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,I don't understand the difference between  events  and  metrics  in  DataDog .
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,I'm trying to create a count indicator in my  dashboard  so I can now how many times some type of event has happened.
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,"There is a lot of events named  some.event.name , but no matter what query I use, it always returns  1 ."
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,"I've tried with this queries,"
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,sum:some.event.name{*}
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,count_nonzero(sum:some.event.name{*})
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,count_not_null(sum:some.event.name{*})
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,I've also tried with other aggregation functions  avg|max|min|sum  and allways the result is  1 .
Datadog,42927552,nan,3,"2017/03/21, 14:45:18",True,"2021/01/04, 18:05:02",nan,2599875.0,1929.0,6,3263,How to count events in Datadog Screenboard,Any help will be highly appreaciated.
Datadog,45506314,58493964.0,1,"2017/08/04, 15:00:49",True,"2019/10/21, 23:51:33","2017/08/05, 12:59:51",401581.0,6126.0,6,297,Send Datadog tags in exometer,I am using  exometer  and the  exometer_report_statsd  reporter to report Phoenix endpoints response times to Datadog via dogstatsd.
Datadog,45506314,58493964.0,1,"2017/08/04, 15:00:49",True,"2019/10/21, 23:51:33","2017/08/05, 12:59:51",401581.0,6126.0,6,297,Send Datadog tags in exometer,"From a Plug, I am calling  :exometer.update/2  to send the response time to Datadog."
Datadog,45506314,58493964.0,1,"2017/08/04, 15:00:49",True,"2019/10/21, 23:51:33","2017/08/05, 12:59:51",401581.0,6126.0,6,297,Send Datadog tags in exometer,E.g:
Datadog,45506314,58493964.0,1,"2017/08/04, 15:00:49",True,"2019/10/21, 23:51:33","2017/08/05, 12:59:51",401581.0,6126.0,6,297,Send Datadog tags in exometer,":exometer.update [:app_name, :webapp, :resp_time], 25"
Datadog,45506314,58493964.0,1,"2017/08/04, 15:00:49",True,"2019/10/21, 23:51:33","2017/08/05, 12:59:51",401581.0,6126.0,6,297,Send Datadog tags in exometer,"Now, I want to have only one metric  app_name.webapp.resp_time  instead of one metric per endpoint and version so I thought of using tags."
Datadog,45506314,58493964.0,1,"2017/08/04, 15:00:49",True,"2019/10/21, 23:51:33","2017/08/05, 12:59:51",401581.0,6126.0,6,297,Send Datadog tags in exometer,"The question is, where should I include the tags?"
Datadog,42588899,nan,2,"2017/03/03, 23:41:56",False,"2019/01/18, 16:57:13","2017/03/04, 13:02:15",524430.0,121.0,4,2097,Is there a way to report custom DataDog metrics from AWS Lambda?,I'm looking to report custom metrics from Lambda functions to Datadog.
Datadog,42588899,nan,2,"2017/03/03, 23:41:56",False,"2019/01/18, 16:57:13","2017/03/04, 13:02:15",524430.0,121.0,4,2097,Is there a way to report custom DataDog metrics from AWS Lambda?,"I need things like counters, gauges, histograms."
Datadog,42588899,nan,2,"2017/03/03, 23:41:56",False,"2019/01/18, 16:57:13","2017/03/04, 13:02:15",524430.0,121.0,4,2097,Is there a way to report custom DataDog metrics from AWS Lambda?,Datadog  documentation  outlines two options for reporting metrics from AWS Lambda:
Datadog,42588899,nan,2,"2017/03/03, 23:41:56",False,"2019/01/18, 16:57:13","2017/03/04, 13:02:15",524430.0,121.0,4,2097,Is there a way to report custom DataDog metrics from AWS Lambda?,"The fine print in the document above mentions that the printing method only supports counters and gauges, so that's obviously not enough for my usecase (I also need histograms)."
Datadog,42588899,nan,2,"2017/03/03, 23:41:56",False,"2019/01/18, 16:57:13","2017/03/04, 13:02:15",524430.0,121.0,4,2097,Is there a way to report custom DataDog metrics from AWS Lambda?,"Now, the second method - the API - only supports reporting time series points, which I'm assuming are just gauges (right?"
Datadog,42588899,nan,2,"2017/03/03, 23:41:56",False,"2019/01/18, 16:57:13","2017/03/04, 13:02:15",524430.0,121.0,4,2097,Is there a way to report custom DataDog metrics from AWS Lambda?,"), according to the  API documentation ."
Datadog,42588899,nan,2,"2017/03/03, 23:41:56",False,"2019/01/18, 16:57:13","2017/03/04, 13:02:15",524430.0,121.0,4,2097,Is there a way to report custom DataDog metrics from AWS Lambda?,"So, is there a way to report metrics to Datadog from my Lambda functions, short of setting up a statsd server in EC2 and calling out to it using dogstatsd?"
Datadog,42588899,nan,2,"2017/03/03, 23:41:56",False,"2019/01/18, 16:57:13","2017/03/04, 13:02:15",524430.0,121.0,4,2097,Is there a way to report custom DataDog metrics from AWS Lambda?,Anyone have any luck getting around this?
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,New to datadog so I'm just really confused.
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,First configuration was fast and simple.
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,"However as I want some app specific charts, it doesn't seem as clear as before for my current scenario."
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,"We have one host with several docker machines, one for each service:
- nginx
- varnish
- apache
- database (mysql)"
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,We've installed datadog client inside the host and also docker integration and everything works fine.
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,"What I don't get is how get metrics from apache or varnish, or whatever service that is inside docker."
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,Reading the docs in varnish  for example you have to execute:
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,"However, where should I run the command?"
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,"dd-agent user exists only in the host, not in the docker container."
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,Varnish is just the other way round.
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,Should I need to install the agent on each container?
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,It would be considered as another host for pricing?
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,"In mysql case, I just have to configure the agent:"
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,"But as my host and the container are in separate routes, should I create a new docker container with the agent so it cat get to db container (changing server field)?"
Datadog,42722353,nan,1,"2017/03/10, 17:45:24",False,"2018/08/21, 06:57:14",nan,2670996.0,2519.0,4,1141,Datadog integration inside docker containers,Is it considered again as another host?
Datadog,45188547,nan,1,"2017/07/19, 13:58:09",True,"2017/11/07, 23:12:04",nan,902415.0,6992.0,4,1525,Container disk usage in DataDog,Is there any way to monitor disk usage of docker containers in DataDog?
Datadog,45188547,nan,1,"2017/07/19, 13:58:09",True,"2017/11/07, 23:12:04",nan,902415.0,6992.0,4,1525,Container disk usage in DataDog,"I can see in DataDog web all the CPU, RAM and IO metrics for my containers."
Datadog,45188547,nan,1,"2017/07/19, 13:58:09",True,"2017/11/07, 23:12:04",nan,902415.0,6992.0,4,1525,Container disk usage in DataDog,But I can't see any of disk space related metrics.
Datadog,45188547,nan,1,"2017/07/19, 13:58:09",True,"2017/11/07, 23:12:04",nan,902415.0,6992.0,4,1525,Container disk usage in DataDog,Their page  https://docs.datadoghq.com/integrations/docker/  says about:
Datadog,45188547,nan,1,"2017/07/19, 13:58:09",True,"2017/11/07, 23:12:04",nan,902415.0,6992.0,4,1525,Container disk usage in DataDog,I can't find these neither in Dashboards   Docker nor in Metrics   Explorer
Datadog,45188547,nan,1,"2017/07/19, 13:58:09",True,"2017/11/07, 23:12:04",nan,902415.0,6992.0,4,1525,Container disk usage in DataDog,"I'm new to DataDog, so possibly missing something obvious here."
Datadog,45954769,nan,1,"2017/08/30, 10:46:15",True,"2019/04/30, 11:27:17","2017/08/30, 14:49:13",1465701.0,321.0,4,2268,Unable to access datadog agent on host from docker instance,I am unable to access datadog agent on my host from a docker container.
Datadog,45954769,nan,1,"2017/08/30, 10:46:15",True,"2019/04/30, 11:27:17","2017/08/30, 14:49:13",1465701.0,321.0,4,2268,Unable to access datadog agent on host from docker instance,I am using EC2 container service to host my docker containers.
Datadog,45954769,nan,1,"2017/08/30, 10:46:15",True,"2019/04/30, 11:27:17","2017/08/30, 14:49:13",1465701.0,321.0,4,2268,Unable to access datadog agent on host from docker instance,I have already set the option  non_local_traffic : yes  in datadog config.
Datadog,45954769,nan,1,"2017/08/30, 10:46:15",True,"2019/04/30, 11:27:17","2017/08/30, 14:49:13",1465701.0,321.0,4,2268,Unable to access datadog agent on host from docker instance,My config looks like this:
Datadog,45954769,nan,1,"2017/08/30, 10:46:15",True,"2019/04/30, 11:27:17","2017/08/30, 14:49:13",1465701.0,321.0,4,2268,Unable to access datadog agent on host from docker instance,To access the host from the docker instance I use this URL from within the docker container :  http://169.254.169.254/latest/meta-data/local-ipv4/  which is discussed here:  http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html
Datadog,45954769,nan,1,"2017/08/30, 10:46:15",True,"2019/04/30, 11:27:17","2017/08/30, 14:49:13",1465701.0,321.0,4,2268,Unable to access datadog agent on host from docker instance,This URL gives me the IP of the host machine which is then passed over to python datadog client running in the docker machine.
Datadog,52096873,nan,0,"2018/08/30, 15:16:39",False,"2018/08/30, 15:16:39",nan,2336267.0,51.0,4,1283,Unique tag count in a DataDog top list,"I've a metric which has 2 tags (it has more but this is for simplicity),  client  and  rule , and its value of course."
Datadog,52096873,nan,0,"2018/08/30, 15:16:39",False,"2018/08/30, 15:16:39",nan,2336267.0,51.0,4,1283,Unique tag count in a DataDog top list,"With it I can see the total count of the values for each client, each rule and each ruleXclient."
Datadog,52096873,nan,0,"2018/08/30, 15:16:39",False,"2018/08/30, 15:16:39",nan,2336267.0,51.0,4,1283,Unique tag count in a DataDog top list,"Now I want to create a top list which would tell me the amount of unique clients per rule, so if the metric is reporting that 2 clients (2 values of the tag client) have 4 hits each for a rule (single value for the tag rule) I'd like the top list to show me:"
Datadog,52096873,nan,0,"2018/08/30, 15:16:39",False,"2018/08/30, 15:16:39",nan,2336267.0,51.0,4,1283,Unique tag count in a DataDog top list,2: RuleA
Datadog,52096873,nan,0,"2018/08/30, 15:16:39",False,"2018/08/30, 15:16:39",nan,2336267.0,51.0,4,1283,Unique tag count in a DataDog top list,Is that even possible?
Datadog,52096873,nan,0,"2018/08/30, 15:16:39",False,"2018/08/30, 15:16:39",nan,2336267.0,51.0,4,1283,Unique tag count in a DataDog top list,How could I approach this if it isn't?
Datadog,52096873,nan,0,"2018/08/30, 15:16:39",False,"2018/08/30, 15:16:39",nan,2336267.0,51.0,4,1283,Unique tag count in a DataDog top list,I'm using dogstreams to report the metric.
Datadog,53435248,53449880.0,1,"2018/11/22, 18:42:39",True,"2019/07/29, 16:36:02","2019/07/28, 12:02:43",1099819.0,1129.0,4,1284,Can I export Datadog dashboards via Datadog REST API?,Is it possible to export or download Datadog dashboards via Datadog REST API?
Datadog,53435248,53449880.0,1,"2018/11/22, 18:42:39",True,"2019/07/29, 16:36:02","2019/07/28, 12:02:43",1099819.0,1129.0,4,1284,Can I export Datadog dashboards via Datadog REST API?,Export and update of Datadog Monitors works fine.
Datadog,53435248,53449880.0,1,"2018/11/22, 18:42:39",True,"2019/07/29, 16:36:02","2019/07/28, 12:02:43",1099819.0,1129.0,4,1284,Can I export Datadog dashboards via Datadog REST API?,I need the same functionality for dashboards.
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,I'm running web apps as Docker containers in Azure App Service.
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,"I'd like to add Datadog agent to each container to, e.g., read the log files in the background and post them to Datadog log management."
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,This is what I have tried:
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,1) Installing Datadog agent as extension as described in  this post .
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,"This option does not seem to be available for App Service apps, only on VMs."
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,2) Using multi-container apps as described  in this post .
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,"However, we have not found a simple way to integrate this with  Azure DevOps release pipelines ."
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,I guess it might be possible to create a custom deployment task wrapping Azure CLI commands?
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,3) Including Datadog agent into our Dockerfiles by following how Datadog Dockerfiles  are built .
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,The process seems quite complicated and add lots of extra dependencies to our Dockerfile.
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,We'd also not like to inherit our Dockerfiles from Datadog Dockerfile with  FROM datadog/agent .
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,I'd assume this must be a pretty standard problem for Azure+Datadog users.
Datadog,53880368,54194264.0,5,"2018/12/21, 08:50:35",True,"2019/10/25, 02:03:41",nan,10561443.0,121.0,4,1313,How to use Datadog agent in Azure App Service?,Any ideas what's the cleanest option?
Datadog,58597581,nan,0,"2019/10/28, 21:56:46",False,"2019/10/28, 22:23:04","2019/10/28, 22:23:04",3519838.0,125.0,4,1009,Configuring Tracing for Datadog in Spring Boot application,I have a Spring Boot application and want to configure HTTP request tracing via dependency management without having to deal with setting up the java agent.
Datadog,58597581,nan,0,"2019/10/28, 21:56:46",False,"2019/10/28, 22:23:04","2019/10/28, 22:23:04",3519838.0,125.0,4,1009,Configuring Tracing for Datadog in Spring Boot application,Can anyone suggest the best way to do this?
Datadog,58597581,nan,0,"2019/10/28, 21:56:46",False,"2019/10/28, 22:23:04","2019/10/28, 22:23:04",3519838.0,125.0,4,1009,Configuring Tracing for Datadog in Spring Boot application,"I have the  micrometer-registry-datadog  dependency added to my pom and can see that there are a lot of undocumented  com.datadoghq  dependencies, but am unsure if any of these will solve my problem."
Datadog,58597581,nan,0,"2019/10/28, 21:56:46",False,"2019/10/28, 22:23:04","2019/10/28, 22:23:04",3519838.0,125.0,4,1009,Configuring Tracing for Datadog in Spring Boot application,"I'm getting all of the JVM metrics, but want some more APM-type metrics now."
Datadog,58597581,nan,0,"2019/10/28, 21:56:46",False,"2019/10/28, 22:23:04","2019/10/28, 22:23:04",3519838.0,125.0,4,1009,Configuring Tracing for Datadog in Spring Boot application,Ideally I'd like to use the  @Timed  annotation and various others to get detailed metrics around API calls.
Datadog,59500017,61276030.0,2,"2019/12/27, 13:18:42",True,"2020/04/17, 19:13:14",nan,3869978.0,697.0,4,738,How to monitor an ElasticSearch Cluster on the Elastic Cloud with Datadog?,We have an elasticsearch cluster deployed to the Elastic Cloud and would like to send monitoring/health metrics to Datadog.
Datadog,59500017,61276030.0,2,"2019/12/27, 13:18:42",True,"2020/04/17, 19:13:14",nan,3869978.0,697.0,4,738,How to monitor an ElasticSearch Cluster on the Elastic Cloud with Datadog?,What is the best way to do that?
Datadog,59500017,61276030.0,2,"2019/12/27, 13:18:42",True,"2020/04/17, 19:13:14",nan,3869978.0,697.0,4,738,How to monitor an ElasticSearch Cluster on the Elastic Cloud with Datadog?,"It seems like our options are:
* Installing the datadog agent binary via the plugins upload
* Using metric beat -  logstash -  datadog_metrics output"
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,So I have an ongoing metric of events.
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,They are either tagged as success or fail.
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,"So I have 3 numbers; failed, completed, total."
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,This is easily illustrated (in Datadog) using a stacked bar graph like so:
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,So the dark part are the failures.
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,"And by looking at the y scale and the dashed red line for scale, this easily tells a human if the rate is a problem and significant."
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,"Which to mean means that I have a failure rate in excess of 60%, over at least some time (10 minutes?)"
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,and that there are enough events in this period to consider the rate exceptional.
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,So I am looking for some sort of formula that starts with: failures divided by total (giving me a score between 0 and 1) and then multiplies this somehow again with the total and some thresholds that I decide means that the total is high enough for me to get an automated alert.
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,"For extra credit, here is the actual Datadog metric that I am trying to get to work:"
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,"(sum:event{status:fail}.rollup(sum, 300) / sum:event{}.rollup(sum,
  300))"
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,And I am watching for 15 minutes and alert of score above 0.75.
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,"But I am not sure about sum, count, avg, rollup or count."
Datadog,29011338,nan,0,"2015/03/12, 15:39:16",False,"2015/03/12, 15:39:16",nan,4013.0,6034.0,3,515,How do I weight my rate by sample size (in Datadog)?,And ofc this alert will send me mail during the night when the total events goes low enough to were a high failure rate isn't proof of any problem.
Datadog,35224366,35509278.0,1,"2016/02/05, 14:33:08",True,"2016/02/19, 17:47:27",nan,99834.0,135806.0,3,2569,How can I combine datadog io metrics in order to identify disk bottlenecks?,I am trying to create an alert in DataDog that would alert us when disk performance slows down our machines.
Datadog,35224366,35509278.0,1,"2016/02/05, 14:33:08",True,"2016/02/19, 17:47:27",nan,99834.0,135806.0,3,2569,How can I combine datadog io metrics in order to identify disk bottlenecks?,"As a business requirement I would say that if the IO is almost saturated (over 90%) for more than 30 minutes, the alert should be triggered."
Datadog,35224366,35509278.0,1,"2016/02/05, 14:33:08",True,"2016/02/19, 17:47:27",nan,99834.0,135806.0,3,2569,How can I combine datadog io metrics in order to identify disk bottlenecks?,"Here are the current set of metrics that are recorded:
 
sys.cpu.iowait
system.io.avg_q_sz
system.io.avg_rq_sz
system.io.await
system.io.r_await
system.io.r_s
system.io.rkb_s
system.io.rrqm_s
system.io.svctm
system.io.util
system.io.w_await
system.io.w_s
system.io.wkb_s
system.io.wrqm_s"
Datadog,35224366,35509278.0,1,"2016/02/05, 14:33:08",True,"2016/02/19, 17:47:27",nan,99834.0,135806.0,3,2569,How can I combine datadog io metrics in order to identify disk bottlenecks?,"It is possible to use any formulas to combine these, including SUM and AVG values."
Datadog,35554988,35560762.0,1,"2016/02/22, 15:30:54",True,"2016/02/22, 20:06:38","2016/02/22, 15:38:07",2039736.0,4484.0,3,2440,Modify scale of Datadog metric,I have a time series presenting time values like this one:
Datadog,35554988,35560762.0,1,"2016/02/22, 15:30:54",True,"2016/02/22, 20:06:38","2016/02/22, 15:38:07",2039736.0,4484.0,3,2440,Modify scale of Datadog metric,"I want to change the y-axis to represent hours instead of milliseconds, i.e."
Datadog,35554988,35560762.0,1,"2016/02/22, 15:30:54",True,"2016/02/22, 20:06:38","2016/02/22, 15:38:07",2039736.0,4484.0,3,2440,Modify scale of Datadog metric,divide by 3600.
Datadog,35554988,35560762.0,1,"2016/02/22, 15:30:54",True,"2016/02/22, 20:06:38","2016/02/22, 15:38:07",2039736.0,4484.0,3,2440,Modify scale of Datadog metric,Any idea how to do it?
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,I'm running a number of python apps as Replica Sets inside of kubernetes on Google Container Engine (gke).
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,Along side them I've created the Datadog DaemonSet which launches a dd-agent on each node in my cluster.
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,Now I would like to use that agents dogstatsd for metrics logging from python apps as well as try out the new Datadog APM.
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,If I just install the ddtrace python package and use it like documented it fills up my logs with
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,Clearly it don't have magical way to guess how to access port 8126/7777 of the ddagent pods.
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,Ive tried creating a Service which expose the ports:
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,but my python pods still don't seem to be able access for example  os.environ['DATADOG_STATSD_PORT_8126_TCP_ADDR']  and  .._PORT .
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,"They are defined and all, I just still get the connection timed out."
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,If I connect to the dd-agent pods and enable tcpdump I also don't see any trafic on ports 8126 etc.
Datadog,42441120,42481675.0,2,"2017/02/24, 16:27:23",True,"2017/05/18, 07:21:01",nan,86.0,6657.0,3,1333,Accessing dogstatsd (datadog) Pod from adjecent Kubernetes Pods,The dd-agent DaemonSet is defined like this:
Datadog,42538664,nan,1,"2017/03/01, 19:53:01",True,"2017/04/28, 02:11:48",nan,351895.0,862.0,3,2079,Datadog monitor API/terraform process monitor check,"I'm trying to integrate a Datadog monitor check on sshd process in my terraform codebase, but I'm getting  datadog_monitor.host_is_up2: error updating monitor: API error 400 Bad Request: {""errors"":[""The value provided for parameter 'query' is invalid""]}"
Datadog,42538664,nan,1,"2017/03/01, 19:53:01",True,"2017/04/28, 02:11:48",nan,351895.0,862.0,3,2079,Datadog monitor API/terraform process monitor check,What I did was to copy the monitor's query I created on the Datadog panel and pasted it into the tf file:
Datadog,42538664,nan,1,"2017/03/01, 19:53:01",True,"2017/04/28, 02:11:48",nan,351895.0,862.0,3,2079,Datadog monitor API/terraform process monitor check,"ofc the query example  ""avg(last_1h):avg:aws.ec2.cpu{environment:foo,host:foo} by {host} &gt; 2""  works"
Datadog,42538664,nan,1,"2017/03/01, 19:53:01",True,"2017/04/28, 02:11:48",nan,351895.0,862.0,3,2079,Datadog monitor API/terraform process monitor check,"What's the right way to check via Datadog API or terraform if a specific service, like sshd, is up or not?"
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,I'm trying to use the datadog api but the initialize method keeps giving the error 'INFO No agent or invalid configuration file found'.
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,The datadog agent is running:
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,"(PYTHON) daphnepaparis@Daphnes-MBP-2 ~ $ /usr/local/bin/datadog-agent status
Datadog Agent (supervisor) is running all child processes"
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,And the configuration file permissions look alright:
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,"(PYTHON) daphnepaparis@Daphnes-MBP-2 ~ $ ls -l ~/.datadog-agent/datadog.conf
lrwxr-xr-x  1 daphnepaparis  staff  35 Mar 22 12:58 /Users/daphnepaparis/.datadog-agent/datadog.conf -  /opt/datadog-agent/etc/datadog.conf"
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,Original commands I'm running:
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,"In [1]: from datadog import initialize, api"
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,In [2]: options = {'api_key': '***'}
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,In [3]: initialize(**options)
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,2017-03-22 13:24:20 INFO No agent or invalid configuration file found
Datadog,42958937,44193701.0,1,"2017/03/22, 19:38:23",True,"2017/05/26, 07:35:57","2017/03/22, 22:21:51",5575741.0,33.0,3,2709,Datadog python api error INFO No agent or invalid configuration file found,Anyone able to help?
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,"I was able to follow these instructions carefully and thoroughly  https://docs.datadoghq.com/tracing/setup/python/ ,"
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,"I successfully installed DataDog Agent following this guide  https://docs.datadoghq.com/tracing/setup/ ,"
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,"I was also able to install MacOS tracer since it is required for mac user:  https://github.com/DataDog/datadog-trace-agent#run-on-osx ,"
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,I enabled apm_config in the configuration file found here:  https://docs.datadoghq.com/agent/faq/agent-configuration-files/?tab=agentv6#agent-main-configuration-file
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,I leave the  env: none  since I only need to run it in on development/debug mode.
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,Now Im currently on the step 4:  Instrument your application  guide for Flask and here the steps I took:
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,Add integration for flask:
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,And also my application runs in a docker container and this is what I get from the output log:
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,ERROR:ddtrace.writer:cannot send services to localhost:8126
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,Additional Information
Datadog,52390804,nan,1,"2018/09/18, 19:19:01",False,"2019/06/16, 18:14:27",nan,6143656.0,6349.0,3,1114,Datadog how to implement ddtrace on Flask application?,On the tracer agent:
Datadog,52587443,nan,0,"2018/10/01, 11:45:48",False,"2018/10/01, 11:45:48",nan,8511619.0,325.0,3,224,How do I setup Datadog with Google App Engine node.js runtime?,"According to the  gae_datadog  Github repo , the way to setup datadog in app engine is to clone the repo and add the following in  app.yaml :"
Datadog,52587443,nan,0,"2018/10/01, 11:45:48",False,"2018/10/01, 11:45:48",nan,8511619.0,325.0,3,224,How do I setup Datadog with Google App Engine node.js runtime?,"However, this doesn't appear to work with their nodejs runtime."
Datadog,52587443,nan,0,"2018/10/01, 11:45:48",False,"2018/10/01, 11:45:48",nan,8511619.0,325.0,3,224,How do I setup Datadog with Google App Engine node.js runtime?,Here is my  app.yaml :
Datadog,52587443,nan,0,"2018/10/01, 11:45:48",False,"2018/10/01, 11:45:48",nan,8511619.0,325.0,3,224,How do I setup Datadog with Google App Engine node.js runtime?,"It seems like the datadog url handler isn't used at all, because it 404."
Datadog,52587443,nan,0,"2018/10/01, 11:45:48",False,"2018/10/01, 11:45:48",nan,8511619.0,325.0,3,224,How do I setup Datadog with Google App Engine node.js runtime?,"I assume the node.js app takes precedence here, but I don't know how to change that."
Datadog,58476988,58681226.0,1,"2019/10/20, 23:23:28",True,"2019/11/03, 16:41:58",nan,712543.0,2206.0,3,1584,How to filter metrics in Telegraf before sending them to Datadog?,I have a service that exposes metrics in statsd format and telegraf instance which picks those metrics and sends them to both Prometheus and Datadog (there are two output plugin configurations for both of these).
Datadog,58476988,58681226.0,1,"2019/10/20, 23:23:28",True,"2019/11/03, 16:41:58",nan,712543.0,2206.0,3,1584,How to filter metrics in Telegraf before sending them to Datadog?,This works correctly.
Datadog,58476988,58681226.0,1,"2019/10/20, 23:23:28",True,"2019/11/03, 16:41:58",nan,712543.0,2206.0,3,1584,How to filter metrics in Telegraf before sending them to Datadog?,"However, I have a special requirement where I would need to filter certain metrics that will be sent to Datadog."
Datadog,58476988,58681226.0,1,"2019/10/20, 23:23:28",True,"2019/11/03, 16:41:58",nan,712543.0,2206.0,3,1584,How to filter metrics in Telegraf before sending them to Datadog?,My first inclination was to make change in  [[outputs.datadog]]  section of  telegraf.conf .
Datadog,58476988,58681226.0,1,"2019/10/20, 23:23:28",True,"2019/11/03, 16:41:58",nan,712543.0,2206.0,3,1584,How to filter metrics in Telegraf before sending them to Datadog?,"However, I don't see any specific configuration part where I could, for example, list just metrics that I need to be seen on Datadog."
Datadog,58476988,58681226.0,1,"2019/10/20, 23:23:28",True,"2019/11/03, 16:41:58",nan,712543.0,2206.0,3,1584,How to filter metrics in Telegraf before sending them to Datadog?,Is there any way to achieve this?
Datadog,58476988,58681226.0,1,"2019/10/20, 23:23:28",True,"2019/11/03, 16:41:58",nan,712543.0,2206.0,3,1584,How to filter metrics in Telegraf before sending them to Datadog?,Thanks.
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,I have been working with Datadog log ingestion for about a year now.
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,It's been (mostly) great to work with.
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,The documentation around running it inside of Kubernetes is a bit lacking though.
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,"Their documentation covers Docker thoroughly, but Kubernetes less so."
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,"When I installed Datadog into our Kubernetes clusters a year ago, there were two ways to do it, you could use a DaemonSet to ensure at least 1 Pod of Datadog runs on every Node."
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,Or you could install it as a Deployment.
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,I went with the DaemonSet option and used Helm to install it.
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,That worked quite well!
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,"Then we wanted to start using DogStatsD to ingest metrics about our applications, and it seemed at the time like this required the ""cluster-agent"" to run."
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,I have serious doubts about this part.
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,If I get all of the Datadog-related objects in my cluster I see the DaemonSet ( daemonset.apps/dd-agent-datadog ) and I also see a Deployment ( daemonset..apps/dd-agent-datadog ) on my cluster.
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,Is this right?
Datadog,60064427,60213083.0,1,"2020/02/04, 21:35:51",True,"2020/02/14, 09:17:50",nan,13800.0,13499.0,3,250,Proper Setup of Datadog Log Ingestion on Kubernetes,Do I really need to run both of those things to get log ingestion and metrics?
Datadog,60290897,64168376.0,1,"2020/02/19, 01:28:06",True,"2020/10/02, 11:24:58",nan,2884555.0,846.0,3,67,Is it possible to construct a datadog graph with a metric that matches two values for one tag?,"I  think  this should be possible, but I can't find documented syntax."
Datadog,60290897,64168376.0,1,"2020/02/19, 01:28:06",True,"2020/10/02, 11:24:58",nan,2884555.0,846.0,3,67,Is it possible to construct a datadog graph with a metric that matches two values for one tag?,I would like to construct a DD graph that displays metrics matching tag_one:A or tag_one:B.
Datadog,60290897,64168376.0,1,"2020/02/19, 01:28:06",True,"2020/10/02, 11:24:58",nan,2884555.0,846.0,3,67,Is it possible to construct a datadog graph with a metric that matches two values for one tag?,Is this possible?
Datadog,60290897,64168376.0,1,"2020/02/19, 01:28:06",True,"2020/10/02, 11:24:58",nan,2884555.0,846.0,3,67,Is it possible to construct a datadog graph with a metric that matches two values for one tag?,"If so, what is the syntax?"
Datadog,60662893,60677166.0,2,"2020/03/13, 01:04:25",True,"2020/03/13, 22:56:50","2020/03/13, 13:19:07",1363715.0,787.0,3,523,How do you pass file_system_blacklist arg to Datadog Docker Agent run command?,I want to exclude a path to avoid getting my logs spammed like so:
Datadog,60662893,60677166.0,2,"2020/03/13, 01:04:25",True,"2020/03/13, 22:56:50","2020/03/13, 13:19:07",1363715.0,787.0,3,523,How do you pass file_system_blacklist arg to Datadog Docker Agent run command?,"I'm running datadog as a docker agent using the command here:
 https://docs.datadoghq.com/agent/docker/?tab=standard#installation"
Datadog,60662893,60677166.0,2,"2020/03/13, 01:04:25",True,"2020/03/13, 22:56:50","2020/03/13, 13:19:07",1363715.0,787.0,3,523,How do you pass file_system_blacklist arg to Datadog Docker Agent run command?,how do I specify files to exclude in the docker run command?
Datadog,60662893,60677166.0,2,"2020/03/13, 01:04:25",True,"2020/03/13, 22:56:50","2020/03/13, 13:19:07",1363715.0,787.0,3,523,How do you pass file_system_blacklist arg to Datadog Docker Agent run command?,is it an environment variable?
Datadog,61900949,nan,1,"2020/05/20, 00:19:16",True,"2020/05/23, 15:46:33",nan,4124267.0,152.0,3,348,Adding tags for Datadog browser logs,I am using Datadog to monitor my browser console logs.
Datadog,61900949,nan,1,"2020/05/20, 00:19:16",True,"2020/05/23, 15:46:33",nan,4124267.0,152.0,3,348,Adding tags for Datadog browser logs,I need different tags in for datadog logs.
Datadog,61900949,nan,1,"2020/05/20, 00:19:16",True,"2020/05/23, 15:46:33",nan,4124267.0,152.0,3,348,Adding tags for Datadog browser logs,"The only option I fount is to add attributes to my logger using,"
Datadog,61900949,nan,1,"2020/05/20, 00:19:16",True,"2020/05/23, 15:46:33",nan,4124267.0,152.0,3,348,Adding tags for Datadog browser logs,"DD_LOGS.addContext('referrer', document.referrer);"
Datadog,61900949,nan,1,"2020/05/20, 00:19:16",True,"2020/05/23, 15:46:33",nan,4124267.0,152.0,3,348,Adding tags for Datadog browser logs,Is there any way for the frontend client application to have tags in datadog?
Datadog,61900949,nan,1,"2020/05/20, 00:19:16",True,"2020/05/23, 15:46:33",nan,4124267.0,152.0,3,348,Adding tags for Datadog browser logs,Or is the attribute and tags are same in Datadog
Datadog,64661079,nan,0,"2020/11/03, 12:32:26",False,"2020/11/03, 12:32:26",nan,5810648.0,2651.0,3,160,String interpolation with the name of metric in the Datadog query,There are a bunch of custom metrics that looks like:
Datadog,64661079,nan,0,"2020/11/03, 12:32:26",False,"2020/11/03, 12:32:26",nan,5810648.0,2651.0,3,160,String interpolation with the name of metric in the Datadog query,There is a template variables dropdown ( https://docs.datadoghq.com/dashboards/template_variables/ ) on the dashboard that contains all keys.
Datadog,64661079,nan,0,"2020/11/03, 12:32:26",False,"2020/11/03, 12:32:26",nan,5810648.0,2651.0,3,160,String interpolation with the name of metric in the Datadog query,"(Name of the variable is  $key  and values key1, key2..)"
Datadog,64661079,nan,0,"2020/11/03, 12:32:26",False,"2020/11/03, 12:32:26",nan,5810648.0,2651.0,3,160,String interpolation with the name of metric in the Datadog query,The metrics query like this works fine:
Datadog,64661079,nan,0,"2020/11/03, 12:32:26",False,"2020/11/03, 12:32:26",nan,5810648.0,2651.0,3,160,String interpolation with the name of metric in the Datadog query,But is it possible to parametrize the query with the  $key  variable by concatenation with the part of the name of metric?
Datadog,64661079,nan,0,"2020/11/03, 12:32:26",False,"2020/11/03, 12:32:26",nan,5810648.0,2651.0,3,160,String interpolation with the name of metric in the Datadog query,So it would look like this:
Datadog,64661079,nan,0,"2020/11/03, 12:32:26",False,"2020/11/03, 12:32:26",nan,5810648.0,2651.0,3,160,String interpolation with the name of metric in the Datadog query,"As I know, it is possible to something like that by using tags but unfortunately, there are no any metrics with the tag &quot;key&quot;."
Datadog,29697464,nan,1,"2015/04/17, 14:05:35",True,"2015/07/28, 19:24:53",nan,99834.0,135806.0,2,1689,How to get intranet uptime (availability reports) from datadog or newrelic?,We are using DataDog and NewRelic to monitor the performance of few DevOps supported systems and we need to provide some uptime reports like:
Datadog,29697464,nan,1,"2015/04/17, 14:05:35",True,"2015/07/28, 19:24:53",nan,99834.0,135806.0,2,1689,How to get intranet uptime (availability reports) from datadog or newrelic?,While we do have URL monitoring configured on DataDog we were not able to find a way to compute the uptime (only to get an alert when the service is down).
Datadog,29697464,nan,1,"2015/04/17, 14:05:35",True,"2015/07/28, 19:24:53",nan,99834.0,135806.0,2,1689,How to get intranet uptime (availability reports) from datadog or newrelic?,"NewRelic is also used but it seems that they have an URL monitoring service which works only on publicly accessible sites, making it useless for 9/10 cases."
Datadog,29949571,nan,1,"2015/04/29, 19:34:08",True,"2015/05/01, 23:26:19","2015/04/29, 19:34:50",2166607.0,63.0,2,908,Datadog dogstream custom parser error,"I wanted to add my custom log parser through dogstream, but there was an exception while restarting datadog agent:"
Datadog,29949571,nan,1,"2015/04/29, 19:34:08",True,"2015/05/01, 23:26:19","2015/04/29, 19:34:50",2166607.0,63.0,2,908,Datadog dogstream custom parser error,The parser code:
Datadog,29949571,nan,1,"2015/04/29, 19:34:08",True,"2015/05/01, 23:26:19","2015/04/29, 19:34:50",2166607.0,63.0,2,908,Datadog dogstream custom parser error,Does anybody know why such thing happend?
Datadog,29949571,nan,1,"2015/04/29, 19:34:08",True,"2015/05/01, 23:26:19","2015/04/29, 19:34:50",2166607.0,63.0,2,908,Datadog dogstream custom parser error,Any ideas?
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,We are trying to integrate DataDog with our Ruby On Rails app.
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,"Our ROR app will continuously add users, update users and delete users every second."
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,I have integrated Datadog to monitor the no.
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,"of users added, updated and deleted through the graph provided by Datadog."
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,I installed the datadog agent using the command for Ubuntu Aws instance.
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,I got a free trial for 14 days.
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,I followed this document for  dogstatd-ruby gem  :  https://github.com/DataDog/dogstatsd-ruby
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,After that i wrote the code in my ruby project like below :
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,"Here i dont see ""custom.users.updated"" and ""custom.users.added"" graph in the metrics explorer."
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,I would really appreciate if any1 help me out to set the graph for these 2 metrics in Datadog account.
Datadog,34893203,nan,1,"2016/01/20, 08:44:51",True,"2016/01/20, 21:54:55",nan,2039486.0,207.0,2,3228,DataDog custom metrics not showing up graph using ROR,please let me know if i missed anything here.
Datadog,41142456,41144060.0,2,"2016/12/14, 14:17:33",True,"2017/08/06, 11:00:57","2017/09/22, 21:01:22",3226384.0,548.0,2,395,cassandra datadog dashboard monitoring,Is there any default dashboard to monitor Cassandra performance in data dog?
Datadog,41142456,41144060.0,2,"2016/12/14, 14:17:33",True,"2017/08/06, 11:00:57","2017/09/22, 21:01:22",3226384.0,548.0,2,395,cassandra datadog dashboard monitoring,https://app.datadoghq.com/account/settings#integrations/cassandra
Datadog,41142456,41144060.0,2,"2016/12/14, 14:17:33",True,"2017/08/06, 11:00:57","2017/09/22, 21:01:22",3226384.0,548.0,2,395,cassandra datadog dashboard monitoring,There are lot of metrics listed.
Datadog,41142456,41144060.0,2,"2016/12/14, 14:17:33",True,"2017/08/06, 11:00:57","2017/09/22, 21:01:22",3226384.0,548.0,2,395,cassandra datadog dashboard monitoring,How do we construct a monitor?
Datadog,41142456,41144060.0,2,"2016/12/14, 14:17:33",True,"2017/08/06, 11:00:57","2017/09/22, 21:01:22",3226384.0,548.0,2,395,cassandra datadog dashboard monitoring,"By default the data dog shows the default system level monitor like CPU, Heap etc... is there anything like it for Cassandra?"
Datadog,41142456,41144060.0,2,"2016/12/14, 14:17:33",True,"2017/08/06, 11:00:57","2017/09/22, 21:01:22",3226384.0,548.0,2,395,cassandra datadog dashboard monitoring,Any info would be a great help for me.
Datadog,41992681,41995839.0,1,"2017/02/02, 03:08:23",True,"2017/02/02, 08:48:29",nan,834045.0,21931.0,2,1139,How to manually create a datadog event metric,"I'm creating my dashboard, and I have the following two metrics:  event.sent  and  event.failed ."
Datadog,41992681,41995839.0,1,"2017/02/02, 03:08:23",True,"2017/02/02, 08:48:29",nan,834045.0,21931.0,2,1139,How to manually create a datadog event metric,"Fortunately, I still haven't had any failed events (knock on wood), so this metric does not exist yet on datadog."
Datadog,41992681,41995839.0,1,"2017/02/02, 03:08:23",True,"2017/02/02, 08:48:29",nan,834045.0,21931.0,2,1139,How to manually create a datadog event metric,But I want to create it so I can add it to my monitors.
Datadog,41992681,41995839.0,1,"2017/02/02, 03:08:23",True,"2017/02/02, 08:48:29",nan,834045.0,21931.0,2,1139,How to manually create a datadog event metric,How do I manually create this metric?
Datadog,43146756,nan,1,"2017/03/31, 20:46:17",False,"2017/04/29, 21:21:56",nan,1993650.0,599.0,2,554,Datadog unit of time for monitor time aggregation,I've been trying to understand the time aggregation for Datadog monitoring alerts.
Datadog,43146756,nan,1,"2017/03/31, 20:46:17",False,"2017/04/29, 21:21:56",nan,1993650.0,599.0,2,554,Datadog unit of time for monitor time aggregation,"The official doc  http://docs.datadoghq.com/guides/monitors/#define-the-conditions 
I understand the idea of time aggregation, but I'm confused about the unit of time as it's not mentioned anywhere."
Datadog,43146756,nan,1,"2017/03/31, 20:46:17",False,"2017/04/29, 21:21:56",nan,1993650.0,599.0,2,554,Datadog unit of time for monitor time aggregation,Is it aggregating over 1 minute intervals?
Datadog,43146756,nan,1,"2017/03/31, 20:46:17",False,"2017/04/29, 21:21:56",nan,1993650.0,599.0,2,554,Datadog unit of time for monitor time aggregation,To rephrase this when I use  sum(last_30m){X}  is it summing the values of  X  for each minute?
Datadog,43146756,nan,1,"2017/03/31, 20:46:17",False,"2017/04/29, 21:21:56",nan,1993650.0,599.0,2,554,Datadog unit of time for monitor time aggregation,What about  sum(last_1h){X} ?
Datadog,43146756,nan,1,"2017/03/31, 20:46:17",False,"2017/04/29, 21:21:56",nan,1993650.0,599.0,2,554,Datadog unit of time for monitor time aggregation,Is it still each minute?
Datadog,43224591,nan,1,"2017/04/05, 10:28:22",True,"2017/04/29, 21:05:40",nan,3570213.0,455.0,2,1072,How to add data in Datadog to create custom dashboard?,I am new to Datadog APM.
Datadog,43224591,nan,1,"2017/04/05, 10:28:22",True,"2017/04/29, 21:05:40",nan,3570213.0,455.0,2,1072,How to add data in Datadog to create custom dashboard?,I have read few tutorials but I am unable to find how to to add data in Datadog to create custom dashboard?
Datadog,47577288,nan,1,"2017/11/30, 17:45:50",False,"2018/08/18, 20:06:12",nan,3148138.0,318.0,2,1140,Monitoring Dataflow pipelines with Datadog,I'm looking for a solution to monitor GCP Dataflow pipelines with Datadog to extract the built in metrics as well as Beam custom metrics.
Datadog,47577288,nan,1,"2017/11/30, 17:45:50",False,"2018/08/18, 20:06:12",nan,3148138.0,318.0,2,1140,Monitoring Dataflow pipelines with Datadog,"Currently Datadog offers integration for other GCP services, but not for Dataflow."
Datadog,47577288,nan,1,"2017/11/30, 17:45:50",False,"2018/08/18, 20:06:12",nan,3148138.0,318.0,2,1140,Monitoring Dataflow pipelines with Datadog,Has anyone done similar work and can share pointers how to build this as custom solution?
Datadog,49690040,49867829.0,1,"2018/04/06, 12:52:09",True,"2018/04/17, 02:53:52","2018/04/10, 09:40:50",4014295.0,360.0,2,112,Datadog Java APIs for alerting,I am new to Datadog and I am trying to implement mute/unmute functions from Datadog on my AWS cloud stack.
Datadog,49690040,49867829.0,1,"2018/04/06, 12:52:09",True,"2018/04/17, 02:53:52","2018/04/10, 09:40:50",4014295.0,360.0,2,112,Datadog Java APIs for alerting,I want to do that using AWS Lambda Functions.
Datadog,49690040,49867829.0,1,"2018/04/06, 12:52:09",True,"2018/04/17, 02:53:52","2018/04/10, 09:40:50",4014295.0,360.0,2,112,Datadog Java APIs for alerting,I am looking for a java based solution.
Datadog,49690040,49867829.0,1,"2018/04/06, 12:52:09",True,"2018/04/17, 02:53:52","2018/04/10, 09:40:50",4014295.0,360.0,2,112,Datadog Java APIs for alerting,Is there any Java based sdk provided for the same?
Datadog,49690040,49867829.0,1,"2018/04/06, 12:52:09",True,"2018/04/17, 02:53:52","2018/04/10, 09:40:50",4014295.0,360.0,2,112,Datadog Java APIs for alerting,I found out that Datadog provides APIs to schedule downtime  here
Datadog,49690040,49867829.0,1,"2018/04/06, 12:52:09",True,"2018/04/17, 02:53:52","2018/04/10, 09:40:50",4014295.0,360.0,2,112,Datadog Java APIs for alerting,"but the support I can see is either Python, Ruby or Curl."
Datadog,49690040,49867829.0,1,"2018/04/06, 12:52:09",True,"2018/04/17, 02:53:52","2018/04/10, 09:40:50",4014295.0,360.0,2,112,Datadog Java APIs for alerting,How can I construct a Java based solution for it?
Datadog,49699969,nan,1,"2018/04/06, 22:39:55",False,"2018/04/07, 13:26:10","2018/04/06, 22:44:25",2854739.0,61.0,2,2883,Datadog: ERROR:ddtrace.writer:cannot send services to localhost:8126: [Errno 111] Connection refused,My application is running in the docker container and it is not able to communicate with dd-trace agent running on host which is ec2
Datadog,49699969,nan,1,"2018/04/06, 22:39:55",False,"2018/04/07, 13:26:10","2018/04/06, 22:44:25",2854739.0,61.0,2,2883,Datadog: ERROR:ddtrace.writer:cannot send services to localhost:8126: [Errno 111] Connection refused,I've done all the configurations and still facing  ERROR:ddtrace.writer:cannot send spans to localhost:8126: [Errno 111] Connection refused
Datadog,49699969,nan,1,"2018/04/06, 22:39:55",False,"2018/04/07, 13:26:10","2018/04/06, 22:44:25",2854739.0,61.0,2,2883,Datadog: ERROR:ddtrace.writer:cannot send services to localhost:8126: [Errno 111] Connection refused,Any idea how to fix this?
Datadog,51124961,nan,2,"2018/07/01, 19:13:47",True,"2018/07/03, 04:00:56",nan,9699047.0,71.0,2,2804,Send data to Datadog using Go,"i'm collect data using Go and want to visualize it, i chose Datadog, but didn't find examples or live projects where Go used for sending metrics to Datadog."
Datadog,51124961,nan,2,"2018/07/01, 19:13:47",True,"2018/07/03, 04:00:56",nan,9699047.0,71.0,2,2804,Send data to Datadog using Go,But in offical site says that Go is supported.
Datadog,51975736,52615071.0,1,"2018/08/23, 00:58:14",True,"2018/10/04, 04:20:38",nan,4386440.0,899.0,2,1737,How to send jmeter test results to datadog?,"I wanted to ask if anyone has ever saved jmeter test results (sampler names, duration, pass/fail) to Datadog?"
Datadog,51975736,52615071.0,1,"2018/08/23, 00:58:14",True,"2018/10/04, 04:20:38",nan,4386440.0,899.0,2,1737,How to send jmeter test results to datadog?,Kinda like the backend listener for influx/graphite... but for Datadog.
Datadog,51975736,52615071.0,1,"2018/08/23, 00:58:14",True,"2018/10/04, 04:20:38",nan,4386440.0,899.0,2,1737,How to send jmeter test results to datadog?,Jmeter-plugins has no such plugin.
Datadog,51975736,52615071.0,1,"2018/08/23, 00:58:14",True,"2018/10/04, 04:20:38",nan,4386440.0,899.0,2,1737,How to send jmeter test results to datadog?,"Datadog seems to offer something called ""JMX integration"" but I'm not sure whether that is what I need."
Datadog,52828258,nan,2,"2018/10/16, 08:06:27",True,"2020/04/18, 02:32:29",nan,5571493.0,31.0,2,529,does else exist on datadog is_match,"I am trying to set up slack monitors with datadog, based on the environment."
Datadog,52828258,nan,2,"2018/10/16, 08:06:27",True,"2020/04/18, 02:32:29",nan,5571493.0,31.0,2,529,does else exist on datadog is_match,For e.g.
Datadog,52828258,nan,2,"2018/10/16, 08:06:27",True,"2020/04/18, 02:32:29",nan,5571493.0,31.0,2,529,does else exist on datadog is_match,if the environment is production got to slack channel A and if it is uat go to slack channel B and all other environments should go to slack channel C.
Datadog,52828258,nan,2,"2018/10/16, 08:06:27",True,"2020/04/18, 02:32:29",nan,5571493.0,31.0,2,529,does else exist on datadog is_match,But I can't find a way to do the last part where all others should go to slack channel B.
Datadog,52828258,nan,2,"2018/10/16, 08:06:27",True,"2020/04/18, 02:32:29",nan,5571493.0,31.0,2,529,does else exist on datadog is_match,Looked at the documentation in  https://docs.datadoghq.com/monitors/notifications  and googled but couldn't find anything that can do an else condition.
Datadog,54508636,nan,1,"2019/02/04, 01:29:07",False,"2020/03/17, 11:20:42",nan,3335579.0,645.0,2,236,Datadog event using micrometer,I am utilizing dogstatsd approach to send metrics to datadog using micrometer.
Datadog,54508636,nan,1,"2019/02/04, 01:29:07",False,"2020/03/17, 11:20:42",nan,3335579.0,645.0,2,236,Datadog event using micrometer,I get the normal metrics like counter and gauge but I am not able to generate events.
Datadog,54508636,nan,1,"2019/02/04, 01:29:07",False,"2020/03/17, 11:20:42",nan,3335579.0,645.0,2,236,Datadog event using micrometer,Is there a way to generate datadog events?
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,I'm trying to send events to my local datadog agent by shell through DataStatsD port.
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,The message is sent without errors but doesn't reach the dashboard.
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,I use datadog agent in version 6.9 and use datadog documentation:
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,https://docs.datadoghq.com/developers/dogstatsd/datagram_shell/#send-metrics-and-events-using-dogstatsd-and-the-shell
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,When I try to send metrics is work fine and I see the metrics in the datadog dashboard but when I send events it's doesn't show in the dashboard.
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,I also see that when I send event via shell and then check agent status the number of metrics packets go up but the number of events is still 0.
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,That's the command i run:
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,Edit  When I changed the following configuration properties it's work.
Datadog,54960747,nan,0,"2019/03/02, 18:47:49",False,"2019/03/04, 09:32:41","2019/03/04, 09:32:41",6552837.0,41.0,2,649,send event to local datadog agent via shell,"The configuration I changed:
         1) dogstatsd_non_local_traffic: yes
         2) bind_host: localhost"
Datadog,55588976,55661412.0,1,"2019/04/09, 12:04:50",True,"2019/04/13, 05:13:40","2019/04/09, 12:30:14",9563249.0,35.0,2,964,Forwarding the containers stdout logs to datadog without datadog agents,We re trying to eliminate Datadog agents from our infrastructure.
Datadog,55588976,55661412.0,1,"2019/04/09, 12:04:50",True,"2019/04/13, 05:13:40","2019/04/09, 12:30:14",9563249.0,35.0,2,964,Forwarding the containers stdout logs to datadog without datadog agents,I am trying to find a solution to forward the containers standard output logs to be visualised on datadog but without the agents and without changing the dockerfiles because there are hundreds of them.
Datadog,55588976,55661412.0,1,"2019/04/09, 12:04:50",True,"2019/04/13, 05:13:40","2019/04/09, 12:30:14",9563249.0,35.0,2,964,Forwarding the containers stdout logs to datadog without datadog agents,I was thinking about trying to centralize the logs with rsyslog but I dont know if its a good idea.
Datadog,55588976,55661412.0,1,"2019/04/09, 12:04:50",True,"2019/04/13, 05:13:40","2019/04/09, 12:30:14",9563249.0,35.0,2,964,Forwarding the containers stdout logs to datadog without datadog agents,Any suggestions ?
Datadog,58414654,58421772.0,1,"2019/10/16, 16:34:39",True,"2019/10/17, 00:11:59",nan,5421539.0,22408.0,2,577,How can I monitor AWS cloudwatch customized metric on datadog?,I am using datadog for monitoring my services on AWS.
Datadog,58414654,58421772.0,1,"2019/10/16, 16:34:39",True,"2019/10/17, 00:11:59",nan,5421539.0,22408.0,2,577,How can I monitor AWS cloudwatch customized metric on datadog?,"On my python application, I use below code to send a data to a metric on Cloudwatch:"
Datadog,58414654,58421772.0,1,"2019/10/16, 16:34:39",True,"2019/10/17, 00:11:59",nan,5421539.0,22408.0,2,577,How can I monitor AWS cloudwatch customized metric on datadog?,I can see this data on Cloudwatch -  Metric.
Datadog,58414654,58421772.0,1,"2019/10/16, 16:34:39",True,"2019/10/17, 00:11:59",nan,5421539.0,22408.0,2,577,How can I monitor AWS cloudwatch customized metric on datadog?,But I don't know how I can create a monitor on Datadog to listen on this metric.
Datadog,58505214,nan,2,"2019/10/22, 16:17:32",True,"2020/05/21, 10:55:37",nan,2194119.0,2120.0,2,2302,How to Add DataDog trace ID in Logs using Spring Boot + Logback,"OK, I spent quiet some time figuring out how to configure stuff to have DataDog trace ID in logs but couldn't get it working."
Datadog,58505214,nan,2,"2019/10/22, 16:17:32",True,"2020/05/21, 10:55:37",nan,2194119.0,2120.0,2,2302,How to Add DataDog trace ID in Logs using Spring Boot + Logback,"To be clear what I'm looking for is to see trace IDs in logs message, the same way that adding  spring-cloud-starter-sleuth  to the classpath, automatically configure Slf4j/Logback to show trace IDs in log messages."
Datadog,58505214,nan,2,"2019/10/22, 16:17:32",True,"2020/05/21, 10:55:37",nan,2194119.0,2120.0,2,2302,How to Add DataDog trace ID in Logs using Spring Boot + Logback,Where I've started:
Datadog,58505214,nan,2,"2019/10/22, 16:17:32",True,"2020/05/21, 10:55:37",nan,2194119.0,2120.0,2,2302,How to Add DataDog trace ID in Logs using Spring Boot + Logback,What I did so far:
Datadog,58505214,nan,2,"2019/10/22, 16:17:32",True,"2020/05/21, 10:55:37",nan,2194119.0,2120.0,2,2302,How to Add DataDog trace ID in Logs using Spring Boot + Logback,Notes:
Datadog,58505214,nan,2,"2019/10/22, 16:17:32",True,"2020/05/21, 10:55:37",nan,2194119.0,2120.0,2,2302,How to Add DataDog trace ID in Logs using Spring Boot + Logback,Can someone tell me how exactly I need to configure the application to see trace IDs in log messages?
Datadog,58505214,nan,2,"2019/10/22, 16:17:32",True,"2020/05/21, 10:55:37",nan,2194119.0,2120.0,2,2302,How to Add DataDog trace ID in Logs using Spring Boot + Logback,Is there any documentation or samples I can look at?
Datadog,58933286,nan,0,"2019/11/19, 13:40:32",False,"2019/11/19, 13:40:32",nan,4033879.0,579.0,2,219,Can I extract Datadog metrics info via Datadog REST API?,Is there any way to extract the Tags info from DataDog via API for a specific metric?
Datadog,58933286,nan,0,"2019/11/19, 13:40:32",False,"2019/11/19, 13:40:32",nan,4033879.0,579.0,2,219,Can I extract Datadog metrics info via Datadog REST API?,"I need the same info that the  Metrics Explorer  displays (list of hosts and tags), for only one metric."
Datadog,58933286,nan,0,"2019/11/19, 13:40:32",False,"2019/11/19, 13:40:32",nan,4033879.0,579.0,2,219,Can I extract Datadog metrics info via Datadog REST API?,I can retrieve the Tags filtered with a regular expression pattern:
Datadog,58933286,nan,0,"2019/11/19, 13:40:32",False,"2019/11/19, 13:40:32",nan,4033879.0,579.0,2,219,Can I extract Datadog metrics info via Datadog REST API?,And I can get all the Hosts from a Metric with:
Datadog,58933286,nan,0,"2019/11/19, 13:40:32",False,"2019/11/19, 13:40:32",nan,4033879.0,579.0,2,219,Can I extract Datadog metrics info via Datadog REST API?,"Using either the  datadogpy  or the  DataDog API ,  how can I get all the Tags from a Metric?"
Datadog,58933286,nan,0,"2019/11/19, 13:40:32",False,"2019/11/19, 13:40:32",nan,4033879.0,579.0,2,219,Can I extract Datadog metrics info via Datadog REST API?,Thanks
Datadog,61769846,61783745.0,1,"2020/05/13, 11:32:22",True,"2020/05/13, 23:10:16",nan,5314903.0,3536.0,2,193,Is there a serverless kubernetes datadog agent?,I have a unique type of Kubernetes cluster that cannot install the  Kubernetes Datadog agent .
Datadog,61769846,61783745.0,1,"2020/05/13, 11:32:22",True,"2020/05/13, 23:10:16",nan,5314903.0,3536.0,2,193,Is there a serverless kubernetes datadog agent?,I would like to collect the logs of individual docker containers in my Kubernetes pods similar to how the  Docker agent  works.
Datadog,61769846,61783745.0,1,"2020/05/13, 11:32:22",True,"2020/05/13, 23:10:16",nan,5314903.0,3536.0,2,193,Is there a serverless kubernetes datadog agent?,I am currently collecting docker logs from Kubernetes and then using a script with the  Datadog custom log forwarder  to upload them to Datadog.
Datadog,61769846,61783745.0,1,"2020/05/13, 11:32:22",True,"2020/05/13, 23:10:16",nan,5314903.0,3536.0,2,193,Is there a serverless kubernetes datadog agent?,I was curious if there is a better way to achieve this serverless collection of docker logs from Kubernetes clusters in datadog?
Datadog,61769846,61783745.0,1,"2020/05/13, 11:32:22",True,"2020/05/13, 23:10:16",nan,5314903.0,3536.0,2,193,Is there a serverless kubernetes datadog agent?,The ideal situation I want is to plug my kubeconfig somewhere and then let Datadog take care of the rest without deploying anything onto my Kubernetes cluster.
Datadog,61769846,61783745.0,1,"2020/05/13, 11:32:22",True,"2020/05/13, 23:10:16",nan,5314903.0,3536.0,2,193,Is there a serverless kubernetes datadog agent?,Is there an option for that outside of creating a custom script?
Datadog,62092243,62096791.0,1,"2020/05/29, 21:10:52",True,"2020/05/30, 04:32:35",nan,7827582.0,119.0,2,1580,Datadog Grok Parsing - extracting fields from nested JSON,Is it possible to extract json fields that are nested inside a log?
Datadog,62092243,62096791.0,1,"2020/05/29, 21:10:52",True,"2020/05/30, 04:32:35",nan,7827582.0,119.0,2,1580,Datadog Grok Parsing - extracting fields from nested JSON,Sample I've been work on:
Datadog,62092243,62096791.0,1,"2020/05/29, 21:10:52",True,"2020/05/30, 04:32:35",nan,7827582.0,119.0,2,1580,Datadog Grok Parsing - extracting fields from nested JSON,what I wanted to achieve was:
Datadog,62092243,62096791.0,1,"2020/05/29, 21:10:52",True,"2020/05/30, 04:32:35",nan,7827582.0,119.0,2,1580,Datadog Grok Parsing - extracting fields from nested JSON,"I tried to combine a sample regex ( ""(extract)""\s*:\s*""([^""]+)"",? )"
Datadog,62092243,62096791.0,1,"2020/05/29, 21:10:52",True,"2020/05/30, 04:32:35",nan,7827582.0,119.0,2,1580,Datadog Grok Parsing - extracting fields from nested JSON,"with  example_parser %{data::json}  (using the JSON as a log sample data, for starters) but I haven't managed to get anything working."
Datadog,62092243,62096791.0,1,"2020/05/29, 21:10:52",True,"2020/05/30, 04:32:35",nan,7827582.0,119.0,2,1580,Datadog Grok Parsing - extracting fields from nested JSON,Thanks in advance!
Datadog,62549173,nan,1,"2020/06/24, 09:41:37",True,"2020/06/25, 05:28:15","2020/06/25, 05:28:15",11195889.0,31.0,2,590,Logging application logs in DataDog,"Using datadog official docs, I am able to print the K8s  stdout/stderr  logs in DataDog UI, my motive is to print the app logs which are generated by spring boot application at a certain location in my pod."
Datadog,62549173,nan,1,"2020/06/24, 09:41:37",True,"2020/06/25, 05:28:15","2020/06/25, 05:28:15",11195889.0,31.0,2,590,Logging application logs in DataDog,Configurations done in cluster :
Datadog,62549173,nan,1,"2020/06/24, 09:41:37",True,"2020/06/25, 05:28:15","2020/06/25, 05:28:15",11195889.0,31.0,2,590,Logging application logs in DataDog,Configurations done in App :
Datadog,62549173,nan,1,"2020/06/24, 09:41:37",True,"2020/06/25, 05:28:15","2020/06/25, 05:28:15",11195889.0,31.0,2,590,Logging application logs in DataDog,After doing above configurations I am able to log  stdout/stderr  logs where as I wanted to log application logs in datadog UI
Datadog,62549173,nan,1,"2020/06/24, 09:41:37",True,"2020/06/25, 05:28:15","2020/06/25, 05:28:15",11195889.0,31.0,2,590,Logging application logs in DataDog,If someone has done this please let me know what am I missing here.
Datadog,62549173,nan,1,"2020/06/24, 09:41:37",True,"2020/06/25, 05:28:15","2020/06/25, 05:28:15",11195889.0,31.0,2,590,Logging application logs in DataDog,"If required, I can share the configurations as well."
Datadog,62549173,nan,1,"2020/06/24, 09:41:37",True,"2020/06/25, 05:28:15","2020/06/25, 05:28:15",11195889.0,31.0,2,590,Logging application logs in DataDog,Thanks in advance
Datadog,63650568,nan,2,"2020/08/29, 21:39:31",True,"2021/04/01, 21:13:00",nan,155963.0,1217.0,2,1146,How to search Datadog logs by Attribute,Question about searching logs in Datadog.
Datadog,63650568,nan,2,"2020/08/29, 21:39:31",True,"2021/04/01, 21:13:00",nan,155963.0,1217.0,2,1146,How to search Datadog logs by Attribute,Search works on regular strings in the CONTENT portion of the log.
Datadog,63650568,nan,2,"2020/08/29, 21:39:31",True,"2021/04/01, 21:13:00",nan,155963.0,1217.0,2,1146,How to search Datadog logs by Attribute,"However, if JSON is passed to the CONTENT portion, the JSON elements are automatically parsed into Attributes."
Datadog,63650568,nan,2,"2020/08/29, 21:39:31",True,"2021/04/01, 21:13:00",nan,155963.0,1217.0,2,1146,How to search Datadog logs by Attribute,But the Attributes are NOT searchable.
Datadog,63650568,nan,2,"2020/08/29, 21:39:31",True,"2021/04/01, 21:13:00",nan,155963.0,1217.0,2,1146,How to search Datadog logs by Attribute,How do I search for logs by Attribute?
Datadog,63650568,nan,2,"2020/08/29, 21:39:31",True,"2021/04/01, 21:13:00",nan,155963.0,1217.0,2,1146,How to search Datadog logs by Attribute,"It seems like a step backwards to supply log data in JSON to improve indexing, but then LOSE the ability to search on those elements."
Datadog,64126426,nan,1,"2020/09/29, 22:33:23",False,"2021/02/22, 00:44:46",nan,598805.0,2176.0,2,82,Log ActiveJob APM Traces with DataDog,my configuration is:
Datadog,64126426,nan,1,"2020/09/29, 22:33:23",False,"2021/02/22, 00:44:46",nan,598805.0,2176.0,2,82,Log ActiveJob APM Traces with DataDog,Where:
Datadog,64126426,nan,1,"2020/09/29, 22:33:23",False,"2021/02/22, 00:44:46",nan,598805.0,2176.0,2,82,Log ActiveJob APM Traces with DataDog,"I'm not getting any errors, but I  am  seeing the subclasses of  ApplicationWorker  in the DataDog dashboard for the APM traces but  not  the  ApplicationJob  subclasses."
Datadog,64126426,nan,1,"2020/09/29, 22:33:23",False,"2021/02/22, 00:44:46",nan,598805.0,2176.0,2,82,Log ActiveJob APM Traces with DataDog,"From the documentation I've found this is the correct way to configure  activeJob  tracing when it uses  resque , but I haven't found such great documentation on the subject."
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),I'm trying to figure out the difference between the  in-application modifier   as_rate()  and the  rollup function   per_second() .
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),"I want a table with two columns: the left column shows the total number of events submitted to a  Distribution  (in query-speak:  count:METRIC{*} by {tag} ), and the right column shows the average rate of events per second."
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),"The table visualization applies a sum rollup on left column, and an average rollup on the right column, so that the left column should equal the right column multiplied by the total number of seconds in the selected time period."
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),From reading the docs I expected either of these queries to work for the right column:
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),count:DISTRIBUTION_METRIC{*} by {tag}.as_rate()
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),per_second(count:DISTRIBUTION_METRIC{*} by {tag})
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),"But, it turns out that these two queries are not the same."
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),as_rate()  is the only one that finds the expected average rate where  left = right * num_seconds .
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),"In fact, the  per_second()  rollup does this extra weird thing where metrics with lower total events have higher average rates."
Datadog,64992563,nan,0,"2020/11/24, 20:21:01",False,"2020/11/24, 20:21:01",nan,6248782.0,21.0,2,141,Datadog: METRIC.as_rate() vs. per_second(METRIC),Is someone able to clarify why these two functions are not synonymous and what  per_second()  does differently?
Datadog,65202464,nan,0,"2020/12/08, 18:04:51",False,"2020/12/08, 18:04:51",nan,1660655.0,984.0,2,142,Can Heroku Postgres dynos talk with Datadog?,I have a Postgres dyno on Heroku and I use Datadog.
Datadog,65202464,nan,0,"2020/12/08, 18:04:51",False,"2020/12/08, 18:04:51",nan,1660655.0,984.0,2,142,Can Heroku Postgres dynos talk with Datadog?,Two postgres dashboards are by default on Datadog: Metrics and Overview.
Datadog,65202464,nan,0,"2020/12/08, 18:04:51",False,"2020/12/08, 18:04:51",nan,1660655.0,984.0,2,142,Can Heroku Postgres dynos talk with Datadog?,"Metrics is working (CPU usage, memory, I/O,...) but Overview is not (deadlocks, indexes usages)"
Datadog,65202464,nan,0,"2020/12/08, 18:04:51",False,"2020/12/08, 18:04:51",nan,1660655.0,984.0,2,142,Can Heroku Postgres dynos talk with Datadog?,Are Heroku Postgres dyno and Datadog fully compatible?
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,I'm trying to send my ECS Fargate logs to Datadog.
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,To do this I need to pass my Datadog API_KEY as a field in the  logConfiguration  object.
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,I need to secure my API_KEY so I am using AWS Secrets Manager via the  secretOptions  key of the  logConfiguration  object.
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,I'm following the steps from AWS laid out  here .
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,The full steps from the Datadog site can be found  here
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,For some reason I dont see the logs show up in datadog.
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,Here is the log config section of my Terraform code under the  container_definitions  object of the  aws_ecs_task_definition  resource:
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,"If I take out the  secretOptions  and add the apikey in plaintext, the logs show up on the datadog console:"
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,I of course cant just send my API_KEY in plaintext.
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,Does the  secretOptions  just not work for Datadog?
Datadog,65586123,nan,0,"2021/01/05, 22:30:06",False,"2021/01/05, 23:02:44","2021/01/05, 23:02:44",9220093.0,127.0,2,72,Fargate container_definition field &quot;secretOptions&quot; not passsing datadog API_KEY to logConfiguration,Any help is appreciated.
Datadog,37188932,nan,1,"2016/05/12, 16:49:10",True,"2016/06/13, 19:53:12",nan,2135526.0,4123.0,1,86,Delay in Datadog reporting the go_expvar,I have  datadog agent  running on ubuntu 14.04.
Datadog,37188932,nan,1,"2016/05/12, 16:49:10",True,"2016/06/13, 19:53:12",nan,2135526.0,4123.0,1,86,Delay in Datadog reporting the go_expvar,And I am trying to monitor page views for the go apps as mentioned in the  link .
Datadog,37188932,nan,1,"2016/05/12, 16:49:10",True,"2016/06/13, 19:53:12",nan,2135526.0,4123.0,1,86,Delay in Datadog reporting the go_expvar,I have checked everything yaml also it is valid.
Datadog,37188932,nan,1,"2016/05/12, 16:49:10",True,"2016/06/13, 19:53:12",nan,2135526.0,4123.0,1,86,Delay in Datadog reporting the go_expvar,But still it doesn't even report for upto go_expvar data even after 30 min.
Datadog,37188932,nan,1,"2016/05/12, 16:49:10",True,"2016/06/13, 19:53:12",nan,2135526.0,4123.0,1,86,Delay in Datadog reporting the go_expvar,I checked in the dashboard it says Last check 29 mins ago.
Datadog,37188932,nan,1,"2016/05/12, 16:49:10",True,"2016/06/13, 19:53:12",nan,2135526.0,4123.0,1,86,Delay in Datadog reporting the go_expvar,Can anyone tell me how to debug this or reduce this time
Datadog,38746266,nan,1,"2016/08/03, 17:19:56",True,"2016/08/03, 23:23:06",nan,1052504.0,5717.0,1,708,Converting Datadog &quot;M%&quot; CPU Unity to Kubernetes cpu unity &quot;m&quot;,"I have to set resource limits for my kubernetes apps, and they use the ""milicore"" unity ""m""."
Datadog,38746266,nan,1,"2016/08/03, 17:19:56",True,"2016/08/03, 23:23:06",nan,1052504.0,5717.0,1,708,Converting Datadog &quot;M%&quot; CPU Unity to Kubernetes cpu unity &quot;m&quot;,"When analyzing my apps in Datadog, I see a unity called M% for CPU usage."
Datadog,38746266,nan,1,"2016/08/03, 17:19:56",True,"2016/08/03, 23:23:06",nan,1052504.0,5717.0,1,708,Converting Datadog &quot;M%&quot; CPU Unity to Kubernetes cpu unity &quot;m&quot;,How do I convert 1.5M% to m?
Datadog,38746266,nan,1,"2016/08/03, 17:19:56",True,"2016/08/03, 23:23:06",nan,1052504.0,5717.0,1,708,Converting Datadog &quot;M%&quot; CPU Unity to Kubernetes cpu unity &quot;m&quot;,Kubernetes resources:  http://kubernetes.io/docs/user-guide/compute-resources/
Datadog,39750699,nan,1,"2016/09/28, 17:40:20",False,"2016/09/28, 18:05:43",nan,491682.0,9917.0,1,477,DataDog api - sending a stack trace,Im replacing our existing  NewRelic  java support code with  DataDog  and am wondering about sending error messages.
Datadog,39750699,nan,1,"2016/09/28, 17:40:20",False,"2016/09/28, 18:05:43",nan,491682.0,9917.0,1,477,DataDog api - sending a stack trace,NewRelic has the  .noticeEvent()  call.
Datadog,39750699,nan,1,"2016/09/28, 17:40:20",False,"2016/09/28, 18:05:43",nan,491682.0,9917.0,1,477,DataDog api - sending a stack trace,The  DDog library  Im using has a  .recordEvent()  but doesn't seem to have a way to send a stack trace.
Datadog,39750699,nan,1,"2016/09/28, 17:40:20",False,"2016/09/28, 18:05:43",nan,491682.0,9917.0,1,477,DataDog api - sending a stack trace,Anyone been down this road before?
Datadog,39750699,nan,1,"2016/09/28, 17:40:20",False,"2016/09/28, 18:05:43",nan,491682.0,9917.0,1,477,DataDog api - sending a stack trace,I can send text via the above but I need a bit more info.
Datadog,41964906,41968804.0,1,"2017/01/31, 20:29:09",True,"2017/02/01, 00:39:07",nan,4254517.0,32372.0,1,1223,Creating a datadog dashboard using REST API,I am trying to create a dashboard in datadog using the REST API described here:  http://docs.datadoghq.com/api/#timeboards
Datadog,41964906,41968804.0,1,"2017/01/31, 20:29:09",True,"2017/02/01, 00:39:07",nan,4254517.0,32372.0,1,1223,Creating a datadog dashboard using REST API,"Whatever I do, however, I keep getting a 400 response back with a message ""Invalid JSON input""."
Datadog,41964906,41968804.0,1,"2017/01/31, 20:29:09",True,"2017/02/01, 00:39:07",nan,4254517.0,32372.0,1,1223,Creating a datadog dashboard using REST API,"I have simplified my json to just a few required fields, and empty ""graphs"" section, and that still doesn't work."
Datadog,41964906,41968804.0,1,"2017/01/31, 20:29:09",True,"2017/02/01, 00:39:07",nan,4254517.0,32372.0,1,1223,Creating a datadog dashboard using REST API,Does anyone have an idea what could be wrong here?
Datadog,41964906,41968804.0,1,"2017/01/31, 20:29:09",True,"2017/02/01, 00:39:07",nan,4254517.0,32372.0,1,1223,Creating a datadog dashboard using REST API,"curl -i -X POST 'https://app.datadoghq.com/api/v1/dash?api_key=&lt;key&gt;&amp;application_key=&lt;the_key&gt;' -d '{""dash"":{""title"":""Foo"",""description"":""bar"",""graphs"":[]}}'"
Datadog,41964906,41968804.0,1,"2017/01/31, 20:29:09",True,"2017/02/01, 00:39:07",nan,4254517.0,32372.0,1,1223,Creating a datadog dashboard using REST API,Response
Datadog,42815252,42839492.0,1,"2017/03/15, 18:16:02",True,"2017/03/16, 18:35:39",nan,4985080.0,123.0,1,625,php curl for datadog api,"i am trying to write a php curl for datadog api,but it return internal error."
Datadog,42815252,42839492.0,1,"2017/03/15, 18:16:02",True,"2017/03/16, 18:35:39",nan,4985080.0,123.0,1,625,php curl for datadog api,this was working in bash script but throwing error while converting in phpcurl.
Datadog,42815252,42839492.0,1,"2017/03/15, 18:16:02",True,"2017/03/16, 18:35:39",nan,4985080.0,123.0,1,625,php curl for datadog api,can someone help me on this.
Datadog,44155361,44193846.0,2,"2017/05/24, 13:11:41",True,"2017/06/07, 10:26:03",nan,4279006.0,2644.0,1,1127,Datadog count metric dropping with multiple containers,I am incrementing a Datadog counter in python:
Datadog,44155361,44193846.0,2,"2017/05/24, 13:11:41",True,"2017/06/07, 10:26:03",nan,4279006.0,2644.0,1,1127,Datadog count metric dropping with multiple containers,"And have set the metric type to ""count"" and the unit to ""requests per none"" in the metadata for the metric."
Datadog,44155361,44193846.0,2,"2017/05/24, 13:11:41",True,"2017/06/07, 10:26:03",nan,4279006.0,2644.0,1,1127,Datadog count metric dropping with multiple containers,The code runs in a docker container on a kubernetes node in a Container Engine in Google Cloud...
Datadog,44155361,44193846.0,2,"2017/05/24, 13:11:41",True,"2017/06/07, 10:26:03",nan,4279006.0,2644.0,1,1127,Datadog count metric dropping with multiple containers,I have docker-dd-agent ( https://github.com/DataDog/docker-dd-agent ) running on each node.
Datadog,44155361,44193846.0,2,"2017/05/24, 13:11:41",True,"2017/06/07, 10:26:03",nan,4279006.0,2644.0,1,1127,Datadog count metric dropping with multiple containers,I can move the container to any node and it logs around 200 requests per minute.
Datadog,44155361,44193846.0,2,"2017/05/24, 13:11:41",True,"2017/06/07, 10:26:03",nan,4279006.0,2644.0,1,1127,Datadog count metric dropping with multiple containers,"But as soon as I scale it up and launch a second container, it only logs around 100 requests per minute."
Datadog,44155361,44193846.0,2,"2017/05/24, 13:11:41",True,"2017/06/07, 10:26:03",nan,4279006.0,2644.0,1,1127,Datadog count metric dropping with multiple containers,"If I scale down to one container again, it spikes to 200 rpm again:"
Datadog,44155361,44193846.0,2,"2017/05/24, 13:11:41",True,"2017/06/07, 10:26:03",nan,4279006.0,2644.0,1,1127,Datadog count metric dropping with multiple containers,What could be causing the requests to drop or get overwritten from other pods?
Datadog,44378111,nan,0,"2017/06/06, 00:39:32",False,"2017/06/06, 01:25:07","2017/06/06, 01:25:07",712543.0,2206.0,1,74,Does the Datadog API support a &quot;partial&quot; update?,"I'm looking for a way to do a ""partial"" update on an existing screenboard/timeboard."
Datadog,44378111,nan,0,"2017/06/06, 00:39:32",False,"2017/06/06, 01:25:07","2017/06/06, 01:25:07",712543.0,2206.0,1,74,Does the Datadog API support a &quot;partial&quot; update?,"By ""partial"", I mean adding some widget to the existing screenboard/timeboard without wiping out the existing widget that already exist."
Datadog,44378111,nan,0,"2017/06/06, 00:39:32",False,"2017/06/06, 01:25:07","2017/06/06, 01:25:07",712543.0,2206.0,1,74,Does the Datadog API support a &quot;partial&quot; update?,Consider the following example:
Datadog,44378111,nan,0,"2017/06/06, 00:39:32",False,"2017/06/06, 01:25:07","2017/06/06, 01:25:07",712543.0,2206.0,1,74,Does the Datadog API support a &quot;partial&quot; update?,Create screenboard:
Datadog,44378111,nan,0,"2017/06/06, 00:39:32",False,"2017/06/06, 01:25:07","2017/06/06, 01:25:07",712543.0,2206.0,1,74,Does the Datadog API support a &quot;partial&quot; update?,Update screenboard:
Datadog,44378111,nan,0,"2017/06/06, 00:39:32",False,"2017/06/06, 01:25:07","2017/06/06, 01:25:07",712543.0,2206.0,1,74,Does the Datadog API support a &quot;partial&quot; update?,"When I run the create and then the update example, the  update_image  widget will overwrite the  create_image  widget and that is the problem I'm trying to avoid."
Datadog,44996259,nan,1,"2017/07/09, 15:11:50",False,"2017/07/10, 17:09:28","2017/07/09, 15:13:09",3793853.0,173.0,1,300,Datadog - Lambda integration,I have a  AWS Lambda  function which filters AWS log events from Cloud-trail and give only my AWS ROLE's events.
Datadog,44996259,nan,1,"2017/07/09, 15:11:50",False,"2017/07/10, 17:09:28","2017/07/09, 15:13:09",3793853.0,173.0,1,300,Datadog - Lambda integration,Can I send this records only to Data-dog?
Datadog,44996259,nan,1,"2017/07/09, 15:11:50",False,"2017/07/10, 17:09:28","2017/07/09, 15:13:09",3793853.0,173.0,1,300,Datadog - Lambda integration,Is there an API in which I can pass this filtered events directly?
Datadog,46122135,46122641.0,1,"2017/09/08, 20:55:30",True,"2017/09/10, 05:40:02","2017/09/10, 05:40:02",3294286.0,1269.0,1,43,How does Datadog measure DAUs?,Where does the  userstats.o1.daus  metric take the data from?
Datadog,46122135,46122641.0,1,"2017/09/08, 20:55:30",True,"2017/09/10, 05:40:02","2017/09/10, 05:40:02",3294286.0,1269.0,1,43,How does Datadog measure DAUs?,"I looked in the metrics list and in the app, but I don't seem to find the source of the metric."
Datadog,46122135,46122641.0,1,"2017/09/08, 20:55:30",True,"2017/09/10, 05:40:02","2017/09/10, 05:40:02",3294286.0,1269.0,1,43,How does Datadog measure DAUs?,The application infrastructure relies on:
Datadog,46300932,49818303.0,1,"2017/09/19, 15:49:48",True,"2018/04/13, 16:56:36",nan,2236401.0,8778.0,1,2336,Getting a true 95th percentile in DataDog,"I have an application that publishes a metric to DataDog with multiple tags, and my DataDog agent has a line that looks like"
Datadog,46300932,49818303.0,1,"2017/09/19, 15:49:48",True,"2018/04/13, 16:56:36",nan,2236401.0,8778.0,1,2336,Getting a true 95th percentile in DataDog,So my metric (lets call it  ResponseTime ) has a metric in the DataDog viewer for each of those (i.e.
Datadog,46300932,49818303.0,1,"2017/09/19, 15:49:48",True,"2018/04/13, 16:56:36",nan,2236401.0,8778.0,1,2336,Getting a true 95th percentile in DataDog,ResponseTime.90perentile ).
Datadog,46300932,49818303.0,1,"2017/09/19, 15:49:48",True,"2018/04/13, 16:56:36",nan,2236401.0,8778.0,1,2336,Getting a true 95th percentile in DataDog,However if you look at this metric carefully it appears to be calculating these percentiles on a short range (not sure what) and for each tuple of the tags that exist.
Datadog,46300932,49818303.0,1,"2017/09/19, 15:49:48",True,"2018/04/13, 16:56:36",nan,2236401.0,8778.0,1,2336,Getting a true 95th percentile in DataDog,Ideally what I'd like to get is a 95th percentile of the  ResponseTime  metric over all the tags (maybe I filter it down by 1 or 2 and have a couple of different graphs) but over the last week or so.
Datadog,46300932,49818303.0,1,"2017/09/19, 15:49:48",True,"2018/04/13, 16:56:36",nan,2236401.0,8778.0,1,2336,Getting a true 95th percentile in DataDog,Is there an easy way to do this?
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,"I have configured DD agent on AWS Ubuntu machine and defined CPU Usage, RAM monitors, and metric is correctly reflecting in the dashboard."
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,Inside  /etc/dd-agent/conf.d  in file  process.yaml :
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,"On the same machine, I have a JAR running as a process with name  ecommerce-order-0.0.1-SNAPSHOT.jar  as a process."
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,When I do:
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,I get:
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,But when I do:
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,I get:
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,I want a process monitor who can check if a  JAR  with some name is currently running or not.
Datadog,46826873,46827281.0,1,"2017/10/19, 12:33:48",True,"2017/10/19, 14:14:51",nan,3913366.0,2207.0,1,1026,DataDog agent cannot find JAR process with matching name,What is it I am doing wrong?
Datadog,47142664,47394149.0,2,"2017/11/06, 19:37:11",True,"2019/03/01, 18:33:14","2019/02/11, 23:53:38",796181.0,2193.0,1,298,Does Datadog support runtime code instrumentation?,I am looking at some alternatives for APM and I like the extensive list of Datadog integration points.
Datadog,47142664,47394149.0,2,"2017/11/06, 19:37:11",True,"2019/03/01, 18:33:14","2019/02/11, 23:53:38",796181.0,2193.0,1,298,Does Datadog support runtime code instrumentation?,"However, it seems that I would have to make code changes to explicitly send stats to Datadog."
Datadog,47142664,47394149.0,2,"2017/11/06, 19:37:11",True,"2019/03/01, 18:33:14","2019/02/11, 23:53:38",796181.0,2193.0,1,298,Does Datadog support runtime code instrumentation?,Doesn't Datadog support runtime instrumentation?
Datadog,47142664,47394149.0,2,"2017/11/06, 19:37:11",True,"2019/03/01, 18:33:14","2019/02/11, 23:53:38",796181.0,2193.0,1,298,Does Datadog support runtime code instrumentation?,My tech stack is MS .NET/C# and SQL Server backend.
Datadog,47142664,47394149.0,2,"2017/11/06, 19:37:11",True,"2019/03/01, 18:33:14","2019/02/11, 23:53:38",796181.0,2193.0,1,298,Does Datadog support runtime code instrumentation?,Thanks!
Datadog,47739117,47769427.0,1,"2017/12/10, 15:03:12",True,"2017/12/12, 11:41:56",nan,5156990.0,2084.0,1,152,"Datadog stop gets Jenkins events, restart solve the issue","we use Jenkins 2.60.1 , with Datadog plugin 0.6.1"
Datadog,47739117,47769427.0,1,"2017/12/10, 15:03:12",True,"2017/12/12, 11:41:56",nan,5156990.0,2084.0,1,152,"Datadog stop gets Jenkins events, restart solve the issue","after first installation all works well , but after some time we stop get events in Datadog."
Datadog,47739117,47769427.0,1,"2017/12/10, 15:03:12",True,"2017/12/12, 11:41:56",nan,5156990.0,2084.0,1,152,"Datadog stop gets Jenkins events, restart solve the issue",restart Jenkins solve the issue.
Datadog,47739117,47769427.0,1,"2017/12/10, 15:03:12",True,"2017/12/12, 11:41:56",nan,5156990.0,2084.0,1,152,"Datadog stop gets Jenkins events, restart solve the issue",any idea ?
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,I am new to Datadog and NGiNX.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,I noticed when I was creating a monitor for some integrations several of the integrations were labeled as misconfigured.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,My guess is someone clicked the install button but did finish the remaining integration steps.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,I started to work with NGiNX and quickly hit a roadblock.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,I verified it is running http status module
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,"The NGiNX install is under a different directory than is usual
and the configuration file is under"
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,I created the status.conf file there.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,When I reload the NGINX I get a failure.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,I don't understand what it means or how to proceed from here.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,There is a logs directory with nothing in it.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,ps -ef|grep nginx
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,I think the issue is that our install doesn't seem to be following the same defaults as the instructions and I'm pretty sure I'm not doing this correctly.
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,If anyone has any insights that would be great!
Datadog,50567698,nan,0,"2018/05/28, 16:35:08",False,"2018/05/28, 16:35:08",nan,9854584.0,37.0,1,268,Datadog integration with NGiNX,Chris
Datadog,50768038,nan,1,"2018/06/08, 23:33:55",False,"2018/06/08, 23:48:11",nan,842302.0,2771.0,1,260,Datadog: Notify about every new log-item with error-level,Is it possible to configure datadog to notify about each new error that got logged?
Datadog,50768038,nan,1,"2018/06/08, 23:33:55",False,"2018/06/08, 23:48:11",nan,842302.0,2771.0,1,260,Datadog: Notify about every new log-item with error-level,I know how to set a threshold for a specified period and how to send the error-count for instance to slack.
Datadog,50768038,nan,1,"2018/06/08, 23:33:55",False,"2018/06/08, 23:48:11",nan,842302.0,2771.0,1,260,Datadog: Notify about every new log-item with error-level,But I am searching for a possibility to send the actual error rather than the number of errors.
Datadog,51033962,nan,1,"2018/06/26, 05:10:21",False,"2018/06/26, 17:52:41","2018/06/26, 17:44:28",9933041.0,631.0,1,1581,How to correctly configure Datadog with Slack?,"I am currently using this  link  for writing a program in Python that will send out curl commands for  POST  ,  PUT , and  DELETE  requests using the Datadog API."
Datadog,51033962,nan,1,"2018/06/26, 05:10:21",False,"2018/06/26, 17:52:41","2018/06/26, 17:44:28",9933041.0,631.0,1,1581,How to correctly configure Datadog with Slack?,"So far, the request seems to be firing as I'd like it to, but it won't take my credentials."
Datadog,51033962,nan,1,"2018/06/26, 05:10:21",False,"2018/06/26, 17:52:41","2018/06/26, 17:44:28",9933041.0,631.0,1,1581,How to correctly configure Datadog with Slack?,"I'm not entirely sure what a service hook url is, but I believe it may be the culprit."
Datadog,51033962,nan,1,"2018/06/26, 05:10:21",False,"2018/06/26, 17:52:41","2018/06/26, 17:44:28",9933041.0,631.0,1,1581,How to correctly configure Datadog with Slack?,Could anyone tell me how to find the following Slack specific elements for this?
Datadog,51033962,nan,1,"2018/06/26, 05:10:21",False,"2018/06/26, 17:52:41","2018/06/26, 17:44:28",9933041.0,631.0,1,1581,How to correctly configure Datadog with Slack?,This is my test script in Python:
Datadog,51033962,nan,1,"2018/06/26, 05:10:21",False,"2018/06/26, 17:52:41","2018/06/26, 17:44:28",9933041.0,631.0,1,1581,How to correctly configure Datadog with Slack?,The results were:
Datadog,51033962,nan,1,"2018/06/26, 05:10:21",False,"2018/06/26, 17:52:41","2018/06/26, 17:44:28",9933041.0,631.0,1,1581,How to correctly configure Datadog with Slack?,I would really appreciate any help in finding this information!
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,"Using datadog docker image, with the following in docker-compos"
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,I am getting the following errors continuously
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,"2018-07-14 16:10:04 UTC | ERROR | (runner.go:277 in work) | Error running check disk: [{""message"": ""[Errno 2] No such file or directory: '/host/proc/filesystems'"", ""traceback"": ""Traceback (most recent call last):\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/checks/base.py\"", line 294, in run\n    self.check(copy.deepcopy(self.instances[0]))\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/disk/disk.py\"", line 43, in check\n    self.collect_metrics_psutil()\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/disk/disk.py\"", line 90, in collect_metrics_psutil\n    for part in psutil.disk_partitions(all=True):\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/ init .py\"", line 1839, in disk_partitions\n    return _psplatform.disk_partitions(all)\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/_pslinux.py\"", line 1000, in disk_partitions\n    with open_text(\""%s/filesystems\"" % get_procfs_path()) as f:\n  File \""/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/_pslinux.py\"", line 194, in open_text\n    return open(fname, \""rt\"", **kwargs)\nIOError: [Errno 2] No such file or directory: '/host/proc/filesystems'\n""}]"
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,and another
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,"2018-07-14 16:10:04 UTC | WARN | (cgroup.go:510 in
  parseCgroupMountPoints) | No mountPoints were detected, current cgroup
  root is: /host/sys/fs/cgroup/"
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,Any ideas what it means or how to debug it?
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,Expecting to get logs into datadog from other containers sysout so I have all logs in one place.
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,I can see it successfully detects the other containers
Datadog,51341426,nan,0,"2018/07/14, 19:43:41",False,"2018/07/14, 19:43:41",nan,1414721.0,2849.0,1,838,DataDog logging image reporting errors in docker logs,"Note the docker image is using version 6 of datadog
Thanks"
Datadog,52891518,nan,1,"2018/10/19, 14:31:11",True,"2018/11/14, 01:03:15","2018/10/20, 16:16:47",1555190.0,2037.0,1,1053,Datadog and kubernetes running check kubelet,"I set up datadog and kubernetes to test to out monitoring, although in datadog i can see some logs and metrics, in the agent in kubernetes I have the following errors:"
Datadog,52891518,nan,1,"2018/10/19, 14:31:11",True,"2018/11/14, 01:03:15","2018/10/20, 16:16:47",1555190.0,2037.0,1,1053,Datadog and kubernetes running check kubelet,"As the logs state the agent cannot connect to Kubectl, has anyone come across this?"
Datadog,52989371,nan,1,"2018/10/25, 15:32:48",True,"2018/10/28, 09:39:29","2018/10/25, 17:07:16",4742614.0,113.0,1,872,Datadog - monitor that alert you if the value of the metric does not change for three days,I am trying to add new monitor to datadog.
Datadog,52989371,nan,1,"2018/10/25, 15:32:48",True,"2018/10/28, 09:39:29","2018/10/25, 17:07:16",4742614.0,113.0,1,872,Datadog - monitor that alert you if the value of the metric does not change for three days,I added the metric to my code.
Datadog,52989371,nan,1,"2018/10/25, 15:32:48",True,"2018/10/28, 09:39:29","2018/10/25, 17:07:16",4742614.0,113.0,1,872,Datadog - monitor that alert you if the value of the metric does not change for three days,And I can see this metric on datadog (goto Metrics -  explorer -  Graph).
Datadog,52989371,nan,1,"2018/10/25, 15:32:48",True,"2018/10/28, 09:39:29","2018/10/25, 17:07:16",4742614.0,113.0,1,872,Datadog - monitor that alert you if the value of the metric does not change for three days,Now I am trying to create monitoring on datadog that will alert me if the value of metric don't change for three days in a row.
Datadog,52989371,nan,1,"2018/10/25, 15:32:48",True,"2018/10/28, 09:39:29","2018/10/25, 17:07:16",4742614.0,113.0,1,872,Datadog - monitor that alert you if the value of the metric does not change for three days,Is it possible to create this kind of monitoring?
Datadog,52989371,nan,1,"2018/10/25, 15:32:48",True,"2018/10/28, 09:39:29","2018/10/25, 17:07:16",4742614.0,113.0,1,872,Datadog - monitor that alert you if the value of the metric does not change for three days,Thanks.
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,"I have a microservice based project with  kafka , which I used for event bus."
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,"I have a business process, which contains multiple microservices."
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,Microservices asynchronous communicate with each other with help kafka.
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,Each instance of business process has unique  process_id .
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,Let's consider a example of some process:
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,"So, I need to measure execution time bwtween differ steps of this process."
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,"For example, I want to know, duration between steps  3  and  5 , or  2  and  6 ."
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,"So, I don't know, how to measure it."
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,I have only one workaround solution.
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,"I can have share memory (e.g., redis), where I can store points of time for each stem of process."
Datadog,54382680,nan,0,"2019/01/26, 22:56:48",False,"2019/01/26, 22:56:48",nan,4167563.0,1383.0,1,456,How to measure execution time of asynchronous process in Datadog?,"In the end of process, I can calculate all metrics and push it datadog."
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,I am using ansible version 2.7 for kubernetes deployment.
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,"For sending logs to datadog on kubernetes one of the way is to configure annotations like below,"
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,this works fine and I could see logs in DataDog.
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,However I would like to achieve above configuration via ansible deployment on kubernetes for which I have used below code
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,and datadog.json.j2 looks like below
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,However the resulting config on deployment is below
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,and this config does not allow datadog agent to parse logs failing with below error
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,if I use ansible code as below (using replace)
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,it generates deployment config as below
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,"Which also fails,"
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,"to configure the working config with ansible, I have to either remove leading pipe (|) or three quotes coming when using replace)."
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,I would like to have jinja variables substitution in place so that I could configure deployment with desired source and service at deployment time.
Datadog,54811591,nan,1,"2019/02/21, 18:14:22",True,"2019/02/21, 20:18:42","2019/02/21, 19:56:32",1958107.0,790.0,1,150,Issue in placing DataDog log annotation on deployment via ansible,kindly suggest
Datadog,55087322,nan,1,"2019/03/10, 13:47:34",False,"2019/03/14, 00:27:39","2019/03/10, 20:16:59",1398083.0,391.0,1,1519,DataDog docker agent doesn&#39;t recieve logs from application container,"I have Golang app, it writes logs to Stdout with Logrus."
Datadog,55087322,nan,1,"2019/03/10, 13:47:34",False,"2019/03/14, 00:27:39","2019/03/10, 20:16:59",1398083.0,391.0,1,1519,DataDog docker agent doesn&#39;t recieve logs from application container,"I was trying to recreate this  https://github.com/DataDog/docker-compose-example  scenario, and replace python app with my app."
Datadog,55087322,nan,1,"2019/03/10, 13:47:34",False,"2019/03/14, 00:27:39","2019/03/10, 20:16:59",1398083.0,391.0,1,1519,DataDog docker agent doesn&#39;t recieve logs from application container,"But logs aren't coming to Datadog dashboad
This is docker-compose I'm trying to make work"
Datadog,55087322,nan,1,"2019/03/10, 13:47:34",False,"2019/03/14, 00:27:39","2019/03/10, 20:16:59",1398083.0,391.0,1,1519,DataDog docker agent doesn&#39;t recieve logs from application container,"I also tried non-compose, but simple docker container installation for the agent by this  https://docs.datadoghq.com/logs/log_collection/docker/?tab=containerinstallation  instructions."
Datadog,55087322,nan,1,"2019/03/10, 13:47:34",False,"2019/03/14, 00:27:39","2019/03/10, 20:16:59",1398083.0,391.0,1,1519,DataDog docker agent doesn&#39;t recieve logs from application container,I run my golang app container with
Datadog,55087322,nan,1,"2019/03/10, 13:47:34",False,"2019/03/14, 00:27:39","2019/03/10, 20:16:59",1398083.0,391.0,1,1519,DataDog docker agent doesn&#39;t recieve logs from application container,with Dockerfile
Datadog,55087322,nan,1,"2019/03/10, 13:47:34",False,"2019/03/14, 00:27:39","2019/03/10, 20:16:59",1398083.0,391.0,1,1519,DataDog docker agent doesn&#39;t recieve logs from application container,and DD agent can see app container ups and downs but receive no logs
Datadog,55632833,55806500.0,2,"2019/04/11, 15:44:34",True,"2019/09/05, 15:28:28",nan,5254815.0,410.0,1,484,Access Denied while running datadog commands,I installed datadog agent locally on my windows 10  machine.
Datadog,55632833,55806500.0,2,"2019/04/11, 15:44:34",True,"2019/09/05, 15:28:28",nan,5254815.0,410.0,1,484,Access Denied while running datadog commands,By default it stored data in ProgramData folder in C drive.
Datadog,55632833,55806500.0,2,"2019/04/11, 15:44:34",True,"2019/09/05, 15:28:28",nan,5254815.0,410.0,1,484,Access Denied while running datadog commands,It does not give option to select different drive while installation.
Datadog,55632833,55806500.0,2,"2019/04/11, 15:44:34",True,"2019/09/05, 15:28:28",nan,5254815.0,410.0,1,484,Access Denied while running datadog commands,"Now when I run any command, it gives me below error."
Datadog,55632833,55806500.0,2,"2019/04/11, 15:44:34",True,"2019/09/05, 15:28:28",nan,5254815.0,410.0,1,484,Access Denied while running datadog commands,Can we edit permissions to it allow access to ProgramData folder.
Datadog,56097993,nan,0,"2019/05/12, 12:45:57",False,"2019/05/12, 13:25:57","2019/05/12, 13:25:57",672798.0,3398.0,1,318,Using template variable in Datadog SLO widget,I have one Datadig dashboard to monitor a particular service.
Datadog,56097993,nan,0,"2019/05/12, 12:45:57",False,"2019/05/12, 13:25:57","2019/05/12, 13:25:57",672798.0,3398.0,1,318,Using template variable in Datadog SLO widget,"To use the same dashboard for other services, I added a couple of template variables to change the queries in the dashboard."
Datadog,56097993,nan,0,"2019/05/12, 12:45:57",False,"2019/05/12, 13:25:57","2019/05/12, 13:25:57",672798.0,3398.0,1,318,Using template variable in Datadog SLO widget,"But, I could not use these variables in the query section of Datadog SLO widget."
Datadog,56097993,nan,0,"2019/05/12, 12:45:57",False,"2019/05/12, 13:25:57","2019/05/12, 13:25:57",672798.0,3398.0,1,318,Using template variable in Datadog SLO widget,"The following query with variables works in the other type of widgets, but not in the SLO widget."
Datadog,56097993,nan,0,"2019/05/12, 12:45:57",False,"2019/05/12, 13:25:57","2019/05/12, 13:25:57",672798.0,3398.0,1,318,Using template variable in Datadog SLO widget,"Is there a way to use variables in SLO widget too, or not possible because it is in beta version or something?"
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,I'm trying to record my website sales $ amount in datadog.
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,However I'm getting way more than the actual value.
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,I'm using java-dogstatsd client and spring.
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,My application is running on 3 hosts.
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,I recorded all metrics (using sendWebOrder method) but no luck.
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,I'm trying to generate a datadog toplist by transactiontype.
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,"I'm not getting the correct amount in any of the metrics (tried mainly count, gauge and histogram.sum)."
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,Here is my datadog config:
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,What am I missing?
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,Is this the correct way to record money value?
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,Do I've to do any rollup in config?
Datadog,56230928,nan,0,"2019/05/21, 07:06:13",False,"2019/05/21, 07:06:13",nan,2666282.0,313.0,1,137,Recording sales amount in datadog metrics,Any help is appreciated.
Datadog,56238847,nan,0,"2019/05/21, 15:57:41",False,"2019/05/21, 15:57:41",nan,785523.0,4123.0,1,94,Telegraf output plugin to submit metric to datadog agent,Is anyone aware if there is any telegraf output plugin to submit metric to datadog agent?
Datadog,56238847,nan,0,"2019/05/21, 15:57:41",False,"2019/05/21, 15:57:41",nan,785523.0,4123.0,1,94,Telegraf output plugin to submit metric to datadog agent,I can see a  datadog output plugin  which calls datadog metric api but not anything to submit data to datadog agent.
Datadog,56748943,nan,0,"2019/06/25, 10:31:33",False,"2019/06/25, 11:07:16","2019/06/25, 11:07:16",9952376.0,11.0,1,358,Per second rate values for a metric are coming different for prometheus and datadog,I am very new to the monitoring of microservices using prometheus and datadog.
Datadog,56748943,nan,0,"2019/06/25, 10:31:33",False,"2019/06/25, 11:07:16","2019/06/25, 11:07:16",9952376.0,11.0,1,358,Per second rate values for a metric are coming different for prometheus and datadog,"I am trying to monitor the rate of event callback requests per second using PromQL queries and datadog queries but when i compared the values from both the tools, they came out to be very different."
Datadog,56748943,nan,0,"2019/06/25, 10:31:33",False,"2019/06/25, 11:07:16","2019/06/25, 11:07:16",9952376.0,11.0,1,358,Per second rate values for a metric are coming different for prometheus and datadog,I want to know if the two values need to be same.
Datadog,56748943,nan,0,"2019/06/25, 10:31:33",False,"2019/06/25, 11:07:16","2019/06/25, 11:07:16",9952376.0,11.0,1,358,Per second rate values for a metric are coming different for prometheus and datadog,"If yes, then how do i write my queries to obtain the correct values"
Datadog,56748943,nan,0,"2019/06/25, 10:31:33",False,"2019/06/25, 11:07:16","2019/06/25, 11:07:16",9952376.0,11.0,1,358,Per second rate values for a metric are coming different for prometheus and datadog,PromQL query
Datadog,56748943,nan,0,"2019/06/25, 10:31:33",False,"2019/06/25, 11:07:16","2019/06/25, 11:07:16",9952376.0,11.0,1,358,Per second rate values for a metric are coming different for prometheus and datadog,Datadog query(json file)
Datadog,56748943,nan,0,"2019/06/25, 10:31:33",False,"2019/06/25, 11:07:16","2019/06/25, 11:07:16",9952376.0,11.0,1,358,Per second rate values for a metric are coming different for prometheus and datadog,Please tell me how can i make the two queries equivalent?
Datadog,58599000,nan,1,"2019/10/29, 00:05:38",False,"2019/12/19, 09:15:25",nan,7148186.0,57.0,1,79,Gcloud app deploy failing with mongo-driver because of DataDog/zstd,"I have been using mongo-driver in my project, deploying with gcloud app deploy for a while now."
Datadog,58599000,nan,1,"2019/10/29, 00:05:38",False,"2019/12/19, 09:15:25",nan,7148186.0,57.0,1,79,Gcloud app deploy failing with mongo-driver because of DataDog/zstd,"I recently rebuild my machine, and simply ran  go get  to get fetch ally my dependencies."
Datadog,58599000,nan,1,"2019/10/29, 00:05:38",False,"2019/12/19, 09:15:25",nan,7148186.0,57.0,1,79,Gcloud app deploy failing with mongo-driver because of DataDog/zstd,"Everything is compiling fine locally, however,  gcloud app deploy  fails:"
Datadog,58599000,nan,1,"2019/10/29, 00:05:38",False,"2019/12/19, 09:15:25",nan,7148186.0,57.0,1,79,Gcloud app deploy failing with mongo-driver because of DataDog/zstd,Any ideas?
Datadog,58599000,nan,1,"2019/10/29, 00:05:38",False,"2019/12/19, 09:15:25",nan,7148186.0,57.0,1,79,Gcloud app deploy failing with mongo-driver because of DataDog/zstd,app.yaml is just  runtime: go113
Datadog,59260837,nan,0,"2019/12/10, 07:34:31",False,"2019/12/10, 07:34:31",nan,1806481.0,1542.0,1,106,Datadog explore facets and measures,We have multiple applications sending logs to  Datadog  via  syslog .
Datadog,59260837,nan,0,"2019/12/10, 07:34:31",False,"2019/12/10, 07:34:31",nan,1806481.0,1542.0,1,106,Datadog explore facets and measures,Every team has created  facets  /  measures  for their respective applications under a particular group.
Datadog,59260837,nan,0,"2019/12/10, 07:34:31",False,"2019/12/10, 07:34:31",nan,1806481.0,1542.0,1,106,Datadog explore facets and measures,Is there a direct way to explore the list of  facets  /  measures  under a specific group?
Datadog,59260837,nan,0,"2019/12/10, 07:34:31",False,"2019/12/10, 07:34:31",nan,1806481.0,1542.0,1,106,Datadog explore facets and measures,I am trying to create a document for our support team to include the list of  facets  and  measures .
Datadog,59260837,nan,0,"2019/12/10, 07:34:31",False,"2019/12/10, 07:34:31",nan,1806481.0,1542.0,1,106,Datadog explore facets and measures,"I am able to view them in the Log Search page, but, cannot copy."
Datadog,59260837,nan,0,"2019/12/10, 07:34:31",False,"2019/12/10, 07:34:31",nan,1806481.0,1542.0,1,106,Datadog explore facets and measures,I am looking something as export the list of  facets / measures  to excel / csv.
Datadog,59260837,nan,0,"2019/12/10, 07:34:31",False,"2019/12/10, 07:34:31",nan,1806481.0,1542.0,1,106,Datadog explore facets and measures,Note: I am restricted to use datadog api.
Datadog,60115340,nan,0,"2020/02/07, 16:24:39",False,"2020/02/07, 16:24:39",nan,7890683.0,49.0,1,91,How to calculate max CPU / Mem usage with Datadog API (Python),I am trying to query the datadog server for some specific metrics ie.
Datadog,60115340,nan,0,"2020/02/07, 16:24:39",False,"2020/02/07, 16:24:39",nan,7890683.0,49.0,1,91,How to calculate max CPU / Mem usage with Datadog API (Python),"""max mem used"" over some period x and I'm doing the following:"
Datadog,60115340,nan,0,"2020/02/07, 16:24:39",False,"2020/02/07, 16:24:39",nan,7890683.0,49.0,1,91,How to calculate max CPU / Mem usage with Datadog API (Python),"I would have expect a single timestamped value to be returned, however I get datapoint for every minute like so:
 [
          1581084600000,
          1339840512
        ],
        [
          1581084660000,
          1339883520
        ],
        [
          1581084720000,
          1339740160
        ]"
Datadog,60115340,nan,0,"2020/02/07, 16:24:39",False,"2020/02/07, 16:24:39",nan,7890683.0,49.0,1,91,How to calculate max CPU / Mem usage with Datadog API (Python),"Is there a way to get a specific result, i.e."
Datadog,60115340,nan,0,"2020/02/07, 16:24:39",False,"2020/02/07, 16:24:39",nan,7890683.0,49.0,1,91,How to calculate max CPU / Mem usage with Datadog API (Python),the maximum out of all these results?
Datadog,60115340,nan,0,"2020/02/07, 16:24:39",False,"2020/02/07, 16:24:39",nan,7890683.0,49.0,1,91,How to calculate max CPU / Mem usage with Datadog API (Python),Thanks.
Datadog,61091978,nan,1,"2020/04/08, 04:29:31",True,"2020/04/14, 06:28:13","2020/04/14, 06:28:13",8686989.0,39.0,1,514,Quarkus logging with Datadog,"In Quarkus, the default logging library is JBoss and using the  quarkus-logging-json  allows you to encode your logs as JSON."
Datadog,61091978,nan,1,"2020/04/08, 04:29:31",True,"2020/04/14, 06:28:13","2020/04/14, 06:28:13",8686989.0,39.0,1,514,Quarkus logging with Datadog,"However, Datadog integration requires custom fields such as  service ,  dd.span_id  and  dd.trace_id  to have the logs associated with the correct syntax."
Datadog,61091978,nan,1,"2020/04/08, 04:29:31",True,"2020/04/14, 06:28:13","2020/04/14, 06:28:13",8686989.0,39.0,1,514,Quarkus logging with Datadog,"Currently, I've tried adding this in  application.properties :"
Datadog,61091978,nan,1,"2020/04/08, 04:29:31",True,"2020/04/14, 06:28:13","2020/04/14, 06:28:13",8686989.0,39.0,1,514,Quarkus logging with Datadog,"However, this seems not to show up in Datadog as expected."
Datadog,61091978,nan,1,"2020/04/08, 04:29:31",True,"2020/04/14, 06:28:13","2020/04/14, 06:28:13",8686989.0,39.0,1,514,Quarkus logging with Datadog,"When we use log4j2, we simply configure it like so."
Datadog,61091978,nan,1,"2020/04/08, 04:29:31",True,"2020/04/14, 06:28:13","2020/04/14, 06:28:13",8686989.0,39.0,1,514,Quarkus logging with Datadog,"Again, cannot find any documentation how to accomplish the same result in Quarkus configs."
Datadog,61091978,nan,1,"2020/04/08, 04:29:31",True,"2020/04/14, 06:28:13","2020/04/14, 06:28:13",8686989.0,39.0,1,514,Quarkus logging with Datadog,Does anyone know how I could inject these custom properties into the JSON logs with Quarkus or how to integrate it with Datadog properly?
Datadog,61092487,nan,1,"2020/04/08, 05:37:29",False,"2021/04/06, 06:12:46","2020/04/09, 00:51:28",13254852.0,11.0,1,40,Datadog: How to automate configuring Log Archives,How can I automate configuring Log Archives on GCP?
Datadog,61092487,nan,1,"2020/04/08, 05:37:29",False,"2021/04/06, 06:12:46","2020/04/09, 00:51:28",13254852.0,11.0,1,40,Datadog: How to automate configuring Log Archives,"I can do it manually by following steps
 https://docs.datadoghq.com/logs/archives/?tab=googlecloudstorage"
Datadog,61092487,nan,1,"2020/04/08, 05:37:29",False,"2021/04/06, 06:12:46","2020/04/09, 00:51:28",13254852.0,11.0,1,40,Datadog: How to automate configuring Log Archives,"I guess selenium can help this 
but I looking for a more programmatic way like Terraform or REST API"
Datadog,61092487,nan,1,"2020/04/08, 05:37:29",False,"2021/04/06, 06:12:46","2020/04/09, 00:51:28",13254852.0,11.0,1,40,Datadog: How to automate configuring Log Archives,Thank you.
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,I am working on to create some custom metrics for my spring boot 2 rest api.
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,I have added the required micro meter and datadog dependency.
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,My office machine works behind a proxy.
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,I have setup proxy through spring boot plugin.
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,below are in my application.properties file.
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,management.metrics.export.datadog.apiKey=mykey
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,But I am getting the socket connection timeout.
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,As far as I debugged the io.micrometer.core.ipc.http.HttpUrlConnectionSender.send method is failing and I dont under how the micro meter data dog takes the proxy details.
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,The micrometer doc says
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,But I dont understand what it means?
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,should I replace this url with my proxy url or is there any specific uri pattern with the proxy?
Datadog,61148607,61247522.0,1,"2020/04/10, 23:59:55",True,"2020/04/16, 13:00:50",nan,4850377.0,846.0,1,631,Spring boot micro meter datadog socket connection error,I am using spring boot 2.2.4.RELEASE
Datadog,62782910,nan,0,"2020/07/07, 22:52:50",False,"2020/07/08, 02:11:06","2020/07/08, 02:11:06",5962766.0,6264.0,1,237,How to calculate duration between logs in Datadog?,Splunk has  transaction  command which can produce  duration  between logs grouped by id:
Datadog,62782910,nan,0,"2020/07/07, 22:52:50",False,"2020/07/08, 02:11:06","2020/07/08, 02:11:06",5962766.0,6264.0,1,237,How to calculate duration between logs in Datadog?,as it is decribed on
Datadog,62782910,nan,0,"2020/07/07, 22:52:50",False,"2020/07/08, 02:11:06","2020/07/08, 02:11:06",5962766.0,6264.0,1,237,How to calculate duration between logs in Datadog?,How to calculate duration between events in Datadog?
Datadog,62787931,62798229.0,1,"2020/07/08, 08:08:21",True,"2020/07/08, 21:49:01","2020/07/08, 21:49:01",10898235.0,87.0,1,285,How do I set a separate message for warning vs alert in terraform for Datadog?,I'm setting up a monitor that looks like this:
Datadog,62787931,62798229.0,1,"2020/07/08, 08:08:21",True,"2020/07/08, 21:49:01","2020/07/08, 21:49:01",10898235.0,87.0,1,285,How do I set a separate message for warning vs alert in terraform for Datadog?,I've also got a widget that looks like this:
Datadog,62787931,62798229.0,1,"2020/07/08, 08:08:21",True,"2020/07/08, 21:49:01","2020/07/08, 21:49:01",10898235.0,87.0,1,285,How do I set a separate message for warning vs alert in terraform for Datadog?,"I'd like to define two different messages, one of which will be sent when the &quot;warning&quot; threshold is crossed, and the other which will be sent when the &quot;critical&quot; threshold is crossed."
Datadog,62787931,62798229.0,1,"2020/07/08, 08:08:21",True,"2020/07/08, 21:49:01","2020/07/08, 21:49:01",10898235.0,87.0,1,285,How do I set a separate message for warning vs alert in terraform for Datadog?,How can I do this?
Datadog,62787931,62798229.0,1,"2020/07/08, 08:08:21",True,"2020/07/08, 21:49:01","2020/07/08, 21:49:01",10898235.0,87.0,1,285,How do I set a separate message for warning vs alert in terraform for Datadog?,Is this correct?
Datadog,63859370,nan,1,"2020/09/12, 13:30:13",False,"2020/09/16, 01:09:03","2020/09/14, 18:47:52",12517930.0,31.0,1,129,datadog facet path with special symbols,"I have an index created on the log and the paths have special character :
 for example:"
Datadog,63859370,nan,1,"2020/09/12, 13:30:13",False,"2020/09/16, 01:09:03","2020/09/14, 18:47:52",12517930.0,31.0,1,129,datadog facet path with special symbols,Sample URL:
Datadog,63859370,nan,1,"2020/09/12, 13:30:13",False,"2020/09/16, 01:09:03","2020/09/14, 18:47:52",12517930.0,31.0,1,129,datadog facet path with special symbols,grok parser:
Datadog,63859370,nan,1,"2020/09/12, 13:30:13",False,"2020/09/16, 01:09:03","2020/09/14, 18:47:52",12517930.0,31.0,1,129,datadog facet path with special symbols,when I try to add facet for  @params.rs:orgId
Datadog,63859370,nan,1,"2020/09/12, 13:30:13",False,"2020/09/16, 01:09:03","2020/09/14, 18:47:52",12517930.0,31.0,1,129,datadog facet path with special symbols,I am getting error as
Datadog,63859370,nan,1,"2020/09/12, 13:30:13",False,"2020/09/16, 01:09:03","2020/09/14, 18:47:52",12517930.0,31.0,1,129,datadog facet path with special symbols,"An error occurred while saving the facet: The Facet path must contain
only letters, digits, or the characters - _ ."
Datadog,63859370,nan,1,"2020/09/12, 13:30:13",False,"2020/09/16, 01:09:03","2020/09/14, 18:47:52",12517930.0,31.0,1,129,datadog facet path with special symbols,@ $
Datadog,64395922,nan,1,"2020/10/16, 23:21:54",True,"2020/10/20, 18:05:44",nan,9053059.0,23.0,1,91,DataDog metric for Kubernetes PersistentVolume usage or remaining space,Is there a DataDog metric to report the space used or remaining in a GCP PersistentVolume.
Datadog,64395922,nan,1,"2020/10/16, 23:21:54",True,"2020/10/20, 18:05:44",nan,9053059.0,23.0,1,91,DataDog metric for Kubernetes PersistentVolume usage or remaining space,"I have found disk use metrics for the container itself, but not for a PersistentVolume."
Datadog,64395922,nan,1,"2020/10/16, 23:21:54",True,"2020/10/20, 18:05:44",nan,9053059.0,23.0,1,91,DataDog metric for Kubernetes PersistentVolume usage or remaining space,I am working in GoogleCloudPlatform.
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,Using:
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,I am deploying Datadog as a DaemonSet and with the cluster-agent enabled to a Kubernetes cluster using the instructions provided  here .
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,I'm configuring Datadog using the  values.yaml  file as specified.
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"I want to do some custom metrics, specifically using the integration formerly known as  postgres.yaml ."
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"I have tried to do this as specified in the  values.yaml  template found  here , like this (putting it in the cluster-agent, since these are cluster-wide metrics):"
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"As per the documentation, I can confirm that using the  |-  prefix this indeed creates a file in the path  /etc/datadog-agent/conf.d/postgres.yaml  on the node, where I would expect it to."
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"The file correctly has all the contents in the block, i.e."
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,starting with  init_config:...
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"Now, when starting the node I see this in the logs (DEBUG):"
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"'/conf.d/postgres.yaml' -&gt; '/etc/datadog-agent/conf.d/postgres.yaml'
/conf.d/..2020_10_22_10_22_27.239825358 -&gt;
/etc/datadog-agent/conf.d/..2020_10_22_10_22_27.239825358
'/conf.d/..2020_10_22_10_22_27.239825358/postgres.yaml' -&gt;
'/etc/datadog-agent/conf.d/..2020_10_22_10_22_27.239825358/postgres.yaml'"
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"2020-10-22 10:22:29 UTC | CLUSTER | DEBUG |
(pkg/autodiscovery/providers/file.go:196 in collectEntry) | Found
valid configuration in file: /etc/datadog-agent/conf.d/postgres.yaml"
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"2020-10-22 10:22:29 UTC | CLUSTER | DEBUG |
(pkg/collector/scheduler.go:154 in getChecks) | Unable to load a check
from instance of config 'postgres': Core Check Loader:  Check postgres
not found in Catalog"
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"2020-10-22 10:22:29 UTC | CLUSTER | ERROR |
(pkg/collector/scheduler.go:201 in GetChecksFromConfigs) |  Unable to
load the check: unable to load any check from config 'postgres'"
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"The documentation  here  states, that the postgres yaml-contents in agents v7.x should actually be in  /etc/datadog-agent/conf.d/postgres.d/conf.yaml  and not in  /etc/datadog-agent/conf.d/postgres.yaml ."
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,"It is not possible to create a subfolder / use forward slashes in the config key (internally, the file is created using ConfigMap)."
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,I'm not even sure if the problem is the yaml-file path or if a core integration is missing.
Datadog,64482767,nan,2,"2020/10/22, 15:58:40",True,"2020/10/22, 19:17:02","2020/10/22, 18:23:04",430885.0,11819.0,1,490,How do I include integration-metrics when deploying Datadog DaemonSet + cluster-agent using helm and values.yaml?,So my broad quest is: how do I enable Datadog postgres-integration correctly in my setup?
Datadog,65580606,nan,1,"2021/01/05, 16:16:24",True,"2021/01/05, 19:47:25",nan,180368.0,45878.0,1,79,How to get Serilog json-formatted logs to appear correctly in Datadog,"I have been asked to implement a centralized monitoring and logging system using DataDog that will receive information from various services and applications, some running as Windows Services on virtual machines and some running inside a Kubernetes cluster."
Datadog,65580606,nan,1,"2021/01/05, 16:16:24",True,"2021/01/05, 19:47:25",nan,180368.0,45878.0,1,79,How to get Serilog json-formatted logs to appear correctly in Datadog,"In order to implement the logging aspect so that DataDog can correctly ingest the logs, I'm using Serilog to do the logging."
Datadog,65580606,nan,1,"2021/01/05, 16:16:24",True,"2021/01/05, 19:47:25",nan,180368.0,45878.0,1,79,How to get Serilog json-formatted logs to appear correctly in Datadog,My plan is currently to write the logs to the console in json format and have the DataDog agent installed on each server or k8s node capture and ship them to DataDog.
Datadog,65580606,nan,1,"2021/01/05, 16:16:24",True,"2021/01/05, 19:47:25",nan,180368.0,45878.0,1,79,How to get Serilog json-formatted logs to appear correctly in Datadog,"This works, at least for the k8s node where I've implemented it so far."
Datadog,65580606,nan,1,"2021/01/05, 16:16:24",True,"2021/01/05, 19:47:25",nan,180368.0,45878.0,1,79,How to get Serilog json-formatted logs to appear correctly in Datadog,(I'm trying to avoid using the custom Serilog sink for DataDog as that's discouraged in the DataDog documentation).
Datadog,65580606,nan,1,"2021/01/05, 16:16:24",True,"2021/01/05, 19:47:25",nan,180368.0,45878.0,1,79,How to get Serilog json-formatted logs to appear correctly in Datadog,My problem is that I cannot get logs ingested correctly on the DataDog side.
Datadog,65580606,nan,1,"2021/01/05, 16:16:24",True,"2021/01/05, 19:47:25",nan,180368.0,45878.0,1,79,How to get Serilog json-formatted logs to appear correctly in Datadog,DataDog expects the json to contain a property call Message but Serilog names this property RenderedMessage (if I use JsonFormatter(renderMessage: true)) or @m (if I use RenderedCompactJsonFormatter()).
Datadog,65580606,nan,1,"2021/01/05, 16:16:24",True,"2021/01/05, 19:47:25",nan,180368.0,45878.0,1,79,How to get Serilog json-formatted logs to appear correctly in Datadog,How can I get my logs shipped to DataDog and ingested correctly on the DataDog end?
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,My current organization is migrating to DataDog for Application Performance Monitoring.
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,I am deploying a Python Flask web application using docker to Azure Container Registry.
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,After the deployment to Azure the app should be listed/available on Datadog portal.
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Please note I just started learning Docker containers.
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,There is a high chance I could do completely wrong.
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Please bear with me
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Steps followed
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Option 1: Create a docker container on local machine and push to ACR
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Added  dd-trace  python library to the docker image
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Added dd-trace run command the docker file
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,build the image
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,run the container on local
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Getting OSError: [Errno 99] Cannot assign requested address
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Option 2: Forward logs to Azure Blob Storage but a heavy process
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,"Option 3: using Serilog but, my organization does not want to use third party logging framework, we have our own logging framework"
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,"Any help is highly appreciated, I am looking for a solution using  Option 1 ."
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,"I went through the Microsoft articles, Datadog documentation but, no luck."
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,"I setup app registrations, Manage reader permissions on Subscription, created ClientID and app secrets on Azure portal."
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,none of them helped
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Could you confirm whether is there a way to collect the APM logs on datadog with out installing agent on Azure.
Datadog,66113635,nan,1,"2021/02/09, 07:46:36",True,"2021/02/19, 21:45:06","2021/02/09, 16:16:15",2032722.0,383.0,1,154,python flask application to track APM logs with Datadog on Azure Linux containers using ACR,Thank you in advance.
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,"I am little embarrased, as this is probably very banal, but it's really not making any sense to me."
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,"I have a little golang web-application, that I for now am just localhosting."
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,"I have some logging with logrus going on in the project, and want to use datadog to get a nice and visual dashboard for my application."
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,"So I follow a bunch of the steps on their page, download their dockerimage and run it, and i am able to see some data!"
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,"I then follow the instructions further on, under a headline called &quot;docker integration&quot;."
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,here I am asked to run the following command:
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,To which the response &quot;cannot find user &quot;dd-agent&quot;&quot; is given.
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,But what is this &quot;user&quot; value?
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,Is it the user to which I have signed up to datadog with?
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,something else?
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,I find it very unspecified?
Datadog,66833514,nan,0,"2021/03/27, 18:19:34",False,"2021/03/27, 18:19:34",nan,15245005.0,49.0,1,28,what is the &quot;user&quot; in datadog,would very much appreciate a helping hand
Datadog,66172172,nan,1,"2021/02/12, 14:46:36",False,"2021/02/23, 10:00:35",nan,7779815.0,871.0,1,94,How to send Confluent cloud metrics to Datadog?,I want to get Confluent cloud metrics into Datadog so I followed the this  instruction .
Datadog,66172172,nan,1,"2021/02/12, 14:46:36",False,"2021/02/23, 10:00:35",nan,7779815.0,871.0,1,94,How to send Confluent cloud metrics to Datadog?,Instead of using CCLOUD_USER: ${CCLOUD_USER} and CCLOUD_PASSWORD: ${CCLOUD_PASSWORD} I used CCLOUD_API_KEY and CCLOUD_API_SECRET as environment variables for the exporter container.
Datadog,66172172,nan,1,"2021/02/12, 14:46:36",False,"2021/02/23, 10:00:35",nan,7779815.0,871.0,1,94,How to send Confluent cloud metrics to Datadog?,I get a Failed to establish a new connection: [Errno 111] Connection refused error:
Datadog,66172172,nan,1,"2021/02/12, 14:46:36",False,"2021/02/23, 10:00:35",nan,7779815.0,871.0,1,94,How to send Confluent cloud metrics to Datadog?,When I tried to curl http://ccloudexporter_ccloud_exporter_1:2112/metrics I got no reply but I did with a curl to http://localhost:2112/metrics.
Datadog,66172172,nan,1,"2021/02/12, 14:46:36",False,"2021/02/23, 10:00:35",nan,7779815.0,871.0,1,94,How to send Confluent cloud metrics to Datadog?,So I adjusted the openmetrics.yml to use prometheus url http://localhost:2112/metrics.
Datadog,66172172,nan,1,"2021/02/12, 14:46:36",False,"2021/02/23, 10:00:35",nan,7779815.0,871.0,1,94,How to send Confluent cloud metrics to Datadog?,Still same error in the DD container.
Datadog,66172172,nan,1,"2021/02/12, 14:46:36",False,"2021/02/23, 10:00:35",nan,7779815.0,871.0,1,94,How to send Confluent cloud metrics to Datadog?,When I go to http://localhost:2112/metrics in my browser I see metrics.
Datadog,66172172,nan,1,"2021/02/12, 14:46:36",False,"2021/02/23, 10:00:35",nan,7779815.0,871.0,1,94,How to send Confluent cloud metrics to Datadog?,No clue on why DD cannot connect to /metrics.
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,Background:
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,I'm trying SLO feature from micrometer and I expect that I can get the number of requests that fulfill the SLO.
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,"For example if I set the SLO to 500ms, then I want to know how many requests &lt;= 500ms."
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,Also I want to know the total requests.
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,Problem:
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,"http.server.requests.count says 24
http.server.requests.histogram with tag le:_inf says 126"
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,I believe both of them should have the same (or at least similar) value
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,I'm using:
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,"Spring Boot 2.3.2.RELEASE
Micrometer: 1.5.2"
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,application.properties
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,Meter Filter
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,Datadog
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,Any clue on what's happening here?
Datadog,65368981,nan,0,"2020/12/19, 12:48:38",False,"2020/12/20, 01:18:54","2020/12/20, 01:18:54",462565.0,378.0,1,95,Micrometer wrong SLO histogram count with statsd datadog,Thanks
Datadog,65348502,nan,0,"2020/12/17, 23:19:53",False,"2020/12/17, 23:19:53",nan,14622925.0,11.0,1,54,How to monitor failed AWS EMR Steps using Datadog,I was reading the  Datadog docs  on how to monitor AWS Elastic Map Reduce using Datadog because I need to get metrics for failed EMR steps.
Datadog,65348502,nan,0,"2020/12/17, 23:19:53",False,"2020/12/17, 23:19:53",nan,14622925.0,11.0,1,54,How to monitor failed AWS EMR Steps using Datadog,LIKE HERE
Datadog,65348502,nan,0,"2020/12/17, 23:19:53",False,"2020/12/17, 23:19:53",nan,14622925.0,11.0,1,54,How to monitor failed AWS EMR Steps using Datadog,"I think the most accurate metric is  aws.elasticmapreduce.jobs_failed  but as the image says, is only available for Hadoop V1, but I'm using Hadoop V2... so I don't see it in my  Datadog Metric Explorer"
Datadog,64929616,64973799.0,1,"2020/11/20, 14:37:15",True,"2020/11/23, 20:08:08",nan,469898.0,1481.0,1,150,Can DataDog dd-trace-js send trace info to the server via http headers?,"I want to trace a request path that has started in the Web application React JS (frontend), then passed to the backend and returned as a response."
Datadog,64929616,64973799.0,1,"2020/11/20, 14:37:15",True,"2020/11/23, 20:08:08",nan,469898.0,1481.0,1,150,Can DataDog dd-trace-js send trace info to the server via http headers?,Can  dd-trace-js  start the span and pass it to the server over HTTP HEADERS?
Datadog,63814864,63835825.0,2,"2020/09/09, 18:28:38",True,"2020/09/10, 22:25:47",nan,8534030.0,205.0,1,716,"Micrometer-springboot: Hikaricp , Tomcat and jdbc metrics are not exported to DataDog","Hikaricp , Tomcat and jdbc metrics are not being exported to DataDog"
Datadog,63814864,63835825.0,2,"2020/09/09, 18:28:38",True,"2020/09/10, 22:25:47",nan,8534030.0,205.0,1,716,"Micrometer-springboot: Hikaricp , Tomcat and jdbc metrics are not exported to DataDog","we have setup springboot app to push the metrics to datadoghq, it does export 60 metrics, however the metrics like hikaricp, tomcat and jdbc are missing."
Datadog,63814864,63835825.0,2,"2020/09/09, 18:28:38",True,"2020/09/10, 22:25:47",nan,8534030.0,205.0,1,716,"Micrometer-springboot: Hikaricp , Tomcat and jdbc metrics are not exported to DataDog","hikaricp, tomcat and jdbc - these mertics are listed under  /actuator/metrics  endpoint, but not exported to datadog."
Datadog,63814864,63835825.0,2,"2020/09/09, 18:28:38",True,"2020/09/10, 22:25:47",nan,8534030.0,205.0,1,716,"Micrometer-springboot: Hikaricp , Tomcat and jdbc metrics are not exported to DataDog","Is there any additional settings required to push hikaricp, tomcat and jdbc metrics ?"
Datadog,63314162,65920537.0,1,"2020/08/08, 13:06:44",True,"2021/01/27, 15:50:52","2020/08/08, 20:16:31",834309.0,1182.0,1,217,How to define source-tag for datadog when sending logs agentless with logback from Spring Boot?,"How can logback be configured to add tags,so that datadog can recognize the source?"
Datadog,63314162,65920537.0,1,"2020/08/08, 13:06:44",True,"2021/01/27, 15:50:52","2020/08/08, 20:16:31",834309.0,1182.0,1,217,How to define source-tag for datadog when sending logs agentless with logback from Spring Boot?,I have the following  logback.xml :
Datadog,63314162,65920537.0,1,"2020/08/08, 13:06:44",True,"2021/01/27, 15:50:52","2020/08/08, 20:16:31",834309.0,1182.0,1,217,How to define source-tag for datadog when sending logs agentless with logback from Spring Boot?,Where the custom field  ddtags  is supposed to set tags for datadog.
Datadog,63314162,65920537.0,1,"2020/08/08, 13:06:44",True,"2021/01/27, 15:50:52","2020/08/08, 20:16:31",834309.0,1182.0,1,217,How to define source-tag for datadog when sending logs agentless with logback from Spring Boot?,"The logs show up in datadog and everything works as expected, despite the  source -tag."
Datadog,63314162,65920537.0,1,"2020/08/08, 13:06:44",True,"2021/01/27, 15:50:52","2020/08/08, 20:16:31",834309.0,1182.0,1,217,How to define source-tag for datadog when sending logs agentless with logback from Spring Boot?,The log messages sent from my service show up with two tags in datadog:  source:java  and  source:undefined :
Datadog,63314162,65920537.0,1,"2020/08/08, 13:06:44",True,"2021/01/27, 15:50:52","2020/08/08, 20:16:31",834309.0,1182.0,1,217,How to define source-tag for datadog when sending logs agentless with logback from Spring Boot?,How do I get rid of the  source:undefined  tag so that datadog correctly recognizes the source?
Datadog,63225377,nan,0,"2020/08/03, 10:28:54",False,"2020/08/08, 17:58:53","2020/08/08, 17:57:33",4386423.0,166.0,1,96,Can I add tags to parent span Datadog,I'm working with express + graphql environment.
Datadog,63225377,nan,0,"2020/08/03, 10:28:54",False,"2020/08/08, 17:58:53","2020/08/08, 17:57:33",4386423.0,166.0,1,96,Can I add tags to parent span Datadog,I want to add tags to express span with the value I derive while resolving the graphql query.
Datadog,63225377,nan,0,"2020/08/03, 10:28:54",False,"2020/08/08, 17:58:53","2020/08/08, 17:57:33",4386423.0,166.0,1,96,Can I add tags to parent span Datadog,"Currently, tags get added to the graphql span with the following code."
Datadog,63225377,nan,0,"2020/08/03, 10:28:54",False,"2020/08/08, 17:58:53","2020/08/08, 17:57:33",4386423.0,166.0,1,96,Can I add tags to parent span Datadog,Let me know if there is a way to add these tags to the parent span instead of the current span.
Datadog,63225377,nan,0,"2020/08/03, 10:28:54",False,"2020/08/08, 17:58:53","2020/08/08, 17:57:33",4386423.0,166.0,1,96,Can I add tags to parent span Datadog,This is required as I don't want to enable analytics on graphql since I already have it enabled in the express app.
Datadog,63225377,nan,0,"2020/08/03, 10:28:54",False,"2020/08/08, 17:58:53","2020/08/08, 17:57:33",4386423.0,166.0,1,96,Can I add tags to parent span Datadog,Basically I want tags to root trace(express) instead of current span(graphql.execute).
Datadog,62570190,nan,0,"2020/06/25, 10:37:07",False,"2020/06/25, 10:37:07",nan,2265497.0,1402.0,1,105,Datadog search by java stacktrace,"Anyway, how can I search for all messages(errors) where stacktrace contains specific piece of code?"
Datadog,62570190,nan,0,"2020/06/25, 10:37:07",False,"2020/06/25, 10:37:07",nan,2265497.0,1402.0,1,105,Datadog search by java stacktrace,According to datadog documentation it search only by message attribute(it's infered from the json-like object sent to datadog when you log something).
Datadog,62570190,nan,0,"2020/06/25, 10:37:07",False,"2020/06/25, 10:37:07",nan,2265497.0,1402.0,1,105,Datadog search by java stacktrace,Stacktrace is a separate property and I cannot understand how to search it.
Datadog,62545185,nan,1,"2020/06/24, 02:16:03",True,"2020/06/24, 02:43:20",nan,12822559.0,11.0,1,542,Creating Datadog alerts for when the percentage difference between two custom metrics goes over a specified percentage threshold,My current situation is that I have two different data feeds (Feed A &amp; Feed B) and I have created custom metrics for both feeds:
Datadog,62545185,nan,1,"2020/06/24, 02:16:03",True,"2020/06/24, 02:43:20",nan,12822559.0,11.0,1,542,Creating Datadog alerts for when the percentage difference between two custom metrics goes over a specified percentage threshold,Next steps is to create alert monitoring for the agreed upon threshold of difference between the two metrics.
Datadog,62545185,nan,1,"2020/06/24, 02:16:03",True,"2020/06/24, 02:43:20",nan,12822559.0,11.0,1,542,Creating Datadog alerts for when the percentage difference between two custom metrics goes over a specified percentage threshold,Say we have agreed that it is acceptable for Order Counts from Feed A to be within ~5% of Order Counts from Feed B.
Datadog,62545185,nan,1,"2020/06/24, 02:16:03",True,"2020/06/24, 02:43:20",nan,12822559.0,11.0,1,542,Creating Datadog alerts for when the percentage difference between two custom metrics goes over a specified percentage threshold,How can I go about creating that threshold and comparison between the two metrics that I have already developed in Datadog?
Datadog,62545185,nan,1,"2020/06/24, 02:16:03",True,"2020/06/24, 02:43:20",nan,12822559.0,11.0,1,542,Creating Datadog alerts for when the percentage difference between two custom metrics goes over a specified percentage threshold,I would like to send alerts to myself when the % difference between the two data feeds is &gt; 5 % for a daily validation.
Datadog,62394798,nan,0,"2020/06/15, 21:38:29",False,"2020/06/15, 21:38:29",nan,13751595.0,11.0,1,362,How Datadog read custom log files in containers kubernetes?,"I have an application running on Kubernetes and this app has log files that I want to stream to datadog log, then set up an alert."
Datadog,62394798,nan,0,"2020/06/15, 21:38:29",False,"2020/06/15, 21:38:29",nan,13751595.0,11.0,1,362,How Datadog read custom log files in containers kubernetes?,"Previously, this app run on bare-metal server, I installed datadog agent on that server, and I used custom log collection to retrieve that logs."
Datadog,62394798,nan,0,"2020/06/15, 21:38:29",False,"2020/06/15, 21:38:29",nan,13751595.0,11.0,1,362,How Datadog read custom log files in containers kubernetes?,It worked perfectly well.
Datadog,62394798,nan,0,"2020/06/15, 21:38:29",False,"2020/06/15, 21:38:29",nan,13751595.0,11.0,1,362,How Datadog read custom log files in containers kubernetes?,"Now, I have an obstacle on how to read the log files in the container."
Datadog,62394798,nan,0,"2020/06/15, 21:38:29",False,"2020/06/15, 21:38:29",nan,13751595.0,11.0,1,362,How Datadog read custom log files in containers kubernetes?,"I have googled and it said I can use annotations and auto discovery, but I can't see where I am supposed to define the log path."
Datadog,62394798,nan,0,"2020/06/15, 21:38:29",False,"2020/06/15, 21:38:29",nan,13751595.0,11.0,1,362,How Datadog read custom log files in containers kubernetes?,Does anyone have an idea how to resolve this or have a similar case with mine?
Datadog,62394798,nan,0,"2020/06/15, 21:38:29",False,"2020/06/15, 21:38:29",nan,13751595.0,11.0,1,362,How Datadog read custom log files in containers kubernetes?,Thank you in advance.
Datadog,61521576,nan,1,"2020/04/30, 13:43:22",True,"2020/04/30, 16:28:56",nan,944768.0,1824.0,1,400,DataDog Log Search empty string facet,"On DataDog log search, I want to search for logs with empty string for a specific facet, e.g."
Datadog,61521576,nan,1,"2020/04/30, 13:43:22",True,"2020/04/30, 16:28:56",nan,944768.0,1824.0,1,400,DataDog Log Search empty string facet,logs with userId is empty.
Datadog,61521576,nan,1,"2020/04/30, 13:43:22",True,"2020/04/30, 16:28:56",nan,944768.0,1824.0,1,400,DataDog Log Search empty string facet,"@userId:''  ,  @userId:"""" ,  -@userId:*  non worked."
Datadog,61108009,nan,1,"2020/04/08, 21:55:29",True,"2020/04/15, 21:52:15",nan,13262459.0,11.0,1,693,How to get the number of different values of a metric&#39;s tag in Datadog,I have a metric which has a tag with lots of different values (the value is a file name).
Datadog,61108009,nan,1,"2020/04/08, 21:55:29",True,"2020/04/15, 21:52:15",nan,13262459.0,11.0,1,693,How to get the number of different values of a metric&#39;s tag in Datadog,How can I create a query that determines the number of different values of that tag exist on a metric?
Datadog,61108009,nan,1,"2020/04/08, 21:55:29",True,"2020/04/15, 21:52:15",nan,13262459.0,11.0,1,693,How to get the number of different values of a metric&#39;s tag in Datadog,"For example if 4 metrics are received during a time frame, with the following tags ""file_name:dir/file1"", ""file_name:dir/file2"", ""file_name:dir/file3"", ""file_name:dir/file1"""
Datadog,61108009,nan,1,"2020/04/08, 21:55:29",True,"2020/04/15, 21:52:15",nan,13262459.0,11.0,1,693,How to get the number of different values of a metric&#39;s tag in Datadog,"I want the query to return the value 3, since of all the metrics received during this timeframe there were 3 distinct values for the file_name tag."
Datadog,60413506,nan,0,"2020/02/26, 14:02:30",False,"2020/02/26, 17:06:38","2020/02/26, 17:06:38",12966427.0,11.0,1,80,Datadog checks not appearing in Datadog console,"This is the installation path of datadog  /etc/datadog-agent 
Under this we have checks under folder  /etc/datadog-agent/conf.d"
Datadog,60413506,nan,0,"2020/02/26, 14:02:30",False,"2020/02/26, 17:06:38","2020/02/26, 17:06:38",12966427.0,11.0,1,80,Datadog checks not appearing in Datadog console,"Under this we have defined a service to report disk space alert under disk.d
 /etc/datadog-agent/conf.d/disk.d"
Datadog,60413506,nan,0,"2020/02/26, 14:02:30",False,"2020/02/26, 17:06:38","2020/02/26, 17:06:38",12966427.0,11.0,1,80,Datadog checks not appearing in Datadog console,We have the file ready in the configuration.
Datadog,60413506,nan,0,"2020/02/26, 14:02:30",False,"2020/02/26, 17:06:38","2020/02/26, 17:06:38",12966427.0,11.0,1,80,Datadog checks not appearing in Datadog console,We did tried to reload the datadog agent to reflect the changes.
Datadog,60413506,nan,0,"2020/02/26, 14:02:30",False,"2020/02/26, 17:06:38","2020/02/26, 17:06:38",12966427.0,11.0,1,80,Datadog checks not appearing in Datadog console,"The expected scenario is it should reflect in datadog console under the service defined 
The query we are using is"
Datadog,60413506,nan,0,"2020/02/26, 14:02:30",False,"2020/02/26, 17:06:38","2020/02/26, 17:06:38",12966427.0,11.0,1,80,Datadog checks not appearing in Datadog console,But we are unable to establish anything.
Datadog,60413506,nan,0,"2020/02/26, 14:02:30",False,"2020/02/26, 17:06:38","2020/02/26, 17:06:38",12966427.0,11.0,1,80,Datadog checks not appearing in Datadog console,Nutshell none of the alerts configured for this host are not reflecting in datadog console.
Datadog,60170609,nan,1,"2020/02/11, 16:01:42",False,"2020/02/11, 16:50:50",nan,1191261.0,31686.0,1,1005,How to post process JSON logs with Datadog?,Our applications log in JSON format.
Datadog,60170609,nan,1,"2020/02/11, 16:01:42",False,"2020/02/11, 16:50:50",nan,1191261.0,31686.0,1,1005,How to post process JSON logs with Datadog?,According to Datadog's documentation JSON logs are not processed by pipelines.
Datadog,60170609,nan,1,"2020/02/11, 16:01:42",False,"2020/02/11, 16:50:50",nan,1191261.0,31686.0,1,1005,How to post process JSON logs with Datadog?,How can I enrich the JSON logs with an additional field that is based on a different value of that same log line?
Datadog,60170609,nan,1,"2020/02/11, 16:01:42",False,"2020/02/11, 16:50:50",nan,1191261.0,31686.0,1,1005,How to post process JSON logs with Datadog?,I have this line:
Datadog,60170609,nan,1,"2020/02/11, 16:01:42",False,"2020/02/11, 16:50:50",nan,1191261.0,31686.0,1,1005,How to post process JSON logs with Datadog?,And I want this line:
Datadog,60170609,nan,1,"2020/02/11, 16:01:42",False,"2020/02/11, 16:50:50",nan,1191261.0,31686.0,1,1005,How to post process JSON logs with Datadog?,Is this possible with Datadog?
Datadog,60170609,nan,1,"2020/02/11, 16:01:42",False,"2020/02/11, 16:50:50",nan,1191261.0,31686.0,1,1005,How to post process JSON logs with Datadog?,I do not want to change our loggers to a the  customerId  to the log output.
Datadog,59914559,nan,0,"2020/01/26, 02:22:00",False,"2020/01/26, 02:22:00",nan,11229708.0,13.0,1,91,When to use Datadog Distribution and Histogram,I cannot find any article that describes the advantages of using datadog histogram compared to datadog distribution for apps that run on multi instance.
Datadog,59914559,nan,0,"2020/01/26, 02:22:00",False,"2020/01/26, 02:22:00",nan,11229708.0,13.0,1,91,When to use Datadog Distribution and Histogram,Would someone kindly help me on deciding the best choice between those two?
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,I want to be able to parameterised my datadog dashboard.
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,I have already introduced a template variable  flavor  which to indicate if it is  dev  or  prod  environment.
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,What I wish to achieve is to switch data from one environment o another when I select a different environment (e.g.
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,from  dev-db-master  to  prod-db-master ).
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,The string interpolation is necessary because I want to display multiple time series within a single chart.
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,However the chart is basically blank
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,The Json tab also shows a pink background which indicates either the json is malformed or the query is too complex.
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,"My goal is to be able to, by changing the template variable  flavor ,"
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,"I can change a group of time series from, says,  'dev-db-master', 'dev-db1-master' and 'dev-db2-master'  to  'prod-db-master', 'prod-db1-master' and 'prod-db2-master' ."
Datadog,59622491,59625224.0,1,"2020/01/07, 07:10:54",True,"2020/01/08, 11:23:00","2020/01/08, 05:30:50",58129.0,29539.0,1,213,How to correct the query statement (string interpolation) in the Datadog dashboard?,Can you suggest a way to construct a string with a template variable?
Datadog,59357130,59359866.0,1,"2019/12/16, 14:58:03",True,"2019/12/16, 17:42:43",nan,8205634.0,73.0,1,281,datadog replace or manually assign log value,"I have a message like 'Service is running' that i'm not able to change, so in log Grok Parser I want to replace it to 'INFO | Service is running' or manually or somehow manually assign like  %{level=INFO}  ."
Datadog,59357130,59359866.0,1,"2019/12/16, 14:58:03",True,"2019/12/16, 17:42:43",nan,8205634.0,73.0,1,281,datadog replace or manually assign log value,Please kindly advice.
Datadog,58873617,nan,0,"2019/11/15, 11:10:04",False,"2019/11/15, 12:02:48","2019/11/15, 12:02:48",4867627.0,135.0,1,125,Datadog event trigger returns no data instead of 0,I have created a event monitor( for example
Datadog,58873617,nan,0,"2019/11/15, 11:10:04",False,"2019/11/15, 12:02:48","2019/11/15, 12:02:48",4867627.0,135.0,1,125,Datadog event trigger returns no data instead of 0,events('sources:rds event_source:db-instance').by('dbinstanceidentifier').rollup('count').last('1d') &gt;= 1 )
Datadog,58873617,nan,0,"2019/11/15, 11:10:04",False,"2019/11/15, 12:02:48","2019/11/15, 12:02:48",4867627.0,135.0,1,125,Datadog event trigger returns no data instead of 0,"but it returns ""NO DATA"" when there are no any events."
Datadog,58873617,nan,0,"2019/11/15, 11:10:04",False,"2019/11/15, 12:02:48","2019/11/15, 12:02:48",4867627.0,135.0,1,125,Datadog event trigger returns no data instead of 0,How to make it return 0 when there are no any events?
Datadog,58725596,nan,1,"2019/11/06, 10:08:40",True,"2019/11/08, 05:46:30","2019/11/07, 14:46:28",10831906.0,21.0,1,91,Is it possible to migrate existing data (Like in Prometheus and elk )to datadog?,Is it possible to migrate existing data (Like in Prometheus and elk )to datadog?
Datadog,58725596,nan,1,"2019/11/06, 10:08:40",True,"2019/11/08, 05:46:30","2019/11/07, 14:46:28",10831906.0,21.0,1,91,Is it possible to migrate existing data (Like in Prometheus and elk )to datadog?,There is a setup for live streaming of Prometheus metrics to Datadog by configuring datadog config.
Datadog,58725596,nan,1,"2019/11/06, 10:08:40",True,"2019/11/08, 05:46:30","2019/11/07, 14:46:28",10831906.0,21.0,1,91,Is it possible to migrate existing data (Like in Prometheus and elk )to datadog?,But what could be done with the past data?
Datadog,58564144,nan,1,"2019/10/25, 21:48:57",True,"2019/11/20, 23:00:45","2019/10/25, 23:33:21",10913713.0,107.0,1,354,Datadog Configuration for Postgres 9.6,I have a question about the configuration setting for datadog for postgres 9.6.
Datadog,58564144,nan,1,"2019/10/25, 21:48:57",True,"2019/11/20, 23:00:45","2019/10/25, 23:33:21",10913713.0,107.0,1,354,Datadog Configuration for Postgres 9.6,(1) How do I get all databases monitored in datadog?
Datadog,58564144,nan,1,"2019/10/25, 21:48:57",True,"2019/11/20, 23:00:45","2019/10/25, 23:33:21",10913713.0,107.0,1,354,Datadog Configuration for Postgres 9.6,(2) How do I get all table level metrics from each database/schema?
Datadog,58564144,nan,1,"2019/10/25, 21:48:57",True,"2019/11/20, 23:00:45","2019/10/25, 23:33:21",10913713.0,107.0,1,354,Datadog Configuration for Postgres 9.6,Here is conf file.
Datadog,58564144,nan,1,"2019/10/25, 21:48:57",True,"2019/11/20, 23:00:45","2019/10/25, 23:33:21",10913713.0,107.0,1,354,Datadog Configuration for Postgres 9.6,Datadog documents are not really helpful.
Datadog,58564144,nan,1,"2019/10/25, 21:48:57",True,"2019/11/20, 23:00:45","2019/10/25, 23:33:21",10913713.0,107.0,1,354,Datadog Configuration for Postgres 9.6,"Instead of listing all the dbs, I want all databases, so if we add a new db, we don't have to change the conf file and same goes for table_name."
Datadog,58564144,nan,1,"2019/10/25, 21:48:57",True,"2019/11/20, 23:00:45","2019/10/25, 23:33:21",10913713.0,107.0,1,354,Datadog Configuration for Postgres 9.6,"According to datadog docs, the table level metrics are collected using pg_stat_user_tables, pg_statio_user_tables etc."
Datadog,58564144,nan,1,"2019/10/25, 21:48:57",True,"2019/11/20, 23:00:45","2019/10/25, 23:33:21",10913713.0,107.0,1,354,Datadog Configuration for Postgres 9.6,And these postgres tables are database specific unlike pg_stat_activity or pg_stat_statements.
Datadog,57557508,nan,0,"2019/08/19, 16:14:04",False,"2019/08/19, 16:20:27","2019/08/19, 16:20:27",3014866.0,8331.0,1,66,Datadog client doesn&#39;t send trace to server,I have following function:
Datadog,57557508,nan,0,"2019/08/19, 16:14:04",False,"2019/08/19, 16:20:27","2019/08/19, 16:20:27",3014866.0,8331.0,1,66,Datadog client doesn&#39;t send trace to server,I use it as following:
Datadog,57557508,nan,0,"2019/08/19, 16:14:04",False,"2019/08/19, 16:20:27","2019/08/19, 16:20:27",3014866.0,8331.0,1,66,Datadog client doesn&#39;t send trace to server,In logs I see spans as following:
Datadog,57557508,nan,0,"2019/08/19, 16:14:04",False,"2019/08/19, 16:20:27","2019/08/19, 16:20:27",3014866.0,8331.0,1,66,Datadog client doesn&#39;t send trace to server,Agent works fine but on datadog server I don't see the trace.
Datadog,57557508,nan,0,"2019/08/19, 16:14:04",False,"2019/08/19, 16:20:27","2019/08/19, 16:20:27",3014866.0,8331.0,1,66,Datadog client doesn&#39;t send trace to server,What is wrong?
Datadog,57524158,nan,0,"2019/08/16, 14:50:53",False,"2019/08/16, 15:20:44","2019/08/16, 15:20:44",3014866.0,8331.0,1,72,datadog: set traceId manually,I am reading this  tutorial :
Datadog,57524158,nan,0,"2019/08/16, 14:50:53",False,"2019/08/16, 15:20:44","2019/08/16, 15:20:44",3014866.0,8331.0,1,72,datadog: set traceId manually,I would like to set  traceId  be equal  requestId .
Datadog,57524158,nan,0,"2019/08/16, 14:50:53",False,"2019/08/16, 15:20:44","2019/08/16, 15:20:44",3014866.0,8331.0,1,72,datadog: set traceId manually,How can I do it using  ddtrace  api?
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,I have a situation where I'm trying to count the number of files loaded into the system I am monitoring.
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,"I'm sending a ""load time"" metric to Datadog each time a file is loaded, and I need to send an alert whenever an expected file does not appear."
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,"To do this, I was going to count up the number of ""load time"" metrics sent to Datadog in a 24 hour period, then use anomaly detection to see whether it was less than the normal number expected."
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,"However, I'm having some trouble finding a way to consistently pull out this count for use in the alert."
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,"I can't use the count_nonzero function, as some of my files are empty and have a load time of 0."
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,"I do know about .as_count() and count:metric{tags}, but I haven't found a way to include an evaluation interval with either of these."
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,"I've tried using .rollup(count, time) to count up the metrics sent, but this call seems to return variable results based on the rollup interval."
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,"For instance, if I compare intervals of 2000 and 4000 seconds, I would expect each 4000 second interval to count up about the sum of two 2000 second intervals over the same time period."
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,This does not seem to be what happens at all - the counts for the smaller intervals seem to add up to much more than the count for the larger one.
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,"Additionally some rollup intervals display decimal numbers as counts, which does not make any sense to me if this function is doing what I thought it was."
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,Does anyone have any thoughts on how to accomplish this?
Datadog,57189512,nan,0,"2019/07/24, 21:52:22",False,"2019/07/24, 22:05:56","2019/07/24, 22:05:56",11736052.0,21.0,1,314,How to count the number of metrics sent to Datadog over a 24 hour period?,I'd really appreciate any new ideas.
Datadog,57165269,nan,0,"2019/07/23, 16:28:19",False,"2019/07/23, 16:28:19",nan,11736052.0,21.0,1,50,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?",I'm trying to make a dashboard to monitor a process which runs on 5 remote machines simultaneously.
Datadog,57165269,nan,0,"2019/07/23, 16:28:19",False,"2019/07/23, 16:28:19",nan,11736052.0,21.0,1,50,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?","I want the dashboard to display the metrics for each machine separately - basically, I want to create five separate graphs, one for each machine that runs the process."
Datadog,57165269,nan,0,"2019/07/23, 16:28:19",False,"2019/07/23, 16:28:19",nan,11736052.0,21.0,1,50,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?","My problem is that the remote machines are reassigned periodically, so I have no way of knowing the name of the host at any given time."
Datadog,57165269,nan,0,"2019/07/23, 16:28:19",False,"2019/07/23, 16:28:19",nan,11736052.0,21.0,1,50,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?","I've tried creating five separate graphs, with each one filtered by a different host name tag, but the graphs do not seem to pick up the new host when the lease for the process is changed."
Datadog,57165269,nan,0,"2019/07/23, 16:28:19",False,"2019/07/23, 16:28:19",nan,11736052.0,21.0,1,50,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?","I also know you can split out one graph for each host using metrics explorer, but I haven't found any way to automatically do that on a dashboard."
Datadog,57165269,nan,0,"2019/07/23, 16:28:19",False,"2019/07/23, 16:28:19",nan,11736052.0,21.0,1,50,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?",Does anyone know if this is possible?
Datadog,57165269,nan,0,"2019/07/23, 16:28:19",False,"2019/07/23, 16:28:19",nan,11736052.0,21.0,1,50,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?","Leases for the process are assigned through AWS, if that is helpful."
Datadog,57165269,nan,0,"2019/07/23, 16:28:19",False,"2019/07/23, 16:28:19",nan,11736052.0,21.0,1,50,"Is there a way to add a separate graph for each host on a Datadog dashboard, when the hosts frequently change?",Thanks in advance for any suggestions.
Datadog,56382266,nan,1,"2019/05/30, 19:40:24",False,"2020/05/21, 07:25:17",nan,3869978.0,697.0,1,395,Log JSON to DataDog log message field,I'd like to be able to send logs to datadog and have the message be a JSON object rather than a string.
Datadog,56382266,nan,1,"2019/05/30, 19:40:24",False,"2020/05/21, 07:25:17",nan,3869978.0,697.0,1,395,Log JSON to DataDog log message field,"The metadata fields aren't searchable unless a facet is created, which I would like to avoid doing."
Datadog,56382266,nan,1,"2019/05/30, 19:40:24",False,"2020/05/21, 07:25:17",nan,3869978.0,697.0,1,395,Log JSON to DataDog log message field,I'm currently using  winston  +  winston-datadog-logs-transporter  to send the logs.
Datadog,56382266,nan,1,"2019/05/30, 19:40:24",False,"2020/05/21, 07:25:17",nan,3869978.0,697.0,1,395,Log JSON to DataDog log message field,"If I do:  logger.info(JSON.stringify(message)) , datadog records the message as blank and adds the stringified message as metadata."
Datadog,56382266,nan,1,"2019/05/30, 19:40:24",False,"2020/05/21, 07:25:17",nan,3869978.0,697.0,1,395,Log JSON to DataDog log message field,"If I do:  logger.info('foo' + JSON.stringify(message) , then the message is interpreted as a string and I can search on it."
Datadog,56382266,nan,1,"2019/05/30, 19:40:24",False,"2020/05/21, 07:25:17",nan,3869978.0,697.0,1,395,Log JSON to DataDog log message field,"If I do:  logger.info('foo', message) , the body is set to  foo  and  message  is interpreted as metadata, which I cannot search for without creating a facet."
Datadog,56382266,nan,1,"2019/05/30, 19:40:24",False,"2020/05/21, 07:25:17",nan,3869978.0,697.0,1,395,Log JSON to DataDog log message field,"Any help is appreciated, thanks!"
Datadog,55794663,nan,1,"2019/04/22, 15:45:55",True,"2019/04/25, 16:15:12","2019/04/22, 19:35:19",7945853.0,11.0,1,1456,How to set APM service name in DataDog for an application using Nodejs,I am not able to see traces for my application under APM --  Service in Datadog.
Datadog,55794663,nan,1,"2019/04/22, 15:45:55",True,"2019/04/25, 16:15:12","2019/04/22, 19:35:19",7945853.0,11.0,1,1456,How to set APM service name in DataDog for an application using Nodejs,I found some sample code from Datadog docs but don't know exactly where it should go inside my application.
Datadog,55794663,nan,1,"2019/04/22, 15:45:55",True,"2019/04/25, 16:15:12","2019/04/22, 19:35:19",7945853.0,11.0,1,1456,How to set APM service name in DataDog for an application using Nodejs,Please let me know if anyone has any idea regarding it.
Datadog,55794663,nan,1,"2019/04/22, 15:45:55",True,"2019/04/25, 16:15:12","2019/04/22, 19:35:19",7945853.0,11.0,1,1456,How to set APM service name in DataDog for an application using Nodejs,I have already tried with following code in my js file.
Datadog,55794663,nan,1,"2019/04/22, 15:45:55",True,"2019/04/25, 16:15:12","2019/04/22, 19:35:19",7945853.0,11.0,1,1456,How to set APM service name in DataDog for an application using Nodejs,My application is based on node js which is serverless.
Datadog,55794663,nan,1,"2019/04/22, 15:45:55",True,"2019/04/25, 16:15:12","2019/04/22, 19:35:19",7945853.0,11.0,1,1456,How to set APM service name in DataDog for an application using Nodejs,"I have also added dependencies for dd-trace in package.json as  ""dd-trace"": ""^0.11.0"""
Datadog,55794663,nan,1,"2019/04/22, 15:45:55",True,"2019/04/25, 16:15:12","2019/04/22, 19:35:19",7945853.0,11.0,1,1456,How to set APM service name in DataDog for an application using Nodejs,I expected to list my application with proper name in APM Services in Datadog.
Datadog,54447779,nan,0,"2019/01/30, 21:02:33",False,"2019/01/30, 21:07:49","2019/01/30, 21:07:49",2271269.0,7779.0,1,59,Istio: create metric that returns a default value with Datadog adapter,"I have a use case where I want to report all HTTP 500 events when they occur as integer counts, but also send a default value of 0 if no 500 event occurred during a request."
Datadog,54447779,nan,0,"2019/01/30, 21:02:33",False,"2019/01/30, 21:07:49","2019/01/30, 21:07:49",2271269.0,7779.0,1,59,Istio: create metric that returns a default value with Datadog adapter,How can I achieve this with the Datadog adapter?
Datadog,54447779,nan,0,"2019/01/30, 21:02:33",False,"2019/01/30, 21:07:49","2019/01/30, 21:07:49",2271269.0,7779.0,1,59,Istio: create metric that returns a default value with Datadog adapter,"As a first pass, I attempted to create a rule that has  match: true , and then a metric that, for value, sets it to read  conditional(response.code.startsWith(""5""), ""1"", ""0"") ."
Datadog,54447779,nan,0,"2019/01/30, 21:02:33",False,"2019/01/30, 21:07:49","2019/01/30, 21:07:49",2271269.0,7779.0,1,59,Istio: create metric that returns a default value with Datadog adapter,"I then told the rule to use the Datadog adaptor, and registered this metric with said adaptor in the rule."
Datadog,54447779,nan,0,"2019/01/30, 21:02:33",False,"2019/01/30, 21:07:49","2019/01/30, 21:07:49",2271269.0,7779.0,1,59,Istio: create metric that returns a default value with Datadog adapter,"This threw errors in the mixer logs, likely because request.code is an integer and startsWith is probably a function that expects a string - we lost all metrics as a consequence."
Datadog,52269765,nan,0,"2018/09/11, 09:18:35",False,"2018/09/11, 09:18:35",nan,3150716.0,518.0,1,129,alert condition over variables in datadog,How can I create a threshold alert by comparing aggregate of 2 metrics ?
Datadog,52269765,nan,0,"2018/09/11, 09:18:35",False,"2018/09/11, 09:18:35",nan,3150716.0,518.0,1,129,alert condition over variables in datadog,"For example, if m1=[2,3,1,5] and m2=[6,7], I want to create an alert when sum(m1)   sum(m2)."
Datadog,52269765,nan,0,"2018/09/11, 09:18:35",False,"2018/09/11, 09:18:35",nan,3150716.0,518.0,1,129,alert condition over variables in datadog,sum method here I assume will add all the data points returned by a query.
Datadog,52269765,nan,0,"2018/09/11, 09:18:35",False,"2018/09/11, 09:18:35",nan,3150716.0,518.0,1,129,alert condition over variables in datadog,"From what I have observed, datadog flow of alert creating is like, define the metric  -  set alert condition on metric."
Datadog,52269765,nan,0,"2018/09/11, 09:18:35",False,"2018/09/11, 09:18:35",nan,3150716.0,518.0,1,129,alert condition over variables in datadog,"That is, it looks like the alert condition will be some condition on metric data type only."
Datadog,52269765,nan,0,"2018/09/11, 09:18:35",False,"2018/09/11, 09:18:35",nan,3150716.0,518.0,1,129,alert condition over variables in datadog,But I am looking for something like storing the aggregate of metrics in a variables and comparing those variables.
Datadog,52269765,nan,0,"2018/09/11, 09:18:35",False,"2018/09/11, 09:18:35",nan,3150716.0,518.0,1,129,alert condition over variables in datadog,How can it be done in datadog ?
Datadog,52176297,nan,1,"2018/09/05, 05:38:01",True,"2018/12/03, 01:28:11",nan,6228435.0,179.0,1,410,"Cannot send &quot;tags&quot; via datadog in Amazon ECS, Fargate","I have configured datadog agent on Amazon ECS, Fargate."
Datadog,52176297,nan,1,"2018/09/05, 05:38:01",True,"2018/12/03, 01:28:11",nan,6228435.0,179.0,1,410,"Cannot send &quot;tags&quot; via datadog in Amazon ECS, Fargate","I can send all the intended metrics but I cannot send ""tags""."
Datadog,52176297,nan,1,"2018/09/05, 05:38:01",True,"2018/12/03, 01:28:11",nan,6228435.0,179.0,1,410,"Cannot send &quot;tags&quot; via datadog in Amazon ECS, Fargate",I've set Environment variables in ECS task definitions.
Datadog,52176297,nan,1,"2018/09/05, 05:38:01",True,"2018/12/03, 01:28:11",nan,6228435.0,179.0,1,410,"Cannot send &quot;tags&quot; via datadog in Amazon ECS, Fargate",I think most of the settings are all right because I can see the metrics which I want to see.
Datadog,52176297,nan,1,"2018/09/05, 05:38:01",True,"2018/12/03, 01:28:11",nan,6228435.0,179.0,1,410,"Cannot send &quot;tags&quot; via datadog in Amazon ECS, Fargate","But tags, especially env:stg is missing in datadog UI and because of this weired error, some metrics is missing."
Datadog,52176297,nan,1,"2018/09/05, 05:38:01",True,"2018/12/03, 01:28:11",nan,6228435.0,179.0,1,410,"Cannot send &quot;tags&quot; via datadog in Amazon ECS, Fargate",Does anyone know the reason of this error and the way to solve this?
Datadog,52176297,nan,1,"2018/09/05, 05:38:01",True,"2018/12/03, 01:28:11",nan,6228435.0,179.0,1,410,"Cannot send &quot;tags&quot; via datadog in Amazon ECS, Fargate",Thanks.
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed",I am attempting to use Datadog to monitor my application via JMX...
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed","I have successfully deployed my app in a docker container, and exposed the JMX port and confirmed I can indeed attach to the port from anywhere and get information."
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed",So I am attempting to set up the datadog docker image to use JMX and connect to the server...
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed","I have it all configured, but at runtime the datadog image attempts to start utilizing JMX, but fails saying it can't find Java on its image..."
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed",I log into the image and sure enough it has no java installed.
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed",From the datadog documentation:
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed","Well that's all nice and well, but if I attempt to expose my host machine java to the image via a volume mount, it doesn't work, as the host machine is Apple and if the image attempts to run the java binary it throws an invalid format for the binary file.. not surprising since its a MACOS binary not a Debian Linux Binary (which the datadog image is)...."
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed","So, I have been attempting to take the datadog image and build a new image with it as the base with Java... but I have been completely unsuccessful, every attempt to install java during docker build fails.."
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed","I have tried every example of how to install java into a debian docker image, but none work... Every one dies with apt-get line returned a non zero"
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed",How the heck do I get JAVA installed on a debian image?
Datadog,51866333,51933718.0,1,"2018/08/15, 23:58:16",True,"2018/08/20, 18:11:48",nan,282172.0,3212.0,1,300,"Datadog with JMX, datadog docker image does not have java installed","Or better yet, how do I get the datadog image with JMX to run properly?"
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,I am having a hard time to collect logs from an python app deployed in ECS using DataDog Agent.
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,I have a dockerized Flask app deployed in ECS.
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,The app spits logs to stdout.
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,I now need to monitor them in DataDog.
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,"I've added a new DataDog agent container (Fargate compatible, since I am using Fargate), which runs as part of the same task as the app."
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,"I can see the CPU and memory metrics for both container in app.datadoghq.com/containers, so that means that DataDog Agent is working."
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,I now need the app logs.
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,"I went through the documentation in  https://app.datadoghq.com/logs/onboarding/container , added"
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,to the app container and the following env.vars to the DataDog container :
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,But that seems to be insufficient.
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,Am I going in the right direction ?
Datadog,51443070,51451079.0,1,"2018/07/20, 15:57:38",True,"2018/07/21, 01:20:42",nan,155722.0,1507.0,1,1581,Collecting Python logs to DataDog in ECS,What am I missing ?
Datadog,51323878,nan,1,"2018/07/13, 14:15:47",True,"2018/07/13, 14:39:40",nan,9763778.0,61.0,1,465,Datadog: PostgreSQL custom_metrics returns a single row,I wanted to create a graph in Datadog to display iddle connections per user.
Datadog,51323878,nan,1,"2018/07/13, 14:15:47",True,"2018/07/13, 14:39:40",nan,9763778.0,61.0,1,465,Datadog: PostgreSQL custom_metrics returns a single row,Following this example:  http://www.miketheman.net/tag/postgres/  I changed my postgres.yaml configuration to:
Datadog,51323878,nan,1,"2018/07/13, 14:15:47",True,"2018/07/13, 14:39:40",nan,9763778.0,61.0,1,465,Datadog: PostgreSQL custom_metrics returns a single row,"I can see the metric is appearing in Datadog, but I can just see one of the rows that should appear (has I have several databases in my PostgreSQL)."
Datadog,51323878,nan,1,"2018/07/13, 14:15:47",True,"2018/07/13, 14:39:40",nan,9763778.0,61.0,1,465,Datadog: PostgreSQL custom_metrics returns a single row,"Here is the datadog connection graph 
Am I missing any step?"
Datadog,51323878,nan,1,"2018/07/13, 14:15:47",True,"2018/07/13, 14:39:40",nan,9763778.0,61.0,1,465,Datadog: PostgreSQL custom_metrics returns a single row,Is postgres.yaml missing any configuration?
Datadog,51323878,nan,1,"2018/07/13, 14:15:47",True,"2018/07/13, 14:39:40",nan,9763778.0,61.0,1,465,Datadog: PostgreSQL custom_metrics returns a single row,"Running that query in psql, I get this (modifying names but not data):"
Datadog,50792558,50793070.0,2,"2018/06/11, 10:35:14",True,"2020/04/05, 17:51:15",nan,9684016.0,383.0,1,1107,Why can&#39;t create principal on aws iam for datadog?,"From the datadog guide, want to integrate aws:"
Datadog,50792558,50793070.0,2,"2018/06/11, 10:35:14",True,"2020/04/05, 17:51:15",nan,9684016.0,383.0,1,1107,Why can&#39;t create principal on aws iam for datadog?,https://docs.datadoghq.com/integrations/amazon_web_services/
Datadog,50792558,50793070.0,2,"2018/06/11, 10:35:14",True,"2020/04/05, 17:51:15",nan,9684016.0,383.0,1,1107,Why can&#39;t create principal on aws iam for datadog?,Created a new policy named  DatadogAWSIntegrationPolicy :
Datadog,50792558,50793070.0,2,"2018/06/11, 10:35:14",True,"2020/04/05, 17:51:15",nan,9684016.0,383.0,1,1107,Why can&#39;t create principal on aws iam for datadog?,"However, when clicked  Review policy  button, it said:"
Datadog,50792558,50793070.0,2,"2018/06/11, 10:35:14",True,"2020/04/05, 17:51:15",nan,9684016.0,383.0,1,1107,Why can&#39;t create principal on aws iam for datadog?,The syntax was followed the  datadog  service:
Datadog,50792558,50793070.0,2,"2018/06/11, 10:35:14",True,"2020/04/05, 17:51:15",nan,9684016.0,383.0,1,1107,Why can&#39;t create principal on aws iam for datadog?,https://help.datadoghq.com/hc/en-us/articles/360002042531-Error-Datadog-is-not-authorized-to-peform-sts-AssumeRole
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],What did I do?
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],via command line on my debian 8.x host.
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],Expected behaviour:
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],Actual behaviour:
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],Problem:
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],I can not start the service due to Result state: start-limit.
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],I waited about 9 hours.
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],My idea was the service state will recover from to many restart attempts.
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],It did not.
Datadog,50009458,nan,0,"2018/04/24, 22:26:16",False,"2018/04/24, 22:26:16",nan,7150628.0,173.0,1,1772,failed to start (Result: start-limit) [debian - datadog agent],Any ideas?
Datadog,49022133,nan,1,"2018/02/28, 06:45:38",True,"2019/03/06, 23:16:09",nan,2308858.0,2735.0,1,812,Why does DataDog prefer the Docker-based Agent installation?,According to the  DataDog Docker Integration Docs :
Datadog,49022133,nan,1,"2018/02/28, 06:45:38",True,"2019/03/06, 23:16:09",nan,2308858.0,2735.0,1,812,Why does DataDog prefer the Docker-based Agent installation?,"There are two ways to run the [DataDog] Agent: directly on each host, or within a docker-dd-agent container."
Datadog,49022133,nan,1,"2018/02/28, 06:45:38",True,"2019/03/06, 23:16:09",nan,2308858.0,2735.0,1,812,Why does DataDog prefer the Docker-based Agent installation?,We recommend the latter.
Datadog,49022133,nan,1,"2018/02/28, 06:45:38",True,"2019/03/06, 23:16:09",nan,2308858.0,2735.0,1,812,Why does DataDog prefer the Docker-based Agent installation?,Why is a Docker-based agent installation preferred over just installing the DataDog agent directly as a service on the box that's running the Docker containers?
Datadog,47069283,nan,2,"2017/11/02, 08:54:36",True,"2018/09/20, 21:00:55",nan,7720467.0,37.0,1,406,Use of Datadog and Webhooks to call multiple numbers,I have my application integrated with Datadog for monitoring purpose.
Datadog,47069283,nan,2,"2017/11/02, 08:54:36",True,"2018/09/20, 21:00:55",nan,7720467.0,37.0,1,406,Use of Datadog and Webhooks to call multiple numbers,At the same time I want the notifications/calls to be sent to the team if any of the metric fails to achieve the desires value.
Datadog,47069283,nan,2,"2017/11/02, 08:54:36",True,"2018/09/20, 21:00:55",nan,7720467.0,37.0,1,406,Use of Datadog and Webhooks to call multiple numbers,I used Webhooks integration in Datadog for this purpose.
Datadog,47069283,nan,2,"2017/11/02, 08:54:36",True,"2018/09/20, 21:00:55",nan,7720467.0,37.0,1,406,Use of Datadog and Webhooks to call multiple numbers,In the webhooks configuration I have set the URL (Twilio request) and I do get a call on my number.
Datadog,47069283,nan,2,"2017/11/02, 08:54:36",True,"2018/09/20, 21:00:55",nan,7720467.0,37.0,1,406,Use of Datadog and Webhooks to call multiple numbers,Now I am looking for an scenario wherein if the user doesn't pick the call for say 30secs then try calling the second number.
Datadog,47069283,nan,2,"2017/11/02, 08:54:36",True,"2018/09/20, 21:00:55",nan,7720467.0,37.0,1,406,Use of Datadog and Webhooks to call multiple numbers,How do I achieve this?
Datadog,45695083,46202190.0,1,"2017/08/15, 17:33:01",True,"2019/12/26, 21:22:21","2019/12/26, 21:22:21",6826691.0,1171.0,1,2174,Datadog AWS integration for multiple aws account,"I have two AWS account , I was able to set AWS integration for the first account using Terraform, but when I try to create AWS integration for my second account I am having an error."
Datadog,45695083,46202190.0,1,"2017/08/15, 17:33:01",True,"2019/12/26, 21:22:21","2019/12/26, 21:22:21",6826691.0,1171.0,1,2174,Datadog AWS integration for multiple aws account,I have created a role with in-line policy and we do not have a cross account set up.
Datadog,45695083,46202190.0,1,"2017/08/15, 17:33:01",True,"2019/12/26, 21:22:21","2019/12/26, 21:22:21",6826691.0,1171.0,1,2174,Datadog AWS integration for multiple aws account,Trust Relationship:
Datadog,45695083,46202190.0,1,"2017/08/15, 17:33:01",True,"2019/12/26, 21:22:21","2019/12/26, 21:22:21",6826691.0,1171.0,1,2174,Datadog AWS integration for multiple aws account,Can anyone please guide me how to solve this error?
Datadog,45438114,45438610.0,1,"2017/08/01, 15:41:15",True,"2017/08/01, 16:05:58","2017/08/01, 16:05:58",8219179.0,21.0,1,778,How to get Datadog Alert data through API call,I have set the threshold value to get the alert in data dog for infrastructure.
Datadog,45438114,45438610.0,1,"2017/08/01, 15:41:15",True,"2017/08/01, 16:05:58","2017/08/01, 16:05:58",8219179.0,21.0,1,778,How to get Datadog Alert data through API call,Alert is coming on  data dog UI but how to get this all alert data through API call either using JAVA or python.
Datadog,45438114,45438610.0,1,"2017/08/01, 15:41:15",True,"2017/08/01, 16:05:58","2017/08/01, 16:05:58",8219179.0,21.0,1,778,How to get Datadog Alert data through API call,I need only alert data.
Datadog,45104434,45124573.0,1,"2017/07/14, 16:37:52",True,"2018/05/24, 20:55:05","2018/05/24, 20:55:05",6826691.0,1171.0,1,711,datadog elasticsearch check failed with ReadTimeout,I am trying to integrate datadog to elasticsearch but the datadog collector shows an error .
Datadog,45104434,45124573.0,1,"2017/07/14, 16:37:52",True,"2018/05/24, 20:55:05","2018/05/24, 20:55:05",6826691.0,1171.0,1,711,datadog elasticsearch check failed with ReadTimeout,i am not able to troubleshoot this.
Datadog,45104434,45124573.0,1,"2017/07/14, 16:37:52",True,"2018/05/24, 20:55:05","2018/05/24, 20:55:05",6826691.0,1171.0,1,711,datadog elasticsearch check failed with ReadTimeout,pls help
Datadog,45104434,45124573.0,1,"2017/07/14, 16:37:52",True,"2018/05/24, 20:55:05","2018/05/24, 20:55:05",6826691.0,1171.0,1,711,datadog elasticsearch check failed with ReadTimeout,My elastic.yaml
Datadog,41007369,nan,1,"2016/12/07, 02:26:32",True,"2016/12/07, 04:05:10",nan,6154172.0,21.0,1,1378,How to get the metrics that I collected using DogStatsD on dataDog?,"I want to use dataDog to see how many times a java method has been called, this is my example code."
Datadog,41007369,nan,1,"2016/12/07, 02:26:32",True,"2016/12/07, 04:05:10",nan,6154172.0,21.0,1,1378,How to get the metrics that I collected using DogStatsD on dataDog?,enter image description here
Datadog,41007369,nan,1,"2016/12/07, 02:26:32",True,"2016/12/07, 04:05:10",nan,6154172.0,21.0,1,1378,How to get the metrics that I collected using DogStatsD on dataDog?,"In this code, I simply want to count how many times that 'multiply' has been called, and I used DogStatsD to record the this metrics."
Datadog,41007369,nan,1,"2016/12/07, 02:26:32",True,"2016/12/07, 04:05:10",nan,6154172.0,21.0,1,1378,How to get the metrics that I collected using DogStatsD on dataDog?,"However, when I go to the infrastructure of my dataDog, I can not find a metrics with a name similar like ""multiply""."
Datadog,41007369,nan,1,"2016/12/07, 02:26:32",True,"2016/12/07, 04:05:10",nan,6154172.0,21.0,1,1378,How to get the metrics that I collected using DogStatsD on dataDog?,What the name of the metrics should be if I set it up correctly?
Datadog,41007369,nan,1,"2016/12/07, 02:26:32",True,"2016/12/07, 04:05:10",nan,6154172.0,21.0,1,1378,How to get the metrics that I collected using DogStatsD on dataDog?,Can anyone help me with how can I get the metrics of the 'multiply'?
Datadog,41007369,nan,1,"2016/12/07, 02:26:32",True,"2016/12/07, 04:05:10",nan,6154172.0,21.0,1,1378,How to get the metrics that I collected using DogStatsD on dataDog?,Thanks!
Datadog,41007369,nan,1,"2016/12/07, 02:26:32",True,"2016/12/07, 04:05:10",nan,6154172.0,21.0,1,1378,How to get the metrics that I collected using DogStatsD on dataDog?,enter image description here
Datadog,40933155,41220782.0,1,"2016/12/02, 15:18:12",True,"2016/12/19, 12:42:07",nan,7241401.0,13.0,1,178,Can I change JMX attribute name in JMXFetch before pushing it to DataDog?,Is it possible to change JMX attribute name in JMXFetch so that a different name shows up in DataDog?
Datadog,40933155,41220782.0,1,"2016/12/02, 15:18:12",True,"2016/12/19, 12:42:07",nan,7241401.0,13.0,1,178,Can I change JMX attribute name in JMXFetch before pushing it to DataDog?,I currently have the following:
Datadog,40933155,41220782.0,1,"2016/12/02, 15:18:12",True,"2016/12/19, 12:42:07",nan,7241401.0,13.0,1,178,Can I change JMX attribute name in JMXFetch before pushing it to DataDog?,This would report two metrics in DataDog:
Datadog,40933155,41220782.0,1,"2016/12/02, 15:18:12",True,"2016/12/19, 12:42:07",nan,7241401.0,13.0,1,178,Can I change JMX attribute name in JMXFetch before pushing it to DataDog?,Is it possible to rename it in the yaml script to something more like:
Datadog,40933155,41220782.0,1,"2016/12/02, 15:18:12",True,"2016/12/19, 12:42:07",nan,7241401.0,13.0,1,178,Can I change JMX attribute name in JMXFetch before pushing it to DataDog?,without the original names ever showing up in DataDog?
Datadog,39235163,39235805.0,1,"2016/08/30, 22:00:17",True,"2016/08/30, 22:40:17","2020/06/20, 12:12:55",4480164.0,43402.0,1,5873,ImportError: No module named datadog,I'm new to datadog.
Datadog,39235163,39235805.0,1,"2016/08/30, 22:00:17",True,"2016/08/30, 22:40:17","2020/06/20, 12:12:55",4480164.0,43402.0,1,5873,ImportError: No module named datadog,"I followed this  post , and replaced in my app/api keys."
Datadog,39235163,39235805.0,1,"2016/08/30, 22:00:17",True,"2016/08/30, 22:40:17","2020/06/20, 12:12:55",4480164.0,43402.0,1,5873,ImportError: No module named datadog,I have :  nginx_dd.py
Datadog,39235163,39235805.0,1,"2016/08/30, 22:00:17",True,"2016/08/30, 22:40:17","2020/06/20, 12:12:55",4480164.0,43402.0,1,5873,ImportError: No module named datadog,"When I run it  python nginx_dd.py , I kept getting"
Datadog,39235163,39235805.0,1,"2016/08/30, 22:00:17",True,"2016/08/30, 22:40:17","2020/06/20, 12:12:55",4480164.0,43402.0,1,5873,ImportError: No module named datadog,ImportError: No module named datadog
Datadog,39235163,39235805.0,1,"2016/08/30, 22:00:17",True,"2016/08/30, 22:40:17","2020/06/20, 12:12:55",4480164.0,43402.0,1,5873,ImportError: No module named datadog,Any hints / suggestions on this will be a huge helps !
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,I have a C# service installed on a machine that I publish stats to a DataDogAgent (which I later monitor).
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,"I uses StatsD as the library to publish the stats, the DataDogAgent is installed locally on the machine."
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,When I first set this up it worked great and no issues.
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,When I released new code to the box and re-installed the service I stopped getting updated stats to DataDog.
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,When I look in the DataDogAgent in the collection log I see
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,I have verified that my code that submits via StatsD has not changed and is correctly sending out the stats I want.
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,"I also repaired, uninstalled and re-installed and restarted the DataDogAgent all with no success."
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,The timing is suspect to when I released code but I don't see what I would have changed that would cause this.
Datadog,39025143,nan,0,"2016/08/18, 21:28:03",False,"2016/08/18, 21:28:03",nan,2236401.0,8778.0,1,264,DataDog fails on Instance#0 for statsd collection,Everything is set to go to port 8125 for StatsD
Datadog,67004358,nan,0,"2021/04/08, 15:50:37",False,"2021/04/08, 15:50:37",nan,3086403.0,1894.0,0,18,How to create datadog &#39;change alerts&#39; using terraform?,I am trying to create a change  monitor using terraform .
Datadog,67004358,nan,0,"2021/04/08, 15:50:37",False,"2021/04/08, 15:50:37",nan,3086403.0,1894.0,0,18,How to create datadog &#39;change alerts&#39; using terraform?,To create a monitor that checks that overtime a count stays at 0 for example every day (the value will go up to one some times and get back to 0).
Datadog,67004358,nan,0,"2021/04/08, 15:50:37",False,"2021/04/08, 15:50:37",nan,3086403.0,1894.0,0,18,How to create datadog &#39;change alerts&#39; using terraform?,I found on the UI the capacity to create a  change alert .
Datadog,67004358,nan,0,"2021/04/08, 15:50:37",False,"2021/04/08, 15:50:37",nan,3086403.0,1894.0,0,18,How to create datadog &#39;change alerts&#39; using terraform?,I cant seem to find a way to define the configuration for this type.
Datadog,67004358,nan,0,"2021/04/08, 15:50:37",False,"2021/04/08, 15:50:37",nan,3086403.0,1894.0,0,18,How to create datadog &#39;change alerts&#39; using terraform?,Is terraform just supporting only a subset of the monitors?
Datadog,67004358,nan,0,"2021/04/08, 15:50:37",False,"2021/04/08, 15:50:37",nan,3086403.0,1894.0,0,18,How to create datadog &#39;change alerts&#39; using terraform?,or does the query need to be change in some specific way that I cant find documentation for?.
Datadog,66943560,nan,1,"2021/04/04, 19:40:15",True,"2021/04/05, 18:20:48",nan,783547.0,508.0,0,24,Break down Datadog COUNT/GAUGE without double-counting,I have a script that queries our CI (Buildkite)'s API once per minute to fetch details of all build agents and emit metrics to Datadog for analysis.
Datadog,66943560,nan,1,"2021/04/04, 19:40:15",True,"2021/04/05, 18:20:48",nan,783547.0,508.0,0,24,Break down Datadog COUNT/GAUGE without double-counting,"Getting an accurate  count  of these agents in the Datadog UI has proven challenging, however."
Datadog,66943560,nan,1,"2021/04/04, 19:40:15",True,"2021/04/05, 18:20:48",nan,783547.0,508.0,0,24,Break down Datadog COUNT/GAUGE without double-counting,"If the script emits a COUNT metric for each agent it sees, then agents will be double-counted in the Datadog UI when the interval is longer than a minute, because the script runs once per minute and sees (mostly) the same agents each time."
Datadog,66943560,nan,1,"2021/04/04, 19:40:15",True,"2021/04/05, 18:20:48",nan,783547.0,508.0,0,24,Break down Datadog COUNT/GAUGE without double-counting,"The script could total up the number of agents it sees each run and emit that as a GAUGE, but then I lose the ability to break down the count in the Datadog UI by agent-specific tags (queue, etc)."
Datadog,66943560,nan,1,"2021/04/04, 19:40:15",True,"2021/04/05, 18:20:48",nan,783547.0,508.0,0,24,Break down Datadog COUNT/GAUGE without double-counting,"I suppose I could emit a GAUGE with a value of 1 for each agent on each run, and add an artificial  index  tag with a value of the numeric index in the agent array, and rely on the Datadog UI to do the summation across  index  values?"
Datadog,66943560,nan,1,"2021/04/04, 19:40:15",True,"2021/04/05, 18:20:48",nan,783547.0,508.0,0,24,Break down Datadog COUNT/GAUGE without double-counting,"I could use the agent ID/host, of course, but Datadog charges by number of tag values and we've got our agents in an auto-scaling group, so hosts change frequently."
Datadog,66943560,nan,1,"2021/04/04, 19:40:15",True,"2021/04/05, 18:20:48",nan,783547.0,508.0,0,24,Break down Datadog COUNT/GAUGE without double-counting,This seems hacky - is there a better solution?
Datadog,66943560,nan,1,"2021/04/04, 19:40:15",True,"2021/04/05, 18:20:48",nan,783547.0,508.0,0,24,Break down Datadog COUNT/GAUGE without double-counting,Am I overthinking this?
Datadog,66767713,nan,0,"2021/03/23, 18:56:15",False,"2021/03/23, 18:56:15",nan,5403764.0,436.0,0,9,Can we setup datadog alert threshold dynamically based on some template variable or tag,"I want to setup datadog alert threshold different for each tag value passed, how can I do this?"
Datadog,66767713,nan,0,"2021/03/23, 18:56:15",False,"2021/03/23, 18:56:15",nan,5403764.0,436.0,0,9,Can we setup datadog alert threshold dynamically based on some template variable or tag,One possibility is I create separate monitor for each tag value and use IN query for a perticular tag value.
Datadog,66767713,nan,0,"2021/03/23, 18:56:15",False,"2021/03/23, 18:56:15",nan,5403764.0,436.0,0,9,Can we setup datadog alert threshold dynamically based on some template variable or tag,it will be bit hectic to manage those numbers of monitors.
Datadog,66639586,nan,0,"2021/03/15, 16:11:28",False,"2021/03/15, 16:11:28",nan,187423.0,3925.0,0,21,How do I query DataDog for a phrase in the stack trace?,DataDog is so useless in its querying and its intuitiveness ...
Datadog,66639586,nan,0,"2021/03/15, 16:11:28",False,"2021/03/15, 16:11:28",nan,187423.0,3925.0,0,21,How do I query DataDog for a phrase in the stack trace?,I'm looking for a custom exception in the stack trace.
Datadog,66639586,nan,0,"2021/03/15, 16:11:28",False,"2021/03/15, 16:11:28",nan,187423.0,3925.0,0,21,How do I query DataDog for a phrase in the stack trace?,"I found individual log entries in the last 18 hours that contain my exception class name, but attempting to write a log query that will find me all the occurrences is returning nothing."
Datadog,66639586,nan,0,"2021/03/15, 16:11:28",False,"2021/03/15, 16:11:28",nan,187423.0,3925.0,0,21,How do I query DataDog for a phrase in the stack trace?,E.g.
Datadog,66639586,nan,0,"2021/03/15, 16:11:28",False,"2021/03/15, 16:11:28",nan,187423.0,3925.0,0,21,How do I query DataDog for a phrase in the stack trace?,:
Datadog,66639586,nan,0,"2021/03/15, 16:11:28",False,"2021/03/15, 16:11:28",nan,187423.0,3925.0,0,21,How do I query DataDog for a phrase in the stack trace?,environment:prod @thrown.extendedStackTrace:UserDoesNotExistException
Datadog,66639586,nan,0,"2021/03/15, 16:11:28",False,"2021/03/15, 16:11:28",nan,187423.0,3925.0,0,21,How do I query DataDog for a phrase in the stack trace?,"I'd like to include more words in the query, but even reducing down a single word fails to find anything."
Datadog,66639586,nan,0,"2021/03/15, 16:11:28",False,"2021/03/15, 16:11:28",nan,187423.0,3925.0,0,21,How do I query DataDog for a phrase in the stack trace?,"I've looked at their documentation, which is zero help."
Datadog,66632769,nan,0,"2021/03/15, 07:24:52",False,"2021/03/15, 08:40:05","2021/03/15, 08:40:05",8063431.0,1.0,0,26,Do Datadog has the ability to exclude some of the websites from profiling?,Do Datadog has the ability to exclude some of the websites from profiling?
Datadog,66632769,nan,0,"2021/03/15, 07:24:52",False,"2021/03/15, 08:40:05","2021/03/15, 08:40:05",8063431.0,1.0,0,26,Do Datadog has the ability to exclude some of the websites from profiling?,"ie, Suppose client has 10 websites hosted in IIS but he needs only 3 websites to be profiled.Is it possible to do so?"
Datadog,66632769,nan,0,"2021/03/15, 07:24:52",False,"2021/03/15, 08:40:05","2021/03/15, 08:40:05",8063431.0,1.0,0,26,Do Datadog has the ability to exclude some of the websites from profiling?,Any help is highly appreciated :)
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,I am currently working on setting up a monitor to monitor slow queries in the Cloud SQL DB.
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,"I built a custom query to get the processes running on the SQL server, because currently slow query monitoring doesn't report until the process is completed."
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,To get a check every 15-20 seconds (or whatever is configured in DD) of currently running queries over 5 minutes I have this in my DD agent's config.
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,And my results in DD are:
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,As you can see it shows the count of queries that have been running for over 5 minutes.
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,How would I be able to get more information about each query.
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,For example I would like to see the exact query statement that is being executed.
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,"I know I can use the query:
 SELECT INFO as QUERY FROM INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query'; 
to get the Query statement, but Id like to be able to click on the graph in DD and dig further into each process to see the statements."
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,Is there a way to add this information or another feature in Datadog where I can  the processes being queried to each process individually?
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,My first thought was to change the custom query to this:
Datadog,66606637,66632382.0,1,"2021/03/12, 22:14:59",True,"2021/03/15, 06:26:59",nan,10709519.0,902.0,0,49,How do I create a custom metric with additional information from a custom SQL query in DataDog?,But this only returns one row with the count number of all process and the ID and Query of the first result.
Datadog,66443203,66642805.0,2,"2021/03/02, 18:24:51",True,"2021/03/18, 16:55:00","2021/03/18, 16:55:00",12649047.0,13.0,0,59,Exclude site from Datadog automatic trace instrumentation on IIS,I was wondering to know if there is a way to exclude a site from Datadog automatic tracing on IIS.
Datadog,66443203,66642805.0,2,"2021/03/02, 18:24:51",True,"2021/03/18, 16:55:00","2021/03/18, 16:55:00",12649047.0,13.0,0,59,Exclude site from Datadog automatic trace instrumentation on IIS,I've read the docs but didn't find anything about.
Datadog,66316616,nan,1,"2021/02/22, 15:28:17",False,"2021/02/25, 00:49:28",nan,1958324.0,1083.0,0,34,Remove particular type of error from Datadog,"Currently, I see error status for all the authentication errors and it feels like a lot of extra noise in the total errors chart."
Datadog,66316616,nan,1,"2021/02/22, 15:28:17",False,"2021/02/25, 00:49:28",nan,1958324.0,1083.0,0,34,Remove particular type of error from Datadog,I looked at  https://github.com/DataDog/dd-trace-js/pull/909  and tried to use the custom execute provided for graphql
Datadog,66316616,nan,1,"2021/02/22, 15:28:17",False,"2021/02/25, 00:49:28",nan,1958324.0,1083.0,0,34,Remove particular type of error from Datadog,"But still, res with only 403 error is going into error status."
Datadog,66316616,nan,1,"2021/02/22, 15:28:17",False,"2021/02/25, 00:49:28",nan,1958324.0,1083.0,0,34,Remove particular type of error from Datadog,Please help me with how can I achieve this.
Datadog,66282520,nan,2,"2021/02/19, 19:45:54",False,"2021/02/24, 19:43:52","2021/02/23, 12:02:30",1526115.0,10994.0,0,44,How to set line color on a timeseries in DataDog,"I'm setting up a timeseries to monitor system problems with the standard levels: Critical, Error, Warn, etc.."
Datadog,66282520,nan,2,"2021/02/19, 19:45:54",False,"2021/02/24, 19:43:52","2021/02/23, 12:02:30",1526115.0,10994.0,0,44,How to set line color on a timeseries in DataDog,I want to set the colors as follows:
Datadog,66282520,nan,2,"2021/02/19, 19:45:54",False,"2021/02/24, 19:43:52","2021/02/23, 12:02:30",1526115.0,10994.0,0,44,How to set line color on a timeseries in DataDog,I can't seem to do this.
Datadog,66282520,nan,2,"2021/02/19, 19:45:54",False,"2021/02/24, 19:43:52","2021/02/23, 12:02:30",1526115.0,10994.0,0,44,How to set line color on a timeseries in DataDog,"There is a color selection drop-down, but options aren't colors... they're themes, like &quot;Classic,&quot; &quot;Cool,&quot; &quot;Warm,&quot; etc."
Datadog,66282520,nan,2,"2021/02/19, 19:45:54",False,"2021/02/24, 19:43:52","2021/02/23, 12:02:30",1526115.0,10994.0,0,44,How to set line color on a timeseries in DataDog,How do I set the color on a line in a time series?
Datadog,66271979,nan,1,"2021/02/19, 06:53:16",False,"2021/02/24, 19:20:38",nan,12543947.0,29.0,0,33,How to send datadog event stream data into datadog logs?,"Actually, we have two datadog accounts: Let me consider it has account A and account B.
when I push the message to data dog event using API I am able to see the events in events stream and I am able to see the same thing in logs also in the account A."
Datadog,66271979,nan,1,"2021/02/19, 06:53:16",False,"2021/02/24, 19:20:38",nan,12543947.0,29.0,0,33,How to send datadog event stream data into datadog logs?,But when I do the same thing in account B I am able to see the data in event stream but not in logs.
Datadog,66271979,nan,1,"2021/02/19, 06:53:16",False,"2021/02/24, 19:20:38",nan,12543947.0,29.0,0,33,How to send datadog event stream data into datadog logs?,can I know what might be the reason ???
Datadog,66271979,nan,1,"2021/02/19, 06:53:16",False,"2021/02/24, 19:20:38",nan,12543947.0,29.0,0,33,How to send datadog event stream data into datadog logs?,Am I missing something to enable ??
Datadog,66271979,nan,1,"2021/02/19, 06:53:16",False,"2021/02/24, 19:20:38",nan,12543947.0,29.0,0,33,How to send datadog event stream data into datadog logs?,if so can someone help me with this?
Datadog,66271979,nan,1,"2021/02/19, 06:53:16",False,"2021/02/24, 19:20:38",nan,12543947.0,29.0,0,33,How to send datadog event stream data into datadog logs?,BELOW CODE IS USED TO PUSH THE EVENT TO DATA EVENTS STREAM.
Datadog,65826349,nan,0,"2021/01/21, 13:23:24",False,"2021/01/21, 13:23:24",nan,5036360.0,370.0,0,43,How to escape the whole string in the DataDog Log Search query?,I query for a particular URL string like  https://docs.python.org/3/reference/datamodel.html  in my logs.
Datadog,65826349,nan,0,"2021/01/21, 13:23:24",False,"2021/01/21, 13:23:24",nan,5036360.0,370.0,0,43,How to escape the whole string in the DataDog Log Search query?,The URL contains lots of special reserved characters.
Datadog,65826349,nan,0,"2021/01/21, 13:23:24",False,"2021/01/21, 13:23:24",nan,5036360.0,370.0,0,43,How to escape the whole string in the DataDog Log Search query?,"Escaping each char as stated by  official doc  works, but requires too much hustle:"
Datadog,65826349,nan,0,"2021/01/21, 13:23:24",False,"2021/01/21, 13:23:24",nan,5036360.0,370.0,0,43,How to escape the whole string in the DataDog Log Search query?,https\:\/\/docs.python.org\/3\/reference\/datamodel.html
Datadog,65826349,nan,0,"2021/01/21, 13:23:24",False,"2021/01/21, 13:23:24",nan,5036360.0,370.0,0,43,How to escape the whole string in the DataDog Log Search query?,Is there any easier way to escape all special chars in the string?
Datadog,65511552,nan,0,"2020/12/30, 20:05:03",False,"2021/01/04, 10:31:49",nan,944768.0,1824.0,0,80,Datadog Trace with Grpc Kotlin Coroutines not working,As the title suggests  @Trace  annotation is not working with Kotlin Grpc Coroutines.
Datadog,65511552,nan,0,"2020/12/30, 20:05:03",False,"2021/01/04, 10:31:49",nan,944768.0,1824.0,0,80,Datadog Trace with Grpc Kotlin Coroutines not working,Is there a way to make it work?
Datadog,65511552,nan,0,"2020/12/30, 20:05:03",False,"2021/01/04, 10:31:49",nan,944768.0,1824.0,0,80,Datadog Trace with Grpc Kotlin Coroutines not working,Unfortunately this gives no error or warning.
Datadog,65511552,nan,0,"2020/12/30, 20:05:03",False,"2021/01/04, 10:31:49",nan,944768.0,1824.0,0,80,Datadog Trace with Grpc Kotlin Coroutines not working,Can I construct Trace programatically and would it work for a  suspend  function?
Datadog,65487235,nan,0,"2020/12/29, 06:19:37",False,"2020/12/29, 06:19:37",nan,3691191.0,141.0,0,14,how do I create monitor in datadog which compare the current with the max value of last month?,I want to create a monitor in datadog that fires an alert if the current value is larger than the maximum value of last month.
Datadog,65487235,nan,0,"2020/12/29, 06:19:37",False,"2020/12/29, 06:19:37",nan,3691191.0,141.0,0,14,how do I create monitor in datadog which compare the current with the max value of last month?,something like this:
Datadog,65487235,nan,0,"2020/12/29, 06:19:37",False,"2020/12/29, 06:19:37",nan,3691191.0,141.0,0,14,how do I create monitor in datadog which compare the current with the max value of last month?,The purpose of this monitor is to check if the monthly increment is larger than 15%.
Datadog,65487235,nan,0,"2020/12/29, 06:19:37",False,"2020/12/29, 06:19:37",nan,3691191.0,141.0,0,14,how do I create monitor in datadog which compare the current with the max value of last month?,anyone know if this is possible?
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,"I am using statsD ( hot-shots ) to try log events in datadog such as when a new item is created and by who but, when I am calling  statsD.event('title', 'description')  I do not see any events in datadog metrics."
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,My statsD client is setup like this:
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,and then I call the event method like so:
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,"In the metrics section of datadog, I do not appear to be seeing any events come through."
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,The only way I can see some logs there is using  increment
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,But increment only seems to tell me how many times that action is called and I am unable to log additional data such as username and alias.
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,I am pretty sure my statsD client and datadog configs are setup correctly as I am able to see data from increment so I suspect it is to do with the way I am trying to use the event method.
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,Am I using the event method incorrectly?
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,Perhaps I am checking in the wrong place in datadog?
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,Perhaps I should be using increment?
Datadog,65179367,nan,0,"2020/12/07, 11:47:27",False,"2020/12/07, 11:47:27",nan,3528590.0,5446.0,0,47,How to view statsD events in Datadog?,How can I log events along with associated data in datadog using statsD?
Datadog,65091266,nan,0,"2020/12/01, 15:20:27",False,"2020/12/01, 15:20:27",nan,41634.0,1585.0,0,10,How can I see traces going from one service to another in Datadog APM,"In Datadog AP, service map view, we can see a lines connecting services."
Datadog,65091266,nan,0,"2020/12/01, 15:20:27",False,"2020/12/01, 15:20:27",nan,41634.0,1585.0,0,10,How can I see traces going from one service to another in Datadog APM,Is there a way to filter the traces going from one service to the other?
Datadog,65091266,nan,0,"2020/12/01, 15:20:27",False,"2020/12/01, 15:20:27",nan,41634.0,1585.0,0,10,How can I see traces going from one service to another in Datadog APM,I've tried to &quot;Inspect&quot; one service and then click &quot;View related Traces&quot; on the other service but that returns all traces for the other service.
Datadog,64937350,nan,0,"2020/11/20, 23:45:42",False,"2020/11/23, 18:48:11","2020/11/23, 18:48:11",926190.0,2467.0,0,46,Can&#39;t install DataDog MSI in Dockerfile,"From everything I have read, I should be able to do this:"
Datadog,64937350,nan,0,"2020/11/20, 23:45:42",False,"2020/11/23, 18:48:11","2020/11/23, 18:48:11",926190.0,2467.0,0,46,Can&#39;t install DataDog MSI in Dockerfile,It looks like it built correctly:
Datadog,64937350,nan,0,"2020/11/20, 23:45:42",False,"2020/11/23, 18:48:11","2020/11/23, 18:48:11",926190.0,2467.0,0,46,Can&#39;t install DataDog MSI in Dockerfile,"But when I check in the container to see if it installed in  C:\Program Files\Datadog , I am not seeing any of the files I am expecting from the installation."
Datadog,64937350,nan,0,"2020/11/20, 23:45:42",False,"2020/11/23, 18:48:11","2020/11/23, 18:48:11",926190.0,2467.0,0,46,Can&#39;t install DataDog MSI in Dockerfile,I added the flag to give the install some extra logs (/L*V C:\install.log) but didn't see much in there.
Datadog,64937350,nan,0,"2020/11/20, 23:45:42",False,"2020/11/23, 18:48:11","2020/11/23, 18:48:11",926190.0,2467.0,0,46,Can&#39;t install DataDog MSI in Dockerfile,"I have confirmed the msiexec command works on a Windows host, just not in the Docker build from what I can tell."
Datadog,64937350,nan,0,"2020/11/20, 23:45:42",False,"2020/11/23, 18:48:11","2020/11/23, 18:48:11",926190.0,2467.0,0,46,Can&#39;t install DataDog MSI in Dockerfile,Is there something simple that I'm missing?
Datadog,64847038,nan,1,"2020/11/15, 18:43:14",True,"2020/11/15, 18:43:14",nan,252340.0,1072.0,0,267,How to log using NLog directly to Datadog,"Using Nlog to log from my asp.net Core application, I would like to review the logs in Datadog."
Datadog,64847038,nan,1,"2020/11/15, 18:43:14",True,"2020/11/15, 18:43:14",nan,252340.0,1072.0,0,267,How to log using NLog directly to Datadog,"Datadog allows me to visualize the log data, and slice, search, select and sort logs in a convenient way to provide support to my customers."
Datadog,64847038,nan,1,"2020/11/15, 18:43:14",True,"2020/11/15, 18:43:14",nan,252340.0,1072.0,0,267,How to log using NLog directly to Datadog,"I was looking for a way to use NLog to directly post to the Datadog API, so I do not need to use the Windows Agent to collect the logs."
Datadog,64847038,nan,1,"2020/11/15, 18:43:14",True,"2020/11/15, 18:43:14",nan,252340.0,1072.0,0,267,How to log using NLog directly to Datadog,"Below how to do this, as I could not find the answer anywhere."
Datadog,64835003,nan,0,"2020/11/14, 16:36:17",False,"2020/11/14, 16:36:17",nan,1744489.0,31.0,0,22,Communicate from Datadog Dashboard to an application running in AWS EC2,"I need to develop a Datadog dashboard which will monitor metrics, logs of the applications running in AWS EC2."
Datadog,64835003,nan,0,"2020/11/14, 16:36:17",False,"2020/11/14, 16:36:17",nan,1744489.0,31.0,0,22,Communicate from Datadog Dashboard to an application running in AWS EC2,At the same time i have some need to send some messages to Application from Datadog Dashboard.
Datadog,64835003,nan,0,"2020/11/14, 16:36:17",False,"2020/11/14, 16:36:17",nan,1744489.0,31.0,0,22,Communicate from Datadog Dashboard to an application running in AWS EC2,Is it possible to do that?
Datadog,64835003,nan,0,"2020/11/14, 16:36:17",False,"2020/11/14, 16:36:17",nan,1744489.0,31.0,0,22,Communicate from Datadog Dashboard to an application running in AWS EC2,If it is not what are the alternative i can use to achieve this.
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,I'm having trouble setting up monitor that will alert me when an event hasn't happened since some period of time following another event.
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,"Basically, for a given task in my application, I have a log that indicates a state of &quot;running&quot; and another log that indicates a state of &quot;finished&quot;."
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,"From these logs, I've defined two custom metrics in datadog."
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,I'm trying to set up a monitor that will alert me when a task has not finished within 2 hours of when it started running.
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,"So for example, if the running metric is observed at 2:00, the monitor shouldn't alert for the absence of the finished until 4:00."
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,"If the finished metric is observed before 4:00, the monitor will not alert for this task."
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,"The way that I've tried implementing this is by using a threshold monitor, and subtracting the count of my running metric from the count of my finished metric."
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,"However, what's challenging here is the time-delta piece."
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,"I've tried using the delay evaluation (delaying by 2 hrs), however, at the time when it starts evaluating, it will only take into account the first metric."
Datadog,64528428,nan,0,"2020/10/25, 22:26:35",False,"2020/10/25, 22:26:35",nan,12534182.0,11.0,0,62,Datadog monitor to alert when one metric hasn&#39;t happened within a period of time since another metric,"It basically, just slides the window back."
Datadog,64476688,64484665.0,1,"2020/10/22, 09:44:41",True,"2020/10/22, 17:40:57",nan,3749845.0,3.0,0,40,Is it possible to take action on an application using datadog or Prometheus?,I want to take some action on the app server based on the server utilisation.
Datadog,64476688,64484665.0,1,"2020/10/22, 09:44:41",True,"2020/10/22, 17:40:57",nan,3749845.0,3.0,0,40,Is it possible to take action on an application using datadog or Prometheus?,The monitoring on the server is done by datadog.
Datadog,64476688,64484665.0,1,"2020/10/22, 09:44:41",True,"2020/10/22, 17:40:57",nan,3749845.0,3.0,0,40,Is it possible to take action on an application using datadog or Prometheus?,So is it possible to take an action on the server using datadog ?
Datadog,64108531,64407940.0,1,"2020/09/28, 22:35:34",True,"2020/10/18, 01:24:48","2020/09/28, 23:25:49",8916618.0,7.0,0,129,How do I configure this PostgreSQL check for DataDog?,I try to setup a postgres check using DD Agent and i'm getting an error thrown by postgres.py script.
Datadog,64108531,64407940.0,1,"2020/09/28, 22:35:34",True,"2020/10/18, 01:24:48","2020/09/28, 23:25:49",8916618.0,7.0,0,129,How do I configure this PostgreSQL check for DataDog?,"As you can see in the screenshot, i'm using this simple query to  get the number of active connections to a db."
Datadog,64108531,64407940.0,1,"2020/09/28, 22:35:34",True,"2020/10/18, 01:24:48","2020/09/28, 23:25:49",8916618.0,7.0,0,129,How do I configure this PostgreSQL check for DataDog?,I've put it inside the /etc/datadog-agent/conf.d/postgres.d/conf.yaml like this :
Datadog,64108531,64407940.0,1,"2020/09/28, 22:35:34",True,"2020/10/18, 01:24:48","2020/09/28, 23:25:49",8916618.0,7.0,0,129,How do I configure this PostgreSQL check for DataDog?,The error i get when i run a config check is the following :
Datadog,64108531,64407940.0,1,"2020/09/28, 22:35:34",True,"2020/10/18, 01:24:48","2020/09/28, 23:25:49",8916618.0,7.0,0,129,How do I configure this PostgreSQL check for DataDog?,If i understood correctly the conf.yaml file is used to call the postgres.py script with certain parameters.
Datadog,64108531,64407940.0,1,"2020/09/28, 22:35:34",True,"2020/10/18, 01:24:48","2020/09/28, 23:25:49",8916618.0,7.0,0,129,How do I configure this PostgreSQL check for DataDog?,"The postgres.py script can be found here :
 https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/postgres.py"
Datadog,63921730,nan,0,"2020/09/16, 17:01:04",False,"2020/09/16, 17:01:04",nan,5371984.0,59.0,0,29,Datadog latency breakdown chart - how to interpret?,I have a problem with reading a DataDog chart.
Datadog,63921730,nan,0,"2020/09/16, 17:01:04",False,"2020/09/16, 17:01:04",nan,5371984.0,59.0,0,29,Datadog latency breakdown chart - how to interpret?,In DataDog latency breakdown chart I see that one method call took 24.3s.
Datadog,63921730,nan,0,"2020/09/16, 17:01:04",False,"2020/09/16, 17:01:04",nan,5371984.0,59.0,0,29,Datadog latency breakdown chart - how to interpret?,"In that method I have DataDog scope logging (operation1, operation2 and operation3) - sum of these three operations is about 1.61% of total time while whole method took about 97.7%."
Datadog,63921730,nan,0,"2020/09/16, 17:01:04",False,"2020/09/16, 17:01:04",nan,5371984.0,59.0,0,29,Datadog latency breakdown chart - how to interpret?,Also SQL operations listed below have a low impact on that method.
Datadog,63921730,nan,0,"2020/09/16, 17:01:04",False,"2020/09/16, 17:01:04",nan,5371984.0,59.0,0,29,Datadog latency breakdown chart - how to interpret?,Where does that difference come from?
Datadog,63895962,nan,0,"2020/09/15, 08:59:04",False,"2020/09/15, 09:28:51","2020/09/15, 09:28:51",12999208.0,418.0,0,69,Monitoring Kafka with Datadog without DDAgent,We want to integrate our Kafka Server with our remote Datadog server.
Datadog,63895962,nan,0,"2020/09/15, 08:59:04",False,"2020/09/15, 09:28:51","2020/09/15, 09:28:51",12999208.0,418.0,0,69,Monitoring Kafka with Datadog without DDAgent,"Due some policies, we decide to use tunnel instead Datadog Agent."
Datadog,63895962,nan,0,"2020/09/15, 08:59:04",False,"2020/09/15, 09:28:51","2020/09/15, 09:28:51",12999208.0,418.0,0,69,Monitoring Kafka with Datadog without DDAgent,"We have set JMX port for each instance (3 Zk, 3 Brokers and 12 Kafka Connect Workers) with same format service background like this:"
Datadog,63895962,nan,0,"2020/09/15, 08:59:04",False,"2020/09/15, 09:28:51","2020/09/15, 09:28:51",12999208.0,418.0,0,69,Monitoring Kafka with Datadog without DDAgent,Each instance has its own port.
Datadog,63895962,nan,0,"2020/09/15, 08:59:04",False,"2020/09/15, 09:28:51","2020/09/15, 09:28:51",12999208.0,418.0,0,69,Monitoring Kafka with Datadog without DDAgent,We found that When we try to curl   curl localhost:19999\metrics  to test the JMX it returns empty which indicate we miss something to collect the JMX report
Datadog,63895962,nan,0,"2020/09/15, 08:59:04",False,"2020/09/15, 09:28:51","2020/09/15, 09:28:51",12999208.0,418.0,0,69,Monitoring Kafka with Datadog without DDAgent,"However, from  Datadog tutorial to integrate kafka with DD , they use jmxfetch which the instalation need DDAgent."
Datadog,63895962,nan,0,"2020/09/15, 08:59:04",False,"2020/09/15, 09:28:51","2020/09/15, 09:28:51",12999208.0,418.0,0,69,Monitoring Kafka with Datadog without DDAgent,We want to know if there's another alternatives to integrate Kafka Server into Datadog without Agent and rely on tunneling.
Datadog,63888511,nan,1,"2020/09/14, 19:40:28",True,"2020/09/16, 05:05:14",nan,1190203.0,2509.0,0,423,Terraform Datadog Query Is Invalid,I'm trying to test out creating a monitor for google pub sub and am getting an &quot;Invalid Query&quot; error.
Datadog,63888511,nan,1,"2020/09/14, 19:40:28",True,"2020/09/16, 05:05:14",nan,1190203.0,2509.0,0,423,Terraform Datadog Query Is Invalid,"This is the query text when i view source of another working monitor, so i'm confused as to why this isn't working."
Datadog,63888511,nan,1,"2020/09/14, 19:40:28",True,"2020/09/16, 05:05:14",nan,1190203.0,2509.0,0,423,Terraform Datadog Query Is Invalid,Error:   Error: error creating monitor: 400 Bad Request: {&quot;errors&quot;:[&quot;The value provided for parameter 'query' is invalid&quot;]}
Datadog,63888511,nan,1,"2020/09/14, 19:40:28",True,"2020/09/16, 05:05:14",nan,1190203.0,2509.0,0,423,Terraform Datadog Query Is Invalid,Terraform:
Datadog,63884785,nan,0,"2020/09/14, 15:55:35",False,"2020/09/14, 16:04:09","2020/09/14, 16:04:09",3729567.0,71.0,0,251,datadog parsing log values in nested json and doing mathematical operations,I am parsing my log statement like below
Datadog,63884785,nan,0,"2020/09/14, 15:55:35",False,"2020/09/14, 16:04:09","2020/09/14, 16:04:09",3729567.0,71.0,0,251,datadog parsing log values in nested json and doing mathematical operations,"I want to extract all values of &quot;ADD&quot;,&quot;UPDATE&quot; under &quot;XE&quot;; Add (sum) them up ; and convert it into a metric."
Datadog,63884785,nan,0,"2020/09/14, 15:55:35",False,"2020/09/14, 16:04:09","2020/09/14, 16:04:09",3729567.0,71.0,0,251,datadog parsing log values in nested json and doing mathematical operations,"Also the depth of ADD within XE can vary with logs and the number of add, update statements can be zero or more."
Datadog,63884785,nan,0,"2020/09/14, 15:55:35",False,"2020/09/14, 16:04:09","2020/09/14, 16:04:09",3729567.0,71.0,0,251,datadog parsing log values in nested json and doing mathematical operations,"I was successful in parsing and displaying values under JSON tree, but since the child depth and a number of occurrences vary, I am not able to achieve what I want."
Datadog,63884785,nan,0,"2020/09/14, 15:55:35",False,"2020/09/14, 16:04:09","2020/09/14, 16:04:09",3729567.0,71.0,0,251,datadog parsing log values in nested json and doing mathematical operations,Any help here would be highly appreciated.
Datadog,63779194,63779195.0,2,"2020/09/07, 17:06:52",True,"2020/12/02, 18:50:18",nan,285601.0,95.0,0,246,"In datadog, how do I query a json formatted log line, WITHOUT adding a facet?","When a message is formatted as json, it is automatically turned into attributes."
Datadog,63779194,63779195.0,2,"2020/09/07, 17:06:52",True,"2020/12/02, 18:50:18",nan,285601.0,95.0,0,246,"In datadog, how do I query a json formatted log line, WITHOUT adding a facet?","It seems like attributes cant be queried without first being turned into facets (which only applies to new log lines, and means you sometimes have to see something show up, then facetize it, then debug it)."
Datadog,63779194,63779195.0,2,"2020/09/07, 17:06:52",True,"2020/12/02, 18:50:18",nan,285601.0,95.0,0,246,"In datadog, how do I query a json formatted log line, WITHOUT adding a facet?","Is there a way to query the message directly, bypassing the attribute facet requirement?"
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,i'm using datadog-agent Agent 7.21.1.
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,Currently i'm working on gathering SNMP data from a Nimble storage device.
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,I already converted the mib file into Python format and i'm able to retrieve metrics using SNMP GET.
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,Inside /etc/datadog-agent/snmp.d/conf.yaml i've setup the following (Some values are censored):
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,DataDog retrieves the all the metrics until the last one &quot;volIoReads&quot;.
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,Using a cli tool i can read out the values using the OID:
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,Using the OID inside snmp conf it doesn't work either:
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,Tried also:
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,But no luck.
Datadog,63414590,nan,0,"2020/08/14, 17:20:27",False,"2020/08/14, 17:20:27",nan,10960667.0,56.0,0,98,DataDog - Can&#39;t read out all SNMP metrics,Does anybody know what's going on?
Datadog,63194517,nan,0,"2020/07/31, 18:11:03",False,"2020/07/31, 18:11:03",nan,4534442.0,37.0,0,31,Any feedback monitoring IBM Websphere WAS with the Datadog trace agent?,We have quite a lot of those endpoints and am trying to set my expectations before i start the POC.
Datadog,63194517,nan,0,"2020/07/31, 18:11:03",False,"2020/07/31, 18:11:03",nan,4534442.0,37.0,0,31,Any feedback monitoring IBM Websphere WAS with the Datadog trace agent?,Were the steps in the documentation enough to integrate the DDagent to the websphere startup ?
Datadog,63165346,nan,0,"2020/07/30, 05:39:45",False,"2020/07/30, 05:39:45",nan,3064090.0,43.0,0,97,Forward Jaeger traces to Datadog,"Is there a way of getting Jaeger traces to Datadog, whether it be through a proxy, scraping traces from Jager and converting them to DD Traces, etc..."
Datadog,63165346,nan,0,"2020/07/30, 05:39:45",False,"2020/07/30, 05:39:45",nan,3064090.0,43.0,0,97,Forward Jaeger traces to Datadog,"We have a vendor provided backend that only supports Jaeger, but the enterprise APM solution is Datadog."
Datadog,63165346,nan,0,"2020/07/30, 05:39:45",False,"2020/07/30, 05:39:45",nan,3064090.0,43.0,0,97,Forward Jaeger traces to Datadog,Thanks!
Datadog,62739543,62747357.0,1,"2020/07/05, 13:34:24",True,"2020/07/06, 02:24:28",nan,2505093.0,2334.0,0,235,NodeJS not posting POST body to DataDog logs,"I'm trying to integrate DataDog with my NodeJS/Express application, however it appears that when a POST request is sent to my app the body of the POST is not being passed along to datadog, how can I fix this?"
Datadog,62739543,62747357.0,1,"2020/07/05, 13:34:24",True,"2020/07/06, 02:24:28",nan,2505093.0,2334.0,0,235,NodeJS not posting POST body to DataDog logs,I have a file called  Winston.js  which looks like so:
Datadog,62739543,62747357.0,1,"2020/07/05, 13:34:24",True,"2020/07/06, 02:24:28",nan,2505093.0,2334.0,0,235,NodeJS not posting POST body to DataDog logs,And then I'm attaching them to my app using the following:
Datadog,62739543,62747357.0,1,"2020/07/05, 13:34:24",True,"2020/07/06, 02:24:28",nan,2505093.0,2334.0,0,235,NodeJS not posting POST body to DataDog logs,Currently my logs in DataDog look like this:
Datadog,62345348,nan,0,"2020/06/12, 16:33:11",False,"2020/06/12, 16:33:11",nan,3286108.0,41.0,0,70,DataDog events are auto-recovered,I created an event monitor that catches events with errors and notifies about the alert in a special messenger.
Datadog,62345348,nan,0,"2020/06/12, 16:33:11",False,"2020/06/12, 16:33:11",nan,3286108.0,41.0,0,70,DataDog events are auto-recovered,"Everything worked out for me, but I noticed that such alerts are auto-recovered on their own for some time."
Datadog,62345348,nan,0,"2020/06/12, 16:33:11",False,"2020/06/12, 16:33:11",nan,3286108.0,41.0,0,70,DataDog events are auto-recovered,As I understand it is because of this parameter:
Datadog,62345348,nan,0,"2020/06/12, 16:33:11",False,"2020/06/12, 16:33:11",nan,3286108.0,41.0,0,70,DataDog events are auto-recovered,"So, datadog catches event, then sets event-monitor in alert status, then wait 5min-48hours and if there are no new events, it is auto-recovered and set status from ""Alert"" to ""OK""."
Datadog,62345348,nan,0,"2020/06/12, 16:33:11",False,"2020/06/12, 16:33:11",nan,3286108.0,41.0,0,70,DataDog events are auto-recovered,It absolutely does not suit me.
Datadog,62345348,nan,0,"2020/06/12, 16:33:11",False,"2020/06/12, 16:33:11",nan,3286108.0,41.0,0,70,DataDog events are auto-recovered,"Can I somehow configure the monitor that the monitor's status does not change automatically from ""Alert"" to ""OK"" until I change it manually?"
Datadog,62094698,nan,1,"2020/05/30, 00:16:40",False,"2020/06/06, 10:54:35",nan,2705849.0,653.0,0,64,Datadog Metric Monitor alert threshold to custom time period?,I want to set a threshold alert for my datadog metric monitor trigger it after 12 hours.
Datadog,62094698,nan,1,"2020/05/30, 00:16:40",False,"2020/06/06, 10:54:35",nan,2705849.0,653.0,0,64,Datadog Metric Monitor alert threshold to custom time period?,However none of the options are 12 hours.
Datadog,62094698,nan,1,"2020/05/30, 00:16:40",False,"2020/06/06, 10:54:35",nan,2705849.0,653.0,0,64,Datadog Metric Monitor alert threshold to custom time period?,I cant seem to add it in.
Datadog,61971889,62008387.0,1,"2020/05/23, 15:17:53",True,"2020/05/25, 21:21:06",nan,842302.0,2771.0,0,132,How to monitor HTTP status codes with serverless datadog plugin,"I am using  serverless-plugin-datadog , which uses  datadog-lambda-layer  under the hood."
Datadog,61971889,62008387.0,1,"2020/05/23, 15:17:53",True,"2020/05/25, 21:21:06",nan,842302.0,2771.0,0,132,How to monitor HTTP status codes with serverless datadog plugin,"The  docs state , that by using this plugin it is not necessary to wrap a handler anymore."
Datadog,61971889,62008387.0,1,"2020/05/23, 15:17:53",True,"2020/05/25, 21:21:06",nan,842302.0,2771.0,0,132,How to monitor HTTP status codes with serverless datadog plugin,"This is, by the way, the main reason why I decided to go for it."
Datadog,61971889,62008387.0,1,"2020/05/23, 15:17:53",True,"2020/05/25, 21:21:06",nan,842302.0,2771.0,0,132,How to monitor HTTP status codes with serverless datadog plugin,"The lambda itself is a REST API, which responds with dedicated status codes."
Datadog,61971889,62008387.0,1,"2020/05/23, 15:17:53",True,"2020/05/25, 21:21:06",nan,842302.0,2771.0,0,132,How to monitor HTTP status codes with serverless datadog plugin,"My question now is, how can I monitor the number of  4xx  and  5xx  http status codes?"
Datadog,61971889,62008387.0,1,"2020/05/23, 15:17:53",True,"2020/05/25, 21:21:06",nan,842302.0,2771.0,0,132,How to monitor HTTP status codes with serverless datadog plugin,Do I have to define custom metrics in datadog for this to work?
Datadog,61971889,62008387.0,1,"2020/05/23, 15:17:53",True,"2020/05/25, 21:21:06",nan,842302.0,2771.0,0,132,How to monitor HTTP status codes with serverless datadog plugin,"I was under the assumption that the plugin comes with those data out-of-the-box, but it looks like I'm missing an important part here."
Datadog,61817555,nan,1,"2020/05/15, 13:55:46",True,"2020/05/15, 18:27:58",nan,12781011.0,73.0,0,628,How to send log to Datadog without Datadog agent,"In my understand, usual case is using Datadog agent to send error to Datadog."
Datadog,61817555,nan,1,"2020/05/15, 13:55:46",True,"2020/05/15, 18:27:58",nan,12781011.0,73.0,0,628,How to send log to Datadog without Datadog agent,"However, I'd like to know there are some ways to send error to Datagog without Datadog agent."
Datadog,61817555,nan,1,"2020/05/15, 13:55:46",True,"2020/05/15, 18:27:58",nan,12781011.0,73.0,0,628,How to send log to Datadog without Datadog agent,"For example, can we send by using Datadog webhooks?"
Datadog,61511271,61513379.0,1,"2020/04/29, 23:40:27",True,"2020/04/30, 02:25:23",nan,1048185.0,1790.0,0,116,What is causing DataDog widgets to display http status code as N/A,"My organization is setting up dashboards for our backend services and after performance testing that we ran, we have noticed that some API calls report http status  N\A ."
Datadog,61511271,61513379.0,1,"2020/04/29, 23:40:27",True,"2020/04/30, 02:25:23",nan,1048185.0,1790.0,0,116,What is causing DataDog widgets to display http status code as N/A,"It is not very helpful, anyone seen something like that?"
Datadog,61511271,61513379.0,1,"2020/04/29, 23:40:27",True,"2020/04/30, 02:25:23",nan,1048185.0,1790.0,0,116,What is causing DataDog widgets to display http status code as N/A,Is that a configuration issue?
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog","we are facing a problem using NUnit, Serilog, and Datadog."
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",We are working with:
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",All packages are NuGet lastest.
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",This is the Serilog configuration we are using:
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",We are testing this configuration both in debug run (F5 key in Visual Studio) and under the NUnit test environment (in Visual Studio).
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",The problem we are facing is that while in debug run all work fine:
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",when we run this code in the NUnit environment:
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",but  no logs arrive at Datadog .
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog","Checking the network stream with Fiddler we notice that while in debug run, logs are sent to Datadog, under the NUnit environment  logs are NOT sent to Datadog ."
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",Any ideas or suggestions?
Datadog,61405563,nan,2,"2020/04/24, 12:48:45",False,"2020/12/30, 10:52:27",nan,5666309.0,365.0,0,551,"NUnit, Serilog and Datadog",Thank you
Datadog,61191225,nan,2,"2020/04/13, 18:30:08",True,"2020/04/15, 18:18:06",nan,9510770.0,80.0,0,129,How to get the metrics collected on datadog monitor using Java code?,I need to monitor fifty application.
Datadog,61191225,nan,2,"2020/04/13, 18:30:08",True,"2020/04/15, 18:18:06",nan,9510770.0,80.0,0,129,How to get the metrics collected on datadog monitor using Java code?,As apart of which I need to perform healthcheck on datadog dashboard to all the application everyday.
Datadog,61191225,nan,2,"2020/04/13, 18:30:08",True,"2020/04/15, 18:18:06",nan,9510770.0,80.0,0,129,How to get the metrics collected on datadog monitor using Java code?,"So, Is it possible to collect the metrics collected in datadog from Java code
.."
Datadog,61191225,nan,2,"2020/04/13, 18:30:08",True,"2020/04/15, 18:18:06",nan,9510770.0,80.0,0,129,How to get the metrics collected on datadog monitor using Java code?,Thanks in advance.
Datadog,60828990,60829558.0,1,"2020/03/24, 12:16:54",True,"2020/03/24, 18:07:20","2020/03/24, 12:25:55",4409319.0,3151.0,0,105,Datadog get logs without facet,"I want to filter logs that either don't have a facet, say half of my logs have some  @facet  but I want the other half"
Datadog,60828990,60829558.0,1,"2020/03/24, 12:16:54",True,"2020/03/24, 18:07:20","2020/03/24, 12:25:55",4409319.0,3151.0,0,105,Datadog get logs without facet,"I tried  -@facet ,  @facet:""""  and  NOT @facet   but doesn't work and google doesn't help"
Datadog,60828990,60829558.0,1,"2020/03/24, 12:16:54",True,"2020/03/24, 18:07:20","2020/03/24, 12:25:55",4409319.0,3151.0,0,105,Datadog get logs without facet,"Feels like there is an easy way for doing this, halp"
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,I'm currently running a spring boot application as container into a kubernetes cluster.
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,Datadog agent is running as containers on the cluster.
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,I have modified the container image build to include the datadog agent before running the application:
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,I also setup the environment variable to indicate the HOST IP of the agent to my container via the Deployment file.
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,The problem now is i'm getting this class not found exception when the application starts:
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,"Quite straigtforward, i need to include some dependencies into the application package."
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,But i could not find anything useful on datadog website nor maven central repository.
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,Including the agent itself or the api libraries fix nothing.
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,This class is present on the agent but under a different path.
Datadog,60715172,nan,1,"2020/03/17, 03:33:22",False,"2020/04/09, 00:50:54","2020/03/17, 03:49:37",1687162.0,1332.0,0,921,Datadog instrumentation on Spring boot container application - class not found,Does anybody know which dependencies should be included in the classpath of the application to fix that ?
Datadog,60615996,nan,1,"2020/03/10, 12:46:31",True,"2020/03/12, 21:36:18",nan,1482709.0,12669.0,0,620,How to import Datadog JSON template in terraform DSL?,I have the above datadog json template with me which I have to just import in terraform instead of recreating it as terraform dsl.
Datadog,60326963,nan,1,"2020/02/20, 21:07:43",False,"2020/10/20, 04:12:43",nan,4579896.0,490.0,0,359,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,"Okay, here is my setup:"
Datadog,60326963,nan,1,"2020/02/20, 21:07:43",False,"2020/10/20, 04:12:43",nan,4579896.0,490.0,0,359,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,"Question: So I am running Ubuntu 18.04LTS instances and as time goes on, it seems to spawn additional devices periodically:"
Datadog,60326963,nan,1,"2020/02/20, 21:07:43",False,"2020/10/20, 04:12:43",nan,4579896.0,490.0,0,359,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,"device:/dev/loop1, /dev/loop2 and so on."
Datadog,60326963,nan,1,"2020/02/20, 21:07:43",False,"2020/10/20, 04:12:43",nan,4579896.0,490.0,0,359,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,"When I first spun up these instances, there were only 3 /dev/loop(1-3) devices, however, over time, a /dev/loop4 showed up and our drive space alert paged me since these are 100% utilized when created."
Datadog,60326963,nan,1,"2020/02/20, 21:07:43",False,"2020/10/20, 04:12:43",nan,4579896.0,490.0,0,359,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,"So, I have to go into each of the monitors (one per environment) and add an exclusion for the new /dev/loop4, but I cannot set the exclusion until it has been created by at least one of the monitored instances."
Datadog,60326963,nan,1,"2020/02/20, 21:07:43",False,"2020/10/20, 04:12:43",nan,4579896.0,490.0,0,359,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,Is there a way in DataDog that you can just add a blanket exclusion like:
Datadog,60326963,nan,1,"2020/02/20, 21:07:43",False,"2020/10/20, 04:12:43",nan,4579896.0,490.0,0,359,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,device:/dev/loop*?
Datadog,60326963,nan,1,"2020/02/20, 21:07:43",False,"2020/10/20, 04:12:43",nan,4579896.0,490.0,0,359,Datadog AWS EBS Monitoring - Drive Space - Exclude /dev/loop* devices,"I have been combing through documentation and have not been able to find anything, so I thought I would ask here."
Datadog,59757258,nan,1,"2020/01/15, 20:08:10",True,"2020/01/15, 20:38:32",nan,47508.0,4471.0,0,150,Datadog Create custom metric from another custom metric,"I have a datadog count metric that I want to create a new metric from which shows the difference between two agent points on the metric, so I can see the change between points."
Datadog,59757258,nan,1,"2020/01/15, 20:08:10",True,"2020/01/15, 20:38:32",nan,47508.0,4471.0,0,150,Datadog Create custom metric from another custom metric,Is there a way to create a metric from another metric using the datadog dashboard.
Datadog,59177043,nan,2,"2019/12/04, 15:20:57",True,"2019/12/04, 16:24:18",nan,2277437.0,2865.0,0,612,How to set cluster name in DataDog Helm chart,I'm using the DataDog Helm chart to install the DataDog agent on my EKS Kubernetes clusters ( https://github.com/helm/charts/tree/master/stable/datadog ).
Datadog,59177043,nan,2,"2019/12/04, 15:20:57",True,"2019/12/04, 16:24:18",nan,2277437.0,2865.0,0,612,How to set cluster name in DataDog Helm chart,The problem I'm having now is that I am not able to filter logs by cluster name.
Datadog,59177043,nan,2,"2019/12/04, 15:20:57",True,"2019/12/04, 16:24:18",nan,2277437.0,2865.0,0,612,How to set cluster name in DataDog Helm chart,I have also set the  DD_CLUSTER_NAME  environment variable but it does not seem to do anything.
Datadog,59177043,nan,2,"2019/12/04, 15:20:57",True,"2019/12/04, 16:24:18",nan,2277437.0,2865.0,0,612,How to set cluster name in DataDog Helm chart,I have set the following in my values.yml file:
Datadog,59088171,nan,1,"2019/11/28, 13:46:24",False,"2020/01/15, 10:05:27",nan,2277437.0,2865.0,0,491,DataDog how to disable Redis integration,I've installed the DataDog agent on my Kubernetes cluster using the Helm chart ( https://github.com/helm/charts/tree/master/stable/datadog ).
Datadog,59088171,nan,1,"2019/11/28, 13:46:24",False,"2020/01/15, 10:05:27",nan,2277437.0,2865.0,0,491,DataDog how to disable Redis integration,This works very well except for one thing.
Datadog,59088171,nan,1,"2019/11/28, 13:46:24",False,"2020/01/15, 10:05:27",nan,2277437.0,2865.0,0,491,DataDog how to disable Redis integration,I have a number of Redis containers that have passwords set.
Datadog,59088171,nan,1,"2019/11/28, 13:46:24",False,"2020/01/15, 10:05:27",nan,2277437.0,2865.0,0,491,DataDog how to disable Redis integration,This seems to be causing issues for the DataDog agent because it can't connect to Redis without a password.
Datadog,59088171,nan,1,"2019/11/28, 13:46:24",False,"2020/01/15, 10:05:27",nan,2277437.0,2865.0,0,491,DataDog how to disable Redis integration,I would like to either disable monitoring Redis completely or somehow bypass the Redis authentication.
Datadog,59088171,nan,1,"2019/11/28, 13:46:24",False,"2020/01/15, 10:05:27",nan,2277437.0,2865.0,0,491,DataDog how to disable Redis integration,If I leave it as is I get a lot of error messages in the DataDog container logs and the redisdb integration shows up in yellow in the DataDog dashboard.
Datadog,59088171,nan,1,"2019/11/28, 13:46:24",False,"2020/01/15, 10:05:27",nan,2277437.0,2865.0,0,491,DataDog how to disable Redis integration,What are my options here?
Datadog,58694140,nan,1,"2019/11/04, 15:11:29",True,"2019/11/04, 16:13:25",nan,12320061.0,1.0,0,183,Checking Datadog agent versions installed on AWS EC2 Instances,Recent Tenable scan highlighted an issue with certain versions of datadog versions.
Datadog,58694140,nan,1,"2019/11/04, 15:11:29",True,"2019/11/04, 16:13:25",nan,12320061.0,1.0,0,183,Checking Datadog agent versions installed on AWS EC2 Instances,This is also brought to attention in Datadog monitor.
Datadog,58694140,nan,1,"2019/11/04, 15:11:29",True,"2019/11/04, 16:13:25",nan,12320061.0,1.0,0,183,Checking Datadog agent versions installed on AWS EC2 Instances,Critical bug in Windows Agent versions 6.14.0 and 6.14.1.
Datadog,58694140,nan,1,"2019/11/04, 15:11:29",True,"2019/11/04, 16:13:25",nan,12320061.0,1.0,0,183,Checking Datadog agent versions installed on AWS EC2 Instances,See --   http://dtdg.co/win-614-fix  &lt;-- for steps to fix the issue.
Datadog,58694140,nan,1,"2019/11/04, 15:11:29",True,"2019/11/04, 16:13:25",nan,12320061.0,1.0,0,183,Checking Datadog agent versions installed on AWS EC2 Instances,As the bulk of our servers are hosted on AWS - just wondered if I could query this through AWS CLI to list which servers were using the affected versions.
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,"I want to disable all builtin metrics (jvm, cpu, etc) but keep my custom metrics."
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,When I enabled Spring Boot Actuator metrics together with Datadog I end up with +320 metrics sent to datadog.
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,"Most of these metrics are from the  builtin core metrics  (JVM metrics, CPU metrics, File description metrics) only 5 of those metrics are my custom metrics that are the ones that I want to send to datadog."
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,According to  this section of the Spring Boot documentation :
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,Spring Boot also configures built-in instrumentation (i.e.
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,"MeterBinder
  implementations) that  you can control via configuration or dedicated
  annotation markers"
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,but there is no direct example on how to exclude the those metrics
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,From what I found in  this other SO question  one way to control it is:
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,and that removes all the metrics except the JVM ones.
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,But it  also removes my custom metrics .
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,I don't see how can I reenable my custom metrics.
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,Just for the record the way I register the custom metrics is this way:
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,"This works ok, as long as `management.metrics.enable.all=true"
Datadog,58641893,nan,1,"2019/10/31, 12:59:02",False,"2019/10/31, 17:55:11",nan,90580.0,14987.0,0,378,How to remove the default core metrics from spring boot metrics (datadog)?,"So how can I disable all core metrics , but keep my custom metrics?"
Datadog,58607248,nan,1,"2019/10/29, 14:12:01",False,"2019/11/04, 10:54:36","2019/10/29, 18:11:06",11680916.0,1.0,0,307,Implement opentracing in Spring Boot for Datadog,I need to implement tracing opentracing (opentelementary) for datadog in my Spring Boot application with rest controller.
Datadog,58607248,nan,1,"2019/10/29, 14:12:01",False,"2019/11/04, 10:54:36","2019/10/29, 18:11:06",11680916.0,1.0,0,307,Implement opentracing in Spring Boot for Datadog,"I have a given kubernetes endpoint, to which I should send traces."
Datadog,58579323,nan,1,"2019/10/27, 14:13:35",True,"2021/02/05, 00:01:35",nan,6493646.0,131.0,0,953,Custom metrics not being sent to datadog,I am running the datadog agent using docker
Datadog,58579323,nan,1,"2019/10/27, 14:13:35",True,"2021/02/05, 00:01:35",nan,6493646.0,131.0,0,953,Custom metrics not being sent to datadog,I want to send custom metrics using dogstatsd.
Datadog,58579323,nan,1,"2019/10/27, 14:13:35",True,"2021/02/05, 00:01:35",nan,6493646.0,131.0,0,953,Custom metrics not being sent to datadog,When I run
Datadog,58579323,nan,1,"2019/10/27, 14:13:35",True,"2021/02/05, 00:01:35",nan,6493646.0,131.0,0,953,Custom metrics not being sent to datadog,I can see in wireshark that the udp packet was successful from the source to the destination but this metric is not being submitted to datadog.
Datadog,58579323,nan,1,"2019/10/27, 14:13:35",True,"2021/02/05, 00:01:35",nan,6493646.0,131.0,0,953,Custom metrics not being sent to datadog,Am I missing some configuration?
Datadog,58502004,58547485.0,1,"2019/10/22, 13:16:10",True,"2019/11/04, 05:49:30","2019/11/04, 05:49:30",4867627.0,135.0,0,130,Datadog alert when Amazon RDS is created,I have an alert in Datadog when CPU Credits are low.
Datadog,58502004,58547485.0,1,"2019/10/22, 13:16:10",True,"2019/11/04, 05:49:30","2019/11/04, 05:49:30",4867627.0,135.0,0,130,Datadog alert when Amazon RDS is created,"The problem is when I create a new RDS in Amazon, initially it has 0 CPU credits and I receive this alert."
Datadog,58502004,58547485.0,1,"2019/10/22, 13:16:10",True,"2019/11/04, 05:49:30","2019/11/04, 05:49:30",4867627.0,135.0,0,130,Datadog alert when Amazon RDS is created,How can I avoid this case?
Datadog,58502004,58547485.0,1,"2019/10/22, 13:16:10",True,"2019/11/04, 05:49:30","2019/11/04, 05:49:30",4867627.0,135.0,0,130,Datadog alert when Amazon RDS is created,"I tried to find ""time since creation"" metric, but with no success."
Datadog,58210228,nan,0,"2019/10/03, 01:42:35",False,"2019/10/03, 01:42:35",nan,5634191.0,751.0,0,50,Datadog string encoding when character is not from English alphabet,I have a service and send Datadog events from it using com.github.arnabk.java-dogstatsd-client.
Datadog,58210228,nan,0,"2019/10/03, 01:42:35",False,"2019/10/03, 01:42:35",nan,5634191.0,751.0,0,50,Datadog string encoding when character is not from English alphabet,In order to send json string I use  JsonObject  where I put all properties which I need then convert it to string using  toString()  method on  JsonObject  and send string as a message body.
Datadog,58210228,nan,0,"2019/10/03, 01:42:35",False,"2019/10/03, 01:42:35",nan,5634191.0,751.0,0,50,Datadog string encoding when character is not from English alphabet,Everything works perfect unless I have a character in a string which is not from english alphabet.
Datadog,58210228,nan,0,"2019/10/03, 01:42:35",False,"2019/10/03, 01:42:35",nan,5634191.0,751.0,0,50,Datadog string encoding when character is not from English alphabet,Example: µ.
Datadog,58210228,nan,0,"2019/10/03, 01:42:35",False,"2019/10/03, 01:42:35",nan,5634191.0,751.0,0,50,Datadog string encoding when character is not from English alphabet,"In this case instead of having correct json  {""Smth"":""µ""}  in Datadog I'm getting incorrect string without closing curly brace  {""Smth"":""µ"" ."
Datadog,58210228,nan,0,"2019/10/03, 01:42:35",False,"2019/10/03, 01:42:35",nan,5634191.0,751.0,0,50,Datadog string encoding when character is not from English alphabet,Has anybody experienced the same and knows how to deal with this?
Datadog,55137456,nan,0,"2019/03/13, 10:36:36",False,"2019/03/17, 12:39:12","2019/03/17, 12:39:12",6552837.0,41.0,0,825,Filter datadog logs in the local agent before sending,"I use datadog agent 6.9 that run on my host(not on a docker), 
and i also run several application on my host (docker images)."
Datadog,55137456,nan,0,"2019/03/13, 10:36:36",False,"2019/03/17, 12:39:12","2019/03/17, 12:39:12",6552837.0,41.0,0,825,Filter datadog logs in the local agent before sending,I try to avoid sending specific logs to the datadoghq from my mongodb.
Datadog,55137456,nan,0,"2019/03/13, 10:36:36",False,"2019/03/17, 12:39:12","2019/03/17, 12:39:12",6552837.0,41.0,0,825,Filter datadog logs in the local agent before sending,"So according to  https://docs.datadoghq.com/logs/log_collection/?tab=tailexistingfiles 
I create mongo.d directory and conf.yaml inside that look like:"
Datadog,55137456,nan,0,"2019/03/13, 10:36:36",False,"2019/03/17, 12:39:12","2019/03/17, 12:39:12",6552837.0,41.0,0,825,Filter datadog logs in the local agent before sending,But when i restart my agent it's still send the unwanted logs to my datadoghq.
Datadog,55137456,nan,0,"2019/03/13, 10:36:36",False,"2019/03/17, 12:39:12","2019/03/17, 12:39:12",6552837.0,41.0,0,825,Filter datadog logs in the local agent before sending,"Thanks in advance for the help,
Baruch"
Datadog,53222715,53935650.0,1,"2018/11/09, 11:08:09",True,"2018/12/26, 20:21:57",nan,2650254.0,347.0,0,72,Monitor Aurora using Datadog doesn&#39;t show changes in Query-volume,I'm using  Datadog dashboard to monitor Aurora clusters  I have in my account.
Datadog,53222715,53935650.0,1,"2018/11/09, 11:08:09",True,"2018/12/26, 20:21:57",nan,2650254.0,347.0,0,72,Monitor Aurora using Datadog doesn&#39;t show changes in Query-volume,"The "" query-volume "" section is always empty, even if I go the mysql shell and do a couple of selects."
Datadog,53222715,53935650.0,1,"2018/11/09, 11:08:09",True,"2018/12/26, 20:21:57",nan,2650254.0,347.0,0,72,Monitor Aurora using Datadog doesn&#39;t show changes in Query-volume,I'd like to make sure it works before I put high load on my db in production.
Datadog,53222715,53935650.0,1,"2018/11/09, 11:08:09",True,"2018/12/26, 20:21:57",nan,2650254.0,347.0,0,72,Monitor Aurora using Datadog doesn&#39;t show changes in Query-volume,for now I only see changes in query-volume section in the charts of  Select latency and DML latency  and in AWS resource metrics in all charts.
Datadog,53222715,53935650.0,1,"2018/11/09, 11:08:09",True,"2018/12/26, 20:21:57",nan,2650254.0,347.0,0,72,Monitor Aurora using Datadog doesn&#39;t show changes in Query-volume,"whereas DiskIO section is totally empty, ( Connection and Replication is empty as well, but I know that because I don't have a replica )"
Datadog,53222715,53935650.0,1,"2018/11/09, 11:08:09",True,"2018/12/26, 20:21:57",nan,2650254.0,347.0,0,72,Monitor Aurora using Datadog doesn&#39;t show changes in Query-volume,Any idea how can I make sure it works?
Datadog,51707255,51871692.0,1,"2018/08/06, 15:05:42",True,"2018/08/16, 10:24:11",nan,99900.0,12626.0,0,105,DataDog Agent Upgrade on Azure Cloud Service,I'm running an Azure Cloud Service with a WebRole.
Datadog,51707255,51871692.0,1,"2018/08/06, 15:05:42",True,"2018/08/16, 10:24:11",nan,99900.0,12626.0,0,105,DataDog Agent Upgrade on Azure Cloud Service,"We run the DataDog Agent on each of our server instances, by running a startup task that executes a .cmd file."
Datadog,51707255,51871692.0,1,"2018/08/06, 15:05:42",True,"2018/08/16, 10:24:11",nan,99900.0,12626.0,0,105,DataDog Agent Upgrade on Azure Cloud Service,"Previously we have been using the latest version of DataDog Agent 5, and installing it using this -"
Datadog,51707255,51871692.0,1,"2018/08/06, 15:05:42",True,"2018/08/16, 10:24:11",nan,99900.0,12626.0,0,105,DataDog Agent Upgrade on Azure Cloud Service,"Now we are trying to upgrade to the latest version of DataDog Agent 6 using this, which is failing to install and register the instance as an available host in DataDogs dashboard -"
Datadog,51707255,51871692.0,1,"2018/08/06, 15:05:42",True,"2018/08/16, 10:24:11",nan,99900.0,12626.0,0,105,DataDog Agent Upgrade on Azure Cloud Service,The URL is of course different in each case.
Datadog,47436429,50785165.0,1,"2017/11/22, 15:52:13",True,"2018/06/10, 18:22:21",nan,3605831.0,518.0,0,337,Symfony 2.8 - Datadog audit bundle retrieve entity,I used the  DataDog Audit bundle  in order to log every action that happens in my MySQL database.
Datadog,47436429,50785165.0,1,"2017/11/22, 15:52:13",True,"2018/06/10, 18:22:21",nan,3605831.0,518.0,0,337,Symfony 2.8 - Datadog audit bundle retrieve entity,However when I check the diff column in the audit_log table I can't find the ID of the respective entity that have been updated/inserted/deleted etc.
Datadog,47436429,50785165.0,1,"2017/11/22, 15:52:13",True,"2018/06/10, 18:22:21",nan,3605831.0,518.0,0,337,Symfony 2.8 - Datadog audit bundle retrieve entity,I also can't find which user is responsible for a certain action.
Datadog,47436429,50785165.0,1,"2017/11/22, 15:52:13",True,"2018/06/10, 18:22:21",nan,3605831.0,518.0,0,337,Symfony 2.8 - Datadog audit bundle retrieve entity,Does anyone know if the DataDog audit bundle saves the ID of the entity to which action are performed and if this is the case where I can retrieve this data?
Datadog,41578245,nan,1,"2017/01/10, 22:55:40",False,"2017/11/14, 22:22:41","2017/01/10, 23:18:53",7401417.0,1.0,0,548,Datadog: Slow queries from MongoDB,I have a MongoDB using the database profiler to collect the slowest queries.
Datadog,41578245,nan,1,"2017/01/10, 22:55:40",False,"2017/11/14, 22:22:41","2017/01/10, 23:18:53",7401417.0,1.0,0,548,Datadog: Slow queries from MongoDB,How can I send this information to Datadog and analyze it in my Datadog dashboard?
Datadog,67109633,nan,0,"2021/04/15, 16:42:18",False,"2021/04/15, 16:42:18",nan,9813109.0,1.0,0,8,datadog &quot;ignored&quot; timestamp in metrics,"i have some problem to settings my dashboard metrics in datadog, the case is about current connection of my apps, for example when there is user connected my app the value goes add by 1, but when its disconnected it will reduced the value by 1. the problem when im using datadog, they will evaluate based on timestamp, so for example if i want to check per 5 minutes, when first 5 minutes there is 10 users connected it will add by 10 the monitoring show 10, it should not be a problem, but the problem when the next 5 minutes when there is 5 disconnected users, it will reduce the value by 5, and it should be  5  not  -5 ."
Datadog,67109633,nan,0,"2021/04/15, 16:42:18",False,"2021/04/15, 16:42:18",nan,9813109.0,1.0,0,8,datadog &quot;ignored&quot; timestamp in metrics,is there any function that i used to somehow ignore the timestamp in datadog ?
Datadog,67109633,nan,0,"2021/04/15, 16:42:18",False,"2021/04/15, 16:42:18",nan,9813109.0,1.0,0,8,datadog &quot;ignored&quot; timestamp in metrics,additional information with last case i mention earlier if the next 5 hours there is no user that connected / disconnected again it should be show as 5 users regardless what time batch series i take.
Datadog,67109633,nan,0,"2021/04/15, 16:42:18",False,"2021/04/15, 16:42:18",nan,9813109.0,1.0,0,8,datadog &quot;ignored&quot; timestamp in metrics,is that possible to do that in datadog ?
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,I am trying to set up hazelcast metrics pushed to datadog.
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,I followed below documents.
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,DD for HazelCast
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,This is what I did:
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,"Now, I have to do same in ci pipeline in my org."
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,So I have to pass these annotation in --set
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,You can see it is little bit complicated:
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,It shows pods running with annotations like this
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,What I also tried:
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,I see pods running but no metrics in Datadog.
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,I am 99% sure that json is screwing things up.
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,Also I tried jq.
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,"where,"
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,and
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,and I got
Datadog,67027425,nan,0,"2021/04/09, 22:48:23",False,"2021/04/09, 23:37:09","2021/04/09, 23:37:09",4987963.0,101.0,0,22,helm --set with json values for datadog/hazelcast,Any help is appreciated.
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,We have a requirement where we need to send airflow metrics to datadog.
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,"I tried to follow the steps mentioned here
 https://docs.datadoghq.com/integrations/airflow/?tab=host"
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,"Likewise, I included statsD in airflow installation and updated the airflow configuration file (Steps 1 and 2)"
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,"After this point, I am not able to figure out how to send my metrics to datadog."
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,Do I follow the Host configurations or containarized configurations?
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,"For the Host configurations, we have to update the datadog.yaml file which is not in our repo and for containerized version, they have specified how to do it for Kubernetics but we don't use Kubernetics."
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,We are using airflow by creating a docker build and running it over on Amazon ECS.
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,We also have a datadog agent running parallely in the same task (not part of our repo).
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,However I am not able to figure out what configurations I need to make in order to send the StatsD metrics to datadog.
Datadog,66900989,nan,0,"2021/04/01, 11:28:36",False,"2021/04/01, 11:28:36",nan,14770558.0,1.0,0,32,How to send Airflow Metrics to datadog,Please let me know if anyone has any answer.
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,I am new to DataDog and getting back into working with Windows Servers.
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,"I am trying to push Event Viewer logs (Security, System, etc) to Datadog logs."
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,I have been successful in terms of setting it up (used their documentation -  https://docs.datadoghq.com/integrations/win32_event_log/ ).
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,I am getting logs into my DD for that server for my System and Security:
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,I know that you can push items from the Event Viewer to Events in DD by using  Instances  and you can be more granular there.
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,But I want that granularity in the logs sections since we rarely view Events.
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,"Right now it is showing me all the items in the logs, success, etc."
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,I am looking to only get the Errors and Warnings piped to the Logs.
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,Thanks for the help.
Datadog,66876280,nan,0,"2021/03/30, 21:09:05",False,"2021/03/30, 21:09:05",nan,2137149.0,87.0,0,27,Datadog Logs from Windows Event Viewer,D
Datadog,66875990,nan,1,"2021/03/30, 20:47:44",False,"2021/03/31, 03:15:15",nan,1467883.0,3.0,0,14,How does the serverless datadog forwarder encrypt/encode their logs?,I am having trouble figuring out how the datadog forward encodes/encrypts its messages from the datadog forwarder.
Datadog,66875990,nan,1,"2021/03/30, 20:47:44",False,"2021/03/31, 03:15:15",nan,1467883.0,3.0,0,14,How does the serverless datadog forwarder encrypt/encode their logs?,We are utilizing the forwarder on datadog using the following documentation:  https://docs.datadoghq.com/serverless/forwarder/  .
Datadog,66875990,nan,1,"2021/03/30, 20:47:44",False,"2021/03/31, 03:15:15",nan,1467883.0,3.0,0,14,How does the serverless datadog forwarder encrypt/encode their logs?,"On that page there, Datadog has an option to send the same event to another lambda that it invokes via the AdditionalTargetLambdaARNs flag."
Datadog,66875990,nan,1,"2021/03/30, 20:47:44",False,"2021/03/31, 03:15:15",nan,1467883.0,3.0,0,14,How does the serverless datadog forwarder encrypt/encode their logs?,"We are doing this and having the other lambda invoke but the event input that we are getting is long string that looks like it is base64 encoded but when I put it into a base64 decoder, I get gibberish back."
Datadog,66875990,nan,1,"2021/03/30, 20:47:44",False,"2021/03/31, 03:15:15",nan,1467883.0,3.0,0,14,How does the serverless datadog forwarder encrypt/encode their logs?,I was wondering if anyone knew how datadog is compressing/encoding/encrypting their data/logs that they send so that I can read the logs in my lambda and be able to preform actions off of the data being forwarded?
Datadog,66875990,nan,1,"2021/03/30, 20:47:44",False,"2021/03/31, 03:15:15",nan,1467883.0,3.0,0,14,How does the serverless datadog forwarder encrypt/encode their logs?,I have been searching google and the datadog site for documentation on this but I can't find any.
Datadog,66671817,nan,0,"2021/03/17, 13:06:12",False,"2021/03/18, 05:36:53","2021/03/18, 05:36:53",10571187.0,1.0,0,10,How to fetch operation name and endpoints associated with its service from DataDog tool through API?,"I'm Using,"
Datadog,66671817,nan,0,"2021/03/17, 13:06:12",False,"2021/03/18, 05:36:53","2021/03/18, 05:36:53",10571187.0,1.0,0,10,How to fetch operation name and endpoints associated with its service from DataDog tool through API?,But I'm not able to get the operation name and endpoints associated with service.
Datadog,66671817,nan,0,"2021/03/17, 13:06:12",False,"2021/03/18, 05:36:53","2021/03/18, 05:36:53",10571187.0,1.0,0,10,How to fetch operation name and endpoints associated with its service from DataDog tool through API?,Help me to find the correct url link.
Datadog,66595606,nan,0,"2021/03/12, 09:03:02",False,"2021/03/12, 09:03:02",nan,5403764.0,436.0,0,22,Terraform Datadog Query Is not working as it contains some datadog methods,"Getting query invalid for below monitor, Please suggest."
Datadog,66553744,nan,0,"2021/03/09, 21:46:50",False,"2021/03/09, 21:46:50",nan,7469990.0,119.0,0,155,Datadog: how to get redis cpu usage metrics?,"Taking a look at  Redis metrics for Datadog  , we can see that  redis.cpu.sys  refers to  System CPU consumed by the Redis server."
Datadog,66553744,nan,0,"2021/03/09, 21:46:50",False,"2021/03/09, 21:46:50",nan,7469990.0,119.0,0,155,Datadog: how to get redis cpu usage metrics?,"But this is metric for CPU usage time, not CPU usage."
Datadog,66553744,nan,0,"2021/03/09, 21:46:50",False,"2021/03/09, 21:46:50",nan,7469990.0,119.0,0,155,Datadog: how to get redis cpu usage metrics?,How do we do if we want to create alerts based on the actual CPU usage?
Datadog,66494646,nan,0,"2021/03/05, 16:48:45",False,"2021/03/05, 16:48:45",nan,2291904.0,1.0,0,17,How does one configure Datadog to customize event correlation?,Am looking for information on configuring Datadog to perform event correlation using custom event attributes (application components output events records in JSON format).
Datadog,66494646,nan,0,"2021/03/05, 16:48:45",False,"2021/03/05, 16:48:45",nan,2291904.0,1.0,0,17,How does one configure Datadog to customize event correlation?,"Also, is it possible in Datadog to then configure notifications based on correlated events?"
Datadog,66494646,nan,0,"2021/03/05, 16:48:45",False,"2021/03/05, 16:48:45",nan,2291904.0,1.0,0,17,How does one configure Datadog to customize event correlation?,Appreciate any pointers on the above or where I can get the above information.
Datadog,66494646,nan,0,"2021/03/05, 16:48:45",False,"2021/03/05, 16:48:45",nan,2291904.0,1.0,0,17,How does one configure Datadog to customize event correlation?,TIA.
Datadog,66494646,nan,0,"2021/03/05, 16:48:45",False,"2021/03/05, 16:48:45",nan,2291904.0,1.0,0,17,How does one configure Datadog to customize event correlation?,RKH
Datadog,66439995,nan,0,"2021/03/02, 15:05:36",False,"2021/03/02, 15:05:36",nan,7515745.0,1.0,0,29,Is it possible to monitor a datadog metric from the start of the day,I want to monitor our application's usage of a 3rd party API with datadog.
Datadog,66439995,nan,0,"2021/03/02, 15:05:36",False,"2021/03/02, 15:05:36",nan,7515745.0,1.0,0,29,Is it possible to monitor a datadog metric from the start of the day,We have a daily quota of x calls we are allowed to perform every day.
Datadog,66439995,nan,0,"2021/03/02, 15:05:36",False,"2021/03/02, 15:05:36",nan,7515745.0,1.0,0,29,Is it possible to monitor a datadog metric from the start of the day,The number of calls resets every day at midnight.
Datadog,66439995,nan,0,"2021/03/02, 15:05:36",False,"2021/03/02, 15:05:36",nan,7515745.0,1.0,0,29,Is it possible to monitor a datadog metric from the start of the day,I have added a datadog metric in our code that increases every time we make a call to that API.
Datadog,66439995,nan,0,"2021/03/02, 15:05:36",False,"2021/03/02, 15:05:36",nan,7515745.0,1.0,0,29,Is it possible to monitor a datadog metric from the start of the day,Now I want to create a monitor that will alert us whenever we reach 80% of our allowed daily calls.
Datadog,66439995,nan,0,"2021/03/02, 15:05:36",False,"2021/03/02, 15:05:36",nan,7515745.0,1.0,0,29,Is it possible to monitor a datadog metric from the start of the day,"In other words, I want to get the calls we made from the beginning of the day."
Datadog,66439995,nan,0,"2021/03/02, 15:05:36",False,"2021/03/02, 15:05:36",nan,7515745.0,1.0,0,29,Is it possible to monitor a datadog metric from the start of the day,Is that possible with datadog?
Datadog,66378338,nan,0,"2021/02/26, 02:13:12",False,"2021/03/02, 01:05:44","2021/03/02, 01:05:44",8797952.0,1.0,0,30,Datadog logging within Gitlab pipeline,Hey I am trying to logging within a script and send a log message to Datadog via their api: POST  https://http-intake.logs.datadoghq.com/v1/input .
Datadog,66378338,nan,0,"2021/02/26, 02:13:12",False,"2021/03/02, 01:05:44","2021/03/02, 01:05:44",8797952.0,1.0,0,30,Datadog logging within Gitlab pipeline,"My issue is that the code works perfectly well when running on the team's machine but once it's in the pipeline, it keeps throwing 400 errors as a response from Datadog."
Datadog,66378338,nan,0,"2021/02/26, 02:13:12",False,"2021/03/02, 01:05:44","2021/03/02, 01:05:44",8797952.0,1.0,0,30,Datadog logging within Gitlab pipeline,The Key is perfectly valid and the log message works on our machine so I cannot see why I am getting a 400 error.
Datadog,66378338,nan,0,"2021/02/26, 02:13:12",False,"2021/03/02, 01:05:44","2021/03/02, 01:05:44",8797952.0,1.0,0,30,Datadog logging within Gitlab pipeline,Wondering if anyone else has run into this problem
Datadog,66378338,nan,0,"2021/02/26, 02:13:12",False,"2021/03/02, 01:05:44","2021/03/02, 01:05:44",8797952.0,1.0,0,30,Datadog logging within Gitlab pipeline,pic of gitlab-cl.yml
Datadog,66378338,nan,0,"2021/02/26, 02:13:12",False,"2021/03/02, 01:05:44","2021/03/02, 01:05:44",8797952.0,1.0,0,30,Datadog logging within Gitlab pipeline,"Thanks,"
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?","Java app, built with Gradle, implementing SLF4J and Logback, exporting with Logstash to Datadog agentless logging."
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?","Can't seem to get the  host ,  service , or  source  properties to transmit:"
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?",Note where I've included the  &lt;host&gt;  and  &lt;service&gt;  tags.
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?",I also tried  &lt;property name=&quot;..&quot; value=&quot;..&quot;&gt;  and  &lt;KeyValuePair key=&quot;service&quot; value=&quot;java-app&quot; /&gt;  to no avail.
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?",Here are the docs I'm reading from Datadog:
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?",https://docs.datadoghq.com/logs/log_collection/java/?tab=log4j#agentless-logging
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?",https://docs.datadoghq.com/tracing/connect_logs_and_traces/java?tab=log4j2
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?",https://www.datadoghq.com/blog/java-logging-guide/
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?",https://docs.datadoghq.com/logs/log_collection/?tab=host
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?",https://docs.datadoghq.com/getting_started/tagging/unified_service_tagging/?tab=kubernetes
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?","Also, the docs  for logstash-logback-encoder  itself states:"
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?","By default, each property of Logback's Context (ch.qos.logback.core.Context) will appear as a field in the LoggingEvent."
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?","By default, each property of Logback's Context  (ch.qos.logback.core.Context)  will appear as a field in the LoggingEvent."
Datadog,66326300,nan,1,"2021/02/23, 04:41:34",False,"2021/02/24, 08:55:54","2021/02/24, 08:54:55",1335245.0,3000.0,0,82,"Java SLF4J→Logback→Logstash→Datadog— Host, Service, and Source?","So, how do I add a property to Logback's Context?"
Datadog,66242314,nan,0,"2021/02/17, 14:51:48",False,"2021/02/17, 14:51:48",nan,15013270.0,13.0,0,17,What Datadog integration shall I use to export k8s metrics from k8s.io/kubernetes/pkg/proxy/metrics?,"I'd like to export a few metrics from  k8s.io/kubernetes/pkg/proxy/metrics : e.g.,  sync_proxy_rules_duration_seconds  and  sync_proxy_rules_last_queued_timestamp_seconds , what datadog integration shall I use for it?"
Datadog,66242314,nan,0,"2021/02/17, 14:51:48",False,"2021/02/17, 14:51:48",nan,15013270.0,13.0,0,17,What Datadog integration shall I use to export k8s metrics from k8s.io/kubernetes/pkg/proxy/metrics?,There's a section of  K8s proxy metrics  that are collected by default but there're not these 2 metrics I'm interested in on that list.
Datadog,66242314,nan,0,"2021/02/17, 14:51:48",False,"2021/02/17, 14:51:48",nan,15013270.0,13.0,0,17,What Datadog integration shall I use to export k8s metrics from k8s.io/kubernetes/pkg/proxy/metrics?,"Moreover, I can't  find  kubernetes proxy integration as well even though there's this  kube-proxy repo  on GitHub."
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,What is the difference between the  count  and the  gauge  metric types in DataDog?
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,"Or rather, when should I prefer one over the other?"
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,The definitions from their website don't help me much:
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,Count:
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,The COUNT metric submission type represents the total number of event occurrences in one time interval.
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,A COUNT can be used to track the total number of connections made to a database or the total number of requests to an endpoint.
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,This number of events can accumulate or decrease over time—it is not monotonically increasing.
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,Gauge:
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,The GAUGE metric submission type represents a snapshot of events in one time interval.
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,This representative snapshot value is the last value submitted to the Agent during a time interval.
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,A GAUGE can be used to take a measure of something reporting continuously—like the available disk space or memory used.
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,"The  count  type seems to be somewhat related to the  rate  type, but for me it is unclear why or when I should use  count  instead of  gauge ."
Datadog,66064803,nan,0,"2021/02/05, 16:11:24",False,"2021/02/05, 16:11:24",nan,2881414.0,3822.0,0,42,What is the difference between the count and the gauge metric type in DataDog?,"I mean in principle a measurement of &quot;something&quot; could always be presented as a gauge, couldn't it?"
Datadog,66004286,nan,0,"2021/02/02, 07:50:02",False,"2021/02/03, 08:59:24","2021/02/03, 08:59:24",6223346.0,416.0,0,73,Python unit test for datadog api monitor creation,how to write pytest for the below module?
Datadog,66004286,nan,0,"2021/02/02, 07:50:02",False,"2021/02/03, 08:59:24","2021/02/03, 08:59:24",6223346.0,416.0,0,73,Python unit test for datadog api monitor creation,I have been trying to write the unit test for the below datadog API monitor creation using python language.
Datadog,66004286,nan,0,"2021/02/02, 07:50:02",False,"2021/02/03, 08:59:24","2021/02/03, 08:59:24",6223346.0,416.0,0,73,Python unit test for datadog api monitor creation,Assume the create method is going to send 200 status.
Datadog,66004286,nan,0,"2021/02/02, 07:50:02",False,"2021/02/03, 08:59:24","2021/02/03, 08:59:24",6223346.0,416.0,0,73,Python unit test for datadog api monitor creation,how do I mock  json[&quot;monitors&quot;][0]['type'] .
Datadog,66004286,nan,0,"2021/02/02, 07:50:02",False,"2021/02/03, 08:59:24","2021/02/03, 08:59:24",6223346.0,416.0,0,73,Python unit test for datadog api monitor creation,I get
Datadog,66004286,nan,0,"2021/02/02, 07:50:02",False,"2021/02/03, 08:59:24","2021/02/03, 08:59:24",6223346.0,416.0,0,73,Python unit test for datadog api monitor creation,Datadog API reference -  https://docs.datadoghq.com/api/latest/monitors/
Datadog,66004286,nan,0,"2021/02/02, 07:50:02",False,"2021/02/03, 08:59:24","2021/02/03, 08:59:24",6223346.0,416.0,0,73,Python unit test for datadog api monitor creation,json content:
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,"Is there a way to easily generate reports of alerts from certain monitors in Datadog, on a weekly or biweekly basis?"
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,"Context: At the moment, these alerts go to a Slack channel."
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,Folks have to scroll through the channel to see all the issues and prioritize investigations (during sprint planning).
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,I am trying to make it easy for sprint planners to pull up the alerts report.
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,I found only a couple related things after googling:
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,"Datadog has a CSV with 6 months of alerts, that you can curl to download."
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,"I guess I could curl, diff with prior week's csv and filter for interesting monitors."
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,But does not seem like the best solution.
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,https://docs.datadoghq.com/monitors/faq/how-can-i-export-alert-history/?tab=us
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,An old article about Monitor Trends Report which I can't find in the app.
Datadog,66001758,nan,0,"2021/02/02, 01:49:06",False,"2021/02/02, 01:49:06",nan,1325849.0,6023.0,0,21,Export Datadog Monitor Alerts Weekly,https://www.datadoghq.com/blog/monitor-alert-status/
Datadog,65836340,nan,0,"2021/01/21, 23:58:37",False,"2021/01/21, 23:58:37",nan,11744097.0,128.0,0,38,DataDog Agent min_collection_interval global,I'm using V6 of the Datadog agent on Ubuntu 18.04.
Datadog,65836340,nan,0,"2021/01/21, 23:58:37",False,"2021/01/21, 23:58:37",nan,11744097.0,128.0,0,38,DataDog Agent min_collection_interval global,I'll like to change the min_collection_interval for all checks from the default 15 seconds to 30 seconds.
Datadog,65836340,nan,0,"2021/01/21, 23:58:37",False,"2021/01/21, 23:58:37",nan,11744097.0,128.0,0,38,DataDog Agent min_collection_interval global,It's unclear from the documentation.
Datadog,65836340,nan,0,"2021/01/21, 23:58:37",False,"2021/01/21, 23:58:37",nan,11744097.0,128.0,0,38,DataDog Agent min_collection_interval global,Is this possible?
Datadog,65829133,nan,1,"2021/01/21, 16:06:28",False,"2021/01/21, 21:29:00","2021/01/21, 19:19:13",119790.0,17806.0,0,78,How to Tag / Label / Filter DataDog logs,I’m trying to group logs from a source so I can filter them in or out in DataDog logs.
Datadog,65829133,nan,1,"2021/01/21, 16:06:28",False,"2021/01/21, 21:29:00","2021/01/21, 19:19:13",119790.0,17806.0,0,78,How to Tag / Label / Filter DataDog logs,"There is already a grok parser that formats the messages, but how can I add a tag to them?"
Datadog,65829133,nan,1,"2021/01/21, 16:06:28",False,"2021/01/21, 21:29:00","2021/01/21, 19:19:13",119790.0,17806.0,0,78,How to Tag / Label / Filter DataDog logs,DataDog seem to use a subset of LogStash grok parsing rules:  https://docs.datadoghq.com/logs/processing/processors/
Datadog,65829133,nan,1,"2021/01/21, 16:06:28",False,"2021/01/21, 21:29:00","2021/01/21, 19:19:13",119790.0,17806.0,0,78,How to Tag / Label / Filter DataDog logs,Eg from Heroku:
Datadog,65829133,nan,1,"2021/01/21, 16:06:28",False,"2021/01/21, 21:29:00","2021/01/21, 19:19:13",119790.0,17806.0,0,78,How to Tag / Label / Filter DataDog logs,as
Datadog,65829133,nan,1,"2021/01/21, 16:06:28",False,"2021/01/21, 21:29:00","2021/01/21, 19:19:13",119790.0,17806.0,0,78,How to Tag / Label / Filter DataDog logs,"What I'd like is to add something like a  type , so I know they are not from the app, eg"
Datadog,65829133,nan,1,"2021/01/21, 16:06:28",False,"2021/01/21, 21:29:00","2021/01/21, 19:19:13",119790.0,17806.0,0,78,How to Tag / Label / Filter DataDog logs,Then I guess I could add the same thing to app logs and add  type: 'app'  to them.
Datadog,65766741,65770731.0,1,"2021/01/18, 00:40:59",True,"2021/01/18, 10:05:03","2021/01/18, 00:48:51",3765444.0,140.0,0,79,Include test run id in k6 metrics sent to Datadog,I use k6 on my local machine to perform load-testing as well as a  Datadog agent  to visualize the metrics in Datadog.
Datadog,65766741,65770731.0,1,"2021/01/18, 00:40:59",True,"2021/01/18, 10:05:03","2021/01/18, 00:48:51",3765444.0,140.0,0,79,Include test run id in k6 metrics sent to Datadog,I'd like to filter k6 metrics in Datadog as the tests aren't distinguishable.
Datadog,65766741,65770731.0,1,"2021/01/18, 00:40:59",True,"2021/01/18, 10:05:03","2021/01/18, 00:48:51",3765444.0,140.0,0,79,Include test run id in k6 metrics sent to Datadog,At this point the  $test_run_id  only shows  *  (refer to the screenshot below):
Datadog,65766741,65770731.0,1,"2021/01/18, 00:40:59",True,"2021/01/18, 10:05:03","2021/01/18, 00:48:51",3765444.0,140.0,0,79,Include test run id in k6 metrics sent to Datadog,"I followed  this the official doc  that suggests to set  include_test_run_id  flag to  true  in k6 config, but I was unsuccessful."
Datadog,65766741,65770731.0,1,"2021/01/18, 00:40:59",True,"2021/01/18, 10:05:03","2021/01/18, 00:48:51",3765444.0,140.0,0,79,Include test run id in k6 metrics sent to Datadog,Here's a k6 config I currently use ( &lt;YOUR_DATADOG_API_KEY&gt;  is replaced with an actual Datadog API key):
Datadog,65751475,nan,0,"2021/01/16, 17:49:45",False,"2021/01/16, 17:49:45",nan,15019392.0,1.0,0,131,How to monitor ssl certificates with Datadog?,I have an nginx-pod which redirects traffic into Kubernetes services and stores related certificates insides its volume.
Datadog,65751475,nan,0,"2021/01/16, 17:49:45",False,"2021/01/16, 17:49:45",nan,15019392.0,1.0,0,131,How to monitor ssl certificates with Datadog?,I want to monitor these certificates - mainly their expiration.
Datadog,65751475,nan,0,"2021/01/16, 17:49:45",False,"2021/01/16, 17:49:45",nan,15019392.0,1.0,0,131,How to monitor ssl certificates with Datadog?,I found out that there is a TLS integration in Datadog (we use Datadog in our cluster):  https://docs.datadoghq.com/integrations/tls/?tab=host .
Datadog,65751475,nan,0,"2021/01/16, 17:49:45",False,"2021/01/16, 17:49:45",nan,15019392.0,1.0,0,131,How to monitor ssl certificates with Datadog?,"They provide sample file, which can be found here:  https://github.com/DataDog/integrations-core/blob/master/tls/datadog_checks/tls/data/conf.yaml.example"
Datadog,65751475,nan,0,"2021/01/16, 17:49:45",False,"2021/01/16, 17:49:45",nan,15019392.0,1.0,0,131,How to monitor ssl certificates with Datadog?,"To be honest, I am completely lost and do not understand comments of the sample file - such as:"
Datadog,65751475,nan,0,"2021/01/16, 17:49:45",False,"2021/01/16, 17:49:45",nan,15019392.0,1.0,0,131,How to monitor ssl certificates with Datadog?,"I want to monitor certificates that are stored in the pod, does it mean this value should be localhost or do I need to somehow iterate over all the certificates that are stored using this value (such as server_names in nginx.conf)?"
Datadog,65751475,nan,0,"2021/01/16, 17:49:45",False,"2021/01/16, 17:49:45",nan,15019392.0,1.0,0,131,How to monitor ssl certificates with Datadog?,"If anyone could help me with setting sample configuration, I would be really grateful - if there are any more details I should provide, that is not a problem at all."
Datadog,65717686,66783806.0,1,"2021/01/14, 12:56:10",True,"2021/03/24, 17:02:55",nan,9063834.0,87.0,0,57,kafka datadog not sending metrics correctly,i am trying to send kafka consumer metrics to datadog but its not showing in monitoring when I select the node.
Datadog,65717686,66783806.0,1,"2021/01/14, 12:56:10",True,"2021/03/24, 17:02:55",nan,9063834.0,87.0,0,57,kafka datadog not sending metrics correctly,The server is giving below check in status
Datadog,65717686,66783806.0,1,"2021/01/14, 12:56:10",True,"2021/03/24, 17:02:55",nan,9063834.0,87.0,0,57,kafka datadog not sending metrics correctly,JMX is as above.
Datadog,65717686,66783806.0,1,"2021/01/14, 12:56:10",True,"2021/03/24, 17:02:55",nan,9063834.0,87.0,0,57,kafka datadog not sending metrics correctly,Please help in finding what could be wrong.
Datadog,65631278,nan,1,"2021/01/08, 16:54:45",False,"2021/01/08, 17:04:29",nan,14965554.0,1.0,0,108,"Google cloud Stackdriver vs third part tools like Grafana, Datadog?","I am looking for the criteria which decide whether we could use the built-in feature of Google cloud i.e Stack driver or we need to go for third-party tools like Grafana, Datadog etc in order to carry out Monitoring and logging."
Datadog,65629898,nan,1,"2021/01/08, 15:28:42",False,"2021/01/08, 17:15:24",nan,11584728.0,678.0,0,68,conf.d/python.d/ is not available in datadog-agent,I was trying to install  data-dog  agent in my  Ubuntu 20.04  for monitoring a python backend with the following command
Datadog,65629898,nan,1,"2021/01/08, 15:28:42",False,"2021/01/08, 17:15:24",nan,11584728.0,678.0,0,68,conf.d/python.d/ is not available in datadog-agent,From the  official documentation  it says
Datadog,65629898,nan,1,"2021/01/08, 15:28:42",False,"2021/01/08, 17:15:24",nan,11584728.0,678.0,0,68,conf.d/python.d/ is not available in datadog-agent,But haven't found any  python.d  inside  /etc/datadog-agent/conf.d .
Datadog,65629898,nan,1,"2021/01/08, 15:28:42",False,"2021/01/08, 17:15:24",nan,11584728.0,678.0,0,68,conf.d/python.d/ is not available in datadog-agent,If I create the  python.d/con.yaml  do I need to do anything else for enabling sending logs?
Datadog,65603045,nan,0,"2021/01/06, 22:36:25",False,"2021/01/06, 22:36:25",nan,6201129.0,19.0,0,53,Missing DataDog metrics when using ThreadStats for a Celery task in Django,"I have a scheduled Celery task that queries  x  number of rows, does some processing and upon success (or error) it increments a specific metric using ThreadStats."
Datadog,65603045,nan,0,"2021/01/06, 22:36:25",False,"2021/01/06, 22:36:25",nan,6201129.0,19.0,0,53,Missing DataDog metrics when using ThreadStats for a Celery task in Django,"For each execution of this task, the metric should be incremented by  x  at a specific time."
Datadog,65603045,nan,0,"2021/01/06, 22:36:25",False,"2021/01/06, 22:36:25",nan,6201129.0,19.0,0,53,Missing DataDog metrics when using ThreadStats for a Celery task in Django,The problem is that some of these increments are not posted to DataDog.
Datadog,65603045,nan,0,"2021/01/06, 22:36:25",False,"2021/01/06, 22:36:25",nan,6201129.0,19.0,0,53,Missing DataDog metrics when using ThreadStats for a Celery task in Django,Ex.
Datadog,65603045,nan,0,"2021/01/06, 22:36:25",False,"2021/01/06, 22:36:25",nan,6201129.0,19.0,0,53,Missing DataDog metrics when using ThreadStats for a Celery task in Django,"if the total number of rows is 100 and the task processes  x=10  at a time, some of those task executions fails to increment the metric and it ends up displaying only 60."
Datadog,65603045,nan,0,"2021/01/06, 22:36:25",False,"2021/01/06, 22:36:25",nan,6201129.0,19.0,0,53,Missing DataDog metrics when using ThreadStats for a Celery task in Django,This is what I tried to do without success:
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,"I'm trying to collect logs from cron jobs running on our self hosted Github runners, but so far can only see the actual github-runner host logs."
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,I've created a self-hosted Github Runner in AWS running on Unbtu with a standard config.
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,"We've also installed the Datadog agent v7 with their script and basic configuration, and added log collection from files using  these instructions"
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,Our config for log collection is below.
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,"After these steps, I can see logs from our Github runners servers."
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,"However, on those runners we have several python cron jobs running in Docker containers, logging to stdout."
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,"I can see those logs in the Github Runner UI, but they're not available in Datadog, and those are the logs I'd really like to capture, so I can extract metrics from."
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,Do the docker containers for the python scripts need some special datadog setup as well?
Datadog,65423039,nan,0,"2020/12/23, 12:49:55",False,"2020/12/23, 12:49:55",nan,6204542.0,1817.0,0,79,Datadog Python log collection from self-hosted Github Runner,Do they need to log to a file that the datadog agents registers as a log file in the setup above?
Datadog,65320127,nan,1,"2020/12/16, 10:58:00",False,"2020/12/22, 06:52:25",nan,6893383.0,97.0,0,24,How to integrate and start Datadog monitoring for Windows Apache web servers?,I need to integrate Datadog monitoring on Apache web servers which are on Windows servers.
Datadog,65320127,nan,1,"2020/12/16, 10:58:00",False,"2020/12/22, 06:52:25",nan,6893383.0,97.0,0,24,How to integrate and start Datadog monitoring for Windows Apache web servers?,Is there a link/blog available detailing the same for Windows server specifically ?
Datadog,65320127,nan,1,"2020/12/16, 10:58:00",False,"2020/12/22, 06:52:25",nan,6893383.0,97.0,0,24,How to integrate and start Datadog monitoring for Windows Apache web servers?,I got a blog link from Datadog but it seems not to cover Windows servers.
Datadog,65320127,nan,1,"2020/12/16, 10:58:00",False,"2020/12/22, 06:52:25",nan,6893383.0,97.0,0,24,How to integrate and start Datadog monitoring for Windows Apache web servers?,Need it specifically for Windows servers.
Datadog,65083824,nan,0,"2020/12/01, 04:15:27",False,"2020/12/01, 04:15:27",nan,13227394.0,23.0,0,23,See the total number on Datadog,Please advise me about Datadog metrics.
Datadog,65083824,nan,0,"2020/12/01, 04:15:27",False,"2020/12/01, 04:15:27",nan,13227394.0,23.0,0,23,See the total number on Datadog,"I'm using the  increment  method to send data to Datadog, but I can't see the total number on the Datadog side."
Datadog,65083824,nan,0,"2020/12/01, 04:15:27",False,"2020/12/01, 04:15:27",nan,13227394.0,23.0,0,23,See the total number on Datadog,I have specified a sample rate of 1 and am sending everything.
Datadog,65083824,nan,0,"2020/12/01, 04:15:27",False,"2020/12/01, 04:15:27",nan,13227394.0,23.0,0,23,See the total number on Datadog,"If you look at the following documentation, you will see that &quot;COUNT type metrics can show a decimal value within Datadog since they are normalized over the flush interval to report per-second units .&quot; and stated."
Datadog,65083824,nan,0,"2020/12/01, 04:15:27",False,"2020/12/01, 04:15:27",nan,13227394.0,23.0,0,23,See the total number on Datadog,https://docs.datadoghq.com/developers/metrics/dogstatsd_metrics_submission/#count
Datadog,65083824,nan,0,"2020/12/01, 04:15:27",False,"2020/12/01, 04:15:27",nan,13227394.0,23.0,0,23,See the total number on Datadog,Isn't there a way to see the total number on Datadog?
Datadog,65083824,nan,0,"2020/12/01, 04:15:27",False,"2020/12/01, 04:15:27",nan,13227394.0,23.0,0,23,See the total number on Datadog,Please let me know if you know.
Datadog,65074877,nan,0,"2020/11/30, 15:54:30",False,"2020/11/30, 15:54:30",nan,3979091.0,11.0,0,19,How to show response time in Datadog using MeterRegistry as a custom metric?,I have a piece of code:
Datadog,65074877,nan,0,"2020/11/30, 15:54:30",False,"2020/11/30, 15:54:30",nan,3979091.0,11.0,0,19,How to show response time in Datadog using MeterRegistry as a custom metric?,I have to show this time difference in Datadog dashboard.
Datadog,65074877,nan,0,"2020/11/30, 15:54:30",False,"2020/11/30, 15:54:30",nan,3979091.0,11.0,0,19,How to show response time in Datadog using MeterRegistry as a custom metric?,Can anyone help me with this?
Datadog,65074877,nan,0,"2020/11/30, 15:54:30",False,"2020/11/30, 15:54:30",nan,3979091.0,11.0,0,19,How to show response time in Datadog using MeterRegistry as a custom metric?,P.S.
Datadog,65074877,nan,0,"2020/11/30, 15:54:30",False,"2020/11/30, 15:54:30",nan,3979091.0,11.0,0,19,How to show response time in Datadog using MeterRegistry as a custom metric?,I have the io.micrometer setup in place and MeterRegistry is autowired in the class.
Datadog,65074877,nan,0,"2020/11/30, 15:54:30",False,"2020/11/30, 15:54:30",nan,3979091.0,11.0,0,19,How to show response time in Datadog using MeterRegistry as a custom metric?,Just need to know the method to show such a metric on the dashboard.
Datadog,64872180,nan,0,"2020/11/17, 11:02:39",False,"2020/11/17, 11:02:39",nan,9554544.0,1.0,0,39,Datadog - eBPF ip-source,We are using the eBPF via the Datadog-agent which is installed in a linux server.
Datadog,64872180,nan,0,"2020/11/17, 11:02:39",False,"2020/11/17, 11:02:39",nan,9554544.0,1.0,0,39,Datadog - eBPF ip-source,"More precisely, we are using the nprobe for gathering “NetFlow data” in the Linux server, and then Datadog via eBPF illustrates these flows on a dashboard."
Datadog,64872180,nan,0,"2020/11/17, 11:02:39",False,"2020/11/17, 11:02:39",nan,9554544.0,1.0,0,39,Datadog - eBPF ip-source,"However, we got an issue as the IP-source is always remains the same."
Datadog,64872180,nan,0,"2020/11/17, 11:02:39",False,"2020/11/17, 11:02:39",nan,9554544.0,1.0,0,39,Datadog - eBPF ip-source,Actually is the IP address where the Linux-Server is receiving the “Netflow data”.
Datadog,64872180,nan,0,"2020/11/17, 11:02:39",False,"2020/11/17, 11:02:39",nan,9554544.0,1.0,0,39,Datadog - eBPF ip-source,That's not normal as Netflow is based on a unique pair of ip.source / ip.destination.
Datadog,64872180,nan,0,"2020/11/17, 11:02:39",False,"2020/11/17, 11:02:39",nan,9554544.0,1.0,0,39,Datadog - eBPF ip-source,It seems that eBPF takes as source/reference the traffic that is receiving on linux-NIC
Datadog,64872180,nan,0,"2020/11/17, 11:02:39",False,"2020/11/17, 11:02:39",nan,9554544.0,1.0,0,39,Datadog - eBPF ip-source,Is there any way to modify this behaviour?
Datadog,64872180,nan,0,"2020/11/17, 11:02:39",False,"2020/11/17, 11:02:39",nan,9554544.0,1.0,0,39,Datadog - eBPF ip-source,Re
Datadog,64772196,nan,1,"2020/11/10, 17:50:54",False,"2020/11/10, 23:11:22",nan,9828969.0,3.0,0,295,Custom metrics sent datadog using micrometer DatadogRegistry not showing up in Datadog metric summary,I want to send custom metrics using io.micrometer.datadog.DatadogMeterRegistry to datadog.
Datadog,64772196,nan,1,"2020/11/10, 17:50:54",False,"2020/11/10, 23:11:22",nan,9828969.0,3.0,0,295,Custom metrics sent datadog using micrometer DatadogRegistry not showing up in Datadog metric summary,Below is the code snippet of the method where I am emitting metrics to Datadog.
Datadog,64772196,nan,1,"2020/11/10, 17:50:54",False,"2020/11/10, 23:11:22",nan,9828969.0,3.0,0,295,Custom metrics sent datadog using micrometer DatadogRegistry not showing up in Datadog metric summary,I am able to see logs &quot;metric sent successfully&quot; with no error but this custom metric is not showing up in Datadog UI under metrics summary.
Datadog,64772196,nan,1,"2020/11/10, 17:50:54",False,"2020/11/10, 23:11:22",nan,9828969.0,3.0,0,295,Custom metrics sent datadog using micrometer DatadogRegistry not showing up in Datadog metric summary,Am I missing anything?
Datadog,64539546,64542859.0,1,"2020/10/26, 16:55:10",True,"2020/10/26, 20:21:25","2020/10/26, 17:05:10",1106539.0,179.0,0,24,Is it possible to create a timeseries graph of an SLO in Datadog?,Is it possible to graph an SLO as a time-series graph using just native Datadog components?
Datadog,64539546,64542859.0,1,"2020/10/26, 16:55:10",True,"2020/10/26, 20:21:25","2020/10/26, 17:05:10",1106539.0,179.0,0,24,Is it possible to create a timeseries graph of an SLO in Datadog?,"If so, how?"
Datadog,64539546,64542859.0,1,"2020/10/26, 16:55:10",True,"2020/10/26, 20:21:25","2020/10/26, 17:05:10",1106539.0,179.0,0,24,Is it possible to create a timeseries graph of an SLO in Datadog?,"I can only find a way to show an SLO as a number, I'd like to show how it changes over time in a graph-format."
Datadog,64453730,nan,1,"2020/10/21, 00:42:34",False,"2020/11/19, 09:38:41",nan,10987929.0,5.0,0,133,Terraform/Datadog Alert Monitoring,I am trying to create an alert Datadog using Terraform for when multiple hosts (1 or more)  are at &gt;= 95% CPU usage.
Datadog,64453730,nan,1,"2020/10/21, 00:42:34",False,"2020/11/19, 09:38:41",nan,10987929.0,5.0,0,133,Terraform/Datadog Alert Monitoring,"So far, with the code I have, the alert would trigger anytime a host exceeds the threshold and that is a little too noisy."
Datadog,64453730,nan,1,"2020/10/21, 00:42:34",False,"2020/11/19, 09:38:41",nan,10987929.0,5.0,0,133,Terraform/Datadog Alert Monitoring,Would you happen to know how to create the logic to satisfy both conditions before the alert gets triggered?
Datadog,64453730,nan,1,"2020/10/21, 00:42:34",False,"2020/11/19, 09:38:41",nan,10987929.0,5.0,0,133,Terraform/Datadog Alert Monitoring,(Alert when Multiple hosts at 95% CPU or higher)
Datadog,64445045,nan,0,"2020/10/20, 15:20:34",False,"2020/10/20, 15:20:34",nan,11775512.0,145.0,0,38,datadog for AWS APIgateway and Glue,I am new to the datadog and I have followed the official docs and installed datadog metric in EC2 and collect the metrics.
Datadog,64445045,nan,0,"2020/10/20, 15:20:34",False,"2020/10/20, 15:20:34",nan,11775512.0,145.0,0,38,datadog for AWS APIgateway and Glue,"I am able to successfully view the metrics like EC2, RDS,S3."
Datadog,64445045,nan,0,"2020/10/20, 15:20:34",False,"2020/10/20, 15:20:34",nan,11775512.0,145.0,0,38,datadog for AWS APIgateway and Glue,I am now trying to integrate API gateway and AWS glue but didnot know how to do that also I couldnot find useful article too.
Datadog,64445045,nan,0,"2020/10/20, 15:20:34",False,"2020/10/20, 15:20:34",nan,11775512.0,145.0,0,38,datadog for AWS APIgateway and Glue,It would be great if some one help me to achieve my requirement.
Datadog,64445045,nan,0,"2020/10/20, 15:20:34",False,"2020/10/20, 15:20:34",nan,11775512.0,145.0,0,38,datadog for AWS APIgateway and Glue,Thanks in Advance
Datadog,64345103,64396016.0,1,"2020/10/14, 03:52:21",True,"2020/10/16, 23:28:57","2020/10/14, 20:38:22",1253272.0,318.0,0,343,How do I set custom &quot;trace_id&quot; for Datadog tracing?,How do I set custom &quot;trace_id&quot; for Datadog tracing?
Datadog,64345103,64396016.0,1,"2020/10/14, 03:52:21",True,"2020/10/16, 23:28:57","2020/10/14, 20:38:22",1253272.0,318.0,0,343,How do I set custom &quot;trace_id&quot; for Datadog tracing?,I searched high and low but can't find an answer to this.
Datadog,64345103,64396016.0,1,"2020/10/14, 03:52:21",True,"2020/10/16, 23:28:57","2020/10/14, 20:38:22",1253272.0,318.0,0,343,How do I set custom &quot;trace_id&quot; for Datadog tracing?,I suspect it's not supported.
Datadog,64345103,64396016.0,1,"2020/10/14, 03:52:21",True,"2020/10/16, 23:28:57","2020/10/14, 20:38:22",1253272.0,318.0,0,343,How do I set custom &quot;trace_id&quot; for Datadog tracing?,Would really appreciate it if I can get some help here.
Datadog,64345103,64396016.0,1,"2020/10/14, 03:52:21",True,"2020/10/16, 23:28:57","2020/10/14, 20:38:22",1253272.0,318.0,0,343,How do I set custom &quot;trace_id&quot; for Datadog tracing?,"As an example, if I can do the following in multiple files, then I can view these spans together in the Datadog UI since they all have the same trace ID:"
Datadog,63910061,nan,0,"2020/09/16, 00:25:17",False,"2020/09/16, 00:25:17",nan,7304157.0,446.0,0,27,Can DataDog Event Monitors be triggered after a delay of n seconds,Is there a way to create a delay of n seconds when making datadog event monitors?
Datadog,63910061,nan,0,"2020/09/16, 00:25:17",False,"2020/09/16, 00:25:17",nan,7304157.0,446.0,0,27,Can DataDog Event Monitors be triggered after a delay of n seconds,I have a monitor set up to ensure that there are 120 events of a kind received in every 24 hour period.
Datadog,63910061,nan,0,"2020/09/16, 00:25:17",False,"2020/09/16, 00:25:17",nan,7304157.0,446.0,0,27,Can DataDog Event Monitors be triggered after a delay of n seconds,"The problem is that the monitor goes off when there are ~117-119 events present, and then it resolves immediately after that - when the remaining events come through in a few minutes."
Datadog,63910061,nan,0,"2020/09/16, 00:25:17",False,"2020/09/16, 00:25:17",nan,7304157.0,446.0,0,27,Can DataDog Event Monitors be triggered after a delay of n seconds,"I want to add a delay of sorts, that will only trigger the monitor if it remains in alarm state for more than 10 minutes at a stretch, rather than triggering alerts as soon as the count dips below 120."
Datadog,63909996,63971375.0,1,"2020/09/16, 00:19:44",True,"2020/09/19, 20:06:40",nan,13615987.0,331.0,0,195,Deleting Datadog logs after 7 days time frame,i am using datadog to monitor my cloud infrastructure(AWS).
Datadog,63909996,63971375.0,1,"2020/09/16, 00:19:44",True,"2020/09/19, 20:06:40",nan,13615987.0,331.0,0,195,Deleting Datadog logs after 7 days time frame,"at present, i am sending aws-logs to datadog and datadog keeping those log data for some default timeframe."
Datadog,63909996,63971375.0,1,"2020/09/16, 00:19:44",True,"2020/09/19, 20:06:40",nan,13615987.0,331.0,0,195,Deleting Datadog logs after 7 days time frame,How i can set some limit so that after that particular limit logs will be deleted from datadog?
Datadog,63909996,63971375.0,1,"2020/09/16, 00:19:44",True,"2020/09/19, 20:06:40",nan,13615987.0,331.0,0,195,Deleting Datadog logs after 7 days time frame,I want to delete datadog logs after 7 days
Datadog,63909996,63971375.0,1,"2020/09/16, 00:19:44",True,"2020/09/19, 20:06:40",nan,13615987.0,331.0,0,195,Deleting Datadog logs after 7 days time frame,Can anyone suggest a solution for this.
Datadog,63828058,nan,0,"2020/09/10, 13:47:28",False,"2020/09/10, 13:47:28",nan,10119234.0,1.0,0,47,DataDog Monitoring for Cloud instance of Talend Server,AWS EC2 instance with Talent Server and agents hosted.two agents are on-perm from where data is pushed to cloud thorugh Talent Job.
Datadog,63828058,nan,0,"2020/09/10, 13:47:28",False,"2020/09/10, 13:47:28",nan,10119234.0,1.0,0,47,DataDog Monitoring for Cloud instance of Talend Server,Please suggest is there integration avaiable for monitoring Talend job for monitoring on DataDog which is again hosted in Same AWS account.
Datadog,63807817,nan,0,"2020/09/09, 11:34:54",False,"2020/09/09, 11:34:54",nan,6082943.0,295.0,0,19,Can I sum up different logs in a query widget for datadog dashboard?,"I have a bunch of logs that are supposed to be summed up into a value, that I would like to monitor."
Datadog,63807817,nan,0,"2020/09/09, 11:34:54",False,"2020/09/09, 11:34:54",nan,6082943.0,295.0,0,19,Can I sum up different logs in a query widget for datadog dashboard?,I tried sum function in the query widget and in table widget but nether seems to work with logs and not metrics.
Datadog,63807817,nan,0,"2020/09/09, 11:34:54",False,"2020/09/09, 11:34:54",nan,6082943.0,295.0,0,19,Can I sum up different logs in a query widget for datadog dashboard?,Anyone did something similar?
Datadog,63807817,nan,0,"2020/09/09, 11:34:54",False,"2020/09/09, 11:34:54",nan,6082943.0,295.0,0,19,Can I sum up different logs in a query widget for datadog dashboard?,Thanks
Datadog,63788641,nan,0,"2020/09/08, 09:59:44",False,"2020/09/08, 09:59:44",nan,3534485.0,149.0,0,29,Sending events to datadog from c# returns error 400,I'm trying to send events to dataDog from c# (unity specifically) but it returns an error 400 every time and I'm at a loss at what else to attempt..
Datadog,63788641,nan,0,"2020/09/08, 09:59:44",False,"2020/09/08, 09:59:44",nan,3534485.0,149.0,0,29,Sending events to datadog from c# returns error 400,This is the error it gives me
Datadog,63788641,nan,0,"2020/09/08, 09:59:44",False,"2020/09/08, 09:59:44",nan,3534485.0,149.0,0,29,Sending events to datadog from c# returns error 400,"The error is actually on the response line
 var httpResponse = (HttpWebResponse) httpWebRequest.GetResponse();"
Datadog,63788641,nan,0,"2020/09/08, 09:59:44",False,"2020/09/08, 09:59:44",nan,3534485.0,149.0,0,29,Sending events to datadog from c# returns error 400,"If I comment that line out, the error goes away but I never receive the events on dataDog, which is understandable considering it's saying it's rejecting my post"
Datadog,63739283,nan,0,"2020/09/04, 13:22:24",False,"2020/09/04, 13:22:24",nan,3423236.0,1253.0,0,12,"In Datadog, how to display monitor&#39;s grouping value?","Given monitor that displays values grouped by language, e.g."
Datadog,63739283,nan,0,"2020/09/04, 13:22:24",False,"2020/09/04, 13:22:24",nan,3423236.0,1253.0,0,12,"In Datadog, how to display monitor&#39;s grouping value?",:
Datadog,63739283,nan,0,"2020/09/04, 13:22:24",False,"2020/09/04, 13:22:24",nan,3423236.0,1253.0,0,12,"In Datadog, how to display monitor&#39;s grouping value?","Is it possible to retrieve the actual value of a language to display it in the Monitor Name (to see it in the potential alert,  here )?"
Datadog,63739283,nan,0,"2020/09/04, 13:22:24",False,"2020/09/04, 13:22:24",nan,3423236.0,1253.0,0,12,"In Datadog, how to display monitor&#39;s grouping value?","I've tried putting into the name the something like:  Errors found in {{language}}  and  {{language.name}} , but it didn't work."
Datadog,63739283,nan,0,"2020/09/04, 13:22:24",False,"2020/09/04, 13:22:24",nan,3423236.0,1253.0,0,12,"In Datadog, how to display monitor&#39;s grouping value?",Maybe variable evaluation is not expected to work in the name of a monitor?
Datadog,63707786,nan,0,"2020/09/02, 17:32:17",False,"2020/09/02, 17:32:17",nan,4268308.0,37.0,0,16,Is there a way to show a sum of all monitors in alert status on a datadog dashboard?,Id like to show a sum of all my monitors that are in alert status on my dashboard.
Datadog,63707786,nan,0,"2020/09/02, 17:32:17",False,"2020/09/02, 17:32:17",nan,4268308.0,37.0,0,16,Is there a way to show a sum of all monitors in alert status on a datadog dashboard?,"so for instance if i have
monitor 1, monitor 2, monitor 3"
Datadog,63707786,nan,0,"2020/09/02, 17:32:17",False,"2020/09/02, 17:32:17",nan,4268308.0,37.0,0,16,Is there a way to show a sum of all monitors in alert status on a datadog dashboard?,"and monitor 1 and monitor 2 are alerting, the dashboard would show"
Datadog,63707786,nan,0,"2020/09/02, 17:32:17",False,"2020/09/02, 17:32:17",nan,4268308.0,37.0,0,16,Is there a way to show a sum of all monitors in alert status on a datadog dashboard?,Current number of alerts: 2
Datadog,63707786,nan,0,"2020/09/02, 17:32:17",False,"2020/09/02, 17:32:17",nan,4268308.0,37.0,0,16,Is there a way to show a sum of all monitors in alert status on a datadog dashboard?,does anyone know if this is possible?
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,Given following scenario:
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,Right now we monitor a custom error-count metric like  myService.errorType .
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,"Which gives us an exact number of how many times an error occurred - independent from a specific entity: If an entity can't be processed like 100 times, then the metric value will be  100 ."
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,"What I'd like to have, though, is a distinct metric based on the UUID."
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,Example:
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,"Then I'd like to have a metric with the value of  2  - because the processes failed for two entities only (and not for 30, as it would be reported right now)."
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,While searching for a solution I found the possibility of using tags.
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,But  as the docs point  out they are not meant for such a use-case:
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,"Tags shouldn’t originate from unbounded sources, such as epoch timestamps, user IDs, or request IDs."
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,Doing so may infinitely increase the number of metrics for your organization and impact your billing.
Datadog,63603260,63668170.0,2,"2020/08/26, 20:58:54",True,"2020/09/01, 09:21:17",nan,842302.0,2771.0,0,182,Datadog distinct-like custom metrics,So are there any other possibilities to achieve my goals?
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,I am trying to connect to datadog from my fastapi backend.
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,I am currently trying to do this on localhost using a docker-compose file to let both my datadog-agent and my backend-container run in the same network.
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,Here is a minimal example
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,docker-compose.yml
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,Dockerfile
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,main.py
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,I run docker-compose up and then check the ip of my dd-container with
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,and update it in the compose file.
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,"When I then again run docker-compose up, I get the following error"
Datadog,63599025,nan,1,"2020/08/26, 16:44:23",False,"2020/10/08, 08:55:02",nan,6419777.0,166.0,0,401,Cannot connect from asgi app (FastApi) to Datadog Agent running on Docker,Any help would be very appreciated
Datadog,63491697,nan,0,"2020/08/19, 20:00:02",False,"2020/08/19, 20:00:02",nan,8908891.0,57.0,0,42,Testing datadog: how to setup the APM for monitoring AWS lambda,I am trying to test Datadog and see if it works for my needs... the problem is that I cannot make it works...
Datadog,63491697,nan,0,"2020/08/19, 20:00:02",False,"2020/08/19, 20:00:02",nan,8908891.0,57.0,0,42,Testing datadog: how to setup the APM for monitoring AWS lambda,I have been following the guide here:  https://docs.datadoghq.com/integrations/amazon_lambda/  nut no luck... is there anybody who has a step by step guide?
Datadog,63491697,nan,0,"2020/08/19, 20:00:02",False,"2020/08/19, 20:00:02",nan,8908891.0,57.0,0,42,Testing datadog: how to setup the APM for monitoring AWS lambda,(I have trouble on getting the IAM datadog policy (should I create it manually or AWS coludformation does that for me?)
Datadog,63491697,nan,0,"2020/08/19, 20:00:02",False,"2020/08/19, 20:00:02",nan,8908891.0,57.0,0,42,Testing datadog: how to setup the APM for monitoring AWS lambda,"I need to test the AMP mainly, because I am curious to see how the service map will work..."
Datadog,63491697,nan,0,"2020/08/19, 20:00:02",False,"2020/08/19, 20:00:02",nan,8908891.0,57.0,0,42,Testing datadog: how to setup the APM for monitoring AWS lambda,any suggestion please?
Datadog,63491697,nan,0,"2020/08/19, 20:00:02",False,"2020/08/19, 20:00:02",nan,8908891.0,57.0,0,42,Testing datadog: how to setup the APM for monitoring AWS lambda,Thank you
Datadog,63404608,nan,0,"2020/08/14, 02:38:59",False,"2020/08/14, 02:38:59",nan,857025.0,19738.0,0,75,How to filter statsd metrics in the DataDog agent?,We're using the DataDog agent with some .Net Core micro-services on Linux.
Datadog,63404608,nan,0,"2020/08/14, 02:38:59",False,"2020/08/14, 02:38:59",nan,857025.0,19738.0,0,75,How to filter statsd metrics in the DataDog agent?,We want to send our statsd metrics directly through the DataDog agent but we need to do some filtering before the agent sends the metrics to DataDog.
Datadog,63404608,nan,0,"2020/08/14, 02:38:59",False,"2020/08/14, 02:38:59",nan,857025.0,19738.0,0,75,How to filter statsd metrics in the DataDog agent?,All I could find was the following:  https://docs.datadoghq.com/tracing/custom_instrumentation/agent_customization/?tab=mongodb
Datadog,63404608,nan,0,"2020/08/14, 02:38:59",False,"2020/08/14, 02:38:59",nan,857025.0,19738.0,0,75,How to filter statsd metrics in the DataDog agent?,"That all appears to be about logging and spans, not metrics."
Datadog,63404608,nan,0,"2020/08/14, 02:38:59",False,"2020/08/14, 02:38:59",nan,857025.0,19738.0,0,75,How to filter statsd metrics in the DataDog agent?,Is it possible to filter the statsd metrics?
Datadog,63326724,63606465.0,1,"2020/08/09, 16:09:42",True,"2020/08/27, 01:15:40",nan,5414176.0,499.0,0,118,Can I prevent DataDog from closing OpsGenie incidents?,The question is about DataDog - OpsGenie integration.
Datadog,63326724,63606465.0,1,"2020/08/09, 16:09:42",True,"2020/08/27, 01:15:40",nan,5414176.0,499.0,0,118,Can I prevent DataDog from closing OpsGenie incidents?,"Whenever a DataDog monitor triggers an alert an incident is opened in OpsGenie (which is good), but when the monitor recovers back to a healthy state the OpsGenie incident is auto-closed (which is bad)."
Datadog,63326724,63606465.0,1,"2020/08/09, 16:09:42",True,"2020/08/27, 01:15:40",nan,5414176.0,499.0,0,118,Can I prevent DataDog from closing OpsGenie incidents?,Is there any way to prevent this behavior?
Datadog,63326724,63606465.0,1,"2020/08/09, 16:09:42",True,"2020/08/27, 01:15:40",nan,5414176.0,499.0,0,118,Can I prevent DataDog from closing OpsGenie incidents?,I want to keep incidents open until they are acked and resolved.
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,I am using the dependency  'io.micrometer:micrometer-registry-prometheus'  to Calculate Average request processing time.
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,For that I wrote the following java code
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,"So with that code,  timer  measures the time taken by the runnable (request method) and also counts the number of times the method was called."
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,As a result two metrics are generated which I can use in Datadog:
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,"Then in order to calculate the average request processing time , I have created a &quot;Query value&quot; graph in datadog and uses the datadog  avg by  function."
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,"But instead of resulting in the  average time , DataDog seems to calculate the  sum of the average request time per event type  and shows an always increasing average request time in seconds (which is wrong because I logged in java code the request times and they are all under 100 milliseconds; so an average must be in a range of milliseconds)."
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,So I suppose that I am doing something wrong.
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,My question How do you compute average response time using DataDog graphs?
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,N.B.
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,what I also tried as a solution is to divide count by sum using the datadog function &quot;Add Query&quot;.
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,It means in short
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,But some colleagues argued that it is not a proper way to calculate the average and that it should be able to calculate the average out-of-the-box using DataDog.
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,What do you think about that solution?
Datadog,63231231,nan,0,"2020/08/03, 17:22:52",False,"2020/08/03, 17:22:52",nan,11702277.0,1.0,0,169,Calculate Average request processing time for all event types in DataDog,Regards
Datadog,63175757,nan,0,"2020/07/30, 17:50:50",False,"2020/08/25, 19:22:29",nan,2127315.0,332.0,0,96,Snowpipe logging to DataDog,Is there a way to connect a Snowflake Snowpipe logging to DataDog.
Datadog,63175757,nan,0,"2020/07/30, 17:50:50",False,"2020/08/25, 19:22:29",nan,2127315.0,332.0,0,96,Snowpipe logging to DataDog,"I would like to setup live monitoring for a Snowpipe, I only know of the debugging tools such as  SYSTEM$PIPE_STATUS   information_schema.copy_histor   information_schema.pipe_usage_histor  and  information_schema.validate_pipe_load  but I would like it to be proactive monitoring and not ad-hoc debugging."
Datadog,63175757,nan,0,"2020/07/30, 17:50:50",False,"2020/08/25, 19:22:29",nan,2127315.0,332.0,0,96,Snowpipe logging to DataDog,From what I understand I could poll the Snowpipe REST API in an automated way and push the logs to a logging system but I'm wondering if there is no easier access to the Snowpipe logs.
Datadog,63175757,nan,0,"2020/07/30, 17:50:50",False,"2020/08/25, 19:22:29",nan,2127315.0,332.0,0,96,Snowpipe logging to DataDog,For example to easier load them into DataDog and be alerted whenever something goes wrong.
Datadog,63175757,nan,0,"2020/07/30, 17:50:50",False,"2020/08/25, 19:22:29",nan,2127315.0,332.0,0,96,Snowpipe logging to DataDog,Thanks for any pointers!
Datadog,63175757,nan,0,"2020/07/30, 17:50:50",False,"2020/08/25, 19:22:29",nan,2127315.0,332.0,0,96,Snowpipe logging to DataDog,Cédric
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,I've installed the free trial version of DataDog on my Windows 10 box.
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,"I'm trying to follow the instructions to monitor a custom file, from  https://docs.datadoghq.com/getting_started/logs/#monitor-a-custom-file"
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,I've created a test log file in  C:\dev\tmp\datadog_logs\first.log .
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,"I've edited  C:\ProgramData\Datadog\datadog.yaml , setting  logs_enabled: true ."
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,"I've created a yaml file for custom log collection in  C:\ProgramData\Datadog\conf.d\custom_log_collection.d\conf.yaml , containing:"
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,"Then I've restarted the agent, and checked its status (from an Administrator Command Prompt):"
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,"From the doc I linked above, I would expect to see a &quot;custom_log_collection&quot; entry under &quot;Logs Agent&quot;."
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,I do not:
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,The obvious place where I might be having problems is in the path to the file.
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,"I've tried several variants, e.g., a unix-style path:  /dev/tmp/datadog_logs/first.log , and nothing works."
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,Any ideas as to what is going wrong?
Datadog,62919649,nan,0,"2020/07/15, 19:37:16",False,"2020/07/15, 19:37:16",nan,243563.0,9466.0,0,280,How to monitor a custom file with DataDog in Windows 10?,Or where to find out what might be going wrong?
Datadog,62828651,nan,0,"2020/07/10, 09:42:23",False,"2020/07/11, 05:30:13","2020/07/11, 05:30:13",2870627.0,111.0,0,26,"Datadog Notepad graph , Name xaxis and y axis",I am trying to generate graphs for the AWS Lambdas in Datadog.
Datadog,62828651,nan,0,"2020/07/10, 09:42:23",False,"2020/07/11, 05:30:13","2020/07/11, 05:30:13",2870627.0,111.0,0,26,"Datadog Notepad graph , Name xaxis and y axis",I want to add titles/units to the x-axis and y-axis.
Datadog,62828651,nan,0,"2020/07/10, 09:42:23",False,"2020/07/11, 05:30:13","2020/07/11, 05:30:13",2870627.0,111.0,0,26,"Datadog Notepad graph , Name xaxis and y axis",Please help on this
Datadog,62815883,nan,0,"2020/07/09, 16:22:15",False,"2020/07/09, 16:22:15",nan,4087549.0,35.0,0,22,How to use variable in container annotations for Datadog logging,"Below is the deployment yaml file spec parts, I am trying to update for datadog logging of containers"
Datadog,62815883,nan,0,"2020/07/09, 16:22:15",False,"2020/07/09, 16:22:15",nan,4087549.0,35.0,0,22,How to use variable in container annotations for Datadog logging,How to use variable &quot;LAUNCH_ID&quot; in the annotations?
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,I am using stats-d [ https://www.npmjs.com/package/node-statsd ] and datadog is connected to it.
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,"I would see metrics which I sent to stat-d, being captured on the datadog UI."
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,However I was asked to add tags.
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,I changed:
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,client.increment(somemetric);
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,to
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,"client.increment(somemetric, [incrementTag]);"
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,Soon after I did that nothing showed up on datadog.
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,Looks like I have followed the stats-d doc.
Datadog,62667694,nan,1,"2020/07/01, 02:46:23",True,"2020/07/02, 17:24:10",nan,2458372.0,4372.0,0,209,Why is Datadog not capturing tags?,What would be my next steps to figure out why datadog cannot read it ?
Datadog,62474317,nan,2,"2020/06/19, 19:08:58",False,"2020/07/01, 11:29:39","2020/06/19, 19:20:31",3327131.0,1.0,0,645,Datadog metric monitor notification on Slack with link to the log explorer,I'm trying to setup a notification message on Slack for a monitor on a custom metric that we created.
Datadog,62474317,nan,2,"2020/06/19, 19:08:58",False,"2020/07/01, 11:29:39","2020/06/19, 19:20:31",3327131.0,1.0,0,645,Datadog metric monitor notification on Slack with link to the log explorer,"I would like the message to include a  timestamp  of the event, and also a link that redirect to the log, to analyze it immediately."
Datadog,62474317,nan,2,"2020/06/19, 19:08:58",False,"2020/07/01, 11:29:39","2020/06/19, 19:20:31",3327131.0,1.0,0,645,Datadog metric monitor notification on Slack with link to the log explorer,"Are there any template variable like {{var}} that let me insert the timestamp and the link to the log, or maybe that let me build the log search query string
dynamically like: 
 https://app.datadoghq.com/logs ?...."
Datadog,62474317,nan,2,"2020/06/19, 19:08:58",False,"2020/07/01, 11:29:39","2020/06/19, 19:20:31",3327131.0,1.0,0,645,Datadog metric monitor notification on Slack with link to the log explorer,(so I will need the timestamp at least)?
Datadog,62474317,nan,2,"2020/06/19, 19:08:58",False,"2020/07/01, 11:29:39","2020/06/19, 19:20:31",3327131.0,1.0,0,645,Datadog metric monitor notification on Slack with link to the log explorer,At the moment we only have this in the message:
Datadog,62474317,nan,2,"2020/06/19, 19:08:58",False,"2020/07/01, 11:29:39","2020/06/19, 19:20:31",3327131.0,1.0,0,645,Datadog metric monitor notification on Slack with link to the log explorer,CHANNEL: {{channel.name}}
Datadog,62474317,nan,2,"2020/06/19, 19:08:58",False,"2020/07/01, 11:29:39","2020/06/19, 19:20:31",3327131.0,1.0,0,645,Datadog metric monitor notification on Slack with link to the log explorer,ENVIRONMENT: {{environment.name}}.
Datadog,62447121,nan,0,"2020/06/18, 12:46:21",False,"2020/06/18, 12:46:21",nan,6166335.0,33.0,0,42,Datadog Integration Extras plugin for Neo4J,"I am trying to build Datadog integration extras plugin which used to connect Neo4J to Datadog, for monitoring."
Datadog,62447121,nan,0,"2020/06/18, 12:46:21",False,"2020/06/18, 12:46:21",nan,6166335.0,33.0,0,42,Datadog Integration Extras plugin for Neo4J,I am following this post  https://docs.datadoghq.com/integrations/neo4j/
Datadog,62447121,nan,0,"2020/06/18, 12:46:21",False,"2020/06/18, 12:46:21",nan,6166335.0,33.0,0,42,Datadog Integration Extras plugin for Neo4J,And the integration plugin code is in this github location  https://github.com/DataDog/integrations-extras
Datadog,62447121,nan,0,"2020/06/18, 12:46:21",False,"2020/06/18, 12:46:21",nan,6166335.0,33.0,0,42,Datadog Integration Extras plugin for Neo4J,I am using Mackbook pro for building the application.
Datadog,62447121,nan,0,"2020/06/18, 12:46:21",False,"2020/06/18, 12:46:21",nan,6166335.0,33.0,0,42,Datadog Integration Extras plugin for Neo4J,As per the article i am not able to execute the following command.
Datadog,62447121,nan,0,"2020/06/18, 12:46:21",False,"2020/06/18, 12:46:21",nan,6166335.0,33.0,0,42,Datadog Integration Extras plugin for Neo4J,"While executing the above command i am getting the following error
 ""Error: accepts 0 arg(s), received 3"""
Datadog,62447121,nan,0,"2020/06/18, 12:46:21",False,"2020/06/18, 12:46:21",nan,6166335.0,33.0,0,42,Datadog Integration Extras plugin for Neo4J,Can anyone please help me what i am missing.
Datadog,62300720,nan,1,"2020/06/10, 12:55:09",False,"2020/06/10, 17:48:07","2020/06/10, 17:48:07",8668364.0,1.0,0,76,Can we visualize the performance of JMS or EMS by integrating to datadog?,We are using JMS (tibco EMS) as messaging service can We visualize the performance using Datadog?
Datadog,62300720,nan,1,"2020/06/10, 12:55:09",False,"2020/06/10, 17:48:07","2020/06/10, 17:48:07",8668364.0,1.0,0,76,Can we visualize the performance of JMS or EMS by integrating to datadog?,If it is not possible do we have any alternate?
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,I'm trying to import my whole Datadog environment to the Terraform configuration.
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,My account has access to multiple organizations.
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,I want to import it to the single Monolithics repository.
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,"Unfortunately, I met the issue with directory layout startegy - I'm not sure how it should look based on  Terraform best practices ."
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,I suggested:
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,or
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,Does somebody have experience with the issue?
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,What do you think?
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,Could you provide me your experience?
Datadog,62221212,nan,1,"2020/06/05, 20:39:25",True,"2020/06/06, 11:04:38",nan,11049682.0,121.0,0,80,Datadog Terraform (IaC) - directory layout strategy with multiple organizations,Thanks in advance!
Datadog,62075607,nan,0,"2020/05/29, 01:30:41",False,"2020/05/29, 01:30:41",nan,13615987.0,331.0,0,42,Datadog chart is not showing accurate results,"Basically, i created aws cloudwatch metrics from cloudwatch logs."
Datadog,62075607,nan,0,"2020/05/29, 01:30:41",False,"2020/05/29, 01:30:41",nan,13615987.0,331.0,0,42,Datadog chart is not showing accurate results,when i create a chart using these metrics i can see correct chart in the cloudwatch Dashboard.
Datadog,62075607,nan,0,"2020/05/29, 01:30:41",False,"2020/05/29, 01:30:41",nan,13615987.0,331.0,0,42,Datadog chart is not showing accurate results,Problem
Datadog,62075607,nan,0,"2020/05/29, 01:30:41",False,"2020/05/29, 01:30:41",nan,13615987.0,331.0,0,42,Datadog chart is not showing accurate results,"I want to create the same chart in the DataDog Monitoring environment, i can able to create the chart using those metrics but i didn't see accurate results."
Datadog,62075607,nan,0,"2020/05/29, 01:30:41",False,"2020/05/29, 01:30:41",nan,13615987.0,331.0,0,42,Datadog chart is not showing accurate results,"In Datadog  I am seeing decimal points as metric values(0.1,0.2,0.25 etc)."
Datadog,62075607,nan,0,"2020/05/29, 01:30:41",False,"2020/05/29, 01:30:41",nan,13615987.0,331.0,0,42,Datadog chart is not showing accurate results,can anyone tell me how i can resolve this issue?
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,I would like to use the  Datadog Oracle Integration  via the  Helm Chart Datadog .
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,"Oracle Integration states  To use the Oracle integration, either install the Oracle Instant Client libraries, or download the Oracle JDBC Driver."
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,"I do not want to use a custom image to package the JDBC-driver, I want to use a standard image such as tag:7-jmx."
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,Other options that come to mind (e.g.
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,EFS volume with the driver inside) seem to be an overkill also.
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,Best option to me seems to be an init container that downloads the JDBC driver.
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,But Datadog Helm Chart does not support custom init containers for the agents.
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,What's the best way to do this?
Datadog,61888648,nan,1,"2020/05/19, 13:19:57",False,"2020/05/26, 10:07:33",nan,10300113.0,744.0,0,116,Helm: Datadog Agent with JDBC driver,To get an Datadog Agent with a JDBC driver via Helm?
Datadog,61852103,nan,0,"2020/05/17, 15:53:15",False,"2020/05/17, 15:53:15",nan,5773092.0,4984.0,0,120,Datadog: Failed to fetch records from the perf schema &#39;events_statements_summary_by_digest&#39; table,I run mysql  Ver 14.14 Distrib 5.7.30 on Debian 10 with the latest DataDog agent
Datadog,61852103,nan,0,"2020/05/17, 15:53:15",False,"2020/05/17, 15:53:15",nan,5773092.0,4984.0,0,120,Datadog: Failed to fetch records from the perf schema &#39;events_statements_summary_by_digest&#39; table,"In  datadog-agent status  I see warning:
 Warning: Failed to fetch records from the perf schema 'events_statements_summary_by_digest' table."
Datadog,61852103,nan,0,"2020/05/17, 15:53:15",False,"2020/05/17, 15:53:15",nan,5773092.0,4984.0,0,120,Datadog: Failed to fetch records from the perf schema &#39;events_statements_summary_by_digest&#39; table,During setup recommended permissions were added:  GRANT SELECT ON performance_schema.
Datadog,61852103,nan,0,"2020/05/17, 15:53:15",False,"2020/05/17, 15:53:15",nan,5773092.0,4984.0,0,120,Datadog: Failed to fetch records from the perf schema &#39;events_statements_summary_by_digest&#39; table,* TO 'datadog'@'localhost';
Datadog,61852103,nan,0,"2020/05/17, 15:53:15",False,"2020/05/17, 15:53:15",nan,5773092.0,4984.0,0,120,Datadog: Failed to fetch records from the perf schema &#39;events_statements_summary_by_digest&#39; table,"..and in fact datadog user can read that table:  mysql -u datadog -p performance_schema -e ""select * from events_statements_summary_by_digest"" :"
Datadog,61852103,nan,0,"2020/05/17, 15:53:15",False,"2020/05/17, 15:53:15",nan,5773092.0,4984.0,0,120,Datadog: Failed to fetch records from the perf schema &#39;events_statements_summary_by_digest&#39; table,My DataDog agent's config:
Datadog,61852103,nan,0,"2020/05/17, 15:53:15",False,"2020/05/17, 15:53:15",nan,5773092.0,4984.0,0,120,Datadog: Failed to fetch records from the perf schema &#39;events_statements_summary_by_digest&#39; table,What am I doing wrong?
Datadog,61834744,nan,0,"2020/05/16, 12:37:34",False,"2020/05/16, 12:37:34",nan,8707637.0,1087.0,0,74,Rename &quot;unnamed-python-service&quot; showing in Datadog,I installed Datadog on my Django 1.8 application.
Datadog,61834744,nan,0,"2020/05/16, 12:37:34",False,"2020/05/16, 12:37:34",nan,8707637.0,1087.0,0,74,Rename &quot;unnamed-python-service&quot; showing in Datadog,"I am getting all the needed services which are running and apart from that I am also getting a service called ""unnamed-python-service"" popping up, inside of which are some HTML resources rendered by  django.template"
Datadog,61834744,nan,0,"2020/05/16, 12:37:34",False,"2020/05/16, 12:37:34",nan,8707637.0,1087.0,0,74,Rename &quot;unnamed-python-service&quot; showing in Datadog,I have the below loaders configured and use the default templating engine
Datadog,61834744,nan,0,"2020/05/16, 12:37:34",False,"2020/05/16, 12:37:34",nan,8707637.0,1087.0,0,74,Rename &quot;unnamed-python-service&quot; showing in Datadog,Can anyone help me understand more about the service and also how to rename it?
Datadog,61834744,nan,0,"2020/05/16, 12:37:34",False,"2020/05/16, 12:37:34",nan,8707637.0,1087.0,0,74,Rename &quot;unnamed-python-service&quot; showing in Datadog,Here's how it shows
Datadog,61605929,nan,0,"2020/05/05, 07:42:05",False,"2020/05/16, 11:35:11",nan,12569830.0,315.0,0,57,Querying events in datadog,"Is it possible to query events and it's properties and make chart or dashboard and download the results, like as we do in amplitude analytics or heap analytics?"
Datadog,61605719,nan,0,"2020/05/05, 07:19:05",False,"2020/05/05, 07:19:05",nan,5872464.0,21.0,0,20,"Default Datadog Agent Not Working on Ubuntu, what other options do i have?","I have installed Datadog directly on 2 of my Ubuntu servers;
however in my third server I am facing problems."
Datadog,61605719,nan,0,"2020/05/05, 07:19:05",False,"2020/05/05, 07:19:05",nan,5872464.0,21.0,0,20,"Default Datadog Agent Not Working on Ubuntu, what other options do i have?","Same procedure same steps, only difference between these two are that the 3rd server contains multiple virtualhost entries."
Datadog,61576615,nan,0,"2020/05/03, 18:00:34",False,"2020/05/03, 18:00:34",nan,5761011.0,1000.0,0,2048,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,Datadog installation: using helm was successful
Datadog,61576615,nan,0,"2020/05/03, 18:00:34",False,"2020/05/03, 18:00:34",nan,5761011.0,1000.0,0,2048,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,Docs used
Datadog,61576615,nan,0,"2020/05/03, 18:00:34",False,"2020/05/03, 18:00:34",nan,5761011.0,1000.0,0,2048,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,Agent version (7.19.0)
Datadog,61576615,nan,0,"2020/05/03, 18:00:34",False,"2020/05/03, 18:00:34",nan,5761011.0,1000.0,0,2048,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,Agent Status
Datadog,61576615,nan,0,"2020/05/03, 18:00:34",False,"2020/05/03, 18:00:34",nan,5761011.0,1000.0,0,2048,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,Error in Logs
Datadog,61576615,nan,0,"2020/05/03, 18:00:34",False,"2020/05/03, 18:00:34",nan,5761011.0,1000.0,0,2048,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,ERROR which i see in log agent and the same is true for all pods
Datadog,61576615,nan,0,"2020/05/03, 18:00:34",False,"2020/05/03, 18:00:34",nan,5761011.0,1000.0,0,2048,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,Not sure where I am going wrong.
Datadog,61576615,nan,0,"2020/05/03, 18:00:34",False,"2020/05/03, 18:00:34",nan,5761011.0,1000.0,0,2048,Datadog Agent Error : Unable to collect configurations from provider docker: temporary failure in dockerutil,Any help is highly appreciated.
Datadog,61454059,nan,0,"2020/04/27, 10:53:38",False,"2020/04/27, 10:53:38",nan,1170940.0,2523.0,0,30,How to only monitor individual deployment with datadog kubernetes monitoring?,We want to monitor an individual deployment of datadog.
Datadog,61454059,nan,0,"2020/04/27, 10:53:38",False,"2020/04/27, 10:53:38",nan,1170940.0,2523.0,0,30,How to only monitor individual deployment with datadog kubernetes monitoring?,"By default, the agent gets installed on all nodes and we start paying for each node as a host."
Datadog,61454059,nan,0,"2020/04/27, 10:53:38",False,"2020/04/27, 10:53:38",nan,1170940.0,2523.0,0,30,How to only monitor individual deployment with datadog kubernetes monitoring?,"However, we're interested in using datadog to monitor our ""agents"" running in customer clusters, therefore we only want to monitor that one deployment."
Datadog,61454059,nan,0,"2020/04/27, 10:53:38",False,"2020/04/27, 10:53:38",nan,1170940.0,2523.0,0,30,How to only monitor individual deployment with datadog kubernetes monitoring?,Paying 15$ per node for each client would blow up our costs.
Datadog,61454059,nan,0,"2020/04/27, 10:53:38",False,"2020/04/27, 10:53:38",nan,1170940.0,2523.0,0,30,How to only monitor individual deployment with datadog kubernetes monitoring?,"Is there a way to only monitor a specific deployment on K8S with datadog and only pay a fee linked to that deployment, not to the size of the entire cluster?"
Datadog,61091505,nan,0,"2020/04/08, 03:26:48",False,"2020/04/08, 03:40:08","2020/04/08, 03:40:08",5437911.0,2230.0,0,387,Datadog regex to find a text that has double quote,I have logs that contain this kind of line:
Datadog,61091505,nan,0,"2020/04/08, 03:26:48",False,"2020/04/08, 03:40:08","2020/04/08, 03:40:08",5437911.0,2230.0,0,387,Datadog regex to find a text that has double quote,"I want to filter out ones with  ""event_artist_id"": 100"
Datadog,61091505,nan,0,"2020/04/08, 03:26:48",False,"2020/04/08, 03:40:08","2020/04/08, 03:40:08",5437911.0,2230.0,0,387,Datadog regex to find a text that has double quote,How do I do that?
Datadog,61091505,nan,0,"2020/04/08, 03:26:48",False,"2020/04/08, 03:40:08","2020/04/08, 03:40:08",5437911.0,2230.0,0,387,Datadog regex to find a text that has double quote,"I tried many options, but no luck yet, for example:  \/""event_artist_id"": 100\/*"
Datadog,61010490,nan,1,"2020/04/03, 14:06:52",True,"2020/04/04, 03:48:56","2020/04/04, 03:48:56",1215756.0,967.0,0,733,Is it possible to use tags for excluding instances in DataDog while creating a graph?,I have DataDog with Amazon AWS RDS integration configured.
Datadog,61010490,nan,1,"2020/04/03, 14:06:52",True,"2020/04/04, 03:48:56","2020/04/04, 03:48:56",1215756.0,967.0,0,733,Is it possible to use tags for excluding instances in DataDog while creating a graph?,Is it possible to create a graph and use a tag to exclude some hosts from the result.
Datadog,61010490,nan,1,"2020/04/03, 14:06:52",True,"2020/04/04, 03:48:56","2020/04/04, 03:48:56",1215756.0,967.0,0,733,Is it possible to use tags for excluding instances in DataDog while creating a graph?,I have let's say 100 hosts with tag  environment:live  and 10 of them are also tagged with tag  importance:ignore .
Datadog,61010490,nan,1,"2020/04/03, 14:06:52",True,"2020/04/04, 03:48:56","2020/04/04, 03:48:56",1215756.0,967.0,0,733,Is it possible to use tags for excluding instances in DataDog while creating a graph?,So I need to create a graph which will include metrics for 90 hosts that are tagged with first tag but don't tagged with a second one.
Datadog,61010490,nan,1,"2020/04/03, 14:06:52",True,"2020/04/04, 03:48:56","2020/04/04, 03:48:56",1215756.0,967.0,0,733,Is it possible to use tags for excluding instances in DataDog while creating a graph?,Is it possible?
Datadog,60940393,nan,1,"2020/03/31, 01:13:16",False,"2021/04/13, 03:24:17",nan,554481.0,1447.0,0,303,Datadog spans lost in python thread pool,"I have a function that runs in a thread pool, but it only shows up in the Datadog tracing UI when I run it outside of my threadpool."
Datadog,60940393,nan,1,"2020/03/31, 01:13:16",False,"2021/04/13, 03:24:17",nan,554481.0,1447.0,0,303,Datadog spans lost in python thread pool,In the screenshot below you can see it show up in  sync_work  but not in  async_work .
Datadog,60940393,nan,1,"2020/03/31, 01:13:16",False,"2021/04/13, 03:24:17",nan,554481.0,1447.0,0,303,Datadog spans lost in python thread pool,"Here is my code, contained in a script called  ddtrace_threadpool_example.py :"
Datadog,60940393,nan,1,"2020/03/31, 01:13:16",False,"2021/04/13, 03:24:17",nan,554481.0,1447.0,0,303,Datadog spans lost in python thread pool,I run the script like this:  python ddtrace_threadpool_example.py .
Datadog,60940393,nan,1,"2020/03/31, 01:13:16",False,"2021/04/13, 03:24:17",nan,554481.0,1447.0,0,303,Datadog spans lost in python thread pool,"I'm using Python 3.7, and  pip freeze  shows  ddtrace==0.29.0 ."
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,I am running a  Python Pyramid  app and I want to measure the number of requests coming in per second.
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,"For this, I am using  datadog ."
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,We have a company wide datadog server.
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,On my instance where my app is running we have a  dd-agent  running.
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,I have initialized datadog client in my app and I am using the following call to capture number of requests per second.
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,I am trying to capture all requests and also specific gameplay requests like for  game1  and  game2 .
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,So when the incoming request is for either of these two games I added the following tags depending on the game being requested:
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,But for requests other than these two games the tags is empty as  tags=[]
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,"My question is:
Is this the right way to do it?"
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,So when I go to the datadog for this metric and I don't give any tags to filter I should be able to capture all incoming requests per second?
Datadog,60851292,nan,0,"2020/03/25, 16:50:27",False,"2020/03/25, 16:50:27",nan,2017412.0,1927.0,0,316,How to capture requests per second using datadog?,"And when I use the filter as  game:game1 , I should be able to see incoming requests for only  game1  ?"
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,"I am trying to create a ""Top List"" visualization in DataDog and I would like to graph my data which should be grouped by error code."
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,This error code is a substring in logs.
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,An example of a line in the log is given below.
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,"I have tried to group my data by message but this is not working, I would like to group my data by substring of the message."
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,Can someone guide me on this?
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,"...Server Error {""error"":{""code"":1001,""type"":""MATCH"",""message"":""Invoke
  failed: Failed...
  ...Server Error {""error"":{""code"":2001,""type"":""MATCH"",""message"":""Invoke
  failed: Failed..."
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,Currently I get the visualization as follows
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,1.0 is the number of occurrence
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,Rather I want the visualization as follows
Datadog,60369412,nan,1,"2020/02/24, 06:07:49",False,"2020/02/24, 19:41:28","2020/02/24, 06:21:18",3553292.0,335.0,0,252,Datadog - group by substring of logs,2.0 will be total 2 occurrences of error 1001 and 1.0 will be the occurrences of error 2001
Datadog,60216953,nan,1,"2020/02/13, 23:46:17",False,"2020/02/19, 21:07:06",nan,5602489.0,11.0,0,939,Anyone have experience integrating datadog monitoring with Snowflake?,Does anyone know if Datadog agent works on snowflake?
Datadog,60216953,nan,1,"2020/02/13, 23:46:17",False,"2020/02/19, 21:07:06",nan,5602489.0,11.0,0,939,Anyone have experience integrating datadog monitoring with Snowflake?,"We want to use Datadog to collect snowflake metrics, traces and logs and create dashboards, graphs, and monitors."
Datadog,59614515,nan,0,"2020/01/06, 17:12:18",False,"2020/01/06, 17:12:18",nan,10522758.0,476.0,0,58,Having datadog listen to a socket in C++,I have a the following function to publish datadog metrics
Datadog,59614515,nan,0,"2020/01/06, 17:12:18",False,"2020/01/06, 17:12:18",nan,10522758.0,476.0,0,58,Having datadog listen to a socket in C++,"I am now trying to change it so it will go through a socket, as I find in this following page:"
Datadog,59614515,nan,0,"2020/01/06, 17:12:18",False,"2020/01/06, 17:12:18",nan,10522758.0,476.0,0,58,Having datadog listen to a socket in C++,https://github.com/garrettsickles/DogFood   under  UDS - Unix Domain Sockets (Custom) .
Datadog,59614515,nan,0,"2020/01/06, 17:12:18",False,"2020/01/06, 17:12:18",nan,10522758.0,476.0,0,58,Having datadog listen to a socket in C++,Now the function looks like this:
Datadog,59614515,nan,0,"2020/01/06, 17:12:18",False,"2020/01/06, 17:12:18",nan,10522758.0,476.0,0,58,Having datadog listen to a socket in C++,"When I try to compile it, I get 'not a member' errors."
Datadog,59614515,nan,0,"2020/01/06, 17:12:18",False,"2020/01/06, 17:12:18",nan,10522758.0,476.0,0,58,Having datadog listen to a socket in C++,How can I include them as members?
Datadog,59092413,59096255.0,1,"2019/11/28, 17:49:13",True,"2019/11/28, 23:20:29","2019/11/28, 23:20:29",5556466.0,1639.0,0,216,Error 500 when requesting Datadog logs by Python Requests,I have the following  curl  command which works fine:
Datadog,59092413,59096255.0,1,"2019/11/28, 17:49:13",True,"2019/11/28, 23:20:29","2019/11/28, 23:20:29",5556466.0,1639.0,0,216,Error 500 when requesting Datadog logs by Python Requests,"Then I convert this requests to Python Requests, and the  curl  method works but Python returns a 500 error without any details."
Datadog,59092413,59096255.0,1,"2019/11/28, 17:49:13",True,"2019/11/28, 23:20:29","2019/11/28, 23:20:29",5556466.0,1639.0,0,216,Error 500 when requesting Datadog logs by Python Requests,"I tried it outside my Docker guessing that maybe connection was the key, but it doesn't work either."
Datadog,58873873,nan,1,"2019/11/15, 11:25:26",False,"2019/11/19, 14:49:29","2019/11/15, 12:40:41",12377556.0,1.0,0,266,Not able to search for $ in datadog logs,I am searching for the occurrence of character  $  in a log file on datadog log explorer which uses lucene syntax.
Datadog,58873873,nan,1,"2019/11/15, 11:25:26",False,"2019/11/19, 14:49:29","2019/11/15, 12:40:41",12377556.0,1.0,0,266,Not able to search for $ in datadog logs,It's pretty similar to kibana.
Datadog,58873873,nan,1,"2019/11/15, 11:25:26",False,"2019/11/19, 14:49:29","2019/11/15, 12:40:41",12377556.0,1.0,0,266,Not able to search for $ in datadog logs,I have logged a string for testing  Testing $ pattern datadog  but when I search for  $  it doesn't show any results.
Datadog,58873873,nan,1,"2019/11/15, 11:25:26",False,"2019/11/19, 14:49:29","2019/11/15, 12:40:41",12377556.0,1.0,0,266,Not able to search for $ in datadog logs,On searching for  Testing  I get  Testing $ pattern datadog  in response.
Datadog,58873873,nan,1,"2019/11/15, 11:25:26",False,"2019/11/19, 14:49:29","2019/11/15, 12:40:41",12377556.0,1.0,0,266,Not able to search for $ in datadog logs,Please tell me how can list the occurrences of  $  any help is much appreciated.
Datadog,58765810,58771357.0,1,"2019/11/08, 13:35:53",True,"2019/11/08, 19:36:03",nan,4867627.0,135.0,0,145,Datadog event monitor aggregation,I have created a Multi alert event monitor
Datadog,58765810,58771357.0,1,"2019/11/08, 13:35:53",True,"2019/11/08, 19:36:03",nan,4867627.0,135.0,0,145,Datadog event monitor aggregation,"I wanted it to be aggregated by ""dbinstanceidentifier"" but it shows the accumulative count for ""* (Entire Infrastructure)""."
Datadog,58765810,58771357.0,1,"2019/11/08, 13:35:53",True,"2019/11/08, 19:36:03",nan,4867627.0,135.0,0,145,Datadog event monitor aggregation,Basically it doesn't see any groups.
Datadog,58765810,58771357.0,1,"2019/11/08, 13:35:53",True,"2019/11/08, 19:36:03",nan,4867627.0,135.0,0,145,Datadog event monitor aggregation,But I can see them in the infrastructure.
Datadog,58765810,58771357.0,1,"2019/11/08, 13:35:53",True,"2019/11/08, 19:36:03",nan,4867627.0,135.0,0,145,Datadog event monitor aggregation,Is it a problem of datadog?
Datadog,58765810,58771357.0,1,"2019/11/08, 13:35:53",True,"2019/11/08, 19:36:03",nan,4867627.0,135.0,0,145,Datadog event monitor aggregation,"May be it's only available in a kind of ""premium"" subscription?"
Datadog,58630932,58689757.0,1,"2019/10/30, 19:37:51",True,"2019/11/04, 11:05:42",nan,11936102.0,3.0,0,683,How to display correct monetary value in Datadog Dashboard Widget,"I've created a custom Datadog metric in a Springboot Java App, and turned on the management end-points."
Datadog,58630932,58689757.0,1,"2019/10/30, 19:37:51",True,"2019/11/04, 11:05:42",nan,11936102.0,3.0,0,683,How to display correct monetary value in Datadog Dashboard Widget,"I am incrementing a MeterRegistry Counter with a double value (relating to the monetary value of an order)
When I use the /management/metrics end-point, I can see the correct value being stored."
Datadog,58630932,58689757.0,1,"2019/10/30, 19:37:51",True,"2019/11/04, 11:05:42",nan,11936102.0,3.0,0,683,How to display correct monetary value in Datadog Dashboard Widget,"However, when I create a widget in my Datadog dashboard, it is only displaying the pre-decimal point value of the data."
Datadog,58630932,58689757.0,1,"2019/10/30, 19:37:51",True,"2019/11/04, 11:05:42",nan,11936102.0,3.0,0,683,How to display correct monetary value in Datadog Dashboard Widget,"e.g the order value is 61.67 and in Datadog it is displaying 61, so it's not even doing any rounding !"
Datadog,58630932,58689757.0,1,"2019/10/30, 19:37:51",True,"2019/11/04, 11:05:42",nan,11936102.0,3.0,0,683,How to display correct monetary value in Datadog Dashboard Widget,Is there any way to display the raw value of the counter in a Datadog Dashboard widget?
Datadog,58630932,58689757.0,1,"2019/10/30, 19:37:51",True,"2019/11/04, 11:05:42",nan,11936102.0,3.0,0,683,How to display correct monetary value in Datadog Dashboard Widget,Thanks in advance
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,Im trying to deploy my service and read my local logfile from the inside pod.
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,Using DataDog's helm chart values with the following configs :
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,as you can see I expect my logs to be available at /app/logs/service.log and thats what Im supplying to my conf.d :
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,"In my service, I use WinstonLogger using file transport with the JSON format."
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,process.env.LOGS_PATH = '/app/logs'
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,"After all, exploring my pod and tail -f my service.log in the expected /app/logs folder I see that the application actually writes the logs in a JSON format as expected."
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,My DataDog doesn't pick up the logs and they are not showing in the log section ..
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,NOTE:: I do not mount any volume to and from my service ..
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,What am I missing?
Datadog,58267953,nan,2,"2019/10/07, 13:42:31",True,"2019/10/07, 16:41:01",nan,3599936.0,71.0,0,594,DataDog GKE NESTJS integration using DataDog&#39;s helm chart,Should I mount my local log to /var/log/pods/[service_name]/   ?
Datadog,58266541,nan,1,"2019/10/07, 12:11:02",False,"2019/10/07, 16:28:46",nan,11950262.0,33.0,0,88,Datadog log summarization,My team and I are trying to add a table to summarize our logs.
Datadog,58266541,nan,1,"2019/10/07, 12:11:02",False,"2019/10/07, 16:28:46",nan,11950262.0,33.0,0,88,Datadog log summarization,Our system logs to datadog (not always but sometimes) as follows:
Datadog,58266541,nan,1,"2019/10/07, 12:11:02",False,"2019/10/07, 16:28:46",nan,11950262.0,33.0,0,88,Datadog log summarization,To group a set of logs (Large operation) and know they are related I added to each a  trace_id  facet (to each log in between).
Datadog,58266541,nan,1,"2019/10/07, 12:11:02",False,"2019/10/07, 16:28:46",nan,11950262.0,33.0,0,88,Datadog log summarization,Eventually I want to quickly be able to see
Datadog,58266541,nan,1,"2019/10/07, 12:11:02",False,"2019/10/07, 16:28:46",nan,11950262.0,33.0,0,88,Datadog log summarization,What's the best approach?
Datadog,58266541,nan,1,"2019/10/07, 12:11:02",False,"2019/10/07, 16:28:46",nan,11950262.0,33.0,0,88,Datadog log summarization,I don't want to query each time I need the information - I want to create an overview and to apply the system's analytic tool to this summary
Datadog,57870216,nan,0,"2019/09/10, 14:52:10",False,"2019/09/10, 14:52:10",nan,4582679.0,21.0,0,101,"How to mute Monitor in Datadog, when Windows Service has status Disabled?","Is it possible to automatically mute created Datadog Monitor(which is monitoring Windows Service), when Windows service state was changed from Automatic to Disabled?"
Datadog,57870216,nan,0,"2019/09/10, 14:52:10",False,"2019/09/10, 14:52:10",nan,4582679.0,21.0,0,101,"How to mute Monitor in Datadog, when Windows Service has status Disabled?","How can it be configured
Thanks."
Datadog,55953321,nan,1,"2019/05/02, 16:14:08",False,"2019/05/02, 19:18:34",nan,9497116.0,1.0,0,328,How to set downtime duration for datadog downtime recuring weekly,I'm trying to create a datadog monitor that only alerts on Wednesdays and Fridays.
Datadog,55953321,nan,1,"2019/05/02, 16:14:08",False,"2019/05/02, 19:18:34",nan,9497116.0,1.0,0,328,How to set downtime duration for datadog downtime recuring weekly,"I have created the metric and monitor, and I think the best solution is to create a schedualed downtime that repeats for the days I'm not interessted in."
Datadog,55953321,nan,1,"2019/05/02, 16:14:08",False,"2019/05/02, 19:18:34",nan,9497116.0,1.0,0,328,How to set downtime duration for datadog downtime recuring weekly,Ive created the downtime window as:
Datadog,55953321,nan,1,"2019/05/02, 16:14:08",False,"2019/05/02, 19:18:34",nan,9497116.0,1.0,0,328,How to set downtime duration for datadog downtime recuring weekly,"This creates a window for only 1hr, ideally this should be 24hr"
Datadog,54920625,54938650.0,1,"2019/02/28, 09:37:13",True,"2019/03/01, 07:49:07",nan,11043786.0,9.0,0,1224,When installed datadog APM ddtrace php then my app not working,"I was installed the ""datadog-php-tracer_0.14.1-beta_amd64.deb"" on my server and after installed my application return 500 error."
Datadog,54920625,54938650.0,1,"2019/02/28, 09:37:13",True,"2019/03/01, 07:49:07",nan,11043786.0,9.0,0,1224,When installed datadog APM ddtrace php then my app not working,Below is the things which I have configured or my server related information:
Datadog,54920625,54938650.0,1,"2019/02/28, 09:37:13",True,"2019/03/01, 07:49:07",nan,11043786.0,9.0,0,1224,When installed datadog APM ddtrace php then my app not working,"I am using Ubuntu, NGINX and php-fpm 7.0."
Datadog,54920625,54938650.0,1,"2019/02/28, 09:37:13",True,"2019/03/01, 07:49:07",nan,11043786.0,9.0,0,1224,When installed datadog APM ddtrace php then my app not working,I have installed datadog agent v6.
Datadog,54920625,54938650.0,1,"2019/02/28, 09:37:13",True,"2019/03/01, 07:49:07",nan,11043786.0,9.0,0,1224,When installed datadog APM ddtrace php then my app not working,"When I am checking my php-fpm log file, it shows the PDO error about ""Slim\PDO\Statement\StatementContainer- execute()""."
Datadog,54920625,54938650.0,1,"2019/02/28, 09:37:13",True,"2019/03/01, 07:49:07",nan,11043786.0,9.0,0,1224,When installed datadog APM ddtrace php then my app not working,But when I disabled the Datadog Agent or APM trace then my application working normally.
Datadog,54920625,54938650.0,1,"2019/02/28, 09:37:13",True,"2019/03/01, 07:49:07",nan,11043786.0,9.0,0,1224,When installed datadog APM ddtrace php then my app not working,In short when I am enable ddtrace my app not working and return 500 error.
Datadog,54920625,54938650.0,1,"2019/02/28, 09:37:13",True,"2019/03/01, 07:49:07",nan,11043786.0,9.0,0,1224,When installed datadog APM ddtrace php then my app not working,Can you please look in it and let me know how can resolved the issue and APM work well with my app.
Datadog,53459133,53624609.0,1,"2018/11/24, 16:25:03",True,"2018/12/05, 05:10:35",nan,7651428.0,165.0,0,93,How to find average CPU utilization over a period of time on datadog,In our infra CPU utilization is being monitored by datadog SAS.
Datadog,53459133,53624609.0,1,"2018/11/24, 16:25:03",True,"2018/12/05, 05:10:35",nan,7651428.0,165.0,0,93,How to find average CPU utilization over a period of time on datadog,The dashboard shows  CPU utilization over a period time graphically.
Datadog,53459133,53624609.0,1,"2018/11/24, 16:25:03",True,"2018/12/05, 05:10:35",nan,7651428.0,165.0,0,93,How to find average CPU utilization over a period of time on datadog,How do I find the average CPU utilization over that period of time?
Datadog,53382882,nan,1,"2018/11/19, 23:27:17",True,"2018/11/19, 23:55:24",nan,3753348.0,353.0,0,151,How can I alert that a particular process is crashing in Datadog?,I'm trying to figure out how to create an alert around a process that may be crashing and restarting repeatedly.
Datadog,53382882,nan,1,"2018/11/19, 23:27:17",True,"2018/11/19, 23:55:24",nan,3753348.0,353.0,0,151,How can I alert that a particular process is crashing in Datadog?,"It might be providing some data to Datadog while it's up, so a ""no data"" alert won't do because the lack of data never hits the duration threshold as the process restarts."
Datadog,53382882,nan,1,"2018/11/19, 23:27:17",True,"2018/11/19, 23:55:24",nan,3753348.0,353.0,0,151,How can I alert that a particular process is crashing in Datadog?,"I was thinking of alerting on a changing PID, but I cannot for the life of me figure out how to create a PID-based Monitor."
Datadog,53382882,nan,1,"2018/11/19, 23:27:17",True,"2018/11/19, 23:55:24",nan,3753348.0,353.0,0,151,How can I alert that a particular process is crashing in Datadog?,Is it possible?
Datadog,53382882,nan,1,"2018/11/19, 23:27:17",True,"2018/11/19, 23:55:24",nan,3753348.0,353.0,0,151,How can I alert that a particular process is crashing in Datadog?,And how?
Datadog,53382882,nan,1,"2018/11/19, 23:27:17",True,"2018/11/19, 23:55:24",nan,3753348.0,353.0,0,151,How can I alert that a particular process is crashing in Datadog?,Does anyone have any other suggestions for this situation?
Datadog,53231545,nan,1,"2018/11/09, 20:36:28",True,"2018/11/09, 21:54:22",nan,2307671.0,586.0,0,510,How to install Datadog agent in AWS lambda,We want to collect metrics from machines running AWS lambda in AWS.
Datadog,53231545,nan,1,"2018/11/09, 20:36:28",True,"2018/11/09, 21:54:22",nan,2307671.0,586.0,0,510,How to install Datadog agent in AWS lambda,How can I get access to these machines and get DD agent installed on them.
Datadog,53226600,nan,1,"2018/11/09, 15:27:03",False,"2018/11/20, 21:44:45","2018/11/20, 21:26:43",6458418.0,557.0,0,184,For datadog how to check posgresql up and running (on Linuxredhat server),We are not willing to use DD agent.
Datadog,53226600,nan,1,"2018/11/09, 15:27:03",False,"2018/11/20, 21:44:45","2018/11/20, 21:26:43",6458418.0,557.0,0,184,For datadog how to check posgresql up and running (on Linuxredhat server),How can we know if PostgresSQL is up and running on my Redhat linux server so that I can create an alert when postgres is down.
Datadog,52311463,nan,1,"2018/09/13, 13:09:31",True,"2018/11/02, 10:10:01",nan,3387304.0,1.0,0,329,How to display service-name prefix to all metrics in Kamon(1.x) Datadog dashboard,I am using Kamon DatadogAgentReporter to record different metrics in my application.
Datadog,52311463,nan,1,"2018/09/13, 13:09:31",True,"2018/11/02, 10:10:01",nan,3387304.0,1.0,0,329,How to display service-name prefix to all metrics in Kamon(1.x) Datadog dashboard,"After migrating Kamon from 0.6.x to 1.x, I can see only the list of metrics with tags without any service name."
Datadog,52311463,nan,1,"2018/09/13, 13:09:31",True,"2018/11/02, 10:10:01",nan,3387304.0,1.0,0,329,How to display service-name prefix to all metrics in Kamon(1.x) Datadog dashboard,"I added the reporter like this, Kamon.addReporter(new DatadogAgentReporter()) and the config as given below,"
Datadog,52311463,nan,1,"2018/09/13, 13:09:31",True,"2018/11/02, 10:10:01",nan,3387304.0,1.0,0,329,How to display service-name prefix to all metrics in Kamon(1.x) Datadog dashboard,Did I miss something?
Datadog,52311463,nan,1,"2018/09/13, 13:09:31",True,"2018/11/02, 10:10:01",nan,3387304.0,1.0,0,329,How to display service-name prefix to all metrics in Kamon(1.x) Datadog dashboard,How do I get the display service-name prefix for my metrices?
Datadog,52311463,nan,1,"2018/09/13, 13:09:31",True,"2018/11/02, 10:10:01",nan,3387304.0,1.0,0,329,How to display service-name prefix to all metrics in Kamon(1.x) Datadog dashboard,Thanks in advance!
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,I have written a Datadog Agent check in Python following the instructions on this page:  https://docs.datadoghq.com/developers/agent_checks/ .
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,The agent check is supposed to read all files in a specified network folder and then send certain metrics to Datadog.
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,The folder to be read is specified like this in the Yaml file:
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,"This is the code used to read the folder, it is Python 2.7 because that is required by Datadog"
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,If I just run the Python script in my IDE everything works correctly.
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,When the check is added to the Datadog Agent Manager on the same machine that the IDE is on and the check is run an error is thrown in the Datadog Agent Manager Log saying:
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,"2018-08-14 14:33:26 EEST | ERROR | (runner.go:277 in work) | Error running check TaskResultErrorReader: [{""message"": ""[Error 3] The system cannot find the path specified: 'Z:/TaskResults/ ."
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,"'"", ""traceback"": ""Traceback (most recent call last):\n File \""C:\Program Files\Datadog\Datadog Agent\embedded\lib\site-packages\datadog_checks\checks\base.py\"", line 294, in run\n self.check(copy.deepcopy(self.instances[0]))\n File \""c:\programdata\datadog\checks.d\TaskResultErrorReader.py\"", line 42, in check\n for file in os.listdir(task_result_location):\nWindowsError: [Error 3] The system cannot find the path specified: 'Z:/TaskResults/ ."
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,"'\n""}]"
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,"I have tried specifying the folder location in multiple ways with single and double quotes, forward and back slashes and double slashes but the same error is thrown."
Datadog,51840681,51894917.0,2,"2018/08/14, 14:47:12",True,"2018/08/17, 14:54:34",nan,831608.0,410.0,0,698,Datadog Agent check cannot find the path specified,Would anyone know if this is a Yaml syntax error or some sort of issue with Datadog or the Python?
Datadog,51377442,nan,1,"2018/07/17, 12:11:25",False,"2018/07/31, 02:52:59",nan,2446102.0,4319.0,0,131,Manually insert metrics to Datadog,I have file full of metrics:
Datadog,51377442,nan,1,"2018/07/17, 12:11:25",False,"2018/07/31, 02:52:59",nan,2446102.0,4319.0,0,131,Manually insert metrics to Datadog,"I would like to send them (once) to DataDog to create dashboard, because our infrastructure team didn't install DogStatsD agents on our containers yet."
Datadog,50148430,nan,1,"2018/05/03, 09:38:09",False,"2018/05/04, 19:46:07","2018/05/03, 09:43:25",1214497.0,95.0,0,536,Why won&#39;t Micrometer stop sending data to datadog and just close already?,"I have a Spring Boot app which due to weird restrictions needs to run once every three hours, and won't work with Quartz, so I've been running it once every three hours from OS cron and it quits when it's done."
Datadog,50148430,nan,1,"2018/05/03, 09:38:09",False,"2018/05/04, 19:46:07","2018/05/03, 09:43:25",1214497.0,95.0,0,536,Why won&#39;t Micrometer stop sending data to datadog and just close already?,"After adding micrometer-registry-datadog (and spring-legacy) however, it never quits, it just sends metrics every 20 seconds or whatever the default period is, even after calling registry.close()."
Datadog,50148430,nan,1,"2018/05/03, 09:38:09",False,"2018/05/04, 19:46:07","2018/05/03, 09:43:25",1214497.0,95.0,0,536,Why won&#39;t Micrometer stop sending data to datadog and just close already?,"Am I doomed like the dutchman to sail the seas of processing forever, or is there an obvious error I have made?"
Datadog,50148430,nan,1,"2018/05/03, 09:38:09",False,"2018/05/04, 19:46:07","2018/05/03, 09:43:25",1214497.0,95.0,0,536,Why won&#39;t Micrometer stop sending data to datadog and just close already?,"Code: It reaches SpringApplication.exit(ctx), but it does not actually exit cleanly."
Datadog,50148430,nan,1,"2018/05/03, 09:38:09",False,"2018/05/04, 19:46:07","2018/05/03, 09:43:25",1214497.0,95.0,0,536,Why won&#39;t Micrometer stop sending data to datadog and just close already?,(service is a TimedExecutorService.)
Datadog,49381672,nan,2,"2018/03/20, 12:23:52",True,"2021/01/01, 21:35:13",nan,1549119.0,668.0,0,1893,"What is the right metric type if I want to emit number of results of a query, as datadog metric?",My daemon keeps querying db on a cronly basis.
Datadog,49381672,nan,2,"2018/03/20, 12:23:52",True,"2021/01/01, 21:35:13",nan,1549119.0,668.0,0,1893,"What is the right metric type if I want to emit number of results of a query, as datadog metric?","In every iteration, (a) the deamon makes a DB query (b) receives some documents from db (c) processes those results."
Datadog,49381672,nan,2,"2018/03/20, 12:23:52",True,"2021/01/01, 21:35:13",nan,1549119.0,668.0,0,1893,"What is the right metric type if I want to emit number of results of a query, as datadog metric?",I want to emit  the number of documents returned for the query  on Datadog.
Datadog,49381672,nan,2,"2018/03/20, 12:23:52",True,"2021/01/01, 21:35:13",nan,1549119.0,668.0,0,1893,"What is the right metric type if I want to emit number of results of a query, as datadog metric?",What is the right metric type?
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,I created a wrapper cookbook to retrieve my datadog api keys from an encrypted data bag but it looks like it is not running during the execution.
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,Here is my code:
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,attributes/default.rb
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,recipes/set_key.rb:
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,and del_key:
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,I created a role named datadog and run list of this role looks like:
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,"I'm expecting this wrapper recipe load datadog keys, then datadog recipes to run and finally another wrapper recipe to remove keys."
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,"But when Chef is running, I receive an error message like:"
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,"Since I'm new to Chef and data bags use, I'm a bit confused."
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,Why my setter recipe is not running?
Datadog,48922641,48946203.0,1,"2018/02/22, 10:25:03",True,"2018/02/23, 12:55:16","2018/02/22, 19:29:42",2103312.0,638.0,0,158,Chef datadog wrapper cookbook won&#39;t run with &quot;missing api key&quot; error,Thanks.
Datadog,47003531,47003630.0,1,"2017/10/29, 20:07:46",True,"2017/10/29, 20:17:50",nan,6020610.0,2010.0,0,382,error while executing datadog-agent info command : instance #0 [ERROR]: &#39;Failed connection [Errno 61] Connection refused&#39;,statd.yaml  in  conf.d  is configured as follows
Datadog,47003531,47003630.0,1,"2017/10/29, 20:07:46",True,"2017/10/29, 20:17:50",nan,6020610.0,2010.0,0,382,error while executing datadog-agent info command : instance #0 [ERROR]: &#39;Failed connection [Errno 61] Connection refused&#39;,after starting Datadog-agent I get one error as shown below
Datadog,47003531,47003630.0,1,"2017/10/29, 20:07:46",True,"2017/10/29, 20:17:50",nan,6020610.0,2010.0,0,382,error while executing datadog-agent info command : instance #0 [ERROR]: &#39;Failed connection [Errno 61] Connection refused&#39;,"everything else runs fine, Also in  datadog-conf  I have mentioned the forwarder's IP and API key as well, but it is not showing in host map in Datadog webUI"
Datadog,46984902,46985445.0,1,"2017/10/28, 02:53:49",True,"2017/10/28, 04:36:56",nan,6020610.0,2010.0,0,621,Integrating Flink with Datadog,I am having several issues regarding Flink and Datadog integration.
Datadog,46984902,46985445.0,1,"2017/10/28, 02:53:49",True,"2017/10/28, 04:36:56",nan,6020610.0,2010.0,0,621,Integrating Flink with Datadog,"First, the issue is that Datadog uses dogstatsD instead of statsD which is not included in Flink documentation"
Datadog,46984902,46985445.0,1,"2017/10/28, 02:53:49",True,"2017/10/28, 04:36:56",nan,6020610.0,2010.0,0,621,Integrating Flink with Datadog,"Another issue is that if you go to Datadog's  Integrations  page, Flink integration is missing."
Datadog,46984902,46985445.0,1,"2017/10/28, 02:53:49",True,"2017/10/28, 04:36:56",nan,6020610.0,2010.0,0,621,Integrating Flink with Datadog,"I have tried installing graphite but I am having several issues with that as well due to python 3.6, I tried virtualenv as well, but thought of going with datadog, which is giving me hard time as well."
Datadog,46449330,46572247.0,1,"2017/09/27, 16:36:02",True,"2017/10/04, 21:57:04",nan,529618.0,24594.0,0,968,Create DataDog dashboard for all hosts from a template for a single host,"Not sure if there are any pro DataDog users on here, but I'm hoping."
Datadog,46449330,46572247.0,1,"2017/09/27, 16:36:02",True,"2017/10/04, 21:57:04",nan,529618.0,24594.0,0,968,Create DataDog dashboard for all hosts from a template for a single host,I've created a template DataDog dashboard template that captures the memory usage of a host by docker container.
Datadog,46449330,46572247.0,1,"2017/09/27, 16:36:02",True,"2017/10/04, 21:57:04",nan,529618.0,24594.0,0,968,Create DataDog dashboard for all hosts from a template for a single host,"The ""hostname"" appears in 5 or so places:"
Datadog,46449330,46572247.0,1,"2017/09/27, 16:36:02",True,"2017/10/04, 21:57:04",nan,529618.0,24594.0,0,968,Create DataDog dashboard for all hosts from a template for a single host,"I'm trying to set up a dashboard right now that displays this template for each of my 20 or so hosts, but it's a painful process of cloning the chart and editing the host name in all 5 places."
Datadog,46449330,46572247.0,1,"2017/09/27, 16:36:02",True,"2017/10/04, 21:57:04",nan,529618.0,24594.0,0,968,Create DataDog dashboard for all hosts from a template for a single host,"Whenever I make a change to the template, I have to painfully paste the changes into each host chart and change the hostname in applicable places."
Datadog,46449330,46572247.0,1,"2017/09/27, 16:36:02",True,"2017/10/04, 21:57:04",nan,529618.0,24594.0,0,968,Create DataDog dashboard for all hosts from a template for a single host,Is there a way I can set up this template (perhaps with a variable in place of the host name) and have a dashboard automatically create a chart for each host from this template?
Datadog,46449330,46572247.0,1,"2017/09/27, 16:36:02",True,"2017/10/04, 21:57:04",nan,529618.0,24594.0,0,968,Create DataDog dashboard for all hosts from a template for a single host,"Failing that, is there a way this can be scripted?"
Datadog,46449330,46572247.0,1,"2017/09/27, 16:36:02",True,"2017/10/04, 21:57:04",nan,529618.0,24594.0,0,968,Create DataDog dashboard for all hosts from a template for a single host,Thank you.
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"i am struggling with importing metrics with datadog...I am getting below error in spite of installing all required packages...
( - instance #0 [ERROR]: Exception('You need the ""psutil"" package to run this check',)"
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,request you to please help me out here as this is prove to be a major showstopper.
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,[root@mudcsftpup01 init.d]# ./datadog-agent info
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"Status date: 2017-08-31 11:31:19 (1s ago)
  Pid: 32028
  Platform: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
  Python Version: 2.7.5
  Logs: , /var/log/datadog/collector.log, syslog:/dev/log"
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"Clocks
  ======"
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"Paths
  ====="
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"Hostnames
  ========="
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"Checks
  ======"
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"Emitters
  ========"
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"Status date: 2017-08-31 11:31:23 (2s ago)
  Pid: 32053
  Platform: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
  Python Version: 2.7.5
  Logs: , /var/log/datadog/dogstatsd.log, syslog:/dev/log"
Datadog,45974396,nan,2,"2017/08/31, 09:09:53",False,"2017/09/01, 12:04:48",nan,8541547.0,1.0,0,470,issue while importing metrics with datadog,"Flush count: 1
  Packet Count: 0
  Packets per second: 0.0
  Metric count: 0
  Event count: 0"
Datadog,45232107,45239352.0,1,"2017/07/21, 10:53:55",True,"2017/07/23, 13:54:45","2017/07/23, 13:54:45",5183411.0,15.0,0,518,Send Event to Datadog,Unfortunately there is not an official Go Datadog API.
Datadog,45232107,45239352.0,1,"2017/07/21, 10:53:55",True,"2017/07/23, 13:54:45","2017/07/23, 13:54:45",5183411.0,15.0,0,518,Send Event to Datadog,I am currently using this one instead  https://github.com/zorkian/go-datadog-api .
Datadog,45232107,45239352.0,1,"2017/07/21, 10:53:55",True,"2017/07/23, 13:54:45","2017/07/23, 13:54:45",5183411.0,15.0,0,518,Send Event to Datadog,Datadog forked the first version of it and recommend using it.
Datadog,45232107,45239352.0,1,"2017/07/21, 10:53:55",True,"2017/07/23, 13:54:45","2017/07/23, 13:54:45",5183411.0,15.0,0,518,Send Event to Datadog,I am able to connect to my Dashboard:
Datadog,45232107,45239352.0,1,"2017/07/21, 10:53:55",True,"2017/07/23, 13:54:45","2017/07/23, 13:54:45",5183411.0,15.0,0,518,Send Event to Datadog,But I do not know how to send create/track an event.
Datadog,45232107,45239352.0,1,"2017/07/21, 10:53:55",True,"2017/07/23, 13:54:45","2017/07/23, 13:54:45",5183411.0,15.0,0,518,Send Event to Datadog,This is my current approach but if fails badly.
Datadog,45232107,45239352.0,1,"2017/07/21, 10:53:55",True,"2017/07/23, 13:54:45","2017/07/23, 13:54:45",5183411.0,15.0,0,518,Send Event to Datadog,"From my understanding and the missing documentation, I would have to fill out some of these variables in this struct ( https://github.com/zorkian/go-datadog-api/blob/master/events.go )"
Datadog,45232107,45239352.0,1,"2017/07/21, 10:53:55",True,"2017/07/23, 13:54:45","2017/07/23, 13:54:45",5183411.0,15.0,0,518,Send Event to Datadog,Can you please help me with that?
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,I’m getting a connection refused from my datadog-agent that is trying to collect JMX (via RMI) metrics from an in-house application that exists in its own docker container.
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,"However, jconsole is able to collect the metrics from the application that exists within its own docker container."
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,The datadog-agent exists within a container of its own.
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,Both containers exist within the same network on the same host.
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,Any ideas?
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,I have looked at the other stack overflow questions.
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,"Docker Container 0: 
* Runs the my_streams_app that outputs kafka streams metrics 
* Executed via:"
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,"Docker Container 1: 
* Runs datadog-agent within container
* Datadog-agent uses JMX default (RMI) to fetch the metrics from my_streams_app that exists in container 0, above."
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,"* both containers run on the same network within the same host (my laptop MAC OSX) 
* able to netcat from within datadog-agent in docker container to the my_streams_app ip and port in the other container."
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,"Using 0.0.0.0 and 9998, can also use specific IP addresses 
* command to run the datadog agent from within a container"
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,jmx configuration for collecting metrics by datadog jmx from within the container:
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,instances:
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,"docker_images:
        - my_streams_app"
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,"init_config:
    is_jmx: true
    conf:
        - include:
            domain: '""kafka.streams""'
            bean: '""kafka.streams"":type=""stream-metrics"",client-id=“my_test-1-StreamThread-1""'
            attribute:
                commit-calls-rate:
                    metric_type: gauge
                commit-time-avg: 
                    metric_type: gauge
                commit-time-max:
                    metric_type: gauge
                poll-calls-rate:
                    metric_type: gauge"
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,"JConsole: 
* collects metrics from my_streams_app within the docker container 0, above via:"
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,Error output:
Datadog,44933565,nan,1,"2017/07/05, 21:31:00",False,"2017/07/06, 03:26:50","2017/07/05, 23:59:16",6688988.0,1.0,0,874,Datadog-agent spun up within docker container outputs JMX RMI connection error,rmiregistry has been started as per  Failed to retrieve RMIServer stub
Datadog,44620516,nan,0,"2017/06/19, 02:38:16",False,"2017/06/19, 21:13:20","2017/06/19, 21:13:20",461112.0,3391.0,0,130,How to upgrade datadog collector agent?,"Iam using a puppetized datadog agent install(version 1.10.0), and looks like i'm on default dd-agent version - 5.9.1."
Datadog,44620516,nan,0,"2017/06/19, 02:38:16",False,"2017/06/19, 21:13:20","2017/06/19, 21:13:20",461112.0,3391.0,0,130,How to upgrade datadog collector agent?,Would like to upgrade.
Datadog,44620516,nan,0,"2017/06/19, 02:38:16",False,"2017/06/19, 21:13:20","2017/06/19, 21:13:20",461112.0,3391.0,0,130,How to upgrade datadog collector agent?,I dont see any docs related to that.
Datadog,44620516,nan,0,"2017/06/19, 02:38:16",False,"2017/06/19, 21:13:20","2017/06/19, 21:13:20",461112.0,3391.0,0,130,How to upgrade datadog collector agent?,Can someone point me on how to do this?
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,I'm attempting to query our DATADOG hub and display some metric graphs.
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,"However, it appears the default way to do this is using an embed script generated by DATADOG and utilizing that in your app."
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,"I'm actually wanting to draw the graphs on my side, using their API data so I'm better able to control the size, look and flexibility of the graphs."
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,Is this something that is possible?
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,"Rather new to DATADOG and everything seems to be done in an iFrame, which I do not want."
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,"Additionally, I found a package which I believe may be of use for Node?"
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,:  http://brettlangdon.github.io/node-dogapi/#embed-create
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,D.D.
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,Graphs Docs:  http://docs.datadoghq.com/api/#graphs
Datadog,44335719,nan,1,"2017/06/02, 21:56:29",True,"2017/06/02, 22:48:19",nan,4634691.0,225.0,0,139,Creating &#39;non-embedded&#39; DATADOG graphs in Node/Express Application,"Any advice would be greatly appreciated, I have not seen anything similar on S.O."
Datadog,44034151,44034370.0,1,"2017/05/17, 23:24:02",True,"2017/05/17, 23:37:34",nan,116891.0,15129.0,0,536,"Filter datadog data by user id (or a similar, unique key with many possible values)","Graphing Primer  and  Getting started with tags – Datadog  suggest that Tags are  the  way to filter data, but the latter article cautions:"
Datadog,44034151,44034370.0,1,"2017/05/17, 23:24:02",True,"2017/05/17, 23:37:34",nan,116891.0,15129.0,0,536,"Filter datadog data by user id (or a similar, unique key with many possible values)","Please don't include endlessly growing tags in your metrics, like timestamps or user ids."
Datadog,44034151,44034370.0,1,"2017/05/17, 23:24:02",True,"2017/05/17, 23:37:34",nan,116891.0,15129.0,0,536,"Filter datadog data by user id (or a similar, unique key with many possible values)",Please limit each metric to 1000 tags.
Datadog,44034151,44034370.0,1,"2017/05/17, 23:24:02",True,"2017/05/17, 23:37:34",nan,116891.0,15129.0,0,536,"Filter datadog data by user id (or a similar, unique key with many possible values)","So, if I want to filter by user id, how can I do that without using Tags?"
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,"There are logs with &quot;events&quot; that have attributes such as names, statuses, etc as well as &quot;amount&quot; which corresponds to a dollar amount."
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,I'm sending requests to datadog's timeseries api to get back data about these logs.
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,"I'm starting with a base query:
 sum:events{event_name:reload,event_result:success,event_environment:main,event_market:gb}.as_count()"
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,This query just returns the total number of these &quot;reload&quot; events that have occurred in the timeframe.
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,Each of these events has an attribute &quot;amount&quot; which I want returned instead.
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,I can't figure out how to format the syntax for the query to get it to return the sum of these amounts instead of the sum of the occurrences of events.
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,Here are some queries I have tried which do not work:
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,I've had difficulty finding anything about this in the datadog documentation.
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,If anyone understands how to perform this with datadog's query syntax I would very much appreciate the help.
Datadog,67201672,nan,0,"2021/04/21, 21:37:05",False,"2021/04/21, 21:37:05",nan,11501494.0,1.0,0,4,How to query Datadog to return the values of an attribute instead of the overall count of events,Thanks
Datadog,66357361,nan,0,"2021/02/24, 21:10:40",False,"2021/02/24, 21:10:40",nan,1303244.0,1.0,0,10,What graphing library does the Datadog use on its dashboard page for showingthe graphs?,I've been using the Datadog dashboard and I really like how strong and flexible it is.
Datadog,66357361,nan,0,"2021/02/24, 21:10:40",False,"2021/02/24, 21:10:40",nan,1303244.0,1.0,0,10,What graphing library does the Datadog use on its dashboard page for showingthe graphs?,I am wondering what type of API they are using
Datadog,66354474,nan,1,"2021/02/24, 18:02:55",False,"2021/02/24, 21:03:12",nan,15013270.0,13.0,0,43,How can I detect SNAT port exhaustion on Azure using Datadog metrics?,Here's a  list  of DataDog metrics of Azure Load Balancer that are available to use.
Datadog,66354474,nan,1,"2021/02/24, 18:02:55",False,"2021/02/24, 21:03:12",nan,15013270.0,13.0,0,43,How can I detect SNAT port exhaustion on Azure using Datadog metrics?,It seems like
Datadog,66354474,nan,1,"2021/02/24, 18:02:55",False,"2021/02/24, 21:03:12",nan,15013270.0,13.0,0,43,How can I detect SNAT port exhaustion on Azure using Datadog metrics?,are the most releveant.
Datadog,66354474,nan,1,"2021/02/24, 18:02:55",False,"2021/02/24, 21:03:12",nan,15013270.0,13.0,0,43,How can I detect SNAT port exhaustion on Azure using Datadog metrics?,"When SNAT port resources are exhausted, outbound flows fail."
Datadog,66354474,nan,1,"2021/02/24, 18:02:55",False,"2021/02/24, 21:03:12",nan,15013270.0,13.0,0,43,How can I detect SNAT port exhaustion on Azure using Datadog metrics?,You could observe failing outbound connections or are advised by support that you're exhausting SNAT ports.
Datadog,66354474,nan,1,"2021/02/24, 18:02:55",False,"2021/02/24, 21:03:12",nan,15013270.0,13.0,0,43,How can I detect SNAT port exhaustion on Azure using Datadog metrics?,Simply seeing failed connections does not confirm SNAT exhaustion.
Datadog,66354474,nan,1,"2021/02/24, 18:02:55",False,"2021/02/24, 21:03:12",nan,15013270.0,13.0,0,43,How can I detect SNAT port exhaustion on Azure using Datadog metrics?,Seeing the failed connections was a clue we were having an issue but there is no way to confirm SNAT exhaustion w/o a ticket to Microsoft it turns out.
Datadog,66134343,nan,0,"2021/02/10, 11:39:14",False,"2021/02/10, 14:27:09","2021/02/10, 14:27:09",3607337.0,1428.0,0,24,DataDog link custom spans and web spans in Azure AppService,"We have a VM with DataDogAgent in azure, on this VM we host  ASP.Net core API  application."
Datadog,66134343,nan,0,"2021/02/10, 11:39:14",False,"2021/02/10, 14:27:09","2021/02/10, 14:27:09",3607337.0,1428.0,0,24,DataDog link custom spans and web spans in Azure AppService,We collect these kind of metrics:
Datadog,66134343,nan,0,"2021/02/10, 11:39:14",False,"2021/02/10, 14:27:09","2021/02/10, 14:27:09",3607337.0,1428.0,0,24,DataDog link custom spans and web spans in Azure AppService,Here is the code of stub API:
Datadog,66134343,nan,0,"2021/02/10, 11:39:14",False,"2021/02/10, 14:27:09","2021/02/10, 14:27:09",3607337.0,1428.0,0,24,DataDog link custom spans and web spans in Azure AppService,In data dog dashboard these 2 kind of metrics are stacked together for one request into one trace:
Datadog,66134343,nan,0,"2021/02/10, 11:39:14",False,"2021/02/10, 14:27:09","2021/02/10, 14:27:09",3607337.0,1428.0,0,24,DataDog link custom spans and web spans in Azure AppService,"After application migrated to  Azure AppService ( .NET Datadog APM  is used to collect metrics here), spans are not stacked into one trace and appears in dashborad as separate traces."
Datadog,66134343,nan,0,"2021/02/10, 11:39:14",False,"2021/02/10, 14:27:09","2021/02/10, 14:27:09",3607337.0,1428.0,0,24,DataDog link custom spans and web spans in Azure AppService,How to merge them into one trace in  Azure App Service ?
Datadog,65396200,nan,1,"2020/12/21, 18:14:17",True,"2020/12/21, 18:34:57",nan,4476024.0,563.0,0,105,Datadog: How to alert if a pod in running in kubernetes,I'm using datadog to monitor the health of several pods deployed in a kubernetes cluster.
Datadog,65396200,nan,1,"2020/12/21, 18:14:17",True,"2020/12/21, 18:34:57",nan,4476024.0,563.0,0,105,Datadog: How to alert if a pod in running in kubernetes,I use a query like this to check the pods
Datadog,65396200,nan,1,"2020/12/21, 18:14:17",True,"2020/12/21, 18:34:57",nan,4476024.0,563.0,0,105,Datadog: How to alert if a pod in running in kubernetes,"If I stop the pod, there ins't any data for kubernetes.pods.running (so the value is not zero, I don't have any value) ."
Datadog,65396200,nan,1,"2020/12/21, 18:14:17",True,"2020/12/21, 18:34:57",nan,4476024.0,563.0,0,105,Datadog: How to alert if a pod in running in kubernetes,I don't know if it's possible to check from datadog that no pods has kube_service running.
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,"Using the @datadog/browser-rum package from  RUM Browser Monitoring , with Node apollo web client fails on IE11."
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Seeing multiple requests to  https://run-http-intake.logs.datadoghq.com  on IE11 that are different than Chrome.
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Unable to login to the application on IE11.
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Working theory is that datadogRUM is blocking other application requests on IE11.
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,"When datadogRUM is removed, the application is working correctly."
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Chrome and IE11 requests:
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Request Method: POST  RequestURL:  https://rum-http-intake.logs.datadoghq.com/v1/input/pub{id}?_dd.application_id={id}&amp;ddsource=browser&amp;&amp;ddtags=sdk_version:1.25.2.env:local&amp;batch_time={timestamp}
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Only IE11 request:
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Request Method: CONNECT RequestURL:  https://rum-http-intake.logs.datadoghq.com  Proxy-Connection: Keep-Alive
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Please let me know what additional information I should find to help with this issue.
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,IE11 issues are no fun.
Datadog,64720852,64825884.0,1,"2020/11/06, 21:57:14",True,"2020/11/24, 23:46:04",nan,872145.0,177.0,0,134,Datadog Real User Monitoring breaks node server on IE11,Thank you!
Datadog,64671093,nan,0,"2020/11/03, 23:39:02",False,"2020/11/03, 23:39:02",nan,4323514.0,357.0,0,128,DataDog metric query to PromQL,I'm creating alerts in Prometheus and migrating from Datadog.
Datadog,64671093,nan,0,"2020/11/03, 23:39:02",False,"2020/11/03, 23:39:02",nan,4323514.0,357.0,0,128,DataDog metric query to PromQL,I have two metrics queries that I'm not able to understand yet.
Datadog,64671093,nan,0,"2020/11/03, 23:39:02",False,"2020/11/03, 23:39:02",nan,4323514.0,357.0,0,128,DataDog metric query to PromQL,"In this query I understand the  avg:default.burrow_kafka_consumer_lag_total{*} by {consumer_group,env}  part but not the rest and how to translate it to PromQL."
Datadog,64671093,nan,0,"2020/11/03, 23:39:02",False,"2020/11/03, 23:39:02",nan,4323514.0,357.0,0,128,DataDog metric query to PromQL,Second
Datadog,64671093,nan,0,"2020/11/03, 23:39:02",False,"2020/11/03, 23:39:02",nan,4323514.0,357.0,0,128,DataDog metric query to PromQL,"I don't understand the rollup part, how to translate it into PromQL?"
Datadog,64671093,nan,0,"2020/11/03, 23:39:02",False,"2020/11/03, 23:39:02",nan,4323514.0,357.0,0,128,DataDog metric query to PromQL,"how is  pct_change(avg(last_1h),last_1h)  part of the query?"
Datadog,64671093,nan,0,"2020/11/03, 23:39:02",False,"2020/11/03, 23:39:02",nan,4323514.0,357.0,0,128,DataDog metric query to PromQL,I'm new to this.
Datadog,64671093,nan,0,"2020/11/03, 23:39:02",False,"2020/11/03, 23:39:02",nan,4323514.0,357.0,0,128,DataDog metric query to PromQL,I have translated other queries but these I don't understand.
Datadog,64611252,nan,1,"2020/10/30, 17:25:29",False,"2021/03/02, 21:08:29",nan,13053313.0,33.0,0,38,is it possible to access a DataDog trace_id from within a Rails application,We are using DataDog's Distributed tracing in a rails application and would like to write the trace_id (for a controller#action) so that we could access the url later to our Rails logs.
Datadog,64611252,nan,1,"2020/10/30, 17:25:29",False,"2021/03/02, 21:08:29",nan,13053313.0,33.0,0,38,is it possible to access a DataDog trace_id from within a Rails application,How could I do this?
Datadog,64611252,nan,1,"2020/10/30, 17:25:29",False,"2021/03/02, 21:08:29",nan,13053313.0,33.0,0,38,is it possible to access a DataDog trace_id from within a Rails application,something like?
Datadog,64611252,nan,1,"2020/10/30, 17:25:29",False,"2021/03/02, 21:08:29",nan,13053313.0,33.0,0,38,is it possible to access a DataDog trace_id from within a Rails application,:
Datadog,62449447,nan,0,"2020/06/18, 14:56:53",False,"2020/06/18, 14:56:53",nan,3014866.0,8331.0,0,336,Datadog: ddtrace patch_all() and Sanic,I would like to integrate my  Sanic==19.9.0  application with Datadog  ddtrace==0.34.1  tracer.
Datadog,62449447,nan,0,"2020/06/18, 14:56:53",False,"2020/06/18, 14:56:53",nan,3014866.0,8331.0,0,336,Datadog: ddtrace patch_all() and Sanic,So far I have following code:
Datadog,62449447,nan,0,"2020/06/18, 14:56:53",False,"2020/06/18, 14:56:53",nan,3014866.0,8331.0,0,336,Datadog: ddtrace patch_all() and Sanic,But trace logs are empty:
Datadog,62449447,nan,0,"2020/06/18, 14:56:53",False,"2020/06/18, 14:56:53",nan,3014866.0,8331.0,0,336,Datadog: ddtrace patch_all() and Sanic,What do I do wrong?
Datadog,62074299,nan,1,"2020/05/28, 23:50:30",False,"2020/05/29, 00:23:05",nan,12822559.0,11.0,0,42,Datadog data validation between on-prem and cloud database,Very new to Datadog and need some help.
Datadog,62074299,nan,1,"2020/05/28, 23:50:30",False,"2020/05/29, 00:23:05",nan,12822559.0,11.0,0,42,Datadog data validation between on-prem and cloud database,I have crafted 2 SQL queries (one for on-prem database and one for cloud database) and I would like to run those queries through Datadog and be able display the query results and validate that the daily results fall within an expected variance between the two systems.
Datadog,62074299,nan,1,"2020/05/28, 23:50:30",False,"2020/05/29, 00:23:05",nan,12822559.0,11.0,0,42,Datadog data validation between on-prem and cloud database,I have already set up Datadog on the cloud environment and believe I should use DogStatsD to create a custom metric but I am pretty lost with how I can incorporate my necessary SQL queries in the code to create the metric for eventual display on a dashboard.
Datadog,62074299,nan,1,"2020/05/28, 23:50:30",False,"2020/05/29, 00:23:05",nan,12822559.0,11.0,0,42,Datadog data validation between on-prem and cloud database,Any help will be greatly appreciated!!
Datadog,62074299,nan,1,"2020/05/28, 23:50:30",False,"2020/05/29, 00:23:05",nan,12822559.0,11.0,0,42,Datadog data validation between on-prem and cloud database,!
Datadog,60748902,nan,1,"2020/03/19, 01:31:14",False,"2020/03/19, 02:12:25",nan,9933041.0,631.0,0,122,"In Datadog, is there a JavaScript library that allows you to get existing metric data?","For example, suppose I have a Node library that I could use something like:"
Datadog,60748902,nan,1,"2020/03/19, 01:31:14",False,"2020/03/19, 02:12:25",nan,9933041.0,631.0,0,122,"In Datadog, is there a JavaScript library that allows you to get existing metric data?","The few I've looked at all seem to be good for posting new data to Datadog, however, are there any that can pull data from Datadog?"
Datadog,60748902,nan,1,"2020/03/19, 01:31:14",False,"2020/03/19, 02:12:25",nan,9933041.0,631.0,0,122,"In Datadog, is there a JavaScript library that allows you to get existing metric data?",Does anything like this exist?
Datadog,60748902,nan,1,"2020/03/19, 01:31:14",False,"2020/03/19, 02:12:25",nan,9933041.0,631.0,0,122,"In Datadog, is there a JavaScript library that allows you to get existing metric data?","I like the  Datadog API  but it seems as though that is only in Curl, Python, and Ruby."
Datadog,55187016,55188839.0,2,"2019/03/15, 18:34:09",True,"2019/04/06, 01:12:12","2019/04/06, 01:12:12",553029.0,2348.0,0,1370,Datadog - Monitoring multiple applications in the same site hosted by IIS,I'm trying to monitor several applications within the same site in IIS.
Datadog,55187016,55188839.0,2,"2019/03/15, 18:34:09",True,"2019/04/06, 01:12:12","2019/04/06, 01:12:12",553029.0,2348.0,0,1370,Datadog - Monitoring multiple applications in the same site hosted by IIS,"With just running the  msi  of the tracer  dd-trace-dotnet , I started to see the events, but these are registered as  [site name]/[application]  e.g  default_web_site/docs_webhook 
I would love to be able to logs them under a custom service name for each application, but according to the  documentation , this is only possible at the site level."
Datadog,55187016,55188839.0,2,"2019/03/15, 18:34:09",True,"2019/04/06, 01:12:12","2019/04/06, 01:12:12",553029.0,2348.0,0,1370,Datadog - Monitoring multiple applications in the same site hosted by IIS,"Manual instrumentation is described for windows services, setting the environment variable  DD_SERVICE_NAME  in the registry entry  HKLM\System\CurrentControlSet\Services\{service name}\Environment  is enough, but does not apply to IIS applications."
Datadog,55187016,55188839.0,2,"2019/03/15, 18:34:09",True,"2019/04/06, 01:12:12","2019/04/06, 01:12:12",553029.0,2348.0,0,1370,Datadog - Monitoring multiple applications in the same site hosted by IIS,NOTE: Creating separate sites for each application is not an option right now.
Datadog,52066057,nan,1,"2018/08/28, 23:53:39",False,"2018/08/29, 00:28:16",nan,1819254.0,8466.0,0,1104,Easiest way to rename a metric in datadog?,I'm using a  statsd.timed  to send some time metrics to datadog.
Datadog,52066057,nan,1,"2018/08/28, 23:53:39",False,"2018/08/29, 00:28:16",nan,1819254.0,8466.0,0,1104,Easiest way to rename a metric in datadog?,These metrics are being used in a few Datadog dashboards.
Datadog,52066057,nan,1,"2018/08/28, 23:53:39",False,"2018/08/29, 00:28:16",nan,1819254.0,8466.0,0,1104,Easiest way to rename a metric in datadog?,"Changing the metric name being sent is straightforward and can be done by updating the name of the metric in the statsd.timed call/decorator in the code, however, the old metric name may already be in use in existing datadog dashboards."
Datadog,52066057,nan,1,"2018/08/28, 23:53:39",False,"2018/08/29, 00:28:16",nan,1819254.0,8466.0,0,1104,Easiest way to rename a metric in datadog?,"Is there a quick and easy way to rename a metric in Datadog so that all dependencies such as Dashboards using the metric are also updated, without having to go through each dashboard and updating them independently?"
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,I currently have a program that allows me to create and post dashboards to Datadog programmatically.
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"Using the API functions  here , I was successfully able to create, update, and remove dashboards as I please."
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"However, now I'd like to extract the skeleton of existing dashboards that I have already created from Datadog to see what has been added or removed."
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"To do this, I need to figure out how to send the API key along with a request."
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"I have no problem getting the higher level information about the boards, but I'd like to go a step further."
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,This is what I get by calling  api.ScreenBoard.get_all()
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"Now, the end goal is simply to pull JSON from the ""resource"" link given from this command."
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"I've tried to use urllib and urllib2 to merge that link with the host site (like  https://www.foo.com/{resource-link} ), but I keep getting the following results:"
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,OR
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,The code that triggered this error is:
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"As you can see, my ""data"" variable returns the error."
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"So, all I need is to figure out how to send the API key along with my request to resolve the issue."
Datadog,50827869,50827969.0,2,"2018/06/13, 04:18:40",True,"2018/06/13, 04:33:31",nan,9933041.0,631.0,0,1387,How do you send an API key to Datadog using urllib?,"If anyone knows how to perform this task, I would really appreciate it."
Datadog,50460608,50957805.0,1,"2018/05/22, 08:46:51",True,"2018/06/21, 01:13:11",nan,972789.0,26114.0,0,50,Showing a service&#39;s version in a dashboard in Datadog,Is there a way to display a service version (as a string) in Datadog?
Datadog,50460608,50957805.0,1,"2018/05/22, 08:46:51",True,"2018/06/21, 01:13:11",nan,972789.0,26114.0,0,50,Showing a service&#39;s version in a dashboard in Datadog,(Like sending an API request with the text to display)?
Datadog,49443977,nan,1,"2018/03/23, 08:53:01",False,"2018/05/16, 06:35:48","2018/05/16, 06:35:48",1029926.0,4190.0,0,144,Datadog: kafka.messages_in.rate kafka metirc meaning,Does  kafka.messages_in.rate  represents the number of events from producer to broker or it also includes the replication events from other brokers.
Datadog,49443977,nan,1,"2018/03/23, 08:53:01",False,"2018/05/16, 06:35:48","2018/05/16, 06:35:48",1029926.0,4190.0,0,144,Datadog: kafka.messages_in.rate kafka metirc meaning,The official doc is useless just presents the same metric in plain English without an possible explanation
Datadog,49328198,nan,1,"2018/03/16, 21:28:58",True,"2018/03/16, 21:59:02",nan,8277577.0,51.0,0,1400,Datadog Agent installation on Windows,"I am trying to install the Datadog agent on windows using powershell only, not manualHowever, the APIKEY is not being setup."
Datadog,49328198,nan,1,"2018/03/16, 21:28:58",True,"2018/03/16, 21:59:02",nan,8277577.0,51.0,0,1400,Datadog Agent installation on Windows,Is there a way to update/set the APIKEY after installation?
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,I am trying to integrate activemq with datadog.
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,I have modified /Users//.datadog-agent/conf.d/activemq_58.yaml.
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,Changes are:
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,"instances:
   - host: localhost
     port: 8161
      user: admin
      password: admin"
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,activemq is running in localhost at default port with jmx enabled.
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,"Restarted datadog agent 
I could see error after running info command."
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,Error is
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,activemq_58
Datadog,48314382,nan,1,"2018/01/18, 07:27:43",False,"2018/01/18, 17:39:30",nan,3479700.0,47.0,0,197,Getting error while integrating activemq with datadog agent,Can anybody suggest that why I am getting this error?
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,"In my case scenario, Flink is sending the metrics to Datadog."
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,Datadog Host map is as shown below { I have no Idea why is showing me latency here }
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,Flink metrics are sent to localhost.
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,The issue here is that when
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,flink-conf.yaml  file configuration is as follows
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,"The issue is that Datadog is showing 163 metrics which I don't understand,  which I will explain in a while"
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,I don't understand the metrics format in datadog as it shows me metrics something like this
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,Now as shown in above Image
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,So my question is that which metric is this?
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,"Also, the execution plan of my job is something like this"
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,How do  I relate the metrics in Datadog with execution plan operators in Flink?
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,"I have read in   Flink API 1.3.2  that I can use tags, I have tried to use them in flink-conf.yaml file but I don't have complete Idea what sense they make here."
Datadog,47003909,47014511.0,1,"2017/10/29, 20:45:00",True,"2017/10/30, 13:47:57",nan,6020610.0,2010.0,0,377,How to I relate the metrics in Datadog with execution plan operators in Flink?,"My ultimate goal is to find operator latency, number of records out and in /second at each operator in this case"
Datadog,46846059,nan,1,"2017/10/20, 12:21:20",True,"2017/10/20, 13:03:32",nan,5527959.0,58.0,0,825,How to check wheather Datadog agent is installed in UNIX box?,Just wanted to check weather Datadog agent is installed in UNIX box or not.
Datadog,46846059,nan,1,"2017/10/20, 12:21:20",True,"2017/10/20, 13:03:32",nan,5527959.0,58.0,0,825,How to check wheather Datadog agent is installed in UNIX box?,"I ran a command  sudo /etc/init.d/datadog-agent status  but got below output
 sudo: /etc/init.d/datadog-agent: command not found"
Datadog,46846059,nan,1,"2017/10/20, 12:21:20",True,"2017/10/20, 13:03:32",nan,5527959.0,58.0,0,825,How to check wheather Datadog agent is installed in UNIX box?,Please advice
Datadog,45370569,45742428.0,1,"2017/07/28, 13:03:53",True,"2017/08/17, 21:19:43",nan,8152.0,17229.0,0,496,DataDog system.disk_write_time_pct / system.disk_read_time_pct,I'm trying to get DataDog to display a dashboard of system information.
Datadog,45370569,45742428.0,1,"2017/07/28, 13:03:53",True,"2017/08/17, 21:19:43",nan,8152.0,17229.0,0,496,DataDog system.disk_write_time_pct / system.disk_read_time_pct,One piece of information is the percentage of time the system is reading/writing from the disk expressed in the metrics  system.disk.read_time_pct  and  system.disk.write_time_pct
Datadog,45370569,45742428.0,1,"2017/07/28, 13:03:53",True,"2017/08/17, 21:19:43",nan,8152.0,17229.0,0,496,DataDog system.disk_write_time_pct / system.disk_read_time_pct,"However, when I put this graph on my dashboard it shows some parts at well over 5000%, which clearly can't be right."
Datadog,45370569,45742428.0,1,"2017/07/28, 13:03:53",True,"2017/08/17, 21:19:43",nan,8152.0,17229.0,0,496,DataDog system.disk_write_time_pct / system.disk_read_time_pct,"As you can see from the preview above, it is showing a disk read time of 5430%."
Datadog,45370569,45742428.0,1,"2017/07/28, 13:03:53",True,"2017/08/17, 21:19:43",nan,8152.0,17229.0,0,496,DataDog system.disk_write_time_pct / system.disk_read_time_pct,If I constrain the Y-axis to 100 it regularly goes above 100%.
Datadog,45370569,45742428.0,1,"2017/07/28, 13:03:53",True,"2017/08/17, 21:19:43",nan,8152.0,17229.0,0,496,DataDog system.disk_write_time_pct / system.disk_read_time_pct,I can't find anything to explain this or how to graph it properly.
Datadog,45370569,45742428.0,1,"2017/07/28, 13:03:53",True,"2017/08/17, 21:19:43",nan,8152.0,17229.0,0,496,DataDog system.disk_write_time_pct / system.disk_read_time_pct,"So, how do I properly graph  system.disk.read_time_pct  and  system.disk.write_time_pct  with DataDog?"
Datadog,33960552,33961640.0,1,"2015/11/27, 17:29:16",True,"2015/11/27, 18:39:07","2015/11/27, 17:38:03",1991579.0,5244.0,0,800,Add server name to metrics for Datadog,I use  dropwizard metrics  with  metrics-datadog .
Datadog,33960552,33961640.0,1,"2015/11/27, 17:29:16",True,"2015/11/27, 18:39:07","2015/11/27, 17:38:03",1991579.0,5244.0,0,800,Add server name to metrics for Datadog,Create reported like this:
Datadog,33960552,33961640.0,1,"2015/11/27, 17:29:16",True,"2015/11/27, 18:39:07","2015/11/27, 17:38:03",1991579.0,5244.0,0,800,Add server name to metrics for Datadog,But there is no host(server name) param in datadog.
Datadog,33960552,33961640.0,1,"2015/11/27, 17:29:16",True,"2015/11/27, 18:39:07","2015/11/27, 17:38:03",1991579.0,5244.0,0,800,Add server name to metrics for Datadog,How can I add host (server name) for metrics to filter them in datadog control panel?
Datadog,33960552,33961640.0,1,"2015/11/27, 17:29:16",True,"2015/11/27, 18:39:07","2015/11/27, 17:38:03",1991579.0,5244.0,0,800,Add server name to metrics for Datadog,Metrics from default datadog agent has server name attribute.
Datadog,66416385,nan,1,"2021/03/01, 05:49:21",False,"2021/03/02, 00:30:18",nan,10164463.0,13.0,-1,20,What is the best way to represent a chart of distribution of time intervals in Datadog?,I have a server that processes packets from different devices.
Datadog,66416385,nan,1,"2021/03/01, 05:49:21",False,"2021/03/02, 00:30:18",nan,10164463.0,13.0,-1,20,What is the best way to represent a chart of distribution of time intervals in Datadog?,Devices can report in different intervals.
Datadog,66416385,nan,1,"2021/03/01, 05:49:21",False,"2021/03/02, 00:30:18",nan,10164463.0,13.0,-1,20,What is the best way to represent a chart of distribution of time intervals in Datadog?,"I would like to make a chart showing the distribution of intervals by the count of devices (how many devices are reporting within 5 sec/10 sec/60 sec ...)
Intervals for each device can vary."
Datadog,66416385,nan,1,"2021/03/01, 05:49:21",False,"2021/03/02, 00:30:18",nan,10164463.0,13.0,-1,20,What is the best way to represent a chart of distribution of time intervals in Datadog?,"Now I'm sending metric with  Set  using deviceId with tags that represent interval (5 sec, 10 sec, 30 sec, and more) but I'm not sure that it is correct."
Datadog,66416385,nan,1,"2021/03/01, 05:49:21",False,"2021/03/02, 00:30:18",nan,10164463.0,13.0,-1,20,What is the best way to represent a chart of distribution of time intervals in Datadog?,What is the best way to realize it?
Datadog,53044670,53458998.0,1,"2018/10/29, 13:34:10",True,"2018/11/24, 16:09:27","2018/10/30, 16:12:37",2650254.0,347.0,-1,63,Is it possible to export Datadog Aurora dashboard as cloud formation?,"I'm using  this built-in dashboard  for monitoring Aurora and was wondering how can I have as code, as cloud formation stack precisely."
Datadog,53044670,53458998.0,1,"2018/10/29, 13:34:10",True,"2018/11/24, 16:09:27","2018/10/30, 16:12:37",2650254.0,347.0,-1,63,Is it possible to export Datadog Aurora dashboard as cloud formation?,"I'm aware of those three repos which do backup and monitoring of changing of the dashboard in the API and then commit back to GitHub, but I only want to export it once."
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,I have an API with 10 endpoints(contracts).
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,and i am shipping logs to IIS to data-dog from the API.
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,I also installed data-dog agent on the server.
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,Now i am trying to create graph for all the endpoint hits per second.
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,there will be only one graph and all the endpoints TPS will be shown on the graph.
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,How can I achieve this?
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,any suggestions?
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,I tried to create different matrix but not able to achieve this.
Datadog,50134871,nan,1,"2018/05/02, 15:38:49",False,"2018/05/21, 17:41:57","2018/05/02, 20:05:04",2768132.0,298.0,-1,320,create Datadog Graph to show my API endpoint hits per second,I've read that parser file needs to be created.
Datadog,41605761,nan,1,"2017/01/12, 07:31:52",True,"2017/01/12, 18:15:50","2017/01/12, 10:38:43",3226384.0,548.0,-1,197,Datastax driver Metrics on Datadog,Can any one help me in getting the Datastax driver Metrics to Datadog.
Datadog,41605761,nan,1,"2017/01/12, 07:31:52",True,"2017/01/12, 18:15:50","2017/01/12, 10:38:43",3226384.0,548.0,-1,197,Datastax driver Metrics on Datadog,Tried to search but no luck and was not able find any.
Datadog,41605761,nan,1,"2017/01/12, 07:31:52",True,"2017/01/12, 18:15:50","2017/01/12, 10:38:43",3226384.0,548.0,-1,197,Datastax driver Metrics on Datadog,The following are the  metrics  that we need to se in the dataDog.
Datadog,66825909,nan,0,"2021/03/27, 02:28:46",False,"2021/03/27, 02:28:46",nan,15382996.0,17.0,-3,17,How can I check that StatsD Datadog agent collects all the metric I send to it?,"So I use  StatsD  to receive all my metrics from my service, but unfortunately Datadog can't seem to pick up those, how can I debug it?"
Datadog,66825909,nan,0,"2021/03/27, 02:28:46",False,"2021/03/27, 02:28:46",nan,15382996.0,17.0,-3,17,How can I check that StatsD Datadog agent collects all the metric I send to it?,I think it'd be great if there's  metrics  endpoint or something for  StatsD  such that I'd be able to curl it and see which metrics has been collected so far.
Datadog,36459827,36461477.0,2,"2016/04/06, 21:47:13",True,"2016/04/07, 00:21:09","2016/04/07, 00:21:09",97094.0,1631.0,8,2993,How to interpret Go stacktrace,I am getting this stacktrace when running a go program:
Datadog,36459827,36461477.0,2,"2016/04/06, 21:47:13",True,"2016/04/07, 00:21:09","2016/04/07, 00:21:09",97094.0,1631.0,8,2993,How to interpret Go stacktrace,The signature of the Event function is:
Datadog,36459827,36461477.0,2,"2016/04/06, 21:47:13",True,"2016/04/07, 00:21:09","2016/04/07, 00:21:09",97094.0,1631.0,8,2993,How to interpret Go stacktrace,which can also be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L285
Datadog,36459827,36461477.0,2,"2016/04/06, 21:47:13",True,"2016/04/07, 00:21:09","2016/04/07, 00:21:09",97094.0,1631.0,8,2993,How to interpret Go stacktrace,The type definition for  Event  can be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L333
Datadog,36459827,36461477.0,2,"2016/04/06, 21:47:13",True,"2016/04/07, 00:21:09","2016/04/07, 00:21:09",97094.0,1631.0,8,2993,How to interpret Go stacktrace,The type definition for  Client  can be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L59
Datadog,36459827,36461477.0,2,"2016/04/06, 21:47:13",True,"2016/04/07, 00:21:09","2016/04/07, 00:21:09",97094.0,1631.0,8,2993,How to interpret Go stacktrace,"My question is, how do I interpret the memory addresses on this line, and more generally, any stack traces which involve typed variables as targets and as arguments?"
Datadog,36459827,36461477.0,2,"2016/04/06, 21:47:13",True,"2016/04/07, 00:21:09","2016/04/07, 00:21:09",97094.0,1631.0,8,2993,How to interpret Go stacktrace,"When I looked at  http://www.goinggo.net/2015/01/stack-traces-in-go.html  (which is the only information I was able to find on the subject), I didn't see anything about how to interpret the output when structs were involved."
Datadog,45664485,nan,0,"2017/08/13, 23:09:50",False,"2017/09/27, 09:54:39","2017/09/27, 09:54:39",1006944.0,4384.0,7,1682,Display count for a day using counter metrics in data dog,We have a counter metric in one our micro services which pushes data to DataDog.
Datadog,45664485,nan,0,"2017/08/13, 23:09:50",False,"2017/09/27, 09:54:39","2017/09/27, 09:54:39",1006944.0,4384.0,7,1682,Display count for a day using counter metrics in data dog,"I want to display the total count for given time frame, and also the count per day (X axis would have the date and Y axis would have count)."
Datadog,45664485,nan,0,"2017/08/13, 23:09:50",False,"2017/09/27, 09:54:39","2017/09/27, 09:54:39",1006944.0,4384.0,7,1682,Display count for a day using counter metrics in data dog,How do we achive this?
Datadog,45664485,nan,0,"2017/08/13, 23:09:50",False,"2017/09/27, 09:54:39","2017/09/27, 09:54:39",1006944.0,4384.0,7,1682,Display count for a day using counter metrics in data dog,I tried using  sum by  and  diff  with Query value representation.
Datadog,45664485,nan,0,"2017/08/13, 23:09:50",False,"2017/09/27, 09:54:39","2017/09/27, 09:54:39",1006944.0,4384.0,7,1682,Display count for a day using counter metrics in data dog,It gives the total number of the count for given time frame.
Datadog,45664485,nan,0,"2017/08/13, 23:09:50",False,"2017/09/27, 09:54:39","2017/09/27, 09:54:39",1006944.0,4384.0,7,1682,Display count for a day using counter metrics in data dog,But I would like to get a bar graph with the X axis as the date and the Y axis as the count.
Datadog,45664485,nan,0,"2017/08/13, 23:09:50",False,"2017/09/27, 09:54:39","2017/09/27, 09:54:39",1006944.0,4384.0,7,1682,Display count for a day using counter metrics in data dog,Is this possible in DataDog?
Datadog,45589632,nan,0,"2017/08/09, 14:39:24",False,"2017/08/10, 09:07:40","2017/08/10, 09:07:40",3805899.0,453.0,7,5212,Effect of fetchsize and batchsize on Spark,"I want to control the reading and writing speed to an RDB by  Spark  directly, yet the related parameters as the title already revealed seemingly were not working."
Datadog,45589632,nan,0,"2017/08/09, 14:39:24",False,"2017/08/10, 09:07:40","2017/08/10, 09:07:40",3805899.0,453.0,7,5212,Effect of fetchsize and batchsize on Spark,Can I conclude that  fetchsize  and  batchsize  didn't work with my testing method?
Datadog,45589632,nan,0,"2017/08/09, 14:39:24",False,"2017/08/10, 09:07:40","2017/08/10, 09:07:40",3805899.0,453.0,7,5212,Effect of fetchsize and batchsize on Spark,Or they do affect on the facet of reading and writing since the measure result is reasonable based on scale.
Datadog,45589632,nan,0,"2017/08/09, 14:39:24",False,"2017/08/10, 09:07:40","2017/08/10, 09:07:40",3805899.0,453.0,7,5212,Effect of fetchsize and batchsize on Spark,"I created two  m4.xlarge  Linux entities on  AWS , one is for the execution of  Spark , the other is for data storage on an RDB, using  Datadog  to watch the performance of the  Spark  application, especially on the reading and writing to the RDB."
Datadog,45589632,nan,0,"2017/08/09, 14:39:24",False,"2017/08/10, 09:07:40","2017/08/10, 09:07:40",3805899.0,453.0,7,5212,Effect of fetchsize and batchsize on Spark,"Spark  was in the standalone mode, and the application for test is simply pulling some data from a  MySQL  RDB, doing some computation, then pushing back to the  MySQL ."
Datadog,45589632,nan,0,"2017/08/09, 14:39:24",False,"2017/08/10, 09:07:40","2017/08/10, 09:07:40",3805899.0,453.0,7,5212,Effect of fetchsize and batchsize on Spark,Some details are as the following:
Datadog,45589632,nan,0,"2017/08/09, 14:39:24",False,"2017/08/10, 09:07:40","2017/08/10, 09:07:40",3805899.0,453.0,7,5212,Effect of fetchsize and batchsize on Spark,"JDBC properties are put in a file,  application.conf , like the following:"
Datadog,45589632,nan,0,"2017/08/09, 14:39:24",False,"2017/08/10, 09:07:40","2017/08/10, 09:07:40",3805899.0,453.0,7,5212,Effect of fetchsize and batchsize on Spark,"Logging while executing the application is enabled by  log4jx2 , within it, time for writing is measured."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,The problem:
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,I have three instances of a java application running in Kubernetes.
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,My application uses Apache Camel to read from a Kinesis stream.
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,I'm currently observing two related issues:
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"Each of the three running instances of my application is processing the records coming into the stream, when I only want each record to be processed once (I want three up and running for scaling purposes)."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"I was hoping that while one instance is processing record A, a second could be picking up record B, etc."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"Every time my application is re-deployed in Kubernetes, each instance starts every record all over again (in other words, it has no idea where it left off or which records have previously been processed)."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"After 5 minutes, the shard iterator that my application is using to poll kinesis times out."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"I know that this is normal behavior, but what I don't understand is why my application is not grabbing a new iterator."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,This screenshot shows the error from DataDog.
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"What I've tried: 
First off, I believe that this issue is caused by inconsistent shard iterator ids, and kinesis consumer ids across the three instances of my application, and across deploys."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"However, I have been unable to locate where these values are set in code, and how I could go about setting them."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"Of course, there may also be a better solution altogether."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"I have found very little documentation on Kinesis/Kubernetes/Camel working together, and so very little outside sources have been helpful."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"The documentation on  AWS Kinesis :: Apache Camel  is very limited, but what I have tried playing around with the iterator type and building a custom  Client Configuration ."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,"Let me know if you need any additional information, thanks."
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,Configuring the client:
Datadog,66177095,nan,0,"2021/02/12, 20:22:01",False,"2021/02/15, 20:59:22","2021/02/15, 20:59:22",15199494.0,61.0,6,82,Camel Application re-processing Kinesis records,My route:
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,Edit: Tarun's answer does exactly what I asked for.
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,Eugen's answer is also a very good solution.
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"I ended up accepting Tarun's answer as correct, but using Eugen's."
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"If you have a similar issue and are worried about other containers accessing the nginx status server, use Tarun's answer."
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"If you'd rather stick to Docker's normal hostname scheme, use Eugen's."
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,+++ Original Question +++
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,I have an application that I build with docker-compose.
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,I am trying to integrate monitoring through DataDog.
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"I'm using DataDog's Agent container, and so far everything is working."
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,I am trying to get nginx monitoring up and running by adapting  this tutorial .
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,My application is defined in a docker-compose file like this:
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"Per the tutorial, I've added a server block to nginx that looks like this:"
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"With this configuration, I can check the nginx status from within the nginx container."
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"So far, so good."
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"Now I would like to change the ""allow"" directive in the location block to allow access to the datadog-agent service only."
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"However, I don't know the IP of the datadog-agent."
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,"When configuring access to the Flask uwsgi server, I was able to use directives like this:"
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,But this doesn't seem to work for allow directives; if I try:
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,I get the following error:
Datadog,45358188,45359614.0,3,"2017/07/27, 21:13:17",True,"2020/04/17, 20:27:04","2017/07/27, 23:07:17",3334079.0,3108.0,6,4847,Restrict access to nginx server location to a specific Docker container with &quot;allow&quot; directive,How can I safely expose the nginx status to my monitoring container?
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,I am trying to connect to a MySQL database using python but I am getting a strange error.
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,It is compounded by the fact that I can use the same connection values from the  mysql  console command and it connects with no problems.
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,Here is the exact code I am using:
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,"import pymysql
    from checks import AgentCheck"
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,This is the error that I am getting:
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,I am running this code on a Ubuntu box and I though initially that it might be because the SSL CA is a self generated cert.
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,"So I followed the steps  here  But, it did not make any difference."
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,Also I have verified that the process that is running this code has full access to the cert files
Datadog,33929394,nan,1,"2015/11/26, 03:52:28",False,"2015/12/02, 00:18:15","2017/03/20, 12:18:27",5004.0,12944.0,6,2228,pymysql cannot connect with SSL,Any ideas what else might be causing this?
Datadog,60842629,nan,1,"2020/03/25, 05:30:49",True,"2020/03/25, 09:59:29",nan,1212596.0,64185.0,5,339,How to serialize 64-bit integer in JavaScript?,Datadog Tracing API  requires 64-bit integers serialized as JSON numbers.
Datadog,60842629,nan,1,"2020/03/25, 05:30:49",True,"2020/03/25, 09:59:29",nan,1212596.0,64185.0,5,339,How to serialize 64-bit integer in JavaScript?,How can I create JSON with 64-bit integer numbers using JavaScript?
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,I have a graphql server with multiple endpoints.
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,"It is basically just a CRUD app, so I'm honestly not sure why there's a memory leak."
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,The only potentially leaky endpoint I have is one that uploads pics to S3.
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,I've been looking around and have tried taking heap snapshots and comparing them but I'm not even sure which endpoint is the culprit.
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,This is the flow I've been following:
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,Is this the correct flow for finding a memory leak?
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,Is there a better way of doing this without having to guess which endpoint it is coming from?
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,Are there perhaps tools I can use online that can help me find the source of the memory leak in production without having to guess like this?
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,Perhaps something like Datadog or something?
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,"Update: From Heroku's metrics, it looks like the memory usage increases every time a request is made?"
Datadog,58897099,nan,3,"2019/11/17, 04:50:49",False,"2019/12/21, 04:16:20","2019/11/17, 18:29:05",1555312.0,22597.0,5,1634,How can I debug a memory leak on my Apollo GraphQL server?,But my src/index.js file doesn't do anything special:
Datadog,47299307,nan,1,"2017/11/15, 06:19:41",True,"2021/01/29, 10:19:44",nan,1390817.0,268.0,5,1538,Arithmetic operations for Stackdriver monitoring charts,I'm trying to setup a Stackdriver dashboard for my custom metrics that my services provide.
Datadog,47299307,nan,1,"2017/11/15, 06:19:41",True,"2021/01/29, 10:19:44",nan,1390817.0,268.0,5,1538,Arithmetic operations for Stackdriver monitoring charts,In particular I'm starting with general  custom/grpc/time_ms  metric that is a gauge and have  status  label on it.
Datadog,47299307,nan,1,"2017/11/15, 06:19:41",True,"2021/01/29, 10:19:44",nan,1390817.0,268.0,5,1538,Arithmetic operations for Stackdriver monitoring charts,I'd love to be able to set up a chart and alert for success rate of the metric(something like  count:custom/grpc/time_ms{status:OK} / count:custom/grpc/time_ms{*} ).
Datadog,47299307,nan,1,"2017/11/15, 06:19:41",True,"2021/01/29, 10:19:44",nan,1390817.0,268.0,5,1538,Arithmetic operations for Stackdriver monitoring charts,With my previous project I used Datadog and it was  pretty easy to do so there .
Datadog,47299307,nan,1,"2017/11/15, 06:19:41",True,"2021/01/29, 10:19:44",nan,1390817.0,268.0,5,1538,Arithmetic operations for Stackdriver monitoring charts,But I don't see any similar functionality neither in the UI nor in Stackdriver documentation.
Datadog,47299307,nan,1,"2017/11/15, 06:19:41",True,"2021/01/29, 10:19:44",nan,1390817.0,268.0,5,1538,Arithmetic operations for Stackdriver monitoring charts,So I was wondering if it's not documented or simply not supported?
Datadog,39301868,nan,0,"2016/09/03, 02:31:23",False,"2016/09/03, 02:31:23",nan,2371805.0,2563.0,5,217,Rails require statements in environment config files,"In my  config\environments\development.rb  and  config\environments\production.rb  files, I set some global variables."
Datadog,39301868,nan,0,"2016/09/03, 02:31:23",False,"2016/09/03, 02:31:23",nan,2371805.0,2563.0,5,217,Rails require statements in environment config files,"In the example below, I have a a  Redis  instance that points to our cache, and a  Statsd  instance that points to the DataDog agent."
Datadog,39301868,nan,0,"2016/09/03, 02:31:23",False,"2016/09/03, 02:31:23",nan,2371805.0,2563.0,5,217,Rails require statements in environment config files,"In the case of Redis, I added  gem 'redis'  to my gem file, ran  bundle install  and everything worked fine."
Datadog,39301868,nan,0,"2016/09/03, 02:31:23",False,"2016/09/03, 02:31:23",nan,2371805.0,2563.0,5,217,Rails require statements in environment config files,"In the case of StatsD, however, it seems that I need to also add  require 'statsd'  at the top of the  development.rb  and  production.rb  files in order to be able to create the instance."
Datadog,39301868,nan,0,"2016/09/03, 02:31:23",False,"2016/09/03, 02:31:23",nan,2371805.0,2563.0,5,217,Rails require statements in environment config files,"Of course, I also added  gem 'dogstatsd-ruby'  to my gem file and ran  bundle install , but that didn't seem to be enough."
Datadog,39301868,nan,0,"2016/09/03, 02:31:23",False,"2016/09/03, 02:31:23",nan,2371805.0,2563.0,5,217,Rails require statements in environment config files,"If I don't add the  require  statement at the top of the config files, I get the following error when I try to run my Rails app:"
Datadog,39301868,nan,0,"2016/09/03, 02:31:23",False,"2016/09/03, 02:31:23",nan,2371805.0,2563.0,5,217,Rails require statements in environment config files,"Can anyone explain why I have to add the  require  statement only in this particular case (StatsD), or is there is a better way to do this?"
Datadog,39301868,nan,0,"2016/09/03, 02:31:23",False,"2016/09/03, 02:31:23",nan,2371805.0,2563.0,5,217,Rails require statements in environment config files,Thanks!
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"Note: according to tests (see Edit below), this occurs only on a Linux machine."
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),I have an ASP.NET Core Blazor application (using server-side hosting model) running on a Raspberry Pi.
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),Part of the application's functionality is to dim/brighten screen based on when system was last interacted with.
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"To do that, every 1 second or so I spawn a terminal child-process to run  xprintidle , parse its output, and act accordingly."
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"I use DataDog for monitoring, and I am having a memory leak until the system crashes (it takes a few days to use up all memory, but it does occur eventually):"
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"I have pinpointed that the following method is what leaks memory - if I skip calling it and use some constant timespan, the memory does not leak:
I have following code to do so:"
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"I spent a lot of time (over span of months - literally) trying various changes to solve this issue -  WaitForExitAsync  was overhauled a lot, tried different ways of disposing."
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),I attempted to call GC.Collect() periodically.
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),Also tried running the application with both Server and Workstation GC mode.
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"As I mentioned earlier, I am pretty sure it's this code that leaks - if I don't call  ExecuteAndWaitAsync , there's no memory leak."
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),The result class is also not stored by the caller - it simply parses a value and uses it right away:
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),Am I missing something?
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"Is it Process class leaking, or the way I read output?"
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"EDIT : As people in comments asked, I created minimum runnable code, basically fetching all relevant methods in a single class and execute in a loop."
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"The code is available as a gist:  https://gist.github.com/TehGM/c953b670ad8019b2b2be6af7b14807c2 
I ran it both on my Windows machine and Raspberry Pi."
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"On Windows, memory seemed stable, however on Raspberry Pi it was clearly leaking."
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),I tried both  xprintidle  and  ifconfig  to make sure it's not an issue with xprintidle only.
Datadog,63049334,63340327.0,1,"2020/07/23, 10:46:44",True,"2020/08/10, 15:35:10","2020/08/03, 12:25:29",4644581.0,101.0,4,343,C# Starting Process leaks memory even though Killed and Disposed (on Linux),"Tried both .NET Core 3.0 and .NET Core 3.1, and effect was largely the same."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,I have a springboot application which I'm trying to instrument using bytebuddy.
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,I'm running into classpath issues which I'm not able to understand.
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"Firstly, the following is other literature on this:"
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,https://github.com/raphw/byte-buddy/issues/473
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,https://github.com/raphw/byte-buddy/issues/87
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,Unable to instrument apache httpclient using javaagent for spring boot uber jar application
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,https://github.com/raphw/byte-buddy/issues/109
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,https://github.com/raphw/byte-buddy/issues/473
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,https://github.com/raphw/byte-buddy/issues/489
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,https://github.com/spring-projects/spring-boot/issues/4868
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,https://github.com/alibaba/transmittable-thread-local/issues/161
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"Problem is that Spring-boot bundles the application into one uber-jar, which contains other jars inside it"
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"A thing to note here is, that if I run the application using IntelliJ, it doesn't use the uber-jar and runs via main class with a bunch of jars as classpath arguments."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"Due to this difference, When running via uber-jar( java -jar target/demo-0.0.1-SNAPSHOT.jar ), some classes are not available at the time the Agent runs."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"The classes are loadable at the time of application main, as spring-boot uses its own classloader, which is created at some time b/w agent and application main methods."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,I'll describe the behaviour at various points of time below:
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,Both spring and javax classes are not loaded as they are not directly in the classpath as per the App Classloader.
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"It's part of an inner jar, something like  app.jar!/BOOT-INF/lib/some.jar 
App Classloader won't be able to load this."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"After loading,  Class.forName(""javax.servlet.http.HttpServletRequest"").classLoader  is also the same  LaunchedURLClassLoader ."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"I have two classes,  SpringBootInterceptor  (which contains the intercept method) and  SpringBootInterceptorOne  (which contains the entry and exit method)"
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,The function  intercept()  is called in the premain path.
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,The function  visit  is called when the type is actually attempted to load.
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,At that point bytebuddy tris to intercept those methods and modify bytecode.
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"In the current version of this code, the function  intercept()  throws  ClassNotFoundException  and interception doesn't happen."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"So, I tried to change the code, to the following:"
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"Here, basically loading the class lazily."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,Helper functions from datadog:
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,https://github.com/DataDog/dd-trace-java/blob/master/dd-java-agent/agent-tooling/src/main/java/datadog/trace/agent/tooling/ByteBuddyElementMatchers.java
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"After doing this, the  intercept  function doesn't fail, but the  visit()  function behaves strangely:"
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"At this point,"
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"Next, I tried the following:"
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"This causes the interceptor to be ""installed""."
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,The entry and exit methods are called okay.
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,But casting the arguments to proper types gives error for the types of the arguments
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,Here is SpringBootInterceptorOne for reference:
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,UPDATE:
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,"As per the advice offered in an answer, I tried the following:"
Datadog,60237664,60253636.0,1,"2020/02/15, 11:59:32",True,"2020/02/17, 23:43:26","2020/02/17, 15:33:10",459384.0,8884.0,4,751,Classpath problems while Instrumenting Springboot application,It again throws Error:
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,I have a project built in Lumen (php Framework) hosted on a docker container built from alpine as base image using apache2 server with php 7.x
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,Here's part of my Dockerfile:
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,The purpose of this project is to receive http post requests (i.e.
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,webhook events from external system) and process them.
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,"When the project is deployed, it runs fine for several days before this error starts showing up in our datadog logs:"
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,[core:warn] [pid 9] (99)Address not available: AH00056: connect to listener on [::]:80
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,"When this error occurs, the site/project is not publicly accessible but the apache is still running."
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,"If i restart the container, everything goes back to normal."
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,"Upon investigating this further, I noticed this happens every time my api is hit simultaneously."
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,I.e.
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,"3 days ago, project was hit with 145 request simultaneously and since then the app is no longer accessible."
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,Apache is refusing to serve any new request but the container is up and running and there's plenty of memory/disk-space available to the container.
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,Any idea what causes this?
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,do I need to optimise the mpm.conf to allow for more workers / child processes etc.?
Datadog,54185496,56645177.0,1,"2019/01/14, 18:30:08",True,"2019/06/18, 11:49:21","2019/01/14, 18:50:50",2332336.0,19520.0,4,1438,Docker Apache with PHP - Address not available: AH00056: connect to listener on [::]:80,I am currently using stock config.
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,I'm trying to learn how to use docker and am having some troubles.
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,I'm using a  docker-compose.yaml  file for running a python script that connects to a mysql container and I'm trying to use  ddtrace  to send traces to datadog.
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,I'm using the following image from  this github page from datadog
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,And my  docker-compose.yaml  looks like
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,"So then I'm running the command  docker-compose run --rm ddtrace-test python test.py , where  test.py  looks like"
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,"And when I run the command, I'm returned with"
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,I'm not sure what this error means.
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,"When I use my key and run from local instead of over a docker image, it works fine."
Datadog,52390678,52392480.0,1,"2018/09/18, 19:11:07",True,"2018/09/18, 21:18:13",nan,4498684.0,1654.0,4,2123,Sending ddtrace from docker,What could be going wrong here?
Datadog,50096290,50097560.0,1,"2018/04/30, 11:00:03",True,"2018/04/30, 12:52:06","2018/04/30, 12:26:39",2801947.0,4483.0,4,1230,What Kafka broker metrics should be monitored if producer side ack lag is very high,Are there some broker metrics we can use to monitor Kafka broker if acknowledgment lag is very high in the producer side.
Datadog,50096290,50097560.0,1,"2018/04/30, 11:00:03",True,"2018/04/30, 12:52:06","2018/04/30, 12:26:39",2801947.0,4483.0,4,1230,What Kafka broker metrics should be monitored if producer side ack lag is very high,We are using datadog to monitor producer and Kafka broker side.
Datadog,50096290,50097560.0,1,"2018/04/30, 11:00:03",True,"2018/04/30, 12:52:06","2018/04/30, 12:26:39",2801947.0,4483.0,4,1230,What Kafka broker metrics should be monitored if producer side ack lag is very high,It can be seen that the producer ack lag is more than 10 secs.
Datadog,50096290,50097560.0,1,"2018/04/30, 11:00:03",True,"2018/04/30, 12:52:06","2018/04/30, 12:26:39",2801947.0,4483.0,4,1230,What Kafka broker metrics should be monitored if producer side ack lag is very high,"However, on the broker side, I feel like only using  message.in.rate  and  kafka.net.bytes_in.rate  are not very efficient."
Datadog,50096290,50097560.0,1,"2018/04/30, 11:00:03",True,"2018/04/30, 12:52:06","2018/04/30, 12:26:39",2801947.0,4483.0,4,1230,What Kafka broker metrics should be monitored if producer side ack lag is very high,It would be better we can have some LAG metrics in the broker side to indicate  the broker is fully loaded to acknowledge back the producer.
Datadog,50096290,50097560.0,1,"2018/04/30, 11:00:03",True,"2018/04/30, 12:52:06","2018/04/30, 12:26:39",2801947.0,4483.0,4,1230,What Kafka broker metrics should be monitored if producer side ack lag is very high,"Also, we only use  kafka.acks = 1  for partition leader."
Datadog,50096290,50097560.0,1,"2018/04/30, 11:00:03",True,"2018/04/30, 12:52:06","2018/04/30, 12:26:39",2801947.0,4483.0,4,1230,What Kafka broker metrics should be monitored if producer side ack lag is very high,I wonder does anyone has some experience about it and any advice is welcome.
Datadog,50096290,50097560.0,1,"2018/04/30, 11:00:03",True,"2018/04/30, 12:52:06","2018/04/30, 12:26:39",2801947.0,4483.0,4,1230,What Kafka broker metrics should be monitored if producer side ack lag is very high,:) Thanks in advance.
Datadog,44855220,44927321.0,3,"2017/07/01, 00:45:02",True,"2017/07/05, 16:14:08",nan,2794837.0,209.0,4,1013,Sending data from one pod to another pod running specifically on the same host (DaemonSet),"I have an agent (datadog agent but could be something else) running on all the nodes of my cluster, deployed through a DaemonSet."
Datadog,44855220,44927321.0,3,"2017/07/01, 00:45:02",True,"2017/07/05, 16:14:08",nan,2794837.0,209.0,4,1013,Sending data from one pod to another pod running specifically on the same host (DaemonSet),"This agent is collecting diverse metrics about the host: cpu and memory usage, IO, which containers are running."
Datadog,44855220,44927321.0,3,"2017/07/01, 00:45:02",True,"2017/07/05, 16:14:08",nan,2794837.0,209.0,4,1013,Sending data from one pod to another pod running specifically on the same host (DaemonSet),"It can also collect custom metrics, by listening on a specific port 1234."
Datadog,44855220,44927321.0,3,"2017/07/01, 00:45:02",True,"2017/07/05, 16:14:08",nan,2794837.0,209.0,4,1013,Sending data from one pod to another pod running specifically on the same host (DaemonSet),How can I send data from a pod to the instance of the agent running on the same node than the pod?
Datadog,44855220,44927321.0,3,"2017/07/01, 00:45:02",True,"2017/07/05, 16:14:08",nan,2794837.0,209.0,4,1013,Sending data from one pod to another pod running specifically on the same host (DaemonSet),If I use a Kubernetes service the calls to send the metric will be load balanced across all my agents and I'll lose the correlation between the pod emitting the metric and the host it's running on.
Datadog,59881685,nan,2,"2020/01/23, 17:13:53",True,"2020/09/05, 18:28:32",nan,740067.0,4509.0,3,1404,Any way of monitoring Airflow DAG&#39;s execution time?,I'd like to use Airflow with Statsd and DataDog to monitor if DAG takes e.g.
Datadog,59881685,nan,2,"2020/01/23, 17:13:53",True,"2020/09/05, 18:28:32",nan,740067.0,4509.0,3,1404,Any way of monitoring Airflow DAG&#39;s execution time?,twice time as its previous execution(s).
Datadog,59881685,nan,2,"2020/01/23, 17:13:53",True,"2020/09/05, 18:28:32",nan,740067.0,4509.0,3,1404,Any way of monitoring Airflow DAG&#39;s execution time?,"So, I need some kind of a real-time timer for a DAG (or  operator )."
Datadog,59881685,nan,2,"2020/01/23, 17:13:53",True,"2020/09/05, 18:28:32",nan,740067.0,4509.0,3,1404,Any way of monitoring Airflow DAG&#39;s execution time?,I'm aware that Airflow supports  some metrics .
Datadog,59881685,nan,2,"2020/01/23, 17:13:53",True,"2020/09/05, 18:28:32",nan,740067.0,4509.0,3,1404,Any way of monitoring Airflow DAG&#39;s execution time?,"However, to my understanding all of the metrics are related to finished tasks/DAGs, right?"
Datadog,59881685,nan,2,"2020/01/23, 17:13:53",True,"2020/09/05, 18:28:32",nan,740067.0,4509.0,3,1404,Any way of monitoring Airflow DAG&#39;s execution time?,"So, It's not the solution, because I'd like to monitor running DAGs."
Datadog,59881685,nan,2,"2020/01/23, 17:13:53",True,"2020/09/05, 18:28:32",nan,740067.0,4509.0,3,1404,Any way of monitoring Airflow DAG&#39;s execution time?,"I also considered the  timeout_execution / SLA  features, but they are not suitable for this use-case"
Datadog,59881685,nan,2,"2020/01/23, 17:13:53",True,"2020/09/05, 18:28:32",nan,740067.0,4509.0,3,1404,Any way of monitoring Airflow DAG&#39;s execution time?,"I'd like to be notified that some DAG hangs, but I don't want to kill it."
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,"What is the difference between ""hikaricp.connections. """
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,"and ""jdbc.connections. """
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,meter names?
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,I have a Spring Boot 2 application that is defaulting to the Hikari connection pool mechanism and I am tring to understand how to best monitor the database connections in production.
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,"After visualizing my metrics in Datadog, I am seeing a slight difference in the metric data for both hikariCP.connections.active and jdbc.connections.active."
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,Are the JDBC meter names duplicates?
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,Should one be used over the other or does it not matter.
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,I have been struggling to find more detailed documentation on this.
Datadog,59879331,nan,1,"2020/01/23, 15:08:04",False,"2021/01/27, 13:40:25","2020/01/23, 16:18:24",7776674.0,33.0,3,360,HikariCP vs. JDBC Metrics Spring Boot2,Any help is much appreciated.
Datadog,59678757,59679096.0,1,"2020/01/10, 11:26:00",True,"2020/01/10, 12:04:43","2020/01/10, 11:54:51",12413270.0,211.0,3,2295,awsvpc: Network Configuration is not valid for the given networkMode of this task definition,My task definition:
Datadog,59678757,59679096.0,1,"2020/01/10, 11:26:00",True,"2020/01/10, 12:04:43","2020/01/10, 11:54:51",12413270.0,211.0,3,2295,awsvpc: Network Configuration is not valid for the given networkMode of this task definition,My service defintion:
Datadog,59678757,59679096.0,1,"2020/01/10, 11:26:00",True,"2020/01/10, 12:04:43","2020/01/10, 11:54:51",12413270.0,211.0,3,2295,awsvpc: Network Configuration is not valid for the given networkMode of this task definition,I get the following error -
Datadog,59678757,59679096.0,1,"2020/01/10, 11:26:00",True,"2020/01/10, 12:04:43","2020/01/10, 11:54:51",12413270.0,211.0,3,2295,awsvpc: Network Configuration is not valid for the given networkMode of this task definition,Is there something I am missing here?
Datadog,59678757,59679096.0,1,"2020/01/10, 11:26:00",True,"2020/01/10, 12:04:43","2020/01/10, 11:54:51",12413270.0,211.0,3,2295,awsvpc: Network Configuration is not valid for the given networkMode of this task definition,Looking at the Terraform docs and GitHub issues this should have worked.
Datadog,59678757,59679096.0,1,"2020/01/10, 11:26:00",True,"2020/01/10, 12:04:43","2020/01/10, 11:54:51",12413270.0,211.0,3,2295,awsvpc: Network Configuration is not valid for the given networkMode of this task definition,Is it related to running Datadog as a daemon?
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,I trying to run 3 containers and get the error for all of them.
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,"I found the source code, but can't find the answer to why this is happening."
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,"Also, tried to look at the internet and can't find anyone with the same error."
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,"Source code:
 https://github.com/DataDog/datadog-agent/blob/eb35254e9e13165b4148fc9280ef79e2d6bf8235/pkg/util/docker/containers.go"
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,The error from /var/log/syslog:
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,"process-agent[30759]: 2019-12-11 08:58:17 UTC | PROCESS | ERROR |
  (pkg/util/docker/containers.go:110 in ListContainers) | Failed to get
  host IPs."
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,Container XXXXX will be missing network info: %!s()
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,The containers running on the Host network.
Datadog,59282255,nan,0,"2019/12/11, 11:13:32",False,"2020/01/05, 11:51:13",nan,3994390.0,47.0,3,326,Containers - Failed to get host IPs,Thanks
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,We developed our team's new service with spring-webflux.
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,It has been working well.
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,Only one thing we could not figure out is below log
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,"Error [reactor.netty.ReactorNetty$InternalNettyException: io.netty.channel.ExtendedClosedChannelException] for HTTP GET ""[TARGET_URL]"", but ServerHttpResponse already committed (200 OK)"
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,"Although it does not frequently happen, it is logged in our datadog log."
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,The service is a kind of middle man.
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,"One service sends ""get"" request to it, and then it calls other backends to gather data, then return."
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,"In the service layer, we use Mono.zip to combine all response from various backends."
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,Both clients and backends are traditional thread base sprint applications.
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,Our code to build webclient is:
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,connection timeout is set to 2500 milliseconds.
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,read timeout is 50-90ms depends on backends.
Datadog,58345839,nan,0,"2019/10/11, 20:16:35",False,"2019/10/11, 22:41:13","2019/10/11, 22:41:13",12202436.0,31.0,3,1082,When io.netty.channel.ExtendedClosedChannelException] for... but ServerHttpResponse already committed (200 OK) could happen?,Thanks!
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Docker daemon got crashed after short span of time.
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Lately Docker services of stacks doesn't get properly up and resulted in app crash which only got fixed when i removed all the stacks and redeployed them.
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,I'm running my whole Android app and Other APIs on docker swarm cluster.
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,*I have my machine running on Google Cloud platform with around 75 CPUs and 250G memory which is more than enough for all the services I'm running on my machine.
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"I have haproxy in frontend which does reverse proxy, backend as python flask api with 5 replicas, Database connectivity through pgbouncer."
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"Else Logspout, datadog, portainer, redis, etc."
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,*
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"I couldn't understand that even if i have enough resources, proper setup system with enough max_pids still the daemon crashed."
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"
Generally Necessary:
- cgroup hierarchy: properly mounted [/sys/fs/cgroup]"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,apparmor: enabled and tools installed
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NAMESPACES: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NET_NS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_PID_NS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IPC_NS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_UTS_NS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUPS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUP_CPUACCT: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUP_DEVICE: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUP_FREEZER: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUP_SCHED: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CPUSETS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_MEMCG: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_KEYS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_VETH: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_BRIDGE: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_BRIDGE_NETFILTER: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NF_NAT_IPV4: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_NF_FILTER: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_NF_TARGET_MASQUERADE: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NETFILTER_XT_MATCH_ADDRTYPE: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NETFILTER_XT_MATCH_CONNTRACK: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NETFILTER_XT_MATCH_IPVS: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_NF_NAT: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NF_NAT: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NF_NAT_NEEDED: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_POSIX_MQUEUE: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Optional Features:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_USER_NS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_SECCOMP: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUP_PIDS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_MEMCG_SWAP: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_MEMCG_SWAP_ENABLED: missing
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,(cgroup swap accounting is currently enabled)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_LEGACY_VSYSCALL_EMULATE: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_BLK_CGROUP: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_BLK_DEV_THROTTLING: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IOSCHED_CFQ: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CFQ_GROUP_IOSCHED: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUP_PERF: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUP_HUGETLB: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NET_CLS_CGROUP: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CGROUP_NET_PRIO: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CFS_BANDWIDTH: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_FAIR_GROUP_SCHED: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_RT_GROUP_SCHED: missing
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_NF_TARGET_REDIRECT: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_VS: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_VS_NFCT: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_VS_PROTO_TCP: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_VS_PROTO_UDP: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IP_VS_RR: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_EXT4_FS: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_EXT4_FS_POSIX_ACL: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_EXT4_FS_SECURITY: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Network Drivers:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""overlay"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_VXLAN: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_BRIDGE_VLAN_FILTERING: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Optional (for encrypted networks):
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CRYPTO: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CRYPTO_AEAD: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CRYPTO_GCM: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CRYPTO_SEQIV: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_CRYPTO_GHASH: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_XFRM: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_XFRM_USER: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_XFRM_ALGO: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_INET_ESP: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_INET_XFRM_MODE_TRANSPORT: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""ipvlan"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_IPVLAN: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""macvlan"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_MACVLAN: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_DUMMY: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""ftp,tftp client in container"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NF_NAT_FTP: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NF_CONNTRACK_FTP: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NF_NAT_TFTP: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_NF_CONNTRACK_TFTP: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Storage Drivers:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""aufs"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_AUFS_FS: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""btrfs"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_BTRFS_FS: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_BTRFS_FS_POSIX_ACL: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""devicemapper"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_BLK_DEV_DM: enabled
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_DM_THIN_PROVISIONING: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""overlay"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CONFIG_OVERLAY_FS: enabled (as module)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"""zfs"":"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,/dev/zfs: missing
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,zfs command: missing
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,zpool command: missing
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Limits:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,NEED HELP ON UNDERSTANDING THE ISSUE HERE.
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,RESULT OF  docker info  here.
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"
Containers: 170"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Running: 167
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Paused: 0
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Stopped: 3
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Images: 144
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Server Version: 18.09.7
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Storage Driver: overlay2
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Backing Filesystem: extfs
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Supports d_type: true
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Native Overlay Diff: true
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Logging Driver: json-file
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Cgroup Driver: cgroupfs
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Plugins:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Volume: local
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Network: bridge host macvlan null overlay
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Swarm: active
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,NodeID: uqkfe247qfql50b1bft3r205b
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Is Manager: true
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,ClusterID: fe79jmqus0l6zsa7kl41cbqa9
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Managers: 1
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Nodes: 2
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Default Address Pool: 10.0.0.0/8
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,SubnetSize: 24
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Orchestration:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Task History Retention Limit: 5
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Raft:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Snapshot Interval: 10000
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Number of Old Snapshots to Retain: 0
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Heartbeat Tick: 1
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Election Tick: 10
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Dispatcher:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Heartbeat Period: 5 seconds
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CA Configuration:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Expiry Duration: 3 months
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Force Rotate: 0
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Autolock Managers: false
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Root Rotation In Progress: false
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Node Address: 10.160.0.30
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,"Manager Addresses:
  10.160.0.30:2377"
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Runtimes: runc
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Default Runtime: runc
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Init Binary: docker-init
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,containerd version:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,runc version: N/A
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,init version: v0.18.0 (expected: fec3683b971d9c3ef73f284f176672c44b448662)
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Security Options:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,apparmor
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,seccomp
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Profile: default
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Kernel Version: 4.15.0-1040-gcp
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Operating System: Ubuntu 18.04.3 LTS
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,OSType: linux
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Architecture: x86_64
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,CPUs: 76
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Total Memory: 246GiB
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Name: rc-manager-instance
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,ID: 2PEM:4AF6:47RA:EMDM:CIMD:H4OC:5MNG:SXNI:ERFB:ML5G:O3YI:6VWA
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Docker Root Dir: /var/lib/docker
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Debug Mode (client): false
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Debug Mode (server): false
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Registry:  https://index.docker.io/v1/
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Labels:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Experimental: false
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Insecure Registries:
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,10.160.0.30:7000
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,127.0.0.0/8
Datadog,57753638,nan,0,"2019/09/02, 11:25:23",False,"2019/09/02, 11:37:52","2019/09/02, 11:37:52",12008760.0,31.0,3,626,Docker Daemon is crashing and the docker services are not really up lately,Live Restore Enabled: false
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,i am trying to develop generic terraform modules to support data-dog monitors and let the user of the modules to append resources and/or override resources in side generic modules.
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,"terraform  overrides  feature works fine without modules, But not working when using modules."
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,how to override some of the resource parameters inside modules?
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,Requirements:
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,"/modules/datadog/monitors.tf  contains list of resources, each resource represents a generic datadog monitor with default parameter values."
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,Each  individual application may choose to override one or more parameters inside each resource .
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,"/application-1/monitors.tf  contains module with source as  /modules/datadog/  , some more monitors that are not covered in generic monitors and some variables."
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,/application-1/monitors.tf
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,/modules/datadog/monitors.tf
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,Solution 1 : Add overrides.tf to  /modules/datadog  Directory.
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,"terraform  override feature  merges content in overrides.tf to 
 configuration defined in monitors.tf."
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,But the problem with this solution is each application specific overrides.tf needs to be copied over to /modules/datadog Directory before running apply command.
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,overrides.tf
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,Solution 2 : can i use overrides with modules?
Datadog,52934832,nan,1,"2018/10/22, 20:37:18",True,"2018/10/23, 04:17:01","2018/10/23, 04:17:01",5009799.0,120.0,3,1364,How to &quot;override&quot; some of the resource parameters when using terraform modules?,"i tried to override resource parameters by copying overrides.tf to /application-1/ Directory, But terraform is not overriding resources, instead it is considering both as different resources."
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,When I start consumer with single instance it is show in consumer group but it is not consuming data from topic.
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,After that if I start another consumer and my first consumer start consuming data but latest consumer instance doesn't have any partition assigned to it.
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,below is the info log when first consumer instance starts.
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,"INFO:kafka.client:Bootstrapping cluster metadata from [(u'kafka-broker1.ap-south-1.staging.internal', 9092, 0)]
  INFO:kafka.conn:: connecting to 172.31.1.66:9092
  INFO:kafka.client:Bootstrap succeeded: found 3 brokers and 19 topics."
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,INFO:kafka.conn:: Closing connection.
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,"INFO:kafka.conn:: connecting to 172.31.1.148:9092
  INFO:kafka.conn:Broker version identifed as 0.11.0
  INFO:kafka.conn:Set configuration api_version=(0, 11, 0) to skip auto check_version requests on startup
  INFO:kafka.consumer.subscription_state:Subscribing to pattern: /events/
  INFO:kafka.conn:: connecting to 172.31.1.70:9092
  INFO:kafka.cluster:Group coordinator for datadog is BrokerMetadata(nodeId=1, host=u'kafka-broker1.ap-south-1.staging.internal', port=9092, rack=None)
  INFO:kafka.coordinator:Discovered coordinator 1 for group datadog
  INFO:kafka.conn:: connecting to 172.31.1.66:9092
  INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set([]) for group datadog
  INFO:kafka.coordinator:(Re-)joining group datadog
  INFO:kafka.consumer.subscription_state:Updating subscribed topics to: [u'events']
  INFO:kafka.coordinator:Joined group 'datadog' (generation 843) with member_id kafka-python-1.3.5-e3c25fb3-39ea-4550-845f-9b663355b4f5
  INFO:kafka.coordinator:Successfully joined group datadog with generation 843
  INFO:kafka.consumer.subscription_state:Updated partition assignment: []
  INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([]) for group datadog"
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,When I start second instance first instance get partition assigned and second instance have 0 partition assigned and have same info log as first instance before starting second instance.
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,below is the info log of first instance after second instance started.
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,"INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([]) for group datadog
  WARNING:kafka.coordinator:Heartbeat failed for group datadog because it is rebalancing
  WARNING:kafka.coordinator:Heartbeat failed ([Error 27] RebalanceInProgressError); retrying
  INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set([]) for group datadog
  INFO:kafka.coordinator:(Re-)joining group datadog
  INFO:kafka.coordinator:Skipping heartbeat: no auto-assignment or waiting on rebalance
  INFO:kafka.coordinator:Joined group 'datadog' (generation 843) with member_id kafka-python-1.3.5-ddb66185-c615-4f31-9729-9384131f24c9
  INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range
  INFO:kafka.coordinator:Successfully joined group datadog with generation 843
  INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic=u'events', partition=0), TopicPartition(topic=u'events', partition=1), TopicPartition(topic=u'events', partition=2), TopicPartition(topic=u'events', partition=3), TopicPartition(topic=u'events', partition=4), TopicPartition(topic=u'events', partition=5), TopicPartition(topic=u'events', partition=6), TopicPartition(topic=u'events', partition=7), TopicPartition(topic=u'events', partition=8), TopicPartition(topic=u'events', partition=9)]
  INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([TopicPartition(topic=u'events', partition=6), TopicPartition(topic=u'events', partition=7), TopicPartition(topic=u'events', partition=8), TopicPartition(topic=u'events', partition=9), TopicPartition(topic=u'events', partition=0), TopicPartition(topic=u'events', partition=1), TopicPartition(topic=u'events', partition=2), TopicPartition(topic=u'events', partition=3), TopicPartition(topic=u'events', partition=4), TopicPartition(topic=u'events', partition=5)]) for group datadog"
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,"We always have 1 consumer instance with unassigned partitions on all the cases,  last consumer instance always have 0 partition assigned"
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,**Below are the screenshot for the same **
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,No partitions assigned to first consumer instance
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,No partitions assigned to last consumer instance
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,We also suspect clustering of kafka.
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,When we had only 1 kafka node partition assignment to consumer instance and re-balancing was working fine.
Datadog,50577852,nan,0,"2018/05/29, 09:41:06",False,"2018/05/30, 14:19:37","2018/05/30, 14:19:37",1534977.0,31.0,3,3303,Partition are not getting assigned to Kafka consumer instance,But after going into multi-node cluster we are facing this issue
Datadog,48807945,nan,1,"2018/02/15, 15:01:17",False,"2018/02/17, 02:52:20",nan,956974.0,1519.0,3,259,Dropwizard metrics how to count for an action without carrying forward the counter value,I have a very simple case where I want to see how many time a user click on the ButtonA in my app.
Datadog,48807945,nan,1,"2018/02/15, 15:01:17",False,"2018/02/17, 02:52:20",nan,956974.0,1519.0,3,259,Dropwizard metrics how to count for an action without carrying forward the counter value,I'm using DropWizard metrics counter to archive this and the coursera reporter to report them to DataDog every 1 minutes.
Datadog,48807945,nan,1,"2018/02/15, 15:01:17",False,"2018/02/17, 02:52:20",nan,956974.0,1519.0,3,259,Dropwizard metrics how to count for an action without carrying forward the counter value,But what is happening is that this counter doesn't behave like I thought it would.
Datadog,48807945,nan,1,"2018/02/15, 15:01:17",False,"2018/02/17, 02:52:20",nan,956974.0,1519.0,3,259,Dropwizard metrics how to count for an action without carrying forward the counter value,"so for example if the buttonA has been clicked 4 times, the counter will keep the value 4 until the app restart which is not very useful."
Datadog,48807945,nan,1,"2018/02/15, 15:01:17",False,"2018/02/17, 02:52:20",nan,956974.0,1519.0,3,259,Dropwizard metrics how to count for an action without carrying forward the counter value,Is there an other metrics I'm not aware about that would keep a count and at each reports reset to 0 ?
Datadog,48807945,nan,1,"2018/02/15, 15:01:17",False,"2018/02/17, 02:52:20",nan,956974.0,1519.0,3,259,Dropwizard metrics how to count for an action without carrying forward the counter value,So that on Datadog dashboard I can easily sum all the count and manage to get the exact numbers even if the app is restarted it will not affect the metrics.
Datadog,45324189,nan,1,"2017/07/26, 13:22:43",True,"2019/12/06, 05:12:18","2019/12/06, 05:12:18",8047168.0,173.0,3,3987,Python install sub-package from package,it is possible to install some special sub-package from package?
Datadog,45324189,nan,1,"2017/07/26, 13:22:43",True,"2019/12/06, 05:12:18","2019/12/06, 05:12:18",8047168.0,173.0,3,3987,Python install sub-package from package,"For example, I want to create package with slack, datadog, sentry plugins (wrappers)."
Datadog,45324189,nan,1,"2017/07/26, 13:22:43",True,"2019/12/06, 05:12:18","2019/12/06, 05:12:18",8047168.0,173.0,3,3987,Python install sub-package from package,But I want to allow user what he wants to install.
Datadog,45324189,nan,1,"2017/07/26, 13:22:43",True,"2019/12/06, 05:12:18","2019/12/06, 05:12:18",8047168.0,173.0,3,3987,Python install sub-package from package,Like:
Datadog,45324189,nan,1,"2017/07/26, 13:22:43",True,"2019/12/06, 05:12:18","2019/12/06, 05:12:18",8047168.0,173.0,3,3987,Python install sub-package from package,Can it be done without separating all plugins to different packages?
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,I am getting Cassandra timeouts using the Phantom-DSL with the Datastax Cassandra driver.
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,"However, Cassandra does not seem to be overloaded."
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,Below is the exception I get:
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,And here are the statistics I get from the Cassandra Datadog connector over this time period:
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,You can see our read rate (per second) on the top-center graph.
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,Our CPU and memory usage are very low.
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,Here is how we are configuring the Datastax driver:
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,Our  nodetool cfstats  looks like this:
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,"When we ran  cassandra-stress , we didn't experience any issues: we were getting a steady 50k reads per second,  as expected ."
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,Cassandra has this error whenever I make my queries:
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,Why are we getting timeouts?
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,EDIT:  I had the wrong dashboard uploaded.
Datadog,42708258,nan,2,"2017/03/10, 02:55:24",False,"2017/03/12, 00:18:56","2017/03/12, 00:18:56",846389.0,1410.0,3,631,Cassandra Timeouts with No CPU Usage,Please see the new image.
Datadog,28436960,nan,3,"2015/02/10, 18:39:39",True,"2015/03/07, 22:02:50",nan,166612.0,3257.0,3,2033,How to receive webhook signal from 3rd party service,I'm using a SaaS for my AWS instance monitoring and Mandrill for email sending/campaigns.
Datadog,28436960,nan,3,"2015/02/10, 18:39:39",True,"2015/03/07, 22:02:50",nan,166612.0,3257.0,3,2033,How to receive webhook signal from 3rd party service,I had created a simple chart with  Zapier  but I'd rather like to host it myself.
Datadog,28436960,nan,3,"2015/02/10, 18:39:39",True,"2015/03/07, 22:02:50",nan,166612.0,3257.0,3,2033,How to receive webhook signal from 3rd party service,So my question is:
Datadog,28436960,nan,3,"2015/02/10, 18:39:39",True,"2015/03/07, 22:02:50",nan,166612.0,3257.0,3,2033,How to receive webhook signal from 3rd party service,How can I receive a webhook signal from Mandrill and then send it to Datadog from my server?
Datadog,28436960,nan,3,"2015/02/10, 18:39:39",True,"2015/03/07, 22:02:50",nan,166612.0,3257.0,3,2033,How to receive webhook signal from 3rd party service,Then again I guess hosting this script right on the same server I'm monitoring would be a terrible idea...
Datadog,28436960,nan,3,"2015/02/10, 18:39:39",True,"2015/03/07, 22:02:50",nan,166612.0,3257.0,3,2033,How to receive webhook signal from 3rd party service,"Basically I don't know how to ""receive the webhook"" so I can report it back to my Datadog service agent so it gets updated on their website."
Datadog,28436960,nan,3,"2015/02/10, 18:39:39",True,"2015/03/07, 22:02:50",nan,166612.0,3257.0,3,2033,How to receive webhook signal from 3rd party service,I get how to actually report the data to Datadog as explained here  http://docs.datadoghq.com/api/  but I just don't have a clue  how to host a listener for web hooks ?
Datadog,28436960,nan,3,"2015/02/10, 18:39:39",True,"2015/03/07, 22:02:50",nan,166612.0,3257.0,3,2033,How to receive webhook signal from 3rd party service,"Programming language isn't important, I don't have a preference for that case."
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,"I've been using Copperegg for a while now and have generally been happy with it until lately, where I have had a few issues."
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,It's being used to monitor a number of EC2 instances that must be up 24/7.
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,"Last week I was getting phantom alerts that servers had gone down when they hadn't, which I can cope with, but also I didn't get an alert when I should have done."
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,One server had high CPU for over 5 mins when the alert should be triggered after 1 minute.
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,"The Copperegg support weren't not all that helpful, merely agreeing that an alert should have been triggered."
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,The latter of those problems is unacceptable and if it were to happen again outside of working hours then serious problems will follow.
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,"So, I'm looking for alternative services that will do that same job."
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,"I've looked at Datadog and New Relic, but both have a significant problem in that they will only alert me of a problem 5 minutes after it's occurred, rather than the 1 minute I can get with Copperegg."
Datadog,21527387,nan,2,"2014/02/03, 14:24:04",True,"2015/03/19, 19:10:30","2014/02/03, 15:36:29",3265763.0,31.0,3,4116,What good alternatives are there to Copperegg for monitoring EC2 instances?,What else is out there that can do the same job and will also integrate with Pager Duty?
Datadog,62818107,62818348.0,1,"2020/07/09, 18:13:48",True,"2020/07/13, 17:33:38",nan,11612.0,3338.0,2,198,Spring Boot and Camel- Log Route Duration,I am migrating an application to use the latest version of Spring Boot.
Datadog,62818107,62818348.0,1,"2020/07/09, 18:13:48",True,"2020/07/13, 17:33:38",nan,11612.0,3338.0,2,198,Spring Boot and Camel- Log Route Duration,Currently all the camel routes are in XML and I have it running using this approach.
Datadog,62818107,62818348.0,1,"2020/07/09, 18:13:48",True,"2020/07/13, 17:33:38",nan,11612.0,3338.0,2,198,Spring Boot and Camel- Log Route Duration,All the routes currently log a message at the end of processing.
Datadog,62818107,62818348.0,1,"2020/07/09, 18:13:48",True,"2020/07/13, 17:33:38",nan,11612.0,3338.0,2,198,Spring Boot and Camel- Log Route Duration,I was wondering is there a way to detect how long it took a particular route to execute and add to the log message.
Datadog,62818107,62818348.0,1,"2020/07/09, 18:13:48",True,"2020/07/13, 17:33:38",nan,11612.0,3338.0,2,198,Spring Boot and Camel- Log Route Duration,"With this information, we can then create datadog dashboards to show stats on our camel routes"
Datadog,62818107,62818348.0,1,"2020/07/09, 18:13:48",True,"2020/07/13, 17:33:38",nan,11612.0,3338.0,2,198,Spring Boot and Camel- Log Route Duration,"Thanks in advance
Damien"
Datadog,62298190,nan,1,"2020/06/10, 10:39:42",True,"2020/08/21, 07:12:56","2020/06/15, 09:21:14",10558160.0,178.0,2,446,Is there a way to rotate logs in AWS ECS through task definition configuration?,I'm running datadog agent container in EC2 by configuring task definition in AWS ECS.
Datadog,62298190,nan,1,"2020/06/10, 10:39:42",True,"2020/08/21, 07:12:56","2020/06/15, 09:21:14",10558160.0,178.0,2,446,Is there a way to rotate logs in AWS ECS through task definition configuration?,"But at this time, the huge amount of logs is stored in  /var/lib/docker/containers/ ContainerID / ContainerID .json so that I want to rotate it."
Datadog,62298190,nan,1,"2020/06/10, 10:39:42",True,"2020/08/21, 07:12:56","2020/06/15, 09:21:14",10558160.0,178.0,2,446,Is there a way to rotate logs in AWS ECS through task definition configuration?,"In Docker documents, I saw this  link ."
Datadog,62298190,nan,1,"2020/06/10, 10:39:42",True,"2020/08/21, 07:12:56","2020/06/15, 09:21:14",10558160.0,178.0,2,446,Is there a way to rotate logs in AWS ECS through task definition configuration?,There are
Datadog,62298190,nan,1,"2020/06/10, 10:39:42",True,"2020/08/21, 07:12:56","2020/06/15, 09:21:14",10558160.0,178.0,2,446,Is there a way to rotate logs in AWS ECS through task definition configuration?,Now I want to config these options through task definition but I don't know the convention of them.
Datadog,62298190,nan,1,"2020/06/10, 10:39:42",True,"2020/08/21, 07:12:56","2020/06/15, 09:21:14",10558160.0,178.0,2,446,Is there a way to rotate logs in AWS ECS through task definition configuration?,Did anyone have any ideas?
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,I'm currently leveraging celery for periodic tasks.
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,I am new to celery.
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,I have two workers running two different queues.
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,One for slow background jobs and one for jobs user's queue up in the application.
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,I am monitoring my tasks on datadog because it's an easy way to confirm my workers a running appropriately.
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,"What I want to do is after each task completes, record which queue the task was completed on."
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,"The following function is something that I implemented after researching the celery docs and some StackOverflow posts, but it's not working as intended."
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,I get the first statsd increment but the remaining code does not execute.
Datadog,62056153,nan,1,"2020/05/28, 05:46:11",False,"2020/05/28, 19:38:33",nan,7113209.0,1090.0,2,579,How to Inspect the Queue Processing a Celery Task,"I am wondering if there is a simpler way to inspect inside/after each task completes, what queue processed the task."
Datadog,60560889,nan,0,"2020/03/06, 11:17:30",False,"2020/03/06, 11:17:30",nan,1454.0,7501.0,2,59,Tracking sparse metrics with micrometer results in zeroes being published,I'm using micrometer to publish several different metrics to Datadog; among these there is the number of items processed by several different batch jobs which can be pretty sparse.
Datadog,60560889,nan,0,"2020/03/06, 11:17:30",False,"2020/03/06, 11:17:30",nan,1454.0,7501.0,2,59,Tracking sparse metrics with micrometer results in zeroes being published,"Some batch jobs have different intervals, some others are triggered by external events so they don't have a fixed interval at all."
Datadog,60560889,nan,0,"2020/03/06, 11:17:30",False,"2020/03/06, 11:17:30",nan,1454.0,7501.0,2,59,Tracking sparse metrics with micrometer results in zeroes being published,"However, my micrometer configuration has a  step  of 30s, which I don't want to change, as suggested in  this question , because I have denser metrics that need to be tracked every 30s."
Datadog,60560889,nan,0,"2020/03/06, 11:17:30",False,"2020/03/06, 11:17:30",nan,1454.0,7501.0,2,59,Tracking sparse metrics with micrometer results in zeroes being published,"This means that every 30s, unless a batch job has just run, my application is publishing a zero for each batch job metric, polluting them."
Datadog,60560889,nan,0,"2020/03/06, 11:17:30",False,"2020/03/06, 11:17:30",nan,1454.0,7501.0,2,59,Tracking sparse metrics with micrometer results in zeroes being published,"I've been trying to handle this on Datadog by using its  rollup  function, but i don't know the rollup interval beforehand because the interval may be variable for jobs triggered by external events."
Datadog,60560889,nan,0,"2020/03/06, 11:17:30",False,"2020/03/06, 11:17:30",nan,1454.0,7501.0,2,59,Tracking sparse metrics with micrometer results in zeroes being published,"Another solution i'm considering is to extend the  DataDogMeterRegistry  to have it clear all metrics right after publishing, so that it will only publish metrics that registered in the latest 30s interval, but I was wondering if there was a clearer way to prevent micrometer from sending zeroes for sparse metrics."
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,I have a logrus log handler in my Golang application.
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,"Logs are formatted with JSONFormatter and are submitted as a single line to Datadog, which aggregates them and displays them nicely."
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,"However, I recently discovered a case where there's an unhandled panic, and this is  not  captured with the logrus logger."
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,"This results in the actual panic and stack trace being spread across multiple output lines, which Datadog collects individually."
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,This costs us money and makes the logs very difficult to read.
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,"I'm going to fix the issue, but in the event that any further unhandled panics happen, I'd like to be able to capture them using the logrus JSONFormatter."
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,Something like this:
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,This produces the following output.
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,"As you can see, the first two logs use logrus, but the unhandled panic does not."
Datadog,60516923,60517002.0,1,"2020/03/04, 01:17:30",True,"2020/03/04, 01:27:31",nan,13002451.0,23.0,2,688,Logging unhandled Golang panics,Is it possible to get those last several lines to log using logrus?
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,I've been working on adding monitoring metrics in our GraphQL gateway recently.
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,We're using  graphql-spring-boot  starter for the gateway.
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"After reading the following documentations, I manage to send the basic graphql.timer.query."
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,* metrics to Datadog
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"What I've achieved so far is, when I send a GraphQL query/mutation, I'd collect the request count and time accordingly."
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,e.g.
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,sending the query below
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,I'll see metrics  graphql.timer.query.count  /  graphql.timer.query.sum  with tags  operationName=HelloWorldQuery
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"It works like perfectly, until I want to test a query with errors."
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,I realise there is no metrics/tags related to a failed query.
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"For example, if I the above query returns null data and some GraphQL errors, I'd still collect  graphql.timer.query.count (operationName=HelloWorldQuery) , but there's no additional tags for me to tell there is an error for that query."
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"In the gateway, I have implemented a custom  GraphQLErrorHandler , so I was thinking maybe I should add error counter (via MeterRegistry) in that class, but I am unable to get the  operationName  simply from GraphQLError type."
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,the best I can get is error.getPath() which gives the method name (e.g.
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,greeting ) rather than the custom query name ( HelloWorldQuery  - to be consistent with what  graphql.timer.query.
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,*  provides).
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"My question is, how to solve the above problem?"
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,And generally what is the best way of collecting GraphQL query metrics (including errors)?
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,-------------------  Update  -------------------
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"2019-12-31 
I read a bit more about GraphQL Instrumentation  here  and checked the  MetricsInstrumentation  implementation in graphql-spring-boot repo, the I have an idea of extending the MetricsInstrumentation class by adding error metrics there."
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"2020-01-02 
I tried to ingest my CustomMetricsInstrumentation class, but with no luck."
Datadog,59549130,59605166.0,1,"2020/01/01, 01:32:26",True,"2020/01/06, 01:37:29","2020/01/02, 23:19:04",1264461.0,7426.0,2,1258,GraphQL + Spring Boot: how to collect (error) metrics?,"There is internal AutoConfiguration wiring, which I cannot insert my auto configuration in the middle."
Datadog,59102943,nan,1,"2019/11/29, 12:21:36",False,"2021/02/23, 00:44:33",nan,4039431.0,1.0,2,184,Understanding the search thread pool of elasticsearch,"I am using Datadog integration with elasticsearch to monitor the ES clusters, one important metric which it shows on its dashboard is the no of active and waiting for search threads."
Datadog,59102943,nan,1,"2019/11/29, 12:21:36",False,"2021/02/23, 00:44:33",nan,4039431.0,1.0,2,184,Understanding the search thread pool of elasticsearch,"Referring to  this  ES docs, I understand that search threads work on a request queue in ES which is of the fixed size of 1000."
Datadog,59102943,nan,1,"2019/11/29, 12:21:36",False,"2021/02/23, 00:44:33",nan,4039431.0,1.0,2,184,Understanding the search thread pool of elasticsearch,"I am seeing a lot of waiting for threads as shown in the image, but there is no rejected queue exception explained  here ."
Datadog,59102943,nan,1,"2019/11/29, 12:21:36",False,"2021/02/23, 00:44:33",nan,4039431.0,1.0,2,184,Understanding the search thread pool of elasticsearch,So it means ES is not rejecting the requests but still search threads are not able to execute the request fast enough hence ended up in waiting status for a long time.
Datadog,59102943,nan,1,"2019/11/29, 12:21:36",False,"2021/02/23, 00:44:33",nan,4039431.0,1.0,2,184,Understanding the search thread pool of elasticsearch,Questions
Datadog,59102943,nan,1,"2019/11/29, 12:21:36",False,"2021/02/23, 00:44:33",nan,4039431.0,1.0,2,184,Understanding the search thread pool of elasticsearch,"I know its a board question, hence let me know if any additional information is required."
Datadog,55832300,57509622.0,2,"2019/04/24, 17:18:20",True,"2019/08/19, 13:40:01","2019/04/24, 17:43:00",9741769.0,3237.0,2,692,Cluster-autoscaler not triggering scale-up on Daemonset deployment,I deployed the Datadog agent using the  Datadog Helm chart  which deploys a  Daemonset  in Kubernetes.
Datadog,55832300,57509622.0,2,"2019/04/24, 17:18:20",True,"2019/08/19, 13:40:01","2019/04/24, 17:43:00",9741769.0,3237.0,2,692,Cluster-autoscaler not triggering scale-up on Daemonset deployment,However when checking the state of the Daemonset I saw it was not creating all pods:
Datadog,55832300,57509622.0,2,"2019/04/24, 17:18:20",True,"2019/08/19, 13:40:01","2019/04/24, 17:43:00",9741769.0,3237.0,2,692,Cluster-autoscaler not triggering scale-up on Daemonset deployment,When describing the  Daemonset  to figure out what was going wrong I saw it did not have enough resources:
Datadog,55832300,57509622.0,2,"2019/04/24, 17:18:20",True,"2019/08/19, 13:40:01","2019/04/24, 17:43:00",9741769.0,3237.0,2,692,Cluster-autoscaler not triggering scale-up on Daemonset deployment,"However, I have the  Cluster-autoscaler  installed in the cluster and configured properly (It does trigger on regular  Pod  deployments that do not have enough resources to schedule), but it does not seem to trigger on the  Daemonset :"
Datadog,55832300,57509622.0,2,"2019/04/24, 17:18:20",True,"2019/08/19, 13:40:01","2019/04/24, 17:43:00",9741769.0,3237.0,2,692,Cluster-autoscaler not triggering scale-up on Daemonset deployment,The AutoScalingGroup has enough nodes left:
Datadog,55832300,57509622.0,2,"2019/04/24, 17:18:20",True,"2019/08/19, 13:40:01","2019/04/24, 17:43:00",9741769.0,3237.0,2,692,Cluster-autoscaler not triggering scale-up on Daemonset deployment,Did I miss something in the configuration of the Cluster-autoscaler?
Datadog,55832300,57509622.0,2,"2019/04/24, 17:18:20",True,"2019/08/19, 13:40:01","2019/04/24, 17:43:00",9741769.0,3237.0,2,692,Cluster-autoscaler not triggering scale-up on Daemonset deployment,What can I do to make sure it triggers on  Daemonset  resources as well?
Datadog,55832300,57509622.0,2,"2019/04/24, 17:18:20",True,"2019/08/19, 13:40:01","2019/04/24, 17:43:00",9741769.0,3237.0,2,692,Cluster-autoscaler not triggering scale-up on Daemonset deployment,"Edit:
Describe of the Daemonset"
Datadog,54660692,54660782.0,2,"2019/02/13, 02:22:55",True,"2019/02/13, 08:06:26","2019/02/13, 02:51:31",10433931.0,23.0,2,275,Using jq: Only select parents with a certain child key,Example json:
Datadog,54660692,54660782.0,2,"2019/02/13, 02:22:55",True,"2019/02/13, 08:06:26","2019/02/13, 02:51:31",10433931.0,23.0,2,275,Using jq: Only select parents with a certain child key,"I'd like to return a list of services that have the ""build"" key, and not the ""image"" key."
Datadog,54660692,54660782.0,2,"2019/02/13, 02:22:55",True,"2019/02/13, 08:06:26","2019/02/13, 02:51:31",10433931.0,23.0,2,275,Using jq: Only select parents with a certain child key,Note that the value for the build key isn't something I can key off of.
Datadog,54660692,54660782.0,2,"2019/02/13, 02:22:55",True,"2019/02/13, 08:06:26","2019/02/13, 02:51:31",10433931.0,23.0,2,275,Using jq: Only select parents with a certain child key,"Output should be: [""web"", ""datadog""]"
Datadog,53323761,nan,2,"2018/11/15, 18:22:16",False,"2018/11/18, 18:50:17","2018/11/16, 12:17:46",666254.0,3695.0,2,366,Mocking Python modules across multiple test scripts,In my implementation script I have a line which logs a metric:
Datadog,53323761,nan,2,"2018/11/15, 18:22:16",False,"2018/11/18, 18:50:17","2018/11/16, 12:17:46",666254.0,3695.0,2,366,Mocking Python modules across multiple test scripts,"From my test script, I assert that statsd.increment() is called by mocking out the datadog module:"
Datadog,53323761,nan,2,"2018/11/15, 18:22:16",False,"2018/11/18, 18:50:17","2018/11/16, 12:17:46",666254.0,3695.0,2,366,Mocking Python modules across multiple test scripts,This works fine and passes.
Datadog,53323761,nan,2,"2018/11/15, 18:22:16",False,"2018/11/18, 18:50:17","2018/11/16, 12:17:46",666254.0,3695.0,2,366,Mocking Python modules across multiple test scripts,"But as soon as I add ANOTHER script which calls  some_function()  without mocking datadog, that script runs beforehand and loads the real datadog module into the cache."
Datadog,53323761,nan,2,"2018/11/15, 18:22:16",False,"2018/11/18, 18:50:17","2018/11/16, 12:17:46",666254.0,3695.0,2,366,Mocking Python modules across multiple test scripts,"The above test then fails because  some_function()  is no longer using the mock datadog, it uses the real (cached) datadog."
Datadog,53323761,nan,2,"2018/11/15, 18:22:16",False,"2018/11/18, 18:50:17","2018/11/16, 12:17:46",666254.0,3695.0,2,366,Mocking Python modules across multiple test scripts,How can I address this?
Datadog,53323761,nan,2,"2018/11/15, 18:22:16",False,"2018/11/18, 18:50:17","2018/11/16, 12:17:46",666254.0,3695.0,2,366,Mocking Python modules across multiple test scripts,Is it possible to remove the module from the cache?
Datadog,52703024,nan,3,"2018/10/08, 16:06:23",True,"2020/05/26, 22:39:37","2020/05/26, 22:39:37",226968.0,2891.0,2,815,How can I implement a centralized logging solution on AWS?,"We need centralized logging in order to monitor, store, manage, and visualize logs for our infrastructure."
Datadog,52703024,nan,3,"2018/10/08, 16:06:23",True,"2020/05/26, 22:39:37","2020/05/26, 22:39:37",226968.0,2891.0,2,815,How can I implement a centralized logging solution on AWS?,"The logging solution must be able to capture messages from projects written in different languages such as Java, Angular, Scala, and Python."
Datadog,52703024,nan,3,"2018/10/08, 16:06:23",True,"2020/05/26, 22:39:37","2020/05/26, 22:39:37",226968.0,2891.0,2,815,How can I implement a centralized logging solution on AWS?,"Implementing a custom-built solution would lead to additional tasks, costs, and dependencies associated with managing and maintaining its components."
Datadog,52703024,nan,3,"2018/10/08, 16:06:23",True,"2020/05/26, 22:39:37","2020/05/26, 22:39:37",226968.0,2891.0,2,815,How can I implement a centralized logging solution on AWS?,"So instead, we are thinking about using AWS Partner Network (APN) offerings."
Datadog,52703024,nan,3,"2018/10/08, 16:06:23",True,"2020/05/26, 22:39:37","2020/05/26, 22:39:37",226968.0,2891.0,2,815,How can I implement a centralized logging solution on AWS?,"What would be the best managed solution out of Splunk, Sumo Logic, Datadog, Elastic and Loggly?"
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,I'm currently trying to run a playbook that uses a callback plugin.
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,This plugin uses a module called datadog:
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,"When I try to run the playbook, I get an error message:"
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,ImportError: No module named datadog
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,"I order to work around this, I created a virtualenv, activated it and installed the datadog module:"
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,"Then when I launch python and import the module, everything is fine:"
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,Python 2.7.15
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,"Therefore, I believe that the module is used properly."
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,"However, when launching the ansible playbook, the error remains:"
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,"From what I can tell, Ansible is not taking the virtualenv into consideration."
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,I would expect Ansible to use the path of the virtualenv in the  ansible python module location .
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,How can I make ansible use the virtualenv?
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,I didn't find anything related to the python path in the ansible documentation:  https://docs.ansible.com/ansible/2.5/reference_appendices/config.html
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,Note: the issue occurs on the machine running Ansible.
Datadog,52689144,52692334.0,1,"2018/10/07, 16:51:09",True,"2018/10/08, 13:38:48","2018/10/08, 13:38:48",616349.0,1717.0,2,5032,How to configure Ansible for Python virtual environment?,Not the machine being provisioned.
Datadog,51690297,nan,3,"2018/08/05, 02:26:45",True,"2019/04/27, 00:53:24",nan,491682.0,9917.0,2,1787,kibana - metricbeat dashboard for multiple hosts,Im new to kibana but am hoping to migrate away from Datadog.
Datadog,51690297,nan,3,"2018/08/05, 02:26:45",True,"2019/04/27, 00:53:24",nan,491682.0,9917.0,2,1787,kibana - metricbeat dashboard for multiple hosts,In DD I can create 'visualizations' that are specific to a single data source (host in this case) and combine several into one dashboard.
Datadog,51690297,nan,3,"2018/08/05, 02:26:45",True,"2019/04/27, 00:53:24",nan,491682.0,9917.0,2,1787,kibana - metricbeat dashboard for multiple hosts,EG: view CPU load for n hosts on one page
Datadog,51690297,nan,3,"2018/08/05, 02:26:45",True,"2019/04/27, 00:53:24",nan,491682.0,9917.0,2,1787,kibana - metricbeat dashboard for multiple hosts,Am not seeing (yet) how to accomplish this sort of thing via Kibana.
Datadog,51690297,nan,3,"2018/08/05, 02:26:45",True,"2019/04/27, 00:53:24",nan,491682.0,9917.0,2,1787,kibana - metricbeat dashboard for multiple hosts,Suggestions on where to look?
Datadog,51690297,nan,3,"2018/08/05, 02:26:45",True,"2019/04/27, 00:53:24",nan,491682.0,9917.0,2,1787,kibana - metricbeat dashboard for multiple hosts,Using v6.x Kibana/logstash/elasticsearch/metricbeat
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,I have two MySQL server which are running on same group replication.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,The setup had been done by below steps:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"But I found an other error: Every time I reload the fallback MySQL (by restart services) it will auto join to group but stuck at RECOVERING forever, I waited for 3 days but it still in RECOVERING."
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"I checked on the log and don't see any error on both server, everything look good except the fallback is running as readonly and stay at RECOVERING."
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,What step did I missed?
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,My group configuration is (I followed the instruction from DigitalOcean help  page at  https://www.digitalocean.com/community/tutorials/how-to-configure-mysql-group-replication-on-ubuntu-16-04 ):
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"sync_binlog                    = 1 binlog_format                  =
ROW"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"loose-group_replication_start_on_boot = ON
loose-group_replication_ssl_mode = REQUIRED
loose-group_replication_recovery_use_ssl = 1"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"&quot;9dc4ae01-6664-437a-83f8-80546d58e025&quot;
loose-group_replication_ip_whitelist =
&quot;172.AAA.BBB.166,138.AAA.BBB.199&quot; loose-group_replication_group_seeds
= &quot;172.AAA.BBB.166:33061,138.AAA.BBB.199:33061&quot;"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,loose-group_replication_enforce_update_everywhere_checks = ON
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,&quot;138.AAA.BBB.199:33061&quot;
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Below is MySQL log on first server:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-08T06:10:12.167400Z 0 [Warning] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: 'Members removed from the group: 138.AAA.BBB.199:3306'
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-08T06:10:12.167475Z 0 [Note] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: 'Group membership changed to 172.AAA.BBB.166:3306 on view
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,15271181169364149:11.'
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-08T06:11:59.032666Z 0 [Note] Plugin
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication reported: 'Members joined the group:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,138.AAA.BBB.199:3306' 2018-06-08T06:11:59.032722Z 0 [Note] Plugin group_replication reported: 'Group membership changed to
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"172.AAA.BBB.166:3306, 138.AAA.BBB.199:3306 on view 15271181169364149:12.'"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Below is the MySQL log on fallback server:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.490896Z 0 [Warning] option 'max_allowed_packet':
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,unsigned value 3221225472 adjusted to 1073741824
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.490942Z 0 [Warning] The use of InnoDB is mandatory
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,since MySQL 5.7.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,The former options like '--innodb=0/1/OFF/ON' or
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'--skip-innodb' are ignored.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.491057Z 0 [Warning]
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,The syntax '--log_warnings/-W' is deprecated and will be removed in a
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,future release.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Please use '--log_error_verbosity' instead.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.491098Z 0 [Warning] TIMESTAMP with implicit
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,DEFAULT value is deprecated.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Please use
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,--explicit_defaults_for_timestamp server option (see documentation for more details).
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.492972Z 0 [Note] /usr/sbin/mysqld
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,(mysqld 5.7.22-log) starting as process 31633 ...
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.500063Z 0 [Warning] InnoDB: Using
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,innodb_locks_unsafe_for_binlog is DEPRECATED.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,This option may be
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,removed in future releases.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Please use READ COMMITTED transaction
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,isolation level instead; Please refer to
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,http://dev.mysql.com/doc/refman/5.7/en/set-transaction.html
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.500175Z 0 [Note] InnoDB: PUNCH HOLE support
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,available 2018-06-11T09:22:57.500191Z 0 [Note] InnoDB: Mutexes and
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,rw_locks use GCC atomic builtins 2018-06-11T09:22:57.500200Z 0 [Note]
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,InnoDB: Uses event mutexes 2018-06-11T09:22:57.500205Z 0 [Note]
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.500209Z 0 [Note] InnoDB: Compressed tables use
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,zlib 1.2.3 2018-06-11T09:22:57.500213Z 0 [Note] InnoDB: Using Linux
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,native AIO 2018-06-11T09:22:57.500430Z 0 [Note] InnoDB: Number of
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,pools: 1 2018-06-11T09:22:57.500575Z 0 [Note] InnoDB: Using CPU crc32
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,instructions 2018-06-11T09:22:57.501015Z 0 [ERROR] InnoDB: Failed to
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"create check sector file, errno:13 Please confirm O_DIRECT is"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,supported and remove the file /data/check_sector_size if it exists.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"2018-06-11T09:22:57.502305Z 0 [Note] InnoDB: Initializing buffer pool,"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"total size = 4G, instances = 8, chunk size = 128M"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.799065Z 0 [Note] InnoDB: Completed initialization
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,of buffer pool 2018-06-11T09:22:57.857325Z 0 [Note] InnoDB: If the
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"mysqld execution user is authorized, page cleaner thread priority can"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,be changed.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,See the man page of setpriority().
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:57.870317Z 0 [Note] InnoDB: Highest supported file
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,format is Barracuda.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.081570Z 0 [Note] InnoDB:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Creating shared tablespace for temporary tables
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.081656Z 0 [Note] InnoDB: Setting file
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'/data/databases/ibtmp1' size to 12 MB.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Physically writing the file
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,full; Please wait ... 2018-06-11T09:22:58.116190Z 0 [Note] InnoDB:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,File '/data/databases/ibtmp1' size is now 12 MB.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.117279Z 0 [Note] InnoDB: 96 redo rollback
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,segment(s) found.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,96 redo rollback segment(s) are active.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.117293Z 0 [Note] InnoDB: 32 non-redo rollback
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,segment(s) are active.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.117670Z 0 [Note] InnoDB:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Waiting for purge to start 2018-06-11T09:22:58.168094Z 0 [Note]
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,InnoDB: 5.7.22 started; log sequence number 51745666191
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.168309Z 0 [Note] InnoDB: Loading buffer pool(s)
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,from /data/databases/ib_buffer_pool 2018-06-11T09:22:58.168558Z 0
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,[Note] Plugin 'FEDERATED' is disabled.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.183268Z 0
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,[Warning] CA certificate /etc/mysql/mysql-ssl/ca-cert.pem is self
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,signed.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.184615Z 0 [Note] Server hostname
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,(bind-address): '138.AAA.BBB.199'; port: 3306
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.184636Z 0 [Note]   - '138.AAA.BBB.199' resolves to
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'138.AAA.BBB.199'; 2018-06-11T09:22:58.184668Z 0 [Note] Server socket
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,created on IP: '138.AAA.BBB.199'.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186203Z 0
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,[Warning] 'user' entry 'mysql.session@localhost' ignored in
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,--skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186220Z 0 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186238Z 0 [Warning] 'user' entry
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'phpmadsys@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186260Z 0 [Warning] 'user' entry
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'phpmyadmin@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186308Z 0 [Warning] 'db' entry 'performance_schema
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,mysql.session@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186313Z 0 [Warning] 'db' entry 'sys
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,mysql.sys@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186318Z 0 [Warning] 'db' entry 'phpmyadmin
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,phpmadsys@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186322Z 0 [Warning] 'db' entry 'performance_schema
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,datadog@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186327Z 0 [Warning] 'db' entry 'phpmyadmin
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,phpmyadmin@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.186340Z 0 [Warning] 'proxies_priv' entry '@
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,root@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.188628Z 0 [Warning] 'tables_priv' entry 'user
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,mysql.session@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.188649Z 0 [Warning] 'tables_priv' entry
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.192624Z 0 [Warning] Neither --relay-log nor
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,--relay-log-index were used; so replication may break when this MySQL server acts as a slave and has his hostname changed!!
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Please use
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'--relay-log=dvm02-relay-bin' to avoid this problem.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.206545Z 0 [Note] Event Scheduler: Loaded 0 events
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.206745Z 0 [Note] /usr/sbin/mysqld: ready for
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,connections.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Version: '5.7.22-log'  socket:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,(GPL) 2018-06-11T09:22:58.207175Z 2 [Note] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: 'Group communication SSL configuration:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication_ssl_mode: &quot;REQUIRED&quot;; server_key_file:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,&quot;/etc/mysql/mysql-ssl/server-key.pem&quot;; server_cert_file:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,&quot;/etc/mysql/mysql-ssl/server-cert.pem&quot;; client_key_file:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,&quot;/etc/mysql/mysql-ssl/server-key.pem&quot;; client_cert_file:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,&quot;/etc/mysql/mysql-ssl/server-cert.pem&quot;; ca_file:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,&quot;/etc/mysql/mysql-ssl/ca-cert.pem&quot;; ca_path: &quot;&quot;; cipher: &quot;&quot;;
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"tls_version: &quot;TLSv1,TLSv1.1&quot;; crl_file: &quot;&quot;; crl_path: &quot;&quot;'"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.207378Z 2 [Warning] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: '[GCS] Automatically adding IPv4 localhost address to the
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,whitelist.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,It is mandatory that it is added.'
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.207820Z 2 [Note] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: 'Initialized group communication with configuration:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication_group_name: &quot;9dc4ae01-6664-437a-83f8-80546d58e025&quot;;
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication_local_address: &quot;138.AAA.BBB.199:33061&quot;;
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication_group_seeds:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"&quot;172.AAA.BBB.166:33061,138.AAA.BBB.199:33061&quot;;"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication_bootstrap_group: false;
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication_poll_spin_loops: 0;
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication_compression_threshold: 1000000;
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"group_replication_ip_whitelist: &quot;172.AAA.BBB.166,138.AAA.BBB.199&quot;'"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.207853Z 2 [Note] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: '[GCS] Configured number of attempts to join: 0'
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.207859Z 2 [Note] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: '[GCS] Configured time between attempts to join: 5 seconds'
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.207878Z 2 [Note] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: 'Member configuration: member_id: 2; member_uuid:
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,&quot;822868f9-52a0-11e8-aa0e-1e45f9551f27&quot;; single-primary mode: &quot;false&quot;;
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,group_replication_auto_increment_increment: 7; '
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.209024Z 3 [Note] 'CHANGE MASTER TO FOR CHANNEL
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'group_replication_applier' executed'.
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Previous state
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"master_host='', master_port= 0, master_log_file='',"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"master_log_pos= 4, master_bind=''."
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"New state master_host='',"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"master_port= 0, master_log_file='', master_log_pos= 4, master_bind=''."
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.216904Z 6 [Note] Slave SQL thread for channel
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"'group_replication_applier' initialized, starting replication in log"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"'FIRST' at position 0, relay log"
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,'./dvm02-relay-bin-group_replication_applier.000071' position: 4
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.216931Z 2 [Note] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: 'Group Replication applier module successfully initialized!'
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:58.241357Z 0 [Note] Plugin group_replication
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,reported: 'XCom protocol version: 3' 2018-06-11T09:22:58.241397Z 0
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,[Note] Plugin group_replication reported: 'XCom initialized and ready
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,to accept incoming connections on port 33061'
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,2018-06-11T09:22:59.213826Z 0 [Note] InnoDB: Buffer pool(s) load
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,completed at 180611 11:22:59 2018-06-11T09:23:00.316791Z 0 [Note]
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,Plugin group_replication reported: 'Group membership changed to
Datadog,50794695,nan,1,"2018/06/11, 12:41:44",False,"2018/12/20, 21:06:35","2020/06/20, 12:12:55",1107392.0,29.0,2,2041,Mysql group replication: stuck on RECOVERING forever,"172.AAA.BBB.166:3306, 138.AAA.BBB.199:3306 on view 15271181169364149:16.'"
Datadog,47013899,nan,2,"2017/10/30, 13:15:10",False,"2017/10/30, 15:22:49",nan,914411.0,893.0,2,266,Best practice to monitor docker volume available space,What is the best way/tool to monitor an EBS volume available space when mounted inside a Docker container?
Datadog,47013899,nan,2,"2017/10/30, 13:15:10",False,"2017/10/30, 15:22:49",nan,914411.0,893.0,2,266,Best practice to monitor docker volume available space,I really need to monitor the available disk space in order to prevent crash because of no  space left on device .
Datadog,47013899,nan,2,"2017/10/30, 13:15:10",False,"2017/10/30, 15:22:49",nan,914411.0,893.0,2,266,Best practice to monitor docker volume available space,"Do you know of any tool that can monitor that, like datadog, newrelic grafana, prometheus or something opensource?"
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,"My scenario is that currently, I'm running my application as Daemon sets and want to integrate Datadog into my infrastructure."
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,As my understanding is that Daemon sets purpose is to make sure one pod of each set is ran on each node.
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,Here I wanted to point my application at datadog agent so it will feed data into it.
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,I've defined a  Service  of  Nodeport  type to expose the port of the agent.
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,I provided the service name in my application definition and it works.
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,For one node.
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,What happens now when I will have more nodes?
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,Will k8s be clever enough to route to the agent on the same nodes or there is a situation where a pod with my application might call the agent on a different node?
Datadog,46716705,nan,2,"2017/10/12, 21:38:57",False,"2017/10/14, 07:44:41",nan,1121945.0,622.0,2,973,Kubernetes Daemonsets and Nodeports,Is this a correct setup?
Datadog,46608433,nan,0,"2017/10/06, 17:47:22",False,"2017/10/06, 17:47:22",nan,3177795.0,497.0,2,463,Monitoring Undertow by Statsd,"I want to monitor Undertow (mainly IO and worker threads) using statsd, I had in place tomcat and a lot of metrics were exported automatically to jmx but it seems not the case for Undertow."
Datadog,46608433,nan,0,"2017/10/06, 17:47:22",False,"2017/10/06, 17:47:22",nan,3177795.0,497.0,2,463,Monitoring Undertow by Statsd,"I can see XNIO on JConsole for the application(Spring-boot app), but no metrics appear when I try to see them anywhere else."
Datadog,46608433,nan,0,"2017/10/06, 17:47:22",False,"2017/10/06, 17:47:22",nan,3177795.0,497.0,2,463,Monitoring Undertow by Statsd,"(Graphite, Datadog)"
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,I would like to use Datadog to monitor the queue length of some background jobs.
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,"Basically  I need to know the name of the key that represents a queue in Sidekiq , so that I can monitor it as described here:
 https://docs.datadoghq.com/integrations/redisdb/"
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,I've read  that the Sidekiq keys have the form  sidekiq:queue:myqueuename .
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,However I have tried to execute  KEYS *myqueuename*  and I can't find anything.
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,I have also tried to search  KEYS *sidekiq*  but I don't get anything.
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,If I search  KEYS *queue*  I get the key  queues  which represents a set with the names of the queues (e.g.
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,"deliveries, default, low)."
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,However those are only the names: I need the actual queues.
Datadog,46407772,46411356.0,1,"2017/09/25, 17:32:25",True,"2017/09/25, 20:56:47",nan,51387.0,29648.0,2,2389,What is the Redis key for a queue in Sidekiq?,What is the key of a queue?
Datadog,44550138,44550799.0,2,"2017/06/14, 19:30:17",True,"2017/06/14, 21:02:08",nan,6581348.0,93.0,2,2036,Naming Docker Containers on start ECS,Is there a way to name containers when they start through ECS?
Datadog,44550138,44550799.0,2,"2017/06/14, 19:30:17",True,"2017/06/14, 21:02:08",nan,6581348.0,93.0,2,2036,Naming Docker Containers on start ECS,Wondering because I'm currently using Datadog to monitor the system usage and the containers are named something long etc like
Datadog,44550138,44550799.0,2,"2017/06/14, 19:30:17",True,"2017/06/14, 21:02:08",nan,6581348.0,93.0,2,2036,Naming Docker Containers on start ECS,ecs-datadog-agent-task-1-datadog-agent-c0a1f3e8d9e58dd5e901
Datadog,44550138,44550799.0,2,"2017/06/14, 19:30:17",True,"2017/06/14, 21:02:08",nan,6581348.0,93.0,2,2036,Naming Docker Containers on start ECS,would like to set my own name
Datadog,42595614,nan,0,"2017/03/04, 13:44:55",False,"2017/03/04, 14:01:52","2017/03/04, 14:01:52",99834.0,135806.0,2,785,How do I read the current levels of neutron quota usage for my openstack project?,"I want to monitor quota level usage for openstack projects and I need to be able to monitor  current and max levels for networks, ports and routers  (from Python code)."
Datadog,42595614,nan,0,"2017/03/04, 13:44:55",False,"2017/03/04, 14:01:52","2017/03/04, 14:01:52",99834.0,135806.0,2,785,How do I read the current levels of neutron quota usage for my openstack project?,Please note that I am talking about project-level access so the user performing the monitoring is not an open-stack admin.
Datadog,42595614,nan,0,"2017/03/04, 13:44:55",False,"2017/03/04, 14:01:52","2017/03/04, 14:01:52",99834.0,135806.0,2,785,How do I read the current levels of neutron quota usage for my openstack project?,I was able to successfully read current level and max level for nova metrics (compute) but for those related to neutron (networking) it seems that API and command line returns only max-limits and not current levels.
Datadog,42595614,nan,0,"2017/03/04, 13:44:55",False,"2017/03/04, 14:01:52","2017/03/04, 14:01:52",99834.0,135806.0,2,785,How do I read the current levels of neutron quota usage for my openstack project?,It very easy to test it yourself:
Datadog,42595614,nan,0,"2017/03/04, 13:44:55",False,"2017/03/04, 14:01:52","2017/03/04, 14:01:52",99834.0,135806.0,2,785,How do I read the current levels of neutron quota usage for my openstack project?,I should mention that the web interface (horizon) does already report correctly the Floating IPs.
Datadog,42595614,nan,0,"2017/03/04, 13:44:55",False,"2017/03/04, 14:01:52","2017/03/04, 14:01:52",99834.0,135806.0,2,785,How do I read the current levels of neutron quota usage for my openstack project?,It seems that it does not display any gauges for networks.
Datadog,42595614,nan,0,"2017/03/04, 13:44:55",False,"2017/03/04, 14:01:52","2017/03/04, 14:01:52",99834.0,135806.0,2,785,How do I read the current levels of neutron quota usage for my openstack project?,The solution should work with openstack  kilo (7) releases or newer.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,I am very new to writing code.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,I've been looking at every way I can find of finding a string in a text document and then returning part of the string on the following line.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,Ideally with the end goal of putting this extracted string into an excel file but I'm no where near that step yet.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,I've been playing around with a lot of different options and I can not for the life of me get it to work.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,I feel like I'm close and it's killing me because I just can't figure out where I'm going wrong here.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,Goal: to extract the name of the person who posted the job from the text below without knowing the person's name.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,"I know the string ""Job posted by"" will immediately preseed the name I'm looking for and I know "" · "" will immediately follow the name."
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,no where else in the text document do either of these surround strings appear.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,my attempts at this so far are the following (my issue is that it appears to simply return the entire text document as opposed to just the name I'm looking for)
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,any and all help and ideas is enormously appreciated.
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,"sample text in the file:
 9/2/2016 Application Security Engineer Job at Datadog in Greater New York City Area | LinkedIn
    60
 Home Profile
Job description
My Network Jobs
 Search for people, jobs, companies, and more..."
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,"Interests
 Advanced
 
Business Services

Go to Lynda.c
  Application Security Engineer
Datadog
Greater New York City Area
    Posted 15 days ago 93 views
1 alum works here
   Apply on company website
  We’re on a mission to bring sanity to cloud operations and we need you to build resilient and secure applications on our platform."
Datadog,39323306,39333248.0,2,"2016/09/05, 05:51:20",True,"2016/09/05, 19:17:04","2016/09/05, 15:37:00",6794657.0,23.0,2,1840,extract string between two strings from text document using AppleScript,"What you will do
Perform code and design reviews, contribute code that improves security throughout Datadog's products Educate your fellow engineers about security in code and infrastructure
Monitor production applications for anomalous activity
Prioritize and track application security issues across the company
    Help improve our security policies and processes
Job posted by
Ryan Elberg · 2nd
Head of Tech Talent Acquisition at Datadog Greater New York City Area
Send Inmail"
Datadog,38986190,39007294.0,4,"2016/08/17, 02:56:53",True,"2016/08/19, 08:04:25",nan,30997.0,10698.0,2,408,Is there an API call I can make to get the number of App Engine Instances I&#39;m running?,"It's a pretty simple question, really."
Datadog,38986190,39007294.0,4,"2016/08/17, 02:56:53",True,"2016/08/19, 08:04:25",nan,30997.0,10698.0,2,408,Is there an API call I can make to get the number of App Engine Instances I&#39;m running?,"I want to report the number of running instances to datadog, along with a bunch of my other stats."
Datadog,38986190,39007294.0,4,"2016/08/17, 02:56:53",True,"2016/08/19, 08:04:25",nan,30997.0,10698.0,2,408,Is there an API call I can make to get the number of App Engine Instances I&#39;m running?,"There's an irony to the fact that I search Google Web Search for how to do something in Google App Engine and get the crappiest possible result, every time: The Google App Engine documentation pages."
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,There are some not labeled corpus.
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,"I extracted from it triples (OBJECT, RELATION, OBJECT)."
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,For relation extraction I use Stanford OpenIE.
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,But I need only some of this triples.
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,"For example, I need relation "" funded ""."
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,"Text :
 Every startup needs a steady diet of funding to keep it strong and growing."
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,"Datadog, a monitoring service that helps customers bring together data from across a variety of infrastructure and software is no exception."
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,Today it announced a massive $94.5 million Series D Round.
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,The company would not discuss valuation.
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,"From this text i want to extract relation  (Datadog, announced, $94.5 million Round)"
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,I have only one idea:
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,May be there are better approach?
Datadog,36723269,nan,0,"2016/04/19, 18:43:35",False,"2016/04/20, 08:52:58","2016/04/20, 08:52:58",6225656.0,21.0,2,142,How to classify extracted relations (NLP)?,May be I need labeled corpus(i haven't it)?
Datadog,35219961,35230909.0,1,"2016/02/05, 10:58:21",True,"2016/02/05, 20:20:41",nan,2340068.0,555.0,2,1900,Can we write an custom check for monitoring a process with Data Dog,"I want to write an DataDog Check to monitor some process like Puma, delayed_job etc, I can see there are ready plugins available for these for nagios and Sensu but not for DataDog, But can I write my own check/plugin for this services in datadog ?"
Datadog,35219961,35230909.0,1,"2016/02/05, 10:58:21",True,"2016/02/05, 20:20:41",nan,2340068.0,555.0,2,1900,Can we write an custom check for monitoring a process with Data Dog,or can I use existing Nagios/sensu plugins with DataDog ?
Datadog,35219961,35230909.0,1,"2016/02/05, 10:58:21",True,"2016/02/05, 20:20:41",nan,2340068.0,555.0,2,1900,Can we write an custom check for monitoring a process with Data Dog,If yes How should I proceed ?
Datadog,63612353,nan,0,"2020/08/27, 11:43:49",False,"2021/03/08, 12:52:37",nan,1091463.0,1443.0,2,319,How do you integrate APM server monitoring with NextJS server?,I want to integrate server monitoring system like DataDog that implements APM standards.
Datadog,63612353,nan,0,"2020/08/27, 11:43:49",False,"2021/03/08, 12:52:37",nan,1091463.0,1443.0,2,319,How do you integrate APM server monitoring with NextJS server?,"I want to achieve this without using the custom server like Express or Koa, just using the out-of-box NextJS server."
Datadog,63612353,nan,0,"2020/08/27, 11:43:49",False,"2021/03/08, 12:52:37",nan,1091463.0,1443.0,2,319,How do you integrate APM server monitoring with NextJS server?,"My NextJS server is only being used for pages, not APIs."
Datadog,63612353,nan,0,"2020/08/27, 11:43:49",False,"2021/03/08, 12:52:37",nan,1091463.0,1443.0,2,319,How do you integrate APM server monitoring with NextJS server?,It would help if NextJS allows us to insert middleware for all requests.
Datadog,63612353,nan,0,"2020/08/27, 11:43:49",False,"2021/03/08, 12:52:37",nan,1091463.0,1443.0,2,319,How do you integrate APM server monitoring with NextJS server?,However it doesn't look like that's an option.
Datadog,63612353,nan,0,"2020/08/27, 11:43:49",False,"2021/03/08, 12:52:37",nan,1091463.0,1443.0,2,319,How do you integrate APM server monitoring with NextJS server?,Do you know a good way to do this?
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,I set up datadog trace client in my kubernetes cluster to monitor my deployed application.
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,"It was working fine with the kubernetes version 1.15x but as soon as I upgraded the version to 1.16x, the service itself is not showing in the Datadog Dashboard."
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,Currently using:
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,Kubernetes 1.16.9
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,Datadog 0.52.0
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,When checked for agent status.
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,It is giving following exception :
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,This looks like a version issue to me.
Datadog,62436021,62680072.0,2,"2020/06/17, 21:33:30",True,"2021/04/15, 11:08:06","2020/07/01, 18:48:33",11921495.0,844.0,2,814,Unable to detect the kubelet URL automatically : datadog_checks.base.errors.CheckException,If it is which Datadog version I need to use for monitoring?
Datadog,60491872,nan,1,"2020/03/02, 17:22:07",True,"2020/11/12, 13:39:22","2020/03/02, 21:33:17",1489378.0,8619.0,2,632,Error: UPGRADE FAILED: failed to create resource: ConfigMap in version &quot;v1&quot; cannot be handled as a ConfigMap,Per  this spec  on github and these  helm instructions   I'm trying to upgrade our Helm installation of datadog using the following syntax:
Datadog,60491872,nan,1,"2020/03/02, 17:22:07",True,"2020/11/12, 13:39:22","2020/03/02, 21:33:17",1489378.0,8619.0,2,632,Error: UPGRADE FAILED: failed to create resource: ConfigMap in version &quot;v1&quot; cannot be handled as a ConfigMap,"However I'm getting the error below regardless of any attempt at altering the syntax of the  prometheus_url  value (putting the url in quotes, escaping the quotes, etc):"
Datadog,60491872,nan,1,"2020/03/02, 17:22:07",True,"2020/11/12, 13:39:22","2020/03/02, 21:33:17",1489378.0,8619.0,2,632,Error: UPGRADE FAILED: failed to create resource: ConfigMap in version &quot;v1&quot; cannot be handled as a ConfigMap,"Error: UPGRADE FAILED: failed to create resource: ConfigMap in version ""v1"" cannot be handled as a ConfigMap: v1.ConfigMap.Data: ReadString: expects "" or n, but found {, error found in #10 byte of ...|er.yaml"":{""instances|..., bigger context ...|{""apiVersion"":""v1"",""data"":{""kube_scheduler.yaml"":{""instances"":[{""prometheus_url"":""\"" http://localhost| ..."
Datadog,60491872,nan,1,"2020/03/02, 17:22:07",True,"2020/11/12, 13:39:22","2020/03/02, 21:33:17",1489378.0,8619.0,2,632,Error: UPGRADE FAILED: failed to create resource: ConfigMap in version &quot;v1&quot; cannot be handled as a ConfigMap,If I add the  --dry-run --debug  flags I get the following yaml output:
Datadog,60491872,nan,1,"2020/03/02, 17:22:07",True,"2020/11/12, 13:39:22","2020/03/02, 21:33:17",1489378.0,8619.0,2,632,Error: UPGRADE FAILED: failed to create resource: ConfigMap in version &quot;v1&quot; cannot be handled as a ConfigMap,The Yaml output appears to mesh with the integration as specified on this  github page .
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,I have a node.js app already deployed on Elastic Beanstalk.
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,The EC2 instance on which the node app is deployed is running Ubuntu 16.04.5 LTS.
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,I am trying to integrate Datadog APM with Elastic Beanstalk using datadog's config file in .ebextensions folder.
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,I am following the instructions given on their docs page( https://docs.datadoghq.com/integrations/amazon_elasticbeanstalk/#alternate-datadog-agent-configuration )
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,Even though I am following all the mentioned steps I keep getting the following error on AWS ELB.
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,My Datadog config file code:
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,"Even after replacing the  initctl  with  systemctl  in the start &amp; stop scripts, I am still getting the same error."
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,Can't understand where I'm going wrong.
Datadog,59571560,nan,0,"2020/01/03, 01:57:59",False,"2020/01/03, 01:57:59","2020/06/20, 12:12:55",2195092.0,1638.0,2,369,AWS Elastic Beanstalk: Deployment failing,Please Help!
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,I'm just starting out using Apache Beam on Google Cloud Dataflow.
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,I have a project set up with a billing account.
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,"The only things I plan on using this project for are:
1. dataflow - for all data processing
2. pubsub - for exporting stackdriver logs to be consumed by Datadog"
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,"Right now, as I write this, I am not currently running any dataflow jobs."
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,"Looking at the past month, I see ~$15 in dataflow costs and ~$18 in Stackdriver Monitor API costs."
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,It looks as though Stackdriver Monitor API is close to a fixed $1.46/day.
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,I'm curious how to mitigate this.
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,I do not believe I want or need Stackdriver Monitoring.
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,Is it mandatory?
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,"Further, while I feel I have nothing running, I see this over the past hour:"
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,"So I suppose the questions are these:
1. what are these calls?"
Datadog,59145932,nan,2,"2019/12/02, 22:06:32",True,"2019/12/03, 22:32:13",nan,796064.0,1038.0,2,349,stackdriver monitoring api in a dataflow project,2. is it possible to disable Stackdriver Monitoring for dataflow or otherwise mitigate the cost?
Datadog,59046195,59055123.0,1,"2019/11/26, 09:58:50",True,"2019/11/26, 18:14:07",nan,3139545.0,5350.0,2,197,How to add custom MeterRegisty for Spring Boot 2,I am currently exporting Actuator metrics for my Spring Boot Webflux project to DataDog with 10 seconds interval.
Datadog,59046195,59055123.0,1,"2019/11/26, 09:58:50",True,"2019/11/26, 18:14:07",nan,3139545.0,5350.0,2,197,How to add custom MeterRegisty for Spring Boot 2,I would like to add another exporter for one of our internal system that is not in the list of supported backends.
Datadog,59046195,59055123.0,1,"2019/11/26, 09:58:50",True,"2019/11/26, 18:14:07",nan,3139545.0,5350.0,2,197,How to add custom MeterRegisty for Spring Boot 2,Looking at the implementation from  DataDogMeterRegistry  I came up with the following.
Datadog,59046195,59055123.0,1,"2019/11/26, 09:58:50",True,"2019/11/26, 18:14:07",nan,3139545.0,5350.0,2,197,How to add custom MeterRegisty for Spring Boot 2,However this is not working since no logs are printed.
Datadog,59046195,59055123.0,1,"2019/11/26, 09:58:50",True,"2019/11/26, 18:14:07",nan,3139545.0,5350.0,2,197,How to add custom MeterRegisty for Spring Boot 2,My question is how can I add and implement another MeterRegistry for Spring Boot Micrometer?
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,We're running NodeJS application inside docker container hosted on Amazon EC2 instance.
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,To
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,To enable Monitoring for Node.js app with Datadog we are using datadog-metrics library and integrate it with our application.
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,We basically require to save the below Javascript code into a file called example_app.js
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,"
 
 var metrics = require('datadog-metrics');
metrics.init({ **host: 'myhost', prefix: 'myapp."
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,"'** });

function collectMemoryStats() {
    var memUsage = process.memoryUsage();
    metrics.gauge('memory.rss', memUsage.rss);
    metrics.gauge('memory.heapTotal', memUsage.heapTotal);
    metrics.gauge('memory.heapUsed', memUsage.heapUsed);
    metrics.increment('memory.statsReported');
}

setInterval(collectMemoryStats, 5000);"
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,"Although, we are able to successfully publish metrics to datadog but we're wondering if this can be automated."
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,"We want build this into our docker image, hence require an automatic way to pick up the hostname, at the very least be able to use the docker hosts name if possible..Because till now we're manually specifying ""myhost"" and ""myapp"" values manually."
Datadog,38088122,nan,1,"2016/06/29, 02:55:24",True,"2016/06/29, 06:35:15",nan,5173487.0,545.0,2,2056,Automatic way to pick up the hostname inside docker container,Any better way to fetch the AWS instance hostname value into %myhost?
Datadog,66775454,nan,1,"2021/03/24, 07:56:52",True,"2021/03/24, 08:05:50",nan,458060.0,12173.0,1,25,Should 4xx status codes be considered errors in a Frontend/SSR application?,We're implementing logging &amp; monitoring for a Vue/Node application which is using a REST Api.
Datadog,66775454,nan,1,"2021/03/24, 07:56:52",True,"2021/03/24, 08:05:50",nan,458060.0,12173.0,1,25,Should 4xx status codes be considered errors in a Frontend/SSR application?,"Oftentimes the API returns 4xx reponses (401s, 404s) which are currently caught by Axios and returned as &quot;Errors&quot;."
Datadog,66775454,nan,1,"2021/03/24, 07:56:52",True,"2021/03/24, 08:05:50",nan,458060.0,12173.0,1,25,Should 4xx status codes be considered errors in a Frontend/SSR application?,"These end up in our logging solutions (Datadog, Sentry) but dont bring much actionable points."
Datadog,66775454,nan,1,"2021/03/24, 07:56:52",True,"2021/03/24, 08:05:50",nan,458060.0,12173.0,1,25,Should 4xx status codes be considered errors in a Frontend/SSR application?,Should in general  status codes like these be considered Errors?
Datadog,66775454,nan,1,"2021/03/24, 07:56:52",True,"2021/03/24, 08:05:50",nan,458060.0,12173.0,1,25,Should 4xx status codes be considered errors in a Frontend/SSR application?,Are there any best practices for SPA logging and monitoring?
Datadog,66775454,nan,1,"2021/03/24, 07:56:52",True,"2021/03/24, 08:05:50",nan,458060.0,12173.0,1,25,Should 4xx status codes be considered errors in a Frontend/SSR application?,(couldn't find any resources)
Datadog,65951118,65953918.0,1,"2021/01/29, 10:32:50",True,"2021/01/30, 07:27:46","2021/01/30, 07:27:46",8386267.0,27.0,1,64,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,I have a use case wherein I want to publish my spring boot API metrics to Datadog &amp; CloudWatch simultaneously
Datadog,65951118,65953918.0,1,"2021/01/29, 10:32:50",True,"2021/01/30, 07:27:46","2021/01/30, 07:27:46",8386267.0,27.0,1,64,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,I have added the below dependencies to my pom
Datadog,65951118,65953918.0,1,"2021/01/29, 10:32:50",True,"2021/01/30, 07:27:46","2021/01/30, 07:27:46",8386267.0,27.0,1,64,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,Main Application class
Datadog,65951118,65953918.0,1,"2021/01/29, 10:32:50",True,"2021/01/30, 07:27:46","2021/01/30, 07:27:46",8386267.0,27.0,1,64,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,I have added all required properties in the  application.properties  as well.
Datadog,65951118,65953918.0,1,"2021/01/29, 10:32:50",True,"2021/01/30, 07:27:46","2021/01/30, 07:27:46",8386267.0,27.0,1,64,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,"I can see metrics are being published to both datadog &amp; CloudWatch with default metrics name  http.server.request 
But I want the metrics name for datadog to be different &amp; for this, I have added the below property as well"
Datadog,65951118,65953918.0,1,"2021/01/29, 10:32:50",True,"2021/01/30, 07:27:46","2021/01/30, 07:27:46",8386267.0,27.0,1,64,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,management.metrics.web.server.requests-metric-name = i.want.to.be.different
Datadog,65951118,65953918.0,1,"2021/01/29, 10:32:50",True,"2021/01/30, 07:27:46","2021/01/30, 07:27:46",8386267.0,27.0,1,64,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,But this is changing the name for both CloudWatch &amp; datadog
Datadog,65951118,65953918.0,1,"2021/01/29, 10:32:50",True,"2021/01/30, 07:27:46","2021/01/30, 07:27:46",8386267.0,27.0,1,64,Publish Spring MVC Metrics To Multiple Monitoring System Simultaneously Using Micrometer,My question is how can I change the default metrics name for datadog only or keep names different for both
Datadog,63626179,63631475.0,1,"2020/08/28, 04:22:21",True,"2020/08/28, 12:52:58",nan,2397245.0,787.0,1,90,"Kubernetes 1.16 deprecation of daemonset, statefulset, deployments and replicasets",1.16 deprecation notice:
Datadog,63626179,63631475.0,1,"2020/08/28, 04:22:21",True,"2020/08/28, 12:52:58",nan,2397245.0,787.0,1,90,"Kubernetes 1.16 deprecation of daemonset, statefulset, deployments and replicasets","I have about 10 helm charts that contain the old api versions - datadog, nginx-ingress and more."
Datadog,63626179,63631475.0,1,"2020/08/28, 04:22:21",True,"2020/08/28, 12:52:58",nan,2397245.0,787.0,1,90,"Kubernetes 1.16 deprecation of daemonset, statefulset, deployments and replicasets",I don't want to upgrade these different services.
Datadog,63626179,63631475.0,1,"2020/08/28, 04:22:21",True,"2020/08/28, 12:52:58",nan,2397245.0,787.0,1,90,"Kubernetes 1.16 deprecation of daemonset, statefulset, deployments and replicasets",are there any known work arounds?
Datadog,63259337,nan,3,"2020/08/05, 09:07:50",False,"2020/12/06, 18:31:54",nan,14051958.0,11.0,1,410,Kafka consumer lag monitoring visualization,I'm new to Kafka.
Datadog,63259337,nan,3,"2020/08/05, 09:07:50",False,"2020/12/06, 18:31:54",nan,14051958.0,11.0,1,410,Kafka consumer lag monitoring visualization,"During study to kafka, I think monitoring consumer's lag is needed."
Datadog,63259337,nan,3,"2020/08/05, 09:07:50",False,"2020/12/06, 18:31:54",nan,14051958.0,11.0,1,410,Kafka consumer lag monitoring visualization,"When I search from google and docs, I found few ways."
Datadog,63259337,nan,3,"2020/08/05, 09:07:50",False,"2020/12/06, 18:31:54",nan,14051958.0,11.0,1,410,Kafka consumer lag monitoring visualization,I just trying to get less pipeline steps.
Datadog,63259337,nan,3,"2020/08/05, 09:07:50",False,"2020/12/06, 18:31:54",nan,14051958.0,11.0,1,410,Kafka consumer lag monitoring visualization,What should be the simple way to visualize consumer's lag?
Datadog,62177611,nan,1,"2020/06/03, 19:16:07",False,"2020/06/09, 00:40:01",nan,13021621.0,37.0,1,66,Polling consumer group lag over HTTP in Kafka,I want to see the remaining lag in near real-time from Kafka for a particular consumer group.
Datadog,62177611,nan,1,"2020/06/03, 19:16:07",False,"2020/06/09, 00:40:01",nan,13021621.0,37.0,1,66,Polling consumer group lag over HTTP in Kafka,"The closest thing I've done is run the  describe  script from Kafka binaries, but it's slow and unreliable."
Datadog,62177611,nan,1,"2020/06/03, 19:16:07",False,"2020/06/09, 00:40:01",nan,13021621.0,37.0,1,66,Polling consumer group lag over HTTP in Kafka,We are trying to programmatically do this to perform some conditional logic within our ETL pipeline.
Datadog,62177611,nan,1,"2020/06/03, 19:16:07",False,"2020/06/09, 00:40:01",nan,13021621.0,37.0,1,66,Polling consumer group lag over HTTP in Kafka,My first thought is to garner metrics within the consumer and publish over statsD to new relic or datadog then poll over HTTP.
Datadog,62177611,nan,1,"2020/06/03, 19:16:07",False,"2020/06/09, 00:40:01",nan,13021621.0,37.0,1,66,Polling consumer group lag over HTTP in Kafka,This is something I would do long-term.
Datadog,62177611,nan,1,"2020/06/03, 19:16:07",False,"2020/06/09, 00:40:01",nan,13021621.0,37.0,1,66,Polling consumer group lag over HTTP in Kafka,"Is there a shorter-term, simpler approach to poll the consumer lag for a particular group?"
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,I’m looking to enable JMX to allow datadog to monitor our java JBoss wildfly systems but keep hitting runtime errors
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,I have set up the standalone.xml with
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,And
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,As well as
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,Then in my startup.sh i have added
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,But this gives me
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,"java.lang.IllegalStateException: The LogManager was not properly
  installed (you must set the ""java.util.logging.manager"" system
  property to ""org.jboss.logmanage r.LogManager"")"
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,This seems to be fairly common if I look at both here and on google but there seem to be different solutions depending on the version of wildfly.
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,"I think I need to do something like
Set at the start of the standalone.conf"
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,And then
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,At the end.
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,"But I still get errors “Could not load Logmanager ""org.jboss.logmanager.LogManager""”"
Datadog,62123685,nan,1,"2020/06/01, 02:30:52",False,"2020/06/01, 20:43:26",nan,10701833.0,1079.0,1,564,Wildfly 17 enabling JMX remote on the same server gives logmanager errors,Any advice would be appreciated.
Datadog,61119886,nan,1,"2020/04/09, 14:15:10",False,"2020/09/16, 19:57:11",nan,7373440.0,2089.0,1,71,How to include command when executing python entry-point command in AWS Elasticbeanstalk?,I am deploying my flask application in AWS Elasticbeanstalk and I want to add a command for running datadog tracing when executing entry-point command.
Datadog,61119886,nan,1,"2020/04/09, 14:15:10",False,"2020/09/16, 19:57:11",nan,7373440.0,2089.0,1,71,How to include command when executing python entry-point command in AWS Elasticbeanstalk?,How can I do that?
Datadog,61119886,nan,1,"2020/04/09, 14:15:10",False,"2020/09/16, 19:57:11",nan,7373440.0,2089.0,1,71,How to include command when executing python entry-point command in AWS Elasticbeanstalk?,This is the entry-point command to start my flask app in local machine:
Datadog,61119886,nan,1,"2020/04/09, 14:15:10",False,"2020/09/16, 19:57:11",nan,7373440.0,2089.0,1,71,How to include command when executing python entry-point command in AWS Elasticbeanstalk?,This is how to add a command before that entry-point command (using datadog as example):
Datadog,61119886,nan,1,"2020/04/09, 14:15:10",False,"2020/09/16, 19:57:11",nan,7373440.0,2089.0,1,71,How to include command when executing python entry-point command in AWS Elasticbeanstalk?,How to do the same in AWS elasticbeanstalk?
Datadog,61119886,nan,1,"2020/04/09, 14:15:10",False,"2020/09/16, 19:57:11",nan,7373440.0,2089.0,1,71,How to include command when executing python entry-point command in AWS Elasticbeanstalk?,Seems like beanstalk is using apache + mod_wsgi to run python-flask application but I am not sure how to add a command before the entry-point command.
Datadog,61043878,nan,1,"2020/04/05, 17:07:24",True,"2020/04/06, 18:37:38","2020/04/05, 22:20:56",2012635.0,4711.0,1,816,Spring Boot Micrometer for ElasticSearch vs APM Java Agent,I've a Spring Boot REST application.
Datadog,61043878,nan,1,"2020/04/05, 17:07:24",True,"2020/04/06, 18:37:38","2020/04/05, 22:20:56",2012635.0,4711.0,1,816,Spring Boot Micrometer for ElasticSearch vs APM Java Agent,"I use many Spring libraries as: Spring Data REST, HATEOAS, Spring JPA, Hibernate, Redis, ElasticSearch..."
Datadog,61043878,nan,1,"2020/04/05, 17:07:24",True,"2020/04/06, 18:37:38","2020/04/05, 22:20:56",2012635.0,4711.0,1,816,Spring Boot Micrometer for ElasticSearch vs APM Java Agent,I want to track metrics of my application and I did a research to find the best tool to do it.
Datadog,61043878,nan,1,"2020/04/05, 17:07:24",True,"2020/04/06, 18:37:38","2020/04/05, 22:20:56",2012635.0,4711.0,1,816,Spring Boot Micrometer for ElasticSearch vs APM Java Agent,"After have given a try to Micrometer+DataDog, becuase I already use ElasticSearch I did try  APM Java Agent  and I found quite impressive the amount of data I get in Kibana Dashboard."
Datadog,61043878,nan,1,"2020/04/05, 17:07:24",True,"2020/04/06, 18:37:38","2020/04/05, 22:20:56",2012635.0,4711.0,1,816,Spring Boot Micrometer for ElasticSearch vs APM Java Agent,I can see my endpoints and investigate where the time was spent (Mysql queries and others stuff).
Datadog,61043878,nan,1,"2020/04/05, 17:07:24",True,"2020/04/06, 18:37:38","2020/04/05, 22:20:56",2012635.0,4711.0,1,816,Spring Boot Micrometer for ElasticSearch vs APM Java Agent,"I didn't try yet Micrometer+ElasticSearch, but from the documentation it seems to collect less data out of the box."
Datadog,61043878,nan,1,"2020/04/05, 17:07:24",True,"2020/04/06, 18:37:38","2020/04/05, 22:20:56",2012635.0,4711.0,1,816,Spring Boot Micrometer for ElasticSearch vs APM Java Agent,I'd like to know your advice about and what you think is the best tool to collect metric for an application in production.
Datadog,60086524,nan,0,"2020/02/06, 03:35:27",False,"2020/02/06, 03:35:27",nan,12740174.0,108.0,1,67,Is there any way to monitor AKS control plane processes like the scheduler?,Last month we had an outage caused by the AKS Scheduler going down.
Datadog,60086524,nan,0,"2020/02/06, 03:35:27",False,"2020/02/06, 03:35:27",nan,12740174.0,108.0,1,67,Is there any way to monitor AKS control plane processes like the scheduler?,Commands such as  kubectl  were still working but pods weren't starting.
Datadog,60086524,nan,0,"2020/02/06, 03:35:27",False,"2020/02/06, 03:35:27",nan,12740174.0,108.0,1,67,Is there any way to monitor AKS control plane processes like the scheduler?,"When we contacted AKS, they eventually ""restarted the API server"" which resolved this issue."
Datadog,60086524,nan,0,"2020/02/06, 03:35:27",False,"2020/02/06, 03:35:27",nan,12740174.0,108.0,1,67,Is there any way to monitor AKS control plane processes like the scheduler?,It definitely makes me a little worried that we could lose something as critical as the scheduler and we have to call to ask Azure to fix it.
Datadog,60086524,nan,0,"2020/02/06, 03:35:27",False,"2020/02/06, 03:35:27",nan,12740174.0,108.0,1,67,Is there any way to monitor AKS control plane processes like the scheduler?,Azure has made the Control Plane opaque from within the cluster.
Datadog,60086524,nan,0,"2020/02/06, 03:35:27",False,"2020/02/06, 03:35:27",nan,12740174.0,108.0,1,67,Is there any way to monitor AKS control plane processes like the scheduler?,"The API server, scheduler, and controller are not even listed as objects."
Datadog,60086524,nan,0,"2020/02/06, 03:35:27",False,"2020/02/06, 03:35:27",nan,12740174.0,108.0,1,67,Is there any way to monitor AKS control plane processes like the scheduler?,"We are working on a simple healthcheck pod that would start up and send a ping to Datadog saying ""I'm alive"", however, I tend to think that Azure should be providing someway to monitor or view the health of these services."
Datadog,60086524,nan,0,"2020/02/06, 03:35:27",False,"2020/02/06, 03:35:27",nan,12740174.0,108.0,1,67,Is there any way to monitor AKS control plane processes like the scheduler?,Has anyone come up with a better method of monitoring these processes?
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,The app has the following containers
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,In the dev process many feature branches are created to add new features.
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,such as
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,I have an AWS EC2 instance per feature branches running docker engine V.18 and docker compose to build the and run the docker stack that compose the php app.
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,To save operation costs 1 AWS EC2 instance can have 3 feature branches at the same time.
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,I was thinking that there should be a custom docker-compose with special port mapping and docker image tag for each feature branch.
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,The goal of this configuration is to be able to test 3 feature branches and access the app through different ports while saving money.
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,I also thought about using  docker networks  by keeping the same ports and using an nginx to redirect traffic to the different docker network ports.
Datadog,59961483,nan,3,"2020/01/29, 08:20:56",True,"2020/02/26, 04:28:04","2020/02/03, 23:54:55",8379207.0,971.0,1,200,Same Application in a docker compose configuration mapping different ports on AWS EC2 instance,What recommendations do you give?
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,We have an app that was initially created in 2008 on Rails 2.3.
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"This app runs on top of JRuby version 1.7, in Ruby 1.8 compatibility mode."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,This is a largish app with 350 controllers and 450 models.
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"We're (finally) upgrading the app to Rails 5.2, upgrading JRuby to latest version 9.2.8.0."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"We're now noticing a significant performance regression compared to the old app, running on the same server and database."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,Most requests are 30% to 200% slower than in the Rails 2.3 version.
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,The slowdown seems to be linear to the amount of data rendered.
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,I.e.
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"when we render a large table of data, the request may take 3000ms vs 1000ms on the old version, even when there is no DB access happening in the view."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"When rendering a request with little data, the slowdown is much less pronounced."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"For instance, Rails 2.3 will render a certain ""heavy"" page in 3 sec:"
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"Completed in 2962ms (View: 1565, DB: 1384)"
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,Compared to 7 sec for the new version:
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,Completed 200 OK in 7254ms (Views: 5808.5ms | ActiveRecord: 1389.3ms)
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"The DB access time is very similar, but the ""view"" time is much much higher."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"We've tried running our app on MRI Ruby, and it's 3 times faster than on JRuby 9."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"Unfortunately, we make extensive use of Java libraries in our code, so we can't easily switch to MRI."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,Some questions:
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"Is JRuby still a viable option for Rails 5, is anyone using it with good performance?"
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,I couldn't find a recommendation which JDK to run JRuby on.
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,We're now running on OpenJDK 8.
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,I've briefly tried OpenJDK 13 but that seemed even slower.
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,Is there a recommended version?
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,We're using these options:  JRUBY_OPTS=-J-Xmx4g -XX:ReservedCodeCacheSize=300M --server .
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,Any recommendations on other options that may improve performance?
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"We've used ""Datadog"" to produce flame-graphs of our requests, which confirms that most of the time is spent rendering views outside of data access, but we can't really tell what is costing time in the view."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,"We've tried attaching JVisualVM to the app in sampling mode, but it's very hard to get useful metrics out of this."
Datadog,58930227,nan,0,"2019/11/19, 11:07:02",False,"2019/11/19, 11:07:02",nan,27449.0,2601.0,1,152,Performance regression upgrading Rails 2.3 to 5.2 and JRuby 1.7 to JRuby 9.2,Is there a better way of profiling JRuby on Rails applications?
Datadog,58576189,nan,2,"2019/10/27, 04:27:16",True,"2019/11/27, 08:05:17","2019/11/27, 08:05:17",10611720.0,21.0,1,1206,&quot;gcc&quot;: executable file not found in %PATH% when using mongo-go-driver,I want to use mongodb driver.But I get the following error:
Datadog,58576189,nan,2,"2019/10/27, 04:27:16",True,"2019/11/27, 08:05:17","2019/11/27, 08:05:17",10611720.0,21.0,1,1206,&quot;gcc&quot;: executable file not found in %PATH% when using mongo-go-driver,"go.mongodb.org/mongo-driver/vendor/github.com/DataDog/zstd
  exec: ""gcc"": executable file not found in %PATH%"
Datadog,57207906,nan,1,"2019/07/25, 21:08:17",False,"2020/01/29, 02:05:05",nan,11837774.0,11.0,1,58,How to fix &quot;interface ICorProfilerInfo3 not found&quot;,I am trying to trace .NET application using Datadog .NET Tracer.
Datadog,57207906,nan,1,"2019/07/25, 21:08:17",False,"2020/01/29, 02:05:05",nan,11837774.0,11.0,1,58,How to fix &quot;interface ICorProfilerInfo3 not found&quot;,"https://github.com/DataDog/dd-trace-dotnet/releases 
The application and the tracer are installed on Windows 2008R2SP1x64 Std with .NET Framework 4.6.1."
Datadog,57207906,nan,1,"2019/07/25, 21:08:17",False,"2020/01/29, 02:05:05",nan,11837774.0,11.0,1,58,How to fix &quot;interface ICorProfilerInfo3 not found&quot;,It fails to trace with the following warning messages:
Datadog,57207906,nan,1,"2019/07/25, 21:08:17",False,"2020/01/29, 02:05:05",nan,11837774.0,11.0,1,58,How to fix &quot;interface ICorProfilerInfo3 not found&quot;,".net tracer log:
""Failed to attach profiler: interface ICorProfilerInfo3 not found."""
Datadog,57207906,nan,1,"2019/07/25, 21:08:17",False,"2020/01/29, 02:05:05",nan,11837774.0,11.0,1,58,How to fix &quot;interface ICorProfilerInfo3 not found&quot;,"Windows Application Event log:
""NET Runtime version 2.0.50727.8800 - Failed to CoCreate profiler."""
Datadog,57207906,nan,1,"2019/07/25, 21:08:17",False,"2020/01/29, 02:05:05",nan,11837774.0,11.0,1,58,How to fix &quot;interface ICorProfilerInfo3 not found&quot;,The requirement for the .net tracer is .NET CLR 4.5 and above.
Datadog,57207906,nan,1,"2019/07/25, 21:08:17",False,"2020/01/29, 02:05:05",nan,11837774.0,11.0,1,58,How to fix &quot;interface ICorProfilerInfo3 not found&quot;,"My understanding of .NET is that CLR is a component of the framework, hence CLR version is same as framework version."
Datadog,57207906,nan,1,"2019/07/25, 21:08:17",False,"2020/01/29, 02:05:05",nan,11837774.0,11.0,1,58,How to fix &quot;interface ICorProfilerInfo3 not found&quot;,I'm trying to understand why .NET runtime version (2.0.50727.8800 according to Windows event log) is older than the framework (4.6.1 according to the Windows control panel).
Datadog,57071999,nan,0,"2019/07/17, 11:49:55",False,"2019/07/17, 11:49:55",nan,488802.0,3575.0,1,51,Why do I see different values for a gauge in different timeframes?,I have a gauge in datadog and want to see the 'last' value.
Datadog,57071999,nan,0,"2019/07/17, 11:49:55",False,"2019/07/17, 11:49:55",nan,488802.0,3575.0,1,51,Why do I see different values for a gauge in different timeframes?,But when I select  the last day  in the dashboard time-picker I see a different value than when I select  the last week .
Datadog,57071999,nan,0,"2019/07/17, 11:49:55",False,"2019/07/17, 11:49:55",nan,488802.0,3575.0,1,51,Why do I see different values for a gauge in different timeframes?,My natural understanding is that 'last' gauge value should be the same regardless of how far back in time I go.
Datadog,57071999,nan,0,"2019/07/17, 11:49:55",False,"2019/07/17, 11:49:55",nan,488802.0,3575.0,1,51,Why do I see different values for a gauge in different timeframes?,Am I missing some understanding of how gauges or datadog works?
Datadog,56116856,nan,1,"2019/05/13, 20:02:42",True,"2019/05/13, 21:01:29",nan,3223737.0,534.0,1,140,How to get route data from Identity Server 4 endpoints,I have a ResponseTimeMiddleware.cs responsible for getting response time metrics (I am using datadog) for every request made.
Datadog,56116856,nan,1,"2019/05/13, 20:02:42",True,"2019/05/13, 21:01:29",nan,3223737.0,534.0,1,140,How to get route data from Identity Server 4 endpoints,Which is tagged by controller and action names.
Datadog,56116856,nan,1,"2019/05/13, 20:02:42",True,"2019/05/13, 21:01:29",nan,3223737.0,534.0,1,140,How to get route data from Identity Server 4 endpoints,"However when we hit the ""connect/token"" endpoint, the context.GetRouteData() is null, probably because identity server is doing it behind the scenes."
Datadog,56116856,nan,1,"2019/05/13, 20:02:42",True,"2019/05/13, 21:01:29",nan,3223737.0,534.0,1,140,How to get route data from Identity Server 4 endpoints,Is there a way I could get this information or some other unique information where I could tag with?
Datadog,56116856,nan,1,"2019/05/13, 20:02:42",True,"2019/05/13, 21:01:29",nan,3223737.0,534.0,1,140,How to get route data from Identity Server 4 endpoints,here's my code:
Datadog,56116856,nan,1,"2019/05/13, 20:02:42",True,"2019/05/13, 21:01:29",nan,3223737.0,534.0,1,140,How to get route data from Identity Server 4 endpoints,This is my Startup:
Datadog,55009193,55054296.0,1,"2019/03/05, 20:20:11",True,"2019/03/08, 01:11:52",nan,1181073.0,923.0,1,254,How to report StatsD metrics from a Google Cloud Function written in Node.js?,Is there a simple way it to report custom defined stats to our statsd / Datadog infrastructure from a Google Cloud Function written in Node.js?
Datadog,55009193,55054296.0,1,"2019/03/05, 20:20:11",True,"2019/03/08, 01:11:52",nan,1181073.0,923.0,1,254,How to report StatsD metrics from a Google Cloud Function written in Node.js?,"Since it's a high-traffic Javascript Cloud Function, I'd like to avoid heavy initialization of additional libraries every time the cloud function is invoked."
Datadog,55009193,55054296.0,1,"2019/03/05, 20:20:11",True,"2019/03/08, 01:11:52",nan,1181073.0,923.0,1,254,How to report StatsD metrics from a Google Cloud Function written in Node.js?,"Also, by custom stats I mean stats of our own definition (not boilerplate summary statistics via StackDriver or DataDog GCP integration)."
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,Bundler is causing a build fail for my Heroku app.
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,It is a ruby on rails app and was working perfectly and deployed on Heroku.
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,I started getting  the following build error only after I installed the datadog agent.
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,"The build error seemed easy enough to fix, but it has grown difficult."
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,"After removing my Gemfile.lock and reinstalling to make sure bundler version was   2.0, I then started getting another build error."
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,This time it was that the bundler version my project requires is &lt; 2:
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,"I updated the latest version on my system, like it suggests."
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,But then I run back into the first error that Bundler V2 is required for the Heroku build step.
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,This is my Gemfile.lock:
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,Here is my Gemfile if that helps
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,I'm really at a loss an appreciate your help.
Datadog,54729245,nan,1,"2019/02/17, 03:12:14",False,"2019/03/14, 12:00:21",nan,10881188.0,63.0,1,652,Bundler conflict - Heroku requires Bundler 2 but app requires Bundler v&lt; 2,ANY SUGGESTIONS APPRECIATED!
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,I've recently made some updates to a Ruby on Rails Heroku app that was working fine.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,I tried updating the bundler version so that it could be configured with a datadog agent.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Now when I try to push to heroku master I get the following error:
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"
 
 git push heroku master
Counting objects: 28, done."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Delta compression using up to 4 threads.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Compressing objects: 100% (28/28), done."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Writing objects: 100% (28/28), 3.33 KiB | 1.66 MiB/s, done."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Total 28 (delta 19), reused 0 (delta 0)
remote: Compressing source files... done."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: Building source:
remote: 
remote: -----&gt; ActiveStorage Preview app detected
remote: -----&gt; Installing binary dependencies for ActiveStorage Preview
remote:        Downloading packages..
remote:        Installing packages.....
remote: -----&gt; Ruby app detected
remote: 
remote:  !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,remote:  !
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"different prefix: """" and ""/tmp/build_2f915e77052b7fa5cef9531ffdd277e6""
remote:  !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/pathname.rb:522:in `relative_path_from': different prefix: """" and ""/tmp/build_2f915e77052b7fa5cef9531ffdd277e6"" (ArgumentError)
remote: 	from /tmp/d20190215-459-drze92/bundler-1.15.2/gems/bundler-1.15.2/lib/bundler/lockfile_parser.rb:98:in `rescue in initialize'
remote: 	from /tmp/d20190215-459-drze92/bundler-1.15.2/gems/bundler-1.15.2/lib/bundler/lockfile_parser.rb:61:in `initialize'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:130:in `new'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:130:in `block in parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:128:in `parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:112:in `lockfile_parser'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:74:in `specs'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:59:in `block in gem_version'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:58:in `gem_version'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/rails5.rb:9:in `block in use?'"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/base.rb:48:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/rails5.rb:8:in `use?'"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:17:in `block (2 levels) in detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `each'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `block in detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:13:in `detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/bin/support/ruby_compile:17:in `block in &lt;main&gt;'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:35:in `block in trace'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:35:in `trace'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/bin/support/ruby_compile:15:in `&lt;main&gt;'
remote:  !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Push rejected, failed to compile Ruby app."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: 
remote:  !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Push failed
remote: Verifying deploy...
remote: 
remote: !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Push rejected to codereader-backend.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: 
To https://git.heroku.com/codereader-backend.git
 !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"[remote rejected] master -&gt; master (pre-receive hook declined)
error: failed to push some refs"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Before this error I was getting an error about the my lockfile is unreadable.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,I removed the lockfile and rebundled to no avail.That error is below.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"
 
 git push heroku master
Counting objects: 27, done."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Delta compression using up to 4 threads.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Compressing objects: 100% (27/27), done."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Writing objects: 100% (27/27), 3.24 KiB | 1.62 MiB/s, done."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Total 27 (delta 18), reused 0 (delta 0)
remote: Compressing source files... done."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: Building source:
remote: 
remote: -----&gt; ActiveStorage Preview app detected
remote: -----&gt; Installing binary dependencies for ActiveStorage Preview
remote:        Downloading packages..
remote:        Installing packages.....
remote: -----&gt; Ruby app detected
remote: 
remote:  !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,remote:  !
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Your lockfile is unreadable.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Run `rm Gemfile.lock` and then `bundle install` to generate a new lockfile.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,remote:  !
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,remote: /tmp/d20190215-478-1u5qan1/bundler-2.0.1/gems/bundler-2.0.1/lib/bundler/lockfile_parser.rb:98:in `rescue in initialize': Your lockfile is unreadable.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Run `rm Gemfile.lock` and then `bundle install` to generate a new lockfile.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"(Bundler::LockfileError)
remote: 	from /tmp/d20190215-478-1u5qan1/bundler-2.0.1/gems/bundler-2.0.1/lib/bundler/lockfile_parser.rb:61:in `initialize'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:130:in `new'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:130:in `block in parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:128:in `parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:112:in `lockfile_parser'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:74:in `specs'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:59:in `block in gem_version'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:58:in `gem_version'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/rails5.rb:9:in `block in use?'"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/base.rb:48:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/rails5.rb:8:in `use?'"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:17:in `block (2 levels) in detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `each'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `block in detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:13:in `detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/bin/support/ruby_compile:17:in `block in &lt;main&gt;'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:35:in `block in trace'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:35:in `trace'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/bin/support/ruby_compile:15:in `&lt;main&gt;'
remote:  !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Push rejected, failed to compile Ruby app."
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"remote: 
remote:  !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,"Push failed
remote: Verifying deploy...
remote: 
remote: !"
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Push rejected to codereader-backend.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,remote:
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Any help is appreciated.I'm not sure what steps to take and there aren't any similar errors that I've found.
Datadog,54717464,nan,1,"2019/02/15, 23:45:01",False,"2019/06/13, 03:54:58",nan,10881188.0,63.0,1,395,&quot;different prefix&quot; error and &quot;Lockfile is unreadable&quot; error when pushing to Heroku,Thank you.
Datadog,52755069,nan,1,"2018/10/11, 10:59:41",False,"2018/10/11, 14:16:24",nan,7380427.0,79.0,1,155,Is it possible to trigger alarm based on output of bash command,Is it possible to trigger alarm in zabbix or datadog based on output of bash command.
Datadog,52755069,nan,1,"2018/10/11, 10:59:41",False,"2018/10/11, 14:16:24",nan,7380427.0,79.0,1,155,Is it possible to trigger alarm based on output of bash command,I.e.
Datadog,52755069,nan,1,"2018/10/11, 10:59:41",False,"2018/10/11, 14:16:24",nan,7380427.0,79.0,1,155,Is it possible to trigger alarm based on output of bash command,I have a bash command that returns value and I want to trigger an alarm if value rises above some level.
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,I am investigating options for monitoring our installation in Swisscom's cloud-foundry.
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,My objectives are the following:
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,"So far, I understand the options are the following (including some BUTs):"
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,"That is very useful for tracing / ad-hoc monitoring, but not very good for a serious infrastructure monitoring."
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,"This can be deployed as an app to (as far as I understand) do the job in similar way, as the TOP cf plugin."
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,"The problem is, that it requires registered client, so it can authenticate with the doppler endpoint."
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,"For some reason, the top-cf-plugin does that automatically / in another way."
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,That can be for example done with  Datadog .
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,But it seems to also require a dedicated uaa client to register the Nozzle.
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,"I would like to check, if somebody is (was) on the similar road, has some findings."
Datadog,52588302,52590134.0,1,"2018/10/01, 12:39:51",True,"2018/10/02, 11:06:05","2018/10/02, 11:06:05",5150067.0,107.0,1,216,Application Performance monitoring on Swisscom Application Cloud,Eventually I would like to raise the following questions towards the swisscom community support:
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,In a AWS ECS cluster each cluster instance runs the ecs-agent [1] as a docker container.
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,Next to that container I run datadog-agent [2] also as a container.
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,The datadog-agent monitors all other containers and ship their logs to DataDog.
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,In order to have the log of each container tagged by name I've added a specific docker label [3] to each container with the respective name.
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,"However, I'm not been able to add a docker label to the ecs-agent itself."
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,Is there a way to add custom docker labels to the ecs-agent container?
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,[1] -  https://github.com/aws/amazon-ecs-agent
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,[2] -  https://github.com/DataDog/datadog-agent
Datadog,51097821,nan,1,"2018/06/29, 11:50:53",False,"2018/07/08, 06:33:21",nan,2185719.0,768.0,1,1090,How to set docker labels on ECS agent container?,[3] -  https://docs.datadoghq.com/logs/log_collection/docker/
Datadog,50700128,nan,0,"2018/06/05, 15:29:52",False,"2018/06/05, 15:29:52",nan,2908747.0,21.0,1,345,Prometheus google cloud monitoring,"We are Moving from datadog to prometheus,
Datadog have google cloud metrics, for example:
gcp.gcp.instance.is_running 
gcp.gcp.project.quota.networks.limit
and more .. (all metrics starting with gcp)"
Datadog,50700128,nan,0,"2018/06/05, 15:29:52",False,"2018/06/05, 15:29:52",nan,2908747.0,21.0,1,345,Prometheus google cloud monitoring,"i want to have those google cloud metrics using prometheus - 
is there an exporter to get them?couldn't find anything ."
Datadog,50266331,nan,1,"2018/05/10, 08:51:58",False,"2018/05/10, 09:02:22","2018/05/10, 09:02:22",5057152.0,1017.0,1,361,automate exe installation in AWS ec2 instances,Is there any way to install exe/MSI agents in AWS EC2 instances in an automated way??
Datadog,50266331,nan,1,"2018/05/10, 08:51:58",False,"2018/05/10, 09:02:22","2018/05/10, 09:02:22",5057152.0,1017.0,1,361,automate exe installation in AWS ec2 instances,"In specific, I am looking for a counterpart of Azure's Custom Script Extension."
Datadog,50266331,nan,1,"2018/05/10, 08:51:58",False,"2018/05/10, 09:02:22","2018/05/10, 09:02:22",5057152.0,1017.0,1,361,automate exe installation in AWS ec2 instances,[Free of cost]
Datadog,50266331,nan,1,"2018/05/10, 08:51:58",False,"2018/05/10, 09:02:22","2018/05/10, 09:02:22",5057152.0,1017.0,1,361,automate exe installation in AWS ec2 instances,"Scenario:
I want to install BigFix and Datadog agents on 1000 Ec2 instances, this is a one time job, so I am not looking for any solution that involves Chef / Puppet, etc.,"
Datadog,49892339,nan,1,"2018/04/18, 09:08:34",True,"2019/02/11, 05:23:03","2019/02/11, 05:23:03",5057152.0,1017.0,1,114,API to access Cloudlyn,I am looking for any API or any way to access Microsoft's Cloudyn service.
Datadog,49892339,nan,1,"2018/04/18, 09:08:34",True,"2019/02/11, 05:23:03","2019/02/11, 05:23:03",5057152.0,1017.0,1,114,API to access Cloudlyn,"I want to extract data - Azure Storage Cost and utilization per month, this is available under the ""Management Dashboard of Cloudyn"" and want to integrate with Datadog."
Datadog,49741158,nan,0,"2018/04/09, 23:26:10",False,"2018/04/09, 23:26:10",nan,4167402.0,200.0,1,384,What could be causing random spikes of redis commands/second and latency?,I'm using Datadog and NewRelic to try and track down odd behavior that seems to happen at random times.
Datadog,49741158,nan,0,"2018/04/09, 23:26:10",False,"2018/04/09, 23:26:10",nan,4167402.0,200.0,1,384,What could be causing random spikes of redis commands/second and latency?,Recently I have noticed huge spikes in REDIS latency to my application in NewRelic.
Datadog,49741158,nan,0,"2018/04/09, 23:26:10",False,"2018/04/09, 23:26:10",nan,4167402.0,200.0,1,384,What could be causing random spikes of redis commands/second and latency?,I added Datadog to the Redis server and saw these spikes of commands/second going from around ~0.5-2k to over 40-60k!
Datadog,49741158,nan,0,"2018/04/09, 23:26:10",False,"2018/04/09, 23:26:10",nan,4167402.0,200.0,1,384,What could be causing random spikes of redis commands/second and latency?,"Along with those is a spike in bandwidth and load, but only very mindor CPU changes."
Datadog,49741158,nan,0,"2018/04/09, 23:26:10",False,"2018/04/09, 23:26:10",nan,4167402.0,200.0,1,384,What could be causing random spikes of redis commands/second and latency?,"When these were occurring, GoogleAnalytics (GA) was actually showing a rather slow day in comparison."
Datadog,49741158,nan,0,"2018/04/09, 23:26:10",False,"2018/04/09, 23:26:10",nan,4167402.0,200.0,1,384,What could be causing random spikes of redis commands/second and latency?,"In fact, the overall app load today is about 2-3x higher than the day shown in the image below, but today has had perfect REDIS performance without any latency/commands spikes."
Datadog,49741158,nan,0,"2018/04/09, 23:26:10",False,"2018/04/09, 23:26:10",nan,4167402.0,200.0,1,384,What could be causing random spikes of redis commands/second and latency?,Could it be bots/crawlers hitting stale caches that are causing large chunks of data to be inserted at once?
Datadog,49741158,nan,0,"2018/04/09, 23:26:10",False,"2018/04/09, 23:26:10",nan,4167402.0,200.0,1,384,What could be causing random spikes of redis commands/second and latency?,"My app heavily relies on an external API, which does occasionally spike in response time as well, but why would a slow API call cause slower redis calls or massive spikes in redis commands?"
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,Understand the options to secure the docker.sock.
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,"As in those articles, giving access to docker.sock is a risk."
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,However there could be cases where we need to deploy a pod such which needs to talk to docker daemon via the socket for monitoring or controlling.
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,For example  datadog  which mounts the socket via hostPath mount.
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,OpenShift requires explicit grant of SCC e.g.
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,"hostaccess to the service account which runs the pod for the pod to use hostPath, but it is OpenShift proprietary."
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,I suppose SELinux can be used so that any pods who access the docker socker are required to have a certain label.
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,"I would like to know if my understanding of SELinux label is valid, and what other options would be available."
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,volume-mounting the docker socket into a container is unsupported by Red Hat.
Datadog,48475616,nan,1,"2018/01/27, 14:05:54",False,"2018/01/30, 02:19:00","2018/01/30, 02:19:00",4281353.0,8369.0,1,494,Option for Securing docker socket,"This means that while it is entirely possible to do so (as with any other volume mount), Red Hat is unable to assist with configurations using this setup, problems that arise because of this setup or the security implications/concerns surrounding this setup."
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,I'm pushing gunicorn metrics from multiple applications into datadog from the same host however I cannot find a way to group the statsd metrics using either a tag or proc_name.
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,Datadog gunicorn integration
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,https://app.datadoghq.com/account/settings#integrations/gunicorn
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,Datadog agent checks are being updated automatically with the  app:proc_name  tag.
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,I can use this to group and select the data for a specific service.
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,https://github.com/DataDog/dd-agent/blob/5.2.x/checks.d/gunicorn.py#L53
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,"For the statsd metrics however, I do not see how to assign a tag or proc_name."
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,This is not being done automatically nor do I see a way to specify a tag.
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,https://github.com/benoitc/gunicorn/blob/19.6.0/gunicorn/instrument/statsd.py#L90
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,Datadog config:
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,Gunicorn config:
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,Any ideas on how this might be achieved?
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,Examples using notebooks:
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,"In this example, I am able to select  app:service  in either the 'from' or 'avg by' drop downs."
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,For the metrics with the my_namespace prefix I am unable to reference the same application name.
Datadog,48457894,48576285.0,1,"2018/01/26, 10:28:44",True,"2019/03/29, 04:35:10","2018/01/26, 10:55:07",607808.0,253.0,1,449,How to tag gunicorn metrics with proc_name?,Only host and environment related tags are available.
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,I have a squid proxy container on my local Docker for Mac (datadog/squid image).
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,Essentially I use this proxy so that app containers on my local docker and the browser pod (Selenium) on another host use the same network for testing (so that the remote browser can access the app host).
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,"But with my current setup, when I run my tests the browser starts up on the remote host and then after a bit fails the test."
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,The message on the browser is  ERR_PROXY_CONNECTION_FAILED  right before it closes.
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,So I assume that there is an issue with my squid proxy config.
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,I use the default config and on the docker hub site it says
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,"Please note that the stock configuration available with the container is set for local access, you may need to tweak it if your network scenario is different."
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,I'm not really sure how my network scenario is different.
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,What should I be looking into for more information?
Datadog,47873250,nan,0,"2017/12/18, 19:10:13",False,"2017/12/18, 19:10:13",nan,8576661.0,371.0,1,841,ERR_PROXY_CONNECTION_FAILED when using squid proxy for connection,Thanks!
Datadog,47165244,nan,0,"2017/11/07, 20:40:47",False,"2017/11/07, 20:40:47",nan,8576661.0,371.0,1,82,Connect minikube pods to docker-machine network via proxy,I would like my pods in my minikube-vm to use the network that my docker-machine uses.
Datadog,47165244,nan,0,"2017/11/07, 20:40:47",False,"2017/11/07, 20:40:47",nan,8576661.0,371.0,1,82,Connect minikube pods to docker-machine network via proxy,"Inside the docker-machine I set up a proxy container(datadog/squid) for minikube to access, but I'm not sure exactly how to allow my minikube/pods to use this proxy."
Datadog,47165244,nan,0,"2017/11/07, 20:40:47",False,"2017/11/07, 20:40:47",nan,8576661.0,371.0,1,82,Connect minikube pods to docker-machine network via proxy,Should I be setting the env vars HTTP_PROXY or is there something else I need to do?
Datadog,47165244,nan,0,"2017/11/07, 20:40:47",False,"2017/11/07, 20:40:47",nan,8576661.0,371.0,1,82,Connect minikube pods to docker-machine network via proxy,"Not sure what I should be looking into, any help would be appreciated, thanks!"
Datadog,46154416,nan,1,"2017/09/11, 14:16:10",False,"2017/09/11, 14:24:24",nan,5131588.0,51.0,1,293,How to calculate Restcall Response time on Jmeter,For me it is very strange that jmeter does not bring response time for restcall.
Datadog,46154416,nan,1,"2017/09/11, 14:16:10",False,"2017/09/11, 14:24:24",nan,5131588.0,51.0,1,293,How to calculate Restcall Response time on Jmeter,These are all possibilites to be saved on jtl/csv file:
Datadog,46154416,nan,1,"2017/09/11, 14:16:10",False,"2017/09/11, 14:24:24",nan,5131588.0,51.0,1,293,How to calculate Restcall Response time on Jmeter,My question is either if response time is equal one of the data above or if I can manually calculate sum some values and get to it.
Datadog,46154416,nan,1,"2017/09/11, 14:16:10",False,"2017/09/11, 14:24:24",nan,5131588.0,51.0,1,293,How to calculate Restcall Response time on Jmeter,PS: The reason while I don't simply use Jmeter Response Time graph is due to the fact that I send data to Datadog (measurement tool) instead.
Datadog,46079056,46079393.0,1,"2017/09/06, 18:24:55",True,"2017/09/06, 18:42:48",nan,3966601.0,3914.0,1,122,Understanding some postgresql.conf settings I changed,I followed a Youtube video by Chris Pettus called PostgreSQL Proficiency for Python People to edit some of my postgres.conf settings.
Datadog,46079056,46079393.0,1,"2017/09/06, 18:24:55",True,"2017/09/06, 18:42:48",nan,3966601.0,3914.0,1,122,Understanding some postgresql.conf settings I changed,"My server has 28 gigs of RAM and prior to making the changes, my system memory was averaging around 3GB."
Datadog,46079056,46079393.0,1,"2017/09/06, 18:24:55",True,"2017/09/06, 18:42:48",nan,3966601.0,3914.0,1,122,Understanding some postgresql.conf settings I changed,Now it hovers around 10GB.
Datadog,46079056,46079393.0,1,"2017/09/06, 18:24:55",True,"2017/09/06, 18:42:48",nan,3966601.0,3914.0,1,122,Understanding some postgresql.conf settings I changed,"I am not having any issues right now, but I would like to understand the pros and cons of the changes I made."
Datadog,46079056,46079393.0,1,"2017/09/06, 18:24:55",True,"2017/09/06, 18:42:48",nan,3966601.0,3914.0,1,122,Understanding some postgresql.conf settings I changed,I assume that there must be some tangible benefits of tripling the average memory being used in my system (measured with Datadog).
Datadog,46079056,46079393.0,1,"2017/09/06, 18:24:55",True,"2017/09/06, 18:42:48",nan,3966601.0,3914.0,1,122,Understanding some postgresql.conf settings I changed,My server is used to perform ETL (Airflow) and hosts the database.
Datadog,46079056,46079393.0,1,"2017/09/06, 18:24:55",True,"2017/09/06, 18:42:48",nan,3966601.0,3914.0,1,122,Understanding some postgresql.conf settings I changed,"Airflow has a lot of connections but typically the files are pretty small (a few mb) which are processed with pandas, compared with the database to find new rows, and then loaded."
Datadog,45147143,nan,1,"2017/07/17, 17:40:44",True,"2017/07/17, 17:52:41","2017/07/17, 17:52:41",2835257.0,433.0,1,44,Initial and maximum recomended Heap size on server,I have several applications running on a Glassfish application server (4.0).
Datadog,45147143,nan,1,"2017/07/17, 17:40:44",True,"2017/07/17, 17:52:41","2017/07/17, 17:52:41",2835257.0,433.0,1,44,Initial and maximum recomended Heap size on server,"I have recorded some statistics of the java memory usage with DataDog, so I am able to see the historic of the used heap memory along with the  initial  and  maximum  constant heap sizes."
Datadog,45147143,nan,1,"2017/07/17, 17:40:44",True,"2017/07/17, 17:52:41","2017/07/17, 17:52:41",2835257.0,433.0,1,44,Initial and maximum recomended Heap size on server,"
 The image shows the initial (yellow), maximum (blue) and real (purple) heap values."
Datadog,45147143,nan,1,"2017/07/17, 17:40:44",True,"2017/07/17, 17:52:41","2017/07/17, 17:52:41",2835257.0,433.0,1,44,Initial and maximum recomended Heap size on server,"As you can see above, the real heap size is always bellow the initial heap value, so I'm planning to move these parameters to improve the server's performance, but I'm not sure if this is really necessary."
Datadog,45147143,nan,1,"2017/07/17, 17:40:44",True,"2017/07/17, 17:52:41","2017/07/17, 17:52:41",2835257.0,433.0,1,44,Initial and maximum recomended Heap size on server,"So, I have this doubts:"
Datadog,45147143,nan,1,"2017/07/17, 17:40:44",True,"2017/07/17, 17:52:41","2017/07/17, 17:52:41",2835257.0,433.0,1,44,Initial and maximum recomended Heap size on server,"I guess this questions hold true talking of a tomcat, JBoss or any servlet-oriented server."
Datadog,45147143,nan,1,"2017/07/17, 17:40:44",True,"2017/07/17, 17:52:41","2017/07/17, 17:52:41",2835257.0,433.0,1,44,Initial and maximum recomended Heap size on server,Any help will be gratefully received.
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"I recently watched the  OSCON Austin 2016 talk  on  ""Detecting outliers and anomalies in realtime at Datadog""  by  Homin Lee  and found proper motivation to ask the following question."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,Basically what I am trying to do is find anomalies in graphs that do not necessarily start at the same time ( t ) but are quite similar (in family) in shape.
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,Separated:
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,Combined:
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"As depicted in my (rough) concept drawing, given two similar frequency ( f ) functions of time, I want to line them up on top of each other on the basis of where each of their inflection points are."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,One of the frequency graphs starts at  t=-2  and the other  t=5 .
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,They both have inflection points around  t_1=8.5  and  t_2=1.5 .
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,This is where I want to line them up.
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"Essentially, the image drawn should be the final output from my algorithm and listing any anomalies that trigger like say for the green curve, if  f=0.2  at  t_1=12 , then that should be an anomaly because it is not in family."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"And as Homin Lee says it, the graph would not be ""within the trained envelope."""
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,Now I want to lay out what my particular approach would be and see if you think the same or have a better approach to develop this algorithm.
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"Before we choose which anomaly detection algorithm to use, we need to discuss how to prepare this data."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,We will continue to use frequency-versus-time data for example purposes.
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"To prepare the data, we need to (1) find the inflection points, (2) scale the data so that the data all have the same time domain duration (i.e.,  12-5=7=7=5-(-2) ), and (3) find a way to match (line up) the times of each graph (i.e., 5 to -2, 6 to -1, and so on)."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"Once the data is prepared, now it is onto the algorithm."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"For (robust) anomaly detection, I was thinking about using  one-class / multi-class  Support Vector Machines (SVM) because we are going to be training a huge set of graphs to form the ""envelope."""
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,This section is also open for suggestions.
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"As a moonshot thought, I would like to eventually be able to put all of the graphs onto one huge plot and point out the anomalies from there."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,The problem is that there would be so many different time intervals.
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"So one solution would be to create a singular universal ( u ) time interval so that you don't have to deal with the differing intervals (e.g.,  t_1=5,9  would become  t_u=1,5  and same goes for  t_2 )."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"So to recap, I am looking to analyze similar graphs on different time intervals for anomalies."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"Find critical/key points (not necessarily inflection points), scale, plot graphs, and check for anomalies."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"I have rambled on for long enough but if something does not make sense and you would like me to clarify or elaborate, let me know and I will."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"Feel free to make suggestions, submit to me some code, and/or any other ideas or approaches that I did not necessary think of before."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,Thank you.
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,"P.S., sorry about the drawings; I tried my best."
Datadog,44914194,nan,0,"2017/07/05, 00:28:39",False,"2017/07/05, 00:49:53","2017/07/05, 00:49:53",8099160.0,545.0,1,331,Anomaly detection between similar (in family) graphs starting at different times,:P
Datadog,41025255,nan,1,"2016/12/07, 20:58:40",True,"2016/12/08, 00:14:48",nan,1384132.0,41.0,1,91,Ansible : Install a role package and run it,I'm using Datadog for Ansible.
Datadog,41025255,nan,1,"2016/12/07, 20:58:40",True,"2016/12/08, 00:14:48",nan,1384132.0,41.0,1,91,Ansible : Install a role package and run it,I have a role which installs the Datadog package but doesn't run the datadog role automatically after the package installation.
Datadog,41025255,nan,1,"2016/12/07, 20:58:40",True,"2016/12/08, 00:14:48",nan,1384132.0,41.0,1,91,Ansible : Install a role package and run it,"Currently, we need in each project to call Datadog role manually."
Datadog,41025255,nan,1,"2016/12/07, 20:58:40",True,"2016/12/08, 00:14:48",nan,1384132.0,41.0,1,91,Ansible : Install a role package and run it,"Is it possible to call Datadog role in my role1 instead of having to write ""datadog.datadog"" everywhere after role1."
Datadog,41025255,nan,1,"2016/12/07, 20:58:40",True,"2016/12/08, 00:14:48",nan,1384132.0,41.0,1,91,Ansible : Install a role package and run it,"Precisely, can we execute a role after a task which is responsible to install this role ?"
Datadog,41025255,nan,1,"2016/12/07, 20:58:40",True,"2016/12/08, 00:14:48",nan,1384132.0,41.0,1,91,Ansible : Install a role package and run it,Thank you in advance :)
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,In DockerCloud I am trying to get my container to speak with the other container.
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,I believe the problem is the hostname not resolving (this is set in  /conf.d/kafka.yaml  shown below).
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,"To get DockerCloud to have the two containers communicate, I have tried many variations including the full host-name  kafka-development-1  and  kafka-development-1.kafka , etc."
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,Within the container I run  ./etc/init.d/datadog-agent info  and receive:
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,SSH Into Docker Node:
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,"I log into the containers to see their values, this is the  datadog-agent :"
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,This is the  kafka container :
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,Datadog  conf.d/kafka.yaml :
Datadog,40776142,nan,0,"2016/11/24, 02:03:05",False,"2016/11/24, 02:03:05",nan,216909.0,4985.0,1,115,Docker Cloud Service Discovery Two Containers,Can anyone see what I am doing wrong?
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"I am implementing a recommendation engine in .Net C#, I am using Cassandra to store the data."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"I am still new in using C*, just started using it 2 months ago."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"At the moment I have only 2 nodes in my cluster (single DC), deployed in Azure DS2 VM (each has 7Gb RAM, 2 Cores)."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"I set  RF=2, CL=1  for both read and write."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,I set the timeouts in yaml config file as below
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,I set lower read query timeout in client side (30 secs each).
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"The data stored in cassandra is user history, item counter, and recommended items data."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"I created an API (stands in equinix DC) for my recommendation engine, its work is very simple, only reading all recommended_items Id from recommended_items table in C* everytime a user opens the website page."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,It means that the query is very simple for each user :
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"When I did load testing for up to 500 users/threads, it was fine and very fast."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"But when the online site calls API to read from C* table, I got read timeouts very often."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,There were usually only less than 20 users at the same time though.
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"I monitor the cassandra nodes activity using DataDog and I found that only node #2 that keeps getting timeouts (the seed node is node #1, though what I understand is seed doesn't really matter except during bootstrapping step)."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"However, everytime the timeout happens, I tried to query using cqlsh in both nodes, and node #1 is the one that return"
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,OperationTimeOut Exception.
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,I have been trying to find the main root of this issue.
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,Does that have anything to do with coordinator node being down ( I read this article ) ?
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,Or is that because I have only 2 nodes?
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"When the timeout happens (the webpage shows nothing), then I tried to refresh the page that calls the API, it will be loading for long time before showing nothing again (because of the timeout)."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,"But surprisingly, I will get the log that all those requests were actually successful after few minutes even though the web page has been closed."
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,It's like the read request was still running even though the page has been closed.
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,The exception are like these (they didn't happen together) :
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,OR
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,Does anyone have any suggestion about my problem?
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,thank you.
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,output of cfstats .recommended_items
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,NODE #1
Datadog,39629477,nan,0,"2016/09/22, 05:49:27",False,"2018/03/28, 10:25:48","2017/09/22, 21:01:22",6794459.0,11.0,1,349,Cassandra Cluster with 2 Nodes got Read TimeOut/NoHostAvailable Exception,NODE #2
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","My name is Daniel, 
I'm a newcomer accountwise but a long time lurker."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","I decided to learn Apache Cassandra for my next ""lets write some code while the kids are sleeping"" project."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",What i'm writing is a neat little api that will do read and writes against a cassandra database.
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","I had a lot of the db layout figured out in mongodb, but for me it's time to move on and grow as a engineer :)"
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","Mission:
I will collect metrics from the servers in my rack, an agent will send a payload of metrics every minute."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","I have the api part pretty much figured out, will use JWT tokens signing the payloads."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",The type of data i will store can be seen below.
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","cpuload, cpuusage, memusage, diskusage etc."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","The part where i am confused with cassandra is how to write the actual model, i understand the storagengines sort of writes it all as a time serie
on disk for me making reads quite amazing."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","i know anything i would whip together now would work for my lab since it's jsut 30 machines, 
but i'm trying to understand how these things are done properly and how it could be done for a real life scenario like server density, datadog , ""insert your prefered server monitoring service""."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",:)
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",But how are you more experienced engineers designing a schema like this ?
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",Usage scenarios for the database:
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",Read the assets associated with ones userid
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",Generate monthly pdf reports showing uptime and such.
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","Should i insert the rows containing the full payload or am i better of inserting them per service basis: timeuid|cpuusage 
Per service row"
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",All in one
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","In mongo i would preallocate the buckets, and also keep a quick read avg inside of the document."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",So in the webgui i could simply show the avg stats for pre-defined time periods.
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",Examples for dumbasses are highly appreciated.
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics",Hope you can decipher my rather poor english.
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","Just found this url in the SO suggestions:
 Cassandra data model for time series 
i guess that is something that applies to me aswell."
Datadog,38359717,38387725.0,1,"2016/07/13, 22:00:44",True,"2017/08/27, 05:43:11","2017/09/22, 21:01:22",6586071.0,13.0,1,460,"How to model Cassandra DB for Time Series, server metrics","Sincerly
Daniel Olsson"
Datadog,35866315,36528782.0,1,"2016/03/08, 13:37:07",True,"2016/04/10, 13:51:31",nan,769189.0,508.0,1,572,Adding portMappings to Dockerrun.aws.json with Single container configuartion,I am using  ElasticBeanstalk with single docker container .
Datadog,35866315,36528782.0,1,"2016/03/08, 13:37:07",True,"2016/04/10, 13:51:31",nan,769189.0,508.0,1,572,Adding portMappings to Dockerrun.aws.json with Single container configuartion,I am using DataDog(statsd client) for pushing metrics from the docker container.
Datadog,35866315,36528782.0,1,"2016/03/08, 13:37:07",True,"2016/04/10, 13:51:31",nan,769189.0,508.0,1,572,Adding portMappings to Dockerrun.aws.json with Single container configuartion,I have a running datadog-agent which is technically a statsd client on the host machine.
Datadog,35866315,36528782.0,1,"2016/03/08, 13:37:07",True,"2016/04/10, 13:51:31",nan,769189.0,508.0,1,572,Adding portMappings to Dockerrun.aws.json with Single container configuartion,The issue I am facing is to connect that client running at port 8125 from the container.
Datadog,35866315,36528782.0,1,"2016/03/08, 13:37:07",True,"2016/04/10, 13:51:31",nan,769189.0,508.0,1,572,Adding portMappings to Dockerrun.aws.json with Single container configuartion,What I have tried is:
Datadog,35866315,36528782.0,1,"2016/03/08, 13:37:07",True,"2016/04/10, 13:51:31",nan,769189.0,508.0,1,572,Adding portMappings to Dockerrun.aws.json with Single container configuartion,Thanks in Advance
Datadog,66005706,nan,1,"2021/02/02, 10:01:11",False,"2021/03/07, 08:31:28","2021/02/03, 11:32:19",15128440.0,11.0,1,74,Time sync deviation and offsets between VM&#39;s in Azure (VMSS),"We have been handling a rather peculiar issue with  clock offsets between VM's (Windows Server 2019) on Azure, that are hosted in the same region and datacenter, moreover in a VMSS ."
Datadog,66005706,nan,1,"2021/02/02, 10:01:11",False,"2021/03/07, 08:31:28","2021/02/03, 11:32:19",15128440.0,11.0,1,74,Time sync deviation and offsets between VM&#39;s in Azure (VMSS),"Several facts regarding the issue, after doing some experiments in the last three months:"
Datadog,66005706,nan,1,"2021/02/02, 10:01:11",False,"2021/03/07, 08:31:28","2021/02/03, 11:32:19",15128440.0,11.0,1,74,Time sync deviation and offsets between VM&#39;s in Azure (VMSS),Would appreciate any ideas.
Datadog,65785913,65786188.0,1,"2021/01/19, 07:26:06",True,"2021/01/19, 08:18:40",nan,7409481.0,72.0,1,95,How to automate the creation of elasticsearch index patterns for all days?,I am using cloudwatch subscription filter which automatically sends logs to elasticsearch aws and then I use Kibana from there.
Datadog,65785913,65786188.0,1,"2021/01/19, 07:26:06",True,"2021/01/19, 08:18:40",nan,7409481.0,72.0,1,95,How to automate the creation of elasticsearch index patterns for all days?,The issue is that everyday cloudwatch creates a new indice due to which I have to manually create the new index pattern each day in kibana.
Datadog,65785913,65786188.0,1,"2021/01/19, 07:26:06",True,"2021/01/19, 08:18:40",nan,7409481.0,72.0,1,95,How to automate the creation of elasticsearch index patterns for all days?,Accordingly I will have to create new monitors and alerts in kibana as well each day.
Datadog,65785913,65786188.0,1,"2021/01/19, 07:26:06",True,"2021/01/19, 08:18:40",nan,7409481.0,72.0,1,95,How to automate the creation of elasticsearch index patterns for all days?,I have to automate this somehow.
Datadog,65785913,65786188.0,1,"2021/01/19, 07:26:06",True,"2021/01/19, 08:18:40",nan,7409481.0,72.0,1,95,How to automate the creation of elasticsearch index patterns for all days?,Also if there is better option with which I can go forward would be great.
Datadog,65785913,65786188.0,1,"2021/01/19, 07:26:06",True,"2021/01/19, 08:18:40",nan,7409481.0,72.0,1,95,How to automate the creation of elasticsearch index patterns for all days?,I know datadog is one good option.
Datadog,63338416,nan,1,"2020/08/10, 13:28:10",False,"2021/02/05, 09:17:00",nan,13958041.0,434.0,1,122,Kafka Follower Fetch is high,I have setup a 4 broker Kafka cluster on AWS MSK (version 2.2.1).
Datadog,63338416,nan,1,"2020/08/10, 13:28:10",False,"2021/02/05, 09:17:00",nan,13958041.0,434.0,1,122,Kafka Follower Fetch is high,I am monitoring the same through datadog (crawler setup from  https://docs.datadoghq.com/integrations/amazon_msk/ ).
Datadog,63338416,nan,1,"2020/08/10, 13:28:10",False,"2021/02/05, 09:17:00",nan,13958041.0,434.0,1,122,Kafka Follower Fetch is high,"Now as per my understanding, the fetch follower total time is the sum of the other metrics."
Datadog,63338416,nan,1,"2020/08/10, 13:28:10",False,"2021/02/05, 09:17:00",nan,13958041.0,434.0,1,122,Kafka Follower Fetch is high,But you can see all of them &lt;1ms while fetch follower total time is over 200ms.
Datadog,63338416,nan,1,"2020/08/10, 13:28:10",False,"2021/02/05, 09:17:00",nan,13958041.0,434.0,1,122,Kafka Follower Fetch is high,I have not really changed the default Kafka config much:
Datadog,63338416,nan,1,"2020/08/10, 13:28:10",False,"2021/02/05, 09:17:00",nan,13958041.0,434.0,1,122,Kafka Follower Fetch is high,Can someone suggest a possible cause of this high follower fetch time and how to reduce it?
Datadog,63338416,nan,1,"2020/08/10, 13:28:10",False,"2021/02/05, 09:17:00",nan,13958041.0,434.0,1,122,Kafka Follower Fetch is high,Any approches to identify the cause are also welcome.
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,I'm sending UDP packets (statsd) from pods on a host to  &lt;hostIP&gt;:8125 .
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,"On the other end, a collector (datadog-agent using  hostPort ; one per host via DaemonSet) picks up the packets and does it's thing."
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,"Generally this works fine, but if I ever delete + re-create the collector ( kubectl delete pod datadog-agent-xxxx ; new pod is started on same IP/port a few seconds later), traffic from  existing  client-sockets stop arriving at the collector (UDP sockets created  after  the pod-rescheduling works fine)."
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,Re-starting just the agent inside the collector pod ( kubectl exec -it datadog-agent-xxxxx agent stop ; auto-restarts after ~30s) the same old traffic  does  show up.
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,So containers somehow must have an impact.
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,"While UDP are (supposedly) stateless, something, somewhere is obviously keeping state around!?"
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,Any ideas/pointers?
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,"Each ""client"" pod has something like this in the deployment/pod:"
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,On the collector (following  datadog's k8s docs ):
Datadog,58094024,58132394.0,1,"2019/09/25, 11:13:42",True,"2019/09/27, 13:06:50","2019/09/25, 11:38:27",145307.0,5802.0,1,272,Intra-host UDP traffic missing after destination-pod re-created,This happens on Kubernetes 1.12 on Google Kubernetes Engine.
Datadog,51727504,nan,1,"2018/08/07, 16:02:04",True,"2018/08/07, 16:19:11",nan,2371715.0,1428.0,1,359,Kafka Broker Health Metrics,"I am looking for JMX metric(s) for Kafka Broker [Not more than 1 or 2, if possible] which at a high level can identify the health of the cluster?"
Datadog,51727504,nan,1,"2018/08/07, 16:02:04",True,"2018/08/07, 16:19:11",nan,2371715.0,1428.0,1,359,Kafka Broker Health Metrics,"I have referred to the list compiled by datadog and confluent, but couldn't find anything similar."
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,Absolutely love micrometer.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,Running a spring boot JAX RS app.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,Am using datadog registry and wanted to time some service methods.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,I saw all rest end points are timed and detail sent - thats great.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,However here is the issue I am facing.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,"If I have a GET call that takes a path param, for example  /employees/{empId} , And If I have a million users in the system, then I guess we will have 1 million tag combinations pushed to datadog."
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,So question is how to get around this.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,In this case I am only interested in all of the GET calls and not a specific call to get a specific employee.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,So how do I also tell micrometer/spring-boot to use the pattern rather than actual values.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,(I am not using @Timed on the JAX-RS resource classes.
Datadog,49939500,nan,0,"2018/04/20, 13:24:36",False,"2018/04/20, 13:24:36",nan,5118663.0,1563.0,1,94,Limit tags on REST endpoints,Is that the way to go ?)
Datadog,41252657,nan,3,"2016/12/21, 01:09:55",True,"2016/12/21, 01:31:54","2016/12/21, 01:13:29",7323221.0,19.0,1,3715,Best way to use split and strip,"My function scrapes my servers for the command and outputs something along the lines of  offset=1.3682  which  metrics_emit  uses to send to our metrics collector/visualizer, datadog."
Datadog,41252657,nan,3,"2016/12/21, 01:09:55",True,"2016/12/21, 01:31:54","2016/12/21, 01:13:29",7323221.0,19.0,1,3715,Best way to use split and strip,What I need to do is strip off the  offset=  part because  metrics_emit  only wants the numerical value.
Datadog,41252657,nan,3,"2016/12/21, 01:09:55",True,"2016/12/21, 01:31:54","2016/12/21, 01:13:29",7323221.0,19.0,1,3715,Best way to use split and strip,What would be the best way of stripping  offset=  as well as calling  strip()  on  i  so that it gets rid of all newlines and trailing/leading whitespaces?
Datadog,39624962,nan,0,"2016/09/21, 22:24:06",False,"2016/09/21, 22:24:06",nan,655900.0,1656.0,1,121,How can ASP.NET performance counters be monitored reliably in production?,We have a requeriment to monitor certain counters on our production applications.
Datadog,39624962,nan,0,"2016/09/21, 22:24:06",False,"2016/09/21, 22:24:06",nan,655900.0,1656.0,1,121,How can ASP.NET performance counters be monitored reliably in production?,Each application has it's own AppPool.
Datadog,39624962,nan,0,"2016/09/21, 22:24:06",False,"2016/09/21, 22:24:06",nan,655900.0,1656.0,1,121,How can ASP.NET performance counters be monitored reliably in production?,"We know how to attach and monitor the performance counters, but the problem we face i sthat IIS worker processes are recycled, and thus, the instance names for those performance counters, invariably change."
Datadog,39624962,nan,0,"2016/09/21, 22:24:06",False,"2016/09/21, 22:24:06",nan,655900.0,1656.0,1,121,How can ASP.NET performance counters be monitored reliably in production?,Our operations and sysadmins use external tools to monitor several counters and metrics.
Datadog,39624962,nan,0,"2016/09/21, 22:24:06",False,"2016/09/21, 22:24:06",nan,655900.0,1656.0,1,121,How can ASP.NET performance counters be monitored reliably in production?,"In order to include .NET CLR counters, we need to rely on the instanceId being stable, or else the monitoring needs to be setup again if the worker process dies or gets recycled."
Datadog,39624962,nan,0,"2016/09/21, 22:24:06",False,"2016/09/21, 22:24:06",nan,655900.0,1656.0,1,121,How can ASP.NET performance counters be monitored reliably in production?,Does anybody know a way around this limitation?
Datadog,39624962,nan,0,"2016/09/21, 22:24:06",False,"2016/09/21, 22:24:06",nan,655900.0,1656.0,1,121,How can ASP.NET performance counters be monitored reliably in production?,Maybe keeping the instanceIds stable (same PID!?)
Datadog,39624962,nan,0,"2016/09/21, 22:24:06",False,"2016/09/21, 22:24:06",nan,655900.0,1656.0,1,121,How can ASP.NET performance counters be monitored reliably in production?,"or by hooking these external tools (Datadog, WhatsUp, PerfMon, etc) to something that doesn't change?"
Datadog,39060624,39060683.0,2,"2016/08/21, 06:33:54",True,"2016/08/21, 10:42:11","2016/08/21, 06:43:09",1970079.0,2372.0,1,2485,Mockito - Test if a method of a class is called,"I am new to writing tests in java, and seem to be unable to test if a method of a class is called."
Datadog,39060624,39060683.0,2,"2016/08/21, 06:33:54",True,"2016/08/21, 10:42:11","2016/08/21, 06:43:09",1970079.0,2372.0,1,2485,Mockito - Test if a method of a class is called,"I am sending metrics to datadog, and want to test in the code if a function of another class was called."
Datadog,39060624,39060683.0,2,"2016/08/21, 06:33:54",True,"2016/08/21, 10:42:11","2016/08/21, 06:43:09",1970079.0,2372.0,1,2485,Mockito - Test if a method of a class is called,"It says I need to mock first, but I couldn't get it to work."
Datadog,39060624,39060683.0,2,"2016/08/21, 06:33:54",True,"2016/08/21, 10:42:11","2016/08/21, 06:43:09",1970079.0,2372.0,1,2485,Mockito - Test if a method of a class is called,MetricRecorder.java
Datadog,39060624,39060683.0,2,"2016/08/21, 06:33:54",True,"2016/08/21, 10:42:11","2016/08/21, 06:43:09",1970079.0,2372.0,1,2485,Mockito - Test if a method of a class is called,MetricRecorderTest.java
Datadog,39060624,39060683.0,2,"2016/08/21, 06:33:54",True,"2016/08/21, 10:42:11","2016/08/21, 06:43:09",1970079.0,2372.0,1,2485,Mockito - Test if a method of a class is called,"When I run the test I get this =  org.mockito.exceptions.misusing.NotAMockException: 
Argument passed to verify() is of type NonBlockingStatsDClient and is not a mock!"
Datadog,39060624,39060683.0,2,"2016/08/21, 06:33:54",True,"2016/08/21, 10:42:11","2016/08/21, 06:43:09",1970079.0,2372.0,1,2485,Mockito - Test if a method of a class is called,"Any idea of how I should be testing if recordHistogramValue was called, and if so with what arguments?"
Datadog,24536971,24537072.0,1,"2014/07/02, 20:02:24",True,"2014/07/07, 19:45:33",nan,3794458.0,29.0,0,199,Kamon with Datadog throwing java.lang.UnsupportedClassVersionError major/minor 51.0,I'm trying to send data to datadog using kamon.
Datadog,24536971,24537072.0,1,"2014/07/02, 20:02:24",True,"2014/07/07, 19:45:33",nan,3794458.0,29.0,0,199,Kamon with Datadog throwing java.lang.UnsupportedClassVersionError major/minor 51.0,My setup is the following:
Datadog,24536971,24537072.0,1,"2014/07/02, 20:02:24",True,"2014/07/07, 19:45:33",nan,3794458.0,29.0,0,199,Kamon with Datadog throwing java.lang.UnsupportedClassVersionError major/minor 51.0,I'm getting the following exception at akka startup:
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,"I am using Ansible  Datadog role  and trying to install and configure datadog agents in target servers however, i am stuck at a point where i need to use host variables and update a section of the playbook using these variables."
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,The variable has got multiple values separated by a space.
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,I want to ensure that these values are added in the playbook based on the variable values.
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,Following example will help in understanding the requirement.
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,Playbook:
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,"Here, the tag value AID is using a host variable with the same name i.e., AID and in some cases this host variable can have values like the following:"
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,AID: 100 101 102 103
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,Is there a way that the while executing tag section of the playbook is parsed based on the variable values in following format.
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,I believe i cannot use templates for such requirements since the configurations are used under vars in the role.
Datadog,67161917,nan,1,"2021/04/19, 15:21:25",False,"2021/04/20, 22:09:14",nan,1945171.0,255.0,0,21,How to update arguments of a role using host vars values in Ansible?,Any suggests would be appreciated.
Datadog,66761816,66871585.0,1,"2021/03/23, 12:59:04",True,"2021/03/30, 16:10:14","2021/03/30, 16:10:14",6301287.0,329.0,0,26,How can I add pg_monitor role to a postgresql user on heroku,I'm trying to set up a Datadog PostgreSQL integration that requires a user with  pg_monitor  role and  SELECT  permission on  pg_stat_database  as described on their own  documentation .
Datadog,66761816,66871585.0,1,"2021/03/23, 12:59:04",True,"2021/03/30, 16:10:14","2021/03/30, 16:10:14",6301287.0,329.0,0,26,How can I add pg_monitor role to a postgresql user on heroku,"My database is currently hosted on Heroku and it seems the default user doesn't have  SUPERUSER permissions because, when I try to apply the above role and permission to a &quot;monitor&quot; user I have the following error message:"
Datadog,66761816,66871585.0,1,"2021/03/23, 12:59:04",True,"2021/03/30, 16:10:14","2021/03/30, 16:10:14",6301287.0,329.0,0,26,How can I add pg_monitor role to a postgresql user on heroku,ERROR:  must have admin option on role &quot;pg_monitor&quot;
Datadog,66761816,66871585.0,1,"2021/03/23, 12:59:04",True,"2021/03/30, 16:10:14","2021/03/30, 16:10:14",6301287.0,329.0,0,26,How can I add pg_monitor role to a postgresql user on heroku,So I'm looking for some way of:
Datadog,66761816,66871585.0,1,"2021/03/23, 12:59:04",True,"2021/03/30, 16:10:14","2021/03/30, 16:10:14",6301287.0,329.0,0,26,How can I add pg_monitor role to a postgresql user on heroku,Someone has ever faced this issue?
Datadog,66761816,66871585.0,1,"2021/03/23, 12:59:04",True,"2021/03/30, 16:10:14","2021/03/30, 16:10:14",6301287.0,329.0,0,26,How can I add pg_monitor role to a postgresql user on heroku,There is a way to handle this case?
Datadog,66346003,nan,0,"2021/02/24, 08:53:14",False,"2021/02/25, 09:22:13","2021/02/25, 09:22:13",1117563.0,1291.0,0,28,Nodejs server getting latent when any of the backend dependent service gets latent,Our infra for web application looks like this
Datadog,66346003,nan,0,"2021/02/24, 08:53:14",False,"2021/02/25, 09:22:13","2021/02/25, 09:22:13",1117563.0,1291.0,0,28,Nodejs server getting latent when any of the backend dependent service gets latent,Nodejs Web application -&gt; GraphQL + Nodejs as middleware (BE for FE) -&gt; Lot's of BE services in ROR -&gt; DB/ES etc etc
Datadog,66346003,nan,0,"2021/02/24, 08:53:14",False,"2021/02/25, 09:22:13","2021/02/25, 09:22:13",1117563.0,1291.0,0,28,Nodejs server getting latent when any of the backend dependent service gets latent,We have witness the whole middleware layer of GrpahQL+Nodejs gets latent whenever any of the multiple crucial BE service gets latent and request queuing starts happening.
Datadog,66346003,nan,0,"2021/02/24, 08:53:14",False,"2021/02/25, 09:22:13","2021/02/25, 09:22:13",1117563.0,1291.0,0,28,Nodejs server getting latent when any of the backend dependent service gets latent,When we tried to compare it with number of requests during the period it got latent it was &lt;1k request which is much lower than the claimed 10k concurrent request handling of nodejs.
Datadog,66346003,nan,0,"2021/02/24, 08:53:14",False,"2021/02/25, 09:22:13","2021/02/25, 09:22:13",1117563.0,1291.0,0,28,Nodejs server getting latent when any of the backend dependent service gets latent,Looking for pointers to debug this issue further.
Datadog,66346003,nan,0,"2021/02/24, 08:53:14",False,"2021/02/25, 09:22:13","2021/02/25, 09:22:13",1117563.0,1291.0,0,28,Nodejs server getting latent when any of the backend dependent service gets latent,Analysis done so far from our end:
Datadog,65744466,nan,1,"2021/01/16, 00:40:33",True,"2021/01/16, 06:12:41",nan,13053313.0,33.0,0,19,is it possible to specify a cache with fragment caching or determine cache hit / miss ratio for a specific call,We have a Rails 4.2 app and are currently using a shared cache across several apps.
Datadog,65744466,nan,1,"2021/01/16, 00:40:33",True,"2021/01/16, 06:12:41",nan,13053313.0,33.0,0,19,is it possible to specify a cache with fragment caching or determine cache hit / miss ratio for a specific call,Our memcached miss rate is pretty high (like 85% hits and 15% misses) but this is complicated by the fact that multiple apps are sharing the same memcached instance.
Datadog,65744466,nan,1,"2021/01/16, 00:40:33",True,"2021/01/16, 06:12:41",nan,13053313.0,33.0,0,19,is it possible to specify a cache with fragment caching or determine cache hit / miss ratio for a specific call,So we might be getting a high miss rate for a couple of critical cache processes (our DataDog data would support this).
Datadog,65744466,nan,1,"2021/01/16, 00:40:33",True,"2021/01/16, 06:12:41",nan,13053313.0,33.0,0,19,is it possible to specify a cache with fragment caching or determine cache hit / miss ratio for a specific call,Is it possible to specify a cache store on a fragement cache call like:
Datadog,65744466,nan,1,"2021/01/16, 00:40:33",True,"2021/01/16, 06:12:41",nan,13053313.0,33.0,0,19,is it possible to specify a cache with fragment caching or determine cache hit / miss ratio for a specific call,I think this is possible with object caching by doing something like:
Datadog,65744466,nan,1,"2021/01/16, 00:40:33",True,"2021/01/16, 06:12:41",nan,13053313.0,33.0,0,19,is it possible to specify a cache with fragment caching or determine cache hit / miss ratio for a specific call,Would there be other ways to untangle the hit / miss ratio of specific cache actions?
Datadog,65672955,nan,0,"2021/01/11, 20:59:54",False,"2021/01/11, 20:59:54",nan,12022112.0,33.0,0,38,nestjs terminus - tons of failed calls to fs.statSync() for simple typeorm healthcheck,I set up a simple health check in nestjs using terminus.
Datadog,65672955,nan,0,"2021/01/11, 20:59:54",False,"2021/01/11, 20:59:54",nan,12022112.0,33.0,0,38,nestjs terminus - tons of failed calls to fs.statSync() for simple typeorm healthcheck,"In Datadog APM, I see dozens of failed fs.stat() calls for various paths under /dist"
Datadog,65672955,nan,0,"2021/01/11, 20:59:54",False,"2021/01/11, 20:59:54",nan,12022112.0,33.0,0,38,nestjs terminus - tons of failed calls to fs.statSync() for simple typeorm healthcheck,But the health check seems to complete fine after those calls.
Datadog,65672955,nan,0,"2021/01/11, 20:59:54",False,"2021/01/11, 20:59:54",nan,12022112.0,33.0,0,38,nestjs terminus - tons of failed calls to fs.statSync() for simple typeorm healthcheck,Any idea why this is happening?
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,"Let's say I have a daemonset running in k8s cluster, and the cluster has other deployments pods (e.g."
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,nginx webserver).
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,I would like the daemonset change/update config files in the deployment pods (e.g.
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,change nginx worker_processes or worker_connections in /etc/nginx/nginx.conf of the nginx pods from the daemonset).
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,This process is part of the automated or continuous performance tuning workflow that I would like to implement.
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,"In short, I'm looking for ways to have the daemonset change config files in other pods depending on performance metric continuously."
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,"I will have the daemonset continuously measure the performance or get performance metrics from another metric servers (Prometheus, new-relic, datadog, or others)."
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,"Depending on the performance measures, I want to take action (tune the pods) from the daemonset."
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,So the only problem I have from this workflow is how to access pods from the daemonset.
Datadog,65583942,nan,0,"2021/01/05, 19:44:49",False,"2021/01/05, 19:44:49",nan,12178460.0,858.0,0,15,Accessing kubernetes pods from daemonSet,I appreciate any feedback or solutions on how to achieve this.
Datadog,65009092,65026062.0,1,"2020/11/25, 18:43:43",True,"2020/11/26, 18:39:43","2020/11/25, 19:52:16",1253272.0,318.0,0,51,How to integrate C++ library when its package is not named,C++ and CMake newbie question regarding how to integrate a third-party library into my own code.
Datadog,65009092,65026062.0,1,"2020/11/25, 18:43:43",True,"2020/11/26, 18:39:43","2020/11/25, 19:52:16",1253272.0,318.0,0,51,How to integrate C++ library when its package is not named,I'm trying to add Datadog metrics to C++ application.
Datadog,65009092,65026062.0,1,"2020/11/25, 18:43:43",True,"2020/11/26, 18:39:43","2020/11/25, 19:52:16",1253272.0,318.0,0,51,How to integrate C++ library when its package is not named,The  officially-endorsed library  doesn't state how it can be integrated.
Datadog,65009092,65026062.0,1,"2020/11/25, 18:43:43",True,"2020/11/26, 18:39:43","2020/11/25, 19:52:16",1253272.0,318.0,0,51,How to integrate C++ library when its package is not named,I imagine it should tell me how to import it like this:
Datadog,65009092,65026062.0,1,"2020/11/25, 18:43:43",True,"2020/11/26, 18:39:43","2020/11/25, 19:52:16",1253272.0,318.0,0,51,How to integrate C++ library when its package is not named,This is my understanding of how to integrate a third-party library (don't you wish there is &quot;pip&quot; in C++?).
Datadog,65009092,65026062.0,1,"2020/11/25, 18:43:43",True,"2020/11/26, 18:39:43","2020/11/25, 19:52:16",1253272.0,318.0,0,51,How to integrate C++ library when its package is not named,But the names in &lt;&gt; are not provided in the README.
Datadog,65009092,65026062.0,1,"2020/11/25, 18:43:43",True,"2020/11/26, 18:39:43","2020/11/25, 19:52:16",1253272.0,318.0,0,51,How to integrate C++ library when its package is not named,I certainly don't have to do it like this as long as I can use CMake.
Datadog,65009092,65026062.0,1,"2020/11/25, 18:43:43",True,"2020/11/26, 18:39:43","2020/11/25, 19:52:16",1253272.0,318.0,0,51,How to integrate C++ library when its package is not named,Any help is appreciated!
Datadog,64884497,nan,0,"2020/11/18, 00:47:45",False,"2020/11/18, 00:47:45",nan,1623838.0,3406.0,0,14,How do I get `records-consumed-rate` in librdkadka,We have a consumer using librdkafka (via confluent-kafka-python) that we would like to monitor with DataDog.
Datadog,64884497,nan,0,"2020/11/18, 00:47:45",False,"2020/11/18, 00:47:45",nan,1623838.0,3406.0,0,14,How do I get `records-consumed-rate` in librdkadka,We are trying to get  records-consumed-rate  which is a recommended thing to watch.
Datadog,64884497,nan,0,"2020/11/18, 00:47:45",False,"2020/11/18, 00:47:45",nan,1623838.0,3406.0,0,14,How do I get `records-consumed-rate` in librdkadka,"The problem is, I can't seem to find it in the librdkafka stats object."
Datadog,64884497,nan,0,"2020/11/18, 00:47:45",False,"2020/11/18, 00:47:45",nan,1623838.0,3406.0,0,14,How do I get `records-consumed-rate` in librdkadka,Is there a way to get this metric?
Datadog,64884497,nan,0,"2020/11/18, 00:47:45",False,"2020/11/18, 00:47:45",nan,1623838.0,3406.0,0,14,How do I get `records-consumed-rate` in librdkadka,librdkafka stats docs
Datadog,64884497,nan,0,"2020/11/18, 00:47:45",False,"2020/11/18, 00:47:45",nan,1623838.0,3406.0,0,14,How do I get `records-consumed-rate` in librdkadka,Confluent docs showing that this indeed a thing
Datadog,64884497,nan,0,"2020/11/18, 00:47:45",False,"2020/11/18, 00:47:45",nan,1623838.0,3406.0,0,14,How do I get `records-consumed-rate` in librdkadka,Datadog recommendation to watch this
Datadog,64807110,nan,0,"2020/11/12, 17:45:28",False,"2020/11/12, 17:45:28",nan,3963160.0,83.0,0,13,Response loggin in NGINX Reverse Proxy,I have reverse proxy on running k8s.
Datadog,64807110,nan,0,"2020/11/12, 17:45:28",False,"2020/11/12, 17:45:28",nan,3963160.0,83.0,0,13,Response loggin in NGINX Reverse Proxy,I want to log request and response headers log.
Datadog,64807110,nan,0,"2020/11/12, 17:45:28",False,"2020/11/12, 17:45:28",nan,3963160.0,83.0,0,13,Response loggin in NGINX Reverse Proxy,I did not manage to it.
Datadog,64807110,nan,0,"2020/11/12, 17:45:28",False,"2020/11/12, 17:45:28",nan,3963160.0,83.0,0,13,Response loggin in NGINX Reverse Proxy,Could anyone help me ?
Datadog,64807110,nan,0,"2020/11/12, 17:45:28",False,"2020/11/12, 17:45:28",nan,3963160.0,83.0,0,13,Response loggin in NGINX Reverse Proxy,It could be console log.
Datadog,64807110,nan,0,"2020/11/12, 17:45:28",False,"2020/11/12, 17:45:28",nan,3963160.0,83.0,0,13,Response loggin in NGINX Reverse Proxy,I have Datadog agent on my Cluster.
Datadog,64807110,nan,0,"2020/11/12, 17:45:28",False,"2020/11/12, 17:45:28",nan,3963160.0,83.0,0,13,Response loggin in NGINX Reverse Proxy,So can can get request logs from NGINX.
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,I have recently been creating a POC using the new  DataDog/Azure DevOps Integration .
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,"The purpose of doing this is to aggregate all of my build/release logs, PR data, etc into DataDog to build insights, alerts, dashboards, etc."
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,"The DataDog charts are very nice, but I would prefer to use Azure Log Analytics as this is where most of my company's log and metric data is aggregated already and the ability to correlate it would be helpful."
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,"Note, I realize that Azure DevOps has Analytics charting and PowerBI integration, but I would like to use Log Analytics to store the metric and log data, if possible."
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,The Azure DevOps ServiceHooks do not have Azure Log Analytics as an option (see image below).
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,Maybe the trick is to push it to Azure Service Bus and then push it to Log Analytics?
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,I have looked at the  Azure DevOps Reporting documentation  and I didn't see anything obvious.
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,"If anyone knows of any good blogs on pushing data from Azure DevOps to Log Analytics, I'd really appreciate it."
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,Unfortunately most of my searches come back with advice on how to use Azure DevOps to provision monitoring of external applications with App Insights and Log Analytic rather than the other way around.
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,"I can imagine using a scheduled task calling the Azure DevOps API and pushing it into Log Analytics API, but that seems like the least elegant and most error prone solution."
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,Any thoughts on the best ways to monitor this data are appreciated.
Datadog,64519131,nan,0,"2020/10/25, 03:01:51",False,"2020/10/25, 03:01:51",nan,574588.0,388.0,0,72,Is there an efficient way to stream Azure DevOps data to Azure Log Analytics,Thanks!
Datadog,64332421,nan,0,"2020/10/13, 12:27:19",False,"2020/10/13, 12:27:19",nan,13208939.0,1.0,0,35,How to test DatadogMeterRegistry in spring boot application?,"I have a custom metrics which is pushed to the metrics endpoint, I use the metrics endpoint to push to datadog."
Datadog,64332421,nan,0,"2020/10/13, 12:27:19",False,"2020/10/13, 12:27:19",nan,13208939.0,1.0,0,35,How to test DatadogMeterRegistry in spring boot application?,I wanted to know how to test DatadogMeterRegistry in spring boot application.
Datadog,64332421,nan,0,"2020/10/13, 12:27:19",False,"2020/10/13, 12:27:19",nan,13208939.0,1.0,0,35,How to test DatadogMeterRegistry in spring boot application?,I found something in  GitHub  but it was dead-end
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,Scenario:
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,I have a remote server which is monitored (via DataDog) and sends out a warning when some anomaly is detected.
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,This warning can be fetched via a webhook.
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,"Now I want to connect that webhook ( https://docs.datadoghq.com/integrations/webhooks/ ) with MS Teams (probably via Bot), to receive a warning."
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,Then I want to send a command back to the remote server to resolve the warning.
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,Technology:
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,"MS Teams, Python flask/Django, remote server"
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,Expected Results:
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,I can receive a warning from my remote server to MS Teams via a bot.
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,Then send a command back to the remote server.
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,My initial plan is doing this using Python Flask/Django but not tied to a specific language.
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,Environment:
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,Remote server is a LINUX based system.
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,"we have a internal network that is used within our company, so might need to resolve a firewall problem potentially (idk whole lot about it tho)."
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,Things I have tried:
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,"I just want to see if this is possible or not, so i havent coded up any."
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,But I found some information relevant to our problem:
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,https://docs.datadoghq.com/integrations/webhooks/
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,https://docs.microsoft.com/en-us/microsoftteams/platform/bots/how-to/create-a-bot-for-teams
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/what-are-webhooks-and-connectors
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,"One last note, I am not also tied down to Teams bot."
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,"Our company also uses Azure Devops, so that is another resource I can use to realize the solution."
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,"Btw, this question was posted on  here  but was told to post on MSDN, but I couldn't find an appropriate forum."
Datadog,62819086,nan,1,"2020/07/09, 19:07:30",False,"2020/07/12, 20:27:18","2020/07/12, 00:44:44",10047933.0,1.0,0,88,Communicating with a remote server via microsoft Teams,"Hence, I am posting on stack overflow instead"
Datadog,62699424,62710050.0,1,"2020/07/02, 18:05:40",True,"2020/07/05, 19:24:13",nan,1782683.0,425.0,0,87,spring-cloud-skipper-server - logging in Json format,"We recently started using SCDF on Kubernetes, and we are trying to workout the kinks."
Datadog,62699424,62710050.0,1,"2020/07/02, 18:05:40",True,"2020/07/05, 19:24:13",nan,1782683.0,425.0,0,87,spring-cloud-skipper-server - logging in Json format,"One of thing things that i was'nt able to find is whether there is a way to affect logging format,for ex."
Datadog,62699424,62710050.0,1,"2020/07/02, 18:05:40",True,"2020/07/05, 19:24:13",nan,1782683.0,425.0,0,87,spring-cloud-skipper-server - logging in Json format,switch to using Json format.
Datadog,62699424,62710050.0,1,"2020/07/02, 18:05:40",True,"2020/07/05, 19:24:13",nan,1782683.0,425.0,0,87,spring-cloud-skipper-server - logging in Json format,"Reason for this is simple, we are using Datadog as our logging platform, and with Json, you don't have to write custom log parsing rules."
Datadog,62699424,62710050.0,1,"2020/07/02, 18:05:40",True,"2020/07/05, 19:24:13",nan,1782683.0,425.0,0,87,spring-cloud-skipper-server - logging in Json format,"With regular log format, you will endup with something like this"
Datadog,62448098,62448235.0,1,"2020/06/18, 13:41:35",True,"2020/06/18, 13:49:16",nan,3446208.0,855.0,0,93,"Does Docker have log statuses, e.g. error, warn, info?","For example in Node.js container I do:
 throw new Error('lol');  or  console.error('lol'); 
But when I open container logs:  docker-compose logs -f  nodejs 
there are no any statuses or colors like all logs have info status."
Datadog,62448098,62448235.0,1,"2020/06/18, 13:41:35",True,"2020/06/18, 13:49:16",nan,3446208.0,855.0,0,93,"Does Docker have log statuses, e.g. error, warn, info?",I use Datadog to collect logs from container - it also mark all logs as 'info'.
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,I'm trying to monitor my celery queues and which tasks they are running.
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,The main idea is to get a better understanding of how everything works.
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,I'm fairly new to celery outside of calling  delay  or setting up a  periodic_task .
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,"I'm running into a bit of a pickle, no pun indented and I'm using datadog to monitor some of this information."
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,What I setup is an  after_task_publish  function that I'd like to use to track the task and queue it was processed on.
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,I actually would like the worker as well.
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,I have extended my celery results so I can look things up in redis and see the queue.
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,Currently the tasks  _exec_options  queue is always  None .
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,Below is an example.
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,"Now this I don't understand because I set the queue, and also know the job ran so it had to be queued."
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,I'm trying to figure out if my configuration of celery is not doing what I anticipate or if I am going about trying to find the queue and worker that processed the job in the wrong way.
Datadog,62177405,nan,0,"2020/06/03, 19:04:58",False,"2020/06/03, 19:04:58",nan,7113209.0,1090.0,0,170,Celery Queues and after_task_publish,Any direction would be appreciated.
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,I am currently setting up my microservice application and am currently in the phase of error tracking and logging.
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,I am unsure on what the best practices are when it comes to this.
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,I have done some research on services that provide this.
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,"I feel there is a difference between application logging, like user actions and error tracking."
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,When it comes to error tracking I was planning on using a service like  Bugsnag  or  Sentry  which will allow me to follow a stack trace to pin down the error.
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,However when it comes to application logging I am debating whether to build that in house or also use a service like Datadog or Papertrail.
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,"However it seems like Datadog and Papertrail help with lower level logs like performance, etc."
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,Stripe is a great example of clear logging.
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,You can see the chain of events for each user in an easy to read format.
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,"To achieve a similar experience, I was thinking of creating a logging microservice which will be a consumer on a kafka broker and every microservice will produce a message when a certain function/user action is invoked and I can store in my db the user info and what action was taken."
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,Is this standard industry parctice?
Datadog,61824437,nan,0,"2020/05/15, 19:50:39",False,"2020/05/15, 22:20:38","2020/05/15, 22:20:38",5072383.0,1223.0,0,32,Best logging practices for user actions,Is there a service that can acheive this experience?
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,I'm running Postgres 11.
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,I have a table with 1.000.000 (1 million) rows and each row has a size of 40 bytes (it contains 5 columns).
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,That is equal to 40MB.
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,"When I execute (directly executed on the DB via DBeaver, DataGrid ect.- not called via Node, Python ect."
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,):
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,it takes 40 secs first time (is this not very slow even for the first time).
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,The CREATE statement of my tables:
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,"On 5 random tables: EXPLAIN (ANALYZE, BUFFERS) select * from [table_1...2,3,4,5]"
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,When I add a LIMIT 1.000.000 to table_5 (it contains 1.7 million rows)
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,When I add a WHERE clause between 2 dates (I'm monitored the query below with DataDog software and the results are here (max.~ 31K rows/sec when fetching):  https://www.screencast.com/t/yV0k4ShrUwSd ):
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,All tables has an unique index on the c3 column.
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,The size of the database is like 500GB in total.
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,The server has 16 cores and 112GB M2 memory.
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,"I have tried to optimize Postgres system variables - Like: WorkMem(1GB), shared_buffer(50GB), effective_cache_size (20GB) - But it doesn't seems to change anything (I know the settings has been applied - because I can see a big difference in the amount of idle memory the server has allocated)."
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,I know the database is too big for all data to be in memory.
Datadog,59973631,nan,2,"2020/01/29, 20:44:25",False,"2020/02/02, 13:13:31","2020/02/02, 13:13:31",5800182.0,1419.0,0,440,Postgres slow when selecting many rows,But is there anything I can do to boost the performance / speed of my query?
Datadog,59890153,nan,0,"2020/01/24, 05:49:39",False,"2020/01/24, 06:25:36","2020/01/24, 06:25:36",9576741.0,11.0,0,341,get more metrics with http.server.request metrics using mircrometer and statsd,How do I get all the metrics including status codes and exceptions using micrometer and statsd with flavor datadog.
Datadog,59890153,nan,0,"2020/01/24, 05:49:39",False,"2020/01/24, 06:25:36","2020/01/24, 06:25:36",9576741.0,11.0,0,341,get more metrics with http.server.request metrics using mircrometer and statsd,I am using maven dependency for micrometer-statsd and spring-boot actuator ?
Datadog,59890153,nan,0,"2020/01/24, 05:49:39",False,"2020/01/24, 06:25:36","2020/01/24, 06:25:36",9576741.0,11.0,0,341,get more metrics with http.server.request metrics using mircrometer and statsd,I have added @Timed annotation according to spring boot actuator configuration to a controller.
Datadog,59890153,nan,0,"2020/01/24, 05:49:39",False,"2020/01/24, 06:25:36","2020/01/24, 06:25:36",9576741.0,11.0,0,341,get more metrics with http.server.request metrics using mircrometer and statsd,But in the graphite I only see http.server.requests.max BUT no exceptions or status codes.
Datadog,59890153,nan,0,"2020/01/24, 05:49:39",False,"2020/01/24, 06:25:36","2020/01/24, 06:25:36",9576741.0,11.0,0,341,get more metrics with http.server.request metrics using mircrometer and statsd,Can someone point out what config am I missing ?
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,I would like to create a file to which I can write as described in the  Datadog Datagram docs :
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,Everything that is written to that file should be – instead of being handled by Datadog and sent to them via the agent – written to a log file.
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,After executing the three lines above the log file should contain the following:
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,I thought that a named pipe and a background process that handles that would be perfect.
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,"However, it does not work as expected and the background process never writes anything, even though writing seems to work."
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,I created the following script:
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,And the following systemd service:
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,"The service is started correctly after executing  systemctl enable --now datadog-agent , however, as I said, nothing is ever being written to the log file."
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,This is very strange to me because opening two shell instances where I write the following in the first shell:
Datadog,58625365,58858157.0,1,"2019/10/30, 14:27:02",True,"2019/11/14, 15:40:57","2019/11/14, 15:35:51",1251219.0,14336.0,0,63,Redirect named pipe input to file,And then start sending data in the second shell prints the lines correctly.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,I am looking for any tips or advice on how to troubleshoot our Elasticsearch cluster.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,This cluster has been running flawlessly with only minor maintenance for a couple years.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,We suddenly experienced an outage a week ago today.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"I have been struggling to keep it running ever since (it is actually our dev environment, so not production, but our devs are impacted, and we are worried the same thing can happen in production)."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,The symptom is this:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,We have three client nodes that coordinate requests with the data nodes.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"As soon as there is any kind of traffic, I see constant Garbage Collection in the logs."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Example:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"[2019-08-09T00:38:15,835][WARN ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1034] overhead, spent [694ms] collecting in the last [1s]"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"[2019-08-09T00:38:17,066][WARN ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1035] overhead, spent [693ms] collecting in the last [1.2s]"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"[2019-08-09T00:38:18,079][INFO ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1036] overhead, spent [352ms] collecting in the last [1s]"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,At some point the client loses communication with the master:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"[2019-08-06T14:38:54,403][INFO ][o.e.d.z.ZenDiscovery ] [client-vm0] master_left [{master-vm1}{PZJChTgxT46h4YYOqMr2fg}{1G3fXiSMQ5auVH-i5RH10w}{10.0.0.11}{10.0.0.11:9300}{ml.machine_memory=30064300032, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true}], reason [failed to ping, tried [3] times, each with maximum [30s] timeout]"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"[2019-08-06T14:38:54,419][WARN ][o.e.d.z.ZenDiscovery ] [client-vm0] master left (reason = failed to ping, tried [3] times, each with maximum [30s] timeout), current nodes: nodes:…"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"[2019-08-06T14:38:54,434][INFO ][o.e.x.w.WatcherService ] [client-vm0] stopping watch service, reason [no master node]"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"It tries to find another master, but is unable to:"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"[2019-08-06T14:41:56,528][WARN ][o.e.d.z.ZenDiscovery ] [client-vm0] not enough master nodes discovered during pinging (found [[]], but needed  2 ), pinging again"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"During this time, the masters and all the data nodes are perfectly find."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,I have usually seen the above when there is a large GC and the *  master  * loses contact with the *  client  * because it is too busy with GC to respond.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"But in this case, the client can’t find the master."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Eventually the client suffers an Out Of Memory failure and the JVM crashes.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"I am assuming that the memory issues, the GC and the crashing are all related, but I am having a problem figuring out what the cause is, and why so sudden."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Cluster details:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,ElasticSearch
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Version: 6.3.2
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,License: Open Source
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Nodes:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Client (3) (D13_v2): 8 CPU; 56 GB RAM; HDD Drives
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Master (3) (D4_v2): 8 CPU; 28 GB RAM; HDD Drives
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Data (35) (DS13): 8 CPU; 56 GB RAM; SSD OS &amp; 3x1TB SSD data drives
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Indexes:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Taxonomy:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Size: 1.45 GB
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Shards: 2
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Replicas: 1
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Support:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Size: 365 GB
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Shards: 30
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Replicas: 1
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,NonSupport:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Size: 2 TB
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Shards: 80
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Replicas: 1
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,JVM
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Java Version: (build 1.8.0_144-b01) Note: we stayed with this build as it was what had been running for most of the last year and we wanted to start from a good state.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,ES_HEAP_SIZE: 28672m (roughly half available memory)
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Note also that we have a current issue with field mapping explosion that has grown.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,This may be a culprit that we are investigating.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"If I shut off external access to the cluster, then everything is roses."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"When I open back up again, the clients go down in minutes."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"If I clear out all pending requests that I can see (we have a lot of queue based traffic), then it seems to be fine for some measure of time (~12 hours), but then it must reach some load where things fall over again."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,I have tried modifying the heap size to try to adjust GC time.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"I have reduced mappings in the taxonomy index, and that seemed to help."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,My current questions:
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"I also find that I am unable to inspect the hprof crash dumps as they are just too huge (~45GB), so I can’t get any info there on why the JVM might be crashing."
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"I set up DataDog when I started the investigation, but it has a lot of data I don’t know how to interpret"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,Any advice would be appreciated.
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,"Thanks,"
Datadog,57423102,nan,1,"2019/08/09, 06:24:39",False,"2019/08/12, 17:56:44",nan,5059173.0,149.0,0,201,Need advice on how to troubleshoot Elastic OOM JVM crashes,~john
Datadog,56993743,nan,1,"2019/07/11, 19:35:47",False,"2019/07/13, 00:03:23","2019/07/11, 21:07:30",1224827.0,8484.0,0,36,"Django2 is trying to render Jinja2 templates, even though it&#39;s a REST API",We're utilizing the  django-rest-framework  to create a RESTful API and using generic views or view sets to create the endpoint views.
Datadog,56993743,nan,1,"2019/07/11, 19:35:47",False,"2019/07/13, 00:03:23","2019/07/11, 21:07:30",1224827.0,8484.0,0,36,"Django2 is trying to render Jinja2 templates, even though it&#39;s a REST API","There is no templating happening, all the frontend is in React."
Datadog,56993743,nan,1,"2019/07/11, 19:35:47",False,"2019/07/13, 00:03:23","2019/07/11, 21:07:30",1224827.0,8484.0,0,36,"Django2 is trying to render Jinja2 templates, even though it&#39;s a REST API","However, upon watching the traces in Datadog, we're seeing that SOMETIMES (not every time), Jinja2 is rendering, causing a 500-800ms latency."
Datadog,56993743,nan,1,"2019/07/11, 19:35:47",False,"2019/07/13, 00:03:23","2019/07/11, 21:07:30",1224827.0,8484.0,0,36,"Django2 is trying to render Jinja2 templates, even though it&#39;s a REST API",Does anyone have any clues to why this might be happening and how to turn it off?
Datadog,56172624,56173299.0,1,"2019/05/16, 19:07:50",True,"2019/05/16, 19:55:12",nan,1608327.0,411.0,0,387,Can you perform equality matching on message template variables in data dog?,I'm setting up datadog monitors/alerts and want to have alerts routed to slack or pagerduty depending on if the issue is in our production environment or not.
Datadog,56172624,56173299.0,1,"2019/05/16, 19:07:50",True,"2019/05/16, 19:55:12",nan,1608327.0,411.0,0,387,Can you perform equality matching on message template variables in data dog?,"I've created multi-alert monitors that alert correctly, but I can't figure out how to make only ones where  environment.name  is equal to  prod  send an alert to pagerduty, and always send them to Slack."
Datadog,56172624,56173299.0,1,"2019/05/16, 19:07:50",True,"2019/05/16, 19:55:12",nan,1608327.0,411.0,0,387,Can you perform equality matching on message template variables in data dog?,I was hoping to be able to do something like the following in the alert message but haven't been able to figure out a syntax that works:
Datadog,56172624,56173299.0,1,"2019/05/16, 19:07:50",True,"2019/05/16, 19:55:12",nan,1608327.0,411.0,0,387,Can you perform equality matching on message template variables in data dog?,"For now, I've found a work around of having two monitors that are duplicates of each other where one has is scoped to production only and alerts pagerduty only and the second is for all environments and alerts slack only."
Datadog,56172624,56173299.0,1,"2019/05/16, 19:07:50",True,"2019/05/16, 19:55:12",nan,1608327.0,411.0,0,387,Can you perform equality matching on message template variables in data dog?,"However, I know this is going to become a maintenance nightmare as we grow and I'd like to know if there's a better solution."
Datadog,53732707,nan,1,"2018/12/11, 23:38:12",False,"2018/12/11, 23:49:57",nan,10054034.0,25.0,0,66,how to write a path using regexp,So i need to figure out how to write the following path into regexp
Datadog,53732707,nan,1,"2018/12/11, 23:38:12",False,"2018/12/11, 23:49:57",nan,10054034.0,25.0,0,66,how to write a path using regexp,/private/toolbox/*
Datadog,53732707,nan,1,"2018/12/11, 23:38:12",False,"2018/12/11, 23:49:57",nan,10054034.0,25.0,0,66,how to write a path using regexp,I'm not sure how to do it because of the *
Datadog,53732707,nan,1,"2018/12/11, 23:38:12",False,"2018/12/11, 23:49:57",nan,10054034.0,25.0,0,66,how to write a path using regexp,I have add the following 2 paths with no problem
Datadog,53732707,nan,1,"2018/12/11, 23:38:12",False,"2018/12/11, 23:49:57",nan,10054034.0,25.0,0,66,how to write a path using regexp,/private/healthcheck
Datadog,53732707,nan,1,"2018/12/11, 23:38:12",False,"2018/12/11, 23:49:57",nan,10054034.0,25.0,0,66,how to write a path using regexp,/private/datadog/dashboards
Datadog,53469205,54224944.0,2,"2018/11/25, 17:54:38",True,"2019/01/16, 22:34:23",nan,5885013.0,2723.0,0,110,statsd&#39;s side effects possibly causing extra latency,I'm using Datadog's  statsd client  to record the duration of a certain server response.
Datadog,53469205,54224944.0,2,"2018/11/25, 17:54:38",True,"2019/01/16, 22:34:23",nan,5885013.0,2723.0,0,110,statsd&#39;s side effects possibly causing extra latency,I used to pass in quite a few number of custom tags when  time -ing these responses.
Datadog,53469205,54224944.0,2,"2018/11/25, 17:54:38",True,"2019/01/16, 22:34:23",nan,5885013.0,2723.0,0,110,statsd&#39;s side effects possibly causing extra latency,So I'm in the process of reducing the number of custom tags.
Datadog,53469205,54224944.0,2,"2018/11/25, 17:54:38",True,"2019/01/16, 22:34:23",nan,5885013.0,2723.0,0,110,statsd&#39;s side effects possibly causing extra latency,"However, the problem is that when I reduce the number of tags passed in, there is extra latency of server response, which isn't intuitive because I'm passing in fewer tags and the implementation hasn't changed."
Datadog,53469205,54224944.0,2,"2018/11/25, 17:54:38",True,"2019/01/16, 22:34:23",nan,5885013.0,2723.0,0,110,statsd&#39;s side effects possibly causing extra latency,"According to Datadog and Etsy (which originally released  statsd ), these methods that record these metrics aren't blocking."
Datadog,53469205,54224944.0,2,"2018/11/25, 17:54:38",True,"2019/01/16, 22:34:23",nan,5885013.0,2723.0,0,110,statsd&#39;s side effects possibly causing extra latency,"However, they must be using some extra threads to perform this."
Datadog,53469205,54224944.0,2,"2018/11/25, 17:54:38",True,"2019/01/16, 22:34:23",nan,5885013.0,2723.0,0,110,statsd&#39;s side effects possibly causing extra latency,What could be the issue?
Datadog,53469205,54224944.0,2,"2018/11/25, 17:54:38",True,"2019/01/16, 22:34:23",nan,5885013.0,2723.0,0,110,statsd&#39;s side effects possibly causing extra latency,Are there possible any side effects associated with using this client?
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,"I have an application running under spring boot utilizing SMBJ to mount and read remote files, and it works perfectly."
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,However I am trying to set up some datadog reporting and trying to use JMX as a datasource for datadog...
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,TO do this I am staring the springboot jar with the following:
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,"And once I do this, SMBJ no longer creates the mount."
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,"If I remove these parameters the code works fine again and SMBJ is able to create/mount to the share, If I have them it simply times out trying to create the share."
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,"I thought maybe it was the RMI hostname change, but removing just this this doesn't seem to fix it."
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,Can anyone offer any help on this?
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,Is SMBJ really dependent on the jmxremote settings?
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,"It certainly seems to be..I have tried removing the overriding of the ports, so they go to their default ports as well, but this didn't fix it either."
Datadog,51933957,nan,0,"2018/08/20, 18:23:46",False,"2018/08/20, 18:23:46",nan,282172.0,3212.0,0,55,SMBJ no longer working when I modify JMX port settings,Any help would be appreciated.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,We need to separate logs generated by a REST web-service on a user-specific basis and eventually import these logs into an aggregation framework like Datadogs.com.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,There are several ways to approach this and I’m interested in getting feedback before selecting an approach.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,The basics would go something like this:
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,Depending on the stage
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,"For development, use the NLOG File Logger, so the developer can simply “tail” the log file."
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,"Use the variable in the nlog filename target as 
 &lt;target filename=""/path/file-${var:userid}.log""/&gt;."
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,"Or use the, it in the nlog target as:
 layout=""${var:userid}-${OtherLayout} "", and have developers do a  tail -f masterFile.log | grep USERID."
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,"In production switch to using the nlog JsonLayout, so a system like DataDog can read Time, Threadid, Userid and message data."
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,Use the JsonFormat and specify a userid attributes.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,I see that the JsonFormat has supports for MappedDiagnosticsLogicalContext but I would prefer the simplicity of the ${var:xxx} format to specify the value.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,Problems:
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,I’ve used the MappedDiagnosticContext in the past and then specified it in the target filename.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,I just attempted to use the ${var} approach and it did not appear to work?
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,?
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,Concerns:
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,I like the filename target approach for development.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,It should work well with 10-100 of users but not scale with 1000's of users.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,"Clearly with 1000’s of users we would need to close the log file after each line is written, as we don’t want to keep 1000’s of files open."
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,A major concern is threading.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,It’s possible that each webservice is called multiple times by different users and all approaches require Nlog MappedDiagnosticContext or ${var} capability’s to be thread safe.
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,Is it?
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,Any issues to consider?
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,"Eventually we would like to introduce some structured logging into the system, but the majority of the code base was built using standard logging techniques."
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,"If the objects being logged in the structured logging included userid then much of this complexity could be avoided, but that would require a lot of work rewriting for structured logging."
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,"Clearly, there is a lot to think about and I know I’m not the first to ponder this."
Datadog,51524214,51552249.0,1,"2018/07/25, 20:00:09",True,"2018/07/27, 10:22:48",nan,682877.0,541.0,0,1211,Best practices for using NLOG with a multi-user based system?,Your input will be appreciated.
Datadog,51521838,52038935.0,2,"2018/07/25, 17:48:19",True,"2018/08/27, 15:09:40",nan,10134087.0,1.0,0,781,show last last_autovacuum/anaylze etc based on the 20 biggest tables,I'm trying to monitor our postgresql DB and identify the 20 largest tables and than see when was the last vacuum and analyse took place.
Datadog,51521838,52038935.0,2,"2018/07/25, 17:48:19",True,"2018/08/27, 15:09:40",nan,10134087.0,1.0,0,781,show last last_autovacuum/anaylze etc based on the 20 biggest tables,I have this query that shows me the largest 20 schema name/relname which is good and that's what I was looking for:
Datadog,51521838,52038935.0,2,"2018/07/25, 17:48:19",True,"2018/08/27, 15:09:40",nan,10134087.0,1.0,0,781,show last last_autovacuum/anaylze etc based on the 20 biggest tables,I also have this query that shows me all the analysis I want to see with schema name and relname:
Datadog,51521838,52038935.0,2,"2018/07/25, 17:48:19",True,"2018/08/27, 15:09:40",nan,10134087.0,1.0,0,781,show last last_autovacuum/anaylze etc based on the 20 biggest tables,But I'm having a real hard time combining them together to one query that will show me when those analysis only for those 20 tables.
Datadog,51521838,52038935.0,2,"2018/07/25, 17:48:19",True,"2018/08/27, 15:09:40",nan,10134087.0,1.0,0,781,show last last_autovacuum/anaylze etc based on the 20 biggest tables,"Once this is done I'm looking to view the results in some sort of a graphic view in datadog, so If anyone have a good idea how to run this query as a datadog posgres query it will be amazing as well."
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,I have a metric client that looks something like:
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,Then in my application I import it and use it
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,This publishes the log  Updating metric metric1  in datadog and I can see it.
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,But this will only publish the first instance.
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,"Until I restart  service nginx restart , I will not get any more increments."
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,Update
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,So I have a  start.lua  in  /etc/nginx/conf.d/start.lua  that is:
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,And the nginx config is
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,"If I were to copy/paste the metric code into  start.lua , then the metric is updated every time."
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,Why is this?
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,!
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,Update
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,I noticed this in the error logs:
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,"This happens on the 2nd request to the nginx; the first time after restart, this is all fine ..."
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,Update 2
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,This happens only if I have a  metrics  file and  require  it in my other.
Datadog,50481980,nan,0,"2018/05/23, 10:10:06",False,"2018/05/23, 21:15:47","2018/05/23, 21:15:47",834045.0,21931.0,0,38,Lua resty_dogstatsd only publishes first instance,"So if I instantiate the  resty_dogstatsd  client inside the main lua file, then everything is fine ..."
Datadog,49280325,49291336.0,2,"2018/03/14, 16:28:51",True,"2018/03/15, 08:44:04","2018/03/15, 03:00:31",5057152.0,1017.0,0,187,How does traffic flow from Azure virtual machine to data dog,"I have installed data dog agent on one of my virtual machines When I have altered my NSG so that all ""Outbound-connections"" are denied, I am still able to see ""CPU metric"" getting updated on Data dog dashboard."
Datadog,49280325,49291336.0,2,"2018/03/14, 16:28:51",True,"2018/03/15, 08:44:04","2018/03/15, 03:00:31",5057152.0,1017.0,0,187,How does traffic flow from Azure virtual machine to data dog,I would like to know where this information is going from Azure to Datadog.
Datadog,48262289,nan,1,"2018/01/15, 13:41:32",False,"2018/01/15, 16:28:30",nan,229106.0,10688.0,0,88,How to print GC Details for hosted apps in Heroku -XX:+PrintGCDetails -Xloggc:gc.log,We have an application hosted on Heroku and logs are getting redirected to Sumologic.
Datadog,48262289,nan,1,"2018/01/15, 13:41:32",False,"2018/01/15, 16:28:30",nan,229106.0,10688.0,0,88,How to print GC Details for hosted apps in Heroku -XX:+PrintGCDetails -Xloggc:gc.log,I see some options can be passed to JVM where log files will be generated locally.
Datadog,48262289,nan,1,"2018/01/15, 13:41:32",False,"2018/01/15, 16:28:30",nan,229106.0,10688.0,0,88,How to print GC Details for hosted apps in Heroku -XX:+PrintGCDetails -Xloggc:gc.log,"Question: Is there a way we can redirect these logs to cloud log analyzers like Splunk, SumoLogic, or Datadog?"
Datadog,45554437,nan,1,"2017/08/07, 22:33:05",True,"2017/08/08, 20:32:37","2017/08/08, 20:32:37",6826691.0,1171.0,0,481,how to set up auto-discovery for a container?,"I have configured the docker-daemon,and also added modified the auto_conf."
Datadog,45554437,nan,1,"2017/08/07, 22:33:05",True,"2017/08/08, 20:32:37","2017/08/08, 20:32:37",6826691.0,1171.0,0,481,how to set up auto-discovery for a container?,How should i pass the  %%host%%  variable?
Datadog,45554437,nan,1,"2017/08/07, 22:33:05",True,"2017/08/08, 20:32:37","2017/08/08, 20:32:37",6826691.0,1171.0,0,481,how to set up auto-discovery for a container?,changed the etcd.yaml
Datadog,45554437,nan,1,"2017/08/07, 22:33:05",True,"2017/08/08, 20:32:37","2017/08/08, 20:32:37",6826691.0,1171.0,0,481,how to set up auto-discovery for a container?,but when i try to do
Datadog,45554437,nan,1,"2017/08/07, 22:33:05",True,"2017/08/08, 20:32:37","2017/08/08, 20:32:37",6826691.0,1171.0,0,481,how to set up auto-discovery for a container?,sudo docker exec -it dd-agent /etc/init.d/datadog-agent configcheck
Datadog,45554437,nan,1,"2017/08/07, 22:33:05",True,"2017/08/08, 20:32:37","2017/08/08, 20:32:37",6826691.0,1171.0,0,481,how to set up auto-discovery for a container?,the collector logs show
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,I'm sending metrics from a C# web service to datadog.
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,I need to track the length of words that are being searched in an api call and display this in a histogram.
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,But datadog is averaging these values which is not what I want.
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,"If one string is 1 character in length and another string is 10 characters in length it records a metric of 5.5, which isn't much use to me."
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,Ideally I would like a histogram graph over a time period e.g.
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,"an hour, showing the number of instances of 1, 2, 3 etc."
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,that were recorded during that time period.
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,Is that possible in datadog?
Datadog,43721365,nan,1,"2017/05/01, 18:09:57",True,"2017/05/01, 22:11:46",nan,1562497.0,591.0,0,553,Showing all data points in Datdog histogram,This is the call I'm making in the code:
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,I have a python program that crunches a large dataset using Pandas.
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,It currently takes about 15 minute to complete.
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,I want to log (stdout &amp; send the metric to Datadog) about the progress of the task.
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,Is there a way to get the %-complete of the task (or a function)?
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,"In the future, I might be dealing with larger datasets."
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,The Python task that I am doing is a simple grouping of a large pandas data frame.
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,Something like this:
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,"here, the categoryList has about 20000 items, and df is a large data frame having (say) a 5 million rows."
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,I am not looking for anything fancy (like progress-bars..).
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,Just percentage complete value.
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,Any ideas?
Datadog,39513085,nan,2,"2016/09/15, 17:01:07",False,"2016/09/15, 17:16:06","2016/09/15, 17:10:30",1717931.0,2143.0,0,426,percentage completion of a long-running python task,Thanks!
Datadog,43641450,nan,1,"2017/04/26, 21:02:50",True,"2017/04/27, 00:34:11",nan,461112.0,3391.0,0,131,Is it possible to detect bad instances using datadog outlier detection?,I would like to detect bad/faulty aws instances using datadog's outlier detection.
Datadog,43641450,nan,1,"2017/04/26, 21:02:50",True,"2017/04/27, 00:34:11",nan,461112.0,3391.0,0,131,Is it possible to detect bad instances using datadog outlier detection?,Is that possible?
Datadog,43641450,nan,1,"2017/04/26, 21:02:50",True,"2017/04/27, 00:34:11",nan,461112.0,3391.0,0,131,Is it possible to detect bad instances using datadog outlier detection?,I'm trying to create an automatic failover scenario using datadog.
Datadog,43641450,nan,1,"2017/04/26, 21:02:50",True,"2017/04/27, 00:34:11",nan,461112.0,3391.0,0,131,Is it possible to detect bad instances using datadog outlier detection?,Any suggestions would be appreciated.
Datadog,67201890,nan,0,"2021/04/21, 21:53:55",False,"2021/04/21, 21:53:55",nan,2965589.0,829.0,0,6,SphinxQL query_log conn value definition,"We're setting up monitoring for SphinxQL query_log in DataDog, and we'd like to understand what each value represents in the logging format."
Datadog,67201890,nan,0,"2021/04/21, 21:53:55",False,"2021/04/21, 21:53:55",nan,2965589.0,829.0,0,6,SphinxQL query_log conn value definition,"We understand all of the values except for  conn , which we're not seeing an explicit definition, but are making an educated guess that it might be a connection id."
Datadog,67201890,nan,0,"2021/04/21, 21:53:55",False,"2021/04/21, 21:53:55",nan,2965589.0,829.0,0,6,SphinxQL query_log conn value definition,We'd like to know for sure.
Datadog,67201890,nan,0,"2021/04/21, 21:53:55",False,"2021/04/21, 21:53:55",nan,2965589.0,829.0,0,6,SphinxQL query_log conn value definition,The standard log format for Sphinx:
Datadog,67201890,nan,0,"2021/04/21, 21:53:55",False,"2021/04/21, 21:53:55",nan,2965589.0,829.0,0,6,SphinxQL query_log conn value definition,The SphinxQL logging format:
Datadog,67201890,nan,0,"2021/04/21, 21:53:55",False,"2021/04/21, 21:53:55",nan,2965589.0,829.0,0,6,SphinxQL query_log conn value definition,"You can see the SphinxQL format adds a  conn  param after the query date, but the docs and referencing the standard log format do not make clear what  conn  is."
Datadog,67201890,nan,0,"2021/04/21, 21:53:55",False,"2021/04/21, 21:53:55",nan,2965589.0,829.0,0,6,SphinxQL query_log conn value definition,"We think it's a connection id, but I'm hoping someone with more expert knowledge of Sphinx can help clarify."
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"I have terraform config as below, which I am using to bring up ECS cluster in various prod/uat/dev environments."
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"In our infra, we configure the  logConfiguration  of container definition, based on targeted environment."
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"if infra being brought up by developer and  env:dev  , we configure the  logConfiguration  to use  awslogs  as logging driver and its subsequent log driver options ( i.e awslogs-region, awslogs-group, awslogs-stream-prefix )"
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"if infra being brought up by our continuous deployment pipeline with  env:prod or uat , we configure the  logConfiguration  to use  awsfirelens  logDriver and its subsequent options , we also add fluentbit container definition ( we use DataDog as logging / monitoring solutions, so we bring up the fluentbit container beside our service container to forward the logs to datadog endpoint )"
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"As of today, below configuration is working out just fine."
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,however it has the code duplication of my service container definition for each log configuration use-case as I am using ternary operator to decide which container definition to generate based on env.
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"This makes my code un-necessary long and error prone to work with ( If I add/substract environment variables for my service, I have to do that in both the use-case."
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"If I change my service config in the future, I have to keep maintaining that in both the use-case etc etc. )"
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"How can I improve above TF config, so that terraform will conditionally generates the container definition based on env: I passed in?"
Datadog,67099819,nan,0,"2021/04/15, 01:21:12",False,"2021/04/15, 01:21:12",nan,5298668.0,67.0,0,22,terraform conditionally creates log configuration and subsequent container definition,"If I pass env:dev it will log to awslogs logging driver and its options, if I pass env:uat/prod , it will log to awsfirelens log driver and also add fluentbit container definition in task definition."
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,During years I used to work with SUMO.
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,I used to do things like:
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,"And from there you would add time slices, to count on time basis."
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,"And then you could also integrate the results with some nice charts like time series, pie chart."
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,All within seconds.
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,"Now, I forced to use DataDog."
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,Imagine you have errors like:
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,I would like to count errors by place.
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,In this case:
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,Well I can't do it with Data Dog.
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,I hate it.
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,"I read about Pipeline, Processors, Metrics."
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,I can't believe this is so difficult compared with SUMO.
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,"I have a Pipeline, who parses my logs."
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,Then I can see all the fields being parsed.
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,But how can I create a metric from there?
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,"My fields are strings, so I can't &quot;measure&quot; them."
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,I just need to count how many &quot;house&quot; errors happened.
Datadog,66377430,nan,0,"2021/02/26, 00:28:04",False,"2021/04/20, 00:01:18","2021/04/20, 00:01:18",3149444.0,1417.0,0,14,Data Dog: parse logs and create dashboards like SUMO,And so on with some other strings.
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,"In Laravel, the general way to debug performance is to use  Laravel Debugbar"
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,What that doesn't include is things like:
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,"With all of this, it's difficult to debug where the performance is being slowed down."
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,Does anyone have any tools or details on how to debug FULLY end to end and each little area?
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,Visual tools preferred.
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,"Haven't looked as much into Xdebug, but I see tools like Datadog/Newrelic but haven't played with those much either."
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,Would be great to be able to debug it locally rather than in production environment.
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,"I want to be able to see that from all my middleware, view composers, included partials that there's one particular function that's causing 200ms due to a query or inefficient piece of code and I haven't found anything that can do that easily without adding breakpoints or start/stop timers everywhere but the app is too big for that."
Datadog,66202271,nan,1,"2021/02/15, 05:11:49",False,"2021/02/15, 07:52:17",nan,1534712.0,124.0,0,36,How to get detailed Performance Tracking in Laravel?,Any help appreciated!
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,"Background of the issue :
We are using Magnolia CMS with customized UI."
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,"As a first step of using Magnolia, we are migrating old content including documents to Magnolia."
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,Content migration is working fine.
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,Issue with DAM.
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,We have more than 50 GB of historical content to be stored in Magnolia.
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,Storing in DB became very expensive.
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,We decided to save them in File system.
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,Magnolia running in Kubernetes cluster as a service.
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,"Problem: After we migrate dam from old system to magnolia, we are trying to publish asset from author to public instance."
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,"After MAx 200 documents published, It is killing the pod."
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,"In logs, we are not seeing anything."
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,Last message is Catalish.sh killed...
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,Could you please suggest if anyone come across such scenarios.
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,Best practices to implement dam and other configuration.
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,"In Datadog, have seen that threadcounts are always more and GC is going crazy."
Datadog,66151045,nan,0,"2021/02/11, 10:19:42",False,"2021/02/11, 10:19:42",nan,11592443.0,5.0,0,37,Magnolia Publishing DAM content from author process killing the pod,Thank you in advance.
Datadog,65790261,65834510.0,1,"2021/01/19, 13:04:21",True,"2021/01/21, 21:32:18","2021/01/20, 12:19:42",13346540.0,3.0,0,48,In yaml how to declare multiple value to the single key in tags,For e.g.
Datadog,65790261,65834510.0,1,"2021/01/19, 13:04:21",True,"2021/01/21, 21:32:18","2021/01/20, 12:19:42",13346540.0,3.0,0,48,In yaml how to declare multiple value to the single key in tags,tags:
Datadog,65790261,65834510.0,1,"2021/01/19, 13:04:21",True,"2021/01/21, 21:32:18","2021/01/20, 12:19:42",13346540.0,3.0,0,48,In yaml how to declare multiple value to the single key in tags,When using above syntax not getting logs in Datadog for component2 only getting logs only for component1 so we want to parse logs for both the component i.e component1 and component2 so how would get it?
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,In my project haproxy is used.
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,But time to time I'm getting backend session limit exceed in haproxy when check with datadog graphs.
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,At the same time when check the frontend session count it also bit high than the normal time.
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,I need to know following things.
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,What is the connection between frontend sessions and user requests come to the frontend?
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,Single user getting one session for all the requests or multiple sessions?
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,For one frontend session backend going to create one session or multiple sessions?
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,If the frontend sessions are hight then can we say requests count to the haproxy is high?
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,"If possible can anyone share a tutorial to clearly understand connections between user requests, frontend sessions and backend sessions."
Datadog,65769361,nan,0,"2021/01/18, 07:41:34",False,"2021/01/18, 07:41:34",nan,9320040.0,153.0,0,47,HAProxy frontend sessions and backend session connection,Thank you.
Datadog,65248919,nan,0,"2020/12/11, 11:40:11",False,"2020/12/11, 11:40:11",nan,4354822.0,191.0,0,34,Is it possible to remap a key/value attribute directly as tags?,"I am using datadog with the agent, parsing logs in a file in json format."
Datadog,65248919,nan,0,"2020/12/11, 11:40:11",False,"2020/12/11, 11:40:11",nan,4354822.0,191.0,0,34,Is it possible to remap a key/value attribute directly as tags?,The way I found to tag json attributes is to edit a pipeline configuration and do something like
Datadog,65248919,nan,0,"2020/12/11, 11:40:11",False,"2020/12/11, 11:40:11",nan,4354822.0,191.0,0,34,Is it possible to remap a key/value attribute directly as tags?,"But I have to do it for each individual tag key, which implies to know the list of tags in advance and maintain it over time."
Datadog,65248919,nan,0,"2020/12/11, 11:40:11",False,"2020/12/11, 11:40:11",nan,4354822.0,191.0,0,34,Is it possible to remap a key/value attribute directly as tags?,It's not ideal.
Datadog,65248919,nan,0,"2020/12/11, 11:40:11",False,"2020/12/11, 11:40:11",nan,4354822.0,191.0,0,34,Is it possible to remap a key/value attribute directly as tags?,I'd like to know if there is a way to log things in this format
Datadog,65248919,nan,0,"2020/12/11, 11:40:11",False,"2020/12/11, 11:40:11",nan,4354822.0,191.0,0,34,Is it possible to remap a key/value attribute directly as tags?,"And have a pipeline configuration taking all key/value pairs in the &quot;tags&quot; value of the log, and tag each one of them."
Datadog,65248919,nan,0,"2020/12/11, 11:40:11",False,"2020/12/11, 11:40:11",nan,4354822.0,191.0,0,34,Is it possible to remap a key/value attribute directly as tags?,What I thought about was doing this
Datadog,65248919,nan,0,"2020/12/11, 11:40:11",False,"2020/12/11, 11:40:11",nan,4354822.0,191.0,0,34,Is it possible to remap a key/value attribute directly as tags?,But datadog's documentation is not clear about native attributes and their names so I don't know what would be the target attribute name.
Datadog,65151286,nan,1,"2020/12/04, 23:55:50",True,"2020/12/05, 06:52:20",nan,6095754.0,79.0,0,95,Long response time in anonymous middleware,I'm using NestJS (with Express Server) for a project and trying to optimize the performance on some of the endpoints.
Datadog,65151286,nan,1,"2020/12/04, 23:55:50",True,"2020/12/05, 06:52:20",nan,6095754.0,79.0,0,95,Long response time in anonymous middleware,Using Datadog I noticed that about 83% of the response time of all endpoints is spent in an anonymous middleware.
Datadog,65151286,nan,1,"2020/12/04, 23:55:50",True,"2020/12/05, 06:52:20",nan,6095754.0,79.0,0,95,Long response time in anonymous middleware,Does anyone know what middleware this is and why it's taking this long?
Datadog,65151286,nan,1,"2020/12/04, 23:55:50",True,"2020/12/05, 06:52:20",nan,6095754.0,79.0,0,95,Long response time in anonymous middleware,I suspect that it has to do with the framework itself due to the similar unanswered question  here .
Datadog,64821299,nan,1,"2020/11/13, 14:55:04",False,"2020/11/15, 10:08:00",nan,11951551.0,37.0,0,129,Monitoring pods on namespace without create a new namespace,My company has a cluster that's already been monitoring by a datadog agent.
Datadog,64821299,nan,1,"2020/11/13, 14:55:04",False,"2020/11/15, 10:08:00",nan,11951551.0,37.0,0,129,Monitoring pods on namespace without create a new namespace,But my team needs a monitoring just for us.
Datadog,64821299,nan,1,"2020/11/13, 14:55:04",False,"2020/11/15, 10:08:00",nan,11951551.0,37.0,0,129,Monitoring pods on namespace without create a new namespace,"I already looked on fluentd, prometheus and so on, but I cound't find an option for use a tool that I don't need to install in my namespace."
Datadog,64821299,nan,1,"2020/11/13, 14:55:04",False,"2020/11/15, 10:08:00",nan,11951551.0,37.0,0,129,Monitoring pods on namespace without create a new namespace,Does anyone know an option that I can collect the logs of my pods just in my namespace?
Datadog,64821299,nan,1,"2020/11/13, 14:55:04",False,"2020/11/15, 10:08:00",nan,11951551.0,37.0,0,129,Monitoring pods on namespace without create a new namespace,"Like, up a pod for grafana and another for collect logs and send to grafana or something like that?"
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,"I´am trying to setup serilog for for a project, but have some trouble understanding how it should be setup according to best practice for production."
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,"I have followed this  https://benfoster.io/blog/serilog-best-practices/  , but I have some questions regarding the section  Configuration ."
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,There he says;
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,"In .NET writing to Console is a blocking call and can have a
significant performance impact."
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,and he links to  https://weblog.west-wind.com/posts/2018/Dec/31/Dont-let-ASPNET-Core-Default-Console-Logging-Slow-your-App-down  .
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,Which again says that console is bad for perfomance and has this solution to the problem:
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,Which only adds console if in development.
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,My question is: If I´am not logging to console for production what should i do?
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,Should i write to file?
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,RollingFile?
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,What is the best perfomance vice and will it work on fargate?
Datadog,62553019,nan,0,"2020/06/24, 13:33:05",False,"2020/06/24, 13:33:05",nan,2144391.0,1606.0,0,62,What should I write logs to in production when using serilog?,My service is running on fargate with firelens sending logs to datadog.
Datadog,62362329,nan,0,"2020/06/13, 19:01:14",False,"2020/06/13, 19:01:14",nan,9471805.0,1.0,0,175,Spring Boot Embedded Tomcat Internal Working,I recently read an article regarding Tomcat architecture and a high level overview of its working and monitoring.
Datadog,62362329,nan,0,"2020/06/13, 19:01:14",False,"2020/06/13, 19:01:14",nan,9471805.0,1.0,0,175,Spring Boot Embedded Tomcat Internal Working,Key metrics for monitoring Tomcat - DataDog
Datadog,62362329,nan,0,"2020/06/13, 19:01:14",False,"2020/06/13, 19:01:14",nan,9471805.0,1.0,0,175,Spring Boot Embedded Tomcat Internal Working,"In this article, it mentions Tomcat having a pool of worker threads per connector that can be configured."
Datadog,62362329,nan,0,"2020/06/13, 19:01:14",False,"2020/06/13, 19:01:14",nan,9471805.0,1.0,0,175,Spring Boot Embedded Tomcat Internal Working,It also mentions about Executors and how it is mainly a thread pool that is shared with multiple connectors.
Datadog,62362329,nan,0,"2020/06/13, 19:01:14",False,"2020/06/13, 19:01:14",nan,9471805.0,1.0,0,175,Spring Boot Embedded Tomcat Internal Working,I have some doubts regarding Spring Boot and its Embedded Tomcat Server
Datadog,62362329,nan,0,"2020/06/13, 19:01:14",False,"2020/06/13, 19:01:14",nan,9471805.0,1.0,0,175,Spring Boot Embedded Tomcat Internal Working,I would be grateful if someone could shed some light on the above.
Datadog,62362329,nan,0,"2020/06/13, 19:01:14",False,"2020/06/13, 19:01:14",nan,9471805.0,1.0,0,175,Spring Boot Embedded Tomcat Internal Working,"In short, I just wanted to know if the server configuration via application.propeties is for an Executor or for the Connector specific pool of worker threads."
Datadog,61448126,nan,0,"2020/04/27, 00:09:14",False,"2020/04/27, 00:09:14",nan,3709060.0,4659.0,0,16,Understanding performance through network packets count,So here's the scenario..
Datadog,61448126,nan,0,"2020/04/27, 00:09:14",False,"2020/04/27, 00:09:14",nan,3709060.0,4659.0,0,16,Understanding performance through network packets count,"I'm seeing higher network packets per second on the older 2 webservers, and lower on the newer one."
Datadog,61448126,nan,0,"2020/04/27, 00:09:14",False,"2020/04/27, 00:09:14",nan,3709060.0,4659.0,0,16,Understanding performance through network packets count,"Datadog defines this metric as ""The number of packets of data received by the interface""."
Datadog,61448126,nan,0,"2020/04/27, 00:09:14",False,"2020/04/27, 00:09:14",nan,3709060.0,4659.0,0,16,Understanding performance through network packets count,The graph looks like this
Datadog,61448126,nan,0,"2020/04/27, 00:09:14",False,"2020/04/27, 00:09:14",nan,3709060.0,4659.0,0,16,Understanding performance through network packets count,To me this looks like there is higher throughput on the older servers.
Datadog,61448126,nan,0,"2020/04/27, 00:09:14",False,"2020/04/27, 00:09:14",nan,3709060.0,4659.0,0,16,Understanding performance through network packets count,"Looking at the nginx requests per 24 hour period on the 3 boxes, I see about  12.5% increase on the new box ."
Datadog,61448126,nan,0,"2020/04/27, 00:09:14",False,"2020/04/27, 00:09:14",nan,3709060.0,4659.0,0,16,Understanding performance through network packets count,This tells me that..
Datadog,61448126,nan,0,"2020/04/27, 00:09:14",False,"2020/04/27, 00:09:14",nan,3709060.0,4659.0,0,16,Understanding performance through network packets count,Can someone help me makes sense of this
Datadog,61360898,nan,0,"2020/04/22, 11:46:49",False,"2020/04/22, 11:46:49",nan,12591655.0,11.0,0,20,problem deployment ANSIBLE via Jenkins (aws),I'm trying to deploy a build Jenking ANSIBLE and i receive this error message.
Datadog,61360898,nan,0,"2020/04/22, 11:46:49",False,"2020/04/22, 11:46:49",nan,12591655.0,11.0,0,20,problem deployment ANSIBLE via Jenkins (aws),The target is to install Datadog Agent on Ubuntu ANSIBLE via playbook that i build.
Datadog,61360898,nan,0,"2020/04/22, 11:46:49",False,"2020/04/22, 11:46:49",nan,12591655.0,11.0,0,20,problem deployment ANSIBLE via Jenkins (aws),the message i receive is the following:
Datadog,61360898,nan,0,"2020/04/22, 11:46:49",False,"2020/04/22, 11:46:49",nan,12591655.0,11.0,0,20,problem deployment ANSIBLE via Jenkins (aws),10:39:16 [INFO] les credentials Conjur n'ont pas été récupérés : Could not find credentials entry with ID 'conjur_api_key'
Datadog,61360898,nan,0,"2020/04/22, 11:46:49",False,"2020/04/22, 11:46:49",nan,12591655.0,11.0,0,20,problem deployment ANSIBLE via Jenkins (aws),and :
Datadog,61360898,nan,0,"2020/04/22, 11:46:49",False,"2020/04/22, 11:46:49",nan,12591655.0,11.0,0,20,problem deployment ANSIBLE via Jenkins (aws),"&lt;10.145.99.222  (0, b'', b'')
10:40:00 fatal: [uamudaxd07]: FAILED!"
Datadog,61360898,nan,0,"2020/04/22, 11:46:49",False,"2020/04/22, 11:46:49",nan,12591655.0,11.0,0,20,problem deployment ANSIBLE via Jenkins (aws),"=  {
10:40:00     ""changed"": false,
10:40:00     ""invocation"": {
10:40:00         ""module_args"": {
10:40:00             ""_original_basename"": null,
10:40:00             ""attributes"": null,
10:40:00             ""backup"": false,
10:40:00             ""checksum"": null,
10:40:00             ""content"": null,
10:40:00             ""delimiter"": null,
10:40:00             ""dest"": ""/tmp/datadoginstall/datadog-agent-base_1.0_all.deb"",
10:40:00             ""directory_mode"": null,
10:40:00             ""follow"": false,
10:40:00             ""force"": true,
10:40:00             ""group"": null,
10:40:00             ""local_follow"": null,
10:40:00             ""mode"": null,
10:40:00             ""owner"": null,
10:40:00             ""regexp"": null,
10:40:00             ""remote_src"": true,
10:40:00             ""selevel"": null,
10:40:00             ""serole"": null,
10:40:00             ""setype"": null,
10:40:00             ""seuser"": null,
10:40:00             ""src"": ""/S3binaries/binaires_mdw_dax/DATADOG_AGENT/Linux/datadog-agent-base_1.0_all.deb"",
10:40:00             ""unsafe_writes"": null,
10:40:00             ""validate"": null
10:40:00         }
10:40:00     },
10:40:00     ""msg"": ""Source /S3binaries/binaires_mdw_dax/DATADOG_AGENT/Linux/datadog-agent-base_1.0_all.deb not found""
10:40:00 }
10:40:00 
10:40:00 PLAY RECAP *********************************************************************
10:40:00 uamudaxd07                 : ok=4    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0 
10:40:00"
Datadog,61360898,nan,0,"2020/04/22, 11:46:49",False,"2020/04/22, 11:46:49",nan,12591655.0,11.0,0,20,problem deployment ANSIBLE via Jenkins (aws),is there someone to help me :)?
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,We have a custom Golang script to publish messages to PubSub.
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,We then use the same client to publish to upto 40 topics.
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,and then based on a certain condition publish to 1 of the topic.
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,Our publisher loop looks like this
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,"We use 3000 Goroutines to publish messages to the topics and synchronously wait for messages to get acknowledged, that means there are at a time only 3000 in flight/waiting for acknowledgement at client."
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,Our current rate of publishing is close to 5K RPS but our latencies are as high as 30 seconds.
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,Below are the stats that I compiled from our Datadog dashboard.
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,When I wrote a small benchmark script to publish messages to a single topic the average latency was 147ms from the same machine.
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,I've tried to tweak the publisher settings for each topic but that did not help.
Datadog,60987316,nan,1,"2020/04/02, 11:33:31",False,"2020/04/02, 16:08:59",nan,1308570.0,4559.0,0,237,High publish latency with PubSub Golang client,Now I have couple of question.
Datadog,60443154,nan,0,"2020/02/28, 01:20:22",False,"2020/02/28, 11:46:04","2020/02/28, 11:46:04",1720048.0,9.0,0,44,Multiple filter and formatting issue,I'm trying to send logs from td-agent to Datadog using the below configuration.
Datadog,60443154,nan,0,"2020/02/28, 01:20:22",False,"2020/02/28, 11:46:04","2020/02/28, 11:46:04",1720048.0,9.0,0,44,Multiple filter and formatting issue,My expectation is filtering some keywords and formating that logs with using CSV format type.
Datadog,60443154,nan,0,"2020/02/28, 01:20:22",False,"2020/02/28, 11:46:04","2020/02/28, 11:46:04",1720048.0,9.0,0,44,Multiple filter and formatting issue,How can I do this?
Datadog,60443154,nan,0,"2020/02/28, 01:20:22",False,"2020/02/28, 11:46:04","2020/02/28, 11:46:04",1720048.0,9.0,0,44,Multiple filter and formatting issue,I tried to grep and format plugin in the filter section as below but it doesn't work as expected.
Datadog,60443154,nan,0,"2020/02/28, 01:20:22",False,"2020/02/28, 11:46:04","2020/02/28, 11:46:04",1720048.0,9.0,0,44,Multiple filter and formatting issue,The current and expected situation as below picture.
Datadog,60443154,nan,0,"2020/02/28, 01:20:22",False,"2020/02/28, 11:46:04","2020/02/28, 11:46:04",1720048.0,9.0,0,44,Multiple filter and formatting issue,How can I solve this situation?
Datadog,60443154,nan,0,"2020/02/28, 01:20:22",False,"2020/02/28, 11:46:04","2020/02/28, 11:46:04",1720048.0,9.0,0,44,Multiple filter and formatting issue,current
Datadog,60443154,nan,0,"2020/02/28, 01:20:22",False,"2020/02/28, 11:46:04","2020/02/28, 11:46:04",1720048.0,9.0,0,44,Multiple filter and formatting issue,expected
Datadog,59734858,nan,1,"2020/01/14, 15:35:20",True,"2020/01/14, 20:52:08","2020/01/14, 20:52:08",12391955.0,35.0,0,55,Regarding Tools installation via azure runbook on VM&#39;s,"I am trying to install datadog agent via runbook on multiple Azure Virtual Machine (VM), I have uploaded binaries on Blob from where I can download on my local computer (for testing, it is working fine), but when I am trying to connect to Azure Vm via  $Session = New-PSSession -ComputerName $vm  -Credential $cred , I am getting an error that Winrm server is having an issue."
Datadog,59734858,nan,1,"2020/01/14, 15:35:20",True,"2020/01/14, 20:52:08","2020/01/14, 20:52:08",12391955.0,35.0,0,55,Regarding Tools installation via azure runbook on VM&#39;s,Even winrm is running fine on that server.
Datadog,59734858,nan,1,"2020/01/14, 15:35:20",True,"2020/01/14, 20:52:08","2020/01/14, 20:52:08",12391955.0,35.0,0,55,Regarding Tools installation via azure runbook on VM&#39;s,Just I wanted to know is there any other way to download binaries on a remote VM and install on it via Powershell or runbook.
Datadog,59734858,nan,1,"2020/01/14, 15:35:20",True,"2020/01/14, 20:52:08","2020/01/14, 20:52:08",12391955.0,35.0,0,55,Regarding Tools installation via azure runbook on VM&#39;s,"If there is an option, please suggest it."
Datadog,59519717,nan,1,"2019/12/29, 15:10:18",False,"2020/03/10, 11:34:48",nan,339202.0,57.0,0,182,Kubernetes nginx ingress controller with opentracing does not show url resource path,we use kubenrnetes nginx ingress controller version 0.25.1 on aws eks (kubernetes version 1.13).
Datadog,59519717,nan,1,"2019/12/29, 15:10:18",False,"2020/03/10, 11:34:48",nan,339202.0,57.0,0,182,Kubernetes nginx ingress controller with opentracing does not show url resource path,we enable opentracing as per the documentation and use Datadog to view the traces.
Datadog,59519717,nan,1,"2019/12/29, 15:10:18",False,"2020/03/10, 11:34:48",nan,339202.0,57.0,0,182,Kubernetes nginx ingress controller with opentracing does not show url resource path,We have a general ingress rule to catch every path:
Datadog,59519717,nan,1,"2019/12/29, 15:10:18",False,"2020/03/10, 11:34:48",nan,339202.0,57.0,0,182,Kubernetes nginx ingress controller with opentracing does not show url resource path,"In the Datadog ui we see the nginx traces, however the ""resource"" column always shows ""/"" rather than the full path which is ""/test"" or ""/ping""."
Datadog,59519717,nan,1,"2019/12/29, 15:10:18",False,"2020/03/10, 11:34:48",nan,339202.0,57.0,0,182,Kubernetes nginx ingress controller with opentracing does not show url resource path,"If we create a separate ingress rule for each resource path, then we see the full path as expected (i.e."
Datadog,59519717,nan,1,"2019/12/29, 15:10:18",False,"2020/03/10, 11:34:48",nan,339202.0,57.0,0,182,Kubernetes nginx ingress controller with opentracing does not show url resource path,"""/test"" or ""//ping"") but it is very inconvenient and tedious to create a ingress rule for each path."
Datadog,59519717,nan,1,"2019/12/29, 15:10:18",False,"2020/03/10, 11:34:48",nan,339202.0,57.0,0,182,Kubernetes nginx ingress controller with opentracing does not show url resource path,is there any way we can see the full resource path in datadog UI without creating a separate ingress rule for each resource path?
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,I am in the process of migrating our Laravel application to EKS Kubernetes which is currently running on Docker however the response time is significantly slower.
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,"The current response time is roughly (Docker): 350-450 ms 
The new response time is roughly (Kubernetes): 750-1100 ms"
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,Notable Environment Differences:
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,APM Findings:
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,"I am running DataDog which shows that a lot of time is being spent on Laravel, rather than DB or Redis which doesn't give me much to work with."
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,"At this point, I am thinking it is infrastructure related rather than an issue with Laravel as the Docker environment already preforms (decently)."
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,I am running this as an init container (which occurs every deployment or pod restart):
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,I am unsure where to start troubleshooting.
Datadog,58662652,nan,0,"2019/11/01, 18:27:01",False,"2019/11/01, 18:27:01",nan,1292036.0,2031.0,0,290,Laravel Application Response Time Slow on Kubernetes (EKS) vs. Baremetal,Any advice will be helpful.
Datadog,57762582,57763275.0,1,"2019/09/02, 23:52:13",True,"2019/09/03, 01:43:07",nan,621056.0,1200.0,0,27,How can i get a list of queries and their execution count,"I want to get a list of queries executed against my mysql instance, I also want to get list of executions counts for them and duration,"
Datadog,57762582,57763275.0,1,"2019/09/02, 23:52:13",True,"2019/09/03, 01:43:07",nan,621056.0,1200.0,0,27,How can i get a list of queries and their execution count,"I can get these stats in something like datadog APM, but I would like to be able to run a query for them locally."
Datadog,57762582,57763275.0,1,"2019/09/02, 23:52:13",True,"2019/09/03, 01:43:07",nan,621056.0,1200.0,0,27,How can i get a list of queries and their execution count,is there a table or schema I need to look at?
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,I have a Flink job which reads from Kafka (v0.9) and writes to Redis.
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,I want to monitor the  records-consumed-rate  and  records-lag-max  metrics emitted by Kafka which Flink should be able to forward.
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,"In this case, I am forwarding to Datadog."
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,"When I start the job with a parallelism of 1, I see this metric emitted just fine."
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,"However, if I make the parallelism greater than 1, this metric is no longer forwarded."
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,The job is running when parallelism   1 because I can see the entries being written to Redis.
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,I'm running Flink (v1.6.2) on AWS EMR:
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,The parallelism is set by streamExecutionEnvironment.setParallelism().
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,Each Kafka Consumer is instantiated with the same group.id and a unique client.id.
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,The DD agent is running just fine on the cluster.
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,Many metrics are being emitted such as numberOfCompletedCheckpoints and upTime etc.
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,Is there any reason Flink would not be forwarding these metrics from Kafka if the parallelism is greater than 1?
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,"Update: 
I also tried sending a custom DD metric ( counter.inc() ) from the Redis RichSinkFunction."
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,"When the parallelism=1, the metric is sent fine."
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,"When parallelism=7, the metric is not sent however it is being called (added a debug line)."
Datadog,55737494,nan,1,"2019/04/18, 03:06:26",False,"2019/04/18, 22:06:04","2019/04/18, 06:58:55",2356037.0,175.0,0,216,Flink not forwarding Kafka metrics when parallelism greater than 1,So it seems its not limited to the forwarded metrics from Kafka.
Datadog,54735683,nan,1,"2019/02/17, 19:18:19",False,"2019/02/18, 18:30:16",nan,1060664.0,456.0,0,143,Track traffic endpoints with Nginx,I'm using Nginx and I want to keep track how many hits we get to what endpoint.
Datadog,54735683,nan,1,"2019/02/17, 19:18:19",False,"2019/02/18, 18:30:16",nan,1060664.0,456.0,0,143,Track traffic endpoints with Nginx,"We have few services in our website, how we can track the number of hits each one gets (not only number of connections but to what path of the platform)?"
Datadog,54735683,nan,1,"2019/02/17, 19:18:19",False,"2019/02/18, 18:30:16",nan,1060664.0,456.0,0,143,Track traffic endpoints with Nginx,That way for example we can see what each point of our API get's the most hits and to improve things there.
Datadog,54735683,nan,1,"2019/02/17, 19:18:19",False,"2019/02/18, 18:30:16",nan,1060664.0,456.0,0,143,Track traffic endpoints with Nginx,If there is a way to get this even further with origin of the request it will be great.
Datadog,54735683,nan,1,"2019/02/17, 19:18:19",False,"2019/02/18, 18:30:16",nan,1060664.0,456.0,0,143,Track traffic endpoints with Nginx,"I've installed Datadog agent but didn't installed anything related to NGINX, there is better tool for this task?"
Datadog,54735683,nan,1,"2019/02/17, 19:18:19",False,"2019/02/18, 18:30:16",nan,1060664.0,456.0,0,143,Track traffic endpoints with Nginx,Thanks!
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,I can't seem to configure my rails app properly.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,I had a perfectly working rails app deployed on Heroku.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,I added the datadog buildpack and then started getting the following build error:
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,"After removing my Gemfile.lock and reinstalling and making sure bunder version was   2.0, I then started getting another build error."
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,This time it was that the bundler version my project needs is &lt; 2:
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,I'm not sure how to move forward debugging this error.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,I don't know how to access the /tmp/build folder and I'm not sure what I would do if I did.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,Please any direction or suggestions would be greatly appreciated.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,I've already tried pushing to heroku master with various versions of bundler.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,There is always a problem with the build.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,"I feel like there are multiple areas where bundler is required in the build and in the actual project, and that they require different versions, but I don't know how to find them."
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,Here is my Gemfile:
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,and here is my Gemfile.lock
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,I'd like to be able to find where the areas are in my code that are requiring different versions of bundler.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,Or be able to circumvent the issue with a different gem.
Datadog,54719749,nan,0,"2019/02/16, 05:53:08",False,"2019/02/16, 08:59:14","2019/02/16, 08:59:14",10881188.0,63.0,0,277,Heroku buliid fails on Bundler version regardless of which is installed,If anyone can help me get the Heroku build to work I would be very grateful.
Datadog,53441210,53449811.0,1,"2018/11/23, 07:45:48",True,"2018/11/23, 18:04:49","2018/11/23, 07:47:40",6206313.0,390.0,0,1444,Stacked bar chart — top X values,I wonder if this is possible to achieve in Datadog.
Datadog,53441210,53449811.0,1,"2018/11/23, 07:45:48",True,"2018/11/23, 18:04:49","2018/11/23, 07:47:40",6206313.0,390.0,0,1444,Stacked bar chart — top X values,"I have a data collected under 1 metric  entity.count  - now the data are being posted to Datadog with multiple tags, for example  entity.count.visits ,  entity.count.payment  and probably another 10 different tags."
Datadog,53441210,53449811.0,1,"2018/11/23, 07:45:48",True,"2018/11/23, 18:04:49","2018/11/23, 07:47:40",6206313.0,390.0,0,1444,Stacked bar chart — top X values,"I'm trying to create Datadog chart in a dashboard, which would display top 5 tags of the entity counts in a stacked bar chart."
Datadog,53441210,53449811.0,1,"2018/11/23, 07:45:48",True,"2018/11/23, 18:04:49","2018/11/23, 07:47:40",6206313.0,390.0,0,1444,Stacked bar chart — top X values,"I know about the option of adding more queries, but since I'm not sure what entities will be available in the future, I would like datadog to always just display dynamically the top 5 entities in the dashboard (Insted of me specifying in the queries what tag to display)."
Datadog,53441210,53449811.0,1,"2018/11/23, 07:45:48",True,"2018/11/23, 18:04:49","2018/11/23, 07:47:40",6206313.0,390.0,0,1444,Stacked bar chart — top X values,"This is what I currently have (and it does the job, it's just not dynamic):"
Datadog,51304311,nan,0,"2018/07/12, 14:19:29",False,"2018/07/12, 15:03:51","2018/07/12, 15:03:51",9104186.0,1.0,0,1489,Static golang build of app using cgo library,I've decided to use zstd library for compression in my code using the  Go wrapper from datadog .
Datadog,51304311,nan,0,"2018/07/12, 14:19:29",False,"2018/07/12, 15:03:51","2018/07/12, 15:03:51",9104186.0,1.0,0,1489,Static golang build of app using cgo library,My build command for app is (I build my app in gitlab-ci using  image: golang:1.10.2-alpine )
Datadog,51304311,nan,0,"2018/07/12, 14:19:29",False,"2018/07/12, 15:03:51","2018/07/12, 15:03:51",9104186.0,1.0,0,1489,Static golang build of app using cgo library,Which fails with
Datadog,51304311,nan,0,"2018/07/12, 14:19:29",False,"2018/07/12, 15:03:51","2018/07/12, 15:03:51",9104186.0,1.0,0,1489,Static golang build of app using cgo library,When I try to enable  CGO_ENABLED  (and keep the rest of the build commands unchanged) I end up with warnings:
Datadog,51304311,nan,0,"2018/07/12, 14:19:29",False,"2018/07/12, 15:03:51","2018/07/12, 15:03:51",9104186.0,1.0,0,1489,Static golang build of app using cgo library,Which doesn't seem ok as I want to have my build statically linked.
Datadog,51304311,nan,0,"2018/07/12, 14:19:29",False,"2018/07/12, 15:03:51","2018/07/12, 15:03:51",9104186.0,1.0,0,1489,Static golang build of app using cgo library,Do I have to firstly build zstd library and then build my app?
Datadog,50468823,nan,1,"2018/05/22, 16:21:44",False,"2018/10/24, 17:06:01",nan,1401826.0,1906.0,0,408,How to configure Kamon for dockerized Akka application?,I created a sample service in Akka for testing Kamon + DataDog monitoring.
Datadog,50468823,nan,1,"2018/05/22, 16:21:44",False,"2018/10/24, 17:06:01",nan,1401826.0,1906.0,0,408,How to configure Kamon for dockerized Akka application?,Here are dependencies which I added:
Datadog,50468823,nan,1,"2018/05/22, 16:21:44",False,"2018/10/24, 17:06:01",nan,1401826.0,1906.0,0,408,How to configure Kamon for dockerized Akka application?,Here are plugins enabled in  build.sbt :
Datadog,50468823,nan,1,"2018/05/22, 16:21:44",False,"2018/10/24, 17:06:01",nan,1401826.0,1906.0,0,408,How to configure Kamon for dockerized Akka application?,Then  application.conf :
Datadog,50468823,nan,1,"2018/05/22, 16:21:44",False,"2018/10/24, 17:06:01",nan,1401826.0,1906.0,0,408,How to configure Kamon for dockerized Akka application?,Finally in the  Main  class I invoke:
Datadog,50468823,nan,1,"2018/05/22, 16:21:44",False,"2018/10/24, 17:06:01",nan,1401826.0,1906.0,0,408,How to configure Kamon for dockerized Akka application?,On EC2 I installed datadog-agent for docker.
Datadog,50468823,nan,1,"2018/05/22, 16:21:44",False,"2018/10/24, 17:06:01",nan,1401826.0,1906.0,0,408,How to configure Kamon for dockerized Akka application?,"When I run the service container on the EC2 instance and then look into the DataDog interface I don't see any related metrics to akka, just a list of standard metrics like:  datadog.process.agent ,  docker.cpu.usage ,  system.io.await  etc"
Datadog,50468823,nan,1,"2018/05/22, 16:21:44",False,"2018/10/24, 17:06:01",nan,1401826.0,1906.0,0,408,How to configure Kamon for dockerized Akka application?,How to enable akka related metrics in case when an akka app is packaged into docker and deployed on EC2?
Datadog,49607708,nan,1,"2018/04/02, 11:16:59",True,"2018/04/04, 13:02:40","2018/04/04, 13:02:40",1835403.0,465.0,0,206,debugging imbalanced kafka message_in rate,I've a 4 node kafka cluster in my production where we are using custom partitioner which does mod 64 of an id to determine the partition.
Datadog,49607708,nan,1,"2018/04/02, 11:16:59",True,"2018/04/04, 13:02:40","2018/04/04, 13:02:40",1835403.0,465.0,0,206,debugging imbalanced kafka message_in rate,"since last week, there has been imbalanced kafka messages_in rate on 1 of our nodes as can been seen in the graph attached ."
Datadog,49607708,nan,1,"2018/04/02, 11:16:59",True,"2018/04/04, 13:02:40","2018/04/04, 13:02:40",1835403.0,465.0,0,206,debugging imbalanced kafka message_in rate,The pink line shows the message in rate on kafka01 node and bluish yellow line shows the message in rate on all other 3 boxes .
Datadog,49607708,nan,1,"2018/04/02, 11:16:59",True,"2018/04/04, 13:02:40","2018/04/04, 13:02:40",1835403.0,465.0,0,206,debugging imbalanced kafka message_in rate,I'm using datadog for monitoring and using the metric kafka.messages_in.rate .
Datadog,49607708,nan,1,"2018/04/02, 11:16:59",True,"2018/04/04, 13:02:40","2018/04/04, 13:02:40",1835403.0,465.0,0,206,debugging imbalanced kafka message_in rate,"Assuming that there has been no change in the id distribution , there should have been no change in distribution of message in rate ."
Datadog,49607708,nan,1,"2018/04/02, 11:16:59",True,"2018/04/04, 13:02:40","2018/04/04, 13:02:40",1835403.0,465.0,0,206,debugging imbalanced kafka message_in rate,Steps I've taken to debug the issue are
Datadog,49607708,nan,1,"2018/04/02, 11:16:59",True,"2018/04/04, 13:02:40","2018/04/04, 13:02:40",1835403.0,465.0,0,206,debugging imbalanced kafka message_in rate,Requesting any help or areas/metrics one can look into to debug this anomaly.
Datadog,49607708,nan,1,"2018/04/02, 11:16:59",True,"2018/04/04, 13:02:40","2018/04/04, 13:02:40",1835403.0,465.0,0,206,debugging imbalanced kafka message_in rate,"For people who are searching about this in future
 https://mail-archives.apache.org/mod_mbox/kafka-users/201710.mbox/%3CCALaekbwkSKapqPwsyuAoHGiSnc1+3jF2wF+2FDZbAVx61E+c2w@mail.gmail.com%3E"
Datadog,49274395,49277969.0,1,"2018/03/14, 11:47:25",True,"2018/04/29, 00:26:33","2018/03/15, 08:01:12",4042016.0,2790.0,0,77,MSSQL Server performance metrics in asp.net mvc,"I am working on a project where i need to display the database mssql server's performance metrics for example memory consumed/free, storage free space etc."
Datadog,49274395,49277969.0,1,"2018/03/14, 11:47:25",True,"2018/04/29, 00:26:33","2018/03/15, 08:01:12",4042016.0,2790.0,0,77,MSSQL Server performance metrics in asp.net mvc,I have researched for this purpose and one thing came up was  DOGSTATSD .
Datadog,49274395,49277969.0,1,"2018/03/14, 11:47:25",True,"2018/04/29, 00:26:33","2018/03/15, 08:01:12",4042016.0,2790.0,0,77,MSSQL Server performance metrics in asp.net mvc,Datadog  provides the library for .net project to get custom metrics but that was not the solution for me because the metrics works on datadog website.
Datadog,49274395,49277969.0,1,"2018/03/14, 11:47:25",True,"2018/04/29, 00:26:33","2018/03/15, 08:01:12",4042016.0,2790.0,0,77,MSSQL Server performance metrics in asp.net mvc,"I have to display the all (in graph or whatever suited) data, received from MSSQL SERVER."
Datadog,49274395,49277969.0,1,"2018/03/14, 11:47:25",True,"2018/04/29, 00:26:33","2018/03/15, 08:01:12",4042016.0,2790.0,0,77,MSSQL Server performance metrics in asp.net mvc,There will be multiple servers/instances.
Datadog,49274395,49277969.0,1,"2018/03/14, 11:47:25",True,"2018/04/29, 00:26:33","2018/03/15, 08:01:12",4042016.0,2790.0,0,77,MSSQL Server performance metrics in asp.net mvc,"Is there a way to do that, our WebApp connected with multiple databases and we receive/display information."
Datadog,49274395,49277969.0,1,"2018/03/14, 11:47:25",True,"2018/04/29, 00:26:33","2018/03/15, 08:01:12",4042016.0,2790.0,0,77,MSSQL Server performance metrics in asp.net mvc,I cannot use already available tools for the insights.
Datadog,49064218,nan,0,"2018/03/02, 09:03:30",False,"2018/03/02, 09:03:30",nan,476917.0,3016.0,0,136,Error CrashLoopBackOff Heapster with Statsd Sink Configuration,I got error with the latest heapster version:  v1.5.1 .
Datadog,49064218,nan,0,"2018/03/02, 09:03:30",False,"2018/03/02, 09:03:30",nan,476917.0,3016.0,0,136,Error CrashLoopBackOff Heapster with Statsd Sink Configuration,I've described in detail in this github issue link:  https://github.com/kubernetes/heapster/issues/1969
Datadog,49064218,nan,0,"2018/03/02, 09:03:30",False,"2018/03/02, 09:03:30",nan,476917.0,3016.0,0,136,Error CrashLoopBackOff Heapster with Statsd Sink Configuration,The error message:
Datadog,49064218,nan,0,"2018/03/02, 09:03:30",False,"2018/03/02, 09:03:30",nan,476917.0,3016.0,0,136,Error CrashLoopBackOff Heapster with Statsd Sink Configuration,Anybody knows how to solve it?
Datadog,49064218,nan,0,"2018/03/02, 09:03:30",False,"2018/03/02, 09:03:30",nan,476917.0,3016.0,0,136,Error CrashLoopBackOff Heapster with Statsd Sink Configuration,Perhaps someone who already successfully integrated the heapster to datadog statsd agent in Kubernetes?
Datadog,49064218,nan,0,"2018/03/02, 09:03:30",False,"2018/03/02, 09:03:30",nan,476917.0,3016.0,0,136,Error CrashLoopBackOff Heapster with Statsd Sink Configuration,Thanks before
Datadog,48181180,nan,0,"2018/01/10, 08:00:11",False,"2018/01/10, 08:25:37","2018/01/10, 08:25:37",6508330.0,3.0,0,58,send graph for specific period of time from aws cloudwatch,Is there any way to send matrics graph over email for a specific period of time from AWS Cloudwatch or by using datadog.
Datadog,48181180,nan,0,"2018/01/10, 08:00:11",False,"2018/01/10, 08:25:37","2018/01/10, 08:25:37",6508330.0,3.0,0,58,send graph for specific period of time from aws cloudwatch,I want to send ec2 system check matrics for a specific time period.
Datadog,48181180,nan,0,"2018/01/10, 08:00:11",False,"2018/01/10, 08:25:37","2018/01/10, 08:25:37",6508330.0,3.0,0,58,send graph for specific period of time from aws cloudwatch,Thanks in advance.
Datadog,48147932,nan,0,"2018/01/08, 12:01:34",False,"2018/01/08, 12:01:34",nan,234475.0,107.0,0,21,How to know if primary &amp; replica shard in elasticsearch land up on same data node ?,Is there a way to alert / detect in elasticsearch when the primary shard and replica shard land up on the same data node ?
Datadog,48147932,nan,0,"2018/01/08, 12:01:34",False,"2018/01/08, 12:01:34",nan,234475.0,107.0,0,21,How to know if primary &amp; replica shard in elasticsearch land up on same data node ?,May be via datadog or any other way ?
Datadog,48147932,nan,0,"2018/01/08, 12:01:34",False,"2018/01/08, 12:01:34",nan,234475.0,107.0,0,21,How to know if primary &amp; replica shard in elasticsearch land up on same data node ?,Looking for an automatic way not manual way like monitoring through head plugin etc.
Datadog,47701528,nan,0,"2017/12/07, 20:30:01",False,"2017/12/07, 21:28:59","2017/12/07, 21:28:59",1469496.0,1126.0,0,968,Set kernel parameter for cgroup_enable=memory,I am on a RHEL system and I would like to add the following parameters so that I can have my datadog + docker integration as described here ( https://github.com/DataDog/docker-dd-agent#cgroups ).
Datadog,47701528,nan,0,"2017/12/07, 20:30:01",False,"2017/12/07, 21:28:59","2017/12/07, 21:28:59",1469496.0,1126.0,0,968,Set kernel parameter for cgroup_enable=memory,I need to set the following kernel parameters:
Datadog,47701528,nan,0,"2017/12/07, 20:30:01",False,"2017/12/07, 21:28:59","2017/12/07, 21:28:59",1469496.0,1126.0,0,968,Set kernel parameter for cgroup_enable=memory,cgroup_enable=memory swapaccount=1
Datadog,47701528,nan,0,"2017/12/07, 20:30:01",False,"2017/12/07, 21:28:59","2017/12/07, 21:28:59",1469496.0,1126.0,0,968,Set kernel parameter for cgroup_enable=memory,I was planning to use something like:
Datadog,47701528,nan,0,"2017/12/07, 20:30:01",False,"2017/12/07, 21:28:59","2017/12/07, 21:28:59",1469496.0,1126.0,0,968,Set kernel parameter for cgroup_enable=memory,cgset -r cgroup_memory=enable
Datadog,47701528,nan,0,"2017/12/07, 20:30:01",False,"2017/12/07, 21:28:59","2017/12/07, 21:28:59",1469496.0,1126.0,0,968,Set kernel parameter for cgroup_enable=memory,but I get the error  wrong -r  parameter (cgroup_enable=memory)
Datadog,46867883,nan,2,"2017/10/21, 23:43:36",True,"2017/10/26, 11:49:53","2017/10/21, 23:56:03",2492242.0,1314.0,0,177,planning for graphite components for big cassandra cluster monitoring,I am planning to setup a 80 nodes cassandra cluster (current version 2.1 but will upgrade to 3 in future).
Datadog,46867883,nan,2,"2017/10/21, 23:43:36",True,"2017/10/26, 11:49:53","2017/10/21, 23:56:03",2492242.0,1314.0,0,177,planning for graphite components for big cassandra cluster monitoring,I have gone though  http://graphite.readthedocs.io/en/latest/tools.html  which has list of tools that graphite supports.
Datadog,46867883,nan,2,"2017/10/21, 23:43:36",True,"2017/10/26, 11:49:53","2017/10/21, 23:56:03",2492242.0,1314.0,0,177,planning for graphite components for big cassandra cluster monitoring,I want to decide which tools to choose as listener and storage so that it could scale.
Datadog,46867883,nan,2,"2017/10/21, 23:43:36",True,"2017/10/26, 11:49:53","2017/10/21, 23:56:03",2492242.0,1314.0,0,177,planning for graphite components for big cassandra cluster monitoring,As a listener should i use the default carbon or should i choose graphite-ng ?
Datadog,46867883,nan,2,"2017/10/21, 23:43:36",True,"2017/10/26, 11:49:53","2017/10/21, 23:56:03",2492242.0,1314.0,0,177,planning for graphite components for big cassandra cluster monitoring,"However as storage component, i am confused that whether default whisper is enough?"
Datadog,46867883,nan,2,"2017/10/21, 23:43:36",True,"2017/10/26, 11:49:53","2017/10/21, 23:56:03",2492242.0,1314.0,0,177,planning for graphite components for big cassandra cluster monitoring,"Or should I look at ohter option (like Influxdata,cynite or some rdms db (postgres/mysql))?"
Datadog,46867883,nan,2,"2017/10/21, 23:43:36",True,"2017/10/26, 11:49:53","2017/10/21, 23:56:03",2492242.0,1314.0,0,177,planning for graphite components for big cassandra cluster monitoring,As gui component i have finalized to use grafana for better visulization.
Datadog,46867883,nan,2,"2017/10/21, 23:43:36",True,"2017/10/26, 11:49:53","2017/10/21, 23:56:03",2492242.0,1314.0,0,177,planning for graphite components for big cassandra cluster monitoring,I think datadog + grafana will work fine but datadog is not opensource.So Please suggest an opensource scalable up to 100 cassandra nodes alternative.
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,It seems like we ran into a OutOfMemoryError: Metaspace before actually running out of available memory for that pool.
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,"More specifically, we appeared to hit that error as soon as the  committed  amount for that pool reached the maximum, instead of when the  used  amount did."
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,Here's the setup:
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,"We have a Jenkins server running on Oracle Java 8 update 121, and have the following metaspace arguments  -XX:MetaspaceSize=10G -XX:MaxMetaspaceSize=10G ."
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,We also have Datadog monitoring heap and non-heap pools.
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,We hit an issue where the Jenkins log indicated that some thread threw an OutOfMemoryError: Metaspace.
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,"However, in Datadog at the time of incident, the amount of non-heap used is shown to be very low (graph below)."
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,"At first I thought Datadog might be measuring it wrong, but using jconsole I get matching results for current usage (I didn't have jconsole open at the time of incident)."
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,"My only other conclusion was that the error originated from trying to allocate more  committed  metaspace, even though there's still plenty of gap between that and the used amount."
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,Am I missing something about how these memory pools are supposed to work?
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,"Note:  I'm perfectly aware that this is a pretty large metaspace to have to begin with, and that we likely have a classloader leak somewhere."
Datadog,42938522,nan,0,"2017/03/21, 23:34:58",False,"2017/03/21, 23:34:58","2020/06/20, 12:12:55",5596700.0,93.0,0,599,OutOfMemoryError: Metaspace without memory pool being maxed out,This is something we hit while trying to investigate that leak.
Datadog,42480375,nan,0,"2017/02/27, 09:52:21",False,"2017/02/27, 09:52:21",nan,7628458.0,1.0,0,55,System load peaks on kubernetes master node on azure,Does anyone know why we are experiencing on our kubernetes master node some system load peaks.
Datadog,42480375,nan,0,"2017/02/27, 09:52:21",False,"2017/02/27, 09:52:21",nan,7628458.0,1.0,0,55,System load peaks on kubernetes master node on azure,I thought that the master node is not doing anything except monitoring our agent nodes.
Datadog,42480375,nan,0,"2017/02/27, 09:52:21",False,"2017/02/27, 09:52:21",nan,7628458.0,1.0,0,55,System load peaks on kubernetes master node on azure,"Each time we have a system load peak of 1.8-2 on our dual-core machine, I see in the kube-controller-manager log that the master tries to start 3 things:"
Datadog,42480375,nan,0,"2017/02/27, 09:52:21",False,"2017/02/27, 09:52:21",nan,7628458.0,1.0,0,55,System load peaks on kubernetes master node on azure,Our kubernetes version is 1.4.6 and is created via the azure portal.
Datadog,42480375,nan,0,"2017/02/27, 09:52:21",False,"2017/02/27, 09:52:21",nan,7628458.0,1.0,0,55,System load peaks on kubernetes master node on azure,The system peaks can we see via datadog monitoring.
Datadog,39172174,nan,2,"2016/08/26, 20:59:11",True,"2018/04/24, 21:19:55","2016/08/27, 13:52:50",1226507.0,869.0,0,1715,Connection Refused localhost:9999 java.io.IOException: Failed to retrieve RMIServer stub: javax.naming.ServiceUnavailableException,I'm using Datadog to collect metrics from Kafka running on my localhost.
Datadog,39172174,nan,2,"2016/08/26, 20:59:11",True,"2018/04/24, 21:19:55","2016/08/27, 13:52:50",1226507.0,869.0,0,1715,Connection Refused localhost:9999 java.io.IOException: Failed to retrieve RMIServer stub: javax.naming.ServiceUnavailableException,When I run the -info command on my Datadog agent this is the error I get for Kafka.
Datadog,39172174,nan,2,"2016/08/26, 20:59:11",True,"2018/04/24, 21:19:55","2016/08/27, 13:52:50",1226507.0,869.0,0,1715,Connection Refused localhost:9999 java.io.IOException: Failed to retrieve RMIServer stub: javax.naming.ServiceUnavailableException,Any ideas whats causing this?
Datadog,39163880,39174290.0,1,"2016/08/26, 13:15:25",True,"2016/08/26, 23:35:25",nan,785523.0,4123.0,0,799,Monitoring nexus OSS 3.0.1,I have some doubts regarding monitoring nexus OSS 3.0.1 server.
Datadog,39163880,39174290.0,1,"2016/08/26, 13:15:25",True,"2016/08/26, 23:35:25",nan,785523.0,4123.0,0,799,Monitoring nexus OSS 3.0.1,Can some please let me know the following:-
Datadog,30155496,30168310.0,1,"2015/05/10, 22:03:27",True,"2015/05/16, 01:14:03","2015/05/16, 01:14:03",2139691.0,6287.0,0,146,Test performance in Openshift and prevent get banned IP,I have an application hosted in openshift.
Datadog,30155496,30168310.0,1,"2015/05/10, 22:03:27",True,"2015/05/16, 01:14:03","2015/05/16, 01:14:03",2139691.0,6287.0,0,146,Test performance in Openshift and prevent get banned IP,Now I want figure out how many request can handle in order to check the speed and availability.
Datadog,30155496,30168310.0,1,"2015/05/10, 22:03:27",True,"2015/05/16, 01:14:03","2015/05/16, 01:14:03",2139691.0,6287.0,0,146,Test performance in Openshift and prevent get banned IP,So my first attempt will be generate a multiple HTTP GET requests to my Rest Service(made in python and hosted in openshift).
Datadog,30155496,30168310.0,1,"2015/05/10, 22:03:27",True,"2015/05/16, 01:14:03","2015/05/16, 01:14:03",2139691.0,6287.0,0,146,Test performance in Openshift and prevent get banned IP,My fear is can get my IP workplace banned regarding this looks like an attack.
Datadog,30155496,30168310.0,1,"2015/05/10, 22:03:27",True,"2015/05/16, 01:14:03","2015/05/16, 01:14:03",2139691.0,6287.0,0,146,Test performance in Openshift and prevent get banned IP,"In the other hand I see there are tools like  New Relic  or  DataDog  to check metrics, but I don't know if I can simulate http requests and then check the response times."
Datadog,30155496,30168310.0,1,"2015/05/10, 22:03:27",True,"2015/05/16, 01:14:03","2015/05/16, 01:14:03",2139691.0,6287.0,0,146,Test performance in Openshift and prevent get banned IP,I finally wrote to Openshift support and they told me I can simulate http requests without worries.
Datadog,66906870,nan,0,"2021/04/01, 17:57:52",False,"2021/04/01, 17:57:52",nan,1832474.0,2638.0,0,8,How does Datadog&#39;s statsd handle a `None` value for a tag,"I'm adding logging to my application, and was wondering what the correct way is to add a tag that may or may not have a value."
Datadog,66906870,nan,0,"2021/04/01, 17:57:52",False,"2021/04/01, 17:57:52",nan,1832474.0,2638.0,0,8,How does Datadog&#39;s statsd handle a `None` value for a tag,My code is
Datadog,66906870,nan,0,"2021/04/01, 17:57:52",False,"2021/04/01, 17:57:52",nan,1832474.0,2638.0,0,8,How does Datadog&#39;s statsd handle a `None` value for a tag,The  config  is a dict of configuration variables constructed from another class.
Datadog,66906870,nan,0,"2021/04/01, 17:57:52",False,"2021/04/01, 17:57:52",nan,1832474.0,2638.0,0,8,How does Datadog&#39;s statsd handle a `None` value for a tag,Is it okay to pass objects that might not have a value like the  sometimes_here  config element?
Datadog,66906870,nan,0,"2021/04/01, 17:57:52",False,"2021/04/01, 17:57:52",nan,1832474.0,2638.0,0,8,How does Datadog&#39;s statsd handle a `None` value for a tag,Or do I need to build the tags list separately like
Datadog,66906870,nan,0,"2021/04/01, 17:57:52",False,"2021/04/01, 17:57:52",nan,1832474.0,2638.0,0,8,How does Datadog&#39;s statsd handle a `None` value for a tag,"Since there might potentially be even more optional elements in config, I'm hoping that the more compact code will be suitable, but I can't find anything in the Datadog documentation on what it would do about those tags."
Datadog,66753780,nan,0,"2021/03/22, 22:59:48",False,"2021/03/22, 22:59:48",nan,6683100.0,21.0,0,10,Datadog:Category processor- Define logs as errors-Unable to parse the logs as error,"We have the below Category processor pipeline for Lambdas such that when there is a log with (@error.kind:* or @error.message:*) , the logs will be defined as an error."
Datadog,66753780,nan,0,"2021/03/22, 22:59:48",False,"2021/03/22, 22:59:48",nan,6683100.0,21.0,0,10,Datadog:Category processor- Define logs as errors-Unable to parse the logs as error,"To test above Category process rule, I have create a simple Lambda called python-test as below"
Datadog,66753780,nan,0,"2021/03/22, 22:59:48",False,"2021/03/22, 22:59:48",nan,6683100.0,21.0,0,10,Datadog:Category processor- Define logs as errors-Unable to parse the logs as error,"When I run this lambda with below test event, I don’t see the log messages being converted to error"
Datadog,66753780,nan,0,"2021/03/22, 22:59:48",False,"2021/03/22, 22:59:48",nan,6683100.0,21.0,0,10,Datadog:Category processor- Define logs as errors-Unable to parse the logs as error,"Eg: In Datadog as you can see below,
it still remains as log ie."
Datadog,66753780,nan,0,"2021/03/22, 22:59:48",False,"2021/03/22, 22:59:48",nan,6683100.0,21.0,0,10,Datadog:Category processor- Define logs as errors-Unable to parse the logs as error,"it is not converted to error (ErrorType –error.kind is not set)
Can you please advise If I am missing something?"
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,I'm trying to implement distributed tracing in my kotlin app using spring cloud sleuth.
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,I'm sending those data to the datadog.
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,Now I'm able to trace my logs but I want to add some extra data to spans.
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,Let's say I want to add info about user and be able to see it in datadog.
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,Am I right that span tags are good for it?
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,I'm sending the logs in json format to datadog but I cannot add tags here.
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,(traceId and spanId are injected).
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,Logback config:
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,gradle:
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,and to add the tag I'm trying
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,example log:
Datadog,66554063,nan,1,"2021/03/09, 22:09:44",True,"2021/03/09, 23:11:35",nan,9824872.0,83.0,0,141,Spring cloud sleuth adding tag,shouldn't be that 'user' injected into MDC and then into logs?
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,I'm trying to configure spring actuator metrics along with micrometer to be sent to Datadog stastd agent.
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,"Still, I'd like to get them all sent with a tag, so that I can filter in my Datadog dashboard just my service metrics, and not considering other services metrics."
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,I've added:
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,"to my service metrics configuration, but I can't see this tag value in Datadog dashboard."
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,I'm not seeing anything weird in app logs nor actuator logfile neither.
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,"I have nothing else regarding metrics in my service, as I don't want to implement custom metrics, just want to use the one provided by actuator."
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,This is how the whole metrics configuration looks like:
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,Versions:
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,micrometer version: 1.6.4
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,actuator version: 2.4.3
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,spring version: 2.3.8
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,Any clue about what I could be missing to get the tag reaching Datadog?
Datadog,66533080,66538157.0,1,"2021/03/08, 17:59:41",True,"2021/03/09, 00:29:39",nan,606250.0,535.0,0,42,Spring micrometer actuator StatsD tags definition,Thanks!
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,I'm exploring the Open Telemetry Collector Project and how this could work with containerized .NET Core apps (or any other apps for that matter).
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"Currently we're using DynaTrace at the company I work for, which requires the DynaTrace 'OneAgent' agent to be installed on hosts."
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,The DynaTrace agent somehow hooks into the dotnet CLR and does bytecode/MSIL instrumentation.
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,Basically this approach allows us to capture APM data to DynaTrace without having to do any code changes in our apps whatsoever.
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"Contrast this with the Open Telemetry approach, which (as far as I can tell) requires additional (nuget) packages to be installed into the services we want instrumented."
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"In .NET Core land, I suspect this is using DiagnosticSource based instrumentation, which I would describe as a type of AOP."
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"That said, it's mostly automatic and integrated into various .NET libraries/frameworks such as ASP.NET, Entity Framework, etc; so the only code changes are; a) installing the Open Telemetry nuget packages, b) some basic Startup.cs configuration, and c) optionally adding additional spans if/when needed."
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"It's minimal code, but it's NOT no-code like the DynaTrace approach."
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,I'd also assume the granularity of spans to be a lot courser than bytecode/MSIL instrumentation approach used by DynaTrace.
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,WRT the Open Telemetry Collecter; I really like the fact that we can configure exporters to send instrumentation data to any supported third party monitoring solution (e.g.
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"DataDog, Elastic, Kafka, etc) without having to install any proprietary agents."
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,IMO this approach means we can more easily change the monitoring service(s) we might be using and therefore mitigates vendor lock.
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,I'm hoping I can find a way to get the best of both worlds;
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,Is this possible?
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,I was looking at how the DataDog agent works  here .
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,It looks like this might be a similar approach to DynaTrace.
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"For DataDog, it looks like you need to set some some CLR environment variables in the app container - presumably this extends the CLR to send bytecode/MSIL instrumentation data to the DataDog agent (running as a sidecar or something)..."
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"Would we be able to use this DataDog approach, but instead of sending the instrumentation data to a DataDog agent, we send it instead to an Open Telemerty Collecter agent?"
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,Technically the Open Telemerty Collecter agent could still send to DataDog if we wanted (i.e.
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,"using the  DataDog exporter ), but at least we'd mitigate the vendor lock using this approach."
Datadog,66486827,nan,0,"2021/03/05, 06:34:53",False,"2021/03/05, 06:42:12","2021/03/05, 06:42:12",824434.0,1520.0,0,30,bytecode instrumentation with Open Telemetry Collector agent,Thoughts...
Datadog,66376322,nan,0,"2021/02/25, 22:56:30",False,"2021/02/25, 22:56:30",nan,1470015.0,485.0,0,39,Django Middleware Slow Performance,Our team has a production Django app that's been experiencing latency spikes throughout the day.
Datadog,66376322,nan,0,"2021/02/25, 22:56:30",False,"2021/02/25, 22:56:30",nan,1470015.0,485.0,0,39,Django Middleware Slow Performance,"Drilling into the requests during those time periods in Datadog,  the recurring theme is that the Django middleware is running very slowly , which you can see in the flame graph."
Datadog,66376322,nan,0,"2021/02/25, 22:56:30",False,"2021/02/25, 22:56:30",nan,1470015.0,485.0,0,39,Django Middleware Slow Performance,"Compared to normal-latency periods, that's the main difference in performance, and this happens for all endpoints."
Datadog,66376322,nan,0,"2021/02/25, 22:56:30",False,"2021/02/25, 22:56:30",nan,1470015.0,485.0,0,39,Django Middleware Slow Performance,You can see from the Heroku metrics that it occurs in an almost regular pattern.
Datadog,66376322,nan,0,"2021/02/25, 22:56:30",False,"2021/02/25, 22:56:30",nan,1470015.0,485.0,0,39,Django Middleware Slow Performance,We haven't added any new middleware recently.
Datadog,66376322,nan,0,"2021/02/25, 22:56:30",False,"2021/02/25, 22:56:30",nan,1470015.0,485.0,0,39,Django Middleware Slow Performance,"Has anyone seen this before, or have any guidance on what could be causing this issue with the middleware?"
Datadog,65619895,65636279.0,1,"2021/01/07, 23:02:26",True,"2021/01/08, 22:56:19",nan,2051074.0,385.0,0,77,Conditional JavaAgent Command for SBT Native Packager,"I'm using scala, sbt, sbt-native-package, and potentially sbt-java-agent to conditionally activate a datadog java agent at runtime w/ kubernetes."
Datadog,65619895,65636279.0,1,"2021/01/07, 23:02:26",True,"2021/01/08, 22:56:19",nan,2051074.0,385.0,0,77,Conditional JavaAgent Command for SBT Native Packager,"By adding the  dd-java-agent  as a dependency and adding a script snippet, I'm able to activate datadog only when a specific env."
Datadog,65619895,65636279.0,1,"2021/01/07, 23:02:26",True,"2021/01/08, 22:56:19",nan,2051074.0,385.0,0,77,Conditional JavaAgent Command for SBT Native Packager,"variable is set, but this is also adding the dd-java-agent to the classpath, which I'm trying to avoid:"
Datadog,65619895,65636279.0,1,"2021/01/07, 23:02:26",True,"2021/01/08, 22:56:19",nan,2051074.0,385.0,0,77,Conditional JavaAgent Command for SBT Native Packager,"Is there a way to have sbt manage the downloading of dd-java-agent.jar, include this jar in the  lib  directory (or a different directory if that's what it takes), but exclude from classpath?"
Datadog,65619895,65636279.0,1,"2021/01/07, 23:02:26",True,"2021/01/08, 22:56:19",nan,2051074.0,385.0,0,77,Conditional JavaAgent Command for SBT Native Packager,"I've tried using  sbt-java-agent  which puts the jar in a  dd-java-agent  directory and excludes the jar from the classpath, but I can not figure out how to wrap the  addJava  statement in an  if  check when using that plugin."
Datadog,65619895,65636279.0,1,"2021/01/07, 23:02:26",True,"2021/01/08, 22:56:19",nan,2051074.0,385.0,0,77,Conditional JavaAgent Command for SBT Native Packager,Thanks for any help you can provide!
Datadog,65274625,nan,0,"2020/12/13, 12:38:06",False,"2020/12/13, 12:38:06",nan,3323914.0,856.0,0,39,Istio tcp metrics not showing,"I’m integrating Istio v1.8.0 with DataDog and their integrations works well, Im getting most of the metrics."
Datadog,65274625,nan,0,"2020/12/13, 12:38:06",False,"2020/12/13, 12:38:06",nan,3323914.0,856.0,0,39,Istio tcp metrics not showing,"But, I’m not getting tcp related metrics for my services:"
Datadog,65274625,nan,0,"2020/12/13, 12:38:06",False,"2020/12/13, 12:38:06",nan,3323914.0,856.0,0,39,Istio tcp metrics not showing,"My services communicate using HTTP with each other, but I want to see tcp metrics as well for open connections to my envoy-proxy and other tcp related metrics."
Datadog,65274625,nan,0,"2020/12/13, 12:38:06",False,"2020/12/13, 12:38:06",nan,3323914.0,856.0,0,39,Istio tcp metrics not showing,Are there any metrics of this kind that I can use?
Datadog,65274625,nan,0,"2020/12/13, 12:38:06",False,"2020/12/13, 12:38:06",nan,3323914.0,856.0,0,39,Istio tcp metrics not showing,I don't see any here -  https://istio.io/latest/docs/reference/config/metrics/
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,I'm trying Migrating from DataDog to prometheus.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,and this one query i'm having some difficulties with.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,I'm unable to find the pct_change functionality available in DD in prometheus.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,I tried using (A-B)/B or A/B but they don't work.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,not appropriate results.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,e.g.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,But it's not correct.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,The alert fires very often.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,Any leads on this would be appreciated.
Datadog,65163032,nan,0,"2020/12/06, 01:22:22",False,"2020/12/06, 01:22:22",nan,4323514.0,357.0,0,50,Prometheus Percentage Change over time,How can I write the PromQl alternative of DD?
Datadog,65125023,nan,0,"2020/12/03, 13:21:15",False,"2020/12/03, 13:57:05","2020/12/03, 13:57:05",9136487.0,159.0,0,44,Monitoring Metrices for AWS ECS,We are using Datadog and AWS ECS.To collect ECS host network related metrices in  DataDog we are following below documentation
Datadog,65125023,nan,0,"2020/12/03, 13:21:15",False,"2020/12/03, 13:57:05","2020/12/03, 13:57:05",9136487.0,159.0,0,44,Monitoring Metrices for AWS ECS,https://docs.datadoghq.com/network_performance_monitoring/installation/?tab=docker
Datadog,65125023,nan,0,"2020/12/03, 13:21:15",False,"2020/12/03, 13:57:05","2020/12/03, 13:57:05",9136487.0,159.0,0,44,Monitoring Metrices for AWS ECS,I was able to add all mentioned parms in DataDog agent container definition except below
Datadog,65125023,nan,0,"2020/12/03, 13:21:15",False,"2020/12/03, 13:57:05","2020/12/03, 13:57:05",9136487.0,159.0,0,44,Monitoring Metrices for AWS ECS,I tried to update same in my ecs-taskdefination cloudformation
Datadog,65125023,nan,0,"2020/12/03, 13:21:15",False,"2020/12/03, 13:57:05","2020/12/03, 13:57:05",9136487.0,159.0,0,44,Monitoring Metrices for AWS ECS,but getting below error when deploying cloudformation
Datadog,65125023,nan,0,"2020/12/03, 13:21:15",False,"2020/12/03, 13:57:05","2020/12/03, 13:57:05",9136487.0,159.0,0,44,Monitoring Metrices for AWS ECS, Model validation failed (#/conatinerdefination/0/Privilaged: expected type: Boolean found: string)
Datadog,64576585,nan,0,"2020/10/28, 17:56:01",False,"2020/10/28, 17:56:01",nan,11138618.0,19.0,0,27,Lots of XHR error GET https://app.domain.com/sockjs/info?cb=79_e31b43d,"We have an electron app that connects to our servers on AWS (ALB, Fargate, lambdas) its a meteor based app (ddp, socks underneath) for the past month we've seen lots of XHR error GET on our datadog logs, it seems it's not disrupting our service, but it's really polluting all of our logs as we get a lot of those, could it be its timing out due to ALB not letting the connection in?"
Datadog,64576585,nan,0,"2020/10/28, 17:56:01",False,"2020/10/28, 17:56:01",nan,11138618.0,19.0,0,27,Lots of XHR error GET https://app.domain.com/sockjs/info?cb=79_e31b43d,"it's odd as I stated our users haven't reported any connection issues and are able to use the app, we have ports 80, 443 open and redirect any calls from Http to Https, so not really sure what would be causing the error."
Datadog,64576585,nan,0,"2020/10/28, 17:56:01",False,"2020/10/28, 17:56:01",nan,11138618.0,19.0,0,27,Lots of XHR error GET https://app.domain.com/sockjs/info?cb=79_e31b43d,Do XHR sockjs are using a different port?
Datadog,64576585,nan,0,"2020/10/28, 17:56:01",False,"2020/10/28, 17:56:01",nan,11138618.0,19.0,0,27,Lots of XHR error GET https://app.domain.com/sockjs/info?cb=79_e31b43d,"We assume it has to be something on our network but we haven't figured it out, we don't experience any of these errors when we use our app locally on development mode, only happens if we have the electron app running and we turn off our server instance, as the connection cannot be established."
Datadog,64576585,nan,0,"2020/10/28, 17:56:01",False,"2020/10/28, 17:56:01",nan,11138618.0,19.0,0,27,Lots of XHR error GET https://app.domain.com/sockjs/info?cb=79_e31b43d,Any insights would be great!
Datadog,64576585,nan,0,"2020/10/28, 17:56:01",False,"2020/10/28, 17:56:01",nan,11138618.0,19.0,0,27,Lots of XHR error GET https://app.domain.com/sockjs/info?cb=79_e31b43d,thanks!
Datadog,64543071,nan,0,"2020/10/26, 20:36:23",False,"2020/10/26, 20:36:23",nan,3149444.0,1417.0,0,24,Data Dog Count by Field query,I'm literarlly new to DataDog.
Datadog,64543071,nan,0,"2020/10/26, 20:36:23",False,"2020/10/26, 20:36:23",nan,3149444.0,1417.0,0,24,Data Dog Count by Field query,I have been assigned into a project where they have set up DataDog to log everything within the code.
Datadog,64543071,nan,0,"2020/10/26, 20:36:23",False,"2020/10/26, 20:36:23",nan,3149444.0,1417.0,0,24,Data Dog Count by Field query,After spending 1hr or so trying to do something like:
Datadog,64543071,nan,0,"2020/10/26, 20:36:23",False,"2020/10/26, 20:36:23",nan,3149444.0,1417.0,0,24,Data Dog Count by Field query,"I concluded that to do a simple &quot;count by&quot;, I need to go over the process of creating a Processor, then a Metric."
Datadog,64543071,nan,0,"2020/10/26, 20:36:23",False,"2020/10/26, 20:36:23",nan,3149444.0,1417.0,0,24,Data Dog Count by Field query,Really is that cumbersome?
Datadog,64543071,nan,0,"2020/10/26, 20:36:23",False,"2020/10/26, 20:36:23",nan,3149444.0,1417.0,0,24,Data Dog Count by Field query,I miss so much SUMO Logic simplicity for this scenario.
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,"In my company, we have Datadog dashboards and monitors which we often make changes to and thus we wish to have some version control."
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,After discussions it was decided that Terraform is the way to go with this.
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,I've successfully been able to manually back up our current Datadog infrastructure using Google's  Terraformer  (to import the infrastructure as resources) and Terraform's CLI commands to import the corresponding infrastructure's states.
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,It is also possible to make changes to the resources through commands such as  terraform plan  and  terraform apply .
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,The manual changes to the resources can then be backed up with Git.
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,Now to the issue I'm having.
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,"We wish to automate the the back-up of our resources, that is, through making changes in the Datadog GUI and to have those changes be reflected in our resource files."
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,That means the other way around of what I've managed to accomplish above.
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,"It is rather straight forward to do this manually, where one imports the infrastructure resources that have been changed using the GUI (using terraformer)."
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,"If &quot;conflicts&quot; arise, one needs to manually remove certain resources and re-import them which I've found difficult to automate."
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,My question is: is there a straight forward way of automating back-up of changes made &quot;remotely&quot; (e.g.
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,in the Datadog GUI) to resource files?
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,My research for Terraform CLI's for this purpose has lead me to nothing thus far.
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,Any suggestions are appreciated!
Datadog,63793906,nan,0,"2020/09/08, 15:25:02",False,"2020/09/08, 15:25:02",nan,13767817.0,11.0,0,43,Terraform: Automate backup of infrastructure,Thanks
Datadog,63054587,nan,1,"2020/07/23, 15:44:20",True,"2020/07/28, 08:43:32","2020/07/28, 08:43:32",6818786.0,103.0,0,48,"How to track and monitoring, jvm class size","Yesterday my JVM application was broken due to high CPU usage, when checking the root cause this is because GC stop the world."
Datadog,63054587,nan,1,"2020/07/23, 15:44:20",True,"2020/07/28, 08:43:32","2020/07/28, 08:43:32",6818786.0,103.0,0,48,"How to track and monitoring, jvm class size","The GC require more time to clean up the memory, this is because I introduce new LRU cache at the app."
Datadog,63054587,nan,1,"2020/07/23, 15:44:20",True,"2020/07/28, 08:43:32","2020/07/28, 08:43:32",6818786.0,103.0,0,48,"How to track and monitoring, jvm class size",So my question is:
Datadog,63054587,nan,1,"2020/07/23, 15:44:20",True,"2020/07/28, 08:43:32","2020/07/28, 08:43:32",6818786.0,103.0,0,48,"How to track and monitoring, jvm class size",Is there any way to track how big memory group by JVM class?
Datadog,63054587,nan,1,"2020/07/23, 15:44:20",True,"2020/07/28, 08:43:32","2020/07/28, 08:43:32",6818786.0,103.0,0,48,"How to track and monitoring, jvm class size","For example, I have 3 classes,  Foo ,  Bar ,  Baz ."
Datadog,63054587,nan,1,"2020/07/23, 15:44:20",True,"2020/07/28, 08:43:32","2020/07/28, 08:43:32",6818786.0,103.0,0,48,"How to track and monitoring, jvm class size",I want to know how big memory used by each class at run time.
Datadog,63054587,nan,1,"2020/07/23, 15:44:20",True,"2020/07/28, 08:43:32","2020/07/28, 08:43:32",6818786.0,103.0,0,48,"How to track and monitoring, jvm class size","I have a Datadog account, so I plan to use tools to send those metrics to Datadog."
Datadog,62331584,nan,1,"2020/06/11, 21:57:48",False,"2020/06/12, 00:23:58",nan,6501026.0,31.0,0,153,How to import 3rd party python libraries for use with glue python shell script,I'm trying to import a 3rd party library (datadog) for use with a glue shell script and I'm running into issues.
Datadog,62331584,nan,1,"2020/06/11, 21:57:48",False,"2020/06/12, 00:23:58",nan,6501026.0,31.0,0,153,How to import 3rd party python libraries for use with glue python shell script,"I've packaged the file as a .egg and given the path to it in the glue job, as instructed  here ."
Datadog,62331584,nan,1,"2020/06/11, 21:57:48",False,"2020/06/12, 00:23:58",nan,6501026.0,31.0,0,153,How to import 3rd party python libraries for use with glue python shell script,This ends up throwing an error saying zipimport.ZipImportError: not a Zip file: '/tmp/glue-python-libs/datadog.egg'.
Datadog,62331584,nan,1,"2020/06/11, 21:57:48",False,"2020/06/12, 00:23:58",nan,6501026.0,31.0,0,153,How to import 3rd party python libraries for use with glue python shell script,"When I try using a zip file instead, it throws ModuleNotFoundError: No module named 'datadog'."
Datadog,62331584,nan,1,"2020/06/11, 21:57:48",False,"2020/06/12, 00:23:58",nan,6501026.0,31.0,0,153,How to import 3rd party python libraries for use with glue python shell script,How do I go about importing the library?
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,"Currently have some task-based automation for ECS that run on a scheduled basis, however sometimes there is a need to run only run task or re-run tasks for only a certain kinds of tasks (for example sql tasks or datadog tasks)."
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,"I know this can be done via console, but it's inefficient."
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,Was thinking of a bash script that calls to start a task from a CLI.
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,"Currently I know I can do this with the AWS CLI using '--task-definition', but it's not much better."
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,"I don't usually write scripts, so I'm basically here to help with brainstorming."
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,I'm wondering if there is a way to make an API call to start tasks.
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,Would I need to type in the ARN every time?
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,Can I just list the tasks on the AWS CLI and have the exported to the script?
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,Would network-config need to be hard-coded?
Datadog,62030485,nan,1,"2020/05/26, 23:03:44",False,"2020/05/27, 05:33:55","2020/05/27, 05:33:55",13622713.0,1.0,0,141,shell script for AWS run-tasks outside of normal schedule,Thanks!
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,I use  Laravel Horizon  to process jobs in my Laravel application with the help of Supervisor.
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,The whole setup runs on docker in ECS.
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,"On the host node of ECS (managed by Fargate) I have datadog agents running on it, which will grab the stdout/stderr of the container and put it into datadog (so I can access the logs centrally)."
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,What I noticed is that I am not getting any logs out of the jobs that is processing.
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,"Even simple  print(""test"");  in the job code does not go to the stdout."
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,Here's my setup:
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,Dockerfile
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,The service definition for this in yaml looks like this:
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,And here's the content of my  horizon.conf :
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,Any idea what I might be missing?
Datadog,61872153,nan,1,"2020/05/18, 17:40:33",False,"2020/05/18, 18:57:42",nan,2332336.0,19520.0,0,878,Getting logs out of Laravel Horizon running in docker in supervisor,What can't I get logs out of the jobs processed by horizon in docker via supervisor?
Datadog,61644174,nan,1,"2020/05/06, 22:54:45",False,"2020/05/20, 15:05:31",nan,13485796.0,1.0,0,370,Permission denied to read datadog.yaml file,I've installed the datadog app on my linux vm but i can't seem to read the datadog.yaml agent file.
Datadog,61644174,nan,1,"2020/05/06, 22:54:45",False,"2020/05/20, 15:05:31",nan,13485796.0,1.0,0,370,Permission denied to read datadog.yaml file,[ Error reading /etc/datadog-agent/datadog.yaml: Permission denied ]
Datadog,61644174,nan,1,"2020/05/06, 22:54:45",False,"2020/05/20, 15:05:31",nan,13485796.0,1.0,0,370,Permission denied to read datadog.yaml file,"My linux box is hosted on GCP, do i need to configure permissions?"
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,I have a Spring Boot (2.2.6) application that uses Log4j2 (with Slf4j).
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,Log4j is configured to use the json layout and in the end I want to ingest the logs in Datadog.
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,For that the 'serviceName' is important as a field in the json.
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,Now according to the log4j docu ( https://logging.apache.org/log4j/2.x/manual/layouts.html#JSONLayout ) one can add a custom field with the 'KeyValuePair' tags and that works.
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,Unfortunately this breaks the normal structure of the spring logs.
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,Log4j2.xml config:
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,Log w/o custom field:
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,log w/ custom field:
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,"Spring docu mentions how this might work with logback, but not log4j ( https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-custom-log-configuration , end of chapter)"
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,I've searched but couldn't find anything useful.
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,Any ideas how i can add a custom field to the json log while still preserving all fields coming from Spring?
Datadog,61588585,61590972.0,1,"2020/05/04, 12:03:03",True,"2020/05/05, 16:48:04","2020/05/05, 16:48:04",13465678.0,3.0,0,1123,Adding custom field to log4j json layout breaks spring logging structure,"Thanks, 
Felix"
Datadog,60933143,nan,1,"2020/03/30, 17:47:27",True,"2020/03/31, 15:56:50","2020/03/30, 18:24:57",9083006.0,143.0,0,207,io.micrometer.core.instrument.config.MeterFilter : DENY is not working in spring boot,I want to expose all metrics on the metrics endpoint but publish some of them to a remote meter registry.
Datadog,60933143,nan,1,"2020/03/30, 17:47:27",True,"2020/03/31, 15:56:50","2020/03/30, 18:24:57",9083006.0,143.0,0,207,io.micrometer.core.instrument.config.MeterFilter : DENY is not working in spring boot,"For doing so, I have a SimpleMeterRegistry for the metrics endpoint and added a  MeterRegistryCustomizer  for the remote meter registry(Datadog) to add some  MeterFilter  to avoid specific metrics using  MeterFilter's DENY  function."
Datadog,60933143,nan,1,"2020/03/30, 17:47:27",True,"2020/03/31, 15:56:50","2020/03/30, 18:24:57",9083006.0,143.0,0,207,io.micrometer.core.instrument.config.MeterFilter : DENY is not working in spring boot,For example :
Datadog,60933143,nan,1,"2020/03/30, 17:47:27",True,"2020/03/31, 15:56:50","2020/03/30, 18:24:57",9083006.0,143.0,0,207,io.micrometer.core.instrument.config.MeterFilter : DENY is not working in spring boot,"However, all jvm related metrics are visible in Datadog."
Datadog,60933143,nan,1,"2020/03/30, 17:47:27",True,"2020/03/31, 15:56:50","2020/03/30, 18:24:57",9083006.0,143.0,0,207,io.micrometer.core.instrument.config.MeterFilter : DENY is not working in spring boot,I tried  MeterFilterReply  but no use.
Datadog,60933143,nan,1,"2020/03/30, 17:47:27",True,"2020/03/31, 15:56:50","2020/03/30, 18:24:57",9083006.0,143.0,0,207,io.micrometer.core.instrument.config.MeterFilter : DENY is not working in spring boot,Please suggest how this can be achieved.
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,I have a climate control automation tool running with Home Assistant.
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,"Hass supports many types of long-term db storage of it's entity states (sensor data etc), like datadog, influbdb, graphite etc."
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,So far I've tried influxdb and graphite.
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,I've been using grafana to visualize the data.
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,"I want to store threshold values in the database, such as min/max temperature."
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,These temperatures can be set using an input slider on the hass UI.
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,"Once set, these controls can be left for days, even weeks."
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,So there may be only one data point in the db for very long periods of time.
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,"If I want to display these on grafana, they disappear from the time range being looked at pretty quickly, and grafana simply removes the entity from the graph."
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,"Influx has a ""use previous value"", and graphite has a ""keepLastValue"" function that I thought I could use to pull the last value of the threshold from the db, but in both cases, the values must exist in the time range selected."
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,"If the previous value for the control was days before the time range, too bad, so sad."
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,"I thought this would be a very common requirement, but perhaps not."
Datadog,60896119,nan,1,"2020/03/28, 03:06:10",False,"2020/06/22, 21:52:24",nan,476436.0,1162.0,0,40,Graphing dashboard with thresholds taken from database,Does anyone know a combination of database and dashboard that can display the last value for an entity even if said last value was recorded far out of the time range selected?
Datadog,60465436,60493178.0,2,"2020/02/29, 15:03:59",True,"2020/03/02, 18:42:31",nan,3709060.0,4659.0,0,149,"NGINX, Extracting url path from http header",So I'm sending my Nginx access logs to Datadog (an APM solution).
Datadog,60465436,60493178.0,2,"2020/02/29, 15:03:59",True,"2020/03/02, 18:42:31",nan,3709060.0,4659.0,0,149,"NGINX, Extracting url path from http header",My log format looks like this
Datadog,60465436,60493178.0,2,"2020/02/29, 15:03:59",True,"2020/03/02, 18:42:31",nan,3709060.0,4659.0,0,149,"NGINX, Extracting url path from http header",I can extract the url from the  referrer  field and it looks like this
Datadog,60465436,60493178.0,2,"2020/02/29, 15:03:59",True,"2020/03/02, 18:42:31",nan,3709060.0,4659.0,0,149,"NGINX, Extracting url path from http header",I only want  /foo/bar  though.
Datadog,60465436,60493178.0,2,"2020/02/29, 15:03:59",True,"2020/03/02, 18:42:31",nan,3709060.0,4659.0,0,149,"NGINX, Extracting url path from http header",Is this something I have to modify in the  log_format ?
Datadog,60465436,60493178.0,2,"2020/02/29, 15:03:59",True,"2020/03/02, 18:42:31",nan,3709060.0,4659.0,0,149,"NGINX, Extracting url path from http header","I saw an example from datadog docs where they're able to extract a url path attribute, but there's no example config."
Datadog,59632924,59633846.0,1,"2020/01/07, 19:02:57",True,"2020/01/07, 20:08:11",nan,663045.0,2398.0,0,1557,How to set annotations for a helm install,I am trying to install the chart  stable/efs-provisioner  and I would like to apply an annotation so that the deployment is correctly tagged in datadog.
Datadog,59632924,59633846.0,1,"2020/01/07, 19:02:57",True,"2020/01/07, 20:08:11",nan,663045.0,2398.0,0,1557,How to set annotations for a helm install,"Datadog requires the  annotation :  ad.datadoghq.com/tags: '{""env"": ""staging""}'"
Datadog,59632924,59633846.0,1,"2020/01/07, 19:02:57",True,"2020/01/07, 20:08:11",nan,663045.0,2398.0,0,1557,How to set annotations for a helm install,"I have tried various incantations of the following, but I keep getting the error below."
Datadog,59632924,59633846.0,1,"2020/01/07, 19:02:57",True,"2020/01/07, 20:08:11",nan,663045.0,2398.0,0,1557,How to set annotations for a helm install,Error:
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,Tomcat getting hang with high load.
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,Tomcat threads DB connections are showing a sudden increase.
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,ex: tomcat threads reach 1000 from 200 within a minute.
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,normally tomcat thread shows 150 - 250 stable pattern
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,We have checked all application metrics and JVM metrics.
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,But cannot identifies the root cause for this sudden instability.
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,below is the list that we have checked.
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,"App - tomcat thread, tomcat CPU, tomcat memory, server memory, server CPU"
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,"DB - CPU,process,connections,memory"
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,JVM - GC calls
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,TomcatThreadGraph-Nagios -  https://imgur.com/NItgPjp
Datadog,58620959,nan,0,"2019/10/30, 10:09:53",False,"2019/10/30, 10:09:53",nan,11152068.0,11.0,0,149,When high load tomcat threads sudden increase and hanged after few minutes,TomcatThreadGraph-DataDog -  https://imgur.com/4RH570B
Datadog,57978390,nan,1,"2019/09/17, 19:25:14",False,"2019/09/17, 19:52:43",nan,nan,nan,0,126,Figure out the problematic index in ES cluster?,"I have  elastic-search cluster which hosts more than 15 indices, I have a Datadog integration which shows me the below view of my elastic-search cluster."
Datadog,57978390,nan,1,"2019/09/17, 19:25:14",False,"2019/09/17, 19:52:43",nan,nan,nan,0,126,Figure out the problematic index in ES cluster?,We have alert integration with DD(datadog) which gives us alert if overall CPU usage goes beyond 60% and also in our application we start getting alerts when  elasticsearch cluster is under stress  as in this case our response time increases beyond a configures threshold.
Datadog,57978390,nan,1,"2019/09/17, 19:25:14",False,"2019/09/17, 19:52:43",nan,nan,nan,0,126,Figure out the problematic index in ES cluster?,"Now my problem is how to know which indices are consuming the ES cluster resources most, so that we can fine either throttle the request from those indices or optimize their requests."
Datadog,57978390,nan,1,"2019/09/17, 19:25:14",False,"2019/09/17, 19:52:43",nan,nan,nan,0,126,Figure out the problematic index in ES cluster?,Some things which we did:
Datadog,57978390,nan,1,"2019/09/17, 19:25:14",False,"2019/09/17, 19:52:43",nan,nan,nan,0,126,Figure out the problematic index in ES cluster?,So my problem is very simple and all I want some metric from DD or elastic which can easily tell me which indices are consuming the most resources on a elastic-search cluster.
Datadog,57637849,nan,0,"2019/08/24, 15:14:58",False,"2019/08/24, 16:34:34","2019/08/24, 16:34:34",911691.0,1582.0,0,84,Microservices: Connection status to external services?,"I've started a new project, and am building my first micro(ish) service."
Datadog,57637849,nan,0,"2019/08/24, 15:14:58",False,"2019/08/24, 16:34:34","2019/08/24, 16:34:34",911691.0,1582.0,0,84,Microservices: Connection status to external services?,"It has a  /health  endpoint, which currently just replies with HTTP status  200  and a  { ""status"": ""OK"" }  body."
Datadog,57637849,nan,0,"2019/08/24, 15:14:58",False,"2019/08/24, 16:34:34","2019/08/24, 16:34:34",911691.0,1582.0,0,84,Microservices: Connection status to external services?,"My service relies on several other (external) services, and I was wondering if there's a standard I can follow for exposing information about the connection between my services and the external ones?"
Datadog,57637849,nan,0,"2019/08/24, 15:14:58",False,"2019/08/24, 16:34:34","2019/08/24, 16:34:34",911691.0,1582.0,0,84,Microservices: Connection status to external services?,I'd like to not invent my own patterns here if there are industry best practices or standards I can follow.
Datadog,57637849,nan,0,"2019/08/24, 15:14:58",False,"2019/08/24, 16:34:34","2019/08/24, 16:34:34",911691.0,1582.0,0,84,Microservices: Connection status to external services?,Something like:
Datadog,57637849,nan,0,"2019/08/24, 15:14:58",False,"2019/08/24, 16:34:34","2019/08/24, 16:34:34",911691.0,1582.0,0,84,Microservices: Connection status to external services?,"My service will be running in Kubernetes on GCP, if that helps."
Datadog,57637849,nan,0,"2019/08/24, 15:14:58",False,"2019/08/24, 16:34:34","2019/08/24, 16:34:34",911691.0,1582.0,0,84,Microservices: Connection status to external services?,We use Datadog for observability.
Datadog,56277311,nan,1,"2019/05/23, 17:20:18",False,"2019/05/26, 11:01:46",nan,8353186.0,21.0,0,48,How to list instances that have tag1:&lt;anyvalue&gt; AND tag2:&lt;anyvalue&gt; AND tag3:&lt;anyvalue&gt;,I need to identify in my infrastructure which hosts have tag1 and tag2 and tag3.
Datadog,56277311,nan,1,"2019/05/23, 17:20:18",False,"2019/05/26, 11:01:46",nan,8353186.0,21.0,0,48,How to list instances that have tag1:&lt;anyvalue&gt; AND tag2:&lt;anyvalue&gt; AND tag3:&lt;anyvalue&gt;,I'm new to datadog but it seems that when filtering I have to specify a value for a specific tag.
Datadog,56277311,nan,1,"2019/05/23, 17:20:18",False,"2019/05/26, 11:01:46",nan,8353186.0,21.0,0,48,How to list instances that have tag1:&lt;anyvalue&gt; AND tag2:&lt;anyvalue&gt; AND tag3:&lt;anyvalue&gt;,"I also need to identify the inverse, list hosts that have tag1 but are missing tag2 OR tag3."
Datadog,56277311,nan,1,"2019/05/23, 17:20:18",False,"2019/05/26, 11:01:46",nan,8353186.0,21.0,0,48,How to list instances that have tag1:&lt;anyvalue&gt; AND tag2:&lt;anyvalue&gt; AND tag3:&lt;anyvalue&gt;,"I have setup a dashboard for each of the tags, I seem to be limited by up 2 tags."
Datadog,56277311,nan,1,"2019/05/23, 17:20:18",False,"2019/05/26, 11:01:46",nan,8353186.0,21.0,0,48,How to list instances that have tag1:&lt;anyvalue&gt; AND tag2:&lt;anyvalue&gt; AND tag3:&lt;anyvalue&gt;,"e.g filter by -  env:dev 
    group by -  tag1 , tag2"
Datadog,56277311,nan,1,"2019/05/23, 17:20:18",False,"2019/05/26, 11:01:46",nan,8353186.0,21.0,0,48,How to list instances that have tag1:&lt;anyvalue&gt; AND tag2:&lt;anyvalue&gt; AND tag3:&lt;anyvalue&gt;,I would expect to be able to see what hosts have tag1 AND tag2 AND tag3
Datadog,56277311,nan,1,"2019/05/23, 17:20:18",False,"2019/05/26, 11:01:46",nan,8353186.0,21.0,0,48,How to list instances that have tag1:&lt;anyvalue&gt; AND tag2:&lt;anyvalue&gt; AND tag3:&lt;anyvalue&gt;,And the inverse -  what hosts do not have all 3 tags.
Datadog,55621848,55661352.0,1,"2019/04/11, 00:54:05",True,"2019/04/13, 04:58:31",nan,3345737.0,280.0,0,129,How to aggregate logs from several Jenkins Jobs\Pipelines in one place?,Our project is responsible for migrating data from one system to another.
Datadog,55621848,55661352.0,1,"2019/04/11, 00:54:05",True,"2019/04/13, 04:58:31",nan,3345737.0,280.0,0,129,How to aggregate logs from several Jenkins Jobs\Pipelines in one place?,"We are going to run transformation, validation and migration scripts using Jenkins."
Datadog,55621848,55661352.0,1,"2019/04/11, 00:54:05",True,"2019/04/13, 04:58:31",nan,3345737.0,280.0,0,129,How to aggregate logs from several Jenkins Jobs\Pipelines in one place?,It's unclear for me how to aggregate logs from several Jobs or Pipelines in Jenkins.
Datadog,55621848,55661352.0,1,"2019/04/11, 00:54:05",True,"2019/04/13, 04:58:31",nan,3345737.0,280.0,0,129,How to aggregate logs from several Jenkins Jobs\Pipelines in one place?,How it can be done?
Datadog,55621848,55661352.0,1,"2019/04/11, 00:54:05",True,"2019/04/13, 04:58:31",nan,3345737.0,280.0,0,129,How to aggregate logs from several Jenkins Jobs\Pipelines in one place?,We'll rely on logs heavily to identify any issues found during validation etc.
Datadog,55621848,55661352.0,1,"2019/04/11, 00:54:05",True,"2019/04/13, 04:58:31",nan,3345737.0,280.0,0,129,How to aggregate logs from several Jenkins Jobs\Pipelines in one place?,In terms of our planned setup we'll have AWS EC2 instances + we can use Datadog (our company uses it).
Datadog,55621848,55661352.0,1,"2019/04/11, 00:54:05",True,"2019/04/13, 04:58:31",nan,3345737.0,280.0,0,129,How to aggregate logs from several Jenkins Jobs\Pipelines in one place?,Can we use Datadog for this purpose?
Datadog,54280115,54284969.0,1,"2019/01/20, 21:24:47",True,"2019/01/21, 09:14:28","2019/01/21, 09:13:13",1066820.0,1317.0,0,57,PostgreSQL ALTER TABLE takes 35 minutes,I've executed this query on production with load on the server.
Datadog,54280115,54284969.0,1,"2019/01/20, 21:24:47",True,"2019/01/21, 09:14:28","2019/01/21, 09:13:13",1066820.0,1317.0,0,57,PostgreSQL ALTER TABLE takes 35 minutes,It took 35 minutes according to the datadog.
Datadog,54280115,54284969.0,1,"2019/01/20, 21:24:47",True,"2019/01/21, 09:14:28","2019/01/21, 09:13:13",1066820.0,1317.0,0,57,PostgreSQL ALTER TABLE takes 35 minutes,this  table1  has around 100 million rows.
Datadog,54280115,54284969.0,1,"2019/01/20, 21:24:47",True,"2019/01/21, 09:14:28","2019/01/21, 09:13:13",1066820.0,1317.0,0,57,PostgreSQL ALTER TABLE takes 35 minutes,Is 35 minutes normal?
Datadog,54280115,54284969.0,1,"2019/01/20, 21:24:47",True,"2019/01/21, 09:14:28","2019/01/21, 09:13:13",1066820.0,1317.0,0,57,PostgreSQL ALTER TABLE takes 35 minutes,Is there any way I can execute such simple migrations (adding a nullable column) without locking the table?
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,I have created a windows cmd file that calls three independent bat files.
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,I want to create a windows task that calls this cmd file and runs every 5 minutes.
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,The problem is that this task runs perfectly fine only when I'm logged into the system.
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,"But I'm unable to make this task continue to run ""whether I'm logged in or not"" ."
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,I even asked my colleague to login to that machine and run this task under his account - it worked.
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,"I created a local admin user on that machine, logged in as that user, tried to run this task - it did not work - the script waits forever while post_results.bat."
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,I even tried to schedule a jenkins job that basically does the same thing - it did not work - the jenkins job waits forever while post_results.bat (I killed the jenkins job after waiting for ~20 min).
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,Here is a summary of what these tasks are doing:
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,run_all.cmd
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,run_test.bat  - executes a jmeter script
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,post_results.bat  - calls a python script that posts the jmeter test results to datadog
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,post_jmeter_results_to_datadog.py  - uses the datadog python api to post metrics to datadog
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,clean.bat  - deletes the jmeter test result files
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,All I need is to be able to run this task every 5 minutes.
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,If anyone is able to see what I'm doing wrong and points that out...
Datadog,52594973,nan,1,"2018/10/01, 19:09:08",False,"2018/10/08, 16:32:20",nan,4386440.0,899.0,0,35,Unable to run a windows task that makes https request under SYSTEM account,I'd be really grateful.
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,We have a cookbook with multiple recipes where we select features.
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,"In this case, it's Couchbase and we want to be able to have  data ,  query , and  index  nodes tagged in Datadog, but that's probably more than you need to know..."
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,"Anyhow, one or more features can be selected."
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,"So, we have 3 recipes, one for each concern."
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,Each recipe adds the feature name to an array and then  include_recipe cookbook::default
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,"With Chef 12, we could select multiple feature recipes and then it seemed to wait until all of them were processed to run the default cookbook, so it could aggregate the array and process all the chosen features together."
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,"With Chef 13, it appears to be run immediately after the first feature recipe is processed, so that subsequent  include_recipe  are skipped."
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,"As a workaround, of course, I've changed some of the logic, but finding details on this behavior change hasn't come up with anything."
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,Thanks for any help...
Datadog,52064818,nan,1,"2018/08/28, 22:13:22",False,"2018/08/28, 22:53:03",nan,1757329.0,892.0,0,33,Chef 12 -&gt; Chef 13 include_recipe changes,-H
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,"I have a linux trusty on aws m4.xlarge so 4 CPU, 16 GB RAM."
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,It's running a java application on tomcat7 and oracle java 8.
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,Very frequently the app will hang and won't accept any other connection.
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,Status cake will report it as down since the response times out.
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,Datadog will show threads are maxed out.
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,But there is no increase in CPU (barely 10% of usage).
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,RAM usage remains unchanged during that period.
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,Only a tomcat restart fixes the problem temporarily(12h approx ).
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,So I have taken a thread dump and seen so many threads in a waiting state.
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,"Since this is very new to me, I am blind even with the data."
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,I was hoping I could get help here and eventually master the art of ciphering a thread dump file.
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,I have attached it here and I have as well uploaded it to  fastthread.io and it says there is no problem .
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,I have also uploaded the full  threadump on zerobin
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,I would be very grateful if anyone here can shed some lights on this and I hope it will help others in the same situation.
Datadog,50526252,50526751.0,1,"2018/05/25, 12:46:57",True,"2018/05/26, 12:52:41","2018/05/26, 12:52:41",92213.0,5924.0,0,58,How to point out what&#39;s putting a thread in a WAITING state in a threadump file,Thanks in advance.
Datadog,49468467,49475152.0,1,"2018/03/24, 20:40:09",True,"2018/03/25, 14:11:43",nan,607038.0,3355.0,0,512,Register bean with Grails for MeterRegistryCustomizer,"Having followed  Spring Boot Metrics documentation , I was able to set metrics logging for datadog easily."
Datadog,49468467,49475152.0,1,"2018/03/24, 20:40:09",True,"2018/03/25, 14:11:43",nan,607038.0,3355.0,0,512,Register bean with Grails for MeterRegistryCustomizer,The only remaining stuff is to set custom tags for my instances.
Datadog,49468467,49475152.0,1,"2018/03/24, 20:40:09",True,"2018/03/25, 14:11:43",nan,607038.0,3355.0,0,512,Register bean with Grails for MeterRegistryCustomizer,"With Spring Boot, you can do it by registering a new bean:"
Datadog,49468467,49475152.0,1,"2018/03/24, 20:40:09",True,"2018/03/25, 14:11:43",nan,607038.0,3355.0,0,512,Register bean with Grails for MeterRegistryCustomizer,"However, I'm not able to register it in Grails 3."
Datadog,49468467,49475152.0,1,"2018/03/24, 20:40:09",True,"2018/03/25, 14:11:43",nan,607038.0,3355.0,0,512,Register bean with Grails for MeterRegistryCustomizer,Not in  resource.groovy  nor in application main class  Application.groovy .
Datadog,49468467,49475152.0,1,"2018/03/24, 20:40:09",True,"2018/03/25, 14:11:43",nan,607038.0,3355.0,0,512,Register bean with Grails for MeterRegistryCustomizer,Is there any way how to set this in Grails 3?
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,I try to use a wrapper Chef recipe to read Datadog API keys from an encrypted data bag and override node default attribute.
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,My confusion here is with  Chef::EncryptedDataBagItem.load method use.
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,I created an encrypted bag with name datadog with an item datadog_keys inside of it.
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,I would like to get api key and app key from inside of this data bag item.
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,So I'm using:
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,"My question, this usage is it correct or should I use:"
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,"Chef::EncryptedDataBagItem.load(""datadog_keys"", ""api_key"")"
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,or
Datadog,48892401,48892885.0,1,"2018/02/20, 20:59:14",True,"2018/02/20, 21:33:25",nan,2103312.0,638.0,0,551,Chef::EncryptedDataBagItem.load method use,"Chef::EncryptedDataBagItem.load(""datadog::datadog_keys"", ""api_key"")"
Datadog,47779576,nan,1,"2017/12/12, 20:48:50",False,"2017/12/12, 21:09:55",nan,1831118.0,3545.0,0,685,Why isn&#39;t netcat udp message being recieved by netcat listener?,I have netcat listening for udp traffic on port 8125 in terminal 1
Datadog,47779576,nan,1,"2017/12/12, 20:48:50",False,"2017/12/12, 21:09:55",nan,1831118.0,3545.0,0,685,Why isn&#39;t netcat udp message being recieved by netcat listener?,nc -ul 8125
Datadog,47779576,nan,1,"2017/12/12, 20:48:50",False,"2017/12/12, 21:09:55",nan,1831118.0,3545.0,0,685,Why isn&#39;t netcat udp message being recieved by netcat listener?,and in terminal 2 I run the following (a test dogstatsd message for troubleshooting a datadog client connection):
Datadog,47779576,nan,1,"2017/12/12, 20:48:50",False,"2017/12/12, 21:09:55",nan,1831118.0,3545.0,0,685,Why isn&#39;t netcat udp message being recieved by netcat listener?,"I would expect to see  test_metric:1|c  show up in the output of terminal 1, but there is no output at all."
Datadog,47779576,nan,1,"2017/12/12, 20:48:50",False,"2017/12/12, 21:09:55",nan,1831118.0,3545.0,0,685,Why isn&#39;t netcat udp message being recieved by netcat listener?,Can you help me understand why the udp message is not showing up and how to successfully send the udp message?
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,I'm running DataDog agent as a container within my AWS CoreOS instance.
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,This is done via running dd-agent as a container.
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,To automate this I have written a systemd unit for enabling and running data dog agent within AWS CoreOS instance.
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,But none of the metrics are being sent into the DataDog side.
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,But the Docker container is running without any issue.
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,Here is my Systemd unit file
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,EDIT - Adding More Info
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,Initially when I ran this on a single CoreOS instance I was able to see docker related metrics of the instance within DataDog dashboard.
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,Then I enabled this on multiple CoreOS AWS instances.
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,From that point on wards none of the metrics which are related to the CoreOS instances or Docker containers are not visible.
Datadog,46224998,nan,0,"2017/09/14, 20:20:40",False,"2017/09/15, 08:42:44","2017/09/15, 08:42:44",4334340.0,3206.0,0,681,Data dog agent doesn&#39;t send any metrics when running in container mode - CoreOS,EDIT - Adding docker logs
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,"I have three slaves (jmeter-servers) running on EC2 instances, and in one case – (1) JMeter GUI on a local laptop, on another – same test plan (2) running from a command line on yet another EC2 instance."
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,"In case of GUI I can see all the aggregated numbers for Throughput, 99%, etc."
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,"in – well, GUI."
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,I'm creating a jtl file with Aggregate Report listener.
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,"From watching Datadog charts monitoring the application server parameters (CPU usage, memory, etc.)"
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,"I see that in case of a command line and everything on EC2 load is more than twice higher than when my local laptop is communicating with the jmeter-servers, meaning probably that the network becomes a bottleneck."
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,So I want to run everything on EC2.
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,But then – how do I get access to the same aggregated numbers when I'm running from the command line when all four machines are EC2 instances?
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,"The huge jtl file contains records for each transaction, not the aggregated one line of the entire run result."
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,On an attempt to download that jtl from EC2 and open it in GUI on a local laptop it generates some error instead of showing aggregated data.
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,Am I using a wrong listener to get to the summary data?
Datadog,36024447,nan,1,"2016/03/16, 01:56:39",False,"2016/03/16, 19:40:27",nan,966021.0,1477.0,0,1405,JMeter distributed testing - how to get aggregated report?,"(Tried Summary Report – it creates even larger jtl file, not the one line I'm looking for.)"
Datadog,30227260,nan,1,"2015/05/14, 03:12:31",False,"2015/07/10, 15:03:45",nan,2605721.0,329.0,0,923,How to get the version of an installed program in windows using chef.,I'm trying to get the version of program that is installed on a windows server and I want it as a variable inside the recipe.
Datadog,30227260,nan,1,"2015/05/14, 03:12:31",False,"2015/07/10, 15:03:45",nan,2605721.0,329.0,0,923,How to get the version of an installed program in windows using chef.,Basically I'm trying to find the version and if it is not what I want it will be removed and the correct version of the program will be installed.
Datadog,30227260,nan,1,"2015/05/14, 03:12:31",False,"2015/07/10, 15:03:45",nan,2605721.0,329.0,0,923,How to get the version of an installed program in windows using chef.,I can't figure out a way to get the version though.
Datadog,30227260,nan,1,"2015/05/14, 03:12:31",False,"2015/07/10, 15:03:45",nan,2605721.0,329.0,0,923,How to get the version of an installed program in windows using chef.,The program I want the version for is the Datadog agent.
Datadog,65042688,65043190.0,1,"2020/11/27, 21:12:03",True,"2020/11/28, 07:37:00","2020/11/28, 07:37:00",9702194.0,23.0,-1,59,socket: too many open files Error for goroutines in indefinite loop,I have a requirement in my program to send metrics to datadog indefinitely (for continuous app monitoring in datadog).
Datadog,65042688,65043190.0,1,"2020/11/27, 21:12:03",True,"2020/11/28, 07:37:00","2020/11/28, 07:37:00",9702194.0,23.0,-1,59,socket: too many open files Error for goroutines in indefinite loop,The program runs for a while and exits with the error &quot;dial udp 127.0.0.1:18125: socket: too many open files&quot;.
Datadog,65042688,65043190.0,1,"2020/11/27, 21:12:03",True,"2020/11/28, 07:37:00","2020/11/28, 07:37:00",9702194.0,23.0,-1,59,socket: too many open files Error for goroutines in indefinite loop,The app names are read from a config.toml file
Datadog,49457370,49457754.0,1,"2018/03/23, 21:49:55",True,"2018/03/24, 05:00:40",nan,4139285.0,137.0,-1,52,Powershell if/else based on repadmin results?,"I'm trying to figure out how to take the results of ""repadmin /syncall /d /e"" and put the results into an if else statement."
Datadog,49457370,49457754.0,1,"2018/03/23, 21:49:55",True,"2018/03/24, 05:00:40",nan,4139285.0,137.0,-1,52,Powershell if/else based on repadmin results?,"I've considered trying to just look for the success string it outputs for the if and, but I'm wondering if there is a more official way to pull the status code?"
Datadog,49457370,49457754.0,1,"2018/03/23, 21:49:55",True,"2018/03/24, 05:00:40",nan,4139285.0,137.0,-1,52,Powershell if/else based on repadmin results?,So if successful use some built in PowerShell feature to know the status is successful.
Datadog,49457370,49457754.0,1,"2018/03/23, 21:49:55",True,"2018/03/24, 05:00:40",nan,4139285.0,137.0,-1,52,Powershell if/else based on repadmin results?,I'm doing this so I can publish a metric to DataDog giving a pass or fail count for cross-site AD Replications.
Datadog,49457370,49457754.0,1,"2018/03/23, 21:49:55",True,"2018/03/24, 05:00:40",nan,4139285.0,137.0,-1,52,Powershell if/else based on repadmin results?,Any ideas?
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","In Linux shell, how to use regex to filter output of other command."
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","like in cisco devices we use sh ver | b interface, which will dispaly info about int only."
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","my requirement is filter output of below command to display from ""Dogstatsd (v 5.12.0)"" and status date &amp; time."
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.",So that i can use this o/p with certain criteria to write a script to auto restart the agent.
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Status date: 2017-05-30 08:20:13 (17s ago)
  Pid: 7864
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/collector.log, syslog:/dev/log"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Clocks
  ======"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","NTP offset: 0.018 s
    System UTC time: 2017-05-30 06:20:31.535928"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Paths
  ====="
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","conf.d: /etc/dd-agent/conf.d
    checks.d: /opt/datadog-agent/agent/checks.d"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Hostnames
  ========="
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","socket-hostname: adcd
    hostname: adcd
    socket-fqdn: adcd"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Checks
  ======"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","apache (5.0)
    ---------------
      - instance #0 [OK]
      - Collected 12 metrics, 0 events &amp; 1 service check"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","network (5.0)
    ----------------
      - instance #0 [OK]
      - Collected 16 metrics, 0 events &amp; 0 service checks"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","directory (5.0)
    ------------------
      - instance #0 [OK]
      - Collected 17 metrics, 0 events &amp; 0 service checks"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","ntp (5.0)
    ------------
      - Collected 0 metrics, 0 events &amp; 0 service checks"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","disk (5.0)
    -------------
      - instance #0 [OK]
      - Collected 24 metrics, 0 events &amp; 0 service checks"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Emitters
  ========"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.",====================
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Status date: 2017-05-30 08:20:24 (7s ago)
  Pid: 7859
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/dogstatsd.log, syslog:/dev/log"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Flush count: 583466
  Packet Count: 333155
  Packets per second: 0.0
  Metric count: 1
  Event count: 0
  Service check count: 1"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.",====================
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Status date: 2017-05-30 08:20:29 (2s ago)
  Pid: 8868
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/forwarder.log, syslog:/dev/log"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.","Queue Size: 422 bytes
  Queue Length: 1
  Flush Count: 1102592
  Transactions received: 879956
  Transactions flushed: 879955
  Transactions rejected: 0
  API Key Status: API Key is valid"
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.",======================
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.",Not running (port 8126)
Datadog,44258173,nan,1,"2017/05/30, 12:16:22",False,"2017/05/30, 17:28:04",nan,8085386.0,1.0,-4,353,"In Linux shell, how to use regex to filter output of other command.",root@adcd:~#
Datadog,42556276,42575657.0,1,"2017/03/02, 15:03:14",True,"2017/10/24, 22:14:29","2017/10/24, 22:14:29",3062659.0,1515.0,-4,645,Monitoring Tomcat processes CPU spikes,"We have several Tomcat servers (in AWS) running under Debian, we have all of them instrumented with Cloudwatch metrics for overall performance (Memory, CPU and others)."
Datadog,42556276,42575657.0,1,"2017/03/02, 15:03:14",True,"2017/10/24, 22:14:29","2017/10/24, 22:14:29",3062659.0,1515.0,-4,645,Monitoring Tomcat processes CPU spikes,"We've detected that in a few of them we have ""spikes"" of either CPU or Memory utilization, and we'd like do detect what is actually clogging those resources."
Datadog,42556276,42575657.0,1,"2017/03/02, 15:03:14",True,"2017/10/24, 22:14:29","2017/10/24, 22:14:29",3062659.0,1515.0,-4,645,Monitoring Tomcat processes CPU spikes,"As all the server runs is java based inside a Tomcat container, the logical would be to hook up some kind of JVM profiler and visually monitor the threads in it, but as we do have Cloudwatch alerts enabled when exceeding a certain threshold (for example CPU over 90%), we'd like to trigger some kind of automated stats collection to see what actual Java thread/code is the root cause of such consumption."
Datadog,42556276,42575657.0,1,"2017/03/02, 15:03:14",True,"2017/10/24, 22:14:29","2017/10/24, 22:14:29",3062659.0,1515.0,-4,645,Monitoring Tomcat processes CPU spikes,Is there any monitoring agent and/or performance collection tool that might help to diagnose those specific spikes and not needing to collect stats for an actually long running process?
Datadog,42556276,42575657.0,1,"2017/03/02, 15:03:14",True,"2017/10/24, 22:14:29","2017/10/24, 22:14:29",3062659.0,1515.0,-4,645,Monitoring Tomcat processes CPU spikes,"We've already tried trial versions of New Relic, DataDog, and Dynatrace (the latest being the most useful, prohibitively expensive due to its business model not suitable for small companies."
Datadog,42556276,42575657.0,1,"2017/03/02, 15:03:14",True,"2017/10/24, 22:14:29","2017/10/24, 22:14:29",3062659.0,1515.0,-4,645,Monitoring Tomcat processes CPU spikes,"), but these solutions gather EVERYTHING, not only required timing windows, as I've asked above...these could work but introduces quite an overhead to the servers if being used 100% time in production servers (where the problem arises and, not in pre-production ones."
Datadog,42556276,42575657.0,1,"2017/03/02, 15:03:14",True,"2017/10/24, 22:14:29","2017/10/24, 22:14:29",3062659.0,1515.0,-4,645,Monitoring Tomcat processes CPU spikes,).
Elastic APM,53428661,nan,1,"2018/11/22, 12:17:40",True,"2019/03/06, 13:22:41",nan,10244234.0,362.0,3,1220,Elastic APM - RUM JS Agent: Integration with React-Redux applications,Are there any npm modules/plugins exists so as integrate Elastic APM's RUM JS Agent with the application ?
Elastic APM,53428661,nan,1,"2018/11/22, 12:17:40",True,"2019/03/06, 13:22:41",nan,10244234.0,362.0,3,1220,Elastic APM - RUM JS Agent: Integration with React-Redux applications,"By integration i mean good coupling with application so as to record all the things /events(such as route loading, all service requests, actions etc.)"
Elastic APM,53428661,nan,1,"2018/11/22, 12:17:40",True,"2019/03/06, 13:22:41",nan,10244234.0,362.0,3,1220,Elastic APM - RUM JS Agent: Integration with React-Redux applications,and provide it to elastic search.
Elastic APM,56589217,nan,0,"2019/06/14, 01:18:38",False,"2019/11/07, 11:26:55",nan,1230961.0,2029.0,3,413,Elastic APM - Linking to transaction logs,I am using the elastic apm java agent to publish tracing information to an elastic apm server.
Elastic APM,56589217,nan,0,"2019/06/14, 01:18:38",False,"2019/11/07, 11:26:55",nan,1230961.0,2029.0,3,413,Elastic APM - Linking to transaction logs,"I am setting the  enable_log_correlation  property to true which makes the span, trace and transaction ids available in the MDC and using logstash to capture all that in an elastic index."
Elastic APM,56589217,nan,0,"2019/06/14, 01:18:38",False,"2019/11/07, 11:26:55",nan,1230961.0,2029.0,3,413,Elastic APM - Linking to transaction logs,The logs are visible in the Discover tab in elastic and tracing information is visible in the APM tab.
Elastic APM,56589217,nan,0,"2019/06/14, 01:18:38",False,"2019/11/07, 11:26:55",nan,1230961.0,2029.0,3,413,Elastic APM - Linking to transaction logs,I have seen in some screenshots a link on the APM   Transactions page to  View Transaction in Discover  to be able the see all logs related to a single transaction.
Elastic APM,56589217,nan,0,"2019/06/14, 01:18:38",False,"2019/11/07, 11:26:55",nan,1230961.0,2029.0,3,413,Elastic APM - Linking to transaction logs,This link is not showing up for me.
Elastic APM,56589217,nan,0,"2019/06/14, 01:18:38",False,"2019/11/07, 11:26:55",nan,1230961.0,2029.0,3,413,Elastic APM - Linking to transaction logs,The span link  Show span in Discover  is showing up and it takes you to Discover and opens up the apm index for this span.
Elastic APM,56589217,nan,0,"2019/06/14, 01:18:38",False,"2019/11/07, 11:26:55",nan,1230961.0,2029.0,3,413,Elastic APM - Linking to transaction logs,So two questions:
Elastic APM,56589217,nan,0,"2019/06/14, 01:18:38",False,"2019/11/07, 11:26:55",nan,1230961.0,2029.0,3,413,Elastic APM - Linking to transaction logs,Thanks!
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,Kibana response is
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,"APM server returns 503 - Internal Server Error, 
Having hard time identifying the root cause."
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,Is it ES queue is full or ran out of memory or cluster is not being setup correctly?
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,According to ES documentation:  https://www.elastic.co/guide/en/apm/server/master/common-problems.html#queue-full
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,"A full queue generally means that the agents collect more data than
  APM server is able to process."
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,"This might happen when APM Server is
  not configured properly for the size of your Elasticsearch cluster, or
  because your Elasticsearch cluster is underpowered or not configured
  properly for the given workload."
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,The queue can also fill up if Elasticsearch runs out of disk space.
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,Documentation doesn't help in identifying what could be the root cause.
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,How do we identify the root cause?
Elastic APM,54623543,nan,1,"2019/02/11, 05:21:28",True,"2020/07/01, 11:25:39","2019/02/11, 05:29:38",2541141.0,625.0,2,6904,Elastic APM HTTP error (503): queue is full,Restarting Kibana and Elasticsearch helps but that does not help to identify the root cause
Elastic APM,58780229,nan,2,"2019/11/09, 16:29:21",False,"2020/05/06, 21:46:03",nan,3593170.0,211.0,2,1934,How to Attach Elastic APM in Spring Application,I am currently doing a PoC to integrate Elastic APM into my spring application.
Elastic APM,58780229,nan,2,"2019/11/09, 16:29:21",False,"2020/05/06, 21:46:03",nan,3593170.0,211.0,2,1934,How to Attach Elastic APM in Spring Application,"I was following this page :-  https://www.elastic.co/guide/en/apm/agent/java/1.x/setup-attach-api.html  
to programatically attach elastic-apm jar."
Elastic APM,58780229,nan,2,"2019/11/09, 16:29:21",False,"2020/05/06, 21:46:03",nan,3593170.0,211.0,2,1934,How to Attach Elastic APM in Spring Application,I have added the required jar into pom.xml but i am not getting how should i attach Elastic Apm (ElasticApmAttacher.attach())into my normal spring code.
Elastic APM,58780229,nan,2,"2019/11/09, 16:29:21",False,"2020/05/06, 21:46:03",nan,3593170.0,211.0,2,1934,How to Attach Elastic APM in Spring Application,Example given is for SpringBoot.
Elastic APM,58780229,nan,2,"2019/11/09, 16:29:21",False,"2020/05/06, 21:46:03",nan,3593170.0,211.0,2,1934,How to Attach Elastic APM in Spring Application,"But my application is on Spring core ( spring-core, spring-web ..) with rest services exposed using Jax-Rs."
Elastic APM,59402562,nan,0,"2019/12/19, 04:31:46",False,"2019/12/19, 08:08:12",nan,5740208.0,2056.0,2,314,Why do we get SyntaxError: Unexpected token in JSON while using elastic-apm-node?,After adding elastic-apm-node on our backend server we receive the below error hundreds of times a day.
Elastic APM,59402562,nan,0,"2019/12/19, 04:31:46",False,"2019/12/19, 08:08:12",nan,5740208.0,2056.0,2,314,Why do we get SyntaxError: Unexpected token in JSON while using elastic-apm-node?,Can anyone suggest what might cause this?
Elastic APM,59402562,nan,0,"2019/12/19, 04:31:46",False,"2019/12/19, 08:08:12",nan,5740208.0,2056.0,2,314,Why do we get SyntaxError: Unexpected token in JSON while using elastic-apm-node?,The error we see is this:
Elastic APM,59402562,nan,0,"2019/12/19, 04:31:46",False,"2019/12/19, 08:08:12",nan,5740208.0,2056.0,2,314,Why do we get SyntaxError: Unexpected token in JSON while using elastic-apm-node?,I think we run node v10.x but I'm not absolutely sure.
Elastic APM,59402562,nan,0,"2019/12/19, 04:31:46",False,"2019/12/19, 08:08:12",nan,5740208.0,2056.0,2,314,Why do we get SyntaxError: Unexpected token in JSON while using elastic-apm-node?,The service is built on feathersJS which again is built on Express.
Elastic APM,59402562,nan,0,"2019/12/19, 04:31:46",False,"2019/12/19, 08:08:12",nan,5740208.0,2056.0,2,314,Why do we get SyntaxError: Unexpected token in JSON while using elastic-apm-node?,We start APM like this:
Elastic APM,59402562,nan,0,"2019/12/19, 04:31:46",False,"2019/12/19, 08:08:12",nan,5740208.0,2056.0,2,314,Why do we get SyntaxError: Unexpected token in JSON while using elastic-apm-node?,The dependencies listed in package.json are:
Elastic APM,61098374,61101616.0,1,"2020/04/08, 13:20:55",True,"2020/04/08, 16:15:18",nan,3026997.0,625.0,2,253,NestJS/Elastic apm: agent does not record database query spans,I'm using  elastic apm  to profiling my  NestJS  application and my apm agent is  elastic-apm-node .
Elastic APM,61098374,61101616.0,1,"2020/04/08, 13:20:55",True,"2020/04/08, 16:15:18",nan,3026997.0,625.0,2,253,NestJS/Elastic apm: agent does not record database query spans,My ORM is  typeOrm  and my database is  Oracle .
Elastic APM,61098374,61101616.0,1,"2020/04/08, 13:20:55",True,"2020/04/08, 16:15:18",nan,3026997.0,625.0,2,253,NestJS/Elastic apm: agent does not record database query spans,My problem is apm agent does not record database query spans and I can't see  database query spans  in kibana ui.
Elastic APM,61098374,61101616.0,1,"2020/04/08, 13:20:55",True,"2020/04/08, 16:15:18",nan,3026997.0,625.0,2,253,NestJS/Elastic apm: agent does not record database query spans,Can anyone help me?
Elastic APM,62511256,62516533.0,1,"2020/06/22, 12:25:28",True,"2020/06/22, 17:17:56","2020/06/22, 14:08:48",13774853.0,33.0,2,285,elastic apm for elixir and use of opentelemtry,We want to track our elixir phoenix app  using elastic apm.
Elastic APM,62511256,62516533.0,1,"2020/06/22, 12:25:28",True,"2020/06/22, 17:17:56","2020/06/22, 14:08:48",13774853.0,33.0,2,285,elastic apm for elixir and use of opentelemtry,But I could not find an apm agent from elastic.
Elastic APM,62511256,62516533.0,1,"2020/06/22, 12:25:28",True,"2020/06/22, 17:17:56","2020/06/22, 14:08:48",13774853.0,33.0,2,285,elastic apm for elixir and use of opentelemtry,Someone suggested to use  opentelemetry  along with exporter but I am unable to understand how to use that from the docs.
Elastic APM,62511256,62516533.0,1,"2020/06/22, 12:25:28",True,"2020/06/22, 17:17:56","2020/06/22, 14:08:48",13774853.0,33.0,2,285,elastic apm for elixir and use of opentelemtry,I want to track details like the new relic does like errors and all things.
Elastic APM,62511256,62516533.0,1,"2020/06/22, 12:25:28",True,"2020/06/22, 17:17:56","2020/06/22, 14:08:48",13774853.0,33.0,2,285,elastic apm for elixir and use of opentelemtry,Previously we used new relic for which there is an open source apm agent but now we want to switch to elastic.I am unable to understand how to use span in the app and how to handle multiple span and where to put them.
Elastic APM,62511256,62516533.0,1,"2020/06/22, 12:25:28",True,"2020/06/22, 17:17:56","2020/06/22, 14:08:48",13774853.0,33.0,2,285,elastic apm for elixir and use of opentelemtry,If anyone can help with that or provide alternate solution to use elastic apm it would be great.
Elastic APM,63573207,63573627.0,1,"2020/08/25, 09:33:22",True,"2020/08/25, 10:04:20",nan,433570.0,32819.0,2,192,"Elastic apm, what is transaction.duration.us?",I'd like to measure avg request-response time for my webserver.
Elastic APM,63573207,63573627.0,1,"2020/08/25, 09:33:22",True,"2020/08/25, 10:04:20",nan,433570.0,32819.0,2,192,"Elastic apm, what is transaction.duration.us?",Apm has  transaction.duration.us  and it seems this could be the metric I'm looking for.
Elastic APM,63573207,63573627.0,1,"2020/08/25, 09:33:22",True,"2020/08/25, 10:04:20",nan,433570.0,32819.0,2,192,"Elastic apm, what is transaction.duration.us?",But I coulnd't find the documentation what it is.
Elastic APM,63573207,63573627.0,1,"2020/08/25, 09:33:22",True,"2020/08/25, 10:04:20",nan,433570.0,32819.0,2,192,"Elastic apm, what is transaction.duration.us?",Where can I find the meaning of the variable?
Elastic APM,49551764,nan,0,"2018/03/29, 11:53:19",False,"2018/03/29, 15:02:46","2018/03/29, 15:02:46",8046113.0,89.0,1,366,Elastic APM agent is inactive due to configuration. What does it mean?,"I have a problem when I want to start the app, when i type  npm run server ."
Elastic APM,49551764,nan,0,"2018/03/29, 11:53:19",False,"2018/03/29, 15:02:46","2018/03/29, 15:02:46",8046113.0,89.0,1,366,Elastic APM agent is inactive due to configuration. What does it mean?,And the error information are like this :
Elastic APM,49551764,nan,0,"2018/03/29, 11:53:19",False,"2018/03/29, 15:02:46","2018/03/29, 15:02:46",8046113.0,89.0,1,366,Elastic APM agent is inactive due to configuration. What does it mean?,"Elastic APM agent is inactive due to configuration, what does it mean?"
Elastic APM,49551764,nan,0,"2018/03/29, 11:53:19",False,"2018/03/29, 15:02:46","2018/03/29, 15:02:46",8046113.0,89.0,1,366,Elastic APM agent is inactive due to configuration. What does it mean?,"Maybe someone can help me, please..."
Elastic APM,49551764,nan,0,"2018/03/29, 11:53:19",False,"2018/03/29, 15:02:46","2018/03/29, 15:02:46",8046113.0,89.0,1,366,Elastic APM agent is inactive due to configuration. What does it mean?,This is the package.json :
Elastic APM,49551764,nan,0,"2018/03/29, 11:53:19",False,"2018/03/29, 15:02:46","2018/03/29, 15:02:46",8046113.0,89.0,1,366,Elastic APM agent is inactive due to configuration. What does it mean?,"{ 
    ""name"": ""opbeans"", 
    ""version"": ""1.0.0"", 
    ""description"": ""The Opbeans inventory management system"", 
    ""main"": ""server.js"", 
    ""dependencies"": { 
      &nbsp;&nbsp;&nbsp;""after-all-results"": ""^2.0.0"", 
      &nbsp;&nbsp;&nbsp;""body-parser"": ""^1.15.2"", 
      &nbsp;&nbsp;&nbsp;""concurrently"": ""^3.1.0"", 
      &nbsp;&nbsp;&nbsp;""detect-port"": ""^1.2.2"", 
      &nbsp;&nbsp;&nbsp;""elastic-apm-node"": ""^1.3.0"", 
      &nbsp;&nbsp;&nbsp;""express"": ""^4.14.0"", 
      &nbsp;&nbsp;&nbsp;""pg"": ""^6.1.2"", 
      &nbsp;&nbsp;&nbsp;""redis"": ""^2.6.3"", 
      &nbsp;&nbsp;&nbsp;""request"": ""^2.79.0"", 
      &nbsp;&nbsp;&nbsp;""webpack"": ""^4.3.0"", 
      &nbsp;&nbsp;&nbsp;""webpack-dev-server"": ""^3.1.1"", 
      &nbsp;&nbsp;&nbsp;""workload"": ""^2.3.0"" 
    }, 
    ""devDependencies"": { 
      &nbsp;&nbsp;&nbsp;""dotenv"": ""^4.0.0"", 
      &nbsp;&nbsp;&nbsp;""react-dev-utils"": ""^5.0.0"", 
      &nbsp;&nbsp;&nbsp;""react-scripts"": ""^1.1.1"", 
      &nbsp;&nbsp;&nbsp;""standard"": ""^10.0.2"" 
    }, 
    ""scripts"": { 
      &nbsp;&nbsp;&nbsp;""db-setup"": ""./db/setup.sh"", 
      &nbsp;&nbsp;&nbsp;""test"": ""standard  .js server/ / .js db/ /*.js"", 
      &nbsp;&nbsp;&nbsp;""server"": ""node server.js"", 
      &nbsp;&nbsp;&nbsp;""client"": ""npm run start --prefix client/"", 
      &nbsp;&nbsp;&nbsp;""client-build"": ""npm run build --prefix client/"", 
      &nbsp;&nbsp;&nbsp;""client-install"": ""npm install --prefix client/"", 
      &nbsp;&nbsp;&nbsp;""postinstall"": ""./client/build-client.sh"", 
      &nbsp;&nbsp;&nbsp;""start"": ""concurrently \""npm run server\"" \""npm run &nbsp;&nbsp;&nbsp;client\"""", 
      &nbsp;&nbsp;&nbsp;""workload"": ""workload -f .workload.js"" 
    }"
Elastic APM,50282965,nan,2,"2018/05/11, 02:40:57",True,"2018/08/02, 12:19:26","2020/06/20, 12:12:55",8267880.0,87.0,1,1681,Elastic APM Stack on Docker,"im trying to install the Elastic APM with Elasticsearch, Kibana and the APM server as 3 services with docker-compose."
Elastic APM,50282965,nan,2,"2018/05/11, 02:40:57",True,"2018/08/02, 12:19:26","2020/06/20, 12:12:55",8267880.0,87.0,1,1681,Elastic APM Stack on Docker,Now im getting confused on how to set the IPs in the app-server.yml file with the documentation  APM Server Configuration .
Elastic APM,50282965,nan,2,"2018/05/11, 02:40:57",True,"2018/08/02, 12:19:26","2020/06/20, 12:12:55",8267880.0,87.0,1,1681,Elastic APM Stack on Docker,The file should look like this:
Elastic APM,50282965,nan,2,"2018/05/11, 02:40:57",True,"2018/08/02, 12:19:26","2020/06/20, 12:12:55",8267880.0,87.0,1,1681,Elastic APM Stack on Docker,"I tried to set ElasticsearchAddress to localhost or 127.0.0.1 but I always get errors like
 Failed to connect: Get http://127.0.0.1:9200: dial tcp 127.0.0.1:9200: getsockopt: connection refused  or  Failed to connect: Get http://localhost:9200: dial tcp [::1]:9200: connect: cannot assign requested address ."
Elastic APM,50282965,nan,2,"2018/05/11, 02:40:57",True,"2018/08/02, 12:19:26","2020/06/20, 12:12:55",8267880.0,87.0,1,1681,Elastic APM Stack on Docker,I also tried it with several other ips.
Elastic APM,50282965,nan,2,"2018/05/11, 02:40:57",True,"2018/08/02, 12:19:26","2020/06/20, 12:12:55",8267880.0,87.0,1,1681,Elastic APM Stack on Docker,Does anyone know how to configure the app server correctly or are there any docker-compose files to do the installation correctly?
Elastic APM,50282965,nan,2,"2018/05/11, 02:40:57",True,"2018/08/02, 12:19:26","2020/06/20, 12:12:55",8267880.0,87.0,1,1681,Elastic APM Stack on Docker,Thanks for ur help
Elastic APM,54418073,nan,0,"2019/01/29, 11:45:03",False,"2019/01/29, 11:45:03",nan,7807754.0,355.0,1,111,How to add elastic-apm-agent.jar to PCF?,I have downloaded elastic-apm-agent.jar on my local which is monitoring locally deployed spring boot micro service.
Elastic APM,54418073,nan,0,"2019/01/29, 11:45:03",False,"2019/01/29, 11:45:03",nan,7807754.0,355.0,1,111,How to add elastic-apm-agent.jar to PCF?,Now I want to add same jar to PCF.
Elastic APM,56665160,nan,1,"2019/06/19, 13:07:24",True,"2019/06/24, 21:20:03",nan,9619535.0,528.0,1,699,"elastic-apm-node with Webpack, Typescript &amp; ES6 not working?",I am using a TypeScript setup with webpack and babel and get the following error when trying to include  elastic-apm-node .
Elastic APM,56665160,nan,1,"2019/06/19, 13:07:24",True,"2019/06/24, 21:20:03",nan,9619535.0,528.0,1,699,"elastic-apm-node with Webpack, Typescript &amp; ES6 not working?",I have the settings in environment variables.
Elastic APM,56665160,nan,1,"2019/06/19, 13:07:24",True,"2019/06/24, 21:20:03",nan,9619535.0,528.0,1,699,"elastic-apm-node with Webpack, Typescript &amp; ES6 not working?",Errors:
Elastic APM,56665160,nan,1,"2019/06/19, 13:07:24",True,"2019/06/24, 21:20:03",nan,9619535.0,528.0,1,699,"elastic-apm-node with Webpack, Typescript &amp; ES6 not working?",Any idea how I can prevent this?
Elastic APM,56665160,nan,1,"2019/06/19, 13:07:24",True,"2019/06/24, 21:20:03",nan,9619535.0,528.0,1,699,"elastic-apm-node with Webpack, Typescript &amp; ES6 not working?","When using just TypeScript with Meteor I don't get the errors, so I think it is connected to Babel / Webpack."
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,I have setup of Elastic with APM server on single machine.
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,I've configured APM java agent to push traces to APM server on localhost.
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,Everything works fine with localhost configuration on Windows.
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,"Now, I'm looking to run apm java agent for application running on different machine on the same network."
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,That is apm java agent on linux &amp; apm server running on windows machine.
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,Default APM-server listen to localhost.
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,I tried to change setting on apm-server.yml file with -
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,default is:
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,"After making apm-server.yml change, process explorer show apm-server.exe process listening to IP- host-ip port- 8200 protocol- TCP."
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,"But, still  http://host-ip:8200  is not accessible from other machine on network."
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,While on the same machine (windows)  http://localhost:8200  &amp;  http://host-ip:8200  works fine &amp; give below response.
Elastic APM,56989849,nan,2,"2019/07/11, 16:00:29",True,"2019/11/05, 20:16:57","2019/07/12, 11:23:10",11770223.0,11.0,1,1329,How do I enable remote access/request to Elastic APM-server?,Thanks for help.
Elastic APM,57211703,57257482.0,1,"2019/07/26, 03:36:52",True,"2020/05/16, 21:59:41","2019/07/26, 03:52:26",6713596.0,1441.0,1,783,Elastic APM Opentracing encountering CORS issue with Docker apm-server,"Given this docker file to setup the backend services that includes: elasticsearch, apm-server, kibana, jaeger-collector, jaeger-agent, jaeger-query, grafana."
Elastic APM,57211703,57257482.0,1,"2019/07/26, 03:36:52",True,"2020/05/16, 21:59:41","2019/07/26, 03:52:26",6713596.0,1441.0,1,783,Elastic APM Opentracing encountering CORS issue with Docker apm-server,I am running Elastic APM with Opentracing from my Angular client:
Elastic APM,57211703,57257482.0,1,"2019/07/26, 03:36:52",True,"2020/05/16, 21:59:41","2019/07/26, 03:52:26",6713596.0,1441.0,1,783,Elastic APM Opentracing encountering CORS issue with Docker apm-server,I am encountering CORS issue:
Elastic APM,57211703,57257482.0,1,"2019/07/26, 03:36:52",True,"2020/05/16, 21:59:41","2019/07/26, 03:52:26",6713596.0,1441.0,1,783,Elastic APM Opentracing encountering CORS issue with Docker apm-server,My goal is to hook up Angular the elastic APM's opentracing client to the services inside docker.
Elastic APM,57211703,57257482.0,1,"2019/07/26, 03:36:52",True,"2020/05/16, 21:59:41","2019/07/26, 03:52:26",6713596.0,1441.0,1,783,Elastic APM Opentracing encountering CORS issue with Docker apm-server,There are some additional issues and documents that covers CORS for apm-server:
Elastic APM,57211703,57257482.0,1,"2019/07/26, 03:36:52",True,"2020/05/16, 21:59:41","2019/07/26, 03:52:26",6713596.0,1441.0,1,783,Elastic APM Opentracing encountering CORS issue with Docker apm-server,Distributed Tracing Guide
Elastic APM,57211703,57257482.0,1,"2019/07/26, 03:36:52",True,"2020/05/16, 21:59:41","2019/07/26, 03:52:26",6713596.0,1441.0,1,783,Elastic APM Opentracing encountering CORS issue with Docker apm-server,Config with RUM enabled
Elastic APM,57211703,57257482.0,1,"2019/07/26, 03:36:52",True,"2020/05/16, 21:59:41","2019/07/26, 03:52:26",6713596.0,1441.0,1,783,Elastic APM Opentracing encountering CORS issue with Docker apm-server,"It looks like the config should work, since  Default value is set to ['*'], which allows everything."
Elastic APM,57696669,61340495.0,1,"2019/08/28, 19:31:21",True,"2020/04/21, 12:44:16",nan,6685358.0,1537.0,1,257,Set (custom) fields in Elastic APM for Python with Flask,"I am quite new to Elastic APM, Kibana, Elasticsearch and APM in general and did not come across any information pointing towards my needs."
Elastic APM,57696669,61340495.0,1,"2019/08/28, 19:31:21",True,"2020/04/21, 12:44:16",nan,6685358.0,1537.0,1,257,Set (custom) fields in Elastic APM for Python with Flask,I set up the  elastic-apm[flask]  module and followed the tutorial.
Elastic APM,57696669,61340495.0,1,"2019/08/28, 19:31:21",True,"2020/04/21, 12:44:16",nan,6685358.0,1537.0,1,257,Set (custom) fields in Elastic APM for Python with Flask,"In the Kibana dashboard I get information like response times and server name, but the fields for client.ip etc."
Elastic APM,57696669,61340495.0,1,"2019/08/28, 19:31:21",True,"2020/04/21, 12:44:16",nan,6685358.0,1537.0,1,257,Set (custom) fields in Elastic APM for Python with Flask,are empty.
Elastic APM,57696669,61340495.0,1,"2019/08/28, 19:31:21",True,"2020/04/21, 12:44:16",nan,6685358.0,1537.0,1,257,Set (custom) fields in Elastic APM for Python with Flask,I would like to track the IP addresses (more exactly where my website visitors come from).
Elastic APM,57696669,61340495.0,1,"2019/08/28, 19:31:21",True,"2020/04/21, 12:44:16",nan,6685358.0,1537.0,1,257,Set (custom) fields in Elastic APM for Python with Flask,"So, how do I get the user's IP address into the client.ip field in Elastic APM?"
Elastic APM,57696669,61340495.0,1,"2019/08/28, 19:31:21",True,"2020/04/21, 12:44:16",nan,6685358.0,1537.0,1,257,Set (custom) fields in Elastic APM for Python with Flask,I don't want to issue an  app.logger.debug  statement everytime a route is being requested.
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,I'm currently checking why the APM UI in Kibana doesn't output me the information when the timeframe is set to more than 24 hours.
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,When checking the config I did notice that we haven't had the Kibana endpoint set for the APM server.
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,Checking the APM logs I can't see an error but when going to the APM UI I can find this error in the Kibana logs:
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,From the output it looks like that Kibana can't find any server configuration but the setup was done successfully but I can't as well access the APM settings that were moved in Kibana (timeout with error 404).
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,Other queries or the query from the same indices work well without any increased delay.
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,The APM agent is the latest version of the Django (Python) agent.
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,"And the resource metrics from clusters as well as status are really good, so the cluster should be powerful enough."
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,We have had as well testing deploy in Kubernetes and the services sometimes show up in the APM UI but it's turned off currently.
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,Environment and versions:
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,"Elasticsearch deployed on GCP in Docker containers (4 nodes on different VMs + 1 VM with APM Server, Kibana, and Elasticsearch client node)."
Elastic APM,59877838,nan,0,"2020/01/23, 13:46:50",False,"2020/01/23, 14:13:54","2020/01/23, 14:13:54",12768421.0,21.0,1,758,Low performance of Elastic APM server,"ES, Kibana, APM-Server versions: 7.3.1"
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?","I'm using Elastic APM, and want to find out how long the garbage collector has been running during a period of time."
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?","This is to understand if the application is running out of memory, which seems more accurate than just checking heap used, as garbage collection could trigger when heap space is limited, and then free up a large amount."
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?","Elastic APM will track  jvm.gc.time , which the Elastic site defines as:"
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?",The approximate accumulated collection elapsed time in milliseconds.
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?",Source
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?",I assumed this meant how much time has been spent garbage collecting since the application started.
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?","My plan was to read this value periodically, and determine how much of the time interval was spent garbage collecting."
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?","When I read this value two different times, it turns out the second, and  later  reading, is actually lower than the first."
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?",First Reading
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?",Second Reading
Elastic APM,60820578,60827426.0,1,"2020/03/23, 21:46:05",True,"2020/03/24, 10:35:00","2020/03/24, 09:04:20",2601060.0,2490.0,1,246,"What exactly is &quot;jvm.gc.time&quot; in Elastic APM, and how to use it?",Can anyone help me understand what  jvm.gc.time  captures?
Elastic APM,60890836,60892441.0,1,"2020/03/27, 19:09:33",True,"2020/03/27, 21:21:59",nan,143327.0,23670.0,1,1414,"Distributed tracing with Elastic APM on .NET Core, non-HTTP","I am interested in using Elastic APM within an ASP.NET Core to instrument traces of a set of services which communicate over a mix of protocols (HTTP, SQS, SNS)."
Elastic APM,60890836,60892441.0,1,"2020/03/27, 19:09:33",True,"2020/03/27, 21:21:59",nan,143327.0,23670.0,1,1414,"Distributed tracing with Elastic APM on .NET Core, non-HTTP","Despite reviewing the documentation, I am not clear how I can use the  Elastic APM Public API  to connect transactions to one another which occur outside of HTTP (HttpClient is automatically instrumented for trace by Elastic APM)."
Elastic APM,60890836,60892441.0,1,"2020/03/27, 19:09:33",True,"2020/03/27, 21:21:59",nan,143327.0,23670.0,1,1414,"Distributed tracing with Elastic APM on .NET Core, non-HTTP","According to the documentation, I should be able to serialize the  CurrentTransaction.OutgoingDistributedTracingData  on the caller and then deserialize it to resume the transaction on the callee, but despite implementing this pattern in memory, my traces in Kibana are missing spans from all but the final transaction."
Elastic APM,60890836,60892441.0,1,"2020/03/27, 19:09:33",True,"2020/03/27, 21:21:59",nan,143327.0,23670.0,1,1414,"Distributed tracing with Elastic APM on .NET Core, non-HTTP",My implementation spike can be found on  Github .
Elastic APM,61065570,61066068.0,1,"2020/04/06, 20:34:29",True,"2020/04/06, 21:03:11",nan,5861620.0,835.0,1,851,elastic apm for c# jobs,I add ElasticAPM to my startup on AspNetCore 3.1
Elastic APM,61065570,61066068.0,1,"2020/04/06, 20:34:29",True,"2020/04/06, 21:03:11",nan,5861620.0,835.0,1,851,elastic apm for c# jobs,"in my project, rest api services logs as transaction tab of kibana-apm."
Elastic APM,61065570,61066068.0,1,"2020/04/06, 20:34:29",True,"2020/04/06, 21:03:11",nan,5861620.0,835.0,1,851,elastic apm for c# jobs,but my background services dos not logged by apm agent &amp; only metrics tab work for me.
Elastic APM,61115721,nan,1,"2020/04/09, 10:09:59",True,"2020/04/13, 23:31:21",nan,8676215.0,167.0,1,237,Elastic: How to enable autodetect on metadata like container id in Elastic APM Agent/APM Server?,"I have set up an elastic stack with elasticsearch, filebeat, kibana and apm server, and an spring-boot-application with the apm java agent and started my setup in a docker compose file."
Elastic APM,61115721,nan,1,"2020/04/09, 10:09:59",True,"2020/04/13, 23:31:21",nan,8676215.0,167.0,1,237,Elastic: How to enable autodetect on metadata like container id in Elastic APM Agent/APM Server?,I have enabled the dashboard and I can see traces about processes in the application.
Elastic APM,61115721,nan,1,"2020/04/09, 10:09:59",True,"2020/04/13, 23:31:21",nan,8676215.0,167.0,1,237,Elastic: How to enable autodetect on metadata like container id in Elastic APM Agent/APM Server?,"But I cannot filter for container id, because there is no id."
Elastic APM,61115721,nan,1,"2020/04/09, 10:09:59",True,"2020/04/13, 23:31:21",nan,8676215.0,167.0,1,237,Elastic: How to enable autodetect on metadata like container id in Elastic APM Agent/APM Server?,"How do I enable my stack apm server/apm agent to receive the metadata about container id, pod id and so on."
Elastic APM,61115721,nan,1,"2020/04/09, 10:09:59",True,"2020/04/13, 23:31:21",nan,8676215.0,167.0,1,237,Elastic: How to enable autodetect on metadata like container id in Elastic APM Agent/APM Server?,Where can I enable metadata for apm server / apm agent to receive the container id for instance.
Elastic APM,61166750,nan,0,"2020/04/12, 07:11:48",False,"2020/04/12, 07:11:48",nan,433570.0,32819.0,1,345,elastic apm can&#39;t connect to elasticsearch,My apm server can't connect to ES with the following log
Elastic APM,61166750,nan,0,"2020/04/12, 07:11:48",False,"2020/04/12, 07:11:48",nan,433570.0,32819.0,1,345,elastic apm can&#39;t connect to elasticsearch,"I tried to 'reset' the index by the following command, it won't work either"
Elastic APM,61166750,nan,0,"2020/04/12, 07:11:48",False,"2020/04/12, 07:11:48",nan,433570.0,32819.0,1,345,elastic apm can&#39;t connect to elasticsearch,"I tried to setup a policy where apm data is deleted after 3 month, and I think I messed up the index setup.. (I can't remember what I did exactly)"
Elastic APM,61166750,nan,0,"2020/04/12, 07:11:48",False,"2020/04/12, 07:11:48",nan,433570.0,32819.0,1,345,elastic apm can&#39;t connect to elasticsearch,How do I reset the index and start using apm again?
Elastic APM,61166750,nan,0,"2020/04/12, 07:11:48",False,"2020/04/12, 07:11:48",nan,433570.0,32819.0,1,345,elastic apm can&#39;t connect to elasticsearch,"(It's a plus if I can retain the data, but I can sacrifice it)"
Elastic APM,61262895,nan,1,"2020/04/17, 04:53:18",False,"2020/04/29, 21:38:55",nan,2553063.0,11.0,1,138,Elastic APM Tomcat zip file or JAR manifest missing,I am trying to instrument our java webapp based on tomcat using Elastic APM.
Elastic APM,61262895,nan,1,"2020/04/17, 04:53:18",False,"2020/04/29, 21:38:55",nan,2553063.0,11.0,1,138,Elastic APM Tomcat zip file or JAR manifest missing,Tomcat starts fine without the javaagent but refuses to start with it.
Elastic APM,61262895,nan,1,"2020/04/17, 04:53:18",False,"2020/04/29, 21:38:55",nan,2553063.0,11.0,1,138,Elastic APM Tomcat zip file or JAR manifest missing,I do not see any logs in /var/log/tomcat.
Elastic APM,61262895,nan,1,"2020/04/17, 04:53:18",False,"2020/04/29, 21:38:55",nan,2553063.0,11.0,1,138,Elastic APM Tomcat zip file or JAR manifest missing,The following is the log:
Elastic APM,61262895,nan,1,"2020/04/17, 04:53:18",False,"2020/04/29, 21:38:55",nan,2553063.0,11.0,1,138,Elastic APM Tomcat zip file or JAR manifest missing,Has anyone faced this before?
Elastic APM,61262895,nan,1,"2020/04/17, 04:53:18",False,"2020/04/29, 21:38:55",nan,2553063.0,11.0,1,138,Elastic APM Tomcat zip file or JAR manifest missing,Any ideas what I could do to fix it?
Elastic APM,62168965,nan,0,"2020/06/03, 12:03:18",False,"2020/06/03, 12:03:18",nan,6696948.0,2890.0,1,194,Elastic APM HTTP error (404): 404 page not found,Getting error while integrating Elastic APM in the Node Express application.
Elastic APM,62168965,nan,0,"2020/06/03, 12:03:18",False,"2020/06/03, 12:03:18",nan,6696948.0,2890.0,1,194,Elastic APM HTTP error (404): 404 page not found,"Nodejs: v6.4.2,"
Elastic APM,62168965,nan,0,"2020/06/03, 12:03:18",False,"2020/06/03, 12:03:18",nan,6696948.0,2890.0,1,194,Elastic APM HTTP error (404): 404 page not found,app.js
Elastic APM,62168965,nan,0,"2020/06/03, 12:03:18",False,"2020/06/03, 12:03:18",nan,6696948.0,2890.0,1,194,Elastic APM HTTP error (404): 404 page not found,package.json
Elastic APM,62168965,nan,0,"2020/06/03, 12:03:18",False,"2020/06/03, 12:03:18",nan,6696948.0,2890.0,1,194,Elastic APM HTTP error (404): 404 page not found,Error:
Elastic APM,62168965,nan,0,"2020/06/03, 12:03:18",False,"2020/06/03, 12:03:18",nan,6696948.0,2890.0,1,194,Elastic APM HTTP error (404): 404 page not found,"You have triggered an unhandledRejection, you may have forgotten to catch a Promise rejection:
  Error: Can't set headers after they are sent."
Elastic APM,62168965,nan,0,"2020/06/03, 12:03:18",False,"2020/06/03, 12:03:18",nan,6696948.0,2890.0,1,194,Elastic APM HTTP error (404): 404 page not found,"at ServerResponse.OutgoingMessage.setHeader (_http_outgoing.js:335:11)
      at /home/ubuntu/depanels/server/api/user/user.controller.js:22:25
      at /home/ubuntu/depanels/server/auth/auth.service.js:149:28
      at _fulfilled (/home/ubuntu/depanels/node_modules/q/q.js:854:54)
      at self.promiseDispatch.done (/home/ubuntu/depanels/node_modules/q/q.js:883:30)
      at Promise.promise.promiseDispatch (/home/ubuntu/depanels/node_modules/q/q.js:816:13)
      at /home/ubuntu/depanels/node_modules/q/q.js:624:44
      at runSingle (/home/ubuntu/depanels/node_modules/q/q.js:137:13)
      at flush (/home/ubuntu/depanels/node_modules/q/q.js:125:13)
      at elasticAPMCallbackWrapper (/home/ubuntu/depanels/node_modules/elastic-apm-node/lib/instrumentation/index.js:191:27)
      at nextTickCallbackWith0Args (node.js:419:9)
      at process._tickDomainCallback [as _tickCallback] (node.js:389:13)
  Elastic APM HTTP error (404): 404 page not found
  Elastic APM HTTP error (404): 404 page not found"
Elastic APM,62168965,nan,0,"2020/06/03, 12:03:18",False,"2020/06/03, 12:03:18",nan,6696948.0,2890.0,1,194,Elastic APM HTTP error (404): 404 page not found,Please suggest where I am doing wrong?
Elastic APM,62394810,nan,0,"2020/06/15, 21:39:12",False,"2020/06/15, 21:46:09","2020/06/15, 21:46:09",4429741.0,21.0,1,148,elastic-apm-node not register span after express responde,I need to send span to elastic-apm after sending express response.
Elastic APM,62394810,nan,0,"2020/06/15, 21:39:12",False,"2020/06/15, 21:46:09","2020/06/15, 21:46:09",4429741.0,21.0,1,148,elastic-apm-node not register span after express responde,"In my code example, i use setInterval."
Elastic APM,62394810,nan,0,"2020/06/15, 21:39:12",False,"2020/06/15, 21:46:09","2020/06/15, 21:46:09",4429741.0,21.0,1,148,elastic-apm-node not register span after express responde,"First express response, after 1 second, i try use startspan, but i got error: Cannot read property 'end' of null"
Elastic APM,62394810,nan,0,"2020/06/15, 21:39:12",False,"2020/06/15, 21:46:09","2020/06/15, 21:46:09",4429741.0,21.0,1,148,elastic-apm-node not register span after express responde,My elastic stack version is 7.6.2
Elastic APM,63394453,nan,0,"2020/08/13, 14:45:05",False,"2020/08/13, 14:45:05",nan,292291.0,73999.0,1,145,Elastic APM: Mongoose spans cannot find transaction when used with apollo/graphQL,I am using Elastic APM.
Elastic APM,63394453,nan,0,"2020/08/13, 14:45:05",False,"2020/08/13, 14:45:05",nan,292291.0,73999.0,1,145,Elastic APM: Mongoose spans cannot find transaction when used with apollo/graphQL,I find that mongoose integration is not working when used with GraphQL/Apollo Server somehow.
Elastic APM,63394453,nan,0,"2020/08/13, 14:45:05",False,"2020/08/13, 14:45:05",nan,292291.0,73999.0,1,145,Elastic APM: Mongoose spans cannot find transaction when used with apollo/graphQL,I wrote an apollo-server plugin like this to start/stop transactions:
Elastic APM,63394453,nan,0,"2020/08/13, 14:45:05",False,"2020/08/13, 14:45:05",nan,292291.0,73999.0,1,145,Elastic APM: Mongoose spans cannot find transaction when used with apollo/graphQL,"It works, but I am missing spans from mongoose, when I enable trace, this is what I see:"
Elastic APM,63394453,nan,0,"2020/08/13, 14:45:05",False,"2020/08/13, 14:45:05",nan,292291.0,73999.0,1,145,Elastic APM: Mongoose spans cannot find transaction when used with apollo/graphQL,"Notice the spans created fine before and after those mongoose calls, but the mongoose calls seem to be unable to find the active transaction somehow."
Elastic APM,63394453,nan,0,"2020/08/13, 14:45:05",False,"2020/08/13, 14:45:05",nan,292291.0,73999.0,1,145,Elastic APM: Mongoose spans cannot find transaction when used with apollo/graphQL,no active transaction found - cannot build new span
Elastic APM,64323562,nan,1,"2020/10/12, 21:40:58",True,"2020/10/13, 10:23:00","2020/10/13, 10:23:00",1221132.0,4687.0,1,115,elastic APM and NestJS does not differentiate graphql queries,I am adding the elastic-apm-node package to our nestjs backend.
Elastic APM,64323562,nan,1,"2020/10/12, 21:40:58",True,"2020/10/13, 10:23:00","2020/10/13, 10:23:00",1221132.0,4687.0,1,115,elastic APM and NestJS does not differentiate graphql queries,I am using the graphql feature of nestjs.
Elastic APM,64323562,nan,1,"2020/10/12, 21:40:58",True,"2020/10/13, 10:23:00","2020/10/13, 10:23:00",1221132.0,4687.0,1,115,elastic APM and NestJS does not differentiate graphql queries,"Because of this, all requests are merged together as  /graphql  in elastic."
Elastic APM,64323562,nan,1,"2020/10/12, 21:40:58",True,"2020/10/13, 10:23:00","2020/10/13, 10:23:00",1221132.0,4687.0,1,115,elastic APM and NestJS does not differentiate graphql queries,Is this how it is supposed to be?
Elastic APM,64323562,nan,1,"2020/10/12, 21:40:58",True,"2020/10/13, 10:23:00","2020/10/13, 10:23:00",1221132.0,4687.0,1,115,elastic APM and NestJS does not differentiate graphql queries,"I imagined that since apollo-server-express is supported by elastic-apm-node, which is also used by nestjs, it should be displaying it better."
Elastic APM,64323562,nan,1,"2020/10/12, 21:40:58",True,"2020/10/13, 10:23:00","2020/10/13, 10:23:00",1221132.0,4687.0,1,115,elastic APM and NestJS does not differentiate graphql queries,Am I missing something?
Elastic APM,64323562,nan,1,"2020/10/12, 21:40:58",True,"2020/10/13, 10:23:00","2020/10/13, 10:23:00",1221132.0,4687.0,1,115,elastic APM and NestJS does not differentiate graphql queries,UPDATE
Elastic APM,64323562,nan,1,"2020/10/12, 21:40:58",True,"2020/10/13, 10:23:00","2020/10/13, 10:23:00",1221132.0,4687.0,1,115,elastic APM and NestJS does not differentiate graphql queries,The graphql feature is set up using the docs for nestjs:  https://docs.nestjs.com/graphql/quick-start  it is basically their recommended setup I am using.
Elastic APM,64471632,nan,0,"2020/10/21, 23:55:01",False,"2020/10/21, 23:55:01",nan,2376081.0,11.0,1,56,How to configure wso2 with elastic apm?,Has anyone already tried to connect wso2 production with elastic apm server ?
Elastic APM,64471632,nan,0,"2020/10/21, 23:55:01",False,"2020/10/21, 23:55:01",nan,2376081.0,11.0,1,56,How to configure wso2 with elastic apm?,I have done this
Elastic APM,64471632,nan,0,"2020/10/21, 23:55:01",False,"2020/10/21, 23:55:01",nan,2376081.0,11.0,1,56,How to configure wso2 with elastic apm?,But i dont have the http request of the API in the APM in kibana
Elastic APM,65488128,65496630.0,1,"2020/12/29, 08:21:39",True,"2020/12/29, 20:01:53","2020/12/29, 08:35:20",5850195.0,1223.0,1,74,Elastic APM : cpu usage when running application.jar in local system,I am running a sample application jar on local system using elasticAPM agent.
Elastic APM,65488128,65496630.0,1,"2020/12/29, 08:21:39",True,"2020/12/29, 20:01:53","2020/12/29, 08:35:20",5850195.0,1223.0,1,74,Elastic APM : cpu usage when running application.jar in local system,Elastic APM show 2 different cpu stats (system/process).
Elastic APM,65488128,65496630.0,1,"2020/12/29, 08:21:39",True,"2020/12/29, 20:01:53","2020/12/29, 08:35:20",5850195.0,1223.0,1,74,Elastic APM : cpu usage when running application.jar in local system,"Metrics explanation on official site says the same thing for both stats
 https://www.elastic.co/guide/en/apm/server/current/exported-fields-system.html"
Elastic APM,65488128,65496630.0,1,"2020/12/29, 08:21:39",True,"2020/12/29, 20:01:53","2020/12/29, 08:35:20",5850195.0,1223.0,1,74,Elastic APM : cpu usage when running application.jar in local system,"Please explain, Is the &quot;system cpu stats&quot; is of my system even when the agent is connected to application.jar only using java command?"
Elastic APM,65488128,65496630.0,1,"2020/12/29, 08:21:39",True,"2020/12/29, 20:01:53","2020/12/29, 08:35:20",5850195.0,1223.0,1,74,Elastic APM : cpu usage when running application.jar in local system,"If so, how can I check on elastic apm what else on my system in consuming cpu since only application is running during the load test."
Elastic APM,65488128,65496630.0,1,"2020/12/29, 08:21:39",True,"2020/12/29, 20:01:53","2020/12/29, 08:35:20",5850195.0,1223.0,1,74,Elastic APM : cpu usage when running application.jar in local system,java -javaagent:&lt;agent.jar&gt; -jar &lt;app.jar&gt;
Elastic APM,65488128,65496630.0,1,"2020/12/29, 08:21:39",True,"2020/12/29, 20:01:53","2020/12/29, 08:35:20",5850195.0,1223.0,1,74,Elastic APM : cpu usage when running application.jar in local system,The CPU usage shown below
Elastic APM,52708201,nan,1,"2018/10/08, 21:33:21",False,"2020/12/14, 09:41:02","2020/12/14, 09:41:02",2741462.0,2369.0,0,183,Is it safe to run Elastic APM on production servers?,"Elastic APM uses a Java agent to collect application performance metrics and sends it to the Elastic APM server, which means it will use Java instrumentation for underlying metrics in JVM."
Elastic APM,52708201,nan,1,"2018/10/08, 21:33:21",False,"2020/12/14, 09:41:02","2020/12/14, 09:41:02",2741462.0,2369.0,0,183,Is it safe to run Elastic APM on production servers?,My question is:
Elastic APM,52708201,nan,1,"2018/10/08, 21:33:21",False,"2020/12/14, 09:41:02","2020/12/14, 09:41:02",2741462.0,2369.0,0,183,Is it safe to run Elastic APM on production servers?,Is there any vulnerability issue or security risk for using it?
Elastic APM,55319800,nan,1,"2019/03/24, 02:56:41",False,"2019/03/25, 09:50:57",nan,1727204.0,1365.0,0,117,Does Elastic APM support Spring Cloud Stream,I am very new to Elastic APM and not sure how it can support different frameworks.
Elastic APM,55319800,nan,1,"2019/03/24, 02:56:41",False,"2019/03/25, 09:50:57",nan,1727204.0,1365.0,0,117,Does Elastic APM support Spring Cloud Stream,I can see that from the documentation APM supports Spring Boot.
Elastic APM,55319800,nan,1,"2019/03/24, 02:56:41",False,"2019/03/25, 09:50:57",nan,1727204.0,1365.0,0,117,Does Elastic APM support Spring Cloud Stream,I have tested a Spring Boot application with the APM and it looks promising.
Elastic APM,55319800,nan,1,"2019/03/24, 02:56:41",False,"2019/03/25, 09:50:57",nan,1727204.0,1365.0,0,117,Does Elastic APM support Spring Cloud Stream,I was wondering if APM supports Spring Cloud Stream as well.
Elastic APM,55319800,nan,1,"2019/03/24, 02:56:41",False,"2019/03/25, 09:50:57",nan,1727204.0,1365.0,0,117,Does Elastic APM support Spring Cloud Stream,Spring Cloud Stream provides Event Driven Architecture by using Spring Boot and messaging middleware.
Elastic APM,55319800,nan,1,"2019/03/24, 02:56:41",False,"2019/03/25, 09:50:57",nan,1727204.0,1365.0,0,117,Does Elastic APM support Spring Cloud Stream,"Middleware can be Kafka, RabbitMQ, etc."
Elastic APM,57297752,nan,2,"2019/07/31, 22:32:16",False,"2020/04/07, 07:20:17",nan,2601060.0,2490.0,0,377,Can Elastic APM track Java Garbage Collection?,Is there a way for me to track garbage collection of my Java application using Elastic APM and the associated Java APM agent?
Elastic APM,57297752,nan,2,"2019/07/31, 22:32:16",False,"2020/04/07, 07:20:17",nan,2601060.0,2490.0,0,377,Can Elastic APM track Java Garbage Collection?,"I'm using Spring Boot, if that makes a difference."
Elastic APM,57297752,nan,2,"2019/07/31, 22:32:16",False,"2020/04/07, 07:20:17",nan,2601060.0,2490.0,0,377,Can Elastic APM track Java Garbage Collection?,"Out-of-the-box I'm able to see the heap and non-heap memory utilization, but I'm not sure if there is also a way to view garbage collection."
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,When nesting spans in Elsatic APM through either the Opentracing API or Elastic APM's API.
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,Some spans are never recorded.
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,Using  import * as apm from '@elastic/apm-rum'; :
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,Using Elastic's OpenTracing API:
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,The behavior for spans are just as inconsistent.
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,It is unclear when a transaction begins or ends.
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,"Some spans are translated into transactions, and nested spans may not be recorded."
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,"If I declare a page wide transaction, Angular's  ngOnInit  can be recorded by a span, but other event hooks are never recorded."
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,I have tried variations of this.
Elastic APM,57298727,nan,1,"2019/07/31, 23:55:46",False,"2019/08/01, 00:04:16",nan,6713596.0,1441.0,0,408,Elastic APM RUM Missing nested spans,"Wrapping span in span, childOf, app-level span, individual instances of span."
Elastic APM,57782547,nan,1,"2019/09/04, 09:21:45",True,"2019/12/10, 15:25:10",nan,10504469.0,2167.0,0,755,How to detach Elastic APM agent?,I am trying out Elastic APM.
Elastic APM,57782547,nan,1,"2019/09/04, 09:21:45",True,"2019/12/10, 15:25:10",nan,10504469.0,2167.0,0,755,How to detach Elastic APM agent?,I used automatic setup with  apm-agent-attach-standalone.jar .
Elastic APM,57782547,nan,1,"2019/09/04, 09:21:45",True,"2019/12/10, 15:25:10",nan,10504469.0,2167.0,0,755,How to detach Elastic APM agent?,"(I also used  -javaagent  flag (manual setup), and it worked fine)"
Elastic APM,57782547,nan,1,"2019/09/04, 09:21:45",True,"2019/12/10, 15:25:10",nan,10504469.0,2167.0,0,755,How to detach Elastic APM agent?,"Data was successfully recieved from the agents, and I used APM UI to monitor."
Elastic APM,57782547,nan,1,"2019/09/04, 09:21:45",True,"2019/12/10, 15:25:10",nan,10504469.0,2167.0,0,755,How to detach Elastic APM agent?,How do I detach this agent from the process?
Elastic APM,57782547,nan,1,"2019/09/04, 09:21:45",True,"2019/12/10, 15:25:10",nan,10504469.0,2167.0,0,755,How to detach Elastic APM agent?,What do I do if I want to stop this agent?
Elastic APM,57814434,nan,0,"2019/09/06, 03:45:10",False,"2019/09/06, 03:45:10",nan,11879477.0,177.0,0,504,"Elastic APM Node.js - Don&#39;t monitor HTTP, custom transactions only",I'm looking to use Elastic APM to monitor my own custom events only (user action events) in Node.js
Elastic APM,57814434,nan,0,"2019/09/06, 03:45:10",False,"2019/09/06, 03:45:10",nan,11879477.0,177.0,0,504,"Elastic APM Node.js - Don&#39;t monitor HTTP, custom transactions only",I can't find in the docs how to turn off all HTTP monitoring and also how to send a custom event.
Elastic APM,57814434,nan,0,"2019/09/06, 03:45:10",False,"2019/09/06, 03:45:10",nan,11879477.0,177.0,0,504,"Elastic APM Node.js - Don&#39;t monitor HTTP, custom transactions only",They have custom span and custom transaction in the docs but I'm not sure when or how to use either for my use case.
Elastic APM,57814434,nan,0,"2019/09/06, 03:45:10",False,"2019/09/06, 03:45:10",nan,11879477.0,177.0,0,504,"Elastic APM Node.js - Don&#39;t monitor HTTP, custom transactions only",For example in the docs they describe a custom transaction as follows:
Elastic APM,57814434,nan,0,"2019/09/06, 03:45:10",False,"2019/09/06, 03:45:10",nan,11879477.0,177.0,0,504,"Elastic APM Node.js - Don&#39;t monitor HTTP, custom transactions only","I'm not looking to send an error, I'm looking to send a custom event so I'm not sure what's up with that."
Elastic APM,58074198,58075056.0,2,"2019/09/24, 09:23:05",True,"2020/08/10, 15:09:15","2019/09/24, 10:19:10",10504469.0,2167.0,0,1309,How can I delete services in Elastic APM?,I'm trying out Elastic APM.
Elastic APM,58074198,58075056.0,2,"2019/09/24, 09:23:05",True,"2020/08/10, 15:09:15","2019/09/24, 10:19:10",10504469.0,2167.0,0,1309,How can I delete services in Elastic APM?,I have successfully created a service with data flowing in.
Elastic APM,58074198,58075056.0,2,"2019/09/24, 09:23:05",True,"2020/08/10, 15:09:15","2019/09/24, 10:19:10",10504469.0,2167.0,0,1309,How can I delete services in Elastic APM?,I wanted to see if I can have multiple services.
Elastic APM,58074198,58075056.0,2,"2019/09/24, 09:23:05",True,"2020/08/10, 15:09:15","2019/09/24, 10:19:10",10504469.0,2167.0,0,1309,How can I delete services in Elastic APM?,"Somehow, I ran into problems, so I wanted to delete some services."
Elastic APM,58074198,58075056.0,2,"2019/09/24, 09:23:05",True,"2020/08/10, 15:09:15","2019/09/24, 10:19:10",10504469.0,2167.0,0,1309,How can I delete services in Elastic APM?,"However, I couldn't find a way to delete a service."
Elastic APM,58074198,58075056.0,2,"2019/09/24, 09:23:05",True,"2020/08/10, 15:09:15","2019/09/24, 10:19:10",10504469.0,2167.0,0,1309,How can I delete services in Elastic APM?,Question : How can I delete a service in APM?
Elastic APM,58074198,58075056.0,2,"2019/09/24, 09:23:05",True,"2020/08/10, 15:09:15","2019/09/24, 10:19:10",10504469.0,2167.0,0,1309,How can I delete services in Elastic APM?,Indexes related to APM :
Elastic APM,58074198,58075056.0,2,"2019/09/24, 09:23:05",True,"2020/08/10, 15:09:15","2019/09/24, 10:19:10",10504469.0,2167.0,0,1309,How can I delete services in Elastic APM?,Above contains the service that I want to remove.
Elastic APM,58076441,58087113.0,1,"2019/09/24, 11:49:01",True,"2019/09/24, 22:36:31",nan,10504469.0,2167.0,0,46,Can Elastic APM measure the performance of applications other than web applications?,I've seen some APM that only measures web applications which run on WAS.
Elastic APM,58076441,58087113.0,1,"2019/09/24, 11:49:01",True,"2019/09/24, 22:36:31",nan,10504469.0,2167.0,0,46,Can Elastic APM measure the performance of applications other than web applications?,Can Elastic APM meausure the performance of other applications like pure Java application and etc?
Elastic APM,58076441,58087113.0,1,"2019/09/24, 11:49:01",True,"2019/09/24, 22:36:31",nan,10504469.0,2167.0,0,46,Can Elastic APM measure the performance of applications other than web applications?,"If not,  can I use  https://www.elastic.co/guide/en/apm/agent/java/1.x/public-api.html  (Public API) so that it can measure the performance of non web applications?"
Elastic APM,58076441,58087113.0,1,"2019/09/24, 11:49:01",True,"2019/09/24, 22:36:31",nan,10504469.0,2167.0,0,46,Can Elastic APM measure the performance of applications other than web applications?,I will appreciate any advice.
Elastic APM,58076441,58087113.0,1,"2019/09/24, 11:49:01",True,"2019/09/24, 22:36:31",nan,10504469.0,2167.0,0,46,Can Elastic APM measure the performance of applications other than web applications?,Cheers.
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,I integrated  Elastic APM  as follows to my Vue.js App.
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,It logs successfully to Elastic APM.
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,However it is not showing the current page name in logs correctly.
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,It seems to always show the first page on which the App has been mounted in APM.
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,I would like to always see the current page to be linked to the corresponding APM events.
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,Any suggestions how to achieve this?
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,The documentation says to set  pageLoadTransactionName .
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,"However it seems to not update this on route change:
 https://www.elastic.co/guide/en/apm/agent/rum-js/current/custom-transaction-name.html"
Elastic APM,58728047,58750060.0,2,"2019/11/06, 12:30:21",True,"2020/11/20, 21:05:02","2019/11/06, 14:21:15",6080017.0,1792.0,0,789,How to integrate Elastic APM to Vuejs SPA?,App.js
Elastic APM,58738337,nan,2,"2019/11/06, 22:55:10",True,"2020/04/14, 00:09:21",nan,1045178.0,133.0,0,335,Does elastic APM for Java captures logs?,I can't understand if Elastic APM for Java should capture logs from slf4j or it can track only exceptions?
Elastic APM,58738337,nan,2,"2019/11/06, 22:55:10",True,"2020/04/14, 00:09:21",nan,1045178.0,133.0,0,335,Does elastic APM for Java captures logs?,I have Spring Boot service that uses slf4j but I can't find log entries in apm index.
Elastic APM,58738337,nan,2,"2019/11/06, 22:55:10",True,"2020/04/14, 00:09:21",nan,1045178.0,133.0,0,335,Does elastic APM for Java captures logs?,Can anybody clarify the expectation?
Elastic APM,58738337,nan,2,"2019/11/06, 22:55:10",True,"2020/04/14, 00:09:21",nan,1045178.0,133.0,0,335,Does elastic APM for Java captures logs?,Thank you
Elastic APM,58833454,nan,0,"2019/11/13, 10:58:32",False,"2019/11/13, 10:58:32",nan,433570.0,32819.0,0,434,Elastic APM: How to turn off logging for django,flushing due to time since last flush 9.060s   max_flush_time 9.060s
Elastic APM,58833454,nan,0,"2019/11/13, 10:58:32",False,"2019/11/13, 10:58:32",nan,433570.0,32819.0,0,434,Elastic APM: How to turn off logging for django,I 'm getting tone of those message in django debug.
Elastic APM,58833454,nan,0,"2019/11/13, 10:58:32",False,"2019/11/13, 10:58:32",nan,433570.0,32819.0,0,434,Elastic APM: How to turn off logging for django,I tried changing to their default setting
Elastic APM,58833454,nan,0,"2019/11/13, 10:58:32",False,"2019/11/13, 10:58:32",nan,433570.0,32819.0,0,434,Elastic APM: How to turn off logging for django,Still getting lots of logs.
Elastic APM,58833454,nan,0,"2019/11/13, 10:58:32",False,"2019/11/13, 10:58:32",nan,433570.0,32819.0,0,434,Elastic APM: How to turn off logging for django,How to turn this off?
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,"I'm having trouble getting the APM ""button"" and dashboard to appear on the Kibana page."
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,"Yes, there is the ""Add APM"" button which tells you what to do, but it doesn't seem to actually work."
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,"Actually, this is not entirely true - I was able to get the APM ""button"" and corresponding dashboard ""installed"" in my Kibana view,  but I can't remember what I had to do to make that happen."
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,"I believe that I have the various components (Elasticsearch, Kibana, APM server) installed and running."
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,"The ""check APM server status"" button indicates that it's been set up properly."
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,"If I click the ""APM dashboard"" button at the bottom of the page, it gives me a list of items, but I don't know what they are or whether they have anything to do with APM."
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,I am at a loss as to how to get APM to appear in Kibana.
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,Does anybody have any ideas?
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,UPDATE :
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,https://www.elastic.co/guide/en/apm/server/current/getting-started-apm-server.html
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,then
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,https://www.elastic.co/guide/en/apm/server/current/installing.html
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,then
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,https://www.elastic.co/guide/en/apm/server/current/apm-server-configuration.html
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,This seems to provide specific information I haven't been able to find elsewhere.
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,The usage of  apm-server setup &lt;flags&gt;  seems promising.
Elastic APM,59328108,nan,1,"2019/12/13, 20:36:04",False,"2020/05/14, 18:18:04","2019/12/13, 21:37:23",2261831.0,904.0,0,644,How to install/enable Elastic APM dashboard in Kibana,I'm not sure which flags (if any) I should use?
Elastic APM,59352981,60702327.0,1,"2019/12/16, 10:37:57",True,"2020/03/16, 10:08:57",nan,6080017.0,1792.0,0,192,How to get the current Elastic APM instance in Vue.js?,I have integrated  Elastic APM  to my Vue.js App accordingly to the documentation ( https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html )
Elastic APM,59352981,60702327.0,1,"2019/12/16, 10:37:57",True,"2020/03/16, 10:08:57",nan,6080017.0,1792.0,0,192,How to get the current Elastic APM instance in Vue.js?,In addition to the default events  page-load  and  route-change  I want to add custom transactions/spans for some button clicks.
Elastic APM,59352981,60702327.0,1,"2019/12/16, 10:37:57",True,"2020/03/16, 10:08:57",nan,6080017.0,1792.0,0,192,How to get the current Elastic APM instance in Vue.js?,I am stucked with checking if there is already an existing transaction start which I could use to add a custom span:
Elastic APM,59352981,60702327.0,1,"2019/12/16, 10:37:57",True,"2020/03/16, 10:08:57",nan,6080017.0,1792.0,0,192,How to get the current Elastic APM instance in Vue.js?,However getting the current transaction fails (first line).
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,I am trying to integrate Elastic APM and Sentry into my website using Buffalo.
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,The interesting files are as follows:
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,handlers/sentryHandler.go
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,handlers/elasticAPMHandler.go
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,actions/app.go
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,"The problem I'm running into is if I have the Sentry/APM handlers at the top, then I get errors like  application.html: line 24: ""showPagePath"": unknown identifier ."
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,"However, if I move it to just before I set up the routes, then I get a no transaction found error."
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,"So, I'm guessing that the handler wrappers are dropping the  buffalo.Context  information."
Elastic APM,60920952,nan,1,"2020/03/30, 00:29:01",False,"2020/04/02, 06:15:04",nan,3108288.0,350.0,0,132,Integrating Sentry and Elastic APM in Buffalo,"So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?"
Elastic APM,61066877,61067229.0,1,"2020/04/06, 21:49:38",True,"2020/04/06, 22:11:02",nan,120885.0,3557.0,0,256,How can I drill down on stacktrace in ASP.NET MVC application using Elastic APM?,I am using Elastic APM agent ( https://www.elastic.co/guide/en/apm/agent/dotnet/current/index.html ) to instrument an ASP.NET MVC Application.
Elastic APM,61066877,61067229.0,1,"2020/04/06, 21:49:38",True,"2020/04/06, 22:11:02",nan,120885.0,3557.0,0,256,How can I drill down on stacktrace in ASP.NET MVC application using Elastic APM?,I added the nuget packages and added the module entry in the web.config.
Elastic APM,61066877,61067229.0,1,"2020/04/06, 21:49:38",True,"2020/04/06, 22:11:02",nan,120885.0,3557.0,0,256,How can I drill down on stacktrace in ASP.NET MVC application using Elastic APM?,I am able to get data in the Kibana APM tab and nicely shows the time spent by each call.
Elastic APM,61066877,61067229.0,1,"2020/04/06, 21:49:38",True,"2020/04/06, 22:11:02",nan,120885.0,3557.0,0,256,How can I drill down on stacktrace in ASP.NET MVC application using Elastic APM?,(see screenshot below).
Elastic APM,61066877,61067229.0,1,"2020/04/06, 21:49:38",True,"2020/04/06, 22:11:02",nan,120885.0,3557.0,0,256,How can I drill down on stacktrace in ASP.NET MVC application using Elastic APM?,Mu Question is: How can I drill down inside each of these calls to see where the time is spent in the stackstace?
Elastic APM,61066877,61067229.0,1,"2020/04/06, 21:49:38",True,"2020/04/06, 22:11:02",nan,120885.0,3557.0,0,256,How can I drill down on stacktrace in ASP.NET MVC application using Elastic APM?,Is there something I am missing?
Elastic APM,61108716,61171704.0,1,"2020/04/08, 22:38:35",True,"2020/04/12, 15:46:20",nan,9358877.0,33.0,0,212,Elastic APM Agent KIbana UI Transactions are NA,We have newly introduced elastic APM to monitor application performance.
Elastic APM,61108716,61171704.0,1,"2020/04/08, 22:38:35",True,"2020/04/12, 15:46:20",nan,9358877.0,33.0,0,212,Elastic APM Agent KIbana UI Transactions are NA,"After the deployment, we have an issue with Kibana APM UI that doesn't show up any transactions for few services."
Elastic APM,61108716,61171704.0,1,"2020/04/08, 22:38:35",True,"2020/04/12, 15:46:20",nan,9358877.0,33.0,0,212,Elastic APM Agent KIbana UI Transactions are NA,we are having data in the APM indice but doesn't transactions show up in the Kibana APM dashboard   UI we get Avg.response time N/A and Trans.per minute is 0.
Elastic APM,61108716,61171704.0,1,"2020/04/08, 22:38:35",True,"2020/04/12, 15:46:20",nan,9358877.0,33.0,0,212,Elastic APM Agent KIbana UI Transactions are NA,"APM-Server 7.6.1
Elastic Agent - 1.15.0
ELK Stack - 7.6.1"
Elastic APM,61108716,61171704.0,1,"2020/04/08, 22:38:35",True,"2020/04/12, 15:46:20",nan,9358877.0,33.0,0,212,Elastic APM Agent KIbana UI Transactions are NA,Please help me on this to identify the issue.
Elastic APM,61108716,61171704.0,1,"2020/04/08, 22:38:35",True,"2020/04/12, 15:46:20",nan,9358877.0,33.0,0,212,Elastic APM Agent KIbana UI Transactions are NA,Thanks in advance.
Elastic APM,61163723,61195913.0,1,"2020/04/12, 00:06:44",True,"2020/04/13, 23:09:07",nan,10912170.0,509.0,0,567,Call Elastic APM Properties From Spring Boot Application Properties,I try to implement profile based application in Spring Boot and this works for Spring Boot clearly.
Elastic APM,61163723,61195913.0,1,"2020/04/12, 00:06:44",True,"2020/04/13, 23:09:07",nan,10912170.0,509.0,0,567,Call Elastic APM Properties From Spring Boot Application Properties,"But when I try to implement it for elastic-search APM, it doesn't work."
Elastic APM,61163723,61195913.0,1,"2020/04/12, 00:06:44",True,"2020/04/13, 23:09:07",nan,10912170.0,509.0,0,567,Call Elastic APM Properties From Spring Boot Application Properties,According to  here : We can describe properties like with elastic.apm prefix:
Elastic APM,61163723,61195913.0,1,"2020/04/12, 00:06:44",True,"2020/04/13, 23:09:07",nan,10912170.0,509.0,0,567,Call Elastic APM Properties From Spring Boot Application Properties,but it doesn't work.
Elastic APM,61163723,61195913.0,1,"2020/04/12, 00:06:44",True,"2020/04/13, 23:09:07",nan,10912170.0,509.0,0,567,Call Elastic APM Properties From Spring Boot Application Properties,"When I call it with elasticapm.properties, it works."
Elastic APM,61163723,61195913.0,1,"2020/04/12, 00:06:44",True,"2020/04/13, 23:09:07",nan,10912170.0,509.0,0,567,Call Elastic APM Properties From Spring Boot Application Properties,Do you have any suggestion?
Elastic APM,61524132,61524739.0,1,"2020/04/30, 16:07:20",True,"2020/04/30, 16:40:12",nan,7980673.0,694.0,0,60,Code snippets from .NET Agent not showing in Elastic APM,We're using the .NET agent (1.4) and Elastic ECE 7.6.
Elastic APM,61524132,61524739.0,1,"2020/04/30, 16:07:20",True,"2020/04/30, 16:40:12",nan,7980673.0,694.0,0,60,Code snippets from .NET Agent not showing in Elastic APM,Activating the APM server and instrumenting our application was quite easy.
Elastic APM,61524132,61524739.0,1,"2020/04/30, 16:07:20",True,"2020/04/30, 16:40:12",nan,7980673.0,694.0,0,60,Code snippets from .NET Agent not showing in Elastic APM,Everything seems to work up until the point where it needs to show the code snippet related to a particular span.
Elastic APM,61524132,61524739.0,1,"2020/04/30, 16:07:20",True,"2020/04/30, 16:40:12",nan,7980673.0,694.0,0,60,Code snippets from .NET Agent not showing in Elastic APM,Below an example of what I'm looking for from a Node application:
Elastic APM,61524132,61524739.0,1,"2020/04/30, 16:07:20",True,"2020/04/30, 16:40:12",nan,7980673.0,694.0,0,60,Code snippets from .NET Agent not showing in Elastic APM,My question:
Elastic APM,61524132,61524739.0,1,"2020/04/30, 16:07:20",True,"2020/04/30, 16:40:12",nan,7980673.0,694.0,0,60,Code snippets from .NET Agent not showing in Elastic APM,"Is this not yet included in the .NET agent, or is there additional configuration necessary to get this working?"
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,I have the Elastic Stack running with Docker.
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,I've also added an Angular 9 App via Docker and enabled CORS on Elastic.
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,"I am trying to send logs from the Angular 9 app to APM (on port 9200), using the  APM Angular Package ."
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,However I'm getting a 400 error.
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,"I'm doing what the docs say, their NPM package and running:"
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,But still getting the error in the screenshot  [Elastic APM] Failed sending events!
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,"Error: ""http://localhost:9200/intake/v2/rum/events HTTP status: 400"""
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,Any advice on how to resolve this?
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,docker-compose.yml  file:
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,Elastic yml:
Elastic APM,61640028,nan,0,"2020/05/06, 19:15:25",False,"2020/05/06, 19:15:25",nan,1159590.0,2587.0,0,339,Elastic APM Angular Package Returns 400 with Angular 9,APM Server yml:
Elastic APM,61642883,nan,2,"2020/05/06, 21:43:28",False,"2021/03/11, 12:42:08",nan,1351859.0,81.0,0,845,Spring Boot Webflux + Elastic APM Monitoring,I am currently new to Elastic APM.
Elastic APM,61642883,nan,2,"2020/05/06, 21:43:28",False,"2021/03/11, 12:42:08",nan,1351859.0,81.0,0,845,Spring Boot Webflux + Elastic APM Monitoring,"I am currently developing an application using spring-webflux and want to monitor my application using Elastic APM, but unfortunately, it's not working for me."
Elastic APM,61642883,nan,2,"2020/05/06, 21:43:28",False,"2021/03/11, 12:42:08",nan,1351859.0,81.0,0,845,Spring Boot Webflux + Elastic APM Monitoring,Dependecies
Elastic APM,61642883,nan,2,"2020/05/06, 21:43:28",False,"2021/03/11, 12:42:08",nan,1351859.0,81.0,0,845,Spring Boot Webflux + Elastic APM Monitoring,"APM Java Agent Version -  1.8.0
Elastic search - 7.2.0
APM server - 7.2.0"
Elastic APM,61642883,nan,2,"2020/05/06, 21:43:28",False,"2021/03/11, 12:42:08",nan,1351859.0,81.0,0,845,Spring Boot Webflux + Elastic APM Monitoring,Exception observed -
Elastic APM,61642883,nan,2,"2020/05/06, 21:43:28",False,"2021/03/11, 12:42:08",nan,1351859.0,81.0,0,845,Spring Boot Webflux + Elastic APM Monitoring,Could someone please suggest what I am missing ?
Elastic APM,61900606,nan,0,"2020/05/19, 23:54:29",False,"2020/05/19, 23:54:29",nan,3297434.0,589.0,0,142,How to instrument @Async annotation functions with elastic APM,I am trying to check for the instrumentation of a function which runs asynchronously through the @Async annotation in spring boot.
Elastic APM,61900606,nan,0,"2020/05/19, 23:54:29",False,"2020/05/19, 23:54:29",nan,3297434.0,589.0,0,142,How to instrument @Async annotation functions with elastic APM,"I am using the following elastic APM library : 
  co.elastic.apm:elastic-apm-agent:1.9.0"
Elastic APM,61900606,nan,0,"2020/05/19, 23:54:29",False,"2020/05/19, 23:54:29",nan,3297434.0,589.0,0,142,How to instrument @Async annotation functions with elastic APM,"As of now, with general configurations, I am able to see the instrumentation of API requests and scheduled jobs (@Scheduled annotation in spring boot), but I am not able to check the details of @Async annotated functions."
Elastic APM,61900606,nan,0,"2020/05/19, 23:54:29",False,"2020/05/19, 23:54:29",nan,3297434.0,589.0,0,142,How to instrument @Async annotation functions with elastic APM,Is there any specific configuration that I need to do?
Elastic APM,61900606,nan,0,"2020/05/19, 23:54:29",False,"2020/05/19, 23:54:29",nan,3297434.0,589.0,0,142,How to instrument @Async annotation functions with elastic APM,"As far as I know, it is available in newrelic."
Elastic APM,61900606,nan,0,"2020/05/19, 23:54:29",False,"2020/05/19, 23:54:29",nan,3297434.0,589.0,0,142,How to instrument @Async annotation functions with elastic APM,It would be great if someone can point to a documentation / example for the same in elastic APM.
Elastic APM,61900606,nan,0,"2020/05/19, 23:54:29",False,"2020/05/19, 23:54:29",nan,3297434.0,589.0,0,142,How to instrument @Async annotation functions with elastic APM,Thanks in advance.
Elastic APM,62529783,nan,0,"2020/06/23, 10:44:31",False,"2020/06/23, 10:44:31",nan,5051764.0,131.0,0,135,What is the version of Elastic APM which is compatible with Django 1.6.11?,I am getting errors with ElasticAPM version 5.5.2 while running
Elastic APM,62529783,nan,0,"2020/06/23, 10:44:31",False,"2020/06/23, 10:44:31",nan,5051764.0,131.0,0,135,What is the version of Elastic APM which is compatible with Django 1.6.11?,"Error: logging_config_func(self.LOGGING) File &quot;/usr/lib/python2.7/logging/config.py&quot;, line 795, in dictConfig dictConfigClass(config).configure() File &quot;/usr/lib/python2.7/logging/config.py&quot;, line 577, in configure '%r: %s' % (name, e)) ValueError: Unable to configure handler 'elasticapm': Cannot resolve 'elasticapm.contrib.django.handlers.LoggingHandler': No module named apps"
Elastic APM,62529783,nan,0,"2020/06/23, 10:44:31",False,"2020/06/23, 10:44:31",nan,5051764.0,131.0,0,135,What is the version of Elastic APM which is compatible with Django 1.6.11?,"Django: 1.6.11
Python: 2.7
ElastcAPM: 5.5.2"
Elastic APM,62556704,nan,0,"2020/06/24, 16:50:35",False,"2020/06/24, 16:50:35",nan,1703696.0,3755.0,0,53,How to send a debug message to elastic APM Real User Monitoring from my application?,I have a JavaScript application and configured Elastic APM/RUM on that.
Elastic APM,62556704,nan,0,"2020/06/24, 16:50:35",False,"2020/06/24, 16:50:35",nan,1703696.0,3755.0,0,53,How to send a debug message to elastic APM Real User Monitoring from my application?,As soon as I start to interact with the application some metrics start sending to APM Server (page_load for example).
Elastic APM,62556704,nan,0,"2020/06/24, 16:50:35",False,"2020/06/24, 16:50:35",nan,1703696.0,3755.0,0,53,How to send a debug message to elastic APM Real User Monitoring from my application?,I want to know if it's possible to send a specific text message to APM Server.
Elastic APM,62556704,nan,0,"2020/06/24, 16:50:35",False,"2020/06/24, 16:50:35",nan,1703696.0,3755.0,0,53,How to send a debug message to elastic APM Real User Monitoring from my application?,Example:
Elastic APM,62556704,nan,0,"2020/06/24, 16:50:35",False,"2020/06/24, 16:50:35",nan,1703696.0,3755.0,0,53,How to send a debug message to elastic APM Real User Monitoring from my application?,I'm trying to inject myDebugMessage this way:
Elastic APM,62556704,nan,0,"2020/06/24, 16:50:35",False,"2020/06/24, 16:50:35",nan,1703696.0,3755.0,0,53,How to send a debug message to elastic APM Real User Monitoring from my application?,payload.myDebugMessage = debugMessage
Elastic APM,62556704,nan,0,"2020/06/24, 16:50:35",False,"2020/06/24, 16:50:35",nan,1703696.0,3755.0,0,53,How to send a debug message to elastic APM Real User Monitoring from my application?,See the code below:
Elastic APM,62677891,62680831.0,1,"2020/07/01, 16:24:33",True,"2020/07/01, 19:02:17",nan,13409135.0,45.0,0,1088,Elastic APM Java - Transactions &amp; Spans are recorded but not reported to Elastic APM Server or Kibana,I have a standalone JAVA application.
Elastic APM,62677891,62680831.0,1,"2020/07/01, 16:24:33",True,"2020/07/01, 19:02:17",nan,13409135.0,45.0,0,1088,Elastic APM Java - Transactions &amp; Spans are recorded but not reported to Elastic APM Server or Kibana,And have integrated it successfully with Elastic APM (+ElasticSearch +Kibana) for capturing telemetries.
Elastic APM,62677891,62680831.0,1,"2020/07/01, 16:24:33",True,"2020/07/01, 19:02:17",nan,13409135.0,45.0,0,1088,Elastic APM Java - Transactions &amp; Spans are recorded but not reported to Elastic APM Server or Kibana,"Java Version:  8 - OpenJDK 
 Elastic Agent &amp; Library Version:  1.16 
 Elastic Search, APM and Kibana Version:  7.7.1"
Elastic APM,62677891,62680831.0,1,"2020/07/01, 16:24:33",True,"2020/07/01, 19:02:17",nan,13409135.0,45.0,0,1088,Elastic APM Java - Transactions &amp; Spans are recorded but not reported to Elastic APM Server or Kibana,Below are the relevant JVM Options being used:
Elastic APM,62677891,62680831.0,1,"2020/07/01, 16:24:33",True,"2020/07/01, 19:02:17",nan,13409135.0,45.0,0,1088,Elastic APM Java - Transactions &amp; Spans are recorded but not reported to Elastic APM Server or Kibana,However some out of the box transactions from Quartz(which is being used in the application) are shown as expected.
Elastic APM,62677891,62680831.0,1,"2020/07/01, 16:24:33",True,"2020/07/01, 19:02:17",nan,13409135.0,45.0,0,1088,Elastic APM Java - Transactions &amp; Spans are recorded but not reported to Elastic APM Server or Kibana,Which should mean the integration with the Elastic APM Server is fine.
Elastic APM,62677891,62680831.0,1,"2020/07/01, 16:24:33",True,"2020/07/01, 19:02:17",nan,13409135.0,45.0,0,1088,Elastic APM Java - Transactions &amp; Spans are recorded but not reported to Elastic APM Server or Kibana,"It appears to me, even though the transactions are being captured successfully, those are not reported(sent) to the APM Server"
Elastic APM,62677891,62680831.0,1,"2020/07/01, 16:24:33",True,"2020/07/01, 19:02:17",nan,13409135.0,45.0,0,1088,Elastic APM Java - Transactions &amp; Spans are recorded but not reported to Elastic APM Server or Kibana,Refer some relevant apm logs:
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,I have a standalone Python application.
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,The python process is not using any framework.
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,And is a simple standalone python process.
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,This has been successfully integrated with Elastic APM (+ElasticSearch +Kibana) for capturing telemetries.
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,"Python version:  3.7 
 elastic-apm python agent:  5.8.0 
 Elastic Search, APM and Kibana Version:  7.7.1 
 
As mentioned in the  official doc , I have used the following statements to start capturing metrics from my python process"
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,"But on Kibana, I am able to see only the following 3 system metrics (under 2 visualizations):"
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,"As per the  python code  analysis, as well as per what I have  read ."
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,Elastic APM Agent collects other process related metrics like:
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,"Refer the screenshot 
 
 
 
 Additionally, I expect the Elastic APM Python agent to collect other informations like :"
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,Which are already available for Elastic APM Java agent.
Elastic APM,62815678,62903745.0,1,"2020/07/09, 16:12:23",True,"2020/07/14, 23:46:57",nan,13409135.0,45.0,0,227,Elastic APM Python - System Metrics don&#39;t show process related metrics like memory on Kibana,Refer the screenshot
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,"As of a few weeks ago we added filebeat, metricbeat and apm to our dotnet core application ran on our kubernetes cluster."
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,It works all nice and recently we discovered filebeat and metricbeat are able to write a different index upon several rules.
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,"We wanted to do the same for APM, however  searching the documentation  we can't find any option to set the name of the index to write to."
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,"Is this even possible, and if yes how is it configured?"
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,I also tried finding the current name  apm-*  within the codebase but couldn't find any matches upon configuring it.
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,The problem which we'd like to fix is that every space in kibana gets to see the apm metrics of every application.
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,Certain applications shouldn't be within this space so therefore i thought a new  apm-application-*  index would do the trick...
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,Since it shouldn't be configured on the agent but instead in the cloud service console.
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,I'm having troubles to 'user-override' the settings to my likings.
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,The rules i want to have:
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,I see you can add  output.elasticsearch.indices  to make this happen:  Array of index selector rules supporting conditionals and formatted string.
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,I tried this by copying the same i had for metricbeat and updated it to use the apm syntax and came to the following 'user-override'
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,but when i use this setup it tells me:
Elastic APM,63116459,nan,1,"2020/07/27, 16:31:58",True,"2020/07/28, 12:49:15","2020/07/28, 12:49:15",2724940.0,3026.0,0,509,Elastic APM different index name,Then i updated the example but came to the same conclusion as it was not valid either..
Elastic APM,63129247,nan,1,"2020/07/28, 10:43:10",False,"2020/08/06, 23:12:47",nan,1807384.0,281.0,0,129,How to enable Spring batch to be monitored in Elastic APM?,Currently spring boot micorservices are enabled in Elastic APM.
Elastic APM,63129247,nan,1,"2020/07/28, 10:43:10",False,"2020/08/06, 23:12:47",nan,1807384.0,281.0,0,129,How to enable Spring batch to be monitored in Elastic APM?,We can also trace at method level and DB queries are shown.
Elastic APM,63129247,nan,1,"2020/07/28, 10:43:10",False,"2020/08/06, 23:12:47",nan,1807384.0,281.0,0,129,How to enable Spring batch to be monitored in Elastic APM?,But Spring batch job(Spring boot based) does not show any method level details and Oracle transaction details.
Elastic APM,63129247,nan,1,"2020/07/28, 10:43:10",False,"2020/08/06, 23:12:47",nan,1807384.0,281.0,0,129,How to enable Spring batch to be monitored in Elastic APM?,Does anything needs to be explicitly configured in Elastic APM for Spring batch applications.
Elastic APM,63339667,63351255.0,2,"2020/08/10, 14:54:18",True,"2020/08/11, 10:06:22","2020/08/11, 08:07:47",433570.0,32819.0,0,445,"elastic apm, turn off ssl verification",logstash 's elasticsearch output has option to turn off SSL verification
Elastic APM,63339667,63351255.0,2,"2020/08/10, 14:54:18",True,"2020/08/11, 10:06:22","2020/08/11, 08:07:47",433570.0,32819.0,0,445,"elastic apm, turn off ssl verification",https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-ssl_certificate_verification
Elastic APM,63339667,63351255.0,2,"2020/08/10, 14:54:18",True,"2020/08/11, 10:06:22","2020/08/11, 08:07:47",433570.0,32819.0,0,445,"elastic apm, turn off ssl verification",Is there a similar option for apm?
Elastic APM,63339667,63351255.0,2,"2020/08/10, 14:54:18",True,"2020/08/11, 10:06:22","2020/08/11, 08:07:47",433570.0,32819.0,0,445,"elastic apm, turn off ssl verification","Elasticsearch is using self signed certificate, and apm is complaining when connecting to ES."
Elastic APM,63601093,nan,0,"2020/08/26, 18:42:15",False,"2020/09/17, 12:36:37","2020/09/17, 12:36:37",2080099.0,852.0,0,61,Elastic APM Server returns interleaved 503 and 202 responses on ES Cluster,I have setup two Elastic APM servers which sends data to Elastic-search.
Elastic APM,63601093,nan,0,"2020/08/26, 18:42:15",False,"2020/09/17, 12:36:37","2020/09/17, 12:36:37",2080099.0,852.0,0,61,Elastic APM Server returns interleaved 503 and 202 responses on ES Cluster,"When I setup Elastic-search with single instance, Elastic APM server working fine responding 202 response."
Elastic APM,63601093,nan,0,"2020/08/26, 18:42:15",False,"2020/09/17, 12:36:37","2020/09/17, 12:36:37",2080099.0,852.0,0,61,Elastic APM Server returns interleaved 503 and 202 responses on ES Cluster,"When I setup Elastic-search cluster(3 master, 3 data nodes, 2 ingest, 2 coordinate nodes) , APM server server misbehaves responding interleaved 202 and 503
Using coordinate host in Elastic APM."
Elastic APM,63601093,nan,0,"2020/08/26, 18:42:15",False,"2020/09/17, 12:36:37","2020/09/17, 12:36:37",2080099.0,852.0,0,61,Elastic APM Server returns interleaved 503 and 202 responses on ES Cluster,This link  https://www.elastic.co/guide/en/apm/server/master/common-problems.html#queue-full   has some info but I am unable to resolve issue.
Elastic APM,63601093,nan,0,"2020/08/26, 18:42:15",False,"2020/09/17, 12:36:37","2020/09/17, 12:36:37",2080099.0,852.0,0,61,Elastic APM Server returns interleaved 503 and 202 responses on ES Cluster,"Edited: 
Problem is one of Elastic APM server is not able to push data on ES cluster due to index."
Elastic APM,63601093,nan,0,"2020/08/26, 18:42:15",False,"2020/09/17, 12:36:37","2020/09/17, 12:36:37",2080099.0,852.0,0,61,Elastic APM Server returns interleaved 503 and 202 responses on ES Cluster,But I am still don't know why index name is different on single ES node vs Cluster
Elastic APM,63931550,63934347.0,1,"2020/09/17, 08:03:01",True,"2020/09/17, 11:40:33",nan,2467842.0,165.0,0,344,Elastic APM show total number of SQL Queries executed on .Net Core API Endpoint,Currently have Elastic Apm setup with:  app.UseAllElasticApm(Configuration);  which is working correctly.
Elastic APM,63931550,63934347.0,1,"2020/09/17, 08:03:01",True,"2020/09/17, 11:40:33",nan,2467842.0,165.0,0,344,Elastic APM show total number of SQL Queries executed on .Net Core API Endpoint,I've just been trying to find a way to record exactly how many SQL Queries are run via Entity Framework for each transaction.
Elastic APM,63931550,63934347.0,1,"2020/09/17, 08:03:01",True,"2020/09/17, 11:40:33",nan,2467842.0,165.0,0,344,Elastic APM show total number of SQL Queries executed on .Net Core API Endpoint,Ideally when viewing the Apm data in Kibana the metadata tab could just include an  EntityFramework.ExecutedSqlQueriesCount .
Elastic APM,63931550,63934347.0,1,"2020/09/17, 08:03:01",True,"2020/09/17, 11:40:33",nan,2467842.0,165.0,0,344,Elastic APM show total number of SQL Queries executed on .Net Core API Endpoint,Currently on .Net Core 2.2.3
Elastic APM,64463745,nan,0,"2020/10/21, 15:40:00",False,"2020/10/21, 15:40:00",nan,3032338.0,3047.0,0,203,elastic apm node agent not showing any data on kibana APM UI,I have a react application with SSR implementation using node js Express.
Elastic APM,64463745,nan,0,"2020/10/21, 15:40:00",False,"2020/10/21, 15:40:00",nan,3032338.0,3047.0,0,203,elastic apm node agent not showing any data on kibana APM UI,I am trying to instrument the application with elastic APM.
Elastic APM,64463745,nan,0,"2020/10/21, 15:40:00",False,"2020/10/21, 15:40:00",nan,3032338.0,3047.0,0,203,elastic apm node agent not showing any data on kibana APM UI,Following  Elastic APM   docs I added below changes to index js file to start APM agent.
Elastic APM,64463745,nan,0,"2020/10/21, 15:40:00",False,"2020/10/21, 15:40:00",nan,3032338.0,3047.0,0,203,elastic apm node agent not showing any data on kibana APM UI,index.js :
Elastic APM,64463745,nan,0,"2020/10/21, 15:40:00",False,"2020/10/21, 15:40:00",nan,3032338.0,3047.0,0,203,elastic apm node agent not showing any data on kibana APM UI,"Getting below message on local env console: APM Server transport error
(ECONNREFUSED): connect ECONNREFUSED 127.0.0.1:8200"
Elastic APM,64463745,nan,0,"2020/10/21, 15:40:00",False,"2020/10/21, 15:40:00",nan,3032338.0,3047.0,0,203,elastic apm node agent not showing any data on kibana APM UI,On Kibana APM dashboard UI we get Avg.response time N/A and Trans.per minute is 0.
Elastic APM,64463745,nan,0,"2020/10/21, 15:40:00",False,"2020/10/21, 15:40:00",nan,3032338.0,3047.0,0,203,elastic apm node agent not showing any data on kibana APM UI,Please suggest how this can be debugged.
Elastic APM,64463745,nan,0,"2020/10/21, 15:40:00",False,"2020/10/21, 15:40:00",nan,3032338.0,3047.0,0,203,elastic apm node agent not showing any data on kibana APM UI,Reference Article:  Distributed Tracing in your Kibana with Node.JS
Elastic APM,65441133,65484213.0,1,"2020/12/24, 18:51:41",True,"2020/12/28, 23:19:29",nan,1175351.0,109.0,0,75,Elastic apm - Disable transaction/span programatically for specific endpoint,I am using elastic-apm with spring application to monitor API requests and track all SQL's executed for given endpoint.
Elastic APM,65441133,65484213.0,1,"2020/12/24, 18:51:41",True,"2020/12/28, 23:19:29",nan,1175351.0,109.0,0,75,Elastic apm - Disable transaction/span programatically for specific endpoint,The problem is give the amount of traffic elastic search is collecting huge magnitude of data and I would like to enable capturing span only for specific endpoints.
Elastic APM,65441133,65484213.0,1,"2020/12/24, 18:51:41",True,"2020/12/28, 23:19:29",nan,1175351.0,109.0,0,75,Elastic apm - Disable transaction/span programatically for specific endpoint,"I tried using public api of elastic-apm  https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html 
I can customize a transaction and span but I couldn't find a way to enable/disable to specific endpoints."
Elastic APM,65441133,65484213.0,1,"2020/12/24, 18:51:41",True,"2020/12/28, 23:19:29",nan,1175351.0,109.0,0,75,Elastic apm - Disable transaction/span programatically for specific endpoint,I have tried this but no luck -
Elastic APM,66026533,66062278.0,1,"2021/02/03, 13:24:26",True,"2021/02/05, 13:15:44","2021/02/03, 13:30:27",8278891.0,59.0,0,136,Elastic Apm python agent connection problem,I have a very basic Django APM agent setup:
Elastic APM,66026533,66062278.0,1,"2021/02/03, 13:24:26",True,"2021/02/05, 13:15:44","2021/02/03, 13:30:27",8278891.0,59.0,0,136,Elastic Apm python agent connection problem,My APM server is up and running on localhost:8200.
Elastic APM,66026533,66062278.0,1,"2021/02/03, 13:24:26",True,"2021/02/05, 13:15:44","2021/02/03, 13:30:27",8278891.0,59.0,0,136,Elastic Apm python agent connection problem,But it seems my APM agent can't make a connection to the APM server.
Elastic APM,66026533,66062278.0,1,"2021/02/03, 13:24:26",True,"2021/02/05, 13:15:44","2021/02/03, 13:30:27",8278891.0,59.0,0,136,Elastic Apm python agent connection problem,Here is a part of my log file that I think cause the problem:
Elastic APM,66026533,66062278.0,1,"2021/02/03, 13:24:26",True,"2021/02/05, 13:15:44","2021/02/03, 13:30:27",8278891.0,59.0,0,136,Elastic Apm python agent connection problem,Here are my APM logs:
Elastic APM,66026533,66062278.0,1,"2021/02/03, 13:24:26",True,"2021/02/05, 13:15:44","2021/02/03, 13:30:27",8278891.0,59.0,0,136,Elastic Apm python agent connection problem,"On my APM server, I'm not receiving any requests from my agent."
Elastic APM,66026533,66062278.0,1,"2021/02/03, 13:24:26",True,"2021/02/05, 13:15:44","2021/02/03, 13:30:27",8278891.0,59.0,0,136,Elastic Apm python agent connection problem,I checked the APM server log files.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,I'm trying out the .Net agent in Elastic APM and I'm using a C# application which is created using a framework called ASP.net Boilerplate.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,I've added the core libraries as mentioned in the documentation and added the settings in appsettings.json.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,This enables the default instrumentation and I got traces in the APM visualized through Kibana.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,Currently I've got a node.js application running and I publish a message to a RabbitMQ queue with the traceparent in the message payload.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,The C# app reads the published message.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,I need to create a transaction or span using this traceparent / trace id so that Kibana would show the trace among the distributed systems.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,I've checked the Elastic APM agent documentation -&gt; Public API for information but couldnt find any information on this.
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,Is there a way?
Elastic APM,66088061,66089252.0,2,"2021/02/07, 14:43:54",True,"2021/02/08, 03:19:53",nan,2803435.0,417.0,0,161,Elastic APM - Creating Transaction / Span using traceparent / trace_id in C# Agent Libraries,Thanks.
Elastic APM,66219545,nan,1,"2021/02/16, 08:19:28",False,"2021/02/16, 10:51:24",nan,2803435.0,417.0,0,23,Setting up Elastic APM .Net Agent in Visual Studio 2019,I'm trying to figure out a way to add a custom implementation to the Elastic APM .Net Agent code.
Elastic APM,66219545,nan,1,"2021/02/16, 08:19:28",False,"2021/02/16, 10:51:24",nan,2803435.0,417.0,0,23,Setting up Elastic APM .Net Agent in Visual Studio 2019,Does anyone know how to set it up in Visual Studio 2019?
Elastic APM,66219545,nan,1,"2021/02/16, 08:19:28",False,"2021/02/16, 10:51:24",nan,2803435.0,417.0,0,23,Setting up Elastic APM .Net Agent in Visual Studio 2019,Any references that I could use to refer on setting it up.
Elastic APM,66219545,nan,1,"2021/02/16, 08:19:28",False,"2021/02/16, 10:51:24",nan,2803435.0,417.0,0,23,Setting up Elastic APM .Net Agent in Visual Studio 2019,Thanks.
Elastic APM,66316731,nan,1,"2021/02/22, 15:34:40",True,"2021/02/23, 00:45:53",nan,3296031.0,1.0,0,56,How to create custom span with all details as it&#39;s created by elastic-apm-agent?,I'm creating custom spans for outgoing requests for my java app.
Elastic APM,66316731,nan,1,"2021/02/22, 15:34:40",True,"2021/02/23, 00:45:53",nan,3296031.0,1.0,0,56,How to create custom span with all details as it&#39;s created by elastic-apm-agent?,"It works and it's great ;)
But... when I compare my custom span on Kibana's APM board with other spans from springboot applications, which are created by elastic-apm-agent, then I can see that my spans are very low on additional details."
Elastic APM,66316731,nan,1,"2021/02/22, 15:34:40",True,"2021/02/23, 00:45:53",nan,3296031.0,1.0,0,56,How to create custom span with all details as it&#39;s created by elastic-apm-agent?,I would like to have at least the URL details included in my custom span.
Elastic APM,66316731,nan,1,"2021/02/22, 15:34:40",True,"2021/02/23, 00:45:53",nan,3296031.0,1.0,0,56,How to create custom span with all details as it&#39;s created by elastic-apm-agent?,The apm-agent-api doesn't allow this.
Elastic APM,66316731,nan,1,"2021/02/22, 15:34:40",True,"2021/02/23, 00:45:53",nan,3296031.0,1.0,0,56,How to create custom span with all details as it&#39;s created by elastic-apm-agent?,"Is there a way to set additional details, like url, to custom span?"
Elastic APM,66316731,nan,1,"2021/02/22, 15:34:40",True,"2021/02/23, 00:45:53",nan,3296031.0,1.0,0,56,How to create custom span with all details as it&#39;s created by elastic-apm-agent?,(I don't want to use labels for this)
Elastic APM,66316731,nan,1,"2021/02/22, 15:34:40",True,"2021/02/23, 00:45:53",nan,3296031.0,1.0,0,56,How to create custom span with all details as it&#39;s created by elastic-apm-agent?,Thanks
Elastic APM,66553111,nan,0,"2021/03/09, 20:56:23",False,"2021/03/09, 20:56:23",nan,15171016.0,1.0,0,50,How can install elastic apm for log in laravel?,I want to install apm elastic for log errors and transactions in lumen.
Elastic APM,66553111,nan,0,"2021/03/09, 20:56:23",False,"2021/03/09, 20:56:23",nan,15171016.0,1.0,0,50,How can install elastic apm for log in laravel?,I found a package for elastic called anik/elastic-apm-php but when I install with composer I see below error
Elastic APM,66553111,nan,0,"2021/03/09, 20:56:23",False,"2021/03/09, 20:56:23",nan,15171016.0,1.0,0,50,How can install elastic apm for log in laravel?,How to use elastic log monitoring?
Elastic APM,66560978,nan,0,"2021/03/10, 10:30:01",False,"2021/03/10, 10:30:01",nan,5421539.0,22408.0,0,27,How can I make elastic APM work with AWS xray?,I am planning to deploy ELK to monitor my application running in AWS.
Elastic APM,66560978,nan,0,"2021/03/10, 10:30:01",False,"2021/03/10, 10:30:01",nan,5421539.0,22408.0,0,27,How can I make elastic APM work with AWS xray?,My apps are using AWS xray for trace data.
Elastic APM,66560978,nan,0,"2021/03/10, 10:30:01",False,"2021/03/10, 10:30:01",nan,5421539.0,22408.0,0,27,How can I make elastic APM work with AWS xray?,I am reading the doc about elastic APM to see how to ingest AWS xray to elasticsearch but I can't find any solution.
Elastic APM,66560978,nan,0,"2021/03/10, 10:30:01",False,"2021/03/10, 10:30:01",nan,5421539.0,22408.0,0,27,How can I make elastic APM work with AWS xray?,I have read the agent doc  https://www.elastic.co/guide/en/apm/agent/nodejs/3.x/intro.html  but xray is not listed as supported framework.
Elastic APM,66560978,nan,0,"2021/03/10, 10:30:01",False,"2021/03/10, 10:30:01",nan,5421539.0,22408.0,0,27,How can I make elastic APM work with AWS xray?,Does this mean I need to build a xray agent and send the trace data to APM server?
Elastic APM,66560978,nan,0,"2021/03/10, 10:30:01",False,"2021/03/10, 10:30:01",nan,5421539.0,22408.0,0,27,How can I make elastic APM work with AWS xray?,Or is there an easier way to do that?
Elastic APM,66760339,nan,0,"2021/03/23, 11:21:49",False,"2021/03/23, 11:21:49",nan,5421539.0,22408.0,0,12,How can I create traceId and start/end time when using elastic APM API?,I am using  https://github.com/elastic/apm-agent-nodejs  in a node application to send trace data to Elastic APM Server.
Elastic APM,66760339,nan,0,"2021/03/23, 11:21:49",False,"2021/03/23, 11:21:49",nan,5421539.0,22408.0,0,12,How can I create traceId and start/end time when using elastic APM API?,I will send trace data to APM server and I am able to view them via Kibana.
Elastic APM,66760339,nan,0,"2021/03/23, 11:21:49",False,"2021/03/23, 11:21:49",nan,5421539.0,22408.0,0,12,How can I create traceId and start/end time when using elastic APM API?,"The trace id, start/end time is calculated by APM client or server."
Elastic APM,66760339,nan,0,"2021/03/23, 11:21:49",False,"2021/03/23, 11:21:49",nan,5421539.0,22408.0,0,12,How can I create traceId and start/end time when using elastic APM API?,How can I provide my own trace id and start/end time for each transaction and its spans?
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?","I'm able to install elasticseach and kibana, both are up and running."
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?","In Kibana dashboard APM server is setup, and indices are showing up."
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?",I am getting the following error for APM-Agent when I trace the log.
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?",ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Error trying to connect to APM Server.
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?",Some details about SSL configurations corresponding the current connection are logged at INFO level.
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?",ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Failed to handle event of type JSON_WRITER withthis error: connect timed out
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?",APM-Agent Yaml File
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?",ElasticSearch Yaml
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?",Kibana Yaml
Elastic APM,66781290,nan,2,"2021/03/24, 14:42:08",True,"2021/03/25, 13:27:56","2021/03/24, 14:49:57",15469419.0,1.0,0,134,"How to install ELK in Kubernetes (AKS) with Elastic APM, Metricbeat and Filebeat?",APM Server Yaml
Elastic APM,66868273,66869872.0,1,"2021/03/30, 12:31:58",True,"2021/03/30, 15:56:11","2021/03/30, 13:47:01",4189363.0,71.0,0,42,How to Viewing trace logs from OpenTelemetry in Elastic APM,"I receive logs from opentelemetry-collector in Elastic APM
logs structure :"
Elastic APM,66868273,66869872.0,1,"2021/03/30, 12:31:58",True,"2021/03/30, 15:56:11","2021/03/30, 13:47:01",4189363.0,71.0,0,42,How to Viewing trace logs from OpenTelemetry in Elastic APM,&quot;{Timestamp:HH:mm:ss} {Level:u3} trace.id={TraceId} transaction.id={SpanId}{NewLine}{Message:lj}{NewLine}{Exception}&quot;
Elastic APM,66868273,66869872.0,1,"2021/03/30, 12:31:58",True,"2021/03/30, 15:56:11","2021/03/30, 13:47:01",4189363.0,71.0,0,42,How to Viewing trace logs from OpenTelemetry in Elastic APM,example:
Elastic APM,66868273,66869872.0,1,"2021/03/30, 12:31:58",True,"2021/03/30, 15:56:11","2021/03/30, 13:47:01",4189363.0,71.0,0,42,How to Viewing trace logs from OpenTelemetry in Elastic APM,I tried use pipeline
Elastic APM,66868273,66869872.0,1,"2021/03/30, 12:31:58",True,"2021/03/30, 15:56:11","2021/03/30, 13:47:01",4189363.0,71.0,0,42,How to Viewing trace logs from OpenTelemetry in Elastic APM,My goal see logs in Elastic APM
Elastic APM,66930365,nan,0,"2021/04/03, 13:57:06",False,"2021/04/04, 08:45:02","2021/04/04, 08:45:02",936651.0,8972.0,0,17,Elastic apm over nodejs is not sending error messages,I have the following simple code below to run APM with logs.
Elastic APM,66930365,nan,0,"2021/04/03, 13:57:06",False,"2021/04/04, 08:45:02","2021/04/04, 08:45:02",936651.0,8972.0,0,17,Elastic apm over nodejs is not sending error messages,APM is firing stats and looks like it works great.
Elastic APM,66930365,nan,0,"2021/04/03, 13:57:06",False,"2021/04/04, 08:45:02","2021/04/04, 08:45:02",936651.0,8972.0,0,17,Elastic apm over nodejs is not sending error messages,"But the problems that the errors are not appearing anywhere in Kibana 
What am I missing?"
Elastic APM,66930365,nan,0,"2021/04/03, 13:57:06",False,"2021/04/04, 08:45:02","2021/04/04, 08:45:02",936651.0,8972.0,0,17,Elastic apm over nodejs is not sending error messages,Thanks
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,"Elasticsearch,kibana and apm-server are installed in a ec2 server
I have installed automatic java agent attach to another server to track jenkins app"
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,Agent is getting attached to the process but dynamic configuration options are not working
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,Apmagent directory: (command ls)
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,apm-agent-attach-standalone.jar elasticapm.properties
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,elasticapm.properties file
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,Attach Command:
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,sudo java -jar apm-agent-attach-standalone.jar --include '.
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,jenkins. '
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,-&gt;This doesn't pick configuration file but attached the agent
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,so i used below command to update
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,Log:
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,Query:
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,1.Which is the right way to use the configuration options in command line?
Elastic APM,67057172,67065273.0,1,"2021/04/12, 13:58:28",True,"2021/04/13, 00:52:22",nan,13610953.0,31.0,0,26,Elastic APM Agent Configuration Options are not working,2.Do we need to create a log file or it will create if log_file is used..now its polluting the application log
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,I am implementing apm agent to sonarqube and I can connect to apm server using curl
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,curl -v &quot;&quot; returns 200
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,Command:
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,sudo java -jar apm-agent-attach-standalone.jar --include '.
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,sonar. '
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,---config config_file=elasticapm.properties
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,Command output:
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,It shows agent as attached
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,Logfile:
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,Query:
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,It shows access denied exception ... How to fix this?
Elastic APM,67194918,nan,0,"2021/04/21, 14:25:02",False,"2021/04/21, 14:30:42","2021/04/21, 14:30:42",13610953.0,31.0,0,12,Elastic APM Agent Access Denied with AccessControlException,"Note: 
For server_url i tried using properties file, --config server_url, --args server_url which usually works"
Elastic APM,60947830,nan,1,"2020/03/31, 12:56:30",False,"2020/10/23, 18:22:38","2020/10/23, 18:22:38",12960007.0,1.0,-2,145,In elastic APM does @CaptureTransaction create a new tab for external calls,"for the code below I wanted to create a new tab which would store all the external method calls and would be aggregated, but I am not getting any such results."
Elastic APM,59597548,nan,0,"2020/01/05, 08:28:54",False,"2020/01/10, 06:38:01","2020/01/10, 06:38:01",4431422.0,1989.0,5,654,Advantage of opentracing/jaeger over APM tracing capabilities,I was looking at APM tools.
Elastic APM,59597548,nan,0,"2020/01/05, 08:28:54",False,"2020/01/10, 06:38:01","2020/01/10, 06:38:01",4431422.0,1989.0,5,654,Advantage of opentracing/jaeger over APM tracing capabilities,Essentially Dynatrace and I could see that it also provides tracing capabilities that seem to be language agnostic and also without code modifications.
Elastic APM,59597548,nan,0,"2020/01/05, 08:28:54",False,"2020/01/10, 06:38:01","2020/01/10, 06:38:01",4431422.0,1989.0,5,654,Advantage of opentracing/jaeger over APM tracing capabilities,Where would jaeger/open tracing be a better option than a tool like dynatrace?
Elastic APM,59597548,nan,0,"2020/01/05, 08:28:54",False,"2020/01/10, 06:38:01","2020/01/10, 06:38:01",4431422.0,1989.0,5,654,Advantage of opentracing/jaeger over APM tracing capabilities,"Yes, dynatrace (or others like Elastic APM) is capable of providing a lot more insight into an application other than tracing."
Elastic APM,59597548,nan,0,"2020/01/05, 08:28:54",False,"2020/01/10, 06:38:01","2020/01/10, 06:38:01",4431422.0,1989.0,5,654,Advantage of opentracing/jaeger over APM tracing capabilities,"But just from tracing perspective...
What advantages or capabilities does jaeger have that are better than APM tooling or not available in APMs."
Elastic APM,59597548,nan,0,"2020/01/05, 08:28:54",False,"2020/01/10, 06:38:01","2020/01/10, 06:38:01",4431422.0,1989.0,5,654,Advantage of opentracing/jaeger over APM tracing capabilities,ONLY from the tracing perspective.
Elastic APM,55457083,64793314.0,1,"2019/04/01, 17:05:06",True,"2020/11/11, 21:58:43",nan,3019008.0,2674.0,5,261,How do I set up Elastic Node APM distributed tracing to work with Kafka and multiple Node services?,"I'm using Kafka for a queue, with Node services producing and consuming messages to Kafka topics using  Kafka-Node ."
Elastic APM,55457083,64793314.0,1,"2019/04/01, 17:05:06",True,"2020/11/11, 21:58:43",nan,3019008.0,2674.0,5,261,How do I set up Elastic Node APM distributed tracing to work with Kafka and multiple Node services?,"I've been using a home-brewed distributed tracing solution, but now we are moving to the Elastic APM."
Elastic APM,55457083,64793314.0,1,"2019/04/01, 17:05:06",True,"2020/11/11, 21:58:43",nan,3019008.0,2674.0,5,261,How do I set up Elastic Node APM distributed tracing to work with Kafka and multiple Node services?,"This seems to be tailored to HTTP servers, but how do I configure it to work with Kafka?"
Elastic APM,55457083,64793314.0,1,"2019/04/01, 17:05:06",True,"2020/11/11, 21:58:43",nan,3019008.0,2674.0,5,261,How do I set up Elastic Node APM distributed tracing to work with Kafka and multiple Node services?,"I want to be able to track transactions like the following: Service A sends an HTTP request to Service B, which produces it to Kafka Topic C, from which it is consumed by Service D, which puts some data into Kafka Topic E, from which it is consumed by Service B."
Elastic APM,52696696,nan,1,"2018/10/08, 09:40:50",False,"2018/10/09, 09:00:41","2018/10/09, 08:50:42",1014471.0,73.0,4,426,Kubernetes drops HTTP connection initialized by node.js / postgres,I have a very simple piece of code written in node.js which runs on Kubernetes and AWS.
Elastic APM,52696696,nan,1,"2018/10/08, 09:40:50",False,"2018/10/09, 09:00:41","2018/10/09, 08:50:42",1014471.0,73.0,4,426,Kubernetes drops HTTP connection initialized by node.js / postgres,The app just does POST/GET request to create and get data from other services.
Elastic APM,52696696,nan,1,"2018/10/08, 09:40:50",False,"2018/10/09, 09:00:41","2018/10/09, 08:50:42",1014471.0,73.0,4,426,Kubernetes drops HTTP connection initialized by node.js / postgres,service1-- service2- service3
Elastic APM,52696696,nan,1,"2018/10/08, 09:40:50",False,"2018/10/09, 09:00:41","2018/10/09, 08:50:42",1014471.0,73.0,4,426,Kubernetes drops HTTP connection initialized by node.js / postgres,"Service1 get post request and call service2, service2 calls postgres DB (using sequlize) and create a new row and then call service3, service3 get data from the DB and returns the response to service2, service2 returns the response to service1."
Elastic APM,52696696,nan,1,"2018/10/08, 09:40:50",False,"2018/10/09, 09:00:41","2018/10/09, 08:50:42",1014471.0,73.0,4,426,Kubernetes drops HTTP connection initialized by node.js / postgres,"Most of the times it works, but once in 4-5 attempts + concurrency, it dropped and I got a timeout."
Elastic APM,52696696,nan,1,"2018/10/08, 09:40:50",False,"2018/10/09, 09:00:41","2018/10/09, 08:50:42",1014471.0,73.0,4,426,Kubernetes drops HTTP connection initialized by node.js / postgres,the problem is that the service1 receives the response back (according to the logs and network traces) but it seems that the connection was dropped somewhere between the services and I got a timeout ( ESOCKETTIMEDOUT ).
Elastic APM,52696696,nan,1,"2018/10/08, 09:40:50",False,"2018/10/09, 09:00:41","2018/10/09, 08:50:42",1014471.0,73.0,4,426,Kubernetes drops HTTP connection initialized by node.js / postgres,Is it possible Kubernetes drops my connection?
Elastic APM,60777942,nan,0,"2020/03/20, 18:22:45",False,"2021/02/14, 12:23:15","2021/02/14, 12:23:15",489003.0,5147.0,2,141,Injecting proxy credentials to all HttpClient requests,I'm working on a corporate ASP.NET Core application that needs to reach out to a cloud resource (Elastic APM).
Elastic APM,60777942,nan,0,"2020/03/20, 18:22:45",False,"2021/02/14, 12:23:15","2021/02/14, 12:23:15",489003.0,5147.0,2,141,Injecting proxy credentials to all HttpClient requests,"Unfortunately, our corporate proxy is swatting down the request before it can complete."
Elastic APM,60777942,nan,0,"2020/03/20, 18:22:45",False,"2021/02/14, 12:23:15","2021/02/14, 12:23:15",489003.0,5147.0,2,141,Injecting proxy credentials to all HttpClient requests,"It's not my code that making the requests but code within a NuGet package, so I can't easily change how it's making the connection."
Elastic APM,60777942,nan,0,"2020/03/20, 18:22:45",False,"2021/02/14, 12:23:15","2021/02/14, 12:23:15",489003.0,5147.0,2,141,Injecting proxy credentials to all HttpClient requests,I'm hoping to use middleware to inject the proxy details before it goes out the door.
Elastic APM,60777942,nan,0,"2020/03/20, 18:22:45",False,"2021/02/14, 12:23:15","2021/02/14, 12:23:15",489003.0,5147.0,2,141,Injecting proxy credentials to all HttpClient requests,"I've tried this below, but it doesn't seem to be working:"
Elastic APM,60777942,nan,0,"2020/03/20, 18:22:45",False,"2021/02/14, 12:23:15","2021/02/14, 12:23:15",489003.0,5147.0,2,141,Injecting proxy credentials to all HttpClient requests,"Long-term, I am going to whitelist the cloud resource."
Elastic APM,60777942,nan,0,"2020/03/20, 18:22:45",False,"2021/02/14, 12:23:15","2021/02/14, 12:23:15",489003.0,5147.0,2,141,Injecting proxy credentials to all HttpClient requests,"But as I prototype this solution, I'd prefer not to go through that red tape..."
Elastic APM,56832275,nan,1,"2019/07/01, 11:25:14",False,"2019/07/01, 15:24:24",nan,4108584.0,1252.0,2,398,How to manage dependencies for javaagent?,"I am using elastic apm agent for monitoring, I have to download the apm-agent.jar and included it in my start entry point like  java -javaagent:/path/to/apm-agent.jar app.jar ."
Elastic APM,56832275,nan,1,"2019/07/01, 11:25:14",False,"2019/07/01, 15:24:24",nan,4108584.0,1252.0,2,398,How to manage dependencies for javaagent?,"The problem is I have to manually download the apm-agent.jar, is there a way that I can configure the apm agent in my Gradle dependencies?"
Elastic APM,56832275,nan,1,"2019/07/01, 11:25:14",False,"2019/07/01, 15:24:24",nan,4108584.0,1252.0,2,398,How to manage dependencies for javaagent?,and then refer to the path of the jar file that was downloaded by gradle in the Dockerfile?
Elastic APM,56832275,nan,1,"2019/07/01, 11:25:14",False,"2019/07/01, 15:24:24",nan,4108584.0,1252.0,2,398,How to manage dependencies for javaagent?,What is the proper way of dependency management for jar files like java agent?
Elastic APM,65703890,nan,2,"2021/01/13, 16:26:30",False,"2021/01/14, 05:02:19",nan,2501609.0,31.0,1,55,Using ByteBuddy to instrument another java agent,I'm using the  Elastic APM Agent  as a Java Agent to monitor usages of various methods in my spring boot microservice.
Elastic APM,65703890,nan,2,"2021/01/13, 16:26:30",False,"2021/01/14, 05:02:19",nan,2501609.0,31.0,1,55,Using ByteBuddy to instrument another java agent,This all works fine and we're able to graph various metrics in Kibana.
Elastic APM,65703890,nan,2,"2021/01/13, 16:26:30",False,"2021/01/14, 05:02:19",nan,2501609.0,31.0,1,55,Using ByteBuddy to instrument another java agent,Unfortunately what it doesn't do is consistently attach the same labels to all spans within a transaction e.g.
Elastic APM,65703890,nan,2,"2021/01/13, 16:26:30",False,"2021/01/14, 05:02:19",nan,2501609.0,31.0,1,55,Using ByteBuddy to instrument another java agent,details of the user who made the original request.
Elastic APM,65703890,nan,2,"2021/01/13, 16:26:30",False,"2021/01/14, 05:02:19",nan,2501609.0,31.0,1,55,Using ByteBuddy to instrument another java agent,To work around this I thought I could use ByteBuddy (which I've never used before) to wrap any usages of the APM Span class and attach that information (as it's readily available from ThreadLocal) to each instance.
Elastic APM,65703890,nan,2,"2021/01/13, 16:26:30",False,"2021/01/14, 05:02:19",nan,2501609.0,31.0,1,55,Using ByteBuddy to instrument another java agent,I am however having issues accessing the Span class as it's in the APM Java Agent and with the following code I get the following logs where it appears that it's not able to find the Span class...
Elastic APM,65703890,nan,2,"2021/01/13, 16:26:30",False,"2021/01/14, 05:02:19",nan,2501609.0,31.0,1,55,Using ByteBuddy to instrument another java agent,"I've tried using ByteBuddy for my own classes and it all works without issue, but I'm getting very confused around which classloader has loaded what and how to point ByteBuddy at them."
Elastic APM,65484826,65498418.0,2,"2020/12/29, 00:19:51",True,"2021/01/31, 12:36:15","2020/12/29, 00:58:47",6352160.0,1764.0,1,176,What is the best way to add APM to NuxtJS project,What is the right way to configure/enable an Elastic APM agent in a Nuxtjs project?
Elastic APM,65484826,65498418.0,2,"2020/12/29, 00:19:51",True,"2021/01/31, 12:36:15","2020/12/29, 00:58:47",6352160.0,1764.0,1,176,What is the best way to add APM to NuxtJS project,I referred  this documentation  for a custom NodeJS app.
Elastic APM,65484826,65498418.0,2,"2020/12/29, 00:19:51",True,"2021/01/31, 12:36:15","2020/12/29, 00:58:47",6352160.0,1764.0,1,176,What is the best way to add APM to NuxtJS project,The key takeaway was:
Elastic APM,65484826,65498418.0,2,"2020/12/29, 00:19:51",True,"2021/01/31, 12:36:15","2020/12/29, 00:58:47",6352160.0,1764.0,1,176,What is the best way to add APM to NuxtJS project,"It’s important that the agent is started before you require any other
modules in your Node.js application - i.e."
Elastic APM,65484826,65498418.0,2,"2020/12/29, 00:19:51",True,"2021/01/31, 12:36:15","2020/12/29, 00:58:47",6352160.0,1764.0,1,176,What is the best way to add APM to NuxtJS project,"before http and before your
router etc."
Elastic APM,65484826,65498418.0,2,"2020/12/29, 00:19:51",True,"2021/01/31, 12:36:15","2020/12/29, 00:58:47",6352160.0,1764.0,1,176,What is the best way to add APM to NuxtJS project,"I added the following snippet in nuxt.config.js, but the APM agent is not started or working."
Elastic APM,65484826,65498418.0,2,"2020/12/29, 00:19:51",True,"2021/01/31, 12:36:15","2020/12/29, 00:58:47",6352160.0,1764.0,1,176,What is the best way to add APM to NuxtJS project,I do not see any errors in the app logs.
Elastic APM,65484826,65498418.0,2,"2020/12/29, 00:19:51",True,"2021/01/31, 12:36:15","2020/12/29, 00:58:47",6352160.0,1764.0,1,176,What is the best way to add APM to NuxtJS project,Is there any other way to do this?
Elastic APM,65380989,65384228.0,1,"2020/12/20, 16:34:07",True,"2020/12/20, 22:08:14",nan,11412317.0,192.0,1,172,Passing Java APM Agent settings in Docker,"I monitor my jar using Elastic APM Agent, i run these commands manually :"
Elastic APM,65380989,65384228.0,1,"2020/12/20, 16:34:07",True,"2020/12/20, 22:08:14",nan,11412317.0,192.0,1,172,Passing Java APM Agent settings in Docker,"Now , i want to pass these parameters using docker run , i create the image and i try with this command to pass these settings , but the application is not starting"
Elastic APM,65380989,65384228.0,1,"2020/12/20, 16:34:07",True,"2020/12/20, 22:08:14",nan,11412317.0,192.0,1,172,Passing Java APM Agent settings in Docker,any idea to resolve this ?
Elastic APM,65380989,65384228.0,1,"2020/12/20, 16:34:07",True,"2020/12/20, 22:08:14",nan,11412317.0,192.0,1,172,Passing Java APM Agent settings in Docker,Thanks
Elastic APM,59228679,59233925.0,1,"2019/12/07, 19:43:51",True,"2019/12/08, 11:07:43",nan,1516717.0,13.0,1,683,Illegal state exception: Error during attachment using: co.elastic.apm.attach.bytebuddy.agent.ByteBuddyAgent,"I just created one application with springboot , used the Elastic APM attacher of APM tool."
Elastic APM,59228679,59233925.0,1,"2019/12/07, 19:43:51",True,"2019/12/08, 11:07:43",nan,1516717.0,13.0,1,683,Illegal state exception: Error during attachment using: co.elastic.apm.attach.bytebuddy.agent.ByteBuddyAgent,"When i run the apm attacher, it generates error exception as shown below."
Elastic APM,59228679,59233925.0,1,"2019/12/07, 19:43:51",True,"2019/12/08, 11:07:43",nan,1516717.0,13.0,1,683,Illegal state exception: Error during attachment using: co.elastic.apm.attach.bytebuddy.agent.ByteBuddyAgent,Code used to generate the error:
Elastic APM,59228679,59233925.0,1,"2019/12/07, 19:43:51",True,"2019/12/08, 11:07:43",nan,1516717.0,13.0,1,683,Illegal state exception: Error during attachment using: co.elastic.apm.attach.bytebuddy.agent.ByteBuddyAgent,"But if I try to run the application commenting the line i.e //ElasticApmAttacher.attach(); from same code, it runs successfully"
Elastic APM,59228679,59233925.0,1,"2019/12/07, 19:43:51",True,"2019/12/08, 11:07:43",nan,1516717.0,13.0,1,683,Illegal state exception: Error during attachment using: co.elastic.apm.attach.bytebuddy.agent.ByteBuddyAgent,i am searching for solution but so far clueless.
Elastic APM,59228679,59233925.0,1,"2019/12/07, 19:43:51",True,"2019/12/08, 11:07:43",nan,1516717.0,13.0,1,683,Illegal state exception: Error during attachment using: co.elastic.apm.attach.bytebuddy.agent.ByteBuddyAgent,Can someone please suggest how to resolve it
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,I wish to make a change to a standard singleton design pattern that follows the  System.Lazy&lt;T&gt;  design as described  here .
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,The change is to  Agent  in Elastic APM Agent which can be seen in GitHub  here .
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,This is the code broken down for brevity:
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"The clear problem with this is that if  Agent.Instance  is accessed before  Agent.Setup  is called, the  Foo  object in  Agent.Lazy  is instantiated with null ( _bar ) passed to its constructor."
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"Therefore, the expectation that the  Bar  object passed to  Setup  will be used for the underlying  Foo  will not be met."
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"The problem of course is that this is an antipattern as described  here , because this singleton is encapsulating global state."
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,As this link describes:
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,A singleton is a convenient way for accessing the service from anywhere in the application code.
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"The model quickly falls apart when the service not only provides access to operations but also encapsulates state, which affects how other code behaves."
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,Application configuration is a good example of this.
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"In the best case, the configuration is read once at the application start and does not change for the entire lifetime of the application."
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"However, different configuration can cause a method to return different results although no visible dependencies have changed, i.e."
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,the constructor and the method have been called with the same parameters.
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"This can become an even bigger problem if the singleton state can change at runtime, either by rereading the configuration file or by programmatic manipulation."
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,Such code can quickly become very difficult to reason with:
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"Without comments, an uninformed reader of the code above could not expect the values of  before  and  after  to be different, and could only explain it after looking into the implementation of the individual methods, which read and modify global state hidden in  Configuration  singleton."
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,The  article  advocates using DI to resolve this.
Elastic APM,58916834,58917376.0,1,"2019/11/18, 16:30:44",True,"2020/04/19, 16:00:12","2020/06/20, 12:12:55",197591.0,3114.0,1,120,How to refactor stateful singleton while still allowing configurability/setup?,"However, is there a simpler way to resolve this situation where DI is not possible or would involve too much of a refactor?"
Elastic APM,56738411,nan,0,"2019/06/24, 17:10:10",False,"2019/06/24, 17:10:10",nan,3187837.0,162.0,1,353,APM tags/labels not showing up in Kibana,I would like to send some labels along with the stacktrace to the Elastic APM server from the client side.
Elastic APM,56738411,nan,0,"2019/06/24, 17:10:10",False,"2019/06/24, 17:10:10",nan,3187837.0,162.0,1,353,APM tags/labels not showing up in Kibana,"Using the  @elastic/apm-rum  javascript agent, I'm calling  apm.addLabels({ ... })  and passing my object."
Elastic APM,56738411,nan,0,"2019/06/24, 17:10:10",False,"2019/06/24, 17:10:10",nan,3187837.0,162.0,1,353,APM tags/labels not showing up in Kibana,"Looking at the request in my browser, the labels look like they are being sent properly."
Elastic APM,56738411,nan,0,"2019/06/24, 17:10:10",False,"2019/06/24, 17:10:10",nan,3187837.0,162.0,1,353,APM tags/labels not showing up in Kibana,Here's the relevant part of the request.
Elastic APM,56738411,nan,0,"2019/06/24, 17:10:10",False,"2019/06/24, 17:10:10",nan,3187837.0,162.0,1,353,APM tags/labels not showing up in Kibana,"I am expecting that the  browser  and  os  labels show under the  Labels  tab in the error report, but instead I'm seeing this."
Elastic APM,56738411,nan,0,"2019/06/24, 17:10:10",False,"2019/06/24, 17:10:10",nan,3187837.0,162.0,1,353,APM tags/labels not showing up in Kibana,"So my question as an elk novice is, what additional configuration do I need to get the labels in the report?"
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,I installed latest version of ELK stack on Azure (7.0.1).
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,I have apm-server on kubernetes with this docker image:  docker.elastic.co/apm/apm-server:7.0.1
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,"But, it's not connecting with elasticsearch server."
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,Error:
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,ERROR   pipeline/output.go:100  Failed to connect to backoff(elasticsearch( http://x.x.x.x:9200 )): Connection marked as failed because the onConnect callback failed: This Beat requires the default distribution of Elasticsearch.
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,"Please upgrade to the default distribution of Elasticsearch from elastic.co, or downgrade to the oss-only distribution of beats"
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,INFO    pipeline/output.go:93   Attempting to reconnect to backoff(elasticsearch( http://x.x.x.x:9200 )) with 11 reconnect attempt(s)
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,INFO    [publisher]     pipeline/retry.go:189   retryer: send unwait-signal to consumer
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,INFO    [publisher]     pipeline/retry.go:191     done
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,INFO    [publisher]     pipeline/retry.go:166   retryer: send wait signal to consumer
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,INFO    [publisher]     pipeline/retry.go:168     done
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,INFO    elasticsearch/client.go:734     Attempting to connect to Elasticsearch version 7.0.1
Elastic APM,56065263,nan,1,"2019/05/09, 21:09:11",False,"2019/05/22, 10:56:32","2020/06/20, 12:12:55",3067928.0,5033.0,1,1331,Failed to connect to Elasticsearch server,"INFO    [request]       beater/common_handler.go:185    handled request {&quot;request_id&quot;: &quot;2e79d623-b8fb-4743-8b50-b516db256d5b&quot;, &quot;method&quot;: &quot;POST&quot;, &quot;URL&quot;: &quot;/intake/v2/events&quot;, &quot;content_length&quot;: -1, &quot;remote_address&quot;: &quot;10.0.11.11&quot;, &quot;user-agent&quot;: &quot;elastic-apm-node/2.11.0 elastic-apm-http-client/7.3.0&quot;, &quot;response_code&quot;: 202}"
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,For logging in our microservice applications we simply log to stdout/console and the docker logging driver handles and redirects these logs somewhere e.g.
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,"gelf/logstash, fluentd, etc."
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,"Basically, we're following  12 factor  guidelines for logging."
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,This means that developers working on the application code don't need to know anything about the underlying logging solution (e.g.
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,"Elasticsearch, Graylog, Splunk, etc) - it's entirely an ops/configuration concern."
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,In theory we should be able to change the underlying logging solution without any code changes.
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,I'd like something similar for traces and my research has led me to OpenTracing.
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,Developers shouldn't need to know the underlying tracing solution (e.g.
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,"Jaeger, Zipkin, Elastic APM, etc) and as per logging; in theory we should be able to change the underlying tracing solution without any code changes."
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,I've successfully got a .NET core POC sending traces to Jaeger using the  opentracing/opentracing-csharp  and  jaegertracing/jaeger-client-csharp  libraries.
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,"I'm still trying to fully get my head around OpenTracing, but I'm wondering if there's a way to send traces to an OpenTracing compliant API without having to take a hard dependency on a particular solution such as Jaeger (i.e."
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,the jaeger-client-csharp library).
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,Based on my understanding OpenTracing is just a standard.
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,Shouldn't I just be able to configure an OpenTracing endpoint with some sampling options without needing the jaeger-client-csharp library?
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,Or is it that the jaeger-client-csharp is not actually Jaeger specific and can actually send traces to any OpenTracing API?
Elastic APM,55962977,57365645.0,1,"2019/05/03, 06:36:37",True,"2019/08/05, 23:23:09","2019/05/28, 03:03:47",824434.0,1520.0,1,1454,OpenTracing in .NET core without taking dependency on specific solution library,"Example configuration shown below, which uses jaeger client library:"
Elastic APM,67173030,nan,1,"2021/04/20, 08:36:07",False,"2021/04/20, 18:01:19","2021/04/20, 08:52:21",13610953.0,31.0,0,20,Elastic Uptime Monitors using Heartbeat --Few Monitors are missing in kibana,"I have the elk setup in a ec2 server.With Beats like metricbeat,filebeat,heartbeat."
Elastic APM,67173030,nan,1,"2021/04/20, 08:36:07",False,"2021/04/20, 18:01:19","2021/04/20, 08:52:21",13610953.0,31.0,0,20,Elastic Uptime Monitors using Heartbeat --Few Monitors are missing in kibana,I have setup the elastic apm for some applications like jenkins &amp; sonarqube.
Elastic APM,67173030,nan,1,"2021/04/20, 08:36:07",False,"2021/04/20, 18:01:19","2021/04/20, 08:52:21",13610953.0,31.0,0,20,Elastic Uptime Monitors using Heartbeat --Few Monitors are missing in kibana,"Now In uptime I can see only few monitors like sonarqube and jenkins
Other application are missing.."
Elastic APM,67173030,nan,1,"2021/04/20, 08:36:07",False,"2021/04/20, 18:01:19","2021/04/20, 08:52:21",13610953.0,31.0,0,20,Elastic Uptime Monitors using Heartbeat --Few Monitors are missing in kibana,When I see data from yesterday not available in elasticsearch for particular application
Elastic APM,66752733,nan,0,"2021/03/22, 21:34:49",False,"2021/03/22, 21:34:49",nan,14759662.0,1.0,0,27,Instrumenting the included jar library classes,I have been using elastic APM for tracing a application.
Elastic APM,66752733,nan,0,"2021/03/22, 21:34:49",False,"2021/03/22, 21:34:49",nan,14759662.0,1.0,0,27,Instrumenting the included jar library classes,I have a main Spring boot application and another spring based jar file(my own) which is included as a dependency in main Spring boot project.
Elastic APM,66752733,nan,0,"2021/03/22, 21:34:49",False,"2021/03/22, 21:34:49",nan,14759662.0,1.0,0,27,Instrumenting the included jar library classes,"When I add add custom context &amp; indexed labels from main Spring boot project , elastic APM console does shows up that trace data."
Elastic APM,66752733,nan,0,"2021/03/22, 21:34:49",False,"2021/03/22, 21:34:49",nan,14759662.0,1.0,0,27,Instrumenting the included jar library classes,"However , when I write some tracing code inside ( adding index label or adding custom context) inside that spring based project which is then included as a jar libaray inside my spring boot project , that is not shown up on console."
Elastic APM,66752733,nan,0,"2021/03/22, 21:34:49",False,"2021/03/22, 21:34:49",nan,14759662.0,1.0,0,27,Instrumenting the included jar library classes,Does this mean I can only instrument only main project and not instrument included jar library&gt; I have configured packages for both main spring boot project as well included spring dependency.
Elastic APM,66752733,nan,0,"2021/03/22, 21:34:49",False,"2021/03/22, 21:34:49",nan,14759662.0,1.0,0,27,Instrumenting the included jar library classes,Any help is highly appreciated.
Elastic APM,66752733,nan,0,"2021/03/22, 21:34:49",False,"2021/03/22, 21:34:49",nan,14759662.0,1.0,0,27,Instrumenting the included jar library classes,"ElasticApm.currentTransaction().setLabel(&quot;test1&quot;, &quot;test2&quot;) 
 ElasticApm.currentTransaction().addCustomContext(&quot;test3&quot;, &quot;test4&quot;)"
Elastic APM,66752733,nan,0,"2021/03/22, 21:34:49",False,"2021/03/22, 21:34:49",nan,14759662.0,1.0,0,27,Instrumenting the included jar library classes,"Attached ELastic APM before starting spring boot app as :
 ElasticApmAttacher.attach(configMap);"
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,I'm currently facing a very strange issue.
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"I did some optimizations in my queries, which improved quite a lot the overall performance for my Django application's GET requests."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"However, I'm still facing a few very slow ones (1000ms or more)."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"Checking on Elastic APM, I noticed that for all those cases, there was a DB reconnection."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"While it's obvious those requests would take more time, it's still takes WAY more time than the amount required for the reconnection itself."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"I have set the DB_CONN_MAX_AGE to  None , therefore the connections themselves shouldn't be closed at all."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,Which makes me think the reason for the disconnection itself could also indicate the cause for this.
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,The blue bars represent the amount of time a given transaction took.
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,The total amount of time for this particular request was 1599ms.
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"The DB reconnection took 100ms, the the queries about ~20ms."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"Adding those up, gives a total time of 120ms."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,I'm a bit clueless how to find out where the rest of the 1479ms.
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"I did some load tests locally, but couldn't reproduce this particular issue."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"Of course is serializations, middlewares, authentication and other things that might add up some time to requests, but not nearly the 1479ms shown here."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,"It certainly related to the DB connection itself, or better yet, something that happens before that."
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,But I'm not sure how to diagnose it.
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,Specially being unable to reproduce this locally.
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,I'm open to any ideas that could lead to more information on how to solve this problem.
Elastic APM,65658887,nan,0,"2021/01/10, 23:49:20",False,"2021/01/10, 23:49:20",nan,1212095.0,147.0,0,30,Slow requests whenever Psycopg2 reconnects to DB,Or maybe someone had a similar experience and could share it with me?
Elastic APM,64923213,65005103.0,1,"2020/11/20, 05:06:52",True,"2020/11/25, 14:52:51",nan,433570.0,32819.0,0,39,"Elastic, Is there a way to see live configuration after changing setting?",I change some settings to elastic-apm.
Elastic APM,64923213,65005103.0,1,"2020/11/20, 05:06:52",True,"2020/11/25, 14:52:51",nan,433570.0,32819.0,0,39,"Elastic, Is there a way to see live configuration after changing setting?",https://www.elastic.co/guide/en/apm/server/current/configuration-process.html
Elastic APM,64923213,65005103.0,1,"2020/11/20, 05:06:52",True,"2020/11/25, 14:52:51",nan,433570.0,32819.0,0,39,"Elastic, Is there a way to see live configuration after changing setting?",I want to verify if the setting is actually changed.
Elastic APM,64923213,65005103.0,1,"2020/11/20, 05:06:52",True,"2020/11/25, 14:52:51",nan,433570.0,32819.0,0,39,"Elastic, Is there a way to see live configuration after changing setting?",but not sure how to check ..
Elastic APM,64923213,65005103.0,1,"2020/11/20, 05:06:52",True,"2020/11/25, 14:52:51",nan,433570.0,32819.0,0,39,"Elastic, Is there a way to see live configuration after changing setting?",Is there an endpoint where I can view the current configuration?
Elastic APM,64854472,nan,1,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",nan,1973221.0,31.0,0,30,"Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6 registered in the application config",I am using Elastic Search in my MVC Application and getting en error when adding migration.
Elastic APM,64854472,nan,1,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",nan,1973221.0,31.0,0,30,"Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6 registered in the application config",Flow is:
Elastic APM,64854472,nan,1,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",nan,1973221.0,31.0,0,30,"Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6 registered in the application config",There is an warnin message like that
Elastic APM,64854472,nan,1,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",nan,1973221.0,31.0,0,30,"Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6 registered in the application config","The type 'Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6' registered in the application config file as an IDbInterceptor not be loaded."
Elastic APM,64854472,nan,1,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",nan,1973221.0,31.0,0,30,"Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6 registered in the application config",Make sure that the assembly-qualified name is used and that the assembly is available to the running application.
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,Running a .NET Core 3.1 API with an async Controller Method that runs multiple DatabaseRepository async methods.
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,I'm calling DatabaseRepository Tasks like this:
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,DatabaseRepository method all look like this:
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,"What I expected is that most of these async methods will run concurrently, but that doesn't happen."
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,How do I know?
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,With Elastic APM.
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,This is what I see:
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,https://cdn.discordapp.com/attachments/195830344715337728/757569429683830795/unknown.png
Elastic APM,63991710,nan,0,"2020/09/21, 14:51:06",False,"2020/09/21, 20:41:18","2020/09/21, 15:26:29",4553642.0,511.0,0,40,.NET Core with Oracle database async methods not run concurrently,What do I do wrong?
Elastic APM,63576369,nan,1,"2020/08/25, 12:56:51",True,"2020/08/26, 16:09:20","2020/08/25, 13:48:36",716691.0,3816.0,0,45,Elastic.co APM with gke network policies,"I have gke clusters, and I have elasticsearch deployments on elastic.co."
Elastic APM,63576369,nan,1,"2020/08/25, 12:56:51",True,"2020/08/26, 16:09:20","2020/08/25, 13:48:36",716691.0,3816.0,0,45,Elastic.co APM with gke network policies,Now on my gke cluster I have network policies for each pod with egress and ingress rules.
Elastic APM,63576369,nan,1,"2020/08/25, 12:56:51",True,"2020/08/26, 16:09:20","2020/08/25, 13:48:36",716691.0,3816.0,0,45,Elastic.co APM with gke network policies,My issue is that in order to use elastic APM I need to allow egress to my elastic deployment.
Elastic APM,63576369,nan,1,"2020/08/25, 12:56:51",True,"2020/08/26, 16:09:20","2020/08/25, 13:48:36",716691.0,3816.0,0,45,Elastic.co APM with gke network policies,Anyone has an idea how to do that?
Elastic APM,63576369,nan,1,"2020/08/25, 12:56:51",True,"2020/08/26, 16:09:20","2020/08/25, 13:48:36",716691.0,3816.0,0,45,Elastic.co APM with gke network policies,"I am thinking either a list of IPs for elastic.co on the gcp instances to be able to whitelist them in my egress rules, or some kind of proxy between my gke cluster and elastic apm."
Elastic APM,63576369,nan,1,"2020/08/25, 12:56:51",True,"2020/08/26, 16:09:20","2020/08/25, 13:48:36",716691.0,3816.0,0,45,Elastic.co APM with gke network policies,"I know a solution can be to have a local elastic cluster on gcp, but I prefer not to go this way."
Elastic APM,63480314,63480663.0,1,"2020/08/19, 08:06:09",True,"2020/08/19, 08:40:43",nan,14020357.0,41.0,0,390,APM Go Agent isn&#39;t Sending Data to the APM Server,I have an Elastic APM-Server up and running and it has successfully established connection with Elasticsearch.
Elastic APM,63480314,63480663.0,1,"2020/08/19, 08:06:09",True,"2020/08/19, 08:40:43",nan,14020357.0,41.0,0,390,APM Go Agent isn&#39;t Sending Data to the APM Server,Then I installed an Elastic APM Go agent:
Elastic APM,63480314,63480663.0,1,"2020/08/19, 08:06:09",True,"2020/08/19, 08:40:43",nan,14020357.0,41.0,0,390,APM Go Agent isn&#39;t Sending Data to the APM Server,It returned the following:
Elastic APM,63480314,63480663.0,1,"2020/08/19, 08:06:09",True,"2020/08/19, 08:40:43",nan,14020357.0,41.0,0,390,APM Go Agent isn&#39;t Sending Data to the APM Server,Then I setup the  ELASTIC_APM_SERVER_URL  and  ELASTIC_APM_SERVICE_NAME :
Elastic APM,63480314,63480663.0,1,"2020/08/19, 08:06:09",True,"2020/08/19, 08:40:43",nan,14020357.0,41.0,0,390,APM Go Agent isn&#39;t Sending Data to the APM Server,"However, I don't see the agent getting registered in the APM dashboard."
Elastic APM,63480314,63480663.0,1,"2020/08/19, 08:06:09",True,"2020/08/19, 08:40:43",nan,14020357.0,41.0,0,390,APM Go Agent isn&#39;t Sending Data to the APM Server,It isn't sending any data to the APM Server.
Elastic APM,63480314,63480663.0,1,"2020/08/19, 08:06:09",True,"2020/08/19, 08:40:43",nan,14020357.0,41.0,0,390,APM Go Agent isn&#39;t Sending Data to the APM Server,How do I make sure that the agent is running?
Elastic APM,63480314,63480663.0,1,"2020/08/19, 08:06:09",True,"2020/08/19, 08:40:43",nan,14020357.0,41.0,0,390,APM Go Agent isn&#39;t Sending Data to the APM Server,How do I check the agent log as to why it isn't able to connect to the APM server?
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,I am trying to run  Java APM agent on Kubernetes with Springboot 2.3.1.RELEASE
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,I get the following error
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,[elastic-apm-server-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler -  Failed to handle event of type METRICS with this error: / by zero
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,It works well if I run it on VM with same Java version
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,How I am using
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,"APM Agent language and version : Java, elastic-apm-agent-1.17.0.jar,1.16.0.jar, 1.15.0.jar"
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,Java version
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,More logs
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,I tried with previous agent versions 1.16.0 &amp; 1.15.0 but I still get the same error.
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,Can anyone please help me.
Elastic APM,62865170,62871884.0,1,"2020/07/12, 21:44:41",True,"2020/07/13, 11:20:58",nan,2259926.0,31.0,0,896,Error with Java APM agent on kubernetes: Failed to handle event of type METRICS with this error: / by zero,Thank you very much in advance
Elastic APM,60326859,60327397.0,2,"2020/02/20, 21:01:12",True,"2020/02/20, 21:40:06",nan,7133255.0,1.0,0,223,How remove double quotes from postgres table and field names on django?,"I'm working on a django project with postgres where table and field names are generated with
double quotes."
Elastic APM,60326859,60327397.0,2,"2020/02/20, 21:01:12",True,"2020/02/20, 21:40:06",nan,7133255.0,1.0,0,223,How remove double quotes from postgres table and field names on django?,Anyone knows how can I disable this behavior?
Elastic APM,60326859,60327397.0,2,"2020/02/20, 21:01:12",True,"2020/02/20, 21:40:06",nan,7133255.0,1.0,0,223,How remove double quotes from postgres table and field names on django?,[Model definition]
Elastic APM,60326859,60327397.0,2,"2020/02/20, 21:01:12",True,"2020/02/20, 21:40:06",nan,7133255.0,1.0,0,223,How remove double quotes from postgres table and field names on django?,[Migtation]
Elastic APM,60326859,60327397.0,2,"2020/02/20, 21:01:12",True,"2020/02/20, 21:40:06",nan,7133255.0,1.0,0,223,How remove double quotes from postgres table and field names on django?,[DDL Generated]
Elastic APM,60326859,60327397.0,2,"2020/02/20, 21:01:12",True,"2020/02/20, 21:40:06",nan,7133255.0,1.0,0,223,How remove double quotes from postgres table and field names on django?,[DDL Expected]
Elastic APM,60326859,60327397.0,2,"2020/02/20, 21:01:12",True,"2020/02/20, 21:40:06",nan,7133255.0,1.0,0,223,How remove double quotes from postgres table and field names on django?,[requirements]
Elastic APM,58615241,nan,1,"2019/10/29, 22:51:31",False,"2019/12/24, 11:52:49",nan,9684264.0,65.0,0,321,How to monitor queries in Elasticsearch?,We are using  Elastic APM  for monitoring our APIs.
Elastic APM,58615241,nan,1,"2019/10/29, 22:51:31",False,"2019/12/24, 11:52:49",nan,9684264.0,65.0,0,321,How to monitor queries in Elasticsearch?,It shows queries status and useful information about the queries.
Elastic APM,58615241,nan,1,"2019/10/29, 22:51:31",False,"2019/12/24, 11:52:49",nan,9684264.0,65.0,0,321,How to monitor queries in Elasticsearch?,I want to have the same information about the queries which are sent to Elasticsearch server.
Elastic APM,58615241,nan,1,"2019/10/29, 22:51:31",False,"2019/12/24, 11:52:49",nan,9684264.0,65.0,0,321,How to monitor queries in Elasticsearch?,"I want to have information about queries, time, status code, etc."
Elastic APM,58615241,nan,1,"2019/10/29, 22:51:31",False,"2019/12/24, 11:52:49",nan,9684264.0,65.0,0,321,How to monitor queries in Elasticsearch?,Is there any plugin in Elastic stack that I can use for this purpose?
Elastic APM,55688256,nan,0,"2019/04/15, 14:30:48",False,"2019/04/15, 14:30:48",nan,3879339.0,87.0,0,276,read apm env variable from env file golang,"I'm using elastic apm for my go app during
 development i export env variable through the terminal and it's working."
Elastic APM,55688256,nan,0,"2019/04/15, 14:30:48",False,"2019/04/15, 14:30:48",nan,3879339.0,87.0,0,276,read apm env variable from env file golang,but now i want to deploy the app so i need to read variables from the  .env file
Elastic APM,55688256,nan,0,"2019/04/15, 14:30:48",False,"2019/04/15, 14:30:48",nan,3879339.0,87.0,0,276,read apm env variable from env file golang,explanation
Elastic APM,55688256,nan,0,"2019/04/15, 14:30:48",False,"2019/04/15, 14:30:48",nan,3879339.0,87.0,0,276,read apm env variable from env file golang,"i use  go.elastic.co/apm/module/apmhttp  at my app
and when  go.elastic.co/apm  read env variables, 
it can't see  ELASTIC_APM_SERVICE_NAME ,  ELASTIC_APM_SERVER_URL  or  ELASTIC_APM_SECRET_TOKEN 
that existed at my .env on my app files."
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,I'm trying out different approaches for microservices tracing (i'm mostly working with event-driven services using RabbitMQ).
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,What I'm testing:
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,Given the code
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,My questions / remarks / issues
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,"To have this new span, I'm using  ScopedSpan sp = tracer.startScopedSpanWithParent(""getUrlConnection"", tracer.currentSpan().context());  and  sp.finish() ."
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,The span is visible in ZipKin but it's not really appealing compared to only putting a  @NewSpan .
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,Am I missing something ?
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,Elasticsearch APM agent + API seems to handle this properly by just requiring the addition of  @CaptureTransaction  and  @CaptureSpan .
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,I know it's not perfect because it doesn't hook directly on the consumer call nor support tracing effectively with my use case.
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,But it also requires to add the agent.
Elastic APM,54559465,nan,1,"2019/02/06, 19:35:34",True,"2020/10/23, 18:23:54","2020/10/23, 18:23:54",2429178.0,542.0,0,624,Custom spans using annotations,Thank you :).
Jaeger,51785812,51790666.0,5,"2018/08/10, 14:45:27",True,"2020/11/09, 18:47:27","2018/08/10, 19:14:54",10206966.0,373.0,12,7412,How to configure Jaeger with elasticsearch?,I have tried executing this docker command to setup Jaeger Agent and jaeger collector with elasticsearch.
Jaeger,51785812,51790666.0,5,"2018/08/10, 14:45:27",True,"2020/11/09, 18:47:27","2018/08/10, 19:14:54",10206966.0,373.0,12,7412,How to configure Jaeger with elasticsearch?,but this command gives the below error.
Jaeger,51785812,51790666.0,5,"2018/08/10, 14:45:27",True,"2020/11/09, 18:47:27","2018/08/10, 19:14:54",10206966.0,373.0,12,7412,How to configure Jaeger with elasticsearch?,How to configure Jaeger with ElasticSearch?
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,There is an existing Spring Boot app which is using SLF4J logger.
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,I decided to add the support of distributed tracing via standard  opentracing  API with Jaeger as the tracer.
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,It is really amazing how easy the initial setup is - all that is required is just adding two dependencies to the  pom.xml :
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,and providing the  Tracer  bean with the configuration:
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,All works like a charm - app requests are processed by Jaeger and spans are created:
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,"However, in the span  Logs  there are only  preHandle  &amp;  afterCompletion  events with info about the class / method that were called during request execution (no logs produced by  slf4j  logger are collected) :"
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,The question is if it is possible to configure the Tracer to pickup the logs produced by the app logger ( slf4j  in my case) so that all the application logs done via:  LOG.info  /  LOG.warn  /  LOG.error  etc.
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,would be also reflected in Jaeger
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,NOTE : I have figured out how to log to span  manually  via  opentracing  API e.g.
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,:
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,And do some  manual  manipulations with the  ERROR  tag for exception processing in filters e.g.
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,"But, I'm still wondering if it is possible to configure the tracer to pickup the application logs  automatically :"
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,UPDATE : I was able to add the application logs to the tracer by adding wrapper for the logger e.g.
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,"However, so far I was not able to find opentracing configuration options that would allow to add the application logs to the tracer automatically by default."
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,"Basically, it seems that it is expected that dev would add extra logs to tracer programmatically if needed."
Jaeger,50855480,50985064.0,3,"2018/06/14, 13:37:22",True,"2019/11/11, 10:48:18","2018/06/15, 18:28:32",2470533.0,1619.0,10,9432,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?,"Also, after investigating tracing more it appeared to be that normally  logging  and  tracing  are handled separately and adding all the application logs to the tracer is not a good idea (tracer should mainly include sample data and tags for request identification)"
Jaeger,57268147,nan,1,"2019/07/30, 12:11:47",False,"2020/07/20, 15:44:52",nan,966914.0,886.0,6,661,Adding JDBC query information to opentracing/Jaeger spans,We're using Opentracing/Jaeger in Istio for tracing multiple Spring Boot/Spring Cloud based microservices.
Jaeger,57268147,nan,1,"2019/07/30, 12:11:47",False,"2020/07/20, 15:44:52",nan,966914.0,886.0,6,661,Adding JDBC query information to opentracing/Jaeger spans,I'm currently wondering if there's an option to enrich the tracing spans by providing information about executed query (i.e.
Jaeger,57268147,nan,1,"2019/07/30, 12:11:47",False,"2020/07/20, 15:44:52",nan,966914.0,886.0,6,661,Adding JDBC query information to opentracing/Jaeger spans,SQL statement)?
Jaeger,57268147,nan,1,"2019/07/30, 12:11:47",False,"2020/07/20, 15:44:52",nan,966914.0,886.0,6,661,Adding JDBC query information to opentracing/Jaeger spans,Tracing JDBC connection info is working fine using  opentracing-contrib/java-spring-cloud  but additional information is missing.
Jaeger,57268147,nan,1,"2019/07/30, 12:11:47",False,"2020/07/20, 15:44:52",nan,966914.0,886.0,6,661,Adding JDBC query information to opentracing/Jaeger spans,I know that e.g.
Jaeger,57268147,nan,1,"2019/07/30, 12:11:47",False,"2020/07/20, 15:44:52",nan,966914.0,886.0,6,661,Adding JDBC query information to opentracing/Jaeger spans,glowroot  is capable of tracing the statements itself but haven't found anything related to Opentracing or Jaeger.
Jaeger,57268147,nan,1,"2019/07/30, 12:11:47",False,"2020/07/20, 15:44:52",nan,966914.0,886.0,6,661,Adding JDBC query information to opentracing/Jaeger spans,Would be great if anybody could show some directions for research!
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,"I want to use istio with existing jaeger tracing system in K8S, I began with installing jaeger system following  the official link  with cassandra as backend storage."
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,"Then installed istio by  the helm way , but with only some selected components enabled:"
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,"Jaeger and istio are installed inside the same namespace  istio-sytem , after all done, all pods inside it looks like this:"
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,"Then I followed  the link  to deploy the bookinfo sample into another namespace  istio-play , which has label  istio-injection=enabled , but no matter how I flush the  productpage  page, there's no tracing data be filled into jaeger."
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,"I guess maybe tracing spans are sent to jaeger by mixer, like the way istio do all other telementry stuff, so I  -set mixer.enabled=true , but unfortunately only some services like  istio-mixer  or  istio-telementry  are displayed."
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,"Finally I cleaned up all the above installation and followed  this task  step by step, but the tracing data of bookinfo app are still not there."
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,My questions is: How indeed istio send tracing data to jaeger?
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,"Does sidecar proxy send it directly to jaeger-collector( zipkin.istio-system:9411 ) like  how envoy does , or the data flows like this:  sidecar-proxy -&gt; mixer -&gt; jaeger-collector ?"
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,And how could I debug how the data flow between all kinds of components inside the istio mesh?
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,Thanks for any help and info :-)
Jaeger,53459759,nan,1,"2018/11/24, 17:42:52",True,"2019/05/11, 09:44:30","2018/11/25, 16:19:50",1000254.0,9431.0,6,2690,How istio send tracing spans to jaeger?,"Update : I tried again by installing istio without helm:  kubectl -n istio-system apply -f install/kubernetes/istio-demo.yaml , this time everything works just fine, there must be something different between  kubectl way  and  helm way ."
Jaeger,50173643,50230156.0,2,"2018/05/04, 14:16:09",True,"2021/04/03, 06:01:36",nan,9725840.0,193.0,6,4184,Tracing with Jaeger doesn&#39;t work with docker-compose,"I instrumented a simple Spring-Boot application with Jaeger, but when I run the application within a Docker container with docker-compose, I can't see any traces in the Jaeger frontend."
Jaeger,50173643,50230156.0,2,"2018/05/04, 14:16:09",True,"2021/04/03, 06:01:36",nan,9725840.0,193.0,6,4184,Tracing with Jaeger doesn&#39;t work with docker-compose,I'm creating the tracer configuration by reading the properties from environment variables that I set in the docker-compose file.
Jaeger,50173643,50230156.0,2,"2018/05/04, 14:16:09",True,"2021/04/03, 06:01:36",nan,9725840.0,193.0,6,4184,Tracing with Jaeger doesn&#39;t work with docker-compose,This is how I create the tracer:
Jaeger,50173643,50230156.0,2,"2018/05/04, 14:16:09",True,"2021/04/03, 06:01:36",nan,9725840.0,193.0,6,4184,Tracing with Jaeger doesn&#39;t work with docker-compose,And this is my docker-compose file:
Jaeger,50173643,50230156.0,2,"2018/05/04, 14:16:09",True,"2021/04/03, 06:01:36",nan,9725840.0,193.0,6,4184,Tracing with Jaeger doesn&#39;t work with docker-compose,You can also find my project on  GitHub .
Jaeger,50173643,50230156.0,2,"2018/05/04, 14:16:09",True,"2021/04/03, 06:01:36",nan,9725840.0,193.0,6,4184,Tracing with Jaeger doesn&#39;t work with docker-compose,What am I doing wrong?
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,Created a deployment on  Google Kubernetes  using  jaegertracing/all-in-one  public image from  Docker Hub
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,"Then, exposed the deployment with Service type as  LoadBalancer ."
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,"Now, launched the Jagger UI and it is working, but it do not show any service  except jagger-query ."
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,I have deployed my .net web api application for testing on kubernetes.
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,My application has only one single web API which is successfully running on Google Kubernetes engine and exposed via load balancer service type.
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,The API has all the data hard-coded and do not call any other service or database.
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,Used following nuget packages in the project:
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,"Jaeger -Version 0.2.2 
OpenTracing.Contrib.NetCore -Version 0.5.0"
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,I have added following code in  Startup.cs  file of my API:
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,jaegerHost  environment variable is assigned the  IP  of the jaeger service created.
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,"The question is how and where to make changes so that my application service gets available in the Jaeger UI, so that I can see it's traces."
Jaeger,62216558,nan,1,"2020/06/05, 16:33:39",True,"2020/06/15, 11:38:18","2020/06/15, 11:38:18",5184406.0,577.0,5,245,Jaeger Service : How to configure on Google Kubernetes?,"I am stuck here, can anyone please help how to proceed ahead?"
Jaeger,57694618,nan,0,"2019/08/28, 17:22:38",False,"2019/08/29, 21:18:32","2019/08/29, 21:18:32",8372389.0,61.0,5,320,Does someone implement Jaeger with Spring Cloud Gateway?,We are re-building our software platform using a microservice architecture approach.
Jaeger,57694618,nan,0,"2019/08/28, 17:22:38",False,"2019/08/29, 21:18:32","2019/08/29, 21:18:32",8372389.0,61.0,5,320,Does someone implement Jaeger with Spring Cloud Gateway?,"Most of it using  Spring Boot  libraries, and for our entry points we are using  Spring Cloud Gateway  which can be easily integrated with  Jaeger  to have tracing in the platform."
Jaeger,57694618,nan,0,"2019/08/28, 17:22:38",False,"2019/08/29, 21:18:32","2019/08/29, 21:18:32",8372389.0,61.0,5,320,Does someone implement Jaeger with Spring Cloud Gateway?,We do that using the following Maven dependency:
Jaeger,57694618,nan,0,"2019/08/28, 17:22:38",False,"2019/08/29, 21:18:32","2019/08/29, 21:18:32",8372389.0,61.0,5,320,Does someone implement Jaeger with Spring Cloud Gateway?,"And it is working pretty well, we can see the trace on the  Jaeger UI  web console."
Jaeger,57694618,nan,0,"2019/08/28, 17:22:38",False,"2019/08/29, 21:18:32","2019/08/29, 21:18:32",8372389.0,61.0,5,320,Does someone implement Jaeger with Spring Cloud Gateway?,The problem now is how to send the tracking information to the next inner service in the platform to have every service correlated.
Jaeger,57694618,nan,0,"2019/08/28, 17:22:38",False,"2019/08/29, 21:18:32","2019/08/29, 21:18:32",8372389.0,61.0,5,320,Does someone implement Jaeger with Spring Cloud Gateway?,Any ideas?
Jaeger,57694618,nan,0,"2019/08/28, 17:22:38",False,"2019/08/29, 21:18:32","2019/08/29, 21:18:32",8372389.0,61.0,5,320,Does someone implement Jaeger with Spring Cloud Gateway?,We have tried  to inject the tracking information as HTML Headers using a  GlobalFilter  but it is using the incoming request and we need to put them on the downstream request... any clue on how to override  HTTPClient  used underneath.
Jaeger,56895100,nan,1,"2019/07/05, 02:35:45",True,"2019/12/17, 10:16:44",nan,1669583.0,350.0,5,485,Python jaeger-client tracer not reporting on reuse,Tracing tasks in celery 4.1.1 using sample code.
Jaeger,56895100,nan,1,"2019/07/05, 02:35:45",True,"2019/12/17, 10:16:44",nan,1669583.0,350.0,5,485,Python jaeger-client tracer not reporting on reuse,Each worker runs:
Jaeger,56895100,nan,1,"2019/07/05, 02:35:45",True,"2019/12/17, 10:16:44",nan,1669583.0,350.0,5,485,Python jaeger-client tracer not reporting on reuse,When I first start celery and run tasks each worker gets a working tracer and there is a log output for each of:
Jaeger,56895100,nan,1,"2019/07/05, 02:35:45",True,"2019/12/17, 10:16:44",nan,1669583.0,350.0,5,485,Python jaeger-client tracer not reporting on reuse,"Any task that runs after the initial gets the global tracer 
 from  Config.initialze_tracer  (which returns  None ) and a log warning  Jaeger tracer already initialized, skipping ."
Jaeger,56895100,nan,1,"2019/07/05, 02:35:45",True,"2019/12/17, 10:16:44",nan,1669583.0,350.0,5,485,Python jaeger-client tracer not reporting on reuse,"Watching tcpdump on the console shows that the UDP packets aren't being sent, I think I'm getting an uninitialized default tracer and it's using the noop reporter."
Jaeger,56895100,nan,1,"2019/07/05, 02:35:45",True,"2019/12/17, 10:16:44",nan,1669583.0,350.0,5,485,Python jaeger-client tracer not reporting on reuse,I've pored over the code in opentracing and jaeger_client and I can't find a canonical way around this.
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,I have a problem using the jaeger open tracing project within our microservice system.
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,The config I use is as below.
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,The origin of the trace is the same as this.
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,This works fine and is logged within the UI and with printed results below.
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,"But then when I extract {'uber-trace-id': '2b55203a8773aa14:77cceca94f4cfe74:0:1'} within another service it says a new span has been created as expected, but nothing is logged within the UI."
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,Print result:
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,With both Format.HTTP_HEADERS and Format.TEXT_MAP it is the same result.
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,Does anyone know why nothing is logged in the UI and how to fix this?
Jaeger,61725667,nan,0,"2020/05/11, 11:46:35",False,"2020/05/11, 11:46:35",nan,13516234.0,41.0,4,325,python opentracing with jaeger extract span not logging in UI using microservices,Thanks in advance.
Jaeger,61104999,nan,0,"2020/04/08, 19:01:42",False,"2020/04/08, 19:01:42",nan,6209643.0,107.0,4,735,Angular open tracing to Jaeger,I am looking at open tracing implementations to trace the application to JaegerUI.
Jaeger,61104999,nan,0,"2020/04/08, 19:01:42",False,"2020/04/08, 19:01:42",nan,6209643.0,107.0,4,735,Angular open tracing to Jaeger,"Our application Front end is angular, backend end is Asp.net Web api."
Jaeger,61104999,nan,0,"2020/04/08, 19:01:42",False,"2020/04/08, 19:01:42",nan,6209643.0,107.0,4,735,Angular open tracing to Jaeger,I am able to trace the webapi using Jaeger C# nuget packages.
Jaeger,61104999,nan,0,"2020/04/08, 19:01:42",False,"2020/04/08, 19:01:42",nan,6209643.0,107.0,4,735,Angular open tracing to Jaeger,"However, I have not found a way/ npm package to trace the angular trace messages to Jaeger."
Jaeger,61104999,nan,0,"2020/04/08, 19:01:42",False,"2020/04/08, 19:01:42",nan,6209643.0,107.0,4,735,Angular open tracing to Jaeger,I understand there's a Jaeger npm package for node.js(server) but not for client javascript/typescript that runs in browser.
Jaeger,61104999,nan,0,"2020/04/08, 19:01:42",False,"2020/04/08, 19:01:42",nan,6209643.0,107.0,4,735,Angular open tracing to Jaeger,Could you let me know how can we implement opentracing with Jaeger at front end(Javascript/angular)- so that we can see the full span traced (front end to backend) in Jaeger
Jaeger,52886127,52894567.0,1,"2018/10/19, 08:00:46",True,"2019/03/04, 21:46:57","2018/10/19, 08:08:50",10328864.0,73.0,4,2551,Jaeger Service not shown in Jaeger UI,I installed the jaeger all in one in Docker with:
Jaeger,52886127,52894567.0,1,"2018/10/19, 08:00:46",True,"2019/03/04, 21:46:57","2018/10/19, 08:08:50",10328864.0,73.0,4,2551,Jaeger Service not shown in Jaeger UI,And below is the sample code on how I initialize the tracer and spans.
Jaeger,52886127,52894567.0,1,"2018/10/19, 08:00:46",True,"2019/03/04, 21:46:57","2018/10/19, 08:08:50",10328864.0,73.0,4,2551,Jaeger Service not shown in Jaeger UI,I get the logs in my console but it does not reflect in my Jaeger UI.
Jaeger,52886127,52894567.0,1,"2018/10/19, 08:00:46",True,"2019/03/04, 21:46:57","2018/10/19, 08:08:50",10328864.0,73.0,4,2551,Jaeger Service not shown in Jaeger UI,Could anyone please help me with this?
Jaeger,47706011,47756263.0,1,"2017/12/08, 02:33:32",True,"2020/01/02, 09:57:13",nan,8585641.0,65.0,4,2136,Jaeger back end isn&#39;t recieving any data (using Jager Node.js client),"I'm trying to get a little example of Jaeger working using Node.js, but I can't get the Jaeger UI to display any data or show anything."
Jaeger,47706011,47756263.0,1,"2017/12/08, 02:33:32",True,"2020/01/02, 09:57:13",nan,8585641.0,65.0,4,2136,Jaeger back end isn&#39;t recieving any data (using Jager Node.js client),I have read this question:  uber/jaeger-client-node: backend wont receive data  but this hasn't helped in my case.
Jaeger,47706011,47756263.0,1,"2017/12/08, 02:33:32",True,"2020/01/02, 09:57:13",nan,8585641.0,65.0,4,2136,Jaeger back end isn&#39;t recieving any data (using Jager Node.js client),I'm running the Jaeger back end in a docker container using:
Jaeger,47706011,47756263.0,1,"2017/12/08, 02:33:32",True,"2020/01/02, 09:57:13",nan,8585641.0,65.0,4,2136,Jaeger back end isn&#39;t recieving any data (using Jager Node.js client),The code for my example is:
Jaeger,47706011,47756263.0,1,"2017/12/08, 02:33:32",True,"2020/01/02, 09:57:13",nan,8585641.0,65.0,4,2136,Jaeger back end isn&#39;t recieving any data (using Jager Node.js client),Any help as to what I am doing wrong would be much appreciated!
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,In a spring boot application (just one at the moment) I included jaeger by adding dependency  opentracing-spring-jaeger-web-starter  and the below beans
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,After starting Jaeger in docker  docker run -d --name jaeger -p 16686:16686 -p 6831:6831/udp jaegertracing/all-in-one:1.9  I get the traces.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,I found now another dependency and read different tutorials which made me somehow unsure on what is the right way to use Jaeger with spring boot.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,Which dependency would I use?
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,https://github.com/opentracing-contrib/java-spring-cloud
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,https://github.com/signalfx/tracing-examples/tree/master/jaeger-java-spring-boot-web
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,Following the  Jaeger documentation  possibly
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,would be enough!
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,?
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,Before trying Jaeger I used Zipkin which is very easy to integrate in Spring since there is a starter for sleuth.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,The logs contain trace and span id's in separate fields so they can be searched for e.g.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,in Kibana.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,Jaeger does  not .
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,Can that be customized and if so - how?
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,Is it possibly to use Jaeger with Brave for instrumentation.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,The project e.g.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,includes  spring-cloud-starter-sleuth  as a dependency.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,There are some conflicts with already existing beans.
Jaeger,63407052,65029618.0,2,"2020/08/14, 08:26:45",True,"2020/11/26, 23:49:03","2020/08/14, 08:42:10",10196632.0,351.0,3,2831,Jaeger with Spring Boot,Can Jaeger be used with brave at all?
Jaeger,61154096,61154180.0,3,"2020/04/11, 11:39:52",True,"2020/04/11, 22:39:56","2020/04/11, 19:57:23",6715384.0,1784.0,3,371,How to deploy Jaeger on Kubernetes GKE,I have added these fields in application.yml of microservices and dependency in pom.xml.Jaeger running on my local is abl to identify the services as well
Jaeger,61154096,61154180.0,3,"2020/04/11, 11:39:52",True,"2020/04/11, 22:39:56","2020/04/11, 19:57:23",6715384.0,1784.0,3,371,How to deploy Jaeger on Kubernetes GKE,I have deployed all my microservices on kubernetes.
Jaeger,61154096,61154180.0,3,"2020/04/11, 11:39:52",True,"2020/04/11, 22:39:56","2020/04/11, 19:57:23",6715384.0,1784.0,3,371,How to deploy Jaeger on Kubernetes GKE,Please help me in deploying jaeger on kubernetes.
Jaeger,61154096,61154180.0,3,"2020/04/11, 11:39:52",True,"2020/04/11, 22:39:56","2020/04/11, 19:57:23",6715384.0,1784.0,3,371,How to deploy Jaeger on Kubernetes GKE,"UPDATE: 
I have reached this step."
Jaeger,61154096,61154180.0,3,"2020/04/11, 11:39:52",True,"2020/04/11, 22:39:56","2020/04/11, 19:57:23",6715384.0,1784.0,3,371,How to deploy Jaeger on Kubernetes GKE,I have a load balancer IP for jaeger-query.
Jaeger,61154096,61154180.0,3,"2020/04/11, 11:39:52",True,"2020/04/11, 22:39:56","2020/04/11, 19:57:23",6715384.0,1784.0,3,371,How to deploy Jaeger on Kubernetes GKE,But on which host and port will my microservice send the logs to  ?
Jaeger,61154096,61154180.0,3,"2020/04/11, 11:39:52",True,"2020/04/11, 22:39:56","2020/04/11, 19:57:23",6715384.0,1784.0,3,371,How to deploy Jaeger on Kubernetes GKE,?
Jaeger,61149872,nan,3,"2020/04/11, 01:57:42",True,"2021/04/10, 03:12:05","2020/04/11, 10:49:35",7847042.0,382.0,3,1049,jaeger configuration in spring boot application when both of them are deployed on kubernetes,"So, I am trying to trace logs of my spring boot application with jaeger so what are the steps that should be perform if my application and jaeger is deploy on kubernetes."
Jaeger,61149872,nan,3,"2020/04/11, 01:57:42",True,"2021/04/10, 03:12:05","2020/04/11, 10:49:35",7847042.0,382.0,3,1049,jaeger configuration in spring boot application when both of them are deployed on kubernetes,"I have successfully deployed jaeger and spring boot application 
now how will I configure jaeger in my service."
Jaeger,61149872,nan,3,"2020/04/11, 01:57:42",True,"2021/04/10, 03:12:05","2020/04/11, 10:49:35",7847042.0,382.0,3,1049,jaeger configuration in spring boot application when both of them are deployed on kubernetes,My services are not visible in the jaegar console.
Jaeger,61149872,nan,3,"2020/04/11, 01:57:42",True,"2021/04/10, 03:12:05","2020/04/11, 10:49:35",7847042.0,382.0,3,1049,jaeger configuration in spring boot application when both of them are deployed on kubernetes,I have added the following configuration to the yml:
Jaeger,61149872,nan,3,"2020/04/11, 01:57:42",True,"2021/04/10, 03:12:05","2020/04/11, 10:49:35",7847042.0,382.0,3,1049,jaeger configuration in spring boot application when both of them are deployed on kubernetes,Output og kubectl get service jaeger-query
Jaeger,58409078,nan,1,"2019/10/16, 11:36:00",False,"2019/12/10, 06:32:34",nan,3380878.0,2468.0,3,171,Jaeger space between spans,"I'm using jaeger with spring boot to trace a test application, sometimes I get some extra space or overlap that appears between spans in a single-threaded trace that takes up to 20ms."
Jaeger,58409078,nan,1,"2019/10/16, 11:36:00",False,"2019/12/10, 06:32:34",nan,3380878.0,2468.0,3,171,Jaeger space between spans,I am confused about this extra space because there aren't any codes between these spans and I expected to see spans starting after each other.
Jaeger,58409078,nan,1,"2019/10/16, 11:36:00",False,"2019/12/10, 06:32:34",nan,3380878.0,2468.0,3,171,Jaeger space between spans,Here are my output results.
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?",We lately setup a jaeger server in order to trace all requests throughout our system.
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?",The initial setup worked pretty nicely by simply adding the necessary spring (cloud) starter dependencies to our build files.
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?","Each time, a request hits one of our servers, a new span is created and reported to the jaeger server which was setup by using the all-in-one docker image."
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?",The most important dependencies are the following:
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?","While spans are created on the server, the necessary headers are not forwarded to the feign clients."
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?","According to the documentation, the addition of  opentracing-spring-cloud-feign-starter  dependency should to the trick, but so far, none of the feign clients worked."
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?",I also added a breakpoint to the auto configure class provided by opentracing
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?","and this method is invoked, when the application starts up."
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?",There are also is also some information in the logs regarding the initialization of jaeger/opentracing:
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?","I spent quite some time, reading into documentation and looking for examples how to configure a spring boot/cloud application correctly in order to work with feign clients but so far I had no luck."
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?",Most examples out there use Springs' RestTemplate instead of Feign clients.
Jaeger,53453160,55138790.0,2,"2018/11/23, 23:32:39",True,"2020/08/12, 02:50:32",nan,2104921.0,1678.0,3,1596,"How to integrate opentracing/jaeger with spring cloud, hystrix and feign?","I would be very happy, if somebody could point me towards the right direction."
Jaeger,53322900,53343467.0,1,"2018/11/15, 17:37:37",True,"2018/11/16, 20:27:10","2018/11/16, 08:50:45",1749786.0,463.0,3,2802,How to use Jaeger with Spring WebFlux?,We are trying to go reactive with Webflux.
Jaeger,53322900,53343467.0,1,"2018/11/15, 17:37:37",True,"2018/11/16, 20:27:10","2018/11/16, 08:50:45",1749786.0,463.0,3,2802,How to use Jaeger with Spring WebFlux?,We are using Jaegar with Istio for instrumentation purposes.
Jaeger,53322900,53343467.0,1,"2018/11/15, 17:37:37",True,"2018/11/16, 20:27:10","2018/11/16, 08:50:45",1749786.0,463.0,3,2802,How to use Jaeger with Spring WebFlux?,"Jaegar understands Spring MVC endpoints well, but don't seem to work at all for WebFlux."
Jaeger,53322900,53343467.0,1,"2018/11/15, 17:37:37",True,"2018/11/16, 20:27:10","2018/11/16, 08:50:45",1749786.0,463.0,3,2802,How to use Jaeger with Spring WebFlux?,I am looking for suggestions to make my webflux endpoints appear in Jaeger.
Jaeger,53322900,53343467.0,1,"2018/11/15, 17:37:37",True,"2018/11/16, 20:27:10","2018/11/16, 08:50:45",1749786.0,463.0,3,2802,How to use Jaeger with Spring WebFlux?,Thanks in advance.
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,I'm trying to set up a local k8s cluster and on  minikube  with installed  istio  and I have an issue with enabling distributed tracing with Jaeger.
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,I have 3 microservices  A -&gt; B -&gt; C .
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,I am propagating the all the headers that are needed:
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,"But on Jaeger interface, I can only see the request to the service A and I cannot see the request going to service B."
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,I have logged the headers that are sent in the request.
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,Headers from service A:
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,Headers from service B:
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,"So the  x-request-id ,  x-b3-traceid ,  x-b3-sampled , and  x-b3-spanid  mathces."
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,There are some headers that aren't set.
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,"Also, I'm accessing service A via k8s Service IP of type LoadBalancer, not via ingress."
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,Don't know if this could be the issue.
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,UPD: I have setup istio gateway so now I'm accessing service  A  via istio gateway.
Jaeger,52038025,52111563.0,1,"2018/08/27, 14:12:27",True,"2018/08/31, 11:36:35","2018/08/27, 20:44:23",5604676.0,7277.0,3,1136,Istio distributed tracing with Jaeger not working,"However the result is the same, I can see the trace for  gateway-&gt;A  but no any further tracing"
Jaeger,51838896,nan,1,"2018/08/14, 13:06:39",False,"2018/08/20, 14:39:54",nan,10206966.0,373.0,3,578,How to secure the Jaeger UI from a keycloak security proxy (login),"After login to the Keycloak Jaeger(realm) client, the keycloak server doesn't navigate to the Jaeger UI path -  localhost:16686."
Jaeger,51838896,nan,1,"2018/08/14, 13:06:39",False,"2018/08/20, 14:39:54",nan,10206966.0,373.0,3,578,How to secure the Jaeger UI from a keycloak security proxy (login),It seems keycloak verifies the user (see below code)
Jaeger,51838896,nan,1,"2018/08/14, 13:06:39",False,"2018/08/20, 14:39:54",nan,10206966.0,373.0,3,578,How to secure the Jaeger UI from a keycloak security proxy (login),proxy.json
Jaeger,51838896,nan,1,"2018/08/14, 13:06:39",False,"2018/08/20, 14:39:54",nan,10206966.0,373.0,3,578,How to secure the Jaeger UI from a keycloak security proxy (login),keycloak.json
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,We have selected to use Jaeger API is used for tracing.
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,"There , we have setup the Jaeger locally using docker as mentioned below."
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,"In the  ServletContextListener , we created new cofigurations like below."
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,Now this works fine and I can see the tracing in  http://localhost:16686
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,"Problem :  
I want to make the Jager setup in an external environment and connect from another application server (application server is running on wildfly 10 docker under host mode)."
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,In future that Jaeger instance may used by more than one server instances for tracings.
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,After looking at the source and various references as mentioned below I tried below options.
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,But it connects to local always.
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,"I tried various ports like 5775, 6831, 6832 also but result was same."
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,Further I tried setting JAEGER_ENDPOINT and JAEGER_SAMPLER_MANAGER_HOST_PORT as environment variables also.
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,But failed.
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,"In one  reference  I found that  ""Jaeger client libraries expect jaeger-agent process to run locally on each host..."" ."
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,Is that means I can't use it in cebtrelized manner and I need to setup Jaeger in each application server instances?
Jaeger,51719575,nan,2,"2018/08/07, 08:44:30",True,"2019/12/04, 13:31:17","2018/08/07, 08:49:45",1057291.0,8280.0,3,2264,How to change the Host and port for Jaeger API in java,Otherwise how to do that?
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,I am trying to integrate jaeger tracing in my java spring application.
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,I have added following code in the configuration file :
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,"@Bean
    public io.opentracing.Tracer jaegerTracer(){"
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,and used following docker command :
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,docker run -d -p 5775:5775/udp -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14268:14268 jaegertracing/all-in-one:latest
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,"Still, I am not able to find my service in jaeger-ui"
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,"Upon hitting this url :
 http://localhost:5778/?service=pilot-tracking 
Output is :
tcollector error: tchannel error ErrCodeBadRequest: no handler for service ""jaeger-collector"" and method ""SamplingManager::getSamplingStrategy"""
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,Please help!
Jaeger,48423910,nan,1,"2018/01/24, 15:41:31",False,"2018/10/15, 12:16:23",nan,986156.0,51.0,3,1160,Unable to see service name in jaeger ui - Java spring application,!
Jaeger,64391505,64572391.0,2,"2020/10/16, 17:51:51",True,"2020/10/28, 13:57:45","2020/10/22, 18:47:50",1141160.0,1101.0,2,327,Jaeger/Opentracing with Kafka and docker compose,I can only find old and incomplete examples of using opentracing/jaeger with Kafka.
Jaeger,64391505,64572391.0,2,"2020/10/16, 17:51:51",True,"2020/10/28, 13:57:45","2020/10/22, 18:47:50",1141160.0,1101.0,2,327,Jaeger/Opentracing with Kafka and docker compose,I want to run an example locally as a proof of concept - opentracing spans to kafka.
Jaeger,64391505,64572391.0,2,"2020/10/16, 17:51:51",True,"2020/10/28, 13:57:45","2020/10/22, 18:47:50",1141160.0,1101.0,2,327,Jaeger/Opentracing with Kafka and docker compose,"I managed to get some of this working, but on  jeager-query  service I keep getting:"
Jaeger,64391505,64572391.0,2,"2020/10/16, 17:51:51",True,"2020/10/28, 13:57:45","2020/10/22, 18:47:50",1141160.0,1101.0,2,327,Jaeger/Opentracing with Kafka and docker compose,I'm not sure if I need to use some sort of storage i.e.
Jaeger,64391505,64572391.0,2,"2020/10/16, 17:51:51",True,"2020/10/28, 13:57:45","2020/10/22, 18:47:50",1141160.0,1101.0,2,327,Jaeger/Opentracing with Kafka and docker compose,cassandra too?
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,"I'm trying to instrument a Spring Cloud RxJava sample app with Jaeger, and for some reason I'm failing!"
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,"I have a couple of other SpringCloud apps, like Hystrix, JDBC and JMS working fine with the tracing being reported to Jeager by just adding the maven dependency to it."
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,"For RxJava, on the other hand, I can't figure it out why I'm not able to follow the same approach..."
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,"When I leave the App without a  Tracer @Bean , I don't get anything in Jaeger and I get this message:"
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,"All the other SpringCloud apps are working without the  Tracer @Bean , so I was expecting the same behavior for the RxJava..."
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,"The worst part is that whenever I add the  Tracer @Bean , the bean is initialized, but still no data is sent to Jaeger...
Not sure if it is related to this message:"
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,Does anyone have any idea?
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,Do I need to set anything in the  application.properties ?
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,I'm posting below my  pom.xml  file:
Jaeger,64018706,nan,1,"2020/09/23, 01:52:02",False,"2020/09/28, 12:50:40","2020/09/28, 12:50:40",9291851.0,669.0,2,386,Jaeger instrumentation for Spring Cloud RxJava,Sample app committed here:  https://github.com/julianocosta89/rxjava-jeager
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,I am trying to trace in a front end app.
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,I am not be able to use  @opentelemetry/exporter-jaeger  since  I believe it is for Node.js back end app only .
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,So I am trying to use  @opentelemetry/exporter-collector .
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,First I tried to print the trace data in the browser console.
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"And
the code below succeed printing the trace data."
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,Now I want to forward them to Jaeger.
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,I am running  Jaeger all-in-one  by
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"Based on the  Jaeger port document , I might be able to use these two ports (if other ports work, that will be great too!"
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,):
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,Then I further found  more info about this port :
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,Zipkin Formats (stable)
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"Jaeger Collector can also accept spans in several Zipkin data format,
namely JSON v1/v2 and Thrift."
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"The Collector needs to be configured to
enable Zipkin HTTP server, e.g."
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"on port 9411 used by Zipkin
collectors."
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"The server enables two endpoints that expect POST
requests:"
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,I updated my codes to
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"However, I got bad request for both v1 and v2 endpoints without any response body returned"
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,POST http://localhost:9411/api/v1/spans 400 (Bad Request)
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,POST http://localhost:9411/api/v2/spans 400 (Bad Request)
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,Any idea how can I make the request format correct?
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,Thanks
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,I think Andrew is right that I should use OpenTelemetry collector.
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"I also got some help from Valentin Marchaud and Deniz Gurkaynak
at Gitter."
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,"Just add the link here for further people who meet same issue:
 https://gitter.im/open-telemetry/opentelemetry-node?at=5f3aa9481226fc21335ce61a"
Jaeger,63443921,63455905.0,1,"2020/08/17, 05:49:09",True,"2020/08/19, 18:52:07","2020/08/19, 18:52:07",2000548.0,30911.0,2,1151,How to send trace data to Jaeger through OpenTelemetry in front end app?,My final working solution posted at  https://stackoverflow.com/a/63489195/2000548
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,I am following all the instructions mentioned here:  https://github.com/opentracing-contrib/java-jdbc
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,I assumed that with these steps the traces related to JDBC operations will automatically start getting reported.
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,"However, in the logs, I only see this below and nothing in Jaeger UI."
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,Can someone please show an example of how to achieve this?
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,I am using below versions:
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,"opentracing-api-0.33.0
opentracing-jdbc-0.2.10
opentracing-util-0.33.0
jaeger-core-0.35.4"
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,Here's the abstract that I have attempted.
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,Is there anything else I need to do?
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,And then this is the code around database call:
Jaeger,62292611,nan,0,"2020/06/10, 01:03:32",False,"2020/06/10, 05:25:50","2020/06/10, 05:25:50",5432511.0,79.0,2,652,OpenTracing + Jaeger: SQL traces not getting reported,The database config is:
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,I would like to know what's the minimal configuration for a spring-boot app in terms of dependencies if I need to report traces to Jaeger in Istio.
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,I was expecting that by adding only
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,the traces will be present in Jaeger.
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,But it's true that the envoy can not correlate the requests that go to the service with the response if the headers with the tracing details are not in the response.
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,It seems that sleuth only propagates the headers when calling by RestTemplate or Feign or Spring Integration... but the headers are not there in the response for an API.
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,"In order to make it work, I added this other dependency to the service"
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,Or even propagating the headers manually I can see traces in Jaeger.
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,I am confused because it seems that sleuth should be doing this task without additional dependencies as I understood from @spencergibb in this video  https://youtu.be/AMJQO9zs2eo?t=1045
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,In case of Sleuth dependency is not enough.
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,What dependencies will be required?
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,"I can see several dependencies that seem to do similar things like the previous one, like"
Jaeger,62236562,nan,0,"2020/06/06, 21:53:12",False,"2020/06/06, 21:53:12",nan,7605799.0,207.0,2,438,Spring Boot app with Spring Cloud Sleuth integration with Jaeger in Istio,Thanks in advance.
Jaeger,59153293,59260037.0,1,"2019/12/03, 10:53:57",True,"2019/12/10, 14:41:51","2019/12/06, 07:17:46",4431422.0,1989.0,2,605,Advantages of using Jaeger Agent,"So I am exploring Jaeger for Tracing and I saw that we can directly send spans from the client to the collector in HTTP (PORT: 14268), if so then what is the advantage of using the jaeger agent."
Jaeger,59153293,59260037.0,1,"2019/12/03, 10:53:57",True,"2019/12/10, 14:41:51","2019/12/06, 07:17:46",4431422.0,1989.0,2,605,Advantages of using Jaeger Agent,When to go for the Jaeger Agent Approach and when to go with the direct HTTP Approach.
Jaeger,59153293,59260037.0,1,"2019/12/03, 10:53:57",True,"2019/12/10, 14:41:51","2019/12/06, 07:17:46",4431422.0,1989.0,2,605,Advantages of using Jaeger Agent,What is the disadvantage of going with the Direct approach to the collector
Jaeger,58511640,58511794.0,2,"2019/10/22, 23:12:43",True,"2019/10/22, 23:39:29",nan,12259533.0,23.0,2,604,"Trying to get bash through docker exec, but nothing works (Jaeger container) --&gt; OCI runtime exec failed","I've been trying to run basic shell commands, ls as an example, but any of them work."
Jaeger,58511640,58511794.0,2,"2019/10/22, 23:12:43",True,"2019/10/22, 23:39:29",nan,12259533.0,23.0,2,604,"Trying to get bash through docker exec, but nothing works (Jaeger container) --&gt; OCI runtime exec failed","So, I've tried to validate if the container has a bash enabled, and answers to similar posts say to run:"
Jaeger,58511640,58511794.0,2,"2019/10/22, 23:12:43",True,"2019/10/22, 23:39:29",nan,12259533.0,23.0,2,604,"Trying to get bash through docker exec, but nothing works (Jaeger container) --&gt; OCI runtime exec failed",But any of them work (neither docker exec -it amazing_robinson ls).
Jaeger,58511640,58511794.0,2,"2019/10/22, 23:12:43",True,"2019/10/22, 23:39:29",nan,12259533.0,23.0,2,604,"Trying to get bash through docker exec, but nothing works (Jaeger container) --&gt; OCI runtime exec failed",This is the error:
Jaeger,58511640,58511794.0,2,"2019/10/22, 23:12:43",True,"2019/10/22, 23:39:29",nan,12259533.0,23.0,2,604,"Trying to get bash through docker exec, but nothing works (Jaeger container) --&gt; OCI runtime exec failed",The container is
Jaeger,58373932,nan,0,"2019/10/14, 12:33:15",False,"2019/10/14, 12:33:15",nan,11695449.0,23.0,2,65,could not found jaeger distributed latency metrics of micro services in prometheus dashboard,"We are using Jaeger with camel open tracing to get metrics, we are able to see all the latency metrics in Jaeger UI."
Jaeger,58373932,nan,0,"2019/10/14, 12:33:15",False,"2019/10/14, 12:33:15",nan,11695449.0,23.0,2,65,could not found jaeger distributed latency metrics of micro services in prometheus dashboard,"In Prometheus we are able to see few jaeger metrics request count, but the some metrics like latency we are not able to found in prometheus dashboard."
Jaeger,58373932,nan,0,"2019/10/14, 12:33:15",False,"2019/10/14, 12:33:15",nan,11695449.0,23.0,2,65,could not found jaeger distributed latency metrics of micro services in prometheus dashboard,Prometheus dashborad
Jaeger,58373932,nan,0,"2019/10/14, 12:33:15",False,"2019/10/14, 12:33:15",nan,11695449.0,23.0,2,65,could not found jaeger distributed latency metrics of micro services in prometheus dashboard,"We are exposing metrics using below:
host:14269/metrics, host:16687/metrics"
Jaeger,58373932,nan,0,"2019/10/14, 12:33:15",False,"2019/10/14, 12:33:15",nan,11695449.0,23.0,2,65,could not found jaeger distributed latency metrics of micro services in prometheus dashboard,Can some one help me to get the jaeger service latency metrics in prometheus.
Jaeger,57976158,57986473.0,1,"2019/09/17, 17:10:25",True,"2019/09/18, 09:36:52","2019/09/17, 18:13:26",10489752.0,31.0,2,746,PHP OpenTracing + Jaeger: service does not appear at Jaeger,"I was trying to implement the OpenTracing + Jaeger to my PHP project, following the ""Get started"" example there  https://github.com/jonahgeorge/jaeger-client-php"
Jaeger,57976158,57986473.0,1,"2019/09/17, 17:10:25",True,"2019/09/18, 09:36:52","2019/09/17, 18:13:26",10489752.0,31.0,2,746,PHP OpenTracing + Jaeger: service does not appear at Jaeger,"Tracer, spans and scopes have been created successfully, but Jaeger does not see my service."
Jaeger,57976158,57986473.0,1,"2019/09/17, 17:10:25",True,"2019/09/18, 09:36:52","2019/09/17, 18:13:26",10489752.0,31.0,2,746,PHP OpenTracing + Jaeger: service does not appear at Jaeger,There are my  .php  and  docker-compose.yml  files below:
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,I have distribute application that consists of several Go services.
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,Some of those use Kafka as data bus.
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,I was able trace down calls between services using  opentracing  with Jaeger.
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,"I have problem plotting Kafka spans on graph, them appear as gaps."
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,Here is what i was able to do.
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,Initial spans been created by gRPC middleware.
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,Producer side:
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,Consumer side:
Jaeger,56847558,56927362.0,1,"2019/07/02, 10:38:44",True,"2019/07/08, 04:26:18",nan,2915603.0,1084.0,2,752,Tracing Kafka bus in distributed app using Jaeger,How should i modify this to plot span on graph when message was in Kafka?
Jaeger,55384578,nan,0,"2019/03/27, 20:53:25",False,"2020/03/12, 13:55:13","2019/03/27, 22:32:26",7023897.0,192.0,2,343,How to configure Jaeger collector with ElasticSearch in Windows Server,I am trying to setup jaeger-collector on one server with jaeger-agent running in another server.
Jaeger,55384578,nan,0,"2019/03/27, 20:53:25",False,"2020/03/12, 13:55:13","2019/03/27, 22:32:26",7023897.0,192.0,2,343,How to configure Jaeger collector with ElasticSearch in Windows Server,"If I run the exe jaeger-all-in-one, everything works as expected (using in memory)."
Jaeger,55384578,nan,0,"2019/03/27, 20:53:25",False,"2020/03/12, 13:55:13","2019/03/27, 22:32:26",7023897.0,192.0,2,343,How to configure Jaeger collector with ElasticSearch in Windows Server,"In order to see the options available with ES, i am not able to run a help command."
Jaeger,55384578,nan,0,"2019/03/27, 20:53:25",False,"2020/03/12, 13:55:13","2019/03/27, 22:32:26",7023897.0,192.0,2,343,How to configure Jaeger collector with ElasticSearch in Windows Server,"When I run jaeger-collector --help, it shows only cassandra related flags."
Jaeger,55384578,nan,0,"2019/03/27, 20:53:25",False,"2020/03/12, 13:55:13","2019/03/27, 22:32:26",7023897.0,192.0,2,343,How to configure Jaeger collector with ElasticSearch in Windows Server,How do I check the elastic search specific details.
Jaeger,55384578,nan,0,"2019/03/27, 20:53:25",False,"2020/03/12, 13:55:13","2019/03/27, 22:32:26",7023897.0,192.0,2,343,How to configure Jaeger collector with ElasticSearch in Windows Server,"Now, my requirement is to specify and elastic search url."
Jaeger,55384578,nan,0,"2019/03/27, 20:53:25",False,"2020/03/12, 13:55:13","2019/03/27, 22:32:26",7023897.0,192.0,2,343,How to configure Jaeger collector with ElasticSearch in Windows Server,"I have set up the Environment variables SPAN_STORAGE_TYPES and ES_SERVER_URLS, but couldn't find how to run jaeger-collector.exe by asking it to take in these environment variables."
Jaeger,55384578,nan,0,"2019/03/27, 20:53:25",False,"2020/03/12, 13:55:13","2019/03/27, 22:32:26",7023897.0,192.0,2,343,How to configure Jaeger collector with ElasticSearch in Windows Server,"Thanks,
Minu"
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,I would like to configure Jaeger in my Spring application.
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,Somehow I cannot find a proper way to do this.
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,Almost all Spring-Jaeger-related documentation is for Spring Boot where most of the properties are auto configured.
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,Here's my approach.
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,Maven dependency:
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,Spring config for Jaeger:
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,Jaeger is running locally in docker on port 6831.
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,"Once my application starts, I noticed that application slows down considerably, I assume that is because of metrics logged heavily to console by LoggingReporter."
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,"However, My Spring app does not show up in Jaeger UI."
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,In the beginning I would like to trace my REST endpoints.
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,Can someone point me in the right direction why my app is missing from UI and how I configure Jaeger properly?
Jaeger,55256948,56581518.0,2,"2019/03/20, 11:01:18",True,"2020/07/25, 02:41:06",nan,3349358.0,2678.0,2,6929,Configuring Jaeger in Spring application,Is there perhaps a sample project with Spring+Jaeger that does not rely on outdated Jaeger?
Jaeger,53754605,53761233.0,2,"2018/12/13, 05:27:05",True,"2018/12/13, 13:50:31",nan,7757608.0,145.0,2,1742,"Enable grafana, kiali and jaeger after istio Installation?",I have installed ISTIO using Helm .
Jaeger,53754605,53761233.0,2,"2018/12/13, 05:27:05",True,"2018/12/13, 13:50:31",nan,7757608.0,145.0,2,1742,"Enable grafana, kiali and jaeger after istio Installation?","I forgot to enable grafana, kiali and jaeger."
Jaeger,53754605,53761233.0,2,"2018/12/13, 05:27:05",True,"2018/12/13, 13:50:31",nan,7757608.0,145.0,2,1742,"Enable grafana, kiali and jaeger after istio Installation?",How can i enable all these above services after i have installed istio?
Jaeger,53172758,nan,1,"2018/11/06, 15:20:44",False,"2018/11/21, 11:57:59","2018/11/06, 18:21:28",10108895.0,21.0,2,1037,Tracing Spring Boot Micro services with Jaeger deployed on AKS,I had setup Jaeger in Azure Kubernetes Cluster in monitoring namespace and I deployed my container which is instrumented with jaeger client libraries in monitoring domain.
Jaeger,53172758,nan,1,"2018/11/06, 15:20:44",False,"2018/11/21, 11:57:59","2018/11/06, 18:21:28",10108895.0,21.0,2,1037,Tracing Spring Boot Micro services with Jaeger deployed on AKS,The service is up and running and I'm able to see the traces using actuator when I specify the :/actuator in the browser.
Jaeger,53172758,nan,1,"2018/11/06, 15:20:44",False,"2018/11/21, 11:57:59","2018/11/06, 18:21:28",10108895.0,21.0,2,1037,Tracing Spring Boot Micro services with Jaeger deployed on AKS,But the same microservice is not populating in the service dropdown in Jaeger UI.
Jaeger,53172758,nan,1,"2018/11/06, 15:20:44",False,"2018/11/21, 11:57:59","2018/11/06, 18:21:28",10108895.0,21.0,2,1037,Tracing Spring Boot Micro services with Jaeger deployed on AKS,Below are the files i'm using.
Jaeger,53172758,nan,1,"2018/11/06, 15:20:44",False,"2018/11/21, 11:57:59","2018/11/06, 18:21:28",10108895.0,21.0,2,1037,Tracing Spring Boot Micro services with Jaeger deployed on AKS,DemoOpentracingApplication.java
Jaeger,53172758,nan,1,"2018/11/06, 15:20:44",False,"2018/11/21, 11:57:59","2018/11/06, 18:21:28",10108895.0,21.0,2,1037,Tracing Spring Boot Micro services with Jaeger deployed on AKS,Why the instrumented service is not populating in Jaeger UI in Kubernetes?
Jaeger,51929412,nan,1,"2018/08/20, 14:07:08",True,"2020/12/19, 21:55:48","2018/08/20, 15:17:23",8949401.0,21.0,2,3282,Add jaeger trace id and span id to log4j2 logs,I want to inject x-b3-traceid and x-b3-spanid in logs  with pattern as shown-
Jaeger,51929412,nan,1,"2018/08/20, 14:07:08",True,"2020/12/19, 21:55:48","2018/08/20, 15:17:23",8949401.0,21.0,2,3282,Add jaeger trace id and span id to log4j2 logs,"For zipkins, there are libraries available like"
Jaeger,51929412,nan,1,"2018/08/20, 14:07:08",True,"2020/12/19, 21:55:48","2018/08/20, 15:17:23",8949401.0,21.0,2,3282,Add jaeger trace id and span id to log4j2 logs,"brave-context-log4j2 –
  ( https://github.com/openzipkin/brave/tree/master/context/log4j2 )"
Jaeger,51929412,nan,1,"2018/08/20, 14:07:08",True,"2020/12/19, 21:55:48","2018/08/20, 15:17:23",8949401.0,21.0,2,3282,Add jaeger trace id and span id to log4j2 logs,Spring cloud sleuth.
Jaeger,51929412,nan,1,"2018/08/20, 14:07:08",True,"2020/12/19, 21:55:48","2018/08/20, 15:17:23",8949401.0,21.0,2,3282,Add jaeger trace id and span id to log4j2 logs,"( https://cloud.spring.io/spring-cloud-sleuth/ )
How can I add that while using jaeger?"
Jaeger,50179555,59675721.0,1,"2018/05/04, 19:36:11",True,"2020/01/10, 06:59:31","2018/06/12, 22:15:20",446554.0,40043.0,2,885,Install development Jaeger All-In-One into a local Kubernetes cluster via Helm,Jaeger provides  an all-in-one  configuration for a development setup of Jaeger that doesn't use tons of memory.
Jaeger,50179555,59675721.0,1,"2018/05/04, 19:36:11",True,"2020/01/10, 06:59:31","2018/06/12, 22:15:20",446554.0,40043.0,2,885,Install development Jaeger All-In-One into a local Kubernetes cluster via Helm,The instructions  show how to easily install this via:
Jaeger,50179555,59675721.0,1,"2018/05/04, 19:36:11",True,"2020/01/10, 06:59:31","2018/06/12, 22:15:20",446554.0,40043.0,2,885,Install development Jaeger All-In-One into a local Kubernetes cluster via Helm,However I manage my development environment using Helm.
Jaeger,50179555,59675721.0,1,"2018/05/04, 19:36:11",True,"2020/01/10, 06:59:31","2018/06/12, 22:15:20",446554.0,40043.0,2,885,Install development Jaeger All-In-One into a local Kubernetes cluster via Helm,Is there a Helm chart for this setup that I can use instead?
Jaeger,66916739,66917361.0,1,"2021/04/02, 11:46:04",True,"2021/04/02, 12:39:37",nan,10894456.0,1364.0,1,27,"Can I use together: Zipkin, Sleuth, MDC, ELK, Jaeger?",I really read many articles.
Jaeger,66916739,66917361.0,1,"2021/04/02, 11:46:04",True,"2021/04/02, 12:39:37",nan,10894456.0,1364.0,1,27,"Can I use together: Zipkin, Sleuth, MDC, ELK, Jaeger?",I figure out that need to just include a starters in spring boot )))
Jaeger,66916739,66917361.0,1,"2021/04/02, 11:46:04",True,"2021/04/02, 12:39:37",nan,10894456.0,1364.0,1,27,"Can I use together: Zipkin, Sleuth, MDC, ELK, Jaeger?",Can anyone sort it out: is Sleuth create MDC (Mapped Diagnostic Context)?
Jaeger,66916739,66917361.0,1,"2021/04/02, 11:46:04",True,"2021/04/02, 12:39:37",nan,10894456.0,1364.0,1,27,"Can I use together: Zipkin, Sleuth, MDC, ELK, Jaeger?",Is sleuth create a record's ID which used by Zipkin?
Jaeger,66916739,66917361.0,1,"2021/04/02, 11:46:04",True,"2021/04/02, 12:39:37",nan,10894456.0,1364.0,1,27,"Can I use together: Zipkin, Sleuth, MDC, ELK, Jaeger?",Can I see this ID in Kibana?
Jaeger,66916739,66917361.0,1,"2021/04/02, 11:46:04",True,"2021/04/02, 12:39:37",nan,10894456.0,1364.0,1,27,"Can I use together: Zipkin, Sleuth, MDC, ELK, Jaeger?",Or do I need to use zipkin API?
Jaeger,66916739,66917361.0,1,"2021/04/02, 11:46:04",True,"2021/04/02, 12:39:37",nan,10894456.0,1364.0,1,27,"Can I use together: Zipkin, Sleuth, MDC, ELK, Jaeger?",Are there best practice to use all of this together?
Jaeger,66916739,66917361.0,1,"2021/04/02, 11:46:04",True,"2021/04/02, 12:39:37",nan,10894456.0,1364.0,1,27,"Can I use together: Zipkin, Sleuth, MDC, ELK, Jaeger?",Is Jaeger substitute both Zipkin and Sleuth or how?
Jaeger,66527792,66584227.0,1,"2021/03/08, 12:04:06",True,"2021/03/11, 16:09:56","2021/03/09, 10:50:13",2519395.0,2573.0,1,58,istio:error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid,I'm trying to run  istioctl install istio-config.yaml  command within CodeBuild on AWS but I get this error:
Jaeger,66527792,66584227.0,1,"2021/03/08, 12:04:06",True,"2021/03/11, 16:09:56","2021/03/09, 10:50:13",2519395.0,2573.0,1,58,istio:error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid,"error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid:
spec.resources.requests.storage: Forbidden: field can not be less than
previous value"
Jaeger,66527792,66584227.0,1,"2021/03/08, 12:04:06",True,"2021/03/11, 16:09:56","2021/03/09, 10:50:13",2519395.0,2573.0,1,58,istio:error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid,even though I don't have the path  spec.resources.requests.storage  in my configuration file!
Jaeger,66527792,66584227.0,1,"2021/03/08, 12:04:06",True,"2021/03/11, 16:09:56","2021/03/09, 10:50:13",2519395.0,2573.0,1,58,istio:error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid,This is the content of my file:
Jaeger,66527792,66584227.0,1,"2021/03/08, 12:04:06",True,"2021/03/11, 16:09:56","2021/03/09, 10:50:13",2519395.0,2573.0,1,58,istio:error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid,and this is the whole log of the command:
Jaeger,66527792,66584227.0,1,"2021/03/08, 12:04:06",True,"2021/03/11, 16:09:56","2021/03/09, 10:50:13",2519395.0,2573.0,1,58,istio:error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid,This is more details about the pvc  istio-jaeger-pvc :
Jaeger,66299365,66314862.0,1,"2021/02/21, 07:12:58",True,"2021/02/22, 13:29:44","2021/02/21, 20:47:42",4836988.0,25.0,1,39,Jaeger Streaming Strategy: How to configure Kafka credentials in CRD,I'm trying to install Jaeger into my K8s cluster using the streaming strategy.
Jaeger,66299365,66314862.0,1,"2021/02/21, 07:12:58",True,"2021/02/22, 13:29:44","2021/02/21, 20:47:42",4836988.0,25.0,1,39,Jaeger Streaming Strategy: How to configure Kafka credentials in CRD,I need to use the existing Kafka cluster from my cloud provider.
Jaeger,66299365,66314862.0,1,"2021/02/21, 07:12:58",True,"2021/02/22, 13:29:44","2021/02/21, 20:47:42",4836988.0,25.0,1,39,Jaeger Streaming Strategy: How to configure Kafka credentials in CRD,It requires a username and password.
Jaeger,66299365,66314862.0,1,"2021/02/21, 07:12:58",True,"2021/02/22, 13:29:44","2021/02/21, 20:47:42",4836988.0,25.0,1,39,Jaeger Streaming Strategy: How to configure Kafka credentials in CRD,Jaeger documentation mentions only broker and topic:
Jaeger,66299365,66314862.0,1,"2021/02/21, 07:12:58",True,"2021/02/22, 13:29:44","2021/02/21, 20:47:42",4836988.0,25.0,1,39,Jaeger Streaming Strategy: How to configure Kafka credentials in CRD,How can I configure Kafka credentials in CRD?
Jaeger,66299365,66314862.0,1,"2021/02/21, 07:12:58",True,"2021/02/22, 13:29:44","2021/02/21, 20:47:42",4836988.0,25.0,1,39,Jaeger Streaming Strategy: How to configure Kafka credentials in CRD,-Thanks in advance!
Jaeger,65138941,nan,2,"2020/12/04, 08:31:22",False,"2020/12/15, 04:01:30","2020/12/04, 08:48:01",9247465.0,1384.0,1,122,How to download traces from Jaeger?,"In the Jaeger UI  http://localhost:16686/search , there is an option to upload JSON files for traces."
Jaeger,65138941,nan,2,"2020/12/04, 08:31:22",False,"2020/12/15, 04:01:30","2020/12/04, 08:48:01",9247465.0,1384.0,1,122,How to download traces from Jaeger?,I wonder can we download the traces from Jaeger itself and use them in the future for finding performance issues?
Jaeger,65138941,nan,2,"2020/12/04, 08:31:22",False,"2020/12/15, 04:01:30","2020/12/04, 08:48:01",9247465.0,1384.0,1,122,How to download traces from Jaeger?,"How can we do that, I see no option to download traces from Jaeger Ui."
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,I am using the Jaeger Operator to deploy the Jaeger Query and Collector services to Kubernetes (K3S actually) along with an ElasticSearch instance for the storage backend.
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,The Jaeger Operator creates an Ingress instance for the Jaeger Query service but it assumes that all of your Jaeger Agents will also be running inside the Kubernetes cluster.
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,"Unfortunately, that is not the case for me as some applications that I am tracing are not run within the cluster so I need my Jaeger Collector to be accessible from outside."
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,This Jaeger GitHub issue discusses a potential enhancement to the Jaeger Operator for this functionality  and it suggests creating your own Ingress outside of the Operator to expose the Jaeger Collector but doesn't go into details.
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,I also want to utilize gRPC for the communication between the Agent outside the cluster and the Collector in the cluster and  this article describes how to set up an Ingress for gRPC  (though it is not specific to Jaeger).
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,"I used the  example ingress spec there , made some tweaks for my scenario, and deployed it to my cluster:"
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,This creates an Ingress for me alongside the simple-prod-query ingress that is created by the Jaeger Operator:
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,Here are the services behind the ingress:
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,"Unfortunately, my Jaeger Agent can't seem to speak to it still..."
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,"I am actually deploying my Jaeger Agent via docker-compose and as you can see here, I am configuring it to connect to jaeger-collector.my-container-dev:80:"
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,"I can see that something is wrong with the connection because when I hit the Jaeger Agent's Sampling Strategy service with an  HTTP GET to http://localhost:5778/sampling?service=myservice , I get an error back that says the following:"
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,Is there something wrong with my Ingress spec?
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,No trace data seems to be making it from my Agent to the Collector and I get errors when hitting the Jaeger Agent Sampling Service.
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,"Also, I find it a little strange that there is no IP Address listed in the  kubectl get ing  output but perhaps that is a red herring."
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,"As mentioned above, I am using K3S which seems to use traefik for its ingress controller (as opposed to nginx)."
Jaeger,64938228,64970836.0,1,"2020/11/21, 01:25:20",True,"2020/11/23, 16:58:53","2020/11/23, 16:58:53",1221718.0,328.0,1,440,Expose Jaeger Collector to clients outside of cluster,I checked the logs for the traefik controller and I didn't see anything helpful there either.
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,I am prototyping the use of  Jaeger  in an ASP.NET Core (3.1) Web API using the  Jaeger C# Client  and I got it working with the  All in One approach they mention in their Getting Started documentation .
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,This works fine for initial prototyping but I also wanted to test with storing to an instance of ElasticSearch.
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,"Luckily, I found  another Stack Overflow post about this which contains a docker-compose.yaml for deploying Elastic Search and all the Jaeger components  and I got that working after a few tweaks to the slightly outdated docker-compose ( details in my answer for that post )."
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,"However, while digging through the Jaeger documentation, I found the  CLI Flags reference for the jaeger-all-in-one distribution  that seems to contradict itself."
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,"First, it says"
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,"Jaeger all-in-one distribution with agent, collector and query."
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,Use with caution this version by default uses only in-memory database.
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,But then it also proceeds to say
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,jaeger-all-in-one can be used with these storage backends:
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,and then lists jager-all-in-one distribution CLI Flag details for:
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,"So this implies that the Jaeger All in One distribution can be used with Elastic Search, etc."
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,I am guessing the initial comment about the all-in-one distribution only supporting an in-memory database applies to the  jaeger-all-in-one with memory  option and not the others as otherwise it doesn't make sense.
Jaeger,64755896,64814852.0,1,"2020/11/09, 18:57:53",True,"2020/11/13, 05:00:24",nan,1221718.0,328.0,1,206,Jaeger all-in-one with ElasticSearch,Can someone with Jaeger experience clarify?
Jaeger,64484169,64532290.0,1,"2020/10/22, 17:14:17",True,"2020/10/26, 08:17:55",nan,261708.0,2287.0,1,124,Azure Kubernetes - Jaeger UI is not showing the Service deployed as a part of ISTIO?,I have used the following configuration to setup the Istio
Jaeger,64484169,64532290.0,1,"2020/10/22, 17:14:17",True,"2020/10/26, 08:17:55",nan,261708.0,2287.0,1,124,Azure Kubernetes - Jaeger UI is not showing the Service deployed as a part of ISTIO?,and exposed the jaeger-query service as mentioned below
Jaeger,64484169,64532290.0,1,"2020/10/22, 17:14:17",True,"2020/10/26, 08:17:55",nan,261708.0,2287.0,1,124,Azure Kubernetes - Jaeger UI is not showing the Service deployed as a part of ISTIO?,I couldn't see the below deployed application in Jaeger
Jaeger,64484169,64532290.0,1,"2020/10/22, 17:14:17",True,"2020/10/26, 08:17:55",nan,261708.0,2287.0,1,124,Azure Kubernetes - Jaeger UI is not showing the Service deployed as a part of ISTIO?,and have deployed the application as mentioned below
Jaeger,64484169,64532290.0,1,"2020/10/22, 17:14:17",True,"2020/10/26, 08:17:55",nan,261708.0,2287.0,1,124,Azure Kubernetes - Jaeger UI is not showing the Service deployed as a part of ISTIO?,I could access the service as shown below
Jaeger,64484169,64532290.0,1,"2020/10/22, 17:14:17",True,"2020/10/26, 08:17:55",nan,261708.0,2287.0,1,124,Azure Kubernetes - Jaeger UI is not showing the Service deployed as a part of ISTIO?,I do know why the service is not listed in the Jaeger UI?
Jaeger,64211500,nan,0,"2020/10/05, 18:19:38",False,"2021/03/15, 04:00:46",nan,2077917.0,763.0,1,137,Jaeger with webflux and webclient,Please take a look at  https://github.com/winster/jaeger-trace-reactive/blob/master/src/main/java/com/example/demo/JaegerTraceReactiveApplication.java  (readme might help to understand the problem better  https://github.com/winster/jaeger-trace-reactive )
Jaeger,64211500,nan,0,"2020/10/05, 18:19:38",False,"2021/03/15, 04:00:46",nan,2077917.0,763.0,1,137,Jaeger with webflux and webclient,This is a spring boot application with opentracing-jaeger.
Jaeger,64211500,nan,0,"2020/10/05, 18:19:38",False,"2021/03/15, 04:00:46",nan,2077917.0,763.0,1,137,Jaeger with webflux and webclient,"As per the doc, jaeger supports webflux and webclient."
Jaeger,64211500,nan,0,"2020/10/05, 18:19:38",False,"2021/03/15, 04:00:46",nan,2077917.0,763.0,1,137,Jaeger with webflux and webclient,"But it has been noted that, the trace skips the reactive flow when there is a webclient call."
Jaeger,64211500,nan,0,"2020/10/05, 18:19:38",False,"2021/03/15, 04:00:46",nan,2077917.0,763.0,1,137,Jaeger with webflux and webclient,Is there a way to fix this?
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,I am doing some prototyping of Jaeger Tracing for an ASP.NET Core Web API and I am able to get it working using the  All in One instance of Jaeger described in the Getting Started documentation  and the following code in my  Startup.ConfigureServices()  method:
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,"To use all this, you need to add a few packages to your project:"
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,So this works OK and I get Traces with their Spans showing up in my Jaeger Search UI (http://localhost:16686/search) but it just shows the Trace with my service name (in this case &quot;MySuperCoolWebAPI&quot;) followed by &quot;HTTP GET&quot;:
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,This is not terribly useful to see &quot;HTTP GET&quot; there.
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,"Instead, I want to see the Web API action name or something else that lets me know what kind of request this really is."
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,"As you can see from my sample code above, I have tried setting the  HttpHandlerDiagnosticOptions.OperationNameResolver  but this only affects  HttpClient  calls I make from within my web service."
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,It does not seem to affect how the Trace/Span associated with the request I received is named.
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,I also tried setting the Span OperationName in my Web API Controller method using the GlobalTracer like this but this affects an inner span and NOT the main Trace/Span that shows up on the main Jaeger UI search results page:
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,Here you can see the first child Span has its name set to what I forced it to but the main level Span (the parent of the one I changed) is not affected:
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,Is there a way I can set the operation name of the main Span with the Jaeger C# Client?
Jaeger,64036098,64039073.0,1,"2020/09/23, 23:56:54",True,"2020/09/24, 06:22:45",nan,1221718.0,328.0,1,619,ASP.NET Core Web API and Jaeger - Can I change the top level Span name?,"Also, I am using .NET Core 3.1 in case that is relevant."
Jaeger,63129021,nan,0,"2020/07/28, 10:27:24",False,"2021/03/18, 00:00:09","2021/03/18, 00:00:09",2841947.0,537.0,1,89,Spans are not getting connected with Spring Integration and Jaeger,I have 2 services A and B. I'm calling an endpoint in B from A using Spring Integration with Spring Boot 2.1.4.
Jaeger,63129021,nan,0,"2020/07/28, 10:27:24",False,"2021/03/18, 00:00:09","2021/03/18, 00:00:09",2841947.0,537.0,1,89,Spans are not getting connected with Spring Integration and Jaeger,In service A logs:
Jaeger,63129021,nan,0,"2020/07/28, 10:27:24",False,"2021/03/18, 00:00:09","2021/03/18, 00:00:09",2841947.0,537.0,1,89,Spans are not getting connected with Spring Integration and Jaeger,In service  B logs:
Jaeger,63129021,nan,0,"2020/07/28, 10:27:24",False,"2021/03/18, 00:00:09","2021/03/18, 00:00:09",2841947.0,537.0,1,89,Spans are not getting connected with Spring Integration and Jaeger,Clearly traceId is different in service B.
Jaeger,63129021,nan,0,"2020/07/28, 10:27:24",False,"2021/03/18, 00:00:09","2021/03/18, 00:00:09",2841947.0,537.0,1,89,Spans are not getting connected with Spring Integration and Jaeger,"I think due to this, I'm not seeing the service B span under service A span in JaegerUi."
Jaeger,63129021,nan,0,"2020/07/28, 10:27:24",False,"2021/03/18, 00:00:09","2021/03/18, 00:00:09",2841947.0,537.0,1,89,Spans are not getting connected with Spring Integration and Jaeger,Any idea what I'm doing wrong?
Jaeger,62992614,nan,1,"2020/07/20, 12:37:27",False,"2020/07/27, 16:44:44","2020/07/20, 17:54:29",7017126.0,43.0,1,198,Span not exported to Jaeger Collector using OpenTelemetry Java,"I have configured  JaegerGrpcSpanExporter  , so that it can export the created spans to Jaeger-Collector."
Jaeger,62992614,nan,1,"2020/07/20, 12:37:27",False,"2020/07/27, 16:44:44","2020/07/20, 17:54:29",7017126.0,43.0,1,198,Span not exported to Jaeger Collector using OpenTelemetry Java,I don't want to export the spans to Jaeger-Agent.
Jaeger,62992614,nan,1,"2020/07/20, 12:37:27",False,"2020/07/27, 16:44:44","2020/07/20, 17:54:29",7017126.0,43.0,1,198,Span not exported to Jaeger Collector using OpenTelemetry Java,I have written down below code.
Jaeger,62992614,nan,1,"2020/07/20, 12:37:27",False,"2020/07/27, 16:44:44","2020/07/20, 17:54:29",7017126.0,43.0,1,198,Span not exported to Jaeger Collector using OpenTelemetry Java,when i change the port to 14250 i.e.
Jaeger,62992614,nan,1,"2020/07/20, 12:37:27",False,"2020/07/27, 16:44:44","2020/07/20, 17:54:29",7017126.0,43.0,1,198,Span not exported to Jaeger Collector using OpenTelemetry Java,Jaeger-agent port spans are exported to UI but with 14268 I am not able to find any trace at Jaeger UI.
Jaeger,62992614,nan,1,"2020/07/20, 12:37:27",False,"2020/07/27, 16:44:44","2020/07/20, 17:54:29",7017126.0,43.0,1,198,Span not exported to Jaeger Collector using OpenTelemetry Java,Do i need to change the above code?
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,I'm working on a POC and was able to integrate 2 microservices with JaegerUI.
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,Request to an endpoint in serviceA calls an endpoint in serviceB and returns a response.
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,I have used below dependencies:
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,spring.boot.version : 2.1.4.RELEASE
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,Spring autoconfiguration takes care of everything so just added the required properties:
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,I want to achieve the below:
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,Based on the answer in below SO question:
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,opentracing-spring-cloud-starter dependency should automatically take care of sending app logs to span in JaegerUI.
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,I have a log statement like below in serviceA:
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,logger.info(&quot;sending request to serviceB.&quot;);
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,But above log is not getting captured in corresponding span and not visible in JaegerUI.
Jaeger,62948787,nan,2,"2020/07/17, 10:02:17",True,"2020/07/25, 02:23:11",nan,2841947.0,537.0,1,784,Customize spring-boot jaeger auto-configuration,Any suggestions on how to achieve the above scenarios are appreciated!
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,I'm using this library ( grpc-spring-boot-starter ) so I can have gRPC capabilities in a Spring Boot app.
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,I want to know how to properly integrate this with Istio + Jaeger tracing.
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,I need to know what are the needed dependencies for this to happen.
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,"I have two (2) apps, one serves as the gRPC client and one serves as the gRPC server,"
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,The expectation is that the trace between the gRPC client and the gRPC server must be reflected in Jaeger.
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,But it's not happening.
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,I am inside a Kubernetes cluster that has Istio.
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,What really happens is HTTP request -  Envoy sidecar -  gRPC Client's Spring Boot @RestController -  get the Headers from HTTP request -  copy those to gRPC call before making the call -  gRPC Service.
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,How can I make the gRPC client &lt;-  gRPC Service trace shown to Jaeger?
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,Are there any dependencies that needs to be imported?
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,Right now I have:
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,"I also have done something like this to ""propagate the headers"":"
Jaeger,62420728,nan,1,"2020/06/17, 05:54:46",True,"2020/09/23, 23:55:37","2020/06/17, 06:24:25",7209628.0,501.0,1,459,Istio + Jaeger tracing with gRPC calls using Spring Boot,But it doesn't seem to work..
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,I've added opentracing to my web app and am using the Jaeger all-in-one docker image as the collector.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,I'm running docker on windows 10 (hyper-v) and am using the Jaeger java client.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,When I test the web app locally on the host machine it sends traces successfully to the Jaeger collector docker instance.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,"However, when I run the web app in another docker container no traces are registered in the Jaeger UI."
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,I've tried both containers on the same docker network with no success.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,The web app in the docker container can access other services in dockers contains such as a DB and an ETCD server with no issues.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,I thought I might have the wrong ports but given the fact it works from the host environment I'm assuming these are correct and that it is a docker configuration issue.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,I've also set  JAEGER_SAMPLER_TYPE environment variable to const and the JAEGER_SAMPLER_PARAM to 1 to ensure all traces are logged.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,Edit  - when I look at the metrics it seems like the jaeger is receiving the spans.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,Every call I make to the app increments this count by one.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,I also tried the sample project  Hotrod  as suggested by Yuri Shkuro on a similar issue someone had.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,Exact same result as above.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,Metrics shows spans being received but nothing is displayed in UI.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,Edit 2  - I've narrowed it down to happening in a hyper-v windows 10 VM.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,Any help would be appreciated.
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,Thanks
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,192.168.0.15 is host machine
Jaeger,62273414,62300680.0,2,"2020/06/09, 04:01:17",True,"2021/02/19, 14:49:58","2020/06/09, 23:45:38",1222237.0,155.0,1,287,Issue sending traces from a docker container to a Jaeger docker container running in a VM,Setting up Jaeger tracer in Java
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,Unable to trace a services for springboot application on Jaeger UI(localhost:16686/search).
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,"Here I am able to run an application successfully, but unable to get a services in jaeger ui(Except defalut one jaeger-query)."
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,"I used docker cmd to start the jaeger service,"
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,Open the Jaeger UI on  http://localhost:16686/search
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,}
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,}
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,"Image 1 
 Image 2"
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,Github repository with demo:  https://github.com/pavolloffay/opentracing-java-examples .
Jaeger,61565012,nan,1,"2020/05/02, 22:17:48",False,"2020/05/08, 14:18:10",nan,13453214.0,11.0,1,605,Not able to trace a service from Jaeger UI using spring-boot application,Please help me to solve this.
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,Following Spring Cloud Sleuth's documentation I've set up a Spring Boot application with a Zipkin client:
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,Gradle config:
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,With this I start a Zipkin Server instance:
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,And I get traces in Zipkin.
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,So far so good.
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,Then I switch to a Jaeger server:
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,And without any change to my application I can see those traces in Jaeger.
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,Great.
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,Sleuth docs states:
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,15.1.
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,OpenTracing
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,Spring Cloud Sleuth is compatible with OpenTracing.
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,"If you have
  OpenTracing on the classpath, we automatically register the
  OpenTracing Tracer bean."
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,"If you wish to disable this, set
  spring.sleuth.opentracing.enabled to false"
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,Following this I added the Open Tracing dependency:
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,During startup the  OpentracingAutoConfiguration  creates a  BraveTracer .
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,"The point is I've placed a breakpoint in methods such as  scopeManager() ,  activeSpan() ,  activateSpan() ,  buildSpan()  from that  BraveTracer  and none of them is invoked during the execution of the application; the traces keep showing up in Jaeger though."
Jaeger,61439293,nan,0,"2020/04/26, 13:36:18",False,"2020/04/26, 14:55:01","2020/04/26, 14:55:01",1729795.0,17805.0,1,751,Application with Spring Cloud Sleuth reporting to Jaeger with OpenTracing is not using the BraveTracer,What am I missing here?
Jaeger,61163242,nan,1,"2020/04/11, 23:24:38",False,"2020/04/14, 21:51:42",nan,5919518.0,11.0,1,300,How to use user jaeger in node js when both installed in kubernetes?,I am trying to implement Jaeger in the node js project.
Jaeger,61163242,nan,1,"2020/04/11, 23:24:38",False,"2020/04/14, 21:51:42",nan,5919518.0,11.0,1,300,How to use user jaeger in node js when both installed in kubernetes?,I have deployed this node js project(using docker image) and Jaegaer in k8s (kubectl create -f  https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/all-in-one/jaeger-all-in-one-template.yml )
Jaeger,61163242,nan,1,"2020/04/11, 23:24:38",False,"2020/04/14, 21:51:42",nan,5919518.0,11.0,1,300,How to use user jaeger in node js when both installed in kubernetes?,Both are working individually but traces are not visible in the service
Jaeger,60831515,nan,0,"2020/03/24, 14:56:43",False,"2020/03/24, 14:56:43",nan,5262722.0,95.0,1,105,Directly connecting jaeger client to remote collector using kafka as intermediate buffer,I am trying to connect to jaeger collector which uses Kafka as intermediate buffer.
Jaeger,60831515,nan,0,"2020/03/24, 14:56:43",False,"2020/03/24, 14:56:43",nan,5262722.0,95.0,1,105,Directly connecting jaeger client to remote collector using kafka as intermediate buffer,Here are my doubts could any one please point to some docs .
Jaeger,60831515,nan,0,"2020/03/24, 14:56:43",False,"2020/03/24, 14:56:43",nan,5262722.0,95.0,1,105,Directly connecting jaeger client to remote collector using kafka as intermediate buffer,"QUESTION
  1."
Jaeger,60831515,nan,0,"2020/03/24, 14:56:43",False,"2020/03/24, 14:56:43",nan,5262722.0,95.0,1,105,Directly connecting jaeger client to remote collector using kafka as intermediate buffer,"How to connect to collector by skipping agent and use kafka as intermediate buffer.Please provide me command or configuration
  2."
Jaeger,60831515,nan,0,"2020/03/24, 14:56:43",False,"2020/03/24, 14:56:43",nan,5262722.0,95.0,1,105,Directly connecting jaeger client to remote collector using kafka as intermediate buffer,"Whats the configuration for kafka to connect to particular host.When I tried to use below command its still pointing to localhost and failing
     docker run  -e SPAN_STORAGE_TYPE=kafka jaegertracing/jaeger-collector:1.17"
Jaeger,60727887,nan,0,"2020/03/17, 20:03:41",False,"2020/03/18, 04:54:48","2020/03/18, 04:54:48",2888668.0,756.0,1,63,Configure Springboot Jaeger and RSocket,I have Springboot Webflux main application that talks to other services using RSocket and I want to configure the stack so I want to see the traces looking like this:
Jaeger,60727887,nan,0,"2020/03/17, 20:03:41",False,"2020/03/18, 04:54:48","2020/03/18, 04:54:48",2888668.0,756.0,1,63,Configure Springboot Jaeger and RSocket,As today I need to check each app to check the flow
Jaeger,60727887,nan,0,"2020/03/17, 20:03:41",False,"2020/03/18, 04:54:48","2020/03/18, 04:54:48",2888668.0,756.0,1,63,Configure Springboot Jaeger and RSocket,I'm using implementation  'io.opentracing.contrib:opentracing-spring-jaeger-cloud-starter'  dependency
Jaeger,60727887,nan,0,"2020/03/17, 20:03:41",False,"2020/03/18, 04:54:48","2020/03/18, 04:54:48",2888668.0,756.0,1,63,Configure Springboot Jaeger and RSocket,How does one integrate opentracing (jaeger) with RSocket?
Jaeger,60727887,nan,0,"2020/03/17, 20:03:41",False,"2020/03/18, 04:54:48","2020/03/18, 04:54:48",2888668.0,756.0,1,63,Configure Springboot Jaeger and RSocket,"I saw that for WebClient, you have autoconfiguration but found little or no instructions on how to do it besides  Tracking-Zipkin  page which I don't really know how to procced from that."
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,so I'm playing aroung with Jaeger and OpenTracing to trace the requests between my Spring Boot microservices.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,I have setup all necessary configurations and added the dependency:
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,So far all works fine.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,"I see all the traces and spans in my Jaeger UI, no problem."
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,"But now I have the challenge to add new spans to a specific trace, that is already finished."
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Think of it like this.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,A client calls one of the services and the tracing starts.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,"After the work is done, I see the trace in my Jaeger UI."
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,But now the invoking clients wants to add some additional tracing data to the specific trace.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,"Like tracing information from other service, that are not within the scope of my microservices."
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,I've added a filter so I can extract the trace id and send it to the client.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Now the client does a request containing the additional trace information and the trace id.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,These informationa should then be added as an additional span in the already finished trace.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Now to my question.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Is there a way to create a span and add it to a trace with only having the trace id as a String?
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,I've tried Zipkin and I could just do:
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,That span could then be added by doing a POST request to my zipkin server on port 9411 which did the magic of adding this span to the trace with the given id.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Using OpenTracing I can do:
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Unfortunately this approach needs the trace in form of a span to create the new span as a child of that trace.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,"Given the fact that I can only provide the trace id, I don't know how to get the needed span of that trace."
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Do I really need to make a call to my the Jaeger query to get the trace span needed or is there another approach I haven't been thinking of?
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Would really like to get some help on this.
Jaeger,60651104,nan,0,"2020/03/12, 11:31:22",False,"2020/03/12, 11:31:22",nan,13050176.0,11.0,1,1189,Spring Boot Jaeger create new Span for given TraceId,Cheers!
Jaeger,60489344,60500953.0,1,"2020/03/02, 14:56:56",True,"2020/03/03, 08:01:57",nan,1900417.0,4124.0,1,452,Ambassador tracing integration with Istio&#39;s Jaeger,I have a working Ambassador and a working Istio and I use the default Jaeger tracer in Istio which works fine.
Jaeger,60489344,60500953.0,1,"2020/03/02, 14:56:56",True,"2020/03/03, 08:01:57",nan,1900417.0,4124.0,1,452,Ambassador tracing integration with Istio&#39;s Jaeger,Now I would like to make Ambassador report trace data to Istio's Jaeger.
Jaeger,60489344,60500953.0,1,"2020/03/02, 14:56:56",True,"2020/03/03, 08:01:57",nan,1900417.0,4124.0,1,452,Ambassador tracing integration with Istio&#39;s Jaeger,"Ambassador documentation suggests that Jaeger is supported with the Zipkin driver, but gives example only for usage with Zipkin."
Jaeger,60489344,60500953.0,1,"2020/03/02, 14:56:56",True,"2020/03/03, 08:01:57",nan,1900417.0,4124.0,1,452,Ambassador tracing integration with Istio&#39;s Jaeger,https://www.getambassador.io/user-guide/with-istio/#tracing-integration
Jaeger,60489344,60500953.0,1,"2020/03/02, 14:56:56",True,"2020/03/03, 08:01:57",nan,1900417.0,4124.0,1,452,Ambassador tracing integration with Istio&#39;s Jaeger,"So I checked the ports of jaeger-collector service, and picked the http: jaeger-collector-http  14268/TCP"
Jaeger,60489344,60500953.0,1,"2020/03/02, 14:56:56",True,"2020/03/03, 08:01:57",nan,1900417.0,4124.0,1,452,Ambassador tracing integration with Istio&#39;s Jaeger,And modified the TracingService shown in the Ambassador docs:
Jaeger,60489344,60500953.0,1,"2020/03/02, 14:56:56",True,"2020/03/03, 08:01:57",nan,1900417.0,4124.0,1,452,Ambassador tracing integration with Istio&#39;s Jaeger,But I cannot see trace data from Ambassador in Jaeger.
Jaeger,60489344,60500953.0,1,"2020/03/02, 14:56:56",True,"2020/03/03, 08:01:57",nan,1900417.0,4124.0,1,452,Ambassador tracing integration with Istio&#39;s Jaeger,Does anyone have any experience on this topic?
Jaeger,59655255,nan,1,"2020/01/09, 01:32:29",False,"2020/01/10, 11:30:18",nan,6907909.0,842.0,1,345,Jaeger-query and Jaeger-collector pods in CrashLoopBackOff,I am trying to deploy the Jaeger Helm chart with Azure Cosmos DB acting as the Cassandra Storage Backend.
Jaeger,59655255,nan,1,"2020/01/09, 01:32:29",False,"2020/01/10, 11:30:18",nan,6907909.0,842.0,1,345,Jaeger-query and Jaeger-collector pods in CrashLoopBackOff,I have set up the CosmosDB and created a values file as below:
Jaeger,59655255,nan,1,"2020/01/09, 01:32:29",False,"2020/01/10, 11:30:18",nan,6907909.0,842.0,1,345,Jaeger-query and Jaeger-collector pods in CrashLoopBackOff,The command that I am using to deploy jaeger with the values file is:
Jaeger,59655255,nan,1,"2020/01/09, 01:32:29",False,"2020/01/10, 11:30:18",nan,6907909.0,842.0,1,345,Jaeger-query and Jaeger-collector pods in CrashLoopBackOff,"However, on checking the pods, the collector and the query are in CrashLoopBackOff"
Jaeger,59655255,nan,1,"2020/01/09, 01:32:29",False,"2020/01/10, 11:30:18",nan,6907909.0,842.0,1,345,Jaeger-query and Jaeger-collector pods in CrashLoopBackOff,Both the containers on running the describe command complain:
Jaeger,59655255,nan,1,"2020/01/09, 01:32:29",False,"2020/01/10, 11:30:18",nan,6907909.0,842.0,1,345,Jaeger-query and Jaeger-collector pods in CrashLoopBackOff,I am not sure what am I missing here?
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,"I'm currently playing around with  Jaeger Query  and trying to access its content through the  API , which uses gRPC."
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,"I'm not familiar with gRPC, but my understanding is that I need to use the Python gRPC compiler (grpcio_tools.protoc) on the relevant proto file to get useful Python definitions."
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,"What I'm trying to do is find out ways to access Jaeger Query by API, without the frontend UI."
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,"Currently, I'm very stuck on compiling the proto files."
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,"Every time I try, I get dependency issues ( Import ""fileNameHere"" was not found or has errors."
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,).
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,The Jaeger  query.proto  file contains import references to files outside the repo.
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,"Whilst I can find these and manually collect them, they also have dependencies."
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,I get the impression that following through and collecting each of these one by one is not how this was intended to be done.
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,Am I doing something wrong here?
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,The direct documentation through Jaeger is limited for this.
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,"The below is my basic terminal session, before including any manually found files (which themselves have dependencies I would have to go and find the files for)."
Jaeger,59577629,nan,1,"2020/01/03, 13:16:07",True,"2020/01/03, 13:53:57",nan,nan,nan,1,352,Compiling Jaeger gRPC proto files with Python,Thanks for any help.
Jaeger,59303550,nan,1,"2019/12/12, 13:26:47",True,"2019/12/17, 21:23:20",nan,12000584.0,11.0,1,404,"Tracing only error traces in jaeger, finding out only the error traces in jaeger","I actually was trying to sample only the error traces in my application but i already have a probabilistic sampler parameter set in my application which samples the span at the beginning itself and the rest span follow the same pattern after then, i tried using force sampling option in jaeger but it doesnt seem to override the original decision made by the initial span of getting sampled or not."
Jaeger,59303550,nan,1,"2019/12/12, 13:26:47",True,"2019/12/17, 21:23:20",nan,12000584.0,11.0,1,404,"Tracing only error traces in jaeger, finding out only the error traces in jaeger",Kindly help me out here.
Jaeger,59044026,59260094.0,2,"2019/11/26, 06:59:46",True,"2019/12/10, 06:10:40",nan,4833237.0,374.0,1,320,Why go-jaeger-client requires wrappers for metrics and logs?,I’ve started with instrumenting my gRPC service using go-gRPC-middleware.
Jaeger,59044026,59260094.0,2,"2019/11/26, 06:59:46",True,"2019/12/10, 06:10:40",nan,4833237.0,374.0,1,320,Why go-jaeger-client requires wrappers for metrics and logs?,I’ve got logs working using zap and metrics exposed for Prometheus.
Jaeger,59044026,59260094.0,2,"2019/11/26, 06:59:46",True,"2019/12/10, 06:10:40",nan,4833237.0,374.0,1,320,Why go-jaeger-client requires wrappers for metrics and logs?,Now that I’m trying to configure tracing using jaeger go client it requires me to add wrapper around metrics storage and logger.
Jaeger,59044026,59260094.0,2,"2019/11/26, 06:59:46",True,"2019/12/10, 06:10:40",nan,4833237.0,374.0,1,320,Why go-jaeger-client requires wrappers for metrics and logs?,I’m not sure I understand why those wrappers are required
Jaeger,59044026,59260094.0,2,"2019/11/26, 06:59:46",True,"2019/12/10, 06:10:40",nan,4833237.0,374.0,1,320,Why go-jaeger-client requires wrappers for metrics and logs?,https://github.com/jaegertracing/jaeger/blob/bf64373d1e690594fd8c279720faf32722cf1494/examples/hotrod/pkg/tracing/init.go#L46
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,In our application a Node.js front end talks to a Java Spring backend.
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,Everything is containerized and running in Kubernetes.
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,Some time ago we added support for Jaeger distribtued tracing across the front end and back end services.
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,Jaeger has been running fine until recently.
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,Our Elasticsearch cluster was out of date so we upgraded.
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,That mandated an upgrade of Jaeger--we ended up with the following bits:
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,Both of the opentracing libraries have a dependency on the version 0.35.1 of the Jaeger Java client.
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,"Since upgrading, traces that are created on one side or the other seem to be fine."
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,But traces that span the boundary (i.e.
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,start on the Node.js front end and complete on the Java backend) generate errors in the jaeger-agent pod like this:
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,"For these traces, the Jaeger UI shows us the spans that were created by the front end before invoking the backend API, but the child backend spans do not show up as you would expect."
Jaeger,58947001,59507206.0,1,"2019/11/20, 07:01:21",True,"2019/12/28, 02:23:31",nan,11518489.0,63.0,1,217,Get Jaeger agent error when distributed trace spans Node.js and Java services,What might cause this sort of processor error?
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",Language:  Java
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",Framework:  Spring boot
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",Tools:  Jaeger
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",I have done the following configuration for put whole trace on logs.
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",But at controller level log not shown a trace.
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",when hibernate query executed than after trace is put on logs(on service and repository level logs)
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",application.log
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",Reporter class
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",Appender class
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",Main spring boot class
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",Application.properties
Jaeger,58763972,nan,1,"2019/11/08, 11:39:12",False,"2020/11/12, 18:51:10","2019/11/09, 02:00:24",9250232.0,175.0,1,1159,"Trace id is not shown on controller level log (Spring boot, Jaeger configuration)",pom.xml
Jaeger,58642294,nan,2,"2019/10/31, 13:22:20",False,"2019/12/10, 06:26:49","2019/11/01, 11:16:49",1185148.0,1071.0,1,336,How to send entire logs to jaeger span,Is there any way to put the std logs provided by the application and the errors to a span?
Jaeger,58642294,nan,2,"2019/10/31, 13:22:20",False,"2019/12/10, 06:26:49","2019/11/01, 11:16:49",1185148.0,1071.0,1,336,How to send entire logs to jaeger span,I know that I can send some logs with  span.LogKV()  or  span.LogFields()  but it makes code look bad while there are same logs with both application logger and span logger.
Jaeger,58642294,nan,2,"2019/10/31, 13:22:20",False,"2019/12/10, 06:26:49","2019/11/01, 11:16:49",1185148.0,1071.0,1,336,How to send entire logs to jaeger span,I'm looking for an automated way to put all logs to the corresponding span.
Jaeger,58504344,nan,1,"2019/10/22, 15:30:22",True,"2019/11/04, 17:45:56",nan,6848921.0,369.0,1,577,jaeger tracing and spring cloud OpenFeign,I am using a spring Cloud openFeign for making request from service#1 to service#2
Jaeger,58504344,nan,1,"2019/10/22, 15:30:22",True,"2019/11/04, 17:45:56",nan,6848921.0,369.0,1,577,jaeger tracing and spring cloud OpenFeign,When I use restTemplate I can correctly see 2 requests in jaeger tracing.
Jaeger,58504344,nan,1,"2019/10/22, 15:30:22",True,"2019/11/04, 17:45:56",nan,6848921.0,369.0,1,577,jaeger tracing and spring cloud OpenFeign,But when using openFeign I see only 1 request.
Jaeger,58504344,nan,1,"2019/10/22, 15:30:22",True,"2019/11/04, 17:45:56",nan,6848921.0,369.0,1,577,jaeger tracing and spring cloud OpenFeign,Is there any way of integrating jaeger and openFeign?
Jaeger,58504344,nan,1,"2019/10/22, 15:30:22",True,"2019/11/04, 17:45:56",nan,6848921.0,369.0,1,577,jaeger tracing and spring cloud OpenFeign,"I found this:
 https://www.baeldung.com/spring-cloud-openfeign 
 https://github.com/OpenFeign/feign-opentracing"
Jaeger,58406367,58440891.0,1,"2019/10/16, 08:36:38",True,"2019/10/18, 00:21:43",nan,8754016.0,105.0,1,659,Error while tracing Java Application with Jaeger,I've a simple Java application that I wanted to test tracing with Jaeger but encountered error.
Jaeger,58406367,58440891.0,1,"2019/10/16, 08:36:38",True,"2019/10/18, 00:21:43",nan,8754016.0,105.0,1,659,Error while tracing Java Application with Jaeger,maven dependency -
Jaeger,58406367,58440891.0,1,"2019/10/16, 08:36:38",True,"2019/10/18, 00:21:43",nan,8754016.0,105.0,1,659,Error while tracing Java Application with Jaeger,jaeger all-in-one -
Jaeger,58406367,58440891.0,1,"2019/10/16, 08:36:38",True,"2019/10/18, 00:21:43",nan,8754016.0,105.0,1,659,Error while tracing Java Application with Jaeger,Here is the code -
Jaeger,58406367,58440891.0,1,"2019/10/16, 08:36:38",True,"2019/10/18, 00:21:43",nan,8754016.0,105.0,1,659,Error while tracing Java Application with Jaeger,and I'm getting error -
Jaeger,58406367,58440891.0,1,"2019/10/16, 08:36:38",True,"2019/10/18, 00:21:43",nan,8754016.0,105.0,1,659,Error while tracing Java Application with Jaeger,Appreciate any help !
Jaeger,58134618,59260378.0,1,"2019/09/27, 15:31:24",True,"2019/12/10, 06:43:54",nan,11268156.0,17.0,1,52,What are the advatages of the jaeger tracing with istio and without istio?,What are the advatages of the jaeger tracing with istio and without istio?
Jaeger,58134618,59260378.0,1,"2019/09/27, 15:31:24",True,"2019/12/10, 06:43:54",nan,11268156.0,17.0,1,52,What are the advatages of the jaeger tracing with istio and without istio?,For example with istio it will reduce the latency for collecting the more traces
Jaeger,57870219,nan,1,"2019/09/10, 14:52:27",False,"2019/09/23, 11:55:31",nan,11214075.0,23.0,1,484,Having problem sending trace to jaeger agent,I am trying to get mongo logs in jaeger.
Jaeger,57870219,nan,1,"2019/09/10, 14:52:27",False,"2019/09/23, 11:55:31",nan,11214075.0,23.0,1,484,Having problem sending trace to jaeger agent,Basically I want my jaeger to show my mongo application errors.
Jaeger,57870219,nan,1,"2019/09/10, 14:52:27",False,"2019/09/23, 11:55:31",nan,11214075.0,23.0,1,484,Having problem sending trace to jaeger agent,What is the best method to do this?
Jaeger,57870219,nan,1,"2019/09/10, 14:52:27",False,"2019/09/23, 11:55:31",nan,11214075.0,23.0,1,484,Having problem sending trace to jaeger agent,I have tried using the maven repo- opentracing-mongo-driver (version 0.1.4)
Jaeger,57870219,nan,1,"2019/09/10, 14:52:27",False,"2019/09/23, 11:55:31",nan,11214075.0,23.0,1,484,Having problem sending trace to jaeger agent,and in my code I have configured it using -
Jaeger,57870219,nan,1,"2019/09/10, 14:52:27",False,"2019/09/23, 11:55:31",nan,11214075.0,23.0,1,484,Having problem sending trace to jaeger agent,But I am getting this error-
Jaeger,57870219,nan,1,"2019/09/10, 14:52:27",False,"2019/09/23, 11:55:31",nan,11214075.0,23.0,1,484,Having problem sending trace to jaeger agent,What is it that I am doing wrong?
Jaeger,57854200,57855614.0,1,"2019/09/09, 15:36:12",True,"2019/09/09, 17:02:34","2019/09/09, 16:14:47",2979325.0,315.0,1,737,How do I enable Jaeger JDBC tracing in Quarkus,How do I enable  Jaeger jdbc  tracing in  Quarkus ?
Jaeger,57854200,57855614.0,1,"2019/09/09, 15:36:12",True,"2019/09/09, 17:02:34","2019/09/09, 16:14:47",2979325.0,315.0,1,737,How do I enable Jaeger JDBC tracing in Quarkus,I've followed the  Quarkus  guides for  Opentracing  and didn't see any info about this.
Jaeger,57854200,57855614.0,1,"2019/09/09, 15:36:12",True,"2019/09/09, 17:02:34","2019/09/09, 16:14:47",2979325.0,315.0,1,737,How do I enable Jaeger JDBC tracing in Quarkus,I'm using  Quarkus  v0.21.2 with the following extensions:
Jaeger,57854200,57855614.0,1,"2019/09/09, 15:36:12",True,"2019/09/09, 17:02:34","2019/09/09, 16:14:47",2979325.0,315.0,1,737,How do I enable Jaeger JDBC tracing in Quarkus,And my code is just a basic Rest endpoint which calls my entity's Panache CRUD operation.
Jaeger,57854200,57855614.0,1,"2019/09/09, 15:36:12",True,"2019/09/09, 17:02:34","2019/09/09, 16:14:47",2979325.0,315.0,1,737,How do I enable Jaeger JDBC tracing in Quarkus,Any help is appreciated.
Jaeger,57854200,57855614.0,1,"2019/09/09, 15:36:12",True,"2019/09/09, 17:02:34","2019/09/09, 16:14:47",2979325.0,315.0,1,737,How do I enable Jaeger JDBC tracing in Quarkus,I've tried the following and it didn't work:
Jaeger,57854200,57855614.0,1,"2019/09/09, 15:36:12",True,"2019/09/09, 17:02:34","2019/09/09, 16:14:47",2979325.0,315.0,1,737,How do I enable Jaeger JDBC tracing in Quarkus,"What I expect in  Jaeger  is, 2 spans for 1 trace, one for the REST call and another one for the  JDBC  call."
Jaeger,57854200,57855614.0,1,"2019/09/09, 15:36:12",True,"2019/09/09, 17:02:34","2019/09/09, 16:14:47",2979325.0,315.0,1,737,How do I enable Jaeger JDBC tracing in Quarkus,But what I see is just 1 span for the REST call.
Jaeger,57608146,66138678.0,2,"2019/08/22, 14:16:34",True,"2021/02/10, 16:19:25","2019/08/22, 15:47:17",11863447.0,55.0,1,592,Tracing apache camel route with Jaeger,I have developed a camel route with  Spring boot .
Jaeger,57608146,66138678.0,2,"2019/08/22, 14:16:34",True,"2021/02/10, 16:19:25","2019/08/22, 15:47:17",11863447.0,55.0,1,592,Tracing apache camel route with Jaeger,Now I want to trace the route using  jaeger .
Jaeger,57608146,66138678.0,2,"2019/08/22, 14:16:34",True,"2021/02/10, 16:19:25","2019/08/22, 15:47:17",11863447.0,55.0,1,592,Tracing apache camel route with Jaeger,"I tried  this example  to trace the route using  camel-opentracing  component, but I am unable to get the traces to  jaeger ."
Jaeger,57608146,66138678.0,2,"2019/08/22, 14:16:34",True,"2021/02/10, 16:19:25","2019/08/22, 15:47:17",11863447.0,55.0,1,592,Tracing apache camel route with Jaeger,I can only see it in the console.
Jaeger,57608146,66138678.0,2,"2019/08/22, 14:16:34",True,"2021/02/10, 16:19:25","2019/08/22, 15:47:17",11863447.0,55.0,1,592,Tracing apache camel route with Jaeger,One thing i am not clear is where to add the  jaeger  url?
Jaeger,57608146,66138678.0,2,"2019/08/22, 14:16:34",True,"2021/02/10, 16:19:25","2019/08/22, 15:47:17",11863447.0,55.0,1,592,Tracing apache camel route with Jaeger,Any working example will be helpful.
Jaeger,55239593,55255528.0,2,"2019/03/19, 13:08:59",True,"2019/03/20, 09:29:23","2019/03/20, 06:59:59",2485062.0,347.0,1,792,Service dependencies not shown in Jaeger between Spring Boot Applications,I'm currently trying to trace two Spring Boot (2.1.1) applications with Jaeger using  https://github.com/opentracing-contrib/java-spring-web
Jaeger,55239593,55255528.0,2,"2019/03/19, 13:08:59",True,"2019/03/20, 09:29:23","2019/03/20, 06:59:59",2485062.0,347.0,1,792,Service dependencies not shown in Jaeger between Spring Boot Applications,also tryed with no success
Jaeger,55239593,55255528.0,2,"2019/03/19, 13:08:59",True,"2019/03/20, 09:29:23","2019/03/20, 06:59:59",2485062.0,347.0,1,792,Service dependencies not shown in Jaeger between Spring Boot Applications,"The tracing of the Spans for every single service / app works fine, but not over REST requests on a global level."
Jaeger,55239593,55255528.0,2,"2019/03/19, 13:08:59",True,"2019/03/20, 09:29:23","2019/03/20, 06:59:59",2485062.0,347.0,1,792,Service dependencies not shown in Jaeger between Spring Boot Applications,There is no dependency shown between the services like you can see in the image.
Jaeger,55239593,55255528.0,2,"2019/03/19, 13:08:59",True,"2019/03/20, 09:29:23","2019/03/20, 06:59:59",2485062.0,347.0,1,792,Service dependencies not shown in Jaeger between Spring Boot Applications,Shouldn't this work out of the box through the library?
Jaeger,55239593,55255528.0,2,"2019/03/19, 13:08:59",True,"2019/03/20, 09:29:23","2019/03/20, 06:59:59",2485062.0,347.0,1,792,Service dependencies not shown in Jaeger between Spring Boot Applications,"Or do I have to implement some interceptors and request filters by my own and if so, how?"
Jaeger,55239593,55255528.0,2,"2019/03/19, 13:08:59",True,"2019/03/20, 09:29:23","2019/03/20, 06:59:59",2485062.0,347.0,1,792,Service dependencies not shown in Jaeger between Spring Boot Applications,You can CHECKOUT a minimalistic project containing the problem   here
Jaeger,55239593,55255528.0,2,"2019/03/19, 13:08:59",True,"2019/03/20, 09:29:23","2019/03/20, 06:59:59",2485062.0,347.0,1,792,Service dependencies not shown in Jaeger between Spring Boot Applications,Btw: Jaeger runs as all-in-one via docker and works as expected
Jaeger,55216969,55225031.0,1,"2019/03/18, 10:09:05",True,"2019/03/18, 17:41:35","2019/03/18, 10:33:46",11219342.0,13.0,1,545,Possible to configure Jaeger via application.properties?,According to  https://quarkus.io/guides/opentracing-guide  all Jeager configuration is via JVM args (-DJAEGER_ENDPOINT...) but I'd like to use either  application.properties  or  microprofile-config.properties  to configure tracing.
Jaeger,55216969,55225031.0,1,"2019/03/18, 10:09:05",True,"2019/03/18, 17:41:35","2019/03/18, 10:33:46",11219342.0,13.0,1,545,Possible to configure Jaeger via application.properties?,I've tried the following but the only config that seems to be picked up by Quarkus is the service-name all other properties are ignored.
Jaeger,55216969,55225031.0,1,"2019/03/18, 10:09:05",True,"2019/03/18, 17:41:35","2019/03/18, 10:33:46",11219342.0,13.0,1,545,Possible to configure Jaeger via application.properties?,"So, question is if it is possible to configure via config-files or this is not currently supported?"
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,"I have a kubernetes cluster on google cloud platform, and on it, I have a jaeger deployment via development setup of  jaeger-kubernetes templates  
because my purpose is setup  elasticsearch  like backend storage, due to this, I follow the jaeger-kubernetes github documentation with the following actions"
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,Here are configured the URLs to access to  elasticsearch  server and username and password and ports
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,"And here, there are configured the download of docker images of the elasticsearch service and their volume mounts."
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,"And then, at this moment we have a elasticsearch service running over 9200 and 9300 ports"
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,"According to the  Jaeger architecture , the  jaeger-collector  and  jaeger-query  services require access to backend storage."
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,"And so, these are my services running on my kubernetes cluster:"
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,I execute it:
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,And I get the following edit entry:
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,"Here ... do I need setup our own URLs to collector and query services, which will be connect wiht elasticsearch backend service?"
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,How to can I setup the elasticsearch IP address or URLs here?
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,"In the jaeger components, the query and collector need access to storage, but I don't know what is the elastic endpoint ..."
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,Is this  server-urls: http://elasticsearch:9200  a correct endpoint?
Jaeger,53914858,53949856.0,1,"2018/12/24, 16:48:17",True,"2019/01/01, 03:38:13","2018/12/27, 19:22:19",2773461.0,2095.0,1,882,Connecting jaeger with elasticsearch backend storage on kubernetes cluster,"I am starting in the kubernetes and DevOps world, and I appreciate if someone can help me in the concepts and point me in the right address in order to setup jaeger and elasticsearch as a backend storage."
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,I am having problems pointing a jaeger agent to a collector running in openshift.
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,I am able to browse my OCP collector endpoint doing this:
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,My jaeger agent Dockerfile currently looks like this
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,I get the expected result when i point my agent to a collector running locally per the first commented line.
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,I get the following error using the second uncommented CMD flag.
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,"When i attempt the agent to the collector running on openshift, i get the error below"
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,I am able to successfully curl the collector endpoint by doing this
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,I get the following error when i attempt to curl the endpoint this way:
Jaeger,53709029,nan,1,"2018/12/10, 17:44:19",False,"2019/01/25, 16:46:35",nan,7089682.0,531.0,1,283,Connect a Jaeger agent to a collector running in Openshift,I need help setting up a proper  --collector.host-port  flag that will connect to a collector running remotely behind an HTTPS protocol.
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,I'm struggling with setting up OpenTracing/Jaeger for a Spring Boot 2.0.2 application.
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,Starting from a working but very sample for Spring Boot 1.5.3 I moved on to Spring Boot 2.0.2 -- which properly sent the traces.
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,"But the dependencies used there were ridiculously old (like 0.0.4 for opentracing-spring-web-autoconfigure, which is now available in 0.3.2)."
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,So I migrated the application to the latest dependencies which resulted in no traces appearing anymore in Jaeger.
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,I've upload my tests to  https://gitlab.com/ceedee_/opentracing-spring-boot .
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,The branches are as follows:
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,Differences from 2. to 3. are as follows:
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,Does anyone have a clue what I'm doing wrong in order to properly put traces into Jaeger?
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,Hints on debugging OpenTracing/Jaeger are appreciated as well!
Jaeger,53518480,53537859.0,1,"2018/11/28, 13:32:15",True,"2018/11/29, 13:19:37",nan,4125383.0,321.0,1,860,OpenTracing with Spring Boot 2.0.2 does not yield any traces in Jaeger,"Best regards,
cd_"
Jaeger,53441689,nan,0,"2018/11/23, 08:36:49",False,"2018/11/23, 08:36:49",nan,4620187.0,87.0,1,449,Jaeger agent unable to connect to collector,I have started Jaeger standalone binary on a Linux box and am trying to run Jaeger agent binary on Mac that tries to connect to the Jaeger collector of the standalone process.
Jaeger,53441689,nan,0,"2018/11/23, 08:36:49",False,"2018/11/23, 08:36:49",nan,4620187.0,87.0,1,449,Jaeger agent unable to connect to collector,"However it keeps failing with ""error"":""tchannel error ErrCodeTimeout: timeout""."
Jaeger,53441689,nan,0,"2018/11/23, 08:36:49",False,"2018/11/23, 08:36:49",nan,4620187.0,87.0,1,449,Jaeger agent unable to connect to collector,The problem is not with different OS versions as I get the same error when trying from another Linux box.
Jaeger,53441689,nan,0,"2018/11/23, 08:36:49",False,"2018/11/23, 08:36:49",nan,4620187.0,87.0,1,449,Jaeger agent unable to connect to collector,I used telnet to confirm that the collector port was open for connection.
Jaeger,53441689,nan,0,"2018/11/23, 08:36:49",False,"2018/11/23, 08:36:49",nan,4620187.0,87.0,1,449,Jaeger agent unable to connect to collector,"The stack trace is below-
./cmd/agent/agent- --collector.host-port=172.xx.2.4:14267
{""level"":""info"",""ts"":1542954225.5485492,""caller"":""tchannel/builder.go:94"",""msg"":""Enabling service discovery"",""service"":""jaeger-collector""}
{""level"":""info"",""ts"":1542954225.5489438,""caller"":""peerlistmgr/peer_list_mgr.go:111"",""msg"":""Registering active peer"",""peer"":""172.xx.2.4:14267""}
{""level"":""info"",""ts"":1542954225.5502574,""caller"":""agent/main.go:62"",""msg"":""Starting agent""}
{""level"":""info"",""ts"":1542954226.5518098,""caller"":""peerlistmgr/peer_list_mgr.go:157"",""msg"":""Not enough connected peers"",""connected"":0,""required"":1}
{""level"":""info"",""ts"":1542954226.552439,""caller"":""peerlistmgr/peer_list_mgr.go:166"",""msg"":""Trying to connect to peer"",""host:port"":""172.xx.2.4:14267""}
{""level"":""error"",""ts"":1542954226.8054206,""caller"":""peerlistmgr/peer_list_mgr.go:171"",""msg"":""Unable to connect"",""host:port"":""172.xx.2.4:14267"",""connCheckTimeout"":0.25,""error"":""tchannel error ErrCodeTimeout: timeout"",""stacktrace"":""github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr.(*PeerListManager).ensureConnections\n\t/Users/swarnim/go/src/github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr/peer_list_mgr.go:171\ngithub.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr."
Jaeger,53441689,nan,0,"2018/11/23, 08:36:49",False,"2018/11/23, 08:36:49",nan,4620187.0,87.0,1,449,Jaeger agent unable to connect to collector,"(*PeerListManager).maintainConnections\n\t/Users/swarnim/go/src/github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr/peer_list_mgr.go:101""}"
Jaeger,52764397,nan,1,"2018/10/11, 19:00:53",False,"2020/08/21, 02:12:43","2020/08/21, 02:12:43",4749104.0,195.0,1,1210,Jaeger Tracing Config Endpoint,I'm trying setting up a spring application which use Jaeger/Prometheus.
Jaeger,52764397,nan,1,"2018/10/11, 19:00:53",False,"2020/08/21, 02:12:43","2020/08/21, 02:12:43",4749104.0,195.0,1,1210,Jaeger Tracing Config Endpoint,"I already configured Prometheus successfully by prometheus.yaml file, but I haven't understood how configure Jaeger target endpoint."
Jaeger,52764397,nan,1,"2018/10/11, 19:00:53",False,"2020/08/21, 02:12:43","2020/08/21, 02:12:43",4749104.0,195.0,1,1210,Jaeger Tracing Config Endpoint,Must I create a new yaml file and specify into it the configuration?
Jaeger,52764397,nan,1,"2018/10/11, 19:00:53",False,"2020/08/21, 02:12:43","2020/08/21, 02:12:43",4749104.0,195.0,1,1210,Jaeger Tracing Config Endpoint,"If yes, with which syntax?"
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET","I am using  Zuul  as an api-gateway in a spring-cloud micro-service app, so that every access  to  api-gateway/some-service/a_route  is redirected to  /a_route  in a generic way (the discovery is backed by consul)."
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET",I am trying to use  Jaeger  to instrument this system.
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET","And at this point I am using   opentracing-spring-web-autoconfigure , because I cannot upgrade my spring boot/cloud version easily (I am using1.4.5.RELEASE Camden.SR7)."
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET","So I just added the dependency, created the Jaeger tracer and redirect it to the docker all in one collector."
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET","I have begin by instrumenting the gateway and It somewhat works =  It generate span on the gateway, but all the route are marked :"
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET",apigateway-service: GET
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET","and there is no information concerning the forwarded route at this level, the full route itself is store in a tag : http.url 
"" http://localhost:8080/collection-service/collections/projects/ """
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET",To be useful I would prefer to have span named :
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET",apigateway-service: GET collection-service/collections/projects/
Jaeger,52628054,52721877.0,1,"2018/10/03, 16:23:13",True,"2018/10/09, 16:09:20","2018/10/09, 16:07:06",51386.0,564.0,1,232,"When Instrumenting Zuul gateway with Jaeger, all routes marked GET",Can this be configured somewhere ?
Jaeger,52522049,nan,1,"2018/09/26, 19:08:02",True,"2019/12/18, 12:08:53",nan,10419854.0,23.0,1,353,opentracing and jaeger on a .netcore 2.1 application?,"Our application consist of angular 6 for the UI and .netcore 2.0 for the back end, looking to implement tracing to it and so far opentracing seems the most prominent but I can't seem to find any good help documentations for .netcore 2.0 apps."
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,My java code:
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,My gradle dependencies:
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,This code works in localhost.
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,I have already passed the  JAEGER_AGENT_HOST  and  JAEGER_AGENT_PORT  env to the container.
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,And I can see the Jaeger Initialized log in remote:
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,Using the UDP Sender to send spans to the agent.
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,"Using sender UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@27e16046, receiveBuf=null, receiveOffSet=-1, receiveLength=0))
  Using sender UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@27e16046, receiveBuf=null, receiveOffSet=-1, receiveLength=0))
  2018-08-16 13:24:32.809  INFO 1 --- [http-nio-8080-exec-1] io.jaegertracing.Configuration           : Initialized tracer=JaegerTracer(version=Java-0.30.4, serviceName="
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,But I can see it in Jaeger UI.
Jaeger,51878525,51977735.0,1,"2018/08/16, 16:42:22",True,"2018/08/23, 05:56:58","2018/08/17, 16:27:38",1047335.0,4269.0,1,294,Application can not report to jaeger from istio container,"And I tried to use  tcpdump , I cannot find the udp package."
Jaeger,51720184,nan,1,"2018/08/07, 09:28:57",True,"2018/08/10, 22:38:32","2018/08/07, 11:02:02",4239025.0,19.0,1,1109,How to connect Opentracing application to a remote Jaeger collector,I am using Jaeger UI to display traces from my application.
Jaeger,51720184,nan,1,"2018/08/07, 09:28:57",True,"2018/08/10, 22:38:32","2018/08/07, 11:02:02",4239025.0,19.0,1,1109,How to connect Opentracing application to a remote Jaeger collector,It's work fine for me if both application an Jaeger are running on same server.
Jaeger,51720184,nan,1,"2018/08/07, 09:28:57",True,"2018/08/10, 22:38:32","2018/08/07, 11:02:02",4239025.0,19.0,1,1109,How to connect Opentracing application to a remote Jaeger collector,But I need to run my Jaeger collector on a different server.
Jaeger,51720184,nan,1,"2018/08/07, 09:28:57",True,"2018/08/10, 22:38:32","2018/08/07, 11:02:02",4239025.0,19.0,1,1109,How to connect Opentracing application to a remote Jaeger collector,"I tried out with JAEGER_ENDPOINT, JAEGER_AGENT_HOST and JAEGER_AGENT_PORT, but it failed."
Jaeger,51720184,nan,1,"2018/08/07, 09:28:57",True,"2018/08/10, 22:38:32","2018/08/07, 11:02:02",4239025.0,19.0,1,1109,How to connect Opentracing application to a remote Jaeger collector,"I don't know, whether my values setting for these variables is wrong or not."
Jaeger,51720184,nan,1,"2018/08/07, 09:28:57",True,"2018/08/10, 22:38:32","2018/08/07, 11:02:02",4239025.0,19.0,1,1109,How to connect Opentracing application to a remote Jaeger collector,Whether it required any configuration settings inside application code?
Jaeger,51720184,nan,1,"2018/08/07, 09:28:57",True,"2018/08/10, 22:38:32","2018/08/07, 11:02:02",4239025.0,19.0,1,1109,How to connect Opentracing application to a remote Jaeger collector,Can you provide me any documentation for this problem?
Jaeger,49179249,49311159.0,1,"2018/03/08, 19:39:44",True,"2018/03/16, 01:55:29",nan,1953109.0,1116.0,1,585,"Node/Express service, span generated in logs but can not see my service in Jaeger UI",I am trying to setup Jaeger tracing for my micro service that is written in Node.js using Express.js.
Jaeger,49179249,49311159.0,1,"2018/03/08, 19:39:44",True,"2018/03/16, 01:55:29",nan,1953109.0,1116.0,1,585,"Node/Express service, span generated in logs but can not see my service in Jaeger UI","I have added a simple get request handler in my express app and when I hit the endpoint via curl, I can see that a span is generated in logs, but I do not see the name of my service in Jaeger UI."
Jaeger,49179249,49311159.0,1,"2018/03/08, 19:39:44",True,"2018/03/16, 01:55:29",nan,1953109.0,1116.0,1,585,"Node/Express service, span generated in logs but can not see my service in Jaeger UI",// server.js
Jaeger,49179249,49311159.0,1,"2018/03/08, 19:39:44",True,"2018/03/16, 01:55:29",nan,1953109.0,1116.0,1,585,"Node/Express service, span generated in logs but can not see my service in Jaeger UI",// tracing.js
Jaeger,49179249,49311159.0,1,"2018/03/08, 19:39:44",True,"2018/03/16, 01:55:29",nan,1953109.0,1116.0,1,585,"Node/Express service, span generated in logs but can not see my service in Jaeger UI",I see in logs:
Jaeger,49179249,49311159.0,1,"2018/03/08, 19:39:44",True,"2018/03/16, 01:55:29",nan,1953109.0,1116.0,1,585,"Node/Express service, span generated in logs but can not see my service in Jaeger UI","
2018-03-08T01:03:34.519134479Z INFO  Reporting span 9b88812951bcd52f:9b88812951bcd52f:0:1"
Jaeger,48095718,nan,1,"2018/01/04, 14:50:19",True,"2018/01/25, 00:12:09","2018/01/04, 17:29:54",5568862.0,410.0,1,1637,No service dependencies found in Jaeger UI,I am new to jaeger and I am facing issues with finding the services list in the jaeger UI.
Jaeger,48095718,nan,1,"2018/01/04, 14:50:19",True,"2018/01/25, 00:12:09","2018/01/04, 17:29:54",5568862.0,410.0,1,1637,No service dependencies found in Jaeger UI,Below are the .yaml configurations I prepared to run jaeger with my spring boot app on Kubernetes using minikube locally.
Jaeger,48095718,nan,1,"2018/01/04, 14:50:19",True,"2018/01/25, 00:12:09","2018/01/04, 17:29:54",5568862.0,410.0,1,1637,No service dependencies found in Jaeger UI,kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/production-elasticsearch/elasticsearch.yml --namespace=kube-system
Jaeger,48095718,nan,1,"2018/01/04, 14:50:19",True,"2018/01/25, 00:12:09","2018/01/04, 17:29:54",5568862.0,410.0,1,1637,No service dependencies found in Jaeger UI,kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/jaeger-production-template.yml --namespace=kube-system
Jaeger,48095718,nan,1,"2018/01/04, 14:50:19",True,"2018/01/25, 00:12:09","2018/01/04, 17:29:54",5568862.0,410.0,1,1637,No service dependencies found in Jaeger UI,Created deployment for my spring boot app and jaeger agent to run on the same container
Jaeger,48095718,nan,1,"2018/01/04, 14:50:19",True,"2018/01/25, 00:12:09","2018/01/04, 17:29:54",5568862.0,410.0,1,1637,No service dependencies found in Jaeger UI,And the spring boot app service yaml
Jaeger,48095718,nan,1,"2018/01/04, 14:50:19",True,"2018/01/25, 00:12:09","2018/01/04, 17:29:54",5568862.0,410.0,1,1637,No service dependencies found in Jaeger UI,I am getting
Jaeger,48095718,nan,1,"2018/01/04, 14:50:19",True,"2018/01/25, 00:12:09","2018/01/04, 17:29:54",5568862.0,410.0,1,1637,No service dependencies found in Jaeger UI,No service dependencies found
Jaeger,46561079,48432603.0,2,"2017/10/04, 12:09:44",True,"2021/02/19, 14:43:21","2019/12/10, 07:36:03",209334.0,1001.0,1,762,Jaeger standalone without docker,Cannot find any information if Jaeger can be executed without docker?
Jaeger,46561079,48432603.0,2,"2017/10/04, 12:09:44",True,"2021/02/19, 14:43:21","2019/12/10, 07:36:03",209334.0,1001.0,1,762,Jaeger standalone without docker,"Does a standalone jar exist, or will there be a release in the future for Jaeger like Zipkin has ?"
Jaeger,66236610,nan,1,"2021/02/17, 08:13:16",False,"2021/02/19, 14:35:11",nan,12665854.0,11.0,0,7,planning to use Jaeger for distributed tracing,Planning to use Jaeger for distributed tracing of our Application.
Jaeger,66236610,nan,1,"2021/02/17, 08:13:16",False,"2021/02/19, 14:35:11",nan,12665854.0,11.0,0,7,planning to use Jaeger for distributed tracing,"Need to use elasticsearch as db backend, rather than cassandra for Jaeger."
Jaeger,66073948,nan,0,"2021/02/06, 07:44:23",False,"2021/02/06, 07:44:23",nan,10690874.0,607.0,0,20,How to use Jaeger &#39;well&#39;?,I have a very general question about jaeger (opentracing).
Jaeger,66073948,nan,0,"2021/02/06, 07:44:23",False,"2021/02/06, 07:44:23",nan,10690874.0,607.0,0,20,How to use Jaeger &#39;well&#39;?,"I set up Jaeger, and can find the spans and traces - where each originiated from, where it ends up, and so on."
Jaeger,66073948,nan,0,"2021/02/06, 07:44:23",False,"2021/02/06, 07:44:23",nan,10690874.0,607.0,0,20,How to use Jaeger &#39;well&#39;?,"However, I am curious how to use Jaeger 'well'."
Jaeger,66073948,nan,0,"2021/02/06, 07:44:23",False,"2021/02/06, 07:44:23",nan,10690874.0,607.0,0,20,How to use Jaeger &#39;well&#39;?,"I think Jaeger itself doesn't give much information, except for the fact I can check which server or api is the bottleneck."
Jaeger,66073948,nan,0,"2021/02/06, 07:44:23",False,"2021/02/06, 07:44:23",nan,10690874.0,607.0,0,20,How to use Jaeger &#39;well&#39;?,"The scenario I have in mind, is to get an alert for an error or warning from the logging system (probably it will be ElasticSearch), and get the trace id from it, and check the whole trace from Jaeger."
Jaeger,66073948,nan,0,"2021/02/06, 07:44:23",False,"2021/02/06, 07:44:23",nan,10690874.0,607.0,0,20,How to use Jaeger &#39;well&#39;?,Any suggestions on how to use Jaeger 'well'?
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,i have been having some problem.
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,i have a k8s cluster up and running and wanted it to connect to jaeger.
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,followed this page(ReadMe.md)  https://github.com/jaegertracing/jaeger-operator?utm_source=thenewstack&amp;utm_medium=website&amp;utm_campaign=platform
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,i have the jaeger ui now but it not able to recognize the services.
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,can someone give any suggestions??
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,"In the above link i didnt understand what this line means, &quot; You probably also want to download and customize the operator.yaml, setting the env var WATCH_NAMESPACE to have an empty value, so that it can watch for instances across all namespaces."
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,&quot;
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,kubectl logs -n observability deployment/jaeger-operator
Jaeger,65608240,nan,0,"2021/01/07, 09:32:48",False,"2021/01/07, 09:32:48",nan,14316740.0,59.0,0,41,Jaeger Operator not create query &amp; collector pods,"time=&quot;2021-01-07T06:48:47Z&quot; level=info msg=Versions arch=amd64 identity=observability.jaeger-operator jaeger=1.21.0 jaeger-operator=v1.21.2 operator-sdk=v0.18.2 os=linux version=go1.14.12
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Consider running the operator in a cluster-wide scope for extra features&quot;
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Auto-detected the platform&quot; platform=kubernetes
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Auto-detected ingress api&quot; ingress-api=networking
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Automatically adjusted the 'es-provision' flag&quot; es-provision=no
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Automatically adjusted the 'kafka-provision' flag&quot; kafka-provision=no
time=&quot;2021-01-07T06:48:50Z&quot; level=info msg=&quot;Install prometheus-operator in your cluster to create ServiceMonitor objects&quot; error=&quot;no ServiceMonitor registered with the API&quot;"
Jaeger,65490691,65888740.0,1,"2020/12/29, 12:26:29",True,"2021/01/25, 18:32:56",nan,804401.0,1906.0,0,169,Jaeger error No suitable sender found. Using NoopSender,"I am using below example of OpenTracing, Jaeger
 https://github.com/yurishkuro/opentracing-tutorial/tree/master/csharp/src/lesson01"
Jaeger,65490691,65888740.0,1,"2020/12/29, 12:26:29",True,"2021/01/25, 18:32:56",nan,804401.0,1906.0,0,169,Jaeger error No suitable sender found. Using NoopSender,I have my Jaeger UI running for which I used the below command.
Jaeger,65490691,65888740.0,1,"2020/12/29, 12:26:29",True,"2021/01/25, 18:32:56",nan,804401.0,1906.0,0,169,Jaeger error No suitable sender found. Using NoopSender,Below is my code which I tried from above github link
Jaeger,65490691,65888740.0,1,"2020/12/29, 12:26:29",True,"2021/01/25, 18:32:56",nan,804401.0,1906.0,0,169,Jaeger error No suitable sender found. Using NoopSender,Above is the Class Library project.
Jaeger,65490691,65888740.0,1,"2020/12/29, 12:26:29",True,"2021/01/25, 18:32:56",nan,804401.0,1906.0,0,169,Jaeger error No suitable sender found. Using NoopSender,Below is my console app where I am calling the TriggerTrace Method.
Jaeger,65490691,65888740.0,1,"2020/12/29, 12:26:29",True,"2021/01/25, 18:32:56",nan,804401.0,1906.0,0,169,Jaeger error No suitable sender found. Using NoopSender,"When that function is executed I would expect some traces in the Jaeger UI,"
Jaeger,65490691,65888740.0,1,"2020/12/29, 12:26:29",True,"2021/01/25, 18:32:56",nan,804401.0,1906.0,0,169,Jaeger error No suitable sender found. Using NoopSender,"Below is the error I see, I do not see any traces in the UI,
Am I missing any configuration ?"
Jaeger,65149873,nan,1,"2020/12/04, 21:53:42",True,"2020/12/08, 23:57:48","2020/12/05, 00:36:26",3703933.0,357.0,0,194,Spring jaeger propagating span to an Async method,I have a simple Spring Boot 2.x RestController with an endpoint performing certain remote calls as well as controller is also calling an Async method that in turn makes several remote HTTP calls.
Jaeger,65149873,nan,1,"2020/12/04, 21:53:42",True,"2020/12/08, 23:57:48","2020/12/05, 00:36:26",3703933.0,357.0,0,194,Spring jaeger propagating span to an Async method,I'm having opentracing-spring-jaeger-web-starter in classpath with tracing enabled.
Jaeger,65149873,nan,1,"2020/12/04, 21:53:42",True,"2020/12/08, 23:57:48","2020/12/05, 00:36:26",3703933.0,357.0,0,194,Spring jaeger propagating span to an Async method,"If i invoke my REST endpoint, It creates a span for the endpoint call as well as remote calls that the controller is making synchronously."
Jaeger,65149873,nan,1,"2020/12/04, 21:53:42",True,"2020/12/08, 23:57:48","2020/12/05, 00:36:26",3703933.0,357.0,0,194,Spring jaeger propagating span to an Async method,However the remote calls made by Async method is getting reported in its own span.
Jaeger,65149873,nan,1,"2020/12/04, 21:53:42",True,"2020/12/08, 23:57:48","2020/12/05, 00:36:26",3703933.0,357.0,0,194,Spring jaeger propagating span to an Async method,Is this by design or is there a way to propagate some context information to the Async method to better group/relate the spans ?
Jaeger,65059109,65059275.0,1,"2020/11/29, 12:26:15",True,"2020/11/29, 12:48:21",nan,12517329.0,3.0,0,75,EKS connection refused when trying to talk to Jaeger agent daemonset,I recently deployed the jaeger agent as a daemonset on my k8s cluster alongside a collector.
Jaeger,65059109,65059275.0,1,"2020/11/29, 12:26:15",True,"2020/11/29, 12:48:21",nan,12517329.0,3.0,0,75,EKS connection refused when trying to talk to Jaeger agent daemonset,When trying to send spans to the agent using:
Jaeger,65059109,65059275.0,1,"2020/11/29, 12:26:15",True,"2020/11/29, 12:48:21",nan,12517329.0,3.0,0,75,EKS connection refused when trying to talk to Jaeger agent daemonset,When looking at the application logs I see:
Jaeger,65059109,65059275.0,1,"2020/11/29, 12:26:15",True,"2020/11/29, 12:48:21",nan,12517329.0,3.0,0,75,EKS connection refused when trying to talk to Jaeger agent daemonset,"All nodes can access each other as the security group does not block ports between them, when using a sidecar agent the spans are sent without issue."
Jaeger,65059109,65059275.0,1,"2020/11/29, 12:26:15",True,"2020/11/29, 12:48:21",nan,12517329.0,3.0,0,75,EKS connection refused when trying to talk to Jaeger agent daemonset,Replicate:
Jaeger,65059109,65059275.0,1,"2020/11/29, 12:26:15",True,"2020/11/29, 12:48:21",nan,12517329.0,3.0,0,75,EKS connection refused when trying to talk to Jaeger agent daemonset,Deploy agent using:
Jaeger,65059109,65059275.0,1,"2020/11/29, 12:26:15",True,"2020/11/29, 12:48:21",nan,12517329.0,3.0,0,75,EKS connection refused when trying to talk to Jaeger agent daemonset,Then deploy hotrod application:
Jaeger,65056880,nan,1,"2020/11/29, 05:55:10",False,"2020/12/15, 04:11:35",nan,1532146.0,166.0,0,22,how to setup jaeger backend when services are running in different hosts?,I'm following this tutorial:  https://github.com/yurishkuro/opentracing-tutorial/tree/master/java/src/main/java/lesson03 .
Jaeger,65056880,nan,1,"2020/11/29, 05:55:10",False,"2020/12/15, 04:11:35",nan,1532146.0,166.0,0,22,how to setup jaeger backend when services are running in different hosts?,What need to be set so that services running in different hosts can send the data to the same backend?
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),I am playing with quarkus and jaeger by opentracing integration.
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),After run the jaeger server and the  https://github.com/quarkusio/quarkus-quickstarts/tree/master/opentracing-quickstart  repo I found the traces at http://localhost:16686/search.
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),"But I only found the Resource class, arguments, and Process name , but the &quot;Logs&quot; is not shown on trace detail expand."
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),The steps are easy:
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),1.Run jaeger server  docker run --rm=true --name erp_jaeger_server -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 -p 5775:5775/udp -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14268:14268 -p 9411:9411 jaegertracing/all-in-one:latest
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),"clone the example repo and run it
  https://github.com/quarkusio/quarkus-quickstarts/tree/master/opentracing-quickstart 
(no further configuration)"
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),run-&gt;  mvn quarkus:dev
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),visit  http://localhost:8080/hello/
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),5.Explore on jaeger ui 'http://localhost:16686/'
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),"6.Found the traces Tags, and Process Details but detailes content Log.info('hello') is not shown"
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),I was trying with @Slfj but i got the same result
Jaeger,63634257,63654371.0,2,"2020/08/28, 15:58:07",True,"2020/09/01, 14:23:48","2020/08/28, 16:24:47",977959.0,363.0,0,273,unable to obtain detail trace message on jaeger ui (using quarkus),Thanks in advance.
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,"I have two endpoints, the first one uses Feign implementation:"
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,Controller:
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,Feign Client:
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,"the second, do the same thing, but uses RetroFit implementation:"
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,Controller:
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,Retrofit Client:
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,Bean:
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,"With Feign implementation, Jaeger UI shows me two steps: Controller Request and the Http Request [GET] to  http://viacep.com.br ."
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,"With Retrofit, Jaeger does not show the request to  http://viacep.com.br ."
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,Only the controller step.
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,Why?
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,Is there some configuration that I can do at okHttpClient builder?
Jaeger,63162669,nan,0,"2020/07/30, 00:26:13",False,"2020/07/30, 00:26:13",nan,7200236.0,145.0,0,62,Jaeger does work with RetroFit and Spring Boot?,I can`t use feign in this project.
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,I am trying to instrument my program with jaeger-tracing (c++).
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,"I was able to view my traces when I compliled the program with yaml-cpp version 0.5.3, but when I changed my yaml-cpp version to 0.6.x, I am unable to view my traces."
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,Dont know why its happening.
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,JaegerProgram Source code;
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,compiling command -  g++ -std=c++1z test.cpp -L /usr/local/lib/libyaml-cpp.a -ljaegertracing -lyaml-cpp
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,Yaml file
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,"OS : ubuntu 18.04
jaegerTracing : master branch version"
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,UPDATE
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,"after little digging I found some fact, When I parse the above mention config file try to print the result I get the same value as written in config file, but when I parse same file using  yaml-cpp-0.6.x  the sampler.type is showing 'remote' and sampler.param to be '0.001' and when I manually change this change these values to be same as in  config.yaml  it has started showing traces."
Jaeger,60751821,nan,0,"2020/03/19, 08:15:15",False,"2020/03/26, 23:18:48","2020/03/26, 23:18:48",8575474.0,184.0,0,84,Jaeger Tracing not working with yaml-cpp version 0.6.x,The error is present in parsing the yaml file as I could clearly see different values is loaded as configuration.
Jaeger,55220796,nan,1,"2019/03/18, 13:55:40",False,"2019/04/08, 10:37:58","2019/03/18, 16:28:50",10856824.0,105.0,0,129,Jaeger Operator for Openshift trouble with the guide,Following  this  guide I I don't really see how to implement the operator with elasticsearch.
Jaeger,55220796,nan,1,"2019/03/18, 13:55:40",False,"2019/04/08, 10:37:58","2019/03/18, 16:28:50",10856824.0,105.0,0,129,Jaeger Operator for Openshift trouble with the guide,"Ok, so I install the operator and after that follwing the  example which is with :"
Jaeger,55220796,nan,1,"2019/03/18, 13:55:40",False,"2019/04/08, 10:37:58","2019/03/18, 16:28:50",10856824.0,105.0,0,129,Jaeger Operator for Openshift trouble with the guide,which is not supported by openshift as an api.
Jaeger,55220796,nan,1,"2019/03/18, 13:55:40",False,"2019/04/08, 10:37:58","2019/03/18, 16:28:50",10856824.0,105.0,0,129,Jaeger Operator for Openshift trouble with the guide,"I just need to deploy jeager operator for with 1 elasticsearch, but this guide is quite confusing."
Jaeger,55220796,nan,1,"2019/03/18, 13:55:40",False,"2019/04/08, 10:37:58","2019/03/18, 16:28:50",10856824.0,105.0,0,129,Jaeger Operator for Openshift trouble with the guide,Does anyone know a quick and easy guide on how to do it?
Jaeger,67191786,nan,0,"2021/04/21, 11:12:22",False,"2021/04/21, 11:16:37","2021/04/21, 11:16:37",15716851.0,1.0,0,7,I&#39;m using Jaeger tracing python functions. Do i have to create spans in every function manually?,I've found an example:  https://medium.com/velotio-perspectives/a-comprehensive-tutorial-to-implementing-opentracing-with-jaeger-a01752e1a8ce
Jaeger,67191786,nan,0,"2021/04/21, 11:12:22",False,"2021/04/21, 11:16:37","2021/04/21, 11:16:37",15716851.0,1.0,0,7,I&#39;m using Jaeger tracing python functions. Do i have to create spans in every function manually?,I have a pretty large codebase and I really don't want to modify every function by adding a line like ' with tracer.start_span('booking') as span:'.
Jaeger,67191786,nan,0,"2021/04/21, 11:12:22",False,"2021/04/21, 11:16:37","2021/04/21, 11:16:37",15716851.0,1.0,0,7,I&#39;m using Jaeger tracing python functions. Do i have to create spans in every function manually?,Is there any way to do it?
Jaeger,67191786,nan,0,"2021/04/21, 11:12:22",False,"2021/04/21, 11:16:37","2021/04/21, 11:16:37",15716851.0,1.0,0,7,I&#39;m using Jaeger tracing python functions. Do i have to create spans in every function manually?,Thanks in advance.
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,I use Jaeger with Elasticsearch and I want to remove old indices.
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,"I tried  jaeger-es-index-cleaner , see  Remove old data :"
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,Remove old data
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,The historical data can be removed with the  jaeger-es-index-cleaner  that is also used for daily indices.
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,&lt;1&gt; Remove indices older than 14 days.
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,Log
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,"I tried to delete all indices older than 2 days, but no indice was deleted:"
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,Indices
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,"If I list all indices with  http://localhost:9200/_cat/indices , I still see old indices:"
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,Question
Jaeger,67072664,nan,0,"2021/04/13, 12:58:47",False,"2021/04/13, 12:58:47",nan,5277820.0,12820.0,0,16,How to remove old indices of Jaeger in Elasticsearch?,How to delete old indices of Jaeger from Elasticsearch?
Jaeger,67003774,nan,1,"2021/04/08, 15:13:29",False,"2021/04/13, 17:44:16",nan,3774803.0,193.0,0,19,Balancing export to jaeger in openTelemetry collector,I have configuration as  documentation  says
Jaeger,67003774,nan,1,"2021/04/08, 15:13:29",False,"2021/04/13, 17:44:16",nan,3774803.0,193.0,0,19,Balancing export to jaeger in openTelemetry collector,Collector produces error.
Jaeger,67003774,nan,1,"2021/04/08, 15:13:29",False,"2021/04/13, 17:44:16",nan,3774803.0,193.0,0,19,Balancing export to jaeger in openTelemetry collector,How I can configure collector to balance exporter for sending requests in different backends?
Jaeger,67003774,nan,1,"2021/04/08, 15:13:29",False,"2021/04/13, 17:44:16",nan,3774803.0,193.0,0,19,Balancing export to jaeger in openTelemetry collector,info    exporterhelper/queued_retry.go:276      Exporting failed.
Jaeger,67003774,nan,1,"2021/04/08, 15:13:29",False,"2021/04/13, 17:44:16",nan,3774803.0,193.0,0,19,Balancing export to jaeger in openTelemetry collector,Will retry the request after interval.
Jaeger,67003774,nan,1,"2021/04/08, 15:13:29",False,"2021/04/13, 17:44:16",nan,3774803.0,193.0,0,19,Balancing export to jaeger in openTelemetry collector,"{&quot;component_kind&quot;: &quot;exporter&quot;, &quot;component_type&quot;: &quot;jaeger&quot;, &quot;component_name&quot;: &quot;jaeger&quot;, &quot;error&quot;: &quot;failed to push trace data via Jaeger exporter: rpc error: code = Unavailable desc = last connection error: connection error: desc = &quot;transport: Error while dialing dial tcp: address ipv4:firstHost:14250,secondHost:14250: too many colons in address&quot;&quot;, &quot;interval&quot;: &quot;30.456378855s&quot;}"
Jaeger,66976277,nan,0,"2021/04/06, 23:49:18",False,"2021/04/07, 03:37:55","2021/04/07, 03:37:55",7728877.0,39.0,0,24,Spring Cloud Zipkin with Jaeger logs,"I have been reading the documentation from Spring Cloud Sleuth and Zipkin, and I did not locate anything about how to show in Jaeger the logs came from Zipkin."
Jaeger,66976277,nan,0,"2021/04/06, 23:49:18",False,"2021/04/07, 03:37:55","2021/04/07, 03:37:55",7728877.0,39.0,0,24,Spring Cloud Zipkin with Jaeger logs,This is an example using the jaeger stack:
Jaeger,66976277,nan,0,"2021/04/06, 23:49:18",False,"2021/04/07, 03:37:55","2021/04/07, 03:37:55",7728877.0,39.0,0,24,Spring Cloud Zipkin with Jaeger logs,"And this is an example using the same stack, but exporting to Jaeger using the Zipkin collector (port 9411 on Jaeger)"
Jaeger,66976277,nan,0,"2021/04/06, 23:49:18",False,"2021/04/07, 03:37:55","2021/04/07, 03:37:55",7728877.0,39.0,0,24,Spring Cloud Zipkin with Jaeger logs,There is no  Logs  table.
Jaeger,66976277,nan,0,"2021/04/06, 23:49:18",False,"2021/04/07, 03:37:55","2021/04/07, 03:37:55",7728877.0,39.0,0,24,Spring Cloud Zipkin with Jaeger logs,"Does anyone knows if would be possible show those logs there, like Jaeger implementation does?"
Jaeger,66707644,nan,0,"2021/03/19, 13:50:45",False,"2021/03/19, 13:50:45",nan,12210331.0,43.0,0,23,Jaeger UI group axon events,I have a spring boot application with several microservices.
Jaeger,66707644,nan,0,"2021/03/19, 13:50:45",False,"2021/03/19, 13:50:45",nan,12210331.0,43.0,0,23,Jaeger UI group axon events,There are about 100+ different events.
Jaeger,66707644,nan,0,"2021/03/19, 13:50:45",False,"2021/03/19, 13:50:45",nan,12210331.0,43.0,0,23,Jaeger UI group axon events,And I wanted to see in convenient UI to see sequence of them.
Jaeger,66707644,nan,0,"2021/03/19, 13:50:45",False,"2021/03/19, 13:50:45",nan,12210331.0,43.0,0,23,Jaeger UI group axon events,"I googled about Jaeger UI, ran it via docker container and everything works almost fine, except one important thing, events are not grouped, I just see multiple independent events."
Jaeger,66707644,nan,0,"2021/03/19, 13:50:45",False,"2021/03/19, 13:50:45",nan,12210331.0,43.0,0,23,Jaeger UI group axon events,"I give 2 examples,
First: how I want it to see
Second: how I see."
Jaeger,66707644,nan,0,"2021/03/19, 13:50:45",False,"2021/03/19, 13:50:45",nan,12210331.0,43.0,0,23,Jaeger UI group axon events,Thanks for any suggestions.
Jaeger,66707644,nan,0,"2021/03/19, 13:50:45",False,"2021/03/19, 13:50:45",nan,12210331.0,43.0,0,23,Jaeger UI group axon events,"networks:
axonnet:
driver: bridge"
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,"I am new to Jaeger and Kafka,"
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,I am trying to use Kafka as intermediate buffer.
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,I am using OpenTelemetry to send data to Jaeger-Collector directly using  -Dotel.exporter.jaeger.endpoint .
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,Jaeger-Collector is deployed on Kubernetes and the Kafka is on another network but is accessible.
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,I can confirm that the traces are sent to Jaeger-collector.
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,On hitting the /metrics of collector and output tells me that spans were written successfully to Kafka.
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,jaeger_kafka_spans_written_total{status=&quot;success&quot;} 21
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,The Collector logs indicate what topic I am writing to
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,"{&quot;Brokers&quot;:[&quot;myKafkaBroker......&quot;}},&quot;topic&quot;:&quot;tp6&quot;}"
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,I want to get this (Span) data from Kafka Queue to ElasticSearch.
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,To do this I am starting the Jaeger Ingester as follows
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,docker run  -e &quot;SPAN_STORAGE_TYPE=elasticsearch&quot;  jaegertracing/jaeger-ingester:1.22 --kafka.consumer.topic=tp6 --kafka.consumer.brokers='myKafkaBroker'   --es.tls.skip-host-verify
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,But the container is stopped after fatal error
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,The elasticsearch and ingester are being run on the same machine using docker.
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,The elasticsearch is running on docker using
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,docker run -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot;ocker.elastic.co/elasticsearch/elasticsearch:7.11.2
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,I have disabled TLS so that shouldn't be a problem.
Jaeger,66598804,nan,0,"2021/03/12, 13:10:12",False,"2021/03/12, 13:10:12",nan,3081553.0,71.0,0,38,JaegerTracing : Jaeger Ingester unable to read from Kafka Queue and store into ElasticSearch,I am unable to get this to work.
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,　　I am trying to learn kubernetes recently.
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,"I have already deployed jaeger (all-in-one) by istio on kubernetes, and everying works well."
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,"Although I can see the trace information on the jaeger UI, I don't know how to extract these trace data by python."
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,I want to use these data to do root causes location of microservices.
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,I think there must be some API to access these data dicectly by python but I don't find it.
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,or can I access the cassandra using python to get these data?.
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,I am searching for a long time on net.
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,But no use.
Jaeger,66597254,nan,1,"2021/03/12, 11:24:24",True,"2021/03/14, 09:14:12","2021/03/14, 07:48:40",15381526.0,13.0,0,33,How to extract trace data from Jaeger,Please help or try to give some ideas how to achieve this.
Jaeger,66375171,nan,0,"2021/02/25, 21:25:32",False,"2021/02/25, 21:25:32",nan,5014844.0,51.0,0,55,Jaeger All in one docker gRPC issues,Getting this error when i startup jaeger allinone docker latest.
Jaeger,66375171,nan,0,"2021/02/25, 21:25:32",False,"2021/02/25, 21:25:32",nan,5014844.0,51.0,0,55,Jaeger All in one docker gRPC issues,Not sure why this is - can anyone help here?
Jaeger,66375171,nan,0,"2021/02/25, 21:25:32",False,"2021/02/25, 21:25:32",nan,5014844.0,51.0,0,55,Jaeger All in one docker gRPC issues,"I am running this on Windows, Docker for desktop."
Jaeger,66375171,nan,0,"2021/02/25, 21:25:32",False,"2021/02/25, 21:25:32",nan,5014844.0,51.0,0,55,Jaeger All in one docker gRPC issues,"This is behind a corp proxy, if that's helpful."
Jaeger,66375171,nan,0,"2021/02/25, 21:25:32",False,"2021/02/25, 21:25:32",nan,5014844.0,51.0,0,55,Jaeger All in one docker gRPC issues,This is the command i am using to startup
Jaeger,66348255,nan,0,"2021/02/24, 11:41:31",False,"2021/02/25, 17:34:59","2021/02/24, 19:30:19",4836988.0,25.0,0,23,What datatypes should I use to read Jaeger logs from Kafka into Clickhouse?,I'm new to Clickhouse.
Jaeger,66348255,nan,0,"2021/02/24, 11:41:31",False,"2021/02/25, 17:34:59","2021/02/24, 19:30:19",4836988.0,25.0,0,23,What datatypes should I use to read Jaeger logs from Kafka into Clickhouse?,I'm trying to read Jaeger logs from Kafka into Clickhouse db.
Jaeger,66348255,nan,0,"2021/02/24, 11:41:31",False,"2021/02/25, 17:34:59","2021/02/24, 19:30:19",4836988.0,25.0,0,23,What datatypes should I use to read Jaeger logs from Kafka into Clickhouse?,I have following Kafka messages format:
Jaeger,66348255,nan,0,"2021/02/24, 11:41:31",False,"2021/02/25, 17:34:59","2021/02/24, 19:30:19",4836988.0,25.0,0,23,What datatypes should I use to read Jaeger logs from Kafka into Clickhouse?,"I was able to input traceID, spanID and Operation into Clickhouse using the following table:"
Jaeger,66348255,nan,0,"2021/02/24, 11:41:31",False,"2021/02/25, 17:34:59","2021/02/24, 19:30:19",4836988.0,25.0,0,23,What datatypes should I use to read Jaeger logs from Kafka into Clickhouse?,But I failed to input tags.
Jaeger,66348255,nan,0,"2021/02/24, 11:41:31",False,"2021/02/25, 17:34:59","2021/02/24, 19:30:19",4836988.0,25.0,0,23,What datatypes should I use to read Jaeger logs from Kafka into Clickhouse?,Any idea which Clickhouse data type I should use for it?
Jaeger,66334630,nan,0,"2021/02/23, 16:02:56",False,"2021/02/23, 17:34:41",nan,5431429.0,362.0,0,21,"How can I disable INFO jaeger logging entries, in a tornado integration?",I have a tornado application in which I use jaeger for tracing.
Jaeger,66334630,nan,0,"2021/02/23, 16:02:56",False,"2021/02/23, 17:34:41",nan,5431429.0,362.0,0,21,"How can I disable INFO jaeger logging entries, in a tornado integration?","My problem is that jaeger keeps logging at the INFO level, tons of entries like this :"
Jaeger,66334630,nan,0,"2021/02/23, 16:02:56",False,"2021/02/23, 17:34:41",nan,5431429.0,362.0,0,21,"How can I disable INFO jaeger logging entries, in a tornado integration?","I've tried a bunch of configuration to try to remove these entries, but so far without luck."
Jaeger,66334630,nan,0,"2021/02/23, 16:02:56",False,"2021/02/23, 17:34:41",nan,5431429.0,362.0,0,21,"How can I disable INFO jaeger logging entries, in a tornado integration?",My current jaeger logger configurations is:
Jaeger,66334630,nan,0,"2021/02/23, 16:02:56",False,"2021/02/23, 17:34:41",nan,5431429.0,362.0,0,21,"How can I disable INFO jaeger logging entries, in a tornado integration?","How can I turn off, INFO logging for jaeger ?"
Jaeger,65997540,nan,0,"2021/02/01, 19:40:26",False,"2021/02/01, 19:40:26",nan,447344.0,18271.0,0,18,Alerting from Jaeger data,Using jaeger to instrutment our HTTP API (nestJS application).
Jaeger,65997540,nan,0,"2021/02/01, 19:40:26",False,"2021/02/01, 19:40:26",nan,447344.0,18271.0,0,18,Alerting from Jaeger data,I would like to put an alert if span duration exceeds a threshold.
Jaeger,65997540,nan,0,"2021/02/01, 19:40:26",False,"2021/02/01, 19:40:26",nan,447344.0,18271.0,0,18,Alerting from Jaeger data,"We are using elasticsearch as backend, so we could setup elasticsearch watcher, but I am wondering if jaeger eco-system bring a better solution?"
Jaeger,65881076,nan,0,"2021/01/25, 10:14:59",False,"2021/01/25, 10:14:59",nan,10471650.0,168.0,0,36,"OpenTracing/Jaeger UI - Traces real time rendering, is that possible?",I was trying to use Jaeger to trace some DAG execution (with some long tasks execution time) and I have some pretty good results in term of visualization &quot;post-execution&quot;.
Jaeger,65881076,nan,0,"2021/01/25, 10:14:59",False,"2021/01/25, 10:14:59",nan,10471650.0,168.0,0,36,"OpenTracing/Jaeger UI - Traces real time rendering, is that possible?",I know this is not a common usage of Jaeger (by extension OpenTracing) but this is doing  almost  what I was looking for.
Jaeger,65881076,nan,0,"2021/01/25, 10:14:59",False,"2021/01/25, 10:14:59",nan,10471650.0,168.0,0,36,"OpenTracing/Jaeger UI - Traces real time rendering, is that possible?",I am saying  almost  because I thought that a span would be displayed in the UI timeline as soon as it was &quot;started&quot; in the code.
Jaeger,65881076,nan,0,"2021/01/25, 10:14:59",False,"2021/01/25, 10:14:59",nan,10471650.0,168.0,0,36,"OpenTracing/Jaeger UI - Traces real time rendering, is that possible?","But, as far as I understand, to be displayed in a trace, spans need to be complete and Jaeger is not yet able to store incomplete spans (I have seen some open PRs in GitHub)."
Jaeger,65881076,nan,0,"2021/01/25, 10:14:59",False,"2021/01/25, 10:14:59",nan,10471650.0,168.0,0,36,"OpenTracing/Jaeger UI - Traces real time rendering, is that possible?","My need is that I would like to do some real-time monitoring to know where are my bottlenecks on the DAG that I need to execute, and viewing those tasks execution in a single timeline as soon as they are started would have been awesome."
Jaeger,65881076,nan,0,"2021/01/25, 10:14:59",False,"2021/01/25, 10:14:59",nan,10471650.0,168.0,0,36,"OpenTracing/Jaeger UI - Traces real time rendering, is that possible?",Do you know if such a behavior is possible with Jaeger or Zipkin to do some real-time traces rendering ?
Jaeger,65881076,nan,0,"2021/01/25, 10:14:59",False,"2021/01/25, 10:14:59",nan,10471650.0,168.0,0,36,"OpenTracing/Jaeger UI - Traces real time rendering, is that possible?",Or if there is an open-source tool capable of doing that ?
Jaeger,65881076,nan,0,"2021/01/25, 10:14:59",False,"2021/01/25, 10:14:59",nan,10471650.0,168.0,0,36,"OpenTracing/Jaeger UI - Traces real time rendering, is that possible?",Cheers !
Jaeger,65824708,nan,0,"2021/01/21, 11:40:35",False,"2021/01/21, 15:56:30","2021/01/21, 15:56:30",1709397.0,457.0,0,47,How to control ingress deployed by Jaeger operator,I have a question regarding the Jaeger Operator which I hope someone could help me with.
Jaeger,65824708,nan,0,"2021/01/21, 11:40:35",False,"2021/01/21, 15:56:30","2021/01/21, 15:56:30",1709397.0,457.0,0,47,How to control ingress deployed by Jaeger operator,I am deploying the Jaeger operator on k3s by simply adding the below template to my helm chart (templates/jaeger.yaml):
Jaeger,65824708,nan,0,"2021/01/21, 11:40:35",False,"2021/01/21, 15:56:30","2021/01/21, 15:56:30",1709397.0,457.0,0,47,How to control ingress deployed by Jaeger operator,I have imported the Jaeger-Operator dependency in my Chart.yaml as below:
Jaeger,65824708,nan,0,"2021/01/21, 11:40:35",False,"2021/01/21, 15:56:30","2021/01/21, 15:56:30",1709397.0,457.0,0,47,How to control ingress deployed by Jaeger operator,The operator deploys it's own ingress controller.
Jaeger,65824708,nan,0,"2021/01/21, 11:40:35",False,"2021/01/21, 15:56:30","2021/01/21, 15:56:30",1709397.0,457.0,0,47,How to control ingress deployed by Jaeger operator,What changes do I need to make to my templates so that I can disable the deployment of ingress from Jaeger Operator and have it done through the ingress.yaml that I will define ?
Jaeger,65686960,nan,0,"2021/01/12, 17:28:38",False,"2021/01/12, 17:28:38",nan,14991866.0,1.0,0,72,Is it possible to use both Jaeger uber-trace -id and Zipkin B3 format for propagating Context using jaeger go client library?,B3 headers can be propagated using  zipkin.NewZipkinB3HTTPHeaderPropagator()
Jaeger,65686960,nan,0,"2021/01/12, 17:28:38",False,"2021/01/12, 17:28:38",nan,14991866.0,1.0,0,72,Is it possible to use both Jaeger uber-trace -id and Zipkin B3 format for propagating Context using jaeger go client library?,as explained here
Jaeger,65686960,nan,0,"2021/01/12, 17:28:38",False,"2021/01/12, 17:28:38",nan,14991866.0,1.0,0,72,Is it possible to use both Jaeger uber-trace -id and Zipkin B3 format for propagating Context using jaeger go client library?,Can uber-trace-id also be propagated along with this ?
Jaeger,65686960,nan,0,"2021/01/12, 17:28:38",False,"2021/01/12, 17:28:38",nan,14991866.0,1.0,0,72,Is it possible to use both Jaeger uber-trace -id and Zipkin B3 format for propagating Context using jaeger go client library?,uber-trace-id is the default format in jaeger but I need both uber-trace-id as well as Zipkin B3 headers
Jaeger,65686960,nan,0,"2021/01/12, 17:28:38",False,"2021/01/12, 17:28:38",nan,14991866.0,1.0,0,72,Is it possible to use both Jaeger uber-trace -id and Zipkin B3 format for propagating Context using jaeger go client library?,Specifically can we add more injectors and extractors like this
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,I'm trying to use Jaeger to manage tracing system.
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,Docker is running locally &quot;all-in-one&quot; image with application (on the same host) without any issues.
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,My question is how to configure jaeger agent on host1 that would send traces jaeger collector on another host2.
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,Host2 is configured with &quot;all-in-one&quot;.
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,I can see Jaeger UI on host2 but it doesn't seem getting any traces from host1.
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,Configure tracer:
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,Added environment variables in yaml file on host1:
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,Added jaeger image in yaml file on host2:
Jaeger,65606234,nan,0,"2021/01/07, 05:25:45",False,"2021/01/07, 05:25:45",nan,8184914.0,1.0,0,79,How to configure Jaeger agent to send traces to collector on another server,Any suggestions will be appreciated.
Jaeger,65595635,65910568.0,2,"2021/01/06, 14:18:34",True,"2021/01/27, 01:06:48",nan,9126136.0,160.0,0,502,Opentelemetry Java auto-instrumentation data to Jaeger,I'm new to the world of Opentelemetry and would like to send the  Spring-petclinic  instrumentation data to Jaeger which is running on my remote cloud system
Jaeger,65595635,65910568.0,2,"2021/01/06, 14:18:34",True,"2021/01/27, 01:06:48",nan,9126136.0,160.0,0,502,Opentelemetry Java auto-instrumentation data to Jaeger,"Here is the bat file:
 java -javaagent:opentelemetry-javaagent-all.jar -Dotel.exporter=jaeger -Dotel.exporter.jaeger.endpoint=50.18.XXX.XX:14250 -Dotel.otlp.span.timeout=4000 -Dotel.jaeger.service.name=otel-ui -jar target/spring-petclinic-2.4.0.BUILD-SNAPSHOT.jar"
Jaeger,65595635,65910568.0,2,"2021/01/06, 14:18:34",True,"2021/01/27, 01:06:48",nan,9126136.0,160.0,0,502,Opentelemetry Java auto-instrumentation data to Jaeger,"When I run the bat file, I'm abe to open the petclinic app in browser (http://localhost:8080), I get the following error in the console:
 [opentelemetry.auto.trace 2021-01-06 17:22:21:008 +0530] [grpc-default-executor-1] WARN io.opentelemetry.exporter.otlp.OtlpGrpcSpanExporter - Failed to export spans."
Jaeger,65595635,65910568.0,2,"2021/01/06, 14:18:34",True,"2021/01/27, 01:06:48",nan,9126136.0,160.0,0,502,Opentelemetry Java auto-instrumentation data to Jaeger,Error message: UNAVAILABLE: io exception
Jaeger,65595635,65910568.0,2,"2021/01/06, 14:18:34",True,"2021/01/27, 01:06:48",nan,9126136.0,160.0,0,502,Opentelemetry Java auto-instrumentation data to Jaeger,How to resolve this issue?
Jaeger,65595635,65910568.0,2,"2021/01/06, 14:18:34",True,"2021/01/27, 01:06:48",nan,9126136.0,160.0,0,502,Opentelemetry Java auto-instrumentation data to Jaeger,Are there any other dependencies to be the added to the petclinic pom.xml or to the code?
Jaeger,65492225,nan,0,"2020/12/29, 14:35:31",False,"2020/12/29, 14:35:31",nan,13753895.0,1.0,0,10,Can we load Jaeger embeded ui with https,Is there any way to load jaeger embeded ui with https instead of http.
Jaeger,65492225,nan,0,"2020/12/29, 14:35:31",False,"2020/12/29, 14:35:31",nan,13753895.0,1.0,0,10,Can we load Jaeger embeded ui with https,"from this : http://jaegerip/search?&amp;uiEmbed=v0
to this : https://jaegerip/search?&amp;uiEmbed=v0"
Jaeger,65475357,nan,0,"2020/12/28, 11:20:29",False,"2020/12/28, 11:20:29",nan,804401.0,1906.0,0,20,Jaeger configuration for class library,I am trying to incorporate Jaeger and OpenTracing into my Class Library project in .Net core.
Jaeger,65475357,nan,0,"2020/12/28, 11:20:29",False,"2020/12/28, 11:20:29",nan,804401.0,1906.0,0,20,Jaeger configuration for class library,Most of the Jaeger documentation shows how to configure Jaeger for Web API's in Startup.cs file.
Jaeger,65475357,nan,0,"2020/12/28, 11:20:29",False,"2020/12/28, 11:20:29",nan,804401.0,1906.0,0,20,Jaeger configuration for class library,The below configuration works for .Net core API project
Jaeger,65475357,nan,0,"2020/12/28, 11:20:29",False,"2020/12/28, 11:20:29",nan,804401.0,1906.0,0,20,Jaeger configuration for class library,but I would like to configure the same for my Class LIbrary project.
Jaeger,65475357,nan,0,"2020/12/28, 11:20:29",False,"2020/12/28, 11:20:29",nan,804401.0,1906.0,0,20,Jaeger configuration for class library,Is it possible ?
Jaeger,65475357,nan,0,"2020/12/28, 11:20:29",False,"2020/12/28, 11:20:29",nan,804401.0,1906.0,0,20,Jaeger configuration for class library,I mean for example someone else uses my class library in their App I should be able to see the tracing.
Jaeger,65475357,nan,0,"2020/12/28, 11:20:29",False,"2020/12/28, 11:20:29",nan,804401.0,1906.0,0,20,Jaeger configuration for class library,The Jaeger configuration should be made in Class LIbarary.
Jaeger,65436493,nan,0,"2020/12/24, 11:36:55",False,"2020/12/24, 11:36:55",nan,747456.0,5301.0,0,114,ClassNotFoundException: io.opentelemetry.common.AttributeKey while using Open telemetry jaeger Exporter,I am getting the following exception while creating a span.
Jaeger,65436493,nan,0,"2020/12/24, 11:36:55",False,"2020/12/24, 11:36:55",nan,747456.0,5301.0,0,114,ClassNotFoundException: io.opentelemetry.common.AttributeKey while using Open telemetry jaeger Exporter,It works fine for Logging Exporter but for jaeger gives the following
Jaeger,65436493,nan,0,"2020/12/24, 11:36:55",False,"2020/12/24, 11:36:55",nan,747456.0,5301.0,0,114,ClassNotFoundException: io.opentelemetry.common.AttributeKey while using Open telemetry jaeger Exporter,At App startup I do
Jaeger,65436493,nan,0,"2020/12/24, 11:36:55",False,"2020/12/24, 11:36:55",nan,747456.0,5301.0,0,114,ClassNotFoundException: io.opentelemetry.common.AttributeKey while using Open telemetry jaeger Exporter,And at runtime
Jaeger,65329406,nan,0,"2020/12/16, 20:52:35",False,"2020/12/16, 20:52:35",nan,14536374.0,1.0,0,29,Setup Jaeger agent in Camel Quarkus,I'm running my Camel Quarkus service on Openshift where Jaeger is also installed.
Jaeger,65329406,nan,0,"2020/12/16, 20:52:35",False,"2020/12/16, 20:52:35",nan,14536374.0,1.0,0,29,Setup Jaeger agent in Camel Quarkus,The Jaeger agent is running as a daemon set.
Jaeger,65329406,nan,0,"2020/12/16, 20:52:35",False,"2020/12/16, 20:52:35",nan,14536374.0,1.0,0,29,Setup Jaeger agent in Camel Quarkus,I'm not getting any traces from my Camel service in the Jaeger UI using following properties:
Jaeger,65329406,nan,0,"2020/12/16, 20:52:35",False,"2020/12/16, 20:52:35",nan,14536374.0,1.0,0,29,Setup Jaeger agent in Camel Quarkus,I also have some Spring Boot services running in the same namespace and they work as they should.
Jaeger,65329406,nan,0,"2020/12/16, 20:52:35",False,"2020/12/16, 20:52:35",nan,14536374.0,1.0,0,29,Setup Jaeger agent in Camel Quarkus,"Therefore, I think the Camel service is configured incorrectly."
Jaeger,65329406,nan,0,"2020/12/16, 20:52:35",False,"2020/12/16, 20:52:35",nan,14536374.0,1.0,0,29,Setup Jaeger agent in Camel Quarkus,Please help.
Jaeger,65329406,nan,0,"2020/12/16, 20:52:35",False,"2020/12/16, 20:52:35",nan,14536374.0,1.0,0,29,Setup Jaeger agent in Camel Quarkus,What am I missing?
Jaeger,65202244,nan,1,"2020/12/08, 17:51:19",False,"2021/01/22, 23:41:07","2020/12/11, 08:48:18",8394088.0,51.0,0,34,Jaeger traces not capturing for spring-data,Jaeger traces to spring-boot application are not able to capture traces for the DB calls made using spring-data.
Jaeger,65202244,nan,1,"2020/12/08, 17:51:19",False,"2021/01/22, 23:41:07","2020/12/11, 08:48:18",8394088.0,51.0,0,34,Jaeger traces not capturing for spring-data,All other calls like RESTTemplate are able to have the traces captured.
Jaeger,65202244,nan,1,"2020/12/08, 17:51:19",False,"2021/01/22, 23:41:07","2020/12/11, 08:48:18",8394088.0,51.0,0,34,Jaeger traces not capturing for spring-data,"Using springboot version 2.2.2.RELEASE and added below jaeger dependencies,"
Jaeger,65202244,nan,1,"2020/12/08, 17:51:19",False,"2021/01/22, 23:41:07","2020/12/11, 08:48:18",8394088.0,51.0,0,34,Jaeger traces not capturing for spring-data,Any additional dependencies are missing here?
Jaeger,65040519,65041185.0,1,"2020/11/27, 18:17:00",True,"2020/11/27, 19:06:58",nan,1221718.0,328.0,0,68,Do metrics published by the Jaeger Operator include those from the Collector and Query services too?,I am deploying Jaeger using the  Jaeger Operator  and it seems to be working fine.
Jaeger,65040519,65041185.0,1,"2020/11/27, 18:17:00",True,"2020/11/27, 19:06:58",nan,1221718.0,328.0,0,68,Do metrics published by the Jaeger Operator include those from the Collector and Query services too?,"However, now I am trying to set up Prometheus metrics scraping (using the  Prometheus Operator ) but I am not seeing a  Service  in my cluster that exposes the metrics ports for the Jaeger Collector (port 14269) or Query services (port 16687) ( port number reference from the Jeager Monitoring documentation )."
Jaeger,65040519,65041185.0,1,"2020/11/27, 18:17:00",True,"2020/11/27, 19:06:58",nan,1221718.0,328.0,0,68,Do metrics published by the Jaeger Operator include those from the Collector and Query services too?,The only relevant  Service  I see is  jaeger-operator-metrics :
Jaeger,65040519,65041185.0,1,"2020/11/27, 18:17:00",True,"2020/11/27, 19:06:58",nan,1221718.0,328.0,0,68,Do metrics published by the Jaeger Operator include those from the Collector and Query services too?,I am able to set up a Prometheus  ServiceMonitor  to scrape metrics from this service but I am not sure if this includes the metrics that are normally gathered by the Collector and Query microservices or not...
Jaeger,65040519,65041185.0,1,"2020/11/27, 18:17:00",True,"2020/11/27, 19:06:58",nan,1221718.0,328.0,0,68,Do metrics published by the Jaeger Operator include those from the Collector and Query services too?,I am guessing not as that would seem to violate the premise of microservices.
Jaeger,65040519,65041185.0,1,"2020/11/27, 18:17:00",True,"2020/11/27, 19:06:58",nan,1221718.0,328.0,0,68,Do metrics published by the Jaeger Operator include those from the Collector and Query services too?,Is there some setting in the Jaeger Operator spec that I missed for exposing those metrics endpoints in the other components?
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,I have the problem that I cannot seem to get Grafana Tempo working with a Jaeger client.
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,Following their official docker-compose example everything should be straight forward:  https://github.com/grafana/tempo/tree/master/example/docker-compose
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,I've basically just adapted the official Jaeger python client example.
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,Since Grafana Tempo is running on the same machine the reporting_host is set to localhost and since the synthetic-load-generator uses port 14268 in the JAEGER_COLLECTOR_URL I'm using this as well.
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,I can query for traces generated by the synthetic-load-generator without a problem but I cannot seem to get it working with my script.
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,If I use e.g.
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,the all-in-one Jaeger container I can query for my traces.
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,As suggested here  https://grafana.com/docs/tempo/latest/getting-started/  I've also tried to use the &quot;Jaeger - Thrift Compact&quot; Protocol on port 6831 but it doesn't seem to work either.
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,Can somebody point me into the right direction what I might be doing wrong?
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,Thanks in advance!
Jaeger,64898096,nan,0,"2020/11/18, 19:21:35",False,"2020/11/18, 19:21:35",nan,4839488.0,139.0,0,298,Grafana Tempo + Python Jaeger/OpenTelemetry Client,!
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,"Appreciate your support for the below issue as I built my demo as the below steps,"
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,"I built two microservices one by Django and the another by Go,
Django send HTTP request to Go service,"
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,"Jaeger tool is configured for UI tracing,"
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,"Django tracing and Go tracing are separated in Jaeger tool and I do not know the reason although I received Django parent trace id in the request header and it is normal to be all Django request tracing including Go tracing as one request tracing in Jaeger,"
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,My Git repo:  https://github.com/OmarEltamasehy/django-gotracing-example
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,The below is Django code for calling Golang service
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,Golang service code
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,Jaeger UI output
Jaeger,64833551,nan,0,"2020/11/14, 13:47:42",False,"2021/02/13, 01:57:39","2020/11/14, 13:56:34",14637417.0,11.0,0,133,The full request tracing is not complete for micro services via Jaeger UI,this image refer to my inquiry why my request show Django tracing only without Go tracing for the same request
Jaeger,64791751,nan,0,"2020/11/11, 20:02:41",False,"2020/11/11, 20:02:41",nan,5377141.0,46.0,0,28,Not able to project grpc span on jaeger UI,"As the doc  https://www.jaegertracing.io/docs/1.19/troubleshooting/  says &quot;The logging reporter follows the sampling decision made by the sampler, meaning that if the span is logged, it should also reach the agent or collector.&quot;"
Jaeger,64791751,nan,0,"2020/11/11, 20:02:41",False,"2020/11/11, 20:02:41",nan,5377141.0,46.0,0,28,Not able to project grpc span on jaeger UI,"But thats not happening, my gRPC server is logging the span -  &quot;{&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;Reporting span 1559209397c51d88:3217766e13b74f76:1559209397c51d88:1&quot;,&quot;time&quot;:&quot;2020-11-11T17:31:47Z&quot;}&quot;  , but I am not able to see the same span on jaeger-all-in-one UI."
Jaeger,64791751,nan,0,"2020/11/11, 20:02:41",False,"2020/11/11, 20:02:41",nan,5377141.0,46.0,0,28,Not able to project grpc span on jaeger UI,The REST client span is projected onto the UI and is the parent of the trace.
Jaeger,64791751,nan,0,"2020/11/11, 20:02:41",False,"2020/11/11, 20:02:41",nan,5377141.0,46.0,0,28,Not able to project grpc span on jaeger UI,Please help me to be able to project the grpc spans onto UI.
Jaeger,64486524,nan,1,"2020/10/22, 19:28:28",False,"2020/10/29, 15:30:59","2020/10/22, 19:39:44",8394088.0,51.0,0,105,Kafka Streams with Jaeger tracing: headers() should only be called while a record is processed,"Referring to the link ( https://github.com/burkaa01/jaeger-tracing-kafka/tree/master/stream-app ), created stream pipeline with Jaeger enabled."
Jaeger,64486524,nan,1,"2020/10/22, 19:28:28",False,"2020/10/29, 15:30:59","2020/10/22, 19:39:44",8394088.0,51.0,0,105,Kafka Streams with Jaeger tracing: headers() should only be called while a record is processed,It is a springboot application but bean configurations are defined in spring xml file.
Jaeger,64486524,nan,1,"2020/10/22, 19:28:28",False,"2020/10/29, 15:30:59","2020/10/22, 19:39:44",8394088.0,51.0,0,105,Kafka Streams with Jaeger tracing: headers() should only be called while a record is processed,"As part of stream topology, in transformation, while getting processorContext.headers(), i am getting error."
Jaeger,64486524,nan,1,"2020/10/22, 19:28:28",False,"2020/10/29, 15:30:59","2020/10/22, 19:39:44",8394088.0,51.0,0,105,Kafka Streams with Jaeger tracing: headers() should only be called while a record is processed,Stream pipeline works if jaeger is disabled.
Jaeger,64486524,nan,1,"2020/10/22, 19:28:28",False,"2020/10/29, 15:30:59","2020/10/22, 19:39:44",8394088.0,51.0,0,105,Kafka Streams with Jaeger tracing: headers() should only be called while a record is processed,Also it works if the beans are defined in annotations .
Jaeger,64486524,nan,1,"2020/10/22, 19:28:28",False,"2020/10/29, 15:30:59","2020/10/22, 19:39:44",8394088.0,51.0,0,105,Kafka Streams with Jaeger tracing: headers() should only be called while a record is processed,"Referred JIRA,  https://issues.apache.org/jira/browse/KAFKA-4344 
 
Clueless on the issue, here is the error stack for reference"
Jaeger,64465875,64481993.0,1,"2020/10/21, 17:33:15",True,"2020/10/22, 16:39:57","2020/10/22, 15:06:41",261708.0,2287.0,0,109,"Azure Kubernetes - Istio accessing grafana, prometheus, jaeger, kiali &amp; envoy externally?",I have used the following configuration to setup the Istio
Jaeger,64465875,64481993.0,1,"2020/10/21, 17:33:15",True,"2020/10/22, 16:39:57","2020/10/22, 15:06:41",261708.0,2287.0,0,109,"Azure Kubernetes - Istio accessing grafana, prometheus, jaeger, kiali &amp; envoy externally?","I want to access the services like grafana, prometheus, jaeger, kiali &amp; envoy externally - eg:  https://grafana.mycompany.com , how can I do it?"
Jaeger,64465875,64481993.0,1,"2020/10/21, 17:33:15",True,"2020/10/22, 16:39:57","2020/10/22, 15:06:41",261708.0,2287.0,0,109,"Azure Kubernetes - Istio accessing grafana, prometheus, jaeger, kiali &amp; envoy externally?","Update: 
I have tried below however it doesn't work"
Jaeger,64342820,64356244.0,1,"2020/10/13, 23:38:13",True,"2020/10/14, 18:06:36",nan,785745.0,39034.0,0,72,How can I send traces to Jaeger from C#?,I'm trying to use the  Jaeger  package to send traces to Jaeger from a C# app.
Jaeger,64342820,64356244.0,1,"2020/10/13, 23:38:13",True,"2020/10/14, 18:06:36",nan,785745.0,39034.0,0,72,How can I send traces to Jaeger from C#?,"There are no minimal examples in the jaeger-client-csharp documentation, but from what I read, I think this should work."
Jaeger,64342820,64356244.0,1,"2020/10/13, 23:38:13",True,"2020/10/14, 18:06:36",nan,785745.0,39034.0,0,72,How can I send traces to Jaeger from C#?,I have jaeger-all-in-one.exe running but when I run this code there's no sign of any new traces.
Jaeger,64342820,64356244.0,1,"2020/10/13, 23:38:13",True,"2020/10/14, 18:06:36",nan,785745.0,39034.0,0,72,How can I send traces to Jaeger from C#?,"I've tried manually configuring samplers, senders, reporters, etc."
Jaeger,64342820,64356244.0,1,"2020/10/13, 23:38:13",True,"2020/10/14, 18:06:36",nan,785745.0,39034.0,0,72,How can I send traces to Jaeger from C#?,but nothing I tried worked.
Jaeger,64342820,64356244.0,1,"2020/10/13, 23:38:13",True,"2020/10/14, 18:06:36",nan,785745.0,39034.0,0,72,How can I send traces to Jaeger from C#?,What do I need to add to get my traces to appear in Jaeger?
Jaeger,64266056,nan,1,"2020/10/08, 18:42:49",False,"2020/10/26, 13:43:36",nan,1700378.0,1157.0,0,20,GitLab integration with Jaeger in Azure kubernetes service,"I have installed Gitlab in AKS cluster, and I also installed Jaeger."
Jaeger,64266056,nan,1,"2020/10/08, 18:42:49",False,"2020/10/26, 13:43:36",nan,1700378.0,1157.0,0,20,GitLab integration with Jaeger in Azure kubernetes service,Both the applications are up and running.
Jaeger,64266056,nan,1,"2020/10/08, 18:42:49",False,"2020/10/26, 13:43:36",nan,1700378.0,1157.0,0,20,GitLab integration with Jaeger in Azure kubernetes service,I want to integrate GitLab with jaeger.
Jaeger,64266056,nan,1,"2020/10/08, 18:42:49",False,"2020/10/26, 13:43:36",nan,1700378.0,1157.0,0,20,GitLab integration with Jaeger in Azure kubernetes service,"I searched for any documentation on how to do in AKS , but didnt find any."
Jaeger,64266056,nan,1,"2020/10/08, 18:42:49",False,"2020/10/26, 13:43:36",nan,1700378.0,1157.0,0,20,GitLab integration with Jaeger in Azure kubernetes service,Any suggestion is welcome.
Jaeger,64201190,nan,1,"2020/10/05, 03:37:54",True,"2020/10/05, 16:34:52",nan,10690874.0,607.0,0,197,Istio zipkin address configuration problem when using Jaeger Agent as Daemonset,I have question regarding  global.tracer.zipkin.address  while deploying istio.
Jaeger,64201190,nan,1,"2020/10/05, 03:37:54",True,"2020/10/05, 16:34:52",nan,10690874.0,607.0,0,197,Istio zipkin address configuration problem when using Jaeger Agent as Daemonset,"I am using Jaeger, and have Jaeger Agents deployed in DaemonSet."
Jaeger,64201190,nan,1,"2020/10/05, 03:37:54",True,"2020/10/05, 16:34:52",nan,10690874.0,607.0,0,197,Istio zipkin address configuration problem when using Jaeger Agent as Daemonset,"As I have each Jaeger Agents (on each nodes), Jaeger Collector, and Jaeger Query, I believe global.tracer.zipkin.address should be configured as Jaeger Agent host."
Jaeger,64201190,nan,1,"2020/10/05, 03:37:54",True,"2020/10/05, 16:34:52",nan,10690874.0,607.0,0,197,Istio zipkin address configuration problem when using Jaeger Agent as Daemonset,"However, Agents are on each nodes, and I have hard time specifying the host."
Jaeger,64201190,nan,1,"2020/10/05, 03:37:54",True,"2020/10/05, 16:34:52",nan,10690874.0,607.0,0,197,Istio zipkin address configuration problem when using Jaeger Agent as Daemonset,How should I specify it?
Jaeger,64201190,nan,1,"2020/10/05, 03:37:54",True,"2020/10/05, 16:34:52",nan,10690874.0,607.0,0,197,Istio zipkin address configuration problem when using Jaeger Agent as Daemonset,Thanks in advance.
Jaeger,64201190,nan,1,"2020/10/05, 03:37:54",True,"2020/10/05, 16:34:52",nan,10690874.0,607.0,0,197,Istio zipkin address configuration problem when using Jaeger Agent as Daemonset,"FYI) If I understood correctly, Jaeger Client will send the data to the Jaeger Agent via Envoy, and then to Jaeger Collector."
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,I am working with  Vert.X 3.9  and  Java 8  and I'm trying to implement  Opentrace  with  Jaeger .
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,I have an issue on sending method for the  Spans .
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,"There is a  jaeger-collector  already working for another services (Not using Vert.X), but for some reason it is not receiving traces from the Vert.X app."
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,So far I noticed that the Sender is setted as  UdpSender  instead of  httpSender  althought I already set the env variable  JAEGER_ENDPOINT .
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,This is the Java code:
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,This code creates some logs that show the Tracer and Spam creation:
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,"Regarding docs the sender should be httpSender, but here is udp"
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,"sender=UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@55c10031, receiveBuf=null, receiveOffSet=-1, receiveLength=0)), closeEnqueueTimeout=1000)"
Jaeger,64164909,nan,0,"2020/10/02, 03:45:05",False,"2020/10/02, 03:45:05",nan,4130521.0,71.0,0,121,Use httpSender for Jaeger in Vert.x,These are the dependencies I am using:
Jaeger,64070408,nan,0,"2020/09/25, 22:39:45",False,"2020/12/26, 21:25:16","2020/12/26, 21:25:16",5988562.0,195.0,0,105,Having trouble under Docker Compose getting OpenTelemetry trace information from my app into jaeger,I have a simple Node.js app the uses the Open Telemetry Jaeger exporter to send trace information into Jaeger.
Jaeger,64070408,nan,0,"2020/09/25, 22:39:45",False,"2020/12/26, 21:25:16","2020/12/26, 21:25:16",5988562.0,195.0,0,105,Having trouble under Docker Compose getting OpenTelemetry trace information from my app into jaeger,It runs fine when I fire up the Jaeger as a Docker container then run the code from my machine's command line against  localhost .
Jaeger,64070408,nan,0,"2020/09/25, 22:39:45",False,"2020/12/26, 21:25:16","2020/12/26, 21:25:16",5988562.0,195.0,0,105,Having trouble under Docker Compose getting OpenTelemetry trace information from my app into jaeger,"However, when I try to run both the app and jaeger under Docker Compose within a Docker Compose network, the service registers and is apparent in the Jaeger UI, but the trace/span information never gets received into Jaeger."
Jaeger,64070408,nan,0,"2020/09/25, 22:39:45",False,"2020/12/26, 21:25:16","2020/12/26, 21:25:16",5988562.0,195.0,0,105,Having trouble under Docker Compose getting OpenTelemetry trace information from my app into jaeger,Here is the code:  https://github.com/reselbob/simpletracing/tree/releases/v1.0
Jaeger,64070408,nan,0,"2020/09/25, 22:39:45",False,"2020/12/26, 21:25:16","2020/12/26, 21:25:16",5988562.0,195.0,0,105,Having trouble under Docker Compose getting OpenTelemetry trace information from my app into jaeger,I attached a screenshot to demonstrate that the service seems to be registering but the spans are not getting through.
Jaeger,64070408,nan,0,"2020/09/25, 22:39:45",False,"2020/12/26, 21:25:16","2020/12/26, 21:25:16",5988562.0,195.0,0,105,Having trouble under Docker Compose getting OpenTelemetry trace information from my app into jaeger,.
Jaeger,63852120,nan,0,"2020/09/11, 21:00:21",False,"2020/09/11, 21:00:21",nan,1729409.0,147.0,0,30,How to trace auto-generated code using jaeger,"I am manually instrumenting code using jaeger, and have a question on how to instrument code that is generated automatically for me?"
Jaeger,63852120,nan,0,"2020/09/11, 21:00:21",False,"2020/09/11, 21:00:21",nan,1729409.0,147.0,0,30,How to trace auto-generated code using jaeger,An example is when I try to instrument code that uses Spring's CrudRepository and MongoRepository.
Jaeger,63852120,nan,0,"2020/09/11, 21:00:21",False,"2020/09/11, 21:00:21",nan,1729409.0,147.0,0,30,How to trace auto-generated code using jaeger,Anyone have any ideas?
Jaeger,63852120,nan,0,"2020/09/11, 21:00:21",False,"2020/09/11, 21:00:21",nan,1729409.0,147.0,0,30,How to trace auto-generated code using jaeger,"When I use opentelemetry's auto instrumentation javaagent jar located here,  https://github.com/open-telemetry/opentelemetry-java-instrumentation  it is able to trace the MongoRepository method that is a generated."
Jaeger,63835165,63837122.0,1,"2020/09/10, 20:52:12",True,"2020/09/10, 23:16:48",nan,1729409.0,147.0,0,174,Jaeger log warning messages saying no sender factories available,I am trying to set up Jaeger to collect traces from a spring boot application.
Jaeger,63835165,63837122.0,1,"2020/09/10, 20:52:12",True,"2020/09/10, 23:16:48",nan,1729409.0,147.0,0,174,Jaeger log warning messages saying no sender factories available,"When my app starts up, I am getting this warning message"
Jaeger,63835165,63837122.0,1,"2020/09/10, 20:52:12",True,"2020/09/10, 23:16:48",nan,1729409.0,147.0,0,174,Jaeger log warning messages saying no sender factories available,warn io.jaegertracing.internal.senders.SenderResolver - No sender factories available.
Jaeger,63835165,63837122.0,1,"2020/09/10, 20:52:12",True,"2020/09/10, 23:16:48",nan,1729409.0,147.0,0,174,Jaeger log warning messages saying no sender factories available,"Using NoopSender, meaning that data will not be sent anywhere!"
Jaeger,63835165,63837122.0,1,"2020/09/10, 20:52:12",True,"2020/09/10, 23:16:48",nan,1729409.0,147.0,0,174,Jaeger log warning messages saying no sender factories available,I use this method to get the jaeger tracer
Jaeger,63835165,63837122.0,1,"2020/09/10, 20:52:12",True,"2020/09/10, 23:16:48",nan,1729409.0,147.0,0,174,Jaeger log warning messages saying no sender factories available,"I have manually instrumented the code, but no traces show up in the jaeger UI."
Jaeger,63835165,63837122.0,1,"2020/09/10, 20:52:12",True,"2020/09/10, 23:16:48",nan,1729409.0,147.0,0,174,Jaeger log warning messages saying no sender factories available,I have been stuck on this problem for a few days now and would appreciate any help given!
Jaeger,63835165,63837122.0,1,"2020/09/10, 20:52:12",True,"2020/09/10, 23:16:48",nan,1729409.0,147.0,0,174,Jaeger log warning messages saying no sender factories available,"In my pom file, I have dependencies on jaeger-core and opentracing-api"
Jaeger,63776496,nan,0,"2020/09/07, 14:14:23",False,"2020/09/07, 14:14:23",nan,13965271.0,160.0,0,14,Allow Jaeger-Agent to only accept traces in binary format,I am required to run a jaeger-agent on a bare-metal server that doesn't have support for docker.
Jaeger,63776496,nan,0,"2020/09/07, 14:14:23",False,"2020/09/07, 14:14:23",nan,13965271.0,160.0,0,14,Allow Jaeger-Agent to only accept traces in binary format,I have downloaded a jaeger-binary on it and am able to successfully accept binary traces on the udp port I specified in the config.
Jaeger,63776496,nan,0,"2020/09/07, 14:14:23",False,"2020/09/07, 14:14:23",nan,13965271.0,160.0,0,14,Allow Jaeger-Agent to only accept traces in binary format,"But I have the use-case where I'm only required to accept traces in binary format, and this means that I do NOT want to open the port for accepting compact thrifts."
Jaeger,63776496,nan,0,"2020/09/07, 14:14:23",False,"2020/09/07, 14:14:23",nan,13965271.0,160.0,0,14,Allow Jaeger-Agent to only accept traces in binary format,"Could anyone help me in achieving this (essentially, I should be able to open a minimal set of ports to run my agent)."
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,I have a microservices-based application Running on Kubernetes.
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,The microservices are built using dropwizard framework.
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,I would like to enable tracing in order to track the requests and have a solution that can help debug stuff.
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,"Basically, I know the implementation using Spring boot which is pretty straightforward."
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,but I'm wondering how it could be in dropwizard based application?
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,"actually, Is this is possible?"
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,Can someone share his experience with this topic?
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,And provide me with resources or examples of how I can do that?
Jaeger,63745859,64240007.0,1,"2020/09/04, 20:47:28",True,"2020/10/07, 11:18:29",nan,6452043.0,391.0,0,132,Enable tracing using jaeger on Java dropwizard microservices,Please make sure that I'm not using a service mesh.
Jaeger,63581052,nan,2,"2020/08/25, 17:27:02",True,"2020/12/03, 10:54:28",nan,2135819.0,1287.0,0,120,How to enable Jaeger opentracing in WSO2-APIM 3.1.0?,I'm not able to get any tracing on Jaeger.
Jaeger,63581052,nan,2,"2020/08/25, 17:27:02",True,"2020/12/03, 10:54:28",nan,2135819.0,1287.0,0,120,How to enable Jaeger opentracing in WSO2-APIM 3.1.0?,I did this configuration:
Jaeger,63581052,nan,2,"2020/08/25, 17:27:02",True,"2020/12/03, 10:54:28",nan,2135819.0,1287.0,0,120,How to enable Jaeger opentracing in WSO2-APIM 3.1.0?,Should I keep the double quotes in the hostname and port ?
Jaeger,63581052,nan,2,"2020/08/25, 17:27:02",True,"2020/12/03, 10:54:28",nan,2135819.0,1287.0,0,120,How to enable Jaeger opentracing in WSO2-APIM 3.1.0?,What is the correct port to use ?
Jaeger,63573926,nan,0,"2020/08/25, 10:24:55",False,"2020/08/25, 10:24:55",nan,6113381.0,27.0,0,11,Enable Jaeger Tracing to OSB layer,"I am using Spring Boot as Microservice, I am using Jaeger  for Monitor and troubleshoot transactions in complex distributed systems."
Jaeger,63573926,nan,0,"2020/08/25, 10:24:55",False,"2020/08/25, 10:24:55",nan,6113381.0,27.0,0,11,Enable Jaeger Tracing to OSB layer,I could see all microservices Span with respect to given trace id/ call.
Jaeger,63573926,nan,0,"2020/08/25, 10:24:55",False,"2020/08/25, 10:24:55",nan,6113381.0,27.0,0,11,Enable Jaeger Tracing to OSB layer,"But When I am calling OSB layer service from Spring Boot Service, I could not OSB layer as a SPAN."
Jaeger,63573926,nan,0,"2020/08/25, 10:24:55",False,"2020/08/25, 10:24:55",nan,6113381.0,27.0,0,11,Enable Jaeger Tracing to OSB layer,what could will be possible solution for this.
Jaeger,63573926,nan,0,"2020/08/25, 10:24:55",False,"2020/08/25, 10:24:55",nan,6113381.0,27.0,0,11,Enable Jaeger Tracing to OSB layer,Thank You
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,I am trying to trace logs of my spring-boot-application with jaeger .
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,Both spring-boot microservice and jaeger are running on kubernetes ( local set-up on docker-desktop ) .
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,My services traces are not visible in jaeger UI .
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,The same spring boot microservice and Jaeger local set up (without kuberntes ) is working fine .
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,Below is the configuration in my application.properties to interact with jaeger-agent in kubernetes .
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,Below is my code :
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,https://github.com/anuragk3334/Spring-boot-and-Jaeger/tree/master/HelloWorld
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,Jaeger configuration is :
Jaeger,63470148,nan,0,"2020/08/18, 16:52:39",False,"2020/08/18, 17:47:30","2020/08/18, 17:47:30",4019105.0,51.0,0,245,Jaeger with spring boot is not working when both are deployed at kubernetes,https://github.com/anuragk3334/Spring-boot-and-Jaeger/blob/master/HelloWorld/k8s/jaeger.yaml
Jaeger,63225503,nan,0,"2020/08/03, 10:38:43",False,"2020/08/03, 10:44:50","2020/08/03, 10:44:50",2077917.0,763.0,0,36,"spring boot app with opentracing and cloud-bus, disables logging autoconfiguration of jaeger",I created an application with  spring-cloud-bus  (for the auto-refresh from spring-cloud-config-server) and  opentracing-jaeger .
Jaeger,63225503,nan,0,"2020/08/03, 10:38:43",False,"2020/08/03, 10:44:50","2020/08/03, 10:44:50",2077917.0,763.0,0,36,"spring boot app with opentracing and cloud-bus, disables logging autoconfiguration of jaeger","Without  spring-cloud-bus , jaeger shows the application logs in traces."
Jaeger,63225503,nan,0,"2020/08/03, 10:38:43",False,"2020/08/03, 10:44:50","2020/08/03, 10:44:50",2077917.0,763.0,0,36,"spring boot app with opentracing and cloud-bus, disables logging autoconfiguration of jaeger","But with  spring-cloud-bus , the logs are missing."
Jaeger,63225503,nan,0,"2020/08/03, 10:38:43",False,"2020/08/03, 10:44:50","2020/08/03, 10:44:50",2077917.0,763.0,0,36,"spring boot app with opentracing and cloud-bus, disables logging autoconfiguration of jaeger","On debugging, the following details were found."
Jaeger,63225503,nan,0,"2020/08/03, 10:38:43",False,"2020/08/03, 10:44:50","2020/08/03, 10:44:50",2077917.0,763.0,0,36,"spring boot app with opentracing and cloud-bus, disables logging autoconfiguration of jaeger",Please find a  sample application  with pre-configured settings here.
Jaeger,63225503,nan,0,"2020/08/03, 10:38:43",False,"2020/08/03, 10:44:50","2020/08/03, 10:44:50",2077917.0,763.0,0,36,"spring boot app with opentracing and cloud-bus, disables logging autoconfiguration of jaeger","Can someone guide me, how to work around this issue?"
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,Most of the integrations I have come across uses the java-agent to push the traces to a central collector and in turn one can view traces in Jaeger.
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,"However in my case I can't use the java agent, hence I decided to go with the custom tracing api which seems fine and there are many examples for this."
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,"By design my low latency application limits me from making any connections to external components/ports hence I am also trying to avoid pushing the traces/spans to the local Jaeger agent or Collector endpoint, rather have the traces logged via the LogReporter."
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,Beyond this I am wondering how to build a pipeline for pushing the trace logs in to Jaeger.
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,The logs themselves are in AWS cloudwatch as streams so I am thinking if I use a Serveless Lambda to subscribe and parse these trace log events then I could ship them myself to Jaeger using may be the HTTP /api/traces endpoint (not much details but read somewhere that this exists in some form).
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,At this point my question is if this is the right way or there is a better mechanism to achieve this.
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,As I have no idea if the traces themselves can be replayed in this fashion to the Collector.
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,Also not sure what format the endpoint accepts as I don't see much documentation or example around this.
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,The objective is for my application to &quot;not&quot; connect to any external monitoring infrastructure via push events so if there is any better way for Jaeger integration I would love to hear.
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,"Also I am okay if any other API in the form of OpenTracing, OpenCensus or even the latest OpenTelemetry can help with this."
Jaeger,63221267,nan,1,"2020/08/03, 00:35:01",False,"2020/08/18, 16:51:58",nan,1333243.0,236.0,0,59,Jaeger integration for a Java low latency application,Thanks
Jaeger,63218451,nan,1,"2020/08/02, 19:30:58",False,"2020/08/02, 19:38:53",nan,8957330.0,408.0,0,52,Compiling without &quot;-L /usr/local/lib&quot; yields a &quot;cannot connect to socket&quot; error while trying out jaeger opentracing in C++,I'm trying out OpenTracing Jaeger and have the following file  test.cpp :
Jaeger,63218451,nan,1,"2020/08/02, 19:30:58",False,"2020/08/02, 19:38:53",nan,8957330.0,408.0,0,52,Compiling without &quot;-L /usr/local/lib&quot; yields a &quot;cannot connect to socket&quot; error while trying out jaeger opentracing in C++,And consider  config.yml  to be:
Jaeger,63218451,nan,1,"2020/08/02, 19:30:58",False,"2020/08/02, 19:38:53",nan,8957330.0,408.0,0,52,Compiling without &quot;-L /usr/local/lib&quot; yields a &quot;cannot connect to socket&quot; error while trying out jaeger opentracing in C++,"Now if I compile  test.cpp  with  g++ test.cpp -lopentracing -ljaegertracing -lyaml-cpp  and run  ./a.out config.yml , I get a"
Jaeger,63218451,nan,1,"2020/08/02, 19:30:58",False,"2020/08/02, 19:38:53",nan,8957330.0,408.0,0,52,Compiling without &quot;-L /usr/local/lib&quot; yields a &quot;cannot connect to socket&quot; error while trying out jaeger opentracing in C++,"While if I compile with  g++ test.cpp -L /usr/local/lib -lopentracing -ljaegertracing -lyaml-cpp , I get a good"
Jaeger,63218451,nan,1,"2020/08/02, 19:30:58",False,"2020/08/02, 19:38:53",nan,8957330.0,408.0,0,52,Compiling without &quot;-L /usr/local/lib&quot; yields a &quot;cannot connect to socket&quot; error while trying out jaeger opentracing in C++,The contents of my  /usr/local/lib  are:
Jaeger,63218451,nan,1,"2020/08/02, 19:30:58",False,"2020/08/02, 19:38:53",nan,8957330.0,408.0,0,52,Compiling without &quot;-L /usr/local/lib&quot; yields a &quot;cannot connect to socket&quot; error while trying out jaeger opentracing in C++,"I'm using Jaeger in conjunction with a larger project involving ROS and get the same error even if I  add_compile_options(-L /usr/local/lib)  for  CMakeLists.txt  of the appropriate package; so, I wanted to better understand what the exact cause of the above error is, so hopefully that helps me to fix the one involved in ROS."
Jaeger,63218451,nan,1,"2020/08/02, 19:30:58",False,"2020/08/02, 19:38:53",nan,8957330.0,408.0,0,52,Compiling without &quot;-L /usr/local/lib&quot; yields a &quot;cannot connect to socket&quot; error while trying out jaeger opentracing in C++,Thanks!
Jaeger,63198673,63198835.0,1,"2020/07/31, 23:24:38",True,"2020/07/31, 23:41:17","2020/07/31, 23:37:23",2352611.0,539.0,0,94,Search in Jaeger ElasticSearch,My query from the Jaeger ElasticSearc returns the following entry
Jaeger,63198673,63198835.0,1,"2020/07/31, 23:24:38",True,"2020/07/31, 23:41:17","2020/07/31, 23:37:23",2352611.0,539.0,0,94,Search in Jaeger ElasticSearch,My goal is to find entries which have tag[&quot;internal.span&quot;] &quot;zipkin&quot;.
Jaeger,63198673,63198835.0,1,"2020/07/31, 23:24:38",True,"2020/07/31, 23:41:17","2020/07/31, 23:37:23",2352611.0,539.0,0,94,Search in Jaeger ElasticSearch,Field &quot;tags&quot; is &quot;nested&quot;.
Jaeger,63198673,63198835.0,1,"2020/07/31, 23:24:38",True,"2020/07/31, 23:41:17","2020/07/31, 23:37:23",2352611.0,539.0,0,94,Search in Jaeger ElasticSearch,I am trying a query like the one below.
Jaeger,63198673,63198835.0,1,"2020/07/31, 23:24:38",True,"2020/07/31, 23:41:17","2020/07/31, 23:37:23",2352611.0,539.0,0,94,Search in Jaeger ElasticSearch,I do not get hits.
Jaeger,63198673,63198835.0,1,"2020/07/31, 23:24:38",True,"2020/07/31, 23:41:17","2020/07/31, 23:37:23",2352611.0,539.0,0,94,Search in Jaeger ElasticSearch,What do I miss?
Jaeger,62830150,nan,1,"2020/07/10, 11:24:29",False,"2020/07/15, 11:34:09",nan,8575474.0,184.0,0,44,Is there a way to know whether Tracer is success fully connected to jaeger backend server in jaegerclientcpp?,"In  jaeger-client-cpp  when I connect my  Tracer  variable to jaeger backend (I m using  jaeger-all-in-one  server) then upon successful connection  LOG INFO  message is shown telling me the connection is successful, but when connection is unsuccessful is just shows  LOG ERROR  message telling me that connection with server not successful."
Jaeger,62830150,nan,1,"2020/07/10, 11:24:29",False,"2020/07/15, 11:34:09",nan,8575474.0,184.0,0,44,Is there a way to know whether Tracer is success fully connected to jaeger backend server in jaegerclientcpp?,So is there any way to check this programatically about the status of connection of  Tracer  with server.
Jaeger,62830150,nan,1,"2020/07/10, 11:24:29",False,"2020/07/15, 11:34:09",nan,8575474.0,184.0,0,44,Is there a way to know whether Tracer is success fully connected to jaeger backend server in jaegerclientcpp?,"OS-ubuntu 18.04 
 jaeger-client-cpp-v0.5.0"
Jaeger,62813323,nan,0,"2020/07/09, 13:57:42",False,"2020/07/09, 13:57:42",nan,8443942.0,21.0,0,65,OpenTelemetry Jaeger collector exporter implemented with grpc blocking stub. Isn&#39;t it risky?,The  JaegerGrpcSpanExporter  of the Java OpenTelemetry API implements the export method with a grpc blocking stub ( CollectorServiceGrpc.CollectorServiceBlockingStub ).
Jaeger,62813323,nan,0,"2020/07/09, 13:57:42",False,"2020/07/09, 13:57:42",nan,8443942.0,21.0,0,65,OpenTelemetry Jaeger collector exporter implemented with grpc blocking stub. Isn&#39;t it risky?,"In case of high latence or slowness on the collector side, isn't it dangerous to block the thread?"
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,How to add method name to the log section in Jaeger.
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,I just included opentracing-spring-jaeger-cloud-starter in my spring boot application and it amazingly contextualized logs.
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,"I can also see method name, but only in tags."
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,"The problem is that when there are multiple methods invoked and if the logging is not explicitly referring method name, it is difficult (a lil bit) to trace it."
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,"Currently level, logger, message and thread are present in the log section."
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,I want to add method name as well.
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,Does anyone know how to do it?
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,"In case you want to see a sample application, this is one  https://github.com/winster/spring-tracer"
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,"Note that, I have already set a console pattern with method name."
Jaeger,62755628,nan,0,"2020/07/06, 15:00:50",False,"2020/07/06, 16:35:40","2020/07/06, 16:35:40",2077917.0,763.0,0,257,open tracing java spring jaeger log configuration,But the log fields always remains the same.
Jaeger,62655002,nan,0,"2020/06/30, 13:07:08",False,"2020/06/30, 13:07:08",nan,8039395.0,21.0,0,113,Distributed Tracing on Multicluster single Control plane Istio(Jaeger/Zipkin),We are using Multi cluster single plane Istio on GKE.
Jaeger,62655002,nan,0,"2020/06/30, 13:07:08",False,"2020/06/30, 13:07:08",nan,8039395.0,21.0,0,113,Distributed Tracing on Multicluster single Control plane Istio(Jaeger/Zipkin),We are trying to get traces of all the remote cluster services into the main cluster tracing pod.
Jaeger,62655002,nan,0,"2020/06/30, 13:07:08",False,"2020/06/30, 13:07:08",nan,8039395.0,21.0,0,113,Distributed Tracing on Multicluster single Control plane Istio(Jaeger/Zipkin),We have exposed the tracing pod via ILB in main cluster.
Jaeger,62655002,nan,0,"2020/06/30, 13:07:08",False,"2020/06/30, 13:07:08",nan,8039395.0,21.0,0,113,Distributed Tracing on Multicluster single Control plane Istio(Jaeger/Zipkin),And created a headless zipkin service on remote cluster with ILB IP as endpoint.
Jaeger,62655002,nan,0,"2020/06/30, 13:07:08",False,"2020/06/30, 13:07:08",nan,8039395.0,21.0,0,113,Distributed Tracing on Multicluster single Control plane Istio(Jaeger/Zipkin),As of now it's not working.
Jaeger,62655002,nan,0,"2020/06/30, 13:07:08",False,"2020/06/30, 13:07:08",nan,8039395.0,21.0,0,113,Distributed Tracing on Multicluster single Control plane Istio(Jaeger/Zipkin),We have tried setting up &quot;remoteTelemetryAddress&quot; flag as well on remote clusters but still no result.
Jaeger,62655002,nan,0,"2020/06/30, 13:07:08",False,"2020/06/30, 13:07:08",nan,8039395.0,21.0,0,113,Distributed Tracing on Multicluster single Control plane Istio(Jaeger/Zipkin),"Istio Version: 1.5.5
GKE version: 1.16.8-gke.15
Network: Single VPC"
Jaeger,62323955,nan,0,"2020/06/11, 15:02:46",False,"2020/06/12, 11:38:28","2020/06/12, 11:38:28",13719454.0,1.0,0,55,How can spans of a Distributed Tracing Software (such as Jaeger/Zipkin) be transmitted between applications using thrift RPC messages?,I have a node.js application using RPC messages over thrift API to communicate with another server application.
Jaeger,62323955,nan,0,"2020/06/11, 15:02:46",False,"2020/06/12, 11:38:28","2020/06/12, 11:38:28",13719454.0,1.0,0,55,How can spans of a Distributed Tracing Software (such as Jaeger/Zipkin) be transmitted between applications using thrift RPC messages?,"I want to integrate distributed tracing, such as Jaeger or Zipkin, which traces the requests through these applications."
Jaeger,62323955,nan,0,"2020/06/11, 15:02:46",False,"2020/06/12, 11:38:28","2020/06/12, 11:38:28",13719454.0,1.0,0,55,How can spans of a Distributed Tracing Software (such as Jaeger/Zipkin) be transmitted between applications using thrift RPC messages?,"The problem is that every approach and example I found to this topic is given with HTTP requests, which I don't use."
Jaeger,62323955,nan,0,"2020/06/11, 15:02:46",False,"2020/06/12, 11:38:28","2020/06/12, 11:38:28",13719454.0,1.0,0,55,How can spans of a Distributed Tracing Software (such as Jaeger/Zipkin) be transmitted between applications using thrift RPC messages?,The span context is added to the HTTP header with an inject function of the tracer.
Jaeger,62323955,nan,0,"2020/06/11, 15:02:46",False,"2020/06/12, 11:38:28","2020/06/12, 11:38:28",13719454.0,1.0,0,55,How can spans of a Distributed Tracing Software (such as Jaeger/Zipkin) be transmitted between applications using thrift RPC messages?,"On server-side, the extract function can be used to get these information, which was added to the header before."
Jaeger,62323955,nan,0,"2020/06/11, 15:02:46",False,"2020/06/12, 11:38:28","2020/06/12, 11:38:28",13719454.0,1.0,0,55,How can spans of a Distributed Tracing Software (such as Jaeger/Zipkin) be transmitted between applications using thrift RPC messages?,"Generally, this basic approach for this inter-process context propagation using RPC messages over thrift API should be the same, but I simply don't know how to start, because I cannot find any examples, recommended ways or best practises."
Jaeger,62323955,nan,0,"2020/06/11, 15:02:46",False,"2020/06/12, 11:38:28","2020/06/12, 11:38:28",13719454.0,1.0,0,55,How can spans of a Distributed Tracing Software (such as Jaeger/Zipkin) be transmitted between applications using thrift RPC messages?,"The only advice that this should be possible, I found on Zipkin's website (sub-item Thrift Tracing):
 https://zipkin.io/pages/instrumenting.html"
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,I'm a newbie on Jaeger and would like to know if I could trace an end-to-end transaction with a parent span &amp; child ones like the one described below with polling from child components (no direct invocation from parent to child) and callback from childs to the parent component.
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,First let's describe a simplified view of what I'd like to do.
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,A solution made of several components exposes a REST API to submit transactions.
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,It synchronously returns a transaction Id after invocation and will callback the invoker upon completion or failure of the transaction.
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,So what I want to trace is the overall transaction from 1 to 3:
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,Under the hood:
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,"I'm assuming that it will be possible to trace the entire transaction with Jaeger, but here are my questions:
Question:"
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,Any hints &amp; tips will be welcome.
Jaeger,62112814,nan,1,"2020/05/31, 10:12:22",False,"2020/06/02, 01:34:32",nan,4573609.0,113.0,0,220,Can jaeger be used to trace an end-to-end transaction with polling &amp; callback used by the underlying components fulfilling steps of the transaction,Thx.
Jaeger,61619122,61666591.0,1,"2020/05/05, 20:22:02",True,"2020/05/07, 22:54:39","2020/05/05, 20:30:03",7069613.0,13.0,0,106,Jaeger helm chart without cassandra overwrite,Using the helm chart for Jaeger I see that it makes use of the cassandra subchart.
Jaeger,61619122,61666591.0,1,"2020/05/05, 20:22:02",True,"2020/05/07, 22:54:39","2020/05/05, 20:30:03",7069613.0,13.0,0,106,Jaeger helm chart without cassandra overwrite,Looking at the documentation and config files it looks like by setting the provisionDataStore.cassandra override to false that the cassandra subchart shouldn't be getting installed.
Jaeger,61619122,61666591.0,1,"2020/05/05, 20:22:02",True,"2020/05/07, 22:54:39","2020/05/05, 20:30:03",7069613.0,13.0,0,106,Jaeger helm chart without cassandra overwrite,"However, when the override is set I can still see the cassandra service being installed on my cluster."
Jaeger,61619122,61666591.0,1,"2020/05/05, 20:22:02",True,"2020/05/07, 22:54:39","2020/05/05, 20:30:03",7069613.0,13.0,0,106,Jaeger helm chart without cassandra overwrite,Anybody know why and how I can prevent cassandra service from being deployed to my cluster?
Jaeger,61619122,61666591.0,1,"2020/05/05, 20:22:02",True,"2020/05/07, 22:54:39","2020/05/05, 20:30:03",7069613.0,13.0,0,106,Jaeger helm chart without cassandra overwrite,I was expecting that when I set the provisionDataStore.cassandra=false that I shouldn't see any cassandra services being deployed to my cluster.
Jaeger,61619122,61666591.0,1,"2020/05/05, 20:22:02",True,"2020/05/07, 22:54:39","2020/05/05, 20:30:03",7069613.0,13.0,0,106,Jaeger helm chart without cassandra overwrite,"This is what the requirements.yaml file looks like for the Jaeger helm chart:
dependencies:
  - name: cassandra
    version: ^0.13.1
    repository:  https://kubernetes-charts-incubator.storage.googleapis.com/ 
    condition: provisionDataStore.cassandra
  - name: elasticsearch
    version: ^7.5.1
    repository:  https://helm.elastic.co 
    condition: provisionDataStore.elasticsearch"
Jaeger,61431244,nan,1,"2020/04/25, 22:18:57",True,"2021/01/22, 23:26:39",nan,13145553.0,3.0,0,96,Custom Tags with opentracing-spring-jaeger-web-starter,I am using opentracing-spring-jaeger-web-starter in my spring boot project.
Jaeger,61431244,nan,1,"2020/04/25, 22:18:57",True,"2021/01/22, 23:26:39",nan,13145553.0,3.0,0,96,Custom Tags with opentracing-spring-jaeger-web-starter,It create auto spans for all rest call and do tag with standard tags.
Jaeger,61431244,nan,1,"2020/04/25, 22:18:57",True,"2021/01/22, 23:26:39",nan,13145553.0,3.0,0,96,Custom Tags with opentracing-spring-jaeger-web-starter,How can i add custom tags for rest call?
Jaeger,61073781,nan,1,"2020/04/07, 09:09:15",False,"2020/04/11, 23:15:30","2020/04/08, 10:06:51",13245224.0,1.0,0,159,How to use Jaeger with Helidon Microprofile,I have created a Helidon Microprofile quickstart project from helidon.io get started while configuring with Jaeger I am unable to find the Trace in Jaeger UI below are the steps which I have followed:
Jaeger,61073781,nan,1,"2020/04/07, 09:09:15",False,"2020/04/11, 23:15:30","2020/04/08, 10:06:51",13245224.0,1.0,0,159,How to use Jaeger with Helidon Microprofile,Created project using
Jaeger,61073781,nan,1,"2020/04/07, 09:09:15",False,"2020/04/11, 23:15:30","2020/04/08, 10:06:51",13245224.0,1.0,0,159,How to use Jaeger with Helidon Microprofile,Updated  pom.xml  with Jaeger dependencies
Jaeger,61073781,nan,1,"2020/04/07, 09:09:15",False,"2020/04/11, 23:15:30","2020/04/08, 10:06:51",13245224.0,1.0,0,159,How to use Jaeger with Helidon Microprofile,Updated GreetApplication
Jaeger,61073781,nan,1,"2020/04/07, 09:09:15",False,"2020/04/11, 23:15:30","2020/04/08, 10:06:51",13245224.0,1.0,0,159,How to use Jaeger with Helidon Microprofile,Updated /helidon-quickstart-mp/src/main/resources/META-INF/microprofile-config.properties
Jaeger,61073781,nan,1,"2020/04/07, 09:09:15",False,"2020/04/11, 23:15:30","2020/04/08, 10:06:51",13245224.0,1.0,0,159,How to use Jaeger with Helidon Microprofile,Executed mvn package and then  target&gt;java -jar helidon-quickstart-mp.jar
Jaeger,61073781,nan,1,"2020/04/07, 09:09:15",False,"2020/04/11, 23:15:30","2020/04/08, 10:06:51",13245224.0,1.0,0,159,How to use Jaeger with Helidon Microprofile,Now in my Jaeger UI I am unable to trace the running Service:
Jaeger,61073781,nan,1,"2020/04/07, 09:09:15",False,"2020/04/11, 23:15:30","2020/04/08, 10:06:51",13245224.0,1.0,0,159,How to use Jaeger with Helidon Microprofile,So how can I configure Jaeger UI to my helidon Microprofile project?
Jaeger,60793517,nan,1,"2020/03/21, 23:33:28",False,"2020/03/22, 14:06:25","2020/03/22, 14:06:25",1543826.0,99.0,0,367,"Can&#39;t see my service traces in Jaeger UI (local node, Docker for Mac)",I'm trying to play around with Jaeger and open-tracing in my local k8s node (Docker for Mac) and having some trouble see traces in the UI.
Jaeger,60793517,nan,1,"2020/03/21, 23:33:28",False,"2020/03/22, 14:06:25","2020/03/22, 14:06:25",1543826.0,99.0,0,367,"Can&#39;t see my service traces in Jaeger UI (local node, Docker for Mac)",I'm using the Jaeger operator and deployment annotations to inject the jaeger sidecar.
Jaeger,60793517,nan,1,"2020/03/21, 23:33:28",False,"2020/03/22, 14:06:25","2020/03/22, 14:06:25",1543826.0,99.0,0,367,"Can&#39;t see my service traces in Jaeger UI (local node, Docker for Mac)",The Jaeger cr is configured to sample constantly every request.
Jaeger,60793517,nan,1,"2020/03/21, 23:33:28",False,"2020/03/22, 14:06:25","2020/03/22, 14:06:25",1543826.0,99.0,0,367,"Can&#39;t see my service traces in Jaeger UI (local node, Docker for Mac)","Up until this point, everything seems to be fine but when I send some HTTP traffic to my pods (Through nginx-ingress) I can see it coming but can't find any traces in Jaeger UI."
Jaeger,60793517,nan,1,"2020/03/21, 23:33:28",False,"2020/03/22, 14:06:25","2020/03/22, 14:06:25",1543826.0,99.0,0,367,"Can&#39;t see my service traces in Jaeger UI (local node, Docker for Mac)","From reading the documentation, these steps should've implicitly collect and send the traces."
Jaeger,60793517,nan,1,"2020/03/21, 23:33:28",False,"2020/03/22, 14:06:25","2020/03/22, 14:06:25",1543826.0,99.0,0,367,"Can&#39;t see my service traces in Jaeger UI (local node, Docker for Mac)",Am I missing something?
Jaeger,60655072,nan,1,"2020/03/12, 15:24:19",False,"2020/07/10, 09:20:22","2020/03/12, 17:58:39",13051129.0,1.0,0,65,how to configure elastic search in jaeger without docker,I am trying to setup jaeger-all-in-one on one server.
Jaeger,60655072,nan,1,"2020/03/12, 15:24:19",False,"2020/07/10, 09:20:22","2020/03/12, 17:58:39",13051129.0,1.0,0,65,how to configure elastic search in jaeger without docker,"If I run the exe jaeger-all-in-one, everything works as expected (using in-memory)."
Jaeger,60655072,nan,1,"2020/03/12, 15:24:19",False,"2020/07/10, 09:20:22","2020/03/12, 17:58:39",13051129.0,1.0,0,65,how to configure elastic search in jaeger without docker,"In order to see the options available with ES, I am not able to run a help command."
Jaeger,60655072,nan,1,"2020/03/12, 15:24:19",False,"2020/07/10, 09:20:22","2020/03/12, 17:58:39",13051129.0,1.0,0,65,how to configure elastic search in jaeger without docker,"Now, my requirement is to specify an elastic search URL."
Jaeger,60655072,nan,1,"2020/03/12, 15:24:19",False,"2020/07/10, 09:20:22","2020/03/12, 17:58:39",13051129.0,1.0,0,65,how to configure elastic search in jaeger without docker,"I have set up the environment variables  SPAN_STORAGE_TYPES  and  ES_SERVER_URLS , but couldn't find how to run jaeger-all-in-one.exe by asking it to take in these environment variables."
Jaeger,60506609,61860648.0,1,"2020/03/03, 13:49:05",True,"2020/05/18, 03:34:12",nan,8367387.0,63.0,0,166,Set up Jaeger with OpenTelemetry,I have built a sample app to understand the trace and span using OpenTelemetry.
Jaeger,60506609,61860648.0,1,"2020/03/03, 13:49:05",True,"2020/05/18, 03:34:12",nan,8367387.0,63.0,0,166,Set up Jaeger with OpenTelemetry,I want to see them in Jaeger UI.
Jaeger,60506609,61860648.0,1,"2020/03/03, 13:49:05",True,"2020/05/18, 03:34:12",nan,8367387.0,63.0,0,166,Set up Jaeger with OpenTelemetry,How to set up Jaeger with my application which uses OpenTelemetry for tracing?
Jaeger,59782209,nan,2,"2020/01/17, 08:27:13",False,"2021/03/17, 17:02:16",nan,12729435.0,1.0,0,74,How can I get traces of Nservicebus using Jaeger?,I have gone through Jaeger Documentation.
Jaeger,59782209,nan,2,"2020/01/17, 08:27:13",False,"2021/03/17, 17:02:16",nan,12729435.0,1.0,0,74,How can I get traces of Nservicebus using Jaeger?,They have specified that how will Jaeger will work the HTTP request kind of scenario but if I want to get traces of Nservicebus's to publish/subscribe method then How will I get using Jaeger?
Jaeger,59782209,nan,2,"2020/01/17, 08:27:13",False,"2021/03/17, 17:02:16",nan,12729435.0,1.0,0,74,How can I get traces of Nservicebus using Jaeger?,Is it possible?
Jaeger,59782209,nan,2,"2020/01/17, 08:27:13",False,"2021/03/17, 17:02:16",nan,12729435.0,1.0,0,74,How can I get traces of Nservicebus using Jaeger?,Or Jaeger only works with HTTP requests?
Jaeger,59561262,nan,1,"2020/01/02, 11:48:35",False,"2020/01/07, 14:31:30",nan,4977370.0,13.0,0,150,jaeger endpoint isnt recieving data using jaeger Node.js client,Was trying to connect to jaeger using HTTP request using nodejs but the spans are not reaching the jaeger endpoint.
Jaeger,59561262,nan,1,"2020/01/02, 11:48:35",False,"2020/01/07, 14:31:30",nan,4977370.0,13.0,0,150,jaeger endpoint isnt recieving data using jaeger Node.js client,"please help with this code snippet.,"
Jaeger,59561262,nan,1,"2020/01/02, 11:48:35",False,"2020/01/07, 14:31:30",nan,4977370.0,13.0,0,150,jaeger endpoint isnt recieving data using jaeger Node.js client,Any help would be much appreciated!
Jaeger,59372759,59391314.0,1,"2019/12/17, 12:57:32",True,"2019/12/18, 13:44:56","2019/12/17, 13:16:35",9112151.0,425.0,0,1320,Jaeger traceID in response headers,I'm playing with JaegerTracing in Django using tutorial  https://github.com/contino/jaeger-django-docker-tutorial .
Jaeger,59372759,59391314.0,1,"2019/12/17, 12:57:32",True,"2019/12/18, 13:44:56","2019/12/17, 13:16:35",9112151.0,425.0,0,1320,Jaeger traceID in response headers,Now I don't know how to take out traceId from response headers because it's not there.
Jaeger,59372759,59391314.0,1,"2019/12/17, 12:57:32",True,"2019/12/18, 13:44:56","2019/12/17, 13:16:35",9112151.0,425.0,0,1320,Jaeger traceID in response headers,When finding traces in Jaeger UI it returns response with data (see also screenshot below):
Jaeger,59372759,59391314.0,1,"2019/12/17, 12:57:32",True,"2019/12/18, 13:44:56","2019/12/17, 13:16:35",9112151.0,425.0,0,1320,Jaeger traceID in response headers,I suspected it in response headers but it is not.
Jaeger,59372759,59391314.0,1,"2019/12/17, 12:57:32",True,"2019/12/18, 13:44:56","2019/12/17, 13:16:35",9112151.0,425.0,0,1320,Jaeger traceID in response headers,How can I do this?
Jaeger,59076369,nan,1,"2019/11/27, 20:22:10",False,"2019/11/27, 20:38:47",nan,369759.0,12299.0,0,285,quarkus reactive-postgresql-client with jaeger opentracing support,Based on  this  and  this
Jaeger,59076369,nan,1,"2019/11/27, 20:22:10",False,"2019/11/27, 20:38:47",nan,369759.0,12299.0,0,285,quarkus reactive-postgresql-client with jaeger opentracing support,How would I enable tracing for  reactive-sql-clients  ?
Jaeger,59076369,nan,1,"2019/11/27, 20:22:10",False,"2019/11/27, 20:38:47",nan,369759.0,12299.0,0,285,quarkus reactive-postgresql-client with jaeger opentracing support,"Now use  %dev.quarkus.datasource.url=vertx-reactive:postgresql://dev-db-server:5432/mydb  - it works, but no tracing support though."
Jaeger,59076369,nan,1,"2019/11/27, 20:22:10",False,"2019/11/27, 20:38:47",nan,369759.0,12299.0,0,285,quarkus reactive-postgresql-client with jaeger opentracing support,I can see racing for my rest calls but not the db.
Jaeger,59076369,nan,1,"2019/11/27, 20:22:10",False,"2019/11/27, 20:38:47",nan,369759.0,12299.0,0,285,quarkus reactive-postgresql-client with jaeger opentracing support,Tried to use  %dev.quarkus.datasource.url=vertx-reactive:tracing:postgresql://dev-db-server:5432/mydb
Jaeger,59076369,nan,1,"2019/11/27, 20:22:10",False,"2019/11/27, 20:38:47",nan,369759.0,12299.0,0,285,quarkus reactive-postgresql-client with jaeger opentracing support,my deps:
Jaeger,58879198,nan,0,"2019/11/15, 16:36:15",False,"2019/12/10, 09:27:49","2019/12/10, 09:27:49",6309206.0,77.0,0,149,Handle Jaeger errors,I am planning to use  Jaeger  tracing in on my  Golang  server.
Jaeger,58879198,nan,0,"2019/11/15, 16:36:15",False,"2019/12/10, 09:27:49","2019/12/10, 09:27:49",6309206.0,77.0,0,149,Handle Jaeger errors,Everything is ok but I haven't found a way to handle  Jaeger  errors.
Jaeger,58879198,nan,0,"2019/11/15, 16:36:15",False,"2019/12/10, 09:27:49","2019/12/10, 09:27:49",6309206.0,77.0,0,149,Handle Jaeger errors,"I want to catch, for example, connection error to  Jaeger  backend while sending trace and write it to  loggly ."
Jaeger,58879198,nan,0,"2019/11/15, 16:36:15",False,"2019/12/10, 09:27:49","2019/12/10, 09:27:49",6309206.0,77.0,0,149,Handle Jaeger errors,Code example:
Jaeger,58704937,nan,1,"2019/11/05, 07:20:26",False,"2019/11/07, 12:06:45",nan,nan,nan,0,262,How can I deploy Istio Jaeger UI tracing in production without using kubectl port forwarding,I am trying to deploy Istio Jaeger UI for distributed tracing.
Jaeger,58704937,nan,1,"2019/11/05, 07:20:26",False,"2019/11/07, 12:06:45",nan,nan,nan,0,262,How can I deploy Istio Jaeger UI tracing in production without using kubectl port forwarding,Currently I am using kubectl port forwarding using the command  kubectl port-forward -n monitoring prometheus-prometheus-operator-prometheus-0 9090 .
Jaeger,58704937,nan,1,"2019/11/05, 07:20:26",False,"2019/11/07, 12:06:45",nan,nan,nan,0,262,How can I deploy Istio Jaeger UI tracing in production without using kubectl port forwarding,But it runs on  http://localhost:port  So how can I do it in production?
Jaeger,58704937,nan,1,"2019/11/05, 07:20:26",False,"2019/11/07, 12:06:45",nan,nan,nan,0,262,How can I deploy Istio Jaeger UI tracing in production without using kubectl port forwarding,Is there any other way to deploy in production.
Jaeger,58704937,nan,1,"2019/11/05, 07:20:26",False,"2019/11/07, 12:06:45",nan,nan,nan,0,262,How can I deploy Istio Jaeger UI tracing in production without using kubectl port forwarding,And also how can I make it run on  https ?
Jaeger,58514716,nan,1,"2019/10/23, 05:40:41",True,"2019/10/25, 18:12:58",nan,11695449.0,23.0,0,152,How to export multiple microservices jaeger metrics stored in elasticsearch to prometheus,We are able to get latency metrics of multiple microservices using Jaeger.
Jaeger,58514716,nan,1,"2019/10/23, 05:40:41",True,"2019/10/25, 18:12:58",nan,11695449.0,23.0,0,152,How to export multiple microservices jaeger metrics stored in elasticsearch to prometheus,Currently Jaeger stores application metrics in elasticsearch.
Jaeger,58514716,nan,1,"2019/10/23, 05:40:41",True,"2019/10/25, 18:12:58",nan,11695449.0,23.0,0,152,How to export multiple microservices jaeger metrics stored in elasticsearch to prometheus,My usecase is to get the latency of application from elasticsearch to prometheus.
Jaeger,58514716,nan,1,"2019/10/23, 05:40:41",True,"2019/10/25, 18:12:58",nan,11695449.0,23.0,0,152,How to export multiple microservices jaeger metrics stored in elasticsearch to prometheus,Is there anyway to read the elasticsearch metrics of Jaeger?
Jaeger,58514716,nan,1,"2019/10/23, 05:40:41",True,"2019/10/25, 18:12:58",nan,11695449.0,23.0,0,152,How to export multiple microservices jaeger metrics stored in elasticsearch to prometheus,I already used elasticsearch-prometheus-exporter which only exports cluster details of ES.
Jaeger,58011545,nan,1,"2019/09/19, 15:58:00",False,"2019/09/27, 19:26:25",nan,6323547.0,63.0,0,30,Jaeger integration Effort in Microservices in nodejs,I am analysing at a very hight level how much effort would it be for jaeger integration in nodejs microservices.
Jaeger,58011545,nan,1,"2019/09/19, 15:58:00",False,"2019/09/27, 19:26:25",nan,6323547.0,63.0,0,30,Jaeger integration Effort in Microservices in nodejs,Does it require code changes or only deployment.
Jaeger,58011545,nan,1,"2019/09/19, 15:58:00",False,"2019/09/27, 19:26:25",nan,6323547.0,63.0,0,30,Jaeger integration Effort in Microservices in nodejs,"and if code changes is required, is code changes needed in first service (i.e."
Jaeger,58011545,nan,1,"2019/09/19, 15:58:00",False,"2019/09/27, 19:26:25",nan,6323547.0,63.0,0,30,Jaeger integration Effort in Microservices in nodejs,api-gateway) or all the services need to have code changes.
Jaeger,58011545,nan,1,"2019/09/19, 15:58:00",False,"2019/09/27, 19:26:25",nan,6323547.0,63.0,0,30,Jaeger integration Effort in Microservices in nodejs,I would really appreciate if someone can give a rough idea of tasks and effort.
Jaeger,57473539,nan,2,"2019/08/13, 11:03:26",True,"2019/08/13, 14:30:22",nan,3279191.0,285.0,0,654,Jaeger with ElasticSearch,I have created a microservice based architecture using Spring Boot and deployed the application on Kubernetes/Istio platform.
Jaeger,57473539,nan,2,"2019/08/13, 11:03:26",True,"2019/08/13, 14:30:22",nan,3279191.0,285.0,0,654,Jaeger with ElasticSearch,The different microservices communicate with each other using either JMS (ActiveMQ) or REST API.
Jaeger,57473539,nan,2,"2019/08/13, 11:03:26",True,"2019/08/13, 14:30:22",nan,3279191.0,285.0,0,654,Jaeger with ElasticSearch,I am getting the tracing of REST communication on Istio's Jaeger but the JMS based communication is missing in Jaeger.
Jaeger,57473539,nan,2,"2019/08/13, 11:03:26",True,"2019/08/13, 14:30:22",nan,3279191.0,285.0,0,654,Jaeger with ElasticSearch,I am using ElasticSearch to store my application logs.
Jaeger,57473539,nan,2,"2019/08/13, 11:03:26",True,"2019/08/13, 14:30:22",nan,3279191.0,285.0,0,654,Jaeger with ElasticSearch,Is it possible to use the same ElasticSearch as a backend(DB) of Jaeger?
Jaeger,57473539,nan,2,"2019/08/13, 11:03:26",True,"2019/08/13, 14:30:22",nan,3279191.0,285.0,0,654,Jaeger with ElasticSearch,If yes then I will store tracing specific logs in ElasticSearch and query them  on Jaeger UI.
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,I'm setting up a proof of concept featuring two ASP.NET Core applications that are both instrumented with  Jaeger  to demonstrate how it can propagate a trace between services over the wire.
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,Both applications are being deployed to Azure App Services.
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,I'm using the  OpenTracing Contrib  package to automatically inject the Jaeger trace context into my inter-service traffic in the form of HTTP Headers (the package is hardcoded to use that form of transmission).
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,"But it appears that those headers are going missing along the way, as the receiving application is unable to resume the tracing context."
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,"Before deploying to Azure, I'm testing the applications locally with Docker Compose, and  with that setup the context propagation works fine ."
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,It's only once the apps are in Azure that things break.
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,"The applications communicate over HTTPS and I've disabled HSTS and HTTPS redirection in case that might be causing Azure to drop the headers, based on the answer in  this previous thread ."
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,"I've also tried running both applications in Azure Container Instances, and that seems to be a non-starter - it doesn't fix the context propagation and seems to introduce more bugs around span relationships."
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,"The two applications are nearly identical in their setup, and differ only in the API endpoints they serve."
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,My CreateWebHostBuild from program.cs:
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,The contents of the AddJaeger extension method which is largely borrowed from  the Contrib sample :
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,My startup.cs configure method to show I'm not doing anything weird with the headers (the metrics extensions are for prometheus-net)
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,I expect any calls from one application to the other to propagate the active Jaeger trace context.
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,"Instead, the two applications log their traces separately and no link can be discerned between them in the Jaeger UI."
Jaeger,57419866,nan,1,"2019/08/08, 23:05:33",False,"2019/08/09, 00:26:00","2019/08/08, 23:25:06",5799778.0,121.0,0,287,Azure App Service removing Jaeger HTTP headers from inter-service requests,"Here's a screenshot of a trace that should have spanned both services, but instead only shows spans from the first service:"
Jaeger,57278976,nan,1,"2019/07/30, 22:43:00",False,"2019/07/31, 19:41:53","2019/07/30, 22:53:55",3105192.0,459.0,0,70,OpenTracing+Jaeger Language Agnostic,"I have three services A, B, and C that communicate like so"
Jaeger,57278976,nan,1,"2019/07/30, 22:43:00",False,"2019/07/31, 19:41:53","2019/07/30, 22:53:55",3105192.0,459.0,0,70,OpenTracing+Jaeger Language Agnostic,I'm using OpenTracing and Jaeger for distributed tracing.
Jaeger,57278976,nan,1,"2019/07/30, 22:43:00",False,"2019/07/31, 19:41:53","2019/07/30, 22:53:55",3105192.0,459.0,0,70,OpenTracing+Jaeger Language Agnostic,"The problem is these services are in different languages, but I'm still trying to propagate the information that A is the parent span so that the span tree looks like this."
Jaeger,57278976,nan,1,"2019/07/30, 22:43:00",False,"2019/07/31, 19:41:53","2019/07/30, 22:53:55",3105192.0,459.0,0,70,OpenTracing+Jaeger Language Agnostic,"Right now, A, B, and C are being reported as individual traces with no causality relations."
Jaeger,57278976,nan,1,"2019/07/30, 22:43:00",False,"2019/07/31, 19:41:53","2019/07/30, 22:53:55",3105192.0,459.0,0,70,OpenTracing+Jaeger Language Agnostic,All the examples I've seen involved propagating causality between different microservices in the same language and in the same project build.
Jaeger,57278976,nan,1,"2019/07/30, 22:43:00",False,"2019/07/31, 19:41:53","2019/07/30, 22:53:55",3105192.0,459.0,0,70,OpenTracing+Jaeger Language Agnostic,None involved entirely separate services.
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,"Unfortunately, I'm not able to use PyInstaller with  jaeger ."
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,The problem is some sort of a thrift error between PyInstaller and  jaeger .
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,Like discussed  here .
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,Are they any workarounds or fixes?
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,I have tried it with python 3.6 and the newest jaeger-client.
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,There I get an Errno 2 -  Even I don't even use a Config file
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,The script runs as expected -  Spans are created and web server starts.
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,"Only in the executable, it does not run."
Jaeger,57038416,nan,1,"2019/07/15, 14:04:48",False,"2019/07/15, 21:05:52","2019/07/15, 21:05:52",9368881.0,11.0,0,80,Is there a workaround for using jaeger 4.0.0 with pyinstaller(python3.7)?,And shows the following error:
Jaeger,55236000,nan,3,"2019/03/19, 09:59:09",True,"2019/04/03, 11:33:16","2019/03/19, 10:38:40",10362433.0,1.0,0,275,"How can I open the Jaeger UI(run in Istio) in a remote browser, not the localhost machine",The  Istio (version 1.0.6)  official document says:
Jaeger,55236000,nan,3,"2019/03/19, 09:59:09",True,"2019/04/03, 11:33:16","2019/03/19, 10:38:40",10362433.0,1.0,0,275,"How can I open the Jaeger UI(run in Istio) in a remote browser, not the localhost machine",We can access the Jaeger UI by the following action:
Jaeger,55236000,nan,3,"2019/03/19, 09:59:09",True,"2019/04/03, 11:33:16","2019/03/19, 10:38:40",10362433.0,1.0,0,275,"How can I open the Jaeger UI(run in Istio) in a remote browser, not the localhost machine",Kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath=’{.items[0].metadata.name}’) 16686:16686 &amp;
Jaeger,55236000,nan,3,"2019/03/19, 09:59:09",True,"2019/04/03, 11:33:16","2019/03/19, 10:38:40",10362433.0,1.0,0,275,"How can I open the Jaeger UI(run in Istio) in a remote browser, not the localhost machine",Then we can use  http://localhost:16686 .
Jaeger,55236000,nan,3,"2019/03/19, 09:59:09",True,"2019/04/03, 11:33:16","2019/03/19, 10:38:40",10362433.0,1.0,0,275,"How can I open the Jaeger UI(run in Istio) in a remote browser, not the localhost machine","But the localhost is a Linux machine, it doesn't have a browser."
Jaeger,55236000,nan,3,"2019/03/19, 09:59:09",True,"2019/04/03, 11:33:16","2019/03/19, 10:38:40",10362433.0,1.0,0,275,"How can I open the Jaeger UI(run in Istio) in a remote browser, not the localhost machine",I must open the browser on a remote machine.
Jaeger,55236000,nan,3,"2019/03/19, 09:59:09",True,"2019/04/03, 11:33:16","2019/03/19, 10:38:40",10362433.0,1.0,0,275,"How can I open the Jaeger UI(run in Istio) in a remote browser, not the localhost machine",How can I do this?
Jaeger,55236000,nan,3,"2019/03/19, 09:59:09",True,"2019/04/03, 11:33:16","2019/03/19, 10:38:40",10362433.0,1.0,0,275,"How can I open the Jaeger UI(run in Istio) in a remote browser, not the localhost machine",Thanks.
Jaeger,55222810,55277542.0,1,"2019/03/18, 15:44:25",True,"2019/03/21, 11:44:21","2019/03/18, 17:46:07",955379.0,654.0,0,1442,Logging Exceptions with Opentracing and Jaeger,I've set up Jaeger with Opentracing in a Java environment and it works nicely with logging messages with spans and tracing.
Jaeger,55222810,55277542.0,1,"2019/03/18, 15:44:25",True,"2019/03/21, 11:44:21","2019/03/18, 17:46:07",955379.0,654.0,0,1442,Logging Exceptions with Opentracing and Jaeger,But I am a bit stuck when it comes to catching and logging exceptions.
Jaeger,55222810,55277542.0,1,"2019/03/18, 15:44:25",True,"2019/03/21, 11:44:21","2019/03/18, 17:46:07",955379.0,654.0,0,1442,Logging Exceptions with Opentracing and Jaeger,But this way does not format error logging in a good readable way.
Jaeger,55222810,55277542.0,1,"2019/03/18, 15:44:25",True,"2019/03/21, 11:44:21","2019/03/18, 17:46:07",955379.0,654.0,0,1442,Logging Exceptions with Opentracing and Jaeger,I have looked around for information about this as it feels pretty obvious there should be as this is one of its components for logging.
Jaeger,55222810,55277542.0,1,"2019/03/18, 15:44:25",True,"2019/03/21, 11:44:21","2019/03/18, 17:46:07",955379.0,654.0,0,1442,Logging Exceptions with Opentracing and Jaeger,But I have somehow never seen anything about this.
Jaeger,55222810,55277542.0,1,"2019/03/18, 15:44:25",True,"2019/03/21, 11:44:21","2019/03/18, 17:46:07",955379.0,654.0,0,1442,Logging Exceptions with Opentracing and Jaeger,It is mostly about building and structuring spans.
Jaeger,55222810,55277542.0,1,"2019/03/18, 15:44:25",True,"2019/03/21, 11:44:21","2019/03/18, 17:46:07",955379.0,654.0,0,1442,Logging Exceptions with Opentracing and Jaeger,Hope anyone can help me with this when it comes to capturing and logging exceptions.
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,I have two basic Springboot microservices and I am using Jaeger.
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,Lets say two services are  foo  and  bar .
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,I am able to send  User-Agent  header from foo to bar service using Tracing Baggage property.
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,"From  foo  service, I will be calling  bar  service using  localhost:port  as of now."
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,The users will also send an  x-api-key  header in the request.
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,This header is not being forward from  foo  to  bar  service.
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,"This is my code snippet,"
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,"On the logs of my  bar  service, it is receiving these headers,
 uberctx-user-agent  and  uberctx-x-api-key"
Jaeger,54675467,nan,1,"2019/02/13, 18:51:16",False,"2019/02/25, 15:32:18",nan,4732294.0,567.0,0,599,Forward request headers to next microservice using Jaeger in SpringBoot,"I am not sure why  uber-ctx-*  is appended, I only want  x-api-key  header to be forwarded."
Jaeger,54365010,nan,1,"2019/01/25, 14:03:41",False,"2019/01/25, 16:31:14","2019/01/25, 16:31:14",3698532.0,417.0,0,281,How add variable –es.tags-as-fields.all –es.index-prefix –es.server-urls to jaeger yaml config?,I run  ./jaeger-all-in-one --es.tags-as-fields.all=true --es.index-prefix=myteam.jaeger --es.server-urls=http://ip-server:9200
Jaeger,54365010,nan,1,"2019/01/25, 14:03:41",False,"2019/01/25, 16:31:14","2019/01/25, 16:31:14",3698532.0,417.0,0,281,How add variable –es.tags-as-fields.all –es.index-prefix –es.server-urls to jaeger yaml config?,"How add the variables  --es.tags-as-fields.all ,  --es.index-prefix  and  --es.server-urls  to the YAML config ?"
Jaeger,54365010,nan,1,"2019/01/25, 14:03:41",False,"2019/01/25, 16:31:14","2019/01/25, 16:31:14",3698532.0,417.0,0,281,How add variable –es.tags-as-fields.all –es.index-prefix –es.server-urls to jaeger yaml config?,Thanks!
Jaeger,53885456,nan,1,"2018/12/21, 15:17:15",True,"2019/01/23, 13:00:46",nan,4120230.0,382.0,0,397,Traces not getting sampled in jaeger tracing,I'm new to using Jaeger tracing system and have been trying to implement it for a flask based microservices architecture.
Jaeger,53885456,nan,1,"2018/12/21, 15:17:15",True,"2019/01/23, 13:00:46",nan,4120230.0,382.0,0,397,Traces not getting sampled in jaeger tracing,Below is my jaeger client config implemented in python:
Jaeger,53885456,nan,1,"2018/12/21, 15:17:15",True,"2019/01/23, 13:00:46",nan,4120230.0,382.0,0,397,Traces not getting sampled in jaeger tracing,I read somewhere that Sampling strategy is being used to sample the number of traces especially for the trace which doesn't have any metadata.
Jaeger,53885456,nan,1,"2018/12/21, 15:17:15",True,"2019/01/23, 13:00:46",nan,4120230.0,382.0,0,397,Traces not getting sampled in jaeger tracing,"So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?"
Jaeger,53885456,nan,1,"2018/12/21, 15:17:15",True,"2019/01/23, 13:00:46",nan,4120230.0,382.0,0,397,Traces not getting sampled in jaeger tracing,"Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes."
Jaeger,53885456,nan,1,"2018/12/21, 15:17:15",True,"2019/01/23, 13:00:46",nan,4120230.0,382.0,0,397,Traces not getting sampled in jaeger tracing,I would like to understand this configuration spec more but not able to.
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,"Describe the bug 
We have a container running with envoy sidecar proxy with service/deployment for port 443 using Istio's own example: sample/https/nginx."
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,We can curl the container to get nginx page just fine but see absolutely no traces in Jaeger for https calls.
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,We see HTTP calls in Jaeger as soon as we switch the port to 80 in deployment/service
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,"Expected behavior 
We should see traces for both HTTP/HTTPS calls to the container."
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,Steps to reproduce the bug:
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,create nginx config:
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,Create nginx deployment :
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,curl -kv https://service-ip  gives 200
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,Version
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,Installation
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,"Environment 
- Running this within AWS EKS"
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,"Cluster state 
- Attached 
 archite.tar.gz"
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,Edit 1
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,yaml for service -  jaeger-query  :
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,"
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-10-02T02:32:23Z
  labels:
    app: jaeger
    chart: tracing-1.0.1
    heritage: Tiller
    jaeger-infra: jaeger-service
    release: istio
  name: jaeger-query
  namespace: istio-system
  resourceVersion: ""5259733""
  selfLink: /api/v1/namespaces/istio-system/services/jaeger-query
  uid: 6513eded-c5eb-11e8-860c-12504ba0df7c
spec:
  clusterIP: 172.20.14.251
  ports:
  - name: query-http
    port: 16686
    protocol: TCP
    targetPort: 16686
  selector:
    app: jaeger
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}"
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,Deployment :  istio-tracing  :
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,"
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""1""
  creationTimestamp: 2018-10-02T02:32:23Z
  generation: 1
  labels:
    app: istio-tracing
    chart: tracing-1.0.1
    heritage: Tiller
    release: istio
  name: istio-tracing
  namespace: istio-system
  resourceVersion: ""5259783""
  selfLink: /apis/extensions/v1beta1/namespaces/istio-system/deployments/istio-tracing
  uid: 65056099-c5eb-11e8-860c-12504ba0df7c
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: jaeger
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: """"
        sidecar.istio.io/inject: ""false""
      creationTimestamp: null
      labels:
        app: jaeger
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - ppc64le
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - s390x
            weight: 2
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
                - ppc64le
                - s390x
      containers:
      - env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: COLLECTOR_ZIPKIN_HTTP_PORT
          value: ""9411""
        - name: MEMORY_MAX_TRACES
          value: ""50000""
        image: docker.io/jaegertracing/all-in-one:1.5
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 16686
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: jaeger
        ports:
        - containerPort: 9411
          protocol: TCP
        - containerPort: 16686
          protocol: TCP
        - containerPort: 5775
          protocol: UDP
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 16686
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 10m
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: 2018-10-02T02:32:23Z
    lastUpdateTime: 2018-10-02T02:32:23Z
    message: Deployment has minimum availability."
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,"reason: MinimumReplicasAvailable
    status: ""True""
    type: Available
  - lastTransitionTime: 2018-10-02T02:32:23Z
    lastUpdateTime: 2018-10-02T02:32:27Z
    message: ReplicaSet ""istio-tracing-ff94688bb"" has successfully progressed."
Jaeger,52656987,nan,0,"2018/10/05, 04:05:03",False,"2018/10/05, 17:50:36","2018/10/05, 17:50:36",2487334.0,2769.0,0,515,Istio not reporting https traces to Jaeger,"reason: NewReplicaSetAvailable
    status: ""True""
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1"
Jaeger,52145774,52329845.0,2,"2018/09/03, 11:19:14",True,"2021/01/27, 14:59:44",nan,9973416.0,5.0,0,415,does Jaeger provide a trace api,does jaeger provide a way of querying the trace data without using the UI provided.
Jaeger,52145774,52329845.0,2,"2018/09/03, 11:19:14",True,"2021/01/27, 14:59:44",nan,9973416.0,5.0,0,415,does Jaeger provide a trace api,I'm aware that zipkin provides an API to directly access the trace data etc.
Jaeger,52145774,52329845.0,2,"2018/09/03, 11:19:14",True,"2021/01/27, 14:59:44",nan,9973416.0,5.0,0,415,does Jaeger provide a trace api,Use-case: i'm trying to use the trace data to pull together a custom report for internal purposes.
Jaeger,52145774,52329845.0,2,"2018/09/03, 11:19:14",True,"2021/01/27, 14:59:44",nan,9973416.0,5.0,0,415,does Jaeger provide a trace api,I could scrape the data from the UI but wondered if there was an easier way.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",I was developing a Spring Boot application in which the loging is done by logback and Jaeger is integrated for instrumentation.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",Myservice.java
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",logback.xml
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",pom.xml
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",The Jeager is properly connected to server and its getting the traces.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",The problem is with the logback logs.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",The traceId and spanId are not getting printed in the logs.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",But I myself found a solution for that.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",I added Spring Cloud Sleuth with my Spring Boot application.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",Now all the trace information was available in the logback log.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",But the problem is that Jaeger stopped registering traces to Jaeger server.
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together","I tried Zipkin instead of Jaeger, but the same thing happened."
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",What's wrong with my application?
Jaeger,52104427,nan,1,"2018/08/30, 22:53:08",False,"2018/09/16, 12:32:35","2018/09/16, 12:32:35",4909177.0,525.0,0,2048,"Problem with Jaeger, logback and Sleuth working together",Is something wrong with the dependencies?
Jaeger,50906213,52722089.0,1,"2018/06/18, 12:19:42",True,"2018/10/09, 16:19:28",nan,4620187.0,87.0,0,620,Does stopping jaeger collector stop OpenTracing,I have certain applications that run jaeger-client when I enable OpenTracing and start them.
Jaeger,50906213,52722089.0,1,"2018/06/18, 12:19:42",True,"2018/10/09, 16:19:28",nan,4620187.0,87.0,0,620,Does stopping jaeger collector stop OpenTracing,"First I start Jaeger collector using the command-
docker run -d -e   COLLECTOR_ZIPKIN_HTTP_PORT=9411   -p 5775:5775/udp   -p 6831:6831/udp   -p 6832:6832/udp   -p 5778:5778   -p 16686:16686   -p 14268:14268   -p 9411:9411   jaegertracing/all-in-one:latest"
Jaeger,50906213,52722089.0,1,"2018/06/18, 12:19:42",True,"2018/10/09, 16:19:28",nan,4620187.0,87.0,0,620,Does stopping jaeger collector stop OpenTracing,"Then I start the applications like user-
start.sh user -apiserver=localhost:9900 -configfile=conf/configuration.json -traceroption enabled=true 
following which they become visible as enabled services  http://localhost:16686/api/services"
Jaeger,50906213,52722089.0,1,"2018/06/18, 12:19:42",True,"2018/10/09, 16:19:28",nan,4620187.0,87.0,0,620,Does stopping jaeger collector stop OpenTracing,"The problem is that if I kill the Docker running the jaeger collector- 
systemctl stop docker
and later restart docker and jaegertracing/all-in-one, the services are no longer up at  http://localhost:16686/api/services"
Jaeger,50906213,52722089.0,1,"2018/06/18, 12:19:42",True,"2018/10/09, 16:19:28",nan,4620187.0,87.0,0,620,Does stopping jaeger collector stop OpenTracing,Does the jaeger client die on its own in absence of a Jaeger collector?
Jaeger,50906213,52722089.0,1,"2018/06/18, 12:19:42",True,"2018/10/09, 16:19:28",nan,4620187.0,87.0,0,620,Does stopping jaeger collector stop OpenTracing,Does the Jaeger collector needs to be running before starting the Jaeger clients?
Jaeger,50906213,52722089.0,1,"2018/06/18, 12:19:42",True,"2018/10/09, 16:19:28",nan,4620187.0,87.0,0,620,Does stopping jaeger collector stop OpenTracing,"If so, how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?"
Jaeger,50906213,52722089.0,1,"2018/06/18, 12:19:42",True,"2018/10/09, 16:19:28",nan,4620187.0,87.0,0,620,Does stopping jaeger collector stop OpenTracing,I wasn't able to find any clear API in RegisterRoutes method of  https://github.com/jaegertracing/jaeger/blob/master/cmd/query/app/handler.go
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,I'm running a Spring-Boot application inside a docker container and want to instrument it with OpenTracing using the Jaeger client from Uber.
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,For the instrumentation I'm using the  OpenTracing Spring Web  library in combination with the  Jaeger  client.
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,The following code snippet configures the tracer in the application:
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,I can see the traces when I run the application (not inside a Docker container) and start Jaeger with the following command:
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces.
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,After that I tried to declare the Jaeger docker container in the same docker-compose file and added a link from the  demo  service to the  jaeger  service:
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,But I still can't see any traces in the Jaeger client.
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,"For hours I have tried different approaches but didn't make any progress so far, if somebody could help me out I would greatly appreciate it!"
Jaeger,50118186,52813470.0,1,"2018/05/01, 17:20:35",True,"2018/10/15, 12:20:43",nan,9725840.0,193.0,0,481,Instrument Spring-Boot application that&#39;s executed in Docker container with Jaeger tracing,You can find my demo project on  GitHub .
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,I am hosting my application on GCP and I want to use stackdriver as my backend storage for trace spans with jaeger collectors.
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,I can't seem to find anything related to that.
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,In GCP I can find clearly that they support zipkin.
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,I am not sure what to do here.
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,Should I create some translation layer to push the data to stackdriver ?
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,Or is it supported somehow by the current zipkin connector ?
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,I truly wouldn't want to host my full tracing solution to avoid having to maintain it.
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,Can I run the Jaeger collector and somehow pass it to stackdriver ?
Jaeger,49909075,50339003.0,1,"2018/04/19, 00:11:33",True,"2018/05/15, 00:05:40","2018/04/19, 13:26:17",118116.0,611.0,0,1393,Jaeger tracing with stackdriver,Thanks
Jaeger,49624555,nan,2,"2018/04/03, 10:29:19",True,"2018/04/04, 08:31:53","2018/04/03, 10:42:23",2819181.0,81.0,0,1402,How to use jaeger-client from react component?,From react application (App.js ) imported jaeger-client.
Jaeger,49624555,nan,2,"2018/04/03, 10:29:19",True,"2018/04/04, 08:31:53","2018/04/03, 10:42:23",2819181.0,81.0,0,1402,How to use jaeger-client from react component?,import jaegerClient from 'jaeger-client'
Jaeger,49624555,nan,2,"2018/04/03, 10:29:19",True,"2018/04/04, 08:31:53","2018/04/03, 10:42:23",2819181.0,81.0,0,1402,How to use jaeger-client from react component?,"Got exception 'TypeError: _fs2.default.readFileSync is not a function' from following line of /node_modules/jaeger-client/dist/src/thrift.js:168
 
source: _fs2.default.readFileSync(_path2.default.join(__dirname, './jaeger-idl/thrift/jaeger.thrift'), 'ascii')"
Jaeger,49624555,nan,2,"2018/04/03, 10:29:19",True,"2018/04/04, 08:31:53","2018/04/03, 10:42:23",2819181.0,81.0,0,1402,How to use jaeger-client from react component?,Trying to solve it.
Jaeger,49624555,nan,2,"2018/04/03, 10:29:19",True,"2018/04/04, 08:31:53","2018/04/03, 10:42:23",2819181.0,81.0,0,1402,How to use jaeger-client from react component?,Thanks for any help.
Jaeger,49624555,nan,2,"2018/04/03, 10:29:19",True,"2018/04/04, 08:31:53","2018/04/03, 10:42:23",2819181.0,81.0,0,1402,How to use jaeger-client from react component?,"Complete package.json is like below
 
{
  ""name"": ""calculator"",
  ""version"": ""0.1.0"",
  ""private"": true,
  ""homepage"": ""http://ahfarmer.github.io/calculator"",
  ""devDependencies"": {
    ""gh-pages"": ""^1.1.0"",
    ""react-scripts"": ""^1.0.17""
  },
  ""dependencies"": {
    ""ajv"": ""^6.4.0"",
    ""ajv-keywords"": ""^3.1.0"",
    ""big.js"": ""^5.0.3"",
    ""bufferutil"": ""^3.0.3"",
    ""fs"": ""0.0.1-security"",
    ""github-fork-ribbon-css"": ""^0.2.1"",
    ""hexer"": ""^1.5.0"",
    ""jaeger-client"": ""^3.10.0"",
    ""react"": ""^16.2.0"",
    ""react-dom"": ""^16.2.0"",
    ""react-tracing"": ""^0.1.5"",
    ""thrift"": ""^0.11.0""
  },
  ""scripts"": {
    ""start"": ""react-scripts start"",
    ""build"": ""react-scripts build"",
    ""test"": ""react-scripts test --env=jsdom"",
    ""eject"": ""react-scripts eject"",
     ""deploy"": ""gh-pages -d build""
    },
    ""eslintConfig"": {
        ""extends"": ""./node_modules/react-scripts/config/eslint.js""
    }
}"
Jaeger,49624555,nan,2,"2018/04/03, 10:29:19",True,"2018/04/04, 08:31:53","2018/04/03, 10:42:23",2819181.0,81.0,0,1402,How to use jaeger-client from react component?,Forked from  https://github.com/ahfarmer/calculator  and I am trying to trace every user action ( button press ).
Jaeger,49624555,nan,2,"2018/04/03, 10:29:19",True,"2018/04/04, 08:31:53","2018/04/03, 10:42:23",2819181.0,81.0,0,1402,How to use jaeger-client from react component?,To test tracing from react.js application.
Jaeger,43766832,44337567.0,1,"2017/05/03, 20:47:57",True,"2017/08/02, 09:09:03","2017/08/02, 09:09:03",5277456.0,3.0,0,778,uber/jaeger-client-node: backend wont receive data,I'm currently looking into different openTracing Tracer-Implementations.
Jaeger,43766832,44337567.0,1,"2017/05/03, 20:47:57",True,"2017/08/02, 09:09:03","2017/08/02, 09:09:03",5277456.0,3.0,0,778,uber/jaeger-client-node: backend wont receive data,I want to use  uber/jaeger-client-node  but the backend won't receive my traces.
Jaeger,43766832,44337567.0,1,"2017/05/03, 20:47:57",True,"2017/08/02, 09:09:03","2017/08/02, 09:09:03",5277456.0,3.0,0,778,uber/jaeger-client-node: backend wont receive data,"Here is what I did:
I started the all-in-one docker image:
 docker run -d -p5775:5775/udp -p16686:16686 jaegertracing/all-in-one:latest"
Jaeger,43766832,44337567.0,1,"2017/05/03, 20:47:57",True,"2017/08/02, 09:09:03","2017/08/02, 09:09:03",5277456.0,3.0,0,778,uber/jaeger-client-node: backend wont receive data,"Next, i wrote a simple example application:
 Gist"
Jaeger,43766832,44337567.0,1,"2017/05/03, 20:47:57",True,"2017/08/02, 09:09:03","2017/08/02, 09:09:03",5277456.0,3.0,0,778,uber/jaeger-client-node: backend wont receive data,"But when I go to Jaeger UI, nothing is shown about the example service."
Jaeger,43766832,44337567.0,1,"2017/05/03, 20:47:57",True,"2017/08/02, 09:09:03","2017/08/02, 09:09:03",5277456.0,3.0,0,778,uber/jaeger-client-node: backend wont receive data,What did I do wrong?
Jaeger,43766832,44337567.0,1,"2017/05/03, 20:47:57",True,"2017/08/02, 09:09:03","2017/08/02, 09:09:03",5277456.0,3.0,0,778,uber/jaeger-client-node: backend wont receive data,Thanks
Jaeger,54371809,nan,1,"2019/01/25, 21:38:58",False,"2019/01/25, 22:56:14",nan,10969193.0,1.0,-1,209,Implementing Jaeger Open-Tracing on Websphere,I have not found a way to implement a Jaeger Open-Tracing framework implementation on the IBM Websphere Server platform.
Jaeger,54371809,nan,1,"2019/01/25, 21:38:58",False,"2019/01/25, 22:56:14",nan,10969193.0,1.0,-1,209,Implementing Jaeger Open-Tracing on Websphere,All the examples I've seen point to environment variables to be set to specify  where  to communicate to a Jaeger collection endpoint.
Jaeger,54371809,nan,1,"2019/01/25, 21:38:58",False,"2019/01/25, 22:56:14",nan,10969193.0,1.0,-1,209,Implementing Jaeger Open-Tracing on Websphere,I wanted to ask the community if anyone had experience with this.
Jaeger,52881039,nan,1,"2018/10/18, 22:15:37",False,"2018/10/19, 17:29:25","2018/10/19, 01:09:11",7089682.0,531.0,-3,126,Using centos7 as base image for Jaeger,I am trying to setup Jaeger using a CentOS base image instead of Alpine.
Jaeger,52881039,nan,1,"2018/10/18, 22:15:37",False,"2018/10/19, 17:29:25","2018/10/19, 01:09:11",7089682.0,531.0,-3,126,Using centos7 as base image for Jaeger,"The agent, collector, and Cassandra containers all work fine except for the query container."
Jaeger,52881039,nan,1,"2018/10/18, 22:15:37",False,"2018/10/19, 17:29:25","2018/10/19, 01:09:11",7089682.0,531.0,-3,126,Using centos7 as base image for Jaeger,The Jaeger repository is  here .
Jaeger,52881039,nan,1,"2018/10/18, 22:15:37",False,"2018/10/19, 17:29:25","2018/10/19, 01:09:11",7089682.0,531.0,-3,126,Using centos7 as base image for Jaeger,"After changing the base image to CentOS 7, commenting out the sections that apply to copying  ca-certificates.crt  and running  docker-compose , I get the following nil pointer error message when tailing the query container"
Jaeger,52881039,nan,1,"2018/10/18, 22:15:37",False,"2018/10/19, 17:29:25","2018/10/19, 01:09:11",7089682.0,531.0,-3,126,Using centos7 as base image for Jaeger,panic: runtime error: invalid memory address or nil pointer dereference
Jaeger,52881039,nan,1,"2018/10/18, 22:15:37",False,"2018/10/19, 17:29:25","2018/10/19, 01:09:11",7089682.0,531.0,-3,126,Using centos7 as base image for Jaeger,I ran the makefile with the necessary flags to compile the code in the app directory.
Jaeger,52881039,nan,1,"2018/10/18, 22:15:37",False,"2018/10/19, 17:29:25","2018/10/19, 01:09:11",7089682.0,531.0,-3,126,Using centos7 as base image for Jaeger,Has anyone ever setup Jaeger using CentOS as a base image?
Jaeger,52881039,nan,1,"2018/10/18, 22:15:37",False,"2018/10/19, 17:29:25","2018/10/19, 01:09:11",7089682.0,531.0,-3,126,Using centos7 as base image for Jaeger,Below is the full stack error from the container
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,I'm trying to use  OpenTracing.Contrib.NetCore  with Serilog.
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,I need to send to Jaeger my custom logs.
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,"Now, it works only when I use default logger factory  Microsoft.Extensions.Logging.ILoggerFactory"
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,My Startup:
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,and somewhere in controller:
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,in a result I will able to see that log in Jaeger UI
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,"But when I use Serilog, there are no any custom logs."
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,"I've added  UseSerilog()  to  WebHostBuilder , and all custom logs I can see in console but not in Jaeger."
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,There is open issue in  github .
Jaeger,56156809,56227367.0,1,"2019/05/15, 23:02:43",True,"2019/05/20, 22:55:44",nan,6692626.0,386.0,15,1944,OpenTracing doesn&#39;t send logs with Serilog,Could you please suggest how I can use Serilog with OpenTracing?
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","I have a Django web app served from Apache2 with mod_wsgi in docker containers running on a Kubernetes cluster in Google Cloud Platform, protected by Identity-Aware Proxy."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","Everything is working great, but I want to send GCP Stackdriver traces for all requests without writing one for each view in my project."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","I found middleware to handle this, using Opencensus."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","I went through  this documentation , and was able to manually generate traces that exported to Stackdriver Trace in my project by specifying the  StackdriverExporter  and passing the  project_id  parameter as the Google Cloud Platform  Project Number  for my project."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","Now to make this automatic for ALL requests, I followed the instructions to set up the middleware."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","In settings.py, I added the module to  INSTALLED_APPS ,  MIDDLEWARE , and set up the  OPENCENSUS_TRACE  dictionary of options."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",I also added the  OPENCENSUS_TRACE_PARAMS .
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","This works great with the default exporter 'opencensus.trace.exporters.print_exporter.PrintExporter', as I can see the Trace and Span information, including Trace ID and all details in my Apache2 web server logs."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","However, I want to send these to my Stackdriver Trace processor for analysis."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","I tried setting the  EXPORTER  parameter to  opencensus.trace.exporters.stackdriver_exporter.StackdriverExporter , which works when run manually from the shell, as long as you supply the project number."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","When it is set up to use  StackdriverExporter , the web page will not respond load, the health check starts to fail, and ultimately the web page comes back with a 502 error, stating I should try again in 30 seconds (I believe the Identity-Aware Proxy is generating this error, once it detects the failed health check), but the server generates no errors, and there are no logs in access or errors for Apache2."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","There is another dictionary in settings.py named  OPENCENSUS_TRACE_PARAMS , which I presume is needed to determine which project number the exporter should be using."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","The example has  GCP_EXPORTER_PROJECT  set as  None , and  SERVICE_NAME  set as  'my_service' ."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",What options do I need to set to get the exporter to send back to Stackdriver instead of printing to logs?
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",Do you have any idea about how I can set this up?
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",settings.py
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",Here's an example (I prettified the format for readability) of the Apache2 log when it is set to use the  PrintExporter :
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","Thanks in advance for any tips, assistance, or troubleshooting advice!"
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",Edit 2019-02-08 6:56 PM UTC:
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",I found this in the middleware:
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","The exporter is now named  StackdriverExporter , instead of  GoogleCloudExporter ."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","I set up a class in my app named  GoogleCloudExporter  that inherits  StackdriverExporter , and updated my settings.py to use  GoogleCloudExporter , but it didn't seem to work, I wonder if there is other code referencing these old naming schemes, possibly for the transport."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",I'm searching the source code for clues...
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","This at least tells me I can get rid of the ZIPKIN and JAEGER param options, as this is determined on the  EXPORTER  param."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",Edit 2019-02-08 11:58 PM UTC:
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","I scrapped Apache2 to isolate the problem and just set my docker image to use Django's built in webserver  CMD [""python"", ""/path/to/manage.py"", ""runserver"", ""0.0.0.0:80""]  and it works!"
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","When I go to the site, it writes traces to Stackdriver Trace for each request, the Span name is the module and method being executed."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","Somehow Apache2 is not being allowed to send these, but I can do so from the shell when running as root."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","I'm adding Apache2 and mod-wsgi tags to the question, because I have a funny feeling this has to do with forking child processes in Apache2 and mod-WSGI."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","Would it be the child process being unable to be created as apache2's child process is sandboxed, or could this be a permissions thing?"
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace","It seems strange, because it is just calling python modules, no external system OS binaries, that I am aware of."
Jaeger,54597464,nan,1,"2019/02/08, 19:25:59",False,"2020/06/01, 23:33:01","2019/02/09, 01:59:05",965668.0,7262.0,13,457,"Django, Apache2 on Google Kubernetes Engine writing Opencensus Traces to Stackdriver Trace",Any other ideas would be greatly appreciated!
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,"I am building a JSON validator from scratch, but I am quite stuck with the string part."
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,My hope was building a regex which would match the following sequence found on JSON.org:
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,My regex so far is:
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,It does match the criteria with a backslash following by a character and an empty string.
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,But I'm not sure how to use the UNICODE part.
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,"Is there a regex to match any UNICODE character expert "" or \ or control character?"
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,And will it match a newline or horizontal tab?
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,"The last question is because the regex match the string ""\t"", but not ""    "" (four spaces, but the idea is to be a tab)."
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,"Otherwise I will need to expand the regex with it, which is not a problem, but my guess is the horizontal tab is a UNICODE character."
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,"Thanks to Jaeger Kor, I now have the following regex:"
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,"It appears to be correct, but is there any way to check for control characters or is this unneeded as they appear on the non-printable characters on regular-expressions.info?"
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,The input to validate is always text from a textarea.
Jaeger,32155133,32155765.0,1,"2015/08/22, 13:37:16",True,"2016/11/03, 17:57:34","2015/08/22, 20:49:37",1328421.0,553.0,13,42854,Regex to match a JSON String,Update: the regex is as following in case anyone needs it:
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,Is there way to configure opentracing-spring-jaeger-cloud-starter to handle any other header than Uber-Trace-Id?
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,I have Traefik as an ingress in my kubernetes cluster.
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,Traefik can be configured to change traceContextHeaderName.
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,"Default value is ""uber-trace-id""."
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,"When I change it to some custom, there is no connection (I mean span connection) between services."
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,I believe that opentracing works only with Uber-Trace-Id.
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,Is there way to configure that?
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,I test this in minikube with Traefik as an ingress.
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,Then all requests go to spring-cloud-gateway and are propagate to services.
Jaeger,56863669,nan,1,"2019/07/03, 08:50:38",True,"2019/09/12, 20:27:02",nan,1078895.0,103.0,7,2756,How to change trace-id header in opentracing?,Thanks for help!
Jaeger,44166525,44370244.0,2,"2017/05/24, 21:58:59",True,"2018/11/19, 09:38:53",nan,3669961.0,171.0,6,8454,Spring Cloud Sleuth with OpenTracing,Is there a way to use Spring Cloud Sleuth with OpenTracing?
Jaeger,44166525,44370244.0,2,"2017/05/24, 21:58:59",True,"2018/11/19, 09:38:53",nan,3669961.0,171.0,6,8454,Spring Cloud Sleuth with OpenTracing,I want to connect Spring clients with Jaeger
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,I got the video to work on Android 4.0.3 (API 15) and up using  Jaeger 25's Html5Video plugin .
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,"However, With the addition of Android 4.4 and its revised Chromium-based WebView, this plugin no longer suffices,  as mentioned by its awesome developer ."
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,"Now, I would much rather play these video's without any plugins anyway (which works perfectly on iOS...), so I went back to trying that."
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,"Is was hoping that this Chromium-based webview would be friendlier with basic HTML5 playback, but instead, I just get the exact same error as before:  MediaPlayer(30579): Error (1,-2147483648) ."
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,"I've spent hours trying several different approaches, all to no avail."
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,I'll list a few things that I've tried below.
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,"Please, does anyone have any clues to point me in the right direction?"
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,I'm out of ideas...
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,Normal file reference using  file:///
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,Code:
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,Results in:
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,File reference using Phonegap's Filesystem API
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,Code:
Jaeger,23266043,23347674.0,1,"2014/04/24, 13:17:05",True,"2014/04/28, 20:38:39",nan,216769.0,1161.0,4,3525,Playing local/in-app mp4 files in a HTML5 videoplayer in a Cordova PhoneGap app on both Android 4.4 AND lower,"Results in (contains that same MediaPlayer (1, -2147483648) error):"
Jaeger,16491312,17730606.0,2,"2013/05/11, 00:52:54",True,"2016/03/04, 16:45:12",nan,2226079.0,1996.0,4,761,I&#39;m having trouble with limitkeypress.js and IE10,"In a project I'm working on, I have a textbox where the user has to input his name."
Jaeger,16491312,17730606.0,2,"2013/05/11, 00:52:54",True,"2016/03/04, 16:45:12",nan,2226079.0,1996.0,4,761,I&#39;m having trouble with limitkeypress.js and IE10,To avoid the user from entering numbers I used the     jquery.limitkeypress.js  library written by  Brian Jaeger  and every thing was working perfectly until I tested the website in Internet Explorer 10.
Jaeger,16491312,17730606.0,2,"2013/05/11, 00:52:54",True,"2016/03/04, 16:45:12",nan,2226079.0,1996.0,4,761,I&#39;m having trouble with limitkeypress.js and IE10,"In IE10, you can input all the letters you want, and you can not input number or weird symbols just as I wanted but when I type a space and then a letter, I see the letter print right to the space and then the space disappearing and the latter shifting to the left."
Jaeger,16491312,17730606.0,2,"2013/05/11, 00:52:54",True,"2016/03/04, 16:45:12",nan,2226079.0,1996.0,4,761,I&#39;m having trouble with limitkeypress.js and IE10,The weird thing is that if I wait like 30 seconds after typing the space to type the letter it works fine.
Jaeger,63507749,nan,1,"2020/08/20, 17:53:35",True,"2020/08/25, 18:46:03",nan,14137910.0,61.0,3,436,x-b3-sampled header is always set to 0 when accessing service through ingress controller,I have Kubernetes 1.17.5 and Istio 1.6.8 installed with demo profile.
Jaeger,63507749,nan,1,"2020/08/20, 17:53:35",True,"2020/08/25, 18:46:03",nan,14137910.0,61.0,3,436,x-b3-sampled header is always set to 0 when accessing service through ingress controller,And here is my test setup [nginx-ingress-controller] -&gt; [proxy&lt;-&gt;ServiceA] -&gt; [proxy&lt;-&gt;ServiceB]
Jaeger,63507749,nan,1,"2020/08/20, 17:53:35",True,"2020/08/25, 18:46:03",nan,14137910.0,61.0,3,436,x-b3-sampled header is always set to 0 when accessing service through ingress controller,When I'm sending requests to ingress controller I can see that ServiceA receives all required tracing headers from the proxy
Jaeger,63507749,nan,1,"2020/08/20, 17:53:35",True,"2020/08/25, 18:46:03",nan,14137910.0,61.0,3,436,x-b3-sampled header is always set to 0 when accessing service through ingress controller,Problem is  x-b3-sampled  is always set to 0 and no spans/traces are getting pushed to Jaeger
Jaeger,63507749,nan,1,"2020/08/20, 17:53:35",True,"2020/08/25, 18:46:03",nan,14137910.0,61.0,3,436,x-b3-sampled header is always set to 0 when accessing service through ingress controller,Few things I've tried
Jaeger,63507749,nan,1,"2020/08/20, 17:53:35",True,"2020/08/25, 18:46:03",nan,14137910.0,61.0,3,436,x-b3-sampled header is always set to 0 when accessing service through ingress controller,Here is the config I've tried to use
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,I am trying to make OpenTelemetry exporter to work with OpenTelemetry collector.
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,I found this  OpenTelemetry collector demo .
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,So I copied these four config files
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,to my app.
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,Also based on these two demos in open-telemetry/opentelemetry-js repo:
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,"I came up with my version (sorry for a bit long, really hard to set up a minimum working version due to the lack of docs):"
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,.env
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,docker-compose.yml
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,otel-agent-config.yaml
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,otel-collector-config.yaml
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,"After running  docker-compose up -d , I can open Jaeger (http://localhost:16686) and Zipkin UI (http://localhost:9411)."
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,And my  ConsoleSpanExporter  works in both web client and Express.js server.
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,"However, I tried this OpenTelemetry exporter code in both client and server, I am still having issue to connect OpenTelemetry collector."
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,Please see my comment about URL inside of the code
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,Any idea?
Jaeger,63485673,63486777.0,2,"2020/08/19, 14:08:36",True,"2020/08/19, 18:54:58","2020/08/19, 14:33:55",2000548.0,30911.0,3,3069,How to correctly use OpenTelemetry exporter with OpenTelemetry collector in client and server?,Thanks
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,"Input:
GCP, Kubernetes, java 11 spring boot 2 application"
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,Container is started with memory limit 1.6GB.
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,Java application is limiting memory as well -XX:MaxRAMPercentage=80.0.
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,"Under a ""heavy"" (not really) load - about 1 http request per 100 ms during about 4 hours application is killed by OOMKiller."
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,Internal diagnostic tools is showing that memory is far from limit:
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,However GCP tools is showing the following:
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,There is a suspicious that GCP is measuring something else?
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,POD contains only java app (+jaeger agent).
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,The odd thing that after restart GCP shows almost maximum memory usage instead of slowly growing if it was memory leak.
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,EDIT:
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,Docker file:
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,and run it with Kubernetes (extra details are ommited):
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,"UPDATE 
according top command memory limit is also far from limit however CPU utilization became more then 100% before container is OOMKilled."
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,Is it possible that Kubernetes kills container that is trying to get more CPU then allowed?
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,UPDATE2
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,this pmap was called not far before OOMKilled.
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,5Gb?
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,Why top is not showing this?
Jaeger,59679605,60649575.0,3,"2020/01/10, 12:12:46",True,"2020/03/12, 09:43:50","2020/01/14, 10:46:15",1004374.0,2227.0,3,724,GCP calls OOMKiller for java app however it doesn&#39;t consume max allowed memory,Also not sure how to interpretate pmap command result
Jaeger,58249869,nan,2,"2019/10/05, 18:42:47",False,"2019/10/16, 16:14:47",nan,471199.0,1143.0,3,394,Micronaut and OpenTracing of method calls,We are building a web-app using Micronaut ( v1.2.0 ) which will be deployed in a Kubernetes cluster (we are using Istio as the service-mesh).
Jaeger,58249869,nan,2,"2019/10/05, 18:42:47",False,"2019/10/16, 16:14:47",nan,471199.0,1143.0,3,394,Micronaut and OpenTracing of method calls,We would like to instrument the critical method calls so that they can generate their own spans within a HTTP request span context.
Jaeger,58249869,nan,2,"2019/10/05, 18:42:47",False,"2019/10/16, 16:14:47",nan,471199.0,1143.0,3,394,Micronaut and OpenTracing of method calls,For this we are using the Micronaut OpenTracing support and Jaeger integration.
Jaeger,58249869,nan,2,"2019/10/05, 18:42:47",False,"2019/10/16, 16:14:47",nan,471199.0,1143.0,3,394,Micronaut and OpenTracing of method calls,The following dependencies are included in the  pom.xml
Jaeger,58249869,nan,2,"2019/10/05, 18:42:47",False,"2019/10/16, 16:14:47",nan,471199.0,1143.0,3,394,Micronaut and OpenTracing of method calls,Have implemented Filter method with  @ContinueSpan  (also tried the same with  @NewSpan ) as shown below
Jaeger,58249869,nan,2,"2019/10/05, 18:42:47",False,"2019/10/16, 16:14:47",nan,471199.0,1143.0,3,394,Micronaut and OpenTracing of method calls,The following is maintained in the  application-k8s.yml  (also have an  application.yml  with the same settings)
Jaeger,58249869,nan,2,"2019/10/05, 18:42:47",False,"2019/10/16, 16:14:47",nan,471199.0,1143.0,3,394,Micronaut and OpenTracing of method calls,However we only see the trace entries that are generated by Istio (Envoy proxies) but we don't see the details of the method calls itself.
Jaeger,58249869,nan,2,"2019/10/05, 18:42:47",False,"2019/10/16, 16:14:47",nan,471199.0,1143.0,3,394,Micronaut and OpenTracing of method calls,Any ideas as to what could be going wrong here?
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,docker pull jaegertracing/jaeger-agent:latest
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,Jaeger is just for illustration.
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,But my question is more generic.
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,The above command pulls the  latest  version of the  jaeger-agent  from docker-hub.
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,The docker-hub page for this is :  https://hub.docker.com/r/jaegertracing/jaeger-agent
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,My question is how do I find the actual version of  latest  ?
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,"I looked in to the tags here, but there is not much info :
 https://hub.docker.com/r/jaegertracing/jaeger-agent/tags"
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,"Also I tried doing an  inspect  after pulling the image, but could not get necessary details."
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,docker image inspect jaegertracing/jaeger-agent:latest
Jaeger,58209785,nan,2,"2019/10/03, 00:47:51",True,"2019/10/05, 04:10:08",nan,8754016.0,105.0,3,1413,Finding the actual version of latest version of docker image,Where can we get this information from ?
Jaeger,52875462,52908915.0,1,"2018/10/18, 16:43:47",True,"2018/10/20, 21:37:17","2018/10/18, 21:30:34",1208187.0,786.0,3,2316,OpenTracing with Kafka Streams - How to?,I am trying to integrate Jaeger tracing into K-Streams.
Jaeger,52875462,52908915.0,1,"2018/10/18, 16:43:47",True,"2018/10/20, 21:37:17","2018/10/18, 21:30:34",1208187.0,786.0,3,2316,OpenTracing with Kafka Streams - How to?,I was planning to add tracing to few of my most important pipelines and was wondering what would be a good way to pass traceid from one piepline to another?
Jaeger,52875462,52908915.0,1,"2018/10/18, 16:43:47",True,"2018/10/20, 21:37:17","2018/10/18, 21:30:34",1208187.0,786.0,3,2316,OpenTracing with Kafka Streams - How to?,"Here is what I have so far - At the start of stream processing pipeline, I start a server span and save the traceid into a state store."
Jaeger,52875462,52908915.0,1,"2018/10/18, 16:43:47",True,"2018/10/20, 21:37:17","2018/10/18, 21:30:34",1208187.0,786.0,3,2316,OpenTracing with Kafka Streams - How to?,"Later on, in a transform pipeline, I access the statestore and capture the  trace from the transform() method."
Jaeger,52875462,52908915.0,1,"2018/10/18, 16:43:47",True,"2018/10/20, 21:37:17","2018/10/18, 21:30:34",1208187.0,786.0,3,2316,OpenTracing with Kafka Streams - How to?,Is this a good way to handle tracing in stream processing?
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,I am trying to integrate a legacy system with microservice hosted on Red Hat OpenShift platform.
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,The service is a java app behind ingress gateway.
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,The legacy app passes unique operation identifier as a custom header  uniqueId .
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,The microservice leverages openshift service mesh support for Jaeger so I can pass tracing headers such as  x-b3-traceid  and see the request trace in Jaeger UI.
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,"Unfortunately, the legacy app cannot be modified and won't send jaeger headers but  uniqueId  conforms jaeger rules and seems ok to be used for tracing."
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,I am trying to transform  uniqueId  into  x-b3-traceid  on an envoy filter.
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,"The problem is that I can copy it to any other header, but cannot modify  x-b3-*  headers."
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,Istio keeps generating new set of  x-b3-*  headers no matter what I do in envoy filter.
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,See filter code below.
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,"I tried different filter positions (on ingress gateway, on pod sidecar, before envoy.router, etc)."
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,Seems nothing works.
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,Can anyone recommend how can I pass custom header as a traceId for service mesh's jaeger?
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,I can create a custom proxy service transforming one header with another but it looks redundant.
Jaeger,66983636,nan,0,"2021/04/07, 12:52:57",False,"2021/04/07, 12:52:57",nan,509723.0,1670.0,2,34,Red Hat Service Mesh: custom header transformed into x-b3-traceId is lost,Is it possible to achieve that with service mesh only?
Jaeger,66454367,nan,0,"2021/03/03, 11:38:55",False,"2021/03/10, 16:47:25",nan,1749786.0,463.0,2,54,Micronaut: How to print traceId and spanId in logs?,I've been using Spring Boot for a long time.
Jaeger,66454367,nan,0,"2021/03/03, 11:38:55",False,"2021/03/10, 16:47:25",nan,1749786.0,463.0,2,54,Micronaut: How to print traceId and spanId in logs?,I'm working on Micronaut now.
Jaeger,66454367,nan,0,"2021/03/03, 11:38:55",False,"2021/03/10, 16:47:25",nan,1749786.0,463.0,2,54,Micronaut: How to print traceId and spanId in logs?,I'm used to using Sleuth to print trace and span IDs automatically on logs.
Jaeger,66454367,nan,0,"2021/03/03, 11:38:55",False,"2021/03/10, 16:47:25",nan,1749786.0,463.0,2,54,Micronaut: How to print traceId and spanId in logs?,What is the sleuth equivalent in Micronaut?
Jaeger,66454367,nan,0,"2021/03/03, 11:38:55",False,"2021/03/10, 16:47:25",nan,1749786.0,463.0,2,54,Micronaut: How to print traceId and spanId in logs?,"If there is no equivalent, how to print the trace and span IDs in Micronaut using Jaeger?"
Jaeger,64317644,nan,0,"2020/10/12, 15:11:52",False,"2020/10/12, 15:11:52",nan,10690874.0,607.0,2,117,How to inject secret value to Kubernetes crd?,I want to inject secret values to Kubernetes crd.
Jaeger,64317644,nan,0,"2020/10/12, 15:11:52",False,"2020/10/12, 15:11:52",nan,10690874.0,607.0,2,117,How to inject secret value to Kubernetes crd?,"For example, suppose I have Jaeger crd yaml file, and as the Elasticsearch server-url, password are secret values, I want them to be injected using Vault."
Jaeger,64317644,nan,0,"2020/10/12, 15:11:52",False,"2020/10/12, 15:11:52",nan,10690874.0,607.0,2,117,How to inject secret value to Kubernetes crd?,"When using Deployment, I can inject the secrets using Vault secret, by first creating secrets and loading them from envs in container."
Jaeger,64317644,nan,0,"2020/10/12, 15:11:52",False,"2020/10/12, 15:11:52",nan,10690874.0,607.0,2,117,How to inject secret value to Kubernetes crd?,"However, as crd cannot be done that way, I don't know how to inject the values from outside securely in code."
Jaeger,64317644,nan,0,"2020/10/12, 15:11:52",False,"2020/10/12, 15:11:52",nan,10690874.0,607.0,2,117,How to inject secret value to Kubernetes crd?,Any ideas?
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",I have installed istio using the official reference as on  Getting Started  page.
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?","Below are the commands i used: 
 $ curl -L https://istio.io/downloadIstio | sh -"
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",$ istioctl install --set profile=demo
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",$ kubectl label namespace default istio-injection=enabled
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",I ended up with below version of istio:
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",and my kubernetes version is:
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",Every thing seems fine until i verify the objects installed in  istio-system  namespace
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?","As, you can see there are few missing components -
There are few  pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc."
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",These components were  available in 1.4.2.
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?","In 1.4.2 installation I could see  grafana, jaeger, kiali, prometheus, zipkin dashboards."
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",But these are now  missing .
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",Example:
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",Is this expected behaviour in 1.7.2 or is my installation broken.
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?","If the installation is broken, how else can i fix it."
Jaeger,64050154,nan,1,"2020/09/24, 19:00:44",True,"2020/09/24, 22:01:08",nan,nan,nan,2,301,"istio-1.7.2 installation, missing dashboards and pods?",After all I followed the instruction from the Starter Guide.
Jaeger,62859856,62861784.0,1,"2020/07/12, 13:20:12",True,"2020/07/12, 16:39:14",nan,955379.0,654.0,2,1300,Connecting to remote Jaegertracing with C# .Net Core,Im having some issues here with Opentracing and Jaegertracing when it comes to C#.
Jaeger,62859856,62861784.0,1,"2020/07/12, 13:20:12",True,"2020/07/12, 16:39:14",nan,955379.0,654.0,2,1300,Connecting to remote Jaegertracing with C# .Net Core,"I have had this working before, but with Java projects."
Jaeger,62859856,62861784.0,1,"2020/07/12, 13:20:12",True,"2020/07/12, 16:39:14",nan,955379.0,654.0,2,1300,Connecting to remote Jaegertracing with C# .Net Core,So I start to wonder what Im missing when it comes to C# .NET Core web service.
Jaeger,62859856,62861784.0,1,"2020/07/12, 13:20:12",True,"2020/07/12, 16:39:14",nan,955379.0,654.0,2,1300,Connecting to remote Jaegertracing with C# .Net Core,This is my class to start my tracer to be used
Jaeger,62859856,62861784.0,1,"2020/07/12, 13:20:12",True,"2020/07/12, 16:39:14",nan,955379.0,654.0,2,1300,Connecting to remote Jaegertracing with C# .Net Core,Controller code that should report to the Jaeger agent and collector for show in the UI.
Jaeger,62859856,62861784.0,1,"2020/07/12, 13:20:12",True,"2020/07/12, 16:39:14",nan,955379.0,654.0,2,1300,Connecting to remote Jaegertracing with C# .Net Core,Startup.cs
Jaeger,62859856,62861784.0,1,"2020/07/12, 13:20:12",True,"2020/07/12, 16:39:14",nan,955379.0,654.0,2,1300,Connecting to remote Jaegertracing with C# .Net Core,But this is not working at all.
Jaeger,62859856,62861784.0,1,"2020/07/12, 13:20:12",True,"2020/07/12, 16:39:14",nan,955379.0,654.0,2,1300,Connecting to remote Jaegertracing with C# .Net Core,What am I missing here to get it work with a remote server?
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,I am using Spring Boot 2 Microservices with Spring Cloud Sleuth with the Dependency Management and Spring Cloud Version Greenwich.SR2.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,My service is running in an Istio service mesh.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,Sample policy of istio is set to 100 (pilot.traceSampling: 100.0).
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,"To use distributed tracing in the mesh, the applications needs to forward HTTP headers like the X-B3-TraceId and X-B3-SpanID."
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,This is achieved by simply adding Sleuth.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,All my HTTP request are are traced correctly.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,The sidecar proxies of Istio (Envoy) send the traces to the Jaeger backend.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,Sleuth is also supposed to work with Spring WebSocket.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,"But my incoming websocket requests do not get any trace or span id by sleuth; Logs look like [-,,,]."
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,1.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,Question: Why is Sleuth not working for websocket?
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,My WS-Config:
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,My clients are able to connect to my Service via Websocket.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,I am implementing WebSocketHandler interface to handle WS messages.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,"To achieve that my WS connections are logged by Sleuth, I annotate the method that handles my connection with @NewSpan:"
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,"With this, Sleuth creates trace and spanId and also propagates them to the other Services, which are called via the restTemplate in this method."
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,But HTTP calls are not send to Jaeger.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,The x-B3-Sampled Header is always set to 0 by the sidcar.
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,2 Question: Why are those traces not send to the tracing backend?
Jaeger,61733363,nan,0,"2020/05/11, 18:28:03",False,"2020/05/11, 19:23:34","2020/05/11, 19:23:34",13399972.0,105.0,2,356,Traces not sampled with Istio and Sleuth,Thank you in advance!
Jaeger,60636937,nan,1,"2020/03/11, 15:15:29",True,"2020/03/16, 14:29:14","2020/03/11, 18:51:14",12999963.0,21.0,2,542,How to trace rabbitmq end to end between producer and consumer for multiple microservices?,"I am using rabbitmq as one of the microservice and for that I want trace the rabbitmq spans,
I have used following dependencies for tracing the rabbitmq spans through opentracing,"
Jaeger,60636937,nan,1,"2020/03/11, 15:15:29",True,"2020/03/16, 14:29:14","2020/03/11, 18:51:14",12999963.0,21.0,2,542,How to trace rabbitmq end to end between producer and consumer for multiple microservices?,I am getting only producer side spans for this microservice.
Jaeger,60636937,nan,1,"2020/03/11, 15:15:29",True,"2020/03/16, 14:29:14","2020/03/11, 18:51:14",12999963.0,21.0,2,542,How to trace rabbitmq end to end between producer and consumer for multiple microservices?,producer
Jaeger,60636937,nan,1,"2020/03/11, 15:15:29",True,"2020/03/16, 14:29:14","2020/03/11, 18:51:14",12999963.0,21.0,2,542,How to trace rabbitmq end to end between producer and consumer for multiple microservices?,I want to get end to end tracing for the request which passed through multiple micro services and one of them is  rabbitmq
Jaeger,60636937,nan,1,"2020/03/11, 15:15:29",True,"2020/03/16, 14:29:14","2020/03/11, 18:51:14",12999963.0,21.0,2,542,How to trace rabbitmq end to end between producer and consumer for multiple microservices?,like  microservice1==&gt;rabbitMQ(Producer)==&gt;Microservice2==&gt;rabbitMQ(Consumer)==&gt;Response Service
Jaeger,60636937,nan,1,"2020/03/11, 15:15:29",True,"2020/03/16, 14:29:14","2020/03/11, 18:51:14",12999963.0,21.0,2,542,How to trace rabbitmq end to end between producer and consumer for multiple microservices?,How can I achieve this kind of tracing in jaeger UI?
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces",I have Kubernetes cluster with multiple Java services deployed to AWS with Istio.
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces",I built a ServerInterceptor where I had the Istio needed B3 headers to the gRPC context.
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces",I then implemented a ClientInterceptor where I parse those headers from the gRPC context and insert them into the outgoing headers.
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces",I also implemented OpenTracing Server and Client Tracing Interceptors per:  https://github.com/opentracing-contrib/java-grpc
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces","When I view the traces in the Jaeger UI, all I am seeing is spans reported by the sidecar."
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces","They are properly linked as it gets called from service to service, but they are always of type ""client."""
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces",I don't see any of the spans from when I call the database.
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces",It seems the context is not transferred from the side car to my actual app.
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces","Only sidecar to sidecar to sidecar, etc."
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces","When I run the stack locally in a simple docker compose (without kubernetes and istio) configuration, I see the server spans being created and reported."
Jaeger,60291892,nan,0,"2020/02/19, 03:51:15",False,"2020/02/19, 03:51:15",nan,2740937.0,418.0,2,199,"Istio, gRPC, not getting Server Span traces",How do I get my Java gRPC server to expand on the spans created by the Istio side cars?
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,I am trying to set up distributed event tracing throughout out microservice architecture.
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,Here is some preamble about our architecture:
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,Traefik load balancer that forwards request to the appropriate backend service based on the route pathname.
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,"Frontend application on a ""catchall"" route that is served whenever a route is not caught by another microservice."
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,Various backend services in node/dotnetcore listening on  /api/&lt;serviceName&gt;
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,"traefik is setup with the  traceContextHeaderName  set to  ""trace-id"" ."
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,"How I imagine this would work is that the frontend application receives a header ""trace-id"" from the load balancer with a value that can be used to ""link"" the spans together for requests that are related."
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,Example scenario:
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,"When a customer loads attempts to sign in, they make a request for the web application, receive and render the HTML/CSS/JS, then the subsequent requests to  /api/auth/login  can be POSTed with the login data and the value of the  ""trace-id""  header supplied by traefik."
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,"The backend service that handles the  /api/auth/login  endpoint can capture this  ""trace-id""  header value and publish some spans to jaeger related to the work that it is doing to validate the user."
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,What is happening:
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,"When the request is made for the frontend HTML, no  ""trace-id""  header is received so any subsequent spans that are published are all considered individual traces and are not linked together."
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,traefik.toml:
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,"I understand that StackOverflow is not a ""code it for me"" service."
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,I am looking for guidance on what could possibly be going wrong as I am new to distributed event tracing.
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,I have tried googling and searching for answers but I have come to a dead end.
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,Any help/suggestions on where to look would be greatly appreciated.
Jaeger,57119864,57119911.0,1,"2019/07/20, 00:40:31",True,"2019/07/20, 00:47:14",nan,1795610.0,1067.0,2,360,Trying to set up distributed event tracing,"Please let me know if I am barking up the wrong tree, approaching this incorrectly, or if my understanding of how the  traceContextHeaderName  should work is incorrect."
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,Using MassTransit.RabbitMQ v5.3.2 and OpenTracing.Contrib.NetCore v0.5.0.
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,"I'm able publish and consume events to RabbitMQ using MassTransit and I've got OpenTracing working with Jaeger, but I haven't managed to get my OpenTracing TraceIds propogated from my message publisher to my message consumer - The publisher and consumer traces have different TraceIds."
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,I've configured MassTransit with the following filter:
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,"I'm not actually sure what the listener name should be, hence ""test""."
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,The  documentation  doesn't have an example for OpenTracing.
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,"Anyways, this adds a 'Publishing Message' span to the active trace on the publish side, and automatically sets up a 'Consuming Message' trace on the consumer side; however they're separate traces."
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,How would I go about consolidating this into a single trace?
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,I could set a TraceId header using:
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,but then how would I configure my message consumer so that this is the root TraceId?
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,"Interested to see how I might do this, or if there's a different approach..."
Jaeger,56054438,56143481.0,1,"2019/05/09, 10:39:25",True,"2019/07/17, 12:26:23","2019/05/13, 01:27:09",824434.0,1520.0,2,656,propagate OpenTracing TraceIds from publisher to consumer using MassTransit.RabbitMQ,Thanks!
Jaeger,55506381,55858211.0,1,"2019/04/04, 04:00:59",True,"2019/04/26, 01:01:46",nan,1989579.0,3008.0,2,253,Istio missing metrics,"I am testing Istio 1.1, but the collection of metrics is not working correctly."
Jaeger,55506381,55858211.0,1,"2019/04/04, 04:00:59",True,"2019/04/26, 01:01:46",nan,1989579.0,3008.0,2,253,Istio missing metrics,I can not find what the problem is.
Jaeger,55506381,55858211.0,1,"2019/04/04, 04:00:59",True,"2019/04/26, 01:01:46",nan,1989579.0,3008.0,2,253,Istio missing metrics,I followed  this tutorial  and I was able to verify all the steps without problems.
Jaeger,55506381,55858211.0,1,"2019/04/04, 04:00:59",True,"2019/04/26, 01:01:46",nan,1989579.0,3008.0,2,253,Istio missing metrics,If I access prometheus I can see the log of some requests.
Jaeger,55506381,55858211.0,1,"2019/04/04, 04:00:59",True,"2019/04/26, 01:01:46",nan,1989579.0,3008.0,2,253,Istio missing metrics,"On the other hand, if I access Jaeger, I can not see any service (only 1 from Istio)"
Jaeger,55506381,55858211.0,1,"2019/04/04, 04:00:59",True,"2019/04/26, 01:01:46",nan,1989579.0,3008.0,2,253,Istio missing metrics,"Grafana is also having some strange behavior, most of the graphs do not show data."
Jaeger,54325234,54325585.0,1,"2019/01/23, 12:35:10",True,"2019/01/23, 12:54:00",nan,233632.0,3641.0,2,450,What is the Opentracing API actually and what is its role in a tracing system,On  https://opentracing.io/  they state that opentracing API is:
Jaeger,54325234,54325585.0,1,"2019/01/23, 12:35:10",True,"2019/01/23, 12:54:00",nan,233632.0,3641.0,2,450,What is the Opentracing API actually and what is its role in a tracing system,A Vendor-neutral APIs and instrumentation for distributed tracing
Jaeger,54325234,54325585.0,1,"2019/01/23, 12:35:10",True,"2019/01/23, 12:54:00",nan,233632.0,3641.0,2,450,What is the Opentracing API actually and what is its role in a tracing system,Okay great but what does that actually mean in the context of an actual application?
Jaeger,54325234,54325585.0,1,"2019/01/23, 12:35:10",True,"2019/01/23, 12:54:00",nan,233632.0,3641.0,2,450,What is the Opentracing API actually and what is its role in a tracing system,"What parts does this Opentracing API actually consist of, what is its purpose and how does it interact with other logging related systems like ""zipkin"" and ""jaeger"""
Jaeger,54325234,54325585.0,1,"2019/01/23, 12:35:10",True,"2019/01/23, 12:54:00",nan,233632.0,3641.0,2,450,What is the Opentracing API actually and what is its role in a tracing system,"Is using  Opentracing API for Java   a requirement to be able to claim ""My App supports"" opentracing?"
Jaeger,54325234,54325585.0,1,"2019/01/23, 12:35:10",True,"2019/01/23, 12:54:00",nan,233632.0,3641.0,2,450,What is the Opentracing API actually and what is its role in a tracing system,Is there one Opentracing protocol (e.g data send over the wire) or are they just saying opentracing is a middle layer which allows multiple other tracing frameworks to interoperate with each other?
Jaeger,54325234,54325585.0,1,"2019/01/23, 12:35:10",True,"2019/01/23, 12:54:00",nan,233632.0,3641.0,2,450,What is the Opentracing API actually and what is its role in a tracing system,Especially  this diagram  makes me think that.
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,I want to implement a jaeger installation with persistent storage using elasticsearch like backend on my Kubernetes cluster on Google cloud platform.
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,I am using the jaeger kubernetes templates and I am starting with elasticsearch  production setup .
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,I've downloaded and modified the  configmap.yml  file in order to change the password field value and the  elasticsearch.yml  file in order to fix the password value which I've changed.
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,My customized  .yml  files has stayed of this way:
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,configmap.yml
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,elasticsearch.yml
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,"And then, I've created the kubernetes cluster configuration with the new password value from my machine to my KGE via  kubectl  command"
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,And I've created the elasticsearch service via StatefulSet specialized pod  (also with the new password value) from my machine to my KGE via  kubectl  command
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,I can see that I have the elasticsearch service created on my GKE cluster
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,And I have the  elasticsearch-0  pod which have the docker container of elasticsearch service
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,"But when I can detail my pod on KGE, I see that my pod have some warnings and is not healthy ..."
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,I get the pod description detail and I get this warning
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,"Here, some part of my entire output to  describe  command"
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,I go to the container log section on GCP and I get the following:
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,And in the audit log section I can see something like this:
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,If I try with the original files and I change the password via KGE on GCP I get this error:
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,"After that I've create a pod, is not possible update or perform some changes?"
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,kubectl apply -f .....   ?
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,...
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,I suposse
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,How to can I change the elasticsearch password?
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,"If I want configure a persistent volume claim on this pod, can I perform this before the  kubectl create -f command and my volume and mountPath will be created on container and KGE?"
Jaeger,54093626,nan,0,"2019/01/08, 16:11:14",False,"2019/01/08, 20:00:54","2019/01/08, 20:00:54",2773461.0,2095.0,2,846,Editing configmap.yml and elasticsearch.yml in order to change password,"If somebody can point me in the correct address, their support will be highly appreciated."
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,I have made a trivial 3 tier services similar to the bookinfo app on the Istio site.
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,"Everything seems to work fine, except for the tracing with zipkin or jaeger."
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,"To clarify, I have 3 services S1, S2, S3, all pretty similar and trivial passing requests downstream and doing some work."
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,"I can see S1 and S2 in the trace, but not S3."
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,"I have narrowed this down a bit further, when i use Istio version 0.5.0, I can see S3 in the trace as well, but only after some time, however, with Istio version 0.5.1, I can only see S1 and S2 in the trace, even though the services are working properly and the calls are propagating down all the way to S3."
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,"The only difference that I can see, which I am not sure if this is even an issue or not, is this output in istio-proxy for S3 using istio version 0.5.0, but not in 0.5.1"
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,"""GET /readiness HTTP/1.1"" 200 - 0 39 1 1 ""-"" ""kube-probe/1.9+"" ""0969a5a3-f6c0-9f8e-a449-d8617c3a5f9f"" ""10.X.X.18:8080"" ""127.0.0.1:8080"""
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,I can add the exact yaml files if need.
Jaeger,48794097,nan,1,"2018/02/14, 20:39:59",False,"2019/12/30, 03:11:09","2018/02/14, 21:36:08",6475860.0,343.0,2,277,Istio Tracing Issues,"Also, I am not sure if the tracing is supposed to be coming from istio-proxy as it shows in the istio docs, but in my case, I do not see istio-proxy but rather istio-ingress only."
Jaeger,25715304,25756628.0,2,"2014/09/08, 01:21:18",True,"2014/09/10, 08:14:52","2014/09/10, 02:45:58",2827945.0,575.0,2,3200,Issues with openpgp golang gpg library,So I'm pretty new to golang and i'm struggling to get a working example going of encrypting some text with openpgp and decrypting it again.
Jaeger,25715304,25756628.0,2,"2014/09/08, 01:21:18",True,"2014/09/10, 08:14:52","2014/09/10, 02:45:58",2827945.0,575.0,2,3200,Issues with openpgp golang gpg library,Here is what I have so far: ( https://gist.github.com/93750a142d3de4e8fdd2.git )
Jaeger,25715304,25756628.0,2,"2014/09/08, 01:21:18",True,"2014/09/10, 08:14:52","2014/09/10, 02:45:58",2827945.0,575.0,2,3200,Issues with openpgp golang gpg library,This is based off of  https://github.com/jyap808/jaeger
Jaeger,25715304,25756628.0,2,"2014/09/08, 01:21:18",True,"2014/09/10, 08:14:52","2014/09/10, 02:45:58",2827945.0,575.0,2,3200,Issues with openpgp golang gpg library,"When I run it, it seems to partially work, but only outputs some of the characters of the original string... Changing the original string causes some very weird issues."
Jaeger,25715304,25756628.0,2,"2014/09/08, 01:21:18",True,"2014/09/10, 08:14:52","2014/09/10, 02:45:58",2827945.0,575.0,2,3200,Issues with openpgp golang gpg library,"Clearly there is something I'm not understanding, so would appreciate any assistance given."
Jaeger,67182181,nan,0,"2021/04/20, 18:45:43",False,"2021/04/20, 18:45:43",nan,9428538.0,829.0,1,8,How to do trace propagation with nested spans across services using Opentelemtry in python?,I am using opentelemetry api and sdk version 1.0.0 in python and Jaeger to see traces.
Jaeger,67182181,nan,0,"2021/04/20, 18:45:43",False,"2021/04/20, 18:45:43",nan,9428538.0,829.0,1,8,How to do trace propagation with nested spans across services using Opentelemtry in python?,I have two services that communicates between each other and I can see traces for each service individually on Jaeger but spans are not nested (while they should).
Jaeger,67182181,nan,0,"2021/04/20, 18:45:43",False,"2021/04/20, 18:45:43",nan,9428538.0,829.0,1,8,How to do trace propagation with nested spans across services using Opentelemtry in python?,This snippet show you what I do to propagate the trace between the services.
Jaeger,67182181,nan,0,"2021/04/20, 18:45:43",False,"2021/04/20, 18:45:43",nan,9428538.0,829.0,1,8,How to do trace propagation with nested spans across services using Opentelemtry in python?,"In previous opentelemetry versions (0.7b1), I could use directly  ctx_parent  without using  set_span_in_context  and it was working fine (I visualized nested spans on Jaeger), but unfortunately they removed the packages from pypi so I can not build anymore my project..."
Jaeger,67182181,nan,0,"2021/04/20, 18:45:43",False,"2021/04/20, 18:45:43",nan,9428538.0,829.0,1,8,How to do trace propagation with nested spans across services using Opentelemtry in python?,Thanks for any help !
Jaeger,64743246,64743914.0,1,"2020/11/08, 23:31:24",True,"2020/11/09, 02:02:44",nan,1912404.0,356.0,1,66,Select and decode blob using python cassandra driver,I am trying to query the traces Cassandra table which is part of the Jaeger architecture.
Jaeger,64743246,64743914.0,1,"2020/11/08, 23:31:24",True,"2020/11/09, 02:02:44",nan,1912404.0,356.0,1,66,Select and decode blob using python cassandra driver,As you can see the refs field is a list:
Jaeger,64743246,64743914.0,1,"2020/11/08, 23:31:24",True,"2020/11/09, 02:02:44",nan,1912404.0,356.0,1,66,Select and decode blob using python cassandra driver,from the python code:
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,"I am working on adding opentracing in our micro services, using Jaeger."
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,I have two GRPC server and one REST server.
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,The default opentracing with perfectly fine with both GRPC server and all the rest-grpc request are tracked under one parent span.
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,With Java GRPC I am able to add custom child spans and it appears in perfect hierarchy in the Jaeger UI.
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,"But When I am trying to add same custom child in Go Lang, it is not added to the parent Rest Service span which has called the GRPC service."
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,Below is the golang code
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,"I do not want to do the whole http headers extraction, as that is already taken care by GRPC library."
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,Even with java GRPC I do not do any extraction.
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,The scope manager that I use with opentracing is not available with go lang opentracing.
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,Thanks in advance!!
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,!
Jaeger,62560345,62563034.0,1,"2020/06/24, 20:00:24",True,"2020/06/27, 20:08:16","2020/06/27, 20:08:16",5427159.0,88.0,1,594,Child span in Go GRPC opentracing,Cheers.
Jaeger,60753860,61468536.0,1,"2020/03/19, 11:04:25",True,"2020/04/28, 00:11:21",nan,8575474.0,184.0,1,63,Ceph RGW unable to upload object if JaegerTracing is implemented,"OS : ubuntu 18.04
ceph : octopus
jaeger : master"
Jaeger,60753860,61468536.0,1,"2020/03/19, 11:04:25",True,"2020/04/28, 00:11:21",nan,8575474.0,184.0,1,63,Ceph RGW unable to upload object if JaegerTracing is implemented,"When I implement jaegertracer in the function that is responsibe for writing file to ceph via RGW, I am unable to upload my file Im getting this error"
Jaeger,60753860,61468536.0,1,"2020/03/19, 11:04:25",True,"2020/04/28, 00:11:21",nan,8575474.0,184.0,1,63,Ceph RGW unable to upload object if JaegerTracing is implemented,But when I remove my tracer from the code it uploads the file successfully
Jaeger,60753860,61468536.0,1,"2020/03/19, 11:04:25",True,"2020/04/28, 00:11:21",nan,8575474.0,184.0,1,63,Ceph RGW unable to upload object if JaegerTracing is implemented,source code
Jaeger,60753860,61468536.0,1,"2020/03/19, 11:04:25",True,"2020/04/28, 00:11:21",nan,8575474.0,184.0,1,63,Ceph RGW unable to upload object if JaegerTracing is implemented,When I remove the tracer it compiles fine again
Jaeger,57588551,nan,1,"2019/08/21, 12:29:50",False,"2019/09/10, 22:21:23",nan,1093496.0,391.0,1,27,generate statistics from opentracing measures,I have activated  opentracing  on my Spring Boot micro-service application using  Jaeger  as a collector and it all works fine.
Jaeger,57588551,nan,1,"2019/08/21, 12:29:50",False,"2019/09/10, 22:21:23",nan,1093496.0,391.0,1,27,generate statistics from opentracing measures,"I manage to get a full trace of my calls from different components, it is very useful to understand the calls to the application."
Jaeger,57588551,nan,1,"2019/08/21, 12:29:50",False,"2019/09/10, 22:21:23",nan,1093496.0,391.0,1,27,generate statistics from opentracing measures,"Now, in the scope of performance testing, I need to generate statistics from the different readings."
Jaeger,57588551,nan,1,"2019/08/21, 12:29:50",False,"2019/09/10, 22:21:23",nan,1093496.0,391.0,1,27,generate statistics from opentracing measures,That is e.g.
Jaeger,57588551,nan,1,"2019/08/21, 12:29:50",False,"2019/09/10, 22:21:23",nan,1093496.0,391.0,1,27,generate statistics from opentracing measures,average time of traces during a time period or number of occurrences of a specific span.
Jaeger,57588551,nan,1,"2019/08/21, 12:29:50",False,"2019/09/10, 22:21:23",nan,1093496.0,391.0,1,27,generate statistics from opentracing measures,Is there any tool to achieve that?
Jaeger,57588551,nan,1,"2019/08/21, 12:29:50",False,"2019/09/10, 22:21:23",nan,1093496.0,391.0,1,27,generate statistics from opentracing measures,Is there a standard query language/api/tool to allow to extract big numbers of opentracing metrics?
Jaeger,56135086,56584198.0,2,"2019/05/14, 19:47:08",True,"2019/06/13, 18:46:22","2019/05/15, 11:37:47",474819.0,5906.0,1,1794,get trace ID of sent request,I'm using the Open Tracing Python library for GRPC and am trying to build off of the example script here:  https://github.com/opentracing-contrib/python-grpc/blob/master/examples/trivial/trivial_client.py .
Jaeger,56135086,56584198.0,2,"2019/05/14, 19:47:08",True,"2019/06/13, 18:46:22","2019/05/15, 11:37:47",474819.0,5906.0,1,1794,get trace ID of sent request,"Once I have sent a request through the intercepted channel, how do I find the trace-id value for the request?"
Jaeger,56135086,56584198.0,2,"2019/05/14, 19:47:08",True,"2019/06/13, 18:46:22","2019/05/15, 11:37:47",474819.0,5906.0,1,1794,get trace ID of sent request,I want to use this to look at the traced data in the Jaeger UI.
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,I'm working with a few services in a kubernetes cluster.
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"I'm trying to implement telepresence to allow local debugging of code in the cluster, or potential changes in pull requests."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,The services in the cluster are running SpringBoot REST services.
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,I have a simple test case of using curl to reach a REST endpoint running in the cluster.
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,I can reach it successfully without telepresence.
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"I'm on a Win7 laptop, running an Ubuntu VM with NAT networking."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"I can run the telepresence command line, whose ""--run"" section runs ""mvn spring-boot:run""."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"This defaults to proxy method ""vpn-tcp""."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,The service appears to start up fine.
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"I can hit the service endpoint with ""localhost:8080"" successfully."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"However, if I rerun the test case to reach the service in the cluster, it fails with a 502 (Bad Gateway)."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"When I run telepresence, I can watch it replace two pods running the springboot image with a single pod running the telepresence image."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"I've looked at the detailed properties of the service and pods both before and after running telepresence, and I don't see any obvious issues in the minor differences."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"If I then kill the telepresence process, it eventually restores the original pods and my test case works again."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,Note that I'm currently doing this testing while connected to our corp network with VPN.
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"The instructions in the telepresence docs say to not mix ""vpn-tcp"" with another VPN."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,I'm not sure if this is relevant.
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"The first time I tried this test, I was in the office, not on VPN, and I saw the same results."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"I also tried changing the proxy method to ""inject-tcp""."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"This resulted in the SpringBoot service failing to start, referring to a Jaeger client that couldn't connect to a server."
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,"If it matters, here is the telepresence command that I'm executing (reverting inject-tcp change) and some initial output:"
Jaeger,55751558,nan,0,"2019/04/18, 21:05:18",False,"2020/07/23, 14:32:35","2020/07/23, 14:32:35",10508.0,11218.0,1,152,Connections to Telepresenced service get 502 errors,I'm looking for ideas to move forward from this.
Jaeger,53596298,nan,0,"2018/12/03, 16:56:33",False,"2018/12/03, 23:30:55","2018/12/03, 23:30:55",1676426.0,63.0,1,71,How to use AspectJ to profile lambda functions in Java Play Framework 2.5.x?,I have a Java application built using Play 2.5x.
Jaeger,53596298,nan,0,"2018/12/03, 16:56:33",False,"2018/12/03, 23:30:55","2018/12/03, 23:30:55",1676426.0,63.0,1,71,How to use AspectJ to profile lambda functions in Java Play Framework 2.5.x?,"I am using AspectJ with Kamon to profile methods in my selected packages, and am reporting the execution details in Jaeger."
Jaeger,53596298,nan,0,"2018/12/03, 16:56:33",False,"2018/12/03, 23:30:55","2018/12/03, 23:30:55",1676426.0,63.0,1,71,How to use AspectJ to profile lambda functions in Java Play Framework 2.5.x?,It works fine with synchronous methods but fails when it comes to reporting asynchronous ones.
Jaeger,53596298,nan,0,"2018/12/03, 16:56:33",False,"2018/12/03, 23:30:55","2018/12/03, 23:30:55",1676426.0,63.0,1,71,How to use AspectJ to profile lambda functions in Java Play Framework 2.5.x?,"For example, if I have a thenApply block which is executed after a future completes, then I cannot profile the lambda inside the thenApply in the current scheme of things."
Jaeger,53596298,nan,0,"2018/12/03, 16:56:33",False,"2018/12/03, 23:30:55","2018/12/03, 23:30:55",1676426.0,63.0,1,71,How to use AspectJ to profile lambda functions in Java Play Framework 2.5.x?,I do not want to add any extra code to my Play app and want the aspect to take care of the profiling.
Jaeger,53596298,nan,0,"2018/12/03, 16:56:33",False,"2018/12/03, 23:30:55","2018/12/03, 23:30:55",1676426.0,63.0,1,71,How to use AspectJ to profile lambda functions in Java Play Framework 2.5.x?,Any help will be greatly appreciated :)
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,I'm trying to crawl a specific page of a website ( https://www.johnlewis.com/jaeger-wool-check-knit-shift-dress-navy-check/p3767291 ) to get used to Scrapy and its features.
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,"However, I can't get Scrapy to see the 'li' that contains the thumbnail images on the carousel."
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,My  parse  Function currently looks as follows:
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,"No matter what Scrapy isn't ""seeing"" the li."
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,I've tried viewing the page in a scrapy shell to check Scrapy could see the images and they are showing up in the response for that (so I'm assuming Scrapy can definitely see the list/images in the list).
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,I've tried alternative lists and I've got a different list to work (as per the comment in the code).
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,My only thoughts are that the carousel may be loaded with JavaScript / AJAX but I can't be too sure.
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,"I do know that the list class will change if it is the selected image from ""li.thumbnail-slide"" to ""li.thumbnail-slide thumbnail-slide-active"" however, I've tried the following in my script to no avail:"
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,Nothing works.
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,Does anyone have any suggestions on what I may be doing wrong?
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,Or suggest any further reading that may help?
Jaeger,53131007,53131562.0,1,"2018/11/03, 13:47:34",True,"2018/12/15, 18:33:26","2018/12/15, 18:33:26",10600051.0,40.0,1,117,Scrapy can&#39;t see a list,Thanks in advance!
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,EDIT:  I have edited in the output of the program.
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,The program calls for estimating a given value mu.
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,"User gives a value of mu, and also provides four different numbers not equal to 1 (call them w, x, y, z)."
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,The program then attempts to find an estimate of the mu value by using the de Jaeger formula.
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,"If I enter values of 238,900 for mu, and w=14, x=102329, y=1936, z=13
then the value of estimate should be 239,103, and the error about .08%."
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,My code with the for loops works perfectly fine:
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,Output:
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,"However, with the while loops, I am unable to replicate this."
Jaeger,48592875,nan,2,"2018/02/03, 02:52:40",True,"2018/02/03, 08:59:49","2018/02/03, 03:28:44",9307576.0,11.0,1,129,Program works with for loops but not with while loops?,Output:
Jaeger,36819104,36821033.0,1,"2016/04/24, 06:53:22",True,"2016/04/24, 20:03:05","2016/04/24, 20:03:05",6246333.0,31.0,1,734,Use a date value in Excel to apply conditional formatting with VBA,I am creating an Excel sheet that will show when customers have paid and when they are late.
Jaeger,36819104,36821033.0,1,"2016/04/24, 06:53:22",True,"2016/04/24, 20:03:05","2016/04/24, 20:03:05",6246333.0,31.0,1,734,Use a date value in Excel to apply conditional formatting with VBA,"With the help of a friend, I was able to come up with a formula that will tell us if a customer is past their due date by highlighting the cell red."
Jaeger,36819104,36821033.0,1,"2016/04/24, 06:53:22",True,"2016/04/24, 20:03:05","2016/04/24, 20:03:05",6246333.0,31.0,1,734,Use a date value in Excel to apply conditional formatting with VBA,"As far as I've seen, there's no way to get conditional formatting to change the blank cell to say ""LATE"", so I was suggested to try out VBA."
Jaeger,36819104,36821033.0,1,"2016/04/24, 06:53:22",True,"2016/04/24, 20:03:05","2016/04/24, 20:03:05",6246333.0,31.0,1,734,Use a date value in Excel to apply conditional formatting with VBA,"I have been able to change the colors and add the words I need, but how to get VBA to check the dates like the conditional formatting formula?"
Jaeger,36819104,36821033.0,1,"2016/04/24, 06:53:22",True,"2016/04/24, 20:03:05","2016/04/24, 20:03:05",6246333.0,31.0,1,734,Use a date value in Excel to apply conditional formatting with VBA,Here is the formula and what I have in my VBA sheet:
Jaeger,36819104,36821033.0,1,"2016/04/24, 06:53:22",True,"2016/04/24, 20:03:05","2016/04/24, 20:03:05",6246333.0,31.0,1,734,Use a date value in Excel to apply conditional formatting with VBA,Formula
Jaeger,36819104,36821033.0,1,"2016/04/24, 06:53:22",True,"2016/04/24, 20:03:05","2016/04/24, 20:03:05",6246333.0,31.0,1,734,Use a date value in Excel to apply conditional formatting with VBA,VBA
Jaeger,36819104,36821033.0,1,"2016/04/24, 06:53:22",True,"2016/04/24, 20:03:05","2016/04/24, 20:03:05",6246333.0,31.0,1,734,Use a date value in Excel to apply conditional formatting with VBA,"I used some code I found online (thank you Rolf Jaeger from nullskull.com) for this so I'm not 100% if I need everything there, but it seems to be working fine, so I left it for now until I learn more about VBA."
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",I just started using OpenTracing with the Jaeger Cloud starter.
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",For this I added the folllowing dependency to my project:
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",The  io.opentracing.contrib.spring.integration.messaging.OpenTracingChannelInterceptor  of the  opentracing-spring-messaging  project adds the Scope (ThreadLocalScope) to the message header.
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",The Solace binder output message handler only supports header values that are instances of Serializable.
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",So it doesn't work out of the box.
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",I implemented an aspect around this message handler that looks like this (just parts of it):
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",With this hack it works.
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",What would be a proper way to avoid this hack?
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",Another implementation of TextMap?
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",Which one?
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",Update:
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace","I found a way, but I'm not sure this is the right way:"
Jaeger,63491625,nan,0,"2020/08/19, 19:55:17",False,"2020/08/21, 19:46:10","2020/08/21, 19:46:10",14132619.0,11.0,1,108,"OpenTracing, Spring Cloud Stream and Solace",}
Jaeger,60944306,nan,0,"2020/03/31, 08:59:55",False,"2020/03/31, 08:59:55",nan,5262722.0,95.0,1,37,COnfiguring Jager-agent to UDP server like FluentD using docker command,"After going through different blogs/docs, we came to know that we can skip agent and send spans to jaeger collector from client."
Jaeger,60944306,nan,0,"2020/03/31, 08:59:55",False,"2020/03/31, 08:59:55",nan,5262722.0,95.0,1,37,COnfiguring Jager-agent to UDP server like FluentD using docker command,But I am thinking of skipping collector and trying to send spans from jaeger agent to UDP server like fluentD.
Jaeger,60944306,nan,0,"2020/03/31, 08:59:55",False,"2020/03/31, 08:59:55",nan,5262722.0,95.0,1,37,COnfiguring Jager-agent to UDP server like FluentD using docker command,Is it possible ?
Jaeger,60944306,nan,0,"2020/03/31, 08:59:55",False,"2020/03/31, 08:59:55",nan,5262722.0,95.0,1,37,COnfiguring Jager-agent to UDP server like FluentD using docker command,ANy pointers will be great help
Jaeger,60846142,nan,1,"2020/03/25, 11:35:20",False,"2020/03/26, 02:44:07","2020/03/25, 12:29:48",1009437.0,109.0,1,47,can I trace sub called function with OpenCensus?,I want to trace the whole project with Opencensus and Jaeger.
Jaeger,60846142,nan,1,"2020/03/25, 11:35:20",False,"2020/03/26, 02:44:07","2020/03/25, 12:29:48",1009437.0,109.0,1,47,can I trace sub called function with OpenCensus?,I added HTTP trace in entry services and add   stratspan  in middleware surrounded whol my services and this two-span called and show on Jaeger.
Jaeger,60846142,nan,1,"2020/03/25, 11:35:20",False,"2020/03/26, 02:44:07","2020/03/25, 12:29:48",1009437.0,109.0,1,47,can I trace sub called function with OpenCensus?,My problem is each service contain a lot of function and I want see a trace of all my functions but in this way not show overall service not shown each function.
Jaeger,60846142,nan,1,"2020/03/25, 11:35:20",False,"2020/03/26, 02:44:07","2020/03/25, 12:29:48",1009437.0,109.0,1,47,can I trace sub called function with OpenCensus?,I don't like add per function add one  stratspan .
Jaeger,60846142,nan,1,"2020/03/25, 11:35:20",False,"2020/03/26, 02:44:07","2020/03/25, 12:29:48",1009437.0,109.0,1,47,can I trace sub called function with OpenCensus?,I use  ctx context.Context  entry all my function but not different!
Jaeger,58148102,58197357.0,1,"2019/09/28, 19:09:41",True,"2019/10/02, 10:55:46",nan,1579289.0,1005.0,1,54,Tracing Che on OpenShift,https://www.eclipse.org/che/docs/che-7/tracing-che/  has a number of  environment variables to set to enable tracing.
Jaeger,58148102,58197357.0,1,"2019/09/28, 19:09:41",True,"2019/10/02, 10:55:46",nan,1579289.0,1005.0,1,54,Tracing Che on OpenShift,Is it possible to set these when working on OpenShift?
Jaeger,58148102,58197357.0,1,"2019/09/28, 19:09:41",True,"2019/10/02, 10:55:46",nan,1579289.0,1005.0,1,54,Tracing Che on OpenShift,If so; where would the correct place be; and where will the Jaeger interface be available?
Jaeger,58148102,58197357.0,1,"2019/09/28, 19:09:41",True,"2019/10/02, 10:55:46",nan,1579289.0,1005.0,1,54,Tracing Che on OpenShift,Thanks
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,I am getting the following error while creating a gateway for the sample bookinfo application
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,"Internal error occurred: failed calling admission webhook
  ""pilot.validation.istio.io"": Post
   https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s :
  Address is not allowed"
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,"I have created a EKS poc cluster using two node-groups (each with two instances), one with t2.medium and another one is with t2.large type of instances in my dev AWS account using two subnets with /26 subnet with default VPC-CNI provided by EKS"
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,"But as the cluster is growing with multiple services running, I started facing issues of IPs not available (as per docs default vpc-cni driver treat pods as an EC2 instance)"
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,to avoid same I followed following post to change networking from default to weave
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,https://medium.com/codeops/installing-weave-cni-on-aws-eks-51c2e6b7abc8
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,"because of same I have resolved IPs unavailability issue,"
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,Now after network reconfiguration from vpc-cni to weave
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,I am started getting above issue as per subject line for my service mesh configured using Istio
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,"There are a couple of services running inside the mesh and also integrated kiali, prometheus, jaeger with the same."
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,"I tried to have a look at Github ( https://github.com/istio/istio/issues/9998 ) and docs
( https://istio.io/docs/ops/setup/validation/ ), but could not get a proper valid answer."
Jaeger,57292517,nan,1,"2019/07/31, 16:56:24",True,"2019/07/31, 22:41:03","2019/07/31, 21:20:16",7144790.0,66.0,1,727,Istio: failed calling admission webhook Address is not allowed,Let me if anyone face this issue and have partial/full solution on this.
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,My application (hosted in a Kubernetes cluster with Istio installed) does NOT propagate distributed tracing headers (as described  here ).
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,"My expectation is that istio-proxy should still generate a trace (consisting of a single call) that would be visible in Jaeger, even though of course the entire chain of calls would not be stitched together."
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,"However, that doesn't appear to be the case, as I'm not seeing any calls to my application in Jaeger."
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,In attempt to troubleshoot I have tried the following:
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,"I have enabled tracing in Mixer's configuration, and I can now see Mixer's activity in Jaeger UI (but no traces of calls to my application still)."
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,"I'm new to Istio, and it appears I have run out of option."
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,"First off, is my expectation correct?"
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,Am I supposed to be seeing traces - each consisting of a single call - in Jaeger UI when the application doesn't propagate distributed tracing headers?
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,"If my expectation is correct, how can I troubleshoot further?"
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,Can I somehow verify Envoy configuration and check that it's indeed tracing data to Mixer?
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,"If my expectation is incorrect, can Istio's behavior be overridden so that I get what I need?"
Jaeger,54430148,nan,0,"2019/01/29, 23:50:37",False,"2019/01/29, 23:50:37",nan,433863.0,108.0,1,91,Distributed tracing in Istio - expected behavior when the application does NOT propagate headers,Thank you.
Jaeger,54084447,54199894.0,1,"2019/01/08, 04:20:09",True,"2019/01/15, 15:32:19",nan,7013547.0,463.0,1,279,"If I use Opentracing , do I need to use NLog again?",I log application tracing informations using Jaeger.
Jaeger,54084447,54199894.0,1,"2019/01/08, 04:20:09",True,"2019/01/15, 15:32:19",nan,7013547.0,463.0,1,279,"If I use Opentracing , do I need to use NLog again?",Do I need to use other log package again?
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,"I've created an elasticsearch service to apply it like backend to jaeger tracing, using this  guide , all over Kubernetes GCP cluster."
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,I have the elasticsearch service:
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,And their respective pod called elasticsearch-0
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,"I've enter to my pod configuration on GCP, and I can see that my elasticsearch-0 pod have limited resources:"
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,"And then, I want to assign it specific CPU request and CPU limit  according to the documentation , and then, I proceed to modufy the pod manifest, adding the following directives:"
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,"- cpu ""2""  in the  args  section:"
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,"And I am including a  resources:requests  field in the container resource, in order to specify a request of 0.5 CPU and  I've include a  resources:limits  in order to specify a CPU limit of this way:"
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,"My complete pod manifest is this (See numerals 1,2,3,4 and 5 numerals commented with # symbol):"
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,"But when I apply my pod manifest file, I get the following output:"
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,The complete output of my  kubectl apply  command is this:
Jaeger,54021635,nan,1,"2019/01/03, 13:44:42",True,"2019/01/03, 21:35:20",nan,2773461.0,2095.0,1,927,Assigning specific CPU resources to pod - kubernetes.io/limit-ranger: &#39;LimitRanger plugin set: cpu request for container elasticsearch&#39;,How to can I modify my pod yaml file in order to assign it more resources and solve the  kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu request for container elasticsearch'  message?
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,What is the difference between  span.kind=server  and  span.kind=client in terms of OpenTracing?
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,How do I know which one to pick?
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,What does exactly it mean?
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,E.g.
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,there is my service Foo which is initially called by an external service Bar.
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,I start tracing on my Foo side and the logic is to call another service Buzz in my system to continue the flow chain.
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,"I would rather assume that external service Bar is a  client , but I can't start my trace from there."
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,What would you suggest - start tracing as  client  in my service Foo and then just consider all following services as servers?
Jaeger,53428114,54200196.0,1,"2018/11/22, 11:50:25",True,"2019/01/15, 15:48:26",nan,4990600.0,140.0,1,678,What is the difference between span type client and span type server?,"Setup: k8s microservices, jaeger metrics."
Jaeger,51723902,52545681.0,2,"2018/08/07, 12:55:16",True,"2018/09/28, 00:56:57",nan,6554935.0,15.0,1,699,Can istio use existing services?,I already have some services in my k8s cluster and want to mantain them separately.
Jaeger,51723902,52545681.0,2,"2018/08/07, 12:55:16",True,"2018/09/28, 00:56:57",nan,6554935.0,15.0,1,699,Can istio use existing services?,Examples:
Jaeger,51723902,52545681.0,2,"2018/08/07, 12:55:16",True,"2018/09/28, 00:56:57",nan,6554935.0,15.0,1,699,Can istio use existing services?,Is it possible to use existing instances instead of creating istio-specific ones?
Jaeger,51723902,52545681.0,2,"2018/08/07, 12:55:16",True,"2018/09/28, 00:56:57",nan,6554935.0,15.0,1,699,Can istio use existing services?,Can istio communicate with them or it's hardcoded?
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,"I have deployed ingress-nginx helm chart ( 3.20.1 ,  https://github.com/kubernetes/ingress-nginx/tree/master/charts/ingress-nginx  ) into k8s cluster."
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,Some of the Ingresses configured for applications use basic auth - which works as expected.
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,I.e.
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,"I can access some applications in the cluster without basic auth ( as configured )
and some applications require dedicated basic auth credentials ( as configured )."
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,But trying an unknown/unsupported URL/Path is handled awkward by default-backend.
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,Somehow the default backend is now requiring basic auth too ( as one of the ingress with basic-auth ) ?
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,How can I find out why/where this basic auth kicks in when it shouldn't ?
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,I do want default-backend to serve error pages for 404 without basic auth !
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,Currently deployed Ingresses:
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,E.g.
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,trying a simple curl on an unknown path ( where I would expect default backend to given 404 response ) returns data that is coming from my Prometheus endpoint being basic auth protected :
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,(The real hostname/ip has been obfuscated by me !)
Jaeger,67157641,nan,0,"2021/04/19, 10:27:32",False,"2021/04/21, 19:02:32","2021/04/21, 19:02:32",10748166.0,11.0,0,32,Ingress default backend is basic auth returns 401 instead of 404,or similar we trying on the other host I'll get 401 from something that looks like my Jaeger endpoint instead of default-backend 404:
Jaeger,67005067,nan,0,"2021/04/08, 16:29:54",False,"2021/04/08, 16:29:54",nan,517558.0,19599.0,0,15,test opentelemetry from web application to api,I have a web application APPA that calls a webservice  master  that calls another webservice  slave .
Jaeger,67005067,nan,0,"2021/04/08, 16:29:54",False,"2021/04/08, 16:29:54",nan,517558.0,19599.0,0,15,test opentelemetry from web application to api,"To corelate the traces, I'm using opentelemetry."
Jaeger,67005067,nan,0,"2021/04/08, 16:29:54",False,"2021/04/08, 16:29:54",nan,517558.0,19599.0,0,15,test opentelemetry from web application to api,"When  master  calls  slave , I can see the traces in Jaeger, but when the web application calls  master  I see that only the web application is called and no traces are corelated with the webservice and I can see separatly the traces from the 2 webservies correlated:"
Jaeger,67005067,nan,0,"2021/04/08, 16:29:54",False,"2021/04/08, 16:29:54",nan,517558.0,19599.0,0,15,test opentelemetry from web application to api,"but when I cann the web application, I see only the traces from the web application and not all the 3 components:"
Jaeger,67005067,nan,0,"2021/04/08, 16:29:54",False,"2021/04/08, 16:29:54",nan,517558.0,19599.0,0,15,test opentelemetry from web application to api,Here is what I am doing in the web application:
Jaeger,67005067,nan,0,"2021/04/08, 16:29:54",False,"2021/04/08, 16:29:54",nan,517558.0,19599.0,0,15,test opentelemetry from web application to api,What am I doing wrong in corellating all 3 components?
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,"I have a Apache Camel application with an  Aggregate EIP , which uses a  JdbcAggregationRepository  to persist the exchanges."
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,I also use the  OpenTracing component  with Jaeger to trace my application.
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,But after aggregation the parent span of the following span is wrong.
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,Jaeger UI
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,The span of route with direct  output  should be a child of one of the spans of route with direct  aggregate .
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,Spring Boot application
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,Logs
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,See also  Trace/Span Identity .
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,Research
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,"If I remove  .to(file(&quot;d:/tmp/backup&quot;))  from my route, I also lose the trace ID."
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,The span for the route with direct  output  uses a new trace ID.
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,"Therefore, it is seperated from the other spans."
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,Question
Jaeger,67001507,nan,0,"2021/04/08, 12:52:14",False,"2021/04/08, 15:31:07","2021/04/08, 15:31:07",5277820.0,12820.0,0,19,How to preserve parent span for aggregation with Apache Camel?,How can I preserve the parent span after an aggregation?
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),in a single application i can easily create (nested) spans but i am trying to trace http requests throughout multiple services and nothing i try with context propagation is working.
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),so maybe my setup is wrong.
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),can someone please explain the exact requirements for this in python?
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),"so as far as i understand, in each microservice i have to setup my span exporter, the collector and trace providers."
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),"after that is done in the classes that i want to instrument, i would just navigate to the block of code in my service that i wanna trace:"
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),when i start my service the traces work perfectly and i can see them on my Jaeger UI but the spans are not aggregated under a single trace for that one individual http request.
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),"I know there should be some context propagation in there to achieve this, but i can't find any way to get this done in python."
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),I'm using the W3C ContextTrace but i can't get it to work and i'm not really sure if i should setup a trace provider in every service or what is wrong exactly.
Jaeger,66743389,nan,0,"2021/03/22, 11:29:51",False,"2021/03/22, 11:29:51",nan,6088917.0,353.0,0,26,how to use opentelmetry with multiple microservices (python),I've read a lot of documentation but i still don't know how to get this to work.
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,I'm attempting to configure the open telemetry collector in Kubernetes.
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,I took the jaeger all in one deployment which is here:  https://www.jaegertracing.io/docs/1.22/opentelemetry/  and ported it to kubernete running on my minikube.
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,The problem is I can't seem to get the open telemetry collector to receive the jaeger traces and send it to my proxy container.
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,My jaeger all in one app seems to be working in my minikube instance.
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,Traces are being sent through the hot rap app and I can view the traces in the jaeger UI.
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,My open telemetry collector looks like the following:
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,It doesn't seem that the open-tel collector is even receiving the jaeger traces.
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,The logs from the container are below..
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,Even when I send a ton of jaeger traces nothing ever seems to be received by the collector.
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,Is there a way to debug further or a configuration I'm missing?
Jaeger,66569690,nan,0,"2021/03/10, 19:14:38",False,"2021/03/10, 19:31:12","2021/03/10, 19:31:12",769946.0,2522.0,0,51,Configuring OpenTelemetry Collector to Export Zipkin traces,Any help would be greatly appreciated.
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,I am trying to implement a solution to the heat equation (in 1-D) utilizing Python's Fast Fourier Transform (FFT).
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,The objective is an efficient numerical scheme to account for a time-dependent temperature specified at the boundary of a semi-infinite domain.
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"This is known in the heat-transfer literature as Duhamel's problem (see, e.g., Carslaw and Jaeger, 1959, sec."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,2.5).
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,An exact solution is known for the forcing at the boundary specified as a function of time.
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"In the present case, I seek the response to a boundary history given as discrete data collected at a regular interval."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"Taking the Fourier transform of the heat equation and the appropriate boundary conditions
( !"
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,[equation] https://latex.codecogs.com/gif.download?T%280%2Ct%29%20%3D%20f%28t%29%3B%20%5Clim_%7Bx%20%5Crightarrow%20%5Cinfty%7D%20T%28x%2Ct%29%20%3D%200  ) and initial condition ( !
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"[equation] https://latex.codecogs.com/gif.download?T%28x%2C0%29%20%3D%200  ) with respect to time, the problem has a solution given by !"
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"[equation] https://latex.codecogs.com/gif.download?%5Ctilde%7BT%7D%20%3D%20%5Ctilde%7Bf%7D%28%5Calpha%29%20%5Cexp%20%5B-%28i%20%5Calpha%20/%20%5Ckappa%29%5E%7B1/2%7D%20x%5D  , where the tilde indicates the Fourier transform of the function, alpha is the transform variable (frequency), and kappa is the thermal diffusivity."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"So, the scheme is to take the FFT of the boundary data, multiply by
!"
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"[equation] https://latex.codecogs.com/gif.download?%5Cexp%20%5B%20-%20%28i%20%5Calpha%20/%20%5Ckappa%29%5E%7B1/2%7D%20x%5D  , and invert (using IFFT) to obtain T."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,I have constructed a benchmark problem with which to validate my coding.
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"In particular, I consider the forcing function !"
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"[equation] https://latex.codecogs.com/gif.download?f%28t%29%20%3D%20a%20%5Cexp%20%5B%20-%20b%5E2%20%28t-t_0%29%5E2%5D  , where a, b, and t_0 are fixed constants."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,This function can be put into the classical Duhamel solution and the result can be evaluated by numerical integration.
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"In addition, the FT of this function has a simple, closed form, which can be substituted into the solution given in the foregoing paragraph, and the FT of the solution can be inverted by direct numerical integration."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"Results of these two independent evaluations are identical, verifying the FT solution given above."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,I have attempted to code this problem for the above forcing function given by discrete data sampled at a regular interval of time.
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"I call Python's FFT routine to generate
the FT of the forcing function from the data."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"Then, I multiply by the decaying exponential in x, and finally call IFFT to invert for the temperature history at a given fixed value of x."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"The code returns values that exhibit some of the right qualitative behavior (e.g., delayed arrival of the peak temperature;  temperature tailing off at late time), but is way off both quantitatively (e.g., temperature much too high) and qualitatively (e.g., temperature at early time is not asymptotically zero)."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,"I am clearly doing something wrong in my coding of this problem, but I have been unable to identify the issue."
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,A possible error may be found in the way I specify the frequency in the decaying exponential.
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,I would greatly appreciate any help that someone more familiar with the use of Python's FFT and IFFT can offer.
Jaeger,66324797,nan,0,"2021/02/23, 01:14:30",False,"2021/02/24, 06:35:50","2021/02/24, 06:35:50",5999520.0,1.0,0,42,problem coding a solution to the heat equation using Python&#39;s FFT and IFFT,My code follows:
Jaeger,66213790,nan,1,"2021/02/15, 20:54:39",True,"2021/02/19, 11:41:07",nan,12530530.0,699.0,0,81,Can we trace external API calls with Istio behind proxy via Kiali?,We have a Nodejs based microservices running in our on-prem kubernetes v1.19 with Istio v1.8.0.
Jaeger,66213790,nan,1,"2021/02/15, 20:54:39",True,"2021/02/19, 11:41:07",nan,12530530.0,699.0,0,81,Can we trace external API calls with Istio behind proxy via Kiali?,What I would like to achieve is trace or display the external API calls in Kiali where we have Jaeger clients for each microservices and able to trace internal traffics.
Jaeger,66213790,nan,1,"2021/02/15, 20:54:39",True,"2021/02/19, 11:41:07",nan,12530530.0,699.0,0,81,Can we trace external API calls with Istio behind proxy via Kiali?,But so far I could not able to trace any external API calls hits from any microservices.The only thing that I can see the traffic for proxy in Kiali's graph overview.
Jaeger,66213790,nan,1,"2021/02/15, 20:54:39",True,"2021/02/19, 11:41:07",nan,12530530.0,699.0,0,81,Can we trace external API calls with Istio behind proxy via Kiali?,"We have a cooperate proxy, and each container have env proxies set for both http_proxy, https_proxy.Any external service accessible via a cooperate proxy thus traffics should go through the our cooperate proxy first."
Jaeger,66213790,nan,1,"2021/02/15, 20:54:39",True,"2021/02/19, 11:41:07",nan,12530530.0,699.0,0,81,Can we trace external API calls with Istio behind proxy via Kiali?,We have a secured gateway with TLS and we do not have egressgateway where only have istio-ingressgateway.
Jaeger,66213790,nan,1,"2021/02/15, 20:54:39",True,"2021/02/19, 11:41:07",nan,12530530.0,699.0,0,81,Can we trace external API calls with Istio behind proxy via Kiali?,So is there anyway to trace external traffics likewise the internal traffics inside cluster?If yes what might be the missing thing?
Jaeger,66213790,nan,1,"2021/02/15, 20:54:39",True,"2021/02/19, 11:41:07",nan,12530530.0,699.0,0,81,Can we trace external API calls with Istio behind proxy via Kiali?,Here are the ServiceEntries and VirtualServices that I created where I would like to use the retry feature as well the calls for proxy and externalAPI
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,I am trying to use OpenCensus and Linkerd.
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,"Though Linkerd has an option to automatically provision OpenCensus and jaeger in its namespace, I don't want to use them."
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,"Instead, I deployed them independently by myself under the namespace named 'ops'."
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,"At the end (exactly 4th line from the last) of the the official  docs , it says,"
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,Ensure the OpenCensus collector is injected with the Linkerd proxy.
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,What does this mean?
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,Should I inject linkerd sidecar into OpenCensus collector pod?
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,"If so, why?"
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,"For example, let's say I've configured the default namespace like this."
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,"my-opencensus-collector  is in  ops  namespace, so I put  .ops  at the end of its service name, resulting  my-opencensus-collector.ops:12345 ."
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,"And the dedicated service account for the OpenCensus collector exists in  ops  namespace, too."
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,"In this case, should I put the namespace name at the end of service account name as well?"
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,Which one would be right?
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,or
Jaeger,65377607,nan,1,"2020/12/20, 08:58:29",False,"2020/12/29, 01:18:49",nan,8700958.0,183.0,0,68,Linkerd distributed tracing with OpenCensus,Thanks!
Jaeger,65108719,nan,0,"2020/12/02, 15:07:41",False,"2020/12/02, 15:07:41",nan,6223368.0,53.0,0,51,How can I send traces directly to webservice without using opentelemtry collector using oltp exporter in http protocol?,"I am trying to send 2 different microservice data to my web service using open telemetry-javaagent ,one with jaeger exporter and the other with otlp, it looks like all the traces of jaeger are sent successfully and otlp are dropped as my web service only support HTTP protocol And javaagent has otlp grpc exporter."
Jaeger,65108719,nan,0,"2020/12/02, 15:07:41",False,"2020/12/02, 15:07:41",nan,6223368.0,53.0,0,51,How can I send traces directly to webservice without using opentelemtry collector using oltp exporter in http protocol?,I am not able to find any other agent which would exporter data using otlp in HTTP format.
Jaeger,65108719,nan,0,"2020/12/02, 15:07:41",False,"2020/12/02, 15:07:41",nan,6223368.0,53.0,0,51,How can I send traces directly to webservice without using opentelemtry collector using oltp exporter in http protocol?,are there any references through which I can get such an agent
Jaeger,64223755,nan,0,"2020/10/06, 13:15:39",False,"2020/10/06, 13:15:39",nan,3129897.0,35.0,0,22,Open tracing cannot track Cassandra (micro services with Java 11),"Everyone,"
Jaeger,64223755,nan,0,"2020/10/06, 13:15:39",False,"2020/10/06, 13:15:39",nan,3129897.0,35.0,0,22,Open tracing cannot track Cassandra (micro services with Java 11),I have the following inquiry:
Jaeger,64223755,nan,0,"2020/10/06, 13:15:39",False,"2020/10/06, 13:15:39",nan,3129897.0,35.0,0,22,Open tracing cannot track Cassandra (micro services with Java 11),"Java 11 is used along with Spring-boot, Cassandra 3.4, JRPC and Jaeger."
Jaeger,64223755,nan,0,"2020/10/06, 13:15:39",False,"2020/10/06, 13:15:39",nan,3129897.0,35.0,0,22,Open tracing cannot track Cassandra (micro services with Java 11),"All queries to the database are made solely by Spring, with annotation."
Jaeger,64223755,nan,0,"2020/10/06, 13:15:39",False,"2020/10/06, 13:15:39",nan,3129897.0,35.0,0,22,Open tracing cannot track Cassandra (micro services with Java 11),"Thus, there are no further queries made, clusters or sessions in the code."
Jaeger,64223755,nan,0,"2020/10/06, 13:15:39",False,"2020/10/06, 13:15:39",nan,3129897.0,35.0,0,22,Open tracing cannot track Cassandra (micro services with Java 11),How to track Cassandra with Jaeger?
Jaeger,64174906,nan,2,"2020/10/02, 19:13:20",False,"2020/10/02, 19:31:24","2020/10/02, 19:24:59",14380750.0,1.0,0,121,&quot;Configuration config = new Configuration&quot;: &#39;Configuration&#39; is a namespace but is used like a type,very new to .net and this seems like a simple questions.
Jaeger,64174906,nan,2,"2020/10/02, 19:13:20",False,"2020/10/02, 19:31:24","2020/10/02, 19:24:59",14380750.0,1.0,0,121,&quot;Configuration config = new Configuration&quot;: &#39;Configuration&#39; is a namespace but is used like a type,Trying to get this code to work from the Jaeger website:  https://ocelot.readthedocs.io/en/latest/features/tracing.html
Jaeger,64174906,nan,2,"2020/10/02, 19:13:20",False,"2020/10/02, 19:31:24","2020/10/02, 19:24:59",14380750.0,1.0,0,121,&quot;Configuration config = new Configuration&quot;: &#39;Configuration&#39; is a namespace but is used like a type,in the startup.cs file under the ConfigureServices.
Jaeger,64174906,nan,2,"2020/10/02, 19:13:20",False,"2020/10/02, 19:31:24","2020/10/02, 19:24:59",14380750.0,1.0,0,121,&quot;Configuration config = new Configuration&quot;: &#39;Configuration&#39; is a namespace but is used like a type,"I have added the proper references, but am getting an error with the:"
Jaeger,64174906,nan,2,"2020/10/02, 19:13:20",False,"2020/10/02, 19:31:24","2020/10/02, 19:24:59",14380750.0,1.0,0,121,&quot;Configuration config = new Configuration&quot;: &#39;Configuration&#39; is a namespace but is used like a type,Visual studio keeps complaining:
Jaeger,64174906,nan,2,"2020/10/02, 19:13:20",False,"2020/10/02, 19:31:24","2020/10/02, 19:24:59",14380750.0,1.0,0,121,&quot;Configuration config = new Configuration&quot;: &#39;Configuration&#39; is a namespace but is used like a type,'Configuration' is a namespace but is used like a type
Jaeger,64174906,nan,2,"2020/10/02, 19:13:20",False,"2020/10/02, 19:31:24","2020/10/02, 19:24:59",14380750.0,1.0,0,121,&quot;Configuration config = new Configuration&quot;: &#39;Configuration&#39; is a namespace but is used like a type,Any ideas on how to fix.
Jaeger,64174906,nan,2,"2020/10/02, 19:13:20",False,"2020/10/02, 19:31:24","2020/10/02, 19:24:59",14380750.0,1.0,0,121,&quot;Configuration config = new Configuration&quot;: &#39;Configuration&#39; is a namespace but is used like a type,Here is what I have in startup so far:
Jaeger,63843312,63843385.0,2,"2020/09/11, 11:16:39",True,"2020/09/11, 11:43:16",nan,9291851.0,669.0,0,62,Link docker containers in the Dockerfile,I have a  Jaeger  running in a docker container in my local machine.
Jaeger,63843312,63843385.0,2,"2020/09/11, 11:16:39",True,"2020/09/11, 11:43:16",nan,9291851.0,669.0,0,62,Link docker containers in the Dockerfile,I've created a sample app which sends trace data to Jaeger.
Jaeger,63843312,63843385.0,2,"2020/09/11, 11:16:39",True,"2020/09/11, 11:43:16",nan,9291851.0,669.0,0,62,Link docker containers in the Dockerfile,"When running from the IDE, the data is sent perfectly."
Jaeger,63843312,63843385.0,2,"2020/09/11, 11:16:39",True,"2020/09/11, 11:43:16",nan,9291851.0,669.0,0,62,Link docker containers in the Dockerfile,"I've containerized my app, and now I'm deploying it as a container, but the communication only works when I use  --link jaeger  to link both containers (expected)."
Jaeger,63843312,63843385.0,2,"2020/09/11, 11:16:39",True,"2020/09/11, 11:43:16",nan,9291851.0,669.0,0,62,Link docker containers in the Dockerfile,My question is:
Jaeger,63843312,63843385.0,2,"2020/09/11, 11:16:39",True,"2020/09/11, 11:43:16",nan,9291851.0,669.0,0,62,Link docker containers in the Dockerfile,"Is there a way of adding the  --link  parameter within my Dockerfile, so then I don't need to specify it when running the  docker run  command?"
Jaeger,63705587,nan,0,"2020/09/02, 15:31:14",False,"2020/09/03, 01:06:24","2020/09/03, 01:06:24",13310948.0,3.0,0,169,Which deployment strategy for Jaegertracing on AWS and Lambda,I'm looking for the best strategy to deploy Jaegertracing on AWS.
Jaeger,63705587,nan,0,"2020/09/02, 15:31:14",False,"2020/09/03, 01:06:24","2020/09/03, 01:06:24",13310948.0,3.0,0,169,Which deployment strategy for Jaegertracing on AWS and Lambda,"My goal is to trace my Lambda functions (round about 10), which send the spans directly to the Collector over HTTP."
Jaeger,63705587,nan,0,"2020/09/02, 15:31:14",False,"2020/09/03, 01:06:24","2020/09/03, 01:06:24",13310948.0,3.0,0,169,Which deployment strategy for Jaegertracing on AWS and Lambda,Therefore I don't need the Jaeger-Agent.
Jaeger,63705587,nan,0,"2020/09/02, 15:31:14",False,"2020/09/03, 01:06:24","2020/09/03, 01:06:24",13310948.0,3.0,0,169,Which deployment strategy for Jaegertracing on AWS and Lambda,My question is now how I should deploy the jaeger-query and collector to AWS.
Jaeger,63705587,nan,0,"2020/09/02, 15:31:14",False,"2020/09/03, 01:06:24","2020/09/03, 01:06:24",13310948.0,3.0,0,169,Which deployment strategy for Jaegertracing on AWS and Lambda,Should I rather deploy them via ECS or via an EKS-Cluster (Jaeger Operator) or is there another solution I didn't have mentioned yet?
Jaeger,63705587,nan,0,"2020/09/02, 15:31:14",False,"2020/09/03, 01:06:24","2020/09/03, 01:06:24",13310948.0,3.0,0,169,Which deployment strategy for Jaegertracing on AWS and Lambda,What is the advantage of Kubernetes over ECS in this case?
Jaeger,63705587,nan,0,"2020/09/02, 15:31:14",False,"2020/09/03, 01:06:24","2020/09/03, 01:06:24",13310948.0,3.0,0,169,Which deployment strategy for Jaegertracing on AWS and Lambda,Thanks in advance for your feedback!
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,"I want to use  static target  for jaeger instead of linking to a dynamic target,
static target compiles fine but when I use it in my codebase, I see  undefined reference errors :
tried:"
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,"on  using **nm -uC libjaegertracing.a**  gives  hint for missing definitions , but I don't know why it is not getting included in the static file, while when I link to static target for example code(standalone) not with code I want to link it with, everything works fine."
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,I want to know how can I know and include missing library/definition so that I don't get these errors while trying to use a manually created target with libjaegertracing.a
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,"also to verify whether I had the dependency library compiled to build static library, I used:"
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,which gave the complied files as expected:
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,"target that builds fine: [works]
for adding dependency:"
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,for adding source files:
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,linking with example code:
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,with source code  [ where it doesn't work]
Jaeger,63672697,nan,0,"2020/08/31, 17:27:03",False,"2020/08/31, 18:17:27","2020/08/31, 18:17:27",8329150.0,43.0,0,95,how to find missing library dependency(or root cause) in static library and when cmake target is built using externalproject method,then:
Jaeger,62320724,nan,0,"2020/06/11, 12:00:53",False,"2020/06/11, 12:00:53",nan,417045.0,6683.0,0,75,Spring Sleuth ignore &quot;get /**&quot; either by pattern or &quot;mvc.controller.class&quot;,"TLDR : How can we ignore sending traces to jaeger (or zipkin) where the header is ""get /**"", and if that is not possible ignore sending traces where  ""mvc.controller.class"" is ""RequestHttpRequestHandler"""
Jaeger,62320724,nan,0,"2020/06/11, 12:00:53",False,"2020/06/11, 12:00:53",nan,417045.0,6683.0,0,75,Spring Sleuth ignore &quot;get /**&quot; either by pattern or &quot;mvc.controller.class&quot;,"We have a app which uses spring sleuth, in this app we have some requests sent to jaeger which result in get the header as ""get /**"" and the controller for these cases are ""RequestHttpRequestHandler"""
Jaeger,62320724,nan,0,"2020/06/11, 12:00:53",False,"2020/06/11, 12:00:53",nan,417045.0,6683.0,0,75,Spring Sleuth ignore &quot;get /**&quot; either by pattern or &quot;mvc.controller.class&quot;,"However it seems that we cant use a simply ""**"" path filter and after debugging the spring code a bit each specific pattern comes up in the skip section as follows"
Jaeger,61654705,nan,0,"2020/05/07, 12:47:03",False,"2020/05/07, 12:47:03",nan,5950320.0,23.0,0,47,SignalFx Ruby tracing: encoding error while sending spans,In my Rails app I'm trying to set up auto-instrumentation with SignalFx using  signalfx-ruby-tracing  gem which is using  jaeger-client-ruby .
Jaeger,61654705,nan,0,"2020/05/07, 12:47:03",False,"2020/05/07, 12:47:03",nan,5950320.0,23.0,0,47,SignalFx Ruby tracing: encoding error while sending spans,In the output of  rails sever  I'm getting this error every 30 seconds or so:
Jaeger,61654705,nan,0,"2020/05/07, 12:47:03",False,"2020/05/07, 12:47:03",nan,5950320.0,23.0,0,47,SignalFx Ruby tracing: encoding error while sending spans,"ERROR -- : Failure while sending a batch of spans: ""\x81"" from
  ASCII-8BIT to UTF-8"
Jaeger,61654705,nan,0,"2020/05/07, 12:47:03",False,"2020/05/07, 12:47:03",nan,5950320.0,23.0,0,47,SignalFx Ruby tracing: encoding error while sending spans,which comes from  this part of code .
Jaeger,61654705,nan,0,"2020/05/07, 12:47:03",False,"2020/05/07, 12:47:03",nan,5950320.0,23.0,0,47,SignalFx Ruby tracing: encoding error while sending spans,The traces are sent but I'd like to know why does this error happen because it might impact the quality of the monitoring.
Jaeger,61654705,nan,0,"2020/05/07, 12:47:03",False,"2020/05/07, 12:47:03",nan,5950320.0,23.0,0,47,SignalFx Ruby tracing: encoding error while sending spans,Thanks!
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,My pods in my kubernetes cluster crashing after startup.
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,"They are in a separate namespace (not default)
I am using microk8s and tilt"
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,"*2020-04-21 22:38:48.766  INFO 8 --- [           main] o.s.web.context.ContextLoader 
        : Root WebApplicationContext: initialization completed in 29903 ms"
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,xargs: java: terminated by signal 9
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,[K8s EVENT: Pod com-config-6867587f48-9zsxs (ns: common-dev)] Back-off restarting failed container
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,[K8s EVENT: Pod com-config-6867587f48-9zsxs (ns: common-dev)] Back-off restarting failed container
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,warte auf debugger: n
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,Listening for transport dt_socket at address: 5005*
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,My microk8s.status
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,"dashboard: enabled
dns: enabled
ingress: enabled
registry: enabled
storage: enabled
cilium: disabled
fluentd: disabled
gpu: disabled
helm: disabled
helm3: disabled
istio: disabled
jaeger: disabled
knative: disabled
kubeflow: disabled
linkerd: disabled
metallb: disabled
metrics-server: disabled
prometheus: disabled
rbac: disabled"
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,Any suggestions why my pods don´t get a signal and Signal 9 comes up?
Jaeger,61353221,nan,0,"2020/04/22, 00:04:03",False,"2020/04/22, 00:04:03",nan,13375516.0,1.0,0,49,Pods terminated by signal 9 - Kubernetes - Microk8s,"(Port-Forwarding is set, Firewall rules are set)"
Jaeger,60462137,nan,1,"2020/02/29, 07:02:39",True,"2020/03/01, 14:12:35",nan,8575474.0,184.0,0,347,Jaegerw with yaml-cpp Linking Errror,I have a c++ program which using Jaeger for tracing
Jaeger,60462137,nan,1,"2020/02/29, 07:02:39",True,"2020/03/01, 14:12:35",nan,8575474.0,184.0,0,347,Jaegerw with yaml-cpp Linking Errror,"I compile it using 
 g++ -std=c++1z test.cpp -I /usr/local/lib/ -ljaegertracing -lyaml-cpp  where
 /usr/local/lib/libyaml-cpp.a  is the installation path."
Jaeger,60462137,nan,1,"2020/02/29, 07:02:39",True,"2020/03/01, 14:12:35",nan,8575474.0,184.0,0,347,Jaegerw with yaml-cpp Linking Errror,Error message -
Jaeger,60462137,nan,1,"2020/02/29, 07:02:39",True,"2020/03/01, 14:12:35",nan,8575474.0,184.0,0,347,Jaegerw with yaml-cpp Linking Errror,"I have installed  yaml-cpp-0.6.0  by downloading source code  .tar  version did  mkdir build , cd build , sudo make , sudo make install"
Jaeger,60462137,nan,1,"2020/02/29, 07:02:39",True,"2020/03/01, 14:12:35",nan,8575474.0,184.0,0,347,Jaegerw with yaml-cpp Linking Errror,I dont know why my compilation is failing.
Jaeger,60462137,nan,1,"2020/02/29, 07:02:39",True,"2020/03/01, 14:12:35",nan,8575474.0,184.0,0,347,Jaegerw with yaml-cpp Linking Errror,I have  libyaml-cppd.so.0.6  in  yaml-cpp/build  directory and tried this path to compile but still it is failing.
Jaeger,60462137,nan,1,"2020/02/29, 07:02:39",True,"2020/03/01, 14:12:35",nan,8575474.0,184.0,0,347,Jaegerw with yaml-cpp Linking Errror,OS - ubuntu 18.04
Jaeger,59276096,nan,1,"2019/12/11, 00:09:39",True,"2019/12/11, 13:47:46",nan,10276821.0,13.0,0,424,Prometheus Operator fails after Istio install,I'm trying to install the prometheus operator and inject using the sidecar.
Jaeger,59276096,nan,1,"2019/12/11, 00:09:39",True,"2019/12/11, 13:47:46",nan,10276821.0,13.0,0,424,Prometheus Operator fails after Istio install,Mutual TLS is turned on and works okay for Jaeger.
Jaeger,59276096,nan,1,"2019/12/11, 00:09:39",True,"2019/12/11, 13:47:46",nan,10276821.0,13.0,0,424,Prometheus Operator fails after Istio install,For the operator though we get a failure on the oper-admission job (see image).
Jaeger,59276096,nan,1,"2019/12/11, 00:09:39",True,"2019/12/11, 13:47:46",nan,10276821.0,13.0,0,424,Prometheus Operator fails after Istio install,"I believe Istio is causing this as if I release prometheus-operator prior to istio or without istio it works okay, but then it isn't injected."
Jaeger,59276096,nan,1,"2019/12/11, 00:09:39",True,"2019/12/11, 13:47:46",nan,10276821.0,13.0,0,424,Prometheus Operator fails after Istio install,I've tried setting the following in the istio operator sidecar settings:
Jaeger,59276096,nan,1,"2019/12/11, 00:09:39",True,"2019/12/11, 13:47:46",nan,10276821.0,13.0,0,424,Prometheus Operator fails after Istio install,I've also tried to extend the readinessInitialDelaySeconds to 10s but still get the error.
Jaeger,59276096,nan,1,"2019/12/11, 00:09:39",True,"2019/12/11, 13:47:46",nan,10276821.0,13.0,0,424,Prometheus Operator fails after Istio install,Does anyone else have any ideas?
Jaeger,58999288,59144234.0,1,"2019/11/22, 19:44:10",True,"2019/12/02, 19:50:35","2019/12/02, 19:44:46",1264920.0,3020.0,0,658,Tracer sending a span,I am trying to use instrumentation in Go with Jaeger.
Jaeger,58999288,59144234.0,1,"2019/11/22, 19:44:10",True,"2019/12/02, 19:50:35","2019/12/02, 19:44:46",1264920.0,3020.0,0,658,Tracer sending a span,I am running the Jaeger backend with docker like this (as explained in  https://www.jaegertracing.io/docs/1.15/getting-started/ ):
Jaeger,58999288,59144234.0,1,"2019/11/22, 19:44:10",True,"2019/12/02, 19:50:35","2019/12/02, 19:44:46",1264920.0,3020.0,0,658,Tracer sending a span,"However, after running the following Go code, I am not able to see spans in the Jaeger UI at  http://localhost:16686  and I am not sure what's wrong with this code?"
Jaeger,58999288,59144234.0,1,"2019/11/22, 19:44:10",True,"2019/12/02, 19:50:35","2019/12/02, 19:44:46",1264920.0,3020.0,0,658,Tracer sending a span,I started from a similar piece of Python code and that is able to publish spans on the Jaeger UI.
Jaeger,58999288,59144234.0,1,"2019/11/22, 19:44:10",True,"2019/12/02, 19:50:35","2019/12/02, 19:44:46",1264920.0,3020.0,0,658,Tracer sending a span,I am digging in the docs here  https://godoc.org/github.com/uber/jaeger-client-go  and the ones for the open tracing project here  https://godoc.org/github.com/opentracing/opentracing-go  but I am a bit confused by the jargon and the library functions/methods.
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,I'm trying out the  Jaeger/OpenTracing tutorial  and finding that none of my changes to the HotROD application code have any effect.
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,The project structure is something like (abridged):
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,I start the application by running  go run main.go all .
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,"It behaves as expected, the traces on Jaeger all match the screenshots on Medium."
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,I edit  services/config/config.go  to change the RouteWorkerPoolSize and MySQLGetDelay variables as directed.
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,Then stop the server and start it again with  go run main.go all
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,"I'd expect these changes to be reflected in the newly running server, but they aren't."
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,The behaviour is the exact same as before.
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,It's like go is running the old code.
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,Am I misunderstanding something about  go run ?
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,Environment variables:
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,working directory:
Jaeger,57068664,nan,1,"2019/07/17, 07:46:02",True,"2019/07/17, 08:00:18","2019/07/17, 07:57:29",3641332.0,69.0,0,41,go run uses stale versions of sub-packages,Go version 1.12.6 running on Kubuntu 18.04
Jaeger,54134885,54185032.0,1,"2019/01/10, 20:31:29",True,"2019/01/14, 18:14:37",nan,4151103.0,169.0,0,1112,Disable opentracing globally,I am using Jaeger opentracing in an instrumented standalone non-spring java app.
Jaeger,54134885,54185032.0,1,"2019/01/10, 20:31:29",True,"2019/01/14, 18:14:37",nan,4151103.0,169.0,0,1112,Disable opentracing globally,Does opentacing/Jaeger expose any config or api or any other mechanism to disable it globally?
Jaeger,54134885,54185032.0,1,"2019/01/10, 20:31:29",True,"2019/01/14, 18:14:37",nan,4151103.0,169.0,0,1112,Disable opentracing globally,Which mechanism are you using to enable/disable opentracing if you are in the same boat?
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,I successfully added Apache Camel's  OpenTracing  component to my application.
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,I can see traces in Jaeger UI.
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,But the traces for the  RabbitMQ  component show only the exchange name without the routing key as operation name.
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,"Because of my application uses only one exchange with different routing keys, I need to see the routing key as operation name in my traces."
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,Research
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,"With  OpenTracing Spring RabbitMQ  I could expose another customized  RabbitMqSpanDecorator , see  Span decorator :"
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,Note: you can customize your spans by declaring an overridden  RabbitMqSpanDecorator  bean.
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,"(However, I coulnd't change the operation name with the  RabbitMqSpanDecorator  at all, because the operation name is hard coded to  producer  or  consumer .)"
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,Unfortunately Apache Camel uses its own different implementation of a  RabbitmqSpanDecorator  to decorate spans.
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,"I wrote a custom class by overiding Apache Camel's  RabbitmqSpanDecorator , but my custom class wasn't used."
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,Question
Jaeger,66642457,nan,0,"2021/03/15, 19:07:27",False,"2021/03/15, 19:17:06","2021/03/15, 19:17:06",5277820.0,12820.0,0,51,How to change the operation name of a span with Apache Camel OpenTracing component?,How can I change the operation name of a span with Apache Camel OpenTracing component for Apache Camel RabbitMQ component?
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,I have an application which is made up of multiple services built in proprietary language.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,I want to collect traces and ingest into Jaeger or APM solution.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,There is no instrumentation library.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,"However, these services produce traces in a proprietary format."
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,I want to convert these traces to OpenTelemetry traces and ingest into Jaeger or APM solution.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,I started using OpenTelemetry-Java SDK.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,Configured appropriate exporter.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,I can see the traces in Jeager as expected.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,BUT it shows only one service.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,Whereas I want to depict two different services.
Jaeger,66487682,66519661.0,2,"2021/03/05, 08:13:32",True,"2021/03/07, 19:57:21","2021/03/05, 17:13:00",2500390.0,343.0,0,82,How to create multiple services using opentelemetry java sdk?,Is that possible using OpenTelemetry?
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,I am implementing service to service integration that using spring webflux.
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,Each microservice is isolated and running a different port.
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,I would like to see an end to end trace using jaeger.
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,Preference-service  is receiving the request and forwarding  customer-service .
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,Using sleuth and zipkin for collecting trace
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,Preference service code part
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,Preference service prop file
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,Customer-service code part
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,Customer service prop file
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,Jaeger is running in the docker-compose file
Jaeger,66141019,nan,1,"2021/02/10, 18:34:54",False,"2021/02/11, 02:50:17",nan,15184407.0,1.0,0,127,How to implement trace service to service using spring webflux?,"Preference service trace in Jaeger  
 Customer service trace in Jaeger"
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,"Tracing makes finding parts in code, worthwhile a developers time and attention, much easier."
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,"For that reason, I attached  Jaeger  as tracer to a set of microservices inside Docker containers."
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,I use  Traefik  as ingress controller/ service-mesh to route and proxy requests.
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,"The problem I am facing is, that something's wrong with the  tracing  config in Traefik."
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,Jaeger can not find the span context to connect the single/ service-dependend spans to a whole trace.
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,The following line appears in the logs:
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,The following snippets describe the  Docker Compose  setup.
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,This is a stripped down version of the traefik Container.
Jaeger,66048959,66151385.0,2,"2021/02/04, 17:44:07",True,"2021/02/11, 10:45:37",nan,376483.0,19808.0,0,53,Tracing requests over their lifetime … through Docker Containers?,Traefik is set up using the  file provider  as base and Docker Compose labels on top of it:
Jaeger,65395723,nan,0,"2020/12/21, 17:44:44",False,"2020/12/21, 17:44:44",nan,433253.0,2122.0,0,20,OpenTelemetry arbitrary application information,We would like to use  OpenTelemetry  to collect information about our running servers.
Jaeger,65395723,nan,0,"2020/12/21, 17:44:44",False,"2020/12/21, 17:44:44",nan,433253.0,2122.0,0,20,OpenTelemetry arbitrary application information,"But our primry focus is not so much about metrics per se, but actually more about the metadata."
Jaeger,65395723,nan,0,"2020/12/21, 17:44:44",False,"2020/12/21, 17:44:44",nan,433253.0,2122.0,0,20,OpenTelemetry arbitrary application information,"So we would like to know which version of an application is installed (on QS, Prod, ...), which .Net assembly versions are used, which RabbitMQ messages are provided and which are consumed, REST Endpoints, gRPC-Endpoints, ..."
Jaeger,65395723,nan,0,"2020/12/21, 17:44:44",False,"2020/12/21, 17:44:44",nan,433253.0,2122.0,0,20,OpenTelemetry arbitrary application information,I assume that all of the above are valid sources for OpenTelemetry metrics?
Jaeger,65395723,nan,0,"2020/12/21, 17:44:44",False,"2020/12/21, 17:44:44",nan,433253.0,2122.0,0,20,OpenTelemetry arbitrary application information,Can I use OpenTelemetry to create this sort of infrastructure registry?
Jaeger,65395723,nan,0,"2020/12/21, 17:44:44",False,"2020/12/21, 17:44:44",nan,433253.0,2122.0,0,20,OpenTelemetry arbitrary application information,Actual distributed traces and metrics are as of now actually of second concern.
Jaeger,65395723,nan,0,"2020/12/21, 17:44:44",False,"2020/12/21, 17:44:44",nan,433253.0,2122.0,0,20,OpenTelemetry arbitrary application information,What would be my GUI?
Jaeger,65395723,nan,0,"2020/12/21, 17:44:44",False,"2020/12/21, 17:44:44",nan,433253.0,2122.0,0,20,OpenTelemetry arbitrary application information,"Zipkin, Jaeger, another tool?"
Jaeger,65219729,65298980.0,1,"2020/12/09, 17:26:07",True,"2020/12/15, 03:56:21","2020/12/09, 23:51:36",3703933.0,357.0,0,41,Open tracing instrumentation for ForkJoinPool.commonPool(),"My application extensively uses  CompletableFuture.supplyAsync(() -&gt; someService(context, args));  &amp; we rely on supplyAsync to use  ForkJoinPool.commonPool()  thread pool to get the service run in its own thread."
Jaeger,65219729,65298980.0,1,"2020/12/09, 17:26:07",True,"2020/12/15, 03:56:21","2020/12/09, 23:51:36",3703933.0,357.0,0,41,Open tracing instrumentation for ForkJoinPool.commonPool(),Is there a way to instrument someService call in open tracing without passing in a custom  Executor  as argument to supplyAsync() ?
Jaeger,65219729,65298980.0,1,"2020/12/09, 17:26:07",True,"2020/12/15, 03:56:21","2020/12/09, 23:51:36",3703933.0,357.0,0,41,Open tracing instrumentation for ForkJoinPool.commonPool(),I'm using spring and jaeger and have the below dependency
Jaeger,64878686,66257794.0,2,"2020/11/17, 17:54:57",True,"2021/02/25, 01:42:56",nan,4436650.0,1055.0,0,151,Is OpenTracing enabled for Reactive Routes in Quarkus?,I have recently changed my Quarkus application from RestEasy to  Reactive Routes  to implement my HTTP endpoints.
Jaeger,64878686,66257794.0,2,"2020/11/17, 17:54:57",True,"2021/02/25, 01:42:56",nan,4436650.0,1055.0,0,151,Is OpenTracing enabled for Reactive Routes in Quarkus?,My Quarkus app had OpenTracing enabled and it was working fine.
Jaeger,64878686,66257794.0,2,"2020/11/17, 17:54:57",True,"2021/02/25, 01:42:56",nan,4436650.0,1055.0,0,151,Is OpenTracing enabled for Reactive Routes in Quarkus?,After changing the HTTP resource layer I can not see any trace in Jaeger.
Jaeger,64878686,66257794.0,2,"2020/11/17, 17:54:57",True,"2021/02/25, 01:42:56",nan,4436650.0,1055.0,0,151,Is OpenTracing enabled for Reactive Routes in Quarkus?,After setting log level in DEBUG I can see my application is registered in Jaeger but I don't see any traceId or spanId in logs neither traces in Jaeger:
Jaeger,64878686,66257794.0,2,"2020/11/17, 17:54:57",True,"2021/02/25, 01:42:56",nan,4436650.0,1055.0,0,151,Is OpenTracing enabled for Reactive Routes in Quarkus?,I'm using the latest version of Quarkus which is 1.9.2.Final.
Jaeger,64878686,66257794.0,2,"2020/11/17, 17:54:57",True,"2021/02/25, 01:42:56",nan,4436650.0,1055.0,0,151,Is OpenTracing enabled for Reactive Routes in Quarkus?,Is it enabled OpenTracing when I'm using Reactive Routes?
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,Thanks for building this marvelous library and sample app.
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,I struggled with a lot of libraries but ended up using this one.
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,I have already started spreading the word in my circle!
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,I want to instrument Oatpp CRUD service using  Opentracing CPP  (with  Jaeger tracer  in place).
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,I have already  integrated  these into CRUD service and that is working well.
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,"However, I want to call CRUD service from a Java app and extract the TextMapWriter so that the the traces from Java app and CRUD service could be correlated."
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,Do you mind suggesting some sample code to achieve this?
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,I know how to pass in required params from Java side.
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,Here's the sample Java code to inject the required headers in GET request to CRUD service.
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,This is only an abstract to give you an idea of how the injection is done.
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,I have put in certain &quot;imports&quot; to the class names so that you know where each reference is coming from.
Jaeger,64636582,nan,0,"2020/11/01, 22:23:09",False,"2020/11/01, 23:11:29","2020/11/01, 23:11:29",5432511.0,79.0,0,37,Opentracing with Oatpp CRUD service,}
Jaeger,64524266,nan,0,"2020/10/25, 15:31:00",False,"2020/10/25, 15:31:00",nan,2055886.0,77.0,0,52,Grafana cloud datasource - How to get datasource URL,I'm trying to set up a Jaeger datasource in grafana cloud.
Jaeger,64524266,nan,0,"2020/10/25, 15:31:00",False,"2020/10/25, 15:31:00",nan,2055886.0,77.0,0,52,Grafana cloud datasource - How to get datasource URL,"In Jaeger datasource page - URL field is empty, my question is: Where can I find the required URL?"
Jaeger,64524266,nan,0,"2020/10/25, 15:31:00",False,"2020/10/25, 15:31:00",nan,2055886.0,77.0,0,52,Grafana cloud datasource - How to get datasource URL,"I have tried to write an imaginary URL, for test purpose, and when I clicked on 'Save &amp; Test' I've got no error feedback, but when I tried to pick Jaeger datasource in explore page I've got 'Failed to load services from Jaeger."
Jaeger,64524266,nan,0,"2020/10/25, 15:31:00",False,"2020/10/25, 15:31:00",nan,2055886.0,77.0,0,52,Grafana cloud datasource - How to get datasource URL,Failed to fetch'.
Jaeger,64524266,nan,0,"2020/10/25, 15:31:00",False,"2020/10/25, 15:31:00",nan,2055886.0,77.0,0,52,Grafana cloud datasource - How to get datasource URL,"So I'm confused, can someone help me understand which URL should I use and where can I find it?"
Jaeger,64524266,nan,0,"2020/10/25, 15:31:00",False,"2020/10/25, 15:31:00",nan,2055886.0,77.0,0,52,Grafana cloud datasource - How to get datasource URL,(I'm using grafana cloud)
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,Setting up jaeger tracing...So previously I had a binary executable (jaeger-agent) running on a Linux CentOS 8 box alongside a server side application.
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,The tracing spans were being sent to the jaeger-collector service (setup by kubernetes) on port 14250 and everything was working.
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,Then recently we had to reboot the jaeger tracing service due to a system crash.
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,Now things have stopped working and from the logs there's a &quot;504 Gateway Timeout&quot; and the agent can no longer communicate with the collector.
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,"In AWS, we were running a kubernetes service &quot;jaeger-collector&quot; that conformed to the service here
 https://github.com/jaegertracing/jaeger-kubernetes/blob/master/jaeger-production-template.yml  with the only difference is that I'm using version 1.16."
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,There's no external IP with the service.
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,How do I use  curl  to test the communication with the jaeger-collector service?
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,Or do I need an external IP and perhaps that's the reason for the Gateway Timeout?
Jaeger,64051610,nan,1,"2020/09/24, 20:37:29",False,"2020/09/25, 18:50:14","2020/09/25, 01:19:25",3681199.0,4504.0,0,55,How to troubleshoot communication with a kubernetes service,I tried using  curl  with the ClusterIP and that didn't seem to work.
Jaeger,63441241,nan,0,"2020/08/16, 22:42:18",False,"2020/08/16, 22:42:18",nan,4776486.0,674.0,0,48,How to customize sleuth span in spring reactor?,I enabled sleuth in spring reactor:
Jaeger,63441241,nan,0,"2020/08/16, 22:42:18",False,"2020/08/16, 22:42:18",nan,4776486.0,674.0,0,48,How to customize sleuth span in spring reactor?,In spring boot sample app declared and run my program:
Jaeger,63441241,nan,0,"2020/08/16, 22:42:18",False,"2020/08/16, 22:42:18",nan,4776486.0,674.0,0,48,How to customize sleuth span in spring reactor?,Then I run jaeger and see traces coming from my app.
Jaeger,63441241,nan,0,"2020/08/16, 22:42:18",False,"2020/08/16, 22:42:18",nan,4776486.0,674.0,0,48,How to customize sleuth span in spring reactor?,"There are only traces with name &quot;async&quot;, and there are no tags with the key &quot;sample-key&quot;."
Jaeger,63441241,nan,0,"2020/08/16, 22:42:18",False,"2020/08/16, 22:42:18",nan,4776486.0,674.0,0,48,How to customize sleuth span in spring reactor?,"Help me please, how to assign tags to span in my flux?"
Jaeger,62418175,62430615.0,1,"2020/06/17, 00:54:29",True,"2020/06/17, 16:50:21",nan,13680535.0,25.0,0,149,Opentelemetry with nodejs,I wanna integrate open telemetry to my node.js and I have a few questions about this project.
Jaeger,62418175,62430615.0,1,"2020/06/17, 00:54:29",True,"2020/06/17, 16:50:21",nan,13680535.0,25.0,0,149,Opentelemetry with nodejs,I am particularly interested in metrics and tracing Is it worth it to go for open telemetry or just get a Prometheus exporter and Zipkin/jaeger?
Jaeger,62418175,62430615.0,1,"2020/06/17, 00:54:29",True,"2020/06/17, 16:50:21",nan,13680535.0,25.0,0,149,Opentelemetry with nodejs,"Also, I am a little bit confused about metrics in open telemetry for js."
Jaeger,62418175,62430615.0,1,"2020/06/17, 00:54:29",True,"2020/06/17, 16:50:21",nan,13680535.0,25.0,0,149,Opentelemetry with nodejs,There arent any default basic metrics that I can use?
Jaeger,61363408,nan,0,"2020/04/22, 13:57:27",False,"2020/04/26, 11:44:25",nan,11961540.0,11.0,0,203,How to add a custom exporter for capturing traces from opentelemetry?,I have created a custom exporter by Implementing the SpanExporter class of OpenTelemetry.Trace.Export in .NET Core.
Jaeger,61363408,nan,0,"2020/04/22, 13:57:27",False,"2020/04/26, 11:44:25",nan,11961540.0,11.0,0,203,How to add a custom exporter for capturing traces from opentelemetry?,I need to configure the opentelemetry traces to use this as an exporter.
Jaeger,61363408,nan,0,"2020/04/22, 13:57:27",False,"2020/04/26, 11:44:25",nan,11961540.0,11.0,0,203,How to add a custom exporter for capturing traces from opentelemetry?,I am not using any collector here I want to directly use this exporter in-process.
Jaeger,61363408,nan,0,"2020/04/22, 13:57:27",False,"2020/04/26, 11:44:25",nan,11961540.0,11.0,0,203,How to add a custom exporter for capturing traces from opentelemetry?,Earlier we were using Jaeger exporter as follows:
Jaeger,61363408,nan,0,"2020/04/22, 13:57:27",False,"2020/04/26, 11:44:25",nan,11961540.0,11.0,0,203,How to add a custom exporter for capturing traces from opentelemetry?,"I need to use the custom exporter inplace of JaegerExporter now, how to configure this?"
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,I am using Istio with tracing.
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,I have two REST services A1 which calls A2.
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,The A1 service will forward the headers received from the client to REST service A2.
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,"However, I am not able to see the tracing in my Jaeger UI."
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,I am sending some random values in the header using curl to my service A1:
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,Another header parameters I tried also is:
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,"However, I don't see any trace on Jaeger ui."
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,If just call a service A3 without passing any headers then I see that traced in the JaegerUI.
Jaeger,60978505,nan,1,"2020/04/01, 22:06:24",False,"2020/04/06, 19:57:38","2020/04/06, 19:57:38",4611186.0,842.0,0,93,Istio tracing tying multiple requests together using header,thanks
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,I'm using the Python opentracing module with an RPC (over HTTP) client.
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,At the moment I'm not interested in sending the tracing logs to an application like Jaeger - I just want to examine the span (and child spans) in the client when the RPC call returns.
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,So far I have this:
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,I found I had to use  MockTracer()  to get anything at all.
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,"The base  Tracer()  class didn't seem to make any of the basic information ( start_time ,  finish_time ,  tags  etc) of the spans publicly accessible."
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,I can't currently figure out how to retrieve the updated span (in order to read any tags the server might have added) and any child spans created by the server from the results of the request.
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,(I'm also a bit puzzled about how the server will know what kind of child spans to create - obviously they need to be the same kind as the span that is passed in through the headers.)
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,"In a nutshell, while reporting traces to a central server like Jaeger is useful, my purpose here is to have the RPC client print out all the server's tracing information."
Jaeger,59875985,59894312.0,1,"2020/01/23, 12:10:11",True,"2020/01/24, 12:23:57",nan,127670.0,4418.0,0,182,How can RPC client access span attributes/child spans created by server,(Not to say I don't want the traces in Jaeger as well but I'll deal with that once the client trace reporting is working.)
Jaeger,59839687,nan,2,"2020/01/21, 13:01:34",True,"2020/02/05, 17:23:23","2020/06/20, 12:12:55",4431422.0,1989.0,0,168,Auto propogate traces in envoy,Based on the  Documentation  envoy is capable of generating and propagating the traces to the Jaeger service cluster.
Jaeger,59839687,nan,2,"2020/01/21, 13:01:34",True,"2020/02/05, 17:23:23","2020/06/20, 12:12:55",4431422.0,1989.0,0,168,Auto propogate traces in envoy,It also states that
Jaeger,59839687,nan,2,"2020/01/21, 13:01:34",True,"2020/02/05, 17:23:23","2020/06/20, 12:12:55",4431422.0,1989.0,0,168,Auto propogate traces in envoy,"in order to fully take advantage of tracing, the application has to propagate trace headers that Envoy generates while making calls to other services."
Jaeger,59839687,nan,2,"2020/01/21, 13:01:34",True,"2020/02/05, 17:23:23","2020/06/20, 12:12:55",4431422.0,1989.0,0,168,Auto propogate traces in envoy,"So assuming if a client calls -&gt; service A -&gt; calls Service B, service A being proxied behind envoy."
Jaeger,59839687,nan,2,"2020/01/21, 13:01:34",True,"2020/02/05, 17:23:23","2020/06/20, 12:12:55",4431422.0,1989.0,0,168,Auto propogate traces in envoy,"If service A calls service B, this call from A to B would also have to go through envoy right."
Jaeger,59839687,nan,2,"2020/01/21, 13:01:34",True,"2020/02/05, 17:23:23","2020/06/20, 12:12:55",4431422.0,1989.0,0,168,Auto propogate traces in envoy,"So the traced Id that was originally generated by envoy when the client called Service A, wouldn't this be propagated to Service B."
Jaeger,59839687,nan,2,"2020/01/21, 13:01:34",True,"2020/02/05, 17:23:23","2020/06/20, 12:12:55",4431422.0,1989.0,0,168,Auto propogate traces in envoy,Why does the application (Service A) need to forward these headers?
Jaeger,59576942,nan,1,"2020/01/03, 12:28:48",True,"2020/05/11, 13:35:24","2020/01/03, 12:32:27",8870505.0,841.0,0,131,Logging with istio,I have a project with microservices in kubernetes connected though rest swagger clients.
Jaeger,59576942,nan,1,"2020/01/03, 12:28:48",True,"2020/05/11, 13:35:24","2020/01/03, 12:32:27",8870505.0,841.0,0,131,Logging with istio,I want to make logging of all request and response payloads.
Jaeger,59576942,nan,1,"2020/01/03, 12:28:48",True,"2020/05/11, 13:35:24","2020/01/03, 12:32:27",8870505.0,841.0,0,131,Logging with istio,So that for each request there is an id and information about where it came from with full payload.
Jaeger,59576942,nan,1,"2020/01/03, 12:28:48",True,"2020/05/11, 13:35:24","2020/01/03, 12:32:27",8870505.0,841.0,0,131,Logging with istio,And for each service to service call as well.
Jaeger,59576942,nan,1,"2020/01/03, 12:28:48",True,"2020/05/11, 13:35:24","2020/01/03, 12:32:27",8870505.0,841.0,0,131,Logging with istio,Is it possible to do that with Istio?
Jaeger,59576942,nan,1,"2020/01/03, 12:28:48",True,"2020/05/11, 13:35:24","2020/01/03, 12:32:27",8870505.0,841.0,0,131,Logging with istio,"There is distributed tracing tools: zipkin, jaeger."
Jaeger,59576942,nan,1,"2020/01/03, 12:28:48",True,"2020/05/11, 13:35:24","2020/01/03, 12:32:27",8870505.0,841.0,0,131,Logging with istio,But looks like they log only time.
Jaeger,59576942,nan,1,"2020/01/03, 12:28:48",True,"2020/05/11, 13:35:24","2020/01/03, 12:32:27",8870505.0,841.0,0,131,Logging with istio,Or better to handle it each app code internally?
Jaeger,59574885,59580066.0,1,"2020/01/03, 09:43:41",True,"2020/01/03, 17:50:28",nan,476917.0,3016.0,0,596,How to configure istio-proxy to log traceId?,I am using istio with version 1.3.5.
Jaeger,59574885,59580066.0,1,"2020/01/03, 09:43:41",True,"2020/01/03, 17:50:28",nan,476917.0,3016.0,0,596,How to configure istio-proxy to log traceId?,Is there any configuration to be set to allow istio-proxy to log traceId?
Jaeger,59574885,59580066.0,1,"2020/01/03, 09:43:41",True,"2020/01/03, 17:50:28",nan,476917.0,3016.0,0,596,How to configure istio-proxy to log traceId?,I am using jaeger tracing (wit zipkin protocol) being enabled.
Jaeger,59574885,59580066.0,1,"2020/01/03, 09:43:41",True,"2020/01/03, 17:50:28",nan,476917.0,3016.0,0,596,How to configure istio-proxy to log traceId?,"There is one thing I want to accomplish by having traceId logging:
- log correlation in multiple services upstream."
Jaeger,59574885,59580066.0,1,"2020/01/03, 09:43:41",True,"2020/01/03, 17:50:28",nan,476917.0,3016.0,0,596,How to configure istio-proxy to log traceId?,Basically I can filter all logs by certain traceId.
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,My question about Istio in Kubernetes.
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,I have Istio sample rate of 1% and I have error which is not included in 1%.
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,Would I see in Jaeger trace for this error?
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,I kind of new to Kubernetes and Istio.
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,That's why can't tested on my own.
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,I have been playing with Istio's example of  Book Application  and I wonder would I see trace with error which not included in 1% of sample rate.
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,Configure Istio when installing with:
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,As result want to know can I see error which not included in sample rate.
Jaeger,58228451,58269994.0,1,"2019/10/04, 03:33:20",True,"2019/10/07, 15:54:44","2019/10/04, 06:35:21",9571426.0,395.0,0,327,How Istio&#39;s sampling rate works with errors?,"If no, how I configure Istio to see it if possible?"
Jaeger,57913923,57918375.0,2,"2019/09/12, 23:14:18",True,"2019/09/23, 11:52:53",nan,471199.0,1143.0,0,217,Istio services and their usage,I have installed Istio using the helm chart with the following settings:
Jaeger,57913923,57918375.0,2,"2019/09/12, 23:14:18",True,"2019/09/23, 11:52:53",nan,471199.0,1143.0,0,217,Istio services and their usage,When I check the services running in the cluster under the  istio-system  namespace I see multiple services around tracing.
Jaeger,57913923,57918375.0,2,"2019/09/12, 23:14:18",True,"2019/09/23, 11:52:53",nan,471199.0,1143.0,0,217,Istio services and their usage,"Since Jaeger is the default setting, I was expecting to see only the  jaeger-collector ."
Jaeger,57913923,57918375.0,2,"2019/09/12, 23:14:18",True,"2019/09/23, 11:52:53",nan,471199.0,1143.0,0,217,Istio services and their usage,"It is not clear as to what the role of  jaeger-agent ,  tracing  and  zipkin  are, any ideas ?"
Jaeger,57913923,57918375.0,2,"2019/09/12, 23:14:18",True,"2019/09/23, 11:52:53",nan,471199.0,1143.0,0,217,Istio services and their usage,","
Jaeger,57473932,57474533.0,1,"2019/08/13, 11:29:57",True,"2019/08/13, 12:10:40","2019/08/13, 11:40:50",11268156.0,17.0,0,88,Jager with istio,I am facing difficulty in working with jaeger and Istio.
Jaeger,57473932,57474533.0,1,"2019/08/13, 11:29:57",True,"2019/08/13, 12:10:40","2019/08/13, 11:40:50",11268156.0,17.0,0,88,Jager with istio,Can anyone please describe the steps that are to be followed in configuring jaeger and istio for any demo application.
Jaeger,57473932,57474533.0,1,"2019/08/13, 11:29:57",True,"2019/08/13, 12:10:40","2019/08/13, 11:40:50",11268156.0,17.0,0,88,Jager with istio,"I have tried a few blogs and sites but unfortunately, nothing worked for me."
Jaeger,57473932,57474533.0,1,"2019/08/13, 11:29:57",True,"2019/08/13, 12:10:40","2019/08/13, 11:40:50",11268156.0,17.0,0,88,Jager with istio,if anyone could help me in this that would be great.
Jaeger,57380205,nan,1,"2019/08/06, 19:16:30",False,"2019/08/07, 17:43:53",nan,3966540.0,527.0,0,171,Unable to use opencensus collector to post traces to backend,I have these containers running on my localhost
Jaeger,57380205,nan,1,"2019/08/06, 19:16:30",False,"2019/08/07, 17:43:53",nan,3966540.0,527.0,0,171,Unable to use opencensus collector to post traces to backend,"openzipkin/zipkin                   |   0.0.0.0:9410- 9410/tcp, 0.0.0.0:9412- 9411/tcp"
Jaeger,57380205,nan,1,"2019/08/06, 19:16:30",False,"2019/08/07, 17:43:53",nan,3966540.0,527.0,0,171,Unable to use opencensus collector to post traces to backend,"omnition/opencensus-collector:0.1.9 |         0.0.0.0:1777- 1777/tcp, 0.0.0.0:8888- 8888/tcp, 0.0.0.0:9411- 9411/tcp, 0.0.0.0:32776- 55678/tcp, 0.0.0.0:55680- 55679/tcp   |"
Jaeger,57380205,nan,1,"2019/08/06, 19:16:30",False,"2019/08/07, 17:43:53",nan,3966540.0,527.0,0,171,Unable to use opencensus collector to post traces to backend,Trying to directly use the opencensus collector and my collector configuration looks like this
Jaeger,57380205,nan,1,"2019/08/06, 19:16:30",False,"2019/08/07, 17:43:53",nan,3966540.0,527.0,0,171,Unable to use opencensus collector to post traces to backend,"When I run this sample, collector logs have lots of errors"
Jaeger,57380205,nan,1,"2019/08/06, 19:16:30",False,"2019/08/07, 17:43:53",nan,3966540.0,527.0,0,171,Unable to use opencensus collector to post traces to backend,I am unable to use the collector to sends the traces from opencensus collector to the zipkin backend.
Jaeger,57380205,nan,1,"2019/08/06, 19:16:30",False,"2019/08/07, 17:43:53",nan,3966540.0,527.0,0,171,Unable to use opencensus collector to post traces to backend,"Also tried to use the jaeger backend to post the traces from collector, but I still see the same errors."
Jaeger,57273076,nan,1,"2019/07/30, 16:37:01",False,"2019/07/31, 19:48:01",nan,2410547.0,2100.0,0,120,Understand Opentracing Standard and Tracers Providers,"I'm studying the Opentracing Standard and reading the docs I didn't found the API default Endpoints that should be used by Tracer Providers (Jaeger, LightStep...)."
Jaeger,57273076,nan,1,"2019/07/30, 16:37:01",False,"2019/07/31, 19:48:01",nan,2410547.0,2100.0,0,120,Understand Opentracing Standard and Tracers Providers,"Today I'm using Spring Cloud Sleuth to send metrics do Zipkin, and now I have the option to use Opentracing (brave), but How Spring Cloud Sleuth will know the correct API URL if Opentracing docs don't have a API URL standard."
Jaeger,57273076,nan,1,"2019/07/30, 16:37:01",False,"2019/07/31, 19:48:01",nan,2410547.0,2100.0,0,120,Understand Opentracing Standard and Tracers Providers,i.e:  Jaeger and LightStep (both Opentracing providers) have different API URL.
Jaeger,56202494,56462471.0,1,"2019/05/18, 23:10:41",True,"2019/06/05, 17:23:36",nan,1989579.0,3008.0,0,313,istio - tracing egress traffic,I installed Istio with
Jaeger,56202494,56462471.0,1,"2019/05/18, 23:10:41",True,"2019/06/05, 17:23:36",nan,1989579.0,3008.0,0,313,istio - tracing egress traffic,"I have a service that consumes external services, so I define the following egress rule."
Jaeger,56202494,56462471.0,1,"2019/05/18, 23:10:41",True,"2019/06/05, 17:23:36",nan,1989579.0,3008.0,0,313,istio - tracing egress traffic,"But using Jaeger I can not see the traffic to the external service, and thus be able to detect problems in the network."
Jaeger,56202494,56462471.0,1,"2019/05/18, 23:10:41",True,"2019/06/05, 17:23:36",nan,1989579.0,3008.0,0,313,istio - tracing egress traffic,"I'm forwarding the appropriate headers to the external service (x-request-id, x-b3-traceid, x-b3-spanid, b3-parentspanid, x-b3-sampled, x-b3-flags, x-ot-span-context)"
Jaeger,56202494,56462471.0,1,"2019/05/18, 23:10:41",True,"2019/06/05, 17:23:36",nan,1989579.0,3008.0,0,313,istio - tracing egress traffic,Is this the correct behavior?
Jaeger,56202494,56462471.0,1,"2019/05/18, 23:10:41",True,"2019/06/05, 17:23:36",nan,1989579.0,3008.0,0,313,istio - tracing egress traffic,what is happening?
Jaeger,56202494,56462471.0,1,"2019/05/18, 23:10:41",True,"2019/06/05, 17:23:36",nan,1989579.0,3008.0,0,313,istio - tracing egress traffic,Can I only have statistics of internal calls?
Jaeger,56202494,56462471.0,1,"2019/05/18, 23:10:41",True,"2019/06/05, 17:23:36",nan,1989579.0,3008.0,0,313,istio - tracing egress traffic,How can I have statistics for egress traffic?
Jaeger,54078937,54367053.0,1,"2019/01/07, 19:19:19",True,"2019/10/27, 12:01:12",nan,453158.0,2207.0,0,901,How to configure an OpenTracing Tracer to push data to Prometheus/Grafana in Java,"I have a Spring Boot app using OpenTracing and I would like to push its data to Prometheus, so I can query all metrics via Grafana (like in this tutorial  https://www.hawkular.org/blog/2017/06/26/opentracing-appmetrics.html )."
Jaeger,54078937,54367053.0,1,"2019/01/07, 19:19:19",True,"2019/10/27, 12:01:12",nan,453158.0,2207.0,0,901,How to configure an OpenTracing Tracer to push data to Prometheus/Grafana in Java,"The problem is, I haven't found any consistent solution to do this, all the examples that I have found so far are outdated, deprecated or lacks documentation."
Jaeger,54078937,54367053.0,1,"2019/01/07, 19:19:19",True,"2019/10/27, 12:01:12",nan,453158.0,2207.0,0,901,How to configure an OpenTracing Tracer to push data to Prometheus/Grafana in Java,"Ideally, I am looking for some solution which returns an instance of io.opentracing.Tracer, similar to what Jaeger does:"
Jaeger,54078937,54367053.0,1,"2019/01/07, 19:19:19",True,"2019/10/27, 12:01:12",nan,453158.0,2207.0,0,901,How to configure an OpenTracing Tracer to push data to Prometheus/Grafana in Java,Best
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer","I’m struggling with the last step of a configuration using MetalLB, Kubernetes, Istio on a bare-metal instance, and that is to have a web page returned from a service to the outside world via an Istio VirtualService route."
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",I’ve just updated the instance to
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",I’ll start with what does work.
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",All complementary services have been deployed and most are working:
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",I say most because since the upgrade to Istio 1.0.3 I've lost the telemetry from istio-ingressgateway in the Jaeger dashboard and I'm not sure how to bring it back.
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",I've dropped the pod and re-created to no-avail.
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer","Outside of that, MetalLB and K8S appear to be working fine and the load-balancer is configured correctly (using ARP)."
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",I can expose my deployment using:
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",it all works perfectly fine and I can hit the webpage from the external load balanced IP address (I deleted the exposed service after this).
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",If I create a K8S Service in the default namespace (I've tried multiple)
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer","followed by a gateway and a route (VirtualService), the only response I get is a 404 outside of the mesh."
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",You'll see in the gateway I'm using the reserved word mesh but I've tried both that and naming the specific gateway.
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",I've also tried different match prefixes for specific URI and the port you can see below.
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",Gateway
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",VirtualService
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",I've double checked it's not the DNS playing up because I can go into the shell of the ingress-gateway either via busybox or using the K8S dashboard
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/shell/istio-system/istio-ingressgateway-6bbdd58f8c-glzvx/?namespace=istio-system
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",and do both an
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",and
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer","and both work successfully, so I know the ingress-gateway pod can see those."
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",The sidecars are set for auto-injection in both the default namespace and the istio-system namespace.
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",The logs for the ingress-gateway show the 404:
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",192.168.224.168:80 is the IP address of the gateway.
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",192.168.1.90:53960 is the IP address of my external client.
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer","Any suggestions, I've tried hitting this from multiple angles for a couple of days now and I feel I'm just missing something simple."
Jaeger,53095177,53600710.0,1,"2018/11/01, 06:22:48",True,"2018/12/03, 21:38:34","2018/11/02, 04:58:13",1404502.0,467.0,0,2548,"Configuring Istio, Kubernetes and MetalLB to use a Istio LoadBalancer",Suggested logs to look at perhaps?
Jaeger,50477228,nan,0,"2018/05/23, 01:20:16",False,"2018/05/23, 01:20:16",nan,9831161.0,1.0,0,2209,Kafka pod in Kubernetes has unbound PersistentVolumeClaims,I am using Kubernetes for my project and I was using Helm to install Jaeger and Kafka in a simple way.
Jaeger,50477228,nan,0,"2018/05/23, 01:20:16",False,"2018/05/23, 01:20:16",nan,9831161.0,1.0,0,2209,Kafka pod in Kubernetes has unbound PersistentVolumeClaims,"The problem is that regarding kafka, the zookeeper pods started correctly I have one pod that is ""Pending"" since the beginning."
Jaeger,50477228,nan,0,"2018/05/23, 01:20:16",False,"2018/05/23, 01:20:16",nan,9831161.0,1.0,0,2209,Kafka pod in Kubernetes has unbound PersistentVolumeClaims,I used this command  kubectl --namespace=default describe pod my-kafka-kafka-0  and got this information:
Jaeger,50477228,nan,0,"2018/05/23, 01:20:16",False,"2018/05/23, 01:20:16",nan,9831161.0,1.0,0,2209,Kafka pod in Kubernetes has unbound PersistentVolumeClaims,Warning  FailedScheduling  55m (x398 over 2h)   default-scheduler  pod has unbound PersistentVolumeClaims (repeated 2 times)
Jaeger,50477228,nan,0,"2018/05/23, 01:20:16",False,"2018/05/23, 01:20:16",nan,9831161.0,1.0,0,2209,Kafka pod in Kubernetes has unbound PersistentVolumeClaims,"I have no idea how to solve this issue since I am new to Kubernetes and i thought that using Helm, deploying apps with it would be straightforward."
Jaeger,50477228,nan,0,"2018/05/23, 01:20:16",False,"2018/05/23, 01:20:16",nan,9831161.0,1.0,0,2209,Kafka pod in Kubernetes has unbound PersistentVolumeClaims,I am sorry if my question is not done in the correct way but it is also my first time here :)
Jaeger,50477228,nan,0,"2018/05/23, 01:20:16",False,"2018/05/23, 01:20:16",nan,9831161.0,1.0,0,2209,Kafka pod in Kubernetes has unbound PersistentVolumeClaims,Thank you in advance for your time and I would be really happy if someone could help me!
Jaeger,17616323,nan,1,"2013/07/12, 16:41:20",False,"2013/07/12, 18:12:11",nan,1278255.0,67.0,0,22,Incorrect BMP displaying in SquareChase,I'm trying the code for the game SquareChase shown in XNA 4.0 Game programming by Example by Jaeger.
Jaeger,17616323,nan,1,"2013/07/12, 16:41:20",False,"2013/07/12, 18:12:11",nan,1278255.0,67.0,0,22,Incorrect BMP displaying in SquareChase,The program is running through the VS2010 ide but the texture it is displaying is not the texture i drew.
Jaeger,17616323,nan,1,"2013/07/12, 16:41:20",False,"2013/07/12, 18:12:11",nan,1278255.0,67.0,0,22,Incorrect BMP displaying in SquareChase,I've checked the Content directory and that holds the correct texture so i dont know where the incorrect texture is coming from.
Jaeger,17616323,nan,1,"2013/07/12, 16:41:20",False,"2013/07/12, 18:12:11",nan,1278255.0,67.0,0,22,Incorrect BMP displaying in SquareChase,Can someone please help with this weird error?
Jaeger,17616323,nan,1,"2013/07/12, 16:41:20",False,"2013/07/12, 18:12:11",nan,1278255.0,67.0,0,22,Incorrect BMP displaying in SquareChase,I have got this code to work okay on another computer so i don't know what the problem is.
Jaeger,17616323,nan,1,"2013/07/12, 16:41:20",False,"2013/07/12, 18:12:11",nan,1278255.0,67.0,0,22,Incorrect BMP displaying in SquareChase,Thanks....
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,I'm in need of a rewrite rule for a site of mine.
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,I have the following type of URLS:
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,But I would like that to read:
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,This is what I think should work:
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,(it doesn't)
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,"But then in addition, the  ?brand  parameter could actually appear as an  &amp;parameter  - e.g."
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,?word=clutch&amp;brand=TedBaker .
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,"To summarise - I would like the brand parameter to be made into the final part of the static URL, preceded by the word 'by'."
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,I would like this to work regardless of where the brand parameter appears in the URL string.
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,Here is the current .htaccess file:
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,And here are all the URLs this needs to apply to:
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,Above need to be converted to /store/category/black-bags-by-jaeger/ or /store/category/black-bags-by-jaeger/?word=Cross%20Body (for 2nd and 3rd options).
Jaeger,12318639,12318996.0,1,"2012/09/07, 16:00:53",True,"2012/09/07, 18:21:40","2012/09/07, 18:21:40",1185445.0,4990.0,0,146,.htaccess URLRewrite rule - regular expression,"I also have this:
 http://bag-saver.com/store/search/?brand=Jaeger  which needs to be converted to /store/search/bags-by-jaeger/."
Jaeger,54061611,54075526.0,1,"2019/01/06, 14:45:11",True,"2019/01/07, 15:41:12",nan,2285330.0,31.0,-1,62,Light Step Integration for Open Tracing,I am exploring the various Tracing Systems.
Jaeger,54061611,54075526.0,1,"2019/01/06, 14:45:11",True,"2019/01/07, 15:41:12",nan,2285330.0,31.0,-1,62,Light Step Integration for Open Tracing,I was looking into Light Step recently.
Jaeger,54061611,54075526.0,1,"2019/01/06, 14:45:11",True,"2019/01/07, 15:41:12",nan,2285330.0,31.0,-1,62,Light Step Integration for Open Tracing,I have integrated my application for OpenTracing where I use the tracer from Light Step.
Jaeger,54061611,54075526.0,1,"2019/01/06, 14:45:11",True,"2019/01/07, 15:41:12",nan,2285330.0,31.0,-1,62,Light Step Integration for Open Tracing,Now how can view the traces I am generating.
Jaeger,54061611,54075526.0,1,"2019/01/06, 14:45:11",True,"2019/01/07, 15:41:12",nan,2285330.0,31.0,-1,62,Light Step Integration for Open Tracing,For example in Jaeger they had a ready to use docker image which can be used for quick demo.
Jaeger,54061611,54075526.0,1,"2019/01/06, 14:45:11",True,"2019/01/07, 15:41:12",nan,2285330.0,31.0,-1,62,Light Step Integration for Open Tracing,Can somebody please help me here ?
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,I have parsed texts from several scientific pdf files.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"All of these files contain a reference list at the end, where the authors and their publications are listed, + when and where they were released."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"Also, there are cross-references in the text."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,For example:
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"('1', ' I.  Altintas, C.  Berkley, E.  Jaeger, M.  Jones, B.  Lud-scher, and S.  Mock."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,Kepler: An extensible system fordesign and execution of scientiﬁc workﬂows.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"In In SS-DBM, pages 21–23, 2004. ')"
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,and a different one from another text:
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"('1', ' G.  Antoniol, G.  Canfora, G.  Casazza, A.  DeLucia, and E.  Merlo,“Recovering Traceability Links between Code and Documentation,” IEEETrans."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,Software Eng.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,", vol."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"28, no."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"10, pp."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"970-983, Oct."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,2002.')
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"I was able to recognize both with the regex, which gives me 2 capturing groups besides the full-match:"
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"The  first group  I can use to get the  number of the reference , to match with the cross-references in text"
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"The  second  is the  everything else remaining , and i'd like to recognize the  author  and the  title of the publication  from it, regardless of the parsing format, if that's possible."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"Later, I'd like to use these values to write separate  .txt  files with the author + title.txt name, and to append their cross-references found in the text to each file."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,For this is what I have now for this:
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"It was giving UnicodeEncodeError when I was trying to create a file with mode =""a+"", for a suggestion i changed it to bytes."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"It's not giving me an UnicodeEncodeError, it's giving me another now:"
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"f = open(author+"".txt"", ""ab+"")"
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"OSError: [Errno 22] Invalid argument:  "" A.  Yun chung Liu, “The Effect of Oversampling and Undersampling onClassifying Imbalanced Text Datasets,” master’s thesis,  http://www ."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,lans.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,ece.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,utexas.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,edu/aliu/papers/aliu_ masters_thesis.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"pdf, 2004."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"J.  Cleland-Huang, R.  Settimi, X.  Zou, and P.  Solc, “The Detection andClassification of Non-Functional Requirements with Application to EarlyAspects,” Proc."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,Requirements Eng.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,Conf.
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"(RE ’06), pp."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"36-45, 2006."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"'.txt"""
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,"Currently the publication has everything else than the number from the references,  I'd like to switch that to author + title.txt , and I hope that would solve the error above as well."
Jaeger,49571999,nan,1,"2018/03/30, 12:40:27",False,"2018/07/02, 07:29:27","2018/03/30, 12:58:37",9573945.0,1.0,-1,545,Regex for getting the title and the author from reference list,I'd appreciate every suggestion for improvement!
InspectIT,52150469,nan,1,"2018/09/03, 16:10:49",False,"2019/02/27, 12:03:53",nan,66854.0,825.0,1,34,InspectIT APM - Cannot see dig deep down in methods,I have configured InspectIT and a sample springboot application .
InspectIT,52150469,nan,1,"2018/09/03, 16:10:49",False,"2019/02/27, 12:03:53",nan,66854.0,825.0,1,34,InspectIT APM - Cannot see dig deep down in methods,there is a request mapping which was configured to leak some memory .. trouble is InspectIT does not dig deep into the method calls .
InspectIT,52150469,nan,1,"2018/09/03, 16:10:49",False,"2019/02/27, 12:03:53",nan,66854.0,825.0,1,34,InspectIT APM - Cannot see dig deep down in methods,the only level it goes down is doFilter-  service and then no method calls after that .. is this normal for inspectit ?
InspectIT,52150469,nan,1,"2018/09/03, 16:10:49",False,"2019/02/27, 12:03:53",nan,66854.0,825.0,1,34,InspectIT APM - Cannot see dig deep down in methods,... somehow i would have expected it to dig deep down into the method calls
InspectIT,52150469,nan,1,"2018/09/03, 16:10:49",False,"2019/02/27, 12:03:53",nan,66854.0,825.0,1,34,InspectIT APM - Cannot see dig deep down in methods,any help is appreciated
InspectIT,63567504,nan,0,"2020/08/24, 22:24:15",False,"2020/08/25, 04:42:09","2020/08/25, 04:42:09",14158473.0,1.0,0,16,URI Transformation - InspectIT URI Aggregation,I'm looking for defining the regular expression on the HTTP Sensor so that as per the documentation &quot;Apply sensor regular expression on URI&quot; option is visible on the URI Aggregation view.
InspectIT,63567504,nan,0,"2020/08/24, 22:24:15",False,"2020/08/25, 04:42:09","2020/08/25, 04:42:09",14158473.0,1.0,0,16,URI Transformation - InspectIT URI Aggregation,"However, apart from the method definition i cannot see any field where i can define the regular expression for URI Transformation in UI of HTTP Sensor configuration (as shown in attached ss)."
InspectIT,63567504,nan,0,"2020/08/24, 22:24:15",False,"2020/08/25, 04:42:09","2020/08/25, 04:42:09",14158473.0,1.0,0,16,URI Transformation - InspectIT URI Aggregation,Also there is no &quot;http.cfg&quot; file present in the downloaded folders where we can do this out-of-the-box-instrumentation.
InspectIT,63567504,nan,0,"2020/08/24, 22:24:15",False,"2020/08/25, 04:42:09","2020/08/25, 04:42:09",14158473.0,1.0,0,16,URI Transformation - InspectIT URI Aggregation,I am using InspectIT 1.9.3.107.
InspectIT,63567504,nan,0,"2020/08/24, 22:24:15",False,"2020/08/25, 04:42:09","2020/08/25, 04:42:09",14158473.0,1.0,0,16,URI Transformation - InspectIT URI Aggregation,Could I get some more information or suggestions on this?
InspectIT,63567504,nan,0,"2020/08/24, 22:24:15",False,"2020/08/25, 04:42:09","2020/08/25, 04:42:09",14158473.0,1.0,0,16,URI Transformation - InspectIT URI Aggregation,since i have already tried playing with http sensors and other trial n errors.
InspectIT,50604004,nan,0,"2018/05/30, 14:53:48",False,"2018/05/30, 14:53:48",nan,3566441.0,1536.0,0,14,Sending commands to console installer in docker,"I am trying to install inspectIT in my docker container, however it seems as if they only package an installer that I must go through."
InspectIT,50604004,nan,0,"2018/05/30, 14:53:48",False,"2018/05/30, 14:53:48",nan,3566441.0,1536.0,0,14,Sending commands to console installer in docker,This seems rather difficult but I was thinking: is it possible to send commands to a console application?
InspectIT,50604004,nan,0,"2018/05/30, 14:53:48",False,"2018/05/30, 14:53:48",nan,3566441.0,1536.0,0,14,Sending commands to console installer in docker,for example if I start the installer it will get to a point like this
InspectIT,50604004,nan,0,"2018/05/30, 14:53:48",False,"2018/05/30, 14:53:48",nan,3566441.0,1536.0,0,14,Sending commands to console installer in docker,Can I send a command to it or script it in some way in order to install it on docker?
Instana,55606472,nan,1,"2019/04/10, 09:56:46",True,"2019/04/12, 10:54:59","2019/04/12, 10:54:59",4665363.0,103.0,3,151,How can we use INSTANA to monitor &amp; manage Apache Kafka and Zookeeper cluster,I have set up a Kafka &amp; Zookeeper cluster in Production.
Instana,55606472,nan,1,"2019/04/10, 09:56:46",True,"2019/04/12, 10:54:59","2019/04/12, 10:54:59",4665363.0,103.0,3,151,How can we use INSTANA to monitor &amp; manage Apache Kafka and Zookeeper cluster,I need to set up INSTANA APM Tool to monitor and manage the Production Kafka &amp; Zookeeper Cluster.
Instana,55606472,nan,1,"2019/04/10, 09:56:46",True,"2019/04/12, 10:54:59","2019/04/12, 10:54:59",4665363.0,103.0,3,151,How can we use INSTANA to monitor &amp; manage Apache Kafka and Zookeeper cluster,Did anyone  has ever used INSTANA to monitor Kafka .
Instana,55606472,nan,1,"2019/04/10, 09:56:46",True,"2019/04/12, 10:54:59","2019/04/12, 10:54:59",4665363.0,103.0,3,151,How can we use INSTANA to monitor &amp; manage Apache Kafka and Zookeeper cluster,Kindly share your thoughts on this.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,I'm trying to integrate Instana into an application.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,More specifically I'm trying to send errors from my Angular app to Instana.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,I have my code 'working' so this is kind of a question about best practice.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,Instana's  Backend Correlation documentation  defines functions in 'window' from what I understand.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,I've set something similar to this in my index.html.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,"The issue I have is when I try to follow Instana's guide for  Angular 2+ Integration  regarding error tracking, where they call one of the methods that I can access from window."
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,The guide just straight up calls the function  ineum(...)  by itself.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,"When I try to do this, my project wouldn't compile."
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,"My current fix is:  (&lt;any&gt;window).ineum('reportError', errorContext);  But I was looking at  another stack overflow question  where they accessed window differently in their javascript."
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,Sorry for my confusion as this may not be an actual issue because my code is 'working' but I just want clarification on this.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,I'm not sure if I just didn't follow Instana's guides correctly but this was the best of what I could find.
Instana,55974537,57088646.0,1,"2019/05/03, 20:23:46",True,"2019/07/18, 09:53:32",nan,11448259.0,33.0,2,493,Integrating Instana error tracking into an Angular 2 application,I tried reaching out via their contact page but I haven't received a response quite yet.
Instana,59921305,nan,1,"2020/01/26, 20:14:22",True,"2020/01/28, 14:25:45","2020/01/26, 21:43:03",2520254.0,618.0,2,151,Instana integration with create-react-app to download sourcemap for error reporting,The source map for CRA is enabled by default.
Instana,59921305,nan,1,"2020/01/26, 20:14:22",True,"2020/01/28, 14:25:45","2020/01/26, 21:43:03",2520254.0,618.0,2,151,Instana integration with create-react-app to download sourcemap for error reporting,"I have given Instana the permission to download source map from my application in production, but the errors reported are still compressed and uglified."
Instana,59921305,nan,1,"2020/01/26, 20:14:22",True,"2020/01/28, 14:25:45","2020/01/26, 21:43:03",2520254.0,618.0,2,151,Instana integration with create-react-app to download sourcemap for error reporting,I guess the configuration has no effect.
Instana,59921305,nan,1,"2020/01/26, 20:14:22",True,"2020/01/28, 14:25:45","2020/01/26, 21:43:03",2520254.0,618.0,2,151,Instana integration with create-react-app to download sourcemap for error reporting,Referring to  this doc .
Instana,59921305,nan,1,"2020/01/26, 20:14:22",True,"2020/01/28, 14:25:45","2020/01/26, 21:43:03",2520254.0,618.0,2,151,Instana integration with create-react-app to download sourcemap for error reporting,"When I do a curl for the source map from the terminal, it works."
Instana,59921305,nan,1,"2020/01/26, 20:14:22",True,"2020/01/28, 14:25:45","2020/01/26, 21:43:03",2520254.0,618.0,2,151,Instana integration with create-react-app to download sourcemap for error reporting,"My site is on HTTPS, but the doc says it makes an HTTP request."
Instana,59921305,nan,1,"2020/01/26, 20:14:22",True,"2020/01/28, 14:25:45","2020/01/26, 21:43:03",2520254.0,618.0,2,151,Instana integration with create-react-app to download sourcemap for error reporting,Is that the root cause?
Instana,59921305,nan,1,"2020/01/26, 20:14:22",True,"2020/01/28, 14:25:45","2020/01/26, 21:43:03",2520254.0,618.0,2,151,Instana integration with create-react-app to download sourcemap for error reporting,How to fix it?
Instana,64357661,nan,0,"2020/10/14, 19:26:14",False,"2020/10/14, 20:49:22",nan,14160043.0,29.0,2,55,Does anyone have a hypothesis at how Instana can avoid sampling and process 100% of traces,Most APM solutions which analyze and store traces will sample traces to manage the volume of traces stored/analyzed (millions per day!
Instana,64357661,nan,0,"2020/10/14, 19:26:14",False,"2020/10/14, 20:49:22",nan,14160043.0,29.0,2,55,Does anyone have a hypothesis at how Instana can avoid sampling and process 100% of traces,).
Instana,64357661,nan,0,"2020/10/14, 19:26:14",False,"2020/10/14, 20:49:22",nan,14160043.0,29.0,2,55,Does anyone have a hypothesis at how Instana can avoid sampling and process 100% of traces,Instana claims to store and analyze all traces without sampling.
Instana,64357661,nan,0,"2020/10/14, 19:26:14",False,"2020/10/14, 20:49:22",nan,14160043.0,29.0,2,55,Does anyone have a hypothesis at how Instana can avoid sampling and process 100% of traces,"I have been doing some research around Instana, but do not see any whitepaper/blog post that hints at how they accomplish this."
Instana,64357661,nan,0,"2020/10/14, 19:26:14",False,"2020/10/14, 20:49:22",nan,14160043.0,29.0,2,55,Does anyone have a hypothesis at how Instana can avoid sampling and process 100% of traces,Wondering if there is a whitepaper / seminar I have missed that may have hinted at this.
Instana,64357661,nan,0,"2020/10/14, 19:26:14",False,"2020/10/14, 20:49:22",nan,14160043.0,29.0,2,55,Does anyone have a hypothesis at how Instana can avoid sampling and process 100% of traces,"For example, Lightstep has a similar claim and does so through its satellite architecture - they provide a high level explanation of how they try and capture 100% of traces (temporarily)."
Instana,64357661,nan,0,"2020/10/14, 19:26:14",False,"2020/10/14, 20:49:22",nan,14160043.0,29.0,2,55,Does anyone have a hypothesis at how Instana can avoid sampling and process 100% of traces,Does the community here have any good hypothesis - high level - how Instana approaches the challenge?
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,I'm having some trouble monitoring my GitLab installation with Instana.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,"GitLab and the nginx shipped with it are running fine, only monitoring does not work."
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,Instana recognises the nginx but cannot get any information because it cannot pick up data from the /nginx_status location.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,I added my additional configuration for /nginx_status to /etc/gitlab/gitlab.rb and installed it using gitlab-ctl reconfigure (just as the gitlab docs say).
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,And basically gitlab works fine and exposes the status page  http://git-test:9999/nginx_status .
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,My config file /var/opt/gitlab/nginx/conf/nginx.conf now has an additional include for /var/opt/gitlab/nginx/conf/nginx-status.conf at its end.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,The file contents seem fine as well.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,My problem now is that instana is not picking up the information exposed via /nginx_status stating that nginx needs configuration.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,Status URL not found.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,"The nginx config file was parsed and no stub_status
  direction could be found."
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,"This directive needs to be
  configured in order to gather nginx metrics."
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,"The
  following snippet shows how to configure stub_status
  within an nginx config file."
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,To me it seems that instana is not following the include and hence not pickung up the configuration from /var/opt/gitlab/nginx/conf/nginx-status.conf.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,So basically instana has no clue about the information it asks for.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,Does anyone of you know how I can feed these status information to instana?
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,Thanks in advance guys and best regards.
Instana,57292600,nan,1,"2019/07/31, 17:00:42",True,"2020/04/01, 19:46:26",nan,11864266.0,113.0,1,146,Include nginx metrics from GitLab installation in Instana,Sebastian
Instana,58518001,nan,1,"2019/10/23, 10:48:26",True,"2019/10/23, 19:43:00",nan,12261731.0,11.0,1,57,"Instana REST API, 405 error - how to fix?",trying to get receive informations via the Instana REST API.
Instana,58518001,nan,1,"2019/10/23, 10:48:26",True,"2019/10/23, 19:43:00",nan,12261731.0,11.0,1,57,"Instana REST API, 405 error - how to fix?",Looks like that:
Instana,58518001,nan,1,"2019/10/23, 10:48:26",True,"2019/10/23, 19:43:00",nan,12261731.0,11.0,1,57,"Instana REST API, 405 error - how to fix?",Getting back that error:
Instana,58518001,nan,1,"2019/10/23, 10:48:26",True,"2019/10/23, 19:43:00",nan,12261731.0,11.0,1,57,"Instana REST API, 405 error - how to fix?",The matching Curl script (which I can't use) looks like that and works:
Instana,58518001,nan,1,"2019/10/23, 10:48:26",True,"2019/10/23, 19:43:00",nan,12261731.0,11.0,1,57,"Instana REST API, 405 error - how to fix?",Any idea?
Instana,64485555,nan,0,"2020/10/22, 18:29:28",False,"2020/10/22, 18:29:28",nan,5494171.0,11.0,1,45,Instana monitoring inside docker swarm cluster?,I'm trying to monitor a springboot microservice with instana on a docker swarm cluster.
Instana,64485555,nan,0,"2020/10/22, 18:29:28",False,"2020/10/22, 18:29:28",nan,5494171.0,11.0,1,45,Instana monitoring inside docker swarm cluster?,the Microservice has 2 replicas per node on a 3 nodes cluster.
Instana,64485555,nan,0,"2020/10/22, 18:29:28",False,"2020/10/22, 18:29:28",nan,5494171.0,11.0,1,45,Instana monitoring inside docker swarm cluster?,it is possible?
Instana,64485555,nan,0,"2020/10/22, 18:29:28",False,"2020/10/22, 18:29:28",nan,5494171.0,11.0,1,45,Instana monitoring inside docker swarm cluster?,do i need to run instana agent image with docker run?
Instana,64485555,nan,0,"2020/10/22, 18:29:28",False,"2020/10/22, 18:29:28",nan,5494171.0,11.0,1,45,Instana monitoring inside docker swarm cluster?,docker service?
Instana,64485555,nan,0,"2020/10/22, 18:29:28",False,"2020/10/22, 18:29:28",nan,5494171.0,11.0,1,45,Instana monitoring inside docker swarm cluster?,any help will be appreciate.
Instana,64485555,nan,0,"2020/10/22, 18:29:28",False,"2020/10/22, 18:29:28",nan,5494171.0,11.0,1,45,Instana monitoring inside docker swarm cluster?,Thanks
Instana,64613844,nan,1,"2020/10/30, 20:22:15",True,"2020/11/02, 11:18:54",nan,3345890.0,498.0,1,42,Instana monitoring tool integration with react native webview,I am trying to integrate instana( https://www.instana.com ) with react native webview.
Instana,64613844,nan,1,"2020/10/30, 20:22:15",True,"2020/11/02, 11:18:54",nan,3345890.0,498.0,1,42,Instana monitoring tool integration with react native webview,My app is a webapp which is being rendered inside webview.
Instana,64613844,nan,1,"2020/10/30, 20:22:15",True,"2020/11/02, 11:18:54",nan,3345890.0,498.0,1,42,Instana monitoring tool integration with react native webview,"My approach is to inject their javascript agent( https://www.instana.com/docs/website_monitoring/api ) to webview, but that doesn't seems to be working."
Instana,64613844,nan,1,"2020/10/30, 20:22:15",True,"2020/11/02, 11:18:54",nan,3345890.0,498.0,1,42,Instana monitoring tool integration with react native webview,Any thoughts on this will be extremely helpful.
Instana,57354975,nan,1,"2019/08/05, 11:42:30",True,"2019/08/06, 12:22:05","2019/08/05, 11:59:31",2630080.0,57.0,0,111,Instana agent one liner installation error on Ubuntu,I tried installing instana agent using docker command and it works but I need it be installed using one liner command and when tried it gives error as below:
Instana,57354975,nan,1,"2019/08/05, 11:42:30",True,"2019/08/06, 12:22:05","2019/08/05, 11:59:31",2630080.0,57.0,0,111,Instana agent one liner installation error on Ubuntu,I tried on private wifi (with no proxy) but still the same.
Instana,57354975,nan,1,"2019/08/05, 11:42:30",True,"2019/08/06, 12:22:05","2019/08/05, 11:59:31",2630080.0,57.0,0,111,Instana agent one liner installation error on Ubuntu,Can anyone help on this error?
Instana,57354975,nan,1,"2019/08/05, 11:42:30",True,"2019/08/06, 12:22:05","2019/08/05, 11:59:31",2630080.0,57.0,0,111,Instana agent one liner installation error on Ubuntu,Thank you!
Instana,58239287,nan,0,"2019/10/04, 18:23:58",False,"2019/10/04, 18:23:58",nan,7758792.0,121.0,3,114,How would I instrument Sequelize with opentracing?,"I'd like to start tracing my sequelize SQL calls using opentracing, but I'm having a hard time figuring out how."
Instana,58239287,nan,0,"2019/10/04, 18:23:58",False,"2019/10/04, 18:23:58",nan,7758792.0,121.0,3,114,How would I instrument Sequelize with opentracing?,I'd like to adapt this code to be more flexible so I can drop it into any sequelize project:  https://github.com/instana/nodejs-sensor/blob/626ab3c8258d4e91d42a61d79603532a921b35b4/packages/core/src/tracing/instrumentation/database/pg.js
Instana,58239287,nan,0,"2019/10/04, 18:23:58",False,"2019/10/04, 18:23:58",nan,7758792.0,121.0,3,114,How would I instrument Sequelize with opentracing?,"I'm using Lightstep as a tracer, but I am using a raw tracer (not one that auto instruments) because I like the control."
Instana,58239287,nan,0,"2019/10/04, 18:23:58",False,"2019/10/04, 18:23:58",nan,7758792.0,121.0,3,114,How would I instrument Sequelize with opentracing?,Do you have suggestions on how I could add tracing to sequelize/postgres(pg)?
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,We are running a set of Java applications in docker containers on OpenShift.
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,On a regular basis we experience oom kills for our containers.
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,To analyse this issue we set up Instana and Grafana for monitoring.
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,In Grafana we have graphs for each of our containers showing memory metrics e.g.
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,"JVM heap, memory.usage and memory.total_rss."
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,From these graphs we know that the heap as well as the memory.total_rss of our containers is pretty stable on a certain level over a week.
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,So we assume that we do not have a memory leak in our Java application.
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,"However, the memeory.total is constantly increasing over the time and after a couple of days it goes beyond the configured memory limit of the docker container."
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,As far as we can see this doesn't cause Openshift to kill the container immediately but sooner or later it happens.
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,We increased the memory limit of all our containers and this seems to help since Openshift is not killing our containers that often anymore.
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,However we still see in Grafana that the memeory.total is exceeding the configures memory limit of our containers significantly after a couple of days (rss memory is fine).
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,"To better understand Openshifts OOM killer, does anybody know which memory metric Openshift takes into account to decide if a container has to be killed or not?"
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,Is the configured container memory limit related to the memory.usage or the memory.total_rss or something completely different?
Instana,59248045,nan,0,"2019/12/09, 13:36:22",False,"2019/12/09, 15:22:35","2019/12/09, 15:22:35",4649331.0,121.0,2,249,Which memory metric does Openshift OOM Killer use,Thanks for help in advance.
Instana,62075759,62222959.0,2,"2020/05/29, 01:45:31",True,"2020/10/19, 07:30:18",nan,4828463.0,4812.0,1,38,How to set mode and time in Dynamic Agents?,I am referring to this page:
Instana,62075759,62222959.0,2,"2020/05/29, 01:45:31",True,"2020/10/19, 07:30:18",nan,4828463.0,4812.0,1,38,How to set mode and time in Dynamic Agents?,https://www.instana.com/docs/setup_and_manage/host_agent/updates/#update-interval
Instana,62075759,62222959.0,2,"2020/05/29, 01:45:31",True,"2020/10/19, 07:30:18",nan,4828463.0,4812.0,1,38,How to set mode and time in Dynamic Agents?,Is there a way to pass mode and time from outside as environment variables or any other way beside logging into the pod and manually changing the files inside etc/instana/com.instana.agent.main.config.UpdateManager.cfg file?
Instana,64340042,nan,0,"2020/10/13, 20:17:12",False,"2020/10/13, 20:19:40","2020/10/13, 20:19:40",812272.0,3161.0,0,22,Which party should set the Timing-Allow-Origin header?,"Given a server APP that serves an Angular App to a web browser, an Apollo GraphQL server and a third-party tracking script by Instana."
Instana,64340042,nan,0,"2020/10/13, 20:17:12",False,"2020/10/13, 20:19:40","2020/10/13, 20:19:40",812272.0,3161.0,0,22,Which party should set the Timing-Allow-Origin header?,"If the tracking script is run in the browser, where do I need to set the Timing-Allow-Origin header in the response?"
Instana,64340042,nan,0,"2020/10/13, 20:17:12",False,"2020/10/13, 20:19:40","2020/10/13, 20:19:40",812272.0,3161.0,0,22,Which party should set the Timing-Allow-Origin header?,Does this need to point to the protocol + host of the third-party script?
Instana,63836703,nan,0,"2020/09/10, 22:45:50",False,"2020/09/10, 22:57:12","2020/09/10, 22:57:12",506078.0,2530.0,0,68,Public IP address used by a kubernetes pod,I wanted to check the address that a pod uses to connect to a FTP server.
Instana,63836703,nan,0,"2020/09/10, 22:45:50",False,"2020/09/10, 22:57:12","2020/09/10, 22:57:12",506078.0,2530.0,0,68,Public IP address used by a kubernetes pod,I wanted to test it by running  kubectl   -n cdol exec -it pod-namer  -- curl ipinfo.io/ip  but the connection is blocked by  an engress policy.
Instana,63836703,nan,0,"2020/09/10, 22:45:50",False,"2020/09/10, 22:57:12","2020/09/10, 22:57:12",506078.0,2530.0,0,68,Public IP address used by a kubernetes pod,I know the CLUSTER-IP of the pod.
Instana,63836703,nan,0,"2020/09/10, 22:45:50",False,"2020/09/10, 22:57:12","2020/09/10, 22:57:12",506078.0,2530.0,0,68,Public IP address used by a kubernetes pod,It has no EXTERNAL-IP.
Instana,63836703,nan,0,"2020/09/10, 22:45:50",False,"2020/09/10, 22:57:12","2020/09/10, 22:57:12",506078.0,2530.0,0,68,Public IP address used by a kubernetes pod,I know the CLUSTER-IP of the service that pod uses.
Instana,63836703,nan,0,"2020/09/10, 22:45:50",False,"2020/09/10, 22:57:12","2020/09/10, 22:57:12",506078.0,2530.0,0,68,Public IP address used by a kubernetes pod,I know the node the pod is running so I am able to check the network interfaces in Instana monitoring tool.
Instana,63836703,nan,0,"2020/09/10, 22:45:50",False,"2020/09/10, 22:57:12","2020/09/10, 22:57:12",506078.0,2530.0,0,68,Public IP address used by a kubernetes pod,"But all the IPs are private IPs, how can I know what IP other services sees when a pod is connecting to them?"
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,I am running a Instana Agent on minishift.
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,I see these logs:
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,2020-05-26T03:10:58.763+00:00 | WARN  | nstana-sensor-scheduler-thread-1 | Kubernetes       | com.instana.sensor-kubernetes - 1.2.106 | Instana agent does not have permission to watch for statefulsets changes.
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,Kubernetes sensor will not work properly without this permission.
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,Please ensure proper permissions for  statefulsets  resource.
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,How to set proper permissions for statefulsets resource in Kubernete?
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,edit
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,I used 2 yaml files.
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,I used this yaml file to install operator ditto as is without any change:  https://github.com/instana/instana-agent-operator/releases/latest/download/instana-agent-operator.yaml
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,And I used this custom resource yaml file  https://github.com/instana/instana-agent-operator/blob/master/deploy/instana-agent.customresource.yaml  with just two changes: I added  size: 1  under spec key and replaced  replace-me  with proper agent key.
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,edit2
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,I used  oc api-resources  as that's how I created the project and deployed definition files.
Instana,62014135,nan,0,"2020/05/26, 06:23:52",False,"2020/05/26, 07:12:55","2020/05/26, 07:12:55",4828463.0,4812.0,0,55,How to set proper permissions for statefulsets resource in Kubernete?,Result can be found here -   https://pastebin.com/A6zRRReS
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,I have a  Spring Batch  application with  JpaPagingItemReader  (i modified it a bit) and 4 Jpa repositories to enrich  Model  which comes from JpaPagingItemReader.
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,My flow is:
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,"All works great, but today i tried to run flow with big amount of data from database."
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,I generated 20 files ( 2.2 GB ).
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,"But sometimes i got  OutOfMemory Java Heap  (I had 1Gb XMS, XSS), then i up it to 2 GB and all works good, but in Instana i see, that  Old gen Java memory  is always  900  in use after GC."
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,It is about 1.3-1.7Gb in use.
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,"So i start to think, how can i optimize GC of Spring Data Jpa objects."
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,I think they are much time in memory.
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,"When i select Model with  JpaPagingItemReader  i detach every Model (with  entityManager.detach ), but when i enrich  Model  with custom  Spring Data Jpa  requests i am not detaching results."
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,Maybe the problem in this and i should detach them?
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,"I do not need to insert data to database, i need just to read it."
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,Or do i need to make page size less and select about  4000  per request?
Instana,57580097,57736812.0,1,"2019/08/20, 22:04:48",True,"2019/08/31, 12:35:09","2019/08/21, 16:35:08",10826472.0,49.0,-1,203,Spring Data JPA garbage collection,I need to process  370 000  records from database and enrich them.
Instana,41053682,nan,1,"2016/12/09, 07:29:50",False,"2016/12/09, 10:58:46","2016/12/09, 10:42:10",7235157.0,55.0,-1,98,Why Cocoapods command gives this error?,Why Cocoapods command gives this error in terminal?
Instana,41053682,nan,1,"2016/12/09, 07:29:50",False,"2016/12/09, 10:58:46","2016/12/09, 10:42:10",7235157.0,55.0,-1,98,Why Cocoapods command gives this error?,Either it is the problem with my settings or I am using VMware?
Instana,41053682,nan,1,"2016/12/09, 07:29:50",False,"2016/12/09, 10:58:46","2016/12/09, 10:42:10",7235157.0,55.0,-1,98,Why Cocoapods command gives this error?,Awaiting for your ideas &amp; tricks.
Instana,41053682,nan,1,"2016/12/09, 07:29:50",False,"2016/12/09, 10:58:46","2016/12/09, 10:42:10",7235157.0,55.0,-1,98,Why Cocoapods command gives this error?,"ERROR:  Could not find a valid gem 'pod' ( = 0) in any repository 
  ERROR:  Could not find a valid gem 'install' ( = 0) in any repository 
  ERROR:  Possible alternatives: installr, instant, instana, instacli, instapi"
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,Background
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,I have a java server that is making an RPC call to a go server.
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,The java rpc client and go rpc server are instrumented with lightstep.
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,"Everything about the trace looks normal except for where in the lightstep UI, the go rpc server span is placed."
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,The java span has ts 1493929521325 which is right before the request is sent to the go server.
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,"The go rpc server has 2 timestamps: 1493929521326 is when it received the request and started the span, 1493929521336 is after it responded and finished the span."
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,Problem
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,I would expect the UI to have the go span horizontally to the immediate right of the java span.
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,"Instead, it is far to the right."
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,The only possible cause I can think of is an incompatibility between v0.10.1 which java code is using and v0.9.1 which go is using.
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,Is this a possibility?
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,Do you have any thoughts on a possible cause?
LightStep,43796314,44529216.0,1,"2017/05/05, 07:05:18",True,"2017/06/13, 23:47:13",nan,1168364.0,1759.0,0,231,Lightstep: Inaccurate UI for child span,The go code is essentially:
LightStep,63724073,63725459.0,1,"2020/09/03, 15:50:52",True,"2020/09/03, 17:13:18","2020/09/03, 15:58:18",2000548.0,30911.0,0,162,How to use lightstep/otel-launcher-node as an OpenTelemetry exporter?,"Before I was using  lightstep/opentelemetry-exporter-js , I can use my own exporters and Lightstep exporter at same time."
LightStep,63724073,63725459.0,1,"2020/09/03, 15:50:52",True,"2020/09/03, 17:13:18","2020/09/03, 15:58:18",2000548.0,30911.0,0,162,How to use lightstep/otel-launcher-node as an OpenTelemetry exporter?,"However, just saw lightstep/opentelemetry-exporter-js is deprecated and replaced by  lightstep/otel-launcher-node ."
LightStep,63724073,63725459.0,1,"2020/09/03, 15:50:52",True,"2020/09/03, 17:13:18","2020/09/03, 15:58:18",2000548.0,30911.0,0,162,How to use lightstep/otel-launcher-node as an OpenTelemetry exporter?,"I checked the source code of it and the demo, it looks like it is a &quot;framework&quot; on top of OpenTelemetry."
LightStep,63724073,63725459.0,1,"2020/09/03, 15:50:52",True,"2020/09/03, 17:13:18","2020/09/03, 15:58:18",2000548.0,30911.0,0,162,How to use lightstep/otel-launcher-node as an OpenTelemetry exporter?,Is it possible to simply use it as one of OpenTelemetry exporters?
LightStep,56085587,nan,0,"2019/05/11, 01:32:50",False,"2020/03/11, 12:02:42",nan,4970846.0,21.0,2,277,How to manually configure OpenTracing Spring Web Instrumentation using xml,"I've inherited a legacy Spring3 application, and I'm trying to add Lightstep instrumentation to it."
LightStep,56085587,nan,0,"2019/05/11, 01:32:50",False,"2020/03/11, 12:02:42",nan,4970846.0,21.0,2,277,How to manually configure OpenTracing Spring Web Instrumentation using xml,I'm having trouble converting the instructions for manually configuration found here.
LightStep,56085587,nan,0,"2019/05/11, 01:32:50",False,"2020/03/11, 12:02:42",nan,4970846.0,21.0,2,277,How to manually configure OpenTracing Spring Web Instrumentation using xml,https://github.com/opentracing-contrib/java-spring-web
LightStep,56085587,nan,0,"2019/05/11, 01:32:50",False,"2020/03/11, 12:02:42",nan,4970846.0,21.0,2,277,How to manually configure OpenTracing Spring Web Instrumentation using xml,"In short, I need to convert the code block below to the xml equivalent."
LightStep,56085587,nan,0,"2019/05/11, 01:32:50",False,"2020/03/11, 12:02:42",nan,4970846.0,21.0,2,277,How to manually configure OpenTracing Spring Web Instrumentation using xml,I've successfully created my Lightstep Tracer bean using the following dependencies.
LightStep,64761133,nan,0,"2020/11/10, 02:23:46",False,"2020/11/10, 02:23:46",nan,399738.0,7716.0,0,75,How do I use opentelemetry node-sdk with graphql instrumentation?,I am trying to use  LightStep OpenTelemetry Launcher for Node.js  (which uses  OpenTelemetry Node SDK ) and  OpenTelemetry GraphQL Instrumentation  together.
LightStep,64761133,nan,0,"2020/11/10, 02:23:46",False,"2020/11/10, 02:23:46",nan,399738.0,7716.0,0,75,How do I use opentelemetry node-sdk with graphql instrumentation?,"However, I cannot see how to change the setup instruction for the GraphQL instrumentation so that they will work with the Node SDK."
LightStep,64761133,nan,0,"2020/11/10, 02:23:46",False,"2020/11/10, 02:23:46",nan,399738.0,7716.0,0,75,How do I use opentelemetry node-sdk with graphql instrumentation?,The Node SDK does not expose a  NodeTracerProvider  instance I can pass to the  GraphQLInstrumentation  instance.
LightStep,64761133,nan,0,"2020/11/10, 02:23:46",False,"2020/11/10, 02:23:46",nan,399738.0,7716.0,0,75,How do I use opentelemetry node-sdk with graphql instrumentation?,Has anyone gotten these to play together?
LightStep,49949470,nan,1,"2018/04/20, 23:44:33",True,"2018/04/21, 01:20:38","2018/04/21, 01:20:38",1967650.0,25.0,0,693,spring-cloud-sleuth with opentracing,I'm using the latest milestone of spring-cloud-sleuth and I can't seem to get traces emitted through opentracing.
LightStep,49949470,nan,1,"2018/04/20, 23:44:33",True,"2018/04/21, 01:20:38","2018/04/21, 01:20:38",1967650.0,25.0,0,693,spring-cloud-sleuth with opentracing,"I have a  Tracer  bean defined and spring boot seems to acknowledge that, but no traces are being emitted."
LightStep,49949470,nan,1,"2018/04/20, 23:44:33",True,"2018/04/21, 01:20:38","2018/04/21, 01:20:38",1967650.0,25.0,0,693,spring-cloud-sleuth with opentracing,Is there a way to check if spring-cloud-sleuth is aware of the Tracer bean?
LightStep,49949470,nan,1,"2018/04/20, 23:44:33",True,"2018/04/21, 01:20:38","2018/04/21, 01:20:38",1967650.0,25.0,0,693,spring-cloud-sleuth with opentracing,update
LightStep,49949470,nan,1,"2018/04/20, 23:44:33",True,"2018/04/21, 01:20:38","2018/04/21, 01:20:38",1967650.0,25.0,0,693,spring-cloud-sleuth with opentracing,"I did see the merged documentation and have a  Tracer  instance on the bean, as defined below:"
LightStep,49949470,nan,1,"2018/04/20, 23:44:33",True,"2018/04/21, 01:20:38","2018/04/21, 01:20:38",1967650.0,25.0,0,693,spring-cloud-sleuth with opentracing,"I'm not explicitly importing the OpenTracing APIs, because the LightStep tracer pulls that in transitively, but I can try doing that."
LightStep,49949470,nan,1,"2018/04/20, 23:44:33",True,"2018/04/21, 01:20:38","2018/04/21, 01:20:38",1967650.0,25.0,0,693,spring-cloud-sleuth with opentracing,I've also explicitly enabled OpenTracing support in my application.yml file.
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,Couldn't see in  skywalking ui  data from  istio  metrics.
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,"Using below guide to installed  istio  and  skywalking  backend to  Kubernetes :
 https://github.com/apache/skywalking/tree/master/docs/en/setup/istio ."
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,Made all steps in the guide and deployed app.
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,Deployed app is example  book app  from  istio .
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,"Could see traces in  Jaeger  and works good, but can't see  istio  metrics in  skywalking ui ."
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,How I can provide metrics from  istio  to  skywalking ui  or what I need to configure to make metrics from  istio  come to  skywalking ui .
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,Mixer logs:
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,Istio policy:
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,"somehow I have failed to connect to my  skywalking  backend, problem was with name and namespace but still getting error but now  skywalking  backend getting data but ui no updates."
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,Istio  telemetry:
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,rpc error: code = Unavailable desc = upstream connect error or disconnect/reset before headers.
SkyWalking,58314080,58586629.0,1,"2019/10/10, 04:05:36",True,"2019/10/28, 08:40:49","2019/10/16, 10:00:18",9571426.0,395.0,0,336,Can&#39;t see Istio metrics in Apache SkyWalking,reset reason: connection failure
SkyWalking,60476903,nan,1,"2020/03/01, 17:56:16",False,"2020/06/15, 16:48:11","2020/06/15, 07:54:41",2628868.0,6436.0,0,127,apache skywalking 7.0.0 could not get service name in kubernetes cluster,"I am using apache skywalking(7.0.0) in Kubernetes(v1.16.0) cluster to be my APM tool,but now I could not get service name in dashboard."
SkyWalking,60476903,nan,1,"2020/03/01, 17:56:16",False,"2020/06/15, 16:48:11","2020/06/15, 07:54:41",2628868.0,6436.0,0,127,apache skywalking 7.0.0 could not get service name in kubernetes cluster,This is my collector config in Dockerfile:
SkyWalking,60476903,nan,1,"2020/03/01, 17:56:16",False,"2020/06/15, 16:48:11","2020/06/15, 07:54:41",2628868.0,6436.0,0,127,apache skywalking 7.0.0 could not get service name in kubernetes cluster,and this is my dashboard UI:
SkyWalking,60476903,nan,1,"2020/03/01, 17:56:16",False,"2020/06/15, 16:48:11","2020/06/15, 07:54:41",2628868.0,6436.0,0,127,apache skywalking 7.0.0 could not get service name in kubernetes cluster,Is something I am missing?
SkyWalking,60476903,nan,1,"2020/03/01, 17:56:16",False,"2020/06/15, 16:48:11","2020/06/15, 07:54:41",2628868.0,6436.0,0,127,apache skywalking 7.0.0 could not get service name in kubernetes cluster,all data collected(Endpoint\Cache\Database\MQ) except the service.What should I do to make service data collect?
SkyWalking,60476903,nan,1,"2020/03/01, 17:56:16",False,"2020/06/15, 16:48:11","2020/06/15, 07:54:41",2628868.0,6436.0,0,127,apache skywalking 7.0.0 could not get service name in kubernetes cluster,When I see the log output:
SkyWalking,62261285,62261701.0,1,"2020/06/08, 14:42:24",True,"2020/06/10, 14:58:52","2020/06/10, 14:58:52",2628868.0,6436.0,0,308,Error: could not find skywalking: stat skywalking: no such file or directory,I am follow this  docs  to install skywalking using helm 3.2.1 :
SkyWalking,62261285,62261701.0,1,"2020/06/08, 14:42:24",True,"2020/06/10, 14:58:52","2020/06/10, 14:58:52",2628868.0,6436.0,0,308,Error: could not find skywalking: stat skywalking: no such file or directory,but when I execute the second command:
SkyWalking,62261285,62261701.0,1,"2020/06/08, 14:42:24",True,"2020/06/10, 14:58:52","2020/06/10, 14:58:52",2628868.0,6436.0,0,308,Error: could not find skywalking: stat skywalking: no such file or directory,and I create skywalking directory:
SkyWalking,62261285,62261701.0,1,"2020/06/08, 14:42:24",True,"2020/06/10, 14:58:52","2020/06/10, 14:58:52",2628868.0,6436.0,0,308,Error: could not find skywalking: stat skywalking: no such file or directory,so what should I do to make it work?
SkyWalking,62261285,62261701.0,1,"2020/06/08, 14:42:24",True,"2020/06/10, 14:58:52","2020/06/10, 14:58:52",2628868.0,6436.0,0,308,Error: could not find skywalking: stat skywalking: no such file or directory,This is I am trying follow:
SkyWalking,62358639,62358989.0,1,"2020/06/13, 13:43:35",True,"2020/06/13, 14:19:26","2020/06/13, 13:47:50",2628868.0,6436.0,0,58,what is the skywalking agent image address about 7.0.0,"I am now using skywalking as my apm, and now I  am configuring the address of my skywalking agent like this:"
SkyWalking,62358639,62358989.0,1,"2020/06/13, 13:43:35",True,"2020/06/13, 14:19:26","2020/06/13, 13:47:50",2628868.0,6436.0,0,58,what is the skywalking agent image address about 7.0.0,but it tells me this address is not correct.
SkyWalking,62358639,62358989.0,1,"2020/06/13, 13:43:35",True,"2020/06/13, 14:19:26","2020/06/13, 13:47:50",2628868.0,6436.0,0,58,what is the skywalking agent image address about 7.0.0,Is skywalking agent having docker image?
SkyWalking,62358639,62358989.0,1,"2020/06/13, 13:43:35",True,"2020/06/13, 14:19:26","2020/06/13, 13:47:50",2628868.0,6436.0,0,58,what is the skywalking agent image address about 7.0.0,What is the docker image address to use in kubernetes v1.16.0 cluster?
SkyWalking,62358639,62358989.0,1,"2020/06/13, 13:43:35",True,"2020/06/13, 14:19:26","2020/06/13, 13:47:50",2628868.0,6436.0,0,58,what is the skywalking agent image address about 7.0.0,I am searching from internet and only find a skywalking  base image .
SkyWalking,62359365,nan,0,"2020/06/13, 14:52:56",False,"2020/06/13, 14:52:56",nan,2628868.0,6436.0,0,17,the skywalking 7.0.0 has no data in kubernetes cluster,"I am install skywalking in kubernetes cluster v1.16.8 using this command, follow  this :"
SkyWalking,62359365,nan,0,"2020/06/13, 14:52:56",False,"2020/06/13, 14:52:56",nan,2628868.0,6436.0,0,17,the skywalking 7.0.0 has no data in kubernetes cluster,"all component runs fine, and I configurate the skywalking agent as sidecar in my pod like this:"
SkyWalking,62359365,nan,0,"2020/06/13, 14:52:56",False,"2020/06/13, 14:52:56",nan,2628868.0,6436.0,0,17,the skywalking 7.0.0 has no data in kubernetes cluster,"but now the skywalking UI has no data, am I missing something to be configurated?"
SkyWalking,63509093,63516070.0,1,"2020/08/20, 19:09:09",True,"2020/08/21, 06:24:43",nan,2628868.0,6436.0,0,64,how to add jdbc driver jar into skywalking 6.5.0 image file in kubernetes,"I am using skywalking 6.5.0 to monitor my apps in kubernetes cluster, this is my skywalking ui yaml config:"
SkyWalking,63509093,63516070.0,1,"2020/08/20, 19:09:09",True,"2020/08/21, 06:24:43",nan,2628868.0,6436.0,0,64,how to add jdbc driver jar into skywalking 6.5.0 image file in kubernetes,"when the pod start, the log output like this:"
SkyWalking,63509093,63516070.0,1,"2020/08/20, 19:09:09",True,"2020/08/21, 06:24:43",nan,2628868.0,6436.0,0,64,how to add jdbc driver jar into skywalking 6.5.0 image file in kubernetes,"I read the skywalking official issue and tell me because the mysql jdbc was GPL licence and SkyWalking is Apache license,so I must add the jdbc driver by myself, but how to add the jdbc driver jar into the image file?"
SkyWalking,63509093,63516070.0,1,"2020/08/20, 19:09:09",True,"2020/08/21, 06:24:43",nan,2628868.0,6436.0,0,64,how to add jdbc driver jar into skywalking 6.5.0 image file in kubernetes,I have no ideas.
SkyWalking,65259756,65282846.0,1,"2020/12/12, 01:00:08",True,"2020/12/14, 04:23:10",nan,5196039.0,1070.0,0,27,Can Skywalking create ES indexes with lifecycle policies or index templates?,I am having trouble finding any information about this in documentation.
SkyWalking,65259756,65282846.0,1,"2020/12/12, 01:00:08",True,"2020/12/14, 04:23:10",nan,5196039.0,1070.0,0,27,Can Skywalking create ES indexes with lifecycle policies or index templates?,In the config/application.yml file under  storage.elasticsearch7  I see various configuration options.
SkyWalking,65259756,65282846.0,1,"2020/12/12, 01:00:08",True,"2020/12/14, 04:23:10",nan,5196039.0,1070.0,0,27,Can Skywalking create ES indexes with lifecycle policies or index templates?,Is there a way to ensure that the indexes that get created are created using a given index template or ILM policy?
SkyWalking,65259756,65282846.0,1,"2020/12/12, 01:00:08",True,"2020/12/14, 04:23:10",nan,5196039.0,1070.0,0,27,Can Skywalking create ES indexes with lifecycle policies or index templates?,I am running the helm chart for the ELK stack and ES version 8.0.0-SNAPSHOT.
SkyWalking,65259756,65282846.0,1,"2020/12/12, 01:00:08",True,"2020/12/14, 04:23:10",nan,5196039.0,1070.0,0,27,Can Skywalking create ES indexes with lifecycle policies or index templates?,My goal is to just delete indexes from SW after 2 weeks so that my cluster doesn't run out of shards.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,I have recently started exploring skywalking as APM tool.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,I am interested in looking at the time spent by methods/functions at application layer.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,Basically a instrumentation sort of thing for the JAVA application.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,With Skywalking I just get 3 spans(methods) that have one root function and two DB execute functions.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,I tried adding the property
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,But this dint work.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,I could still see only 3 spans in the dashboard for the API being hit.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,Under Profile feature I can get the thread stack.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,But i am only interested with Hotspot methods.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,Am i missing something in configuration?
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,I want classes starting with particular pattern to be instrumented and captured in trace.
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,How can I achieve this?
SkyWalking,66424243,nan,0,"2021/03/01, 16:47:32",False,"2021/03/01, 17:42:37","2021/03/01, 17:42:37",2122943.0,557.0,0,28,Apache Skywalking - Instrumentation of Application Classes,Or is there any other open source APM tool I can start with?
SkyWalking,60465004,nan,1,"2020/02/29, 14:10:24",True,"2020/03/01, 06:50:19","2020/03/01, 05:32:35",2628868.0,6436.0,0,33,how to keep kubernetes pod&#39;s status the same or recover automatic,"I am using kubernetes(v1.15.2) to manage my skywalking-ui(v6.5.0) apps,recently I found some app's not accessable but the pod is still running, I am not sure the app is works fine,there is no error output in pod's logs.But the pod status icon give tips:  the pod is in pending state ."
SkyWalking,60465004,nan,1,"2020/02/29, 14:10:24",True,"2020/03/01, 06:50:19","2020/03/01, 05:32:35",2628868.0,6436.0,0,33,how to keep kubernetes pod&#39;s status the same or recover automatic,Why the status not same in different places?The service is down now.How to avoid this situation or make the service recover automatic?
SkyWalking,60465004,nan,1,"2020/02/29, 14:10:24",True,"2020/03/01, 06:50:19","2020/03/01, 05:32:35",2628868.0,6436.0,0,33,how to keep kubernetes pod&#39;s status the same or recover automatic,This is pod info:
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,"I am using a inital container(k8s version:v1.15.2) to initial skywalking(6.5.0) jar file before container startup.But I could not found the file and directory the intial container create,this is my initial container define:"
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,"now the initial container execute success,I am check the log output like this:"
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,now something I am confusing is where the directory locate?
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,where is the file I am copy?
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,I am login my container and do not find the jar file:
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,now I am starting my app to collection metrics data like this:
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,obviously it tell me could not fond the jar file error:
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,so what should I do to fix this?
SkyWalking,60462786,60463565.0,1,"2020/02/29, 09:05:57",True,"2020/02/29, 11:08:36",nan,2628868.0,6436.0,0,83,could not find file and folder created by initial container in kubernetes pod,I already searching from internet bu found no useful way to solve my situation.
SkyWalking,57086811,nan,0,"2019/07/18, 06:43:47",False,"2019/07/18, 07:55:33","2019/07/18, 07:55:33",7112933.0,1.0,0,94,How to use “jstat”,jstat[option vmid[interval[s|ms][count]]]
SkyWalking,57086811,nan,0,"2019/07/18, 06:43:47",False,"2019/07/18, 07:55:33","2019/07/18, 07:55:33",7112933.0,1.0,0,94,How to use “jstat”,i input jstat -gc 12285 get 12285 not found
SkyWalking,57086811,nan,0,"2019/07/18, 06:43:47",False,"2019/07/18, 07:55:33","2019/07/18, 07:55:33",7112933.0,1.0,0,94,How to use “jstat”,"so, why?"
SkyWalking,57086811,nan,0,"2019/07/18, 06:43:47",False,"2019/07/18, 07:55:33","2019/07/18, 07:55:33",7112933.0,1.0,0,94,How to use “jstat”,and how to fix it?
SkyWalking,57086811,nan,0,"2019/07/18, 06:43:47",False,"2019/07/18, 07:55:33","2019/07/18, 07:55:33",7112933.0,1.0,0,94,How to use “jstat”,"so, the vmid is 12285"
SkyWalking,57086811,nan,0,"2019/07/18, 06:43:47",False,"2019/07/18, 07:55:33","2019/07/18, 07:55:33",7112933.0,1.0,0,94,How to use “jstat”,"2.but jstat -gc 12285 get ""12285 not found"""
SkyWalking,57086811,nan,0,"2019/07/18, 06:43:47",False,"2019/07/18, 07:55:33","2019/07/18, 07:55:33",7112933.0,1.0,0,94,How to use “jstat”,"I expect the output of the java JVM's heap status, including Eden, survivor, Old, Permian's status, and GCtime information"
Stagemonitor,33430332,nan,0,"2015/10/30, 08:55:58",False,"2017/09/04, 21:23:12","2016/04/19, 09:24:10",4290480.0,77.0,3,568,How to setup and run StageMonitor tool (https://github.com/stagemonitor/stagemonitor)?,"I am new to Stagemonitor, and I found that it is one of the best open source tool to get the performace metrics."
Stagemonitor,33430332,nan,0,"2015/10/30, 08:55:58",False,"2017/09/04, 21:23:12","2016/04/19, 09:24:10",4290480.0,77.0,3,568,How to setup and run StageMonitor tool (https://github.com/stagemonitor/stagemonitor)?,"So, I tried to implement this for my tomcat server."
Stagemonitor,33430332,nan,0,"2015/10/30, 08:55:58",False,"2017/09/04, 21:23:12","2016/04/19, 09:24:10",4290480.0,77.0,3,568,How to setup and run StageMonitor tool (https://github.com/stagemonitor/stagemonitor)?,But I cannot get a clear documentation to implement and setup this.
Stagemonitor,33430332,nan,0,"2015/10/30, 08:55:58",False,"2017/09/04, 21:23:12","2016/04/19, 09:24:10",4290480.0,77.0,3,568,How to setup and run StageMonitor tool (https://github.com/stagemonitor/stagemonitor)?,The wiki provided in the github page is not enough to setup the application.
Stagemonitor,33430332,nan,0,"2015/10/30, 08:55:58",False,"2017/09/04, 21:23:12","2016/04/19, 09:24:10",4290480.0,77.0,3,568,How to setup and run StageMonitor tool (https://github.com/stagemonitor/stagemonitor)?,Can anyone guide me how to install this tool and make it a go.!
Stagemonitor,33430332,nan,0,"2015/10/30, 08:55:58",False,"2017/09/04, 21:23:12","2016/04/19, 09:24:10",4290480.0,77.0,3,568,How to setup and run StageMonitor tool (https://github.com/stagemonitor/stagemonitor)?,!
Stagemonitor,44347364,44376892.0,1,"2017/06/03, 21:40:28",True,"2017/06/06, 10:41:02","2017/06/06, 10:41:02",1239904.0,554.0,1,334,Benefits of Stagemonitor over JMX monitoring,How does  Stagemonitor  compare over simple JMX metrics?
Stagemonitor,44347364,44376892.0,1,"2017/06/03, 21:40:28",True,"2017/06/06, 10:41:02","2017/06/06, 10:41:02",1239904.0,554.0,1,334,Benefits of Stagemonitor over JMX monitoring,"Unlike java-native JMX MBeans, Stagemonitor includes an agent that sits in your Java application, sending metrics and request traces at the central database, which can be Elasticsearch."
Stagemonitor,44347364,44376892.0,1,"2017/06/03, 21:40:28",True,"2017/06/06, 10:41:02","2017/06/06, 10:41:02",1239904.0,554.0,1,334,Benefits of Stagemonitor over JMX monitoring,Since both ways can serve as input for an ELK Monitoring Stack (JMX see this blog  post ) what are the benefits of Stagemonitor?
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,I am trying to using stagemonitor for get metrics for different methods.
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,I used sample PetClinic application locally for get a idea.
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,"I want to get metrics only what i need , not all of them is any possible to do that."
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,I change some code for my testing purpose
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,"when i click test button in html it will go to this method and i can get the metrics form stagemonitor.If i not need to see this method metrics ,how to stop showing that in stagemonitor"
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,@Felix
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,I trying to use browser-widget in example and i change code for testing.
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,"This output I'm getting i don't want to see details about Testing.I only want to see some method i need, even other methods used i don't want to see about them."
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,is it possible  ?
Stagemonitor,48147866,nan,1,"2018/01/08, 11:57:28",False,"2018/01/08, 17:00:09","2018/01/08, 17:00:09",8109504.0,239.0,1,110,Find Metrics using Stagemonitor,?
Stagemonitor,48434015,nan,0,"2018/01/25, 02:52:19",False,"2018/01/25, 02:52:19",nan,7011404.0,113.0,1,43,Stagemonitor: In browser widget enabled for standalone,I am trying stagemonitor with standalone applicaion.
Stagemonitor,48434015,nan,0,"2018/01/25, 02:52:19",False,"2018/01/25, 02:52:19",nan,7011404.0,113.0,1,43,Stagemonitor: In browser widget enabled for standalone,And i get metrics for jmx reporter correctly.
Stagemonitor,48434015,nan,0,"2018/01/25, 02:52:19",False,"2018/01/25, 02:52:19",nan,7011404.0,113.0,1,43,Stagemonitor: In browser widget enabled for standalone,I want to get that in browser-widget.
Stagemonitor,48434015,nan,0,"2018/01/25, 02:52:19",False,"2018/01/25, 02:52:19",nan,7011404.0,113.0,1,43,Stagemonitor: In browser widget enabled for standalone,I saw config  stagemonitor.web.widget.enabled = true  will enable this.
Stagemonitor,48434015,nan,0,"2018/01/25, 02:52:19",False,"2018/01/25, 02:52:19",nan,7011404.0,113.0,1,43,Stagemonitor: In browser widget enabled for standalone,But how to get dashbord.
Stagemonitor,48434015,nan,0,"2018/01/25, 02:52:19",False,"2018/01/25, 02:52:19",nan,7011404.0,113.0,1,43,Stagemonitor: In browser widget enabled for standalone,Do we have to use grafana for this ?
Stagemonitor,30493074,30496430.0,1,"2015/05/28, 00:02:00",True,"2015/05/28, 09:22:42","2015/05/28, 09:22:42",64809.0,995.0,0,184,Get stagemonitor working with Grails 2.5?,I'm trying to get  www.stagemonitor.org  working with Grails.
Stagemonitor,30493074,30496430.0,1,"2015/05/28, 00:02:00",True,"2015/05/28, 09:22:42","2015/05/28, 09:22:42",64809.0,995.0,0,184,Get stagemonitor working with Grails 2.5?,I've created a sample project here:  https://github.com/jbwiv/teststagemonitor
Stagemonitor,30493074,30496430.0,1,"2015/05/28, 00:02:00",True,"2015/05/28, 09:22:42","2015/05/28, 09:22:42",64809.0,995.0,0,184,Get stagemonitor working with Grails 2.5?,I've added stagemonitor to grails-app/conf/BuildConfig.groovy as both a compile and runtime dependency.
Stagemonitor,30493074,30496430.0,1,"2015/05/28, 00:02:00",True,"2015/05/28, 09:22:42","2015/05/28, 09:22:42",64809.0,995.0,0,184,Get stagemonitor working with Grails 2.5?,"It indeed gets installed to my maven directory after calling ""grails refresh-dependencies"":"
Stagemonitor,30493074,30496430.0,1,"2015/05/28, 00:02:00",True,"2015/05/28, 09:22:42","2015/05/28, 09:22:42",64809.0,995.0,0,184,Get stagemonitor working with Grails 2.5?,"I've also placed a stagemonitor.properties file in src/java, which at runtime gets moved to target/work/resources/stagemonitor.properties."
Stagemonitor,30493074,30496430.0,1,"2015/05/28, 00:02:00",True,"2015/05/28, 09:22:42","2015/05/28, 09:22:42",64809.0,995.0,0,184,Get stagemonitor working with Grails 2.5?,I believe I have that properties file configured properly.
Stagemonitor,30493074,30496430.0,1,"2015/05/28, 00:02:00",True,"2015/05/28, 09:22:42","2015/05/28, 09:22:42",64809.0,995.0,0,184,Get stagemonitor working with Grails 2.5?,I've installed the templates Grails uses for web.xml and modified to insure metadata-complete=true is not present in the generated web.xml.
Stagemonitor,30493074,30496430.0,1,"2015/05/28, 00:02:00",True,"2015/05/28, 09:22:42","2015/05/28, 09:22:42",64809.0,995.0,0,184,Get stagemonitor working with Grails 2.5?,"However, after  grails run-app  and navigating to  http://localhost:8080/main/index , I get my index page as expected, but no stagemonitor icon to click and it appears no stagemonitor assets are included."
Stagemonitor,38184669,38195456.0,1,"2016/07/04, 15:21:39",True,"2017/07/01, 16:10:01","2017/07/01, 16:10:01",1679544.0,3568.0,0,203,StageMonitor: Disable Browser Widget,I would like to disable the browser widget in production release.
Stagemonitor,38184669,38195456.0,1,"2016/07/04, 15:21:39",True,"2017/07/01, 16:10:01","2017/07/01, 16:10:01",1679544.0,3568.0,0,203,StageMonitor: Disable Browser Widget,I would like to define the disabling the browser widget in  stagemonitor.properties .
Stagemonitor,38184669,38195456.0,1,"2016/07/04, 15:21:39",True,"2017/07/01, 16:10:01","2017/07/01, 16:10:01",1679544.0,3568.0,0,203,StageMonitor: Disable Browser Widget,"Is there any java system properties like as "" stagemonitor.browserwidget.activate=false """
Stagemonitor,43806990,nan,1,"2017/05/05, 17:01:27",False,"2017/05/05, 21:23:27",nan,4279220.0,11.0,0,53,Using Stagemonitor/Hyperic HQ to monitor client specific performance stats in a multi tenancy environment,Using a multi-tenant server is it possible to use these tools to get stats regarding specific clients performance usage.
Stagemonitor,43806990,nan,1,"2017/05/05, 17:01:27",False,"2017/05/05, 21:23:27",nan,4279220.0,11.0,0,53,Using Stagemonitor/Hyperic HQ to monitor client specific performance stats in a multi tenancy environment,The setup would be a MySQL DB which holds users belonging to organisations.
Stagemonitor,43806990,nan,1,"2017/05/05, 17:01:27",False,"2017/05/05, 21:23:27",nan,4279220.0,11.0,0,53,Using Stagemonitor/Hyperic HQ to monitor client specific performance stats in a multi tenancy environment,When the Java application is running all actions will be carried out by a User collection which has the Organisation ID variable.
Stagemonitor,43806990,nan,1,"2017/05/05, 17:01:27",False,"2017/05/05, 21:23:27",nan,4279220.0,11.0,0,53,Using Stagemonitor/Hyperic HQ to monitor client specific performance stats in a multi tenancy environment,"Could this data then be used to work out how much CPU, memory, heap, processes etc are being used per Organisation?"
Stagemonitor,43806990,nan,1,"2017/05/05, 17:01:27",False,"2017/05/05, 21:23:27",nan,4279220.0,11.0,0,53,Using Stagemonitor/Hyperic HQ to monitor client specific performance stats in a multi tenancy environment,Thanks
Stagemonitor,48162254,48163586.0,1,"2018/01/09, 08:03:04",True,"2018/01/09, 09:52:48",nan,8109504.0,239.0,0,78,How to use Metrics in Stagemonitor,I am using stagemonitor for get metrics for different methods.
Stagemonitor,48162254,48163586.0,1,"2018/01/09, 08:03:04",True,"2018/01/09, 09:52:48",nan,8109504.0,239.0,0,78,How to use Metrics in Stagemonitor,"I'm using PetClinic application locally for get a idea and using browser-widget for testing.I find that we can track metrics using @Timed , @Metered annotations."
Stagemonitor,48162254,48163586.0,1,"2018/01/09, 08:03:04",True,"2018/01/09, 09:52:48",nan,8109504.0,239.0,0,78,How to use Metrics in Stagemonitor,Is it possible to use them in the petclinic application and view them from browser-widget.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,I have a Spring MVC web application running on tomcat.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,"I need to monitor my application for performance, log the time taken by each method call along with the values of the parameters."
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,"I would need this logging for all the methods in all the controllers, services, util classes inside the application."
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,I have seen the question posted earlier here :  How to log the time taken by methods in Springframework?
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,"As for the solutions proposed for that question., I have the following concerns in my case."
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,1) Using Spring AOP for logging - Closely matches the requirements but as far as I know it requires adding annotations to each and every method - would prefer to avoid changing current application.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,2) Stagemonitor - Could not follow the installation instructions - it requires installation of docker which I could not because of OS limitation.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,I am working on openSUSE 11.3 where as docker is available for openSUSE 12.3+
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,3) SpringInsight - It is a great tool and exactly matches my requirements.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,"But the problem is, it runs is vfabric-tc-server instance."
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,I tried setting up it on tomcat 7 using the steps mentioned by Daniel in  Using Spring Insight with Tomcat 6  but it did not workout as none of the jars in insight application has the class com.springsource.insight.collection.tcserver.ltw.TomcatWeavingInsightClassLoader which was supposed to be referred from server.xml.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,Tried adding external jar but it did not work.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,I'm wondering if there are any other tools which
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,-- will not require changing existing application MUCH - simple configuration should be acceptable.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,-- will give method level performance monitoring .
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,-- strictly should not need to migrate the existing applications to other server.
Stagemonitor,25204392,nan,1,"2014/08/08, 16:13:29",True,"2014/09/23, 23:23:43","2017/05/23, 13:30:33",837881.0,261.0,6,4671,Logging and Monitoring Spring MVC Web application,Thanks in advance :)
Wavefront VMware,63635003,nan,1,"2020/08/28, 16:43:08",False,"2020/09/15, 03:17:06",nan,14086712.0,11.0,0,35,Facing issue with using Gemfire cache using PCF Tanzu Gemfire service Springboot microservice,I am working on Springboot REST microservice.
Wavefront VMware,63635003,nan,1,"2020/08/28, 16:43:08",False,"2020/09/15, 03:17:06",nan,14086712.0,11.0,0,35,Facing issue with using Gemfire cache using PCF Tanzu Gemfire service Springboot microservice,Facing issue while doing clear region on partition region type.
Wavefront VMware,63635003,nan,1,"2020/08/28, 16:43:08",False,"2020/09/15, 03:17:06",nan,14086712.0,11.0,0,35,Facing issue with using Gemfire cache using PCF Tanzu Gemfire service Springboot microservice,How to configure Gemfire cache so that my client should be able to add region to server while startup.
Wavefront VMware,63635003,nan,1,"2020/08/28, 16:43:08",False,"2020/09/15, 03:17:06",nan,14086712.0,11.0,0,35,Facing issue with using Gemfire cache using PCF Tanzu Gemfire service Springboot microservice,Also I am thinking of exposing an API to evict cache explicitly.
Wavefront VMware,63635003,nan,1,"2020/08/28, 16:43:08",False,"2020/09/15, 03:17:06",nan,14086712.0,11.0,0,35,Facing issue with using Gemfire cache using PCF Tanzu Gemfire service Springboot microservice,Is there a way we can do it?
Wavefront VMware,63635003,nan,1,"2020/08/28, 16:43:08",False,"2020/09/15, 03:17:06",nan,14086712.0,11.0,0,35,Facing issue with using Gemfire cache using PCF Tanzu Gemfire service Springboot microservice,We had Redis cache implemented now but we want to switch to gemfire as it offers nice GUI to track regions and data using PULSE.
Wavefront VMware,63635003,nan,1,"2020/08/28, 16:43:08",False,"2020/09/15, 03:17:06",nan,14086712.0,11.0,0,35,Facing issue with using Gemfire cache using PCF Tanzu Gemfire service Springboot microservice,Questions:
Wavefront VMware,63635003,nan,1,"2020/08/28, 16:43:08",False,"2020/09/15, 03:17:06",nan,14086712.0,11.0,0,35,Facing issue with using Gemfire cache using PCF Tanzu Gemfire service Springboot microservice,Code:
Wavefront VMware,66545598,66565598.0,2,"2021/03/09, 12:58:51",True,"2021/03/15, 09:54:59",nan,8368744.0,13.0,0,22,"Deployed small footprint tanzu application service(tas) in Azure,without no domains.Can i access the ccapi and apps manager with the IP?","Could deploy Bosh and small footprint tanzu application service(tas) in Azure, without using the domains.All Vms are running.Can i access the ccapi and apps manager with the IP address instead of the api.SYSTEMDOMAIN?"
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,This is a follow-up question of  How to implement HTTP request/reply when the response comes from a rabbitMQ reply queue using Spring Integration DSL?
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,.
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,We were able to build the Spring Integration application and the SCDF stream successfully locally.
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,We could send a http request to the rabbitMQ request queue which was bound to the SCDF stream rabbit source.
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,We could also receive the response back from the rabbitMQ response queue which was bound to the SCDF stream rabbit sink.
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,We have deployed the SCDF stream into PCF environment which had a binding of an internal rabbitMQ broker.
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,"Now we need to specify the spring rabbitMQ connection information in the Spring Integration application properties - currently it's using the default localhost@5762, which is no longer valid."
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,Does anyone know how to get this rabbitMQ configuration properties?
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,We already checked the SCDF stream rabbit source/sink log files but couldn't find the information.
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,"I know we probably need to check internally whoever set up the SCDF/rabbitMQ in PCF environment, but so far we haven't heard the answers from them."
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,"Also, it appears we can have a different approach that binds both the SCDF stream and the integration application to a separate rabbitMQ instance (instead of using the existing one bundled with the SCDF configuration)."
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,Is it a recommended solution?
Wavefront VMware,67007310,nan,1,"2021/04/08, 18:34:38",True,"2021/04/08, 18:47:38","2021/04/08, 18:43:37",15230464.0,11.0,0,38,How do I find the connection information of a RabbitMQ server that is bound to a SCDF stream deployed on Tanzu (Pivotal/PCF) environment?,"Thanks,"
Wavefront VMware,63327611,nan,2,"2020/08/09, 17:37:16",True,"2021/04/17, 07:49:29","2020/08/18, 20:55:48",8859965.0,11.0,-1,149,VMware Tanzu (former PCF) App Autoscaler force scale-down?,I am autoscaling my application based on the HTTP throughput.
Wavefront VMware,63327611,nan,2,"2020/08/09, 17:37:16",True,"2021/04/17, 07:49:29","2020/08/18, 20:55:48",8859965.0,11.0,-1,149,VMware Tanzu (former PCF) App Autoscaler force scale-down?,My question here is when it reaches min threshold it tries to reduce the instance created.
Wavefront VMware,63327611,nan,2,"2020/08/09, 17:37:16",True,"2021/04/17, 07:49:29","2020/08/18, 20:55:48",8859965.0,11.0,-1,149,VMware Tanzu (former PCF) App Autoscaler force scale-down?,But during reducing the instance count if my instance is running or it is processing prev HTTP request.
Wavefront VMware,63327611,nan,2,"2020/08/09, 17:37:16",True,"2021/04/17, 07:49:29","2020/08/18, 20:55:48",8859965.0,11.0,-1,149,VMware Tanzu (former PCF) App Autoscaler force scale-down?,"In this case, it will wait till the processing completes or it forcibly reduces the instance count when reached threshold."
Wavefront VMware,59047752,nan,1,"2019/11/26, 11:32:58",False,"2020/01/11, 21:31:36",nan,12436298.0,11.0,1,419,Workaround to run Sonobuoy e2e tests on K8S cluster having node taints,"I'm trying to run E2E Kubernetes tests using Sonobuoy, but if one of the nodes has custom taints (NoSchedule) I'm getting"
Wavefront VMware,59047752,nan,1,"2019/11/26, 11:32:58",False,"2020/01/11, 21:31:36",nan,12436298.0,11.0,1,419,Workaround to run Sonobuoy e2e tests on K8S cluster having node taints,Kubernetes version is 1.12.
Wavefront VMware,59047752,nan,1,"2019/11/26, 11:32:58",False,"2020/01/11, 21:31:36",nan,12436298.0,11.0,1,419,Workaround to run Sonobuoy e2e tests on K8S cluster having node taints,If I remove taints in my test cluster E2E tests passed successfully.
Wavefront VMware,59047752,nan,1,"2019/11/26, 11:32:58",False,"2020/01/11, 21:31:36",nan,12436298.0,11.0,1,419,Workaround to run Sonobuoy e2e tests on K8S cluster having node taints,I've found that it was fixed in 1.17 version (see:  https://github.com/vmware-tanzu/sonobuoy/issues/599   https://github.com/kubernetes/kubernetes/issues/74282   https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.17.md  )
Wavefront VMware,59047752,nan,1,"2019/11/26, 11:32:58",False,"2020/01/11, 21:31:36",nan,12436298.0,11.0,1,419,Workaround to run Sonobuoy e2e tests on K8S cluster having node taints,Is there a workaround to run e2e tests on production 1.12 Kubernetes cluster having node taints?
Wavefront VMware,59047752,nan,1,"2019/11/26, 11:32:58",False,"2020/01/11, 21:31:36",nan,12436298.0,11.0,1,419,Workaround to run Sonobuoy e2e tests on K8S cluster having node taints,Except for waiting for 1.17 version and cluster upgrade of course.
Wavefront VMware,64453467,nan,1,"2020/10/21, 00:19:21",True,"2020/10/21, 20:36:57",nan,967522.0,796.0,0,682,Is Pivotal Cloud Foundry dead?,"Today I created a PCF Account (for testing purposes) and it will not let me do anything , it is telling me this:"
Wavefront VMware,64453467,nan,1,"2020/10/21, 00:19:21",True,"2020/10/21, 20:36:57",nan,967522.0,796.0,0,682,Is Pivotal Cloud Foundry dead?,"&quot;We will no longer be accepting any new PWS account sign-ups after September 17, 2020."
Wavefront VMware,64453467,nan,1,"2020/10/21, 00:19:21",True,"2020/10/21, 20:36:57",nan,967522.0,796.0,0,682,Is Pivotal Cloud Foundry dead?,Please do not hesitate to contact us with any questions.
Wavefront VMware,64453467,nan,1,"2020/10/21, 00:19:21",True,"2020/10/21, 20:36:57",nan,967522.0,796.0,0,682,Is Pivotal Cloud Foundry dead?,"If you are interested in an enterprise-grade service for hosting applications, Tanzu Application Service (TAS) offers this capability."
Wavefront VMware,64453467,nan,1,"2020/10/21, 00:19:21",True,"2020/10/21, 20:36:57",nan,967522.0,796.0,0,682,Is Pivotal Cloud Foundry dead?,The VMware Team&quot;
Wavefront VMware,64453467,nan,1,"2020/10/21, 00:19:21",True,"2020/10/21, 20:36:57",nan,967522.0,796.0,0,682,Is Pivotal Cloud Foundry dead?,Does this mean that I need to move my production applications from PCF to VMware Tanzu ?
Wavefront VMware,64453467,nan,1,"2020/10/21, 00:19:21",True,"2020/10/21, 20:36:57",nan,967522.0,796.0,0,682,Is Pivotal Cloud Foundry dead?,Is PCF Dead?
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,"So, I have followed the documentation in:"
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,"And I am still at a loss of how do I install Velero using its helm chart at  https://github.com/vmware-tanzu/helm-charts , because I cannot reconcile the helm chart based installation with what is documented in
 https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure"
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,Here is what I have done so far:
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,"Now I need to update the  values.yaml  file, but I am stuck at the credentials section."
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,I am unable to reconcile the  velero install  instructions given on  https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure  with what I see in the values.yaml file.
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,E.g.
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,I have created the  credentials-velero  file:
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,"Of course, it does not contain the MSI credentials and the MSI name would be associated using the dedicated label."
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,The  doc  says clearly:
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,"If you're using AAD Pod Identity, you now need to add the
aadpodidbinding=$IDENTITY_NAME label to the Velero pod(s), preferably
through the Deployment's pod template."
Wavefront VMware,64327529,nan,2,"2020/10/13, 05:14:34",False,"2020/12/19, 18:50:18","2020/10/27, 01:35:44",80002.0,48572.0,0,382,How to install Velero with helm and with Azure Managed Identity?,But how do I do it when I install Velero using a helm chart?
Zipkin,50642453,nan,0,"2018/06/01, 14:30:22",False,"2018/06/03, 16:17:05","2018/06/01, 18:15:44",4403581.0,703.0,10,201,Logs are not received in Hawkular APM from Zipkin Client,I have client application instrumented with Zipkin library with configuration in spring application.properties .
Zipkin,50642453,nan,0,"2018/06/01, 14:30:22",False,"2018/06/03, 16:17:05","2018/06/01, 18:15:44",4403581.0,703.0,10,201,Logs are not received in Hawkular APM from Zipkin Client,Maven dependency
Zipkin,50642453,nan,0,"2018/06/01, 14:30:22",False,"2018/06/03, 16:17:05","2018/06/01, 18:15:44",4403581.0,703.0,10,201,Logs are not received in Hawkular APM from Zipkin Client,The hawkular apm server console is reachable from the local machine.
Zipkin,50642453,nan,0,"2018/06/01, 14:30:22",False,"2018/06/03, 16:17:05","2018/06/01, 18:15:44",4403581.0,703.0,10,201,Logs are not received in Hawkular APM from Zipkin Client,"However, when the rest api exposed in the client application is invoked, the zipkin trace is logged but they are not collected at the hawkular apm server."
Zipkin,50642453,nan,0,"2018/06/01, 14:30:22",False,"2018/06/03, 16:17:05","2018/06/01, 18:15:44",4403581.0,703.0,10,201,Logs are not received in Hawkular APM from Zipkin Client,"I am not sure if its a configuration issue at client application, as the Hawkular APM UI is opening properly."
Zipkin,50642453,nan,0,"2018/06/01, 14:30:22",False,"2018/06/03, 16:17:05","2018/06/01, 18:15:44",4403581.0,703.0,10,201,Logs are not received in Hawkular APM from Zipkin Client,"As per my understanding, Zipkin client can be integrated with Hawkular apm by simple replacing the hawkular url in place of zipkin server, but this does not seem to work."
Zipkin,50642453,nan,0,"2018/06/01, 14:30:22",False,"2018/06/03, 16:17:05","2018/06/01, 18:15:44",4403581.0,703.0,10,201,Logs are not received in Hawkular APM from Zipkin Client,"Any suggestions on this, unfortunately I could not find any examples too."
Zipkin,47670883,47672314.0,3,"2017/12/06, 11:31:36",True,"2019/01/27, 02:18:35",nan,1358676.0,3566.0,9,6777,Sleuth not sending trace information to Zipkin,"Sleuth is not sending the trace information to Zipkin, even though Zipkin is running fine."
Zipkin,47670883,47672314.0,3,"2017/12/06, 11:31:36",True,"2019/01/27, 02:18:35",nan,1358676.0,3566.0,9,6777,Sleuth not sending trace information to Zipkin,"I am using Spring 1.5.8.RELEASE, spring cloud Dalston.SR4 and I have added the below dependencies in my microservices:"
Zipkin,47670883,47672314.0,3,"2017/12/06, 11:31:36",True,"2019/01/27, 02:18:35",nan,1358676.0,3566.0,9,6777,Sleuth not sending trace information to Zipkin,"My Logs are always coming false:
[FOOMS,2e740f33c26e286d,2e740f33c26e286d,false]"
Zipkin,47670883,47672314.0,3,"2017/12/06, 11:31:36",True,"2019/01/27, 02:18:35",nan,1358676.0,3566.0,9,6777,Sleuth not sending trace information to Zipkin,My Zipkin dependencies are:
Zipkin,47670883,47672314.0,3,"2017/12/06, 11:31:36",True,"2019/01/27, 02:18:35",nan,1358676.0,3566.0,9,6777,Sleuth not sending trace information to Zipkin,Why am I getting false instead of true in my slueth statements?
Zipkin,47670883,47672314.0,3,"2017/12/06, 11:31:36",True,"2019/01/27, 02:18:35",nan,1358676.0,3566.0,9,6777,Sleuth not sending trace information to Zipkin,The traceId and SpanId are properly generated for all the calls though.
Zipkin,47670883,47672314.0,3,"2017/12/06, 11:31:36",True,"2019/01/27, 02:18:35",nan,1358676.0,3566.0,9,6777,Sleuth not sending trace information to Zipkin,My Zipkin is running in port 9411
Zipkin,34272334,nan,2,"2015/12/14, 18:46:57",True,"2020/06/13, 00:52:04",nan,757695.0,4471.0,7,834,zipkin examples not on Github,Does anybody know where the zipking examples are located ?
Zipkin,34272334,nan,2,"2015/12/14, 18:46:57",True,"2020/06/13, 00:52:04",nan,757695.0,4471.0,7,834,zipkin examples not on Github,"https://twitter.github.io/zipkin/Quickstart.html#super-quickstart 
 In the following I can read:"
Zipkin,34272334,nan,2,"2015/12/14, 18:46:57",True,"2020/06/13, 00:52:04",nan,757695.0,4471.0,7,834,zipkin examples not on Github,I have found the zipking-example on Maven Central only.
Zipkin,34272334,nan,2,"2015/12/14, 18:46:57",True,"2020/06/13, 00:52:04",nan,757695.0,4471.0,7,834,zipkin examples not on Github,Not on Github.
Zipkin,34272334,nan,2,"2015/12/14, 18:46:57",True,"2020/06/13, 00:52:04",nan,757695.0,4471.0,7,834,zipkin examples not on Github,"1.2.1-rc24 
 It is still a bug in the documentation?"
Zipkin,34272334,nan,2,"2015/12/14, 18:46:57",True,"2020/06/13, 00:52:04",nan,757695.0,4471.0,7,834,zipkin examples not on Github,Documentation mentions zipkin-example which doesn't exist
Zipkin,59353834,nan,0,"2019/12/16, 11:33:23",False,"2019/12/25, 07:53:31","2019/12/16, 11:39:49",5559599.0,439.0,6,662,Zipkin with Spring Boot 2.2 and Hoxton Cloud,I wanted to update my project Zipkin setup to Spring Boot 2.2.2.RELEASE and Spring Cloud Hoxton.RELEASE but it looks like simple jars update is not enough.
Zipkin,59353834,nan,0,"2019/12/16, 11:33:23",False,"2019/12/25, 07:53:31","2019/12/16, 11:39:49",5559599.0,439.0,6,662,Zipkin with Spring Boot 2.2 and Hoxton Cloud,I thought the old setup (it was working fine for Spring Boot 2.1.5.RELEASE and Greenwich.SR2) would also work for Boot 2.2.2.RELEASE and Hoxton.RELEASE but it appears I still miss something here.
Zipkin,59353834,nan,0,"2019/12/16, 11:33:23",False,"2019/12/25, 07:53:31","2019/12/16, 11:39:49",5559599.0,439.0,6,662,Zipkin with Spring Boot 2.2 and Hoxton Cloud,"I'm getting the following exception ( java.lang.NoClassDefFoundError: zipkin2
/internal/Buffer$Writer ):"
Zipkin,45046528,nan,2,"2017/07/12, 03:40:57",True,"2018/10/30, 08:55:59",nan,775553.0,418.0,5,2805,Spring Initializer - Zipkin Server missing?,Can't see Zipkin Server when using Spring Initializer .
Zipkin,45046528,nan,2,"2017/07/12, 03:40:57",True,"2018/10/30, 08:55:59",nan,775553.0,418.0,5,2805,Spring Initializer - Zipkin Server missing?,Has it been removed?
Zipkin,45046528,nan,2,"2017/07/12, 03:40:57",True,"2018/10/30, 08:55:59",nan,775553.0,418.0,5,2805,Spring Initializer - Zipkin Server missing?,What is the alternative?
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,I have a Spring Boot app that I'm updating to 1.5.1.
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,"It works great, until I add Sleuth and Zipkin to classpath"
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,"when these lines are present, I get"
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,This is my dep.
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,management
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,I tried change to Dalston
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,but the errors get even stranger
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,and
Zipkin,42402849,nan,2,"2017/02/22, 23:42:28",True,"2017/11/07, 21:14:23",nan,587406.0,5707.0,5,7229,app fails to start when sleuth and zipkin are added,am I missing something I haven't noticed yet?
Zipkin,39174579,39174580.0,1,"2016/08/27, 00:02:27",True,"2018/03/22, 21:19:16",nan,2733462.0,473.0,5,6818,How to configure Spring Cloud Zipkin Server with MySQL for persistence?,Which exact dependencies and  application.yml  configuration are required for Spring Boot/Cloud Zipkin server (potentially Zipkin Stream server) to persist the tracing data using MySQL?
Zipkin,60023762,nan,1,"2020/02/02, 08:27:45",False,"2021/03/27, 13:21:49",nan,12826907.0,41.0,4,1561,zipkin error Error creating bean with name &#39;webMvcMetricsFilter&#39; defined in class path resource,When I tried to integrate zipkin.
Zipkin,60023762,nan,1,"2020/02/02, 08:27:45",False,"2021/03/27, 13:21:49",nan,12826907.0,41.0,4,1561,zipkin error Error creating bean with name &#39;webMvcMetricsFilter&#39; defined in class path resource,It threw this error
Zipkin,60023762,nan,1,"2020/02/02, 08:27:45",False,"2021/03/27, 13:21:49",nan,12826907.0,41.0,4,1561,zipkin error Error creating bean with name &#39;webMvcMetricsFilter&#39; defined in class path resource,version:
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,"Prerequisites: 
 Node.js  application 
 Opencensus  library 
 Zipkin Exporter  and local Zipkin service"
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,app.js :
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,package.json :
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,Zipkin  server started locally with command:
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,"after triggering  /service1  Zipkin Ui displays 2 spans for 2 different requests: 
first  /service1  incoming request that is configured in Node.js routers 
second  /external_service_2  is subsequent call to external service"
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,Problem
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,"The problem is that after triggering  /service1 : 
  1."
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,"Zipkin UI displays 2 spans with same name  MyApplication (see image),  but expected 2 different span names"
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,2.
Zipkin,59444729,59544246.0,1,"2019/12/22, 15:45:56",True,"2019/12/31, 19:46:58","2019/12/23, 19:25:31",170830.0,12422.0,4,431,Zipkin (Opencensus) - 2 Spans with same names instead of different,"As far Zipkin UI displays 2 spans with same name,  service dependencies page contains one Service only(see image)"
Zipkin,35215821,nan,1,"2016/02/05, 05:30:37",False,"2016/07/23, 06:56:00",nan,5886280.0,41.0,4,1749,Add RabbitMQ distributed tracing to Zipkin,We have been playing around with Brave(Java implementation of Zipkin) and successfully added tracing for REST and database calls.
Zipkin,35215821,nan,1,"2016/02/05, 05:30:37",False,"2016/07/23, 06:56:00",nan,5886280.0,41.0,4,1749,Add RabbitMQ distributed tracing to Zipkin,We would like to also add RabbitMQ to the tracing and would like some thoughts from anyone who may have had similar experiences that they could share.
Zipkin,35215821,nan,1,"2016/02/05, 05:30:37",False,"2016/07/23, 06:56:00",nan,5886280.0,41.0,4,1749,Add RabbitMQ distributed tracing to Zipkin,We have tried to find some stuff online but can't seem to find an interceptor we could add to our rabbit implementation.
Zipkin,35215821,nan,1,"2016/02/05, 05:30:37",False,"2016/07/23, 06:56:00",nan,5886280.0,41.0,4,1749,Add RabbitMQ distributed tracing to Zipkin,Can you recommend anything?
Zipkin,35215821,nan,1,"2016/02/05, 05:30:37",False,"2016/07/23, 06:56:00",nan,5886280.0,41.0,4,1749,Add RabbitMQ distributed tracing to Zipkin,Thanks in advance.
Zipkin,16981751,16981952.0,3,"2013/06/07, 13:17:46",True,"2018/10/17, 13:15:26","2013/06/07, 13:30:42",1542639.0,557.0,4,2050,Is zipkin suitable for tracing method invocation?,"I read about zipkin, but from my understanding, zipkin is suitable for tracking history of network requests and time (via Finagle)."
Zipkin,16981751,16981952.0,3,"2013/06/07, 13:17:46",True,"2018/10/17, 13:15:26","2013/06/07, 13:30:42",1542639.0,557.0,4,2050,Is zipkin suitable for tracing method invocation?,"However, is it possible for me to use zipkin to track java method invocation time and location?"
Zipkin,16981751,16981952.0,3,"2013/06/07, 13:17:46",True,"2018/10/17, 13:15:26","2013/06/07, 13:30:42",1542639.0,557.0,4,2050,Is zipkin suitable for tracing method invocation?,"For example, I want to track how long it takes for  foobar()  to execute, and what are other methods internally called by  foobar()  and its execution time and so on."
Zipkin,54262815,54816556.0,3,"2019/01/19, 01:46:21",True,"2020/06/03, 21:51:55",nan,10935837.0,53.0,3,2502,Spring Cloud Feign + Sleuth + Zipkin - original request is required,I have multiservices application which is using Spring Cloud OpenFeign.
Zipkin,54262815,54816556.0,3,"2019/01/19, 01:46:21",True,"2020/06/03, 21:51:55",nan,10935837.0,53.0,3,2502,Spring Cloud Feign + Sleuth + Zipkin - original request is required,Now I have to use zipkin with that app.
Zipkin,54262815,54816556.0,3,"2019/01/19, 01:46:21",True,"2020/06/03, 21:51:55",nan,10935837.0,53.0,3,2502,Spring Cloud Feign + Sleuth + Zipkin - original request is required,I remember that when i had app without Feign I just added Sleuth and Zipkin starters dependencies and run zipkin server on port 9411.
Zipkin,54262815,54816556.0,3,"2019/01/19, 01:46:21",True,"2020/06/03, 21:51:55",nan,10935837.0,53.0,3,2502,Spring Cloud Feign + Sleuth + Zipkin - original request is required,After that Zipkin worked well..
Zipkin,54262815,54816556.0,3,"2019/01/19, 01:46:21",True,"2020/06/03, 21:51:55",nan,10935837.0,53.0,3,2502,Spring Cloud Feign + Sleuth + Zipkin - original request is required,"But now, when i try same in my app with Feign i get error 500  ""original request is required"" ."
Zipkin,54262815,54816556.0,3,"2019/01/19, 01:46:21",True,"2020/06/03, 21:51:55",nan,10935837.0,53.0,3,2502,Spring Cloud Feign + Sleuth + Zipkin - original request is required,I guess that Feign has some problems with headers when Sleuth add traces informations.
Zipkin,54262815,54816556.0,3,"2019/01/19, 01:46:21",True,"2020/06/03, 21:51:55",nan,10935837.0,53.0,3,2502,Spring Cloud Feign + Sleuth + Zipkin - original request is required,Can you help me fix this?
Zipkin,43025795,nan,1,"2017/03/26, 09:29:08",True,"2018/03/23, 16:18:26","2018/03/23, 16:18:26",7768589.0,31.0,3,695,Cannot see the trace data in zipkin,I'm new to zipkin and brave api for distribute tracing.
Zipkin,43025795,nan,1,"2017/03/26, 09:29:08",True,"2018/03/23, 16:18:26","2018/03/23, 16:18:26",7768589.0,31.0,3,695,Cannot see the trace data in zipkin,I've setup a zipkin server on my localhost listening on port 9411.
Zipkin,43025795,nan,1,"2017/03/26, 09:29:08",True,"2018/03/23, 16:18:26","2018/03/23, 16:18:26",7768589.0,31.0,3,695,Cannot see the trace data in zipkin,I've executed below function but there is no trace data show in my zipkin server.
Zipkin,43025795,nan,1,"2017/03/26, 09:29:08",True,"2018/03/23, 16:18:26","2018/03/23, 16:18:26",7768589.0,31.0,3,695,Cannot see the trace data in zipkin,Could someone point out what I'm missing?
Zipkin,41508180,41628945.0,1,"2017/01/06, 16:50:58",True,"2017/01/13, 08:52:38","2017/01/13, 08:42:34",633961.0,26851.0,3,453,Zipkin for profiling the internals of a traditional progamm,I want to use zipkin to profile the internals of a traditional program.
Zipkin,41508180,41628945.0,1,"2017/01/06, 16:50:58",True,"2017/01/13, 08:52:38","2017/01/13, 08:42:34",633961.0,26851.0,3,453,Zipkin for profiling the internals of a traditional progamm,"I use the term ""traditional"", since AFAIK zipkin is for tracing in a microservice environment where one request gets computed by N sub-requests."
Zipkin,41508180,41628945.0,1,"2017/01/06, 16:50:58",True,"2017/01/13, 08:52:38","2017/01/13, 08:42:34",633961.0,26851.0,3,453,Zipkin for profiling the internals of a traditional progamm,I would like to analyse the performance of my python program.
Zipkin,41508180,41628945.0,1,"2017/01/06, 16:50:58",True,"2017/01/13, 08:52:38","2017/01/13, 08:42:34",633961.0,26851.0,3,453,Zipkin for profiling the internals of a traditional progamm,I would like to trace all python method calls and all linux syscalls which gets done.
Zipkin,41508180,41628945.0,1,"2017/01/06, 16:50:58",True,"2017/01/13, 08:52:38","2017/01/13, 08:42:34",633961.0,26851.0,3,453,Zipkin for profiling the internals of a traditional progamm,How to trace the python method calls and linux syscalls to get the spans into zipkin?
Zipkin,41508180,41628945.0,1,"2017/01/06, 16:50:58",True,"2017/01/13, 08:52:38","2017/01/13, 08:42:34",633961.0,26851.0,3,453,Zipkin for profiling the internals of a traditional progamm,"Even if it is not feasible, I am interesting how this could be done."
Zipkin,41508180,41628945.0,1,"2017/01/06, 16:50:58",True,"2017/01/13, 08:52:38","2017/01/13, 08:42:34",633961.0,26851.0,3,453,Zipkin for profiling the internals of a traditional progamm,I would like to learn how zipkin works.
Zipkin,65873809,nan,1,"2021/01/24, 19:32:20",False,"2021/01/25, 11:19:25",nan,3169556.0,2513.0,2,54,Usage example of kafka zipkin interceptors,"So, we are using kafka queues internally for some microservices' communication, also zipkin for distributed tracing."
Zipkin,65873809,nan,1,"2021/01/24, 19:32:20",False,"2021/01/25, 11:19:25",nan,3169556.0,2513.0,2,54,Usage example of kafka zipkin interceptors,Would you suggest how to bring in kafka traces in zipkin server for debugability.
Zipkin,65873809,nan,1,"2021/01/24, 19:32:20",False,"2021/01/25, 11:19:25",nan,3169556.0,2513.0,2,54,Usage example of kafka zipkin interceptors,"I came across the  brave-kafka-interceptor , but could not understand it with with kafka from the minimal example provided."
Zipkin,65873809,nan,1,"2021/01/24, 19:32:20",False,"2021/01/25, 11:19:25",nan,3169556.0,2513.0,2,54,Usage example of kafka zipkin interceptors,"Is there any other example around, or something altogether different library is used."
Zipkin,64362503,nan,0,"2020/10/15, 01:45:54",False,"2020/10/15, 01:45:54",nan,8432036.0,43.0,2,53,How to output zipkin tracing results to a file in python?,I am using py_zipkin in my code.
Zipkin,64362503,nan,0,"2020/10/15, 01:45:54",False,"2020/10/15, 01:45:54",nan,8432036.0,43.0,2,53,How to output zipkin tracing results to a file in python?,And I can see the tracing result on the Zipkin UI.
Zipkin,64362503,nan,0,"2020/10/15, 01:45:54",False,"2020/10/15, 01:45:54",nan,8432036.0,43.0,2,53,How to output zipkin tracing results to a file in python?,"But I don't know how to output the tracing results to a file with specified format, like a log file."
Zipkin,64362503,nan,0,"2020/10/15, 01:45:54",False,"2020/10/15, 01:45:54",nan,8432036.0,43.0,2,53,How to output zipkin tracing results to a file in python?,Here is a example of my code:
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,Small question about the possibility to integrate Zipkin with Prometheus.
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,"Currently, we have a working Zipkin instance fully ready, with its web UI."
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,"Zipkin is super cool, everything is fine."
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,"We are able to have all micro services sending traces to Zipkin, and having Zipkin aggregating them."
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,"We can also search the traces in the UI, etc, super cool."
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,"On the other hand, we also have a very mature battle tested Prometheus Grafana, where container level metrics, application level metrics, and many other observations are already present in it."
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,"Hence, currently, we have two places where we have to look at for production."
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,"Our everything in one place Prometheus, and this super cool Zipkin."
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,"I was wondering, would it be possible to have Prometheus as the back end, or some kind of Prometheus consuming Zipkin data to display in Grafana, so we truly have all in one place please?"
Zipkin,64094479,64096381.0,1,"2020/09/28, 03:22:06",True,"2020/09/28, 08:27:01",nan,10461625.0,1094.0,2,246,Zipkin traces in Prometheus,Thank you
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,I am trying to add tracing on a Wildfly server (specifically Keycloak Docker image)
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,Following this document  https://docs.wildfly.org/19/Admin_Guide.html#MicroProfile_OpenTracing_SmallRye
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,I got as far as
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,But I can't get the next parts working to set it to point to zipkin:9411
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,The next command in the instructions failed
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,"However, doing it using  /opt/jboss/startup-scripts/  also fails"
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,Using @ehsavoie answer I got a bit further
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,but still does not log to zipkin which uses B3.
Zipkin,61570149,nan,2,"2020/05/03, 08:48:16",True,"2020/05/07, 07:30:02","2020/05/07, 07:30:02",242042.0,22372.0,2,293,Adding Zipkin tracing to WildFly via CLI using startup scripts,I also tried
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,When i run my spring boot application locally i always have trace information with exportable information = true
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,but when app running on AWS ECS in docker container i have always exportable false for all logs
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,except classpath  org.hibernate .
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,On this classpath exportable = true
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,I use below dependencies
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,And below properties :
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,Which may be the reason for different actions ?
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,Maybe log levels ?
Zipkin,56238284,nan,0,"2019/05/21, 15:27:03",False,"2019/05/21, 15:27:03",nan,6380991.0,171.0,2,563,Sleuth can&#39;t send traces to zipkin. Exportable always false,base-url:  http://zipkin-server   must be reachable ?
Zipkin,54551878,nan,0,"2019/02/06, 12:52:06",False,"2019/02/06, 12:52:06",nan,3801239.0,3013.0,2,124,how to display trace data in zipkin using playframework,"i am running zipkin via docker with this command docker  run -d -p 9411:9411 openzipkin/zipkin  and accessing its server at  http://localhost:9411/zipkin/  
I am using playframework-2.4 i am not getting the service name in zipkin ui also trace data is now showing up it shows  0 of 0 services"
Zipkin,54551878,nan,0,"2019/02/06, 12:52:06",False,"2019/02/06, 12:52:06",nan,3801239.0,3013.0,2,124,how to display trace data in zipkin using playframework,"here is my code 
application.conf"
Zipkin,54551878,nan,0,"2019/02/06, 12:52:06",False,"2019/02/06, 12:52:06",nan,3801239.0,3013.0,2,124,how to display trace data in zipkin using playframework,build.sbt
Zipkin,54551878,nan,0,"2019/02/06, 12:52:06",False,"2019/02/06, 12:52:06",nan,3801239.0,3013.0,2,124,how to display trace data in zipkin using playframework,"i am accessing  http://localhost:9000/direct-user/test1  it first then  http://localhost:9411  but trace data is not showing up 
is there any thing missing ?please help"
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),I need to send spans through RabbitMQ to Zipkin.
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),I'm Using Spring-Cloud-Sleuth Edgware-SR5 version and SpringBoot 1.5.3.RELEASE versions.
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),With older Spring-cloud sleuth version (spring-cloud-stream-binder-rabbit -  v1.1.4.RELEASE) it was working fine.
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),"When I try to start the service, I'm getting ""  ""AsyncReporter{RabbitMQSender{addresses=[localhost:5672], queue=zipkin}}."
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),"Unable to establish connection to RabbitMQ server"" error."
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),"I have gone through the documentations, but I could't able to resolve this issue."
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),Gradle Configuration:
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),Application.yml:
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),Exception StackTrace:
Zipkin,53208598,nan,1,"2018/11/08, 15:19:39",True,"2018/11/08, 16:31:04","2018/11/08, 15:46:22",1256795.0,31.0,2,1254,How to resolve RabbitMQ Server connection error in Spring-cloud-sleuth-zipkin (Edgware.SR5),"Thanks and Regards
Suresh"
Zipkin,52759855,52814922.0,2,"2018/10/11, 15:15:53",True,"2019/12/06, 18:13:45",nan,8460313.0,108.0,2,1391,Is there any method to get TraceId of Zipkin in Java Code,"I want to retrieve the TraceId of Zipkin, is there any method to get it ?"
Zipkin,52029459,52031368.0,2,"2018/08/26, 22:10:10",True,"2018/09/16, 13:54:14","2018/09/16, 13:48:30",6452043.0,391.0,2,8977,Spring cloud sleuth did not run with Zipkin,first i had a small issue with this class   brave.sampler.Sampler
Zipkin,52029459,52031368.0,2,"2018/08/26, 22:10:10",True,"2018/09/16, 13:54:14","2018/09/16, 13:48:30",6452043.0,391.0,2,8977,Spring cloud sleuth did not run with Zipkin,"could not import this class,  only imported when i added this dependency"
Zipkin,52029459,52031368.0,2,"2018/08/26, 22:10:10",True,"2018/09/16, 13:54:14","2018/09/16, 13:48:30",6452043.0,391.0,2,8977,Spring cloud sleuth did not run with Zipkin,"and my big problem is, when i tried to use zipkin  for disturbed tracing, i added the required dependency but whenever i start the applications, it through an exception in start."
Zipkin,52029459,52031368.0,2,"2018/08/26, 22:10:10",True,"2018/09/16, 13:54:14","2018/09/16, 13:48:30",6452043.0,391.0,2,8977,Spring cloud sleuth did not run with Zipkin,and this is the stack trace.
Zipkin,52029459,52031368.0,2,"2018/08/26, 22:10:10",True,"2018/09/16, 13:54:14","2018/09/16, 13:48:30",6452043.0,391.0,2,8977,Spring cloud sleuth did not run with Zipkin,my pom.xml
Zipkin,52029459,52031368.0,2,"2018/08/26, 22:10:10",True,"2018/09/16, 13:54:14","2018/09/16, 13:48:30",6452043.0,391.0,2,8977,Spring cloud sleuth did not run with Zipkin,"i would someone to help me fix those issues, also i want to understand why this exception comes, and why the sampler class does not imported only when i add it's dependency, but i see in other projects codes there are no needs for the dependency."
Zipkin,51688362,nan,0,"2018/08/04, 21:11:52",False,"2018/08/04, 21:11:52",nan,4403581.0,703.0,2,574,Zipkin Server not logging Http request traces in debug mode,"I have enabled the distributed tracing using Zipkin tracer as mentioned in 
 https://github.com/openzipkin/brave  for the microservices."
Zipkin,51688362,nan,0,"2018/08/04, 21:11:52",False,"2018/08/04, 21:11:52",nan,4403581.0,703.0,2,574,Zipkin Server not logging Http request traces in debug mode,I could see the service call info and time taken in the Zipkin server running in local machine.
Zipkin,51688362,nan,0,"2018/08/04, 21:11:52",False,"2018/08/04, 21:11:52",nan,4403581.0,703.0,2,574,Zipkin Server not logging Http request traces in debug mode,"I have usecase to capture the trace logs of every http requests alongside the JSON messages (with the Trace ,span and parent IDs) received by to Zipkin server from the client."
Zipkin,51688362,nan,0,"2018/08/04, 21:11:52",False,"2018/08/04, 21:11:52",nan,4403581.0,703.0,2,574,Zipkin Server not logging Http request traces in debug mode,"Zipkin server is started in debug mode to enable the logging, however the http requests are not logged in the Zipkin server logs."
Zipkin,51688362,nan,0,"2018/08/04, 21:11:52",False,"2018/08/04, 21:11:52",nan,4403581.0,703.0,2,574,Zipkin Server not logging Http request traces in debug mode,Can someone throw some light please?
Zipkin,50027127,nan,2,"2018/04/25, 19:27:13",True,"2019/01/31, 10:42:24",nan,3673633.0,507.0,2,2062,Zipkin UI displaying error message &#39;ERROR: cannot load service names: No message available&#39;,"Trying to create zipkin server with the dependencies added in gradle as below,"
Zipkin,50027127,nan,2,"2018/04/25, 19:27:13",True,"2019/01/31, 10:42:24",nan,3673633.0,507.0,2,2062,Zipkin UI displaying error message &#39;ERROR: cannot load service names: No message available&#39;,"Also,"
Zipkin,50027127,nan,2,"2018/04/25, 19:27:13",True,"2019/01/31, 10:42:24",nan,3673633.0,507.0,2,2062,Zipkin UI displaying error message &#39;ERROR: cannot load service names: No message available&#39;,"i have added properties in both application.properties and bootstrap.properties files like,"
Zipkin,50027127,nan,2,"2018/04/25, 19:27:13",True,"2019/01/31, 10:42:24",nan,3673633.0,507.0,2,2062,Zipkin UI displaying error message &#39;ERROR: cannot load service names: No message available&#39;,application.properties
Zipkin,50027127,nan,2,"2018/04/25, 19:27:13",True,"2019/01/31, 10:42:24",nan,3673633.0,507.0,2,2062,Zipkin UI displaying error message &#39;ERROR: cannot load service names: No message available&#39;,bootstrap.properties
Zipkin,50027127,nan,2,"2018/04/25, 19:27:13",True,"2019/01/31, 10:42:24",nan,3673633.0,507.0,2,2062,Zipkin UI displaying error message &#39;ERROR: cannot load service names: No message available&#39;,"Once i start the server and load the UI page i am getting error in UI as,"
Zipkin,49676752,nan,1,"2018/04/05, 19:00:43",False,"2018/06/15, 17:39:40",nan,3092618.0,143.0,2,154,Can Istio use existing zipkin?,Can istio send tracing information to an external zipkin instance?
Zipkin,49676752,nan,1,"2018/04/05, 19:00:43",False,"2018/06/15, 17:39:40",nan,3092618.0,143.0,2,154,Can Istio use existing zipkin?,It seems like istio has hard coded zipkin address several places to expect it in the istio-system namespace(by referencing it without a namespace).
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,By default Spring Sleuth only sends 10% of requests to Zipkin.
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,By setting  spring.sleuth.sampler.percentage  you can increase the percentage.
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,Unfortunately it is stuck at 10% regardless of what value I set it to.
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,"I have tried 1.0, 0.5, 1, 100."
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,Output from  /env
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,"Regardless of the value, when I make multiple requests, only 10% make it to Zipkin."
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,We are using version Finchley.M8 of Spring Cloud and 2.0.0.RELEASE of Spring Boot.
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,Below are relevant POM settings.
Zipkin,49182626,nan,1,"2018/03/08, 23:17:52",True,"2018/03/08, 23:38:09",nan,1490322.0,9818.0,2,2244,Spring Sleuth stuck sending 10 percent of request to Zipkin,Could this be a bug?
Zipkin,47764295,47777632.0,3,"2017/12/12, 04:03:27",True,"2020/10/02, 11:59:59",nan,6720038.0,35.0,2,2988,How to configure Spring Cloud Zipkin server with Elasticsearch for persistence?,I want to load my Spring Cloud zipkin-server with elasticsearch.
Zipkin,47764295,47777632.0,3,"2017/12/12, 04:03:27",True,"2020/10/02, 11:59:59",nan,6720038.0,35.0,2,2988,How to configure Spring Cloud Zipkin server with Elasticsearch for persistence?,"I think, I tried almost everything I could."
Zipkin,47764295,47777632.0,3,"2017/12/12, 04:03:27",True,"2020/10/02, 11:59:59",nan,6720038.0,35.0,2,2988,How to configure Spring Cloud Zipkin server with Elasticsearch for persistence?,"but, It still running with in-memory."
Zipkin,47764295,47777632.0,3,"2017/12/12, 04:03:27",True,"2020/10/02, 11:59:59",nan,6720038.0,35.0,2,2988,How to configure Spring Cloud Zipkin server with Elasticsearch for persistence?,"(when I restart zipkin-server, all data is lost.)"
Zipkin,47764295,47777632.0,3,"2017/12/12, 04:03:27",True,"2020/10/02, 11:59:59",nan,6720038.0,35.0,2,2988,How to configure Spring Cloud Zipkin server with Elasticsearch for persistence?,I want to set up zipkin with elasticsearch.
Zipkin,47764295,47777632.0,3,"2017/12/12, 04:03:27",True,"2020/10/02, 11:59:59",nan,6720038.0,35.0,2,2988,How to configure Spring Cloud Zipkin server with Elasticsearch for persistence?,Please tell me which exact  dependencies  and  applicartion.yml  or any other things needed.
Zipkin,47195614,47195615.0,1,"2017/11/09, 08:50:24",True,"2017/11/09, 09:08:28","2017/11/09, 09:08:28",3067542.0,3909.0,2,1092,How to prevent sleuth / zipkin to trace catalog-services-watch?,"I've enabled zipkin on my application and it works fine, I see the traces."
Zipkin,47195614,47195615.0,1,"2017/11/09, 08:50:24",True,"2017/11/09, 09:08:28","2017/11/09, 09:08:28",3067542.0,3909.0,2,1092,How to prevent sleuth / zipkin to trace catalog-services-watch?,"My application is using Consul service discovery, and I see a lot of traffic being traced in Zipkin."
Zipkin,47195614,47195615.0,1,"2017/11/09, 08:50:24",True,"2017/11/09, 09:08:28","2017/11/09, 09:08:28",3067542.0,3909.0,2,1092,How to prevent sleuth / zipkin to trace catalog-services-watch?,"Traces are like have names like ""catalog-services_watch"" and contain things like :"
Zipkin,47195614,47195615.0,1,"2017/11/09, 08:50:24",True,"2017/11/09, 09:08:28","2017/11/09, 09:08:28",3067542.0,3909.0,2,1092,How to prevent sleuth / zipkin to trace catalog-services-watch?,How can I disable these traces ?
Zipkin,47195614,47195615.0,1,"2017/11/09, 08:50:24",True,"2017/11/09, 09:08:28","2017/11/09, 09:08:28",3067542.0,3909.0,2,1092,How to prevent sleuth / zipkin to trace catalog-services-watch?,"I've tried the spring.sleuth.instrument.web.skipPattern parameter, but it's not working."
Zipkin,45227503,nan,0,"2017/07/21, 04:20:34",False,"2017/07/21, 04:26:33","2017/07/21, 04:26:33",3179897.0,186.0,2,124,Zipkin Instrumentation for socket based app,"We have a socket based web app we currently developing using feathersJS, and we are currently leaning on using zipkin for performance tracking, but it seems that there's no instrumentation yet for socket based app, anyone have implemented Zipkin on socket based webapps?"
Zipkin,45227503,nan,0,"2017/07/21, 04:20:34",False,"2017/07/21, 04:26:33","2017/07/21, 04:26:33",3179897.0,186.0,2,124,Zipkin Instrumentation for socket based app,or any alternatives  you recommend?
Zipkin,45227503,nan,0,"2017/07/21, 04:20:34",False,"2017/07/21, 04:26:33","2017/07/21, 04:26:33",3179897.0,186.0,2,124,Zipkin Instrumentation for socket based app,Thanks you so much.
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,I'm using these dependencies:
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,Is there a possibility to add the current active profile(s) to each log line?
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,This would make it possible to filter logs based on the profiles in Splunk/ELK/...
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,So instead of
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,it should log
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,"EDIT: 
Based on Marcin's answer, I implemented it as follows:"
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,application.yml
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,ProfileLogger.java
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,LogConfig.java
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,This prints logs like the following:
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,This is already good but not completely what I'm looking for yet.
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,"I'd like to add the profile from the beginning -  even the ""Started Application"" should contain the profile - if possible."
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,"Secondly, I'd like to move the  profiles  between  INFO  and  22481 ."
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,One more question came up during implementation: In the linked implementation there is this statement:
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,does that mean you only send traces if log-level is set to TRACE?
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,"If so, how could I improve logging to stdout with that approach (given a log-level of debug/info/warn)?"
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,"I think the log-pattern is overriden by Sleuth/Zipkin upon importing the dependencies and thus, local logging looks the same as tracing."
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,Eventually I'm interested in having the profile displayed in local stdout as well as in Zipkin.
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,"EDIT 2:  With the help of Marcin, I have changed the pattern by introducing a  resources/logback-spring.xml  file containing these lines:"
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,Note that you have to add a  bootstrap.yml  file too in order to have the application name correctly displayed.
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,"Without a  bootstrap.yml  file, the above log-pattern just prints ""bootstrap"" as application name."
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,The  bootstrap.yml  just contains
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,in my case.
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,Everything else is configured in application-[profile].yml
Zipkin,42764069,42764225.0,1,"2017/03/13, 14:50:06",True,"2017/03/13, 17:25:56","2017/03/13, 17:22:23",3105453.0,1509.0,2,2962,Add Spring Boot Profile to Sleuth/Zipkin logs,Now everything works as desired:
Zipkin,40860284,40861860.0,1,"2016/11/29, 09:42:35",True,"2016/11/29, 11:14:50",nan,7219867.0,123.0,2,79,Is it possible to monitor Windows Services with Perfino and Zipkin?,I want to monitor some Windows Services with either perfino or Zipkin.
Zipkin,40860284,40861860.0,1,"2016/11/29, 09:42:35",True,"2016/11/29, 11:14:50",nan,7219867.0,123.0,2,79,Is it possible to monitor Windows Services with Perfino and Zipkin?,Does anyone know if that is possible?
Zipkin,40860284,40861860.0,1,"2016/11/29, 09:42:35",True,"2016/11/29, 11:14:50",nan,7219867.0,123.0,2,79,Is it possible to monitor Windows Services with Perfino and Zipkin?,Cheers.
Zipkin,40654863,nan,1,"2016/11/17, 14:26:22",True,"2017/05/11, 04:27:33",nan,2176876.0,49.0,2,5994,Spring Cloud Sleuth error posting spans to Zipkin,Has anyone else encountered the following problem with using Zipkin &amp; Spring Cloud Sleuth?
Zipkin,40654863,nan,1,"2016/11/17, 14:26:22",True,"2017/05/11, 04:27:33",nan,2176876.0,49.0,2,5994,Spring Cloud Sleuth error posting spans to Zipkin,Seems to be a problem posting out data to my localhost Zipkin server.
Zipkin,40654863,nan,1,"2016/11/17, 14:26:22",True,"2017/05/11, 04:27:33",nan,2176876.0,49.0,2,5994,Spring Cloud Sleuth error posting spans to Zipkin,Is there any need to configure proxy settings on Zipkin?
Zipkin,39597545,39597817.0,1,"2016/09/20, 18:02:13",True,"2016/09/20, 18:15:25",nan,831553.0,1076.0,2,2515,"How to pass the traceid, spanid in threads used in threadpool in zipkin?",I am using zipkin to track the requests across microservices.
Zipkin,39597545,39597817.0,1,"2016/09/20, 18:02:13",True,"2016/09/20, 18:15:25",nan,831553.0,1076.0,2,2515,"How to pass the traceid, spanid in threads used in threadpool in zipkin?",One of my service is running jobs using a thread pool.
Zipkin,39597545,39597817.0,1,"2016/09/20, 18:02:13",True,"2016/09/20, 18:15:25",nan,831553.0,1076.0,2,2515,"How to pass the traceid, spanid in threads used in threadpool in zipkin?",How do I transfer the zipkin header values to the threads?
Zipkin,39597545,39597817.0,1,"2016/09/20, 18:02:13",True,"2016/09/20, 18:15:25",nan,831553.0,1076.0,2,2515,"How to pass the traceid, spanid in threads used in threadpool in zipkin?",is there a Zipkin wrapped thread pool/executor available?
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka",I would like to implement tracing in my microservices architecture.
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka",I am using Apache Kafka as message broker and I am not using Spring Framework.
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka",Tracing is a new concept for me.
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka","At first I wanted to create my own implementation, but now I would like to use existing libraries."
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka",Brave looks like the one I will want to use.
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka","I would like to know if there are some guides, examples or docs on how to do this."
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka","Documentation on Github page is minimal, and I find it hard to start using Brave."
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka","Or maybe there is better library with proper documentation, that is easier to use."
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka",I will be looking at Apache HTrace because it looks promising.
Zipkin,38738337,nan,2,"2016/08/03, 11:29:00",True,"2017/03/15, 11:36:01","2016/08/03, 12:29:51",5382693.0,75.0,2,1033,"Zipkin, using existing libraries to handle tracing in microservices connected with Apache Kafka",Some getting started guides will be nice.
Zipkin,66911879,nan,1,"2021/04/02, 00:37:44",True,"2021/04/04, 07:53:10","2021/04/04, 07:53:10",15534461.0,11.0,1,32,How to run zipkin on another ip address than localhost when starting server with &quot;java -jar zipkin.jar&quot;,Did saw  this ...
Zipkin,66911879,nan,1,"2021/04/02, 00:37:44",True,"2021/04/04, 07:53:10","2021/04/04, 07:53:10",15534461.0,11.0,1,32,How to run zipkin on another ip address than localhost when starting server with &quot;java -jar zipkin.jar&quot;,"But I'm not able to make it run, whatever I have tried, I get either still on localhost, either an exception on armeria bind ( I have stuff running on :8080) and server crash..."
Zipkin,66911879,nan,1,"2021/04/02, 00:37:44",True,"2021/04/04, 07:53:10","2021/04/04, 07:53:10",15534461.0,11.0,1,32,How to run zipkin on another ip address than localhost when starting server with &quot;java -jar zipkin.jar&quot;,"In short what I have tried (Windows Server 2016, so no Linux Docker containers ) with no avail."
Zipkin,66911879,nan,1,"2021/04/02, 00:37:44",True,"2021/04/04, 07:53:10","2021/04/04, 07:53:10",15534461.0,11.0,1,32,How to run zipkin on another ip address than localhost when starting server with &quot;java -jar zipkin.jar&quot;,Variation on these batch file commands:
Zipkin,66911879,nan,1,"2021/04/02, 00:37:44",True,"2021/04/04, 07:53:10","2021/04/04, 07:53:10",15534461.0,11.0,1,32,How to run zipkin on another ip address than localhost when starting server with &quot;java -jar zipkin.jar&quot;,"SET &quot;SERVER_ADDRESS=xx.xx.xx.xx&quot; 
SET &quot;ZIPKIN_HOST=xx.xx.xx.xx&quot; 
java -jar zipkin-server-2.23.2-exec.jar --armeria.ports[0].port=9411 --&gt; armeria.ports[0].protocols[0]=http"
Zipkin,66911879,nan,1,"2021/04/02, 00:37:44",True,"2021/04/04, 07:53:10","2021/04/04, 07:53:10",15534461.0,11.0,1,32,How to run zipkin on another ip address than localhost when starting server with &quot;java -jar zipkin.jar&quot;,"It should be simple to run Zipkin on another ip, but I'm fighting."
Zipkin,66911879,nan,1,"2021/04/02, 00:37:44",True,"2021/04/04, 07:53:10","2021/04/04, 07:53:10",15534461.0,11.0,1,32,How to run zipkin on another ip address than localhost when starting server with &quot;java -jar zipkin.jar&quot;,"Could you help me, maybe I'm missing something absolutely obvious..."
Zipkin,66911879,nan,1,"2021/04/02, 00:37:44",True,"2021/04/04, 07:53:10","2021/04/04, 07:53:10",15534461.0,11.0,1,32,How to run zipkin on another ip address than localhost when starting server with &quot;java -jar zipkin.jar&quot;,"But is a pretty common scenario, and is not that well documented."
Zipkin,66777772,nan,1,"2021/03/24, 11:03:03",False,"2021/03/26, 00:23:10",nan,2715083.0,1426.0,1,41,Make Zipkin (or any open-tracing framework) work with existing &quot;trace id&quot;,"We have around  10 Spring boot microservices , which communicate with each other via kafka."
Zipkin,66777772,nan,1,"2021/03/24, 11:03:03",False,"2021/03/26, 00:23:10",nan,2715083.0,1426.0,1,41,Make Zipkin (or any open-tracing framework) work with existing &quot;trace id&quot;,"The logs of each microservice are sent to Kibana, and in case of any errors, we have to sift through Kibana logs."
Zipkin,66777772,nan,1,"2021/03/24, 11:03:03",False,"2021/03/26, 00:23:10",nan,2715083.0,1426.0,1,41,Make Zipkin (or any open-tracing framework) work with existing &quot;trace id&quot;,"The good thing is:  at the start of any flow, a message-id is generated by one of our microservices, and that is propagated to all the others as part of the message transfer (which happens through kafka), so we can search for the message-id in the logs, and we can see the footprint of that flow across all our microservices."
Zipkin,66777772,nan,1,"2021/03/24, 11:03:03",False,"2021/03/26, 00:23:10",nan,2715083.0,1426.0,1,41,Make Zipkin (or any open-tracing framework) work with existing &quot;trace id&quot;,The bad part:  having to sift through tons of logs to get a basic idea of where things broke and why.
Zipkin,66777772,nan,1,"2021/03/24, 11:03:03",False,"2021/03/26, 00:23:10",nan,2715083.0,1426.0,1,41,Make Zipkin (or any open-tracing framework) work with existing &quot;trace id&quot;,"So I was wondering if we can have some distributed tracing implemented, maybe through Zipkin (or some other open-tracing framework) that can work with the message-id that our ecosystem already produces, instead of generating a new one ?"
Zipkin,66777772,nan,1,"2021/03/24, 11:03:03",False,"2021/03/26, 00:23:10",nan,2715083.0,1426.0,1,41,Make Zipkin (or any open-tracing framework) work with existing &quot;trace id&quot;,Thank you for your time :)
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,Traces that should have been sent by dapr runtime to zipkin server somehow fails to reach it.
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,The situation is the following:
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,I'm using Docker Desktop on my Windows PC.
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,I have downloaded the sample from dapr repository ( https://github.com/dapr/samples/tree/master/hello-docker-compose ) which runs perfectly out of the box with  docker-compose up .
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,Then I've added Zipkin support as per dapr documentation:
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,"When application runs, it should send traces to the server, but nothing is found in zipkin UI and logs."
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,Strange thing start to appear in the logs from  nodeapp-dapr_1  service:  error while reading spiffe id from client cert
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,Additional info - current dapr version used is 1.0.1.
Zipkin,66646250,nan,0,"2021/03/15, 23:59:47",False,"2021/03/15, 23:59:47",nan,1100024.0,657.0,1,76,Zipkin tracing not working for docker-compose and Dapr,I made sure that security (mtls) is disabled in config file.
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,I am very new to using OpenTelemetry and have just tried configuring it to send traces to my Zipkin server.
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,"Unfortunately , after configuring the agent by specifying zipkin exporter details , I could see an exception in the console."
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,I used Petclic as sample spring boot and have followed the documentation here  https://github.com/open-telemetry/opentelemetry-java-instrumentation
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,Here is the command that I used to start spring-boot app(I have my zipkin server running at localhost:9411):
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,java -javaagent:opentelemetry-javaagent-all.jar -Dotel.exporter=zipkin -Dotel.exporter.zipkin.endpoint=localhost:9411 -jar spring-petclinic-2.4.2.jar
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,Exception in the console (It is trying to connect to gRpc exporter instead of Zipkins):
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,Please can you let me know what is wrong with this.
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,"EDIT : 
I already tried passing -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin but was not successful."
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,I am just trying to start my spring boot app through eclipse by passing these params as jvm arguments.
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,Every tutorial on OpenTelemetery seems to be using Docker setup.
Zipkin,66517888,66782647.0,2,"2021/03/07, 17:01:25",True,"2021/03/26, 09:05:17","2021/03/26, 09:05:17",13117688.0,13.0,1,78,Opentelemetry with Zipkin exporter is not working as expected. Throws StatusRuntimeException: UNAVAILABLE: io exception,Please could someone help
Zipkin,66284701,nan,1,"2021/02/19, 22:40:16",True,"2021/02/24, 13:52:09",nan,1038268.0,285.0,1,297,Opentelemetry java Automatic Instrumentation with zipkin exporter option is using OtlpGrpcSpanExporter,I have a simple spring boot hello world application.
Zipkin,66284701,nan,1,"2021/02/19, 22:40:16",True,"2021/02/24, 13:52:09",nan,1038268.0,285.0,1,297,Opentelemetry java Automatic Instrumentation with zipkin exporter option is using OtlpGrpcSpanExporter,Trying to send data to the Zipkin collector.
Zipkin,66284701,nan,1,"2021/02/19, 22:40:16",True,"2021/02/24, 13:52:09",nan,1038268.0,285.0,1,297,Opentelemetry java Automatic Instrumentation with zipkin exporter option is using OtlpGrpcSpanExporter,"But as per logs, it's trying to use  OtlpGrpcSpanExporter ."
Zipkin,66284701,nan,1,"2021/02/19, 22:40:16",True,"2021/02/24, 13:52:09",nan,1038268.0,285.0,1,297,Opentelemetry java Automatic Instrumentation with zipkin exporter option is using OtlpGrpcSpanExporter,My application exposes a simple post rest API.
Zipkin,66284701,nan,1,"2021/02/19, 22:40:16",True,"2021/02/24, 13:52:09",nan,1038268.0,285.0,1,297,Opentelemetry java Automatic Instrumentation with zipkin exporter option is using OtlpGrpcSpanExporter,"Following Opentelemetry
docs  https://opentelemetry.io/docs/java/getting_started/"
Zipkin,66284701,nan,1,"2021/02/19, 22:40:16",True,"2021/02/24, 13:52:09",nan,1038268.0,285.0,1,297,Opentelemetry java Automatic Instrumentation with zipkin exporter option is using OtlpGrpcSpanExporter,[opentelemetry.auto.trace 2021-02-20 01:48:44:490 +0530] [grpc-default-executor-1] WARN io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter - Failed to export spans.
Zipkin,66284701,nan,1,"2021/02/19, 22:40:16",True,"2021/02/24, 13:52:09",nan,1038268.0,285.0,1,297,Opentelemetry java Automatic Instrumentation with zipkin exporter option is using OtlpGrpcSpanExporter,"Error message: UNAVAILABLE: io exception
[opentelemetry.auto.trace 2021-02-20 01:49:14:106 +0530] [grpc-default-executor-2] WARN io.opentelemetry.exporter.otlp.metrics.OtlpGrpcMetricExporter - Failed to export metrics
io.grpc.StatusRuntimeException: UNAVAILABLE: io exception"
Zipkin,66284701,nan,1,"2021/02/19, 22:40:16",True,"2021/02/24, 13:52:09",nan,1038268.0,285.0,1,297,Opentelemetry java Automatic Instrumentation with zipkin exporter option is using OtlpGrpcSpanExporter,Please let me know if I have to change anything.
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,"I'm using istio with ingress gateway, and added zipkin tracing."
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,All my apps are using spring boot with sleuth zipkin.
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,I've deployed 2 zipkin for testing
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,"the spring boot configuration are pointing to the zipkin namespace, with always sampled configuration."
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,"Problem is when I'm using ingress gateway, the trace id looks like request id and it does propagate to my sub systems."
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,"But when I query to zipkin (deployed both in istio-system from istio documentation, and manually deployed to another namespace) the trace id are not present."
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,"Interestingly when I do port-forward of my outer most system, and hit the spring boot with grpc, the trace id are being propagated to the sub systems, and it does show in the zipkin dashboard."
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,although the trace id are different when using ingressgateway and port-forward direct grpc call :
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,"ingressgateway : 0672471566b9305f7dcaadecaf1a8c71
direct call : cdc337ec90b8c085"
Zipkin,63610200,nan,0,"2020/08/27, 09:15:42",False,"2020/08/27, 09:15:42",nan,13211927.0,11.0,1,126,Trace ID is not forwarded to zipkin if using ingressgateway istio,Thanks!
Zipkin,63489826,63969131.0,1,"2020/08/19, 18:00:02",True,"2020/09/19, 16:03:50",nan,9671280.0,1639.0,1,87,What are keys differences between OpenTracing and Zipkin?,I am looking into distribution tracing tools.
Zipkin,63489826,63969131.0,1,"2020/08/19, 18:00:02",True,"2020/09/19, 16:03:50",nan,9671280.0,1639.0,1,87,What are keys differences between OpenTracing and Zipkin?,Found there two very popular.
Zipkin,63489826,63969131.0,1,"2020/08/19, 18:00:02",True,"2020/09/19, 16:03:50",nan,9671280.0,1639.0,1,87,What are keys differences between OpenTracing and Zipkin?,What are key differences between them ?
Zipkin,63489826,63969131.0,1,"2020/08/19, 18:00:02",True,"2020/09/19, 16:03:50",nan,9671280.0,1639.0,1,87,What are keys differences between OpenTracing and Zipkin?,Which one would you recommend ?
Zipkin,63489826,63969131.0,1,"2020/08/19, 18:00:02",True,"2020/09/19, 16:03:50",nan,9671280.0,1639.0,1,87,What are keys differences between OpenTracing and Zipkin?,Will you recommend other open source distributed tracking tool ?
Zipkin,63429753,nan,0,"2020/08/15, 21:56:38",False,"2020/08/15, 21:56:38",nan,9921565.0,62.0,1,584,Difference between Zipkin and Elastic Stack(ELK)?,Spring Cloud Sleuth is used for creating traceIds (Unique to request across services) and spanId (Same for one unit for work).
Zipkin,63429753,nan,0,"2020/08/15, 21:56:38",False,"2020/08/15, 21:56:38",nan,9921565.0,62.0,1,584,Difference between Zipkin and Elastic Stack(ELK)?,My idea is that Zipkin server is used to get collective visualization of these logs across service.
Zipkin,63429753,nan,0,"2020/08/15, 21:56:38",False,"2020/08/15, 21:56:38",nan,9921565.0,62.0,1,584,Difference between Zipkin and Elastic Stack(ELK)?,But I know and have used ELK stack which does necessarily the same function.
Zipkin,63429753,nan,0,"2020/08/15, 21:56:38",False,"2020/08/15, 21:56:38",nan,9921565.0,62.0,1,584,Difference between Zipkin and Elastic Stack(ELK)?,"I mean we can group requests with the same traceId for visualising, using ELK stack."
Zipkin,63429753,nan,0,"2020/08/15, 21:56:38",False,"2020/08/15, 21:56:38",nan,9921565.0,62.0,1,584,Difference between Zipkin and Elastic Stack(ELK)?,"But I do see people trying to implement distributed tracing with Sleuth, ELK along with Zipkin, as in these examples ( Link1 , Link2 )."
Zipkin,63429753,nan,0,"2020/08/15, 21:56:38",False,"2020/08/15, 21:56:38",nan,9921565.0,62.0,1,584,Difference between Zipkin and Elastic Stack(ELK)?,But why do we need Zipkin if there is already ELK for log collection and visualising?
Zipkin,63429753,nan,0,"2020/08/15, 21:56:38",False,"2020/08/15, 21:56:38",nan,9921565.0,62.0,1,584,Difference between Zipkin and Elastic Stack(ELK)?,Where I am missing?
Zipkin,63385866,66499651.0,1,"2020/08/13, 02:12:54",True,"2021/03/05, 23:09:02",nan,1423685.0,1301.0,1,86,Exportable Zipkin/Brave/Spring-Cloud-Sleuth Span cannot be found in Zipkin,"I can't find a below  exportable  Span in Zipkin neither by it's traceId nor by spanId (some other spans appear, so Zipkin server seems to work)"
Zipkin,63385866,66499651.0,1,"2020/08/13, 02:12:54",True,"2021/03/05, 23:09:02",nan,1423685.0,1301.0,1,86,Exportable Zipkin/Brave/Spring-Cloud-Sleuth Span cannot be found in Zipkin,I also can't find it's parent &quot;parentId&quot;:&quot;37eca1021fd5241c&quot; in Zipkin.
Zipkin,63385866,66499651.0,1,"2020/08/13, 02:12:54",True,"2021/03/05, 23:09:02",nan,1423685.0,1301.0,1,86,Exportable Zipkin/Brave/Spring-Cloud-Sleuth Span cannot be found in Zipkin,Where can be a problem?
Zipkin,63385866,66499651.0,1,"2020/08/13, 02:12:54",True,"2021/03/05, 23:09:02",nan,1423685.0,1301.0,1,86,Exportable Zipkin/Brave/Spring-Cloud-Sleuth Span cannot be found in Zipkin,How can I bite/debug it?
Zipkin,63385866,66499651.0,1,"2020/08/13, 02:12:54",True,"2021/03/05, 23:09:02",nan,1423685.0,1301.0,1,86,Exportable Zipkin/Brave/Spring-Cloud-Sleuth Span cannot be found in Zipkin,"Possibly this span is in a flow, that was triggered by a rabbit message, not a rest request."
Zipkin,63385866,66499651.0,1,"2020/08/13, 02:12:54",True,"2021/03/05, 23:09:02",nan,1423685.0,1301.0,1,86,Exportable Zipkin/Brave/Spring-Cloud-Sleuth Span cannot be found in Zipkin,Spans from a trace that were triggered by http rest request are correctly visible in Zipkin.
Zipkin,63385866,66499651.0,1,"2020/08/13, 02:12:54",True,"2021/03/05, 23:09:02",nan,1423685.0,1301.0,1,86,Exportable Zipkin/Brave/Spring-Cloud-Sleuth Span cannot be found in Zipkin,But I can't find traces from flows triggered by rabbit message.
Zipkin,63385866,66499651.0,1,"2020/08/13, 02:12:54",True,"2021/03/05, 23:09:02",nan,1423685.0,1301.0,1,86,Exportable Zipkin/Brave/Spring-Cloud-Sleuth Span cannot be found in Zipkin,What problem could be here?
Zipkin,63073886,nan,1,"2020/07/24, 15:47:37",True,"2020/08/31, 17:15:00",nan,12964338.0,376.0,1,740,Getting error while starting zipkin server : Prometheus requires that all meters with the same name have the same set of tag keys,We have  slueth  in other microservices and we wants to send data to zipkin server for consolidated logging.I am trying to start my zipkin server.I am getting the following error:
Zipkin,63073886,nan,1,"2020/07/24, 15:47:37",True,"2020/08/31, 17:15:00",nan,12964338.0,376.0,1,740,Getting error while starting zipkin server : Prometheus requires that all meters with the same name have the same set of tag keys,"I tried to use the sleuth and zipkin older versions, but getting conflicts and spring application is failing to start
We are not able to see Zipkin UI."
Zipkin,63073886,nan,1,"2020/07/24, 15:47:37",True,"2020/08/31, 17:15:00",nan,12964338.0,376.0,1,740,Getting error while starting zipkin server : Prometheus requires that all meters with the same name have the same set of tag keys,And  pom.xml  is like:
Zipkin,63073886,nan,1,"2020/07/24, 15:47:37",True,"2020/08/31, 17:15:00",nan,12964338.0,376.0,1,740,Getting error while starting zipkin server : Prometheus requires that all meters with the same name have the same set of tag keys,Any suggestions will be helpful.
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,I am trying to link three HTTP service hops in NodeJS together into a single Zipkin trace.
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,I have three services
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,"The service  service-main  calls  service-hello , and  service-hello  needs to call  service-goodbye  to complete."
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,"Zipkin can see these calls, but links them together as two separate traces."
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,"( service-main  calling  service-hello , and  service-hello  calling  service-goodbye ."
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,"The services are implemented in  express , and the fetching happens via  node-fetch ."
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,I create an instrumented service fetcher with code that looks like this
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,and I instrument express with code that looks like this
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,"and finally, I create my tracer with code that looks like this"
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,You can see the above code in context in  the following github repository .
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,"I've done all this by cargo-culting the code samples from the zipkin github repositories, and I don't know enough about zipkin's implementation to diagnose this further."
Zipkin,61800994,61862789.0,2,"2020/05/14, 18:17:11",True,"2020/05/18, 19:34:48",nan,4668.0,156911.0,1,52,Linking Service Hops with Zipkin and NodeJS,How to I get zipkin to see the  service-main  -   service-hello  -   service-goodbye  call chain as a single trace?
Zipkin,61368689,61401330.0,1,"2020/04/22, 18:17:45",True,"2020/06/09, 20:37:01","2020/06/20, 12:12:55",11657025.0,21.0,1,660,zipkin2.reporter.Sender bean not found when using with Zipkin with kafka in docker,I am working on micro service where micro services are communicating with each other.
Zipkin,61368689,61401330.0,1,"2020/04/22, 18:17:45",True,"2020/06/09, 20:37:01","2020/06/20, 12:12:55",11657025.0,21.0,1,660,zipkin2.reporter.Sender bean not found when using with Zipkin with kafka in docker,I am using Zipkin with Sleuth and Apache Kafka as a message broker and running micro service and kafka using docker-compose.
Zipkin,61368689,61401330.0,1,"2020/04/22, 18:17:45",True,"2020/06/09, 20:37:01","2020/06/20, 12:12:55",11657025.0,21.0,1,660,zipkin2.reporter.Sender bean not found when using with Zipkin with kafka in docker,"I am using Spring Boot (2.2.6.RELEASE), Spring Cloud (Hoxton.SR3), Kafka Image(wurstmeister/kafka:2.12-2.4.1) and latest image of Zipkin."
Zipkin,61368689,61401330.0,1,"2020/04/22, 18:17:45",True,"2020/06/09, 20:37:01","2020/06/20, 12:12:55",11657025.0,21.0,1,660,zipkin2.reporter.Sender bean not found when using with Zipkin with kafka in docker,"When i try to spin the container, micro service is giving below error:"
Zipkin,61368689,61401330.0,1,"2020/04/22, 18:17:45",True,"2020/06/09, 20:37:01","2020/06/20, 12:12:55",11657025.0,21.0,1,660,zipkin2.reporter.Sender bean not found when using with Zipkin with kafka in docker,APPLICATION FAILED TO START
Zipkin,61368689,61401330.0,1,"2020/04/22, 18:17:45",True,"2020/06/09, 20:37:01","2020/06/20, 12:12:55",11657025.0,21.0,1,660,zipkin2.reporter.Sender bean not found when using with Zipkin with kafka in docker,Description:
Zipkin,61368689,61401330.0,1,"2020/04/22, 18:17:45",True,"2020/06/09, 20:37:01","2020/06/20, 12:12:55",11657025.0,21.0,1,660,zipkin2.reporter.Sender bean not found when using with Zipkin with kafka in docker,Parameter 2 of method reporter in org.springframework.cloud.sleuth.zipkin2.ZipkinAutoConfiguration &gt; required a bean of type 'zipkin2.reporter.Sender' that could not be found.
Zipkin,61208227,nan,1,"2020/04/14, 15:54:22",False,"2020/07/22, 15:42:19",nan,5129588.0,456.0,1,339,Configure Zipkin to trace the AWS SQS messages?,I have microservice running in AWS ECS and listens to AWS SQS messages.
Zipkin,61208227,nan,1,"2020/04/14, 15:54:22",False,"2020/07/22, 15:42:19",nan,5129588.0,456.0,1,339,Configure Zipkin to trace the AWS SQS messages?,I am using  zipkin-aws  to sent the traces to AWS Kinesis and collected in S3.
Zipkin,61208227,nan,1,"2020/04/14, 15:54:22",False,"2020/07/22, 15:42:19",nan,5129588.0,456.0,1,339,Configure Zipkin to trace the AWS SQS messages?,"When there is any REST Operation, the traces are sent and collected in S3 perfectly."
Zipkin,61208227,nan,1,"2020/04/14, 15:54:22",False,"2020/07/22, 15:42:19",nan,5129588.0,456.0,1,339,Configure Zipkin to trace the AWS SQS messages?,But it doesnt capture the traces when the Microservice listens or send message to AWS queue.
Zipkin,61208227,nan,1,"2020/04/14, 15:54:22",False,"2020/07/22, 15:42:19",nan,5129588.0,456.0,1,339,Configure Zipkin to trace the AWS SQS messages?,Could anyone help in configuring zipkin to listen to SQS messages.
Zipkin,60631319,nan,1,"2020/03/11, 09:47:38",False,"2021/03/01, 07:37:26",nan,8190776.0,545.0,1,362,Register Zipkin with Eureka server,I have an application that is divided into a few microservices (using Spring Eureka project).
Zipkin,60631319,nan,1,"2020/03/11, 09:47:38",False,"2021/03/01, 07:37:26",nan,8190776.0,545.0,1,362,Register Zipkin with Eureka server,"All the services are registered to Eureka Server - so that the communication between the services can be realized through a ""Gateway API"" (Eureka Server)"
Zipkin,60631319,nan,1,"2020/03/11, 09:47:38",False,"2021/03/01, 07:37:26",nan,8190776.0,545.0,1,362,Register Zipkin with Eureka server,The logs produced by the services are reported to a Zipkin server that runs as a separate service.
Zipkin,60631319,nan,1,"2020/03/11, 09:47:38",False,"2021/03/01, 07:37:26",nan,8190776.0,545.0,1,362,Register Zipkin with Eureka server,All works as expected but when I go to the Eureka dashboard I am not able to see my Zipkin service since it is not registered with Eureka.
Zipkin,60631319,nan,1,"2020/03/11, 09:47:38",False,"2021/03/01, 07:37:26",nan,8190776.0,545.0,1,362,Register Zipkin with Eureka server,Question: Is it possible to register Zipkin with Eureka Server?
Zipkin,60631319,nan,1,"2020/03/11, 09:47:38",False,"2021/03/01, 07:37:26",nan,8190776.0,545.0,1,362,Register Zipkin with Eureka server,Here is my docker-compose file:
Zipkin,59028493,nan,0,"2019/11/25, 11:19:17",False,"2019/11/25, 12:22:41","2019/11/25, 12:22:41",5599822.0,11.0,1,185,"spring boot 2.1.6 with zipkin, debugging the project using spring boot application throws an Exception",I am using the  Zipkin 5.6.11
Zipkin,59028493,nan,0,"2019/11/25, 11:19:17",False,"2019/11/25, 12:22:41","2019/11/25, 12:22:41",5599822.0,11.0,1,185,"spring boot 2.1.6 with zipkin, debugging the project using spring boot application throws an Exception",I append  mysql  url:
Zipkin,59028493,nan,0,"2019/11/25, 11:19:17",False,"2019/11/25, 12:22:41","2019/11/25, 12:22:41",5599822.0,11.0,1,185,"spring boot 2.1.6 with zipkin, debugging the project using spring boot application throws an Exception","but when I debug the project using spring boot application, it throws an Exception caused by not found the class TracingStatementInterceptor:"
Zipkin,59028493,nan,0,"2019/11/25, 11:19:17",False,"2019/11/25, 12:22:41","2019/11/25, 12:22:41",5599822.0,11.0,1,185,"spring boot 2.1.6 with zipkin, debugging the project using spring boot application throws an Exception","java.sql.SQLException: Unable to load statement interceptor
  'brave.mysql.TracingStatementInterceptor'."
Zipkin,59028493,nan,0,"2019/11/25, 11:19:17",False,"2019/11/25, 12:22:41","2019/11/25, 12:22:41",5599822.0,11.0,1,185,"spring boot 2.1.6 with zipkin, debugging the project using spring boot application throws an Exception",Caused by: java.lang.ClassNotFoundException: brave.mysql.TracingStatementInterceptor
Zipkin,59028493,nan,0,"2019/11/25, 11:19:17",False,"2019/11/25, 12:22:41","2019/11/25, 12:22:41",5599822.0,11.0,1,185,"spring boot 2.1.6 with zipkin, debugging the project using spring boot application throws an Exception",The dependency is
Zipkin,58214695,58216010.0,2,"2019/10/03, 10:54:55",True,"2019/10/03, 16:58:34",nan,7502630.0,29.0,1,282,Is there a possible way of using Zipkin without Sleuth?,I have some distributed micro services written in Spring Boot and I am using RabbitMQ.
Zipkin,58214695,58216010.0,2,"2019/10/03, 10:54:55",True,"2019/10/03, 16:58:34",nan,7502630.0,29.0,1,282,Is there a possible way of using Zipkin without Sleuth?,I want to track my requests.
Zipkin,58214695,58216010.0,2,"2019/10/03, 10:54:55",True,"2019/10/03, 16:58:34",nan,7502630.0,29.0,1,282,Is there a possible way of using Zipkin without Sleuth?,Is there a possible way of tracking without using Spring Cloud or Sleuth
Zipkin,57892994,57911578.0,1,"2019/09/11, 19:05:01",True,"2019/09/12, 20:10:36","2019/09/12, 15:06:42",7474991.0,99.0,1,577,"traces are not being shown in zipkin, when sender type is kafka",I want to send my spring boot log traces using Kafka to Zipkin server
Zipkin,57892994,57911578.0,1,"2019/09/11, 19:05:01",True,"2019/09/12, 20:10:36","2019/09/12, 15:06:42",7474991.0,99.0,1,577,"traces are not being shown in zipkin, when sender type is kafka",I started my Kafka and Zipkin servers and I started a Kafka consumer which is listening to the topic Zipkin and I am able to see my logs here but when I open my Zipkin dashboard I can't find any traces
Zipkin,57892994,57911578.0,1,"2019/09/11, 19:05:01",True,"2019/09/12, 20:10:36","2019/09/12, 15:06:42",7474991.0,99.0,1,577,"traces are not being shown in zipkin, when sender type is kafka","Spring boot version  :- 2.1.7.RELEASE 
 Spring Cloud version  :- Greenwich.SR2"
Zipkin,57892994,57911578.0,1,"2019/09/11, 19:05:01",True,"2019/09/12, 20:10:36","2019/09/12, 15:06:42",7474991.0,99.0,1,577,"traces are not being shown in zipkin, when sender type is kafka","when i make  spring.zipkin.sender.type=web  the traces are being shown in Zipkin dashboard 
Is there anything I'm missing here."
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,I have some services.
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,I want to trace those services using zipkin-go.
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,"In every service, I am calling some of my other internal services or db calls."
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,I want to trace every activity like how much time it has taken to call internal services or db.
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,I have implemented using available tutorials on internet.
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,Below is my code:
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,I am getting my request traced but I am not able to trace what is happening inside the  uploadimage  controller.
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,Below is the screenshot of my zipkin UI:
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,I want to trace all the activities happening inside uploadimage controller.
Zipkin,57703884,nan,1,"2019/08/29, 09:03:05",False,"2019/09/21, 15:42:54",nan,6573592.0,835.0,1,472,Complete tracing using zipkin-go,What should I need to pass so that I can trace all.
Zipkin,57448326,nan,0,"2019/08/11, 11:02:03",False,"2019/08/13, 21:34:17","2020/06/20, 12:12:55",2580681.0,329.0,1,243,Query parameters in Zipkin,"I may have missed it, but how do save query parameters in zipkin?"
Zipkin,57448326,nan,0,"2019/08/11, 11:02:03",False,"2019/08/13, 21:34:17","2020/06/20, 12:12:55",2580681.0,329.0,1,243,Query parameters in Zipkin,My service is running:
Zipkin,57448326,nan,0,"2019/08/11, 11:02:03",False,"2019/08/13, 21:34:17","2020/06/20, 12:12:55",2580681.0,329.0,1,243,Query parameters in Zipkin,"I'm missing host too, but I suspect that is because I'm running services in docker containers."
Zipkin,57448326,nan,0,"2019/08/11, 11:02:03",False,"2019/08/13, 21:34:17","2020/06/20, 12:12:55",2580681.0,329.0,1,243,Query parameters in Zipkin,update 8/13/2019
Zipkin,57448326,nan,0,"2019/08/11, 11:02:03",False,"2019/08/13, 21:34:17","2020/06/20, 12:12:55",2580681.0,329.0,1,243,Query parameters in Zipkin,I make a call to  https://test-service.mydomain.com/api/conn/parallel2?repetitions=20&amp;delay=10000&amp;bypassTokenCache=true&amp;overrideTokenRefreshSeconds=10
Zipkin,57448326,nan,0,"2019/08/11, 11:02:03",False,"2019/08/13, 21:34:17","2020/06/20, 12:12:55",2580681.0,329.0,1,243,Query parameters in Zipkin,Notice in the trace I'm missing host and query parameters.
Zipkin,57448326,nan,0,"2019/08/11, 11:02:03",False,"2019/08/13, 21:34:17","2020/06/20, 12:12:55",2580681.0,329.0,1,243,Query parameters in Zipkin,Is there any way to record those values in zipkin?
Zipkin,57180252,nan,1,"2019/07/24, 13:00:48",False,"2020/04/16, 13:11:53","2019/07/24, 13:05:56",10819668.0,65.0,1,327,Zipkin Tracing for Project Reactor Kafka,I need to implement Zipkin tracing in one java-based service which is using Project Reactor Kafka for reactive streams and non-blocking IO operations.
Zipkin,57180252,nan,1,"2019/07/24, 13:00:48",False,"2020/04/16, 13:11:53","2019/07/24, 13:05:56",10819668.0,65.0,1,327,Zipkin Tracing for Project Reactor Kafka,I could not find any brave instrumentation library which supports reactive-Kafka.
Zipkin,57180252,nan,1,"2019/07/24, 13:00:48",False,"2020/04/16, 13:11:53","2019/07/24, 13:05:56",10819668.0,65.0,1,327,Zipkin Tracing for Project Reactor Kafka,The standard Kafka-client brave instrumentation:
Zipkin,57180252,nan,1,"2019/07/24, 13:00:48",False,"2020/04/16, 13:11:53","2019/07/24, 13:05:56",10819668.0,65.0,1,327,Zipkin Tracing for Project Reactor Kafka,https://github.com/openzipkin/brave/tree/master/instrumentation/kafka-clients
Zipkin,57180252,nan,1,"2019/07/24, 13:00:48",False,"2020/04/16, 13:11:53","2019/07/24, 13:05:56",10819668.0,65.0,1,327,Zipkin Tracing for Project Reactor Kafka,has no support for Reactive-Kafka.
Zipkin,57180252,nan,1,"2019/07/24, 13:00:48",False,"2020/04/16, 13:11:53","2019/07/24, 13:05:56",10819668.0,65.0,1,327,Zipkin Tracing for Project Reactor Kafka,Is there a library or repo which can help me with Zipkin tracing for reactive-Kafka in java?
Zipkin,55646088,nan,0,"2019/04/12, 09:56:28",False,"2019/04/12, 09:56:28",nan,10819668.0,65.0,1,127,"Is there a way to enable zipkin tracing in a jar such that wherever the jar is used, it initialises a separate span by-default","I am creating a client sdk with retrofit calls to a service, packaged as a separate jar."
Zipkin,55646088,nan,0,"2019/04/12, 09:56:28",False,"2019/04/12, 09:56:28",nan,10819668.0,65.0,1,127,"Is there a way to enable zipkin tracing in a jar such that wherever the jar is used, it initialises a separate span by-default","I have to include zipkin tracer/tracing in this jar so that any application using this jar to communicate with the service, have a separate span created automatically for every call to the service."
Zipkin,55646088,nan,0,"2019/04/12, 09:56:28",False,"2019/04/12, 09:56:28",nan,10819668.0,65.0,1,127,"Is there a way to enable zipkin tracing in a jar such that wherever the jar is used, it initialises a separate span by-default",Is there a feasible solution to my problem?
Zipkin,55646088,nan,0,"2019/04/12, 09:56:28",False,"2019/04/12, 09:56:28",nan,10819668.0,65.0,1,127,"Is there a way to enable zipkin tracing in a jar such that wherever the jar is used, it initialises a separate span by-default","I have been trying to solve the problem using the ""io.zipkin.brave:brave-instrumentation-okhttp3"" library."
Zipkin,55646088,nan,0,"2019/04/12, 09:56:28",False,"2019/04/12, 09:56:28",nan,10819668.0,65.0,1,127,"Is there a way to enable zipkin tracing in a jar such that wherever the jar is used, it initialises a separate span by-default","I have also added the ""org.springframework.cloud:spring-cloud-starter-sleuth"" dependency so that the tracer-id is by-default generated."
Zipkin,55646088,nan,0,"2019/04/12, 09:56:28",False,"2019/04/12, 09:56:28",nan,10819668.0,65.0,1,127,"Is there a way to enable zipkin tracing in a jar such that wherever the jar is used, it initialises a separate span by-default","But adding this jar to a project which uses kafka-streams and the ""io.zipkin.brave:brave-instrumentation-kafka-streams"" dependency, doesn't automatically initialses a new span."
Zipkin,55646088,nan,0,"2019/04/12, 09:56:28",False,"2019/04/12, 09:56:28",nan,10819668.0,65.0,1,127,"Is there a way to enable zipkin tracing in a jar such that wherever the jar is used, it initialises a separate span by-default",What I expect is that applications using this jar have by-default a separate span for every retrofit call they make through this jar.
Zipkin,55001316,60081185.0,1,"2019/03/05, 13:01:48",True,"2020/02/05, 19:34:06",nan,4939853.0,2993.0,1,661,Configure basic authentication with Spring Sleuth Zipkin,I'm trying to configure my spring boot app to log into a zipkin server.
Zipkin,55001316,60081185.0,1,"2019/03/05, 13:01:48",True,"2020/02/05, 19:34:06",nan,4939853.0,2993.0,1,661,Configure basic authentication with Spring Sleuth Zipkin,The problem is that this server is protected by a proxy (with basic auth) and I cannot find any documentation describing how to configure authorization with spring-sleuth.
Zipkin,55001316,60081185.0,1,"2019/03/05, 13:01:48",True,"2020/02/05, 19:34:06",nan,4939853.0,2993.0,1,661,Configure basic authentication with Spring Sleuth Zipkin,I have tried to use that kind of configuration :
Zipkin,55001316,60081185.0,1,"2019/03/05, 13:01:48",True,"2020/02/05, 19:34:06",nan,4939853.0,2993.0,1,661,Configure basic authentication with Spring Sleuth Zipkin,"But without success, logs indicating :"
Zipkin,55001316,60081185.0,1,"2019/03/05, 13:01:48",True,"2020/02/05, 19:34:06",nan,4939853.0,2993.0,1,661,Configure basic authentication with Spring Sleuth Zipkin,I have tried with curl and it works.
Zipkin,55001316,60081185.0,1,"2019/03/05, 13:01:48",True,"2020/02/05, 19:34:06",nan,4939853.0,2993.0,1,661,Configure basic authentication with Spring Sleuth Zipkin,Has someone already succeed to configure authentication with spring-sleuth ?
Zipkin,54986635,nan,1,"2019/03/04, 17:42:50",False,"2020/08/18, 07:12:18","2019/03/04, 20:31:03",11149117.0,11.0,1,212,Spring boot 2.1 application not able to publish traces to Spring boot 1.5 Zipkin server,I'm working on a spring boot application version 2.1.2 with below dependency
Zipkin,54986635,nan,1,"2019/03/04, 17:42:50",False,"2020/08/18, 07:12:18","2019/03/04, 20:31:03",11149117.0,11.0,1,212,Spring boot 2.1 application not able to publish traces to Spring boot 1.5 Zipkin server,The application is not able to send the traces to Zipkin server which is running on Spring boot 1.5.
Zipkin,54986635,nan,1,"2019/03/04, 17:42:50",False,"2020/08/18, 07:12:18","2019/03/04, 20:31:03",11149117.0,11.0,1,212,Spring boot 2.1 application not able to publish traces to Spring boot 1.5 Zipkin server,When I tried downgrading my application to Spring 1.5 it started sending traces to the Zipkin server.
Zipkin,54986635,nan,1,"2019/03/04, 17:42:50",False,"2020/08/18, 07:12:18","2019/03/04, 20:31:03",11149117.0,11.0,1,212,Spring boot 2.1 application not able to publish traces to Spring boot 1.5 Zipkin server,Can someone please assist.
Zipkin,54986635,nan,1,"2019/03/04, 17:42:50",False,"2020/08/18, 07:12:18","2019/03/04, 20:31:03",11149117.0,11.0,1,212,Spring boot 2.1 application not able to publish traces to Spring boot 1.5 Zipkin server,Am I missing any configuration for Spring boot 2.1?
Zipkin,54986635,nan,1,"2019/03/04, 17:42:50",False,"2020/08/18, 07:12:18","2019/03/04, 20:31:03",11149117.0,11.0,1,212,Spring boot 2.1 application not able to publish traces to Spring boot 1.5 Zipkin server,Below are the dependency &amp; configuration for Spring cloud Sleuth and Starter Zipkin
Zipkin,52550644,52585202.0,1,"2018/09/28, 10:43:10",True,"2018/10/01, 08:47:20",nan,5238035.0,568.0,1,233,How to get the zipkin version,Is there any CURL to get the latest available zipkin version and another CURL to get the currently running version in my local environment ?
Zipkin,52550644,52585202.0,1,"2018/09/28, 10:43:10",True,"2018/10/01, 08:47:20",nan,5238035.0,568.0,1,233,How to get the zipkin version,I have to do a version comparison on currently running Zipkin version with related to the latest available.
Zipkin,52550644,52585202.0,1,"2018/09/28, 10:43:10",True,"2018/10/01, 08:47:20",nan,5238035.0,568.0,1,233,How to get the zipkin version,I even walked through all the issues raised in github and couldn't find a solution.
Zipkin,51151547,51354529.0,1,"2018/07/03, 12:51:54",True,"2018/07/16, 07:19:17",nan,8542187.0,1533.0,1,410,Zipkin for Java application,I have Java application that is sending requests to Spring Boot applications.
Zipkin,51151547,51354529.0,1,"2018/07/03, 12:51:54",True,"2018/07/16, 07:19:17",nan,8542187.0,1533.0,1,410,Zipkin for Java application,I have implemented zipkin+sleuth on the Spring Boot applications and get traces.
Zipkin,51151547,51354529.0,1,"2018/07/03, 12:51:54",True,"2018/07/16, 07:19:17",nan,8542187.0,1533.0,1,410,Zipkin for Java application,Now I want to implement zipkin on the java application and to collect the traces.
Zipkin,51151547,51354529.0,1,"2018/07/03, 12:51:54",True,"2018/07/16, 07:19:17",nan,8542187.0,1533.0,1,410,Zipkin for Java application,How to implement that?
Zipkin,51151547,51354529.0,1,"2018/07/03, 12:51:54",True,"2018/07/16, 07:19:17",nan,8542187.0,1533.0,1,410,Zipkin for Java application,I need to use brave?
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,I have started zipkin-server and I can see the dashboard.
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,I have tested it with simple projects and it is okay.
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,But when I test it with my app I have a problem.
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,I have Spring Boot project that produce to kafka if the property for kafka is set on true in application.properties.
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,In my case it is always set to false and it is working correctly.
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,But when I added zipkin dependency it start to send to kafka.
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,And also I can not see my client app in the zipkin dashboard.
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,I am using Spring Boot 1.5.6.RELEASE version
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,This are my dependencies:
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,And this are my properties for zipkin and sleuth.
Zipkin,51040315,51042639.0,1,"2018/06/26, 13:12:05",True,"2018/06/26, 15:11:31",nan,8542187.0,1533.0,1,1161,Zipkin Client can not connect to the Zipkin Server,"By adding the first 3 properties the application is not sending requests on the beggining, but it start after I send a request to my application."
Zipkin,50753384,51073739.0,1,"2018/06/08, 07:28:46",True,"2018/06/28, 05:12:27",nan,5254103.0,1728.0,1,638,Why my zipkin client do not connect to my zipkin server,"With dependency spring-cloud-starter-zipkin, App should connect to zipkin server when sleuth triggered."
Zipkin,50753384,51073739.0,1,"2018/06/08, 07:28:46",True,"2018/06/28, 05:12:27",nan,5254103.0,1728.0,1,638,Why my zipkin client do not connect to my zipkin server,"I did not started zipkin server, so it should throw a connection exception."
Zipkin,50753384,51073739.0,1,"2018/06/08, 07:28:46",True,"2018/06/28, 05:12:27",nan,5254103.0,1728.0,1,638,Why my zipkin client do not connect to my zipkin server,But nothing happened.
Zipkin,50753384,51073739.0,1,"2018/06/08, 07:28:46",True,"2018/06/28, 05:12:27",nan,5254103.0,1728.0,1,638,Why my zipkin client do not connect to my zipkin server,"And when I start zipkin server, it can not receive anything."
Zipkin,50753384,51073739.0,1,"2018/06/08, 07:28:46",True,"2018/06/28, 05:12:27",nan,5254103.0,1728.0,1,638,Why my zipkin client do not connect to my zipkin server,pom.xml
Zipkin,50753384,51073739.0,1,"2018/06/08, 07:28:46",True,"2018/06/28, 05:12:27",nan,5254103.0,1728.0,1,638,Why my zipkin client do not connect to my zipkin server,App.java
Zipkin,50753384,51073739.0,1,"2018/06/08, 07:28:46",True,"2018/06/28, 05:12:27",nan,5254103.0,1728.0,1,638,Why my zipkin client do not connect to my zipkin server,application.properties
Zipkin,50753384,51073739.0,1,"2018/06/08, 07:28:46",True,"2018/06/28, 05:12:27",nan,5254103.0,1728.0,1,638,Why my zipkin client do not connect to my zipkin server,And logs
Zipkin,50619067,nan,0,"2018/05/31, 10:36:32",False,"2018/05/31, 10:42:08","2018/05/31, 10:42:08",8542187.0,1533.0,1,119,How to implement Zipkin in vert.x,I want to implement Zipkin and Sleuth on vert.x application.
Zipkin,50619067,nan,0,"2018/05/31, 10:36:32",False,"2018/05/31, 10:42:08","2018/05/31, 10:42:08",8542187.0,1533.0,1,119,How to implement Zipkin in vert.x,I have Zipkin server application with Spring Boot and Spring Boot Services and its okay.
Zipkin,50619067,nan,0,"2018/05/31, 10:36:32",False,"2018/05/31, 10:42:08","2018/05/31, 10:42:08",8542187.0,1533.0,1,119,How to implement Zipkin in vert.x,I have added dependencies and setup them in application.properties and its working good.
Zipkin,50619067,nan,0,"2018/05/31, 10:36:32",False,"2018/05/31, 10:42:08","2018/05/31, 10:42:08",8542187.0,1533.0,1,119,How to implement Zipkin in vert.x,But I have one vert.x application and I have found it difficult to implement the Zipkin there.
Zipkin,50619067,nan,0,"2018/05/31, 10:36:32",False,"2018/05/31, 10:42:08","2018/05/31, 10:42:08",8542187.0,1533.0,1,119,How to implement Zipkin in vert.x,I have searched but I did not found some good example on how to implement it on vert.x application.
Zipkin,50619067,nan,0,"2018/05/31, 10:36:32",False,"2018/05/31, 10:42:08","2018/05/31, 10:42:08",8542187.0,1533.0,1,119,How to implement Zipkin in vert.x,"So, what I need to do to implement the Zipkin in the vert.x application?"
Zipkin,50599433,nan,1,"2018/05/30, 11:05:35",False,"2018/05/30, 11:16:48","2018/05/30, 11:16:48",2343510.0,1247.0,1,106,Disable send Zipkin spans on Pilot,How can I disable Istio to send spans to zipkin?
Zipkin,50599433,nan,1,"2018/05/30, 11:05:35",False,"2018/05/30, 11:16:48","2018/05/30, 11:16:48",2343510.0,1247.0,1,106,Disable send Zipkin spans on Pilot,If I'm not wrong this is not a mixer adapter right?
Zipkin,50599433,nan,1,"2018/05/30, 11:05:35",False,"2018/05/30, 11:16:48","2018/05/30, 11:16:48",2343510.0,1247.0,1,106,Disable send Zipkin spans on Pilot,It is something directly done from the pilot.
Zipkin,50599433,nan,1,"2018/05/30, 11:05:35",False,"2018/05/30, 11:16:48","2018/05/30, 11:16:48",2343510.0,1247.0,1,106,Disable send Zipkin spans on Pilot,How can I disable it?
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,I'm adding Spring Cloud Sleuth with Zipkin to existing code to collect trace information and eventually log arbitrary messages.
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,Regular request spans are correctly sent to Zipkin:
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,"However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans)."
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,"If I use  org.slf4j.Logger  to simply  LOG.info(""something"") , I see the  INFO  message in console output, with the  exportable  flag set to true:"
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,"Checking the traces in Zipkin, the span is correctly found, but the message used in the  LOG.info()  line is nowhere to be seen -- which suggests me that I'm doing something wrong here, or maybe it's just not supposed to work this way."
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,My sampling percentage is set to 100%.
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,Using the slf4j interface would be convenient because the existing code is already instrumented that way.
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,Is it possible?
Zipkin,50533178,50539945.0,1,"2018/05/25, 19:10:18",True,"2018/05/26, 09:21:55",nan,3249650.0,108.0,1,718,Send Spring Cloud Sleuth log messages to Zipkin,"If so, what could be a good way to implement it?"
Zipkin,50506432,nan,1,"2018/05/24, 13:01:19",True,"2018/05/25, 14:08:47",nan,1826788.0,1408.0,1,64,Zipkin With Spring Boot,I am using spring boot and zipkin.
Zipkin,50506432,nan,1,"2018/05/24, 13:01:19",True,"2018/05/25, 14:08:47",nan,1826788.0,1408.0,1,64,Zipkin With Spring Boot,And using spring slueth to generate trace Id.
Zipkin,50506432,nan,1,"2018/05/24, 13:01:19",True,"2018/05/25, 14:08:47",nan,1826788.0,1408.0,1,64,Zipkin With Spring Boot,Is there a way I can generate this trace Id on my own?
Zipkin,50506432,nan,1,"2018/05/24, 13:01:19",True,"2018/05/25, 14:08:47",nan,1826788.0,1408.0,1,64,Zipkin With Spring Boot,"Also I want to log only specific requests say with 500 error or response time   threshold, how to do this?"
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,I have a system where we have 2 modules.
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,"1) Module 1 is a webapp with multiple endpoints, deployed on Tomcat."
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,"2) Module 2 is an executable jar,(not a web-app) which spins up 2 Kafka consumers (K1 and K2) listening to topic1 and topic2 respectively."
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,The web-app (Module 1) pushes messages to topic1.
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,"K1 listens to topic1.It receives messages, processes them and pushes the processed messages to topic2."
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,K2 listens to topic2.
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,The messages are fully processed by K2 and do not propagate further.
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,There are multiple points where errors can occur in this flow.
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,"I wanted to use Zipkin/ Jaegar to trace the entire flow, and also link the logs to the trace id, so that any issue can be easily and quickly investigated."
Zipkin,50389884,nan,1,"2018/05/17, 14:04:37",True,"2018/05/23, 04:20:43",nan,1808266.0,51.0,1,494,Linking log4j logs and zipkin trace id,Can anyone suggest me the way to go forward?
Zipkin,49730454,nan,0,"2018/04/09, 13:09:39",False,"2018/04/09, 13:09:39",nan,4745150.0,51.0,1,61,Does zipkin support activemq as a collector?,I want to use ActiveMQ as a zipkin collector.
Zipkin,49730454,nan,0,"2018/04/09, 13:09:39",False,"2018/04/09, 13:09:39",nan,4745150.0,51.0,1,61,Does zipkin support activemq as a collector?,"I have already used rabbitmq as collector, but my client is specific to using ActiveMQ."
Zipkin,49730454,nan,0,"2018/04/09, 13:09:39",False,"2018/04/09, 13:09:39",nan,4745150.0,51.0,1,61,Does zipkin support activemq as a collector?,Please let know if Zipkin supports ActiveMQ as collector and what are the configurations needed for that?
Zipkin,49558386,nan,0,"2018/03/29, 17:14:09",False,"2018/03/29, 17:14:09",nan,9569751.0,13.0,1,68,why don&#39;t zipkin have support to trace database container?,I have the microservice architecture of E-commerce website with all kinds of service containers and its corresponding database containers.
Zipkin,49558386,nan,0,"2018/03/29, 17:14:09",False,"2018/03/29, 17:14:09",nan,9569751.0,13.0,1,68,why don&#39;t zipkin have support to trace database container?,"I failed to find a library to purely trace database running time for each request in like mongodb or mysql, so I used tcpdump to check the HTTP request roundtime at the port of service container."
Zipkin,49558386,nan,0,"2018/03/29, 17:14:09",False,"2018/03/29, 17:14:09",nan,9569751.0,13.0,1,68,why don&#39;t zipkin have support to trace database container?,The total communication time is always shorter than Zipkin's log time for that HTTP request.
Zipkin,49558386,nan,0,"2018/03/29, 17:14:09",False,"2018/03/29, 17:14:09",nan,9569751.0,13.0,1,68,why don&#39;t zipkin have support to trace database container?,"So I am assuming that Zipkin include some service container process time in the tracing, which is not what I desire to have."
Zipkin,49558386,nan,0,"2018/03/29, 17:14:09",False,"2018/03/29, 17:14:09",nan,9569751.0,13.0,1,68,why don&#39;t zipkin have support to trace database container?,"I plan to use tcpdump on database container and decode the package for tracing id, which could be a lot of hassle."
Zipkin,49558386,nan,0,"2018/03/29, 17:14:09",False,"2018/03/29, 17:14:09",nan,9569751.0,13.0,1,68,why don&#39;t zipkin have support to trace database container?,"Why can't zipkin track this activity, or there might be some tools I can use?"
Zipkin,49396204,nan,1,"2018/03/21, 02:19:28",False,"2018/03/21, 15:35:06","2018/03/21, 02:46:43",7472851.0,11.0,1,613,zipkin 2.6 with rabbitmq collector,"I am trying to run zipkin with rabbitmq collector enabled, like this:"
Zipkin,49396204,nan,1,"2018/03/21, 02:19:28",False,"2018/03/21, 15:35:06","2018/03/21, 02:46:43",7472851.0,11.0,1,613,zipkin 2.6 with rabbitmq collector,I can resolve tracing to an IP address and port 5672 is open.
Zipkin,49396204,nan,1,"2018/03/21, 02:19:28",False,"2018/03/21, 15:35:06","2018/03/21, 02:46:43",7472851.0,11.0,1,613,zipkin 2.6 with rabbitmq collector,A queue called zipkin has been created in RabbitMQ.
Zipkin,49396204,nan,1,"2018/03/21, 02:19:28",False,"2018/03/21, 15:35:06","2018/03/21, 02:46:43",7472851.0,11.0,1,613,zipkin 2.6 with rabbitmq collector,Here is the exception thrown:
Zipkin,49396204,nan,1,"2018/03/21, 02:19:28",False,"2018/03/21, 15:35:06","2018/03/21, 02:46:43",7472851.0,11.0,1,613,zipkin 2.6 with rabbitmq collector,Does anybody have any idea what I am doing wrong?
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,I am trying to integrate the Brave MySql Instrumentation into my Spring Boot 2.x service to automatically let its interceptor enrich my traces with spans concerning MySql-Queries.
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,The current Gradle-Dependencies are the following
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,I already configured Sleuth successfully to send traces concerning HTTP-Request to my Zipkin-Server and now I wanted to add some spans for each MySql-Query the service does.
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,The TracingConfiguration it this:
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,"The Query-Interceptor works properly, but my problem now is that the spans are not added to the existing trace but each are added to a new one."
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,"I guess its because of the creation of a new sender/reporter in the configuration, but I have not been able to reuse the existing one created by the Spring Boot Autoconfiguration."
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,That would moreover remove the necessity to redundantly define the Zipkin-Url (because it is already defined for Zipkin in my application.yml).
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,"I already tried autowiring the Zipkin-Reporter to my Bean, but all I got is a  SpanReporter  - but the Brave-Tracer-Builder requries a  Reporter&lt;Span&gt;"
Zipkin,48626548,48628487.0,1,"2018/02/05, 18:00:23",True,"2018/02/05, 19:47:34",nan,3042624.0,212.0,1,993,Spring Boot 2 integrate Brave MySQL-Integration into Zipkin,Do you have any advice for me how to properly wire things up?
Zipkin,47820538,nan,0,"2017/12/14, 21:10:34",False,"2017/12/14, 21:10:34",nan,4436650.0,1055.0,1,151,What about WSO2 ESB Analytics and Zipkin integration?,I am thinking of integrating WSO2 ESB and Zipkin following Open Tracing using a  Customize Statics Publisher implemented  by me.
Zipkin,47820538,nan,0,"2017/12/14, 21:10:34",False,"2017/12/14, 21:10:34",nan,4436650.0,1055.0,1,151,What about WSO2 ESB Analytics and Zipkin integration?,The idea is enable statics and trace in WSO2 ESB.
Zipkin,47820538,nan,0,"2017/12/14, 21:10:34",False,"2017/12/14, 21:10:34",nan,4436650.0,1055.0,1,151,What about WSO2 ESB Analytics and Zipkin integration?,Is there any other approach to to achieve this?
Zipkin,47820538,nan,0,"2017/12/14, 21:10:34",False,"2017/12/14, 21:10:34",nan,4436650.0,1055.0,1,151,What about WSO2 ESB Analytics and Zipkin integration?,Is this approach correct?
Zipkin,47656678,nan,0,"2017/12/05, 17:02:16",False,"2017/12/05, 17:02:16",nan,5360410.0,11.0,1,232,Zipkin Collector Sampling based on Traces/End To End Transaction &amp; not based on individual spans,"The Collector Sampler today, samples the spans as received by the Zipkins Collector based on the rate."
Zipkin,47656678,nan,0,"2017/12/05, 17:02:16",False,"2017/12/05, 17:02:16",nan,5360410.0,11.0,1,232,Zipkin Collector Sampling based on Traces/End To End Transaction &amp; not based on individual spans,"But for a micro-services architecture where a single transaction traverses through multiple applications and spans, having a sampler logic to sample based on just the number of spans might cause us to loose a whole picture of a single transaction."
Zipkin,47656678,nan,0,"2017/12/05, 17:02:16",False,"2017/12/05, 17:02:16",nan,5360410.0,11.0,1,232,Zipkin Collector Sampling based on Traces/End To End Transaction &amp; not based on individual spans,So we are looking for an ideal solution where the Zipkin (not the app's sleuth implementation) has options to sample the transactions based on trace(or a whole transaction with all its spans) and not based on individual spans.
Zipkin,47656678,nan,0,"2017/12/05, 17:02:16",False,"2017/12/05, 17:02:16",nan,5360410.0,11.0,1,232,Zipkin Collector Sampling based on Traces/End To End Transaction &amp; not based on individual spans,And we expect this to be more asynchronous sampler technique.
Zipkin,47656678,nan,0,"2017/12/05, 17:02:16",False,"2017/12/05, 17:02:16",nan,5360410.0,11.0,1,232,Zipkin Collector Sampling based on Traces/End To End Transaction &amp; not based on individual spans,Looking forward for your thoughts...
Zipkin,47564534,nan,0,"2017/11/30, 03:34:59",False,"2017/11/30, 03:34:59",nan,1339422.0,2434.0,1,78,"Alternate graphs, UI for zipkin?","I'm looking for better graphs for the traces stored in zipkin.io database, wondering if anyone developed anything better than out of box zipkin UI..are there any alternate GUIs for Zipkin?"
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,"I have mircroservice environment based on spring-boot, where i am using zipkin server and discovery-server(eureka) and config-server."
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,Now i have a rest-microservice which sends logs to zipkin server and this microservice is required to resolve where is zipkin server using discovery-server.
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,following is zipkin configuration i have in my rest-microservice's application.properties(pulled from config-server).
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,here MTD-ZIPKIN-SERVER is zipkin-server name in discovery-server.
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,discovery-server dashboard.
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,"but it does not try to resolve zipkin from discovery-server, instead it tries to connect directly using spring.zipkin.baseUrl, and i get below exception."
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,"Dropped 1 spans due to ResourceAccessException(I/O error on POST request for "" http://MTD-ZIPKIN-SERVER/api/v1/spans "":
  MTD-ZIPKIN-SERVER; nested exception is java.net.UnknownHostException:
  MTD-ZIPKIN-SERVER)"
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,"org.springframework.web.client.ResourceAccessException: I/O error on
  POST request for "" http://MTD-ZIPKIN-SERVER/api/v1/spans "":
  MTD-ZIPKIN-SERVER; nested exception is java.net.UnknownHostException:
  MTD-ZIPKIN-SERVER     at
  org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:666)
    at
  org.springframework.web.client.RestTemplate.execute(RestTemplate.java:628)
    at
  org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:590)
    at
  org.springframework.cloud.sleuth.zipkin.RestTemplateSender.post(RestTemplateSender.java:73)
    at
  org.springframework.cloud.sleuth.zipkin.RestTemplateSender.sendSpans(RestTemplateSender.java:46)
    at
  zipkin.reporter.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:245)
    at
  zipkin.reporter.AsyncReporter$Builder.lambda$build$0(AsyncReporter.java:166)
    at zipkin.reporter.AsyncReporter$Builder$$Lambda$1.run(Unknown
  Source)   at java.lang.Thread.run(Thread.java:745) Caused by:
  java.net.UnknownHostException: MTD-ZIPKIN-SERVER  at
  java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)"
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,if i provide exact zipkin url in property spring.zipkin.baseUrl like below
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,then my rest-microservice is able to connect to zipkin-server.
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,My goal here is to read zipkin-server location from discovery-srever.
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,What am i doing wrong?
Zipkin,47343439,nan,1,"2017/11/17, 06:44:23",True,"2018/11/03, 20:12:50","2018/11/03, 20:12:50",1372978.0,261.0,1,2639,microservice not able to locate zipkin service using discovery-server,Do i need to add some zipkin enabling annotation on my spring-boot rest-microservice?
Zipkin,46650765,nan,0,"2017/10/09, 19:12:17",False,"2017/10/11, 20:50:50","2017/10/09, 19:57:31",68612.0,8770.0,1,407,Zipkin using Consul service name,For some reason Zipkin is using the Consul discovery name instead of the base spring.application.name property.
Zipkin,46650765,nan,0,"2017/10/09, 19:12:17",False,"2017/10/11, 20:50:50","2017/10/09, 19:57:31",68612.0,8770.0,1,407,Zipkin using Consul service name,But I want it to use the non-randomized application name (so myservice instead of myservice-67gg8d368).
Zipkin,46650765,nan,0,"2017/10/09, 19:12:17",False,"2017/10/11, 20:50:50","2017/10/09, 19:57:31",68612.0,8770.0,1,407,Zipkin using Consul service name,If I set the Zipkin property  zipkin.service.name  then Consul throws errors saying it cannot find the service.
Zipkin,46650765,nan,0,"2017/10/09, 19:12:17",False,"2017/10/11, 20:50:50","2017/10/09, 19:57:31",68612.0,8770.0,1,407,Zipkin using Consul service name,I'm unsure why the two are even sharing properties and not just adhering to their own.
Zipkin,46650765,nan,0,"2017/10/09, 19:12:17",False,"2017/10/11, 20:50:50","2017/10/09, 19:57:31",68612.0,8770.0,1,407,Zipkin using Consul service name,"I'd like the service to use it's base application name because otherwise Zipkin is hard to use, as it lists every new container as a completely new service, making it very difficult to see over time how code changes have changed timing."
Zipkin,46650765,nan,0,"2017/10/09, 19:12:17",False,"2017/10/11, 20:50:50","2017/10/09, 19:57:31",68612.0,8770.0,1,407,Zipkin using Consul service name,"UPDATE:
This is the error I get in my logs if I set the zipkin.service.name"
Zipkin,46356678,nan,0,"2017/09/22, 07:05:11",False,"2017/11/27, 11:29:59","2017/09/22, 07:18:17",8460095.0,11.0,1,330,Zipkin data not able to persist in Elastic search,I am having problem persisting the zipkin data.
Zipkin,46356678,nan,0,"2017/09/22, 07:05:11",False,"2017/11/27, 11:29:59","2017/09/22, 07:18:17",8460095.0,11.0,1,330,Zipkin data not able to persist in Elastic search,I did not get any error message.
Zipkin,46356678,nan,0,"2017/09/22, 07:05:11",False,"2017/11/27, 11:29:59","2017/09/22, 07:18:17",8460095.0,11.0,1,330,Zipkin data not able to persist in Elastic search,So I am sharing configuration to get a help.
Zipkin,46356678,nan,0,"2017/09/22, 07:05:11",False,"2017/11/27, 11:29:59","2017/09/22, 07:18:17",8460095.0,11.0,1,330,Zipkin data not able to persist in Elastic search,"I can see my logs in zipkin UI, but not able to persist in elastic search."
Zipkin,46356678,nan,0,"2017/09/22, 07:05:11",False,"2017/11/27, 11:29:59","2017/09/22, 07:18:17",8460095.0,11.0,1,330,Zipkin data not able to persist in Elastic search,My zipkin-service pom file shared below.
Zipkin,46356678,nan,0,"2017/09/22, 07:05:11",False,"2017/11/27, 11:29:59","2017/09/22, 07:18:17",8460095.0,11.0,1,330,Zipkin data not able to persist in Elastic search,And my properties looks like this
Zipkin,46287877,nan,1,"2017/09/18, 23:41:02",False,"2017/09/19, 02:30:27",nan,4033050.0,2343.0,1,430,extract Zipkin data,I am currently using zipkin and I am trying to build a python script to extract the data saved and process them differently.
Zipkin,46287877,nan,1,"2017/09/18, 23:41:02",False,"2017/09/19, 02:30:27",nan,4033050.0,2343.0,1,430,extract Zipkin data,zipkin logs and UI are working fine.
Zipkin,46287877,nan,1,"2017/09/18, 23:41:02",False,"2017/09/19, 02:30:27",nan,4033050.0,2343.0,1,430,extract Zipkin data,I have done this :
Zipkin,46287877,nan,1,"2017/09/18, 23:41:02",False,"2017/09/19, 02:30:27",nan,4033050.0,2343.0,1,430,extract Zipkin data,I have used wget and request and both are giving me the result below:
Zipkin,46287877,nan,1,"2017/09/18, 23:41:02",False,"2017/09/19, 02:30:27",nan,4033050.0,2343.0,1,430,extract Zipkin data,but If I copy paste the URL used in the request or wget in a web browser the result is shown
Zipkin,46287877,nan,1,"2017/09/18, 23:41:02",False,"2017/09/19, 02:30:27",nan,4033050.0,2343.0,1,430,extract Zipkin data,any idea how to extract the data in JSON or any other format for a Zipkin server?
Zipkin,46287877,nan,1,"2017/09/18, 23:41:02",False,"2017/09/19, 02:30:27",nan,4033050.0,2343.0,1,430,extract Zipkin data,Thanks
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,So I'm having zipkin gathering my data inside kubernetes from other services.
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,I'm having nginx ingress controller defined to expose my services and all works nice.
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,As zipkin is admin thing I'd love to have it behind some security ie.
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,basic auth.
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,"If I add 3 lines marked as ""#problematic lines - start"" and ""#problematic lines - stop"" below my zipkin front is no longer visible and I get 503."
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,"It's created with  https://github.com/kubernetes/ingress/tree/master/examples/auth/basic/nginx 
and no difficult things here."
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,I'm not sure if it's not about possible infulence but I used  https://github.com/kubernetes/ingress/blob/master/controllers/nginx/rootfs/etc/nginx/nginx.conf  file as template for my nginx ingress controller as I needed to modify some CORS rules.
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,I see there part:
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,but I don't see result in:  kubectl exec nginx-ingress-controller-lalala-lalala -n kube-system cat /etc/nginx/nginx.conf | grep auth .
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,Due to this my guess is that I need to add some annotation to make this  {{ if $location.BasicDigestAuth.Secured }}  part work.
Zipkin,45492904,nan,1,"2017/08/03, 22:22:41",False,"2017/08/11, 13:02:33",nan,641071.0,935.0,1,874,kubernetes nginx ingress zipkin basic-auth,Unfortunately I cannot find anything about it.
Zipkin,45071000,nan,0,"2017/07/13, 06:36:37",False,"2017/07/13, 07:46:17","2017/07/13, 07:46:17",8288080.0,11.0,1,120,Zipkin ui can&#39;t load the trace diagram,"I saw the services had send the trace to zipkin server, and also found the trace result list in zipkin ui, when I clicked one of them to see the trace detail and timeline diagram, but there had no diagram just some duration as the title and json data is ok."
Zipkin,45071000,nan,0,"2017/07/13, 06:36:37",False,"2017/07/13, 07:46:17","2017/07/13, 07:46:17",8288080.0,11.0,1,120,Zipkin ui can&#39;t load the trace diagram,Is there something I can do ?
Zipkin,45071000,nan,0,"2017/07/13, 06:36:37",False,"2017/07/13, 07:46:17","2017/07/13, 07:46:17",8288080.0,11.0,1,120,Zipkin ui can&#39;t load the trace diagram,zipkin
Zipkin,44795774,nan,0,"2017/06/28, 10:07:45",False,"2017/06/28, 10:07:45",nan,2721139.0,6642.0,1,379,How to configure Zipkin to use MS SQLServer database?,"On the official Zipkin repository  README  I see how to configure Zipkin with either Cassandra, MySql or Elastic Search."
Zipkin,44795774,nan,0,"2017/06/28, 10:07:45",False,"2017/06/28, 10:07:45",nan,2721139.0,6642.0,1,379,How to configure Zipkin to use MS SQLServer database?,"However in my current job, we'd like to make use of Zipkin, but we are limited to Microsoft SQL Server as the only supported database."
Zipkin,44795774,nan,0,"2017/06/28, 10:07:45",False,"2017/06/28, 10:07:45",nan,2721139.0,6642.0,1,379,How to configure Zipkin to use MS SQLServer database?,"AFAIK Zipkin works with MySql via standard JDBC driver, therefore I think it would be possible to make it work with SqlServer as well."
Zipkin,44795774,nan,0,"2017/06/28, 10:07:45",False,"2017/06/28, 10:07:45",nan,2721139.0,6642.0,1,379,How to configure Zipkin to use MS SQLServer database?,Am I right?
Zipkin,44795774,nan,0,"2017/06/28, 10:07:45",False,"2017/06/28, 10:07:45",nan,2721139.0,6642.0,1,379,How to configure Zipkin to use MS SQLServer database?,"If so, how can I configure it to work over it?"
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,"I recently upgraded my project from Spring Boot 1.4.1, Spring Cloud Sleuth 1.1.0, Spring Cloud Zipkin 1.1.0 to Spring Boot 1.5.3, Spring Cloud Sleuth 1.2.0, Spring Cloud Zipkin 1.2.0."
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,"Read that with the latest version of Spring Cloud Sleuth, they had added ""error"" tags which will get reported to Zipkin automatically in case of any exceptions."
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,I have a @ControllerAdvice class extending ResponseEntityExceptionHandler for custom exception handling.
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,"I was able to report errors to the Tracer and visualize the same in Zipkin when using the old versions (Spring Boot 1.4.1, Spring Cloud Sleuth 1.1.0, Spring Cloud Zipkin 1.1.0) using the below method:"
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,"After I upgraded, this doesn't seem to work and spring cloud sleuth's default error reporting was also not happening."
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,"Only after commenting out the @ControllerAdvice and letting Spring Boot's default ErrorController to handle the exceptions, I was able to visualize the errors in Zipkin."
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,"However, we need the custom exception handling to format the error response in a standard way with error codes across all our PaaS services."
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,Is there a way to do this?
Zipkin,43790619,43843859.0,1,"2017/05/04, 21:47:51",True,"2017/09/27, 16:17:19","2017/09/27, 16:17:19",7964976.0,13.0,1,763,Sleuth/Zipkin tracing with @ControllerAdvice,Should I use any other Sleuth objects to achieve this?
Zipkin,40701663,41723986.0,1,"2016/11/20, 09:26:06",True,"2018/10/30, 09:14:34",nan,240928.0,347.0,1,1714,Unable to start the Spring Cloud ZipKin Server,"I am not able to start the Spring Cloud ZipKin Server, It is giving below mentioned exception."
Zipkin,40701663,41723986.0,1,"2016/11/20, 09:26:06",True,"2018/10/30, 09:14:34",nan,240928.0,347.0,1,1714,Unable to start the Spring Cloud ZipKin Server,"BeanCreationException: Cannot create binder factory, no  META-INF/spring.binders  resources found on the classpath"
Zipkin,40701663,41723986.0,1,"2016/11/20, 09:26:06",True,"2018/10/30, 09:14:34",nan,240928.0,347.0,1,1714,Unable to start the Spring Cloud ZipKin Server,Below are my maven dependencies -
Zipkin,40701663,41723986.0,1,"2016/11/20, 09:26:06",True,"2018/10/30, 09:14:34",nan,240928.0,347.0,1,1714,Unable to start the Spring Cloud ZipKin Server,Also my application startup class looks as below.
Zipkin,40701663,41723986.0,1,"2016/11/20, 09:26:06",True,"2018/10/30, 09:14:34",nan,240928.0,347.0,1,1714,Unable to start the Spring Cloud ZipKin Server,Any help is highly appreciated.
Zipkin,37842256,37867634.0,1,"2016/06/15, 20:42:53",True,"2016/06/16, 22:15:19","2016/06/15, 20:52:20",1822028.0,911.0,1,628,Does Brave (Zipkin) have support for parallel requests from client to servers,"I have a distributed system, where a client needs information from multiple sources."
Zipkin,37842256,37867634.0,1,"2016/06/15, 20:42:53",True,"2016/06/16, 22:15:19","2016/06/15, 20:52:20",1822028.0,911.0,1,628,Does Brave (Zipkin) have support for parallel requests from client to servers,Is there any support for marking parallel processed spans for the same trace in Brave (Java implementation of Zipkin framework)?
Zipkin,37842256,37867634.0,1,"2016/06/15, 20:42:53",True,"2016/06/16, 22:15:19","2016/06/15, 20:52:20",1822028.0,911.0,1,628,Does Brave (Zipkin) have support for parallel requests from client to servers,"Currently, before sending a message I call clientRequestInterceptor.handle(...) and after receiving response clientResponseInterceptor.handle(...), but there is only one instance, so only one span is recorded."
Zipkin,37842256,37867634.0,1,"2016/06/15, 20:42:53",True,"2016/06/16, 22:15:19","2016/06/15, 20:52:20",1822028.0,911.0,1,628,Does Brave (Zipkin) have support for parallel requests from client to servers,P.S.
Zipkin,37842256,37867634.0,1,"2016/06/15, 20:42:53",True,"2016/06/16, 22:15:19","2016/06/15, 20:52:20",1822028.0,911.0,1,628,Does Brave (Zipkin) have support for parallel requests from client to servers,I found the following project on GitHub that specified that Brave only supports one level of nested client call:  https://github.com/leigu/brave-tracer-example .
Zipkin,37842256,37867634.0,1,"2016/06/15, 20:42:53",True,"2016/06/16, 22:15:19","2016/06/15, 20:52:20",1822028.0,911.0,1,628,Does Brave (Zipkin) have support for parallel requests from client to servers,Maybe the same is valid for parallel client calls.
Zipkin,17017657,nan,1,"2013/06/10, 08:59:51",True,"2019/09/02, 10:04:21",nan,1542639.0,557.0,1,1068,How do I setup zipkin to trace different applications running on activemq?,The infrastructure of system used by my company consists of different application running on a  ActiveMQ .
Zipkin,17017657,nan,1,"2013/06/10, 08:59:51",True,"2019/09/02, 10:04:21",nan,1542639.0,557.0,1,1068,How do I setup zipkin to trace different applications running on activemq?,I want to know how would I setup  Zipkin  to trace the communication between different applications under  ActiveMQ ?
Zipkin,17017657,nan,1,"2013/06/10, 08:59:51",True,"2019/09/02, 10:04:21",nan,1542639.0,557.0,1,1068,How do I setup zipkin to trace different applications running on activemq?,Thanks
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,"Please,  notice  that this question can seem a duplicate of  this one , but it's not the case."
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,Below I include my rational
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,I'm trying to add Sleuth/Zipkin trace to my project.
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,For this I have followed this  tutorial .
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,My project is already using RabbitMQ for communication among the different microservices working fine.
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,"My problem is that I'm able to get the traces fine when I use the web connection, but I get an unable to connect error when I try to communicate using RabbitMQ."
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,"As commented in the first line, my problem does not seem to be related to rabbitmq host itself, because it is up and running, and providing service to my microservices."
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,"Sure that I missing something in the configuration, but I cannot find it (I have also checked  this post ,  this  and  this ."
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,My files (I have removed not relevant part):
Zipkin,65943408,nan,2,"2021/01/28, 20:48:26",True,"2021/01/30, 11:30:34",nan,7349864.0,830.0,0,180,Zipkin is not connecting to RabbitMQ,"Notice that is started with profile 'docker', so host for rabbit is rabbitmq"
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,I have been searching if there is a way to instrument winstonJS with zipkin-js related data for each log.
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,"However, I have not been able to find something."
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,I saw some example where it mentioned to configure winston as follows
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,"However, the formatter is not available in the latest version of winston."
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,I was trying out to configure winston as
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,But the fields are never received.
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,"Also, when using express and configuring the winstonjs with"
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,My implementation is available at:  https://github.com/vtapadia/sample-node-app/blob/zipkin-tracing/src/config/logger.ts
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,I just want my normal logging to have the tracing information auto injected.
Zipkin,61993025,nan,0,"2020/05/25, 01:10:53",False,"2020/05/25, 01:10:53",nan,2984491.0,1159.0,0,128,How to instrument winstonjs logger with zipkin traceID/spanID auto addition,Is it possible to achieve and can someone point to an example for this.
Zipkin,58551796,58554326.0,1,"2019/10/25, 06:28:06",True,"2019/10/25, 10:49:15","2019/10/25, 06:34:46",5871191.0,144.0,0,118,Sending SpringBoot / Sleuth Zipkin Traces to Honeycomb.io,Sending traces from an existing instrumented Spring Boot application to  honeycomb-opentracing-proxy  is failing with the following error in the proxy console:
Zipkin,58551796,58554326.0,1,"2019/10/25, 06:28:06",True,"2019/10/25, 10:49:15","2019/10/25, 06:34:46",5871191.0,144.0,0,118,Sending SpringBoot / Sleuth Zipkin Traces to Honeycomb.io,"Spring Boot Version: 2.1.3.RELEASE
Spring Cloud Sleuth Version: 2.1.1.RELEASE"
Zipkin,58551796,58554326.0,1,"2019/10/25, 06:28:06",True,"2019/10/25, 10:49:15","2019/10/25, 06:34:46",5871191.0,144.0,0,118,Sending SpringBoot / Sleuth Zipkin Traces to Honeycomb.io,Running the open tracing proxy with the following docker command:
Zipkin,58551796,58554326.0,1,"2019/10/25, 06:28:06",True,"2019/10/25, 10:49:15","2019/10/25, 06:34:46",5871191.0,144.0,0,118,Sending SpringBoot / Sleuth Zipkin Traces to Honeycomb.io,From reading the documentation  here  the honeycomb-opentracing-proxy only supports v1 of the JSON API so I have explicitly set that in spring cloud config as this appears to default to v2.
Zipkin,58551796,58554326.0,1,"2019/10/25, 06:28:06",True,"2019/10/25, 10:49:15","2019/10/25, 06:34:46",5871191.0,144.0,0,118,Sending SpringBoot / Sleuth Zipkin Traces to Honeycomb.io,application.properties
Zipkin,58551796,58554326.0,1,"2019/10/25, 06:28:06",True,"2019/10/25, 10:49:15","2019/10/25, 06:34:46",5871191.0,144.0,0,118,Sending SpringBoot / Sleuth Zipkin Traces to Honeycomb.io,Any help would be greatly appreciated
Zipkin,56923548,nan,1,"2019/07/07, 18:28:13",False,"2020/05/23, 19:50:21",nan,1592149.0,830.0,0,103,How does Zipkin generate and store the 16 char trace id used in the get Api of /traces/{traceId},I am using Zipkin for distributed tracing.
Zipkin,56923548,nan,1,"2019/07/07, 18:28:13",False,"2020/05/23, 19:50:21",nan,1592149.0,830.0,0,103,How does Zipkin generate and store the 16 char trace id used in the get Api of /traces/{traceId},I have added zipkin-storage-mysql dependency in order to save the traces in MySQL DB.
Zipkin,56923548,nan,1,"2019/07/07, 18:28:13",False,"2020/05/23, 19:50:21",nan,1592149.0,830.0,0,103,How does Zipkin generate and store the 16 char trace id used in the get Api of /traces/{traceId},"When I query ZIPKIN_SPANS table, I don't find the 16 char trace id in TRACE_ID Colum that I use in order to load the trace on zipkin UI."
Zipkin,56923548,nan,1,"2019/07/07, 18:28:13",False,"2020/05/23, 19:50:21",nan,1592149.0,830.0,0,103,How does Zipkin generate and store the 16 char trace id used in the get Api of /traces/{traceId},for ex:  localhost:9411/traces/4bcdd0bd5d2f70c0
Zipkin,56923548,nan,1,"2019/07/07, 18:28:13",False,"2020/05/23, 19:50:21",nan,1592149.0,830.0,0,103,How does Zipkin generate and store the 16 char trace id used in the get Api of /traces/{traceId},Please help me understand how can I figure it out.
Zipkin,56923548,nan,1,"2019/07/07, 18:28:13",False,"2020/05/23, 19:50:21",nan,1592149.0,830.0,0,103,How does Zipkin generate and store the 16 char trace id used in the get Api of /traces/{traceId},"Also, How can I add a new column to the table for associating an application-specific id with it"
Zipkin,56032752,nan,1,"2019/05/08, 05:33:11",False,"2020/08/19, 12:00:44","2020/08/19, 12:00:30",486631.0,167.0,0,297,Spring boot 2 fail to enable zipkin/sleuth with default configuration,Spring boot application with below configuration :
Zipkin,56032752,nan,1,"2019/05/08, 05:33:11",False,"2020/08/19, 12:00:44","2020/08/19, 12:00:30",486631.0,167.0,0,297,Spring boot 2 fail to enable zipkin/sleuth with default configuration,Required zipkin tracing feature.
Zipkin,56032752,nan,1,"2019/05/08, 05:33:11",False,"2020/08/19, 12:00:44","2020/08/19, 12:00:30",486631.0,167.0,0,297,Spring boot 2 fail to enable zipkin/sleuth with default configuration,But my application not listing on zipkin server used proper jar also as below
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,Spring Doc says
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,Spring Cloud Sleuth is compatible with OpenTracing.
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,"If you have OpenTracing on the classpath, we automatically register the OpenTracing Tracer bean ."
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,"If you wish to disable this, set spring.sleuth.opentracing.enabled to false"
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,I have the below dependency in my POM.
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,"But, I get the following print out it the logs when I try to print the trace and span information :  tracer: NoopTracer"
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,Why am I getting a NopTracer?
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,Why isn't Brave being registered automatically as promised?
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,Am I doing something wrong?
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,I am using
Zipkin,53369694,53370111.0,1,"2018/11/19, 08:56:55",True,"2018/11/19, 09:32:06",nan,1749786.0,463.0,0,485,OpenTracing not activating for Sleuth-Zipkin in Spring Boot App,Finchley.SR2
Zipkin,52377663,nan,1,"2018/09/18, 04:26:02",False,"2018/10/10, 15:12:13",nan,2590960.0,4585.0,0,1124,why services are not shown in zipkin?,My Sampler looks like that:
Zipkin,52377663,nan,1,"2018/09/18, 04:26:02",False,"2018/10/10, 15:12:13",nan,2590960.0,4585.0,0,1124,why services are not shown in zipkin?,In Eureka everything seems well:
Zipkin,52377663,nan,1,"2018/09/18, 04:26:02",False,"2018/10/10, 15:12:13",nan,2590960.0,4585.0,0,1124,why services are not shown in zipkin?,But my problem is that in Zipkin I can't see services at all.
Zipkin,52377663,nan,1,"2018/09/18, 04:26:02",False,"2018/10/10, 15:12:13",nan,2590960.0,4585.0,0,1124,why services are not shown in zipkin?,"I found only debug logs, I have no errors:"
Zipkin,52377663,nan,1,"2018/09/18, 04:26:02",False,"2018/10/10, 15:12:13",nan,2590960.0,4585.0,0,1124,why services are not shown in zipkin?,Question is simple.
Zipkin,52377663,nan,1,"2018/09/18, 04:26:02",False,"2018/10/10, 15:12:13",nan,2590960.0,4585.0,0,1124,why services are not shown in zipkin?,Why can't I see anything in Zipkin?
Zipkin,52377663,nan,1,"2018/09/18, 04:26:02",False,"2018/10/10, 15:12:13",nan,2590960.0,4585.0,0,1124,why services are not shown in zipkin?,This samples are written by  Josh Long.
Zipkin,50295992,nan,1,"2018/05/11, 18:53:27",False,"2019/11/28, 10:07:48","2019/11/28, 10:07:48",9569751.0,13.0,0,669,java spring boot app failed to passing zipkin trace info to rabbitmq,I was working on an Ecommerce microservice-based application.
Zipkin,50295992,nan,1,"2018/05/11, 18:53:27",False,"2019/11/28, 10:07:48","2019/11/28, 10:07:48",9569751.0,13.0,0,669,java spring boot app failed to passing zipkin trace info to rabbitmq,HTTP request structure: frontend -  order -  shipping -  rabbitmq -  queue-master.
Zipkin,50295992,nan,1,"2018/05/11, 18:53:27",False,"2019/11/28, 10:07:48","2019/11/28, 10:07:48",9569751.0,13.0,0,669,java spring boot app failed to passing zipkin trace info to rabbitmq,The shipping container get the trace info from order but it doesn't pass along to rabbitmq.
Zipkin,50295992,nan,1,"2018/05/11, 18:53:27",False,"2019/11/28, 10:07:48","2019/11/28, 10:07:48",9569751.0,13.0,0,669,java spring boot app failed to passing zipkin trace info to rabbitmq,Is there any configuration I should do?
Zipkin,50295992,nan,1,"2018/05/11, 18:53:27",False,"2019/11/28, 10:07:48","2019/11/28, 10:07:48",9569751.0,13.0,0,669,java spring boot app failed to passing zipkin trace info to rabbitmq,"I thought using ""spring-cloud-starter-zipkin"" will help me handle it effortlessly."
Zipkin,50295992,nan,1,"2018/05/11, 18:53:27",False,"2019/11/28, 10:07:48","2019/11/28, 10:07:48",9569751.0,13.0,0,669,java spring boot app failed to passing zipkin trace info to rabbitmq,(tcpdump) shipping get post request with trace info:
Zipkin,50295992,nan,1,"2018/05/11, 18:53:27",False,"2019/11/28, 10:07:48","2019/11/28, 10:07:48",9569751.0,13.0,0,669,java spring boot app failed to passing zipkin trace info to rabbitmq,shipping send out to rabbitmq missing tracing data.
Zipkin,50295992,nan,1,"2018/05/11, 18:53:27",False,"2019/11/28, 10:07:48","2019/11/28, 10:07:48",9569751.0,13.0,0,669,java spring boot app failed to passing zipkin trace info to rabbitmq,"EDITED:
Some code about post shipping request: (shipment is just an object with id and name)"
Zipkin,47992456,nan,1,"2017/12/27, 14:46:08",False,"2018/01/02, 17:22:45",nan,8224996.0,61.0,0,173,how can i deploy a scaleable zipkin deployment?,"I found that the bottleneck of zipkin is collector and API, are these two components stateless so i can deploy multi collectors and multi API?"
Zipkin,47992456,nan,1,"2017/12/27, 14:46:08",False,"2018/01/02, 17:22:45",nan,8224996.0,61.0,0,173,how can i deploy a scaleable zipkin deployment?,I want to deploy zipkin in kubernetes.
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,I have zipkin server running as spring boot app.
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,I have exported jar to docker container.
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,My dockerfile looks like:
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,I have one mysql container.
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,Got this from official docker hub.
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,when I link my zipkin container to mysql using this command :
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,I am getting refused connection exception
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,Although if I run my jar from host OS then it is able to connect mysql container and I can see spans stored in mysql db
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,My application.yml is :
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,How can I connect java docker container to mysql docker container?
Zipkin,47630294,47630471.0,1,"2017/12/04, 11:35:57",True,"2017/12/04, 13:10:19",nan,6275041.0,117.0,0,626,Could not connect zipkin docker container to mysql container,I have already explored this  link.
Zipkin,45012331,45014834.0,1,"2017/07/10, 15:39:25",True,"2018/10/29, 07:37:42","2017/07/10, 16:27:26",7605799.0,207.0,0,1078,Error with Zipkin in unit test (cannot mock Span.baggageItems),I am getting this error when I try to run the unitTest in my spring boot application.
Zipkin,45012331,45014834.0,1,"2017/07/10, 15:39:25",True,"2018/10/29, 07:37:42","2017/07/10, 16:27:26",7605799.0,207.0,0,1078,Error with Zipkin in unit test (cannot mock Span.baggageItems),I notice that I only get this error when I use this version for  spring-cloud-dependencies :
Zipkin,45012331,45014834.0,1,"2017/07/10, 15:39:25",True,"2018/10/29, 07:37:42","2017/07/10, 16:27:26",7605799.0,207.0,0,1078,Error with Zipkin in unit test (cannot mock Span.baggageItems),but if I use this previous version:
Zipkin,45012331,45014834.0,1,"2017/07/10, 15:39:25",True,"2018/10/29, 07:37:42","2017/07/10, 16:27:26",7605799.0,207.0,0,1078,Error with Zipkin in unit test (cannot mock Span.baggageItems),everything works as I expect
Zipkin,45012331,45014834.0,1,"2017/07/10, 15:39:25",True,"2018/10/29, 07:37:42","2017/07/10, 16:27:26",7605799.0,207.0,0,1078,Error with Zipkin in unit test (cannot mock Span.baggageItems),What can I do in order to avoid this error with the last version?
Zipkin,45012331,45014834.0,1,"2017/07/10, 15:39:25",True,"2018/10/29, 07:37:42","2017/07/10, 16:27:26",7605799.0,207.0,0,1078,Error with Zipkin in unit test (cannot mock Span.baggageItems),The unit tests are extended this class in order to mock the SpanAccessor
Zipkin,45012331,45014834.0,1,"2017/07/10, 15:39:25",True,"2018/10/29, 07:37:42","2017/07/10, 16:27:26",7605799.0,207.0,0,1078,Error with Zipkin in unit test (cannot mock Span.baggageItems),}
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"I'm trying to integrate sleuth into a Spring Boot application so that it will talk to a zipkin server for tracing, but I'm not having much luck."
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"I've followed a few tutorials ( link to tutorial ) and have no problems getting them to talk to zipkin, but it's not translating well to my application, and I'm not sure where to look."
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"essentially, in the build.gradle file, to the dependencies section, I added:"
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"In the controller, I added these two beans:"
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"and, I added these to the application.properties file:"
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"When I do all that in the demos, they send traces to my Zipkin host at localhost:4911 just fine (For the time being, I'm just running the quickstart jar file)."
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"When I do all that in my application, I see that I have sleuth log entries with strings like:"
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"so, I know that Sleuth is working."
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"When I run a demo app with the zipkin server application shut off, the application looks like it's working fine, but, reasonably enough, the log files show a big old ConnectionRefused stack trace."
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"When I try the same experiment with my application, I see no stack trace in my application logs, and the application also ran just fine."
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,"Outside of my larger application, I can't reproduce my problem, and I don't know what else to share with you."
Zipkin,44611370,nan,1,"2017/06/18, 06:04:58",False,"2017/08/26, 03:33:26","2017/08/26, 03:33:26",440061.0,35.0,0,1531,Spring Boot Sleuth not sending traces to Zipkin,Anyone have any suggestions about where to start?
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,"TL;DR: 
I want to persist data in ElasticSearch, how i can do this?"
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,I have zipkin and Kafka and ElasticSearch.
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,Kafka as transport for traces.
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,"When i send trace to Kafka, i got it in zipkin UI, it is persistent in memory."
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,I want to persist all traces in ES 5.0 for some time and when zipkin starts or when i search traces i want to search in ES or load trace from ES at start time of zipkin.
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,I start zipkin like this integrated with Kafka:
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,Here is  description of zipkin-storage/elasticsearch-http :
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,This is output:
Zipkin,43394714,43401515.0,1,"2017/04/13, 17:17:33",True,"2017/04/13, 23:52:30",nan,2151387.0,1136.0,0,1103,How to persist zipkin data in ES 5.0?,But in ES in index zipkin there are no data.
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,Springboot app.
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,I'm using Brave v4 and trying not to use the brave-core module for when it is deprecated in the future.
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,With Brave v3 it was easy to pass around the current span as it was kept within the thread and handled by the Brave class.
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,I'm using the async-http-client client and I've created request and response filters which propagate to and from the header as well as starting and submitting spans.
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,This all works as expected.
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,"The async-http-client pool is wired up at startup with the listeners attached, the listeners receive a TracingImpl which is just a wrapper for the Tracer class so that the listeners can do the submitting etc (well the spans inside can start/finish themselves)"
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,"My problem is for example: a request comes into the controller, I extract the Span from the request and now I want to use the async-http-client to make another request which would be a child of the one coming in."
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,I'm unsure how I should get the Span object I now have in my controller to the async-http-client object to it.
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,Any ideas or help would be greatly appreciated.
Zipkin,41823536,nan,1,"2017/01/24, 10:39:57",False,"2017/01/25, 04:43:12",nan,7457627.0,1.0,0,845,Zipkin Brave v4 - Propagating spans from request to,Thanks
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,I am working on implementing distributed tracing for a micro-services application.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,In zipkin UI I could not able to get child URL paths.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,For parent call whole URL is being captured like &quot;post/cart/create&quot;.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,But for child calls only &quot;get&quot; and &quot;post&quot;.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,Not showing entire URL for child calls.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,How to fix this?
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,Need help.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,The data in Zipkin_spans table is as shown above.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,My calling service Code:
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,Dependecies that i am using are
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,This problem seems to be inconsistent.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,Sometimes it shows entire URL and sometimes not.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,Only after testing multiple times I came to know about this.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,I have even tested with spring-cloud- starter -zipkin.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,But it is also not showing any promising result.
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,So if entire URL is necessary then shouldn't I be using zipkin at all?
Zipkin,67157380,nan,0,"2021/04/19, 10:05:40",False,"2021/04/20, 10:18:30","2021/04/20, 10:18:30",7650860.0,79.0,0,15,URL path is not being captured for child calls while using zipkin,suggest any other tracing mechanisms for microservices using spring.
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,Small question on how to pass around the original trace id  traceId  (and not X-B3-TraceId ) please.
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,The entire flow is a call between a client ClientA to three other micro services (4 in total).
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,ClientA (team A) -&gt; Service B1 (team B) -&gt; Service B2 (team B) -&gt; Service C (team C)
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Team A owns the client, the originator of the call, team C owns the last service."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"My group, team B owns two out of the four micro services."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"All subsequent micro services just perform computation and aggregate the response of each other back to the caller, ClientA."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Our team, B, had two micro services, hence we were the first out of the four to use Spring Cloud Sleuth and Zipkin."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Very happy, we could see out of the four services, in our services only, the traceId."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"This helped us in debugging, timing calls, etc, very happy."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Since everyone saw benefits, now ClientA and ServiceC (the two others we do not own) also integrated with Sleuth-Zipkin."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Unfortunately, it seems we are not able to &quot;chain everything&quot;."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,🤯
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"We looked at each others traceId, and saw something like:"
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Hence, we are now all very puzzled."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,As we thought we would see something like:
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Since for the two servicesB1 and B2, we did not do anything in particular, out of the box, we could see same traceId between the two, we thought, out of the box, we would be able to see between the four services."
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"May I ask, is my understanding not correct?"
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,What did I miss please?
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Do I have to do anything special, if yes, what, in order to ask ClientA to send me its traceId?"
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,"Or maybe ClientA is already sending it, I am not getting it properly?"
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,(same applies to ServiceC)
Zipkin,67143853,nan,0,"2021/04/18, 02:20:48",False,"2021/04/19, 17:34:27","2021/04/19, 17:34:27",10461625.0,1094.0,0,14,SpringBoot - SpringCloud Sleuth Zipkin - Tracing - Passing around traceId,Thank you for your help.
Zipkin,66896358,nan,0,"2021/04/01, 01:59:02",False,"2021/04/01, 01:59:02",nan,15526080.0,1.0,0,19,zipkin added to spring boot app cause failure to startup Application,spring-boot 2.3.4-RELEASE
Zipkin,66896358,nan,0,"2021/04/01, 01:59:02",False,"2021/04/01, 01:59:02",nan,15526080.0,1.0,0,19,zipkin added to spring boot app cause failure to startup Application,Error is shown after adding the zipkin dependencies to a project and running with
Zipkin,66896358,nan,0,"2021/04/01, 01:59:02",False,"2021/04/01, 01:59:02",nan,15526080.0,1.0,0,19,zipkin added to spring boot app cause failure to startup Application,Cannot figure out the error causing it.
Zipkin,66896358,nan,0,"2021/04/01, 01:59:02",False,"2021/04/01, 01:59:02",nan,15526080.0,1.0,0,19,zipkin added to spring boot app cause failure to startup Application,Other spring-boot apps in the project are running fine with the same zipkin configs and application.yaml.
Zipkin,66896358,nan,0,"2021/04/01, 01:59:02",False,"2021/04/01, 01:59:02",nan,15526080.0,1.0,0,19,zipkin added to spring boot app cause failure to startup Application,Any help to figure out how to debug this would be helpful.
Zipkin,66896358,nan,0,"2021/04/01, 01:59:02",False,"2021/04/01, 01:59:02",nan,15526080.0,1.0,0,19,zipkin added to spring boot app cause failure to startup Application,"java.lang.IllegalStateException: Error processing condition on org.springframework.cloud.sleuth.zipkin2.ZipkinBackwardsCompatibilityAutoConfiguration$BackwardsCompatibilityConfiguration
at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:60) ~[spring-boot-autoconfigure-2.3.4.RELEASE.jar:2.3.4.RELEASE]"
Zipkin,66896358,nan,0,"2021/04/01, 01:59:02",False,"2021/04/01, 01:59:02",nan,15526080.0,1.0,0,19,zipkin added to spring boot app cause failure to startup Application,pom.xml
Zipkin,66896358,nan,0,"2021/04/01, 01:59:02",False,"2021/04/01, 01:59:02",nan,15526080.0,1.0,0,19,zipkin added to spring boot app cause failure to startup Application,application.yaml
Zipkin,66581369,nan,0,"2021/03/11, 13:10:22",False,"2021/03/11, 13:10:22",nan,15358162.0,1.0,0,11,Zipkin UI showing &quot;ERROR: cannot load service names: server error (OK)&quot;,I am using zipkin tracing in node.js application and I am getting  this  error.
Zipkin,66581369,nan,0,"2021/03/11, 13:10:22",False,"2021/03/11, 13:10:22",nan,15358162.0,1.0,0,11,Zipkin UI showing &quot;ERROR: cannot load service names: server error (OK)&quot;,How to resolve this error?
Zipkin,66581369,nan,0,"2021/03/11, 13:10:22",False,"2021/03/11, 13:10:22",nan,15358162.0,1.0,0,11,Zipkin UI showing &quot;ERROR: cannot load service names: server error (OK)&quot;,Thanks in advance...
Zipkin,66517644,66524575.0,1,"2021/03/07, 16:33:30",True,"2021/03/08, 07:25:55",nan,10461625.0,1094.0,0,47,Java Spring Sleuth Zipkin - X-Span-Export not being displayed,Small question as I am not able to see  X-Span-Export  in my logs please.
Zipkin,66517644,66524575.0,1,"2021/03/07, 16:33:30",True,"2021/03/08, 07:25:55",nan,10461625.0,1094.0,0,47,Java Spring Sleuth Zipkin - X-Span-Export not being displayed,"I have an app, which is Java based, with Springboot 2.4.2, with Sleuth and Zipkin."
Zipkin,66517644,66524575.0,1,"2021/03/07, 16:33:30",True,"2021/03/08, 07:25:55",nan,10461625.0,1094.0,0,47,Java Spring Sleuth Zipkin - X-Span-Export not being displayed,"In my log4j2.xml, I configured such:"
Zipkin,66517644,66524575.0,1,"2021/03/07, 16:33:30",True,"2021/03/08, 07:25:55",nan,10461625.0,1094.0,0,47,Java Spring Sleuth Zipkin - X-Span-Export not being displayed,"However, in my logs, I am only able to see:"
Zipkin,66517644,66524575.0,1,"2021/03/07, 16:33:30",True,"2021/03/08, 07:25:55",nan,10461625.0,1094.0,0,47,Java Spring Sleuth Zipkin - X-Span-Export not being displayed,"I was expecting to see a &quot;true&quot; or &quot;false&quot;, like  [myservice,336a9463f46cf034,18bffe81e7500eae,true]  however, it is not here."
Zipkin,66517644,66524575.0,1,"2021/03/07, 16:33:30",True,"2021/03/08, 07:25:55",nan,10461625.0,1094.0,0,47,Java Spring Sleuth Zipkin - X-Span-Export not being displayed,May I know what is the issue please?
Zipkin,66517644,66524575.0,1,"2021/03/07, 16:33:30",True,"2021/03/08, 07:25:55",nan,10461625.0,1094.0,0,47,Java Spring Sleuth Zipkin - X-Span-Export not being displayed,Thank you
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,I'm running a Spring Boot app using:
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,"I've declared the  spring-cloud-starter-zipkin  and  spring-cloud-starter-openfeign  dependencies, and have configured my app to point to a Zipkin server."
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,Its a pretty vanilla setup and configuration (I also declare the  spring-cloud-starter-netflix-ribbon  and  spring-cloud-starter-kubernetes-all  dependencies o allow Spring Feign to use k8s service discovery).
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,My app declares a  @SpringFeign  annotated interface with a method to call to a remote service  S .
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,So generally zipkin is getting spans from my app (for e.g.
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,incoming REST calls) and B3 headers are being propagated via HTTP to the service  S  being called through feign.
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,"But zipkin does not report a span from my app representing the Feign call to S.
Is that something that should &quot;just happen&quot;, or am I missing a piece of the puzzle?"
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,I can e.g.
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,"add  @NewSpan  to the feign interface method, but that doesn't give me HTTP details for the request/response as span tags."
Zipkin,66479206,nan,1,"2021/03/04, 18:34:12",False,"2021/03/06, 12:32:35","2021/03/06, 12:32:35",2282523.0,117.0,0,85,Sending Zipkin Spans for @FeignClient,And I rather not do that if this is supposed to work out of the box.
Zipkin,66383029,66384663.0,1,"2021/02/26, 11:13:55",True,"2021/03/01, 11:25:08","2021/03/01, 11:25:08",2432030.0,609.0,0,26,Spring Cloud Sleuth: Export data from remote system to local Zipkin,In my Spring Boot application I use spring-cloud-starter-sleuth (Version Hoxton.SR10) for tracing.
Zipkin,66383029,66384663.0,1,"2021/02/26, 11:13:55",True,"2021/03/01, 11:25:08","2021/03/01, 11:25:08",2432030.0,609.0,0,26,Spring Cloud Sleuth: Export data from remote system to local Zipkin,It is (still) a monolithic application so I widely use the  @NewSpan  annotation for creating new spans.
Zipkin,66383029,66384663.0,1,"2021/02/26, 11:13:55",True,"2021/03/01, 11:25:08","2021/03/01, 11:25:08",2432030.0,609.0,0,26,Spring Cloud Sleuth: Export data from remote system to local Zipkin,"In my development environment I also use spring-cloud-starter-zipkin, which works great."
Zipkin,66383029,66384663.0,1,"2021/02/26, 11:13:55",True,"2021/03/01, 11:25:08","2021/03/01, 11:25:08",2432030.0,609.0,0,26,Spring Cloud Sleuth: Export data from remote system to local Zipkin,But on the servers of our customers I don't have access to any Zipkin server nor am allowed to install one.
Zipkin,66383029,66384663.0,1,"2021/02/26, 11:13:55",True,"2021/03/01, 11:25:08","2021/03/01, 11:25:08",2432030.0,609.0,0,26,Spring Cloud Sleuth: Export data from remote system to local Zipkin,Is there any possibility to save the data Spring is sending to Zipkin and import that to my local Zipkin server?
Zipkin,66383029,66384663.0,1,"2021/02/26, 11:13:55",True,"2021/03/01, 11:25:08","2021/03/01, 11:25:08",2432030.0,609.0,0,26,Spring Cloud Sleuth: Export data from remote system to local Zipkin,Solution thanks to Marcin's inspiration:
Zipkin,66383029,66384663.0,1,"2021/02/26, 11:13:55",True,"2021/03/01, 11:25:08","2021/03/01, 11:25:08",2432030.0,609.0,0,26,Spring Cloud Sleuth: Export data from remote system to local Zipkin,"Implement  convertByteArrayToList  and  saveToFile  your own, because my solution depends on custom libraries."
Zipkin,66366382,nan,0,"2021/02/25, 12:03:36",False,"2021/02/25, 13:17:55","2021/02/25, 13:17:55",3801239.0,3013.0,0,9,opencensus view manager is not exporting to zipkin,i am new to  opencensus  and i am trying this  quick-start-example  here i want to export the traces to zipkin for that i added the exporter code and zipkin is running on docker
Zipkin,66366382,nan,0,"2021/02/25, 12:03:36",False,"2021/02/25, 13:17:55","2021/02/25, 13:17:55",3801239.0,3013.0,0,9,opencensus view manager is not exporting to zipkin,traces are not exporting to zipkin here is my example code
Zipkin,66366382,nan,0,"2021/02/25, 12:03:36",False,"2021/02/25, 13:17:55","2021/02/25, 13:17:55",3801239.0,3013.0,0,9,opencensus view manager is not exporting to zipkin,i have tried to search by trace id as well but it did not show up
Zipkin,66327574,nan,0,"2021/02/23, 07:31:14",False,"2021/02/23, 07:31:14",nan,242042.0,22372.0,0,14,How do you limit the number of spans stored in Zipkin server when using Cassandra DB?,"I am using the Cassandra 3 storage type for Zipkin, but I can't find the option for it to clear the older data."
Zipkin,66327574,nan,0,"2021/02/23, 07:31:14",False,"2021/02/23, 07:31:14",nan,242042.0,22372.0,0,14,How do you limit the number of spans stored in Zipkin server when using Cassandra DB?,I am looking for something like  MEM_MAX_SPANS=1000000  as specified in the  README.md .
Zipkin,66327574,nan,0,"2021/02/23, 07:31:14",False,"2021/02/23, 07:31:14",nan,242042.0,22372.0,0,14,How do you limit the number of spans stored in Zipkin server when using Cassandra DB?,"In ElasticSearch I can set up a rule to clean up older records, maybe there is something like that in Cassandra?"
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"Question regarding Spring and Zipkin/Open tracing, in particularly, traces where an app in the middle does not have Zipkin/Open Tracing please."
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,Setup:
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"Step1: A client, front end app, without Zipkin is making a call to my-micro-service-A-with-zipkin."
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,Step2: my-micro-service-A-with-zipkin will then call my-micro-service-B-with-zipkin
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,Step3: my-micro-service-B-with-zipkin will then call  third-party-service-C-without-zipkin
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,Step4:  third-party-service-C-without-zipkin  will then call my-micro-service-D-with-zipkin
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"Step5: After all those calls, my-micro-service-A-with-zipkin will return the response to the client."
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"A bit surprised, when searching for the trace, I could only find the call my-micro-service-A-with-zipkin -&gt; my-micro-service-B-with-zipkin, and  none of the other  calls."
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"Basically, just one step our of the 5."
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"Question:
Is this expected please?"
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"I understand third-party-service-C-without-zipkin does not have Zipkin, but the for me, this is kinda defeating the purpose."
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"In this example scenario, I will never be able to see further traces other than A to B (Step2) ?"
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"Also, is there a way to remediate to this?"
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,"The third party is a third party API, out of our control, we cannot just go and say &quot;please install Zipkin/open tracing, it will be helpful to everyone&quot;."
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,Really wondering how to achieve the correct traces in this described scenario.
Zipkin,66244791,nan,0,"2021/02/17, 17:15:09",False,"2021/04/19, 17:23:14","2021/04/19, 17:23:14",10461625.0,1094.0,0,31,Lost trace from app if no dependency to Zipkin?,Thank you
Zipkin,66164971,nan,0,"2021/02/12, 02:34:47",False,"2021/04/19, 17:22:12","2021/04/19, 17:22:12",10461625.0,1094.0,0,46,Spring Webflux 2.4.2 + Spring Cloud Ilford + Zipkin: Not able to get Zipkin metrics,"Question regarding Spring Webflux 2.4.2+ project, with Spring Cloud Ilford 2020.0.0 please."
Zipkin,66164971,nan,0,"2021/02/12, 02:34:47",False,"2021/04/19, 17:22:12","2021/04/19, 17:22:12",10461625.0,1094.0,0,46,Spring Webflux 2.4.2 + Spring Cloud Ilford + Zipkin: Not able to get Zipkin metrics,"In my small Spring Webflux app, I am currently using"
Zipkin,66164971,nan,0,"2021/02/12, 02:34:47",False,"2021/04/19, 17:22:12","2021/04/19, 17:22:12",10461625.0,1094.0,0,46,Spring Webflux 2.4.2 + Spring Cloud Ilford + Zipkin: Not able to get Zipkin metrics,"I see the traces fine in m Zipkin server, very happy."
Zipkin,66164971,nan,0,"2021/02/12, 02:34:47",False,"2021/04/19, 17:22:12","2021/04/19, 17:22:12",10461625.0,1094.0,0,46,Spring Webflux 2.4.2 + Spring Cloud Ilford + Zipkin: Not able to get Zipkin metrics,"But I was also expecting to be able to get some metrics, such as"
Zipkin,66164971,nan,0,"2021/02/12, 02:34:47",False,"2021/04/19, 17:22:12","2021/04/19, 17:22:12",10461625.0,1094.0,0,46,Spring Webflux 2.4.2 + Spring Cloud Ilford + Zipkin: Not able to get Zipkin metrics,"However, while I am seeing the traces in Zipkin server, and &quot;some&quot; zipkin metrics, such as  zipkin_reporter_spans_total  I am not getting Zipkin related metrics at all."
Zipkin,66164971,nan,0,"2021/02/12, 02:34:47",False,"2021/04/19, 17:22:12","2021/04/19, 17:22:12",10461625.0,1094.0,0,46,Spring Webflux 2.4.2 + Spring Cloud Ilford + Zipkin: Not able to get Zipkin metrics,May I ask how to get above mentioned metrics please?
Zipkin,66164971,nan,0,"2021/02/12, 02:34:47",False,"2021/04/19, 17:22:12","2021/04/19, 17:22:12",10461625.0,1094.0,0,46,Spring Webflux 2.4.2 + Spring Cloud Ilford + Zipkin: Not able to get Zipkin metrics,Thank you
Zipkin,66049448,nan,0,"2021/02/04, 18:10:48",False,"2021/02/04, 18:10:48",nan,12858138.0,75.0,0,12,How to get dependencies json file from zipkin?,Can I query a json file with the dependencies suing zipkin API?
Zipkin,66049448,nan,0,"2021/02/04, 18:10:48",False,"2021/02/04, 18:10:48",nan,12858138.0,75.0,0,12,How to get dependencies json file from zipkin?,like with traces:
Zipkin,65990254,nan,0,"2021/02/01, 11:34:20",False,"2021/02/01, 11:34:20",nan,3898024.0,55.0,0,8,Support of tags in zipkin vert.x integration,Seems like tags aren't working with vert.x zipkin integration.
Zipkin,65990254,nan,0,"2021/02/01, 11:34:20",False,"2021/02/01, 11:34:20",nan,3898024.0,55.0,0,8,Support of tags in zipkin vert.x integration,Below is the sample code snippet
Zipkin,65990254,nan,0,"2021/02/01, 11:34:20",False,"2021/02/01, 11:34:20",nan,3898024.0,55.0,0,8,Support of tags in zipkin vert.x integration,We use vert.x application as api gateway which takes requests and forward the requests to other services.
Zipkin,65990254,nan,0,"2021/02/01, 11:34:20",False,"2021/02/01, 11:34:20",nan,3898024.0,55.0,0,8,Support of tags in zipkin vert.x integration,"On the server side, Tracing.currentTracer().currentSpan() always gives a null object."
Zipkin,65990254,nan,0,"2021/02/01, 11:34:20",False,"2021/02/01, 11:34:20",nan,3898024.0,55.0,0,8,Support of tags in zipkin vert.x integration,Why is the span going out of context here before the request is forwarded down to other service?
Zipkin,65990254,nan,0,"2021/02/01, 11:34:20",False,"2021/02/01, 11:34:20",nan,3898024.0,55.0,0,8,Support of tags in zipkin vert.x integration,Is there any bug in the vert.z zipkin code?
Zipkin,65386973,nan,0,"2020/12/21, 04:48:20",False,"2021/01/08, 04:20:52","2021/01/08, 04:20:52",3898024.0,55.0,0,34,Does vert.x zipkin integration provide support for sampling?,Use Case :
Zipkin,65386973,nan,0,"2020/12/21, 04:48:20",False,"2021/01/08, 04:20:52","2021/01/08, 04:20:52",3898024.0,55.0,0,34,Does vert.x zipkin integration provide support for sampling?,Using vert.x application as an API gateway for all incoming traffic.
Zipkin,65386973,nan,0,"2020/12/21, 04:48:20",False,"2021/01/08, 04:20:52","2021/01/08, 04:20:52",3898024.0,55.0,0,34,Does vert.x zipkin integration provide support for sampling?,Want to sample incoming requests through vert.x
Zipkin,65386973,nan,0,"2020/12/21, 04:48:20",False,"2021/01/08, 04:20:52","2021/01/08, 04:20:52",3898024.0,55.0,0,34,Does vert.x zipkin integration provide support for sampling?,"Sampling option inside ZipkinTracingOptions is hard coded to Sampler.ALWAYS_SAMPLE
I feel, we should provide the application an ability to supply Sampler from outside so that all sampling options supported by Brave can be used in ZipkinTracingOptions"
Zipkin,65386973,nan,0,"2020/12/21, 04:48:20",False,"2021/01/08, 04:20:52","2021/01/08, 04:20:52",3898024.0,55.0,0,34,Does vert.x zipkin integration provide support for sampling?,Does vert.x supports any other Sampling Option or does it intend to do that in future?
Zipkin,65228873,65231143.0,1,"2020/12/10, 07:16:02",True,"2020/12/10, 10:43:15",nan,3898024.0,55.0,0,28,Zipkin integration with vert.x with 3.x version,I see active work going on for vert.x integration with zipkin.
Zipkin,65228873,65231143.0,1,"2020/12/10, 07:16:02",True,"2020/12/10, 10:43:15",nan,3898024.0,55.0,0,28,Zipkin integration with vert.x with 3.x version,https://github.com/eclipse-vertx/vertx-tracing
Zipkin,65228873,65231143.0,1,"2020/12/10, 07:16:02",True,"2020/12/10, 10:43:15",nan,3898024.0,55.0,0,28,Zipkin integration with vert.x with 3.x version,Will it be backward compatible with vert.x 3.8.x version?
Zipkin,65228873,65231143.0,1,"2020/12/10, 07:16:02",True,"2020/12/10, 10:43:15",nan,3898024.0,55.0,0,28,Zipkin integration with vert.x with 3.x version,"If yes, can you share tentative timelines for the same?"
Zipkin,65188721,nan,0,"2020/12/07, 22:15:10",False,"2020/12/07, 22:15:10",nan,4312720.0,180.0,0,105,Zipkin UI with user password authentication,I have set up a docker image of  Zipkin  to enable distributed tracing in my organization.
Zipkin,65188721,nan,0,"2020/12/07, 22:15:10",False,"2020/12/07, 22:15:10",nan,4312720.0,180.0,0,105,Zipkin UI with user password authentication,"The information needs to be available for specific users, Can I add some user authentication in front of the UI, without protecting the current flow of span information?"
Zipkin,65169398,nan,1,"2020/12/06, 16:53:03",False,"2020/12/06, 20:34:26",nan,10867131.0,21.0,0,39,Zipkin and Sleuth integration for non spring boot projects,"Can anyone tell me, is there any way to  integrate Zipkin (if possible Sleuth also) for non-spring boot projects ?"
Zipkin,65169398,nan,1,"2020/12/06, 16:53:03",False,"2020/12/06, 20:34:26",nan,10867131.0,21.0,0,39,Zipkin and Sleuth integration for non spring boot projects,I am trying to integrate Zipkin for my traditional spring application.
Zipkin,65169398,nan,1,"2020/12/06, 16:53:03",False,"2020/12/06, 20:34:26",nan,10867131.0,21.0,0,39,Zipkin and Sleuth integration for non spring boot projects,It is not a microservice.
Zipkin,65169398,nan,1,"2020/12/06, 16:53:03",False,"2020/12/06, 20:34:26",nan,10867131.0,21.0,0,39,Zipkin and Sleuth integration for non spring boot projects,How could I do this?
Zipkin,65169398,nan,1,"2020/12/06, 16:53:03",False,"2020/12/06, 20:34:26",nan,10867131.0,21.0,0,39,Zipkin and Sleuth integration for non spring boot projects,"Any suggestions, please?"
Zipkin,65169398,nan,1,"2020/12/06, 16:53:03",False,"2020/12/06, 20:34:26",nan,10867131.0,21.0,0,39,Zipkin and Sleuth integration for non spring boot projects,Thanks.
Zipkin,65064734,65347921.0,1,"2020/11/29, 22:18:20",True,"2020/12/17, 22:30:30","2020/11/29, 22:50:20",1351859.0,81.0,0,88,Spring Sleuth Zipkin Extra Field Propagation,I am new to distributed logging and I need help around the propagation of extra fields across Http Request and Messaging Request.
Zipkin,65064734,65347921.0,1,"2020/11/29, 22:18:20",True,"2020/12/17, 22:30:30","2020/11/29, 22:50:20",1351859.0,81.0,0,88,Spring Sleuth Zipkin Extra Field Propagation,"Currently, I am able to propagate the traceId and spanId, but I need to pass  correlationId  to be propagated across all the microservices."
Zipkin,65064734,65347921.0,1,"2020/11/29, 22:18:20",True,"2020/12/17, 22:30:30","2020/11/29, 22:50:20",1351859.0,81.0,0,88,Spring Sleuth Zipkin Extra Field Propagation,logback.xml
Zipkin,65064734,65347921.0,1,"2020/11/29, 22:18:20",True,"2020/12/17, 22:30:30","2020/11/29, 22:50:20",1351859.0,81.0,0,88,Spring Sleuth Zipkin Extra Field Propagation,I am a bit curious about how to pass the correlation id to other services.
Zipkin,64560533,nan,0,"2020/10/27, 20:13:55",False,"2020/10/27, 20:13:55",nan,13008503.0,11.0,0,75,Exclude url pattern from spring sleuth tracing and Zipkin,I have integrated spring cloud sleuth (2.2.5.RELEASE) and Zipkin in my spring boot application.
Zipkin,64560533,nan,0,"2020/10/27, 20:13:55",False,"2020/10/27, 20:13:55",nan,13008503.0,11.0,0,75,Exclude url pattern from spring sleuth tracing and Zipkin,I am trying to exclude certain url pattern from being traced and show up in Zipkin.
Zipkin,64560533,nan,0,"2020/10/27, 20:13:55",False,"2020/10/27, 20:13:55",nan,13008503.0,11.0,0,75,Exclude url pattern from spring sleuth tracing and Zipkin,I have this in my spring boot application.properties file
Zipkin,64560533,nan,0,"2020/10/27, 20:13:55",False,"2020/10/27, 20:13:55",nan,13008503.0,11.0,0,75,Exclude url pattern from spring sleuth tracing and Zipkin,But still I see that traceId is being generated for url /api/v1/hello and I also can visualize this request in Zipkin.
Zipkin,64560533,nan,0,"2020/10/27, 20:13:55",False,"2020/10/27, 20:13:55",nan,13008503.0,11.0,0,75,Exclude url pattern from spring sleuth tracing and Zipkin,I have also tried spring.sleuth.web.additional-skip-pattern to achieve this but that also doesn't help.
Zipkin,64560533,nan,0,"2020/10/27, 20:13:55",False,"2020/10/27, 20:13:55",nan,13008503.0,11.0,0,75,Exclude url pattern from spring sleuth tracing and Zipkin,How  can I achieve this?
Zipkin,64560533,nan,0,"2020/10/27, 20:13:55",False,"2020/10/27, 20:13:55",nan,13008503.0,11.0,0,75,Exclude url pattern from spring sleuth tracing and Zipkin,Thanks in advance.
Zipkin,64246361,nan,0,"2020/10/07, 17:35:27",False,"2020/10/07, 17:35:27",nan,972830.0,1546.0,0,20,How to configure two separate sets of kafka brokers along with zipkin via spring cloud stream?,I have to configure two separate Kafka brokers (as on example below) and this is working just fine - I have my writes and reads on different Kafkas.
Zipkin,64246361,nan,0,"2020/10/07, 17:35:27",False,"2020/10/07, 17:35:27",nan,972830.0,1546.0,0,20,How to configure two separate sets of kafka brokers along with zipkin via spring cloud stream?,But also I need to configure zipkin+sleuth in application - and only way I can do this is via adding:
Zipkin,64246361,nan,0,"2020/10/07, 17:35:27",False,"2020/10/07, 17:35:27",nan,972830.0,1546.0,0,20,How to configure two separate sets of kafka brokers along with zipkin via spring cloud stream?,"As soon, as  spring.kafka.bootstrap-servers  added, it started to override  kafka2.environment.spring.cloud.stream.kafka.binder.brokers  - so application just trying to write to kafka on dev1-stage.dub, instead of dev2-stage.dub."
Zipkin,64246361,nan,0,"2020/10/07, 17:35:27",False,"2020/10/07, 17:35:27",nan,972830.0,1546.0,0,20,How to configure two separate sets of kafka brokers along with zipkin via spring cloud stream?,How can I prevent this overriding?
Zipkin,64246361,nan,0,"2020/10/07, 17:35:27",False,"2020/10/07, 17:35:27",nan,972830.0,1546.0,0,20,How to configure two separate sets of kafka brokers along with zipkin via spring cloud stream?,Or how I should rework configuration to support both sets of kafka brokers along with zipkin?
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,I am trying to deploy zipkin within k8s.
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,I am using elasticsearch (version 6.8.8) as storage.
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,The deployment works fine and the server starts.
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,"However, I only can access the server with a port-forward."
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,$ kubectl -n ns-zipkin port-forward zipkin-bdcf7f78b-shd9p 8888:9411
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,After that I can access  http://localhost:8888/zipkin/
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,What could be the reason?
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,Already the deployment of the service does not get an endpoint (see  in output below) which I would expect.
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,deployment.yaml
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,$ kubectl -n ns-zipkin describe deployment zipkin
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,service.yaml
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,$ kubectl -n ns-zipkin describe service zipkin
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,ingress.yaml
Zipkin,63737361,63778418.0,2,"2020/09/04, 11:16:08",True,"2020/09/07, 18:08:33",nan,10196632.0,351.0,0,159,Zipkin with elasticsearch on k8s no access,$ kubectl -n ns-zipkin describe ingress
Zipkin,63394496,nan,0,"2020/08/13, 14:47:43",False,"2020/08/13, 15:10:17","2020/08/13, 15:10:17",1423685.0,1301.0,0,66,Zipkin server configuration and statistics,"I'm using openzipkin/zipkin:2.21.5 run on okd openshift with some basic default conf (possibly docker run -d -p 9411:9411 openzipkin/zipkin - but I'm not sure, it was installed by admin)."
Zipkin,63394496,nan,0,"2020/08/13, 14:47:43",False,"2020/08/13, 15:10:17","2020/08/13, 15:10:17",1423685.0,1301.0,0,66,Zipkin server configuration and statistics,After about one minute I can't go into details of trace from search page to which I could go before moment.
Zipkin,63394496,nan,0,"2020/08/13, 14:47:43",False,"2020/08/13, 15:10:17","2020/08/13, 15:10:17",1423685.0,1301.0,0,66,Zipkin server configuration and statistics,Maybe Zipkin server is running out of memory and deletes old spans - are there any statistics about it?
Zipkin,63394496,nan,0,"2020/08/13, 14:47:43",False,"2020/08/13, 15:10:17","2020/08/13, 15:10:17",1423685.0,1301.0,0,66,Zipkin server configuration and statistics,"Maybe it has some default number of detained traces in some circular buffer, if so, how can I increase it?"
Zipkin,63394496,nan,0,"2020/08/13, 14:47:43",False,"2020/08/13, 15:10:17","2020/08/13, 15:10:17",1423685.0,1301.0,0,66,Zipkin server configuration and statistics,How can it be checked and configured?
Zipkin,63394496,nan,0,"2020/08/13, 14:47:43",False,"2020/08/13, 15:10:17","2020/08/13, 15:10:17",1423685.0,1301.0,0,66,Zipkin server configuration and statistics,Where can I find any documentation about Zipkin server configuration?
Zipkin,63394496,nan,0,"2020/08/13, 14:47:43",False,"2020/08/13, 15:10:17","2020/08/13, 15:10:17",1423685.0,1301.0,0,66,Zipkin server configuration and statistics,"https://hub.docker.com/r/openzipkin/zipkin  states mysql is deprecated,  elasticsearch  link is broken, in-memory is not for prod, where can I find a doc for production configuration?"
Zipkin,63384688,63384808.0,1,"2020/08/13, 00:03:13",True,"2020/08/13, 00:15:25",nan,1423685.0,1301.0,0,101,How to force Zipkin/Brave/Spring-Cloud-Sleuth span to be exportable?,How can I force a Zipkin span to be exportable?
Zipkin,63384688,63384808.0,1,"2020/08/13, 00:03:13",True,"2020/08/13, 00:15:25",nan,1423685.0,1301.0,0,101,How to force Zipkin/Brave/Spring-Cloud-Sleuth span to be exportable?,"In below code spans are sometimes exportable, sometimes not in a non repeatable manner."
Zipkin,63384688,63384808.0,1,"2020/08/13, 00:03:13",True,"2020/08/13, 00:15:25",nan,1423685.0,1301.0,0,101,How to force Zipkin/Brave/Spring-Cloud-Sleuth span to be exportable?,"It seems to me that if I comment first scopedSpan, than second manually created spanInScope is exportable, but how can first scopedSpan prevent second spanInScope from being exportable?"
Zipkin,63384688,63384808.0,1,"2020/08/13, 00:03:13",True,"2020/08/13, 00:15:25",nan,1423685.0,1301.0,0,101,How to force Zipkin/Brave/Spring-Cloud-Sleuth span to be exportable?,How do they interfere?
Zipkin,63111483,nan,0,"2020/07/27, 11:31:10",False,"2020/07/27, 11:31:10",nan,12964338.0,376.0,0,47,How to show custom log messages in zipkin server,We have  spring-cloud-starter-sleuth  and  spring-cloud-starter-zipkin  dependencies in in other microservices and we are able to see the traces on the  zipkin  dashboard.
Zipkin,63111483,nan,0,"2020/07/27, 11:31:10",False,"2020/07/27, 11:31:10",nan,12964338.0,376.0,0,47,How to show custom log messages in zipkin server,We also want to show the custom messages on zipkin i.e whatever log messages we are logging in our controller methods.
Zipkin,63111483,nan,0,"2020/07/27, 11:31:10",False,"2020/07/27, 11:31:10",nan,12964338.0,376.0,0,47,How to show custom log messages in zipkin server,we want to show additional log data on zipkin.
Zipkin,63111483,nan,0,"2020/07/27, 11:31:10",False,"2020/07/27, 11:31:10",nan,12964338.0,376.0,0,47,How to show custom log messages in zipkin server,Is there any way to show these  logger.info  messages on the zipkin dashboard?
Zipkin,63111483,nan,0,"2020/07/27, 11:31:10",False,"2020/07/27, 11:31:10",nan,12964338.0,376.0,0,47,How to show custom log messages in zipkin server,"Also, is there any way to customize the format of messages being sent to zipkin?"
Zipkin,63111483,nan,0,"2020/07/27, 11:31:10",False,"2020/07/27, 11:31:10",nan,12964338.0,376.0,0,47,How to show custom log messages in zipkin server,"we want to show message in something like  %d{yyyy-MMM-dd HH:mm:ss.SSS} %5p [%X{X-B3-TraceId:-},%X{X-B3-SpanId:-}] [%thread] %logger{15} - %replace(%msg %ex){'[\\r\\n]+', '\\n'}%nopex%n  format."
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,"I tried in Windows 10 machine to coonect RabbitMQ (3.6.11 version installed with Erlang 20) to ZipKin, but I got the following error:"
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,"org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Unsatisfied dependency expressed through method 'armeriaServer' parameter 4; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'serverConfigurator' defined in zipkin2.server.internal.ZipkinHttpConfiguration: Unsatisfied dependency expressed through method 'serverConfigurator' parameter 2; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:797) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:538) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:893) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:879) ~[spring-context-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:551) ~[spring-context-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758) ~[spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:140) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at zipkin.server.ZipkinServer.main(ZipkinServer.java:54) [classes!/:?]"
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,":1.8.0_251]
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?"
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,":1.8.0_251]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?"
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,":1.8.0_251]
at java.lang.reflect.Method.invoke(Unknown Source) ~[?"
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,":1.8.0_251]
at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49) [zipkin-server-2.21.5-exec.jar:?]"
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,at org.springframework.boot.loader.Launcher.launch(Launcher.java:109) [zipkin-server-2.21.5-exec.jar:?]
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,at org.springframework.boot.loader.Launcher.launch(Launcher.java:58) [zipkin-server-2.21.5-exec.jar:?]
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:88) [zipkin-server-2.21.5-exec.jar:?]
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'serverConfigurator' defined in zipkin2.server.internal.ZipkinHttpConfiguration: Unsatisfied dependency expressed through method 'serverConfigurator' parameter 2; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,Caused by: java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
Zipkin,62894286,nan,1,"2020/07/14, 14:37:01",True,"2020/08/02, 20:46:19",nan,604019.0,544.0,0,319,zipkin Unable to establish connection to RabbitMQ server,Caused by: java.net.ConnectException: Connection refused: connect
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,I’m trying to implement an async one way span in zipkin.
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,Effectively 2 different processes share a span and need to capture between them.
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,"Per the documentation , I am using the ‘cs’ and ‘sr’ annotations on the span, however don’t see the duration being captured accurately."
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,"Brave version: io.zipkin.brave:brave:5.12.3
Zipkin server version:2.21.4
Using it as an in memory storage currently."
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,"This is what I’m currently doing:
Client Side:
Tracer.tospan(spancontext)."
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,Annotate(cs).start().flush()
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,"Server Side:
Tracer.joinSpan(spancontext)."
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,Annotate(sr).start().flush()
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,The spans Json shows 2 spans of kind Client and server.
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,Their time stamps are accurate and they have no duration.
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,In the trace graph I expect to see the span having duration as the difference between the timestamp of the client and server spans.
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,However I don’t.
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,I do see the expected  duration under the ‘server start’ annotation on the right pane as the ‘relative time’.
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,Is there anyway to see this as a duration on the span itself?
Zipkin,62790770,nan,0,"2020/07/08, 11:38:05",False,"2020/07/08, 17:21:41","2020/07/08, 17:21:41",13890227.0,1.0,0,83,Zipkin merging async one way span,Is this expected behaviour or am I missing anything?
Zipkin,62423737,nan,0,"2020/06/17, 10:33:44",False,"2020/06/17, 10:33:44",nan,2606015.0,351.0,0,26,Integration of zipkin in cxf application having Spring XML-Configuration,"Trying to integrate zipkin and brave in my cxf application, application is having spring xml configuration."
Zipkin,62423737,nan,0,"2020/06/17, 10:33:44",False,"2020/06/17, 10:33:44",nan,2606015.0,351.0,0,26,Integration of zipkin in cxf application having Spring XML-Configuration,reference page  https://cxf.apache.org/docs/using-openzipkin-brave.html  is quite outdated and does not include new implementation of different libraries.
Zipkin,62423737,nan,0,"2020/06/17, 10:33:44",False,"2020/06/17, 10:33:44",nan,2606015.0,351.0,0,26,Integration of zipkin in cxf application having Spring XML-Configuration,Would appreciate any doc or reference for integrating same.
Zipkin,62423737,nan,0,"2020/06/17, 10:33:44",False,"2020/06/17, 10:33:44",nan,2606015.0,351.0,0,26,Integration of zipkin in cxf application having Spring XML-Configuration,cxf-version
Zipkin,61710421,61710527.0,1,"2020/05/10, 13:26:04",True,"2020/05/10, 13:32:48",nan,nan,nan,0,135,"Zipkin &amp; Sleuth, Tracer missing the createSpan method",I am following the tutorial for creating tracing application  zipkin  and  sleuth  but I am having some trouble.
Zipkin,61710421,61710527.0,1,"2020/05/10, 13:26:04",True,"2020/05/10, 13:32:48",nan,nan,nan,0,135,"Zipkin &amp; Sleuth, Tracer missing the createSpan method",I cannot create a span.
Zipkin,61710421,61710527.0,1,"2020/05/10, 13:26:04",True,"2020/05/10, 13:32:48",nan,nan,nan,0,135,"Zipkin &amp; Sleuth, Tracer missing the createSpan method",The problem is that the method does not exist.
Zipkin,61710421,61710527.0,1,"2020/05/10, 13:26:04",True,"2020/05/10, 13:32:48",nan,nan,nan,0,135,"Zipkin &amp; Sleuth, Tracer missing the createSpan method",Also I cannot find the import for the tracer.
Zipkin,61710421,61710527.0,1,"2020/05/10, 13:26:04",True,"2020/05/10, 13:32:48",nan,nan,nan,0,135,"Zipkin &amp; Sleuth, Tracer missing the createSpan method",This is what I am trying to do:
Zipkin,61710421,61710527.0,1,"2020/05/10, 13:26:04",True,"2020/05/10, 13:32:48",nan,nan,nan,0,135,"Zipkin &amp; Sleuth, Tracer missing the createSpan method",Why is the implementation above not working?
Zipkin,61673019,nan,0,"2020/05/08, 09:05:58",False,"2020/05/08, 09:05:58",nan,12013057.0,1.0,0,23,How to integrate zipkin sleuth with elk,I want to integrate spring sleuth zipkin server with elastic server.
Zipkin,61673019,nan,0,"2020/05/08, 09:05:58",False,"2020/05/08, 09:05:58",nan,12013057.0,1.0,0,23,How to integrate zipkin sleuth with elk,Can anyone please help how to integrate?
Zipkin,61198731,nan,1,"2020/04/14, 03:05:39",False,"2020/04/21, 00:41:04",nan,242042.0,22372.0,0,53,Automatically remove older zipkin entries in elasticsearch,This is specifically for Zipkin's Elastic Search storage connector.
Zipkin,61198731,nan,1,"2020/04/14, 03:05:39",False,"2020/04/21, 00:41:04",nan,242042.0,22372.0,0,53,Automatically remove older zipkin entries in elasticsearch,Which does not do the index that you can use Curator.
Zipkin,61198731,nan,1,"2020/04/14, 03:05:39",False,"2020/04/21, 00:41:04",nan,242042.0,22372.0,0,53,Automatically remove older zipkin entries in elasticsearch,Is there a way of  automatically  removing old traces and have that as part of the ElasticSearch configuration (rather than building yet another service or cron job)  Since I am using it for a development server I just need it wiped every hour or so.
Zipkin,60598519,nan,1,"2020/03/09, 12:20:54",False,"2020/07/16, 00:09:39",nan,10041463.0,3.0,0,460,Unable to run zipkin jar file on Linux server &quot;Armeria server failed to start&quot;,I am new to zipkin server.
Zipkin,60598519,nan,1,"2020/03/09, 12:20:54",False,"2020/07/16, 00:09:39",nan,10041463.0,3.0,0,460,Unable to run zipkin jar file on Linux server &quot;Armeria server failed to start&quot;,I am trying to run a zipkin-server-2.12.9-exec on linux server facing the below exception.
Zipkin,60598519,nan,1,"2020/03/09, 12:20:54",False,"2020/07/16, 00:09:39",nan,10041463.0,3.0,0,460,Unable to run zipkin jar file on Linux server &quot;Armeria server failed to start&quot;,"2020-03-09 15:36:28.796  WARN 1685 --- [           main] s.c.a.AnnotationConfigApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.linecorp.armeria.server.Server]: Factory method 'armeriaServer' threw exception; nested exception is java.util.concurrent.CompletionException: java.lang.IllegalStateException: Armeria server failed to start
2020-03-09 15:36:28.805  INFO 1685 --- [           main] ConditionEvaluationReportLoggingListener :
Error starting ApplicationContext."
Zipkin,60598519,nan,1,"2020/03/09, 12:20:54",False,"2020/07/16, 00:09:39",nan,10041463.0,3.0,0,460,Unable to run zipkin jar file on Linux server &quot;Armeria server failed to start&quot;,To display the conditions report re-run your application with 'debug' enabled.
Zipkin,60598519,nan,1,"2020/03/09, 12:20:54",False,"2020/07/16, 00:09:39",nan,10041463.0,3.0,0,460,Unable to run zipkin jar file on Linux server &quot;Armeria server failed to start&quot;,"2020-03-09 15:36:28.806 ERROR 1685 --- [           main] o.s.b.SpringApplication                  : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.linecorp.armeria.server.Server]: Factory method 'armeriaServer' threw exception; nested exception is java.util.concurrent.CompletionException: java.lang.IllegalStateException: Armeria server failed to start"
Zipkin,60170816,nan,1,"2020/02/11, 16:12:31",False,"2020/02/13, 19:56:57",nan,12146243.0,61.0,0,309,How do I specify an external zipkin collector in istio?,I want to use stackdriver trace as a back end for distributed tracing in istio.
Zipkin,60170816,nan,1,"2020/02/11, 16:12:31",False,"2020/02/13, 19:56:57",nan,12146243.0,61.0,0,309,How do I specify an external zipkin collector in istio?,"I installed Docker on the VM of GCP, and run the image of zipkin-gcp."
Zipkin,60170816,nan,1,"2020/02/11, 16:12:31",False,"2020/02/13, 19:56:57",nan,12146243.0,61.0,0,309,How do I specify an external zipkin collector in istio?,"Then, according to the official documentation, I configured istio to send spans to this VM."
Zipkin,60170816,nan,1,"2020/02/11, 16:12:31",False,"2020/02/13, 19:56:57",nan,12146243.0,61.0,0,309,How do I specify an external zipkin collector in istio?,"However, no trace was displayed in the stackdriver trace."
Zipkin,60170816,nan,1,"2020/02/11, 16:12:31",False,"2020/02/13, 19:56:57",nan,12146243.0,61.0,0,309,How do I specify an external zipkin collector in istio?,"To isolate the problem, I stopped zipkin-gcp and checked if packets were being sent with tcpdump."
Zipkin,60170816,nan,1,"2020/02/11, 16:12:31",False,"2020/02/13, 19:56:57",nan,12146243.0,61.0,0,309,How do I specify an external zipkin collector in istio?,"As a result, it was found that nothing was sent."
Zipkin,60170816,nan,1,"2020/02/11, 16:12:31",False,"2020/02/13, 19:56:57",nan,12146243.0,61.0,0,309,How do I specify an external zipkin collector in istio?,I have confirmed port 9411 connectivity from the kubernetes cluster to the VM.
Zipkin,60170816,nan,1,"2020/02/11, 16:12:31",False,"2020/02/13, 19:56:57",nan,12146243.0,61.0,0,309,How do I specify an external zipkin collector in istio?,How do I send a trace to an external zipkin server with istio?
Zipkin,59859303,59877579.0,1,"2020/01/22, 13:59:19",True,"2020/01/23, 13:31:55",nan,5564578.0,5244.0,0,562,Configure tracing backend to be Zipkin in Istio,"I am trying to configure Istio control Plane to use zipkin as tracing backend, but I can't."
Zipkin,59859303,59877579.0,1,"2020/01/22, 13:59:19",True,"2020/01/23, 13:31:55",nan,5564578.0,5244.0,0,562,Configure tracing backend to be Zipkin in Istio,"In their docs, they state that in order to do this, I just have to pass the following parameters when installing Istio:"
Zipkin,59859303,59877579.0,1,"2020/01/22, 13:59:19",True,"2020/01/23, 13:31:55",nan,5564578.0,5244.0,0,562,Configure tracing backend to be Zipkin in Istio,--set values.tracing.enabled=true  and  --set values.tracing.provider=zipkin .
Zipkin,59859303,59877579.0,1,"2020/01/22, 13:59:19",True,"2020/01/23, 13:31:55",nan,5564578.0,5244.0,0,562,Configure tracing backend to be Zipkin in Istio,My problem is that I have installed Istio manually.
Zipkin,59859303,59877579.0,1,"2020/01/22, 13:59:19",True,"2020/01/23, 13:31:55",nan,5564578.0,5244.0,0,562,Configure tracing backend to be Zipkin in Istio,"I found the parameter  provider: jaeger  in the  Configmap   istio-sidecar-injector , and made the change, then killed the control plane so it would be re-deployed with zipkin, but didn't work."
Zipkin,59859303,59877579.0,1,"2020/01/22, 13:59:19",True,"2020/01/23, 13:31:55",nan,5564578.0,5244.0,0,562,Configure tracing backend to be Zipkin in Istio,Does anyone know what object/s should I manipulate to get zipkin?
Zipkin,59679685,nan,0,"2020/01/10, 12:17:23",False,"2020/01/10, 15:58:40","2020/01/10, 15:58:40",10633756.0,17.0,0,52,Issue in passing data in zipkin via kafka - Could not convert message,pom.xml
Zipkin,59679685,nan,0,"2020/01/10, 12:17:23",False,"2020/01/10, 15:58:40","2020/01/10, 15:58:40",10633756.0,17.0,0,52,Issue in passing data in zipkin via kafka - Could not convert message,class
Zipkin,59679685,nan,0,"2020/01/10, 12:17:23",False,"2020/01/10, 15:58:40","2020/01/10, 15:58:40",10633756.0,17.0,0,52,Issue in passing data in zipkin via kafka - Could not convert message,Please tell me how to solve the issue?
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,I have implemented different microservice where internal communication is taking place by camel and rabbitMQ with different exchange and queue.
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,"I'm using  camel-zipkin  for tracing, and  log4j2  for logging."
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,Ex:
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,I am using Springboot  1.5.12.RELEASE  and  Camel  2.21.5 version   with the following dependencies (of camel):
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,I need suggestion for my two problems
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,My Questions are :
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,Sample Code :
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,Pom.xml
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,SpringBoot Application
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,RabbitMQConfig
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,Controller adding message to queue
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,RouteBuilder
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,MessageActivity
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,SmsActivity
Zipkin,59159082,nan,0,"2019/12/03, 16:15:04",False,"2019/12/03, 18:35:11","2019/12/03, 18:35:11",10328531.0,1.0,0,431,How to trace message with camel-zipkin and camel-rabbitmq,Output
Zipkin,58619789,nan,3,"2019/10/30, 08:34:26",False,"2019/11/11, 11:50:52","2019/10/30, 11:59:53",9936877.0,21.0,0,711,Zipkin Server giving Whitelabel Error Page,"While creating a Zipkin Server with Spring Boot(v2), I am facing Whitelabel Error Page. 
        """
Zipkin,58619789,nan,3,"2019/10/30, 08:34:26",False,"2019/11/11, 11:50:52","2019/10/30, 11:59:53",9936877.0,21.0,0,711,Zipkin Server giving Whitelabel Error Page,"Whitelabel Error Page 
        This application has no explicit mapping for /error, so you are seeing this as a fallback."
Zipkin,58619789,nan,3,"2019/10/30, 08:34:26",False,"2019/11/11, 11:50:52","2019/10/30, 11:59:53",9936877.0,21.0,0,711,Zipkin Server giving Whitelabel Error Page,"Wed Oct 30 11:21:35 IST 2019
        There was an unexpected error (type=Not Found, status=404)."
Zipkin,58619789,nan,3,"2019/10/30, 08:34:26",False,"2019/11/11, 11:50:52","2019/10/30, 11:59:53",9936877.0,21.0,0,711,Zipkin Server giving Whitelabel Error Page,"No message available 
        "" 
And also while i run the application in spring boot, i get: 
        ""  Cannot find template location: classpath:/templates/ (please add some templates or check your 
        Thymeleaf configuration) """
Zipkin,58619789,nan,3,"2019/10/30, 08:34:26",False,"2019/11/11, 11:50:52","2019/10/30, 11:59:53",9936877.0,21.0,0,711,Zipkin Server giving Whitelabel Error Page,Please help me to resolve this
Zipkin,58294974,58298923.0,1,"2019/10/09, 01:55:31",True,"2019/10/09, 10:18:06",nan,7758110.0,724.0,0,726,zipkin not showing logs,"In the logs, Zipkin status is coming as true but I can not see it in the Zipkin UI."
Zipkin,58294974,58298923.0,1,"2019/10/09, 01:55:31",True,"2019/10/09, 10:18:06",nan,7758110.0,724.0,0,726,zipkin not showing logs,The same things are working for the zuul but not for the other microservices.
Zipkin,58294974,58298923.0,1,"2019/10/09, 01:55:31",True,"2019/10/09, 10:18:06",nan,7758110.0,724.0,0,726,zipkin not showing logs,dependencies
Zipkin,58294974,58298923.0,1,"2019/10/09, 01:55:31",True,"2019/10/09, 10:18:06",nan,7758110.0,724.0,0,726,zipkin not showing logs,properties:
Zipkin,58294974,58298923.0,1,"2019/10/09, 01:55:31",True,"2019/10/09, 10:18:06",nan,7758110.0,724.0,0,726,zipkin not showing logs,"The only difference between zuul and this microservice is, it is using the spring cloud stream as well."
Zipkin,58294974,58298923.0,1,"2019/10/09, 01:55:31",True,"2019/10/09, 10:18:06",nan,7758110.0,724.0,0,726,zipkin not showing logs,Can it be a reason?
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,"I try to setup zipkin, elasticsearch, prometheus and grafana with docker-compose.yml 
When I run dockers, see in the log:"
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,dependencies_zipkin | 19/09/30 14:37:09 ERROR NetworkClient: Node [172.28.0.2:9200] failed (java.net.ConnectException: Connection refused (Connection refused)); no other nodes left - aborting...
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,I'm on MacOS X with docker 2.1.0.3
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,the content of my docker-compose.yml is this one:
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,"When I connect to localhost:9200, I see that elasticsearch is working fine and on port 9411, zipkin is deployed but I have the error:"
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,ERROR: cannot load service names: server error (Service Unavailable)(due to the network error
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,"In the log, I have this information:"
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,105 ^[[35mdependencies_zipkin |^[[0m 19/09/30 14:45:20 ERROR NetworkClient: Node [172.28.0.2:9200] failed (java.net.ConnectException: Connection refused (Connection refused)); no     other nodes left - aborting...
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,and this one
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,"^[[31mzipkin          |^[[0m java.lang.IllegalStateException: couldn't connect any of [Endpoint{storage:80, ipAddr=172.28.0.2, weight=1000}]"
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,Any idea?
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,"UPDATE
by using mysql it is working fine, so the problem is at the level of elastic search."
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,I tried alsoo by using
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,"""STORAGE_PORT_9200_TCP_ADDR=127.0.0.1"""
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,but the issue still occurs.
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,"UPDATE
As mention is the solution gave by Brian, I have to use:"
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,ES_HOSTS= http://storage:9300
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,"the key is on port, I was using the port 9200"
Zipkin,58170335,nan,2,"2019/09/30, 17:49:15",False,"2019/10/01, 12:23:56","2019/10/01, 10:23:52",9795454.0,35.0,0,1032,docker-compose: zipkin cannot connect to elasticsearch,The error disappear between zipkin and es but still occurs between es and zipkin-dependencies.
Zipkin,57284146,nan,1,"2019/07/31, 08:58:10",True,"2019/07/31, 14:52:33","2019/07/31, 14:52:33",11602721.0,11.0,0,1306,Load data to zipkin with curl post request,I use zipking for testing with curl post.
Zipkin,57284146,nan,1,"2019/07/31, 08:58:10",True,"2019/07/31, 14:52:33","2019/07/31, 14:52:33",11602721.0,11.0,0,1306,Load data to zipkin with curl post request,Examples for post  https://zipkin.io/zipkin-api/#/default/post_spans
Zipkin,57284146,nan,1,"2019/07/31, 08:58:10",True,"2019/07/31, 14:52:33","2019/07/31, 14:52:33",11602721.0,11.0,0,1306,Load data to zipkin with curl post request,503 Service Unavailable
Zipkin,57284146,nan,1,"2019/07/31, 08:58:10",True,"2019/07/31, 14:52:33","2019/07/31, 14:52:33",11602721.0,11.0,0,1306,Load data to zipkin with curl post request,"zipkin in docker, logs in container:"
Zipkin,57284146,nan,1,"2019/07/31, 08:58:10",True,"2019/07/31, 14:52:33","2019/07/31, 14:52:33",11602721.0,11.0,0,1306,Load data to zipkin with curl post request,"2019-07-24 07:05:42.383 WARN 1 --- [orker-epoll-2-5]
  z.s.i.BodyIsExceptionMessage : Unexpected error handling request."
Zipkin,57284146,nan,1,"2019/07/31, 08:58:10",True,"2019/07/31, 14:52:33","2019/07/31, 14:52:33",11602721.0,11.0,0,1306,Load data to zipkin with curl post request,com.linecorp.armeria.common.stream.AbortedStreamException: null
Zipkin,57284146,nan,1,"2019/07/31, 08:58:10",True,"2019/07/31, 14:52:33","2019/07/31, 14:52:33",11602721.0,11.0,0,1306,Load data to zipkin with curl post request,I also tried example:
Zipkin,57284146,nan,1,"2019/07/31, 08:58:10",True,"2019/07/31, 14:52:33","2019/07/31, 14:52:33",11602721.0,11.0,0,1306,Load data to zipkin with curl post request,but I do not sees in web ui.
Zipkin,57282683,nan,0,"2019/07/31, 06:18:07",False,"2019/07/31, 06:18:07",nan,7859610.0,1.0,0,15,How to config zipkin mysql in windows with cmd?,What is the corresponding command for “STORAGE_TYPE=mysql MYSQL_USER=root java -jar zipkin.jar” under cmd?
Zipkin,57282683,nan,0,"2019/07/31, 06:18:07",False,"2019/07/31, 06:18:07",nan,7859610.0,1.0,0,15,How to config zipkin mysql in windows with cmd?,"I succeeded in Ubuntu, but the cmd in Windows has never been successful."
Zipkin,57219354,nan,1,"2019/07/26, 14:41:27",False,"2019/07/27, 15:16:24","2019/07/26, 15:40:55",11841253.0,1.0,0,122,spring-cloud-starter-zipkin clash with jdbc?,My spring boot application has some problem.
Zipkin,57219354,nan,1,"2019/07/26, 14:41:27",False,"2019/07/27, 15:16:24","2019/07/26, 15:40:55",11841253.0,1.0,0,122,spring-cloud-starter-zipkin clash with jdbc?,Zipkin and jdbc can not coexist together.
Zipkin,57219354,nan,1,"2019/07/26, 14:41:27",False,"2019/07/27, 15:16:24","2019/07/26, 15:40:55",11841253.0,1.0,0,122,spring-cloud-starter-zipkin clash with jdbc?,It is normal to have only one zipkin or jdbc.
Zipkin,57219354,nan,1,"2019/07/26, 14:41:27",False,"2019/07/27, 15:16:24","2019/07/26, 15:40:55",11841253.0,1.0,0,122,spring-cloud-starter-zipkin clash with jdbc?,Maven dependency:
Zipkin,57219354,nan,1,"2019/07/26, 14:41:27",False,"2019/07/27, 15:16:24","2019/07/26, 15:40:55",11841253.0,1.0,0,122,spring-cloud-starter-zipkin clash with jdbc?,Exception:
Zipkin,57219354,nan,1,"2019/07/26, 14:41:27",False,"2019/07/27, 15:16:24","2019/07/26, 15:40:55",11841253.0,1.0,0,122,spring-cloud-starter-zipkin clash with jdbc?,Nested exception is:
Zipkin,57219354,nan,1,"2019/07/26, 14:41:27",False,"2019/07/27, 15:16:24","2019/07/26, 15:40:55",11841253.0,1.0,0,122,spring-cloud-starter-zipkin clash with jdbc?,org.springframework.beans.factory.BeanCreationNotAllowedException: Error creating bean with name 'eurekaClientConfigBean': Singleton bean creation not allowed while singletons of this factory are in destruction (Do not request a bean from a BeanFactory in a destroy method implementation!)
Zipkin,56070700,56070984.0,1,"2019/05/10, 07:26:33",True,"2019/05/10, 08:05:46",nan,5871191.0,144.0,0,87,Deadlock starting c3po connection pool with Zipkin TracingStatementInterceptor,Trying to add Zipkin mysql tracing instrumentation by including the following library in a Spring Boot Application
Zipkin,56070700,56070984.0,1,"2019/05/10, 07:26:33",True,"2019/05/10, 08:05:46",nan,5871191.0,144.0,0,87,Deadlock starting c3po connection pool with Zipkin TracingStatementInterceptor,And appending the following to my jdbc connection string:
Zipkin,56070700,56070984.0,1,"2019/05/10, 07:26:33",True,"2019/05/10, 08:05:46",nan,5871191.0,144.0,0,87,Deadlock starting c3po connection pool with Zipkin TracingStatementInterceptor,Existing DB connection is using a c3po connection pool configured as follows:
Zipkin,56070700,56070984.0,1,"2019/05/10, 07:26:33",True,"2019/05/10, 08:05:46",nan,5871191.0,144.0,0,87,Deadlock starting c3po connection pool with Zipkin TracingStatementInterceptor,but running into issue starting service when attempting to acquire connections from the pool:
Zipkin,55512192,nan,1,"2019/04/04, 12:18:41",True,"2019/04/23, 00:16:47",nan,3852723.0,291.0,0,208,Possibilities of tracing REST trace time with azure log analytics similar to zipkin,We are in process to keep all monitoring and logging stuff outside of AKS.
Zipkin,55512192,nan,1,"2019/04/04, 12:18:41",True,"2019/04/23, 00:16:47",nan,3852723.0,291.0,0,208,Possibilities of tracing REST trace time with azure log analytics similar to zipkin,we got some success with Azure log analytics as well.
Zipkin,55512192,nan,1,"2019/04/04, 12:18:41",True,"2019/04/23, 00:16:47",nan,3852723.0,291.0,0,208,Possibilities of tracing REST trace time with azure log analytics similar to zipkin,I am checking if Azure log analytics provide any feature similar to zipkin.
Zipkin,55512192,nan,1,"2019/04/04, 12:18:41",True,"2019/04/23, 00:16:47",nan,3852723.0,291.0,0,208,Possibilities of tracing REST trace time with azure log analytics similar to zipkin,i.e.
Zipkin,55512192,nan,1,"2019/04/04, 12:18:41",True,"2019/04/23, 00:16:47",nan,3852723.0,291.0,0,208,Possibilities of tracing REST trace time with azure log analytics similar to zipkin,providing trace of REST API.
Zipkin,55384623,nan,0,"2019/03/27, 20:56:04",False,"2019/03/27, 20:56:04",nan,5044983.0,11.0,0,582,How can I send data to zipkin from kafka in java?,I hope you can help me.
Zipkin,55384623,nan,0,"2019/03/27, 20:56:04",False,"2019/03/27, 20:56:04",nan,5044983.0,11.0,0,582,How can I send data to zipkin from kafka in java?,Let's use for example this very simple code
Zipkin,55384623,nan,0,"2019/03/27, 20:56:04",False,"2019/03/27, 20:56:04",nan,5044983.0,11.0,0,582,How can I send data to zipkin from kafka in java?,Like you can see this microservice only forwards messages from one kafka-topic to another.
Zipkin,55384623,nan,0,"2019/03/27, 20:56:04",False,"2019/03/27, 20:56:04",nan,5044983.0,11.0,0,582,How can I send data to zipkin from kafka in java?,I also want to send this data to zipkin to see the duration of the messages or something like that.
Zipkin,55384623,nan,0,"2019/03/27, 20:56:04",False,"2019/03/27, 20:56:04",nan,5044983.0,11.0,0,582,How can I send data to zipkin from kafka in java?,Maybe I've seen the solution and don't get it but I realy have looked for a solution but didn't find one.
Zipkin,55384623,nan,0,"2019/03/27, 20:56:04",False,"2019/03/27, 20:56:04",nan,5044983.0,11.0,0,582,How can I send data to zipkin from kafka in java?,You are my last hope.
Zipkin,55384623,nan,0,"2019/03/27, 20:56:04",False,"2019/03/27, 20:56:04",nan,5044983.0,11.0,0,582,How can I send data to zipkin from kafka in java?,I have seen the brave api but I don't realy understand how to use it for kafka.
Zipkin,55384623,nan,0,"2019/03/27, 20:56:04",False,"2019/03/27, 20:56:04",nan,5044983.0,11.0,0,582,How can I send data to zipkin from kafka in java?,I
Zipkin,55364711,55535480.0,1,"2019/03/26, 21:15:10",True,"2019/04/05, 15:32:51",nan,1493432.0,2111.0,0,72,logger messages in zipkin,I am new to sleuth and zipkin.
Zipkin,55364711,55535480.0,1,"2019/03/26, 21:15:10",True,"2019/04/05, 15:32:51",nan,1493432.0,2111.0,0,72,logger messages in zipkin,I have logged some messages and sleuth is appending trace id and space id for those messages.
Zipkin,55364711,55535480.0,1,"2019/03/26, 21:15:10",True,"2019/04/05, 15:32:51",nan,1493432.0,2111.0,0,72,logger messages in zipkin,I am using zipkin to visualize it.
Zipkin,55364711,55535480.0,1,"2019/03/26, 21:15:10",True,"2019/04/05, 15:32:51",nan,1493432.0,2111.0,0,72,logger messages in zipkin,I am able to see timings at different microservices.
Zipkin,55364711,55535480.0,1,"2019/03/26, 21:15:10",True,"2019/04/05, 15:32:51",nan,1493432.0,2111.0,0,72,logger messages in zipkin,Can we see logger messages(we put at different microservices) in zipkin UI by trace id?
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,I am new to distributed tracing and trying to use the example explained in the video  https://www.youtube.com/watch?v=CFLZJSwbYI0
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,In short this has following
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,Now when I run zipkin server using command
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,java -jar zipkin.jar
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,"and accesing zipkin at url
     http://localhost:9411/zipkin"
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,So far everything works fine.
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,Now I am starting zipkin-client/service which is running at port 8081/8082
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,After this i accessed zipkin url ( http://localhost:9411/zipkin ) but it's broken now.
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,I am wondering why starting other service on port 8081/8081 is causing zipkin server to stop responding.
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,Any kind of help is much appreciated!!
Zipkin,55210330,nan,0,"2019/03/17, 20:14:32",False,"2019/03/17, 20:14:32",nan,2614310.0,281.0,0,150,zipkin distributed tracing demo,!
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,"I am upgrading an application to Spring Boot 2.1.3 (from 1.5.x), and I am facing an issue at startup time."
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,Below block can't be bound properly :
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,I am getting this error :
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,A bit before I am getting a WARN log announcing the issue :
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,"I am trying to follow in debug, and I end up pretty deep in Spring Boot internals 
 in  org.springframework.boot.context.properties.bind.Binder  ."
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,I have a similar app with more or less same version for which it works just fine.
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,"I am trying to find a difference, compare the execution flows, but not finding anything obvious."
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,"In IntelliJ, I get the auto-completion so I know my yaml is formatted properly : the ""web"" value is proposed to me."
Zipkin,54758615,54758616.0,1,"2019/02/19, 05:51:06",True,"2019/02/19, 08:31:24","2019/02/19, 08:31:24",3067542.0,3909.0,0,238,Unable to bind a Zipkin enum in Spring Boot 2,Any idea of how to investigate this kind of issue ?
Zipkin,54645785,54766304.0,1,"2019/02/12, 10:31:42",True,"2019/02/19, 14:25:08","2019/02/12, 10:49:01",10857972.0,3.0,0,191,Sleuth/zipkin with ControllerAdvice is not working,"I found there is an old issue  Sleuth/Zipkin tracing with @ControllerAdvice , but I meet the same problem with the latest version(spring-cloud-starter-zipkin:2.1.0.RELEASE), I debug it and find that the error is null, so zipkin just guess with statuscode."
Zipkin,54645785,54766304.0,1,"2019/02/12, 10:31:42",True,"2019/02/19, 14:25:08","2019/02/12, 10:49:01",10857972.0,3.0,0,191,Sleuth/zipkin with ControllerAdvice is not working,I have to throw the exception again to make zipkin notify the exception
Zipkin,54645785,54766304.0,1,"2019/02/12, 10:31:42",True,"2019/02/19, 14:25:08","2019/02/12, 10:49:01",10857972.0,3.0,0,191,Sleuth/zipkin with ControllerAdvice is not working,error is null
Zipkin,54645785,54766304.0,1,"2019/02/12, 10:31:42",True,"2019/02/19, 14:25:08","2019/02/12, 10:49:01",10857972.0,3.0,0,191,Sleuth/zipkin with ControllerAdvice is not working,zipkin result
Zipkin,54645785,54766304.0,1,"2019/02/12, 10:31:42",True,"2019/02/19, 14:25:08","2019/02/12, 10:49:01",10857972.0,3.0,0,191,Sleuth/zipkin with ControllerAdvice is not working,ControllerAdvice
Zipkin,54645785,54766304.0,1,"2019/02/12, 10:31:42",True,"2019/02/19, 14:25:08","2019/02/12, 10:49:01",10857972.0,3.0,0,191,Sleuth/zipkin with ControllerAdvice is not working,"throw the exception again, it works"
Zipkin,54555618,nan,2,"2019/02/06, 16:11:41",True,"2019/02/06, 23:57:39",nan,4911291.0,1168.0,0,395,send only info level logs to zipkin,"In the micro-services based architecture , I have a services which helps me to fetch order details ."
Zipkin,54555618,nan,2,"2019/02/06, 16:11:41",True,"2019/02/06, 23:57:39",nan,4911291.0,1168.0,0,395,send only info level logs to zipkin,"Internally order details fetches - customer details , delivery details , product details ."
Zipkin,54555618,nan,2,"2019/02/06, 16:11:41",True,"2019/02/06, 23:57:39",nan,4911291.0,1168.0,0,395,send only info level logs to zipkin,We have all services developed and the architecture is established.
Zipkin,54555618,nan,2,"2019/02/06, 16:11:41",True,"2019/02/06, 23:57:39",nan,4911291.0,1168.0,0,395,send only info level logs to zipkin,No we export everything to zipkin with the sampler rate of 100%.
Zipkin,54555618,nan,2,"2019/02/06, 16:11:41",True,"2019/02/06, 23:57:39",nan,4911291.0,1168.0,0,395,send only info level logs to zipkin,"So it includes - INFO level logs and error level logs also , currently it is useful but we already have separate mechanism for ERROR level logs."
Zipkin,54555618,nan,2,"2019/02/06, 16:11:41",True,"2019/02/06, 23:57:39",nan,4911291.0,1168.0,0,395,send only info level logs to zipkin,"so , i just want to skip the zipkin logging for ERROR level log and  send only INFO level logs to zipkin"
Zipkin,54555618,nan,2,"2019/02/06, 16:11:41",True,"2019/02/06, 23:57:39",nan,4911291.0,1168.0,0,395,send only info level logs to zipkin,"I tried searching through, but could not get any help, i am newbie to micro services"
Zipkin,54555618,nan,2,"2019/02/06, 16:11:41",True,"2019/02/06, 23:57:39",nan,4911291.0,1168.0,0,395,send only info level logs to zipkin,"any help is appreciated , thank you"
Zipkin,54466528,54468173.0,1,"2019/01/31, 19:52:49",True,"2019/01/31, 21:45:59","2019/01/31, 19:58:53",1575416.0,2296.0,0,735,Fail to start app when adding zipkin and sleuth,I created a spring boot 2.1.2 basic web app using the initializr tool.
Zipkin,54466528,54468173.0,1,"2019/01/31, 19:52:49",True,"2019/01/31, 21:45:59","2019/01/31, 19:58:53",1575416.0,2296.0,0,735,Fail to start app when adding zipkin and sleuth,The app starts fine and responds to a hello world kinda request.
Zipkin,54466528,54468173.0,1,"2019/01/31, 19:52:49",True,"2019/01/31, 21:45:59","2019/01/31, 19:58:53",1575416.0,2296.0,0,735,Fail to start app when adding zipkin and sleuth,"When I then attempted to add zipkin and sleuth, I now get an error."
Zipkin,54466528,54468173.0,1,"2019/01/31, 19:52:49",True,"2019/01/31, 21:45:59","2019/01/31, 19:58:53",1575416.0,2296.0,0,735,Fail to start app when adding zipkin and sleuth,Process finished with exit code 0
Zipkin,54466528,54468173.0,1,"2019/01/31, 19:52:49",True,"2019/01/31, 21:45:59","2019/01/31, 19:58:53",1575416.0,2296.0,0,735,Fail to start app when adding zipkin and sleuth,Dependencies look like this
Zipkin,54466528,54468173.0,1,"2019/01/31, 19:52:49",True,"2019/01/31, 21:45:59","2019/01/31, 19:58:53",1575416.0,2296.0,0,735,Fail to start app when adding zipkin and sleuth,"I tried to go down to spring boot version 2.0.4, which is the next down available through maven, however then the Jersey package started bucking."
Zipkin,54466528,54468173.0,1,"2019/01/31, 19:52:49",True,"2019/01/31, 21:45:59","2019/01/31, 19:58:53",1575416.0,2296.0,0,735,Fail to start app when adding zipkin and sleuth,Is there a way to get zipkin and sleuth to work with spring boot 2.1.2?
Zipkin,54466528,54468173.0,1,"2019/01/31, 19:52:49",True,"2019/01/31, 21:45:59","2019/01/31, 19:58:53",1575416.0,2296.0,0,735,Fail to start app when adding zipkin and sleuth,Swagger seem to have a similar issue.
Zipkin,53428788,nan,1,"2018/11/22, 12:24:32",False,"2018/11/25, 16:03:42",nan,10580534.0,11.0,0,234,How do i write the zipkin trace to a file in a format(json) that is supported by the zipkin ui?,I want to write the zipkin trace data from the recorder to a file using NodeJS in a format which zipkin ui supports so that i can import the file into the zipkin ui later and do analysis.
Zipkin,53218692,nan,3,"2018/11/09, 03:46:21",False,"2018/12/26, 15:12:50",nan,2979435.0,3071.0,0,126,Monitoring application business logic with zipkin tracking information,Actually I have a microservice architecture as follows
Zipkin,53218692,nan,3,"2018/11/09, 03:46:21",False,"2018/12/26, 15:12:50",nan,2979435.0,3071.0,0,126,Monitoring application business logic with zipkin tracking information,So I have 4 microservices and for every microservice I send a notification to zipkin when starts and finish it's objective.
Zipkin,53218692,nan,3,"2018/11/09, 03:46:21",False,"2018/12/26, 15:12:50",nan,2979435.0,3071.0,0,126,Monitoring application business logic with zipkin tracking information,I have to monitor my product to make sure all requested checkouts will have
Zipkin,53218692,nan,3,"2018/11/09, 03:46:21",False,"2018/12/26, 15:12:50",nan,2979435.0,3071.0,0,126,Monitoring application business logic with zipkin tracking information,"zipkin as a tracking system already own all this information cause it keep the checkout track from the very beginning until the end, I'm wondering how I can query at zipkin all checkouts that have been processed by  JAVA REST API  microservice and didn't be processed by at least one of the others ( PAYMENT GATEWAY ,  SALE CREATOR  and  EMAIL NOTIFIER"
Zipkin,53218692,nan,3,"2018/11/09, 03:46:21",False,"2018/12/26, 15:12:50",nan,2979435.0,3071.0,0,126,Monitoring application business logic with zipkin tracking information,How can I query on zipkin which checkouts haven't been processed by all others microservices after  REST API ?
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,Hi everyone!
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,"I have ""ELK"" (6.4.2) working perfectly with filebeat, metricbeat, packetbeat and winlogbeat in CentOS 7 x86_64 (Kernel 3.10.0-862.11.6.el7.x86_64)."
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,"I'm trying to integrate zipkin + elk (see  https://logz.io/blog/zipkin-elk/ ), but Elasticsearch does not create indices with Kibana."
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,"When trying to create the indices in Kibana, the process does not end."
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,(Follow logs below).
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,I suspect the zipkin connection drivers are not compatible with elk 6.4.2.
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,"Has anyone had the same problem and has a ""light at the end of the tunnel""?"
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,Tks for all!
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,Java version:
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,Zipkin startup:
Zipkin,52972425,nan,2,"2018/10/24, 18:10:14",False,"2018/10/25, 13:38:05",nan,10552245.0,1.0,0,698,Zipkin + Elasticsearch (ELK) not create index,Error log in Elasticsearch:
Zipkin,52706088,nan,1,"2018/10/08, 18:59:09",True,"2018/10/15, 14:07:16","2018/10/08, 19:20:08",3699749.0,31.0,0,583,Spring cloud sleuth how to allow certain URL pattern alone to export traces to zipkin,Requirement is to export traces for requests that matches url pattern to zipkin from apps.
Zipkin,52706088,nan,1,"2018/10/08, 18:59:09",True,"2018/10/15, 14:07:16","2018/10/08, 19:20:08",3699749.0,31.0,0,583,Spring cloud sleuth how to allow certain URL pattern alone to export traces to zipkin,I got to know that there are options in sleuth properties to exclude traces from exporting.
Zipkin,52706088,nan,1,"2018/10/08, 18:59:09",True,"2018/10/15, 14:07:16","2018/10/08, 19:20:08",3699749.0,31.0,0,583,Spring cloud sleuth how to allow certain URL pattern alone to export traces to zipkin,But my case is the opposite of it.
Zipkin,52706088,nan,1,"2018/10/08, 18:59:09",True,"2018/10/15, 14:07:16","2018/10/08, 19:20:08",3699749.0,31.0,0,583,Spring cloud sleuth how to allow certain URL pattern alone to export traces to zipkin,Include traces for exporting for only specified url patterns.
Zipkin,52706088,nan,1,"2018/10/08, 18:59:09",True,"2018/10/15, 14:07:16","2018/10/08, 19:20:08",3699749.0,31.0,0,583,Spring cloud sleuth how to allow certain URL pattern alone to export traces to zipkin,I was trying to have a custom httpSampler and mentioned my logic to export the trace based on url patterns.
Zipkin,52706088,nan,1,"2018/10/08, 18:59:09",True,"2018/10/15, 14:07:16","2018/10/08, 19:20:08",3699749.0,31.0,0,583,Spring cloud sleuth how to allow certain URL pattern alone to export traces to zipkin,But it did not work as expected.
Zipkin,52706088,nan,1,"2018/10/08, 18:59:09",True,"2018/10/15, 14:07:16","2018/10/08, 19:20:08",3699749.0,31.0,0,583,Spring cloud sleuth how to allow certain URL pattern alone to export traces to zipkin,"Any samples available on the same, would really be helpful?"
Zipkin,52706088,nan,1,"2018/10/08, 18:59:09",True,"2018/10/15, 14:07:16","2018/10/08, 19:20:08",3699749.0,31.0,0,583,Spring cloud sleuth how to allow certain URL pattern alone to export traces to zipkin,Thanks much.
Zipkin,52503115,nan,0,"2018/09/25, 19:45:13",False,"2018/09/25, 22:49:52","2018/09/25, 22:49:52",976899.0,498.0,0,323,Publish spans to Zipkin via Kafka using Sleuth,I have the following problem: i need to send traces to Zipkin via Kafka using Sleuth.
Zipkin,52503115,nan,0,"2018/09/25, 19:45:13",False,"2018/09/25, 22:49:52","2018/09/25, 22:49:52",976899.0,498.0,0,323,Publish spans to Zipkin via Kafka using Sleuth,Based on what i read in the documentation( https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka ) all i need to do is to add  spring-kafka  dependency:
Zipkin,52503115,nan,0,"2018/09/25, 19:45:13",False,"2018/09/25, 22:49:52","2018/09/25, 22:49:52",976899.0,498.0,0,323,Publish spans to Zipkin via Kafka using Sleuth,But i can't really see there to which kafka bootstrap server should i send or to which topic(is there some default topic that i should know about?
Zipkin,52503115,nan,0,"2018/09/25, 19:45:13",False,"2018/09/25, 22:49:52","2018/09/25, 22:49:52",976899.0,498.0,0,323,Publish spans to Zipkin via Kafka using Sleuth,).
Zipkin,52424864,52776732.0,2,"2018/09/20, 15:09:08",True,"2018/10/12, 12:41:59",nan,976899.0,498.0,0,37,Deletion of service name in Zipkin dropdown list,I have the following problem: i have added numerous services in Zipkin but now i want to remove some of them.
Zipkin,52424864,52776732.0,2,"2018/09/20, 15:09:08",True,"2018/10/12, 12:41:59",nan,976899.0,498.0,0,37,Deletion of service name in Zipkin dropdown list,I keep the data in memory so no persistency involved there.
Zipkin,52424864,52776732.0,2,"2018/09/20, 15:09:08",True,"2018/10/12, 12:41:59",nan,976899.0,498.0,0,37,Deletion of service name in Zipkin dropdown list,Is there any way to delete a service name from Zipkin's service name dropdown list?
Zipkin,51661009,52608036.0,1,"2018/08/02, 22:38:59",True,"2018/10/02, 15:08:15",nan,2999097.0,153.0,0,2218,Zipkin Integration With RabbitMQ for tracing,I have two Microservices (Spring boot application) .
Zipkin,51661009,52608036.0,1,"2018/08/02, 22:38:59",True,"2018/10/02, 15:08:15",nan,2999097.0,153.0,0,2218,Zipkin Integration With RabbitMQ for tracing,For tracing I am using  &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;  along with zipkin.
Zipkin,51661009,52608036.0,1,"2018/08/02, 22:38:59",True,"2018/10/02, 15:08:15",nan,2999097.0,153.0,0,2218,Zipkin Integration With RabbitMQ for tracing,Service A is producer and  send message using RabbitMQ broker.
Zipkin,51661009,52608036.0,1,"2018/08/02, 22:38:59",True,"2018/10/02, 15:08:15",nan,2999097.0,153.0,0,2218,Zipkin Integration With RabbitMQ for tracing,"On other hand Service B is the consumer, their is   @RabbitListener  ."
Zipkin,51661009,52608036.0,1,"2018/08/02, 22:38:59",True,"2018/10/02, 15:08:15",nan,2999097.0,153.0,0,2218,Zipkin Integration With RabbitMQ for tracing,"I want to exchange the traceId(with span details) from service A to Service B.
I have seen the  example  (using brave) but unable to integrate zipkin with rabbitMQ and trace propogation."
Zipkin,51661009,52608036.0,1,"2018/08/02, 22:38:59",True,"2018/10/02, 15:08:15",nan,2999097.0,153.0,0,2218,Zipkin Integration With RabbitMQ for tracing,Can Any One please help me how to acheive this ?Any complete step-by-step and simple example?
Zipkin,51578263,51580557.0,1,"2018/07/29, 10:36:22",True,"2018/07/31, 14:18:12","2018/07/29, 10:42:54",7657943.0,384.0,0,214,Spring Initializer - Zipkin dependencies missing?,"In spring initializer i couldn't find following dependencies  zipkin ui ,  zipkin stream ,  stream rabbit .I know it was available but i don't know why they've deprecated those dependencies."
Zipkin,51578263,51580557.0,1,"2018/07/29, 10:36:22",True,"2018/07/31, 14:18:12","2018/07/29, 10:42:54",7657943.0,384.0,0,214,Spring Initializer - Zipkin dependencies missing?,Are there any other alternatives dependencies spring initializer provide?
Zipkin,51438273,nan,1,"2018/07/20, 11:27:57",True,"2018/07/20, 11:46:33","2018/07/20, 11:42:34",591254.0,1416.0,0,644,zipkin asynch span every second,"We have a spring-boot application (spring-boot-starter-parent-2.0.0.RELEASE) using spring-cloud-starter-zipkin for writing ""spans"" to zipkin."
Zipkin,51438273,nan,1,"2018/07/20, 11:27:57",True,"2018/07/20, 11:46:33","2018/07/20, 11:42:34",591254.0,1416.0,0,644,zipkin asynch span every second,We use spring-integration too (through spring-boot-starter-integration) and we have added an integration flow with a PollableChannel to be used within a poller:
Zipkin,51438273,nan,1,"2018/07/20, 11:27:57",True,"2018/07/20, 11:46:33","2018/07/20, 11:42:34",591254.0,1416.0,0,644,zipkin asynch span every second,"Since adding this configuration, we are having an ""asynch"" span every second."
Zipkin,51438273,nan,1,"2018/07/20, 11:27:57",True,"2018/07/20, 11:46:33","2018/07/20, 11:42:34",591254.0,1416.0,0,644,zipkin asynch span every second,"It seems this span comes from the @Poller, checking whether there are items in the queue."
Zipkin,51438273,nan,1,"2018/07/20, 11:27:57",True,"2018/07/20, 11:46:33","2018/07/20, 11:42:34",591254.0,1416.0,0,644,zipkin asynch span every second,I'd like to know how to control this span.
Zipkin,51438273,nan,1,"2018/07/20, 11:27:57",True,"2018/07/20, 11:46:33","2018/07/20, 11:42:34",591254.0,1416.0,0,644,zipkin asynch span every second,Is it possible to disable?
Zipkin,51438273,nan,1,"2018/07/20, 11:27:57",True,"2018/07/20, 11:46:33","2018/07/20, 11:42:34",591254.0,1416.0,0,644,zipkin asynch span every second,Specially if there are no items.
Zipkin,51438273,nan,1,"2018/07/20, 11:27:57",True,"2018/07/20, 11:46:33","2018/07/20, 11:42:34",591254.0,1416.0,0,644,zipkin asynch span every second,Thanks in advance!
Zipkin,51244941,nan,1,"2018/07/09, 14:52:00",False,"2018/07/16, 07:17:28","2018/07/09, 15:06:43",9751919.0,33.0,0,375,zipkin brave implementation using tomcat server without using profiles,I am looking for a Tracing tool for my spring web-mvc application and i ended up with using Brave-zipkin[ https://github.com/openzipkin/brave-webmvc-example/tree/master/webmvc3] .
Zipkin,51244941,nan,1,"2018/07/09, 14:52:00",False,"2018/07/16, 07:17:28","2018/07/09, 15:06:43",9751919.0,33.0,0,375,zipkin brave implementation using tomcat server without using profiles,"Everything looks fine for me except that, in the given example jetty server deploys the application twice; one for FrontEnd and another for Backend(using two profiles)."
Zipkin,51244941,nan,1,"2018/07/09, 14:52:00",False,"2018/07/16, 07:17:28","2018/07/09, 15:06:43",9751919.0,33.0,0,375,zipkin brave implementation using tomcat server without using profiles,Whereas my project uses tomcat-server.
Zipkin,51244941,nan,1,"2018/07/09, 14:52:00",False,"2018/07/16, 07:17:28","2018/07/09, 15:06:43",9751919.0,33.0,0,375,zipkin brave implementation using tomcat server without using profiles,Can anyone help me how to use this same tool for deploying in Tomcat-server and start the application without using profiles?
Zipkin,51244941,nan,1,"2018/07/09, 14:52:00",False,"2018/07/16, 07:17:28","2018/07/09, 15:06:43",9751919.0,33.0,0,375,zipkin brave implementation using tomcat server without using profiles,or please suggest any other open source tool for tracing simple monolithic spring-web-mvc application (not spring-boot) and i should be able to see the spans and dependency (eg controllerClass- serviceClass- repositoryClass just like we see under dependency tab of openzipkin web page:  http://localhost:9411/zipkin/dependency/ )
Zipkin,51068201,nan,1,"2018/06/27, 20:13:01",False,"2018/07/02, 21:12:12",nan,1826788.0,1408.0,0,245,Password protect zipkin,I am using zipkin to do distributed tracing of my microservice architecture.
Zipkin,51068201,nan,1,"2018/06/27, 20:13:01",False,"2018/07/02, 21:12:12",nan,1826788.0,1408.0,0,245,Password protect zipkin,I have created zipkin server by adding zipkin server dependency and @EnableZipkinServer Annotation.
Zipkin,51068201,nan,1,"2018/06/27, 20:13:01",False,"2018/07/02, 21:12:12",nan,1826788.0,1408.0,0,245,Password protect zipkin,Now is there a way I can add password protection to my zipkin web interface?
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,I am using Zipkin with Spring Sleuth to display traces.
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,"When I use it locally,  http://localhost:9411/zipkin/dependency/  displays a nicely created graph of dependencies within the eco-system."
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,"Sometimes, backends from outside that eco-system get called and those are not displayed in that graph."
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,Is it possible to annotate a call (let's assume RestTemplate and Feign clients) to such an external system so Zipkin would actually draw that dependency?
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,"If it's possible, what do I have to do?"
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,This would be my baseline of code:
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,Somewhere I would like to type  httpbin  so this call gets drawn in the dependency-graph of Zipkin.
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,Thank you!
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,"//  Edit  based on current solution
I'm using Spring Cloud Finchley and added the following line before restTemplate's call:"
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,I simply inject  SpanCustomizer  in this class.
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,The Span is sent to Zipkin and I see the tag is set:
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,"Unfortunately, it is not drawn in the dependencies-view."
Zipkin,50813714,50813959.0,1,"2018/06/12, 12:27:46",True,"2018/06/26, 16:13:12","2018/06/12, 17:29:32",3105453.0,1509.0,0,270,Name an external dependency in Zipkin to have it drawn,"Is there anything else I need to configure, maybe in Zipkin rather than in Sleuth?"
Zipkin,50796087,50798776.0,1,"2018/06/11, 13:52:31",True,"2018/06/11, 16:39:48",nan,7059.0,5434.0,0,604,Sleuth instrumentation of Spring cloud stream messages is lost when using zipkin + kafka,I have the following setup :
Zipkin,50796087,50798776.0,1,"2018/06/11, 13:52:31",True,"2018/06/11, 16:39:48",nan,7059.0,5434.0,0,604,Sleuth instrumentation of Spring cloud stream messages is lost when using zipkin + kafka,There used to be a  StreamEnvironmentPostProcessor  that did the job of adding the trace headers to the kafka bindings when I included the  spring-cloud-sleuth-stream  dependendy in the past.
Zipkin,50796087,50798776.0,1,"2018/06/11, 13:52:31",True,"2018/06/11, 16:39:48",nan,7059.0,5434.0,0,604,Sleuth instrumentation of Spring cloud stream messages is lost when using zipkin + kafka,But the doc clearly states now  :
Zipkin,50796087,50798776.0,1,"2018/06/11, 13:52:31",True,"2018/06/11, 16:39:48",nan,7059.0,5434.0,0,604,Sleuth instrumentation of Spring cloud stream messages is lost when using zipkin + kafka,Note: spring-cloud-sleuth-stream is deprecated and incompatible with these destinations
Zipkin,50796087,50798776.0,1,"2018/06/11, 13:52:31",True,"2018/06/11, 16:39:48",nan,7059.0,5434.0,0,604,Sleuth instrumentation of Spring cloud stream messages is lost when using zipkin + kafka,What should I do to make this work properly now ?
Zipkin,50796087,50798776.0,1,"2018/06/11, 13:52:31",True,"2018/06/11, 16:39:48",nan,7059.0,5434.0,0,604,Sleuth instrumentation of Spring cloud stream messages is lost when using zipkin + kafka,Add the headers to the bindings configuration myself ?
Zipkin,50796087,50798776.0,1,"2018/06/11, 13:52:31",True,"2018/06/11, 16:39:48",nan,7059.0,5434.0,0,604,Sleuth instrumentation of Spring cloud stream messages is lost when using zipkin + kafka,Or is there something I'm missing ?
Zipkin,50599475,50626312.0,1,"2018/05/30, 11:08:03",True,"2018/05/31, 17:09:32","2018/05/30, 11:14:28",2343510.0,1247.0,0,205,Istio. Who generates the initial Zipkin HTTP headers?,in the documentation it is explained that your services have to resend a set of headers to enable pilot/Zipkin to correlate the information.
Zipkin,50599475,50626312.0,1,"2018/05/30, 11:08:03",True,"2018/05/31, 17:09:32","2018/05/30, 11:14:28",2343510.0,1247.0,0,205,Istio. Who generates the initial Zipkin HTTP headers?,But who generates the first headers and set its values?
Zipkin,50599475,50626312.0,1,"2018/05/30, 11:08:03",True,"2018/05/31, 17:09:32","2018/05/30, 11:14:28",2343510.0,1247.0,0,205,Istio. Who generates the initial Zipkin HTTP headers?,The Istio Ingress controller?
Zipkin,50599475,50626312.0,1,"2018/05/30, 11:08:03",True,"2018/05/31, 17:09:32","2018/05/30, 11:14:28",2343510.0,1247.0,0,205,Istio. Who generates the initial Zipkin HTTP headers?,"How can I configure it, enable/disable it?"
Zipkin,50599475,50626312.0,1,"2018/05/30, 11:08:03",True,"2018/05/31, 17:09:32","2018/05/30, 11:14:28",2343510.0,1247.0,0,205,Istio. Who generates the initial Zipkin HTTP headers?,Thank you.
Zipkin,50010083,50010347.0,1,"2018/04/24, 23:13:59",True,"2018/04/24, 23:34:00",nan,3673633.0,507.0,0,583,Spring Cloud Zipkin with RabbitMQ not persisting in MYSQL,I have done all the possible matches and mix-up of dependency and still not able to record traces in zipkin ans store it in MYSQL using RabbitMQ.
Zipkin,50010083,50010347.0,1,"2018/04/24, 23:13:59",True,"2018/04/24, 23:34:00",nan,3673633.0,507.0,0,583,Spring Cloud Zipkin with RabbitMQ not persisting in MYSQL,Still i can see the trace and span id's  in console and nothing beyond this.
Zipkin,50010083,50010347.0,1,"2018/04/24, 23:13:59",True,"2018/04/24, 23:34:00",nan,3673633.0,507.0,0,583,Spring Cloud Zipkin with RabbitMQ not persisting in MYSQL,Someone please take a look at the code in github from below location.
Zipkin,50010083,50010347.0,1,"2018/04/24, 23:13:59",True,"2018/04/24, 23:34:00",nan,3673633.0,507.0,0,583,Spring Cloud Zipkin with RabbitMQ not persisting in MYSQL,Github code:  https://github.com/javayp/distributed-tracing-1
Zipkin,49280873,nan,1,"2018/03/14, 16:52:59",False,"2018/11/09, 10:11:00",nan,1530658.0,588.0,0,464,zipkin spring integration error,I'm testing zipkin to spring boot integration but im facing error like below.
Zipkin,49280873,nan,1,"2018/03/14, 16:52:59",False,"2018/11/09, 10:11:00",nan,1530658.0,588.0,0,464,zipkin spring integration error,The error seems to happen when it tries to send message to zipkin server
Zipkin,49280873,nan,1,"2018/03/14, 16:52:59",False,"2018/11/09, 10:11:00",nan,1530658.0,588.0,0,464,zipkin spring integration error,here is my pom.xml file.
Zipkin,49280873,nan,1,"2018/03/14, 16:52:59",False,"2018/11/09, 10:11:00",nan,1530658.0,588.0,0,464,zipkin spring integration error,it may be have a problem in version.
Zipkin,49280873,nan,1,"2018/03/14, 16:52:59",False,"2018/11/09, 10:11:00",nan,1530658.0,588.0,0,464,zipkin spring integration error,"and this is my application.yml file
im running zipkin server in same machine with different port"
Zipkin,49280873,nan,1,"2018/03/14, 16:52:59",False,"2018/11/09, 10:11:00",nan,1530658.0,588.0,0,464,zipkin spring integration error,any guide or information are welcomed!
Zipkin,48525567,nan,0,"2018/01/30, 17:55:42",False,"2018/01/30, 17:55:42",nan,2480359.0,1109.0,0,174,Stackdriver Trace not showing traces from Zipkin by Express API correctly,"In our cluster, we have set up a Zipkin collector for Stackdriver Trace ( like this ) so we can trace our apps."
Zipkin,48525567,nan,0,"2018/01/30, 17:55:42",False,"2018/01/30, 17:55:42",nan,2480359.0,1109.0,0,174,Stackdriver Trace not showing traces from Zipkin by Express API correctly,I am running the simple  JavaScript web example  that is offered.
Zipkin,48525567,nan,0,"2018/01/30, 17:55:42",False,"2018/01/30, 17:55:42",nan,2480359.0,1109.0,0,174,Stackdriver Trace not showing traces from Zipkin by Express API correctly,It works correctly when I configure the app to send the traces to the collector that is running in the cluster (in  recorder.js ).
Zipkin,48525567,nan,0,"2018/01/30, 17:55:42",False,"2018/01/30, 17:55:42",nan,2480359.0,1109.0,0,174,Stackdriver Trace not showing traces from Zipkin by Express API correctly,"However, when I want to inspect the traces in Stackdriver Trace, something seems to be going wrong:"
Zipkin,48525567,nan,0,"2018/01/30, 17:55:42",False,"2018/01/30, 17:55:42",nan,2480359.0,1109.0,0,174,Stackdriver Trace not showing traces from Zipkin by Express API correctly,"The  HTTP Method  column is empty, and the  URI  column seems to show the HTTP method."
Zipkin,48525567,nan,0,"2018/01/30, 17:55:42",False,"2018/01/30, 17:55:42",nan,2480359.0,1109.0,0,174,Stackdriver Trace not showing traces from Zipkin by Express API correctly,How can I make these columns display the correct information?
Zipkin,48525567,nan,0,"2018/01/30, 17:55:42",False,"2018/01/30, 17:55:42",nan,2480359.0,1109.0,0,174,Stackdriver Trace not showing traces from Zipkin by Express API correctly,Let me know if I need to add more information.
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,"I have a heterogeneous(Java, php, python, C#.Net) micro-service system which was written by several teams."
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,All communication happens over HTTP connections.
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,"I objective is to use Zipkin to trace the path of execution and identify the slowest services and start profiling them using (VisualVM, dotTrace)."
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,I've heard that Zipkin supports HTTP connectivity for tracing.
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,How would I go about doing this?
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,Is Zipkin even the right approach?
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,I'm looking for direction and some Java examples to get started.
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,"Is there a http format I could use or do I need to use multiple (Wingtips, ZipkinTracerModule,Brave) libraries?"
Zipkin,48518928,nan,1,"2018/01/30, 12:16:56",False,"2018/02/26, 08:26:46",nan,3016299.0,2316.0,0,112,Heterogeneous Tracing in ZipKin,Thanks.
Zipkin,48160588,nan,1,"2018/01/09, 04:16:23",False,"2018/01/19, 19:46:45",nan,8373115.0,1.0,0,285,How do zipkin do to merge span for search trace request?,"In the zipkin web ui, when the request url is  http://10.19.138.169:9411/zipkin/api/v1/trace/ae60bd175a61e820"
Zipkin,48160588,nan,1,"2018/01/09, 04:16:23",False,"2018/01/19, 19:46:45",nan,8373115.0,1.0,0,285,How do zipkin do to merge span for search trace request?,"I find the return response is 
[
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""ae60bd175a61e820"",
        ""name"": ""client"",
        ""timestamp"": 1511858133224433,
        ""duration"": 508444,
        ""binaryAnnotations"": [
            {
                ""key"": ""lc"",
                ""value"": """",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-http-c"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""19d69c3e93bc9040"",
        ""name"": ""post"",
        ""parentId"": ""ae60bd175a61e820"",
        ""timestamp"": 1511858133239803,
        ""duration"": 490921,
        ""annotations"": [
            {
                ""timestamp"": 1511858133239803,
                ""value"": ""cs"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-http-c"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133383290,
                ""value"": ""sr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133609368,
                ""value"": ""ss"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133730724,
                ""value"": ""cr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-http-c"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ],
        ""binaryAnnotations"": [
            {
                ""key"": ""ca"",
                ""value"": true,
                ""endpoint"": {
                    ""serviceName"": """",
                    ""ipv4"": ""127.0.0.1"",
                    ""port"": 43928
                }
            },
            {
                ""key"": ""http.path"",
                ""value"": ""/security/gateway"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""key"": ""http.path"",
                ""value"": ""/security/gateway"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-http-c"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""key"": ""sa"",
                ""value"": true,
                ""endpoint"": {
                    ""serviceName"": """",
                    ""ipv4"": ""127.0.0.1"",
                    ""port"": 8090
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""16eefe087852af41"",
        ""name"": ""ennmonitorsecuritygatewayserver/put"",
        ""parentId"": ""19d69c3e93bc9040"",
        ""timestamp"": 1511858133393425,
        ""duration"": 212916,
        ""annotations"": [
            {
                ""timestamp"": 1511858133393425,
                ""value"": ""cs"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133588237,
                ""value"": ""sr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133593907,
                ""value"": ""ss"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133606341,
                ""value"": ""cr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-web"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""8ef78f0edefe3a4b"",
        ""name"": ""data enqueue"",
        ""parentId"": ""16eefe087852af41"",
        ""timestamp"": 1511858133592958,
        ""duration"": 129,
        ""binaryAnnotations"": [
            {
                ""key"": ""lc"",
                ""value"": """",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""97c637bcc891b86a"",
        ""name"": ""data dequeue, send to kafka"",
        ""parentId"": ""16eefe087852af41"",
        ""timestamp"": 1511858133593147,
        ""duration"": 2416,
        ""binaryAnnotations"": [
            {
                ""key"": ""lc"",
                ""value"": """",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""f193c7f4193f2879"",
        ""name"": """",
        ""parentId"": ""16eefe087852af41"",
        ""timestamp"": 1511858133594113,
        ""duration"": 7575,
        ""annotations"": [
            {
                ""timestamp"": 1511858133594113,
                ""value"": ""ms"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133601688,
                ""value"": ""ws"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ],
        ""binaryAnnotations"": [
            {
                ""key"": ""kafka.topic"",
                ""value"": ""rdkafka"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-gw-s"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    },
    {
        ""traceId"": ""ae60bd175a61e820"",
        ""id"": ""54a3f6268df0aaee"",
        ""name"": """",
        ""parentId"": ""f193c7f4193f2879"",
        ""timestamp"": 1511858133600067,
        ""duration"": 5,
        ""annotations"": [
            {
                ""timestamp"": 1511858133600067,
                ""value"": ""wr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-kafka-consumer"",
                    ""ipv4"": ""10.19.138.169""
                }
            },
            {
                ""timestamp"": 1511858133600072,
                ""value"": ""mr"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-kafka-consumer"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ],
        ""binaryAnnotations"": [
            {
                ""key"": ""kafka.topic"",
                ""value"": ""rdkafka"",
                ""endpoint"": {
                    ""serviceName"": ""monitor-kafka-consumer"",
                    ""ipv4"": ""10.19.138.169""
                }
            }
        ]
    }
]"
Zipkin,48160588,nan,1,"2018/01/09, 04:16:23",False,"2018/01/19, 19:46:45",nan,8373115.0,1.0,0,285,How do zipkin do to merge span for search trace request?,It can easy to find that there are 8 spans.
Zipkin,48160588,nan,1,"2018/01/09, 04:16:23",False,"2018/01/19, 19:46:45",nan,8373115.0,1.0,0,285,How do zipkin do to merge span for search trace request?,"When I use the api to get the trace with the same traceId
        ElasticsearchStorage storage = ElasticsearchStorage.newBuilder()
                .hosts(Arrays.asList("" http://10.19.138.169:9200 "")).build();"
Zipkin,48160588,nan,1,"2018/01/09, 04:16:23",False,"2018/01/19, 19:46:45",nan,8373115.0,1.0,0,285,How do zipkin do to merge span for search trace request?,"I get 
[
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""97c637bcc891b86a"",
    ""name"": ""data dequeue, send to kafka"",
    ""timestamp"": 1511858133593147,
    ""duration"": 2416,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""id"": ""ae60bd175a61e820"",
    ""name"": ""client"",
    ""timestamp"": 1511858133224433,
    ""duration"": 508444,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-http-c"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""f193c7f4193f2879"",
    ""id"": ""54a3f6268df0aaee"",
    ""kind"": ""CONSUMER"",
    ""timestamp"": 1511858133600067,
    ""duration"": 5,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-kafka-consumer"",
      ""ipv4"": ""10.19.138.169""
    },
    ""tags"": {
      ""kafka.topic"": ""rdkafka""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""ae60bd175a61e820"",
    ""id"": ""19d69c3e93bc9040"",
    ""kind"": ""SERVER"",
    ""name"": ""post"",
    ""timestamp"": 1511858133383290,
    ""duration"": 226078,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-web"",
      ""ipv4"": ""10.19.138.169""
    },
    ""remoteEndpoint"": {
      ""ipv4"": ""127.0.0.1"",
      ""port"": 43928
    },
    ""tags"": {
      ""http.path"": ""/security/gateway""
    },
    ""shared"": true
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""19d69c3e93bc9040"",
    ""id"": ""16eefe087852af41"",
    ""kind"": ""CLIENT"",
    ""name"": ""ennmonitorsecuritygatewayserver/put"",
    ""timestamp"": 1511858133393425,
    ""duration"": 212916,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-web"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""19d69c3e93bc9040"",
    ""id"": ""16eefe087852af41"",
    ""kind"": ""SERVER"",
    ""name"": ""ennmonitorsecuritygatewayserver/put"",
    ""timestamp"": 1511858133588237,
    ""duration"": 5670,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    },
    ""shared"": true
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""f193c7f4193f2879"",
    ""kind"": ""PRODUCER"",
    ""timestamp"": 1511858133594113,
    ""duration"": 7575,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    },
    ""tags"": {
      ""kafka.topic"": ""rdkafka""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""ae60bd175a61e820"",
    ""id"": ""19d69c3e93bc9040"",
    ""kind"": ""CLIENT"",
    ""name"": ""post"",
    ""timestamp"": 1511858133239803,
    ""duration"": 490921,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-http-c"",
      ""ipv4"": ""10.19.138.169""
    },
    ""remoteEndpoint"": {
      ""ipv4"": ""127.0.0.1"",
      ""port"": 8090
    },
    ""tags"": {
      ""http.path"": ""/security/gateway""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""ae60bd175a61e820"",
    ""id"": ""19d69c3e93bc9040"",
    ""kind"": ""SERVER"",
    ""name"": ""post"",
    ""timestamp"": 1511858133383290,
    ""duration"": 226078,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-web"",
      ""ipv4"": ""10.19.138.169""
    },
    ""remoteEndpoint"": {
      ""ipv4"": ""127.0.0.1"",
      ""port"": 43928
    },
    ""tags"": {
      ""http.path"": ""/security/gateway""
    },
    ""shared"": true
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""19d69c3e93bc9040"",
    ""id"": ""16eefe087852af41"",
    ""kind"": ""SERVER"",
    ""name"": ""ennmonitorsecuritygatewayserver/put"",
    ""timestamp"": 1511858133588237,
    ""duration"": 5670,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    },
    ""shared"": true
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""97c637bcc891b86a"",
    ""name"": ""data dequeue, send to kafka"",
    ""timestamp"": 1511858133593147,
    ""duration"": 2416,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""f193c7f4193f2879"",
    ""kind"": ""PRODUCER"",
    ""timestamp"": 1511858133594113,
    ""duration"": 7575,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    },
    ""tags"": {
      ""kafka.topic"": ""rdkafka""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""19d69c3e93bc9040"",
    ""id"": ""16eefe087852af41"",
    ""kind"": ""CLIENT"",
    ""name"": ""ennmonitorsecuritygatewayserver/put"",
    ""timestamp"": 1511858133393425,
    ""duration"": 212916,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-web"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""ae60bd175a61e820"",
    ""id"": ""19d69c3e93bc9040"",
    ""kind"": ""CLIENT"",
    ""name"": ""post"",
    ""timestamp"": 1511858133239803,
    ""duration"": 490921,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-http-c"",
      ""ipv4"": ""10.19.138.169""
    },
    ""remoteEndpoint"": {
      ""ipv4"": ""127.0.0.1"",
      ""port"": 8090
    },
    ""tags"": {
      ""http.path"": ""/security/gateway""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""id"": ""ae60bd175a61e820"",
    ""name"": ""client"",
    ""timestamp"": 1511858133224433,
    ""duration"": 508444,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-http-c"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""f193c7f4193f2879"",
    ""id"": ""54a3f6268df0aaee"",
    ""kind"": ""CONSUMER"",
    ""timestamp"": 1511858133600067,
    ""duration"": 5,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-kafka-consumer"",
      ""ipv4"": ""10.19.138.169""
    },
    ""tags"": {
      ""kafka.topic"": ""rdkafka""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""8ef78f0edefe3a4b"",
    ""name"": ""data enqueue"",
    ""timestamp"": 1511858133592958,
    ""duration"": 129,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    }
  },
  {
    ""traceId"": ""ae60bd175a61e820"",
    ""parentId"": ""16eefe087852af41"",
    ""id"": ""8ef78f0edefe3a4b"",
    ""name"": ""data enqueue"",
    ""timestamp"": 1511858133592958,
    ""duration"": 129,
    ""localEndpoint"": {
      ""serviceName"": ""monitor-gw-s"",
      ""ipv4"": ""10.19.138.169""
    }
  }
]"
Zipkin,48160588,nan,1,"2018/01/09, 04:16:23",False,"2018/01/19, 19:46:45",nan,8373115.0,1.0,0,285,How do zipkin do to merge span for search trace request?,It is easy to find that there are 18 spans.
Zipkin,48160588,nan,1,"2018/01/09, 04:16:23",False,"2018/01/19, 19:46:45",nan,8373115.0,1.0,0,285,How do zipkin do to merge span for search trace request?,"It seems that some of spans are merged in the web request, I want to know to where is the cource code deal this."
Zipkin,48160588,nan,1,"2018/01/09, 04:16:23",False,"2018/01/19, 19:46:45",nan,8373115.0,1.0,0,285,How do zipkin do to merge span for search trace request?,Thanks!
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,I'm using the camel-zipkin component to trace a request that flows between two different services:
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,"service-a: Camel application running on Spring Boot, acting as a simple HTTP proxy (for the purposes of this proof of concept)."
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,Zipkin support provided by the camel-zipkin module.
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,Route:
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,service-b: Spring Boot application with a REST controller.
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,Zipkin support provided by the spring-cloud-starter-kubernetes-zipkin module from Spring Cloud.
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,"When I make a request to service-a, I see part of the trace in Zipkin: I see the client request from service-a, and I see the server request in service-b, as well as the spans I've added there to instrument various parts of the request-path."
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,"However, I don't see the server request from the Camel portion, including the additional two seconds caused by the delay I've put in the route."
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,"Tracing the camel-zipkin code, I've realized that the server request will only be traced if there is already a trace ID header, due to this line:
 https://github.com/apache/camel/blob/c6c02ff92a536e78f7ed1b9dd550d6531e852cee/components/camel-zipkin/src/main/java/org/apache/camel/zipkin/ZipkinTracer.java#L753"
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,"With this knowledge, I am able to get the entire trace as expected if I manually provide my own tracing headers (X-B3-TraceId, X-B3-Sampled, and X-B3-SpanId)."
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,"However, I would like to be able to start a trace even if the client doesn't specify one."
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,"Based on my reading of the camel-zipkin code, I think I can create a PR that will induce my desired behavior."
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,"Before I do that, though, I want to verify a couple of things:"
Zipkin,48010973,nan,0,"2017/12/28, 18:29:44",False,"2017/12/28, 18:29:44",nan,2879390.0,406.0,0,1159,Tracing entire route in Camel using Zipkin only works if trace headers are provided by the client,Thanks!
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,I have a client application with multiple channels as SOURCE/SINK.
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,I want to send logs to Zipkin server.
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,"According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP."
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,At client side:
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,Q1.
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,Is there an automatic configuration for zipkin rabbit binding in such scenario?
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,"If not, what is default channel name of zipkin SOURCE channel?"
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,Q2.
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,Do I need to configure defaultSampler to AlwaysSampler()?
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,At Server side:
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,Q1.
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,"Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using:
 wget -O zipkin.jar 'https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec' 
...as stated on  https://zipkin.io/pages/quickstart.html  ?"
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,Q2.
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,How do I configure zipkin SINK channel to destination?
Zipkin,47660900,47664992.0,1,"2017/12/05, 21:05:32",True,"2017/12/06, 02:38:32",nan,6319505.0,3.0,0,332,Zipkin stream server and client configuration where client is using multiple input/output channels,"Spring boot version: 1.5.9.RELEASE
Spring cloud version: Edgware.RELEASE"
Zipkin,47655299,nan,1,"2017/12/05, 15:53:27",False,"2017/12/06, 03:58:02","2017/12/05, 16:05:49",811659.0,593.0,0,604,Is it possible to send x-request-id back when using istio with zipkin for distributed tracing?,"Following the link  Istio/Distributed tracing , I can get the tracing working with zipkin."
Zipkin,47655299,nan,1,"2017/12/05, 15:53:27",False,"2017/12/06, 03:58:02","2017/12/05, 16:05:49",811659.0,593.0,0,604,Is it possible to send x-request-id back when using istio with zipkin for distributed tracing?,"Currently in order for the client/caller to know about the x-request-id (in case no id is sent, zipkin creates one), he 
needs to send it as a part of the request."
Zipkin,47655299,nan,1,"2017/12/05, 15:53:27",False,"2017/12/06, 03:58:02","2017/12/05, 16:05:49",811659.0,593.0,0,604,Is it possible to send x-request-id back when using istio with zipkin for distributed tracing?,This gives him the ability of trace the request.
Zipkin,47655299,nan,1,"2017/12/05, 15:53:27",False,"2017/12/06, 03:58:02","2017/12/05, 16:05:49",811659.0,593.0,0,604,Is it possible to send x-request-id back when using istio with zipkin for distributed tracing?,All works well.
Zipkin,47655299,nan,1,"2017/12/05, 15:53:27",False,"2017/12/06, 03:58:02","2017/12/05, 16:05:49",811659.0,593.0,0,604,Is it possible to send x-request-id back when using istio with zipkin for distributed tracing?,"However, I am thinking maybe it is not a good idea for the client to send the x-request-id to avoid issues of constraints/duplication."
Zipkin,47655299,nan,1,"2017/12/05, 15:53:27",False,"2017/12/06, 03:58:02","2017/12/05, 16:05:49",811659.0,593.0,0,604,Is it possible to send x-request-id back when using istio with zipkin for distributed tracing?,"It would be good if it is possible that at the istio level, one should be able to modify the response headers and send the x-request-id back."
Zipkin,47655299,nan,1,"2017/12/05, 15:53:27",False,"2017/12/06, 03:58:02","2017/12/05, 16:05:49",811659.0,593.0,0,604,Is it possible to send x-request-id back when using istio with zipkin for distributed tracing?,I am not finding such capabilities for istio at present.
Zipkin,47655299,nan,1,"2017/12/05, 15:53:27",False,"2017/12/06, 03:58:02","2017/12/05, 16:05:49",811659.0,593.0,0,604,Is it possible to send x-request-id back when using istio with zipkin for distributed tracing?,"If there is a way to achieve this, please let me know."
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,When I use:
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,The error log is:
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,"But when I delete the dependency, it runs normally."
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,"I can't find the reason, I don't know why it's going to be a problem."
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,this is my fegin interface
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,I'm using the feign interface  like this
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,this is my parent pom.xml config
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,"I tried to modify the POM file, like:"
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,still error.
Zipkin,47591972,nan,1,"2017/12/01, 13:00:59",False,"2017/12/02, 11:41:07","2017/12/02, 11:41:07",9038571.0,1.0,0,424,unable use spring cloud zipkin client,How can I solve this?
Zipkin,47026664,nan,2,"2017/10/31, 03:36:35",True,"2020/10/22, 13:48:08","2017/10/31, 03:58:46",451079.0,651.0,0,659,Spring sleuth zipkin spans not nesting in Kotlin coroutine,I can see spans being recorded in the Zipkin UI from the following Spring Boot controller code:
Zipkin,47026664,nan,2,"2017/10/31, 03:36:35",True,"2020/10/22, 13:48:08","2017/10/31, 03:58:46",451079.0,651.0,0,659,Spring sleuth zipkin spans not nesting in Kotlin coroutine,and the log output looks like:
Zipkin,47026664,nan,2,"2017/10/31, 03:36:35",True,"2020/10/22, 13:48:08","2017/10/31, 03:58:46",451079.0,651.0,0,659,Spring sleuth zipkin spans not nesting in Kotlin coroutine,but the traces are independent in the UI.
Zipkin,47026664,nan,2,"2017/10/31, 03:36:35",True,"2020/10/22, 13:48:08","2017/10/31, 03:58:46",451079.0,651.0,0,659,Spring sleuth zipkin spans not nesting in Kotlin coroutine,I would expect the two calls to Google and Facebook urls to be nested concurrently under the call to the  /concurrent1  endpoint.
Zipkin,47026664,nan,2,"2017/10/31, 03:36:35",True,"2020/10/22, 13:48:08","2017/10/31, 03:58:46",451079.0,651.0,0,659,Spring sleuth zipkin spans not nesting in Kotlin coroutine,I suspect that it's due to the thread that coroutines are executed on being different to the one in which the spring application is started but I have no idea how to move forward with Spring Sleuth at this point!
Zipkin,46432583,nan,1,"2017/09/26, 20:32:08",False,"2017/09/27, 21:22:45","2017/09/26, 20:41:01",1813696.0,2661.0,0,659,Spring cloud Zipkin server data retention period,I am using Spring cloud Zipkin to trace calls with sample percentage 0.4.
Zipkin,46432583,nan,1,"2017/09/26, 20:32:08",False,"2017/09/27, 21:22:45","2017/09/26, 20:41:01",1813696.0,2661.0,0,659,Spring cloud Zipkin server data retention period,I am not using any persistent storage like MySQL or Cassandra.
Zipkin,46432583,nan,1,"2017/09/26, 20:32:08",False,"2017/09/27, 21:22:45","2017/09/26, 20:41:01",1813696.0,2661.0,0,659,Spring cloud Zipkin server data retention period,Could you please let me know how to set data retention period in Zipkin server e.g.
Zipkin,46432583,nan,1,"2017/09/26, 20:32:08",False,"2017/09/27, 21:22:45","2017/09/26, 20:41:01",1813696.0,2661.0,0,659,Spring cloud Zipkin server data retention period,I want to check only 6 hours/1 day data.
Zipkin,46432583,nan,1,"2017/09/26, 20:32:08",False,"2017/09/27, 21:22:45","2017/09/26, 20:41:01",1813696.0,2661.0,0,659,Spring cloud Zipkin server data retention period,Or if I can set max span count
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,everyone
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,I am trying to use Zipkin to trace services in OpenStack.
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,I know it is a huge project for me.
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,So I wonder if there is an open source library for Zipkin tracing OpenStack.
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"I think I searched it before and if my mind does not cheat me, there is one presentation (only slices) for this."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"However, I can not find it."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,Can someone help with it?
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"I know there is the library, osprofiler, for tracing OpenStack, while the example of API seems unclear to me."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"Could you please give me a more detailed or even a complete example, maybe like Zipkin  https://github.com/openzipkin/pyramid_zipkin-example"
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,I do not mean it is not helpful.
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"It seems I still have to find the RESTful request point in OpenStack, for example creating an instance may trigger one service to request neutron for networking, and I may have to locate the front end code and add a tracing code."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"If using py_zipkin, I can add decorator @zipkin_span(some params) before it."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"The problem is it is tough for me to find the front end of these services like Nova, neutron, cinder and so on."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,It seems osprofiler does the same thing.
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"My understanding is highly likely wrong, and I appreciate who can help with it."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"By the way, I do not intend to trace a big project like OpenStack."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,I intend to trace a RESTful-like or RPC system with Zipkin to collect the information to analyze.
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"Unfortunately, I have found a middle-size open source project."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,So I choose OpenStack.
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,"If you could provide me something else, that will be very helpful."
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,:)
Zipkin,46164976,47120862.0,1,"2017/09/12, 01:05:52",True,"2017/11/05, 13:08:17","2017/09/12, 04:16:01",7058789.0,65.0,0,499,zipkin tracing openstack and osprofiler example,Thank you very much.
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,I've been roaming the depths of the internet but I find myself unsatisfied by the examples I've found so far.
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,"Can someone point me or, show me, a good starting point to integrate zipkin tracing with jaxrs clients and amqp clients?"
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,My scenario is quite simple and I'd expect this task to be trivial tbh.
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,We have a micro services based architecture and it's time we start tracing our requests and have global perspective of our inter service dependencies and what the requests actually look like (we do have metrics but I want more!)
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,.
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,The communication is done via jax-rs auto generated clients and we use rabbit template for messaging.
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,I've seen brave integrations with jaxrs but they are a bit simplistic.
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,"My zipkin server is a spring boot mini app using stream-rabbit, so zipkin data is sent using rabbitmq."
Zipkin,45174992,45568932.0,1,"2017/07/18, 21:58:27",True,"2017/08/08, 15:53:02","2017/07/18, 22:10:56",4107097.0,45.0,0,265,Jax-rs and amqp zipkin integration,Thanks in advance.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,I am raising this query as a result of seeing another query with no satisfactory answer (and reading the advice to not ask another question in an answer or comment).
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,That reference is  Enabling Sleuth slows requests down (a lot)
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,My issue is similar.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,I am not using Feign.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,I am using the following:
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,"I am using spring-cloud-sleuth, logback and zipkin."
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,When i remove the zipkin pom reference
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,the performance is very quick.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,"But when I put it back, the performance is very poor."
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,Changing my log level from  INFO  to  DEBUG  in the logging changes the non zipkin calls from 701ms to 1051ms.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,Adding zipkin http changes that timing from 1051ms to around 53 seconds.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,"In the code, i make one call to my service which in turn makes 303 calls to an arango database (via its REST interface)."
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,I am using spring.cloud.starter-sleuth and logstash-logback-encoder.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,"By themselves there is no performance issue, it is only when i add the spring-cloud-starter-zipkin that the performance degrades."
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,"I am running zipkin on my local pc in a docker instance, and running my service out of eclipse as a standalone app (spring boot)."
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,Arango is on my local pc running as a service.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,"Unit test methods of my controller (using mockito to mock the mvc calls) that usually take 0.3 to 0.5 seconds each without zipkin, end up taking ~16seconds each when zipkin  is  enabled."
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,The Zipkin UI reports all the calls and the sum of the calls is the roughly the full time of the call as reported by Postman.
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,The Application class looks like this at the top:
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,The end of the log file looks like the following without the zipkin reference:
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,And with the zipkin reference it looks like this:
Zipkin,45046700,nan,1,"2017/07/12, 04:07:23",False,"2017/07/12, 07:41:40","2017/07/12, 06:35:40",8292569.0,1.0,0,842,When i add Zipkin to Sleuth the performance slows by 8200%,some logs removed to fit in the 30k character limit
Zipkin,44476445,nan,0,"2017/06/10, 21:00:45",False,"2017/06/10, 21:00:45",nan,2931493.0,1.0,0,823,spring-cloud-sleuth-error-posting-spans-to-zipkin server,I have zipkin server running and when i hit the zipkin client rest api end point i am getting the error below
Zipkin,44476445,nan,0,"2017/06/10, 21:00:45",False,"2017/06/10, 21:00:45",nan,2931493.0,1.0,0,823,spring-cloud-sleuth-error-posting-spans-to-zipkin server,"2017-06-10 23:16:08.782  INFO [product,d650504f4f922a51,d650504f4f922a51,true] 5676 --- [ix-ProductAPI-3] com.accenture.api.ProductAPI             : List all Product
Hibernate: select product0_.prod_id as prod_id1_0_, product0_.prod_description as prod_des2_0_ from product product0_
2017-06-10 23:16:09.801  WARN [product,,,] 5676 --- [ender@698c7814)] z.r.AsyncReporter$BoundedAsyncReporter   : Dropped 1 spans due to HttpClientErrorException(404 null)"
Zipkin,44476445,nan,0,"2017/06/10, 21:00:45",False,"2017/06/10, 21:00:45",nan,2931493.0,1.0,0,823,spring-cloud-sleuth-error-posting-spans-to-zipkin server,"org.springframework.web.client.HttpClientErrorException: 404 null
    at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:63) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:700) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:653) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:628) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:590) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.cloud.sleuth.zipkin.RestTemplateSender.post(RestTemplateSender.java:73) ~[spring-cloud-sleuth-zipkin-1.2.0.RELEASE.jar:1.2.0.RELEASE]
    at org.springframework.cloud.sleuth.zipkin.RestTemplateSender.sendSpans(RestTemplateSender.java:46) ~[spring-cloud-sleuth-zipkin-1.2.0.RELEASE.jar:1.2.0.RELEASE]
    at zipkin.reporter.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:228) [zipkin-reporter-0.6.12.jar:na]
    at zipkin.reporter.AsyncReporter$Builder.lambda$build$0(AsyncReporter.java:153) [zipkin-reporter-0.6.12.jar:na]
    at zipkin.reporter.AsyncReporter$Builder$$Lambda$1.run(Unknown Source) [zipkin-reporter-0.6.12.jar:na]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]"
Zipkin,44382633,nan,2,"2017/06/06, 09:17:13",False,"2017/06/07, 07:58:45","2017/06/07, 07:19:14",7608262.0,1.0,0,211,Cannot see trace details in zipkin,"we are using elasticsearch as the storage of zipkin, the dependency are as follows:"
Zipkin,44382633,nan,2,"2017/06/06, 09:17:13",False,"2017/06/07, 07:58:45","2017/06/07, 07:19:14",7608262.0,1.0,0,211,Cannot see trace details in zipkin,"now, trace log was storaged in elasticsearch, and we can see trance spans in the zipkin ui,but cannot to see span details or find trace via traceid,with the exception of :"
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,"I am trying to create a tracer, then a span from the tracer."
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,Performing work.
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,Then closing the span.
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,"I would expect that the close action will call spanReporter.report, which should post the data to an available Zipkin server (default localhost)"
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,"However, that is not happening."
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,Code below.
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,I noticed that I was using NeverSampler.
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,Changed it to AlwaysSampler (for now).
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,I was also using NoOpSpanReporter as the default SpanReporter (which does nothing).
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,I want to change it to a ZipkinSpanReporter.
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,(Or something else).
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,This is where I am stuck.
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,Questions:
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,Now the code snippet which uses the above:
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,"Note: 
This particular project does not have any http calls in any of its services."
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,Its a data processing project.
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,@SpringBootApplication has not been used.
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,(All zipkin examples use it though).
Zipkin,44147933,nan,1,"2017/05/24, 05:25:56",False,"2017/05/27, 05:19:34","2017/05/27, 05:19:34",7983801.0,1.0,0,633,How to setup a sleuth span to enable reporting to Zipkin whenever it is closed,Is it necessary?
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Hi  Hoping somebody can help am trying to get a very basic implementation of zipkin working to get to grips with distributed tracing.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,I am using the spring boot to do this but cannot seem to get it to work.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Nothing appears in the zipkin UI when I try to find traces for a my service.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,I have got 2 deployments as follows:
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,My spring boot app which I am wanting to log:
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,pom.xml
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,With these dependencies I do get a connect error because of the rabbit mq dependency.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,I had to include this becuase I got a META-INF/spring binders error.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Wasnt really sure how to get around this other than putting the dependency in.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,My application.class
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,When I run this aplication and call my endpoint I can see through the logging that it should be sending it to zipkin.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,This are my full logs.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Again I get the rabbitmq exception but not sure why i actually need this.
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Can zipkin not work without it
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,My full logs:
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,The 2nd application I deployed is my zipkin client / UI
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Pom .xml
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,My application.class
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,application.properties
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Running the zipkin server it starts up ok but nothing is shown in the trace logs excpet for an error
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,When I go to the client I do get this error though
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Any help is appreciated here
Zipkin,44027882,nan,2,"2017/05/17, 17:36:06",True,"2018/10/12, 12:19:31",nan,1107753.0,1396.0,0,6637,Zipkin not showing trace logs,Thanks in advance
Zipkin,43913962,nan,1,"2017/05/11, 14:21:25",True,"2017/11/22, 11:49:20",nan,5413604.0,1.0,0,454,How to replace storage from MySQL to Elasticsearch to deploy Zipkin Kubernetes,I am using the Kubernetes to deploy and trace data from application using zipkin.
Zipkin,43913962,nan,1,"2017/05/11, 14:21:25",True,"2017/11/22, 11:49:20",nan,5413604.0,1.0,0,454,How to replace storage from MySQL to Elasticsearch to deploy Zipkin Kubernetes,I am facing issue in replacing MySQL with Elasticsearch since I am not able to get the idea.
Zipkin,43913962,nan,1,"2017/05/11, 14:21:25",True,"2017/11/22, 11:49:20",nan,5413604.0,1.0,0,454,How to replace storage from MySQL to Elasticsearch to deploy Zipkin Kubernetes,"Even the replacement is done on command line basis, using STORAGE_TYPE=""Elasticsearch"" but how that can be done through kubernetes?"
Zipkin,43913962,nan,1,"2017/05/11, 14:21:25",True,"2017/11/22, 11:49:20",nan,5413604.0,1.0,0,454,How to replace storage from MySQL to Elasticsearch to deploy Zipkin Kubernetes,I am able to run the container from docker imgaes but is there any way to replace through deployment?
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,I have several services.
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,I am instrumenting them using Zipkin.
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,"In each module, in build.gradle is added a dependency to Zipkin:"
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,"In each module, in application.properties file are following settings:"
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,"I call a specific endpoint that use other 3 modules, in total are 4 modules."
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,Entire setup is on my laptop.
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,I realized that Zipkin introduces a lot of overhead.
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,I used Mozilla to compare the results.
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,The small values are when Zipkin does not record the requests and the big value is when Zipkin records.
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,"
Do you have any idea why there are so much overhead?"
Zipkin,43782954,nan,1,"2017/05/04, 15:29:27",True,"2019/04/18, 07:22:52",nan,4939505.0,275.0,0,480,Zipkin - Distributed tracing,Thank you.
Zipkin,43151368,43153293.0,1,"2017/04/01, 03:14:58",True,"2017/04/01, 08:46:41","2017/04/01, 04:10:10",560043.0,1450.0,0,124,Should I modify zipkin service libraries to pass generic feature flags?,We're looking to implement Zipkin in our stack.
Zipkin,43151368,43153293.0,1,"2017/04/01, 03:14:58",True,"2017/04/01, 08:46:41","2017/04/01, 04:10:10",560043.0,1450.0,0,124,Should I modify zipkin service libraries to pass generic feature flags?,As I look into Zipkin it makes sense to me to extend the Zipkin system to handle generic flags as well.
Zipkin,43151368,43153293.0,1,"2017/04/01, 03:14:58",True,"2017/04/01, 08:46:41","2017/04/01, 04:10:10",560043.0,1450.0,0,124,Should I modify zipkin service libraries to pass generic feature flags?,Observations:
Zipkin,43151368,43153293.0,1,"2017/04/01, 03:14:58",True,"2017/04/01, 08:46:41","2017/04/01, 04:10:10",560043.0,1450.0,0,124,Should I modify zipkin service libraries to pass generic feature flags?,Conclusion:
Zipkin,43095404,43106901.0,1,"2017/03/29, 16:47:50",True,"2017/03/30, 04:24:13","2017/03/29, 16:56:46",7785911.0,35.0,0,1338,Zipkin with elasticsearch storage (QueryParsingException),I have a question regarding Zipkin with elasticsearch storage.
Zipkin,43095404,43106901.0,1,"2017/03/29, 16:47:50",True,"2017/03/30, 04:24:13","2017/03/29, 16:56:46",7785911.0,35.0,0,1338,Zipkin with elasticsearch storage (QueryParsingException),After updating spring-cloud-sleuth to  1.1.1.RELEASE (because we updated spring boot from 1.3.8 to 1.4.4 and spring cloud from Brixton.SR6 to Camden.SR4) we also updated  zipkin-storage-elasticsearch  and  zipkin-autoconfigure-storage-elasticsearch  to version  1.16.2  in the pom.xml of our zipkin service.
Zipkin,43095404,43106901.0,1,"2017/03/29, 16:47:50",True,"2017/03/30, 04:24:13","2017/03/29, 16:56:46",7785911.0,35.0,0,1338,Zipkin with elasticsearch storage (QueryParsingException),We are using elasticsearch version  2.4.1 .
Zipkin,43095404,43106901.0,1,"2017/03/29, 16:47:50",True,"2017/03/30, 04:24:13","2017/03/29, 16:56:46",7785911.0,35.0,0,1338,Zipkin with elasticsearch storage (QueryParsingException),"When we boot up the service we get an 
 error on zipkin's ui  and a stacktrace:"
Zipkin,43095404,43106901.0,1,"2017/03/29, 16:47:50",True,"2017/03/30, 04:24:13","2017/03/29, 16:56:46",7785911.0,35.0,0,1338,Zipkin with elasticsearch storage (QueryParsingException),"Zipkin was working with spring-cloud-sleuth  1.0.0.RELEASE , zipkin-storage-elasticsearch, zipkin-autoconfigure-storage-elasticsearch  1.7.0  and elasticsearch  2.3.5 ."
Zipkin,43095404,43106901.0,1,"2017/03/29, 16:47:50",True,"2017/03/30, 04:24:13","2017/03/29, 16:56:46",7785911.0,35.0,0,1338,Zipkin with elasticsearch storage (QueryParsingException),What am I missing here?
Zipkin,43095404,43106901.0,1,"2017/03/29, 16:47:50",True,"2017/03/30, 04:24:13","2017/03/29, 16:56:46",7785911.0,35.0,0,1338,Zipkin with elasticsearch storage (QueryParsingException),Which versions should work together?
Zipkin,42884672,nan,1,"2017/03/19, 10:52:28",False,"2017/03/20, 12:55:54",nan,4939505.0,275.0,0,78,Can Zipkin be used for console application,Can be Zipkin used to instrument a console/classic application?
Zipkin,42884672,nan,1,"2017/03/19, 10:52:28",False,"2017/03/20, 12:55:54",nan,4939505.0,275.0,0,78,Can Zipkin be used for console application,I mean I have a method foo() and I want to know how much time it took.
Zipkin,42884672,nan,1,"2017/03/19, 10:52:28",False,"2017/03/20, 12:55:54",nan,4939505.0,275.0,0,78,Can Zipkin be used for console application,Does Zipkin can be used just for applications that communicates over http protocol?
Zipkin,42884672,nan,1,"2017/03/19, 10:52:28",False,"2017/03/20, 12:55:54",nan,4939505.0,275.0,0,78,Can Zipkin be used for console application,Thanks
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,I am facing an issue where in the ZipKin UI is failing to load in the traces from MySQL.
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,It is giving me below mentioned error on UI -
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,Error executing query: SQL [select distinct  zipkin_spans .
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,"trace_id 
  from  zipkin_spans  join  zipkin_annotations  on
  ( zipkin_spans ."
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,trace_id  =  zipkin_annotations .
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,"trace_id  and
   zipkin_spans ."
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,id  =  zipkin_annotations .
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,"span_id ) where
  ( zipkin_spans ."
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,start_ts  between ?
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,and ?
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,"and
   zipkin_annotations ."
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,endpoint_service_name  = ?)
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,"order by
   zipkin_spans ."
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,start_ts  desc limit ?
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,"]; Expression #1 of ORDER BY
  clause is not in SELECT list, references column
  'zipkin.zipkin_spans.start_ts' which is not in SELECT list; this is
  incompatible with DISTINCT"
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,I see below exception on ZipKin Server -
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,My ZipKin Server Configuration is a below -
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,"When I query the MySQL schema, I can see the record being populated in ""zipkin.zipkin_spans and zipkin.zipkin_annotations"" table."
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,"But when I try to load the Zipkin UI, it give me above error on UI."
Zipkin,40954585,41798515.0,1,"2016/12/04, 03:55:57",True,"2017/01/23, 04:38:41","2016/12/04, 05:21:48",240928.0,347.0,0,487,ZipKin UI Fail To Load Trace From MySQL,Any help is highly appreciated.
Zipkin,39600581,39601988.0,1,"2016/09/20, 20:46:19",True,"2016/09/20, 22:06:46",nan,831553.0,1076.0,0,553,How to Customize the Zipkin message publisher?,I have created a spring-boot application which publishes zipkin logs to a zipkin consumer.
Zipkin,39600581,39601988.0,1,"2016/09/20, 20:46:19",True,"2016/09/20, 22:06:46",nan,831553.0,1076.0,0,553,How to Customize the Zipkin message publisher?,But the Zipkin consumer(another spring boot application) is behind some authentication filters which check for several parameters/headers in the request before allowing.
Zipkin,39600581,39601988.0,1,"2016/09/20, 20:46:19",True,"2016/09/20, 22:06:46",nan,831553.0,1076.0,0,553,How to Customize the Zipkin message publisher?,How to I use my own custom HttpClient to publish my messages from the producer in this case?
Zipkin,38527466,nan,1,"2016/07/22, 16:16:16",True,"2016/07/23, 06:47:14",nan,5382693.0,75.0,0,1067,Zipkin - Is there any more informtaion about creating spans and traces in Java,"I would like to create my own basic, minimalistic library used for distributed tracing with Zipkin."
Zipkin,38527466,nan,1,"2016/07/22, 16:16:16",True,"2016/07/23, 06:47:14",nan,5382693.0,75.0,0,1067,Zipkin - Is there any more informtaion about creating spans and traces in Java,I will be sending traces via HTTP and nothing more fancy.
Zipkin,38527466,nan,1,"2016/07/22, 16:16:16",True,"2016/07/23, 06:47:14",nan,5382693.0,75.0,0,1067,Zipkin - Is there any more informtaion about creating spans and traces in Java,My question is if there is any more information about this topic than in the Zipkin docs and the source code of Zipkin and Brave?
Zipkin,38527466,nan,1,"2016/07/22, 16:16:16",True,"2016/07/23, 06:47:14",nan,5382693.0,75.0,0,1067,Zipkin - Is there any more informtaion about creating spans and traces in Java,I would like not to rely on Spring Framework.
Zipkin,38527466,nan,1,"2016/07/22, 16:16:16",True,"2016/07/23, 06:47:14",nan,5382693.0,75.0,0,1067,Zipkin - Is there any more informtaion about creating spans and traces in Java,"Spring Cloud Sleuth works very well, but my services are not built using Spring."
Zipkin,38527466,nan,1,"2016/07/22, 16:16:16",True,"2016/07/23, 06:47:14",nan,5382693.0,75.0,0,1067,Zipkin - Is there any more informtaion about creating spans and traces in Java,Do you know any resources?
Zipkin,38527466,nan,1,"2016/07/22, 16:16:16",True,"2016/07/23, 06:47:14",nan,5382693.0,75.0,0,1067,Zipkin - Is there any more informtaion about creating spans and traces in Java,Or do you have any ideas where to start doing this?
Zipkin,37757200,nan,1,"2016/06/11, 00:09:48",True,"2016/07/26, 21:58:32","2016/06/11, 02:04:30",3491416.0,13.0,0,273,Zipkin is not nesting parent trace to its child,We are using finagle stack and taught of adding zipkin for tracing our micro-services.
Zipkin,37757200,nan,1,"2016/06/11, 00:09:48",True,"2016/07/26, 21:58:32","2016/06/11, 02:04:30",3491416.0,13.0,0,273,Zipkin is not nesting parent trace to its child,I am able to see our tracing happening but parent finishes before the child.
Zipkin,37757200,nan,1,"2016/06/11, 00:09:48",True,"2016/07/26, 21:58:32","2016/06/11, 02:04:30",3491416.0,13.0,0,273,Zipkin is not nesting parent trace to its child,"I have already opened an issue here: 
 https://github.com/openzipkin/docker-zipkin/issues/100"
Zipkin,37757200,nan,1,"2016/06/11, 00:09:48",True,"2016/07/26, 21:58:32","2016/06/11, 02:04:30",3491416.0,13.0,0,273,Zipkin is not nesting parent trace to its child,Any help would be really appreciated.
Zipkin,37459261,nan,2,"2016/05/26, 14:14:09",True,"2019/09/20, 13:20:39","2016/05/27, 13:42:58",269830.0,307.0,0,1588,Can&#39;t view any &#39;dependencies&#39; inside zipkin UI dependencies tab,"I do have several services interacting with each other, and all of them, sending traces to openzipkin (  https://github.com/openzipkin/docker-zipkin  )."
Zipkin,37459261,nan,2,"2016/05/26, 14:14:09",True,"2019/09/20, 13:20:39","2016/05/27, 13:42:58",269830.0,307.0,0,1588,Can&#39;t view any &#39;dependencies&#39; inside zipkin UI dependencies tab,"While i can see the system behaviour in detail , looks like the 'dependencies' tab does not display anything at all."
Zipkin,37459261,nan,2,"2016/05/26, 14:14:09",True,"2019/09/20, 13:20:39","2016/05/27, 13:42:58",269830.0,307.0,0,1588,Can&#39;t view any &#39;dependencies&#39; inside zipkin UI dependencies tab,"The trace i check has 6 services, 21 spans and 43 spans, and i believe something should appear."
Zipkin,37459261,nan,2,"2016/05/26, 14:14:09",True,"2019/09/20, 13:20:39","2016/05/27, 13:42:58",269830.0,307.0,0,1588,Can&#39;t view any &#39;dependencies&#39; inside zipkin UI dependencies tab,"I'm using latest ( 1.40.1 ) docker-zipkin, with cassandra as storage, and 
just connecting to the cassandra instance, can see there's no entry in the dependencies 'table'."
Zipkin,37459261,nan,2,"2016/05/26, 14:14:09",True,"2019/09/20, 13:20:39","2016/05/27, 13:42:58",269830.0,307.0,0,1588,Can&#39;t view any &#39;dependencies&#39; inside zipkin UI dependencies tab,why ?
Zipkin,37459261,nan,2,"2016/05/26, 14:14:09",True,"2019/09/20, 13:20:39","2016/05/27, 13:42:58",269830.0,307.0,0,1588,Can&#39;t view any &#39;dependencies&#39; inside zipkin UI dependencies tab,Thanks
Zipkin,21551629,21582785.0,1,"2014/02/04, 13:47:16",True,"2014/02/08, 10:12:19","2014/02/04, 16:52:36",801051.0,3.0,0,733,Issue while installing twitter zipkin,"I am trying to install  zipkin , after following the steps given ( https://github.com/twitter/zipkin/blob/master/doc/install.md ), when I access  http://localhost:8080/  on the web browser, instead of the zipkin UI, it gives,"
Zipkin,21551629,21582785.0,1,"2014/02/04, 13:47:16",True,"2014/02/08, 10:12:19","2014/02/04, 16:52:36",801051.0,3.0,0,733,Issue while installing twitter zipkin,and gives out an error on the query terminal.
Zipkin,21551629,21582785.0,1,"2014/02/04, 13:47:16",True,"2014/02/08, 10:12:19","2014/02/04, 16:52:36",801051.0,3.0,0,733,Issue while installing twitter zipkin,Can please anyone help me how to resolve this ?
Zipkin,21551629,21582785.0,1,"2014/02/04, 13:47:16",True,"2014/02/08, 10:12:19","2014/02/04, 16:52:36",801051.0,3.0,0,733,Issue while installing twitter zipkin,?
Zipkin,66540810,nan,0,"2021/03/09, 06:28:20",False,"2021/03/09, 06:28:20",nan,15358162.0,1.0,0,13,How to create a child span in zipkin in node js?,I am working in the server side of an application and I am getting the tracer id in the form of X-b3 headers from the client side in the request object.
Zipkin,66540810,nan,0,"2021/03/09, 06:28:20",False,"2021/03/09, 06:28:20",nan,15358162.0,1.0,0,13,How to create a child span in zipkin in node js?,Now how can I create a child span for that tracer using these x-b3 headers?
Zipkin,61514326,61514513.0,1,"2020/04/30, 04:16:09",True,"2020/04/30, 04:37:29",nan,12418803.0,159.0,0,99,"What is the best practice to organize kubernetes tools (ELK, zipkin,..) by namespaces?","I have some tools running in my kubernetes cluster (ELK, zipkin,..) and i want to know in which namespace to place them, for example i have fluentd which is a daemonset running in kube-system namespace so should i place elasticsearch in the same namespace or put them together in a custom namespace so they can reach each other, i just want to know what is the best practice to do it"
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin","Im trying to implement distributed tracing using Spring, RabbitMQ, Sleuth and Zipkin."
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",So I added the dependencies:
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",And configured  sleuth  and  zipkin  in my  bootstrap.yml :
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",So starting my services and making some rest calls I get this in the logs:
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",For now it looks good.
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",Sleuth added the tracing ID's.
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",Calling the Zipkin UI I can see that the service names where added:
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",But there are no tracing informations at all:
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",So im wondering what im missing in my configuration.
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",EDIT
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",Turned out there are tracing informations arriving in zipkin.
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",I can use the search bar in the top right corner to search for tracing id's directly:
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",I will then get:
Zipkin,60971627,nan,0,"2020/04/01, 16:09:45",False,"2020/04/01, 19:54:39","2020/04/01, 19:54:39",2391849.0,8135.0,0,241,"Spring, RabbitMQ, Sleuth and Zipkin: No tracings available in Zipkin",So the question is why is there nothing in the overview or queryable via the trace lookup?
Zipkin,60318902,nan,0,"2020/02/20, 13:27:33",False,"2020/02/20, 13:27:33",nan,1842036.0,1049.0,0,49,Disabling Istio/Zipkin interceptor for a single request?,We are using Istio/Zipkin as a tracing system on our server to add the dynamic headers through Istio sidecar proxy for tracing the request later on using Zipkin.
Zipkin,60318902,nan,0,"2020/02/20, 13:27:33",False,"2020/02/20, 13:27:33",nan,1842036.0,1049.0,0,49,Disabling Istio/Zipkin interceptor for a single request?,Is there anyway we could disable istio for a certain request.
Zipkin,60318902,nan,0,"2020/02/20, 13:27:33",False,"2020/02/20, 13:27:33",nan,1842036.0,1049.0,0,49,Disabling Istio/Zipkin interceptor for a single request?,"Problem is, we are working with JMS queues, and when JMS listener tries to listen to a certain queue, it sees the headers like x-request-id added by the istio dynamically and it gives an error (as it'll accept header keys only in camelCase or with underScore or $)."
Zipkin,60318902,nan,0,"2020/02/20, 13:27:33",False,"2020/02/20, 13:27:33",nan,1842036.0,1049.0,0,49,Disabling Istio/Zipkin interceptor for a single request?,"We can change header keys added by istio, so we want either istio to not to add headers in some specific requests (the one queue is making)."
Zipkin,60318902,nan,0,"2020/02/20, 13:27:33",False,"2020/02/20, 13:27:33",nan,1842036.0,1049.0,0,49,Disabling Istio/Zipkin interceptor for a single request?,I've searched google but couldn't find anything about it.
Zipkin,60318902,nan,0,"2020/02/20, 13:27:33",False,"2020/02/20, 13:27:33",nan,1842036.0,1049.0,0,49,Disabling Istio/Zipkin interceptor for a single request?,Following is the error message we are getting:
Zipkin,57143690,nan,1,"2019/07/22, 13:10:19",True,"2019/07/25, 14:23:59","2019/07/22, 13:17:17",4762878.0,9.0,0,153,Show Traces in Red colour for Bad Requests in Zipkin UI,I am using spring Boot Version 1.5.14.RELEASE with spring cloud sleuth zipkin.
Zipkin,57143690,nan,1,"2019/07/22, 13:10:19",True,"2019/07/25, 14:23:59","2019/07/22, 13:17:17",4762878.0,9.0,0,153,Show Traces in Red colour for Bad Requests in Zipkin UI,If I return a ResponseEntity setting its HttpStatus as BAD_REQUEST then I see the trace highlighted in Blue color.
Zipkin,57143690,nan,1,"2019/07/22, 13:10:19",True,"2019/07/25, 14:23:59","2019/07/22, 13:17:17",4762878.0,9.0,0,153,Show Traces in Red colour for Bad Requests in Zipkin UI,Is there a way to highlight the trace in Red color for a bad request with ResponseEntity object?
Zipkin,57143690,nan,1,"2019/07/22, 13:10:19",True,"2019/07/25, 14:23:59","2019/07/22, 13:17:17",4762878.0,9.0,0,153,Show Traces in Red colour for Bad Requests in Zipkin UI,I explicitly threw a custom Exception for bad requests and saw the zipkin trace highlighted in Red color in Zipkin UI.
Zipkin,57143690,nan,1,"2019/07/22, 13:10:19",True,"2019/07/25, 14:23:59","2019/07/22, 13:17:17",4762878.0,9.0,0,153,Show Traces in Red colour for Bad Requests in Zipkin UI,But I don't want to do this as I am returning a body in ResponseEntity.
Zipkin,57143690,nan,1,"2019/07/22, 13:10:19",True,"2019/07/25, 14:23:59","2019/07/22, 13:17:17",4762878.0,9.0,0,153,Show Traces in Red colour for Bad Requests in Zipkin UI,I expect the Zipkin trace to be highlighted in Red color as it is a bad request but the actual color is Blue.
Zipkin,57143690,nan,1,"2019/07/22, 13:10:19",True,"2019/07/25, 14:23:59","2019/07/22, 13:17:17",4762878.0,9.0,0,153,Show Traces in Red colour for Bad Requests in Zipkin UI,Actual Trace
Zipkin,57143690,nan,1,"2019/07/22, 13:10:19",True,"2019/07/25, 14:23:59","2019/07/22, 13:17:17",4762878.0,9.0,0,153,Show Traces in Red colour for Bad Requests in Zipkin UI,Expected Trace
Zipkin,50990531,nan,0,"2018/06/22, 17:48:32",False,"2018/06/23, 10:47:58","2018/06/23, 10:47:58",9978831.0,81.0,0,672,spring-cloud-sleuth with Zipkin with Kafka,I'm using spring-cloud's sleuth with zipkin with kafka.
Zipkin,50990531,nan,0,"2018/06/22, 17:48:32",False,"2018/06/23, 10:47:58","2018/06/23, 10:47:58",9978831.0,81.0,0,672,spring-cloud-sleuth with Zipkin with Kafka,here is my pom.xml configure.
Zipkin,50990531,nan,0,"2018/06/22, 17:48:32",False,"2018/06/23, 10:47:58","2018/06/23, 10:47:58",9978831.0,81.0,0,672,spring-cloud-sleuth with Zipkin with Kafka,"and ,the blew is my application.yaml:"
Zipkin,50990531,nan,0,"2018/06/22, 17:48:32",False,"2018/06/23, 10:47:58","2018/06/23, 10:47:58",9978831.0,81.0,0,672,spring-cloud-sleuth with Zipkin with Kafka,"and then, I start the spring boot application,try to access by browsers,but I can't find the spring cloud send sleuth  to kafka."
Zipkin,50990531,nan,0,"2018/06/22, 17:48:32",False,"2018/06/23, 10:47:58","2018/06/23, 10:47:58",9978831.0,81.0,0,672,spring-cloud-sleuth with Zipkin with Kafka,"So I try to debug the zipkin2.reporter.AsyncReporter, and find that every time the [flush] method return when go to line 265 which is ""if (!bundler.isReady() &amp;&amp; !closed.get()) return;""  the code show as below."
Zipkin,50990531,nan,0,"2018/06/22, 17:48:32",False,"2018/06/23, 10:47:58","2018/06/23, 10:47:58",9978831.0,81.0,0,672,spring-cloud-sleuth with Zipkin with Kafka,I know it means bufferFull is false.but why every time it is not bufferFull even if I try invoke the URL frequently?
Zipkin,50990531,nan,0,"2018/06/22, 17:48:32",False,"2018/06/23, 10:47:58","2018/06/23, 10:47:58",9978831.0,81.0,0,672,spring-cloud-sleuth with Zipkin with Kafka,thank you for attention.
Zipkin,48668426,nan,1,"2018/02/07, 18:14:50",False,"2018/02/13, 13:23:09",nan,4255878.0,681.0,0,296,How to integrate Zipkin tracing to thrift microservices,i know this is a very general question but i simply don't know how to start.
Zipkin,48668426,nan,1,"2018/02/07, 18:14:50",False,"2018/02/13, 13:23:09",nan,4255878.0,681.0,0,296,How to integrate Zipkin tracing to thrift microservices,I have spring-boot applications which serve a thrift API via HTTP.
Zipkin,48668426,nan,1,"2018/02/07, 18:14:50",False,"2018/02/13, 13:23:09",nan,4255878.0,681.0,0,296,How to integrate Zipkin tracing to thrift microservices,The same or another spring-boot app is using the thrift-client of another application to communicate.
Zipkin,48668426,nan,1,"2018/02/07, 18:14:50",False,"2018/02/13, 13:23:09",nan,4255878.0,681.0,0,296,How to integrate Zipkin tracing to thrift microservices,my goal is to trace the communication path with zipkin.
Zipkin,48668426,nan,1,"2018/02/07, 18:14:50",False,"2018/02/13, 13:23:09",nan,4255878.0,681.0,0,296,How to integrate Zipkin tracing to thrift microservices,"i could imagine, i need to somehow intercept incoming and outcoming http-calls with the application-type x-thrift but simply have no idea how to do this and properly integrate with zipkin libraries."
Zipkin,48668426,nan,1,"2018/02/07, 18:14:50",False,"2018/02/13, 13:23:09",nan,4255878.0,681.0,0,296,How to integrate Zipkin tracing to thrift microservices,"any hint how to start on this is highly appreciated, thanks a lot in advance"
Zipkin,58889547,nan,1,"2019/11/16, 11:59:23",False,"2019/11/16, 12:16:29",nan,12382349.0,1.0,-1,17,Spring-Cloud-Zipkin runs incorrectly about &#39;Exception in thread &quot;main&quot; java.lang.ClassNotFoundException&#39;,"I am new in spring-cloud.My projects are about spring-cloud-config, spring-cloud-eureka, spring-cloud-zipkin,I am running projects locally and it is normal.When I put my projects in Ubuntu, the project 'spring-cloud-zipkin' runs incorrectly."
Zipkin,58889547,nan,1,"2019/11/16, 11:59:23",False,"2019/11/16, 12:16:29",nan,12382349.0,1.0,-1,17,Spring-Cloud-Zipkin runs incorrectly about &#39;Exception in thread &quot;main&quot; java.lang.ClassNotFoundException&#39;,"The error is about 'Exception in thread ""main"" java.lang.ClassNotFoundException', thanks for your answer very much."
Zipkin,58889547,nan,1,"2019/11/16, 11:59:23",False,"2019/11/16, 12:16:29",nan,12382349.0,1.0,-1,17,Spring-Cloud-Zipkin runs incorrectly about &#39;Exception in thread &quot;main&quot; java.lang.ClassNotFoundException&#39;,enter image description here
Zipkin,58889547,nan,1,"2019/11/16, 11:59:23",False,"2019/11/16, 12:16:29",nan,12382349.0,1.0,-1,17,Spring-Cloud-Zipkin runs incorrectly about &#39;Exception in thread &quot;main&quot; java.lang.ClassNotFoundException&#39;,and my code:
Zipkin,58889547,nan,1,"2019/11/16, 11:59:23",False,"2019/11/16, 12:16:29",nan,12382349.0,1.0,-1,17,Spring-Cloud-Zipkin runs incorrectly about &#39;Exception in thread &quot;main&quot; java.lang.ClassNotFoundException&#39;,enter image description here
Zipkin,58889547,nan,1,"2019/11/16, 11:59:23",False,"2019/11/16, 12:16:29",nan,12382349.0,1.0,-1,17,Spring-Cloud-Zipkin runs incorrectly about &#39;Exception in thread &quot;main&quot; java.lang.ClassNotFoundException&#39;,enter image description here
Zipkin,58889547,nan,1,"2019/11/16, 11:59:23",False,"2019/11/16, 12:16:29",nan,12382349.0,1.0,-1,17,Spring-Cloud-Zipkin runs incorrectly about &#39;Exception in thread &quot;main&quot; java.lang.ClassNotFoundException&#39;,"in the pom.xml:
 enter image description here"
Zipkin,53692437,53694198.0,2,"2018/12/09, 14:46:15",True,"2018/12/13, 18:20:56","2018/12/13, 18:20:56",4927642.0,29.0,-1,1068,Spans not being sent to zipkin,I need to use Zipkn Serve to trace my spring boot application.Here is my configurations of application.yml
Zipkin,53692437,53694198.0,2,"2018/12/09, 14:46:15",True,"2018/12/13, 18:20:56","2018/12/13, 18:20:56",4927642.0,29.0,-1,1068,Spans not being sent to zipkin,But the spans not being created in Zipkin.I have added all the required dependencies to my service's pom file.
Zipkin,53692437,53694198.0,2,"2018/12/09, 14:46:15",True,"2018/12/13, 18:20:56","2018/12/13, 18:20:56",4927642.0,29.0,-1,1068,Spans not being sent to zipkin,and the zipkin service's pom file.
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),I am using Zipkin Slueth with Spring boot.
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),"Now my zipkin is working fine in normal case but when I spawn 3 new threads from main thread, it generate different traces and not 1 trace."
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),So i am unable to see the complete request.
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),Same starts working if I do everything in main thread?
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),My Pom for including dpendencies
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),Properties
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),My spring cloud version is Dalton.SR5
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),So slueth sends traces to zipkin auto.
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),This is all I have configured for zipkin.
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),Can i use rxjava schedules hook?
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),How?
Zipkin,50836516,nan,1,"2018/06/13, 14:49:17",False,"2018/06/13, 15:31:21","2018/06/13, 15:28:32",1826788.0,1408.0,-1,108,Zipkin Slueth generate different traces for same different thread(in Same request),I am unable to use it?
Zipkin,50267380,nan,1,"2018/05/10, 10:03:36",True,"2018/05/10, 17:30:58","2020/06/20, 12:12:55",9065156.0,5.0,-1,502,use Zipkin+ElasticSearch+Sleuth+rabbitMQ can&#39;t find &quot;services&quot;,"SpringCloud version:Dalston.SR1,
rabbitMQ version:3.6.10,ElasticSearch version:6.2.4"
Zipkin,50267380,nan,1,"2018/05/10, 10:03:36",True,"2018/05/10, 17:30:58","2020/06/20, 12:12:55",9065156.0,5.0,-1,502,use Zipkin+ElasticSearch+Sleuth+rabbitMQ can&#39;t find &quot;services&quot;,There was nothing unusual when I use MySQL as a storage.
Zipkin,50267380,nan,1,"2018/05/10, 10:03:36",True,"2018/05/10, 17:30:58","2020/06/20, 12:12:55",9065156.0,5.0,-1,502,use Zipkin+ElasticSearch+Sleuth+rabbitMQ can&#39;t find &quot;services&quot;,Now I use ElasticSearch.I can't find any services.
Zipkin,50267380,nan,1,"2018/05/10, 10:03:36",True,"2018/05/10, 17:30:58","2020/06/20, 12:12:55",9065156.0,5.0,-1,502,use Zipkin+ElasticSearch+Sleuth+rabbitMQ can&#39;t find &quot;services&quot;,I lost something?
Zipkin,50267380,nan,1,"2018/05/10, 10:03:36",True,"2018/05/10, 17:30:58","2020/06/20, 12:12:55",9065156.0,5.0,-1,502,use Zipkin+ElasticSearch+Sleuth+rabbitMQ can&#39;t find &quot;services&quot;,here is the picture:
Zipkin,50267380,nan,1,"2018/05/10, 10:03:36",True,"2018/05/10, 17:30:58","2020/06/20, 12:12:55",9065156.0,5.0,-1,502,use Zipkin+ElasticSearch+Sleuth+rabbitMQ can&#39;t find &quot;services&quot;,application.properties
Zipkin,50267380,nan,1,"2018/05/10, 10:03:36",True,"2018/05/10, 17:30:58","2020/06/20, 12:12:55",9065156.0,5.0,-1,502,use Zipkin+ElasticSearch+Sleuth+rabbitMQ can&#39;t find &quot;services&quot;,pom.xml
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,zipkin is a tool for tracing request as well as tracking the span of time a service took to process the request useful in multi-service projects it doesnt require much effort for setting up u just have to add zipkin dependency in your services and define a sampler bean.
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,add the following dependency in project
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,"compile group: 'org.springframework.cloud', name: 'spring-cloud-starter-zipkin', version: '1.3.2.RELEASE'"
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,add sampler bean inside ur project
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,`
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,add above bean when u want only fraction of ur requests traces to send to zipkin else define a bean
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,"add 
 spring.zipkin.base-url=localhost:9411  in ur properties file and host the zipkin server on the same port defined above."
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,but if u r using api-gateway for accessing zipkin (in case of deplyment in cloud ) or inside proxy u may face the issue of broken ui elements when accessing thru gateway in this case im  using zuul with propertis as:
Zipkin,50203215,50203216.0,1,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",nan,6816012.0,93.0,-1,511,hosting zipkin inside proxy/zuul-gateway,"zuul.routes.zipkin.path=/zipkin/*
zuul.routes.zipkin.url=http://localhost:9411"
Zipkin,46092526,nan,1,"2017/09/07, 12:17:50",False,"2017/09/07, 18:30:01","2017/09/07, 18:30:01",8573624.0,1.0,-3,117,After I add spring-cloud-sleuth-zipkin-stream into pom.xml. The app can start.But i can&#39;t invoke my controller,First I want to integrate  zipkin  +  rabbitmq  into my project.
Zipkin,46092526,nan,1,"2017/09/07, 12:17:50",False,"2017/09/07, 18:30:01","2017/09/07, 18:30:01",8573624.0,1.0,-3,117,After I add spring-cloud-sleuth-zipkin-stream into pom.xml. The app can start.But i can&#39;t invoke my controller,So my  pom.xml  is below:
Zipkin,46092526,nan,1,"2017/09/07, 12:17:50",False,"2017/09/07, 18:30:01","2017/09/07, 18:30:01",8573624.0,1.0,-3,117,After I add spring-cloud-sleuth-zipkin-stream into pom.xml. The app can start.But i can&#39;t invoke my controller,So after I add this.
Zipkin,46092526,nan,1,"2017/09/07, 12:17:50",False,"2017/09/07, 18:30:01","2017/09/07, 18:30:01",8573624.0,1.0,-3,117,After I add spring-cloud-sleuth-zipkin-stream into pom.xml. The app can start.But i can&#39;t invoke my controller,I can't not invoke my controller.
Zipkin,46092526,nan,1,"2017/09/07, 12:17:50",False,"2017/09/07, 18:30:01","2017/09/07, 18:30:01",8573624.0,1.0,-3,117,After I add spring-cloud-sleuth-zipkin-stream into pom.xml. The app can start.But i can&#39;t invoke my controller,"But if the controller
in the same package with the Application, can the controller be invoked?"
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,"I'm following  this  guide, with Zipkin."
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,"I have 3 microservices involed,  A -&gt; B -&gt; C , I'm propagating headers from A to B and from B to C.
But in the Zipkin dashboard I only see entries for  A -&gt; B  and  B -&gt; C , not  A -&gt; B -&gt; C ."
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,Those are the headers:
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,"I can see that in B  x-b3-parentspanid  is null and I guess that's wrong, but the other are working I think...how is it possible?"
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,"EDIT:
added code snippets to show headers propagation"
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,A -&gt; B  propagation:
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,...
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,B -&gt; C  propagation:
Zipkin,48857181,nan,2,"2018/02/19, 00:13:48",False,"2020/12/04, 19:17:32","2018/02/19, 11:34:48",2535024.0,3499.0,11,596,Istio Distributed Tracing shows just 1 span,...
Zipkin,60190181,60392406.0,3,"2020/02/12, 16:06:26",True,"2020/02/25, 12:26:42","2020/02/21, 03:44:52",1842036.0,1049.0,9,511,"JMS message listener invoker failed, Cause: Identifier contains invalid JMS identifier character &#39;-&#39;: &#39;x-request-id&#39;",I'm working with JMS and queues (Azure queues) for the first time.
Zipkin,60190181,60392406.0,3,"2020/02/12, 16:06:26",True,"2020/02/25, 12:26:42","2020/02/21, 03:44:52",1842036.0,1049.0,9,511,"JMS message listener invoker failed, Cause: Identifier contains invalid JMS identifier character &#39;-&#39;: &#39;x-request-id&#39;",I'm required to make a queue where Rubi server would write some data and Java would read it from queue and will do further executions.
Zipkin,60190181,60392406.0,3,"2020/02/12, 16:06:26",True,"2020/02/25, 12:26:42","2020/02/21, 03:44:52",1842036.0,1049.0,9,511,"JMS message listener invoker failed, Cause: Identifier contains invalid JMS identifier character &#39;-&#39;: &#39;x-request-id&#39;",This process is working fine locally on my machine.
Zipkin,60190181,60392406.0,3,"2020/02/12, 16:06:26",True,"2020/02/25, 12:26:42","2020/02/21, 03:44:52",1842036.0,1049.0,9,511,"JMS message listener invoker failed, Cause: Identifier contains invalid JMS identifier character &#39;-&#39;: &#39;x-request-id&#39;","I've created a REST endpoint which is writing data in the queue and once data is written in the queue, the listener would take over and read the data and execute."
Zipkin,60190181,60392406.0,3,"2020/02/12, 16:06:26",True,"2020/02/25, 12:26:42","2020/02/21, 03:44:52",1842036.0,1049.0,9,511,"JMS message listener invoker failed, Cause: Identifier contains invalid JMS identifier character &#39;-&#39;: &#39;x-request-id&#39;",When we deploy it to Azure the error I can see in logs which is not letting the Queues start is
Zipkin,60190181,60392406.0,3,"2020/02/12, 16:06:26",True,"2020/02/25, 12:26:42","2020/02/21, 03:44:52",1842036.0,1049.0,9,511,"JMS message listener invoker failed, Cause: Identifier contains invalid JMS identifier character &#39;-&#39;: &#39;x-request-id&#39;",Zipkin is also present on the Azure server as a distributed tracing system and I guess this  x-request-id  is related to Zipkin which is creating the problem.
Zipkin,60190181,60392406.0,3,"2020/02/12, 16:06:26",True,"2020/02/25, 12:26:42","2020/02/21, 03:44:52",1842036.0,1049.0,9,511,"JMS message listener invoker failed, Cause: Identifier contains invalid JMS identifier character &#39;-&#39;: &#39;x-request-id&#39;",I've searched Google for the issue but couldn't understand why its happening.
Zipkin,60190181,60392406.0,3,"2020/02/12, 16:06:26",True,"2020/02/25, 12:26:42","2020/02/21, 03:44:52",1842036.0,1049.0,9,511,"JMS message listener invoker failed, Cause: Identifier contains invalid JMS identifier character &#39;-&#39;: &#39;x-request-id&#39;",Following is detailed error message:
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"Spring Boot Cloud Disovery Question,  Problem with Eureka hostname after docker upgrade on windows 10."
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"(Note: docker is not hosting spring services, just mariadb, rabbitmq, and zipkin)"
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,Summary
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"Everything worked fine until the docker update today, after the docker upgrade"
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"Eureka returns  ""host.docker.internal""    as the hostname for my development box (machine hosting the spring boot cloud services)"
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,This has worked fine until the docker updgrade on windows 10 today.
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,Any guidance on this one?
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,------------------------------ Details ----------------------------
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,""""
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,---------------- Versions of spring ----------
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,I am using windows 10 enterprise for java development.
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"I use docker-compose to host mariadb, zipkin, and rabbitmq in my dev env on my windows 10 box"
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,I have a multi-project gradle build with 8 spring boot cloud services
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,One of the services is a spring cloud discovery service hosting Eureke
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,The other spring cloud services are eureka clients.
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"Until today, everything worked
1) Eureka spring boot cloud services are started first
2) Other spring boot cloud services that are clients of the eclipse startup, register and query the spring cloud discovery client code to obtain the URL of the other services"
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"Today,  The latest docker for windows 10 was pushed out, and I installed it (I have been developing this app through several other docker updates)."
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"I updated docker, did a reboot."
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"After the reboot,  The Eureka server is returning  ""host.docker.internal""  as the hostname in the URL  instead of  http:/mymachinename:8080"
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,"I don't have the network data before the upgrade, but now it is"
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,My client application.properties file is:
Zipkin,57319678,63283687.0,6,"2019/08/02, 06:50:26",True,"2021/03/12, 05:34:31","2019/08/02, 15:39:20",4522858.0,103.0,7,3629,spring boot cloud eurka windows 10 eurkea returns host.docker.internal for client host name after latest docker upgrade,Server application.property file
Zipkin,42982050,42982623.0,1,"2017/03/23, 18:40:07",True,"2017/03/23, 19:08:47",nan,6582043.0,403.0,7,1593,Spring cloud sleuth with Spring data jpa,We are trying to implement Spring cloud sleuth with Zipkin in our project and wanted to know if Spring cloud sleuth will support DB calls with Spring data JPA.
Zipkin,42982050,42982623.0,1,"2017/03/23, 18:40:07",True,"2017/03/23, 19:08:47",nan,6582043.0,403.0,7,1593,Spring cloud sleuth with Spring data jpa,I want to trace the time taken for DB calls just like service calls
Zipkin,42982050,42982623.0,1,"2017/03/23, 18:40:07",True,"2017/03/23, 19:08:47",nan,6582043.0,403.0,7,1593,Spring cloud sleuth with Spring data jpa,"When I make a service call with RestTemplate, that gets sent to zipkin and I am able to see that on the dashboard"
Zipkin,42982050,42982623.0,1,"2017/03/23, 18:40:07",True,"2017/03/23, 19:08:47",nan,6582043.0,403.0,7,1593,Spring cloud sleuth with Spring data jpa,But DB interactions with Spring data jpa is not getting displayed in Zipkin
Zipkin,44699269,nan,0,"2017/06/22, 15:20:25",False,"2017/06/22, 15:20:25",nan,8182054.0,61.0,6,754,Auto instrumentation like Spring Cloud Sleuth in Node.js,"While Zipkin sdk is available for Node.js, I'm looking for auto-instrumentation like Spring Cloud Sleuth in Node.js app."
Zipkin,44699269,nan,0,"2017/06/22, 15:20:25",False,"2017/06/22, 15:20:25",nan,8182054.0,61.0,6,754,Auto instrumentation like Spring Cloud Sleuth in Node.js,Is there a module or framework for it in Node.js?
Zipkin,44699269,nan,0,"2017/06/22, 15:20:25",False,"2017/06/22, 15:20:25",nan,8182054.0,61.0,6,754,Auto instrumentation like Spring Cloud Sleuth in Node.js,What I mean by auto-instrumentation above is that in Java I don't have to write code to instrument servlets/filters/rest clients with Zipkin.
Zipkin,44699269,nan,0,"2017/06/22, 15:20:25",False,"2017/06/22, 15:20:25",nan,8182054.0,61.0,6,754,Auto instrumentation like Spring Cloud Sleuth in Node.js,Sleuth automatically does that.
Zipkin,44699269,nan,0,"2017/06/22, 15:20:25",False,"2017/06/22, 15:20:25",nan,8182054.0,61.0,6,754,Auto instrumentation like Spring Cloud Sleuth in Node.js,While Zipkin instrumentation seems manual in Node.js.
Zipkin,51138208,nan,0,"2018/07/02, 17:14:14",False,"2018/07/02, 17:14:14",nan,10022052.0,51.0,5,1032,Append header in flask request,i am using python flask in my application.
Zipkin,51138208,nan,0,"2018/07/02, 17:14:14",False,"2018/07/02, 17:14:14",nan,10022052.0,51.0,5,1032,Append header in flask request,I want to change the header before each request in order to add information for zipkin distributed tracing.
Zipkin,51138208,nan,0,"2018/07/02, 17:14:14",False,"2018/07/02, 17:14:14",nan,10022052.0,51.0,5,1032,Append header in flask request,My current code looks like:
Zipkin,51138208,nan,0,"2018/07/02, 17:14:14",False,"2018/07/02, 17:14:14",nan,10022052.0,51.0,5,1032,Append header in flask request,"Unfortunately, this is not working as the method  append  does not exist."
Zipkin,51138208,nan,0,"2018/07/02, 17:14:14",False,"2018/07/02, 17:14:14",nan,10022052.0,51.0,5,1032,Append header in flask request,What is the right approach?
Zipkin,51138208,nan,0,"2018/07/02, 17:14:14",False,"2018/07/02, 17:14:14",nan,10022052.0,51.0,5,1032,Append header in flask request,"Best regards
Martin"
Zipkin,49211771,49212158.0,2,"2018/03/10, 19:11:09",True,"2018/03/10, 22:18:03",nan,33404.0,13548.0,5,1919,Spring Sleuth and Zipkin:Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT,I have a Spring Boot 2.0.0 REST service where I'm trying to enable Sleuth and Zipkin to send traces to my localhost Zipkin server.
Zipkin,49211771,49212158.0,2,"2018/03/10, 19:11:09",True,"2018/03/10, 22:18:03",nan,33404.0,13548.0,5,1919,Spring Sleuth and Zipkin:Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT,The app worked fine unti I add the two dependencies  spring-cloud-starter-sleuth  and  spring-cloud-sleuth-zipkin  to my pom.xml.
Zipkin,49211771,49212158.0,2,"2018/03/10, 19:11:09",True,"2018/03/10, 22:18:03",nan,33404.0,13548.0,5,1919,Spring Sleuth and Zipkin:Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT,"Once I did that, I'm now getting a compilation error:"
Zipkin,49211771,49212158.0,2,"2018/03/10, 19:11:09",True,"2018/03/10, 22:18:03",nan,33404.0,13548.0,5,1919,Spring Sleuth and Zipkin:Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT,Project build error: Non-resolvable import POM: Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT
Zipkin,49211771,49212158.0,2,"2018/03/10, 19:11:09",True,"2018/03/10, 22:18:03",nan,33404.0,13548.0,5,1919,Spring Sleuth and Zipkin:Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT,I've ensured it's not a corrupt Maven package issue by deleting my .m2 folder and updating (twice).
Zipkin,49211771,49212158.0,2,"2018/03/10, 19:11:09",True,"2018/03/10, 22:18:03",nan,33404.0,13548.0,5,1919,Spring Sleuth and Zipkin:Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT,Why am I getting this error and how can I fix it?
Zipkin,49211771,49212158.0,2,"2018/03/10, 19:11:09",True,"2018/03/10, 22:18:03",nan,33404.0,13548.0,5,1919,Spring Sleuth and Zipkin:Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT,This is my pom.xml:
Zipkin,45115368,nan,0,"2017/07/15, 09:50:29",False,"2017/07/15, 09:50:29",nan,1813696.0,2661.0,5,1043,Autoscaling of Spring cloud OSS microservices,"I have developed several micro services using spring cloud Netflix stack (Eureka, Zuul, Zipkin, config server etc.)"
Zipkin,45115368,nan,0,"2017/07/15, 09:50:29",False,"2017/07/15, 09:50:29",nan,1813696.0,2661.0,5,1043,Autoscaling of Spring cloud OSS microservices,Is there any open source / free solution available to spin up/down microservices instances.
Zipkin,45115368,nan,0,"2017/07/15, 09:50:29",False,"2017/07/15, 09:50:29",nan,1813696.0,2661.0,5,1043,Autoscaling of Spring cloud OSS microservices,e.g.
Zipkin,45115368,nan,0,"2017/07/15, 09:50:29",False,"2017/07/15, 09:50:29",nan,1813696.0,2661.0,5,1043,Autoscaling of Spring cloud OSS microservices,if CPU usage   90% for 3 consecutive checking it will add one more instance.
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,I try to add a distributed tracing in my microservices (under Kubernetes in Azure).
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,I added the dependencies in the parent pom.xml :
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,I use 1.4.1 and CAMDEN.SR4 because fabric8 kubeflix doesn't support newer versions.
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,I forced 1.1.3.RELEASE to try newest sleuth version to see if it was a bug in older version of sleuth.
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,I use this configuration of logback-spring.xml :
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,And here is my application.yml :
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,The zipkin URL is a Kubernetes services exposing the Zipkin server (Spring boot app with @EnableZipkinServer)
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,I then call a first service (services-1) with this code :
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,which produces these logs :
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,"As you can see it calls the services-i18n-2 service with a RestTemplate, which produces these logs :"
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,As you can see the traceId in service-2 (e0c6495a0a598cff) is different from the service-1 (eaf3dbcb2f92091b).
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,And in service-2 the traceId is the same as the spanId.
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,Questions :
Zipkin,43230619,43232907.0,1,"2017/04/05, 15:01:44",True,"2017/04/05, 16:39:23",nan,6921715.0,399.0,5,5235,Sleuth log traceId not propagated to another service,"FYI, I have Hystrix in the dependencies and I have removed the @HystrixCommand to be sure it was not a problem with Hystrix creating a new traceId at each HTTP call."
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",I have implemented a Distributed Transaction Logging library with Tree like Structure as mention in Google Dapper( http://research.google.com/pubs/pub36356.html ) and eBay CAL Transaction Logging Framework( http://devopsdotcom.files.wordpress.com/2012/11/screen-shot-2012-11-11-at-10-06-39-am.png ).
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",Log Format
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",GUID HEX NUMBER FORMAT
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",What I would like to do is to integrate this format with Kibana UI and when user want to search and click on on TRACE_GUID it will show something similar to Distributed CALL graph which show where the time was spent.
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",Here is UI  http://twitter.github.io/zipkin/ .
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",This will be great.
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",I am not UI developer if some can point me how to do this that will be great.
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch","Also I would like to know how I can index elastic search payload data so user specify some expression like in payload (duration   1000) then, Elastic Search will bring all the loglines that satisfy condition."
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch","Also, I would like to index Payload as Name=Value pair so user can query  (key3=value2 or key4 =  exception ) some sort of regular expression."
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",Please let me know if this can be achieved.
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch",Any help pointer would be great..
Zipkin,23032944,nan,1,"2014/04/12, 19:18:18",True,"2015/05/13, 05:49:26","2014/04/15, 18:24:33",828976.0,430.0,4,2588,"Distributed Tracing logging and Integrating with Logstash,Kibana and ElasticSearch","Thanks,
Bhavesh"
Zipkin,19032203,nan,1,"2013/09/26, 18:18:58",True,"2013/09/26, 20:56:50",nan,1526859.0,625.0,4,1143,Distributed tracing solution for Scala?,I'm going to design distributed system with Scala and Akka.
Zipkin,19032203,nan,1,"2013/09/26, 18:18:58",True,"2013/09/26, 20:56:50",nan,1526859.0,625.0,4,1143,Distributed tracing solution for Scala?,I want to aggregate tracing messages from a cluster and have possibility to view them in some kind of UI.
Zipkin,19032203,nan,1,"2013/09/26, 18:18:58",True,"2013/09/26, 20:56:50",nan,1526859.0,625.0,4,1143,Distributed tracing solution for Scala?,"Is Zipkin the best solution, or Flume(+some wrapper?"
Zipkin,19032203,nan,1,"2013/09/26, 18:18:58",True,"2013/09/26, 20:56:50",nan,1526859.0,625.0,4,1143,Distributed tracing solution for Scala?,"), or something else?"
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,We are setting up microservice framework.
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,We use following stack for distributed tracing.
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,Following is how the configuration is done
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,In  gradle.build  (or pom.xml) following starter dependencies added
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,Add one AlwaysSampler bean
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,"If we have  kafka  running, things work automatically."
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,"But if kafka is not running, server does not start - this is mostly the case for development environment."
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,"If I want to stop this, I have to comment out all the code mentioned here (as we use starter dependency with spring boot, it automatically configures as I understand)."
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,Can we just make some changes in properties (or yaml) files so that I don't need to go and comment out all these code?
Zipkin,56525260,nan,1,"2019/06/10, 14:11:00",True,"2019/06/10, 18:54:49","2019/06/10, 14:23:58",1534925.0,2846.0,3,3123,Disable distributed tracing for development,"Or probably another way to disable this without doing some commenting, etc."
Zipkin,49415050,nan,0,"2018/03/21, 21:28:09",False,"2018/06/20, 08:00:58","2018/06/20, 08:00:58",1826788.0,1408.0,3,86,slueth with rxjava2 creating multiple traces,I am using spring cloud slueth with zipkin in spring boot to trace the services calls.
Zipkin,49415050,nan,0,"2018/03/21, 21:28:09",False,"2018/06/20, 08:00:58","2018/06/20, 08:00:58",1826788.0,1408.0,3,86,slueth with rxjava2 creating multiple traces,My spring cloud version is Edgware.RELEASE
Zipkin,49415050,nan,0,"2018/03/21, 21:28:09",False,"2018/06/20, 08:00:58","2018/06/20, 08:00:58",1826788.0,1408.0,3,86,slueth with rxjava2 creating multiple traces,"Now I when I tried to trace my facade layer which uses rxjava, it creates 12 traces for a single request?"
Zipkin,49415050,nan,0,"2018/03/21, 21:28:09",False,"2018/06/20, 08:00:58","2018/06/20, 08:00:58",1826788.0,1408.0,3,86,slueth with rxjava2 creating multiple traces,What should I do ?
Zipkin,49415050,nan,0,"2018/03/21, 21:28:09",False,"2018/06/20, 08:00:58","2018/06/20, 08:00:58",1826788.0,1408.0,3,86,slueth with rxjava2 creating multiple traces,I mean i want just 1 trace should be generated for 1 request of facade layer(facade layer do parallel calls using rxjava).
Zipkin,49415050,nan,0,"2018/03/21, 21:28:09",False,"2018/06/20, 08:00:58","2018/06/20, 08:00:58",1826788.0,1408.0,3,86,slueth with rxjava2 creating multiple traces,I am using slueth with http and have not change any property if properties file
Zipkin,49415050,nan,0,"2018/03/21, 21:28:09",False,"2018/06/20, 08:00:58","2018/06/20, 08:00:58",1826788.0,1408.0,3,86,slueth with rxjava2 creating multiple traces,I have added these 2 dependencies:
Zipkin,49415050,nan,0,"2018/03/21, 21:28:09",False,"2018/06/20, 08:00:58","2018/06/20, 08:00:58",1826788.0,1408.0,3,86,slueth with rxjava2 creating multiple traces,This I have added in properties file:
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,i am trying to evaluate zipkin to enable distributed tracing capability for all our micro-service.
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,Below are versions in my setup.
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,Spring-boot version:  1.5.7.RELEASE
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,"spring-cloud version:
 Camden.SR6"
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,zipkin version :  2.2.1
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,Configuration for seluth in  application.properties
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,spring.sleuth.sampler.percentage=1.0
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,spring.sleuth.web.skipPattern=(^cleanup.
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,|.+favicon. )
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,And i created the ZipkinSpanReporter bean as below.
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,Note that I have setup the Eureka server as all micro services and even zipkin server registerred with Eureka server so that the Zipkin client can resolve zipkin server via eureka
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,What I have observered is that the zipkin client (book) is not reporting all spans back to zipkin server when I checked the zipkin.
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,"Some are reported, almost of spans are dropped"
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,I have enabled the logging for
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,below are logging info:
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,But I could not be able to find the traceId which is logged in  book.log  file from zipkin console
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,Could you please explain why many spans are not reported to zipkin server?
Zipkin,47008485,47139081.0,2,"2017/10/30, 07:15:24",True,"2017/11/06, 16:23:51","2020/06/20, 12:12:55",1190307.0,847.0,3,1705,ZipkinSpanReporter is not working properly,Thanks in advance.
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,"My application is a spring-rabbitmq based application(neither spring-cloud nor spring-boot), requests were received from one queue and sent responses to another queue."
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,I want to use brave to trace the system by injecting Zipkin headers before sending messages and extracting Zipkin headers right after receiving messages.
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,"The problem is  In step3 of the following scenario, how can I get span1 before sending message?"
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,Scenario:
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,Code snippet before sending message:
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,"In the above code,  Span currentSpan = tracer.currentSpan();  , the  currentSpan  is always  null ."
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,Code snippet after receiving message:
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,Brave configuration code:
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,Following are my references:
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,https://github.com/openzipkin/brave/tree/master/brave#one-way-tracing
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,https://github.com/openzipkin/brave/blob/master/brave/src/test/java/brave/features/async/OneWaySpanTest.java
Zipkin,44643259,44664880.0,1,"2017/06/20, 06:15:09",True,"2017/06/23, 08:31:36","2017/06/20, 10:32:28",nan,nan,3,1275,How to get currentSpan for rabbitmq application with brave (java)?,https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30
Zipkin,39870535,nan,1,"2016/10/05, 12:41:00",True,"2016/11/01, 13:16:38",nan,3987705.0,900.0,3,68,Finagle + Thrift: Count method invocations,"I’m working on a system of microservices, implemented in Scala with Finagle and Thrift as the platform."
Zipkin,39870535,nan,1,"2016/10/05, 12:41:00",True,"2016/11/01, 13:16:38",nan,3987705.0,900.0,3,68,Finagle + Thrift: Count method invocations,"As there are a few services that nobody touched for a while, I need to find out if they are used at all anymore (or rather, which parts are not used anymore)."
Zipkin,39870535,nan,1,"2016/10/05, 12:41:00",True,"2016/11/01, 13:16:38",nan,3987705.0,900.0,3,68,Finagle + Thrift: Count method invocations,"For that, IMHO a simple invocation count for each method would suffice (since the service was started, or possibly in the last 24h)."
Zipkin,39870535,nan,1,"2016/10/05, 12:41:00",True,"2016/11/01, 13:16:38",nan,3987705.0,900.0,3,68,Finagle + Thrift: Count method invocations,"As far as I see, the Finagle/Thrift integration does not bring something like this built-in, at least not exposed in the admin panel."
Zipkin,39870535,nan,1,"2016/10/05, 12:41:00",True,"2016/11/01, 13:16:38",nan,3987705.0,900.0,3,68,Finagle + Thrift: Count method invocations,So what would be the most clever way to do this?
Zipkin,39870535,nan,1,"2016/10/05, 12:41:00",True,"2016/11/01, 13:16:38",nan,3987705.0,900.0,3,68,Finagle + Thrift: Count method invocations,Just add a filter that counts the invocations and exposes them via the admin interface?
Zipkin,39870535,nan,1,"2016/10/05, 12:41:00",True,"2016/11/01, 13:16:38",nan,3987705.0,900.0,3,68,Finagle + Thrift: Count method invocations,Or would Zipkin (possibly with custom code) help here?
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,I have a web service written in scala and built on top of twitter finagle RPC system.
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,Now we are hitting some performance issues.
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,We have external API components and database layer.
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,I am planning of installing Zipkin in order to have a service level tracing system.
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,This will allow me to know where the bottleneck is at the service level.
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,I am wondering though if there are framework out there to monitor the performance inside my application layer.
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,The application is a suite of filters that are applied consecutively to my data and I would like to know which filter take time to compute.
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,I heard about JVM profiling but it seems a little overkill for what I want to do.
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,What would you recommend ?
Zipkin,19022191,nan,1,"2013/09/26, 10:41:25",True,"2013/09/26, 11:03:01",nan,1764933.0,728.0,3,1319,Application monitoring performance in scala,Thanks for your help.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",I'm making a sequential request using Feign Builder.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients","There are no x-b3-traceid,x-b3-spanid .. in the title of the request."
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",That's why the log my last client appears on the zipkin.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients","I use spring  boot 2.4.2 , spring cloud 2020.0.0 , feign-core 10.10.1 , feign-okhttp 10.10.1."
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",I have tried spring-cloud-openfeign and i achieved wanted result.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",But i don't want to use this lib.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",There are requests  when using Feign Builder and Rest Template in here.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",I dont' see same log at zipkin.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",My Client1 App.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",I am sending request http://localhost:8082/
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",This is yml of my client1 app.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",I use same yml conf on other client apps which client2 and clint3.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",Only changes port and app name.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",This is my Feign at Client2 app.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",Here impl of ClientFeign2.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",This is my Feign at Client3 app.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",Here impl of Client3 Feign.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",pom.xml from client3 and i use  client3 at client2/pom and the same as client1.
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",diff-feign-rest-image
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",feign-zipkin-img
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",feign-request-img
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",rstlet-rest-tmplte-img
Zipkin,66167917,66205972.0,1,"2021/02/12, 09:36:21",True,"2021/03/01, 10:16:15","2021/02/15, 11:00:33",6249887.0,23.0,2,165,"Using Feign builder requests doesn&#39;t send trace-id, span-id to child clients but using rest template is showing all headers on child clients",rest-zipkin-img
Zipkin,62870927,62871239.0,2,"2020/07/13, 10:16:01",True,"2020/07/13, 12:01:08","2020/07/13, 12:01:08",4969916.0,3982.0,2,566,Read pom dependency version from properties file,Trying to read version from properties file but getting below exception
Zipkin,62870927,62871239.0,2,"2020/07/13, 10:16:01",True,"2020/07/13, 12:01:08","2020/07/13, 12:01:08",4969916.0,3982.0,2,566,Read pom dependency version from properties file,Below is the  pom.xml  and  properties  file
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,I'm comparing different tracing backend using OpenCensus.
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,I already have the simple OpenCensus.io python samples running fine using Zipkin and Azure Monitor.
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,Now I'm trying to test using GCP's Stackdriver...
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,"I have set up the test code from Opencensus
 https://opencensus.io/exporters/supported-exporters/python/stackdriver/  as follows:"
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,I have set the environment variable for  GCP_PROJECT_ID  and also have my key file path for my service account JSON file set in  GOOGLE_APPLICATION_CREDENTIALS .
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,The service account has the  &quot;Cloud trace agent&quot;  role.
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,My code runs through with no errors but I can't see any info appearing in the GCP console under traces or in the monitoring dashboard.
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,Am I missing something?
Zipkin,62674846,nan,1,"2020/07/01, 13:32:00",True,"2020/07/06, 15:42:41","2020/07/01, 23:47:26",44281.0,1225.0,2,151,Opencensus Stackdriver traces from Python app not appearing in the Trace list in GCP,"Environment notes:
I'm testing this from my local Windows machine using Python 3.7.2"
Zipkin,60343965,nan,0,"2020/02/21, 19:54:25",False,"2020/02/21, 19:54:25",nan,6456586.0,2697.0,2,45,Add Spring Sleuth to Spring Oauth2&#39;s requests,We have problem with propagation of  traceId  in requests which are called by spring oauth2 module.
Zipkin,60343965,nan,0,"2020/02/21, 19:54:25",False,"2020/02/21, 19:54:25",nan,6456586.0,2697.0,2,45,Add Spring Sleuth to Spring Oauth2&#39;s requests,For instance consider authorization and resource server.
Zipkin,60343965,nan,0,"2020/02/21, 19:54:25",False,"2020/02/21, 19:54:25",nan,6456586.0,2697.0,2,45,Add Spring Sleuth to Spring Oauth2&#39;s requests,In resource server we have spring security configuration to ensure get rsa public key from authorization server with following property:
Zipkin,60343965,nan,0,"2020/02/21, 19:54:25",False,"2020/02/21, 19:54:25",nan,6456586.0,2697.0,2,45,Add Spring Sleuth to Spring Oauth2&#39;s requests,"When I call controller of resource server with jwt token, I can see in zipkin traces from resource server and authorization server as well, but there is no traceId propagation from resource server to authorization server."
Zipkin,60343965,nan,0,"2020/02/21, 19:54:25",False,"2020/02/21, 19:54:25",nan,6456586.0,2697.0,2,45,Add Spring Sleuth to Spring Oauth2&#39;s requests,"First record is calling rest api to get resources, and second record is produced call to authorization server to find out public RSA key."
Zipkin,59924890,nan,2,"2020/01/27, 05:37:21",False,"2020/01/30, 13:47:03",nan,12146243.0,61.0,2,178,Can envoy in istio trace external https api?,We use istio to use distributed tracing.
Zipkin,59924890,nan,2,"2020/01/27, 05:37:21",False,"2020/01/30, 13:47:03",nan,12146243.0,61.0,2,178,Can envoy in istio trace external https api?,"Our microservices sometimes need to hit external APIs, which usually communicate over https."
Zipkin,59924890,nan,2,"2020/01/27, 05:37:21",False,"2020/01/30, 13:47:03",nan,12146243.0,61.0,2,178,Can envoy in istio trace external https api?,"To measure the exact performance of the whole system, we want to trace the communication when hitting an external API."
Zipkin,59924890,nan,2,"2020/01/27, 05:37:21",False,"2020/01/30, 13:47:03",nan,12146243.0,61.0,2,178,Can envoy in istio trace external https api?,"However, distributed tracing requires access to the header of the request, but https does not allow access because the header is encrypted."
Zipkin,59924890,nan,2,"2020/01/27, 05:37:21",False,"2020/01/30, 13:47:03",nan,12146243.0,61.0,2,178,Can envoy in istio trace external https api?,"For confirmation, I deployed bookinfo on GKE with istio enabled, entered the productpage container of the productpage pod, and executed the following command."
Zipkin,59924890,nan,2,"2020/01/27, 05:37:21",False,"2020/01/30, 13:47:03",nan,12146243.0,61.0,2,178,Can envoy in istio trace external https api?,Only http communication was displayed on zipkin.
Zipkin,59924890,nan,2,"2020/01/27, 05:37:21",False,"2020/01/30, 13:47:03",nan,12146243.0,61.0,2,178,Can envoy in istio trace external https api?,"Is it possible to get a series of traces, including APIs that use external https?"
Zipkin,58562126,nan,3,"2019/10/25, 19:08:37",False,"2019/10/28, 14:35:42","2019/10/28, 10:53:43",4129450.0,211.0,2,574,Nginx ingress error 404 using host and path,"I'm using Kubernetes on Azure cloud, and I have installed zipkin."
Zipkin,58562126,nan,3,"2019/10/25, 19:08:37",False,"2019/10/28, 14:35:42","2019/10/28, 10:53:43",4129450.0,211.0,2,574,Nginx ingress error 404 using host and path,"I already install nginx ingress, and if I use the following host rule, it works fine:"
Zipkin,58562126,nan,3,"2019/10/25, 19:08:37",False,"2019/10/28, 14:35:42","2019/10/28, 10:53:43",4129450.0,211.0,2,574,Nginx ingress error 404 using host and path,But this is not what I want.
Zipkin,58562126,nan,3,"2019/10/25, 19:08:37",False,"2019/10/28, 14:35:42","2019/10/28, 10:53:43",4129450.0,211.0,2,574,Nginx ingress error 404 using host and path,What I want is something like hostname.com/zipkin.
Zipkin,58562126,nan,3,"2019/10/25, 19:08:37",False,"2019/10/28, 14:35:42","2019/10/28, 10:53:43",4129450.0,211.0,2,574,Nginx ingress error 404 using host and path,"I tried with this, but I got a 404 error:"
Zipkin,58562126,nan,3,"2019/10/25, 19:08:37",False,"2019/10/28, 14:35:42","2019/10/28, 10:53:43",4129450.0,211.0,2,574,Nginx ingress error 404 using host and path,What do I have to do?
Zipkin,58562126,nan,3,"2019/10/25, 19:08:37",False,"2019/10/28, 14:35:42","2019/10/28, 10:53:43",4129450.0,211.0,2,574,Nginx ingress error 404 using host and path,"Edit:
I tried to add the host and after doing a describe command i get this"
Zipkin,58562126,nan,3,"2019/10/25, 19:08:37",False,"2019/10/28, 14:35:42","2019/10/28, 10:53:43",4129450.0,211.0,2,574,Nginx ingress error 404 using host and path,"EDIT:
I solved my issue adding a rewrite rule annotation"
Zipkin,57856249,57856470.0,1,"2019/09/09, 17:40:16",True,"2019/09/10, 12:40:49",nan,4850212.0,123.0,2,36,What happens if two dependent java services have different &#39;probability&#39; value?,"I have two services(S1, S2) in a chain."
Zipkin,57856249,57856470.0,1,"2019/09/09, 17:40:16",True,"2019/09/10, 12:40:49",nan,4850212.0,123.0,2,36,What happens if two dependent java services have different &#39;probability&#39; value?,"I call with CURL(or Postman) S1, and S1 sends request to S2."
Zipkin,57856249,57856470.0,1,"2019/09/09, 17:40:16",True,"2019/09/10, 12:40:49",nan,4850212.0,123.0,2,36,What happens if two dependent java services have different &#39;probability&#39; value?,"S1 has -  spring.sleuth.sampler.probability: 0.1 
S2 has -   spring.sleuth.sampler.probability: 0.5"
Zipkin,57856249,57856470.0,1,"2019/09/09, 17:40:16",True,"2019/09/10, 12:40:49",nan,4850212.0,123.0,2,36,What happens if two dependent java services have different &#39;probability&#39; value?,I don't understand how the system will behave.
Zipkin,57856249,57856470.0,1,"2019/09/09, 17:40:16",True,"2019/09/10, 12:40:49",nan,4850212.0,123.0,2,36,What happens if two dependent java services have different &#39;probability&#39; value?,If i send 100 requests:
Zipkin,57856249,57856470.0,1,"2019/09/09, 17:40:16",True,"2019/09/10, 12:40:49",nan,4850212.0,123.0,2,36,What happens if two dependent java services have different &#39;probability&#39; value?,or
Zipkin,57856249,57856470.0,1,"2019/09/09, 17:40:16",True,"2019/09/10, 12:40:49",nan,4850212.0,123.0,2,36,What happens if two dependent java services have different &#39;probability&#39; value?,or
Zipkin,55575721,nan,1,"2019/04/08, 17:27:41",True,"2019/04/13, 04:59:36",nan,10593981.0,123.0,2,636,Nginx: how to generate X-B3-SpanId 16 random bytes for tracing,I'm trying to generate zipkin trace id from nginx in order to be able to trace from nginx to applications.
Zipkin,55575721,nan,1,"2019/04/08, 17:27:41",True,"2019/04/13, 04:59:36",nan,10593981.0,123.0,2,636,Nginx: how to generate X-B3-SpanId 16 random bytes for tracing,"To achieve this, I want to find out how to generate 16 random bytes to be used for X-B3-SpanId since $request_id generates 32 bytes (which can be used for X-B3-TraceId)."
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,"I am trying to deploy to ibm-cloud a node web application with Angular 2, node.js,express and mongo."
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,My app works fine without adding the socket.io require line:
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,"However, every time i try to add the socket.io and execute the container, the console logs (for each request):"
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,Error: Can't set headers after they are sent.
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,"at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
    at SendStream.send (/app/node_modules/send/index.js:618:10)
    at onstat (/app/node_modules/send/index.js:730:10)
    at FSReqWrap.oncomplete (fs.js:153:5)
  Error: Can't set headers after they are sent."
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,"at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
     at SendStream.send (/app/node_modules/send/index.js:618:10)
     at onstat (/app/node_modules/send/index.js:730:10)
     at FSReqWrap.oncomplete (fs.js:153:5)
  Error: Can't set headers after they are sent."
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,"at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
     at SendStream.send (/app/node_modules/send/index.js:618:10)
     at onstat (/app/node_modules/send/index.js:730:10)
     at FSReqWrap.oncomplete (fs.js:153:5)"
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,"This is my code
        // Uncomment following to enable zipkin tracing, tailor to fit your network configuration:
        // var appzip = require('appmetrics-zipkin')({
        //     host: 'localhost',
        //     port: 9411,
        //     serviceName:'frontend'
        // });"
Zipkin,55571957,59222522.0,1,"2019/04/08, 14:02:36",True,"2019/12/07, 05:27:09",nan,11328599.0,23.0,2,191,Socket.io is causing the error &#39;Error: Can&#39;t set headers after they are sent.&#39;,Thank you very much!.
Zipkin,54702777,nan,1,"2019/02/15, 06:41:26",True,"2019/03/15, 01:14:36","2019/03/15, 01:07:52",3534555.0,21.0,2,623,How to monitor AWS SQS with Graphite - Grafana,I have a bunch of micro-services hosted on AWS.
Zipkin,54702777,nan,1,"2019/02/15, 06:41:26",True,"2019/03/15, 01:14:36","2019/03/15, 01:07:52",3534555.0,21.0,2,623,How to monitor AWS SQS with Graphite - Grafana,"I am using StatsD, Graphite and Grafana to monitor them."
Zipkin,54702777,nan,1,"2019/02/15, 06:41:26",True,"2019/03/15, 01:14:36","2019/03/15, 01:07:52",3534555.0,21.0,2,623,How to monitor AWS SQS with Graphite - Grafana,Now I want to expand it to monitor the queues (SQS) through which these micro-services are talking to each other.
Zipkin,54702777,nan,1,"2019/02/15, 06:41:26",True,"2019/03/15, 01:14:36","2019/03/15, 01:07:52",3534555.0,21.0,2,623,How to monitor AWS SQS with Graphite - Grafana,How can I leverage Graphite/ Grafana to do this?
Zipkin,54702777,nan,1,"2019/02/15, 06:41:26",True,"2019/03/15, 01:14:36","2019/03/15, 01:07:52",3534555.0,21.0,2,623,How to monitor AWS SQS with Graphite - Grafana,Or a better approach if there aint any support/ plugin for the same.
Zipkin,54702777,nan,1,"2019/02/15, 06:41:26",True,"2019/03/15, 01:14:36","2019/03/15, 01:07:52",3534555.0,21.0,2,623,How to monitor AWS SQS with Graphite - Grafana,Thanks :)
Zipkin,54702777,nan,1,"2019/02/15, 06:41:26",True,"2019/03/15, 01:14:36","2019/03/15, 01:07:52",3534555.0,21.0,2,623,How to monitor AWS SQS with Graphite - Grafana,"PS : If it's gotta be Zipkin, please tell me they can co-exist or is there a catch to using multiple tracers."
Zipkin,53765439,nan,3,"2018/12/13, 17:45:50",True,"2019/03/22, 05:48:23",nan,7801709.0,183.0,2,2499,Error creating bean with name &#39;rabbitListenerContainerFactory&#39;,"I have a spring boot microservice: Zuul-api-gateway-server, and I am trying to implement a Zipkin server listening to rabbitmq for logging messages within the microservice."
Zipkin,53765439,nan,3,"2018/12/13, 17:45:50",True,"2019/03/22, 05:48:23",nan,7801709.0,183.0,2,2499,Error creating bean with name &#39;rabbitListenerContainerFactory&#39;,I have added the following dependencies to this microservice:
Zipkin,53765439,nan,3,"2018/12/13, 17:45:50",True,"2019/03/22, 05:48:23",nan,7801709.0,183.0,2,2499,Error creating bean with name &#39;rabbitListenerContainerFactory&#39;,I have started the Zipkin server using the following commands:
Zipkin,53765439,nan,3,"2018/12/13, 17:45:50",True,"2019/03/22, 05:48:23",nan,7801709.0,183.0,2,2499,Error creating bean with name &#39;rabbitListenerContainerFactory&#39;,"SET RABBIT_URI=amqp://localhost
java -jar zipkin.jar"
Zipkin,53765439,nan,3,"2018/12/13, 17:45:50",True,"2019/03/22, 05:48:23",nan,7801709.0,183.0,2,2499,Error creating bean with name &#39;rabbitListenerContainerFactory&#39;,I then try to start up the microservice however I get the following error:
Zipkin,53765439,nan,3,"2018/12/13, 17:45:50",True,"2019/03/22, 05:48:23",nan,7801709.0,183.0,2,2499,Error creating bean with name &#39;rabbitListenerContainerFactory&#39;,"org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitListenerContainerFactory' defined in class path resource [org/springframework/boot/autoconfigure/amqp/RabbitAnnotationDrivenConfiguration.class]: Initialization of bean failed; nested exception is java.lang.NullPointerException
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:584) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:846) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:863) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at com.shopping.sandbox.netflixzuulapigatewayserver.NetflixZuulApiGatewayServerApplication.main(NetflixZuulApiGatewayServerApplication.java:16) [classes/:na]
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_171]
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_171]
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_171]
      at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_171]
      at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) [spring-boot-devtools-2.1.1.RELEASE.jar:2.1.1.RELEASE]
  Caused by: java.lang.NullPointerException: null
      at org.springframework.amqp.rabbit.config.AbstractRabbitListenerContainerFactory.getAdviceChain(AbstractRabbitListenerContainerFactory.java:198) ~[spring-rabbit-2.1.2.RELEASE.jar:2.1.2.RELEASE]
      at brave.spring.rabbit.SpringRabbitTracing.decorateSimpleRabbitListenerContainerFactory(SpringRabbitTracing.java:170) ~[brave-instrumentation-spring-rabbit-5.4.4.jar:na]
      at org.springframework.cloud.sleuth.instrument.messaging.SleuthRabbitBeanPostProcessor.postProcessBeforeInitialization(TraceMessagingAutoConfiguration.java:186) ~[spring-cloud-sleuth-core-2.1.0.M2.jar:2.1.0.M2]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:419) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1737) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      ... 20 common frames omitted"
Zipkin,53611081,nan,0,"2018/12/04, 12:40:45",False,"2018/12/04, 12:40:45",nan,8479984.0,237.0,2,229,Setting a traceId with an existing value in Spring Sleuth,We have an application and where we are tracing the whole logs using a custom correlation header i.e.
Zipkin,53611081,nan,0,"2018/12/04, 12:40:45",False,"2018/12/04, 12:40:45",nan,8479984.0,237.0,2,229,Setting a traceId with an existing value in Spring Sleuth,"X-CID with a value as for example, 615b7eea-6d4c-4efb-9431-fcbba084ea3f."
Zipkin,53611081,nan,0,"2018/12/04, 12:40:45",False,"2018/12/04, 12:40:45",nan,8479984.0,237.0,2,229,Setting a traceId with an existing value in Spring Sleuth,Recently we have integrated the Spring Sleuth with the train version as Greenwich.BUILD-SNAPSHOT in our application and using brave's Span and tracer to create trace and span ad sending it to Zipkin.
Zipkin,53611081,nan,0,"2018/12/04, 12:40:45",False,"2018/12/04, 12:40:45",nan,8479984.0,237.0,2,229,Setting a traceId with an existing value in Spring Sleuth,What we are looking for a mechanism where we can use or set the CID value as a traceId rather than using the auto generated one from Sleuth.
Zipkin,53611081,nan,0,"2018/12/04, 12:40:45",False,"2018/12/04, 12:40:45",nan,8479984.0,237.0,2,229,Setting a traceId with an existing value in Spring Sleuth,"The rational behind is, we would like to make it uniform CID value to use or search in Zipkin UI rather than two different values i.e."
Zipkin,53611081,nan,0,"2018/12/04, 12:40:45",False,"2018/12/04, 12:40:45",nan,8479984.0,237.0,2,229,Setting a traceId with an existing value in Spring Sleuth,CID value and Spring Sleuth traceId to trace the complete API calls.
Zipkin,53611081,nan,0,"2018/12/04, 12:40:45",False,"2018/12/04, 12:40:45",nan,8479984.0,237.0,2,229,Setting a traceId with an existing value in Spring Sleuth,"Please note that, we would like to reuse the X-CID from request which is already supplied in the request header."
Zipkin,53611081,nan,0,"2018/12/04, 12:40:45",False,"2018/12/04, 12:40:45",nan,8479984.0,237.0,2,229,Setting a traceId with an existing value in Spring Sleuth,Are there any APIs to override this behavior or any alternate way to achieve this ?
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,TLDR :
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,More details :
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,I'm trying to create a custom Sleuth instrumenting service that can also work when no tracing is available.
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,Here is a really simplified version of the service interface :
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,"I then create an AutoConfiguration (as this code is in a separate module, used by many projects) :"
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,The idea is really to have a kind of Noop version of the service when no Tracer is available.
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,I then declare the the AutoConfiguration in a spring.factories file as usual.
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,This is the application I want to run :
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,"For all of this I want to use Zipkin, with a KafkaSpanReporter."
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,So my application.properties look like this :
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,and my (truncated) pom.xml like this :
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,When I try to run this code I get an error :
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,If I look at the configuration report I see :
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,"This is weird, because there is a  @EnableConfigurationProperties(KafkaProperties.class)  on  KafkaAutoConfiguration  and the configuration report clearly shows :"
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,"What is even weirder is that if I remove the  @AutoConfigureAfter(name = ""org.springframework.cloud.sleuth.autoconfig.TraceAutoConfiguration"")  in my AutoConfiguration, the service starts all right =  but I get the  NoTracedConfiguration  flavour of my bean, so  Tracer  is probably not configured yet."
Zipkin,52486463,nan,1,"2018/09/24, 22:54:50",False,"2018/11/22, 11:13:13",nan,7059.0,5434.0,2,1521,Missing bean &#39;zipkin2.reporter.Sender&#39; when using `@AutoConfigureAfter(TraceAutoConfiguration.class)`,What can I do to fix this problem ?
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,"I'm working on the microservices project that is the dockerized Spring Cloud Netflix project and contains 3 microservices except for some Netflix services which are turbine,zipkin,discovery,configserver etc , yet."
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,(Its just working on locally now..)
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,"Soon, I decided to deploy my project to a cloud provider with an orchestration tool."
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,"After some researches, I decided to use Kuberenetes."
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,"But, both of Spring Cloud Netflix and Kubernetes have some solutions for distributed systems: service discovery, load balancing, fault-tolerance, etc.."
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,"In that case, using Netflix libs."
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,seem unnecessary with Kubernetes.
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,I read  this  and  this .
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,I think Spring Cloud Kubernetes looks like a workaround solution.
Zipkin,51931755,nan,1,"2018/08/20, 16:25:27",True,"2018/08/20, 23:37:41","2018/08/20, 21:19:22",3572215.0,83.0,2,242,Deploying dockerized Spring Cloud Netflix project to Kubernetes,So my questions are :
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,I'm experiencing issues scaling my app with multiple requests.
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,"Each request sends an ask to an actor, which then spawns other actors."
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,"This is fine, however, under load(5+ asks at once), the  ask  takes a massive amount of time to deliver the message to the target actor."
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,"The original design was to bulkhead requests evenly, but this is causing a bottleneck."
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,Example:
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,"In this picture, the  ask  is sent right after the query plan resolver."
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,"However, there is a multi-second gap when the Actor receives this message."
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,This is only experienced under load(5+ requests/sec).
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,I first thought this was a starvation issue.
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,"Design:
Each planner-executor is a seperate instance for each request."
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,It spawns a new 'Request Acceptor' actor each time(it logs 'requesting score' when it receives a message).
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,I'm a bit stumped by this.
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,From these tests it does not look like a thread starvation issue.
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,"Back at square one, I have no idea why the message takes longer and longer to deliver the more concurrent requests I make."
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,The Zipkin trace before reaching this point does not degrade with more requests until it reaches the  ask  here.
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,"Before then, the server is able to handle multiple steps to e.g veify the request, talk to the db, and then finally go inside the planner-executor."
Zipkin,50940162,nan,1,"2018/06/20, 07:20:22",False,"2018/06/21, 08:11:08",nan,138228.0,4732.0,2,944,Akka Actor Messaging Delay,So I doubt the application itself is running out of cpu time.
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,I have a Kubernetes's and spring boot's env variables conflict error.
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,Details is as follows:
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,"When creating my zipkin server pod, I need to set env variable  RABBITMQ_HOST=http://172.16.100.83，RABBITMQ_PORT=5672 ."
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,Initially I define zipkin_pod.yaml as follows:
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,"With this configuration, when I do command"
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,The console throws error:
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,so I modified the last line of zipkin_pod.yaml file as follows: Or use brutal force to make port number as int.
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,"Then pod is successfully created, but spring getProperties throws exception."
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,When I check logs:
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,"My question is how to let kubernetes understand the port number as int, while not breaking spring boot convert rule from string to int?"
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,because spring boot could not convert  !
Zipkin,50562296,50585875.0,1,"2018/05/28, 11:39:35",True,"2018/05/30, 12:06:27",nan,84592.0,4052.0,2,531,Kubernetes and spring boot variable parsing conflict,!31503 to int 31503 .
Zipkin,49880941,49881877.0,2,"2018/04/17, 17:36:09",True,"2020/02/04, 08:23:48",nan,3253853.0,4288.0,2,938,Microservices in Docker Container,I am using Spring Cloud for Creating Microservice Architecture.
Zipkin,49880941,49881877.0,2,"2018/04/17, 17:36:09",True,"2020/02/04, 08:23:48",nan,3253853.0,4288.0,2,938,Microservices in Docker Container,I was using the below feature from the Spring Cloud
Zipkin,49880941,49881877.0,2,"2018/04/17, 17:36:09",True,"2020/02/04, 08:23:48",nan,3253853.0,4288.0,2,938,Microservices in Docker Container,"Now Lets say if I have 100 microservices, then we need 100 servers to maintain each microservices."
Zipkin,49880941,49881877.0,2,"2018/04/17, 17:36:09",True,"2020/02/04, 08:23:48",nan,3253853.0,4288.0,2,938,Microservices in Docker Container,"So I thought of using Kubernetes to solve this issue by  deploying each microservices in a separate docker container, so now since Kubernetes takes care of microserivice health check, autoscaling, load-balancing so do I need to again use Ribbon, Eureka and Zuul."
Zipkin,49880941,49881877.0,2,"2018/04/17, 17:36:09",True,"2020/02/04, 08:23:48",nan,3253853.0,4288.0,2,938,Microservices in Docker Container,Can anyone please help me on this
Zipkin,49686743,49687046.0,1,"2018/04/06, 09:32:39",True,"2018/04/06, 09:52:45",nan,435003.0,2846.0,2,413,Spring Sleuth | Create fresh new (detached/orphaned) Trace,I got a  Spring Boot  application making use of  Spring Sleuth  for tracing inter-service calls.
Zipkin,49686743,49687046.0,1,"2018/04/06, 09:32:39",True,"2018/04/06, 09:52:45",nan,435003.0,2846.0,2,413,Spring Sleuth | Create fresh new (detached/orphaned) Trace,Within that application a  ScheduledExecutorService  exists that performs http requests in a loop (pseudo-code below):
Zipkin,49686743,49687046.0,1,"2018/04/06, 09:32:39",True,"2018/04/06, 09:52:45",nan,435003.0,2846.0,2,413,Spring Sleuth | Create fresh new (detached/orphaned) Trace,If I now have a look at the traces produced by Sleuth and stored in  Zipkin  I can see that all http calls are associated to a single Trace.
Zipkin,49686743,49687046.0,1,"2018/04/06, 09:32:39",True,"2018/04/06, 09:52:45",nan,435003.0,2846.0,2,413,Spring Sleuth | Create fresh new (detached/orphaned) Trace,Most likely because the trace context is handed over during the call to  ScheduledExecutorService::submit .
Zipkin,49686743,49687046.0,1,"2018/04/06, 09:32:39",True,"2018/04/06, 09:52:45",nan,435003.0,2846.0,2,413,Spring Sleuth | Create fresh new (detached/orphaned) Trace,How can I clear the current trace before starting the next iteration so that each http call will result in a new detached/orphaned trace?
Zipkin,45368147,nan,1,"2017/07/28, 11:07:56",True,"2017/07/28, 12:08:22",nan,1189332.0,1591.0,2,3100,spring-cloud-sleuth in a reactive environment,I should say I'm really impressed with the simplicity and usefulness of spring-cloud-sleuth and zipkin.
Zipkin,45368147,nan,1,"2017/07/28, 11:07:56",True,"2017/07/28, 12:08:22",nan,1189332.0,1591.0,2,3100,spring-cloud-sleuth in a reactive environment,"However, I'm working on a POC for which I'm considering reactive toolkits."
Zipkin,45368147,nan,1,"2017/07/28, 11:07:56",True,"2017/07/28, 12:08:22",nan,1189332.0,1591.0,2,3100,spring-cloud-sleuth in a reactive environment,Vertx 3 is the first item in my list to try (with spring cloud ecosystem).
Zipkin,45368147,nan,1,"2017/07/28, 11:07:56",True,"2017/07/28, 12:08:22",nan,1189332.0,1591.0,2,3100,spring-cloud-sleuth in a reactive environment,I'm wondering if Sleuth log tracing would work in a reactive context as I guess it relies on ThreadLocals to pass around the context?
Zipkin,45368147,nan,1,"2017/07/28, 11:07:56",True,"2017/07/28, 12:08:22",nan,1189332.0,1591.0,2,3100,spring-cloud-sleuth in a reactive environment,Keen to understand where Sleuth would stand in a reactive environment.
Zipkin,43937265,59895808.0,3,"2017/05/12, 14:59:57",True,"2020/01/24, 13:46:12",nan,374458.0,1673.0,2,677,Enabling Sleuth slows requests down (a lot),I use Spring Cloud Feign and Sleuth with a Zipkin server.
Zipkin,43937265,59895808.0,3,"2017/05/12, 14:59:57",True,"2020/01/24, 13:46:12",nan,374458.0,1673.0,2,677,Enabling Sleuth slows requests down (a lot),"My problem is that when I enable Sleuth, then any simple request takes at least 600ms."
Zipkin,43937265,59895808.0,3,"2017/05/12, 14:59:57",True,"2020/01/24, 13:46:12",nan,374458.0,1673.0,2,677,Enabling Sleuth slows requests down (a lot),"Note that for tests purposes, I've set the sampler percentage of Sleuth at 1."
Zipkin,43937265,59895808.0,3,"2017/05/12, 14:59:57",True,"2020/01/24, 13:46:12",nan,374458.0,1673.0,2,677,Enabling Sleuth slows requests down (a lot),Can I do something to improve that?
Zipkin,43937265,59895808.0,3,"2017/05/12, 14:59:57",True,"2020/01/24, 13:46:12",nan,374458.0,1673.0,2,677,Enabling Sleuth slows requests down (a lot),Here some log of a request which takes 25ms without Sleuth and 700ms with Sleuth.
Zipkin,43937265,59895808.0,3,"2017/05/12, 14:59:57",True,"2020/01/24, 13:46:12",nan,374458.0,1673.0,2,677,Enabling Sleuth slows requests down (a lot),(user calls /teams which calls /cities):
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,"I'm trying to get an openzipkin server running in a k8s cluster, starting with testing in a minikube."
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,"I'm beginner with k8s config, but here's what I've done so far:"
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,"What I think I'm doing is starting a new pod and deploying the zipkin image, then exposing the Web UI at port 9411 via zipkin-http."
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,After doing this:
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,Then I run the kubectl proxy so I can access the Web UI from my browser:
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,Now if I browse to  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json  I get the config file contents:
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,But if I browse to the root at  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/  I receive an error:
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,The config.json it's attempting to load is the one at :9411/config.json.
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,The request to load /config.json comes from a JS file that was loaded by the html in the root page.
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,"Since it looks like I can get to the json file directly from both inside and outside the cluster, I'm confused as to why the JS file isn't able to load it."
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,What am I doing wrong here?
Zipkin,40090893,40117668.0,1,"2016/10/17, 19:01:16",True,"2016/10/18, 23:39:29",nan,390857.0,151.0,2,352,Web UI for openzipkin run from Docker image shows config.json error,Thanks!
Zipkin,38019422,nan,2,"2016/06/24, 20:54:51",False,"2017/08/19, 11:59:32","2017/03/11, 21:14:59",6509824.0,31.0,2,885,get traceId from zipkintracer,"I am using  zipkin-go-opentracing , which is an implementation of the  opentracing  API for zipkin in go."
Zipkin,38019422,nan,2,"2016/06/24, 20:54:51",False,"2017/08/19, 11:59:32","2017/03/11, 21:14:59",6509824.0,31.0,2,885,get traceId from zipkintracer,For (reasons) I need to get the traceId from a span.
Zipkin,38019422,nan,2,"2016/06/24, 20:54:51",False,"2017/08/19, 11:59:32","2017/03/11, 21:14:59",6509824.0,31.0,2,885,get traceId from zipkintracer,"So the question:
given a opentracing.Span, how do I get the TraceId?"
Zipkin,38019422,nan,2,"2016/06/24, 20:54:51",False,"2017/08/19, 11:59:32","2017/03/11, 21:14:59",6509824.0,31.0,2,885,get traceId from zipkintracer,Everything I've tried has given me some kind of type assertion error.
Zipkin,38019422,nan,2,"2016/06/24, 20:54:51",False,"2017/08/19, 11:59:32","2017/03/11, 21:14:59",6509824.0,31.0,2,885,get traceId from zipkintracer,"Thanks,"
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),I am running mysql and zipkin in kubernetes.
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),Zipkin is failing to connect to mysql database.
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),I've checked environment variables and all variables are set up properly.
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),Exception
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),mysql-deployment.yaml
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),mysql-service.yaml
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),zipkin-deployment.yaml
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),zipkin-service.yaml
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),secret.yaml
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),"I think, I need to change Charset in zipkin."
Zipkin,56328934,nan,1,"2019/05/27, 18:33:38",False,"2020/04/22, 17:01:23",nan,5813215.0,170.0,1,986,java.sql.SQLInvalidAuthorizationSpecException: Access denied for user &#39;root&#39;@&#39;10.24.1.77&#39; (using password: YES),"But, I have no way to change it other than build another image myself."
Zipkin,51187274,nan,1,"2018/07/05, 12:02:04",False,"2018/10/12, 13:43:13",nan,10036064.0,11.0,1,690,Sleuth 2 with spring boot 2 not adding traceid in logs,I have recently started exploring sping boot 2.
Zipkin,51187274,nan,1,"2018/07/05, 12:02:04",False,"2018/10/12, 13:43:13",nan,10036064.0,11.0,1,690,Sleuth 2 with spring boot 2 not adding traceid in logs,I am uisng logaback for logging purpose.
Zipkin,51187274,nan,1,"2018/07/05, 12:02:04",False,"2018/10/12, 13:43:13",nan,10036064.0,11.0,1,690,Sleuth 2 with spring boot 2 not adding traceid in logs,"For distributed logging tracing, I wanted to use sping boot sleuth starter."
Zipkin,51187274,nan,1,"2018/07/05, 12:02:04",False,"2018/10/12, 13:43:13",nan,10036064.0,11.0,1,690,Sleuth 2 with spring boot 2 not adding traceid in logs,But with below dependency in pom.xml without zipkin integration its not adding traceid in logs.Also added spring.application.name property in application.properties file.
Zipkin,51187274,nan,1,"2018/07/05, 12:02:04",False,"2018/10/12, 13:43:13",nan,10036064.0,11.0,1,690,Sleuth 2 with spring boot 2 not adding traceid in logs,Am I missing anything here?
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent","I am using Spring Cloud Sleuth and Zipkin (via HTTP), by adding spring-cloud-starter-zipkin version 2.0.0.M6 to my dependencies (based on Spring Boot 2.0.0.RC1 and Spring Cloud Finchley M6)."
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent",I am using @Newspan annotation to mark a child span around some (expensive) operation.
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent","When the span information is sent to Zipkin, I notice that the timestamp and duration of the child span are missing."
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent",This leads to a strange rendering on Zipking side.
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent","However, when I create the child span by calling tracer#newChild, it works as expected."
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent",Am I missing something?
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent",Would this be an issue with Sleuth 2.0.0.M6?
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent","When I run the same code using Spring Boot 1.5.9 and Spring Cloud Edgware SR2, it behaves as expected."
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent",Here's the JSON received on Zipkin side.
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent","The span named ""child-span-with-annotation"" is the one created using @NewSpan, whereas the span ""childspanwithnewchild"" is created using tracer#newChild."
Zipkin,48759442,48763174.0,1,"2018/02/13, 06:26:08",True,"2018/02/13, 11:15:18","2018/02/13, 06:37:21",5844758.0,717.0,1,3032,"When a span is created using @NewSpan, timestamp and duration are not sent","
 
 [
  {
    ""traceId"": ""b1c2636366c919be"",
    ""id"": ""b1c2636366c919be"",
    ""name"": ""get"",
    ""timestamp"": 1518495271073166,
    ""duration"": 862032,
    ""annotations"": [
      {
        ""timestamp"": 1518495271073166,
        ""value"": ""sr"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      },
      {
        ""timestamp"": 1518495271935198,
        ""value"": ""ss"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      }
    ],
    ""binaryAnnotations"": [
      {
        ""key"": ""ca"",
        ""value"": true,
        ""endpoint"": {
          ""serviceName"": """",
          ""ipv6"": ""::1"",
          ""port"": 51982
        }
      },
      {
        ""key"": ""http.path"",
        ""value"": ""/hello"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      },
      {
        ""key"": ""mvc.controller.class"",
        ""value"": ""MyRestController"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      },
      {
        ""key"": ""mvc.controller.method"",
        ""value"": ""sayHello"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      }
    ]
  },
  {
    ""traceId"": ""b1c2636366c919be"",
    ""id"": ""14be7ac6eafb0e01"",
    ""name"": ""child-span-with-annotation"",
    ""parentId"": ""b1c2636366c919be"",
    ""binaryAnnotations"": [
      {
        ""key"": ""class"",
        ""value"": ""MyService"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      },
      {
        ""key"": ""method"",
        ""value"": ""expensiveOperation1"",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      }
    ]
  },
  {
    ""traceId"": ""b1c2636366c919be"",
    ""id"": ""b34a4f910f27fdb4"",
    ""name"": ""childspanwithnewchild"",
    ""parentId"": ""b1c2636366c919be"",
    ""timestamp"": 1518495271479040,
    ""duration"": 453747,
    ""binaryAnnotations"": [
      {
        ""key"": ""lc"",
        ""value"": """",
        ""endpoint"": {
          ""serviceName"": ""sample-sleuth-app"",
          ""ipv4"": ""---.---.---.---""
        }
      }
    ]
  }
]"
Zipkin,48679359,nan,1,"2018/02/08, 08:55:35",False,"2018/02/08, 09:23:24",nan,1599719.0,85.0,1,34,Co-ordinated delete Mysql query,Can someone provide a delete query to delete older records(older than 5 days)for the below mysql tables with its reference tables?
Zipkin,48679359,nan,1,"2018/02/08, 08:55:35",False,"2018/02/08, 09:23:24",nan,1599719.0,85.0,1,34,Co-ordinated delete Mysql query,(All reference tables records also need to deleted otherwise crashes my application)
Zipkin,48679359,nan,1,"2018/02/08, 08:55:35",False,"2018/02/08, 09:23:24",nan,1599719.0,85.0,1,34,Co-ordinated delete Mysql query,https://github.com/openzipkin/zipkin/blob/master/zipkin-storage/mysql/src/main/resources/mysql.sql
Zipkin,48679359,nan,1,"2018/02/08, 08:55:35",False,"2018/02/08, 09:23:24",nan,1599719.0,85.0,1,34,Co-ordinated delete Mysql query,I have no clue to write a coordinated delete query (deleting multiple tables in same query)
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,Small question regarding some actuator endpoints returning 404 please.
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,"I have a web app, based on Webflux 2.4.2, and for testing this issue only, I am using"
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,"Actuator is working, because a curl will get the response for /health /metrics and other endpoints."
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,"However, for those endpoints  /auditevents /httptrace /integrationgraph /sessions , I am not able to get anything, besides a http 404."
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,[05/Feb/2021:13:00:18 +0000] &quot;OPTIONS /auditevents HTTP/1.1&quot; 404 141 55 ms
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,"Those are really the only endpoints returning 404, still do not know why."
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,Don't want to spam with one question same question per endpoint.
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,All other actuator endpoints are fine.
Zipkin,66064081,66064768.0,1,"2021/02/05, 15:25:46",True,"2021/02/05, 18:51:05","2021/02/05, 16:36:26",10461625.0,1094.0,1,98,Spring Webflux 2.4.2 - 404 on actuator /auditevents /httptrace /integrationgraph /sessions endpoints,Thank you
Zipkin,65841823,nan,0,"2021/01/22, 10:40:34",False,"2021/01/25, 11:15:55",nan,11352672.0,131.0,1,48,Sleuth annotations with Spring Webflux gives CglibAopProxy warnings,I have a reactive application using Spring Webflux.
Zipkin,65841823,nan,0,"2021/01/22, 10:40:34",False,"2021/01/25, 11:15:55",nan,11352672.0,131.0,1,48,Sleuth annotations with Spring Webflux gives CglibAopProxy warnings,"I have used sleuth annotations like  @NewSpan  to create spans, but I am getting warning like"
Zipkin,65841823,nan,0,"2021/01/22, 10:40:34",False,"2021/01/25, 11:15:55",nan,11352672.0,131.0,1,48,Sleuth annotations with Spring Webflux gives CglibAopProxy warnings,"I know  Flux.subscribe  is a final method so proxies are not generated correctly, but I can still see those spans on Zipkin."
Zipkin,65841823,nan,0,"2021/01/22, 10:40:34",False,"2021/01/25, 11:15:55",nan,11352672.0,131.0,1,48,Sleuth annotations with Spring Webflux gives CglibAopProxy warnings,I need to know what are the implications of this warning.
Zipkin,65841823,nan,0,"2021/01/22, 10:40:34",False,"2021/01/25, 11:15:55",nan,11352672.0,131.0,1,48,Sleuth annotations with Spring Webflux gives CglibAopProxy warnings,And how can I avoid this?
Zipkin,64434436,64439121.0,1,"2020/10/19, 23:05:29",True,"2020/10/20, 08:42:11",nan,1722894.0,15.0,1,34,disallowing a particular http method,"Related to  https://github.com/openzipkin/zipkin/pull/3239  , we came across some (maybe) odd behaviour and i wanted to know if below test works as expected or not:"
Zipkin,64434436,64439121.0,1,"2020/10/19, 23:05:29",True,"2020/10/20, 08:42:11",nan,1722894.0,15.0,1,34,disallowing a particular http method,"Basically we wanted to disable all  TRACE  requests, and set  pathPrefix(&quot;/&quot;)  to achieve this."
Zipkin,64434436,64439121.0,1,"2020/10/19, 23:05:29",True,"2020/10/20, 08:42:11",nan,1722894.0,15.0,1,34,disallowing a particular http method,But for some reason the  OPTIONS  call to  /something  gets trapped in the same path.
Zipkin,64434436,64439121.0,1,"2020/10/19, 23:05:29",True,"2020/10/20, 08:42:11",nan,1722894.0,15.0,1,34,disallowing a particular http method,If i remove the route decorator things work as expected.
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,I am new to Microservices.
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,(Learning phase).
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,I have a question.
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,We deploy microservices at cloud.
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,(e.g.
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,AWS).
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,Cloud already provide load balancing and logs.
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,And We also implement Load Balancing(Ribbon) and logs(Rabbit MQ and Zipkin) in Spring Boot.
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,What is the difference in these two implementation?
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,Do we need both?
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,Can some answer these questions.
Zipkin,64040747,nan,1,"2020/09/24, 09:34:10",True,"2020/09/24, 15:23:08","2020/09/24, 15:23:08",2564590.0,101.0,1,101,Spring Boot Microservices load balancing vs cloud load balancing,Thanks in advance.
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,I deploy zipkin in docker (zipkin-server-2.21.7-exec.jar) and I connect with rabbit in docker.
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,I'm using Eureka in docker to register microservices.
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,When I run one this microservices this error compare
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,APPLICATION FAILED TO START
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,Description:
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,Parameter 2 of method reporter in org.springframework.cloud.sleuth.zipkin2.ZipkinAutoConfiguration required a bean of type 'zipkin2.reporter.Sender' that could not be found.
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,The following candidates were found but could not be injected:
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,Bean method 'rabbitSender' in 'ZipkinRabbitSenderConfiguration' not loaded because @ConditionalOnBean (types: org.springframework.amqp.rabbit.connection.CachingConnectionFactory; SearchStrategy: all) did not find any beans of type org.springframework.amqp.rabbit.connection.CachingConnectionFactory
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,Bean method 'restTemplateSender' in 'ZipkinRestTemplateSenderConfiguration' not loaded because ZipkinSender org.springframework.cloud.sleuth.zipkin2.sender.ZipkinRestTemplateSenderConfiguration rabbit sender type
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,Action:
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,Consider revisiting the entries above or defining a bean of type 'zipkin2.reporter.Sender' in your configuration.
Zipkin,63643070,nan,0,"2020/08/29, 06:18:46",False,"2020/08/29, 06:18:46",nan,3000867.0,21.0,1,70,Not be found zipkin2.reporter.Sender with rabbit,"I use this properties
spring.zipkin.sender.type=rabbit
spring.zipkin.base-url=http://zipkin-server:9411/"
Zipkin,62361966,nan,0,"2020/06/13, 18:33:45",False,"2020/06/13, 20:25:11","2020/06/13, 20:25:11",8627956.0,11.0,1,250,Using ALB or ELB as loadbalancer instead of istio-ingressgateway,We have an EKS cluster in AWS and i am using istio as service mesh in my cluster.
Zipkin,62361966,nan,0,"2020/06/13, 18:33:45",False,"2020/06/13, 20:25:11","2020/06/13, 20:25:11",8627956.0,11.0,1,250,Using ALB or ELB as loadbalancer instead of istio-ingressgateway,We are using istio only for injecting the sidecar into applications and to trace the application traffic through zipkin.
Zipkin,62361966,nan,0,"2020/06/13, 18:33:45",False,"2020/06/13, 20:25:11","2020/06/13, 20:25:11",8627956.0,11.0,1,250,Using ALB or ELB as loadbalancer instead of istio-ingressgateway,To access the application from outside we are not using istio-ingressgateway instead we are using ALB &amp; ELBs
Zipkin,62361966,nan,0,"2020/06/13, 18:33:45",False,"2020/06/13, 20:25:11","2020/06/13, 20:25:11",8627956.0,11.0,1,250,Using ALB or ELB as loadbalancer instead of istio-ingressgateway,So my problem is I am not getting any traces to zipkin / kiali when i am accessing my application through AWS LBs.
Zipkin,62361966,nan,0,"2020/06/13, 18:33:45",False,"2020/06/13, 20:25:11","2020/06/13, 20:25:11",8627956.0,11.0,1,250,Using ALB or ELB as loadbalancer instead of istio-ingressgateway,Do i have to use istio-ingressgateway to record the traces in zipkin and view in kiali or is there a way to get traces using ALB/ELB as a loadbalancer?
Zipkin,60258661,nan,1,"2020/02/17, 10:42:53",True,"2020/02/17, 10:53:24",nan,9768260.0,2316.0,1,149,Kubectl exec to the specific container in deployment,Running a deployment with three pod.
Zipkin,60258661,nan,1,"2020/02/17, 10:42:53",True,"2020/02/17, 10:53:24",nan,9768260.0,2316.0,1,149,Kubectl exec to the specific container in deployment,The only one pod container I can connect is zipkin with  kubectl exec -it my-api-XXX -- /bin/bash .
Zipkin,60258661,nan,1,"2020/02/17, 10:42:53",True,"2020/02/17, 10:53:24",nan,9768260.0,2316.0,1,149,Kubectl exec to the specific container in deployment,If I want to access the my-api container using  kubectl exec -it my-api-XXX -c &lt;my-api container ID&gt; -- /bin/bash .
Zipkin,60258661,nan,1,"2020/02/17, 10:42:53",True,"2020/02/17, 10:53:24",nan,9768260.0,2316.0,1,149,Kubectl exec to the specific container in deployment,It report a error show the container is not in that pod.
Zipkin,60258661,nan,1,"2020/02/17, 10:42:53",True,"2020/02/17, 10:53:24",nan,9768260.0,2316.0,1,149,Kubectl exec to the specific container in deployment,Error from server (BadRequest): container my-api_containerID is not valid for pod my-api-XXX
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,I am new to K8s and I am trying to migrate my service (which currently utilizes docker-compose.yml) to k8s.
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,My service
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,deploys zipkin and elasticsearch
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,and these can be accessed at  'localhost:9411'  and  'localhost:9200'  respectively.
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,"The most commonly used solution I found online was 'kompose' and I tried to run,"
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,2.
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,"Once I finish this, I run kubectl get pods and I can see my deployments, but elasticsearch and zipkin are no more responsive on their respective localhost ports."
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,Ouput of  'kubectl get pods'
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,Output of  'docker ps'
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,Output of  curl http://localhost:9200
Zipkin,56655528,nan,2,"2019/06/18, 22:04:52",False,"2019/06/24, 13:49:27","2019/06/18, 22:11:05",3800106.0,694.0,1,92,Kompose doesn&#39;t bring up localhost for openzipkin and Elastic,Can someone tell me why this is happening and how to debug?
Zipkin,55274572,nan,1,"2019/03/21, 08:00:41",True,"2019/06/04, 17:18:45",nan,3303280.0,201.0,1,603,Spring cloud sleuth with Spring cloud stream,We are building event-driven microservices using Spring Cloud Stream(with Kafka binder) and looking at options for tracing Micorservices that are not exposed as http end point.
Zipkin,55274572,nan,1,"2019/03/21, 08:00:41",True,"2019/06/04, 17:18:45",nan,3303280.0,201.0,1,603,Spring cloud sleuth with Spring cloud stream,Please suggest.
Zipkin,55274572,nan,1,"2019/03/21, 08:00:41",True,"2019/06/04, 17:18:45",nan,3303280.0,201.0,1,603,Spring cloud sleuth with Spring cloud stream,I understand that using Sleuth will automatically add trace and span id to logs if it is over http.
Zipkin,55274572,nan,1,"2019/03/21, 08:00:41",True,"2019/06/04, 17:18:45",nan,3303280.0,201.0,1,603,Spring cloud sleuth with Spring cloud stream,Documentation is not clear for using it with Spring Cloud Stream -  https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth#_messaging
Zipkin,55274572,nan,1,"2019/03/21, 08:00:41",True,"2019/06/04, 17:18:45",nan,3303280.0,201.0,1,603,Spring cloud sleuth with Spring cloud stream,Found an example but not sure whether it is the right approach -  https://github.com/bjedrzejewski/food-order-publisher/blob/zipkin-example/src/main/java/com/e4developer/foodorderpublisher/FoodOrderController.java
Zipkin,55274572,nan,1,"2019/03/21, 08:00:41",True,"2019/06/04, 17:18:45",nan,3303280.0,201.0,1,603,Spring cloud sleuth with Spring cloud stream,Thanks
Zipkin,54845174,54914623.0,1,"2019/02/23, 21:10:52",True,"2019/02/27, 23:15:56",nan,4393106.0,343.0,1,391,How to use spring cloud sleuth for RabbitListener and RabbitTemplate,"I want to send RabbitMQ messages tracking event to Zipkin with using spring cloud sleuth, After many research I found some configuration added recently to spring in order to manage it you can find in  here , But unfortunately there is not any  documentation that explains how can we configure it, I tried many ways but I couldn't send tracking events to Zipkin."
Zipkin,54845174,54914623.0,1,"2019/02/23, 21:10:52",True,"2019/02/27, 23:15:56",nan,4393106.0,343.0,1,391,How to use spring cloud sleuth for RabbitListener and RabbitTemplate,Please advice
Zipkin,53653606,nan,1,"2018/12/06, 16:29:15",False,"2018/12/07, 23:02:30","2018/12/06, 19:18:47",8479984.0,237.0,1,2801,How to use TraceContext to set a traceId in Spring Sleuth and brave?,I need to set the traceId with an existing Id (we have created some kind of correlation-id from the main origin app) into brave tracer.
Zipkin,53653606,nan,1,"2018/12/06, 16:29:15",False,"2018/12/07, 23:02:30","2018/12/06, 19:18:47",8479984.0,237.0,1,2801,How to use TraceContext to set a traceId in Spring Sleuth and brave?,I don't want to use the Spring Sleuth/brave created one as  I want to make it consistent throughout my different micro-services.
Zipkin,53653606,nan,1,"2018/12/06, 16:29:15",False,"2018/12/07, 23:02:30","2018/12/06, 19:18:47",8479984.0,237.0,1,2801,How to use TraceContext to set a traceId in Spring Sleuth and brave?,I am able to create traces and span and able to send all details into Zipkin.
Zipkin,53653606,nan,1,"2018/12/06, 16:29:15",False,"2018/12/07, 23:02:30","2018/12/06, 19:18:47",8479984.0,237.0,1,2801,How to use TraceContext to set a traceId in Spring Sleuth and brave?,My sample snippet:
Zipkin,53653606,nan,1,"2018/12/06, 16:29:15",False,"2018/12/07, 23:02:30","2018/12/06, 19:18:47",8479984.0,237.0,1,2801,How to use TraceContext to set a traceId in Spring Sleuth and brave?,I am using: Spring Cloud 'Greenwich.BUILD-SNAPSHOT' and brave.
Zipkin,53653606,nan,1,"2018/12/06, 16:29:15",False,"2018/12/07, 23:02:30","2018/12/06, 19:18:47",8479984.0,237.0,1,2801,How to use TraceContext to set a traceId in Spring Sleuth and brave?,The whole purpose is to search using correlationId rather than traceId in zipkin ui.
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,"In our Spring Boot application (2.0.4.RELEASE), we use Zipkin to integrate distributed tracing."
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,"When creating the integration manually with a 10% sampling rate, meaning with a  @Configuration  like this:"
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,our application has a 50 percentile performance of about 19ms and a 99.9 percentile of about 90ms at around 10 requests per second.
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,When integrating Sleuth 2.0.2.RELEASE instead like this in gradle:
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,the performance drops massively to a p50 of 49ms and a p999 of 120ms.
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,"I tried disabling the different parts of the Sleuth integration ( spring.sleuth.async.enabled ,  spring.sleuth.reactor.enabled , etc."
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,).
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,"Disabling all these integrations brings the performance to p50: 25ms, p999: 103 ms.  Just having Sleuth adds about 15-25% of overhead."
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,It turns out that the one thing with the significant impact is setting  spring.sleuth.log.slf4j.enabled  to  false .
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,"If all other integrations are enabled, but this is disabled, the performance stays within the Sleuth overhead mentioned above, although nothing is logged."
Zipkin,53174880,53184979.0,1,"2018/11/06, 17:22:53",True,"2018/11/07, 09:16:13",nan,53321.0,5669.0,1,826,How to avoid massive overhead of Sleuth Slf4J integration,"So my question is:
Is there a way to avoid the overhead by Sleuth (compared to ""manual"" tracing) and especially the one done by the SLF4J integration?"
Zipkin,51526164,51526235.0,1,"2018/07/25, 22:16:02",True,"2018/07/26, 03:24:14","2018/07/25, 22:23:16",4114841.0,85.0,1,1017,Spring integration DSL Scatter-Gather async/parallel execution for multiple recipientFlows,we are trying to make parallel calls to different recipient using scatter-gather and it works fine.
Zipkin,51526164,51526235.0,1,"2018/07/25, 22:16:02",True,"2018/07/26, 03:24:14","2018/07/25, 22:23:16",4114841.0,85.0,1,1017,Spring integration DSL Scatter-Gather async/parallel execution for multiple recipientFlows,But the second recipient flow is not starting unless the first one is complete(traced in Zipkin).
Zipkin,51526164,51526235.0,1,"2018/07/25, 22:16:02",True,"2018/07/26, 03:24:14","2018/07/25, 22:23:16",4114841.0,85.0,1,1017,Spring integration DSL Scatter-Gather async/parallel execution for multiple recipientFlows,is there is a way to make all recipients async.. very similar to split-aggregate with executor channel.
Zipkin,51526164,51526235.0,1,"2018/07/25, 22:16:02",True,"2018/07/26, 03:24:14","2018/07/25, 22:23:16",4114841.0,85.0,1,1017,Spring integration DSL Scatter-Gather async/parallel execution for multiple recipientFlows,"flow2(),flow3(),flow4() methods are methods with  InterationFlow  as return type."
Zipkin,51526164,51526235.0,1,"2018/07/25, 22:16:02",True,"2018/07/26, 03:24:14","2018/07/25, 22:23:16",4114841.0,85.0,1,1017,Spring integration DSL Scatter-Gather async/parallel execution for multiple recipientFlows,sample code  flow2()  :
Zipkin,51442393,nan,1,"2018/07/20, 15:17:32",False,"2018/07/20, 15:38:54",nan,5391763.0,97.0,1,153,Issues starting Spring Cloud CLI,I'm learning about microservices and I need spring cloud cli for this test project.
Zipkin,51442393,nan,1,"2018/07/20, 15:17:32",False,"2018/07/20, 15:38:54",nan,5391763.0,97.0,1,153,Issues starting Spring Cloud CLI,I have installed spring boot cli (extracted and added to path) version Spring CLI v2.0.3.RELEASE.
Zipkin,51442393,nan,1,"2018/07/20, 15:17:32",False,"2018/07/20, 15:38:54",nan,5391763.0,97.0,1,153,Issues starting Spring Cloud CLI,"I have installed the spring cloud cli plugin (spring-cloud-cli:1.3.1.RELEASE), and verify it with checking its version."
Zipkin,51442393,nan,1,"2018/07/20, 15:17:32",False,"2018/07/20, 15:38:54",nan,5391763.0,97.0,1,153,Issues starting Spring Cloud CLI,"I would like to execute [spring cloud eureka configserver zipkin]
but I'm getting:"
Zipkin,51442393,nan,1,"2018/07/20, 15:17:32",False,"2018/07/20, 15:38:54",nan,5391763.0,97.0,1,153,Issues starting Spring Cloud CLI,File ........\.m2\repository\org\springframework\cloud\launcher\spring-cloud-launcher-deployer\1.3.3.BUILD-SNAPSHOT\spring-cloud-launcher-deployer-1.3.3.BUILD-SNAPSHOT.jar must exist
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,I am using rxjava 2 with spring boot 2 and spring cloud finchley.rc2.
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,Now i am tracing the requests with sleuth / zipkin.
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,But i am getting one issue that IO threads are reusing the olds requests traceids.
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,"When I am doing this, traceids generated are fine(same for all parallel calls in same request )"
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,"But when I am doing this, it is not working properly."
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,"I am assuming since io() threads are reused, thats why they are using old data since they are old threads?"
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,But how can i force them to use the trace id for current main thread?
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,How can i fix this?
Zipkin,50871969,nan,0,"2018/06/15, 11:41:55",False,"2018/06/15, 11:41:55",nan,1826788.0,1408.0,1,290,rxjava2 with spring boot 2,I cant move the webflux as i have lot of code writtenin rxjava?
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,"Currently, I am working on my company's microservices solution which uses  Spring Cloud Edgware.SR1 ."
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,"This solution includes the following main flavors:
 api-gateway(Zuul) ,  service-discovery(Eureka) ,  uaa ,  zipkin-server  and  business logic  services."
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,I am trying to provide a good tracing for all requests in our system.
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,"In Zipkin UI I can see a trace for the request that starts in api-gateway, going through uaa to our business logic service."
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,But requests to the Eureka are missing in the trace.
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,"Instead, there is a separate trace with a single span for  service-discovery  endpoint  http://eureka/apps/** ."
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,I had the same issue with  uaa  request to  http://user/  endpoint and solved it by adding  TraceRestTemplateInterceptor  to our Oauth2 client.
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,"However, I found it difficult to override  EurekaHttpClient  and add the mentioned interceptor."
Zipkin,50211455,nan,0,"2018/05/07, 12:55:59",False,"2018/11/03, 19:56:26","2018/11/03, 19:56:26",4252487.0,43.0,1,262,How to propagate Sleuth Trace Id to EurekaHttpClient?,Is there any other way to propagate trace id to Eureka Clients?
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,I have created a spring boot application with spring cloud sleuth.
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,"For POC purposes, I used zipkin on 
my local machine and I am able to instrument a external service which is not instrumented by creating 
manual span."
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,I reffered below link.
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,https://cloud.spring.io/spring-cloud-sleuth/1.2.x/multi/multi__customizations.html
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,"Now, When I move to PCF environment, then I am unable to collect proper custom spans."
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,PCF metrics always shows parent span and service with total time taken.
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,Could anyone please let me know where I am going wrong.
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,Zipkin Output:-
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,PCF Metrics:-
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,"UPDATE 
screen shot for Zipkin with @NewSpan."
Zipkin,49326587,nan,2,"2018/03/16, 19:37:57",False,"2018/04/18, 18:02:24","2018/03/20, 06:09:23",1659442.0,711.0,1,487,Unable to get custom span in PCF metrics using spring cloud sleuth,PCF metrics screen shot without call hierachy
Zipkin,48940831,49066640.0,1,"2018/02/23, 05:52:04",True,"2018/07/03, 21:39:26","2018/02/23, 06:24:09",435192.0,440.0,1,2900,Spring boot 2.0 and Spring Cloud Sleuth 2.x Working Sample,"I have been trying to get Spring boot 2.0 and Spring Cloud Slueth 2.x (POM= Finchley.M6) working, but no avail."
Zipkin,48940831,49066640.0,1,"2018/02/23, 05:52:04",True,"2018/07/03, 21:39:26","2018/02/23, 06:24:09",435192.0,440.0,1,2900,Spring boot 2.0 and Spring Cloud Sleuth 2.x Working Sample,I have a  service1  calls  service2  and  service3 .
Zipkin,48940831,49066640.0,1,"2018/02/23, 05:52:04",True,"2018/07/03, 21:39:26","2018/02/23, 06:24:09",435192.0,440.0,1,2900,Spring boot 2.0 and Spring Cloud Sleuth 2.x Working Sample,I see that a new  traceId  is created whenever a request is received in  service1  but not passed to  Service2  and  Service3  instead a new  traceid  is being created every time on  Service2  and  Service3 .
Zipkin,48940831,49066640.0,1,"2018/02/23, 05:52:04",True,"2018/07/03, 21:39:26","2018/02/23, 06:24:09",435192.0,440.0,1,2900,Spring boot 2.0 and Spring Cloud Sleuth 2.x Working Sample,Is this anyhow related to  this defect  ?
Zipkin,48940831,49066640.0,1,"2018/02/23, 05:52:04",True,"2018/07/03, 21:39:26","2018/02/23, 06:24:09",435192.0,440.0,1,2900,Spring boot 2.0 and Spring Cloud Sleuth 2.x Working Sample,NOTE: I don't need zipkin support and I need sleuth for distributed tracing and will be using Splunk as log aggregater.
Zipkin,48940831,49066640.0,1,"2018/02/23, 05:52:04",True,"2018/07/03, 21:39:26","2018/02/23, 06:24:09",435192.0,440.0,1,2900,Spring boot 2.0 and Spring Cloud Sleuth 2.x Working Sample,Source Code :  https://github.com/trmsmy/springboot-cloud-examples/tree/springboot2
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,I am currently running Spring Cloud Edgware.SR2.
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,"I am migrating services from RabbitMQ to Kafka, and at this point I don't see any Zipkin traces when I run kafka-console-consumer.sh on the zipkin topic (i.e.,  kafka-console-consumer.sh --new-consumer --bootstrap-server localhost:9092 --topic zipkin --from-beginning )."
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,"As a result, I of course don't see any trace information in the Zipkin UI."
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,Following are the dependencies I have as part of the producer service:
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,These are the dependency overrides I had to make after pulling in the  spring-cloud-stream-binder-kafka11  dependency per the instructions  at the bottom of the Spring Cloud Stream project page .
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,"I also took a look at the instructions for  Sleuth with Zipkin via RabbitMQ or Kafka , and I think I have that part correct."
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,The documentation states  If you want Sleuth over RabbitMQ add the spring-cloud-starter-zipkin and spring-rabbit dependencies.
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,"It specifically mentions  spring-cloud-starter-zipkin  is needed for RabbitMQ, but I added it even though I'm using Kafka since it didn't work without this dependency either."
Zipkin,48756959,48762766.0,1,"2018/02/13, 01:00:37",True,"2018/02/13, 21:29:25","2018/02/13, 17:10:21",7053911.0,463.0,1,3704,No Spring Cloud Sleuth traces over Kafka,Any ideas on what I'm missing or have configured incorrectly to capture Sleuth traces and send them to the Zipkin server using Kafka?
Zipkin,48639074,nan,1,"2018/02/06, 11:01:20",False,"2018/08/08, 13:00:10",nan,9320856.0,11.0,1,44,Tracing Openstack Mitaka,"I'm trying to trace services in Openstack Mitaka using osprofiler, but i'm having some issues."
Zipkin,48639074,nan,1,"2018/02/06, 11:01:20",False,"2018/08/08, 13:00:10",nan,9320856.0,11.0,1,44,Tracing Openstack Mitaka,It seems it's not possible to trace nova service in Mitaka using osprofiler (correct me if i'm wrong).
Zipkin,48639074,nan,1,"2018/02/06, 11:01:20",False,"2018/08/08, 13:00:10",nan,9320856.0,11.0,1,44,Tracing Openstack Mitaka,So i was thinking of using Zipkin.
Zipkin,48639074,nan,1,"2018/02/06, 11:01:20",False,"2018/08/08, 13:00:10",nan,9320856.0,11.0,1,44,Tracing Openstack Mitaka,Can anyone tell me if Zipkin integrates with openstack mitaka?
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,I have a Java Spring-Cloud-based microservice integrated with RabbitMQ using Spring Boot Starter AMQP (extract from a  pom.xml  below):
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,Now I would like to connect this service to the Zipkin monitoring using Sleuth.
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,"According to the  documentation , when AMQP support is enabled, Sleuth sends all its data through a RabbitMQ queue."
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,For some reason I would like to disable this default behaviour and send data via HTTP.
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,Probably there is one magic property which I cannot find.
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,Do you know how I can force my application to send Sleuth-related data via HTTP to a Zipkin Server (also a Spring Boot application with  @EnableZipkinServer  annotation)?
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,"In addition I would like to mention that after removing the AMQP support everything works fine, i.e."
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,Zipkin receives tracing data via HTTP.
Zipkin,48216192,48216934.0,1,"2018/01/11, 23:36:38",True,"2018/01/12, 00:38:24","2018/01/11, 23:43:10",4005266.0,165.0,1,553,Disable AMQP integration with Sleuth,"Moreover, setting both  spring.zipkin.collector.http.enabled: true  and  spring.zipkin.collector.amqp.enabled: false  (and  spring.zipkin.collector.rabbitmq.enabled: false ) does not help."
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),I'm using  Spring Cloud Stream  binder from the Edgware release to send Kafka messages.
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),I'm also using  Spring Sleuth  with  Zipkin .
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),Spring embeds headers into the Kafka message using a custom class  EmbeddedHeaderUtils .
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),This causes a problem for some non-Spring consumers of the message who would have to deal with this custom decoding.
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),My Question:  Is there a way to configure Spring with a custom encoder/decoder for message headers (e.g.
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),plain JSON)?
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),Or possibly use Kafka Headers?
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),Ideally any custom implementation needs to work with Spring Sleuth and Zipkin.
Zipkin,48169950,48170238.0,1,"2018/01/09, 16:05:22",True,"2018/01/09, 16:22:14",nan,5164569.0,323.0,1,659,Spring Cloud Stream embedded header format (Kafka),I've been having a look at the latest Finchley release to see if Kafka headers will be supported but not sure about that.
Zipkin,47925555,nan,1,"2017/12/21, 14:59:09",True,"2017/12/22, 08:59:32","2017/12/22, 08:59:32",661859.0,907.0,1,255,ELK - micro service and network latency monitoring,Does ELK stack provide micro service and network latency monitoring in kibana?
Zipkin,47925555,nan,1,"2017/12/21, 14:59:09",True,"2017/12/22, 08:59:32","2017/12/22, 08:59:32",661859.0,907.0,1,255,ELK - micro service and network latency monitoring,Zipkin  provides details bout service request and service response duration.
Zipkin,47925555,nan,1,"2017/12/21, 14:59:09",True,"2017/12/22, 08:59:32","2017/12/22, 08:59:32",661859.0,907.0,1,255,ELK - micro service and network latency monitoring,In behind ELS stack should trace span events:
Zipkin,47925555,nan,1,"2017/12/21, 14:59:09",True,"2017/12/22, 08:59:32","2017/12/22, 08:59:32",661859.0,907.0,1,255,ELK - micro service and network latency monitoring,cs - Client Sent
Zipkin,47925555,nan,1,"2017/12/21, 14:59:09",True,"2017/12/22, 08:59:32","2017/12/22, 08:59:32",661859.0,907.0,1,255,ELK - micro service and network latency monitoring,sr - Server Received
Zipkin,47925555,nan,1,"2017/12/21, 14:59:09",True,"2017/12/22, 08:59:32","2017/12/22, 08:59:32",661859.0,907.0,1,255,ELK - micro service and network latency monitoring,ss - Server Sent
Zipkin,47925555,nan,1,"2017/12/21, 14:59:09",True,"2017/12/22, 08:59:32","2017/12/22, 08:59:32",661859.0,907.0,1,255,ELK - micro service and network latency monitoring,cr - Client Received
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,I'm trying to create a Zipkin 1.31.1 server using Spring Boot 1.3.5.RELEASE to build a fat executable JAR with with Tomcat 8.0.33 embedded in it.
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,This is failing with the following error message:
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,as described in  Spring Boot Enable Async Supported Like in web.xml  even with the suggested fix.
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,"After setting breakpoints in the debugger, I found that the problem is the same as described in"
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,How to Make LogbackValve async Supported
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,which wasn't answered and ultimately had the following improvement request created:
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,ch.qos.logback.access.tomcat.LogbackValve is not async-supported
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,Does anyone have any recommendations how I can workaround this issue?
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,I need help either:
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,OR
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,Any help you can provide would be much appreciated.
Zipkin,46479182,nan,1,"2017/09/29, 01:29:00",True,"2019/06/12, 01:41:21",nan,1029971.0,691.0,1,176,Workaround LogbackValve Lack of Async Support,Thanks!
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,I'm debugging calls made from my express app to another micro-service on my network.
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,I'm receiving 401 errors and I need to get full raw http logs to give to my security team for analysis.
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,I'm looking for some advice on tracking HTTP calls from a micro-service I have deployed on Pivotal Cloud Foundry.
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,I've been doing some research and ran across tools like Zipkin and OpenTracing etc.. but those appear to be more about debugging latency and probably do not show HTTP logs.
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,I've also tried using Morgan/Winston modules but they do not track internal calls.
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,"Morgan is currently what I'm using to log out the basic HTTP codes but it doesn't pick up on my calls from inside my app either, just the ones made to the app itself from the browser."
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,I need to get the full raw HTTP request to assist the security team.
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,I'm using the default logging output with morgan (STDOUT).
Zipkin,46185813,46188800.0,1,"2017/09/13, 00:47:08",True,"2017/09/13, 07:21:00","2017/09/13, 01:25:05",1348798.0,197.0,1,1305,Tracking and logging http calls made internally from a node.js server,I've console logged the headers to see the headers but would like to get them out in a slightly more readable format.
Zipkin,44762961,44763002.0,1,"2017/06/26, 18:16:42",True,"2018/10/31, 15:55:58",nan,6582043.0,403.0,1,886,Spring cloud sleuth with Webservicetemplate,Does Spring cloud sleuth support WebserviceTemplate?
Zipkin,44762961,44763002.0,1,"2017/06/26, 18:16:42",True,"2018/10/31, 15:55:58",nan,6582043.0,403.0,1,886,Spring cloud sleuth with Webservicetemplate,I mean - I have a service which makes 2 service calls - One using RestTemplate and another using Webservicetemplate.
Zipkin,44762961,44763002.0,1,"2017/06/26, 18:16:42",True,"2018/10/31, 15:55:58",nan,6582043.0,403.0,1,886,Spring cloud sleuth with Webservicetemplate,The Rest call is getting displayed in Zipkin and the Soap call using Webservicetemplate is not.
Zipkin,44762961,44763002.0,1,"2017/06/26, 18:16:42",True,"2018/10/31, 15:55:58",nan,6582043.0,403.0,1,886,Spring cloud sleuth with Webservicetemplate,Do I have to add @NewSpan to all my soap calls ?
Zipkin,44762961,44763002.0,1,"2017/06/26, 18:16:42",True,"2018/10/31, 15:55:58",nan,6582043.0,403.0,1,886,Spring cloud sleuth with Webservicetemplate,Is it not automatically done like Resttemplate?
Zipkin,43998861,nan,0,"2017/05/16, 13:25:23",False,"2017/05/16, 13:25:23",nan,2709459.0,11.0,1,485,NullPointerException occurs when spring-cloud-sleuth(boot) integration with spring-integration,I am using spring-amqp via spring-integration.
Zipkin,43998861,nan,0,"2017/05/16, 13:25:23",False,"2017/05/16, 13:25:23",nan,2709459.0,11.0,1,485,NullPointerException occurs when spring-cloud-sleuth(boot) integration with spring-integration,"By the way, when using spring-cloud-sleuth-zipkin, the following error occurred."
Zipkin,43998861,nan,0,"2017/05/16, 13:25:23",False,"2017/05/16, 13:25:23",nan,2709459.0,11.0,1,485,NullPointerException occurs when spring-cloud-sleuth(boot) integration with spring-integration,"spring-cloud-starter-zipkin:1.0.9 
spring-integration-core:4.2.9 
spring-amqp:1.5.6"
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,"In Brave (zipkin tracer), we attach state read by interceptors by controlling the Dispatcher's ExecutorService."
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,It works like this..
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,This works for synchronous requests (since they don't use the dispatcher thread anyway) and normal asynchronous requests.
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,"It doesn't work when there are more in-flight connections than permitted, because asynchronous requests are pushed into a ready queue before they are executed."
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,"The ready queue is not processed by the calling thread of the request, so the re-attach won't work."
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,"When an interceptor runs on a delayed request, it cannot see its calling span which breaks tracing."
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,I was thinking that maybe the application interceptor might not have this problem.
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,"If the host/connection limit was enforced (via the mentioned queue) at the network level, I would be able to coordinate the state by dual-registering an interceptor."
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,This interceptor could re-attach the state without relying on thread locals by mapping application/network level requests.
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,"Sadly, this doesn't work because the host/connection limit is enforced (via the mentioned queue) at the application level, so I'm stumped."
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,I would like to be able to trace requests especially when they are backlogged.
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,Any ideas?
Zipkin,41040727,41045080.0,1,"2016/12/08, 15:30:42",True,"2016/12/08, 19:11:10",nan,1227937.0,762.0,1,137,propagating state even when backlogged,"Hats off to brianm for finding this problem, btw"
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,"I've successfully used Zipkin with Hadoop Htrace in 2.6.0 x32, on Ubuntu 14.04."
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,"Now I want to use it with Hadoop 2.7.3., but I can't even enable Htrace tracing with this hadoop version."
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,"The setup for HTrace in 2.6.0 is different from 2.7.3, as it can be seen here- 2.6.0  and here- 2.7.3 ."
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,In 2.6.0 I'd have this line in the namenode log file :
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,I have nothing like that in 2.7.3 Namenode log file.
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,"Because of not having success with Zipkin, I tried to use the LocalFileSpanReceiver as described in the online tutorial:"
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,"The /var/log/hadoop/ exists, with 777 rights on it, but nothing..."
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,The TracingFsShell example compiles and runs with the following modification:
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,As it can be found in the source code of hadoop in  hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/SpanReceiverHost.java  although the online tutorial does not use that method signature.
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,(Source  diff )
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,"The environment is the same for both Hadoop versions, java 1.7."
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,"Also, hadoop is compiled from source, as the Ubuntu 14.04 is x32 bit."
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,"Hadoop is deployed in fully-distributed mode, using lxc containers."
Zipkin,40694955,40895971.0,2,"2016/11/19, 18:01:00",True,"2017/02/10, 16:03:38","2016/11/30, 15:49:54",1083913.0,3334.0,1,510,HTrace in Hadoop 2.7.3,core-site.xml  for Zipkin ( Zipkin params  here ):
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,In a microservice environment I see two main benefits from tracing requests through all microservice instances over an entire business process.
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"With  Zipkin  there is a tool, which addresses the first issue."
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,But how can tracing be used to unveil failures in your microservice landscape?
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"I definitely want to trace all error afflicted spans, but not each request, where nothing went wrong."
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,As mentioned  here  a custom Sampler could be used.
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"Alternatively, you may register your own Sampler bean definition and programmatically make the decision which requests should be sampled."
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"You can make more intelligent choices about which things to trace, for example, by ignoring successful requests, perhaps checking whether some component is in an error state, or really anything else."
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"So I tried to implement that, but it doesn't work or I used it wrong."
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"So, as the blog post suggested I registered my own Sampler:"
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"And in my controller I create a new Span, which is being tagged as an error if an exception raises"
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"Now, this doesn't work."
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,If I request /calc/a the method Sampler.isSampled(Span) is being called before the Controller method throws a NumberFormatException.
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"This means, when isSampled() checks the Span, it has no tags yet."
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,And the Sampler method is not being called again later in the process.
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"Only if I open the Sampler and allow every span to be sampled, I see my tagged error-span later on in Zipkin."
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,In this case Sampler.isSampled(Span) was called only 1 time but HttpZipkinSpanReporter.report(Span) was executed 3 times.
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"So what would the use case look like, to transmit only traces, which have error spans ?"
Zipkin,40525453,nan,1,"2016/11/10, 12:36:03",False,"2016/11/10, 20:16:52",nan,3524144.0,106.0,1,1500,Spring Sleuth - Tracing Failures,"Is this even a correct way to tag a span with an arbitrary ""error_"" tag ?"
Zipkin,39862738,nan,2,"2016/10/05, 01:28:57",True,"2017/03/05, 15:21:05","2016/10/06, 00:01:08",1263473.0,2439.0,1,4926,Sending trace data to OpenZIpkin using Spring Cloud Sleuth,Update : I have pushed the code to  my repo  so people can take a look there to see what may be going wrong.
Zipkin,39862738,nan,2,"2016/10/05, 01:28:57",True,"2017/03/05, 15:21:05","2016/10/06, 00:01:08",1263473.0,2439.0,1,4926,Sending trace data to OpenZIpkin using Spring Cloud Sleuth,"Edit : I'm almost sure it's the client code NOT POSTing any stats to the server, but neither guides below explain how should this be enabled: is there a configuration setting that I am missing?"
Zipkin,39862738,nan,2,"2016/10/05, 01:28:57",True,"2017/03/05, 15:21:05","2016/10/06, 00:01:08",1263473.0,2439.0,1,4926,Sending trace data to OpenZIpkin using Spring Cloud Sleuth,I have been following the quick starts on both  OpenZipkin  and  Spring Sleuth : I have a running Zipkin server from  docker-zipkin  using the  docker-compose  and Cassandra as the backend:
Zipkin,39862738,nan,2,"2016/10/05, 01:28:57",True,"2017/03/05, 15:21:05","2016/10/06, 00:01:08",1263473.0,2439.0,1,4926,Sending trace data to OpenZIpkin using Spring Cloud Sleuth,I have created and run the  Spring Sleuth sample app  and it seems to be configured correctly to trace calls:
Zipkin,39862738,nan,2,"2016/10/05, 01:28:57",True,"2017/03/05, 15:21:05","2016/10/06, 00:01:08",1263473.0,2439.0,1,4926,Sending trace data to OpenZIpkin using Spring Cloud Sleuth,The logs seem to show that the traces ought to be logged:
Zipkin,39862738,nan,2,"2016/10/05, 01:28:57",True,"2017/03/05, 15:21:05","2016/10/06, 00:01:08",1263473.0,2439.0,1,4926,Sending trace data to OpenZIpkin using Spring Cloud Sleuth,"However, the UI does not show any traces at all, no matter what I do."
Zipkin,39862738,nan,2,"2016/10/05, 01:28:57",True,"2017/03/05, 15:21:05","2016/10/06, 00:01:08",1263473.0,2439.0,1,4926,Sending trace data to OpenZIpkin using Spring Cloud Sleuth,The weird thing is that the  localhost:9411/trace  does show a bunch of traces (they seem to be mostly from Zipkin itself) but there are none from the  zipkin-demo  app.
Zipkin,39862738,nan,2,"2016/10/05, 01:28:57",True,"2017/03/05, 15:21:05","2016/10/06, 00:01:08",1263473.0,2439.0,1,4926,Sending trace data to OpenZIpkin using Spring Cloud Sleuth,"I believe this due to the demo app not sending the traces to the host, but I'm just using  Spring's example app , so what can I be doing wrong?"
Zipkin,39092064,nan,0,"2016/08/23, 06:52:29",False,"2016/08/23, 06:52:29",nan,3120460.0,2485.0,1,316,string sleuth messages in a NoSql store,I would like to store the spring cloud sleuth messages that come in my zipkin server (ala @EnableZipkinStreamServer) into a NoSql store.
Zipkin,39092064,nan,0,"2016/08/23, 06:52:29",False,"2016/08/23, 06:52:29",nan,3120460.0,2485.0,1,316,string sleuth messages in a NoSql store,I know that the original zipkin impl used cassandra which would work for me but I am curious about say MongoDb or CouchBase.
Zipkin,39092064,nan,0,"2016/08/23, 06:52:29",False,"2016/08/23, 06:52:29",nan,3120460.0,2485.0,1,316,string sleuth messages in a NoSql store,I looked in the documents ( http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_zipkin_consumer ) and saw that you can use spring-boot-starter-jdbc for this and it will write into the MySQL DB instance that you define.
Zipkin,39092064,nan,0,"2016/08/23, 06:52:29",False,"2016/08/23, 06:52:29",nan,3120460.0,2485.0,1,316,string sleuth messages in a NoSql store,I don't see an API/SPI for putting this into anything else.
Zipkin,39092064,nan,0,"2016/08/23, 06:52:29",False,"2016/08/23, 06:52:29",nan,3120460.0,2485.0,1,316,string sleuth messages in a NoSql store,Is there one and what is it.
Zipkin,39092064,nan,0,"2016/08/23, 06:52:29",False,"2016/08/23, 06:52:29",nan,3120460.0,2485.0,1,316,string sleuth messages in a NoSql store,I can implement the NoSql portion myself.
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,"This is a continuation of the  shopping cart sample , where we have an external API that allows checkout from a shopping cart."
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,"To recap, we have a flow where we create an empty shopping, add line item(s) and finally checkout."
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,"All the operations above, happen as enrichments through HTTP calls to an external service."
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,We would like to add line items concurrently (as part of the add line items) call.
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,Our current configuration looks like this:
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,"To add line items in parallel, we are using an executor channel."
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,"However, they still seem to be getting processed sequentially when seen in zipkin:"
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,What are we doing wrong?
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,The source for the whole project is on  github  for reference.
Zipkin,38523705,38528610.0,1,"2016/07/22, 13:07:14",True,"2016/07/22, 17:11:03","2017/05/23, 14:51:29",232231.0,75.0,1,250,Enriching in parallel after a split,Thanks!
Zipkin,28187275,28202807.0,2,"2015/01/28, 09:59:02",True,"2018/02/16, 20:31:16",nan,258632.0,732.0,1,1480,How do you use Thrift protocol via corporate Proxy?,I've had a search over the internet but can't seem to find any straightforward instructions on how to use the Thrift protocol from behind a proxy.
Zipkin,28187275,28202807.0,2,"2015/01/28, 09:59:02",True,"2018/02/16, 20:31:16",nan,258632.0,732.0,1,1480,How do you use Thrift protocol via corporate Proxy?,To give you a bit of background - we have a Zipkin instance setup ( https://github.com/twitter/zipkin ) that uses a Cassandra instance ( http://cassandra.apache.org/ ) to store Zipkin traces.
Zipkin,28187275,28202807.0,2,"2015/01/28, 09:59:02",True,"2018/02/16, 20:31:16",nan,258632.0,732.0,1,1480,How do you use Thrift protocol via corporate Proxy?,Our intention is to negotiate over the thrift protocol to a collector that is then responsible for writing traces to Cassandra.
Zipkin,28187275,28202807.0,2,"2015/01/28, 09:59:02",True,"2018/02/16, 20:31:16",nan,258632.0,732.0,1,1480,How do you use Thrift protocol via corporate Proxy?,What conditions have to be in place for us to negotiate successfully via our corporate proxy?
Zipkin,28187275,28202807.0,2,"2015/01/28, 09:59:02",True,"2018/02/16, 20:31:16",nan,258632.0,732.0,1,1480,How do you use Thrift protocol via corporate Proxy?,Do we just have to set certain proxy properties when trying to negotiate or do we have to set something else up that allows this negotiation to happen?
Zipkin,28187275,28202807.0,2,"2015/01/28, 09:59:02",True,"2018/02/16, 20:31:16",nan,258632.0,732.0,1,1480,How do you use Thrift protocol via corporate Proxy?,Any help people can give in this direction with regards to resources and/or an answer would be greatly appreciated.
Zipkin,26016714,26486619.0,1,"2014/09/24, 15:17:26",True,"2014/10/21, 15:21:43",nan,1860517.0,3414.0,0,437,Zipkin failing to start,I am trying to install Zipkin on CentOS.
Zipkin,26016714,26486619.0,1,"2014/09/24, 15:17:26",True,"2014/10/21, 15:21:43",nan,1860517.0,3414.0,0,437,Zipkin failing to start,"When I try to run  bin/collector , I get the following errors:"
Zipkin,26016714,26486619.0,1,"2014/09/24, 15:17:26",True,"2014/10/21, 15:21:43",nan,1860517.0,3414.0,0,437,Zipkin failing to start,I have installed Java 7 and Scala.
Zipkin,26016714,26486619.0,1,"2014/09/24, 15:17:26",True,"2014/10/21, 15:21:43",nan,1860517.0,3414.0,0,437,Zipkin failing to start,Note: These errors are from a second run of bin/collector.
Zipkin,26016714,26486619.0,1,"2014/09/24, 15:17:26",True,"2014/10/21, 15:21:43",nan,1860517.0,3414.0,0,437,Zipkin failing to start,"The first run downloaded libraries, compiled the scala files and then displayed the erorrs, however they were the same errors."
Zipkin,66213289,nan,0,"2021/02/15, 20:16:48",False,"2021/02/15, 20:16:48",nan,2282523.0,117.0,0,50,Can Apollo Server support open tracing on GraphQL Subscriptions?,While the  apollo-opentracing  node library allows Apollo Server to create trace/span information for forwarding to e.g.
Zipkin,66213289,nan,0,"2021/02/15, 20:16:48",False,"2021/02/15, 20:16:48",nan,2282523.0,117.0,0,50,Can Apollo Server support open tracing on GraphQL Subscriptions?,"Zipkin, it only seems to work for GraphQL mutations and queries."
Zipkin,66213289,nan,0,"2021/02/15, 20:16:48",False,"2021/02/15, 20:16:48",nan,2282523.0,117.0,0,50,Can Apollo Server support open tracing on GraphQL Subscriptions?,"I would like to trace Graphql subscriptions, which includes:"
Zipkin,66213289,nan,0,"2021/02/15, 20:16:48",False,"2021/02/15, 20:16:48",nan,2282523.0,117.0,0,50,Can Apollo Server support open tracing on GraphQL Subscriptions?,I'm having difficulty identifying suitable callbacks (or other mechanisms) to help achieve all of these things; if anyone has any advice or pointers that can offer me that would be most appreciated.
Zipkin,64668339,nan,0,"2020/11/03, 20:17:00",False,"2020/11/03, 20:17:00",nan,9375824.0,19.0,0,51,How to troubleshoot whether the tracingservice we configured for ambassador-gateway is working or not?,"Can anyone explain how to troubleshoot whether the tracingservice we configured for ambassador-gateway is working or not, we actually created the tracingService for our ambassador-gateway service as mentioned here  https://www.getambassador.io/docs/latest/topics/running/services/tracing-service/  ."
Zipkin,64668339,nan,0,"2020/11/03, 20:17:00",False,"2020/11/03, 20:17:00",nan,9375824.0,19.0,0,51,How to troubleshoot whether the tracingservice we configured for ambassador-gateway is working or not?,"Later we restarted the ambassador-gateway pod and checked the logs, we found these"
Zipkin,64668339,nan,0,"2020/11/03, 20:17:00",False,"2020/11/03, 20:17:00",nan,9375824.0,19.0,0,51,How to troubleshoot whether the tracingservice we configured for ambassador-gateway is working or not?,"we haven't found any other info regarding tracing service apart from the above two logs, we had sent some requests to ambassador-gateway ,the related traces were not showing up in the zipkin ui we configured , can some please help me how to troubleshoot this issue?"
Zipkin,64668339,nan,0,"2020/11/03, 20:17:00",False,"2020/11/03, 20:17:00",nan,9375824.0,19.0,0,51,How to troubleshoot whether the tracingservice we configured for ambassador-gateway is working or not?,TracingServiceConfiguration:
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,Let's say I have a NodeJS program that has  two  separate instances of an express server running.
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,"I've been able to instrument a program like this via  open telemetry , and have my spans sent/exported successfully to Zipkin."
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,All I needed to do is/was add code like the following to the start of my program.
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,and make sure that the express and http plugins were/are installed
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,This all works great -- except for  one thing .
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,Open Telemetry sees both my express services running as the same  service-main  service.
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,When I instrumented these services  directly  with Zipkin -- I would add the Zipkin middleware to each running express server
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,"Each tracer could be instantiated with its own service name, which allowed each service to have its individual name and show up as a different service in Zipkin."
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,"
( /main ,  /hello , and  /goobye  are all service via a different express service in the above URL)"
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,Is this sort of thing (instrumenting two services in one program) possible with Open Telemetry?
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,Or would I need to separate these two services out into separate programs in order to have each services have an individual name?
Zipkin,62188289,62196735.0,2,"2020/06/04, 09:35:28",True,"2020/06/04, 17:08:54","2020/06/04, 09:40:03",4668.0,156911.0,0,98,Can Open Telemetry Instrument Two Express Services in the Same Node Program?,"This question is less about solving a particular problem, and more about understanding the semantics of Open Telemetry."
Zipkin,61027513,61028338.0,1,"2020/04/04, 14:39:06",True,"2020/04/04, 15:50:12","2020/04/04, 15:43:29",8390634.0,148.0,0,116,Problem to access Deployment with service,I'm trying to run Zipkin on Kubernetes cluster.
Zipkin,61027513,61028338.0,1,"2020/04/04, 14:39:06",True,"2020/04/04, 15:50:12","2020/04/04, 15:43:29",8390634.0,148.0,0,116,Problem to access Deployment with service,This is my Deployment file:
Zipkin,61027513,61028338.0,1,"2020/04/04, 14:39:06",True,"2020/04/04, 15:50:12","2020/04/04, 15:43:29",8390634.0,148.0,0,116,Problem to access Deployment with service,"when I call  /health endpoint from other deployment  inside of my cluster with pod's ip, I can access to it and this is my output:"
Zipkin,61027513,61028338.0,1,"2020/04/04, 14:39:06",True,"2020/04/04, 15:50:12","2020/04/04, 15:43:29",8390634.0,148.0,0,116,Problem to access Deployment with service,"but when I try to access with service name, I can't."
Zipkin,61027513,61028338.0,1,"2020/04/04, 14:39:06",True,"2020/04/04, 15:50:12","2020/04/04, 15:43:29",8390634.0,148.0,0,116,Problem to access Deployment with service,this is the output of  kubectl describe svc zipkin
Zipkin,61027513,61028338.0,1,"2020/04/04, 14:39:06",True,"2020/04/04, 15:50:12","2020/04/04, 15:43:29",8390634.0,148.0,0,116,Problem to access Deployment with service,"and:
    kubectl logs zipkin-9898fcf7f-2nmkx"
Zipkin,61027513,61028338.0,1,"2020/04/04, 14:39:06",True,"2020/04/04, 15:50:12","2020/04/04, 15:43:29",8390634.0,148.0,0,116,Problem to access Deployment with service,can anybody help me?
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,I have a very big problem I'm struggling with for 3 days.
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,I use docker swarm on the remote server.
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,20 microservices are in the same network NetA and stack StackA.
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,Now I want to add Zipkin and Sleuth to my microservices to trace all requests.
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,All microservices are made by docker-compose file that looks like:
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,Now the question is - HOW to ADD Zipkin Server?
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,I've added Zipkin server from Docker Hub Image.
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,"NOW my ZIpkin Service is:
- in a separate network ZIPN
- in a separate stack ZIPST"
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,What should I do to send data do Zipkin by all my microservices?
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,"What URL should i send in properties file:
spring.zipkin.base-url= http://zipkinserver_network_zipkin_server:9411/"
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,"Should it be maybe:
-container name (like my_zipkin_server) - but I use swarm so container name changes dynamically?"
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,- network name?
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,I added an additional network NetA to my Zipkin container but it didn't solved my problem - there are no traces in my Zipkin UI.
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,"Please help me, I spent 4 days with this problem without any success."
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,Zipkin server should be in a separate Stack because will be used by different applications.
Zipkin,54492867,nan,2,"2019/02/02, 13:59:39",True,"2019/02/02, 17:56:24",nan,6022333.0,1285.0,0,677,Docker swarm and communication between containers,There is only ONE case when Zipkin works: when I set Zipkin container name:
Zipkin,51305899,nan,1,"2018/07/12, 15:38:12",False,"2018/07/12, 15:49:09",nan,4298311.0,402.0,0,659,Send log messages as JSON,I am using spring boot in a project and currently exploring the logging behaviour.
Zipkin,51305899,nan,1,"2018/07/12, 15:38:12",False,"2018/07/12, 15:49:09",nan,4298311.0,402.0,0,659,Send log messages as JSON,For that I'm using the zipkin service.
Zipkin,51305899,nan,1,"2018/07/12, 15:38:12",False,"2018/07/12, 15:49:09",nan,4298311.0,402.0,0,659,Send log messages as JSON,I have exported my logs to a json file using proper logback.xml:
Zipkin,51305899,nan,1,"2018/07/12, 15:38:12",False,"2018/07/12, 15:49:09",nan,4298311.0,402.0,0,659,Send log messages as JSON,Is there a way so that I could insert a jsonObject in my message part of the log.
Zipkin,51305899,nan,1,"2018/07/12, 15:38:12",False,"2018/07/12, 15:49:09",nan,4298311.0,402.0,0,659,Send log messages as JSON,Something like:
Zipkin,51305899,nan,1,"2018/07/12, 15:38:12",False,"2018/07/12, 15:49:09",nan,4298311.0,402.0,0,659,Send log messages as JSON,I have tried searching a way extensively but to no avail.
Zipkin,51305899,nan,1,"2018/07/12, 15:38:12",False,"2018/07/12, 15:49:09",nan,4298311.0,402.0,0,659,Send log messages as JSON,Is it even possible?
Zipkin,21035841,21036699.0,1,"2014/01/10, 05:42:24",True,"2015/05/14, 09:44:09","2015/05/14, 09:44:09",1707629.0,332.0,0,81,Can BAM and CEP monitor requests from client like Zipkin,"I am wondering if I can use BAM and CEP to monitor requests from client, and even find the bottleneck of the service."
Zipkin,21035841,21036699.0,1,"2014/01/10, 05:42:24",True,"2015/05/14, 09:44:09","2015/05/14, 09:44:09",1707629.0,332.0,0,81,Can BAM and CEP monitor requests from client like Zipkin,"I found zipkin, a project that could do this, but the base of my application is WSO2, I don't want to get other projects from scratch."
Zipkin,66969097,nan,0,"2021/04/06, 15:57:31",False,"2021/04/06, 15:57:31",nan,3774803.0,193.0,0,22,OpenTelemetry collector scaling,We are going to have a high load on telemetry service.
Zipkin,66969097,nan,0,"2021/04/06, 15:57:31",False,"2021/04/06, 15:57:31",nan,3774803.0,193.0,0,22,OpenTelemetry collector scaling,"I'm looking for solutions, which be able to scale collector and backend(zipkin)"
Zipkin,66969097,nan,0,"2021/04/06, 15:57:31",False,"2021/04/06, 15:57:31",nan,3774803.0,193.0,0,22,OpenTelemetry collector scaling,There is   solution for scaling zipkin.
Zipkin,66969097,nan,0,"2021/04/06, 15:57:31",False,"2021/04/06, 15:57:31",nan,3774803.0,193.0,0,22,OpenTelemetry collector scaling,Seems simple - just use internal balancing
Zipkin,66969097,nan,0,"2021/04/06, 15:57:31",False,"2021/04/06, 15:57:31",nan,3774803.0,193.0,0,22,OpenTelemetry collector scaling,"But, I can't find examples for using multiple openTelemetry collectors."
Zipkin,66969097,nan,0,"2021/04/06, 15:57:31",False,"2021/04/06, 15:57:31",nan,3774803.0,193.0,0,22,OpenTelemetry collector scaling,"There is no troubles to run several instances of collector, but how can I say &quot;myApp&quot; to balance beetwin them?"
Zipkin,66969097,nan,0,"2021/04/06, 15:57:31",False,"2021/04/06, 15:57:31",nan,3774803.0,193.0,0,22,OpenTelemetry collector scaling,There is no such option in exptorters.
Zipkin,66969097,nan,0,"2021/04/06, 15:57:31",False,"2021/04/06, 15:57:31",nan,3774803.0,193.0,0,22,OpenTelemetry collector scaling,What is the right way to scale such system?
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,i want to implement distributed tracing(using sleuth-otel) in microservice and it should export the trace details to Google cloud tracing Api(not zipkin).
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,"application structure is like this,
service1 -&gt; pubsub -&gt; service2 -&gt; pubsub -&gt; service3
i tried the following"
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,"A) In service1,"
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,"my dobut in this is,"
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,=&gt; why i did't see the application name that i mentioned in application.properties.
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,Is there any other thing i missed?
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,=&gt; i also want to change this default traceId with my own value.
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,is it possible?
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,if yes what should i do for that?
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,(sample code pls)
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,B) in service2
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,doubt
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,=&gt; why i did't get the same traceId logged in service1 here?
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,(it doesn't log spanId aswell)
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,=&gt; How to get the same traceId in service2?
Zipkin,66885062,nan,0,"2021/03/31, 12:30:18",False,"2021/04/01, 06:57:28","2021/04/01, 06:57:28",15260521.0,1.0,0,38,microservice distributed tracing not working,"=&gt; To export the traces to google cloud trace API, this dependency is enough or do i need to add any config or anything?"
Zipkin,65957785,nan,1,"2021/01/29, 17:59:40",False,"2021/02/04, 01:32:16",nan,6206060.0,3.0,0,37,Modifying the micronaut tracing HttpClientSender DEFAULT_PATH to allow data send to New Relic Trace API,Micronaut has a zipkin tracing library where you can easily override the zipkin server http endpoint like this:
Zipkin,65957785,nan,1,"2021/01/29, 17:59:40",False,"2021/02/04, 01:32:16",nan,6206060.0,3.0,0,37,Modifying the micronaut tracing HttpClientSender DEFAULT_PATH to allow data send to New Relic Trace API,"Now by default it adds  DEFAULT_PATH = &quot;/api/v2/spans&quot;  to it, but for new relic the entire path should be like:"
Zipkin,65957785,nan,1,"2021/01/29, 17:59:40",False,"2021/02/04, 01:32:16",nan,6206060.0,3.0,0,37,Modifying the micronaut tracing HttpClientSender DEFAULT_PATH to allow data send to New Relic Trace API,"I tried with Bean replacement and factories, but I just cannot find a proper clean solution."
Zipkin,65957785,nan,1,"2021/01/29, 17:59:40",False,"2021/02/04, 01:32:16",nan,6206060.0,3.0,0,37,Modifying the micronaut tracing HttpClientSender DEFAULT_PATH to allow data send to New Relic Trace API,The only solution I have found is by copying the entire  public final class HttpClientSender extends Sender  class  as  public final class NewRelicSender extends Sender  and just modifying the constructor:
Zipkin,65957785,nan,1,"2021/01/29, 17:59:40",False,"2021/02/04, 01:32:16",nan,6206060.0,3.0,0,37,Modifying the micronaut tracing HttpClientSender DEFAULT_PATH to allow data send to New Relic Trace API,"It does work, but I feel like there is a better way to do this."
Zipkin,65957785,nan,1,"2021/01/29, 17:59:40",False,"2021/02/04, 01:32:16",nan,6206060.0,3.0,0,37,Modifying the micronaut tracing HttpClientSender DEFAULT_PATH to allow data send to New Relic Trace API,I am copying almost 300 lines of code to replace 1.
Zipkin,65957785,nan,1,"2021/01/29, 17:59:40",False,"2021/02/04, 01:32:16",nan,6206060.0,3.0,0,37,Modifying the micronaut tracing HttpClientSender DEFAULT_PATH to allow data send to New Relic Trace API,What would be the Micronaut way of doing this?
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,"All,
I am trying to integrate OpenTelemetry collector with Prometheus in a .Net application."
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,I am using OTLP Exporter in the application and running OpenTelemetry Collector as a windows service on same system.
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,Please find the below 'config.yaml' I am using for collector
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,Here is the 'prometheus.yml'
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,I don't see the metrics exported to Prometheus.
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,I am creating some counters programatically in the application and looking for the same in Prometheus.
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,"Please note, I tried to export traces to Zipkin and it worked fine."
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,Any help is greatly appreciated.
Zipkin,65312795,nan,0,"2020/12/15, 21:54:38",False,"2020/12/15, 21:54:38",nan,5820341.0,121.0,0,304,OpenTelemetry collector export to Prometheus is not working,Prometheus UI target health is showing 'UP' as below.
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,"I have 4 spring-boot applications (A, B, C and D)."
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,The lifecycle of a transaction is as follows :
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,"Now for a single transaction I would expect a single trace across all four application, but instead I get"
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,"I would have uploaded the pictures to show the zipkin spans, but for some reason I am not able to do so."
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,All the above applications are Spring boot applications and they utilize spring-cloud-sleuth for producing transactions traces.
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,I am relying on spring boot's autoconfiguration and these are the properties that I have set in all the applications:
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,I am not able to understand what's exactly happening here.
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,Why the spans are scattered across 2 traces and not one?
Zipkin,64818612,64845200.0,1,"2020/11/13, 11:41:01",True,"2021/02/10, 12:23:11","2021/02/10, 12:23:11",3244615.0,160.0,0,87,Spring cloud Sleuth starts a new trace instead of continuing spans in a single trace,I am using spring-boot 2.3.3 and spring-cloud-dependencies Hoxton.SR8.
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,I have a beginner question with Docker Compose.
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,I am trying to extend the  docker-compose-slim.yml  example file  from  Zipkin GitHub repository .
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,I need to change it so that it can include a simple FastAPI app that I have written.
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,"Unfortunately, I cannot make them connect to each other."
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,"FastAPI gets rejected when it attempts to send a POST request to the Zipkin container, even though they are both connected to the same network with explicit links and port mapping defined in the YAML file."
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,"However, I am able to connect to both of them from the host, however."
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,Could you please tell me what I have done wrong?
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,Here is the error message:
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,Here is the Docker Compose YAML file:
Zipkin,64214980,nan,1,"2020/10/05, 22:28:08",False,"2020/10/06, 10:02:02","2020/10/06, 10:02:02",2136168.0,33.0,0,287,Containers launched with Docker-Compose cannot connect to each other,Here is the Dockerfile:
Zipkin,63935251,nan,0,"2020/09/17, 12:36:59",False,"2020/09/17, 16:41:14","2020/09/17, 16:41:14",13208893.0,1.0,0,49,Opentracing Brave generates same TraceId,"First, we've thought that problem is in the incompatible version of Brave and Zipkin."
Zipkin,63935251,nan,0,"2020/09/17, 12:36:59",False,"2020/09/17, 16:41:14","2020/09/17, 16:41:14",13208893.0,1.0,0,49,Opentracing Brave generates same TraceId,"We have fixed it and now it's exactly like in the snippet above, but the problem remain."
Zipkin,63935251,nan,0,"2020/09/17, 12:36:59",False,"2020/09/17, 16:41:14","2020/09/17, 16:41:14",13208893.0,1.0,0,49,Opentracing Brave generates same TraceId,While debugging we've found out that the problem only appears when in Braves class  ThreadLocalCurrentTraceContext  is creating a new Span in method
Zipkin,63935251,nan,0,"2020/09/17, 12:36:59",False,"2020/09/17, 16:41:14","2020/09/17, 16:41:14",13208893.0,1.0,0,49,Opentracing Brave generates same TraceId,with  local.get()  which return not null value.
Zipkin,63935251,nan,0,"2020/09/17, 12:36:59",False,"2020/09/17, 16:41:14","2020/09/17, 16:41:14",13208893.0,1.0,0,49,Opentracing Brave generates same TraceId,local  is  final ThreadLocal&lt;TraceContext&gt;  variable and it only works when get method returns null value.
Zipkin,63935251,nan,0,"2020/09/17, 12:36:59",False,"2020/09/17, 16:41:14","2020/09/17, 16:41:14",13208893.0,1.0,0,49,Opentracing Brave generates same TraceId,Any idea how could we solve this problem or other possibilities where can be problem?
Zipkin,63935251,nan,0,"2020/09/17, 12:36:59",False,"2020/09/17, 16:41:14","2020/09/17, 16:41:14",13208893.0,1.0,0,49,Opentracing Brave generates same TraceId,Thanks.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,my spring boot microservices run currently with Spring-Boot 2.2.9.RELEASE
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,I also still use
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,dependencies.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,Now I wanted switch to Spring Boot 2.3.2.RELEASE.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,I can compile the code without any errors.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,After start the service I see this
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,INFO: HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,log message.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,There are now errors.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,And then the deployment process stopped.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,Nothing happens.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,At the moment I have no idea why it's not running.
Zipkin,63331441,nan,1,"2020/08/10, 00:22:03",False,"2020/08/30, 22:28:14",nan,6284144.0,11.0,0,500,After upgrade Spring Boot 2.2.9.RELEASE to 2.3.2.RELEASE service isn&#39;t working,Has someone some tip?
Zipkin,62095169,nan,0,"2020/05/30, 00:56:28",False,"2020/05/30, 00:56:28",nan,7775961.0,370.0,0,33,Unable to use spring cloud cli (416 range not satisfiable),"I am trying to use Spring Cloud CLI to start eureka, configserver and zipkin with this command:"
Zipkin,62095169,nan,0,"2020/05/30, 00:56:28",False,"2020/05/30, 00:56:28",nan,7775961.0,370.0,0,33,Unable to use spring cloud cli (416 range not satisfiable),but I get this message:
Zipkin,62095169,nan,0,"2020/05/30, 00:56:28",False,"2020/05/30, 00:56:28",nan,7775961.0,370.0,0,33,Unable to use spring cloud cli (416 range not satisfiable),It seems like the CLI can't find some packages in the maven repos.
Zipkin,62095169,nan,0,"2020/05/30, 00:56:28",False,"2020/05/30, 00:56:28",nan,7775961.0,370.0,0,33,Unable to use spring cloud cli (416 range not satisfiable),Is there a solution for this?
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,"I have 3 spring boot applications ,  spring-cloud-gateway  ,  spring-boot-resource-server  and  spring-boot-authentication-server ."
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,"My spring-boot-resource-server is protected via Oauth.So whenever an api is requested , it is first authenticated by authentication server using ( check_token )  enpoint."
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,"So now,i can see the request flow from gateway to resource server in zipkin as single flow"
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,Request-&gt; Gateway -&gt; Resource Server
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,but the ( check_token ) request shows up as a separate request.
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,Resource Server -&gt; Auth Server
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,If my understanding is right it should be a part of the previous request
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,Request-&gt; Gateway -&gt; Resource Server -&gt;  Auth Server
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,Am i missing something ?
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,I searched on the web and found 2 results :
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,"From the above link i think i need to register TraceFilter before my OauthFilter, but i don't seem to find any Trace filter in current release."
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,I am using  Greenwich.SR3  in all 3 applications.
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,Update
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,I have printed filter chain if it helps.
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,update 2 :
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,Updated Spring Cloud to Hoxton.SR4
Zipkin,62049290,nan,0,"2020/05/27, 20:31:42",False,"2020/05/31, 16:42:21","2020/06/20, 12:12:55",11153640.0,1.0,0,79,Existing Sleuth trace id not passed to Oauth2 Authentication Server,Sample Application
Zipkin,61571538,61571674.0,1,"2020/05/03, 11:22:12",True,"2020/05/03, 11:35:32",nan,nan,nan,0,366,"@EnableZipkinServer vs. @EnableZipkinStreamServer, why are two annotation for the same thing","I am creating a zipkin server, and i have seen tutorial which annotated the boostrap class with @EnableZipkinServer and others with @EnableZipkinStreamServer."
Zipkin,61571538,61571674.0,1,"2020/05/03, 11:22:12",True,"2020/05/03, 11:35:32",nan,nan,nan,0,366,"@EnableZipkinServer vs. @EnableZipkinStreamServer, why are two annotation for the same thing",I am having a hard time understand what is difference between these two.
Zipkin,61571538,61571674.0,1,"2020/05/03, 11:22:12",True,"2020/05/03, 11:35:32",nan,nan,nan,0,366,"@EnableZipkinServer vs. @EnableZipkinStreamServer, why are two annotation for the same thing",Are they interchangeable?
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,I have Spring cloud sleuth(and also zipkin) configured in my application.
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,"I have a controller which calls service, which in turn calls repository and finally database."
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,"The setup works fine, Sleuth is generating span id's and It is visible in zipkin as well."
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,I wanted to try creating span's across multiple internal beans and methods.
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,I came across  Managing Spans with Annotations .
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,This does not seem to work.
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,"When I use any of the annotations mentioned here like  @NewSpan  or  @ContinueSpan , Autowiring stops working."
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,My service class which is autowired in Controller is  null .
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,If I remove these annotations everything works again.
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,I am using.
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,spring-boot 2.2.5.RELEASE
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,"spring-cloud.version Hoxton.SR3 
I have these dependencies in my pom"
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,Here is a sample code
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,And my Service class is like
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,"My guess is, Spring-Aop has something to do with it."
Zipkin,60974758,nan,1,"2020/04/01, 18:38:34",False,"2020/04/02, 12:19:10","2020/04/02, 12:19:10",6565093.0,20266.0,0,180,Spring cloud sleuth Annotations: Autowiring doesn&#39;t work,Any idea?
Zipkin,59405521,nan,1,"2019/12/19, 10:04:32",True,"2019/12/19, 10:24:24",nan,3381825.0,495.0,0,52,Application start even if Kakfa is down,"I have been working on spring cloud sleuth to push traces to zipkin, We are pushing traces to Kafka with the help of  spring-cloud-starter-stream-kafka  and  spring-cloud-sleuth-stream"
Zipkin,59405521,nan,1,"2019/12/19, 10:04:32",True,"2019/12/19, 10:24:24",nan,3381825.0,495.0,0,52,Application start even if Kakfa is down,Below are the dependencies I have added to my application
Zipkin,59405521,nan,1,"2019/12/19, 10:04:32",True,"2019/12/19, 10:24:24",nan,3381825.0,495.0,0,52,Application start even if Kakfa is down,And my application property just have kafka broker which is by default pushing traces to  sleuth  topic  spring.cloud.stream.kafka.binder.brokers=locahost:9092
Zipkin,59405521,nan,1,"2019/12/19, 10:04:32",True,"2019/12/19, 10:24:24",nan,3381825.0,495.0,0,52,Application start even if Kakfa is down,Everything is working fine as of now  but I am not able to figure out if we can start the application if Kafka is down ?
Zipkin,59405521,nan,1,"2019/12/19, 10:04:32",True,"2019/12/19, 10:24:24",nan,3381825.0,495.0,0,52,Application start even if Kakfa is down,"From the logs I see application starts, but since it will be not able to connect to kafka we will be not able to access any of the endpoints (Added some bits of logs below)."
Zipkin,59405521,nan,1,"2019/12/19, 10:04:32",True,"2019/12/19, 10:24:24",nan,3381825.0,495.0,0,52,Application start even if Kakfa is down,Looking for this solution because pushing traces is not the primary function of my application.
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,We use a lot of Finagle  Filter s and  Service s in our code.
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,"However, we don't support Zipkin in our infrastructure."
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,"At times there's a need to  trace  an incoming request across the chain of Filters/Services, especially in the face of concurrent requests."
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,What's the most non-intrusive way to get such a functionality?
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,"Goal: 
Would be great if Finagle itself provided an additional  apply  method like so:"
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,Then the subclasses could opt-in to using this method instead.
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,"The last argument being the identifier that every Filter/Service in the chain could ""append to""."
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,Example:
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,The goal is to have something that can uniquely trace/log the series of filter/services effectively.
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,How may I go about by extending Finagle via traits/monkey-patching?
Zipkin,58349450,nan,1,"2019/10/12, 02:15:35",False,"2019/10/12, 23:01:05",nan,609074.0,10252.0,0,58,What&#39;s the best way to pass a common request identifier across a chain of Filter/Services for logging in Finagle/Scala?,Or is this not doable?
Zipkin,58170407,nan,0,"2019/09/30, 17:53:10",False,"2019/10/12, 01:21:15","2019/10/01, 15:21:31",853928.0,43.0,0,536,Propagating extra fields/New fields distributed tracing solution for SpringCloud. configuring in BootClass,Added dependency for cloud sleuth and zipkin dependency of version below in pom.xml
Zipkin,58170407,nan,0,"2019/09/30, 17:53:10",False,"2019/10/12, 01:21:15","2019/10/01, 15:21:31",853928.0,43.0,0,536,Propagating extra fields/New fields distributed tracing solution for SpringCloud. configuring in BootClass,Simple spring boot class.
Zipkin,58170407,nan,0,"2019/09/30, 17:53:10",False,"2019/10/12, 01:21:15","2019/10/01, 15:21:31",853928.0,43.0,0,536,Propagating extra fields/New fields distributed tracing solution for SpringCloud. configuring in BootClass,Should i add filter to set new fields?
Zipkin,58170407,nan,0,"2019/09/30, 17:53:10",False,"2019/10/12, 01:21:15","2019/10/01, 15:21:31",853928.0,43.0,0,536,Propagating extra fields/New fields distributed tracing solution for SpringCloud. configuring in BootClass,resultant LOG is no showing the newly added property :
Zipkin,58170407,nan,0,"2019/09/30, 17:53:10",False,"2019/10/12, 01:21:15","2019/10/01, 15:21:31",853928.0,43.0,0,536,Propagating extra fields/New fields distributed tracing solution for SpringCloud. configuring in BootClass,"
does something i am missing ..?"
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,I have a system that distributing tasks with queues.
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,I have N services that communicate with each other with N queues.
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,My system diagram looks like this:
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,"As you can see, the outcome results performance depends on multiple services."
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,"So, diagnose failures (or performance issues) is hard task - it's very hard to find which part of the chain was the problem."
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,I was looking for a tool that will provide me a map of all the processing traces.
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,Something like this:
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,"With a visual diagram like this, I can find which part of the chain was the problem."
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,Easily.
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,"So, I found a few solutions that aim to solve this problem:"
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,"Unfortunately, it looks very difficult to:"
Zipkin,57767704,nan,0,"2019/09/03, 11:20:33",False,"2019/09/03, 11:20:33",nan,1846762.0,4659.0,0,40,Easy way to analyze traces in Micro Services architecture,Am I wondering if I can have an easier solution for this simple problem?
Zipkin,56161894,nan,0,"2019/05/16, 09:14:30",False,"2019/05/16, 09:14:30",nan,5404372.0,193.0,0,339,Trace Id not preserved when calling netflix FeignClient,I am using spring-cloud-sleuth-zipkin and spring-cloud-starter-sleuth as dependency.
Zipkin,56161894,nan,0,"2019/05/16, 09:14:30",False,"2019/05/16, 09:14:30",nan,5404372.0,193.0,0,339,Trace Id not preserved when calling netflix FeignClient,When i call a netflix FeignClient call TraceId changes...
Zipkin,56161894,nan,0,"2019/05/16, 09:14:30",False,"2019/05/16, 09:14:30",nan,5404372.0,193.0,0,339,Trace Id not preserved when calling netflix FeignClient,When i call TesterClient the traceId changes?
Zipkin,56161894,nan,0,"2019/05/16, 09:14:30",False,"2019/05/16, 09:14:30",nan,5404372.0,193.0,0,339,Trace Id not preserved when calling netflix FeignClient,How could i preserve the same traceId?
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,I want to pass the span across multiple threads and want to get trace.
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,I have different worker threads.
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,while running I am getting  You may have forgotten to close or detach null
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,pom.xml
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,WorkerSpan1.java
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,WorkerSpan2.java
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,here I am passing span because tracer.getCurrentSpan() is returning null.
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,and my controller
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,TestController.java
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,While running I am not getting trace in zipkin and getting warning:
Zipkin,55090908,nan,1,"2019/03/10, 20:23:17",False,"2019/04/05, 15:37:34",nan,11180764.0,1.0,0,678,Not able to close span in multi thread environment,I also want to know how to pass trace information across different threads.
Zipkin,54445701,nan,1,"2019/01/30, 18:57:36",False,"2019/01/30, 19:00:50",nan,6022333.0,1285.0,0,35,Docker containers on different stacks and networks communicationg each other,in my microservices architecture application I'd like to add Sleuth and Zipkin server (image from Docker Hub).
Zipkin,54445701,nan,1,"2019/01/30, 18:57:36",False,"2019/01/30, 19:00:50",nan,6022333.0,1285.0,0,35,Docker containers on different stacks and networks communicationg each other,Everything works fine locally - each microservice sends data to Zipkin server.
Zipkin,54445701,nan,1,"2019/01/30, 18:57:36",False,"2019/01/30, 19:00:50",nan,6022333.0,1285.0,0,35,Docker containers on different stacks and networks communicationg each other,The problem is more complicated when I deployed all microservices on the server  -Zipkin Web UI is empty - no traces.
Zipkin,54445701,nan,1,"2019/01/30, 18:57:36",False,"2019/01/30, 19:00:50",nan,6022333.0,1285.0,0,35,Docker containers on different stacks and networks communicationg each other,"In application.properties we can explicitly set the url to the Zipkin server:
spring.zipkin.base-url:  http://10.0.44.1:9411/"
Zipkin,54445701,nan,1,"2019/01/30, 18:57:36",False,"2019/01/30, 19:00:50",nan,6022333.0,1285.0,0,35,Docker containers on different stacks and networks communicationg each other,I thought that all containers can communicate each other in different stacks/networks but it's not true.
Zipkin,54445701,nan,1,"2019/01/30, 18:57:36",False,"2019/01/30, 19:00:50",nan,6022333.0,1285.0,0,35,Docker containers on different stacks and networks communicationg each other,What should I do to publish my Zipkin server/container to be availabe for all containers from all stacks/networks?
Zipkin,54445701,nan,1,"2019/01/30, 18:57:36",False,"2019/01/30, 19:00:50",nan,6022333.0,1285.0,0,35,Docker containers on different stacks and networks communicationg each other,Is there a possibility to do this using Portainer?
Zipkin,54445701,nan,1,"2019/01/30, 18:57:36",False,"2019/01/30, 19:00:50",nan,6022333.0,1285.0,0,35,Docker containers on different stacks and networks communicationg each other,Thank you in advance
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,I and my team is stuck in a very silly case of HttpClientErrorException let me first give overview of my work scenario.
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,I have a micro-service stack with following things:
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,In scheduler I am using spring Scheduler with cron
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,The purpose of code in prepareDataForSync is to obtain data about each hotel and check current state and if any change is deducted pass it on to third party.
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,Now here comes the real problem:
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,I make a rest service call to obtain list of hotels from my scheduler:
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,All these services are running in docker environment with each service having own container and communicating via docker networking.
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,"Now when I start the services including Hotel and Scheduler, everything work fine for few hours but then I get following exception in my logs and service is no more syncing with third party."
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,At line 1159 I have a service call only:
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,"I checked logs in corresponding Hotel service, logs there displays the service request was received and data was collected and written to response stream but as the exception shows I never received any response and get this Exception."
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,Logs from Hotel service:
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,"If I restart the Scheduler service it starts working again, but again in few hours i have same issue."
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,"As a workaround currently I have set up a cron on the server to restart the service every 2 hours but it is really a bad workaround, I can't rely on this in the production and need to get to the root of problem."
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,"I have have googled and tried to go through any HttpClientErrorException based question, but nothing made sense to me."
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,Please let me know if more information is required from my end.
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,EDIT:
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,Docker Stats Output:
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,0b7d20c5a566        schedular            0.12%               1.365GiB / 31.41GiB   4.34%               0B / 0B             3.27MB / 0B         64
Zipkin,53985744,nan,1,"2018/12/31, 11:25:55",False,"2019/01/02, 13:55:05","2019/01/02, 13:55:05",3701985.0,407.0,0,156,HttpClientErrorException on Inter service communication after certain period of service running time,TOP Output inside container
Zipkin,53391503,53393882.0,2,"2018/11/20, 12:58:19",True,"2018/11/20, 15:18:12",nan,1270045.0,330.0,0,52,Dependency convergence from the same dependency,i have s basic spring boot/cloud application based on
Zipkin,53391503,53393882.0,2,"2018/11/20, 12:58:19",True,"2018/11/20, 15:18:12",nan,1270045.0,330.0,0,52,Dependency convergence from the same dependency,But i need spring-cloud-sleuth-zipkin with at least 1.3.x.
Zipkin,53391503,53393882.0,2,"2018/11/20, 12:58:19",True,"2018/11/20, 15:18:12",nan,1270045.0,330.0,0,52,Dependency convergence from the same dependency,By importing 1.3.5.RELEASE i get a strange error.
Zipkin,53391503,53393882.0,2,"2018/11/20, 12:58:19",True,"2018/11/20, 15:18:12",nan,1270045.0,330.0,0,52,Dependency convergence from the same dependency,It seems like the same dependency creates a convergence.
Zipkin,53391503,53393882.0,2,"2018/11/20, 12:58:19",True,"2018/11/20, 15:18:12",nan,1270045.0,330.0,0,52,Dependency convergence from the same dependency,Is that easily solavble?
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,I have Spring Cloud Sleuth (2.0.2.RELEASE) working within a (partially) reactive class in some web based request/response system.
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,The code is something like this:
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,"As there are quite many responses coming in all the time, this method is being called several hundreds of times for one single request."
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,I suspect that this call with the  .publishOn  leads to hundreds and thousands of  async  spans in Zipkin (see attached screenshot).
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,At least I assume that the spans are from that because it is what I understand from  the documentation
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,"So my first question would be:
How can I associate a name for such async threads?"
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,I don't have a place to put  @SpanName  here.
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,"As a follow up, is there any way to NOT collect these spans?"
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,"I don't need them, they fill up our Zipkin storage, but I also don't want to disable reactive or Sleuth in general since it is needed in other places ..."
Zipkin,53154813,53185008.0,1,"2018/11/05, 14:50:58",True,"2018/11/07, 09:18:45","2018/11/05, 15:23:57",53321.0,5669.0,0,405,How to name Span in Spring Sleuth in a reactive calling chaining,Screenshot from Zipkin
Zipkin,52156749,nan,2,"2018/09/04, 01:26:56",False,"2018/10/12, 20:33:48",nan,4296607.0,2970.0,0,122,How to track the span through different dependencies with Sleuth?,I'm learning how to track my distributed processes through all the microservices.
Zipkin,52156749,nan,2,"2018/09/04, 01:26:56",False,"2018/10/12, 20:33:48",nan,4296607.0,2970.0,0,122,How to track the span through different dependencies with Sleuth?,"I've been playing with Sleuth, Zipkin and different microservices, and it works fantastic!"
Zipkin,52156749,nan,2,"2018/09/04, 01:26:56",False,"2018/10/12, 20:33:48",nan,4296607.0,2970.0,0,122,How to track the span through different dependencies with Sleuth?,But when I try to do the same in a project interacting between the different dependencies I can not create the same behavior.
Zipkin,52156749,nan,2,"2018/09/04, 01:26:56",False,"2018/10/12, 20:33:48",nan,4296607.0,2970.0,0,122,How to track the span through different dependencies with Sleuth?,This image show how currently is working different microservices.
Zipkin,52156749,nan,2,"2018/09/04, 01:26:56",False,"2018/10/12, 20:33:48",nan,4296607.0,2970.0,0,122,How to track the span through different dependencies with Sleuth?,This is the diagram of microservices:
Zipkin,52156749,nan,2,"2018/09/04, 01:26:56",False,"2018/10/12, 20:33:48",nan,4296607.0,2970.0,0,122,How to track the span through different dependencies with Sleuth?,And this image show how works an application with dependencies.
Zipkin,52156749,nan,2,"2018/09/04, 01:26:56",False,"2018/10/12, 20:33:48",nan,4296607.0,2970.0,0,122,How to track the span through different dependencies with Sleuth?,This is the diagram of application with dependencies:
Zipkin,52156749,nan,2,"2018/09/04, 01:26:56",False,"2018/10/12, 20:33:48",nan,4296607.0,2970.0,0,122,How to track the span through different dependencies with Sleuth?,"I wonder, is it possible to create the same behavior using dependencies as with microservices?"
Zipkin,51934410,51934542.0,1,"2018/08/20, 18:53:21",True,"2018/10/30, 08:56:20",nan,6872018.0,5128.0,0,2290,Project build error: &#39;dependencies.dependency.version&#39; for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing,"I am developing Microservices specifically  ""zipkin-service"" ."
Zipkin,51934410,51934542.0,1,"2018/08/20, 18:53:21",True,"2018/10/30, 08:56:20",nan,6872018.0,5128.0,0,2290,Project build error: &#39;dependencies.dependency.version&#39; for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing,I have taken a reference from  link:  https://www.baeldung.com/tracing-services-with-zipkin .
Zipkin,51934410,51934542.0,1,"2018/08/20, 18:53:21",True,"2018/10/30, 08:56:20",nan,6872018.0,5128.0,0,2290,Project build error: &#39;dependencies.dependency.version&#39; for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing,Could anyone please guide what's the issue ?
Zipkin,51934410,51934542.0,1,"2018/08/20, 18:53:21",True,"2018/10/30, 08:56:20",nan,6872018.0,5128.0,0,2290,Project build error: &#39;dependencies.dependency.version&#39; for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing,"When I added below dependencies, then I get the"
Zipkin,51934410,51934542.0,1,"2018/08/20, 18:53:21",True,"2018/10/30, 08:56:20",nan,6872018.0,5128.0,0,2290,Project build error: &#39;dependencies.dependency.version&#39; for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing,Error:
Zipkin,51934410,51934542.0,1,"2018/08/20, 18:53:21",True,"2018/10/30, 08:56:20",nan,6872018.0,5128.0,0,2290,Project build error: &#39;dependencies.dependency.version&#39; for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing,Project build error: 'dependencies.dependency.version' for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing.
Zipkin,51934410,51934542.0,1,"2018/08/20, 18:53:21",True,"2018/10/30, 08:56:20",nan,6872018.0,5128.0,0,2290,Project build error: &#39;dependencies.dependency.version&#39; for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing,pom.xml
Zipkin,51934410,51934542.0,1,"2018/08/20, 18:53:21",True,"2018/10/30, 08:56:20",nan,6872018.0,5128.0,0,2290,Project build error: &#39;dependencies.dependency.version&#39; for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing,ZipkinServiceApplication.java
Zipkin,51534836,nan,1,"2018/07/26, 11:54:53",False,"2018/10/15, 23:04:45",nan,2410830.0,693.0,0,510,Docker-compose: Exposing Prometheus metrics on Openzipkin,I am running  open-zipkin  with docker-compose for testing purposes.
Zipkin,51534836,nan,1,"2018/07/26, 11:54:53",False,"2018/10/15, 23:04:45",nan,2410830.0,693.0,0,510,Docker-compose: Exposing Prometheus metrics on Openzipkin,The end goal is to have open-zipkin running so that I can successfully do a  curl localhost:9411/prometheus  on the zipkin container itself and view prometheus metrics.
Zipkin,51534836,nan,1,"2018/07/26, 11:54:53",False,"2018/10/15, 23:04:45",nan,2410830.0,693.0,0,510,Docker-compose: Exposing Prometheus metrics on Openzipkin,How can I expose the metrics like this?
Zipkin,51534836,nan,1,"2018/07/26, 11:54:53",False,"2018/10/15, 23:04:45",nan,2410830.0,693.0,0,510,Docker-compose: Exposing Prometheus metrics on Openzipkin,For reference the zipkin portion of the  docker-compose file looks like this:
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,I am using spring boot in a project and currently exploring the logging behaviour.
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,For that I'm using the zipkin service.
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,My  logback.xml  is as follows:
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,"Now for what I have understood, to log in json, you need a custom logger implementation which I have done as follows:"
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,My message class is:
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,Now in my controller class I'm using my custom logger as:
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,My logs end up in kibana as:
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,Now instead of this I would want my  message  part of the logs as a  json  entry.
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,Where am I going wrong?
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,I have tried searching a way extensively but to no avail.
Zipkin,51444018,nan,1,"2018/07/20, 16:47:59",False,"2018/07/24, 11:56:29",nan,4298311.0,402.0,0,330,The message attribute in the JSON log has backslash,Is it even possible?
Zipkin,51228421,nan,0,"2018/07/08, 05:35:43",False,"2021/02/04, 07:27:25","2021/02/04, 07:27:25",893898.0,3914.0,0,2654,Relative path in volume mapping of Docker compose does not work under Windows,I am using Docker Toolbox 1803 under Windows 10 64bit.
Zipkin,51228421,nan,0,"2018/07/08, 05:35:43",False,"2021/02/04, 07:27:25","2021/02/04, 07:27:25",893898.0,3914.0,0,2654,Relative path in volume mapping of Docker compose does not work under Windows,"I am trying to run  docker zipkin  in my local system, and got failure when ran the following command in Toolbox GuickStart Terminal."
Zipkin,51228421,nan,0,"2018/07/08, 05:35:43",False,"2021/02/04, 07:27:25","2021/02/04, 07:27:25",893898.0,3914.0,0,2654,Relative path in volume mapping of Docker compose does not work under Windows,docker-compose -f docker-compose.yml  -f docker-compose-cassandra.yml up
Zipkin,51228421,nan,0,"2018/07/08, 05:35:43",False,"2021/02/04, 07:27:25","2021/02/04, 07:27:25",893898.0,3914.0,0,2654,Relative path in volume mapping of Docker compose does not work under Windows,The following line of docker-compose.yml caused the problem.
Zipkin,51228421,nan,0,"2018/07/08, 05:35:43",False,"2021/02/04, 07:27:25","2021/02/04, 07:27:25",893898.0,3914.0,0,2654,Relative path in volume mapping of Docker compose does not work under Windows,The failure info is like following:
Zipkin,51228421,nan,0,"2018/07/08, 05:35:43",False,"2021/02/04, 07:27:25","2021/02/04, 07:27:25",893898.0,3914.0,0,2654,Relative path in volume mapping of Docker compose does not work under Windows,"UPDATE : I have set env variable  COMPOSE_CONVERT_WINDOWS_PATHS=1 , it still does not work."
Zipkin,51228421,nan,0,"2018/07/08, 05:35:43",False,"2021/02/04, 07:27:25","2021/02/04, 07:27:25",893898.0,3914.0,0,2654,Relative path in volume mapping of Docker compose does not work under Windows,Update : Switched to Docker for Windows and WSL2.
Zipkin,51228421,nan,0,"2018/07/08, 05:35:43",False,"2021/02/04, 07:27:25","2021/02/04, 07:27:25",893898.0,3914.0,0,2654,Relative path in volume mapping of Docker compose does not work under Windows,"The experience is great enough, but sadly I have to give up my VirutalBox."
Zipkin,50922384,nan,1,"2018/06/19, 10:01:08",False,"2018/06/19, 10:19:26",nan,1826788.0,1408.0,0,315,Disable sleuth for from storing some traces,I am using spring cloud finchley.rc2 with spring boot version 2 along with sleuth and zipkin.
Zipkin,50922384,nan,1,"2018/06/19, 10:01:08",False,"2018/06/19, 10:19:26",nan,1826788.0,1408.0,0,315,Disable sleuth for from storing some traces,I have a facade layer which uses reactor project.
Zipkin,50922384,nan,1,"2018/06/19, 10:01:08",False,"2018/06/19, 10:19:26",nan,1826788.0,1408.0,0,315,Disable sleuth for from storing some traces,Facade calls services parallel and each service store some tracing info in rabbit mq.
Zipkin,50922384,nan,1,"2018/06/19, 10:01:08",False,"2018/06/19, 10:19:26",nan,1826788.0,1408.0,0,315,Disable sleuth for from storing some traces,Issue is I am seeing in zipkin some spans like
Zipkin,50922384,nan,1,"2018/06/19, 10:01:08",False,"2018/06/19, 10:19:26",nan,1826788.0,1408.0,0,315,Disable sleuth for from storing some traces,How can i stop such traces from being captured
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"I have a virtual machine centos(ver 7.4) on win10 machine, I do not use AWS, Google cloud service, nor Azure."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,I put master and node in one machine.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"My original problem domain have 5 components, I configure them as ClusterIP, so they could communicate with each other(eureka, config,api,uaa,zipkin)."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,Now I only need api talk outside.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"But for short, I make two components for convenience (api and eureka)."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"But now, api needs to receive from outside of cluster."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,So that I configure ingress.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"When ingress, I need to configure rbac."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,I put my yaml file here with error message.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,eureka_pod.yaml
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,eureka_svc.yaml
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,api_pod.yaml:
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,api_svc.yaml:
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,ingress_nginx_role_rb.yaml:
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,nginx_default_backend.yaml:
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,ingress_nginx_ctl.yaml:
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,ingress_nginx_res.yaml:
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"when I try  172.16.100.88:31080/uaa/login , (my virtual machine current IP is  172.16.100.88 ) it says following connection problme:"
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"I check ingress-nginx pod, it seems request has not yet reached nginx."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"When I login in to pod gearbox-rack-api-gateway, I could see clearly it redirects to the page I expected."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,so there must be some configuration wrong in my yaml files.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,=================================================================
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"In my virtual machine, I type  telnet localhost 31080 , rejected."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,but  telnet -6 localhost 31080  succeed.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,And  netstat -anp | less  find 31080 binding kube-proxy.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"I put  sysctl -q -w net.ipv6.conf.all.disable_ipv6=1  and  sysctl -w net.ipv6.conf.default.disable_ipv6=1  in my starting script, but got same result."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,==============================================================
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,Yesterday question about Ipv6 is stupid.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,I misconfigured /etc/hosts.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"Now I have telnet localhost 31080 work, but when I do curl  http://localhost:31080/uaa/login , it hangs there for long time."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,So pod is listening.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"When I issue the command curl  http://localhost:31080/uaa/login , at the same time, I check several pods' log."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,Log has shown no error and has no log to say the port 31080 has been sent request.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,I checked ingress-nginx pod logs: I paste some here.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"My own wild guess is that ingress nginx pod's namespace is kube-system, nginx service's name space is default, my-ingress's namespace is default."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,nginx-default-backend's name space is kube-system.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,Whether cross namespace traffic is forbidden.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,Experts what kind of logs do you need?
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,=======================================================
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"After define all ingress controller, ingress resources as default namespace, now I did get move further."
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,Now nginx redirect my  http request to https:  request.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,How to disable this feature will make my ingress working wholly.
Zipkin,50691266,nan,1,"2018/06/05, 05:43:56",False,"2018/06/08, 12:41:02","2018/06/08, 12:41:02",84592.0,4052.0,0,1388,kubernetes configuration Ingress，ingress_nginx service external IP pending,"I notice ingress-nginx receive the request, shown as below:"
Zipkin,49899572,49899755.0,1,"2018/04/18, 15:19:37",True,"2018/04/18, 15:29:29",nan,6094820.0,3.0,0,377,Span Customization with Google Cloud Sleuth 2,Is there a way to customize the Span inject and extractor for spring cloud sleuth 2?
Zipkin,49899572,49899755.0,1,"2018/04/18, 15:19:37",True,"2018/04/18, 15:29:29",nan,6094820.0,3.0,0,377,Span Customization with Google Cloud Sleuth 2,In the documentation of the version 1.2 i found a way that is not available on the new version(2).
Zipkin,49899572,49899755.0,1,"2018/04/18, 15:19:37",True,"2018/04/18, 15:29:29",nan,6094820.0,3.0,0,377,Span Customization with Google Cloud Sleuth 2,"I think is because now its use Zipkin brave to take care of Span, right?"
Zipkin,49899572,49899755.0,1,"2018/04/18, 15:19:37",True,"2018/04/18, 15:29:29",nan,6094820.0,3.0,0,377,Span Customization with Google Cloud Sleuth 2,https://cloud.spring.io/spring-cloud-sleuth/1.2.x/multi/multi__customizations.html#_example
Zipkin,49899572,49899755.0,1,"2018/04/18, 15:19:37",True,"2018/04/18, 15:29:29",nan,6094820.0,3.0,0,377,Span Customization with Google Cloud Sleuth 2,"I tried to back to use the stable version(1.3.3) of spring cloud sleuth, but when i use the bom for the project its make conflict in the spring boot version that i am using(2.0)."
Zipkin,49899572,49899755.0,1,"2018/04/18, 15:19:37",True,"2018/04/18, 15:29:29",nan,6094820.0,3.0,0,377,Span Customization with Google Cloud Sleuth 2,Its compactible with the spring boot version 2?
Zipkin,49899572,49899755.0,1,"2018/04/18, 15:19:37",True,"2018/04/18, 15:29:29",nan,6094820.0,3.0,0,377,Span Customization with Google Cloud Sleuth 2,"I am using the spring cloud sleuth to make trace of services on my company, but i have a version of tracing on others services that is not compactible with the opentracing headers, so i want to change the headers of http messages to make the new services compactibles with the current tracing headers that i have in the others components."
Zipkin,49899572,49899755.0,1,"2018/04/18, 15:19:37",True,"2018/04/18, 15:29:29",nan,6094820.0,3.0,0,377,Span Customization with Google Cloud Sleuth 2,Thanks
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,I have setup a demo project using spring-cloud-stream with RabbitMQ-binders and spring-cloud-sleuth.
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,I have a scheduled spring-cloud-stream source:
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,"and then a middle tier, similar to the final sink layer looking like:"
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,I nicely see the TraceId and SpanId automagically passed through the RabbitMQ queues across all processes down to the sink process in the logfiles:
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,"so at this point the final sink tier wants to (explicitly) signal, that this trace (instance of my business process) is  finish ed here."
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,How do I signal that the whole Trace has finished?
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,"I only found  sleuthTracer.currentSpan().finish();  but this only finishes the Span ... not explicitly signalling, that the whole trace is finalized here."
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,Did I miss something?
Zipkin,49393630,49402195.0,1,"2018/03/20, 22:33:54",True,"2018/03/21, 11:13:30",nan,6289425.0,764.0,0,496,spring-cloud-sleuth Trace via spring-cloud-stream with rabbitmq-binder: how to finish the Trace,"(quite new to zipkin, brave and sleuth)"
Zipkin,49241122,nan,1,"2018/03/12, 19:23:39",True,"2018/03/12, 20:53:14",nan,6244459.0,355.0,0,417,how to exclude some calls with the Feign from tracing with the cloud sleuth,"there's a microservice  with spring-boot 1.5  which uses the  Feign  to communicate with others services, also there's  spring-cloud-starter-zipkin  which wrapped all calls through the Feign and sends tracing to zipkin server."
Zipkin,49241122,nan,1,"2018/03/12, 19:23:39",True,"2018/03/12, 20:53:14",nan,6244459.0,355.0,0,417,how to exclude some calls with the Feign from tracing with the cloud sleuth,"The thing is i don't wanna wrap all calls and trace them, there're only several most important to do that."
Zipkin,49241122,nan,1,"2018/03/12, 19:23:39",True,"2018/03/12, 20:53:14",nan,6244459.0,355.0,0,417,how to exclude some calls with the Feign from tracing with the cloud sleuth,How can i exclude some calls(methods) with Feign from tracing or exlude some whole Feign client(interface)?
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,I am trying to use feature from  Spring Cloud  (ex: Feign or Zipkin Client) in a  Spring Boot  micro-service.
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,Whenever I introduce the Spring Cloud dependencies into the pom.xml I get the following error at startup:
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,Below is a sample pom.xml causing this.
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,I am currently on  Spring Boot 2.0.0.RELEASE  and  Spring Cloud Finchley.M8 .
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,What am I doing wrong?
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,Should I switch to another version of Spring Cloud?
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,"UPDATE : It's not me who's doing it wrong, even  Spring Initializr  projects demonstrate this issue."
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,To repro:
Zipkin,49201687,49202659.0,1,"2018/03/09, 22:30:57",True,"2018/03/09, 23:47:19","2018/03/09, 23:20:40",33404.0,13548.0,0,798,ClassNotFoundException using Spring Boot and Spring Cloud,pom.xml:
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,I also want to trace network latency from  App -&gt; Service1 -&gt; App -&gt; Service2 .
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,"Spring sleuth works perferct to find latency between Sleuth aware services say  services1-&gt;service2  as I can see CS,SR tags in Zipkin."
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,Now I also want to track network Latency between devices and other area which we are hopping but those are not Sleuth aware services.
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,How can I do that.
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,Any pointers would be appreciated .
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,"From the Docs,"
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,"That means that the current span has Trace-Id set to X, Span-Id set to
D. It also has emitted  Client Sent event ."
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,Are there any specific headers which needs to be sent from App to Server?
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,I know Sleuth does it out of the box using Rest template.
Zipkin,48893982,48900919.0,1,"2018/02/20, 22:49:45",True,"2018/02/21, 10:12:42","2020/06/20, 12:12:55",2535261.0,344.0,0,73,CS/SC/CR/SR events from non sleuth applications,How can I do the same thing from Apps or other non sleuth services.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,I am trying to integrate my Application with Spring sleuth.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,I am able to do a successfull integration and I can see spans getting exported to Zipkin.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,I am exporting zipkin over http.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,"Spring boot version - 1.5.10.RELEASE 
Sleuth - 1.3.2.RELEASE 
Cloud- Edgware.SR2"
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,But now I need to do this in a more controlled way as application is already running in production and people are scared about the overhead which sleuth can have by adding @NewSpan on the methods.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,I need to decide on runtime wether the Trace should be added or not (Not talking about exporting).
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,Like for actuator trace is not getting added at all.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,I assume this will have no overhead on the application.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,Putting  X-B3-Sampled = 0  is not exporting but adding tracing information.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,Something like skipPattern property but at runtime.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,Always export the trace if service exceeds a certain threshold or in case of Exception.
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,If I am not exporting Spans to zipkin then will there be any overhead by tracing information?
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,What about this solution ?
Zipkin,48873756,48874125.0,1,"2018/02/19, 22:33:47",True,"2018/02/20, 11:15:28","2018/02/20, 11:15:28",2535261.0,344.0,0,622,Spring sleuth Runtime Sampling and Tracing Decision,I guess this will work in sampling specific request at runtime.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,We are trying to add tracing to micro services so it can viewed in the google Stackdriver UI.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,"We are using Java Springboot apps deployed into Kubernetes containers, each microservices communicates over http."
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,We’ve seen that there is Sleuth and Zipkin which if we move our RestTemplate to a bean will work.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,However we don’t really want to have to deploy a zipkin pod in each of our containers or create new zipkin collector pods.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,Ideally we would like to get this working using just the google cloud tracing sdk with using sleuth/zipkin.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,Playing around with the sdk we are able to get data into Stackdriver using the google cloud grpc library which just sends the data directly from the application into Stackdriver.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,"The problem we have now is that we can send the trace id to a downstream micro service but we cannot seem to find a way to create a new span on the same trace id, it always creates a new one."
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,I can’t seem to find any documentation on how to do this.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,Surely what we are doing is what this library was build for?
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,Any pointers help on this would be great.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,Adding a bit more info......
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,"I cannot supply actual code because this is my problem, I can't actually find what I want to do."
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,Let me try to explain with a bit of code/pseudo code.
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,"So lets assume this scenario, I have 3 microservices, A, B and C."
Zipkin,47934052,nan,2,"2017/12/22, 01:25:24",False,"2017/12/28, 22:29:13","2017/12/22, 11:42:13",6203524.0,21.0,0,582,Continue Trace in downstream microservice,Hope this makes sense in what I'm trying to do.
Zipkin,42499468,42800431.0,1,"2017/02/28, 05:08:23",True,"2017/11/24, 08:04:59","2017/02/28, 05:33:54",5460458.0,91.0,0,231,Can I use StackDriver Trace PHP application in GKE?,I want to check latencies of RPC every day about CakePHP Application each endpoints running in GKE cluster.
Zipkin,42499468,42800431.0,1,"2017/02/28, 05:08:23",True,"2017/11/24, 08:04:59","2017/02/28, 05:33:54",5460458.0,91.0,0,231,Can I use StackDriver Trace PHP application in GKE?,"I found it is possible using  php google client  or  zipkin server  by reading documents , but I don't know how easy to introduce to our app though both seem tough for me."
Zipkin,42499468,42800431.0,1,"2017/02/28, 05:08:23",True,"2017/11/24, 08:04:59","2017/02/28, 05:33:54",5460458.0,91.0,0,231,Can I use StackDriver Trace PHP application in GKE?,"In addition, I'm concerned about GKE cluster configuration has StackDriver Trace option though our cluster it sets disabled.Can we trace span if it sets enable?"
Zipkin,42499468,42800431.0,1,"2017/02/28, 05:08:23",True,"2017/11/24, 08:04:59","2017/02/28, 05:33:54",5460458.0,91.0,0,231,Can I use StackDriver Trace PHP application in GKE?,Could you give some advices?
Zipkin,41441661,41441905.0,1,"2017/01/03, 12:42:42",True,"2017/01/03, 12:56:57",nan,4362715.0,984.0,0,426,Spring Cassandra driver always connected to localhost,im trying to connect my Spring Boot app to a Cassandra 2.2.8 cluster on EC2 instances (2 nodes).
Zipkin,41441661,41441905.0,1,"2017/01/03, 12:42:42",True,"2017/01/03, 12:56:57",nan,4362715.0,984.0,0,426,Spring Cassandra driver always connected to localhost,my use is tracing with Sleuth and Zipkin.
Zipkin,41441661,41441905.0,1,"2017/01/03, 12:42:42",True,"2017/01/03, 12:56:57",nan,4362715.0,984.0,0,426,Spring Cassandra driver always connected to localhost,"when the tracing start, the driver always point to localhost :"
Zipkin,41441661,41441905.0,1,"2017/01/03, 12:42:42",True,"2017/01/03, 12:56:57",nan,4362715.0,984.0,0,426,Spring Cassandra driver always connected to localhost,com.datastax.driver.core.Cluster : New Cassandra host localhost/127.0.0.1:9042 added
Zipkin,41441661,41441905.0,1,"2017/01/03, 12:42:42",True,"2017/01/03, 12:56:57",nan,4362715.0,984.0,0,426,Spring Cassandra driver always connected to localhost,this is my application.properties
Zipkin,41441661,41441905.0,1,"2017/01/03, 12:42:42",True,"2017/01/03, 12:56:57",nan,4362715.0,984.0,0,426,Spring Cassandra driver always connected to localhost,and this is my pom.xml :
Zipkin,39075236,nan,0,"2016/08/22, 11:59:42",False,"2016/08/25, 11:01:42","2016/08/25, 11:01:42",5382693.0,75.0,0,255,"Browserify - code works in node, but fails in browser after bundling",I have got problem running my js application in browser.
Zipkin,39075236,nan,0,"2016/08/22, 11:59:42",False,"2016/08/25, 11:01:42","2016/08/25, 11:01:42",5382693.0,75.0,0,255,"Browserify - code works in node, but fails in browser after bundling","I have created client-server application, and I am using  Zipkin  to trace communications between them."
Zipkin,39075236,nan,0,"2016/08/22, 11:59:42",False,"2016/08/25, 11:01:42","2016/08/25, 11:01:42",5382693.0,75.0,0,255,"Browserify - code works in node, but fails in browser after bundling",This is the client that uses Node.js  require() :
Zipkin,39075236,nan,0,"2016/08/22, 11:59:42",False,"2016/08/25, 11:01:42","2016/08/25, 11:01:42",5382693.0,75.0,0,255,"Browserify - code works in node, but fails in browser after bundling",I am using Browserify to bundle dependencies for use in the browser.
Zipkin,39075236,nan,0,"2016/08/22, 11:59:42",False,"2016/08/25, 11:01:42","2016/08/25, 11:01:42",5382693.0,75.0,0,255,"Browserify - code works in node, but fails in browser after bundling",I run  browserify CujojsClient.js -o bundle.js  to create bundle for the browser.
Zipkin,39075236,nan,0,"2016/08/22, 11:59:42",False,"2016/08/25, 11:01:42","2016/08/25, 11:01:42",5382693.0,75.0,0,255,"Browserify - code works in node, but fails in browser after bundling","If I run browserify with  --node --ignore-missing  options everything works well in Node.js, but when I run the bundle in the browser (Firefox 45.3.0 on windows) I only got:"
Zipkin,39075236,nan,0,"2016/08/22, 11:59:42",False,"2016/08/25, 11:01:42","2016/08/25, 11:01:42",5382693.0,75.0,0,255,"Browserify - code works in node, but fails in browser after bundling",This is the problematic part:
Zipkin,39075236,nan,0,"2016/08/22, 11:59:42",False,"2016/08/25, 11:01:42","2016/08/25, 11:01:42",5382693.0,75.0,0,255,"Browserify - code works in node, but fails in browser after bundling",My  index.html :
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,I have added sleuth/zipkin into my project.
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,"I'm using logback, and by default I get very well formatted logs in my console and files as well."
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,"I'm also using a logstash appender, and when I look at how kibana presents those logs - I'm not satisfied at all."
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,Here are details:
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,pom.xml:
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,My logstash appender:
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,And this is what I see in kibana:
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,The only thing that is resolved by the log pattern is the application name.
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,Am I missing some configuration?
Zipkin,38877268,nan,0,"2016/08/10, 18:09:34",False,"2016/08/10, 22:44:16","2016/08/10, 22:44:16",3740914.0,618.0,0,793,Sleuth and java-based logstash appender failing to parse log pattern,Or maybe there's something wrong in my logstash appender?
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,Tracing information do not propagate over kafka messages due to the method SleuthKafkaAspect.wrapProducerFactory() is not triggered.
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,"On the producer side, the message is correctly sent and the tracing information is correctly logged."
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,"On consumer side, instead a new traceId and spanId is created."
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,"The following two logging lines show different values for traceId,spanId (and parentId):"
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,"In first instance, using Krafdrop and also debugging, I verified that the message header doesn't contains any tracing information."
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,"After that, I figured out that the method SleuthKafkaAspect.wrapProducerFactory() is never triggered, instead on consumer side the method SleuthKafkaAspect.anyConsumerFactory() is."
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,The libraries versions used are the following:
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,The kakfa client library version is 2.4.1 is due to a version downgrade related to production bug on 2.5.1 version of kafka client that increase the cpu usage.
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,I also tried to use the following libraries versions combination with no success:
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,"We migrated our project to a different spring boot version, from 2.3.0.RELEASE to 2.3.7.RELEASE."
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,Before everthing was working correctly.
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,Below the old libraries versions:
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,We also introduced a log42/log4j (before it was slf4j with logback).
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,Below the related libraries:
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,The properties configured are the following:
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,The configuration class for the ProducerFactory creation is the following:
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,My spring boot application class:
Zipkin,66766936,66896684.0,1,"2021/03/23, 18:07:37",True,"2021/04/01, 02:45:08",nan,1800752.0,13.0,0,63,Why tracing information do not propagate over kafka messages when Spring Sleuth is in the classpath?,"Here you can find the output of command &quot;mvn dependency:tree&quot;
 mvn_dependency_tree.txt"
Zipkin,66687298,nan,1,"2021/03/18, 10:27:09",False,"2021/03/19, 09:57:02",nan,8278974.0,11.0,0,88,Spring Boot 2.4 changes,Within next few months 6th edition of Spring in Action is going to be published.
Zipkin,66687298,nan,1,"2021/03/18, 10:27:09",False,"2021/03/19, 09:57:02",nan,8278974.0,11.0,0,88,Spring Boot 2.4 changes,"It is said, it will not contain 3 chapters from 5th edition i.e."
Zipkin,66687298,nan,1,"2021/03/18, 10:27:09",False,"2021/03/19, 09:57:02",nan,8278974.0,11.0,0,88,Spring Boot 2.4 changes,"Circut Breaker,
Eureka Service-Client Discovery, Eureka Server-Client Configuration."
Zipkin,66687298,nan,1,"2021/03/18, 10:27:09",False,"2021/03/19, 09:57:02",nan,8278974.0,11.0,0,88,Spring Boot 2.4 changes,"Instead of this, it will include changes made in Spring Boot 2.4."
Zipkin,66687298,nan,1,"2021/03/18, 10:27:09",False,"2021/03/19, 09:57:02",nan,8278974.0,11.0,0,88,Spring Boot 2.4 changes,"I have
alread heard that Circut Breaker (Hystrix) is outdated, but I wonder what about
rest, especially omitted chapters ?"
Zipkin,66687298,nan,1,"2021/03/18, 10:27:09",False,"2021/03/19, 09:57:02",nan,8278974.0,11.0,0,88,Spring Boot 2.4 changes,"I noticed that I can not choose ribbon in newest(2.4.3) Spring Boot version, zipkin also differs from earlies ones."
Zipkin,66687298,nan,1,"2021/03/18, 10:27:09",False,"2021/03/19, 09:57:02",nan,8278974.0,11.0,0,88,Spring Boot 2.4 changes,What is alternative for ribbon in newest version?
Zipkin,66475457,nan,1,"2021/03/04, 14:55:18",False,"2021/03/06, 19:41:31","2021/03/06, 19:41:31",15329554.0,1.0,0,61,Spring cloud sleuth 3.0.1 generate traceid &amp; spanid in logs using logback,I am using  spring-cloud-starter-sleuth:3.0.1  and  spring-cloud-sleuth-zipkin:3.0.1  to generate  traceId  and  spanId  in log file.
Zipkin,66475457,nan,1,"2021/03/04, 14:55:18",False,"2021/03/06, 19:41:31","2021/03/06, 19:41:31",15329554.0,1.0,0,61,Spring cloud sleuth 3.0.1 generate traceid &amp; spanid in logs using logback,I was able to get those in logs using  2.2.7.RELEASE  version.
Zipkin,66475457,nan,1,"2021/03/04, 14:55:18",False,"2021/03/06, 19:41:31","2021/03/06, 19:41:31",15329554.0,1.0,0,61,Spring cloud sleuth 3.0.1 generate traceid &amp; spanid in logs using logback,I have tried using  logback  but not able to have with  3.0.1  version.
Zipkin,66475457,nan,1,"2021/03/04, 14:55:18",False,"2021/03/06, 19:41:31","2021/03/06, 19:41:31",15329554.0,1.0,0,61,Spring cloud sleuth 3.0.1 generate traceid &amp; spanid in logs using logback,"As per 3.0.1 documentation, they have removed Legacy MDC entries but brave  spanId  &amp;  traceId  are there."
Zipkin,66475457,nan,1,"2021/03/04, 14:55:18",False,"2021/03/06, 19:41:31","2021/03/06, 19:41:31",15329554.0,1.0,0,61,Spring cloud sleuth 3.0.1 generate traceid &amp; spanid in logs using logback,Dependency hierarchy:
Zipkin,66475457,nan,1,"2021/03/04, 14:55:18",False,"2021/03/06, 19:41:31","2021/03/06, 19:41:31",15329554.0,1.0,0,61,Spring cloud sleuth 3.0.1 generate traceid &amp; spanid in logs using logback,traceId &amp; spanId are not generated in log:
Zipkin,66475457,nan,1,"2021/03/04, 14:55:18",False,"2021/03/06, 19:41:31","2021/03/06, 19:41:31",15329554.0,1.0,0,61,Spring cloud sleuth 3.0.1 generate traceid &amp; spanid in logs using logback,I have tried to see this request's tracing on zipkin and able to see it with traceid and spanid:
Zipkin,66475457,nan,1,"2021/03/04, 14:55:18",False,"2021/03/06, 19:41:31","2021/03/06, 19:41:31",15329554.0,1.0,0,61,Spring cloud sleuth 3.0.1 generate traceid &amp; spanid in logs using logback,Can anyone help me to get traceid and spanid in log file using logback/log4j?
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,I have a microservices based software architecture.
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,There is a php application which orchestrates the communication among microservices and the application's whole logic.
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,I need to simulate the communication between microservices as a graph.
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,"There will be edges with weights , which will represent the affinities between microservices."
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,I am searching for a tool in order to collect all messages and their size.
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,"I have read that there are distibuted tracing systems like Zipkin which i have already deployed, and could accomplish this task."
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,"But, i cannot find how to collect the messages i want."
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,This is the php library i used for the instrumentation of my app
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,[https://github.com/openzipkin/zipkin-php]
Zipkin,65669868,65718763.0,1,"2021/01/11, 17:35:28",True,"2021/01/14, 14:14:20",nan,9050798.0,109.0,0,78,How to collect messages (total number and size) between microservices?,Any ideas about other tools or how to use Zipkin differently to achieve my goal?
Zipkin,65600807,nan,1,"2021/01/06, 19:47:18",True,"2021/01/06, 21:17:20",nan,2282523.0,117.0,0,59,Can I add tags to messaging spans based on message header values using Spring Cloud Sleuth?,I am using Spring Cloud Sleuth to send spans to zipkin when a Spring Boot application sends a message (to RabbitMQ).
Zipkin,65600807,nan,1,"2021/01/06, 19:47:18",True,"2021/01/06, 21:17:20",nan,2282523.0,117.0,0,59,Can I add tags to messaging spans based on message header values using Spring Cloud Sleuth?,I would like to customize the information sent to zipkin to include some extra tags that are populated from certain headers of the outgoing Message e.g.
Zipkin,65600807,nan,1,"2021/01/06, 19:47:18",True,"2021/01/06, 21:17:20",nan,2282523.0,117.0,0,59,Can I add tags to messaging spans based on message header values using Spring Cloud Sleuth?,the  myCustomTag  below.
Zipkin,65600807,nan,1,"2021/01/06, 19:47:18",True,"2021/01/06, 21:17:20",nan,2282523.0,117.0,0,59,Can I add tags to messaging spans based on message header values using Spring Cloud Sleuth?,Is it possible to do this using Sleuth/brave?
Zipkin,65600807,nan,1,"2021/01/06, 19:47:18",True,"2021/01/06, 21:17:20",nan,2282523.0,117.0,0,59,Can I add tags to messaging spans based on message header values using Spring Cloud Sleuth?,It feels like a messaging equivalent of registering (e.g.)
Zipkin,65600807,nan,1,"2021/01/06, 19:47:18",True,"2021/01/06, 21:17:20",nan,2282523.0,117.0,0,59,Can I add tags to messaging spans based on message header values using Spring Cloud Sleuth?,a bean of type  brave.http.HttpRequestParser  but I couldn't see an obvious way forward.
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",I use the latest Spring Boot 2.4.0.m3 to setup Spring Sleuth.
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow","On the Zipkin, the data outflow from a source is directed to &quot;broker&quot;."
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",The following is the code snap:
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",where fooService is org.springframework.cloud.stream.messaging.Source data type.
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",The sink is connected to &quot;Kafka&quot;.
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",And there isn't a connection between the &quot;broker&quot; and &quot;Kafka&quot;.
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow","Also, the sink has a data outflow to &quot;Kafka&quot;."
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",And the following is a code snap:
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow","To my eyes, there are two errors in the picture."
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",One is the label of &quot;broker&quot;.
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",It should be &quot;Kafka&quot; as well.
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",And the sink should have inflow data from Kafka instead of outflow.
Zipkin,63997270,nan,0,"2020/09/21, 20:31:37",False,"2020/09/22, 06:30:45","2020/09/22, 06:30:45",912772.0,1656.0,0,29,"Spring Cloud Sleuth, Kafka Label &amp; Data Follow",Is some additional configuration needed to resolve those two errors?
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,"We are working on an IOT platform, which ingests many device parameter
values (time series) every second from may devices."
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,"Once ingested the
each JSON (batch of multiple parameter values captured at a particular
instance) What is the best way to track the JSON as it flows through
many microservices down stream in an event driven way?"
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,"We use spring boot technology predominantly and all the services are
containerised."
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,"Eg: Option 1 - Is associating UUID to each object and then updating
the states idempotently in Redis as each microservice processes it
ideal?"
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,"Problem is each microservice will be tied to Redis now and we
have seen performance of Redis going down as number api calls to Redis
increase as it is single threaded (We can scale this out though)."
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,Option 2 - Zipkin?
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,"Note: We use Kafka/RabbitMQ to process the messages in a distributed
way as you mentioned here."
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,"My question is about a strategy to track
each of this message and its status (to enable replay if needed to
attain only once delivery)."
Zipkin,63699141,nan,1,"2020/09/02, 08:20:08",False,"2020/09/02, 08:53:08","2020/09/02, 08:53:08",6384407.0,295.0,0,182,Best way to track/trace a JSON Object (a time series data) as it flows through a system of microservices on a IOT platform,"Let's say a message1 is being by processed
by Service A, Service B, Service C. Now we are having issues to track
if the message failed getting processed at Service B or Service C as
we get a lot of messages"
Zipkin,61646918,nan,1,"2020/05/07, 02:00:23",False,"2020/05/07, 04:50:02","2020/05/07, 02:17:25",12418803.0,159.0,0,68,How to access a service from another namespace in kubernetes,"I have the zipkin deployment and service below as you can see zipkin is located under monitoring namespace the, i have an env variable called  ZIPKIN_URL  in each of my pods which are running under default namespace, this varibale takes this URL  http://zipkin:9411/api/v2/spans  but since zipkin is running in another namespace i tried this :"
Zipkin,61646918,nan,1,"2020/05/07, 02:00:23",False,"2020/05/07, 04:50:02","2020/05/07, 02:17:25",12418803.0,159.0,0,68,How to access a service from another namespace in kubernetes,http://zipkin.monitoring.svc.cluster.local:9411/api/v2/spans
Zipkin,61646918,nan,1,"2020/05/07, 02:00:23",False,"2020/05/07, 04:50:02","2020/05/07, 02:17:25",12418803.0,159.0,0,68,How to access a service from another namespace in kubernetes,i also tried this format :
Zipkin,61646918,nan,1,"2020/05/07, 02:00:23",False,"2020/05/07, 04:50:02","2020/05/07, 02:17:25",12418803.0,159.0,0,68,How to access a service from another namespace in kubernetes,http://zipkin.monitoring:9411/api/v2/spans
Zipkin,61646918,nan,1,"2020/05/07, 02:00:23",False,"2020/05/07, 04:50:02","2020/05/07, 02:17:25",12418803.0,159.0,0,68,How to access a service from another namespace in kubernetes,"but when i check the logs of my pods, i see  connection refused exception"
Zipkin,61646918,nan,1,"2020/05/07, 02:00:23",False,"2020/05/07, 04:50:02","2020/05/07, 02:17:25",12418803.0,159.0,0,68,How to access a service from another namespace in kubernetes,when i exec into one of my pods and try curl  http://zipkin.tools.svc.cluster.local:9411/api/v2/spans
Zipkin,61646918,nan,1,"2020/05/07, 02:00:23",False,"2020/05/07, 04:50:02","2020/05/07, 02:17:25",12418803.0,159.0,0,68,How to access a service from another namespace in kubernetes,its shows me  Mandatory parameter is missing: serviceNameroot
Zipkin,61646918,nan,1,"2020/05/07, 02:00:23",False,"2020/05/07, 04:50:02","2020/05/07, 02:17:25",12418803.0,159.0,0,68,How to access a service from another namespace in kubernetes,Here is zipkin resource :
Zipkin,59867690,nan,2,"2020/01/22, 22:21:51",False,"2020/04/03, 00:47:14",nan,3563501.0,241.0,0,97,ContinueSpan does not work for local method call in Micronaut,I have adapted the sample from Micronaut Users Guide V1.2.10 chapter Tracing Annotations.
Zipkin,59867690,nan,2,"2020/01/22, 22:21:51",False,"2020/04/03, 00:47:14",nan,3563501.0,241.0,0,97,ContinueSpan does not work for local method call in Micronaut,My code looks that like:
Zipkin,59867690,nan,2,"2020/01/22, 22:21:51",False,"2020/04/03, 00:47:14",nan,3563501.0,241.0,0,97,ContinueSpan does not work for local method call in Micronaut,"The question is, why the call for method doubleName is not logged to zipkin (at least it does not show up in the zipkin GUI)."
Zipkin,59867690,nan,2,"2020/01/22, 22:21:51",False,"2020/04/03, 00:47:14",nan,3563501.0,241.0,0,97,ContinueSpan does not work for local method call in Micronaut,The REST call to address() is logged.
Zipkin,59867690,nan,2,"2020/01/22, 22:21:51",False,"2020/04/03, 00:47:14",nan,3563501.0,241.0,0,97,ContinueSpan does not work for local method call in Micronaut,Do only REST calls get logged and no local method calls?
Zipkin,59867690,nan,2,"2020/01/22, 22:21:51",False,"2020/04/03, 00:47:14",nan,3563501.0,241.0,0,97,ContinueSpan does not work for local method call in Micronaut,Actually I don't think that's that the case because the Users Guides sample tells that this should work.
Zipkin,59867690,nan,2,"2020/01/22, 22:21:51",False,"2020/04/03, 00:47:14",nan,3563501.0,241.0,0,97,ContinueSpan does not work for local method call in Micronaut,Any ideas?
Zipkin,58272315,58272901.0,2,"2019/10/07, 18:06:34",True,"2019/10/17, 19:03:55",nan,9052240.0,3.0,0,1273,Adding Custom &quot;trace id with Alpha numeric values and spiting it out in application Log &quot;,I am using  Sleuth 2.1.3.
Zipkin,58272315,58272901.0,2,"2019/10/07, 18:06:34",True,"2019/10/17, 19:03:55",nan,9052240.0,3.0,0,1273,Adding Custom &quot;trace id with Alpha numeric values and spiting it out in application Log &quot;,"I want  to add  a custom ""trace  ID"" as ""correlation id"" with alpha numeric  value and want to spit in logs  with  spanid  and  parent id."
Zipkin,58272315,58272901.0,2,"2019/10/07, 18:06:34",True,"2019/10/17, 19:03:55",nan,9052240.0,3.0,0,1273,Adding Custom &quot;trace id with Alpha numeric values and spiting it out in application Log &quot;,If i use below implementation for creating new custom trace id.
Zipkin,58272315,58272901.0,2,"2019/10/07, 18:06:34",True,"2019/10/17, 19:03:55",nan,9052240.0,3.0,0,1273,Adding Custom &quot;trace id with Alpha numeric values and spiting it out in application Log &quot;,does it  get  printed in logs ?
Zipkin,58272315,58272901.0,2,"2019/10/07, 18:06:34",True,"2019/10/17, 19:03:55",nan,9052240.0,3.0,0,1273,Adding Custom &quot;trace id with Alpha numeric values and spiting it out in application Log &quot;,"I tried  below implementation   but   does not see any custom trace in log 
 https://github.com/openzipkin/zipkin-aws/blob/release-0.11.2/brave-propagation-aws/src/main/java/brave/propagation/aws/AWSPropagation.java"
Zipkin,58272315,58272901.0,2,"2019/10/07, 18:06:34",True,"2019/10/17, 19:03:55",nan,9052240.0,3.0,0,1273,Adding Custom &quot;trace id with Alpha numeric values and spiting it out in application Log &quot;,I tried with above code  from  https://cloud.spring.io/spring-cloud-sleuth/reference/html/#propagation  but didnt see any custom trace id in log
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,I have 2 services which are exchanging events via Kafka.
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,"First service packs necessary tracing info (headers which are set by brave: traceId, spanId and so on) right in message payload."
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,Consumer of the second service retrieve this information and create appropriate consumer span.
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,I can see the whole tracing including both services in Zipkin.
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,But I can't see appropriate tracing information in logs on consumer side.
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,On the producer side all is ok.
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,"The code of consumer side (as the tracings are sending to zipkin I think the most of configuration is ok, but the tracing information just don't propagate to log context...):"
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,What can you advice?
Zipkin,57053344,nan,0,"2019/07/16, 11:41:29",False,"2019/07/16, 11:41:29",nan,7284946.0,119.0,0,379,Trace id and other tracing info doesn&#39;t appear in log,Maybe I should attach something else but it seems like it is pretty much the only code I have around sleuth and brave.
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,"I'm using Spring cloud stream binder kafka, Edgware.SR4 release."
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,I have set custom headers to a message payload and published it but i can't see those headers in consumer end.
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,I have used Message object to bind payload and headers.
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,I have tried adding the property spring.cloud.stream.kafka.binder.headers but it did not work
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,Producer:
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,Application.yml
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,MessageChannelConstants.java
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,SampleMessageChannels.java
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,SampleEventPublisher.java
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,Consumer:
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,application.yml
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,MessageChannelConstants.java
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,SampleMessageChannels.java
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,SampleEventListener.java
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,"Below is the Exception I got,"
Zipkin,56293312,nan,1,"2019/05/24, 16:06:00",False,"2019/05/28, 22:43:26","2019/05/24, 16:25:42",4762878.0,9.0,0,915,Spring Cloud stream binder Kafka custom headers are not received in consumer,Note: I am using spring cloud sleuth and zipkin dependency as well.
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,We are running a java trading application and have around 50 orders per second.
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,"When an order comes in, it jumps between services and we want to measure latency inside every service and between services by an external service which should gather all data with timestamps and produce distributions with percentiles."
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,We want to measure latency for every order in order to find issues and explain them to our members if they have latency-related questions.
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,The issue we are facing is a framework to choose to propagate orders from every service to another service with timestamps attached to calculate and produce latencies.
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,"Given the flow of orders we have, what will be the most promising approach for us?"
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,"We looked into Zipkin, it also supports gRPC - does it fit our use-case?"
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,Any other recommendations?
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,p.s.
Zipkin,55671923,nan,0,"2019/04/14, 07:14:37",False,"2019/04/14, 08:22:19","2019/04/14, 08:22:19",3237407.0,349.0,0,40,Distributes tracing of order flows,we cannot use the transport we use for the business logic as we are going to get rid of it soon.
Zipkin,55036782,nan,1,"2019/03/07, 07:40:34",False,"2019/03/07, 10:20:10",nan,9594894.0,1.0,0,70,Not able to trace all http requests in async parallel with zipkins in Node API,I am new to node js and was trying to integrate zipkins with my node APi using appmetrics-zipkin npm package.
Zipkin,55036782,nan,1,"2019/03/07, 07:40:34",False,"2019/03/07, 10:20:10",nan,9594894.0,1.0,0,70,Not able to trace all http requests in async parallel with zipkins in Node API,"Zipkin works fine except when there are multiple http calls in async  parallel method , it gives trace of only the first http call which was finished...I need trace for all the API calls in async parallel......Please help"
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,I have a dependency  https://mvnrepository.com/artifact/io.zipkin.reporter2/zipkin-sender-okhttp3/2.7.14  declared as
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,This dependency in it's pom has parent dependency with pom file that declares dependency like this:
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,project.groupid equals to io.zipkin.reporter2 and project.version equals to 2.7.14.
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,So maven should import dependency  &lt;artifactId&gt;zipkin-reporter&lt;/artifactId&gt;  with version equals to 2.7.14.
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,BUT  it imports versions 2.2.0.
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,I don't declare this dependency anywhere else and other dependencies don't have this dependency as transative.
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,"I tried reinstall maven, reclone project from git, invalidate cache and restart IDEA, delete .m2 folder - nothing worked."
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,The weird thing is that I have other project that is using the very same dependency ( &lt;artifactId&gt;zipkin-sender-okhttp3&lt;/artifactId&gt; ) and all versions are in order how they should be.
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,Any ideas how I can fix it?
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,Edit: mvn dependency:tree output (I edited out sensitive info): `
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,"`
All modules have version 2.7.7 as they should be but one module with name  ...-deployment has versions 2.2.0."
Zipkin,54943755,nan,1,"2019/03/01, 13:31:16",False,"2019/03/01, 15:00:07","2019/03/01, 14:43:50",10039270.0,3.0,0,193,Maven dependency imports wrong child dependencies versions,It doesn't set explicitly there.
Zipkin,53552113,nan,0,"2018/11/30, 08:04:20",False,"2019/07/30, 14:20:23","2019/07/30, 14:20:23",9491345.0,308.0,0,70,Combining logs from two different servers,"I have a distributed process which runs across two different servers (A and B) and I get two different log files A.log and B.log, I need this merged into a single 
file."
Zipkin,53552113,nan,0,"2018/11/30, 08:04:20",False,"2019/07/30, 14:20:23","2019/07/30, 14:20:23",9491345.0,308.0,0,70,Combining logs from two different servers,I have referred to following links but I am unable to get a merged file from the same:
Zipkin,53552113,nan,0,"2018/11/30, 08:04:20",False,"2019/07/30, 14:20:23","2019/07/30, 14:20:23",9491345.0,308.0,0,70,Combining logs from two different servers,Is there something that I am missing?
Zipkin,53552113,nan,0,"2018/11/30, 08:04:20",False,"2019/07/30, 14:20:23","2019/07/30, 14:20:23",9491345.0,308.0,0,70,Combining logs from two different servers,Edit: I need the logs in something along these lines
Zipkin,51851541,nan,1,"2018/08/15, 03:54:03",False,"2018/08/22, 18:07:28",nan,401596.0,369.0,0,1521,Custom trace logs using spring-cloud-sleuth,I am using spring-cloud-sleuth:2.0.1.RELEASE with Spring Webflux.
Zipkin,51851541,nan,1,"2018/08/15, 03:54:03",False,"2018/08/22, 18:07:28",nan,401596.0,369.0,0,1521,Custom trace logs using spring-cloud-sleuth,"The doc talks about logging trace, span, etc using MDC."
Zipkin,51851541,nan,1,"2018/08/15, 03:54:03",False,"2018/08/22, 18:07:28",nan,401596.0,369.0,0,1521,Custom trace logs using spring-cloud-sleuth,It also talks about sending traces to Zipkin via HTTP.
Zipkin,51851541,nan,1,"2018/08/15, 03:54:03",False,"2018/08/22, 18:07:28",nan,401596.0,369.0,0,1521,Custom trace logs using spring-cloud-sleuth,I am interested in logging the trace information in more elaborate way.
Zipkin,51851541,nan,1,"2018/08/15, 03:54:03",False,"2018/08/22, 18:07:28",nan,401596.0,369.0,0,1521,Custom trace logs using spring-cloud-sleuth,"With every log statement, I want to emit the zipkin traces in the JSON format - very close to what's depicted here:  https://zipkin.io/pages/data_model.html"
Zipkin,51851541,nan,1,"2018/08/15, 03:54:03",False,"2018/08/22, 18:07:28",nan,401596.0,369.0,0,1521,Custom trace logs using spring-cloud-sleuth,What is the best way to accomplish this in sleuth?
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException",I'm using  spring-cloud-sleuth  and zipkin.
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException","In the producer, it worked.I can see message in kafka topic   see image ."
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException","but in the consumer,some exception occur."
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException",solve by this issue .
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException",but next exception cannot solve.
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException",project infomation
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException",pom.xml
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException",application.yml
Zipkin,48659538,48660632.0,1,"2018/02/07, 10:55:29",True,"2020/01/10, 10:32:06",nan,9315447.0,5.0,0,278,"ZipkinStreamServer via kafka, org.springframework.messaging.converter.MessageConversionException",main class
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,I am facing some errors in a spring boot project where I am using spring integration to connect to RabbitMQ.
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,I am doing the configuration for RabbitMQ in XML files like this:
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,But I am creating two of each component.
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,How to set the primaries ones?
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,"Now the problem comes here, I was using this version for spring cloud:"
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,"And everything was working fine, but if I update the version to:"
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,This error is coming:
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,And the error comes because of this dependency:
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,If I remove this dependency the error is not coming.
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,You can find an example project to reproduce this scenario.
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,In the pom file you'll see this:
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,https://github.com/fjmpaez911/spring-integration-zipkin-cloud
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,So I need to know how to set a primary configuration for RabbitMQ and in addition I think that could be an issue because this error only comes if I use this version  Edgware.RELEASE
Zipkin,48137375,nan,2,"2018/01/07, 15:05:24",True,"2018/09/26, 22:55:47",nan,7605799.0,207.0,0,1225,How to set primary rabbit template and rabbit connection factory,Am I missing something?
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,"I have multiple services, some of which use Hystrix's HystrixObservableCommand to call other services and others use HystrixCommand."
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,How do I pass on traceIds from the calling service to the Observables in HystrixObservableCommand and also have them be passed on if the fallback is called?
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,All services are using grpc-java.
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,Sample code that I have:
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,WorldCommand.java
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,I am using Zipkin grpc tracing and MDCCurrentTraceContext to print the traceId and spanId in the logs.
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,"Both the log entries in the WorldCommand do not print out the trace and span ids, they are called on RxIoScheduler thread."
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,EDIT
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,Added ConcurrencyStrategy as suggested by Mike.
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,HelloService calls two services World and Team.
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,"The WorldCommand is a HystrixObservableCommand, the TeamCommand is a HystrixCommand."
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,PreservableContext class
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,The log in PreservableContexts and CustomHystrixConcurrencyStrategy never get printed.
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,I am registering the startegy when I start the HelloServer.
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,EDIT 2
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,Updated how the Observables are set up:
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,"I have a weird problem now, the calls to TeamCommand and WorldCommand doesn't complete as in this code is never executed:"
Zipkin,47293445,nan,2,"2017/11/14, 21:05:37",False,"2017/11/20, 23:40:11","2017/11/20, 23:40:11",2237511.0,899.0,0,642,How to pass traceIds in Hystrix Observables?,"Also, if there is a fallback, the hystrix-timer threads doesn't have the MDC anymore."
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,I have 2 very simple spring-cloud-stream applications.
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,"Service3, the message producer, sends messages to Service4, the consumer, through the binder-kafka."
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,And I use spring-cloud-sleuth to trace the spans among them.
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,But only the spans in Service3 are available in zipkin server.
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,No span shows for Service4.
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,Service3
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,Service4
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,Servic4 (message consumer) is not traced
Zipkin,42386925,nan,2,"2017/02/22, 11:02:42",False,"2017/02/23, 09:11:02",nan,6503007.0,31.0,0,842,Could not trace spring-cloud-stream Listener with spring-cloud-sleuth,What did I miss?
Zipkin,41086850,nan,1,"2016/12/11, 15:47:59",False,"2018/10/15, 17:52:20",nan,520320.0,295.0,0,803,Can Spring Cloud Sleuth trace HTTP calls made through Async RestTemplate,I am trying to trace HTTP calls made through  Async RestTemplate  from a Spring Boot Application.
Zipkin,41086850,nan,1,"2016/12/11, 15:47:59",False,"2018/10/15, 17:52:20",nan,520320.0,295.0,0,803,Can Spring Cloud Sleuth trace HTTP calls made through Async RestTemplate,I have a ZipKin instance running locally to which the microservices in question point to.
Zipkin,41086850,nan,1,"2016/12/11, 15:47:59",False,"2018/10/15, 17:52:20",nan,520320.0,295.0,0,803,Can Spring Cloud Sleuth trace HTTP calls made through Async RestTemplate,"I could see spans recorded at every service in ZipKin UI, however I am not able to see the trace covering all the spans."
Zipkin,41086850,nan,1,"2016/12/11, 15:47:59",False,"2018/10/15, 17:52:20",nan,520320.0,295.0,0,803,Can Spring Cloud Sleuth trace HTTP calls made through Async RestTemplate,With  RestTemplate  the trace is recorded as normal.
Zipkin,41086850,nan,1,"2016/12/11, 15:47:59",False,"2018/10/15, 17:52:20",nan,520320.0,295.0,0,803,Can Spring Cloud Sleuth trace HTTP calls made through Async RestTemplate,i.e.
Zipkin,41086850,nan,1,"2016/12/11, 15:47:59",False,"2018/10/15, 17:52:20",nan,520320.0,295.0,0,803,Can Spring Cloud Sleuth trace HTTP calls made through Async RestTemplate,I am able to see end-to-end via the UI.
Zipkin,41086850,nan,1,"2016/12/11, 15:47:59",False,"2018/10/15, 17:52:20",nan,520320.0,295.0,0,803,Can Spring Cloud Sleuth trace HTTP calls made through Async RestTemplate,"Any pointers will help,
Thanks in advance."
Zipkin,39014203,nan,0,"2016/08/18, 12:11:36",False,"2016/08/18, 14:10:09","2016/08/18, 14:10:09",5382693.0,75.0,0,60,Is it possible to modify Polymer iron-ajax element to use cujojs-rest?,I would like to know if it is possible to modify  iron-ajax  somehow to use  cujojs-rest  to perform all the requests.
Zipkin,39014203,nan,0,"2016/08/18, 12:11:36",False,"2016/08/18, 14:10:09","2016/08/18, 14:10:09",5382693.0,75.0,0,60,Is it possible to modify Polymer iron-ajax element to use cujojs-rest?,I would like to use cujojs-rest zipkin instrumentation for tracing in my app.
Zipkin,39014203,nan,0,"2016/08/18, 12:11:36",False,"2016/08/18, 14:10:09","2016/08/18, 14:10:09",5382693.0,75.0,0,60,Is it possible to modify Polymer iron-ajax element to use cujojs-rest?,Here is an example app using cujojs-rest zipkin instrumentation to generate trace data for Zipkin:  wingtips-cujojs-spark-example
Zipkin,39014203,nan,0,"2016/08/18, 12:11:36",False,"2016/08/18, 14:10:09","2016/08/18, 14:10:09",5382693.0,75.0,0,60,Is it possible to modify Polymer iron-ajax element to use cujojs-rest?,So let's say I have got code like this:
Zipkin,39014203,nan,0,"2016/08/18, 12:11:36",False,"2016/08/18, 14:10:09","2016/08/18, 14:10:09",5382693.0,75.0,0,60,Is it possible to modify Polymer iron-ajax element to use cujojs-rest?,I would like to achieve the same in  iron-ajax
Zipkin,37642783,37706728.0,1,"2016/06/05, 17:02:07",True,"2016/06/08, 18:23:49",nan,1157070.0,4784.0,0,387,"spring-cloud-sleuth one of the microserives out of three does not show the spanid, and token id in logs after adding dependency",I am using  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_adding_to_the_project  for adding Spring Cloud Sleuth without the Zipkin integration
Zipkin,37642783,37706728.0,1,"2016/06/05, 17:02:07",True,"2016/06/08, 18:23:49",nan,1157070.0,4784.0,0,387,"spring-cloud-sleuth one of the microserives out of three does not show the spanid, and token id in logs after adding dependency","But in one of the microserives out of three,  does not show the spanid, and token id in logs  after adding dependency for all services (all are http request services, though there are couple of more services which require JMS - on which I need to work)"
Zipkin,37642783,37706728.0,1,"2016/06/05, 17:02:07",True,"2016/06/08, 18:23:49",nan,1157070.0,4784.0,0,387,"spring-cloud-sleuth one of the microserives out of three does not show the spanid, and token id in logs after adding dependency",Service1
Zipkin,37642783,37706728.0,1,"2016/06/05, 17:02:07",True,"2016/06/08, 18:23:49",nan,1157070.0,4784.0,0,387,"spring-cloud-sleuth one of the microserives out of three does not show the spanid, and token id in logs after adding dependency",Service2
Zipkin,37642783,37706728.0,1,"2016/06/05, 17:02:07",True,"2016/06/08, 18:23:49",nan,1157070.0,4784.0,0,387,"spring-cloud-sleuth one of the microserives out of three does not show the spanid, and token id in logs after adding dependency",Service3
Zipkin,37642783,37706728.0,1,"2016/06/05, 17:02:07",True,"2016/06/08, 18:23:49",nan,1157070.0,4784.0,0,387,"spring-cloud-sleuth one of the microserives out of three does not show the spanid, and token id in logs after adding dependency","Experts, Please suggest what can be done to see the effect in Service2"
Zipkin,37608464,37608801.0,1,"2016/06/03, 10:29:22",True,"2016/06/03, 10:47:09",nan,1729795.0,17805.0,0,640,Spring Cloud Sleuth + Zipking - How to trace individual instances,I am using Spring Cloud Sleuth + Zipkin to have an insight of the service timing and behaviour.
Zipkin,37608464,37608801.0,1,"2016/06/03, 10:29:22",True,"2016/06/03, 10:47:09",nan,1729795.0,17805.0,0,640,Spring Cloud Sleuth + Zipking - How to trace individual instances,"The only downside I have found is: when there are several instances of every microservice I haven't found a way to determine which instance Zipkin information is referring to, since it identifies them all by its service name (which is the same for all)."
Zipkin,37608464,37608801.0,1,"2016/06/03, 10:29:22",True,"2016/06/03, 10:47:09",nan,1729795.0,17805.0,0,640,Spring Cloud Sleuth + Zipking - How to trace individual instances,Is there a way to configure Sleuth to add service-instance dinstintion in Zipkin?
Zipkin,32362058,nan,1,"2015/09/02, 19:49:47",False,"2015/09/07, 04:04:28",nan,2816110.0,85.0,0,1613,"Categorical variable in WinBUGS, OpenBUGS","I am using a script from the following paper (Zipkin, E.F., Royle, J.A., Dawson, D.K., Bates, S., 2010."
Zipkin,32362058,nan,1,"2015/09/02, 19:49:47",False,"2015/09/07, 04:04:28",nan,2816110.0,85.0,0,1613,"Categorical variable in WinBUGS, OpenBUGS",Multi-species occurrence models to evaluate the effects of conservation and management actions.
Zipkin,32362058,nan,1,"2015/09/02, 19:49:47",False,"2015/09/07, 04:04:28",nan,2816110.0,85.0,0,1613,"Categorical variable in WinBUGS, OpenBUGS","Biological Conservation 143, 479-484) to estimate bird species occupancy."
Zipkin,32362058,nan,1,"2015/09/02, 19:49:47",False,"2015/09/07, 04:04:28",nan,2816110.0,85.0,0,1613,"Categorical variable in WinBUGS, OpenBUGS","One of my variables in the detection estimate (the K loop in the code below) is Wind, which is a categorical variable, with levels 1-6."
Zipkin,32362058,nan,1,"2015/09/02, 19:49:47",False,"2015/09/07, 04:04:28",nan,2816110.0,85.0,0,1613,"Categorical variable in WinBUGS, OpenBUGS","I have attempted to use the  dcat  function in OpenBUGS which what I hope is an uniformative prior (beta(1,1)), but OpenBUGS fails with error:"
Zipkin,32362058,nan,1,"2015/09/02, 19:49:47",False,"2015/09/07, 04:04:28",nan,2816110.0,85.0,0,1613,"Categorical variable in WinBUGS, OpenBUGS","Which, when I remove the line  b3[i] ~ dcat(p[i,])#WIND Data  does not happen."
Zipkin,32362058,nan,1,"2015/09/02, 19:49:47",False,"2015/09/07, 04:04:28",nan,2816110.0,85.0,0,1613,"Categorical variable in WinBUGS, OpenBUGS","Any advise on how to specify dcat properly, or how to code categorical variables for WinBUGS/OpenBUGS would be greatly appreciated!"
Zipkin,59356270,nan,1,"2019/12/16, 14:03:19",True,"2019/12/16, 14:47:29",nan,6456586.0,2697.0,-1,200,Tracing of spring boot microservice application,what are the best practices in tracing of spring boot 2 microservice applications?
Zipkin,59356270,nan,1,"2019/12/16, 14:03:19",True,"2019/12/16, 14:47:29",nan,6456586.0,2697.0,-1,200,Tracing of spring boot microservice application,I found some 2 years old tutorials where tracing server was as another spring boot application with following dependencies:
Zipkin,59356270,nan,1,"2019/12/16, 14:03:19",True,"2019/12/16, 14:47:29",nan,6456586.0,2697.0,-1,200,Tracing of spring boot microservice application,and push traces with following configuration:
Zipkin,59356270,nan,1,"2019/12/16, 14:03:19",True,"2019/12/16, 14:47:29",nan,6456586.0,2697.0,-1,200,Tracing of spring boot microservice application,and
Zipkin,59356270,nan,1,"2019/12/16, 14:03:19",True,"2019/12/16, 14:47:29",nan,6456586.0,2697.0,-1,200,Tracing of spring boot microservice application,Is this solution still actual and suitable for production or should we configure standalone docker image of zipkin instead of spring boot app and connect it to ELK stack with logs?
Zipkin,59356270,nan,1,"2019/12/16, 14:03:19",True,"2019/12/16, 14:47:29",nan,6456586.0,2697.0,-1,200,Tracing of spring boot microservice application,What do you recommended?
Zipkin,59356270,nan,1,"2019/12/16, 14:03:19",True,"2019/12/16, 14:47:29",nan,6456586.0,2697.0,-1,200,Tracing of spring boot microservice application,It will be great if you can provide some example what is recommended approach to handle it in.
Zipkin,59356270,nan,1,"2019/12/16, 14:03:19",True,"2019/12/16, 14:47:29",nan,6456586.0,2697.0,-1,200,Tracing of spring boot microservice application,Thank you in advice.
Zipkin,52772025,52852525.0,1,"2018/10/12, 06:55:50",True,"2018/10/17, 13:19:16","2018/10/12, 12:54:13",7561583.0,71.0,-1,4034,Group coordinator lookup failed: The coordinator is not available,i use zipkin with kafka
Zipkin,52772025,52852525.0,1,"2018/10/12, 06:55:50",True,"2018/10/17, 13:19:16","2018/10/12, 12:54:13",7561583.0,71.0,-1,4034,Group coordinator lookup failed: The coordinator is not available,and work log:
Zipkin,52772025,52852525.0,1,"2018/10/12, 06:55:50",True,"2018/10/17, 13:19:16","2018/10/12, 12:54:13",7561583.0,71.0,-1,4034,Group coordinator lookup failed: The coordinator is not available,"and loop log...
Can anyone tell me what is the reason?"
Zipkin,52772025,52852525.0,1,"2018/10/12, 06:55:50",True,"2018/10/17, 13:19:16","2018/10/12, 12:54:13",7561583.0,71.0,-1,4034,Group coordinator lookup failed: The coordinator is not available,"kafka-server: 0.11.0
kafka-client: 1.0.1
zipkin: 2.10.4"
Zipkin,47318596,nan,1,"2017/11/16, 00:43:03",False,"2017/11/16, 09:02:10",nan,7949022.0,191.0,-1,192,Can spring Sleuth be used with Scala?,I'm developing event-driven Microservices which I use Java and Scala.
Zipkin,47318596,nan,1,"2017/11/16, 00:43:03",False,"2017/11/16, 09:02:10",nan,7949022.0,191.0,-1,192,Can spring Sleuth be used with Scala?,"I used Spring Sleuth and Zipkin for request tracing with Java services, can I use Spring Sleuth with Scala?"
Zipkin,47318596,nan,1,"2017/11/16, 00:43:03",False,"2017/11/16, 09:02:10",nan,7949022.0,191.0,-1,192,Can spring Sleuth be used with Scala?,if not how can I generate trace id and span id in Scala to be sent to Zipkin.
Zipkin,52798338,nan,1,"2018/10/14, 02:52:27",False,"2018/10/14, 03:55:06",nan,6289425.0,764.0,-2,375,spring where to find implementation code of annotations,"In spring and spring-boot there is a lot of ""magic"" that happens just by annotating methods and classes."
Zipkin,52798338,nan,1,"2018/10/14, 02:52:27",False,"2018/10/14, 03:55:06",nan,6289425.0,764.0,-2,375,spring where to find implementation code of annotations,"For learning purposes and do-it-yourself stuff I would be interested to have a look at them and so wondering how to find the ""magic code"" that an annotation ""causes"" ..."
Zipkin,52798338,nan,1,"2018/10/14, 02:52:27",False,"2018/10/14, 03:55:06",nan,6289425.0,764.0,-2,375,spring where to find implementation code of annotations,"is there a ""cook-book"" on how to find the implementing code of Annotations inside spring jars?"
Zipkin,46762291,nan,1,"2017/10/16, 05:53:43",True,"2020/10/28, 16:56:35",nan,6530950.0,40.0,-2,362,NiFi - Distributed Tracing capability,We have integrated NiFi within our product suits.
Zipkin,46762291,nan,1,"2017/10/16, 05:53:43",True,"2020/10/28, 16:56:35",nan,6530950.0,40.0,-2,362,NiFi - Distributed Tracing capability,"We would like to track a user request by ""Trace Id"", which spans across different components."
Zipkin,46762291,nan,1,"2017/10/16, 05:53:43",True,"2020/10/28, 16:56:35",nan,6530950.0,40.0,-2,362,NiFi - Distributed Tracing capability,Do let me know whether NiFi have some capability to support something similar to ZipKin.
Zipkin,46762291,nan,1,"2017/10/16, 05:53:43",True,"2020/10/28, 16:56:35",nan,6530950.0,40.0,-2,362,NiFi - Distributed Tracing capability,Thanks
Zipkin,46762291,nan,1,"2017/10/16, 05:53:43",True,"2020/10/28, 16:56:35",nan,6530950.0,40.0,-2,362,NiFi - Distributed Tracing capability,Senthil.
