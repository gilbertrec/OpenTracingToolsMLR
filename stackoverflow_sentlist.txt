Is it ok to deploy Performance Monitoring Tool - AppDynamics - in Production?
The application to be monitored is a standard Java/J2EE Web Application.
I have never worked with AppDynamics, and my concern is that it may actually slow down my application.
Has anyone used AppDynamics in Production?
Or should it be used only in Test kind of enivornments.
How do you build a AppDynamic or New Relic kind of system that collects performance metrics of your application including detailed call tree stats by merely installing a software on the servers where your application runs?
Is it even possible to collect such metrics without compiling your apps with debug information?
What are the performance trade offs to consider when building such a service?
How do such software minimize the performance impact they themselves might be having on the application.
I'm trying to add Appdynamics into my application, I'm doing those steps:  https://docs.appdynamics.com/display/PRO40/Instrument+an+Android+Application#InstrumentanAndroidApplication-ToaddtheAppDynamicsAndroidagentrepositorytoyourproject  but after all I have error:
This is how my build.gradle (for all project) looks like:
and build.gradle (from app module):
and  adeum-maven-repo  paste into project.
Any idea what am I doing wrong?
I followed the instructions on the website of appdynamic, but when I was following this step:
When I ran standalone.bat this error appears:
I have installed Appdynamics lite on my server and it worked fine when I used to run my tomcat instance with root user.
But from the time I have created a new user "Tomcat" and start executing my apache tomcat with this user, I am not able to run appdynamics.
I have copied the javaagent at this location with all rights(read,write,execute) to tomcat "/home/tomcat/profiler/AppServerLite".
It throws an exception as follows :
I wonder what stall means in AppDynamics Pro.
Can you give an example which Appdynamics Pro application name that situation as Stall?
Thanks.
I'm trying to filter on AppDynamics to get all the request to a particular REST URL, the REST URL is not fixed as long as in the URL
/AppEngine/rest/evac/${id}/createNewActivity
On the Transaction Snapshots you have the option to filter results, and in the filters you can filter by URL:
If I search for a concrete URL (with ${id} defined) I can search it, but I cannot find how to use a wildcar to find this URL with any ${id}.
I tried so far to use
With no results.
The one with works just a bit is using /AppEngine/rest/evac/* which also retrieves other REST which start with the same URL, so I can export the results and filter outside AppDynamics.
But there is a way to use a wildcard so I can find the desired results in AppDynamics?
I am now working on Performance Testing of a Java Application that runs on GlassFish Server 4.1.
After going through some statistics that I got from AppDynamics tool, I find that there is no possibility for me to drill down to code/method level issues.
For example, I can see the time taken by each method or function using dotTrace or JProfiler but AppDynamics tool seems to skip all these features.
I was also looking for a free solution, hence I choose AppDynamics.
Now I feel I am not on the right track.
Can someone let me know more about this tool if I am missing something or suggest any other quick and easy solution to this.
Is there a possibility that the monitors on GlassFish server 4.1 can do the same for no cost?
I am trying to a good comparison between AppDynamics and Application Insights in regard to Azure App Service.
I tried to google around but couldn't find any good comparison, if someone can point me or summarize here.
Has anybody solved to monitor Azure functions using AppDynamics ?
I don't see any option to add a AppDynamics extension to the Azure functions app.
For a period of time we might want to have the two analytics together.
Could this be problematic?
Would it be degrade the speed?
Would there be any fight between who captures the crash log?
!
Most popular logging and monitoring stacks like ELK stack or Time series DB-Grafana are designed to be integrated.
Can AppDynamics work with other samplers/DBs, in particular Prometheus?
I have a problem with my health rule configuration.
All I want is to have health rule which will be checking if service is running or not.
I have two types of services:
IIS
Standalone services
The problem is that some services are recognized as critical due to health rule violation.
For example, I have two exactly the same services on two hosts and the only difference is that one of them is in use not so often.
Due to lack of activity on this service appdynamics pointing me it as critical.
Most probably I have done something wrong.
Any ideas?
I'm struggling with it as additional task.
Tried appdynamics community website but nothing which could point me solution.
Here's my health rule configuration :
I have a Spring application monitored by Appdynamics.
This application has a Service Endpoint recognized by Appdynamics, named lets say:  /general/endpoint
Now within the application there are multiple endpoints like:
Now back to Appdynamics: Within the Service Endpoints menu I can find my  /general/endpoint  and click on it.
Then I see a table with the actual REST API calls, their respective execution time, the specific URL (like  /general/endpoint/do-something-1 ) and some more information.
If I am interested in monitoring the requests for a specific URL I can do the following:
Now I see what I want, all the requests and their potential problems for a specific URL inside my application.
Now comes my actual question:
How can I achieve the same for Dashboards?
I was only able to create a dashboard with the  Calls per Minute  for the  /general/endpoint  but not for a specific URL like  /general/endpoint/do-something-1 .
Is there a way to apply the more fine grade filter for dashboards as well?
I am doing some proof of concept to ingest traces and metrics to AppDynamics without installing Appdynamics agent.
I have an application emitting prometheus metrics and traces.
I could not find any Appdynamics documentation talking about opentelemetry Collector.
I could not find exporter in  https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter  either.
Can you please advise on how to use opencollector with Appdynamics?
With appdynamics it's easy to setup metrics that "aggregate" data from different nodes.
What if you want to aggregate some date, and some other data, show as distinct lines in the graph?
Is that possible?
I am trying to install Appdynamics APM tool.
It has three components :
I have few queries:
How can we capture heap dumps with the help of appdynamics?
How does AppDynamics and similar problems retrieve data from apps ?
I read somewhere here on SO that it is based on bytecode injection, but is there some official or reliable source to this information ?
I am trying to integrate apps performance monitoring tool with my Android Application by my gradle fails saying
Below is my gradle root gradle file
And here is my App's gradle file,
I am already having Multidex flag enabled still it gives me the problem while running the Application.
And, also I have in my Application class
I would like to know which existing api in AppDynamics can we use to generate custom reports.
The use case is like, i am consolidating reports from multiple tools so,i would be using the API of app dynamics and doing a backend call to pull the data i need to display and putting it in a csv or excel.All of this will happen in an automated way, would like to know the api or any specific way to do the same in app dynamics.
How to find TimeStamp difference in App Dynamics Query Language (ADQL)?
I tried column1 - column2.
Getting error message "Operator  [SUBTRACTION]  not valid on field types  [DATETIME], [DATETIME] .
I have my java application up and running.
I want to import that application in Appdynamics for monitoring.
Can any one please suggest how to import java applications in appdynamics.
Data is not loading from standalone java applications to agent controller, but only say java agent is connected successfully and waiting for data to load.
In Autodetection and Object tracing dashboard tabs, not able to view the detail or drill down the object tracking instances for standalone Java applications.
Configured adding some of the fully qualified class names from the application.
Please advice, do I need to do any other settings to get the drill down feature?
We have cluster of instances whereas each instance has DropWizard metrics gatherer.
We're also trying to leverage AppDynamics custom metrics and that works so that custom script hits DropWizard exposed endpoint (/metrics) and sends metrics of interest to AppDynamics Controller.
AppDynamics has 2 cluster rollout strategies for how the metric is displayed in a whole application view (tier) - SUM and AVG.
While this works well for stuff like counts (sum is used) and average processing times (avg is used) - we for now don't have any idea of how to aggregate each instance percentiles exposed by DropWizard - neither sum nor avg looks correct.
Example:
sum  will give 1700 what of course isn't useful at all.
avg  will give 600 - which isn't correct either - we're losing track of higher bound.
If AppDynamics had MAX Cluster rollout - that would be more or less fair - still not correct though.
But AppDynamics doesn't have that.
We also understand that the only fully correct way of gathering cluster percentiles is to perform aggregation from all nodes at one place (e.g.
logstash, etc..) and not on each instance.
But for now that's what we have - just sending custom metrics periodically.
It would be great if anyone suggests something regarding that.
Thanks in advance,
I see that Appdynamics 4.2 claims to  support  Java 8 lambda instrumenting, but this support was  removed  in 4.3.
I cannot find anything in  4.3 release notes  that mentions removing support for lambdas.
What's happened?
Is it somehow related to  JDK-8145964 ?
Getting below error during platform installation:
"Required libaio package is not found.
..."
However, above package is already installed:
Here is output from the installation script:
I have read about the Appdynamics in Kubernetes but I got confused.
The scenario is like I am having EC2 under which Kubernetes is running which is having POD and under 1 pod, multiple container is running.
Where I have to install machine-agent?
In EC2 or in daemon set?
And where I have to install app-agent?
do I have to add app-agent in each container Dockerfile?
And lastly, what would be my hostName and uniqueHostId?
I explore AppDynamics and other APM solutions to choose right one for my company.
I have created simple demo .NET application (WCF service and console client to consume it).
Then I installed AppDynamics agent on machine and configure it for both client and service as for standalone applications:
When I start my client and service I see that AppD agent have "injected" code to my applications and write "Running non-obfuscated client" to the console
I want to understand what technics or methods AppDynamics agent use to instrument .NET applications without SDK and being just a separate process (service)?
How does it listen for incoming WCF calls of my service without being directly used by the service (it's not referenced as an assembly, even not mentioned in app.config)?
Has anyone experienced Webpack dependency compiling issues when using the AppDynamics library?
And did you find a way to work around it?
I believe this is an issue stemming from their library.
When trying to install the AppDynamics package for monitoring a Node.js/Express application, our Webpack build process is not able to import a handful of dependencies.
Specifically, the errors output are:
Our project is set up with:
- Webpack v4.29.0
- Node.js v11.0.0
- Appdynamics v4.5
The Appdynamics usage is at the top of our server file as:
And our Webpack configuration is:
So far we have tried downgrading the Webpack version, downgrading the Node environment to 10.15, and using other import methods for the AppDynamics package, but this seems like an issue internal to the Appdynamics library?
The main question is, has anyone experienced Webpack dependency compiling issues when using the Appdynamics library?
And did you find a way to work around it?
Any help or clues would be appreciated ❤️
I am trying to setup the rabbitmq machine agent for AppDynamics with a standalone RabbitMQ.
https://www.appdynamics.com/community/exchange/extension/rabbitmq-monitoring-extension/
itMQ Monitoring Plugin 2.0.2
A curl on the RabbitMQ API works fine
Celery Flower is talking to rabbit fine with the following config options
My rabbitMQ Monitoring plugin is configured like so in config.yml
In my troubleshooting, I followed this guide to add /opt/ca/cacert.pem to a Java keystore.
https://github.com/MichalHecko/SSLPoke
I initialize the machine agent as follows
I am still getting the following error for every RabbitMQ api call by the monitor in machineagent-bundle-64bit-linux-4.5.14.2293/logs/machine-agent.log
What am I missing?
Thank you!
We are instrumenting java agents on Tibco.
There are few JVMs ob each server and we are trying to configure unique node name (since which ever JVM starts first we get data only for that JVM).
We tried to add the following in the startup command in the tra file:
 -Dappdynamics.agent.tierName=%tibco.deployment% -Dappdynamics.agent.nodeName=$HOSTNAME.%tibco.deployment%
which didn't work since $HOSTNAME was not translated.
We need to define this dynamically since at every tibco deployment the configuration is lost and if we indicate a specific node name we have to reconfiure every tra separately.
How can we get the hostname dynamically into the tra file so we won't have to redefine every JVM (and we have many) after every deployment ?
Regards,
Yy
Our existing Infra is hosted on private servers.
AppDynamics  is used for monitoring hundreds of application &amp; host performances.
As a move we are moving all our applications on Azure.
Is this possible to get away from AppDynamics &amp; use any Azure solutions  for the same purpose.
Possibly Azure App Insight/Monitor ?
?
We have tried Java Application Monitoring on  Azure App Insight ; Azure Monitor is useful there.
Also we have used LogAnalytics for creating various performance Dashboards on Azure Monitor.
Can Application Insight support all the similar features of AppDynamics: Like Workflow Monitoring Performance Monitoring etc etc..
I have react app using webpack.
Building a docker image out of it is failing.
It is failing because appdynamics package.
I am getting the error only during docker build and npm run build seems to work fine without any issues.
Environment:
React 16
Node 13
appdynamics var 4.5.20
webpack 4
Docker
package.json
webpack.config.js
ERROR
Im trying to monitor a page availability with Appdynamics 
I have an IIS server with one site and several applications.
Appdynamics .Net agent 20.4.1 installed on the monitored server
each application has a appName.svc web page that I can call to check if the service is up.
Im trying  AppDynamics Extension for URL Monitoring  and followed the installation instructions.
I can see in  Metric browser  the URL monitor section, under that I see 'Metric Uploaded'.
where do I see indication that a URl is down/up?
can I monitor multiple URLs, as i did in yml file?
config.yml  file section looks like this:
log:
With the Analyze -  Metric browser in AppDynamics, you can go in and look at all the various metrics in the tree, but there's no right-click "edit" option.
How do you modify/delete a metric?
HI I'm wondering if anyone has any thoughts on using  Appdynamics  to monitor WSO2.
Out of the box appdynamics detects the servlet request coming in and that it gets written to the database, but beyond that it loses track of the transaction.
so if anyone could give some help as to what other classes I should instrument, It would be a real help.
thanks
Sunil Vanmullem
Normally newer verisons of Appdynamics should display windows services if you add them specificially into the config.xml.
I did that, restartet the services and the agent, but nothing happened.
Did anyone manage to display the Services ?
If yes, where do they appear?
I develop common java web app.
I use  Hystrix  in my app, actually I have a  REST client  whose methods wrapped in  hystrix  commands.
My web app uses this rest client to communicate with remote server.
My web app is configured as described in  hystrix   wiki  (it's needed to calculate statistics for  hystrix  dashboard).
To monitor my web app I use  AppDynamics  tool, but after I started using  rest client  based on  Hystrix  all calls from my web app aren't being displayed in  AppDynamics .
When I switched implementation to client without Hystrix everything is working well as expected.
Maybe somebody knows what is the problem?
Thanks.
I want to analyse my Liferay Server with AppDynamics.
Is there special configuration to better analyse Liferay specific things or is the only way to check the JSP related execution and not the internal service calls from the JSPs?
System: Liferay 6.2 on Tomcat with a MySQL Database
I am trying to find suitable entry point for ES client.
At the moment I have:
Class that implements an Interface which equals org.elasticsearch.client.ElasticsearchClient
and method name: prepareSearch
It seems to collect number of calls but I wonder if there is a better configuration to make ES calls to show up in Tier Flow Map.
When having the AppDynamics performance monitor installed, the servicestack API fails to load with the following exception:
Could not load type 'd__38' from assembly '###, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null'.
StackTrace:
   at ###.BaseService 1.&lt;Any&gt;d__38.MoveNext() in ###\Services\BaseService.cs:line 190
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder 1.Start[TStateMachine](TStateMachine&amp; stateMachine)
   at ###.BaseService 1.Any(T request)
   at ServiceStack.Host.ServiceRunner 1.Execute(IRequest request, Object instance, TRequest requestDto)
Any help is greatly appreciated.
Thank you
I want to execute the below command through Java code.
This is to create connection to my Appdynamics Contoller
curl --user user@customer1:password ' http://192.168.1.11:9090/controller/rest/applications?output=JSON '
The java code that I have written for this is
I keep on getting below error
HTTP/1.1 401 Unauthorized
Error report  HTTP Status 401 -    type  Status report
message
description This request requires HTTP authentication ().
Can anyone please help.
I Have tried to find if Appdynamics provides any API but I could not find any.
So I want to know if there are any APIs available for creating users and project/job in Appdynamics, so that I can automate the process using Python scripts.
We are in the process of building a new server infrastructure and will be using Appdynamics for analytics of the Java applications.
Appdynamics has a lot of features, so it seems that server metrics via collectd to Graphite will no longer be necessary.
Application metrics can also be fed straight into Appdynamics.
How about Logstash, ElasticSearch and Kibana and centralised logging.
Is there still any reason to build an ELK stack for the Java developers when they can use Appdynamics?
I have installed AppDynamics's Java Machine Agent along with the URL Monitoring Extension.
Every day, for 1 or 2 hours, the metrics are not appearing on my metric browser.
I checked the logs corresponding to those time periods, and I see that the HTTP Requests are being made and are getting back HTTP 200 OK responses.
My assumption is that the extension is not sending over the metrics, but I am unable to understand the cause of it.
Can anyone point me into the right direction?
I was not able to find any information regarding configuration of  AppDynamics  agent for  JUnit  tests.
I would like to test performance of Hibernate queries of  Spring  based web service backed by  PostgreSQL  database.
Tests must be able to rollback the data at the termination.
Should it be unit or integration tests?
What is the best way to accomplish it?
How to make  AppDynamics  collect and display graphs of query execution times?
UPDATE:
I was not able to set up addDynamics agent for JUnit tests inside IDEA.
The VM arguments is pointing to agent  -javaagent:"C:\Tools\AppDynamicsAgent\javaagent.jar" , the firewall is off but for some reason in appdynamics web based (SaaS) set up dialog shows that no agent able to connect:
I'm using AppD as APM for my application and in slow transaction reports it shows most of the calls, which is not our application code and we are calling open source libraries method.
For example :
com.google.common.reflect.TypeVisitor.visit  method of google library takes almost 155 ms time and  com.google.common.reflect.TypeToken.equals()  method takes almost 60 ms. and  org.apache.tapestry5.internal.services.RenderQueueImpl.render()  takes almost 50 ms.
I want to highlight that  I've checked and my server is not loaded and both CPU and memory usage is very low as well this time taken is for very small amount of data processing .
Let me know the reason behind this and how can I optimize the performance of my application.
I have an application that is generating 3 kind of log files
and I want to analyse the performance of my server using appdynamics so what kind of data my logs should be generating to generate analytics for server health, performance, throughput, server utilization?
JMX used for monitoring and managing the services/components &amp; devices.
My question is about monitoring,for the monitoring purpose do we have to change any code if we use JMX.
If that is the case, App dynamics will solve this process without doing single line of code change ?
I haven't found anywhere an answer, so I decided to write here.
Is it possible to display AppDynamics Dashboards on the TV display?
Currently I'm using something like  GRUNT  (gruntjs.com), but nowhere can I find whether it is feasible with that?
Currently I'm using GRUNT for displaying tasks from Jenkins, but I don't know how to configure it with AppDynamics.
Regards,
Kamil
I am trying to setup the AppDynamics java agent.
I am facing issues in loading java agent in the JVM.
I try to add the below argument to the start.bat jvm options.
-javaagent:C:\javaagent.jar
However, the aem do not start after this.
I have kept AppMachineAgent folder in the same drive as the AEM installation.
However, javaagent.jar is not kept in the bin folder of the AEM.
Do I need to keep it in the bin folder?
Any suggested steps I am missing?
I am having multiple java application configured in my app-dynamics controller and they have their own java agent running and reporting the metrics.
My problem is that SLA for each application is different and if i change the slow transaction threshold for a single application.
it changes it for other application as well, which is creating lot of trouble for me.
So my question is how to configure separate transaction threshold for each application in AppDynamics controller  ?
I am able to authenticate and fetch details using local user account using the python SDK of App Dynamics is there a way to authenticate using AD from python API or using the REST/curl.
Is it possible to create a stacked area graph in AppDynamics?
I want to show the cumulative effects of API response and browser DOM ready to visualize where variance is originating.
I can put both of these on a single graph, but if I choose Area, they overlap.
How do I get them to stack?
I'm on AppDynamics Version 4.3.1.2, build 47
I just testing AppDynamics for my database, I am able get it work on MySQL 5 and SQL Server 2014, but I got a JDBC error on SQL Server 2005.
Here is the error log:
06 6月 2017 00:55:59,461 ERROR [AD Thread Pool-Global0] DBAgentPollingForUpdate:30 - Fatal transport error while connecting to URL [/controller/instance/DBAGENT_MACHINE_ID/db-monit
  or-config/37784]: org.apache.http.NoHttpResponseException: davinci2017060100542331.saas.appdynamics.com:443 failed to respond
  06 6月 2017 00:55:59,473  WARN [AD Thread Pool-Global0] DBAgentPollingForUpdate:62 - Invalid response for configuration request from controller/could not connect.
Msg: Fatal transp
  ort error while connecting to URL [/controller/instance/DBAGENT_MACHINE_ID/db-monitor-config/37784]
  06 6月 2017 00:56:00,026  INFO [-Scheduler-3] ADBCollector:141 - DB Collector DBSERVER01 is temporarily disabled.
06 6月 2017 00:56:01,026  INFO [-Scheduler-3] ARelationalDBCollector:59 - (Re)initialize the DB collector 'DBSERVER01'.
06 6月 2017 00:56:01,040  INFO [-Scheduler-3] MSSqlCollector:74 - Obtained connection for url jdbc:sqlserver://192.168.1.100:1433
  06 6月 2017 00:56:01,047  INFO [-Scheduler-3] MSSqlCollector:139 - SQL Server Version = 9.00.5057.00 ( 2005.0 )
  06 6月 2017 00:57:00,025 ERROR [-Scheduler-1] ADBCollector:172 -  Error collecting data for database 'DBSERVER01'
  com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near 'sys'.
at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:216)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1515)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:404)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:350)
          at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:5696)
          at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1715)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:180)
          at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:155)
          at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:285)
          at com.singularity.ee.agent.dbagent.collector.db.relational.mssql.AMSSqlCollectorDelegate.collectDBMSMetrics(AMSSqlCollectorDelegate.java:335)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollectorDelegate.collectPerMinute(ADBCollectorDelegate.java:88)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollector.collect(ADBCollector.java:156)
          at com.singularity.ee.agent.dbagent.collector.db.ADBCollector.run(ADBCollector.java:139)
          at com.singularity.ee.util.javaspecific.scheduler.AgentScheduledExecutorServiceImpl$SafeRunnable.run(AgentScheduledExecutorServiceImpl.java:122)
          at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
          at com.singularity.ee.util.javaspecific.scheduler.ADFutureTask$Sync.innerRunAndReset(ADFutureTask.java:335)
          at com.singularity.ee.util.javaspecific.scheduler.ADFutureTask.runAndReset(ADFutureTask.java:152)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.access$101(ADScheduledThreadPoolExecutor.java:119)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.runPeriodic(ADScheduledThreadPoolExecutor.java:206)
          at com.singularity.ee.util.javaspecific.scheduler.ADScheduledThreadPoolExecutor$ADScheduledFutureTask.run(ADScheduledThreadPoolExecutor.java:236)
          at com.singularity.ee.util.javaspecific.scheduler.ADThreadPoolExecutor$Worker.runTask(ADThreadPoolExecutor.java:694)
          at com.singularity.ee.util.javaspecific.scheduler.ADThreadPoolExecutor$Worker.run(ADThreadPoolExecutor.java:726)
          at java.lang.Thread.run(Unknown Source)
I need to fetch the transaction scorecard data of a business application using the REST API of AppDynamics .
Following is the a sample view of AppDynamics Transaction Scorecard
I have done through the AppDynamics REST API documentation to some extent,but not found anything so far .
Can anybody have any idea on this ?
I tried to call AppDynamics API using python requests but face an issue.
I wrote a sample code using the python client as follows...
It works fine.
But if I do a simple call like the following
I get the following response:
Am I doing anything wrong here ?
When I am invoking a REST URI from the browser using an URL like the following
http://:/controller/rest/applications//business-transactions?output=JSON
and this is providing the output as
This output is missing the indicator/field for severity information like WARNING,CRITICAL,NORMAL etc.
How to get the severity information from the AppDynamics REST call ?
We have implemented AppDynamics in Jboss application.
We have load balancer and autoscalling which means we will have node registration when new server comes up.
The problem here is Java and Machine Agent.
Java Agent can reuse name with prefix (Appd Controlled Node Names) , but machine agent node name need to be provided at configuration level.
We are getting two separate agents listed.
One is 100% with Machine Agent and another with Java agent.
We need Machine Agent will ping at same line.
https://i.stack.imgur.com/MhpeU.png
What is the meaning and use of environment variables in the Python Agent configuration for App Dynamics, as documented here:
https://docs.appdynamics.com/display/PRO42/Python+Agent+Settings
More specifically:
If a value is set in the file and the corresponding environment variable is also set, which one takes precedence?
If I want to use environment variables for some of these values, can they be omitted from the file?
Can AppDynamics show the request or response being exchanged between different microservices systems.
They show the call trace, but couldnt find the details of what is passing between the calls.
What are the differences in features between AppDynamics and Zipkin apart from the pricing since zipkin is opensource.
Can any of them show request or response, in their console?
Is this comparison even valid?
Appdynamics does a lot of other things beyond crash logs.
So using Crittercism for crashlogs is a good idea or bad.
Consider below code:
I could use AppDynamics "Java POJO" rule to create a business transaction to track all the calls to Job.process() method.
But the measured response time didn't reflect real cost by the async thread started by java.util.concurrent.ExecutorService.
This exact problem is also described in AppDynamics document:  End-to-End Latency Performance  that:
The return of control stops the clock on the transaction in terms of measuring response time, but meanwhile the logical processing for the transaction continues.
The same AppDynamics document tries to give a solution to address this issue but the instructions it provides is not very clear to me.
Could anyone give more executable guide on how to configure AppD to track async calls like the one shown above?
We are in the process of configuring AppDynamics for one of our applications.
Since there are many instances of the application, we want to add nodename with agentId and hostName so as to identify the different instances.
Below is how we are trying to do, but it does not seems to work:
Once I start the JVM the node name appears as CalculationEngine_null_null.
I was hoping nodename to come CalculationEngine_3_a301-564.com where 3 being the agentid and a301-564 being the host name.
Also even if the host name parameter is not correct at least it should show CalculationEngine_3_null
What could be wrong here?
Or is it not possible?
I am using the ActiveMQ extension of AppDynamics.
It is good to start.
With JMXRemote(enabled in artemis.profile) it is OK.
But, I want it from localhost.
JMX is enabled by default for localhost for AMQ.
AMQ management console use jmx internally and it works without JMXRemote enabled.
What service URL jolokia use internally to connect using JMX from localhost?
I have tryed with following URL:
serviceUrl: "service:jmx:rmi:///jndi/rmi://:1099/jmxrmi"
I'm trying to determine if AppDyanmics support Opentracing.
I've looked in the app dynamics site and stack overflow but can't find a clear answer.
Thanks,
Carlos
We have a JBoss fuse ESB instance Running Version 6.3.0.
With this, Installed as a Service using tanuki wrapper.
When passing the javaagent Argument The Application (hawtio)Breaks.
we have tried passing the argument in the following files:
JBOSSHOME/bin/karaf/
JBOSSHOME/etc/jboss-fuse-wrapper.conf
-javaagent:/agenthome/javaagent.jar
The JVM Loads the argument when it is passing in the wrapper.conf, but as mentioned before the application is not working when the argument is loaded in the JVM.
have anyone instrumented JBoss Fuse ESB with app dynamics before?
I implement AppDynamics in my iOS app, I put everything in doc 
 like initWithKey licence key, but if I write initWithKey in appDelegate my all service request return nil.
Can anyone help me Thank you.
How to copy an existing dashboard to the new project in appdynamics.
How does AppDynamics works internally, In my current company we are planning to use AppDynamics but teams want to know how it actually works such as How it collects data, how does it communicate and how it intercepts java transactions and other related stuff.
So I tried looking into AppDynamics knowledge base but did not get accurate technical answer I need.
not new to coding but I have been working in python before and shaking the rust off my Powershell and working with xml.
We are trying to automate the deployment of AppDynamics's .net agent and we have the deployment piece down as well as upgrade.
Now I am trying to include specific applications and tiers running on IIS.
So again shaking off rust here but I am trying the find a simple script to update the config.xml to add the application and update the tier from IIS.
It's not a complex xml file but any help or direction would be helpful
Just trying to find the right syntax to update the nodes properly and arguments

 
 #what I need updated
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;appdynamics-agent xmlns:xsd="http://stuff.com" xmlns:xsi="http://stuff.com"&gt;
  &lt;controller host="test.saas.appdynamics.com" port="123" ssl="true" enable_tls12="true"&gt;
  &lt;applications&gt; #need to create this
    &lt;application default="true" name="app1" /&gt; #need to add the default='true' if there is more than one app
    &lt;application name="App2"/&gt; #this 
  &lt;/applications&gt; #this
    &lt;account name="testacct" password="123456" /&gt;
  &lt;/controller&gt;
  &lt;machine-agent /&gt;
  &lt;app-agents&gt;
    &lt;IIS&gt;
      &lt;applications&gt; #there by default with the default settings 
        &lt;application controller-application="app1" path="/app1path" site="WebSite1"&gt; # need to add this
          &lt;tier name="app1-1" /&gt; #and this
        &lt;/application&gt; #this too
        &lt;application controller-application="app2" path="/app2path" site="WebSite2"&gt; #some more
          &lt;tier name="app2-2" /&gt; #you guessed it
        &lt;/application&gt; #this as well
      &lt;/applications&gt; #ends with this
    &lt;/IIS&gt;
  &lt;/app-agents&gt;
&lt;/appdynamics-agent&gt;
We have a spring boot app running in pivotal cloud foundry and we have also configured appdynamics and we can now see our app on appd controller GUI.
I would like to build a appd dashboard and call actuator end points like info , health.
How do i do this?
Or I am also open to other ideas on building appd dashboard for micro services
Please advise
I want to load "application flow map" data (that can be seen on the web UI dashboard) from AppDynamics APIs.
My goal is to upload the data in Neo4j so we can study our microservices architecture using graph algorithms.
The AppDynamics Application Model API doesn't seem to provide data up to this level.
I'll build a client later but for now I use curl with requests like:
curl --user MyUserName:MyPassword  https://hostname/controller/rest/applications/OurApp/tiers  and variations of this according to the documentation
https://docs.appdynamics.com/display/PRO45/Application+Model+API
Only curl requests for now.
See point 2.
I expect a JSON output all the tiers of OurApp with interactions between them.
I am facing  Magic v1 does not support record headers  while consuming the messages form Kafka.
I understand this comes for the older kafka client version.
But in my case AppDynamics is injecting a SingularityHeader as given below -
Kafka Client Version - 0.10.2.0.
I need suggestions here other than Upgrading Kafka Client version to 0.11.x from 0.10.2.0 (that is not an option).
Is there a way to disable this from APPD itself ?
I have a set of webservice endpoints.
I'd like to use AppDynamics to collect metrics on the performance &amp; error rate of these endpoints.
Are Business Transactions the right tool to use for this?
If not, then what  are  Business Transactions useful for?
(The documentation explains that Business Transactions monitor a single transaction from end-to-end.
I should conceptualize my transactions "from the end user's perspective" etc.
But this doesn't answer my question - what usecase do Business Transactions fulfill that isn't better suited to Information Points or Service Endpoints etc.?)
I have a web application running on JBoss/Wildfly and using RESTEasy.
I'm monitoring it with AppDynamics.
I've configured my business transaction detection to use a Java Servlet.
This just about works, but some of my REST paths contain UUIDs, for example:
Each time this end-point is invoked with a different UUID, AppD treats it as a different business transaction.
Is there a way to make AppD recognise UUIDs within a path, and group these into a single business transaction?
Something like:
I should be able to do it by applying a regex to the request's path info:
or even just
but I can't figure out how to escape it properly.
doesn't work, and neither does
I cloned the image from  https://github.com/Appdynamics/docker-machine-agent
and executed the  docker-compose up  command after the machine agent installation.
But I am getting below error while starting the machine agent.
docker-compose up
Creating docker-machine-agent ... done
Attaching to docker-machine-agent
docker-machine-agent | /bin/sh: 1: /opt/appdynamics/machine-agent//start-appdynamics: not found
docker-machine-agent exited with code 127
We are using AppDynamics and VisualVM to monitor our application heap memory usage.
We see similar graph as stated in these questions -  this  and  this .
the red boxes show idle system heap usage - peaks are seen only when system is in idle state and are even observed when no application is deployed.
the green arrow points to actual application in use state - When system is in use, we see relatively very less heap usage being reported.
Based on the clarifications in other SO questions, if we say it is due to garbage collection, why would GC not occur during application use?
When system is idle, we see system objects like java.land.String, byte[], int[] etc.
getting reported in AppDynamics, but how to find who is responsible for creating them?
Again, in the heap dumps taken during idle state, we see only 200MB out of 500MB memory used, when the server has dedicated -Xmx4g configuration.
How should we make sense of these observations?
I'm new in LoadRunner.
My problem is:
When I'm running the script from LoadRunner (Http/Https protocol), I'm unable to see the browserName(like firefox, Chrome, IE etc)  in monitoring tool (AppDynamics).
But when I'm running script from LoadRunner (using truClient protocol), I'm able to see the browserName in AppDynamics.
My Objective is to see the browser Name in AppDynamics Monitoring tool when I'm using HTTP/HTTPS protocol.
**
**
I have compared both the header when script was running from    different protocol using Fiddler, Burp-Suite.
(No difference) 
User-Agent string is common in both the protocol.
Can someone suggest/help me on this?
Thanks in advance.
Note:  
Version of LoadRunner is 12.60.
Just running with one transaction i.e launching the home page only from both the protocol.
Please let me know if you need more information from my end.
I'm using Xcode 11.3, my app is in Swift.
When I add AppDynamics pod to my project and try to build, I get "1222 duplicate symbols for architecture arm64" error.
All duplicates are in several .a static libraries that are unrelated to AppDynamics and that are all linked in "Link Binary With Libraries" in Build Phases (there are other .framework-s here too but they don't cause any issue).
I tried all the standard things like cleaning build folder, deleting derived data, restarting computer, building with Xcode 11.2 version, playing with -ObjC flag, but none of this helped.
Since there are so many duplicates, changing those static libraries is not an option as it has been suggested in some threads.
Project also has other pods added that all worked fine.
I see that there were other similar questions, but I could not find an answer that worked in my case, I've been stuck on this for more than a day already.
Does anyone have some other suggestion what I could try?
I would like to understand why this is happening?
I have a jar file of java application.
I have appdynamics agent installed on my machine.
But not sure how to pass paramters so that appdynamics will monitor this java application.
My requirement is - AppD Health rule should trigger alert when call per hour &lt; x (eg: 10).
But while setting this,  I could not find call/hour metric in AppD controller.
It has all perMin metrics.
So how to achieve this alerting criteria in AppD health rule?
I'm trying to get the AppD HTTP Listener working on my Linux system.
My AppDynamics java process has the required argument running: -Dmetric.http.listener=true
I ran a curl command and also SoupUI and received "Couldn't connect to host" error.
I checked and couldn't find a port being listened too on the Linux server.
AppD Reference:  https://docs.appdynamics.com/display/PRO45/Standalone+Machine+Agent+HTTP+Listener
I am trying to configure AppDynamics for a Springboot project and I am getting following exception:
I can provide more information if anybody is familiar with the exception
I am working on spring boot and using gradle build tool.I am just wondering how to configure syntectical transaction in my project.
Any response will be highly appreciated.
we installed grafana-appdynamics plugin in the browser.
appdynamics shows metrics for individual nodes within cluster (it wont sum up for eg: Active sessions)
I wrote different metrics/queries for each node to fetch from appdynamics and plot in grafana under single panel.. now my question is how to sum up the all the values which are output of all the nodes at a given point?
As per app dynamics android documentation, we can set a userdata using below code in android
But is there a way to remove user data?
In ios, the App dynamics library has a method to remove.
below is the code
Is there a method to remove user data in android?
My current implementation to remove userdata is by setting empty string, is that correct?
or do we have similar method as that of IOS appdynamics method?
I've logged in, Created a Tier, and I do not see the &quot;Node&quot; with the name of the server/msmq that I need to add.
What am I missing?
Current setting:
• a collection of 30 or so microservices in Java, Node.JS and PHP running on Fargate (a very small portion of them on EC2 instances)
• AppDynamics agents installed on all of them but out of date by several months at best
Desired outcome:
• upgrade agents on a schedule (once a month)
• the download and unzip should happen on schedule but the deployment should be OKed once confirmed it doesn't introduce bugs
What we have explored:
• because of the second point above, we've dismissed the option of doing this in the Dockerfile
• creating a cronjob on an existing instance with similar function to download and unzip to EFS, EFS volume is then associated with the task definitions.
The problem we encountered is that EFS has a limit on associations per availability zone.
• explored the option of using a Lambda written in Python
• currently exploring the option of automating it with Chef
With the last two options we don't have very much expertise at all in the team.
Does anyone have experience with automating this?
In your experience what would be the best and easiest way to achieve this?
I have a java application that writes some events to a cache (vendor product) .
I would like to set up a health rule violation in AppDynamics to detect when connections to the cache fails.
what would be the best way to do so?
We are using AppDynamics to monitor an iOS app.
In Experience Journey map in AppDynamics what does the &quot;drop-off rate&quot; mean?
drop-off rate
I have an app built with SpringBoot and Spring Cache Abstraction, using Redis through Lettuce.
I need to monitor via APM AppDynamics tool, but by default it only gets data from Jedis.
I can create an exit point in AppDynamics, but I need to know exactly which class and method is responsible for opening the connection and executing commands to REDIS.
Can anyone help me with this issue?
I created python script to promote dashboard form one environment to other like from dev to test and then to prod using Custom Import and Export API ( https://docs.appdynamics.com/display/PRO45/Configuration+Import+and+Export+API )
The problem that I am facing is, If I am sending a dashboard which is in DEV to TEST and if the same dashboard already exists in TEST, then ideally it should overwrite it, but is creating a duplicate for the same, which is not what I want, can you suggest something....
I am wondering if there is a way I can use AppDynamics to calculate an Apdex score for my APIs and Apps.
If so, what would be the best possible way to do so?
We're using Appdynamics Java agent for monitoring our production applications.
We have noticed slow growth in memory and the application eventually stalls.
We ran a head dump on one of the JVMs and got the below reports.
Problem Suspect 1: 
 The thread com.singularity.ee.agent.appagent.kernel.config.xml.a@ 0x1267......
AD thread config Poller keeps local variable config size of 28546.79(15.89%) KB
Problem Suspect 2: 
 280561 Instances of
com.singularity.ee.agent.appagent.services.transactionmonitor.com.exitcall.p loaded by com.singularity.ee.agent.appagent.kernel.classloader.d@ 0x6c000....
occupy 503413.3(28.05%) KB.
These instances are referenced from one instance of java.util.HashMap$Node[]...
We figured that these classes were from the Appdynamics APM that hooks on to the running JVM and sends monitored events to the controller.
There is so much convoluted process associated with reaching out to the vendor, so I am wondering if there are any work arounds for this like we enabling our java apps with JMX and Appd getting the monitoring events from JMX rather than directly hooking on to the applications' JVM.
Thanks for your suggestions.
Do we have any opensource or any proven code which collects  the App Dynamics reports from App Dynamics Servers ?
In regular SQL, I could write a query like:
However in ADQL syntax, the following query will work for a single column, but not for multiple:
Is there any options in ADQL ( https://docs.appdynamics.com/display/PRO21/ADQL+Reference ) to achieve the same end result?
I tried the following &quot;hack&quot; (and with a small set of data it seems to work... but will timeout with a large set of data).
I'm new to APPDYNAMCS and looking for APPDYNAMICS Public Rest APIs for the below data.
I'm able to find out a few of them but not all.
Can someone help me with this?
Thanks in Advance
Looking for REST APIs for the below data.
1.Configuration Items( Business Application, servers, business service, etc) and relationship among them.
2.Service Map data.
3.Raw Event.
4.Alert data.
5.Raw Metrics.
6.Raw Logs.
7.Raw Traces.
8.SLO/SLI data.
9.Real User Monitoring / Synthetic Monitoring data
10.User sessions data
I'm trying to get the list of existing Applications using python script.
Here is my script which fails.
I'm new to python scripting.
Basically I want to login to the application first and list the existing applications using the python script.
go build  is unable to find the 'appdynamics' package, even though the GOPATH is properly set.
I downloaded the package, copied it onto the GOPATH:  ~/go/src/appdynamics  and ran  go install appdynamics .
I am using go v.1.10 on Ubuntu 18.4.
Visual Studio Code is able see the package and code completion works within the IDE.
However, running  go get -fix -v appdynamics  produces the following error:
I have also tested this using the github.com/appdynamics namespace per the appdynamics go-sdk instructions.
Also, I am aware of all the other go build 'cannot find package'  questions  on S.O.
My java application is connected to remote webservice application where appdynamics is not installed.
I am seeing those services as backed services.
The remote webservice application has multiple webservices.
I want to track response time of each webservice separately.
Should i create different tier for each service or resolve all services into single tier?
Is there any other better way of doing this?
has anybody gotten Appdynamics java agent to detect Apache Camel business transactions?
Picking up files from a directory (polling) and then sending off to activemq.
Another case is camel deployed on apache karaf, need to track outgoing http calls using appDynamics
Best
Hi I am fairly new to appdynamics and using it to configure my server for trial period, I have 3 tomcats, I followed the documentation I got to know that we need to put appagent and machineagent to through data back to controller, If I try to download appagent and machineagent jar file fromir their official site I always end up with the same version and which ever tomcat starts first I get data only for that machine
 
This is what I have used for tomcat  catalina.sh '
On our ASP.Net website, we've had some requests timeout.
AppDynamics shows that the SQL procedure calls are returning in a matter of seconds, but we're spending 100+ seconds in SNIReadSyncOverAsync.
Does anyone know what this method is / does and why it would be taking that much time?
We're not using EF which is referenced in every question / post I've been able to find about it.
Thanks in advance
Update
It's been a while and while we never came to a resolution as to why all of the time was being spent in SNIReadSyncOverAsync, I have a few thoughts.
I think that in this case, it may have been the way that specific version of AppDynamics was reporting the time spent on the SQL calls, but I have no real data to back that up, just my guess from what I observed.
We eventually stopped seeing the time reported as being spent in SNIReadSyncOverAsync and it shifted to the queries themselves timing out.
That still didn't make a lot of since because the same queries would run instantly in SSMS on the same database.
The ultimate answer ended up being related to ARITHABORT causing our application and SSMS to use two different execution plans (see  https://dba.stackexchange.com/a/9841 ), explaining why we couldn't reproduce the timeouts with SSMS.
Once we resolved that, we were able to identify a few portions of the procedure that needed tuning and we haven't run into the unexplained timeouts or SNIReadSyncOverAsync since.
I am using
with G1 garbage collector.
JVM argumens are
However, I am experiencing following Full GC scans without any apparent reason, how to get rid of them?
GC log with some tail from preceding events:
Other similar ones:
Other similar issue reports:
http://grokbase.com/t/openjdk/hotspot-gc-use/1192sy84j5/g1c-strange-full-gc-behavior 
   http://grokbase.com/p/openjdk/hotspot-gc-use/123ydf9c92/puzzling-why-is-a-full-gc-triggered-here 
   http://mail.openjdk.java.net/pipermail/hotspot-gc-use/2013-February/001484.html
I have been analyzing the issue using appdynamics profiler and I have found out that every time Full GC occurs, Code Cache (configured to its maximum) is full.
It seems like a bug in GC.
See also the profiler image, two unnecessary Full GC:s in middle between 24/5 and 25/5.
More importantly, they kill the server usability, because they last 60 seconds each:
Profiler log image http://eisler.vps.kotisivut.com/logs/g1gc-code-cache-full-gc-bug-illustration.png
See also discussion about Azul's pauseless GC, they seem to have worked out this kind of issues  http://www.artima.com/lejava/articles/azul_pauseless_gc.html
I'm developing a social-like application which is currently deployed using AWS services.
In particular, the DB runs on RDS using MYSQL.
So far, we're testing the app using a limited number of users (mostly friends) resulting in an average of 15 Write IOPS/sec.
The real problem is related to the very high writing latency of the db, which is always above 100ms.
The RDS instance is a db.m3.xlarge which is much more than what we need.
I tried to perform a load test in a separate instance (identical configuration of DB and EC2) but i've not been able to reproduce such a high latency, even if I was sending a much higher number of requests.
So I thought it may be due to table fragmentation, but i've not yet run a table optimisation, because the db wouldn't be accessible during this procedure.
Do you have any experience with this problem?
MORE INFO
The biggest table (called  Message ) has about 790k rows.
Concerning this table, the following query
took 11s to be executed.
Even worse, the query
took 14s, but the table Comment has about 160k.
Those two tables are generated by:
and
SOME PLOTS
Using  AppDynamics  I've been able to extract the following plots:
Wait States : Isn't the query end time too big?
Page Buffer :
Write Latency and Queue :
Query Cache
Thank for your help!
Andrea
For one installation of our application we have been seeing issues on production that were reported as "system is getting slower" or "requests never returning" by the users.
In the end the server had to be restarted.
We had several of those incidents and a nightly restart of the server seems to be working as a workaround.
Our application makes heavy use of dynamic classloading (.jar files stored in database as blobs) and reflection.
Environment details:
We switched to
but it looks like we are still facing the issue.
What we are seeing in the logfiles, heapdumps, threadumps and gc logs so fare is the following
What we are seeing is
See below for an excerpt of the threadump:
The situation after updating to Java 1.7.80 / G1Gc seems to be similar.
Unfortunately no threadump available, just wicket warnings in the log)
We are currently unable to reproduce this (still working on that).
But maybe somebody in the community has seen something similar and has an idea what could help us to reproduce or solve the issues.
One guess that we currently having is that this is related to native memory consumption (Because of information like  http://www.ibm.com/developerworks/java/library/j-nativememory-linux/ ) but we don't see any hints in this regards in the logs (no OutOfMemory errors, no reports from the linux administrators that the system is running out of memory)
Because of a known (fixed) bug in Hazelcast 2.5 we've decided this would be the next upgrade candidate for our project.
But after dropping in the latest version (3.2.2) we had horrible performance.
The way we are using Hazelcast:
Using Hazelcast 2.5 we had great performance when, instead of using  map.values() , we supplied a list of all contained keys  map.getAll(containedKeys) .
The way we keep track of the containedKeys by adding an  EntryListener  to the map which stores the containedKeys in a concurrent set.
This was added by a colleague and feels like a hack, but works like a charm.
Now when we upgrade to Hazelcast 3.2.2 we instantly see problems with  java.io , for example look at the following snippet from AppDynamics:
This is something we haven't seen in Hazelcast 2.5, but do have in 3.2.2.
It grinds our application to a complete standstill.
Replacing the jar with 2.5 again (and renaming Entry back to MapEntry) and nothing is wrong.
What could be causing this?
Maybe it isn't using the near-cache anymore?
I have a website written in cakephp on linux server.
I have a problem with extremly slow download time of my css and js files.
For example, thats the network tab in chrome when loading my homepage:
As you can see, one of my css files took 59 seconds to download!
Its important to note that it is not always the same css file.
Sometimes its JS file, sometimes other css but they have to be downloaded before other content of the page is displayed, therefore they block the page loading.
Because of waiting for that one file to download, website is not displayed for 59 seconds.
I checked my server and it has a very low load, cpu runs on 10% and there is less than 20% of ram used.
Its an apache server with the following prefork settings:
This mentioned slow download time happened with maybe 3-4 simultaneous users on the website.
I have my app under APM with appdynamics and nothing suspicious is shown there.
I checked php.ini file with server admin and everything seems to be good there as well.
What other software can I use to find the source of this issue?
There is not much info in apache logs either.
Any suggestions would be greatly appreciated
EDIT:
I moved all of my assets to webroot and got these results on another domain that is using that same server:
As you can see, this time its jquery file that took 27 seconds to download.
It is stored in the app/webroot
We have a production web application running on our intranet which:
is configured with:
Each day the heap usage:
at which point the heap rises to 55% in about 40 minutes and is collected back to 37%, ad infinitum until the next restart.
We have AppDynamics installed on the JVM and can see that Major Garbage Collections take place roughly every minute without much of an impact on the memory (except the falls outlined above of course) until the memory reaches 37%, when the Major collections become much less frequent.
There are obviously hundreds of factors external to the behaviour of a web application, but one avenue of research is the fact that Hotspot JIT information is obviously lost when the JVM is stopped.
Are there GC optimisations/etc which are also lost with the shutdown of the JVM?
Is the JVM effectively consuming more memory than it needs to because certain Hotspot optimisations haven't yet taken place?
Is it possible that we would get better memory performance from this application if the JVM wasn't restarted and we found another way to perform a backup of the database?
(Just to reiterate, I know that there are a hundred thousand things that could influence the behaviour of an application, especially an application that hardly anyone else knows!
I really just want to know whether there are certain things to do with the memory performance of a JVM which are lost when it is stopped)
I'm monitoring a production system with  AppDynamics  and we just had the system slow to a crawl and almost freeze up.
Just prior to this event, AppDynamics is showing all GC activity (minor and major alike) flatline for several minutes...and then come back to life.
Even during periods of ultra low load on the system, we still see our JVMs doing  some  GC activity.
We've never had it totally flatline and drop to 0.
Also - the network I/O flatlined at the same instance of time as the GC/memory flatline.
So I ask: can something at the system level cause a JVM to freeze, or cause its garbage collection to hang/freeze?
This is on a CentOS machine.
I have a nodejs project with a few modules, one of them (appdynamics) serving a native binary depending on the platform/architecture of the requester.
This in combination with the fingerprinting of yarn in the lockfile creates an issue:
Yarn does not allow this out of the box because of the aforementioned fingerprinting.
For now the only workaround I've found is to mount the codebase into a Docker container and run yarn commands from there, then commit the lockfile changes.
Is there a better way to achieve this?
We have serious application issue at peak time application get very very slow and when i check on AppDynamics matrix, my heap memory is full and GC kicked in every minute and that make it very very slow.
here is the configuration of my java (tomcat)
Java options are  -Djava.awt.headless=true -Xmx2048m -XX:MaxPermSize=256m -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+DisableExplicitGC
Major GC collection time spend per min (ms)
CMS Old Gen usage in MB
Par Eden space in MB
Any suggestion why par eden space and old gen hitting hard line?
Here is the last 12 Hour picture of Heap usage and Major GC collection (in Green dots), GC was very high between  3:00AM to 7:00AM  but when i restart application around  7:30AM  everything is good and application response time was very fast, why reboot fixed everything?
Major GC collection time spend per min (ms) after 4GB (Zero Major GC)
CMS Old Gen usage in MB after 4GB Heap
Par Eden space in MB after 4GB Heap
I am having issues with concurrency when writing JSON out from my Spring Boot WAR app deployed to Tomcat 8.
In the screenshot from AppDynamics there seems to be a considerable wait when the jackson library is performing _flushBuffer.
This issue arises under load testing for even a small amount (&lt; 10) users.
I have configured the messageConverters in my configuration class.
I am using 
Spring Boot 1.5.4
Java 1.8
Jackson 2.9.7
Tomcat 8.5.33
Is it possible to monitor Play Framework application performance with Javamelody?
I'm using Javamelody with Spring apps.
I find it much better than free version of AppDynamics or Dyna Trace.
You can't use filter for HTTP monitoring or aspect for method monitoring.
I think I should make something like filter or aspect.
I have no idea how to add performance monitoring to JDBC queries.
Any ideas?
My multi-jar app runs in Java 11 and shows a warning related to Log4j2:
WARNING: sun.reflect.Reflection.getCallerClass is not supported.
This will impact performance.
It doesn't crash, but quite bothers me since the Operations team (AppDynamics monitor) has asked me about it.
I read that I need to use the "Multi-Release:true" entry in the manifest, but I don't kow how to tell the Maven Assembly Plugin to add it.
I don't use any other plugin in the pom.xml.
Should I use the  Maven Shade Plugin  instead?
Anyway, here's the Maven Assembly Plugin section of my pom.xml.
The library I'm including (that I also wrote) uses Log4j 2 as a dependency, as shown below:
How can I get rid of this warning?
We have developed set of APIs using spring boot.
When performance test was run and hitting more than 5000 calls/minute, avg response time started increasing.
When we investigated with the help of AppDynamics, more than 2% transactions are having slow response time (more than 1.5 seconds).
But the CPU usage is still under 20%.
And all of them are waiting exactly at the same location i. e  org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter:invokeHandlerMethod:827.
Below is the complete call graph.
What is the root cause for threads handing at this location?
Are there any SpringBooot properties to be updated to eliminate this?
Following is the source code of RequestMappingHandlerAdapter:invokeHandlerMethod and it hangs at invocableMethod.invokeAndHandle(webRequest, mavContainer);
protected ModelAndView invokeHandlerMethod(HttpServletRequest request,
            HttpServletResponse response, HandlerMethod handlerMethod) throws Exception {
Recently, our company is focusing on performance of the application we are developing for long time.
One thing we noticed during performance test, certain methods are making so many database calls (over 500 queries)
Then this brings a question like, which methods are doing so many calls and how should be prioritize which method to refactor first.
When we initially try to refactor some of those methods, we observed that it is requiring a lot of effort to reduce the number of round-trips.
The reason is our data access layer is pretty much depending on NHibernate ORM framework and we figured out that we have totally misused Lazyloading configuration from the beginning of the development.
That is why number of round-trips are huge and impacting the performance a lot.
And just correcting Lazy Loading configuration creates a lot of regression.
Thus, we somehow have to figure out a way to collect number of database call per http request.
I have seen some tools like Application Insight or AppDynamics provides overall result for all Dependent calls.
But I am just wondering is there a way to collect these traces differently than using those frameworks ?
For instance, every time http request is made, can we have attribute in the controller that whenever ExecuteQuery() or SqlDataAdapter.Fill method is called within the call stack of the method, can it increase the counter.
I am looking for a solution something like this.
Any help is greatly appreciated.
Thank you in advance for all suggestions.
We have a situation where we have a Grails 2.3.11 based application that uses Quartz (version 2.2.1 / Grails Quartz plugin version 1.0.2) jobs to do certain long running processes (1-5 minutes) in background so that a polling service allows the browser to fetch the progress.
This is used primarily for import and export of data from the application.
For example, when the application first starts, the export for 200,000+ rows takes approx 2 minutes.
The following day the export takes 3+ minutes.
The third day the export takes more than 6 minutes.
We have narrowed the problem down to just the Quartz jobs.
When the system is in the degraded state all other web pages respond with nearly identical response times as when the system is in optimal condition.
It appears that the Quartz jobs tend to slowdown linearly or incrementally over the period of 2 to 3 days.
This may be usage related or time, for which we are uncertain.
We are familiar with the memory leak bug  reported by Burt Beckwith  and added the fix to our code.
We were experiencing the memory leak before but now memory management appears to be health, even when the job performance is 5-10x slower than
The jobs use GORM for most of the queries.
We've optimized some to use criterias with projects so they are light weight but haven't been able to change all the logic over so there are a number of Gorm objects.
In the case of the exports we've changed the queries to be read-only.
The logic also clears out the hibernate session appropriately to limit the number of objects in memory.
Here are a few additional details:
Any suggestions would be greatly appreciated.
Thanks,
John
The following code:
Has started throwing the following exception on SOME machines.
What could cause this?
EDIT: the machines that experience the error are running Windows Server 2008 R2.
Windows Server 2012 and desktop machines running windows 7 work fine.
(this is true, but I now think a different issue is the relevant difference... see below).
EDIT: as an additional note, this occurred right after updating our codebase to Entity Framework 6.1.1.-beta1.
In the above code, The IDisposable is a class which wraps an EF DbContext.
EDIT: why the votes to close?
EDIT: the stack trace of the failure ends at the  WeakReference&lt;T&gt;  constructor called in the above code:
EDIT: it turns out that the machines having issues with this were running  AppDynamics .
Uninstalling that seems to have removed the issue.
Are there any tools that can connect to a remote JVM and log to a file the information which is displayed in the Monitor tab of JVisualVM or the Overview tab of JConsole?
I am aware of applications such as AppDynamics etc - this is for a little performance test for a machine which is already set up - even though we have AppDynamics licences, using AppDynamics isn't really an option in this scenario.
I have spring enterprise app running on JDK 1.6 under Windows 2008.
The app gets slow or unresponsive at random times.
I suspect it is memory leak and the GC is kicking into over drive.
How can I troubleshoot this without restarting JVM using java.exe -verbose:gc parameter?
I really cannot shutdown this app.
I'm planning on doing AppDynamics on it once I can restart it but for know what can I do?
What are my options?
I have a requirement to pass  cluster, namespace and pod name to AppDynamics agent from my container deployed in Kubernetes cluster.
I tried something as below, but that does not work.
and
Could anyone please help me here how to collect the detail and pass to AppD.
Thanks in advance.
Have a simple method for connection,
In case of URL path not being a valid one, FileNotFoundException is getting logged as an error in AppyDynamics.
How to prevent AppDynamics from catching these exceptions since as part of the code its handled as a boolean return but AppDynamics is flooded with FileNotFoundException.
Thanks in advance.
Update 
As per AppDynamics documentation  https://docs.appdynamics.com/display/PRO44/Errors+and+Exceptions 
 An HTTP error response, such as a status code 404 or 500 response  get recorded as a transaction snapshot error.
As i know at this point in my code above response 404 is legitimate.
How can I modify my code to prevent AppDynamics showing it up ?
Any Suggestions will be helpful.
I am working on the ASP.NET application which is used by 10K people approx.
Till last week it was working fine and suddenly from past week it's performance is degraded.
Application is taking too long time to respond, also opening each page taking a lot of time(1-2 mins approx.)
even though no complex calls or functionality is present.
I have checked all stored procedures in database and all are working fine and their execution time is less than 5 seconds.
Also using SAAS AppDynamics tools I have checked all calls and request-response time in each page of the application, but it seems everything to be fine there also.
All calls between application server and data base server are happening in 2-3 seconds.
I am not able to find where exactly the issue is.
Is there any applicaiton like AppDynamics which can be helpful?
NOTE : I am using linked server in some stored procedures.
Need help in identifying the issue.
Thanks in advance.
My node app's CPU usage is gradually increasing.
I have found that memory leaks are happening.
Through AppDynamics, I have found that there is a significant amount of retained memory which keeps increasing over time under  processImmediate  call tree.
As I drilled in, I found the problem was with  settlePromises  function.
I want to get your opinion on one particular usage of promises I have been using.
Looping of promises.
Below is a sample function structure of such usage.
The heap growth over an hour is plotted in the below picture
The above function has to perform a synchronous update with the objects in data array.
Is there a chance of memory leak with this?
I'm very new to nodejs.
In my dockerized environment, I want to provide appdynamics support to nodejs apps.
This mandates every app to require the following as the first line in their app.
I plan to do that by providing a wrapper called  appdynamics.js  around the app's entry file.
Details:
I run a script in my nodejs docker image to replace the entry file name in the app's package.json with "appdynamics.js", where appdynamics.js has the above appdynamics related require statement.
Ex :  {scripts { "start" : "node server.js" }}  will be replaced with 
  {scripts { "start" : "node appdynamics.js"}}
Then, i "require" the "server.js" inside appdynamics.js.
Invoke npm start.
My only concern is this:
If the  package.json  had something like scripts  { "start" : "coffee server.coffee" } , my script will replace it to  { "start" : "coffee appdynamics.js" } .
and then my script will invoke  npm start , which will error out.
What is the best way to solve this?
This is a follow up question to  Use &quot;coffee&quot; instead of &quot;node&quot; command in production
I am creating a REST api to send message to RabbitMQ and was trying to understand what are the best practice for creating/closing channels.
I am using RabbitMQ Java client api.
Currently I have a class  RabbitMQPublisherConnection  where I spring inject RabbitMQ connection.
This class is then spring injected to another class  RabbitMQPublisherChannel .
This class has the following function to create a channel:
Now I have the third class  RabbitMQPublisher  where I spring inject  RabbitMQPublisherChannel  class.
My application context looks like this:
The class  RabbitMQPublisher  has the function to publish a message to RabbitMQ:
This application is run through tomcat and I noticed with AppDynamics that the closing the channel takes like 47% of the total time taken to publish message.
When I remove the call to close the channel then I save this 47% of time which is like 32ms but then I notice in my RabbitMQ management console that the number of channel is ever increasing for that connection.
So my questions are -
Thanks
So, detailed description.
I'm working on a microservice framework that uses Rabbit as the event bus.
Each service runs on it's own dedicated VM inside a Tomcat container (4 cores, 4GB RAM of which 2 are available to Tomcat).
Each service both consumes and publishes messages back to Rabbit.
When I crank up the consumers, channel settings and prefetch size I can get an individual service to perform well.
My problem comes when I try to test scalability, i.e.
a 2nd VM instance with the service running on it.
Instead of the throughput doubling (or at least increasing), it can actually get slower, and I'm very confused.
I've checked for errors and exceptions in the service, used analytics tools (AppDynamics) to check the time spent in the service and the resources used, and everything looks fine, so as far as I can tell it's my Rabbit configuration that's the problem.
The specific settings used to achieve high performance for one service are:
-Consumers: 20.
-Channel cache size: 200
-Prefetch: 500
Using this it seems to work quite well.
However when I add the second service the queues aren't being consumed as fast and they start to back up quite quickly, and I'm at a loss to understand why.
I've experimented a little with the settings above but can't seem to get anywhere.
I don't have any access to the Rabbit cluster to change settings so I can't do anything there, but I have full control over the VM my service runs in (Java settings, Tomcat, Rabbit settings..)
The service doesn't do anything explicit with connections or ack policies, so it's possible they may need to be tweaked?
A few articles mention that there should be one channel per consumer (or even 1 for consume and 1 for publish), but that makes things slower than the larger figure above..
I'm at a loss, so any help is appreciated, more details can be provided.
Java: 7
Spring Core: 3.2.2
Rabbit AMQP: 3.1.2
Spring Rabbit: 1.3.5
Spring AMQP: 1.3.5
EDIT: I'm using a ConnectionFactory in Spring XML config (defaults to a CachingConnectionFactory I believe) that I set the channel cache size on, and then set the factory into the listener container, dunno if that helps..
I have an Spring+Hibernate+Tomcat+MySql application in production, I'm running into a problem.
I think the application is not closing it's jdbc connections, and when it reaches its limits (currently 200), the application stop responding, and I have to restart tomcat.
Do I need to close this connections somewhere ?
Here is my Datasource:
And here is an image of the appdynamics monitoring the connections, from 3 days until now
Here is a excerpt of the error I get on the catalina.out log file:
type Exception report
message Request processing failed; nested exception is
  org.hibernate.exception.JDBCConnectionException: Cannot open
  connection
description  The server encountered an internal error that prevented it from fulfilling this request.
exception
org.springframework.web.util.NestedServletException: Request
  processing failed; nested exception is
  org.hibernate.exception.JDBCConnectionException: Cannot open
  connection
  org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:932)
  org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:816)
  javax.servlet.http.
.
.
.
root cause
org.hibernate.exception.JDBCConnectionException: Cannot open
  connection
  org.hibernate.exception.SQLStateConverter.convert(SQLStateConverter.java:99)
  org.hibernate.exception.JDBCExceptionHelper.convert(JDBCExceptionHelper.java:66)
  org.hibernate.exception.JDBCExceptionHelper.convert(JDBCExceptionHelper.java:52)
  org.hibernate.jdbc.ConnectionManager.
.
.
.
root cause
com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException:
   Too many connections 
  sun.reflect.GeneratedConstructorAccessor67.newInstance(Unknown Source)
  sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
  java.lang.reflect.Constructor.newInstance(Constructor.java:513)
  com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
  com.mysql.jdbc.Util.getInstance(Util.java:381)
  com.mysql.jdbc.SQLError.
UPDATE
The Category domain object is mapped like this:
So my guess is, taking mithridas comments into account, that I´m using the hibernate  session´s manually, And I would need to close them with something like this:
Or i could implement @PersistenceContext.
Would anyone direct me to this implementations so I can evaluate which is best for us to use ?
Thank you.
UPDATE2: 
Added more info in answer to James Massey comment:
These are my: datasource, sessionFactory, transactionManager, and categoryDAO sessionFactory assignment:
I am confused where to set enviroment variables in Ubuntu 12.04
Now I am giving like this {editing 2 files to set path variables }
export JAVA_OPTS=&quot;$JAVA_OPTS -Xms1024M -Xmx2048M -XX:MaxPermSize=1024M -XX:PermSize=128M&quot;
#------------------- PATH SETTINGS ------------------#
#-------- Ant Home  
ANT_HOME=/programs/apache-ant-1.8.0
#-------- Maven Home  
M2_HOME=/programs/apache-maven-3.2.1
# --------- JDK 1.6 Home  
JAVA_HOME=/programs/java/jdk1.6.0_37
# ----------JDK 1.7 Home  
#JAVA_HOME=/programs/java/jdk1.7.0_09
# ------------- Path Settings  
PATH=$PATH:$JAVA_HOME/bin:$ANT_HOME/bin:$M2_HOME/bin
#----------Enabling AppDynamics Viewer---------  
PATH=$PATH:/programs/AppDynamicsLite/LiteViewer
I am getting no errors as all paths are set and i can use JAVA, JAVAC, ANT &amp; MAVEN
I am not prefixing export command to set paths in .profile
Only heap settings are put in .bashrc
but i havenot used ANT_OPTS and MAVEN_OPTS
Hence i am confused whether they are needed or not
I am using a Java web application called AR System.
After installing the PHP-Java Bridge, I started seeing  java.lang.OutOfMemoryError: PermGen space error  in the Tomcat logs.
(I see in Windows Task Manager that there are 6 PHP-CGI.exe processes, all similar in memory footprint, give or take 5 MB).
It would occur every other day or so and then shortened to every day, sometimes twice a day.
Consequently, the application hangs and I have to restart it.
And I added a Windows Task to restart Tomcat during non-peak hours to give me some cushion.
I suspected a memory leak and started doing some research.
Normally, Tomcat sits at around 300-350 MB.
With the PHP-Java Bridge, memory jumped up significantly.
In fact, the error has occurred anywhere from 450-600 MB.
I learned that default PermGen is 64MB and PermGen should be set to 1/4, up to 1/3 of Tomcat memory (sorry, I don't recall the link).
Tomcat is running under Windows Services at this point, and I added the following to its properties:
I enforced GC on PermGen memory and increased the size from the default 64 MB size to 128-256 MB.
Memory went up all the way to 800-850 MB, slowly, but it wasn't hanging during peak hours, albeit I still had Tomcat intentionally restart during non-peak hours, via a Windows Task.
If I take off the restart, it  MIGHT  eventually hang but I haven't tried it.
I still suspected a memory leak.
I installed a trial version of AppDynamics to monitor the application, its memory, and run leak detection.
Additionally, to use tools like VisualVM and Memory Analyzer (MAT), I disabled The Tomcat Windows service and ran Tomcat from the Windows Command Line, via catalina.bat.
I appended Java Options to the file; I made sure Tomcat memory was 1024 MB, Perm Gen was 128/256 MB, and ensured PHP-Java Bridge and AppDynamics was running.
As of right now, PermGen is holding at 163 MB used, and AppDynamic's Automatic Leak Detection did not detect any leaks with any Java Collections.
I fired up MAT, created a heap dump and analyzed for leaks.
When I ran it yesterday, it found three possible suspects:
When I ran it today, it found 2 possible suspects:
So, with MAT and AppDynamics, it appears that no memory leaks were detected for classes directly related to the PHP-Java Bridge JAR files.
I haven't tried using Plumbr, but I can't find the free beta version.
The free version detects leaks, but you have to pay to see it.
Again, I don't have a source link at this time, but I recall reading that Tomcat 5.x  can  have performance and memory leak issues.
Of course, that doesn't mean everybody will have those issues, just a select number.
I know Tomcat 6 and Tomcat 7 redesigned their memory management or how they structure memory.
I also did speak with someone from BMC, the maker of AR System, and they said the current version of AR System I'm using could suffer from performance and memory issues.
But, again, none of this was a problem before the PHP-Java Bridge.
It was only after I installed it that this PermGen memory issue started.
Since the tools above did not report any leaks, does that mean there are no leaks and PHP-Java Bridge just needed more than 64 MB PermGen memory?
Or, is there an inherit problem with my version of Tomcat and installing the PHP-Java Bridge just broke the proverbial camel's back?
Upgrading to a newer version of AR System and Tomcat is not an option.
If there is a leak, I can uninstall the PHP-Java Bridge or continue trying to find a leak and fix it.
Any help would be appreciated.
Thank you.
Update 1
With MAT, I looked at the thread overview and stacks and you can see below that the PHP-Java Bridge contributes about 2/3 of the total heap memory of Tomcat.
That's a lot of memory!
I think there is a leak, I do.
I can't find any information on the PHP-Java Bridge having inherit memory leak issues.
But, to me, it appears that the problem is not that Tomcat is leaking.
Ideas?
AppDynamics couldn't find any leaks, even when I manually added classes that were suspected in MAT.
What I'm wondering is perhaps the PermGen error is a symptom of that case where the program has no leak and needs more PermGen memory allotted.
It would be helpful to know if the PHP-Java Bridge is designed to eat a lot of memory, this much memory; maybe it's optimized for 64-bit, since the current setup is a 32-bit Java Web application.
If I knew that this bridge needs a lot of memory, I would say OK, fine, and go from there.
But it certainly appears as if there is a memory leak somewhere in the chain.
Update 2
I've been running Plumbr now for 2 hours and almost 10 minutes.
I see that Tomcat memory is shooting up to 960 MB and probably will continue to climb.
For those familiar with the program, the Java web application has been analyzed 3 times.
So far, no leaks have been reported.
If it stays this way, then the two conclusions I've arrived at are a) there are no leaks or b) there is a leak and, somehow, both AppDynamics and Plumbr missed it.
If there are truly no leaks with this set of applications working together, then it must be that the Bridge uses a lot of memory and needs more PermGen memory than Tomcat's default, 64 MB -- at the very least, for 32-bit Java web applications.
I've been reading up on  AppDynamics  Lite all morning and absolutely love it!
It's pretty nice to be able to drop a JAR into your web app (WAR), deploy it and have it automatically run perf tests on your app.
I was wondering if anything similar exist for full-fledged profiling?
Something similar to, say, VisualVM, but that deploys as a JAR and that can be packaged inside a WAR?
Online searches didn't turn up much but then again I might not be searching for the right thing.
I call this an "intra-JVM profiler" because its profiling the same Java process its running inside of (like AppDynamics).
I love open source but this is not a mandatory requisite.
Thanks in advance for any pointers/recommendations!
I have integrated my loopback application with appDynamics library and all the middlewares are not firing from then on.
Used the debug command - DEBUG=* npm start to get the logs and found these logs coming up.
All the middlewares declared through middleware.json and imperatively through app.middleware() command are having this error message -  No matching layer found
Any idea what's going wrong here - Which loopback package is responsible for express routing implementation
Hello good morning community, I am somewhat confused, I am integrating the Cisco AppDynamics tool, when performing the integration as mentioned in the documentation and when running the project it throws the following error.
Event Log:
build.gradel:
build.gradel(:app)
dependencies {
}
I am trying to build a docker image and push it to AWS ECS.
I am currently integrating an API with appdynamics.
I need to issue a command to pip to start a proxy before the app opens.
When running locally I do this with  pyagent proxy start  and it works fine.
The problem comes when using that command with docker.
When I attempt to use that command during the build and push process I get the following error stack.
I run the app locally from mac and install the package to a pipfile on a mac.
I know that darwin corresponds to macOS.
Is this an issue because docker runs Linux or another issue I missing.
The markers the error traces says it is ignoring appear in the pipfile.lock.
I would like some aid in understanding what the error means and any help is appreciated!
Thank you!
I need to suppress AppDynamics alerts on every Sunday between 10ma to 3pm and remaining all the time, they should run.
To achieve this, i need to write a croj expression to satisfy the condition of "run all the time except every Sunday 10am to 3pm".
what could be the cron expression for this ?
Servers: Weblogic 12.1.1 (being upgraded soon, yes); Database: Oracle 12c
Our (very old) JSP monitoring page does  SELECT 1 FROM DUAL  to check for the database being up and a very basic check on response time.
We also have (newer) AppDynamics monitoring on our servers, which includes monitoring the monitoring page.
Lately we have been experiencing overall application slowness, and the monitoring page has been reflecting that.
In particular  SELECT 1 FROM DUAL  query which we expect to be very consistent has been intermittently slow...as in 2+ seconds just for that query as reported by AppDynamics, when a normal response time is 50ms.
Mostly our focus has been on the network because that seems the most likely, but we haven't found anything.
What conditions on the database side could cause this?
The database is run by a separate team, and they're helpful but they're responsible for a large number of applications, so we get the best effect if we can ask for specific, measurable statistics.
I have a requirement to upload zip file to appDynamics, i need to use the httpsrequest plugin for that from my jenkins pipeline
upload request for appdynamics :
we are using a shell to execute the above request now but I am trying to find out how to sent multiple zip files using  httpsRequest plugin
I am trying to assist in setting up AppDynamics with an Angular 2 app that is hosted in IIS.
The app is already up and running.
There is a part I am having trouble on, the instructions for that part say say:
1) From the root directory of your Node.js application, run this command:
    npm install appdynamics@4.3.5
   For every Node.js application you are instrumenting, insert the following call in the application source code at the first line of the main module (such as the server.js file), before any other require statements:
2) Restart you application
I did step 1 locally in the console, but I don't know what to do for step 2.
If I add that script to the page I get "The Reference error: require is not defined".
I learned that that function is not meant to run on the browser.
It's meant to be run server-side, but I do not see node js or any server.js files on our dev web server.
Does anyone have any suggestions on where to put that snippet.
Will it even work with the current setup?
I am using supervisor to manage gunicorn process.
[program:Test app]
command = /env/bin/pyagent run -c /etc/appdynamics.cfg -- /env/bin/gunicorn app:app --bind 0.0.0.0:8000 --worker-class sanic.worker.GunicornWorker
directory = /projects/app_dir/
autorestart=true
Appdynamics versions; pip freeze
Appdynamics.cfg
When i start the procees, i can see that agent is loaded properly and proxy is started as well.
But the problem is i dont see any data reported to controller when i generate load to my app.
(using wrk to generate load)
Agent and proxy  Logs does not have any info about app data/metrics.
Would really appreciate if someone can help me find out  the issue?
Thanks,
Manivasagan
I want to Share the  AppDynamic's Dashboards  in an external website under an &lt; iframe   so that the reports(statistics) can be visible without logging into the  AppDynamic  tool on an external website.
Requirements :
I tried doing this  "Sharing a Custom Dashboard"
By clicking the "copy shared URL" I got the AppDynamics' particular dashboard's URL
Whenever I run that URL on  Chrome , it gives the following  error  :
and when I try on  IE  (Internet Explorer) , it throws this  error  :
On Other tools like  Splunk  and  Sitecatalyst  there is a concept of Sharing the reports by embedded URLs.
Not sure how  AppDynamic  works
Thanks.
I have a generic question here and I have just started using Openshift enterprise and Origin but I would like to know the details on Cloudforms UI, I know that CloudForms UI can do a lot of things including managing Openshift instances but I would like to know the following in terms of managing Openshift instance, can CloudForms be able to do the following :
What I am trying to find here is to see if CloudForms can provide an end to end Openshift solution.
The end user must only have his/her code ready, rest everything could be within the UI.
Kindly let me know what all are possible and what all are not.
I need advice on the API's which can be used to do end to end monitoring on the Cloud instance.
To give the complete picture, I have a model(R/SQL/Scala) running on C3 instance and i want to do end to end monitoring from data fetch to relevant output from the model.
The end to end monitoring also needs to be displayed on a dashboard.
The instance i am using is Linux.
When i googled for this, i cam across lots of technologies like Zabbix, AppDynamics etc.
which are more sort of products.
Is there any existing API which is platform independent and can be integrated with multiple technologies like R/Scala etc?
Also,If I am choosing the existing API do i need to be dependent on the developers for implementing that in my model?
Or Shall i start from the scratch with REST/SOAP web service?
Any help in this regard will be very much appreciated.
I have a dropwizard 0.7.0 service.
Occasionally (1 in 5000 requests) the service will spend 60 seconds writing its response.
AppDynamics is showing that 60 seconds are being spent inside com.sun.jersey.spi.container.servlet.WebComponent$Writer:write:300.
After 60 seconds we get the following exception:
We also see the following in the logs that shows that the connection is idle for at least 30 seconds:
What could cause such behaviour?
A couple of other observations:
UPDATED Q : I have tried to run this sample eCommerce app on android studio 1.2.1, build 141.1903!
[enter image description here][1]..
https://github.com/Appdynamics/ECommerce-Android
and i did what is instructed to run it
but it keeps asking to upgrade gradle to gradle 2.0 or advance.
(after i downloaded the new version 2.4).
how to upgrade or integrate gradle in android studio?
what other problems related to it I have to solve?
error I'm getting
Error:
Could not find com.appdynamics:appdynamics-gradle-plugin:2.0.
Searched in the following locations:
    file //ANDRO1/gradle/m2repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom
file://ANDRO1/gradle/m2repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar
https // repo1.maven.org/maven2/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom
https // repo1.maven.org/maven2/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar
file /c/ Users/Piyush/.m2/repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.pom
file /C /Users/Piyush/.m2/repository/com/appdynamics/appdynamics-gradle-plugin/2.0/appdynamics-gradle-plugin-2.0.jar
Required by:
        ECommerce-Android-master:app:unspecified
We are doing a load test of an application, and after some time AppDynamics reports  "PS Old gen" at 100% in red .
Full GC is running every 10 minutes.
Memory's "Current Utilization" varies between  70-90% , it goes like this for hours and never fails with OOM.
I thought that once old gen utilization is above certain level, GC will try to free/compact old gen area and if nothing is freed it would either fail with OOM and start crazy full GC cycles just before it.
However I don't see any of these.
The application runs fine with 100% old gen utilization.
We are using Oracle Java 7u14 (64b, 4 cpu cores, 10gb RAM) and JVM is configured with
Thank you!
I'm currently testing platforms that provide a monitoring service for nodejs application.
So I found (for now) StrongLoop and AppDynamics (recently acquire nodetime).
Actually I'm testing StrongLoop service.
I have followed all the steps describe in the documentation but I can't see any data on the dashboard, only the StrongLoop Demo App.
Here is all the steps :
Any idea ?
Tank you.
I am looking for something to do for Azure Worker Roles what New Relic and AppDynamics can do for Azure Web Roles.
I have tried both solutions for my background workers with little success.
Am I missing something in configuration for either of these services or is there another service out there that can do what I need?
I am open to hosting my own service if there is an option I run "locally".
What I'm looking for:
Nice to have:
I am looking for an open-source application/tool/technology which can show times of distributed request across distributed system.
I have found some wonderful stuff like  AppDynamics , but they are all commercial.
I don't need such a wide functionality, but simple request tracking.
I have also had a look on  this list , but I have some difficulties to understand it.
Could you recommend some solutions if you are experienced with APM?
I have installed Liferay-Tomcat 6.0.6 on one of my Linux machine having 4GB of RAM and it uses MySQL installed on a different machine.
The liferay runs really slow even for 10 concurrent users.
I have attached the screen shot taken from the AppDynamics which shows EhCache and C3PO both are responding slow at times.
Are there any special config required for EhCache or C3PO?
?
I am currently running with default configurations.
As I instrument my React native application with appdynamics the react native application gets the runtime error
'null is not an object (evaluating
'InstrumentationConstants_1.InstrumentationConstants.BREADCRUMB_VISIBILITY_CRASHES_ONLY')'
As I integrate it remains fine with the integration but as soon as I instument the app stops running.
After integration I have used
and
on the top of the file.
Also did all the steps of manual link for android .
Is there something I am missing here?
Is there an option to create a pdf/bi report from the azure app insights data?
After running performance tests , going to individual tabs(failure/performance/availability etc) is time taking exercise.
Would like to know if there is a plug in/custom controls available like in &quot;CA-Wily&quot; or Appdynamics where you an export all monitoring data to a pdf and share with relevant stakeholders.
Our application is built on .NET 4.0 with Oracle 12c Version 1 as database.
I was assigned to find bottlenecks in the application from the database side.
I am using Appdynamics to find slowest database calls.
I have found that in most cases database queries are executed within few seconds but System.Threading.WaitHandle.WaitOneNative took over 20 seconds.
I do not understand what can be the reason for this.
We don't have a dedicated front end engineer to look at this.
Any help will be appreciated.
I have come to know about opentracing and is even working on a POC with Jaeger and Spring.
We have around 25+ micro services in production.
I have read about it but is a bit confused as how it can be really used.
I'm thinking to use it as a troubleshooting tool to identify the root cause of a failure in the application.
For this, we can search for httpStatus codes, custom tags, traceIds and application logs in JaegerUI.
Also, we can find areas of bottlenecks/slowness by monitoring the traces.
What are the other usages?
Jaeger has a request sampler and I think we should not sample every request in Prod as it may have adverse impact.
Is this true?
If yes, then why and what can be the impact on the application?
I guess it can't be really used for troubleshooting in this case as we won't have data on every request.
What sampling configuration is recommended for Prod?
Also, how a tool like Jaeger is different from APM tools and where does it fit in?
I mean you can do something similar with APM tools as well.
For e.g., one can drill through a service's transaction and jump to corresponding request to other service in AppDynamics.
Alerts can be put on slow transactions.
One can also capture request headers/body so that they can be searched upon, etc.
I have a weburl (Appdynamics) which allows SSO, I want to use this website as the data source in Power BI.
If I let the credentials be the default windows one, Power BI still doesn't move past the login page.
We generally enter the account and get the message on Appdynamics
and it directly logs us in.
How to achieve this in Power BI?
In our project, we are getting AppDynamics logs(application logs) and machine logs and sometime the the size of the logs increase which eats out the disk size.
What I am trying to do it is to get the content between two dates like 10 Nov and 13 Nov and delete the rest.
Since we are working in windows environment, this needs to be done in powershell.
It is easier to handle such things in linux but I am not good at powershell scripting.
Below is the code snippet.
Code Snippet with file paths
The ERROR i get while executing the script.
The powershell and windows version:
Name: Windows PowerShell ISE Host - Version : 5.1.14409.1018
Name: Microsoft Windows Server 2012 R2 Standard 64bit
your help will be highly obliged.
Best regards,
I'm trying to create a user using Appdynamics' Configuration API
I'm trying to use this curl command but am not sure what the parameters after --user are.
This is AppDynamics specific, a better understanding of tenancy or which my users are, or where i can create users would be helpful.
Also should I be using the pem key to communicate with my controller host.
Here's the link to the Documentation page i'm referring to;
 https://docs.appdynamics.com/display/PRO44/Configuration+API#ConfigurationAPI-CreateandModifyAppDynamicsUsers
Hi I am developing a plugin to wrap around AppDynamics.
I need to do the following from this documentation  https://docs.appdynamics.com/display/PRO45/Upload+the+dSYM+File
however since the platform files are regenerated each time, can I do this through hooks?
The  xcode_build_dsym_upload.sh  is part of the podFile and when I try with hooks (which i think is before the xCode build) I'm getting a  no file or directory found  error.
Any ideas?
I have a website that requires a custom header to access.
How do I configure JMeter to only send this custom header to the main site URL/http request sampler, and not send it to any embedded resources such as appdynamics or googleapis?
Right now, I have several "HTTP Request" samplers trying to act like a browser by using HTTP Defaults and checking the "Retrieve All Embedded Resources" box.
The request URL is in the form of " https://example.com/path/ ."
This part needs the custom header to access.
When retrieving embedded resources (like fonts.googleapis.com), the custom header should not be sent.
Any ideas on how I can get this configured?
My organization asked our team to use this new tool AppDynamics for better performance testing results and reports.
For that I have to attach javaagent with running jvm,  on their community this step
However when I run the same I get following result on cmd (Using windows-8 64 bit)
java.lang.reflect.InvocationTargetException
Caused by: java.io.IOException: no such process 
Exception in thread "main" java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
Caused by: java.lang.reflect.InvocationTargetException
Caused by: java.io.IOException: no such process
This is  the link  of their documentation.
The .NET web applications we build all integrate with a third party application through a WCF service.
Every time a page loads a number of WCF service calls are made to retrieve data that are used to populate some user controls.
Through AppDynamics I can tell that there could be up to 8 WCF calls to load a given page.
AppDynamics tells us that the WCF calls cost up to 85% of the load time.
This is a serious impact on developer productivity.
Is there a way to intercept all the outbound WCF calls from our .NET web application and stub them with fake data so that pages will not break and load faster?
The pages do not need these data to run in development environment.
Thanks for your input!
John
I am trying to obtain statistics for an app which is hosted on my Cloud Foundry Pivotal without using any 3rd party applications like "AppDynamics" (or others).
Specifically, I want to find out the  'Requests per second' and 'Response Time'.
I know that it is possible to access memory, disk space and cpu utilization by an app because Pivotal provides these statistics.
So does Pivotal also provide 'Requests per second' and 'Response Time'?
I used some good monitoring tools like javamelody and appDynamics to montior apache-tomcat 5.25 servers performance and get some useful statistics for the deployed web application (running on java 1.5 VM).
Actually I need to monitor apache-tomcat server version 4.1.24 running on java 1.4.2_12-b03 VM.
I could not find some useful tools to monitor tomcat version 4 and java 4 web applications similair to javamelody or any other +java5 monitoring tools.
Any idea abt some useful monitoring tools for apache-tomcat 4/Java 1.4?
Thanks
I am new to Istio service mesh.
I have to integrate/configure appdynamics in istio.
I have no clue how to do that.
Anything related to this would help.
Any example or related links or video...anything.
I am new to jenkins.
I am trying to moniter performance test for my project.
I have my scripts in Jmeter.
I have created paarmeterize job in jenkins as shown.
Threads: 1
RampUp: 1
Loop: 40.
I am using backend listner to check data in Grafana and Appdynamics.
Now when i start the build the scripts run only once, but i am expecting script must run for 40 times (With build success).
But when i run it through jmeter, scripts run for 40 times succesfully.
(Some issue with jenkins i suppose)
Please suggest how can i resolve the issue in jenkins as my project requeiremnt needs it to run script from jenkins.
Thank you in advance!
I'm trying to create a live connection from our AppDynamics data to PowerBI for reporting purposes.
An example of a command I would need to run to get AppD data is below.
Is it possible to run arbitrary commands like this in PowerBI to return JSON data to PowerBI?
We have a system where Chef has deployed a monitoring agent, AppDynamics, as a specific user - lets call that user sysXYZ for the sake of this post.
The AppDynamics agents create a daily log file, all with sysXYZ user ownership.
Tomcat, also being run as sysXYZ user, hosts the application that is being monitored by the AppDynamics agent.
Every day, the Tomcat instance is restarted (project has their reasons) and the start-up process includes a step for renaming yesterdays AppDynamics logs.
However, this is prevented as a permissions issue.
Tomcat running as user sysXYZ cannot amend files owned by user sysXYZ but created by something that is not Tomcat.
I get thet SELinux is meant to prevent unexpected access - say a malicious actor has been introduced - and I am fine with that concept.
What can we do here to allow the Tomcat instance to rename the files appropriately each time it is restarted?
Requirement  : To display conditional prompts, based on the previous selection ( &quot;Analytics&quot;, in this case ).
Problem  : For any selection, it is still displaying the prompt to add Google Analytics's Tracking Id even if i select AppDynamics or none.
Reference  :  Applying Subschemas Conditionally
Note : For Simplicity, i have removed &quot;allOf&quot;,&quot;anyOf&quot; etc.
and code for any other analytics tool, still it was not working.
Code  (Schema.json):
Read many articles on this but still couldn't figure it out.
Is there any way to add these conditional prompts?
 The above query from Hibernate takes only few 100 milliseconds to execute.
But the list() call which executes the above query and returns a list takes more than a minute (sometimes   2mins) to execute.
I am not sure what is causing this.I was able to track down the query timing from AppDynamics, but could not drill down into why the list() call takes more than a minute 
  The number of records returned is around 8-9k (may be more sometimes).
The table has around 800k records and the c3 column referred in the query is indexed.
I have set the query.setMaxresults value to 50k.
The number of columns in the actual query is 24.
I reduced the column in the query here for simplicity
The list() method does some processing on the returned results which is time consuming.
Did someone face this issue before?
Any help is appreciated.
Hibernate version: 2.0, JDK - 1.8, DB - Oracle 11.2.0.4
Thanks
After updating to Spring Boot (1.5.20.RELEASE) - I'm getting the following error in my latest environment (in AWS) but in local it is working fine.
ERROR StatusLogger No log4j2 configuration file found.
Using default configuration: logging only errors to the console.
pom.xml
Please suggest.
This is working fine in my local environment.
Full Agent Registration Info Resolver using node name [b196dc6a-64de-469b-b70d-78e99a12b504]
Install Directory resolved to[/opt/appdynamics]
ERROR StatusLogger No log4j2 configuration file found.
Using default configuration: logging only errors to the console.
I'm trying to upload a multi-line file to a appdynamics controller, using the ansible uri module.
Any advice?
As you'll notice in the snippets, I'm brand new to Ansible...
It works fine with shell: curl, but I can't seem to get the output correct for a more structured play.
Expected outcome - I'm able to successfully import the health rule to AppD
Actual - I get a 200 response, and nothing happens.
Response:
The AppDynamics java agent requires the JBoss Domain.xml and Host.xml files to be modified in order to run.
To make AppDynamics work, I must add a "property" element with two attributes.
&lt;property name="jboss.modules.system.pkgs" value="com.singularity"/&gt;
I have also tried to add the element and attributes int he same code block, but received a ansible error stating the element and attributes are exclusive:
The issue I am running into is this: At the same path there is a duplicate property with different attributes that must remain unchanged
&lt;property name="java.net.preferIPv4Stack" value="true"/&gt;
When using Ansible, I always seem to overwrite ALL the "property" elements.
I have tried adding the property element with the attributes on the same like  (as if it were the property name) but receive a python error:
The latest version of the Ansible code I am using is as follows:
I am at a loss on how to deal with duplicated element names with different attributes via ansible.
In the end I would need to end up with  this:
Any help would be great appreciated.
I know the answer is staring m in the face, but I just don't see it
And added point would also to ensure I don't have identical elements with identical attributes as well.
Thanks, in advance.
I am using Aurora and .NET 3.5 framework in my application which is internally using .NET mysqlconnector 6.9.3 version.
When I enable my general_log, I see below logs.
I am looking forward a way that I can disable the same or Is this how mysql connector designed.
Any thoughts around this!
I use appdynamics to monitor the queries and I see that whenever there is a procedure call, there is a INFORMATION_SCHEMA calls as well.
Please have a look at the below logs.
this link  suggested to set  innodb_stats_on_metadata  to  0 , but didn't help me.
At my new assignment, I need to understand a mid level Java application.
To understand the flow faster, I had this idea that if I could see at runtime functions are being called, which function finally responded, then I could really get the whole map in my mind.
I've worked with tools like AppDynamics which tells the latency/DB calls etc.
But what I am looking after is something which will tell me the flow at Runtime.
Like,  
 Controller.getStudent() -&gt; Service.getStudent() -&gt; Repository.getStudent() -&gt; ....
I am wondering if there are any tools/techniques as such.
Like recording a stacktrace in debug mode.
I can imagine a tool doing  Thread.currentThread().getStackTrace() .
Does anybody have some idea regarding how can do this?
(I'm using Springboot and Jboss)
Edit: I'm not really looking after logging, debugging etc.
I feel there could be something which could tell what functions are being executed.
And determine if there are any limitations.
I had to configure AppDynamics alerts in the past for Java applications I worked for.
I also heard of Nagios, but I am not very sure how that works.
Now, I need to configure alerts for a FlowForce Server, but I don't believe it can be integrated with AppDynamics or Nagios.
I saw FlowForce allow me to send some alerts, like when a step of a job fails, but I would like to have some server alerts, like, for instance, if the license expires and, as a result, the server is automatically shut down.
I am wondering the best way to achieve it.
I am running it on a Windows environment BTW.
Suggestions are welcome.
Thank you in advance!
We have instrumented a .Net 4.0 application, running in IIS8.0 on Windows 2012 with an AppDynamics APM agent (v4.5.2).
This server also has McAfee Endpoint Protection installed, v10.6.0.542, with Threat Prevention v10.6.0.672.
With the APM agent installed, CPU is much higher under typical load (~50-60% with agent vs 10% without, across 2 vCPUs).
Under heavy load, the application also starts becoming unstable (requests start queuing and timing out, response times become very high, errors begin occurring).
We have noticed that with McAfee enabled, it injects two DLLs into the w3wp process - EpMPApi.dll and EpMPThe.dll.
We checked this using Process Explorer, looking at loaded DLLs for the process.
We ran various combinations of performance test:
We attempted to add w3wp.exe as an exception in McAfee, however we saw that the DLLs were still loaded, and the high CPU and poor performance still occurred.
In memory dumps, we consistently saw the application threads waiting on critical sections used by EpMPApi.dll.
It seemed to be related to the application attempting to make socket connections (which it does frequently as all requests involves WCF calls to a downstream system).
We would like to understand if/how we can configure McAfee to either exclude w3wp.exe fully, or perhaps stop whatever activity it is doing that the APM agent seems to interact badly with.
We are also working on the APM agent side to understand if we can do anything there to prevent or work around the behaviour.
Thanks!
Having a nodejs application running inside a docker container orchestrated by Rancher.
Using appdynamics to monitor the service.
For that we need to add few configuration parameters in main js file.
One of the configuration has to be unique for all the containers running for this service.
So, i would like to pass the rancher container name as that configuration.
We are experiencing sporadic long queries execution in our application.
The database is Oracle 12.1 RDS.
I can see in AppDynamics that query was executed for 13s, I'm executing it myself in Oracle SQL Developer and it never takes longer than 0.1s.
I can't put query here as there are 3 of them that sporadically give execution time longer than 10s and for each of them I can't reproduce it in SQL Developer.
We've started to log Execution plan for long running queries using /*+ gather_plan_statistics */ and it is the same as if query executed for 0.1s except the fact that it doesn't have such a record "1 SQL Plan Directive used for this statement".
I'm looking for any ideas that could help to identify the root cause of this behavior.
I have this dictionary below
I am trying to get the key for the value  "art" .
But I am only able to search for the value  "art"  or the list it is in.
Here is my code below
I get the list for the given value
How do I get the associated key in this case  "avgresptime"  for the given list I found for the value and in turn it's given key which is  "Appdynamics" ?
Is there any better way to do it since my approach involves O(n^3) runnning time?
We have a functioning  ASP.NET MVC 5.2.2  website running on  .NET Framework 4.5.1  hosted on a web farm with numerous servers.
The web application integrates with AppyDynamics.
The  AppDynamics .NET Agent 4.3.2.1  is installed on each server.
I noticed that it's an outdated version but we are unable to update yet.
We use StructureMap for our IoC.
Sometimes something happens across the entire farm, perhaps by a:
There is a chance that one of the web applications on an affected node will not function properly.
It will start up but the IoC never executed resulting in the following error:
We can't prove it but we think that the AppDynamics Agent intercepts the IIS application just at the wrong time.
The web application starts but the IoC is not configured hence the error above.
My Questions
Is this a known issue?
I really did search online
If you have experienced this, do you have an idea of what the possible cause(s) can be?
How can we fix this annoying intermittent error for our customers?
Is it as simple as updating something?
Like StructureMap &amp; AppDynamics?
Edit
The web application is deployed to a network shared folder which feeds all the web servers in the farm.
:(
The IoC setup must be working if it is working for all the other nodes.
This problem seemed to have started since the installation of the AppDynamics Agents.
It only happens if IIS is reset across the farm.
One of the sites on that node will throw the error I mentioned above.
I am still investigating on my side
Application_Start()  from the  Global.asax .
For a crisp view of the image, click on it.
We have an issue with our PROD application.. we are seeing random latencies in the transactions(API based, no sessions).
The transactions just freezes b/w the processing, no pattern (not pausing at a section of transaction, or a time of a day), although one pattern would be - It is more occurring when we do more transactions.
We have checked storage, network, db and for other hardware related issues but couldn't find any.
One issue that i could see is - 
GC (Allocation Failure) being written to catalina.out every 2secs - Is this ok?
or is this an indication that there are lot of objects created
Here is our tomcat/JVM and server config
Xmx is set to 4096m, xss set to 256k,connector is bio, http11protocol, maxthreads=150, acceptcount=100, compression=on, compressionminsize=2048
Hardware - 32G memory, 8cpu.
cpu spikes or memory usage are all normal during 
these events.
Even GC collection time per min looks normal too.
We use appdynamics and that doesn't point to any issues as well.
we use log4j that writes the logging which points to this latency.
We use SSL/TLS as well but one way to rule this out is - these latencies are recorded b/w the transactions processing statements which happens after the SSL termination.
what kind of logging should we enable to better understand the delay?
any other recommendation?
Are they are any PROD ready tools that will show SSL performance?
(apparently APPdynamics do not show this)
Does anyone use compression for API based calls when the requests are less than 2kb?
really appreciate you guys for helping out here!
What is an "APM solution"?
I have this term from this blog post:  http://www.myloadtest.com/new-relic-vs-appdynamics/
I am looking to setup a monitoring alert in AppDynamics.
I want to generate an email alert whenever the java process in the Centos machine goes down.
How do i set this up?
I am using AppD version 4.2.
I have tried installing the process monitoring plugin, but I am still not getting any data in custom metrics.
https://www.appdynamics.com/community/exchange/extension/process-monitoring-extension/
I had created a python file which will pull the data from app dynamics for which i had imported the package from appd.request import AppDynamicsClient
it is working fine but now i want to convert it into .exe for which i am using py2exe in py2exe i am trying to include the package appdynamics but it is giving error package not found
these are the packages i am importing in main file which i am trying to convert it into .exe file
setup.py file
)
ERROR MESSAGE:
ImportError: No module named AppDynamicsClient
We have two osb nodes in cluster.
One of node osb1 has less ovearall response time ( 1 sec) when measured in appdynamics,  another  node osb2 has high response(20sec).
We brought down each of this node and tested individually.
We see same behavior.
Any suggestions on what to look into to identify the issue.?
The osb configuration across both the nodes Is identical and jvm configuration also identical.
Heap usage is same.
CPU bit differs.
So I have a code that gets value from Redis using Jedis Client.
But at a time, the Redis was at maximum connection and these exceptions were getting thrown:
When I check an AppDynamics analysis of this scenario, I saw some iteration of some calls over a long period of time (1772 seconds).
The calls are shown in the snips.
Can anyone explain what's happening here?
And why Jedis didn't stop after the Timeout setting (500ms)?
Can I prevent this from happening for long?
This is what my Bean definitions for the Jedis look like:
I am trying to develope an android ecommerce UI for demo.
I downloaded the template files from this source at github  https://github.com/Appdynamics/ECommerce-Android .
When I run the application in android studio everything builds fine but when I go to open the app on my emulator it crashes with this error message in the logcat
I think the problem is coming from this line of code by I'm not sure how to fix it
Here is the preferences.xml file
Any input would be greatly appreciated thanks.
I have a web service which makes readonly call(bunch of select queries) to get data from DB.
But it makes call to the database for the first time only, after that, all entities involved are cached in hibernate second level cache using Ehcache.
The problem is even if my request is not making any DB call to get data, the application is always making a commit call to the database which is effecting my response time of the web service.
My web service response time is 90ms, out of which the commit call is contributing 35ms(40%) all the time.
The datasource is configured as spring bean with data source class com.mchange.v2.c3p0.ComboPooledDataSource as shown below.
It is making a commit call to DB for every call to the web service which I can see it in the call graph taken from appdynamics.
The cache hit ratio is 100% for all those entries involved in the request.
I have my cakePHP application (hosted on centOS 7) monitored by appdynamics APM.
In their monitoring controller I have breakdown of transactions that take too long.
I also installed a simple chrome page timing plugin.
On one of my webpages I got the following results:
As you can see the page loaded after 157 seconds!
However in my APM the slowest Transaction recorded has 'execution time' at 2.1 seconds.
If my server serves the pages in under 2 seconds (and usually in around 0.5 second) where does this terrible 157 seconds come from?
How can I monitor the source of that load time?
Thats another example with firefox plugin for page load times:
This one took nearly 54 seconds and thats a real load time (saw it mysekf).
However firefox Firebug under Net tab shows that for that same page:
6 seconds for that same request?
Why are they so different and why is firebug incorrect?
I saw myself that the load took over 50 seconds
I downloaded Appdynamics agent for Java, which required adding jvm option for glassfish server 3.1.2, for javaagent.jar, giving path of agent.
user which application server runs on has full permissions on this folder.
After adding this jvm in glassfish server 3.1.2, a restart of server is required.
After executing restart, server could not start givng error: error opening ZIP file or JAR manifest missing C:AppServerAgent:javaagent.rar.
I noticed that the option was not added in domain.xml file, but still the option is required for starting the machine.
I tried to add it manually in the domain.xml file, but still no success.
What can I do?
now the appication hosted by glassfish doesnt start because the server is down.
Any help?
Thank you in advance.
We have installed 2 instance of same application in a same datacenter.
Both the app is using same oracle DB.
But we are observing performance issue in one application.
In AppDynamics we can see the response time of one application is much higher that other.
Is it possible to intentionally prioritise/configure the DB such a way.
If yes, where should I look into the database.
Any Idea why this is happening?
I am totally clueless here.
I have a regular expression for the HttpOnly configuration :
Header edit Set-Cookie ^(.
*)$ $1;HttpOnly;Secure
For Appdynamics EUM, i want to exclude from this regular expression everything that begin with "ADRUM" (without quotes).
How can i proceed ?
Thanks a lot for your help
Best regards !
Ludo
My system setup is like: Application A takes requests from outside world and communicates with the backend REST apis.
REST api also communicates with mysql database.
My requirement is to have a tool from which I can just monitor the resource usage and may be the performance of the web server.
I want to have graphs for the resource usage which means I need historical data otherwise I would have just used the windows task manager to see the resource usage.
This means I do not need any load generator(that will be done by the Application A) just a resource monitor.
I googled and found tools like appdynamics, Nagios, munin but not sure if they are what I need.
I haven't done performance testing earlier so there's lot of confusion.
Just looking for some guidance.
Thanks
I am new to the AppDynamics.
We want to integrate AppDynamics in our Angular application (It is intranet Single Page Application).
I saw this page but this is about AngularJS not Angular.
https://www.appdynamics.com/supported-technologies/java/angularjs-monitoring
We are using Cloud Foundry to host our application.
There is no issues at backend service.
Since it comes with property files where we added AppDynamic entries and then when we push our application it will be integrated with AppDynamics.
But where as Angular doesn't have that configuration.
So any suggestion about how to integrate AppDynamics to an Angular Application.
We are facing the below issue on the bundle which is created by auto build.
The build agent is configured with 32 bit server and our app server is with 64 bit server.
Some of our node modules such as Appdynamics are not working with two different OS configurations.
There does not seem to be a way to replace no data by zeros when using formulas in datadog.
I've tried fill zero but it doesn't seem to work
I would simply like my dd agent monitor to display 0 instead of no data when it is down
I want to filter metrics on tag value with a regex.
I can do it in Prometheus but I could not find an equivalent way in Datadog.
For example, to select the following metric whose  status  tag value starts with  2 , I can use the query  http.server.requests.count{status=~"^2..$"}
I have the same metric with the same tags in Datadog too, but couldn't find a way to have the same query.
Does anyone know how to integrate Spring boot metrics with datadog?
Datadog  is a cloud-scale monitoring service for IT.
It allows users to easily visualice their data using a lot of charts and graphs.
I have a spring boot application that is using  dropwizard  metrics to populate a lot of information about all methods I annotated with  @Timed .
On the other hand I'm deploying my application in heroku so I can't install a Datadog agent.
I want to know if there is a way to automatically integrate spring boot metric system reporting with datadog.
Like this one:
[
If yes, how do I create one?
From all documentation I've read so far, it doesn't seem to support it.
But I don't see anyone confirming that it's not supported anywhere.
I installed dd-agent on Amazon linux ec2.
If I run my python script directly on the host machine (I used the SDK named "dogstatsd-python"), all the metrics can be sent to datadog (I logged in to datadoghq.com and saw the metrics there).
the script is something like:
However, I launched a docker container and run the same script from inside the container:
'172.14.0.1' is the IP of the host, which was extracted with command
No metrics were sent to datadog at all.....
I'm guessing that maybe it's due to some configuration issue like "address binding".
Maybe the dd-agent I installed on the host can only receive metrics from 'localhost'.
Hope someone could help me.
Thank you in advance.
I have a timeseries graph in a time board that displays data for one metric that has multiple tags called "page".
The graph has one line for each tag and I'm running functions on the values, so the query for my data is "ewma_5(avg:client.load_time{env:prod}) by {page}".
This query means the tooltip values when I hover on the graph are things like "ewma_5(avg:client.load_time{env:prod})".
I want to know if there is anyway to use the alias function with the tag value in it, so something like "alias": "{page}"?
If you use a stack like ELK or datadog for collecting server-side logs and events, how do you integrate mobile-side metrics?
Is there any way to get these out of crashlytics directly, or does this log aggregation need to be implemented separately?
We are running a Kubernetes Cluster in AWS and we are collecting the metrics in DataDog using the dd-agent DaemonSet.
We have a Pod being displayed in our metrics tagged as "no_pod" and it is using a lot of resources, Memory/CPU/NetworkTx/NetworkRX.
Is there any explanation to what this pod is, how I can find it, kill it, restart it etc?
I have found the dd-agent  source code  which seems to define the "no_pod" label but I can't make much sense of why it is there, where it is coming from and how I can find it through kubectl etc.
I don't understand the difference between  events  and  metrics  in  DataDog .
I'm trying to create a count indicator in my  dashboard  so I can now how many times some type of event has happened.
There is a lot of events named  some.event.name , but no matter what query I use, it always returns  1 .
I've tried with this queries,
sum:some.event.name{*}
count_nonzero(sum:some.event.name{*})
count_not_null(sum:some.event.name{*})
I've also tried with other aggregation functions  avg|max|min|sum  and allways the result is  1 .
Any help will be highly appreaciated.
I am using  exometer  and the  exometer_report_statsd  reporter to report Phoenix endpoints response times to Datadog via dogstatsd.
From a Plug, I am calling  :exometer.update/2  to send the response time to Datadog.
E.g:
:exometer.update [:app_name, :webapp, :resp_time], 25
Now, I want to have only one metric  app_name.webapp.resp_time  instead of one metric per endpoint and version so I thought of using tags.
The question is, where should I include the tags?
I'm looking to report custom metrics from Lambda functions to Datadog.
I need things like counters, gauges, histograms.
Datadog  documentation  outlines two options for reporting metrics from AWS Lambda:
The fine print in the document above mentions that the printing method only supports counters and gauges, so that's obviously not enough for my usecase (I also need histograms).
Now, the second method - the API - only supports reporting time series points, which I'm assuming are just gauges (right?
), according to the  API documentation .
So, is there a way to report metrics to Datadog from my Lambda functions, short of setting up a statsd server in EC2 and calling out to it using dogstatsd?
Anyone have any luck getting around this?
New to datadog so I'm just really confused.
First configuration was fast and simple.
However as I want some app specific charts, it doesn't seem as clear as before for my current scenario.
We have one host with several docker machines, one for each service:
- nginx
- varnish
- apache
- database (mysql)
We've installed datadog client inside the host and also docker integration and everything works fine.
What I don't get is how get metrics from apache or varnish, or whatever service that is inside docker.
Reading the docs in varnish  for example you have to execute:
However, where should I run the command?
dd-agent user exists only in the host, not in the docker container.
Varnish is just the other way round.
Should I need to install the agent on each container?
It would be considered as another host for pricing?
In mysql case, I just have to configure the agent:
But as my host and the container are in separate routes, should I create a new docker container with the agent so it cat get to db container (changing server field)?
Is it considered again as another host?
Is there any way to monitor disk usage of docker containers in DataDog?
I can see in DataDog web all the CPU, RAM and IO metrics for my containers.
But I can't see any of disk space related metrics.
Their page  https://docs.datadoghq.com/integrations/docker/  says about:
I can't find these neither in Dashboards   Docker nor in Metrics   Explorer
I'm new to DataDog, so possibly missing something obvious here.
I am unable to access datadog agent on my host from a docker container.
I am using EC2 container service to host my docker containers.
I have already set the option  non_local_traffic : yes  in datadog config.
My config looks like this:
To access the host from the docker instance I use this URL from within the docker container :  http://169.254.169.254/latest/meta-data/local-ipv4/  which is discussed here:  http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html
This URL gives me the IP of the host machine which is then passed over to python datadog client running in the docker machine.
I've a metric which has 2 tags (it has more but this is for simplicity),  client  and  rule , and its value of course.
With it I can see the total count of the values for each client, each rule and each ruleXclient.
Now I want to create a top list which would tell me the amount of unique clients per rule, so if the metric is reporting that 2 clients (2 values of the tag client) have 4 hits each for a rule (single value for the tag rule) I'd like the top list to show me:
2: RuleA
Is that even possible?
How could I approach this if it isn't?
I'm using dogstreams to report the metric.
Is it possible to export or download Datadog dashboards via Datadog REST API?
Export and update of Datadog Monitors works fine.
I need the same functionality for dashboards.
I'm running web apps as Docker containers in Azure App Service.
I'd like to add Datadog agent to each container to, e.g., read the log files in the background and post them to Datadog log management.
This is what I have tried:
1) Installing Datadog agent as extension as described in  this post .
This option does not seem to be available for App Service apps, only on VMs.
2) Using multi-container apps as described  in this post .
However, we have not found a simple way to integrate this with  Azure DevOps release pipelines .
I guess it might be possible to create a custom deployment task wrapping Azure CLI commands?
3) Including Datadog agent into our Dockerfiles by following how Datadog Dockerfiles  are built .
The process seems quite complicated and add lots of extra dependencies to our Dockerfile.
We'd also not like to inherit our Dockerfiles from Datadog Dockerfile with  FROM datadog/agent .
I'd assume this must be a pretty standard problem for Azure+Datadog users.
Any ideas what's the cleanest option?
I have a Spring Boot application and want to configure HTTP request tracing via dependency management without having to deal with setting up the java agent.
Can anyone suggest the best way to do this?
I have the  micrometer-registry-datadog  dependency added to my pom and can see that there are a lot of undocumented  com.datadoghq  dependencies, but am unsure if any of these will solve my problem.
I'm getting all of the JVM metrics, but want some more APM-type metrics now.
Ideally I'd like to use the  @Timed  annotation and various others to get detailed metrics around API calls.
We have an elasticsearch cluster deployed to the Elastic Cloud and would like to send monitoring/health metrics to Datadog.
What is the best way to do that?
It seems like our options are:
* Installing the datadog agent binary via the plugins upload
* Using metric beat -  logstash -  datadog_metrics output
So I have an ongoing metric of events.
They are either tagged as success or fail.
So I have 3 numbers; failed, completed, total.
This is easily illustrated (in Datadog) using a stacked bar graph like so:
So the dark part are the failures.
And by looking at the y scale and the dashed red line for scale, this easily tells a human if the rate is a problem and significant.
Which to mean means that I have a failure rate in excess of 60%, over at least some time (10 minutes?)
and that there are enough events in this period to consider the rate exceptional.
So I am looking for some sort of formula that starts with: failures divided by total (giving me a score between 0 and 1) and then multiplies this somehow again with the total and some thresholds that I decide means that the total is high enough for me to get an automated alert.
For extra credit, here is the actual Datadog metric that I am trying to get to work:
(sum:event{status:fail}.rollup(sum, 300) / sum:event{}.rollup(sum,
  300))
And I am watching for 15 minutes and alert of score above 0.75.
But I am not sure about sum, count, avg, rollup or count.
And ofc this alert will send me mail during the night when the total events goes low enough to were a high failure rate isn't proof of any problem.
I am trying to create an alert in DataDog that would alert us when disk performance slows down our machines.
As a business requirement I would say that if the IO is almost saturated (over 90%) for more than 30 minutes, the alert should be triggered.
Here are the current set of metrics that are recorded:
 
sys.cpu.iowait
system.io.avg_q_sz
system.io.avg_rq_sz
system.io.await
system.io.r_await
system.io.r_s
system.io.rkb_s
system.io.rrqm_s
system.io.svctm
system.io.util
system.io.w_await
system.io.w_s
system.io.wkb_s
system.io.wrqm_s
It is possible to use any formulas to combine these, including SUM and AVG values.
I have a time series presenting time values like this one:
I want to change the y-axis to represent hours instead of milliseconds, i.e.
divide by 3600.
Any idea how to do it?
I'm running a number of python apps as Replica Sets inside of kubernetes on Google Container Engine (gke).
Along side them I've created the Datadog DaemonSet which launches a dd-agent on each node in my cluster.
Now I would like to use that agents dogstatsd for metrics logging from python apps as well as try out the new Datadog APM.
If I just install the ddtrace python package and use it like documented it fills up my logs with
Clearly it don't have magical way to guess how to access port 8126/7777 of the ddagent pods.
Ive tried creating a Service which expose the ports:
but my python pods still don't seem to be able access for example  os.environ['DATADOG_STATSD_PORT_8126_TCP_ADDR']  and  .._PORT .
They are defined and all, I just still get the connection timed out.
If I connect to the dd-agent pods and enable tcpdump I also don't see any trafic on ports 8126 etc.
The dd-agent DaemonSet is defined like this:
I'm trying to integrate a Datadog monitor check on sshd process in my terraform codebase, but I'm getting  datadog_monitor.host_is_up2: error updating monitor: API error 400 Bad Request: {"errors":["The value provided for parameter 'query' is invalid"]}
What I did was to copy the monitor's query I created on the Datadog panel and pasted it into the tf file:
ofc the query example  "avg(last_1h):avg:aws.ec2.cpu{environment:foo,host:foo} by {host} &gt; 2"  works
What's the right way to check via Datadog API or terraform if a specific service, like sshd, is up or not?
I'm trying to use the datadog api but the initialize method keeps giving the error 'INFO No agent or invalid configuration file found'.
The datadog agent is running:
(PYTHON) daphnepaparis@Daphnes-MBP-2 ~ $ /usr/local/bin/datadog-agent status
Datadog Agent (supervisor) is running all child processes
And the configuration file permissions look alright:
(PYTHON) daphnepaparis@Daphnes-MBP-2 ~ $ ls -l ~/.datadog-agent/datadog.conf
lrwxr-xr-x  1 daphnepaparis  staff  35 Mar 22 12:58 /Users/daphnepaparis/.datadog-agent/datadog.conf -  /opt/datadog-agent/etc/datadog.conf
Original commands I'm running:
In [1]: from datadog import initialize, api
In [2]: options = {'api_key': '***'}
In [3]: initialize(**options)
2017-03-22 13:24:20 INFO No agent or invalid configuration file found
Anyone able to help?
I was able to follow these instructions carefully and thoroughly  https://docs.datadoghq.com/tracing/setup/python/ ,
I successfully installed DataDog Agent following this guide  https://docs.datadoghq.com/tracing/setup/ ,
I was also able to install MacOS tracer since it is required for mac user:  https://github.com/DataDog/datadog-trace-agent#run-on-osx ,
I enabled apm_config in the configuration file found here:  https://docs.datadoghq.com/agent/faq/agent-configuration-files/?tab=agentv6#agent-main-configuration-file
I leave the  env: none  since I only need to run it in on development/debug mode.
Now Im currently on the step 4:  Instrument your application  guide for Flask and here the steps I took:
Add integration for flask:
And also my application runs in a docker container and this is what I get from the output log:
ERROR:ddtrace.writer:cannot send services to localhost:8126
Additional Information
On the tracer agent:
According to the  gae_datadog  Github repo , the way to setup datadog in app engine is to clone the repo and add the following in  app.yaml :
However, this doesn't appear to work with their nodejs runtime.
Here is my  app.yaml :
It seems like the datadog url handler isn't used at all, because it 404.
I assume the node.js app takes precedence here, but I don't know how to change that.
I have a service that exposes metrics in statsd format and telegraf instance which picks those metrics and sends them to both Prometheus and Datadog (there are two output plugin configurations for both of these).
This works correctly.
However, I have a special requirement where I would need to filter certain metrics that will be sent to Datadog.
My first inclination was to make change in  [[outputs.datadog]]  section of  telegraf.conf .
However, I don't see any specific configuration part where I could, for example, list just metrics that I need to be seen on Datadog.
Is there any way to achieve this?
Thanks.
I have been working with Datadog log ingestion for about a year now.
It's been (mostly) great to work with.
The documentation around running it inside of Kubernetes is a bit lacking though.
Their documentation covers Docker thoroughly, but Kubernetes less so.
When I installed Datadog into our Kubernetes clusters a year ago, there were two ways to do it, you could use a DaemonSet to ensure at least 1 Pod of Datadog runs on every Node.
Or you could install it as a Deployment.
I went with the DaemonSet option and used Helm to install it.
That worked quite well!
Then we wanted to start using DogStatsD to ingest metrics about our applications, and it seemed at the time like this required the "cluster-agent" to run.
I have serious doubts about this part.
If I get all of the Datadog-related objects in my cluster I see the DaemonSet ( daemonset.apps/dd-agent-datadog ) and I also see a Deployment ( daemonset..apps/dd-agent-datadog ) on my cluster.
Is this right?
Do I really need to run both of those things to get log ingestion and metrics?
I  think  this should be possible, but I can't find documented syntax.
I would like to construct a DD graph that displays metrics matching tag_one:A or tag_one:B.
Is this possible?
If so, what is the syntax?
I want to exclude a path to avoid getting my logs spammed like so:
I'm running datadog as a docker agent using the command here:
 https://docs.datadoghq.com/agent/docker/?tab=standard#installation
how do I specify files to exclude in the docker run command?
is it an environment variable?
I am using Datadog to monitor my browser console logs.
I need different tags in for datadog logs.
The only option I fount is to add attributes to my logger using,
DD_LOGS.addContext('referrer', document.referrer);
Is there any way for the frontend client application to have tags in datadog?
Or is the attribute and tags are same in Datadog
There are a bunch of custom metrics that looks like:
There is a template variables dropdown ( https://docs.datadoghq.com/dashboards/template_variables/ ) on the dashboard that contains all keys.
(Name of the variable is  $key  and values key1, key2..)
The metrics query like this works fine:
But is it possible to parametrize the query with the  $key  variable by concatenation with the part of the name of metric?
So it would look like this:
As I know, it is possible to something like that by using tags but unfortunately, there are no any metrics with the tag &quot;key&quot;.
We are using DataDog and NewRelic to monitor the performance of few DevOps supported systems and we need to provide some uptime reports like:
While we do have URL monitoring configured on DataDog we were not able to find a way to compute the uptime (only to get an alert when the service is down).
NewRelic is also used but it seems that they have an URL monitoring service which works only on publicly accessible sites, making it useless for 9/10 cases.
I wanted to add my custom log parser through dogstream, but there was an exception while restarting datadog agent:
The parser code:
Does anybody know why such thing happend?
Any ideas?
We are trying to integrate DataDog with our Ruby On Rails app.
Our ROR app will continuously add users, update users and delete users every second.
I have integrated Datadog to monitor the no.
of users added, updated and deleted through the graph provided by Datadog.
I installed the datadog agent using the command for Ubuntu Aws instance.
I got a free trial for 14 days.
I followed this document for  dogstatd-ruby gem  :  https://github.com/DataDog/dogstatsd-ruby
After that i wrote the code in my ruby project like below :
Here i dont see "custom.users.updated" and "custom.users.added" graph in the metrics explorer.
I would really appreciate if any1 help me out to set the graph for these 2 metrics in Datadog account.
please let me know if i missed anything here.
Is there any default dashboard to monitor Cassandra performance in data dog?
https://app.datadoghq.com/account/settings#integrations/cassandra
There are lot of metrics listed.
How do we construct a monitor?
By default the data dog shows the default system level monitor like CPU, Heap etc... is there anything like it for Cassandra?
Any info would be a great help for me.
I'm creating my dashboard, and I have the following two metrics:  event.sent  and  event.failed .
Fortunately, I still haven't had any failed events (knock on wood), so this metric does not exist yet on datadog.
But I want to create it so I can add it to my monitors.
How do I manually create this metric?
I've been trying to understand the time aggregation for Datadog monitoring alerts.
The official doc  http://docs.datadoghq.com/guides/monitors/#define-the-conditions 
I understand the idea of time aggregation, but I'm confused about the unit of time as it's not mentioned anywhere.
Is it aggregating over 1 minute intervals?
To rephrase this when I use  sum(last_30m){X}  is it summing the values of  X  for each minute?
What about  sum(last_1h){X} ?
Is it still each minute?
I am new to Datadog APM.
I have read few tutorials but I am unable to find how to to add data in Datadog to create custom dashboard?
I'm looking for a solution to monitor GCP Dataflow pipelines with Datadog to extract the built in metrics as well as Beam custom metrics.
Currently Datadog offers integration for other GCP services, but not for Dataflow.
Has anyone done similar work and can share pointers how to build this as custom solution?
I am new to Datadog and I am trying to implement mute/unmute functions from Datadog on my AWS cloud stack.
I want to do that using AWS Lambda Functions.
I am looking for a java based solution.
Is there any Java based sdk provided for the same?
I found out that Datadog provides APIs to schedule downtime  here
but the support I can see is either Python, Ruby or Curl.
How can I construct a Java based solution for it?
My application is running in the docker container and it is not able to communicate with dd-trace agent running on host which is ec2
I've done all the configurations and still facing  ERROR:ddtrace.writer:cannot send spans to localhost:8126: [Errno 111] Connection refused
Any idea how to fix this?
i'm collect data using Go and want to visualize it, i chose Datadog, but didn't find examples or live projects where Go used for sending metrics to Datadog.
But in offical site says that Go is supported.
I wanted to ask if anyone has ever saved jmeter test results (sampler names, duration, pass/fail) to Datadog?
Kinda like the backend listener for influx/graphite... but for Datadog.
Jmeter-plugins has no such plugin.
Datadog seems to offer something called "JMX integration" but I'm not sure whether that is what I need.
I am trying to set up slack monitors with datadog, based on the environment.
For e.g.
if the environment is production got to slack channel A and if it is uat go to slack channel B and all other environments should go to slack channel C.
But I can't find a way to do the last part where all others should go to slack channel B.
Looked at the documentation in  https://docs.datadoghq.com/monitors/notifications  and googled but couldn't find anything that can do an else condition.
I am utilizing dogstatsd approach to send metrics to datadog using micrometer.
I get the normal metrics like counter and gauge but I am not able to generate events.
Is there a way to generate datadog events?
I'm trying to send events to my local datadog agent by shell through DataStatsD port.
The message is sent without errors but doesn't reach the dashboard.
I use datadog agent in version 6.9 and use datadog documentation:
https://docs.datadoghq.com/developers/dogstatsd/datagram_shell/#send-metrics-and-events-using-dogstatsd-and-the-shell
When I try to send metrics is work fine and I see the metrics in the datadog dashboard but when I send events it's doesn't show in the dashboard.
I also see that when I send event via shell and then check agent status the number of metrics packets go up but the number of events is still 0.
That's the command i run:
Edit  When I changed the following configuration properties it's work.
The configuration I changed:
         1) dogstatsd_non_local_traffic: yes
         2) bind_host: localhost
We re trying to eliminate Datadog agents from our infrastructure.
I am trying to find a solution to forward the containers standard output logs to be visualised on datadog but without the agents and without changing the dockerfiles because there are hundreds of them.
I was thinking about trying to centralize the logs with rsyslog but I dont know if its a good idea.
Any suggestions ?
I am using datadog for monitoring my services on AWS.
On my python application, I use below code to send a data to a metric on Cloudwatch:
I can see this data on Cloudwatch -  Metric.
But I don't know how I can create a monitor on Datadog to listen on this metric.
OK, I spent quiet some time figuring out how to configure stuff to have DataDog trace ID in logs but couldn't get it working.
To be clear what I'm looking for is to see trace IDs in logs message, the same way that adding  spring-cloud-starter-sleuth  to the classpath, automatically configure Slf4j/Logback to show trace IDs in log messages.
Where I've started:
What I did so far:
Notes:
Can someone tell me how exactly I need to configure the application to see trace IDs in log messages?
Is there any documentation or samples I can look at?
Is there any way to extract the Tags info from DataDog via API for a specific metric?
I need the same info that the  Metrics Explorer  displays (list of hosts and tags), for only one metric.
I can retrieve the Tags filtered with a regular expression pattern:
And I can get all the Hosts from a Metric with:
Using either the  datadogpy  or the  DataDog API ,  how can I get all the Tags from a Metric?
Thanks
I have a unique type of Kubernetes cluster that cannot install the  Kubernetes Datadog agent .
I would like to collect the logs of individual docker containers in my Kubernetes pods similar to how the  Docker agent  works.
I am currently collecting docker logs from Kubernetes and then using a script with the  Datadog custom log forwarder  to upload them to Datadog.
I was curious if there is a better way to achieve this serverless collection of docker logs from Kubernetes clusters in datadog?
The ideal situation I want is to plug my kubeconfig somewhere and then let Datadog take care of the rest without deploying anything onto my Kubernetes cluster.
Is there an option for that outside of creating a custom script?
Is it possible to extract json fields that are nested inside a log?
Sample I've been work on:
what I wanted to achieve was:
I tried to combine a sample regex ( "(extract)"\s*:\s*"([^"]+)",? )
with  example_parser %{data::json}  (using the JSON as a log sample data, for starters) but I haven't managed to get anything working.
Thanks in advance!
Using datadog official docs, I am able to print the K8s  stdout/stderr  logs in DataDog UI, my motive is to print the app logs which are generated by spring boot application at a certain location in my pod.
Configurations done in cluster :
Configurations done in App :
After doing above configurations I am able to log  stdout/stderr  logs where as I wanted to log application logs in datadog UI
If someone has done this please let me know what am I missing here.
If required, I can share the configurations as well.
Thanks in advance
Question about searching logs in Datadog.
Search works on regular strings in the CONTENT portion of the log.
However, if JSON is passed to the CONTENT portion, the JSON elements are automatically parsed into Attributes.
But the Attributes are NOT searchable.
How do I search for logs by Attribute?
It seems like a step backwards to supply log data in JSON to improve indexing, but then LOSE the ability to search on those elements.
my configuration is:
Where:
I'm not getting any errors, but I  am  seeing the subclasses of  ApplicationWorker  in the DataDog dashboard for the APM traces but  not  the  ApplicationJob  subclasses.
From the documentation I've found this is the correct way to configure  activeJob  tracing when it uses  resque , but I haven't found such great documentation on the subject.
I'm trying to figure out the difference between the  in-application modifier   as_rate()  and the  rollup function   per_second() .
I want a table with two columns: the left column shows the total number of events submitted to a  Distribution  (in query-speak:  count:METRIC{*} by {tag} ), and the right column shows the average rate of events per second.
The table visualization applies a sum rollup on left column, and an average rollup on the right column, so that the left column should equal the right column multiplied by the total number of seconds in the selected time period.
From reading the docs I expected either of these queries to work for the right column:
count:DISTRIBUTION_METRIC{*} by {tag}.as_rate()
per_second(count:DISTRIBUTION_METRIC{*} by {tag})
But, it turns out that these two queries are not the same.
as_rate()  is the only one that finds the expected average rate where  left = right * num_seconds .
In fact, the  per_second()  rollup does this extra weird thing where metrics with lower total events have higher average rates.
Is someone able to clarify why these two functions are not synonymous and what  per_second()  does differently?
I have a Postgres dyno on Heroku and I use Datadog.
Two postgres dashboards are by default on Datadog: Metrics and Overview.
Metrics is working (CPU usage, memory, I/O,...) but Overview is not (deadlocks, indexes usages)
Are Heroku Postgres dyno and Datadog fully compatible?
I'm trying to send my ECS Fargate logs to Datadog.
To do this I need to pass my Datadog API_KEY as a field in the  logConfiguration  object.
I need to secure my API_KEY so I am using AWS Secrets Manager via the  secretOptions  key of the  logConfiguration  object.
I'm following the steps from AWS laid out  here .
The full steps from the Datadog site can be found  here
For some reason I dont see the logs show up in datadog.
Here is the log config section of my Terraform code under the  container_definitions  object of the  aws_ecs_task_definition  resource:
If I take out the  secretOptions  and add the apikey in plaintext, the logs show up on the datadog console:
I of course cant just send my API_KEY in plaintext.
Does the  secretOptions  just not work for Datadog?
Any help is appreciated.
I have  datadog agent  running on ubuntu 14.04.
And I am trying to monitor page views for the go apps as mentioned in the  link .
I have checked everything yaml also it is valid.
But still it doesn't even report for upto go_expvar data even after 30 min.
I checked in the dashboard it says Last check 29 mins ago.
Can anyone tell me how to debug this or reduce this time
I have to set resource limits for my kubernetes apps, and they use the "milicore" unity "m".
When analyzing my apps in Datadog, I see a unity called M% for CPU usage.
How do I convert 1.5M% to m?
Kubernetes resources:  http://kubernetes.io/docs/user-guide/compute-resources/
Im replacing our existing  NewRelic  java support code with  DataDog  and am wondering about sending error messages.
NewRelic has the  .noticeEvent()  call.
The  DDog library  Im using has a  .recordEvent()  but doesn't seem to have a way to send a stack trace.
Anyone been down this road before?
I can send text via the above but I need a bit more info.
I am trying to create a dashboard in datadog using the REST API described here:  http://docs.datadoghq.com/api/#timeboards
Whatever I do, however, I keep getting a 400 response back with a message "Invalid JSON input".
I have simplified my json to just a few required fields, and empty "graphs" section, and that still doesn't work.
Does anyone have an idea what could be wrong here?
curl -i -X POST 'https://app.datadoghq.com/api/v1/dash?api_key=&lt;key&gt;&amp;application_key=&lt;the_key&gt;' -d '{"dash":{"title":"Foo","description":"bar","graphs":[]}}'
Response
i am trying to write a php curl for datadog api,but it return internal error.
this was working in bash script but throwing error while converting in phpcurl.
can someone help me on this.
I am incrementing a Datadog counter in python:
And have set the metric type to "count" and the unit to "requests per none" in the metadata for the metric.
The code runs in a docker container on a kubernetes node in a Container Engine in Google Cloud...
I have docker-dd-agent ( https://github.com/DataDog/docker-dd-agent ) running on each node.
I can move the container to any node and it logs around 200 requests per minute.
But as soon as I scale it up and launch a second container, it only logs around 100 requests per minute.
If I scale down to one container again, it spikes to 200 rpm again:
What could be causing the requests to drop or get overwritten from other pods?
I'm looking for a way to do a "partial" update on an existing screenboard/timeboard.
By "partial", I mean adding some widget to the existing screenboard/timeboard without wiping out the existing widget that already exist.
Consider the following example:
Create screenboard:
Update screenboard:
When I run the create and then the update example, the  update_image  widget will overwrite the  create_image  widget and that is the problem I'm trying to avoid.
I have a  AWS Lambda  function which filters AWS log events from Cloud-trail and give only my AWS ROLE's events.
Can I send this records only to Data-dog?
Is there an API in which I can pass this filtered events directly?
Where does the  userstats.o1.daus  metric take the data from?
I looked in the metrics list and in the app, but I don't seem to find the source of the metric.
The application infrastructure relies on:
I have an application that publishes a metric to DataDog with multiple tags, and my DataDog agent has a line that looks like
So my metric (lets call it  ResponseTime ) has a metric in the DataDog viewer for each of those (i.e.
ResponseTime.90perentile ).
However if you look at this metric carefully it appears to be calculating these percentiles on a short range (not sure what) and for each tuple of the tags that exist.
Ideally what I'd like to get is a 95th percentile of the  ResponseTime  metric over all the tags (maybe I filter it down by 1 or 2 and have a couple of different graphs) but over the last week or so.
Is there an easy way to do this?
I have configured DD agent on AWS Ubuntu machine and defined CPU Usage, RAM monitors, and metric is correctly reflecting in the dashboard.
Inside  /etc/dd-agent/conf.d  in file  process.yaml :
On the same machine, I have a JAR running as a process with name  ecommerce-order-0.0.1-SNAPSHOT.jar  as a process.
When I do:
I get:
But when I do:
I get:
I want a process monitor who can check if a  JAR  with some name is currently running or not.
What is it I am doing wrong?
I am looking at some alternatives for APM and I like the extensive list of Datadog integration points.
However, it seems that I would have to make code changes to explicitly send stats to Datadog.
Doesn't Datadog support runtime instrumentation?
My tech stack is MS .NET/C# and SQL Server backend.
Thanks!
we use Jenkins 2.60.1 , with Datadog plugin 0.6.1
after first installation all works well , but after some time we stop get events in Datadog.
restart Jenkins solve the issue.
any idea ?
I am new to Datadog and NGiNX.
I noticed when I was creating a monitor for some integrations several of the integrations were labeled as misconfigured.
My guess is someone clicked the install button but did finish the remaining integration steps.
I started to work with NGiNX and quickly hit a roadblock.
I verified it is running http status module
The NGiNX install is under a different directory than is usual
and the configuration file is under
I created the status.conf file there.
When I reload the NGINX I get a failure.
I don't understand what it means or how to proceed from here.
There is a logs directory with nothing in it.
ps -ef|grep nginx
I think the issue is that our install doesn't seem to be following the same defaults as the instructions and I'm pretty sure I'm not doing this correctly.
If anyone has any insights that would be great!
Chris
Is it possible to configure datadog to notify about each new error that got logged?
I know how to set a threshold for a specified period and how to send the error-count for instance to slack.
But I am searching for a possibility to send the actual error rather than the number of errors.
I am currently using this  link  for writing a program in Python that will send out curl commands for  POST  ,  PUT , and  DELETE  requests using the Datadog API.
So far, the request seems to be firing as I'd like it to, but it won't take my credentials.
I'm not entirely sure what a service hook url is, but I believe it may be the culprit.
Could anyone tell me how to find the following Slack specific elements for this?
This is my test script in Python:
The results were:
I would really appreciate any help in finding this information!
Using datadog docker image, with the following in docker-compos
I am getting the following errors continuously
2018-07-14 16:10:04 UTC | ERROR | (runner.go:277 in work) | Error running check disk: [{"message": "[Errno 2] No such file or directory: '/host/proc/filesystems'", "traceback": "Traceback (most recent call last):\n  File \"/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/checks/base.py\", line 294, in run\n    self.check(copy.deepcopy(self.instances[0]))\n  File \"/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/disk/disk.py\", line 43, in check\n    self.collect_metrics_psutil()\n  File \"/opt/datadog-agent/embedded/lib/python2.7/site-packages/datadog_checks/disk/disk.py\", line 90, in collect_metrics_psutil\n    for part in psutil.disk_partitions(all=True):\n  File \"/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/ init .py\", line 1839, in disk_partitions\n    return _psplatform.disk_partitions(all)\n  File \"/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/_pslinux.py\", line 1000, in disk_partitions\n    with open_text(\"%s/filesystems\" % get_procfs_path()) as f:\n  File \"/opt/datadog-agent/embedded/lib/python2.7/site-packages/psutil/_pslinux.py\", line 194, in open_text\n    return open(fname, \"rt\", **kwargs)\nIOError: [Errno 2] No such file or directory: '/host/proc/filesystems'\n"}]
and another
2018-07-14 16:10:04 UTC | WARN | (cgroup.go:510 in
  parseCgroupMountPoints) | No mountPoints were detected, current cgroup
  root is: /host/sys/fs/cgroup/
Any ideas what it means or how to debug it?
Expecting to get logs into datadog from other containers sysout so I have all logs in one place.
I can see it successfully detects the other containers
Note the docker image is using version 6 of datadog
Thanks
I set up datadog and kubernetes to test to out monitoring, although in datadog i can see some logs and metrics, in the agent in kubernetes I have the following errors:
As the logs state the agent cannot connect to Kubectl, has anyone come across this?
I am trying to add new monitor to datadog.
I added the metric to my code.
And I can see this metric on datadog (goto Metrics -  explorer -  Graph).
Now I am trying to create monitoring on datadog that will alert me if the value of metric don't change for three days in a row.
Is it possible to create this kind of monitoring?
Thanks.
I have a microservice based project with  kafka , which I used for event bus.
I have a business process, which contains multiple microservices.
Microservices asynchronous communicate with each other with help kafka.
Each instance of business process has unique  process_id .
Let's consider a example of some process:
So, I need to measure execution time bwtween differ steps of this process.
For example, I want to know, duration between steps  3  and  5 , or  2  and  6 .
So, I don't know, how to measure it.
I have only one workaround solution.
I can have share memory (e.g., redis), where I can store points of time for each stem of process.
In the end of process, I can calculate all metrics and push it datadog.
I am using ansible version 2.7 for kubernetes deployment.
For sending logs to datadog on kubernetes one of the way is to configure annotations like below,
this works fine and I could see logs in DataDog.
However I would like to achieve above configuration via ansible deployment on kubernetes for which I have used below code
and datadog.json.j2 looks like below
However the resulting config on deployment is below
and this config does not allow datadog agent to parse logs failing with below error
if I use ansible code as below (using replace)
it generates deployment config as below
Which also fails,
to configure the working config with ansible, I have to either remove leading pipe (|) or three quotes coming when using replace).
I would like to have jinja variables substitution in place so that I could configure deployment with desired source and service at deployment time.
kindly suggest
I have Golang app, it writes logs to Stdout with Logrus.
I was trying to recreate this  https://github.com/DataDog/docker-compose-example  scenario, and replace python app with my app.
But logs aren't coming to Datadog dashboad
This is docker-compose I'm trying to make work
I also tried non-compose, but simple docker container installation for the agent by this  https://docs.datadoghq.com/logs/log_collection/docker/?tab=containerinstallation  instructions.
I run my golang app container with
with Dockerfile
and DD agent can see app container ups and downs but receive no logs
I installed datadog agent locally on my windows 10  machine.
By default it stored data in ProgramData folder in C drive.
It does not give option to select different drive while installation.
Now when I run any command, it gives me below error.
Can we edit permissions to it allow access to ProgramData folder.
I have one Datadig dashboard to monitor a particular service.
To use the same dashboard for other services, I added a couple of template variables to change the queries in the dashboard.
But, I could not use these variables in the query section of Datadog SLO widget.
The following query with variables works in the other type of widgets, but not in the SLO widget.
Is there a way to use variables in SLO widget too, or not possible because it is in beta version or something?
I'm trying to record my website sales $ amount in datadog.
However I'm getting way more than the actual value.
I'm using java-dogstatsd client and spring.
My application is running on 3 hosts.
I recorded all metrics (using sendWebOrder method) but no luck.
I'm trying to generate a datadog toplist by transactiontype.
I'm not getting the correct amount in any of the metrics (tried mainly count, gauge and histogram.sum).
Here is my datadog config:
What am I missing?
Is this the correct way to record money value?
Do I've to do any rollup in config?
Any help is appreciated.
Is anyone aware if there is any telegraf output plugin to submit metric to datadog agent?
I can see a  datadog output plugin  which calls datadog metric api but not anything to submit data to datadog agent.
I am very new to the monitoring of microservices using prometheus and datadog.
I am trying to monitor the rate of event callback requests per second using PromQL queries and datadog queries but when i compared the values from both the tools, they came out to be very different.
I want to know if the two values need to be same.
If yes, then how do i write my queries to obtain the correct values
PromQL query
Datadog query(json file)
Please tell me how can i make the two queries equivalent?
I have been using mongo-driver in my project, deploying with gcloud app deploy for a while now.
I recently rebuild my machine, and simply ran  go get  to get fetch ally my dependencies.
Everything is compiling fine locally, however,  gcloud app deploy  fails:
Any ideas?
app.yaml is just  runtime: go113
We have multiple applications sending logs to  Datadog  via  syslog .
Every team has created  facets  /  measures  for their respective applications under a particular group.
Is there a direct way to explore the list of  facets  /  measures  under a specific group?
I am trying to create a document for our support team to include the list of  facets  and  measures .
I am able to view them in the Log Search page, but, cannot copy.
I am looking something as export the list of  facets / measures  to excel / csv.
Note: I am restricted to use datadog api.
I am trying to query the datadog server for some specific metrics ie.
"max mem used" over some period x and I'm doing the following:
I would have expect a single timestamped value to be returned, however I get datapoint for every minute like so:
 [
          1581084600000,
          1339840512
        ],
        [
          1581084660000,
          1339883520
        ],
        [
          1581084720000,
          1339740160
        ]
Is there a way to get a specific result, i.e.
the maximum out of all these results?
Thanks.
In Quarkus, the default logging library is JBoss and using the  quarkus-logging-json  allows you to encode your logs as JSON.
However, Datadog integration requires custom fields such as  service ,  dd.span_id  and  dd.trace_id  to have the logs associated with the correct syntax.
Currently, I've tried adding this in  application.properties :
However, this seems not to show up in Datadog as expected.
When we use log4j2, we simply configure it like so.
Again, cannot find any documentation how to accomplish the same result in Quarkus configs.
Does anyone know how I could inject these custom properties into the JSON logs with Quarkus or how to integrate it with Datadog properly?
How can I automate configuring Log Archives on GCP?
I can do it manually by following steps
 https://docs.datadoghq.com/logs/archives/?tab=googlecloudstorage
I guess selenium can help this 
but I looking for a more programmatic way like Terraform or REST API
Thank you.
I am working on to create some custom metrics for my spring boot 2 rest api.
I have added the required micro meter and datadog dependency.
My office machine works behind a proxy.
I have setup proxy through spring boot plugin.
below are in my application.properties file.
management.metrics.export.datadog.apiKey=mykey
But I am getting the socket connection timeout.
As far as I debugged the io.micrometer.core.ipc.http.HttpUrlConnectionSender.send method is failing and I dont under how the micro meter data dog takes the proxy details.
The micrometer doc says
But I dont understand what it means?
should I replace this url with my proxy url or is there any specific uri pattern with the proxy?
I am using spring boot 2.2.4.RELEASE
Splunk has  transaction  command which can produce  duration  between logs grouped by id:
as it is decribed on
How to calculate duration between events in Datadog?
I'm setting up a monitor that looks like this:
I've also got a widget that looks like this:
I'd like to define two different messages, one of which will be sent when the &quot;warning&quot; threshold is crossed, and the other which will be sent when the &quot;critical&quot; threshold is crossed.
How can I do this?
Is this correct?
I have an index created on the log and the paths have special character :
 for example:
Sample URL:
grok parser:
when I try to add facet for  @params.rs:orgId
I am getting error as
An error occurred while saving the facet: The Facet path must contain
only letters, digits, or the characters - _ .
@ $
Is there a DataDog metric to report the space used or remaining in a GCP PersistentVolume.
I have found disk use metrics for the container itself, but not for a PersistentVolume.
I am working in GoogleCloudPlatform.
Using:
I am deploying Datadog as a DaemonSet and with the cluster-agent enabled to a Kubernetes cluster using the instructions provided  here .
I'm configuring Datadog using the  values.yaml  file as specified.
I want to do some custom metrics, specifically using the integration formerly known as  postgres.yaml .
I have tried to do this as specified in the  values.yaml  template found  here , like this (putting it in the cluster-agent, since these are cluster-wide metrics):
As per the documentation, I can confirm that using the  |-  prefix this indeed creates a file in the path  /etc/datadog-agent/conf.d/postgres.yaml  on the node, where I would expect it to.
The file correctly has all the contents in the block, i.e.
starting with  init_config:...
Now, when starting the node I see this in the logs (DEBUG):
'/conf.d/postgres.yaml' -&gt; '/etc/datadog-agent/conf.d/postgres.yaml'
/conf.d/..2020_10_22_10_22_27.239825358 -&gt;
/etc/datadog-agent/conf.d/..2020_10_22_10_22_27.239825358
'/conf.d/..2020_10_22_10_22_27.239825358/postgres.yaml' -&gt;
'/etc/datadog-agent/conf.d/..2020_10_22_10_22_27.239825358/postgres.yaml'
2020-10-22 10:22:29 UTC | CLUSTER | DEBUG |
(pkg/autodiscovery/providers/file.go:196 in collectEntry) | Found
valid configuration in file: /etc/datadog-agent/conf.d/postgres.yaml
2020-10-22 10:22:29 UTC | CLUSTER | DEBUG |
(pkg/collector/scheduler.go:154 in getChecks) | Unable to load a check
from instance of config 'postgres': Core Check Loader:  Check postgres
not found in Catalog
2020-10-22 10:22:29 UTC | CLUSTER | ERROR |
(pkg/collector/scheduler.go:201 in GetChecksFromConfigs) |  Unable to
load the check: unable to load any check from config 'postgres'
The documentation  here  states, that the postgres yaml-contents in agents v7.x should actually be in  /etc/datadog-agent/conf.d/postgres.d/conf.yaml  and not in  /etc/datadog-agent/conf.d/postgres.yaml .
It is not possible to create a subfolder / use forward slashes in the config key (internally, the file is created using ConfigMap).
I'm not even sure if the problem is the yaml-file path or if a core integration is missing.
So my broad quest is: how do I enable Datadog postgres-integration correctly in my setup?
I have been asked to implement a centralized monitoring and logging system using DataDog that will receive information from various services and applications, some running as Windows Services on virtual machines and some running inside a Kubernetes cluster.
In order to implement the logging aspect so that DataDog can correctly ingest the logs, I'm using Serilog to do the logging.
My plan is currently to write the logs to the console in json format and have the DataDog agent installed on each server or k8s node capture and ship them to DataDog.
This works, at least for the k8s node where I've implemented it so far.
(I'm trying to avoid using the custom Serilog sink for DataDog as that's discouraged in the DataDog documentation).
My problem is that I cannot get logs ingested correctly on the DataDog side.
DataDog expects the json to contain a property call Message but Serilog names this property RenderedMessage (if I use JsonFormatter(renderMessage: true)) or @m (if I use RenderedCompactJsonFormatter()).
How can I get my logs shipped to DataDog and ingested correctly on the DataDog end?
My current organization is migrating to DataDog for Application Performance Monitoring.
I am deploying a Python Flask web application using docker to Azure Container Registry.
After the deployment to Azure the app should be listed/available on Datadog portal.
Please note I just started learning Docker containers.
There is a high chance I could do completely wrong.
Please bear with me
Steps followed
Option 1: Create a docker container on local machine and push to ACR
Added  dd-trace  python library to the docker image
Added dd-trace run command the docker file
build the image
run the container on local
Getting OSError: [Errno 99] Cannot assign requested address
Option 2: Forward logs to Azure Blob Storage but a heavy process
Option 3: using Serilog but, my organization does not want to use third party logging framework, we have our own logging framework
Any help is highly appreciated, I am looking for a solution using  Option 1 .
I went through the Microsoft articles, Datadog documentation but, no luck.
I setup app registrations, Manage reader permissions on Subscription, created ClientID and app secrets on Azure portal.
none of them helped
Could you confirm whether is there a way to collect the APM logs on datadog with out installing agent on Azure.
Thank you in advance.
I am little embarrased, as this is probably very banal, but it's really not making any sense to me.
I have a little golang web-application, that I for now am just localhosting.
I have some logging with logrus going on in the project, and want to use datadog to get a nice and visual dashboard for my application.
So I follow a bunch of the steps on their page, download their dockerimage and run it, and i am able to see some data!
I then follow the instructions further on, under a headline called &quot;docker integration&quot;.
here I am asked to run the following command:
To which the response &quot;cannot find user &quot;dd-agent&quot;&quot; is given.
But what is this &quot;user&quot; value?
Is it the user to which I have signed up to datadog with?
something else?
I find it very unspecified?
would very much appreciate a helping hand
I want to get Confluent cloud metrics into Datadog so I followed the this  instruction .
Instead of using CCLOUD_USER: ${CCLOUD_USER} and CCLOUD_PASSWORD: ${CCLOUD_PASSWORD} I used CCLOUD_API_KEY and CCLOUD_API_SECRET as environment variables for the exporter container.
I get a Failed to establish a new connection: [Errno 111] Connection refused error:
When I tried to curl http://ccloudexporter_ccloud_exporter_1:2112/metrics I got no reply but I did with a curl to http://localhost:2112/metrics.
So I adjusted the openmetrics.yml to use prometheus url http://localhost:2112/metrics.
Still same error in the DD container.
When I go to http://localhost:2112/metrics in my browser I see metrics.
No clue on why DD cannot connect to /metrics.
Background:
I'm trying SLO feature from micrometer and I expect that I can get the number of requests that fulfill the SLO.
For example if I set the SLO to 500ms, then I want to know how many requests &lt;= 500ms.
Also I want to know the total requests.
Problem:
http.server.requests.count says 24
http.server.requests.histogram with tag le:_inf says 126
I believe both of them should have the same (or at least similar) value
I'm using:
Spring Boot 2.3.2.RELEASE
Micrometer: 1.5.2
application.properties
Meter Filter
Datadog
Any clue on what's happening here?
Thanks
I was reading the  Datadog docs  on how to monitor AWS Elastic Map Reduce using Datadog because I need to get metrics for failed EMR steps.
LIKE HERE
I think the most accurate metric is  aws.elasticmapreduce.jobs_failed  but as the image says, is only available for Hadoop V1, but I'm using Hadoop V2... so I don't see it in my  Datadog Metric Explorer
I want to trace a request path that has started in the Web application React JS (frontend), then passed to the backend and returned as a response.
Can  dd-trace-js  start the span and pass it to the server over HTTP HEADERS?
Hikaricp , Tomcat and jdbc metrics are not being exported to DataDog
we have setup springboot app to push the metrics to datadoghq, it does export 60 metrics, however the metrics like hikaricp, tomcat and jdbc are missing.
hikaricp, tomcat and jdbc - these mertics are listed under  /actuator/metrics  endpoint, but not exported to datadog.
Is there any additional settings required to push hikaricp, tomcat and jdbc metrics ?
How can logback be configured to add tags,so that datadog can recognize the source?
I have the following  logback.xml :
Where the custom field  ddtags  is supposed to set tags for datadog.
The logs show up in datadog and everything works as expected, despite the  source -tag.
The log messages sent from my service show up with two tags in datadog:  source:java  and  source:undefined :
How do I get rid of the  source:undefined  tag so that datadog correctly recognizes the source?
I'm working with express + graphql environment.
I want to add tags to express span with the value I derive while resolving the graphql query.
Currently, tags get added to the graphql span with the following code.
Let me know if there is a way to add these tags to the parent span instead of the current span.
This is required as I don't want to enable analytics on graphql since I already have it enabled in the express app.
Basically I want tags to root trace(express) instead of current span(graphql.execute).
Anyway, how can I search for all messages(errors) where stacktrace contains specific piece of code?
According to datadog documentation it search only by message attribute(it's infered from the json-like object sent to datadog when you log something).
Stacktrace is a separate property and I cannot understand how to search it.
My current situation is that I have two different data feeds (Feed A &amp; Feed B) and I have created custom metrics for both feeds:
Next steps is to create alert monitoring for the agreed upon threshold of difference between the two metrics.
Say we have agreed that it is acceptable for Order Counts from Feed A to be within ~5% of Order Counts from Feed B.
How can I go about creating that threshold and comparison between the two metrics that I have already developed in Datadog?
I would like to send alerts to myself when the % difference between the two data feeds is &gt; 5 % for a daily validation.
I have an application running on Kubernetes and this app has log files that I want to stream to datadog log, then set up an alert.
Previously, this app run on bare-metal server, I installed datadog agent on that server, and I used custom log collection to retrieve that logs.
It worked perfectly well.
Now, I have an obstacle on how to read the log files in the container.
I have googled and it said I can use annotations and auto discovery, but I can't see where I am supposed to define the log path.
Does anyone have an idea how to resolve this or have a similar case with mine?
Thank you in advance.
On DataDog log search, I want to search for logs with empty string for a specific facet, e.g.
logs with userId is empty.
@userId:''  ,  @userId:"" ,  -@userId:*  non worked.
I have a metric which has a tag with lots of different values (the value is a file name).
How can I create a query that determines the number of different values of that tag exist on a metric?
For example if 4 metrics are received during a time frame, with the following tags "file_name:dir/file1", "file_name:dir/file2", "file_name:dir/file3", "file_name:dir/file1"
I want the query to return the value 3, since of all the metrics received during this timeframe there were 3 distinct values for the file_name tag.
This is the installation path of datadog  /etc/datadog-agent 
Under this we have checks under folder  /etc/datadog-agent/conf.d
Under this we have defined a service to report disk space alert under disk.d
 /etc/datadog-agent/conf.d/disk.d
We have the file ready in the configuration.
We did tried to reload the datadog agent to reflect the changes.
The expected scenario is it should reflect in datadog console under the service defined 
The query we are using is
But we are unable to establish anything.
Nutshell none of the alerts configured for this host are not reflecting in datadog console.
Our applications log in JSON format.
According to Datadog's documentation JSON logs are not processed by pipelines.
How can I enrich the JSON logs with an additional field that is based on a different value of that same log line?
I have this line:
And I want this line:
Is this possible with Datadog?
I do not want to change our loggers to a the  customerId  to the log output.
I cannot find any article that describes the advantages of using datadog histogram compared to datadog distribution for apps that run on multi instance.
Would someone kindly help me on deciding the best choice between those two?
I want to be able to parameterised my datadog dashboard.
I have already introduced a template variable  flavor  which to indicate if it is  dev  or  prod  environment.
What I wish to achieve is to switch data from one environment o another when I select a different environment (e.g.
from  dev-db-master  to  prod-db-master ).
The string interpolation is necessary because I want to display multiple time series within a single chart.
However the chart is basically blank
The Json tab also shows a pink background which indicates either the json is malformed or the query is too complex.
My goal is to be able to, by changing the template variable  flavor ,
I can change a group of time series from, says,  'dev-db-master', 'dev-db1-master' and 'dev-db2-master'  to  'prod-db-master', 'prod-db1-master' and 'prod-db2-master' .
Can you suggest a way to construct a string with a template variable?
I have a message like 'Service is running' that i'm not able to change, so in log Grok Parser I want to replace it to 'INFO | Service is running' or manually or somehow manually assign like  %{level=INFO}  .
Please kindly advice.
I have created a event monitor( for example
events('sources:rds event_source:db-instance').by('dbinstanceidentifier').rollup('count').last('1d') &gt;= 1 )
but it returns "NO DATA" when there are no any events.
How to make it return 0 when there are no any events?
Is it possible to migrate existing data (Like in Prometheus and elk )to datadog?
There is a setup for live streaming of Prometheus metrics to Datadog by configuring datadog config.
But what could be done with the past data?
I have a question about the configuration setting for datadog for postgres 9.6.
(1) How do I get all databases monitored in datadog?
(2) How do I get all table level metrics from each database/schema?
Here is conf file.
Datadog documents are not really helpful.
Instead of listing all the dbs, I want all databases, so if we add a new db, we don't have to change the conf file and same goes for table_name.
According to datadog docs, the table level metrics are collected using pg_stat_user_tables, pg_statio_user_tables etc.
And these postgres tables are database specific unlike pg_stat_activity or pg_stat_statements.
I have following function:
I use it as following:
In logs I see spans as following:
Agent works fine but on datadog server I don't see the trace.
What is wrong?
I am reading this  tutorial :
I would like to set  traceId  be equal  requestId .
How can I do it using  ddtrace  api?
I have a situation where I'm trying to count the number of files loaded into the system I am monitoring.
I'm sending a "load time" metric to Datadog each time a file is loaded, and I need to send an alert whenever an expected file does not appear.
To do this, I was going to count up the number of "load time" metrics sent to Datadog in a 24 hour period, then use anomaly detection to see whether it was less than the normal number expected.
However, I'm having some trouble finding a way to consistently pull out this count for use in the alert.
I can't use the count_nonzero function, as some of my files are empty and have a load time of 0.
I do know about .as_count() and count:metric{tags}, but I haven't found a way to include an evaluation interval with either of these.
I've tried using .rollup(count, time) to count up the metrics sent, but this call seems to return variable results based on the rollup interval.
For instance, if I compare intervals of 2000 and 4000 seconds, I would expect each 4000 second interval to count up about the sum of two 2000 second intervals over the same time period.
This does not seem to be what happens at all - the counts for the smaller intervals seem to add up to much more than the count for the larger one.
Additionally some rollup intervals display decimal numbers as counts, which does not make any sense to me if this function is doing what I thought it was.
Does anyone have any thoughts on how to accomplish this?
I'd really appreciate any new ideas.
I'm trying to make a dashboard to monitor a process which runs on 5 remote machines simultaneously.
I want the dashboard to display the metrics for each machine separately - basically, I want to create five separate graphs, one for each machine that runs the process.
My problem is that the remote machines are reassigned periodically, so I have no way of knowing the name of the host at any given time.
I've tried creating five separate graphs, with each one filtered by a different host name tag, but the graphs do not seem to pick up the new host when the lease for the process is changed.
I also know you can split out one graph for each host using metrics explorer, but I haven't found any way to automatically do that on a dashboard.
Does anyone know if this is possible?
Leases for the process are assigned through AWS, if that is helpful.
Thanks in advance for any suggestions.
I'd like to be able to send logs to datadog and have the message be a JSON object rather than a string.
The metadata fields aren't searchable unless a facet is created, which I would like to avoid doing.
I'm currently using  winston  +  winston-datadog-logs-transporter  to send the logs.
If I do:  logger.info(JSON.stringify(message)) , datadog records the message as blank and adds the stringified message as metadata.
If I do:  logger.info('foo' + JSON.stringify(message) , then the message is interpreted as a string and I can search on it.
If I do:  logger.info('foo', message) , the body is set to  foo  and  message  is interpreted as metadata, which I cannot search for without creating a facet.
Any help is appreciated, thanks!
I am not able to see traces for my application under APM --  Service in Datadog.
I found some sample code from Datadog docs but don't know exactly where it should go inside my application.
Please let me know if anyone has any idea regarding it.
I have already tried with following code in my js file.
My application is based on node js which is serverless.
I have also added dependencies for dd-trace in package.json as  "dd-trace": "^0.11.0"
I expected to list my application with proper name in APM Services in Datadog.
I have a use case where I want to report all HTTP 500 events when they occur as integer counts, but also send a default value of 0 if no 500 event occurred during a request.
How can I achieve this with the Datadog adapter?
As a first pass, I attempted to create a rule that has  match: true , and then a metric that, for value, sets it to read  conditional(response.code.startsWith("5"), "1", "0") .
I then told the rule to use the Datadog adaptor, and registered this metric with said adaptor in the rule.
This threw errors in the mixer logs, likely because request.code is an integer and startsWith is probably a function that expects a string - we lost all metrics as a consequence.
How can I create a threshold alert by comparing aggregate of 2 metrics ?
For example, if m1=[2,3,1,5] and m2=[6,7], I want to create an alert when sum(m1)   sum(m2).
sum method here I assume will add all the data points returned by a query.
From what I have observed, datadog flow of alert creating is like, define the metric  -  set alert condition on metric.
That is, it looks like the alert condition will be some condition on metric data type only.
But I am looking for something like storing the aggregate of metrics in a variables and comparing those variables.
How can it be done in datadog ?
I have configured datadog agent on Amazon ECS, Fargate.
I can send all the intended metrics but I cannot send "tags".
I've set Environment variables in ECS task definitions.
I think most of the settings are all right because I can see the metrics which I want to see.
But tags, especially env:stg is missing in datadog UI and because of this weired error, some metrics is missing.
Does anyone know the reason of this error and the way to solve this?
Thanks.
I am attempting to use Datadog to monitor my application via JMX...
I have successfully deployed my app in a docker container, and exposed the JMX port and confirmed I can indeed attach to the port from anywhere and get information.
So I am attempting to set up the datadog docker image to use JMX and connect to the server...
I have it all configured, but at runtime the datadog image attempts to start utilizing JMX, but fails saying it can't find Java on its image...
I log into the image and sure enough it has no java installed.
From the datadog documentation:
Well that's all nice and well, but if I attempt to expose my host machine java to the image via a volume mount, it doesn't work, as the host machine is Apple and if the image attempts to run the java binary it throws an invalid format for the binary file.. not surprising since its a MACOS binary not a Debian Linux Binary (which the datadog image is)....
So, I have been attempting to take the datadog image and build a new image with it as the base with Java... but I have been completely unsuccessful, every attempt to install java during docker build fails..
I have tried every example of how to install java into a debian docker image, but none work... Every one dies with apt-get line returned a non zero
How the heck do I get JAVA installed on a debian image?
Or better yet, how do I get the datadog image with JMX to run properly?
I am having a hard time to collect logs from an python app deployed in ECS using DataDog Agent.
I have a dockerized Flask app deployed in ECS.
The app spits logs to stdout.
I now need to monitor them in DataDog.
I've added a new DataDog agent container (Fargate compatible, since I am using Fargate), which runs as part of the same task as the app.
I can see the CPU and memory metrics for both container in app.datadoghq.com/containers, so that means that DataDog Agent is working.
I now need the app logs.
I went through the documentation in  https://app.datadoghq.com/logs/onboarding/container , added
to the app container and the following env.vars to the DataDog container :
But that seems to be insufficient.
Am I going in the right direction ?
What am I missing ?
I wanted to create a graph in Datadog to display iddle connections per user.
Following this example:  http://www.miketheman.net/tag/postgres/  I changed my postgres.yaml configuration to:
I can see the metric is appearing in Datadog, but I can just see one of the rows that should appear (has I have several databases in my PostgreSQL).
Here is the datadog connection graph 
Am I missing any step?
Is postgres.yaml missing any configuration?
Running that query in psql, I get this (modifying names but not data):
From the datadog guide, want to integrate aws:
https://docs.datadoghq.com/integrations/amazon_web_services/
Created a new policy named  DatadogAWSIntegrationPolicy :
However, when clicked  Review policy  button, it said:
The syntax was followed the  datadog  service:
https://help.datadoghq.com/hc/en-us/articles/360002042531-Error-Datadog-is-not-authorized-to-peform-sts-AssumeRole
What did I do?
via command line on my debian 8.x host.
Expected behaviour:
Actual behaviour:
Problem:
I can not start the service due to Result state: start-limit.
I waited about 9 hours.
My idea was the service state will recover from to many restart attempts.
It did not.
Any ideas?
According to the  DataDog Docker Integration Docs :
There are two ways to run the [DataDog] Agent: directly on each host, or within a docker-dd-agent container.
We recommend the latter.
Why is a Docker-based agent installation preferred over just installing the DataDog agent directly as a service on the box that's running the Docker containers?
I have my application integrated with Datadog for monitoring purpose.
At the same time I want the notifications/calls to be sent to the team if any of the metric fails to achieve the desires value.
I used Webhooks integration in Datadog for this purpose.
In the webhooks configuration I have set the URL (Twilio request) and I do get a call on my number.
Now I am looking for an scenario wherein if the user doesn't pick the call for say 30secs then try calling the second number.
How do I achieve this?
I have two AWS account , I was able to set AWS integration for the first account using Terraform, but when I try to create AWS integration for my second account I am having an error.
I have created a role with in-line policy and we do not have a cross account set up.
Trust Relationship:
Can anyone please guide me how to solve this error?
I have set the threshold value to get the alert in data dog for infrastructure.
Alert is coming on  data dog UI but how to get this all alert data through API call either using JAVA or python.
I need only alert data.
I am trying to integrate datadog to elasticsearch but the datadog collector shows an error .
i am not able to troubleshoot this.
pls help
My elastic.yaml
I want to use dataDog to see how many times a java method has been called, this is my example code.
enter image description here
In this code, I simply want to count how many times that 'multiply' has been called, and I used DogStatsD to record the this metrics.
However, when I go to the infrastructure of my dataDog, I can not find a metrics with a name similar like "multiply".
What the name of the metrics should be if I set it up correctly?
Can anyone help me with how can I get the metrics of the 'multiply'?
Thanks!
enter image description here
Is it possible to change JMX attribute name in JMXFetch so that a different name shows up in DataDog?
I currently have the following:
This would report two metrics in DataDog:
Is it possible to rename it in the yaml script to something more like:
without the original names ever showing up in DataDog?
I'm new to datadog.
I followed this  post , and replaced in my app/api keys.
I have :  nginx_dd.py
When I run it  python nginx_dd.py , I kept getting
ImportError: No module named datadog
Any hints / suggestions on this will be a huge helps !
I have a C# service installed on a machine that I publish stats to a DataDogAgent (which I later monitor).
I uses StatsD as the library to publish the stats, the DataDogAgent is installed locally on the machine.
When I first set this up it worked great and no issues.
When I released new code to the box and re-installed the service I stopped getting updated stats to DataDog.
When I look in the DataDogAgent in the collection log I see
I have verified that my code that submits via StatsD has not changed and is correctly sending out the stats I want.
I also repaired, uninstalled and re-installed and restarted the DataDogAgent all with no success.
The timing is suspect to when I released code but I don't see what I would have changed that would cause this.
Everything is set to go to port 8125 for StatsD
I am trying to create a change  monitor using terraform .
To create a monitor that checks that overtime a count stays at 0 for example every day (the value will go up to one some times and get back to 0).
I found on the UI the capacity to create a  change alert .
I cant seem to find a way to define the configuration for this type.
Is terraform just supporting only a subset of the monitors?
or does the query need to be change in some specific way that I cant find documentation for?.
I have a script that queries our CI (Buildkite)'s API once per minute to fetch details of all build agents and emit metrics to Datadog for analysis.
Getting an accurate  count  of these agents in the Datadog UI has proven challenging, however.
If the script emits a COUNT metric for each agent it sees, then agents will be double-counted in the Datadog UI when the interval is longer than a minute, because the script runs once per minute and sees (mostly) the same agents each time.
The script could total up the number of agents it sees each run and emit that as a GAUGE, but then I lose the ability to break down the count in the Datadog UI by agent-specific tags (queue, etc).
I suppose I could emit a GAUGE with a value of 1 for each agent on each run, and add an artificial  index  tag with a value of the numeric index in the agent array, and rely on the Datadog UI to do the summation across  index  values?
I could use the agent ID/host, of course, but Datadog charges by number of tag values and we've got our agents in an auto-scaling group, so hosts change frequently.
This seems hacky - is there a better solution?
Am I overthinking this?
I want to setup datadog alert threshold different for each tag value passed, how can I do this?
One possibility is I create separate monitor for each tag value and use IN query for a perticular tag value.
it will be bit hectic to manage those numbers of monitors.
DataDog is so useless in its querying and its intuitiveness ...
I'm looking for a custom exception in the stack trace.
I found individual log entries in the last 18 hours that contain my exception class name, but attempting to write a log query that will find me all the occurrences is returning nothing.
E.g.
:
environment:prod @thrown.extendedStackTrace:UserDoesNotExistException
I'd like to include more words in the query, but even reducing down a single word fails to find anything.
I've looked at their documentation, which is zero help.
Do Datadog has the ability to exclude some of the websites from profiling?
ie, Suppose client has 10 websites hosted in IIS but he needs only 3 websites to be profiled.Is it possible to do so?
Any help is highly appreciated :)
I am currently working on setting up a monitor to monitor slow queries in the Cloud SQL DB.
I built a custom query to get the processes running on the SQL server, because currently slow query monitoring doesn't report until the process is completed.
To get a check every 15-20 seconds (or whatever is configured in DD) of currently running queries over 5 minutes I have this in my DD agent's config.
And my results in DD are:
As you can see it shows the count of queries that have been running for over 5 minutes.
How would I be able to get more information about each query.
For example I would like to see the exact query statement that is being executed.
I know I can use the query:
 SELECT INFO as QUERY FROM INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query'; 
to get the Query statement, but Id like to be able to click on the graph in DD and dig further into each process to see the statements.
Is there a way to add this information or another feature in Datadog where I can  the processes being queried to each process individually?
My first thought was to change the custom query to this:
But this only returns one row with the count number of all process and the ID and Query of the first result.
I was wondering to know if there is a way to exclude a site from Datadog automatic tracing on IIS.
I've read the docs but didn't find anything about.
Currently, I see error status for all the authentication errors and it feels like a lot of extra noise in the total errors chart.
I looked at  https://github.com/DataDog/dd-trace-js/pull/909  and tried to use the custom execute provided for graphql
But still, res with only 403 error is going into error status.
Please help me with how can I achieve this.
I'm setting up a timeseries to monitor system problems with the standard levels: Critical, Error, Warn, etc..
I want to set the colors as follows:
I can't seem to do this.
There is a color selection drop-down, but options aren't colors... they're themes, like &quot;Classic,&quot; &quot;Cool,&quot; &quot;Warm,&quot; etc.
How do I set the color on a line in a time series?
Actually, we have two datadog accounts: Let me consider it has account A and account B.
when I push the message to data dog event using API I am able to see the events in events stream and I am able to see the same thing in logs also in the account A.
But when I do the same thing in account B I am able to see the data in event stream but not in logs.
can I know what might be the reason ???
Am I missing something to enable ??
if so can someone help me with this?
BELOW CODE IS USED TO PUSH THE EVENT TO DATA EVENTS STREAM.
I query for a particular URL string like  https://docs.python.org/3/reference/datamodel.html  in my logs.
The URL contains lots of special reserved characters.
Escaping each char as stated by  official doc  works, but requires too much hustle:
https\:\/\/docs.python.org\/3\/reference\/datamodel.html
Is there any easier way to escape all special chars in the string?
As the title suggests  @Trace  annotation is not working with Kotlin Grpc Coroutines.
Is there a way to make it work?
Unfortunately this gives no error or warning.
Can I construct Trace programatically and would it work for a  suspend  function?
I want to create a monitor in datadog that fires an alert if the current value is larger than the maximum value of last month.
something like this:
The purpose of this monitor is to check if the monthly increment is larger than 15%.
anyone know if this is possible?
I am using statsD ( hot-shots ) to try log events in datadog such as when a new item is created and by who but, when I am calling  statsD.event('title', 'description')  I do not see any events in datadog metrics.
My statsD client is setup like this:
and then I call the event method like so:
In the metrics section of datadog, I do not appear to be seeing any events come through.
The only way I can see some logs there is using  increment
But increment only seems to tell me how many times that action is called and I am unable to log additional data such as username and alias.
I am pretty sure my statsD client and datadog configs are setup correctly as I am able to see data from increment so I suspect it is to do with the way I am trying to use the event method.
Am I using the event method incorrectly?
Perhaps I am checking in the wrong place in datadog?
Perhaps I should be using increment?
How can I log events along with associated data in datadog using statsD?
In Datadog AP, service map view, we can see a lines connecting services.
Is there a way to filter the traces going from one service to the other?
I've tried to &quot;Inspect&quot; one service and then click &quot;View related Traces&quot; on the other service but that returns all traces for the other service.
From everything I have read, I should be able to do this:
It looks like it built correctly:
But when I check in the container to see if it installed in  C:\Program Files\Datadog , I am not seeing any of the files I am expecting from the installation.
I added the flag to give the install some extra logs (/L*V C:\install.log) but didn't see much in there.
I have confirmed the msiexec command works on a Windows host, just not in the Docker build from what I can tell.
Is there something simple that I'm missing?
Using Nlog to log from my asp.net Core application, I would like to review the logs in Datadog.
Datadog allows me to visualize the log data, and slice, search, select and sort logs in a convenient way to provide support to my customers.
I was looking for a way to use NLog to directly post to the Datadog API, so I do not need to use the Windows Agent to collect the logs.
Below how to do this, as I could not find the answer anywhere.
I need to develop a Datadog dashboard which will monitor metrics, logs of the applications running in AWS EC2.
At the same time i have some need to send some messages to Application from Datadog Dashboard.
Is it possible to do that?
If it is not what are the alternative i can use to achieve this.
I'm having trouble setting up monitor that will alert me when an event hasn't happened since some period of time following another event.
Basically, for a given task in my application, I have a log that indicates a state of &quot;running&quot; and another log that indicates a state of &quot;finished&quot;.
From these logs, I've defined two custom metrics in datadog.
I'm trying to set up a monitor that will alert me when a task has not finished within 2 hours of when it started running.
So for example, if the running metric is observed at 2:00, the monitor shouldn't alert for the absence of the finished until 4:00.
If the finished metric is observed before 4:00, the monitor will not alert for this task.
The way that I've tried implementing this is by using a threshold monitor, and subtracting the count of my running metric from the count of my finished metric.
However, what's challenging here is the time-delta piece.
I've tried using the delay evaluation (delaying by 2 hrs), however, at the time when it starts evaluating, it will only take into account the first metric.
It basically, just slides the window back.
I want to take some action on the app server based on the server utilisation.
The monitoring on the server is done by datadog.
So is it possible to take an action on the server using datadog ?
I try to setup a postgres check using DD Agent and i'm getting an error thrown by postgres.py script.
As you can see in the screenshot, i'm using this simple query to  get the number of active connections to a db.
I've put it inside the /etc/datadog-agent/conf.d/postgres.d/conf.yaml like this :
The error i get when i run a config check is the following :
If i understood correctly the conf.yaml file is used to call the postgres.py script with certain parameters.
The postgres.py script can be found here :
 https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/postgres.py
I have a problem with reading a DataDog chart.
In DataDog latency breakdown chart I see that one method call took 24.3s.
In that method I have DataDog scope logging (operation1, operation2 and operation3) - sum of these three operations is about 1.61% of total time while whole method took about 97.7%.
Also SQL operations listed below have a low impact on that method.
Where does that difference come from?
We want to integrate our Kafka Server with our remote Datadog server.
Due some policies, we decide to use tunnel instead Datadog Agent.
We have set JMX port for each instance (3 Zk, 3 Brokers and 12 Kafka Connect Workers) with same format service background like this:
Each instance has its own port.
We found that When we try to curl   curl localhost:19999\metrics  to test the JMX it returns empty which indicate we miss something to collect the JMX report
However, from  Datadog tutorial to integrate kafka with DD , they use jmxfetch which the instalation need DDAgent.
We want to know if there's another alternatives to integrate Kafka Server into Datadog without Agent and rely on tunneling.
I'm trying to test out creating a monitor for google pub sub and am getting an &quot;Invalid Query&quot; error.
This is the query text when i view source of another working monitor, so i'm confused as to why this isn't working.
Error:   Error: error creating monitor: 400 Bad Request: {&quot;errors&quot;:[&quot;The value provided for parameter 'query' is invalid&quot;]}
Terraform:
I am parsing my log statement like below
I want to extract all values of &quot;ADD&quot;,&quot;UPDATE&quot; under &quot;XE&quot;; Add (sum) them up ; and convert it into a metric.
Also the depth of ADD within XE can vary with logs and the number of add, update statements can be zero or more.
I was successful in parsing and displaying values under JSON tree, but since the child depth and a number of occurrences vary, I am not able to achieve what I want.
Any help here would be highly appreciated.
When a message is formatted as json, it is automatically turned into attributes.
It seems like attributes cant be queried without first being turned into facets (which only applies to new log lines, and means you sometimes have to see something show up, then facetize it, then debug it).
Is there a way to query the message directly, bypassing the attribute facet requirement?
i'm using datadog-agent Agent 7.21.1.
Currently i'm working on gathering SNMP data from a Nimble storage device.
I already converted the mib file into Python format and i'm able to retrieve metrics using SNMP GET.
Inside /etc/datadog-agent/snmp.d/conf.yaml i've setup the following (Some values are censored):
DataDog retrieves the all the metrics until the last one &quot;volIoReads&quot;.
Using a cli tool i can read out the values using the OID:
Using the OID inside snmp conf it doesn't work either:
Tried also:
But no luck.
Does anybody know what's going on?
We have quite a lot of those endpoints and am trying to set my expectations before i start the POC.
Were the steps in the documentation enough to integrate the DDagent to the websphere startup ?
Is there a way of getting Jaeger traces to Datadog, whether it be through a proxy, scraping traces from Jager and converting them to DD Traces, etc...
We have a vendor provided backend that only supports Jaeger, but the enterprise APM solution is Datadog.
Thanks!
I'm trying to integrate DataDog with my NodeJS/Express application, however it appears that when a POST request is sent to my app the body of the POST is not being passed along to datadog, how can I fix this?
I have a file called  Winston.js  which looks like so:
And then I'm attaching them to my app using the following:
Currently my logs in DataDog look like this:
I created an event monitor that catches events with errors and notifies about the alert in a special messenger.
Everything worked out for me, but I noticed that such alerts are auto-recovered on their own for some time.
As I understand it is because of this parameter:
So, datadog catches event, then sets event-monitor in alert status, then wait 5min-48hours and if there are no new events, it is auto-recovered and set status from "Alert" to "OK".
It absolutely does not suit me.
Can I somehow configure the monitor that the monitor's status does not change automatically from "Alert" to "OK" until I change it manually?
I want to set a threshold alert for my datadog metric monitor trigger it after 12 hours.
However none of the options are 12 hours.
I cant seem to add it in.
I am using  serverless-plugin-datadog , which uses  datadog-lambda-layer  under the hood.
The  docs state , that by using this plugin it is not necessary to wrap a handler anymore.
This is, by the way, the main reason why I decided to go for it.
The lambda itself is a REST API, which responds with dedicated status codes.
My question now is, how can I monitor the number of  4xx  and  5xx  http status codes?
Do I have to define custom metrics in datadog for this to work?
I was under the assumption that the plugin comes with those data out-of-the-box, but it looks like I'm missing an important part here.
In my understand, usual case is using Datadog agent to send error to Datadog.
However, I'd like to know there are some ways to send error to Datagog without Datadog agent.
For example, can we send by using Datadog webhooks?
My organization is setting up dashboards for our backend services and after performance testing that we ran, we have noticed that some API calls report http status  N\A .
It is not very helpful, anyone seen something like that?
Is that a configuration issue?
we are facing a problem using NUnit, Serilog, and Datadog.
We are working with:
All packages are NuGet lastest.
This is the Serilog configuration we are using:
We are testing this configuration both in debug run (F5 key in Visual Studio) and under the NUnit test environment (in Visual Studio).
The problem we are facing is that while in debug run all work fine:
when we run this code in the NUnit environment:
but  no logs arrive at Datadog .
Checking the network stream with Fiddler we notice that while in debug run, logs are sent to Datadog, under the NUnit environment  logs are NOT sent to Datadog .
Any ideas or suggestions?
Thank you
I need to monitor fifty application.
As apart of which I need to perform healthcheck on datadog dashboard to all the application everyday.
So, Is it possible to collect the metrics collected in datadog from Java code
..
Thanks in advance.
I want to filter logs that either don't have a facet, say half of my logs have some  @facet  but I want the other half
I tried  -@facet ,  @facet:""  and  NOT @facet   but doesn't work and google doesn't help
Feels like there is an easy way for doing this, halp
I'm currently running a spring boot application as container into a kubernetes cluster.
Datadog agent is running as containers on the cluster.
I have modified the container image build to include the datadog agent before running the application:
I also setup the environment variable to indicate the HOST IP of the agent to my container via the Deployment file.
The problem now is i'm getting this class not found exception when the application starts:
Quite straigtforward, i need to include some dependencies into the application package.
But i could not find anything useful on datadog website nor maven central repository.
Including the agent itself or the api libraries fix nothing.
This class is present on the agent but under a different path.
Does anybody know which dependencies should be included in the classpath of the application to fix that ?
I have the above datadog json template with me which I have to just import in terraform instead of recreating it as terraform dsl.
Okay, here is my setup:
Question: So I am running Ubuntu 18.04LTS instances and as time goes on, it seems to spawn additional devices periodically:
device:/dev/loop1, /dev/loop2 and so on.
When I first spun up these instances, there were only 3 /dev/loop(1-3) devices, however, over time, a /dev/loop4 showed up and our drive space alert paged me since these are 100% utilized when created.
So, I have to go into each of the monitors (one per environment) and add an exclusion for the new /dev/loop4, but I cannot set the exclusion until it has been created by at least one of the monitored instances.
Is there a way in DataDog that you can just add a blanket exclusion like:
device:/dev/loop*?
I have been combing through documentation and have not been able to find anything, so I thought I would ask here.
I have a datadog count metric that I want to create a new metric from which shows the difference between two agent points on the metric, so I can see the change between points.
Is there a way to create a metric from another metric using the datadog dashboard.
I'm using the DataDog Helm chart to install the DataDog agent on my EKS Kubernetes clusters ( https://github.com/helm/charts/tree/master/stable/datadog ).
The problem I'm having now is that I am not able to filter logs by cluster name.
I have also set the  DD_CLUSTER_NAME  environment variable but it does not seem to do anything.
I have set the following in my values.yml file:
I've installed the DataDog agent on my Kubernetes cluster using the Helm chart ( https://github.com/helm/charts/tree/master/stable/datadog ).
This works very well except for one thing.
I have a number of Redis containers that have passwords set.
This seems to be causing issues for the DataDog agent because it can't connect to Redis without a password.
I would like to either disable monitoring Redis completely or somehow bypass the Redis authentication.
If I leave it as is I get a lot of error messages in the DataDog container logs and the redisdb integration shows up in yellow in the DataDog dashboard.
What are my options here?
Recent Tenable scan highlighted an issue with certain versions of datadog versions.
This is also brought to attention in Datadog monitor.
Critical bug in Windows Agent versions 6.14.0 and 6.14.1.
See --   http://dtdg.co/win-614-fix  &lt;-- for steps to fix the issue.
As the bulk of our servers are hosted on AWS - just wondered if I could query this through AWS CLI to list which servers were using the affected versions.
I want to disable all builtin metrics (jvm, cpu, etc) but keep my custom metrics.
When I enabled Spring Boot Actuator metrics together with Datadog I end up with +320 metrics sent to datadog.
Most of these metrics are from the  builtin core metrics  (JVM metrics, CPU metrics, File description metrics) only 5 of those metrics are my custom metrics that are the ones that I want to send to datadog.
According to  this section of the Spring Boot documentation :
Spring Boot also configures built-in instrumentation (i.e.
MeterBinder
  implementations) that  you can control via configuration or dedicated
  annotation markers
but there is no direct example on how to exclude the those metrics
From what I found in  this other SO question  one way to control it is:
and that removes all the metrics except the JVM ones.
But it  also removes my custom metrics .
I don't see how can I reenable my custom metrics.
Just for the record the way I register the custom metrics is this way:
This works ok, as long as `management.metrics.enable.all=true
So how can I disable all core metrics , but keep my custom metrics?
I need to implement tracing opentracing (opentelementary) for datadog in my Spring Boot application with rest controller.
I have a given kubernetes endpoint, to which I should send traces.
I am running the datadog agent using docker
I want to send custom metrics using dogstatsd.
When I run
I can see in wireshark that the udp packet was successful from the source to the destination but this metric is not being submitted to datadog.
Am I missing some configuration?
I have an alert in Datadog when CPU Credits are low.
The problem is when I create a new RDS in Amazon, initially it has 0 CPU credits and I receive this alert.
How can I avoid this case?
I tried to find "time since creation" metric, but with no success.
I have a service and send Datadog events from it using com.github.arnabk.java-dogstatsd-client.
In order to send json string I use  JsonObject  where I put all properties which I need then convert it to string using  toString()  method on  JsonObject  and send string as a message body.
Everything works perfect unless I have a character in a string which is not from english alphabet.
Example: µ.
In this case instead of having correct json  {"Smth":"µ"}  in Datadog I'm getting incorrect string without closing curly brace  {"Smth":"µ" .
Has anybody experienced the same and knows how to deal with this?
I use datadog agent 6.9 that run on my host(not on a docker), 
and i also run several application on my host (docker images).
I try to avoid sending specific logs to the datadoghq from my mongodb.
So according to  https://docs.datadoghq.com/logs/log_collection/?tab=tailexistingfiles 
I create mongo.d directory and conf.yaml inside that look like:
But when i restart my agent it's still send the unwanted logs to my datadoghq.
Thanks in advance for the help,
Baruch
I'm using  Datadog dashboard to monitor Aurora clusters  I have in my account.
The " query-volume " section is always empty, even if I go the mysql shell and do a couple of selects.
I'd like to make sure it works before I put high load on my db in production.
for now I only see changes in query-volume section in the charts of  Select latency and DML latency  and in AWS resource metrics in all charts.
whereas DiskIO section is totally empty, ( Connection and Replication is empty as well, but I know that because I don't have a replica )
Any idea how can I make sure it works?
I'm running an Azure Cloud Service with a WebRole.
We run the DataDog Agent on each of our server instances, by running a startup task that executes a .cmd file.
Previously we have been using the latest version of DataDog Agent 5, and installing it using this -
Now we are trying to upgrade to the latest version of DataDog Agent 6 using this, which is failing to install and register the instance as an available host in DataDogs dashboard -
The URL is of course different in each case.
I used the  DataDog Audit bundle  in order to log every action that happens in my MySQL database.
However when I check the diff column in the audit_log table I can't find the ID of the respective entity that have been updated/inserted/deleted etc.
I also can't find which user is responsible for a certain action.
Does anyone know if the DataDog audit bundle saves the ID of the entity to which action are performed and if this is the case where I can retrieve this data?
I have a MongoDB using the database profiler to collect the slowest queries.
How can I send this information to Datadog and analyze it in my Datadog dashboard?
i have some problem to settings my dashboard metrics in datadog, the case is about current connection of my apps, for example when there is user connected my app the value goes add by 1, but when its disconnected it will reduced the value by 1. the problem when im using datadog, they will evaluate based on timestamp, so for example if i want to check per 5 minutes, when first 5 minutes there is 10 users connected it will add by 10 the monitoring show 10, it should not be a problem, but the problem when the next 5 minutes when there is 5 disconnected users, it will reduce the value by 5, and it should be  5  not  -5 .
is there any function that i used to somehow ignore the timestamp in datadog ?
additional information with last case i mention earlier if the next 5 hours there is no user that connected / disconnected again it should be show as 5 users regardless what time batch series i take.
is that possible to do that in datadog ?
I am trying to set up hazelcast metrics pushed to datadog.
I followed below documents.
DD for HazelCast
This is what I did:
Now, I have to do same in ci pipeline in my org.
So I have to pass these annotation in --set
You can see it is little bit complicated:
It shows pods running with annotations like this
What I also tried:
I see pods running but no metrics in Datadog.
I am 99% sure that json is screwing things up.
Also I tried jq.
where,
and
and I got
Any help is appreciated.
We have a requirement where we need to send airflow metrics to datadog.
I tried to follow the steps mentioned here
 https://docs.datadoghq.com/integrations/airflow/?tab=host
Likewise, I included statsD in airflow installation and updated the airflow configuration file (Steps 1 and 2)
After this point, I am not able to figure out how to send my metrics to datadog.
Do I follow the Host configurations or containarized configurations?
For the Host configurations, we have to update the datadog.yaml file which is not in our repo and for containerized version, they have specified how to do it for Kubernetics but we don't use Kubernetics.
We are using airflow by creating a docker build and running it over on Amazon ECS.
We also have a datadog agent running parallely in the same task (not part of our repo).
However I am not able to figure out what configurations I need to make in order to send the StatsD metrics to datadog.
Please let me know if anyone has any answer.
I am new to DataDog and getting back into working with Windows Servers.
I am trying to push Event Viewer logs (Security, System, etc) to Datadog logs.
I have been successful in terms of setting it up (used their documentation -  https://docs.datadoghq.com/integrations/win32_event_log/ ).
I am getting logs into my DD for that server for my System and Security:
I know that you can push items from the Event Viewer to Events in DD by using  Instances  and you can be more granular there.
But I want that granularity in the logs sections since we rarely view Events.
Right now it is showing me all the items in the logs, success, etc.
I am looking to only get the Errors and Warnings piped to the Logs.
Thanks for the help.
D
I am having trouble figuring out how the datadog forward encodes/encrypts its messages from the datadog forwarder.
We are utilizing the forwarder on datadog using the following documentation:  https://docs.datadoghq.com/serverless/forwarder/  .
On that page there, Datadog has an option to send the same event to another lambda that it invokes via the AdditionalTargetLambdaARNs flag.
We are doing this and having the other lambda invoke but the event input that we are getting is long string that looks like it is base64 encoded but when I put it into a base64 decoder, I get gibberish back.
I was wondering if anyone knew how datadog is compressing/encoding/encrypting their data/logs that they send so that I can read the logs in my lambda and be able to preform actions off of the data being forwarded?
I have been searching google and the datadog site for documentation on this but I can't find any.
I'm Using,
But I'm not able to get the operation name and endpoints associated with service.
Help me to find the correct url link.
Getting query invalid for below monitor, Please suggest.
Taking a look at  Redis metrics for Datadog  , we can see that  redis.cpu.sys  refers to  System CPU consumed by the Redis server.
But this is metric for CPU usage time, not CPU usage.
How do we do if we want to create alerts based on the actual CPU usage?
Am looking for information on configuring Datadog to perform event correlation using custom event attributes (application components output events records in JSON format).
Also, is it possible in Datadog to then configure notifications based on correlated events?
Appreciate any pointers on the above or where I can get the above information.
TIA.
RKH
I want to monitor our application's usage of a 3rd party API with datadog.
We have a daily quota of x calls we are allowed to perform every day.
The number of calls resets every day at midnight.
I have added a datadog metric in our code that increases every time we make a call to that API.
Now I want to create a monitor that will alert us whenever we reach 80% of our allowed daily calls.
In other words, I want to get the calls we made from the beginning of the day.
Is that possible with datadog?
Hey I am trying to logging within a script and send a log message to Datadog via their api: POST  https://http-intake.logs.datadoghq.com/v1/input .
My issue is that the code works perfectly well when running on the team's machine but once it's in the pipeline, it keeps throwing 400 errors as a response from Datadog.
The Key is perfectly valid and the log message works on our machine so I cannot see why I am getting a 400 error.
Wondering if anyone else has run into this problem
pic of gitlab-cl.yml
Thanks,
Java app, built with Gradle, implementing SLF4J and Logback, exporting with Logstash to Datadog agentless logging.
Can't seem to get the  host ,  service , or  source  properties to transmit:
Note where I've included the  &lt;host&gt;  and  &lt;service&gt;  tags.
I also tried  &lt;property name=&quot;..&quot; value=&quot;..&quot;&gt;  and  &lt;KeyValuePair key=&quot;service&quot; value=&quot;java-app&quot; /&gt;  to no avail.
Here are the docs I'm reading from Datadog:
https://docs.datadoghq.com/logs/log_collection/java/?tab=log4j#agentless-logging
https://docs.datadoghq.com/tracing/connect_logs_and_traces/java?tab=log4j2
https://www.datadoghq.com/blog/java-logging-guide/
https://docs.datadoghq.com/logs/log_collection/?tab=host
https://docs.datadoghq.com/getting_started/tagging/unified_service_tagging/?tab=kubernetes
Also, the docs  for logstash-logback-encoder  itself states:
By default, each property of Logback's Context (ch.qos.logback.core.Context) will appear as a field in the LoggingEvent.
By default, each property of Logback's Context  (ch.qos.logback.core.Context)  will appear as a field in the LoggingEvent.
So, how do I add a property to Logback's Context?
I'd like to export a few metrics from  k8s.io/kubernetes/pkg/proxy/metrics : e.g.,  sync_proxy_rules_duration_seconds  and  sync_proxy_rules_last_queued_timestamp_seconds , what datadog integration shall I use for it?
There's a section of  K8s proxy metrics  that are collected by default but there're not these 2 metrics I'm interested in on that list.
Moreover, I can't  find  kubernetes proxy integration as well even though there's this  kube-proxy repo  on GitHub.
What is the difference between the  count  and the  gauge  metric types in DataDog?
Or rather, when should I prefer one over the other?
The definitions from their website don't help me much:
Count:
The COUNT metric submission type represents the total number of event occurrences in one time interval.
A COUNT can be used to track the total number of connections made to a database or the total number of requests to an endpoint.
This number of events can accumulate or decrease over time—it is not monotonically increasing.
Gauge:
The GAUGE metric submission type represents a snapshot of events in one time interval.
This representative snapshot value is the last value submitted to the Agent during a time interval.
A GAUGE can be used to take a measure of something reporting continuously—like the available disk space or memory used.
The  count  type seems to be somewhat related to the  rate  type, but for me it is unclear why or when I should use  count  instead of  gauge .
I mean in principle a measurement of &quot;something&quot; could always be presented as a gauge, couldn't it?
how to write pytest for the below module?
I have been trying to write the unit test for the below datadog API monitor creation using python language.
Assume the create method is going to send 200 status.
how do I mock  json[&quot;monitors&quot;][0]['type'] .
I get
Datadog API reference -  https://docs.datadoghq.com/api/latest/monitors/
json content:
Is there a way to easily generate reports of alerts from certain monitors in Datadog, on a weekly or biweekly basis?
Context: At the moment, these alerts go to a Slack channel.
Folks have to scroll through the channel to see all the issues and prioritize investigations (during sprint planning).
I am trying to make it easy for sprint planners to pull up the alerts report.
I found only a couple related things after googling:
Datadog has a CSV with 6 months of alerts, that you can curl to download.
I guess I could curl, diff with prior week's csv and filter for interesting monitors.
But does not seem like the best solution.
https://docs.datadoghq.com/monitors/faq/how-can-i-export-alert-history/?tab=us
An old article about Monitor Trends Report which I can't find in the app.
https://www.datadoghq.com/blog/monitor-alert-status/
I'm using V6 of the Datadog agent on Ubuntu 18.04.
I'll like to change the min_collection_interval for all checks from the default 15 seconds to 30 seconds.
It's unclear from the documentation.
Is this possible?
I’m trying to group logs from a source so I can filter them in or out in DataDog logs.
There is already a grok parser that formats the messages, but how can I add a tag to them?
DataDog seem to use a subset of LogStash grok parsing rules:  https://docs.datadoghq.com/logs/processing/processors/
Eg from Heroku:
as
What I'd like is to add something like a  type , so I know they are not from the app, eg
Then I guess I could add the same thing to app logs and add  type: 'app'  to them.
I use k6 on my local machine to perform load-testing as well as a  Datadog agent  to visualize the metrics in Datadog.
I'd like to filter k6 metrics in Datadog as the tests aren't distinguishable.
At this point the  $test_run_id  only shows  *  (refer to the screenshot below):
I followed  this the official doc  that suggests to set  include_test_run_id  flag to  true  in k6 config, but I was unsuccessful.
Here's a k6 config I currently use ( &lt;YOUR_DATADOG_API_KEY&gt;  is replaced with an actual Datadog API key):
I have an nginx-pod which redirects traffic into Kubernetes services and stores related certificates insides its volume.
I want to monitor these certificates - mainly their expiration.
I found out that there is a TLS integration in Datadog (we use Datadog in our cluster):  https://docs.datadoghq.com/integrations/tls/?tab=host .
They provide sample file, which can be found here:  https://github.com/DataDog/integrations-core/blob/master/tls/datadog_checks/tls/data/conf.yaml.example
To be honest, I am completely lost and do not understand comments of the sample file - such as:
I want to monitor certificates that are stored in the pod, does it mean this value should be localhost or do I need to somehow iterate over all the certificates that are stored using this value (such as server_names in nginx.conf)?
If anyone could help me with setting sample configuration, I would be really grateful - if there are any more details I should provide, that is not a problem at all.
i am trying to send kafka consumer metrics to datadog but its not showing in monitoring when I select the node.
The server is giving below check in status
JMX is as above.
Please help in finding what could be wrong.
I am looking for the criteria which decide whether we could use the built-in feature of Google cloud i.e Stack driver or we need to go for third-party tools like Grafana, Datadog etc in order to carry out Monitoring and logging.
I was trying to install  data-dog  agent in my  Ubuntu 20.04  for monitoring a python backend with the following command
From the  official documentation  it says
But haven't found any  python.d  inside  /etc/datadog-agent/conf.d .
If I create the  python.d/con.yaml  do I need to do anything else for enabling sending logs?
I have a scheduled Celery task that queries  x  number of rows, does some processing and upon success (or error) it increments a specific metric using ThreadStats.
For each execution of this task, the metric should be incremented by  x  at a specific time.
The problem is that some of these increments are not posted to DataDog.
Ex.
if the total number of rows is 100 and the task processes  x=10  at a time, some of those task executions fails to increment the metric and it ends up displaying only 60.
This is what I tried to do without success:
I'm trying to collect logs from cron jobs running on our self hosted Github runners, but so far can only see the actual github-runner host logs.
I've created a self-hosted Github Runner in AWS running on Unbtu with a standard config.
We've also installed the Datadog agent v7 with their script and basic configuration, and added log collection from files using  these instructions
Our config for log collection is below.
After these steps, I can see logs from our Github runners servers.
However, on those runners we have several python cron jobs running in Docker containers, logging to stdout.
I can see those logs in the Github Runner UI, but they're not available in Datadog, and those are the logs I'd really like to capture, so I can extract metrics from.
Do the docker containers for the python scripts need some special datadog setup as well?
Do they need to log to a file that the datadog agents registers as a log file in the setup above?
I need to integrate Datadog monitoring on Apache web servers which are on Windows servers.
Is there a link/blog available detailing the same for Windows server specifically ?
I got a blog link from Datadog but it seems not to cover Windows servers.
Need it specifically for Windows servers.
Please advise me about Datadog metrics.
I'm using the  increment  method to send data to Datadog, but I can't see the total number on the Datadog side.
I have specified a sample rate of 1 and am sending everything.
If you look at the following documentation, you will see that &quot;COUNT type metrics can show a decimal value within Datadog since they are normalized over the flush interval to report per-second units .&quot; and stated.
https://docs.datadoghq.com/developers/metrics/dogstatsd_metrics_submission/#count
Isn't there a way to see the total number on Datadog?
Please let me know if you know.
I have a piece of code:
I have to show this time difference in Datadog dashboard.
Can anyone help me with this?
P.S.
I have the io.micrometer setup in place and MeterRegistry is autowired in the class.
Just need to know the method to show such a metric on the dashboard.
We are using the eBPF via the Datadog-agent which is installed in a linux server.
More precisely, we are using the nprobe for gathering “NetFlow data” in the Linux server, and then Datadog via eBPF illustrates these flows on a dashboard.
However, we got an issue as the IP-source is always remains the same.
Actually is the IP address where the Linux-Server is receiving the “Netflow data”.
That's not normal as Netflow is based on a unique pair of ip.source / ip.destination.
It seems that eBPF takes as source/reference the traffic that is receiving on linux-NIC
Is there any way to modify this behaviour?
Re
I want to send custom metrics using io.micrometer.datadog.DatadogMeterRegistry to datadog.
Below is the code snippet of the method where I am emitting metrics to Datadog.
I am able to see logs &quot;metric sent successfully&quot; with no error but this custom metric is not showing up in Datadog UI under metrics summary.
Am I missing anything?
Is it possible to graph an SLO as a time-series graph using just native Datadog components?
If so, how?
I can only find a way to show an SLO as a number, I'd like to show how it changes over time in a graph-format.
I am trying to create an alert Datadog using Terraform for when multiple hosts (1 or more)  are at &gt;= 95% CPU usage.
So far, with the code I have, the alert would trigger anytime a host exceeds the threshold and that is a little too noisy.
Would you happen to know how to create the logic to satisfy both conditions before the alert gets triggered?
(Alert when Multiple hosts at 95% CPU or higher)
I am new to the datadog and I have followed the official docs and installed datadog metric in EC2 and collect the metrics.
I am able to successfully view the metrics like EC2, RDS,S3.
I am now trying to integrate API gateway and AWS glue but didnot know how to do that also I couldnot find useful article too.
It would be great if some one help me to achieve my requirement.
Thanks in Advance
How do I set custom &quot;trace_id&quot; for Datadog tracing?
I searched high and low but can't find an answer to this.
I suspect it's not supported.
Would really appreciate it if I can get some help here.
As an example, if I can do the following in multiple files, then I can view these spans together in the Datadog UI since they all have the same trace ID:
Is there a way to create a delay of n seconds when making datadog event monitors?
I have a monitor set up to ensure that there are 120 events of a kind received in every 24 hour period.
The problem is that the monitor goes off when there are ~117-119 events present, and then it resolves immediately after that - when the remaining events come through in a few minutes.
I want to add a delay of sorts, that will only trigger the monitor if it remains in alarm state for more than 10 minutes at a stretch, rather than triggering alerts as soon as the count dips below 120.
i am using datadog to monitor my cloud infrastructure(AWS).
at present, i am sending aws-logs to datadog and datadog keeping those log data for some default timeframe.
How i can set some limit so that after that particular limit logs will be deleted from datadog?
I want to delete datadog logs after 7 days
Can anyone suggest a solution for this.
AWS EC2 instance with Talent Server and agents hosted.two agents are on-perm from where data is pushed to cloud thorugh Talent Job.
Please suggest is there integration avaiable for monitoring Talend job for monitoring on DataDog which is again hosted in Same AWS account.
I have a bunch of logs that are supposed to be summed up into a value, that I would like to monitor.
I tried sum function in the query widget and in table widget but nether seems to work with logs and not metrics.
Anyone did something similar?
Thanks
I'm trying to send events to dataDog from c# (unity specifically) but it returns an error 400 every time and I'm at a loss at what else to attempt..
This is the error it gives me
The error is actually on the response line
 var httpResponse = (HttpWebResponse) httpWebRequest.GetResponse();
If I comment that line out, the error goes away but I never receive the events on dataDog, which is understandable considering it's saying it's rejecting my post
Given monitor that displays values grouped by language, e.g.
:
Is it possible to retrieve the actual value of a language to display it in the Monitor Name (to see it in the potential alert,  here )?
I've tried putting into the name the something like:  Errors found in {{language}}  and  {{language.name}} , but it didn't work.
Maybe variable evaluation is not expected to work in the name of a monitor?
Id like to show a sum of all my monitors that are in alert status on my dashboard.
so for instance if i have
monitor 1, monitor 2, monitor 3
and monitor 1 and monitor 2 are alerting, the dashboard would show
Current number of alerts: 2
does anyone know if this is possible?
Given following scenario:
Right now we monitor a custom error-count metric like  myService.errorType .
Which gives us an exact number of how many times an error occurred - independent from a specific entity: If an entity can't be processed like 100 times, then the metric value will be  100 .
What I'd like to have, though, is a distinct metric based on the UUID.
Example:
Then I'd like to have a metric with the value of  2  - because the processes failed for two entities only (and not for 30, as it would be reported right now).
While searching for a solution I found the possibility of using tags.
But  as the docs point  out they are not meant for such a use-case:
Tags shouldn’t originate from unbounded sources, such as epoch timestamps, user IDs, or request IDs.
Doing so may infinitely increase the number of metrics for your organization and impact your billing.
So are there any other possibilities to achieve my goals?
I am trying to connect to datadog from my fastapi backend.
I am currently trying to do this on localhost using a docker-compose file to let both my datadog-agent and my backend-container run in the same network.
Here is a minimal example
docker-compose.yml
Dockerfile
main.py
I run docker-compose up and then check the ip of my dd-container with
and update it in the compose file.
When I then again run docker-compose up, I get the following error
Any help would be very appreciated
I am trying to test Datadog and see if it works for my needs... the problem is that I cannot make it works...
I have been following the guide here:  https://docs.datadoghq.com/integrations/amazon_lambda/  nut no luck... is there anybody who has a step by step guide?
(I have trouble on getting the IAM datadog policy (should I create it manually or AWS coludformation does that for me?)
I need to test the AMP mainly, because I am curious to see how the service map will work...
any suggestion please?
Thank you
We're using the DataDog agent with some .Net Core micro-services on Linux.
We want to send our statsd metrics directly through the DataDog agent but we need to do some filtering before the agent sends the metrics to DataDog.
All I could find was the following:  https://docs.datadoghq.com/tracing/custom_instrumentation/agent_customization/?tab=mongodb
That all appears to be about logging and spans, not metrics.
Is it possible to filter the statsd metrics?
The question is about DataDog - OpsGenie integration.
Whenever a DataDog monitor triggers an alert an incident is opened in OpsGenie (which is good), but when the monitor recovers back to a healthy state the OpsGenie incident is auto-closed (which is bad).
Is there any way to prevent this behavior?
I want to keep incidents open until they are acked and resolved.
I am using the dependency  'io.micrometer:micrometer-registry-prometheus'  to Calculate Average request processing time.
For that I wrote the following java code
So with that code,  timer  measures the time taken by the runnable (request method) and also counts the number of times the method was called.
As a result two metrics are generated which I can use in Datadog:
Then in order to calculate the average request processing time , I have created a &quot;Query value&quot; graph in datadog and uses the datadog  avg by  function.
But instead of resulting in the  average time , DataDog seems to calculate the  sum of the average request time per event type  and shows an always increasing average request time in seconds (which is wrong because I logged in java code the request times and they are all under 100 milliseconds; so an average must be in a range of milliseconds).
So I suppose that I am doing something wrong.
My question How do you compute average response time using DataDog graphs?
N.B.
what I also tried as a solution is to divide count by sum using the datadog function &quot;Add Query&quot;.
It means in short
But some colleagues argued that it is not a proper way to calculate the average and that it should be able to calculate the average out-of-the-box using DataDog.
What do you think about that solution?
Regards
Is there a way to connect a Snowflake Snowpipe logging to DataDog.
I would like to setup live monitoring for a Snowpipe, I only know of the debugging tools such as  SYSTEM$PIPE_STATUS   information_schema.copy_histor   information_schema.pipe_usage_histor  and  information_schema.validate_pipe_load  but I would like it to be proactive monitoring and not ad-hoc debugging.
From what I understand I could poll the Snowpipe REST API in an automated way and push the logs to a logging system but I'm wondering if there is no easier access to the Snowpipe logs.
For example to easier load them into DataDog and be alerted whenever something goes wrong.
Thanks for any pointers!
Cédric
I've installed the free trial version of DataDog on my Windows 10 box.
I'm trying to follow the instructions to monitor a custom file, from  https://docs.datadoghq.com/getting_started/logs/#monitor-a-custom-file
I've created a test log file in  C:\dev\tmp\datadog_logs\first.log .
I've edited  C:\ProgramData\Datadog\datadog.yaml , setting  logs_enabled: true .
I've created a yaml file for custom log collection in  C:\ProgramData\Datadog\conf.d\custom_log_collection.d\conf.yaml , containing:
Then I've restarted the agent, and checked its status (from an Administrator Command Prompt):
From the doc I linked above, I would expect to see a &quot;custom_log_collection&quot; entry under &quot;Logs Agent&quot;.
I do not:
The obvious place where I might be having problems is in the path to the file.
I've tried several variants, e.g., a unix-style path:  /dev/tmp/datadog_logs/first.log , and nothing works.
Any ideas as to what is going wrong?
Or where to find out what might be going wrong?
I am trying to generate graphs for the AWS Lambdas in Datadog.
I want to add titles/units to the x-axis and y-axis.
Please help on this
Below is the deployment yaml file spec parts, I am trying to update for datadog logging of containers
How to use variable &quot;LAUNCH_ID&quot; in the annotations?
I am using stats-d [ https://www.npmjs.com/package/node-statsd ] and datadog is connected to it.
I would see metrics which I sent to stat-d, being captured on the datadog UI.
However I was asked to add tags.
I changed:
client.increment(somemetric);
to
client.increment(somemetric, [incrementTag]);
Soon after I did that nothing showed up on datadog.
Looks like I have followed the stats-d doc.
What would be my next steps to figure out why datadog cannot read it ?
I'm trying to setup a notification message on Slack for a monitor on a custom metric that we created.
I would like the message to include a  timestamp  of the event, and also a link that redirect to the log, to analyze it immediately.
Are there any template variable like {{var}} that let me insert the timestamp and the link to the log, or maybe that let me build the log search query string
dynamically like: 
 https://app.datadoghq.com/logs ?....
(so I will need the timestamp at least)?
At the moment we only have this in the message:
CHANNEL: {{channel.name}}
ENVIRONMENT: {{environment.name}}.
I am trying to build Datadog integration extras plugin which used to connect Neo4J to Datadog, for monitoring.
I am following this post  https://docs.datadoghq.com/integrations/neo4j/
And the integration plugin code is in this github location  https://github.com/DataDog/integrations-extras
I am using Mackbook pro for building the application.
As per the article i am not able to execute the following command.
While executing the above command i am getting the following error
 "Error: accepts 0 arg(s), received 3"
Can anyone please help me what i am missing.
We are using JMS (tibco EMS) as messaging service can We visualize the performance using Datadog?
If it is not possible do we have any alternate?
I'm trying to import my whole Datadog environment to the Terraform configuration.
My account has access to multiple organizations.
I want to import it to the single Monolithics repository.
Unfortunately, I met the issue with directory layout startegy - I'm not sure how it should look based on  Terraform best practices .
I suggested:
or
Does somebody have experience with the issue?
What do you think?
Could you provide me your experience?
Thanks in advance!
Basically, i created aws cloudwatch metrics from cloudwatch logs.
when i create a chart using these metrics i can see correct chart in the cloudwatch Dashboard.
Problem
I want to create the same chart in the DataDog Monitoring environment, i can able to create the chart using those metrics but i didn't see accurate results.
In Datadog  I am seeing decimal points as metric values(0.1,0.2,0.25 etc).
can anyone tell me how i can resolve this issue?
I would like to use the  Datadog Oracle Integration  via the  Helm Chart Datadog .
Oracle Integration states  To use the Oracle integration, either install the Oracle Instant Client libraries, or download the Oracle JDBC Driver.
I do not want to use a custom image to package the JDBC-driver, I want to use a standard image such as tag:7-jmx.
Other options that come to mind (e.g.
EFS volume with the driver inside) seem to be an overkill also.
Best option to me seems to be an init container that downloads the JDBC driver.
But Datadog Helm Chart does not support custom init containers for the agents.
What's the best way to do this?
To get an Datadog Agent with a JDBC driver via Helm?
I run mysql  Ver 14.14 Distrib 5.7.30 on Debian 10 with the latest DataDog agent
In  datadog-agent status  I see warning:
 Warning: Failed to fetch records from the perf schema 'events_statements_summary_by_digest' table.
During setup recommended permissions were added:  GRANT SELECT ON performance_schema.
* TO 'datadog'@'localhost';
..and in fact datadog user can read that table:  mysql -u datadog -p performance_schema -e "select * from events_statements_summary_by_digest" :
My DataDog agent's config:
What am I doing wrong?
I installed Datadog on my Django 1.8 application.
I am getting all the needed services which are running and apart from that I am also getting a service called "unnamed-python-service" popping up, inside of which are some HTML resources rendered by  django.template
I have the below loaders configured and use the default templating engine
Can anyone help me understand more about the service and also how to rename it?
Here's how it shows
Is it possible to query events and it's properties and make chart or dashboard and download the results, like as we do in amplitude analytics or heap analytics?
I have installed Datadog directly on 2 of my Ubuntu servers;
however in my third server I am facing problems.
Same procedure same steps, only difference between these two are that the 3rd server contains multiple virtualhost entries.
Datadog installation: using helm was successful
Docs used
Agent version (7.19.0)
Agent Status
Error in Logs
ERROR which i see in log agent and the same is true for all pods
Not sure where I am going wrong.
Any help is highly appreciated.
We want to monitor an individual deployment of datadog.
By default, the agent gets installed on all nodes and we start paying for each node as a host.
However, we're interested in using datadog to monitor our "agents" running in customer clusters, therefore we only want to monitor that one deployment.
Paying 15$ per node for each client would blow up our costs.
Is there a way to only monitor a specific deployment on K8S with datadog and only pay a fee linked to that deployment, not to the size of the entire cluster?
I have logs that contain this kind of line:
I want to filter out ones with  "event_artist_id": 100
How do I do that?
I tried many options, but no luck yet, for example:  \/"event_artist_id": 100\/*
I have DataDog with Amazon AWS RDS integration configured.
Is it possible to create a graph and use a tag to exclude some hosts from the result.
I have let's say 100 hosts with tag  environment:live  and 10 of them are also tagged with tag  importance:ignore .
So I need to create a graph which will include metrics for 90 hosts that are tagged with first tag but don't tagged with a second one.
Is it possible?
I have a function that runs in a thread pool, but it only shows up in the Datadog tracing UI when I run it outside of my threadpool.
In the screenshot below you can see it show up in  sync_work  but not in  async_work .
Here is my code, contained in a script called  ddtrace_threadpool_example.py :
I run the script like this:  python ddtrace_threadpool_example.py .
I'm using Python 3.7, and  pip freeze  shows  ddtrace==0.29.0 .
I am running a  Python Pyramid  app and I want to measure the number of requests coming in per second.
For this, I am using  datadog .
We have a company wide datadog server.
On my instance where my app is running we have a  dd-agent  running.
I have initialized datadog client in my app and I am using the following call to capture number of requests per second.
I am trying to capture all requests and also specific gameplay requests like for  game1  and  game2 .
So when the incoming request is for either of these two games I added the following tags depending on the game being requested:
But for requests other than these two games the tags is empty as  tags=[]
My question is:
Is this the right way to do it?
So when I go to the datadog for this metric and I don't give any tags to filter I should be able to capture all incoming requests per second?
And when I use the filter as  game:game1 , I should be able to see incoming requests for only  game1  ?
I am trying to create a "Top List" visualization in DataDog and I would like to graph my data which should be grouped by error code.
This error code is a substring in logs.
An example of a line in the log is given below.
I have tried to group my data by message but this is not working, I would like to group my data by substring of the message.
Can someone guide me on this?
...Server Error {"error":{"code":1001,"type":"MATCH","message":"Invoke
  failed: Failed...
  ...Server Error {"error":{"code":2001,"type":"MATCH","message":"Invoke
  failed: Failed...
Currently I get the visualization as follows
1.0 is the number of occurrence
Rather I want the visualization as follows
2.0 will be total 2 occurrences of error 1001 and 1.0 will be the occurrences of error 2001
Does anyone know if Datadog agent works on snowflake?
We want to use Datadog to collect snowflake metrics, traces and logs and create dashboards, graphs, and monitors.
I have a the following function to publish datadog metrics
I am now trying to change it so it will go through a socket, as I find in this following page:
https://github.com/garrettsickles/DogFood   under  UDS - Unix Domain Sockets (Custom) .
Now the function looks like this:
When I try to compile it, I get 'not a member' errors.
How can I include them as members?
I have the following  curl  command which works fine:
Then I convert this requests to Python Requests, and the  curl  method works but Python returns a 500 error without any details.
I tried it outside my Docker guessing that maybe connection was the key, but it doesn't work either.
I am searching for the occurrence of character  $  in a log file on datadog log explorer which uses lucene syntax.
It's pretty similar to kibana.
I have logged a string for testing  Testing $ pattern datadog  but when I search for  $  it doesn't show any results.
On searching for  Testing  I get  Testing $ pattern datadog  in response.
Please tell me how can list the occurrences of  $  any help is much appreciated.
I have created a Multi alert event monitor
I wanted it to be aggregated by "dbinstanceidentifier" but it shows the accumulative count for "* (Entire Infrastructure)".
Basically it doesn't see any groups.
But I can see them in the infrastructure.
Is it a problem of datadog?
May be it's only available in a kind of "premium" subscription?
I've created a custom Datadog metric in a Springboot Java App, and turned on the management end-points.
I am incrementing a MeterRegistry Counter with a double value (relating to the monetary value of an order)
When I use the /management/metrics end-point, I can see the correct value being stored.
However, when I create a widget in my Datadog dashboard, it is only displaying the pre-decimal point value of the data.
e.g the order value is 61.67 and in Datadog it is displaying 61, so it's not even doing any rounding !
Is there any way to display the raw value of the counter in a Datadog Dashboard widget?
Thanks in advance
Im trying to deploy my service and read my local logfile from the inside pod.
Using DataDog's helm chart values with the following configs :
as you can see I expect my logs to be available at /app/logs/service.log and thats what Im supplying to my conf.d :
In my service, I use WinstonLogger using file transport with the JSON format.
process.env.LOGS_PATH = '/app/logs'
After all, exploring my pod and tail -f my service.log in the expected /app/logs folder I see that the application actually writes the logs in a JSON format as expected.
My DataDog doesn't pick up the logs and they are not showing in the log section ..
NOTE:: I do not mount any volume to and from my service ..
What am I missing?
Should I mount my local log to /var/log/pods/[service_name]/   ?
My team and I are trying to add a table to summarize our logs.
Our system logs to datadog (not always but sometimes) as follows:
To group a set of logs (Large operation) and know they are related I added to each a  trace_id  facet (to each log in between).
Eventually I want to quickly be able to see
What's the best approach?
I don't want to query each time I need the information - I want to create an overview and to apply the system's analytic tool to this summary
Is it possible to automatically mute created Datadog Monitor(which is monitoring Windows Service), when Windows service state was changed from Automatic to Disabled?
How can it be configured
Thanks.
I'm trying to create a datadog monitor that only alerts on Wednesdays and Fridays.
I have created the metric and monitor, and I think the best solution is to create a schedualed downtime that repeats for the days I'm not interessted in.
Ive created the downtime window as:
This creates a window for only 1hr, ideally this should be 24hr
I was installed the "datadog-php-tracer_0.14.1-beta_amd64.deb" on my server and after installed my application return 500 error.
Below is the things which I have configured or my server related information:
I am using Ubuntu, NGINX and php-fpm 7.0.
I have installed datadog agent v6.
When I am checking my php-fpm log file, it shows the PDO error about "Slim\PDO\Statement\StatementContainer- execute()".
But when I disabled the Datadog Agent or APM trace then my application working normally.
In short when I am enable ddtrace my app not working and return 500 error.
Can you please look in it and let me know how can resolved the issue and APM work well with my app.
In our infra CPU utilization is being monitored by datadog SAS.
The dashboard shows  CPU utilization over a period time graphically.
How do I find the average CPU utilization over that period of time?
I'm trying to figure out how to create an alert around a process that may be crashing and restarting repeatedly.
It might be providing some data to Datadog while it's up, so a "no data" alert won't do because the lack of data never hits the duration threshold as the process restarts.
I was thinking of alerting on a changing PID, but I cannot for the life of me figure out how to create a PID-based Monitor.
Is it possible?
And how?
Does anyone have any other suggestions for this situation?
We want to collect metrics from machines running AWS lambda in AWS.
How can I get access to these machines and get DD agent installed on them.
We are not willing to use DD agent.
How can we know if PostgresSQL is up and running on my Redhat linux server so that I can create an alert when postgres is down.
I am using Kamon DatadogAgentReporter to record different metrics in my application.
After migrating Kamon from 0.6.x to 1.x, I can see only the list of metrics with tags without any service name.
I added the reporter like this, Kamon.addReporter(new DatadogAgentReporter()) and the config as given below,
Did I miss something?
How do I get the display service-name prefix for my metrices?
Thanks in advance!
I have written a Datadog Agent check in Python following the instructions on this page:  https://docs.datadoghq.com/developers/agent_checks/ .
The agent check is supposed to read all files in a specified network folder and then send certain metrics to Datadog.
The folder to be read is specified like this in the Yaml file:
This is the code used to read the folder, it is Python 2.7 because that is required by Datadog
If I just run the Python script in my IDE everything works correctly.
When the check is added to the Datadog Agent Manager on the same machine that the IDE is on and the check is run an error is thrown in the Datadog Agent Manager Log saying:
2018-08-14 14:33:26 EEST | ERROR | (runner.go:277 in work) | Error running check TaskResultErrorReader: [{"message": "[Error 3] The system cannot find the path specified: 'Z:/TaskResults/ .
'", "traceback": "Traceback (most recent call last):\n File \"C:\Program Files\Datadog\Datadog Agent\embedded\lib\site-packages\datadog_checks\checks\base.py\", line 294, in run\n self.check(copy.deepcopy(self.instances[0]))\n File \"c:\programdata\datadog\checks.d\TaskResultErrorReader.py\", line 42, in check\n for file in os.listdir(task_result_location):\nWindowsError: [Error 3] The system cannot find the path specified: 'Z:/TaskResults/ .
'\n"}]
I have tried specifying the folder location in multiple ways with single and double quotes, forward and back slashes and double slashes but the same error is thrown.
Would anyone know if this is a Yaml syntax error or some sort of issue with Datadog or the Python?
I have file full of metrics:
I would like to send them (once) to DataDog to create dashboard, because our infrastructure team didn't install DogStatsD agents on our containers yet.
I have a Spring Boot app which due to weird restrictions needs to run once every three hours, and won't work with Quartz, so I've been running it once every three hours from OS cron and it quits when it's done.
After adding micrometer-registry-datadog (and spring-legacy) however, it never quits, it just sends metrics every 20 seconds or whatever the default period is, even after calling registry.close().
Am I doomed like the dutchman to sail the seas of processing forever, or is there an obvious error I have made?
Code: It reaches SpringApplication.exit(ctx), but it does not actually exit cleanly.
(service is a TimedExecutorService.)
My daemon keeps querying db on a cronly basis.
In every iteration, (a) the deamon makes a DB query (b) receives some documents from db (c) processes those results.
I want to emit  the number of documents returned for the query  on Datadog.
What is the right metric type?
I created a wrapper cookbook to retrieve my datadog api keys from an encrypted data bag but it looks like it is not running during the execution.
Here is my code:
attributes/default.rb
recipes/set_key.rb:
and del_key:
I created a role named datadog and run list of this role looks like:
I'm expecting this wrapper recipe load datadog keys, then datadog recipes to run and finally another wrapper recipe to remove keys.
But when Chef is running, I receive an error message like:
Since I'm new to Chef and data bags use, I'm a bit confused.
Why my setter recipe is not running?
Thanks.
statd.yaml  in  conf.d  is configured as follows
after starting Datadog-agent I get one error as shown below
everything else runs fine, Also in  datadog-conf  I have mentioned the forwarder's IP and API key as well, but it is not showing in host map in Datadog webUI
I am having several issues regarding Flink and Datadog integration.
First, the issue is that Datadog uses dogstatsD instead of statsD which is not included in Flink documentation
Another issue is that if you go to Datadog's  Integrations  page, Flink integration is missing.
I have tried installing graphite but I am having several issues with that as well due to python 3.6, I tried virtualenv as well, but thought of going with datadog, which is giving me hard time as well.
Not sure if there are any pro DataDog users on here, but I'm hoping.
I've created a template DataDog dashboard template that captures the memory usage of a host by docker container.
The "hostname" appears in 5 or so places:
I'm trying to set up a dashboard right now that displays this template for each of my 20 or so hosts, but it's a painful process of cloning the chart and editing the host name in all 5 places.
Whenever I make a change to the template, I have to painfully paste the changes into each host chart and change the hostname in applicable places.
Is there a way I can set up this template (perhaps with a variable in place of the host name) and have a dashboard automatically create a chart for each host from this template?
Failing that, is there a way this can be scripted?
Thank you.
i am struggling with importing metrics with datadog...I am getting below error in spite of installing all required packages...
( - instance #0 [ERROR]: Exception('You need the "psutil" package to run this check',)
request you to please help me out here as this is prove to be a major showstopper.
[root@mudcsftpup01 init.d]# ./datadog-agent info
Status date: 2017-08-31 11:31:19 (1s ago)
  Pid: 32028
  Platform: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
  Python Version: 2.7.5
  Logs: , /var/log/datadog/collector.log, syslog:/dev/log
Clocks
  ======
Paths
  =====
Hostnames
  =========
Checks
  ======
Emitters
  ========
Status date: 2017-08-31 11:31:23 (2s ago)
  Pid: 32053
  Platform: Linux-3.10.0-514.el7.x86_64-x86_64-with-redhat-7.3-Maipo
  Python Version: 2.7.5
  Logs: , /var/log/datadog/dogstatsd.log, syslog:/dev/log
Flush count: 1
  Packet Count: 0
  Packets per second: 0.0
  Metric count: 0
  Event count: 0
Unfortunately there is not an official Go Datadog API.
I am currently using this one instead  https://github.com/zorkian/go-datadog-api .
Datadog forked the first version of it and recommend using it.
I am able to connect to my Dashboard:
But I do not know how to send create/track an event.
This is my current approach but if fails badly.
From my understanding and the missing documentation, I would have to fill out some of these variables in this struct ( https://github.com/zorkian/go-datadog-api/blob/master/events.go )
Can you please help me with that?
I’m getting a connection refused from my datadog-agent that is trying to collect JMX (via RMI) metrics from an in-house application that exists in its own docker container.
However, jconsole is able to collect the metrics from the application that exists within its own docker container.
The datadog-agent exists within a container of its own.
Both containers exist within the same network on the same host.
Any ideas?
I have looked at the other stack overflow questions.
Docker Container 0: 
* Runs the my_streams_app that outputs kafka streams metrics 
* Executed via:
Docker Container 1: 
* Runs datadog-agent within container
* Datadog-agent uses JMX default (RMI) to fetch the metrics from my_streams_app that exists in container 0, above.
* both containers run on the same network within the same host (my laptop MAC OSX) 
* able to netcat from within datadog-agent in docker container to the my_streams_app ip and port in the other container.
Using 0.0.0.0 and 9998, can also use specific IP addresses 
* command to run the datadog agent from within a container
jmx configuration for collecting metrics by datadog jmx from within the container:
instances:
docker_images:
        - my_streams_app
init_config:
    is_jmx: true
    conf:
        - include:
            domain: '"kafka.streams"'
            bean: '"kafka.streams":type="stream-metrics",client-id=“my_test-1-StreamThread-1"'
            attribute:
                commit-calls-rate:
                    metric_type: gauge
                commit-time-avg: 
                    metric_type: gauge
                commit-time-max:
                    metric_type: gauge
                poll-calls-rate:
                    metric_type: gauge
JConsole: 
* collects metrics from my_streams_app within the docker container 0, above via:
Error output:
rmiregistry has been started as per  Failed to retrieve RMIServer stub
Iam using a puppetized datadog agent install(version 1.10.0), and looks like i'm on default dd-agent version - 5.9.1.
Would like to upgrade.
I dont see any docs related to that.
Can someone point me on how to do this?
I'm attempting to query our DATADOG hub and display some metric graphs.
However, it appears the default way to do this is using an embed script generated by DATADOG and utilizing that in your app.
I'm actually wanting to draw the graphs on my side, using their API data so I'm better able to control the size, look and flexibility of the graphs.
Is this something that is possible?
Rather new to DATADOG and everything seems to be done in an iFrame, which I do not want.
Additionally, I found a package which I believe may be of use for Node?
:  http://brettlangdon.github.io/node-dogapi/#embed-create
D.D.
Graphs Docs:  http://docs.datadoghq.com/api/#graphs
Any advice would be greatly appreciated, I have not seen anything similar on S.O.
Graphing Primer  and  Getting started with tags – Datadog  suggest that Tags are  the  way to filter data, but the latter article cautions:
Please don't include endlessly growing tags in your metrics, like timestamps or user ids.
Please limit each metric to 1000 tags.
So, if I want to filter by user id, how can I do that without using Tags?
There are logs with &quot;events&quot; that have attributes such as names, statuses, etc as well as &quot;amount&quot; which corresponds to a dollar amount.
I'm sending requests to datadog's timeseries api to get back data about these logs.
I'm starting with a base query:
 sum:events{event_name:reload,event_result:success,event_environment:main,event_market:gb}.as_count()
This query just returns the total number of these &quot;reload&quot; events that have occurred in the timeframe.
Each of these events has an attribute &quot;amount&quot; which I want returned instead.
I can't figure out how to format the syntax for the query to get it to return the sum of these amounts instead of the sum of the occurrences of events.
Here are some queries I have tried which do not work:
I've had difficulty finding anything about this in the datadog documentation.
If anyone understands how to perform this with datadog's query syntax I would very much appreciate the help.
Thanks
I've been using the Datadog dashboard and I really like how strong and flexible it is.
I am wondering what type of API they are using
Here's a  list  of DataDog metrics of Azure Load Balancer that are available to use.
It seems like
are the most releveant.
When SNAT port resources are exhausted, outbound flows fail.
You could observe failing outbound connections or are advised by support that you're exhausting SNAT ports.
Simply seeing failed connections does not confirm SNAT exhaustion.
Seeing the failed connections was a clue we were having an issue but there is no way to confirm SNAT exhaustion w/o a ticket to Microsoft it turns out.
We have a VM with DataDogAgent in azure, on this VM we host  ASP.Net core API  application.
We collect these kind of metrics:
Here is the code of stub API:
In data dog dashboard these 2 kind of metrics are stacked together for one request into one trace:
After application migrated to  Azure AppService ( .NET Datadog APM  is used to collect metrics here), spans are not stacked into one trace and appears in dashborad as separate traces.
How to merge them into one trace in  Azure App Service ?
I'm using datadog to monitor the health of several pods deployed in a kubernetes cluster.
I use a query like this to check the pods
If I stop the pod, there ins't any data for kubernetes.pods.running (so the value is not zero, I don't have any value) .
I don't know if it's possible to check from datadog that no pods has kube_service running.
Using the @datadog/browser-rum package from  RUM Browser Monitoring , with Node apollo web client fails on IE11.
Seeing multiple requests to  https://run-http-intake.logs.datadoghq.com  on IE11 that are different than Chrome.
Unable to login to the application on IE11.
Working theory is that datadogRUM is blocking other application requests on IE11.
When datadogRUM is removed, the application is working correctly.
Chrome and IE11 requests:
Request Method: POST  RequestURL:  https://rum-http-intake.logs.datadoghq.com/v1/input/pub{id}?_dd.application_id={id}&amp;ddsource=browser&amp;&amp;ddtags=sdk_version:1.25.2.env:local&amp;batch_time={timestamp}
Only IE11 request:
Request Method: CONNECT RequestURL:  https://rum-http-intake.logs.datadoghq.com  Proxy-Connection: Keep-Alive
Please let me know what additional information I should find to help with this issue.
IE11 issues are no fun.
Thank you!
I'm creating alerts in Prometheus and migrating from Datadog.
I have two metrics queries that I'm not able to understand yet.
In this query I understand the  avg:default.burrow_kafka_consumer_lag_total{*} by {consumer_group,env}  part but not the rest and how to translate it to PromQL.
Second
I don't understand the rollup part, how to translate it into PromQL?
how is  pct_change(avg(last_1h),last_1h)  part of the query?
I'm new to this.
I have translated other queries but these I don't understand.
We are using DataDog's Distributed tracing in a rails application and would like to write the trace_id (for a controller#action) so that we could access the url later to our Rails logs.
How could I do this?
something like?
:
I would like to integrate my  Sanic==19.9.0  application with Datadog  ddtrace==0.34.1  tracer.
So far I have following code:
But trace logs are empty:
What do I do wrong?
Very new to Datadog and need some help.
I have crafted 2 SQL queries (one for on-prem database and one for cloud database) and I would like to run those queries through Datadog and be able display the query results and validate that the daily results fall within an expected variance between the two systems.
I have already set up Datadog on the cloud environment and believe I should use DogStatsD to create a custom metric but I am pretty lost with how I can incorporate my necessary SQL queries in the code to create the metric for eventual display on a dashboard.
Any help will be greatly appreciated!!
!
For example, suppose I have a Node library that I could use something like:
The few I've looked at all seem to be good for posting new data to Datadog, however, are there any that can pull data from Datadog?
Does anything like this exist?
I like the  Datadog API  but it seems as though that is only in Curl, Python, and Ruby.
I'm trying to monitor several applications within the same site in IIS.
With just running the  msi  of the tracer  dd-trace-dotnet , I started to see the events, but these are registered as  [site name]/[application]  e.g  default_web_site/docs_webhook 
I would love to be able to logs them under a custom service name for each application, but according to the  documentation , this is only possible at the site level.
Manual instrumentation is described for windows services, setting the environment variable  DD_SERVICE_NAME  in the registry entry  HKLM\System\CurrentControlSet\Services\{service name}\Environment  is enough, but does not apply to IIS applications.
NOTE: Creating separate sites for each application is not an option right now.
I'm using a  statsd.timed  to send some time metrics to datadog.
These metrics are being used in a few Datadog dashboards.
Changing the metric name being sent is straightforward and can be done by updating the name of the metric in the statsd.timed call/decorator in the code, however, the old metric name may already be in use in existing datadog dashboards.
Is there a quick and easy way to rename a metric in Datadog so that all dependencies such as Dashboards using the metric are also updated, without having to go through each dashboard and updating them independently?
I currently have a program that allows me to create and post dashboards to Datadog programmatically.
Using the API functions  here , I was successfully able to create, update, and remove dashboards as I please.
However, now I'd like to extract the skeleton of existing dashboards that I have already created from Datadog to see what has been added or removed.
To do this, I need to figure out how to send the API key along with a request.
I have no problem getting the higher level information about the boards, but I'd like to go a step further.
This is what I get by calling  api.ScreenBoard.get_all()
Now, the end goal is simply to pull JSON from the "resource" link given from this command.
I've tried to use urllib and urllib2 to merge that link with the host site (like  https://www.foo.com/{resource-link} ), but I keep getting the following results:
OR
The code that triggered this error is:
As you can see, my "data" variable returns the error.
So, all I need is to figure out how to send the API key along with my request to resolve the issue.
If anyone knows how to perform this task, I would really appreciate it.
Is there a way to display a service version (as a string) in Datadog?
(Like sending an API request with the text to display)?
Does  kafka.messages_in.rate  represents the number of events from producer to broker or it also includes the replication events from other brokers.
The official doc is useless just presents the same metric in plain English without an possible explanation
I am trying to install the Datadog agent on windows using powershell only, not manualHowever, the APIKEY is not being setup.
Is there a way to update/set the APIKEY after installation?
I am trying to integrate activemq with datadog.
I have modified /Users//.datadog-agent/conf.d/activemq_58.yaml.
Changes are:
instances:
   - host: localhost
     port: 8161
      user: admin
      password: admin
activemq is running in localhost at default port with jmx enabled.
Restarted datadog agent 
I could see error after running info command.
Error is
activemq_58
Can anybody suggest that why I am getting this error?
In my case scenario, Flink is sending the metrics to Datadog.
Datadog Host map is as shown below { I have no Idea why is showing me latency here }
Flink metrics are sent to localhost.
The issue here is that when
flink-conf.yaml  file configuration is as follows
The issue is that Datadog is showing 163 metrics which I don't understand,  which I will explain in a while
I don't understand the metrics format in datadog as it shows me metrics something like this
Now as shown in above Image
So my question is that which metric is this?
Also, the execution plan of my job is something like this
How do  I relate the metrics in Datadog with execution plan operators in Flink?
I have read in   Flink API 1.3.2  that I can use tags, I have tried to use them in flink-conf.yaml file but I don't have complete Idea what sense they make here.
My ultimate goal is to find operator latency, number of records out and in /second at each operator in this case
Just wanted to check weather Datadog agent is installed in UNIX box or not.
I ran a command  sudo /etc/init.d/datadog-agent status  but got below output
 sudo: /etc/init.d/datadog-agent: command not found
Please advice
I'm trying to get DataDog to display a dashboard of system information.
One piece of information is the percentage of time the system is reading/writing from the disk expressed in the metrics  system.disk.read_time_pct  and  system.disk.write_time_pct
However, when I put this graph on my dashboard it shows some parts at well over 5000%, which clearly can't be right.
As you can see from the preview above, it is showing a disk read time of 5430%.
If I constrain the Y-axis to 100 it regularly goes above 100%.
I can't find anything to explain this or how to graph it properly.
So, how do I properly graph  system.disk.read_time_pct  and  system.disk.write_time_pct  with DataDog?
I use  dropwizard metrics  with  metrics-datadog .
Create reported like this:
But there is no host(server name) param in datadog.
How can I add host (server name) for metrics to filter them in datadog control panel?
Metrics from default datadog agent has server name attribute.
I have a server that processes packets from different devices.
Devices can report in different intervals.
I would like to make a chart showing the distribution of intervals by the count of devices (how many devices are reporting within 5 sec/10 sec/60 sec ...)
Intervals for each device can vary.
Now I'm sending metric with  Set  using deviceId with tags that represent interval (5 sec, 10 sec, 30 sec, and more) but I'm not sure that it is correct.
What is the best way to realize it?
I'm using  this built-in dashboard  for monitoring Aurora and was wondering how can I have as code, as cloud formation stack precisely.
I'm aware of those three repos which do backup and monitoring of changing of the dashboard in the API and then commit back to GitHub, but I only want to export it once.
I have an API with 10 endpoints(contracts).
and i am shipping logs to IIS to data-dog from the API.
I also installed data-dog agent on the server.
Now i am trying to create graph for all the endpoint hits per second.
there will be only one graph and all the endpoints TPS will be shown on the graph.
How can I achieve this?
any suggestions?
I tried to create different matrix but not able to achieve this.
I've read that parser file needs to be created.
Can any one help me in getting the Datastax driver Metrics to Datadog.
Tried to search but no luck and was not able find any.
The following are the  metrics  that we need to se in the dataDog.
So I use  StatsD  to receive all my metrics from my service, but unfortunately Datadog can't seem to pick up those, how can I debug it?
I think it'd be great if there's  metrics  endpoint or something for  StatsD  such that I'd be able to curl it and see which metrics has been collected so far.
I am getting this stacktrace when running a go program:
The signature of the Event function is:
which can also be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L285
The type definition for  Event  can be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L333
The type definition for  Client  can be seen here:  https://github.com/DataDog/datadog-go/blob/cc2f4770f4d61871e19bfee967bc767fe730b0d9/statsd/statsd.go#L59
My question is, how do I interpret the memory addresses on this line, and more generally, any stack traces which involve typed variables as targets and as arguments?
When I looked at  http://www.goinggo.net/2015/01/stack-traces-in-go.html  (which is the only information I was able to find on the subject), I didn't see anything about how to interpret the output when structs were involved.
We have a counter metric in one our micro services which pushes data to DataDog.
I want to display the total count for given time frame, and also the count per day (X axis would have the date and Y axis would have count).
How do we achive this?
I tried using  sum by  and  diff  with Query value representation.
It gives the total number of the count for given time frame.
But I would like to get a bar graph with the X axis as the date and the Y axis as the count.
Is this possible in DataDog?
I want to control the reading and writing speed to an RDB by  Spark  directly, yet the related parameters as the title already revealed seemingly were not working.
Can I conclude that  fetchsize  and  batchsize  didn't work with my testing method?
Or they do affect on the facet of reading and writing since the measure result is reasonable based on scale.
I created two  m4.xlarge  Linux entities on  AWS , one is for the execution of  Spark , the other is for data storage on an RDB, using  Datadog  to watch the performance of the  Spark  application, especially on the reading and writing to the RDB.
Spark  was in the standalone mode, and the application for test is simply pulling some data from a  MySQL  RDB, doing some computation, then pushing back to the  MySQL .
Some details are as the following:
JDBC properties are put in a file,  application.conf , like the following:
Logging while executing the application is enabled by  log4jx2 , within it, time for writing is measured.
The problem:
I have three instances of a java application running in Kubernetes.
My application uses Apache Camel to read from a Kinesis stream.
I'm currently observing two related issues:
Each of the three running instances of my application is processing the records coming into the stream, when I only want each record to be processed once (I want three up and running for scaling purposes).
I was hoping that while one instance is processing record A, a second could be picking up record B, etc.
Every time my application is re-deployed in Kubernetes, each instance starts every record all over again (in other words, it has no idea where it left off or which records have previously been processed).
After 5 minutes, the shard iterator that my application is using to poll kinesis times out.
I know that this is normal behavior, but what I don't understand is why my application is not grabbing a new iterator.
This screenshot shows the error from DataDog.
What I've tried: 
First off, I believe that this issue is caused by inconsistent shard iterator ids, and kinesis consumer ids across the three instances of my application, and across deploys.
However, I have been unable to locate where these values are set in code, and how I could go about setting them.
Of course, there may also be a better solution altogether.
I have found very little documentation on Kinesis/Kubernetes/Camel working together, and so very little outside sources have been helpful.
The documentation on  AWS Kinesis :: Apache Camel  is very limited, but what I have tried playing around with the iterator type and building a custom  Client Configuration .
Let me know if you need any additional information, thanks.
Configuring the client:
My route:
Edit: Tarun's answer does exactly what I asked for.
Eugen's answer is also a very good solution.
I ended up accepting Tarun's answer as correct, but using Eugen's.
If you have a similar issue and are worried about other containers accessing the nginx status server, use Tarun's answer.
If you'd rather stick to Docker's normal hostname scheme, use Eugen's.
+++ Original Question +++
I have an application that I build with docker-compose.
I am trying to integrate monitoring through DataDog.
I'm using DataDog's Agent container, and so far everything is working.
I am trying to get nginx monitoring up and running by adapting  this tutorial .
My application is defined in a docker-compose file like this:
Per the tutorial, I've added a server block to nginx that looks like this:
With this configuration, I can check the nginx status from within the nginx container.
So far, so good.
Now I would like to change the "allow" directive in the location block to allow access to the datadog-agent service only.
However, I don't know the IP of the datadog-agent.
When configuring access to the Flask uwsgi server, I was able to use directives like this:
But this doesn't seem to work for allow directives; if I try:
I get the following error:
How can I safely expose the nginx status to my monitoring container?
I am trying to connect to a MySQL database using python but I am getting a strange error.
It is compounded by the fact that I can use the same connection values from the  mysql  console command and it connects with no problems.
Here is the exact code I am using:
import pymysql
    from checks import AgentCheck
This is the error that I am getting:
I am running this code on a Ubuntu box and I though initially that it might be because the SSL CA is a self generated cert.
So I followed the steps  here  But, it did not make any difference.
Also I have verified that the process that is running this code has full access to the cert files
Any ideas what else might be causing this?
Datadog Tracing API  requires 64-bit integers serialized as JSON numbers.
How can I create JSON with 64-bit integer numbers using JavaScript?
I have a graphql server with multiple endpoints.
It is basically just a CRUD app, so I'm honestly not sure why there's a memory leak.
The only potentially leaky endpoint I have is one that uploads pics to S3.
I've been looking around and have tried taking heap snapshots and comparing them but I'm not even sure which endpoint is the culprit.
This is the flow I've been following:
Is this the correct flow for finding a memory leak?
Is there a better way of doing this without having to guess which endpoint it is coming from?
Are there perhaps tools I can use online that can help me find the source of the memory leak in production without having to guess like this?
Perhaps something like Datadog or something?
Update: From Heroku's metrics, it looks like the memory usage increases every time a request is made?
But my src/index.js file doesn't do anything special:
I'm trying to setup a Stackdriver dashboard for my custom metrics that my services provide.
In particular I'm starting with general  custom/grpc/time_ms  metric that is a gauge and have  status  label on it.
I'd love to be able to set up a chart and alert for success rate of the metric(something like  count:custom/grpc/time_ms{status:OK} / count:custom/grpc/time_ms{*} ).
With my previous project I used Datadog and it was  pretty easy to do so there .
But I don't see any similar functionality neither in the UI nor in Stackdriver documentation.
So I was wondering if it's not documented or simply not supported?
In my  config\environments\development.rb  and  config\environments\production.rb  files, I set some global variables.
In the example below, I have a a  Redis  instance that points to our cache, and a  Statsd  instance that points to the DataDog agent.
In the case of Redis, I added  gem 'redis'  to my gem file, ran  bundle install  and everything worked fine.
In the case of StatsD, however, it seems that I need to also add  require 'statsd'  at the top of the  development.rb  and  production.rb  files in order to be able to create the instance.
Of course, I also added  gem 'dogstatsd-ruby'  to my gem file and ran  bundle install , but that didn't seem to be enough.
If I don't add the  require  statement at the top of the config files, I get the following error when I try to run my Rails app:
Can anyone explain why I have to add the  require  statement only in this particular case (StatsD), or is there is a better way to do this?
Thanks!
Note: according to tests (see Edit below), this occurs only on a Linux machine.
I have an ASP.NET Core Blazor application (using server-side hosting model) running on a Raspberry Pi.
Part of the application's functionality is to dim/brighten screen based on when system was last interacted with.
To do that, every 1 second or so I spawn a terminal child-process to run  xprintidle , parse its output, and act accordingly.
I use DataDog for monitoring, and I am having a memory leak until the system crashes (it takes a few days to use up all memory, but it does occur eventually):
I have pinpointed that the following method is what leaks memory - if I skip calling it and use some constant timespan, the memory does not leak:
I have following code to do so:
I spent a lot of time (over span of months - literally) trying various changes to solve this issue -  WaitForExitAsync  was overhauled a lot, tried different ways of disposing.
I attempted to call GC.Collect() periodically.
Also tried running the application with both Server and Workstation GC mode.
As I mentioned earlier, I am pretty sure it's this code that leaks - if I don't call  ExecuteAndWaitAsync , there's no memory leak.
The result class is also not stored by the caller - it simply parses a value and uses it right away:
Am I missing something?
Is it Process class leaking, or the way I read output?
EDIT : As people in comments asked, I created minimum runnable code, basically fetching all relevant methods in a single class and execute in a loop.
The code is available as a gist:  https://gist.github.com/TehGM/c953b670ad8019b2b2be6af7b14807c2 
I ran it both on my Windows machine and Raspberry Pi.
On Windows, memory seemed stable, however on Raspberry Pi it was clearly leaking.
I tried both  xprintidle  and  ifconfig  to make sure it's not an issue with xprintidle only.
Tried both .NET Core 3.0 and .NET Core 3.1, and effect was largely the same.
I have a springboot application which I'm trying to instrument using bytebuddy.
I'm running into classpath issues which I'm not able to understand.
Firstly, the following is other literature on this:
https://github.com/raphw/byte-buddy/issues/473
https://github.com/raphw/byte-buddy/issues/87
Unable to instrument apache httpclient using javaagent for spring boot uber jar application
https://github.com/raphw/byte-buddy/issues/109
https://github.com/raphw/byte-buddy/issues/473
https://github.com/raphw/byte-buddy/issues/489
https://github.com/spring-projects/spring-boot/issues/4868
https://github.com/alibaba/transmittable-thread-local/issues/161
Problem is that Spring-boot bundles the application into one uber-jar, which contains other jars inside it
A thing to note here is, that if I run the application using IntelliJ, it doesn't use the uber-jar and runs via main class with a bunch of jars as classpath arguments.
Due to this difference, When running via uber-jar( java -jar target/demo-0.0.1-SNAPSHOT.jar ), some classes are not available at the time the Agent runs.
The classes are loadable at the time of application main, as spring-boot uses its own classloader, which is created at some time b/w agent and application main methods.
I'll describe the behaviour at various points of time below:
Both spring and javax classes are not loaded as they are not directly in the classpath as per the App Classloader.
It's part of an inner jar, something like  app.jar!/BOOT-INF/lib/some.jar 
App Classloader won't be able to load this.
After loading,  Class.forName("javax.servlet.http.HttpServletRequest").classLoader  is also the same  LaunchedURLClassLoader .
I have two classes,  SpringBootInterceptor  (which contains the intercept method) and  SpringBootInterceptorOne  (which contains the entry and exit method)
The function  intercept()  is called in the premain path.
The function  visit  is called when the type is actually attempted to load.
At that point bytebuddy tris to intercept those methods and modify bytecode.
In the current version of this code, the function  intercept()  throws  ClassNotFoundException  and interception doesn't happen.
So, I tried to change the code, to the following:
Here, basically loading the class lazily.
Helper functions from datadog:
https://github.com/DataDog/dd-trace-java/blob/master/dd-java-agent/agent-tooling/src/main/java/datadog/trace/agent/tooling/ByteBuddyElementMatchers.java
After doing this, the  intercept  function doesn't fail, but the  visit()  function behaves strangely:
At this point,
Next, I tried the following:
This causes the interceptor to be "installed".
The entry and exit methods are called okay.
But casting the arguments to proper types gives error for the types of the arguments
Here is SpringBootInterceptorOne for reference:
UPDATE:
As per the advice offered in an answer, I tried the following:
It again throws Error:
I have a project built in Lumen (php Framework) hosted on a docker container built from alpine as base image using apache2 server with php 7.x
Here's part of my Dockerfile:
The purpose of this project is to receive http post requests (i.e.
webhook events from external system) and process them.
When the project is deployed, it runs fine for several days before this error starts showing up in our datadog logs:
[core:warn] [pid 9] (99)Address not available: AH00056: connect to listener on [::]:80
When this error occurs, the site/project is not publicly accessible but the apache is still running.
If i restart the container, everything goes back to normal.
Upon investigating this further, I noticed this happens every time my api is hit simultaneously.
I.e.
3 days ago, project was hit with 145 request simultaneously and since then the app is no longer accessible.
Apache is refusing to serve any new request but the container is up and running and there's plenty of memory/disk-space available to the container.
Any idea what causes this?
do I need to optimise the mpm.conf to allow for more workers / child processes etc.?
I am currently using stock config.
I'm trying to learn how to use docker and am having some troubles.
I'm using a  docker-compose.yaml  file for running a python script that connects to a mysql container and I'm trying to use  ddtrace  to send traces to datadog.
I'm using the following image from  this github page from datadog
And my  docker-compose.yaml  looks like
So then I'm running the command  docker-compose run --rm ddtrace-test python test.py , where  test.py  looks like
And when I run the command, I'm returned with
I'm not sure what this error means.
When I use my key and run from local instead of over a docker image, it works fine.
What could be going wrong here?
Are there some broker metrics we can use to monitor Kafka broker if acknowledgment lag is very high in the producer side.
We are using datadog to monitor producer and Kafka broker side.
It can be seen that the producer ack lag is more than 10 secs.
However, on the broker side, I feel like only using  message.in.rate  and  kafka.net.bytes_in.rate  are not very efficient.
It would be better we can have some LAG metrics in the broker side to indicate  the broker is fully loaded to acknowledge back the producer.
Also, we only use  kafka.acks = 1  for partition leader.
I wonder does anyone has some experience about it and any advice is welcome.
:) Thanks in advance.
I have an agent (datadog agent but could be something else) running on all the nodes of my cluster, deployed through a DaemonSet.
This agent is collecting diverse metrics about the host: cpu and memory usage, IO, which containers are running.
It can also collect custom metrics, by listening on a specific port 1234.
How can I send data from a pod to the instance of the agent running on the same node than the pod?
If I use a Kubernetes service the calls to send the metric will be load balanced across all my agents and I'll lose the correlation between the pod emitting the metric and the host it's running on.
I'd like to use Airflow with Statsd and DataDog to monitor if DAG takes e.g.
twice time as its previous execution(s).
So, I need some kind of a real-time timer for a DAG (or  operator ).
I'm aware that Airflow supports  some metrics .
However, to my understanding all of the metrics are related to finished tasks/DAGs, right?
So, It's not the solution, because I'd like to monitor running DAGs.
I also considered the  timeout_execution / SLA  features, but they are not suitable for this use-case
I'd like to be notified that some DAG hangs, but I don't want to kill it.
What is the difference between "hikaricp.connections. "
and "jdbc.connections. "
meter names?
I have a Spring Boot 2 application that is defaulting to the Hikari connection pool mechanism and I am tring to understand how to best monitor the database connections in production.
After visualizing my metrics in Datadog, I am seeing a slight difference in the metric data for both hikariCP.connections.active and jdbc.connections.active.
Are the JDBC meter names duplicates?
Should one be used over the other or does it not matter.
I have been struggling to find more detailed documentation on this.
Any help is much appreciated.
My task definition:
My service defintion:
I get the following error -
Is there something I am missing here?
Looking at the Terraform docs and GitHub issues this should have worked.
Is it related to running Datadog as a daemon?
I trying to run 3 containers and get the error for all of them.
I found the source code, but can't find the answer to why this is happening.
Also, tried to look at the internet and can't find anyone with the same error.
Source code:
 https://github.com/DataDog/datadog-agent/blob/eb35254e9e13165b4148fc9280ef79e2d6bf8235/pkg/util/docker/containers.go
The error from /var/log/syslog:
process-agent[30759]: 2019-12-11 08:58:17 UTC | PROCESS | ERROR |
  (pkg/util/docker/containers.go:110 in ListContainers) | Failed to get
  host IPs.
Container XXXXX will be missing network info: %!s()
The containers running on the Host network.
Thanks
We developed our team's new service with spring-webflux.
It has been working well.
Only one thing we could not figure out is below log
Error [reactor.netty.ReactorNetty$InternalNettyException: io.netty.channel.ExtendedClosedChannelException] for HTTP GET "[TARGET_URL]", but ServerHttpResponse already committed (200 OK)
Although it does not frequently happen, it is logged in our datadog log.
The service is a kind of middle man.
One service sends "get" request to it, and then it calls other backends to gather data, then return.
In the service layer, we use Mono.zip to combine all response from various backends.
Both clients and backends are traditional thread base sprint applications.
Our code to build webclient is:
connection timeout is set to 2500 milliseconds.
read timeout is 50-90ms depends on backends.
Thanks!
Docker daemon got crashed after short span of time.
Lately Docker services of stacks doesn't get properly up and resulted in app crash which only got fixed when i removed all the stacks and redeployed them.
I'm running my whole Android app and Other APIs on docker swarm cluster.
*I have my machine running on Google Cloud platform with around 75 CPUs and 250G memory which is more than enough for all the services I'm running on my machine.
I have haproxy in frontend which does reverse proxy, backend as python flask api with 5 replicas, Database connectivity through pgbouncer.
Else Logspout, datadog, portainer, redis, etc.
*
I couldn't understand that even if i have enough resources, proper setup system with enough max_pids still the daemon crashed.

Generally Necessary:
- cgroup hierarchy: properly mounted [/sys/fs/cgroup]
apparmor: enabled and tools installed
CONFIG_NAMESPACES: enabled
CONFIG_NET_NS: enabled
CONFIG_PID_NS: enabled
CONFIG_IPC_NS: enabled
CONFIG_UTS_NS: enabled
CONFIG_CGROUPS: enabled
CONFIG_CGROUP_CPUACCT: enabled
CONFIG_CGROUP_DEVICE: enabled
CONFIG_CGROUP_FREEZER: enabled
CONFIG_CGROUP_SCHED: enabled
CONFIG_CPUSETS: enabled
CONFIG_MEMCG: enabled
CONFIG_KEYS: enabled
CONFIG_VETH: enabled (as module)
CONFIG_BRIDGE: enabled (as module)
CONFIG_BRIDGE_NETFILTER: enabled (as module)
CONFIG_NF_NAT_IPV4: enabled (as module)
CONFIG_IP_NF_FILTER: enabled (as module)
CONFIG_IP_NF_TARGET_MASQUERADE: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_ADDRTYPE: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_CONNTRACK: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_IPVS: enabled (as module)
CONFIG_IP_NF_NAT: enabled (as module)
CONFIG_NF_NAT: enabled (as module)
CONFIG_NF_NAT_NEEDED: enabled
CONFIG_POSIX_MQUEUE: enabled
Optional Features:
CONFIG_USER_NS: enabled
CONFIG_SECCOMP: enabled
CONFIG_CGROUP_PIDS: enabled
CONFIG_MEMCG_SWAP: enabled
CONFIG_MEMCG_SWAP_ENABLED: missing
(cgroup swap accounting is currently enabled)
CONFIG_LEGACY_VSYSCALL_EMULATE: enabled
CONFIG_BLK_CGROUP: enabled
CONFIG_BLK_DEV_THROTTLING: enabled
CONFIG_IOSCHED_CFQ: enabled
CONFIG_CFQ_GROUP_IOSCHED: enabled
CONFIG_CGROUP_PERF: enabled
CONFIG_CGROUP_HUGETLB: enabled
CONFIG_NET_CLS_CGROUP: enabled (as module)
CONFIG_CGROUP_NET_PRIO: enabled
CONFIG_CFS_BANDWIDTH: enabled
CONFIG_FAIR_GROUP_SCHED: enabled
CONFIG_RT_GROUP_SCHED: missing
CONFIG_IP_NF_TARGET_REDIRECT: enabled (as module)
CONFIG_IP_VS: enabled (as module)
CONFIG_IP_VS_NFCT: enabled
CONFIG_IP_VS_PROTO_TCP: enabled
CONFIG_IP_VS_PROTO_UDP: enabled
CONFIG_IP_VS_RR: enabled (as module)
CONFIG_EXT4_FS: enabled
CONFIG_EXT4_FS_POSIX_ACL: enabled
CONFIG_EXT4_FS_SECURITY: enabled
Network Drivers:
"overlay":
CONFIG_VXLAN: enabled (as module)
CONFIG_BRIDGE_VLAN_FILTERING: enabled
Optional (for encrypted networks):
CONFIG_CRYPTO: enabled
CONFIG_CRYPTO_AEAD: enabled
CONFIG_CRYPTO_GCM: enabled
CONFIG_CRYPTO_SEQIV: enabled
CONFIG_CRYPTO_GHASH: enabled
CONFIG_XFRM: enabled
CONFIG_XFRM_USER: enabled (as module)
CONFIG_XFRM_ALGO: enabled (as module)
CONFIG_INET_ESP: enabled (as module)
CONFIG_INET_XFRM_MODE_TRANSPORT: enabled (as module)
"ipvlan":
CONFIG_IPVLAN: enabled (as module)
"macvlan":
CONFIG_MACVLAN: enabled (as module)
CONFIG_DUMMY: enabled (as module)
"ftp,tftp client in container":
CONFIG_NF_NAT_FTP: enabled (as module)
CONFIG_NF_CONNTRACK_FTP: enabled (as module)
CONFIG_NF_NAT_TFTP: enabled (as module)
CONFIG_NF_CONNTRACK_TFTP: enabled (as module)
Storage Drivers:
"aufs":
CONFIG_AUFS_FS: enabled (as module)
"btrfs":
CONFIG_BTRFS_FS: enabled (as module)
CONFIG_BTRFS_FS_POSIX_ACL: enabled
"devicemapper":
CONFIG_BLK_DEV_DM: enabled
CONFIG_DM_THIN_PROVISIONING: enabled (as module)
"overlay":
CONFIG_OVERLAY_FS: enabled (as module)
"zfs":
/dev/zfs: missing
zfs command: missing
zpool command: missing
Limits:
NEED HELP ON UNDERSTANDING THE ISSUE HERE.
RESULT OF  docker info  here.

Containers: 170
Running: 167
Paused: 0
Stopped: 3
Images: 144
Server Version: 18.09.7
Storage Driver: overlay2
Backing Filesystem: extfs
Supports d_type: true
Native Overlay Diff: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins:
Volume: local
Network: bridge host macvlan null overlay
Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
Swarm: active
NodeID: uqkfe247qfql50b1bft3r205b
Is Manager: true
ClusterID: fe79jmqus0l6zsa7kl41cbqa9
Managers: 1
Nodes: 2
Default Address Pool: 10.0.0.0/8
SubnetSize: 24
Orchestration:
Task History Retention Limit: 5
Raft:
Snapshot Interval: 10000
Number of Old Snapshots to Retain: 0
Heartbeat Tick: 1
Election Tick: 10
Dispatcher:
Heartbeat Period: 5 seconds
CA Configuration:
Expiry Duration: 3 months
Force Rotate: 0
Autolock Managers: false
Root Rotation In Progress: false
Node Address: 10.160.0.30
Manager Addresses:
  10.160.0.30:2377
Runtimes: runc
Default Runtime: runc
Init Binary: docker-init
containerd version:
runc version: N/A
init version: v0.18.0 (expected: fec3683b971d9c3ef73f284f176672c44b448662)
Security Options:
apparmor
seccomp
Profile: default
Kernel Version: 4.15.0-1040-gcp
Operating System: Ubuntu 18.04.3 LTS
OSType: linux
Architecture: x86_64
CPUs: 76
Total Memory: 246GiB
Name: rc-manager-instance
ID: 2PEM:4AF6:47RA:EMDM:CIMD:H4OC:5MNG:SXNI:ERFB:ML5G:O3YI:6VWA
Docker Root Dir: /var/lib/docker
Debug Mode (client): false
Debug Mode (server): false
Registry:  https://index.docker.io/v1/
Labels:
Experimental: false
Insecure Registries:
10.160.0.30:7000
127.0.0.0/8
Live Restore Enabled: false
i am trying to develop generic terraform modules to support data-dog monitors and let the user of the modules to append resources and/or override resources in side generic modules.
terraform  overrides  feature works fine without modules, But not working when using modules.
how to override some of the resource parameters inside modules?
Requirements:
/modules/datadog/monitors.tf  contains list of resources, each resource represents a generic datadog monitor with default parameter values.
Each  individual application may choose to override one or more parameters inside each resource .
/application-1/monitors.tf  contains module with source as  /modules/datadog/  , some more monitors that are not covered in generic monitors and some variables.
/application-1/monitors.tf
/modules/datadog/monitors.tf
Solution 1 : Add overrides.tf to  /modules/datadog  Directory.
terraform  override feature  merges content in overrides.tf to 
 configuration defined in monitors.tf.
But the problem with this solution is each application specific overrides.tf needs to be copied over to /modules/datadog Directory before running apply command.
overrides.tf
Solution 2 : can i use overrides with modules?
i tried to override resource parameters by copying overrides.tf to /application-1/ Directory, But terraform is not overriding resources, instead it is considering both as different resources.
When I start consumer with single instance it is show in consumer group but it is not consuming data from topic.
After that if I start another consumer and my first consumer start consuming data but latest consumer instance doesn't have any partition assigned to it.
below is the info log when first consumer instance starts.
INFO:kafka.client:Bootstrapping cluster metadata from [(u'kafka-broker1.ap-south-1.staging.internal', 9092, 0)]
  INFO:kafka.conn:: connecting to 172.31.1.66:9092
  INFO:kafka.client:Bootstrap succeeded: found 3 brokers and 19 topics.
INFO:kafka.conn:: Closing connection.
INFO:kafka.conn:: connecting to 172.31.1.148:9092
  INFO:kafka.conn:Broker version identifed as 0.11.0
  INFO:kafka.conn:Set configuration api_version=(0, 11, 0) to skip auto check_version requests on startup
  INFO:kafka.consumer.subscription_state:Subscribing to pattern: /events/
  INFO:kafka.conn:: connecting to 172.31.1.70:9092
  INFO:kafka.cluster:Group coordinator for datadog is BrokerMetadata(nodeId=1, host=u'kafka-broker1.ap-south-1.staging.internal', port=9092, rack=None)
  INFO:kafka.coordinator:Discovered coordinator 1 for group datadog
  INFO:kafka.conn:: connecting to 172.31.1.66:9092
  INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set([]) for group datadog
  INFO:kafka.coordinator:(Re-)joining group datadog
  INFO:kafka.consumer.subscription_state:Updating subscribed topics to: [u'events']
  INFO:kafka.coordinator:Joined group 'datadog' (generation 843) with member_id kafka-python-1.3.5-e3c25fb3-39ea-4550-845f-9b663355b4f5
  INFO:kafka.coordinator:Successfully joined group datadog with generation 843
  INFO:kafka.consumer.subscription_state:Updated partition assignment: []
  INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([]) for group datadog
When I start second instance first instance get partition assigned and second instance have 0 partition assigned and have same info log as first instance before starting second instance.
below is the info log of first instance after second instance started.
INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([]) for group datadog
  WARNING:kafka.coordinator:Heartbeat failed for group datadog because it is rebalancing
  WARNING:kafka.coordinator:Heartbeat failed ([Error 27] RebalanceInProgressError); retrying
  INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set([]) for group datadog
  INFO:kafka.coordinator:(Re-)joining group datadog
  INFO:kafka.coordinator:Skipping heartbeat: no auto-assignment or waiting on rebalance
  INFO:kafka.coordinator:Joined group 'datadog' (generation 843) with member_id kafka-python-1.3.5-ddb66185-c615-4f31-9729-9384131f24c9
  INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range
  INFO:kafka.coordinator:Successfully joined group datadog with generation 843
  INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic=u'events', partition=0), TopicPartition(topic=u'events', partition=1), TopicPartition(topic=u'events', partition=2), TopicPartition(topic=u'events', partition=3), TopicPartition(topic=u'events', partition=4), TopicPartition(topic=u'events', partition=5), TopicPartition(topic=u'events', partition=6), TopicPartition(topic=u'events', partition=7), TopicPartition(topic=u'events', partition=8), TopicPartition(topic=u'events', partition=9)]
  INFO:kafka.coordinator.consumer:Setting newly assigned partitions set([TopicPartition(topic=u'events', partition=6), TopicPartition(topic=u'events', partition=7), TopicPartition(topic=u'events', partition=8), TopicPartition(topic=u'events', partition=9), TopicPartition(topic=u'events', partition=0), TopicPartition(topic=u'events', partition=1), TopicPartition(topic=u'events', partition=2), TopicPartition(topic=u'events', partition=3), TopicPartition(topic=u'events', partition=4), TopicPartition(topic=u'events', partition=5)]) for group datadog
We always have 1 consumer instance with unassigned partitions on all the cases,  last consumer instance always have 0 partition assigned
**Below are the screenshot for the same **
No partitions assigned to first consumer instance
No partitions assigned to last consumer instance
We also suspect clustering of kafka.
When we had only 1 kafka node partition assignment to consumer instance and re-balancing was working fine.
But after going into multi-node cluster we are facing this issue
I have a very simple case where I want to see how many time a user click on the ButtonA in my app.
I'm using DropWizard metrics counter to archive this and the coursera reporter to report them to DataDog every 1 minutes.
But what is happening is that this counter doesn't behave like I thought it would.
so for example if the buttonA has been clicked 4 times, the counter will keep the value 4 until the app restart which is not very useful.
Is there an other metrics I'm not aware about that would keep a count and at each reports reset to 0 ?
So that on Datadog dashboard I can easily sum all the count and manage to get the exact numbers even if the app is restarted it will not affect the metrics.
it is possible to install some special sub-package from package?
For example, I want to create package with slack, datadog, sentry plugins (wrappers).
But I want to allow user what he wants to install.
Like:
Can it be done without separating all plugins to different packages?
I am getting Cassandra timeouts using the Phantom-DSL with the Datastax Cassandra driver.
However, Cassandra does not seem to be overloaded.
Below is the exception I get:
And here are the statistics I get from the Cassandra Datadog connector over this time period:
You can see our read rate (per second) on the top-center graph.
Our CPU and memory usage are very low.
Here is how we are configuring the Datastax driver:
Our  nodetool cfstats  looks like this:
When we ran  cassandra-stress , we didn't experience any issues: we were getting a steady 50k reads per second,  as expected .
Cassandra has this error whenever I make my queries:
Why are we getting timeouts?
EDIT:  I had the wrong dashboard uploaded.
Please see the new image.
I'm using a SaaS for my AWS instance monitoring and Mandrill for email sending/campaigns.
I had created a simple chart with  Zapier  but I'd rather like to host it myself.
So my question is:
How can I receive a webhook signal from Mandrill and then send it to Datadog from my server?
Then again I guess hosting this script right on the same server I'm monitoring would be a terrible idea...
Basically I don't know how to "receive the webhook" so I can report it back to my Datadog service agent so it gets updated on their website.
I get how to actually report the data to Datadog as explained here  http://docs.datadoghq.com/api/  but I just don't have a clue  how to host a listener for web hooks ?
Programming language isn't important, I don't have a preference for that case.
I've been using Copperegg for a while now and have generally been happy with it until lately, where I have had a few issues.
It's being used to monitor a number of EC2 instances that must be up 24/7.
Last week I was getting phantom alerts that servers had gone down when they hadn't, which I can cope with, but also I didn't get an alert when I should have done.
One server had high CPU for over 5 mins when the alert should be triggered after 1 minute.
The Copperegg support weren't not all that helpful, merely agreeing that an alert should have been triggered.
The latter of those problems is unacceptable and if it were to happen again outside of working hours then serious problems will follow.
So, I'm looking for alternative services that will do that same job.
I've looked at Datadog and New Relic, but both have a significant problem in that they will only alert me of a problem 5 minutes after it's occurred, rather than the 1 minute I can get with Copperegg.
What else is out there that can do the same job and will also integrate with Pager Duty?
I am migrating an application to use the latest version of Spring Boot.
Currently all the camel routes are in XML and I have it running using this approach.
All the routes currently log a message at the end of processing.
I was wondering is there a way to detect how long it took a particular route to execute and add to the log message.
With this information, we can then create datadog dashboards to show stats on our camel routes
Thanks in advance
Damien
I'm running datadog agent container in EC2 by configuring task definition in AWS ECS.
But at this time, the huge amount of logs is stored in  /var/lib/docker/containers/ ContainerID / ContainerID .json so that I want to rotate it.
In Docker documents, I saw this  link .
There are
Now I want to config these options through task definition but I don't know the convention of them.
Did anyone have any ideas?
I'm currently leveraging celery for periodic tasks.
I am new to celery.
I have two workers running two different queues.
One for slow background jobs and one for jobs user's queue up in the application.
I am monitoring my tasks on datadog because it's an easy way to confirm my workers a running appropriately.
What I want to do is after each task completes, record which queue the task was completed on.
The following function is something that I implemented after researching the celery docs and some StackOverflow posts, but it's not working as intended.
I get the first statsd increment but the remaining code does not execute.
I am wondering if there is a simpler way to inspect inside/after each task completes, what queue processed the task.
I'm using micrometer to publish several different metrics to Datadog; among these there is the number of items processed by several different batch jobs which can be pretty sparse.
Some batch jobs have different intervals, some others are triggered by external events so they don't have a fixed interval at all.
However, my micrometer configuration has a  step  of 30s, which I don't want to change, as suggested in  this question , because I have denser metrics that need to be tracked every 30s.
This means that every 30s, unless a batch job has just run, my application is publishing a zero for each batch job metric, polluting them.
I've been trying to handle this on Datadog by using its  rollup  function, but i don't know the rollup interval beforehand because the interval may be variable for jobs triggered by external events.
Another solution i'm considering is to extend the  DataDogMeterRegistry  to have it clear all metrics right after publishing, so that it will only publish metrics that registered in the latest 30s interval, but I was wondering if there was a clearer way to prevent micrometer from sending zeroes for sparse metrics.
I have a logrus log handler in my Golang application.
Logs are formatted with JSONFormatter and are submitted as a single line to Datadog, which aggregates them and displays them nicely.
However, I recently discovered a case where there's an unhandled panic, and this is  not  captured with the logrus logger.
This results in the actual panic and stack trace being spread across multiple output lines, which Datadog collects individually.
This costs us money and makes the logs very difficult to read.
I'm going to fix the issue, but in the event that any further unhandled panics happen, I'd like to be able to capture them using the logrus JSONFormatter.
Something like this:
This produces the following output.
As you can see, the first two logs use logrus, but the unhandled panic does not.
Is it possible to get those last several lines to log using logrus?
I've been working on adding monitoring metrics in our GraphQL gateway recently.
We're using  graphql-spring-boot  starter for the gateway.
After reading the following documentations, I manage to send the basic graphql.timer.query.
* metrics to Datadog
What I've achieved so far is, when I send a GraphQL query/mutation, I'd collect the request count and time accordingly.
e.g.
sending the query below
I'll see metrics  graphql.timer.query.count  /  graphql.timer.query.sum  with tags  operationName=HelloWorldQuery
It works like perfectly, until I want to test a query with errors.
I realise there is no metrics/tags related to a failed query.
For example, if I the above query returns null data and some GraphQL errors, I'd still collect  graphql.timer.query.count (operationName=HelloWorldQuery) , but there's no additional tags for me to tell there is an error for that query.
In the gateway, I have implemented a custom  GraphQLErrorHandler , so I was thinking maybe I should add error counter (via MeterRegistry) in that class, but I am unable to get the  operationName  simply from GraphQLError type.
the best I can get is error.getPath() which gives the method name (e.g.
greeting ) rather than the custom query name ( HelloWorldQuery  - to be consistent with what  graphql.timer.query.
*  provides).
My question is, how to solve the above problem?
And generally what is the best way of collecting GraphQL query metrics (including errors)?
-------------------  Update  -------------------
2019-12-31 
I read a bit more about GraphQL Instrumentation  here  and checked the  MetricsInstrumentation  implementation in graphql-spring-boot repo, the I have an idea of extending the MetricsInstrumentation class by adding error metrics there.
2020-01-02 
I tried to ingest my CustomMetricsInstrumentation class, but with no luck.
There is internal AutoConfiguration wiring, which I cannot insert my auto configuration in the middle.
I am using Datadog integration with elasticsearch to monitor the ES clusters, one important metric which it shows on its dashboard is the no of active and waiting for search threads.
Referring to  this  ES docs, I understand that search threads work on a request queue in ES which is of the fixed size of 1000.
I am seeing a lot of waiting for threads as shown in the image, but there is no rejected queue exception explained  here .
So it means ES is not rejecting the requests but still search threads are not able to execute the request fast enough hence ended up in waiting status for a long time.
Questions
I know its a board question, hence let me know if any additional information is required.
I deployed the Datadog agent using the  Datadog Helm chart  which deploys a  Daemonset  in Kubernetes.
However when checking the state of the Daemonset I saw it was not creating all pods:
When describing the  Daemonset  to figure out what was going wrong I saw it did not have enough resources:
However, I have the  Cluster-autoscaler  installed in the cluster and configured properly (It does trigger on regular  Pod  deployments that do not have enough resources to schedule), but it does not seem to trigger on the  Daemonset :
The AutoScalingGroup has enough nodes left:
Did I miss something in the configuration of the Cluster-autoscaler?
What can I do to make sure it triggers on  Daemonset  resources as well?
Edit:
Describe of the Daemonset
Example json:
I'd like to return a list of services that have the "build" key, and not the "image" key.
Note that the value for the build key isn't something I can key off of.
Output should be: ["web", "datadog"]
In my implementation script I have a line which logs a metric:
From my test script, I assert that statsd.increment() is called by mocking out the datadog module:
This works fine and passes.
But as soon as I add ANOTHER script which calls  some_function()  without mocking datadog, that script runs beforehand and loads the real datadog module into the cache.
The above test then fails because  some_function()  is no longer using the mock datadog, it uses the real (cached) datadog.
How can I address this?
Is it possible to remove the module from the cache?
We need centralized logging in order to monitor, store, manage, and visualize logs for our infrastructure.
The logging solution must be able to capture messages from projects written in different languages such as Java, Angular, Scala, and Python.
Implementing a custom-built solution would lead to additional tasks, costs, and dependencies associated with managing and maintaining its components.
So instead, we are thinking about using AWS Partner Network (APN) offerings.
What would be the best managed solution out of Splunk, Sumo Logic, Datadog, Elastic and Loggly?
I'm currently trying to run a playbook that uses a callback plugin.
This plugin uses a module called datadog:
When I try to run the playbook, I get an error message:
ImportError: No module named datadog
I order to work around this, I created a virtualenv, activated it and installed the datadog module:
Then when I launch python and import the module, everything is fine:
Python 2.7.15
Therefore, I believe that the module is used properly.
However, when launching the ansible playbook, the error remains:
From what I can tell, Ansible is not taking the virtualenv into consideration.
I would expect Ansible to use the path of the virtualenv in the  ansible python module location .
How can I make ansible use the virtualenv?
I didn't find anything related to the python path in the ansible documentation:  https://docs.ansible.com/ansible/2.5/reference_appendices/config.html
Note: the issue occurs on the machine running Ansible.
Not the machine being provisioned.
Im new to kibana but am hoping to migrate away from Datadog.
In DD I can create 'visualizations' that are specific to a single data source (host in this case) and combine several into one dashboard.
EG: view CPU load for n hosts on one page
Am not seeing (yet) how to accomplish this sort of thing via Kibana.
Suggestions on where to look?
Using v6.x Kibana/logstash/elasticsearch/metricbeat
I have two MySQL server which are running on same group replication.
The setup had been done by below steps:
But I found an other error: Every time I reload the fallback MySQL (by restart services) it will auto join to group but stuck at RECOVERING forever, I waited for 3 days but it still in RECOVERING.
I checked on the log and don't see any error on both server, everything look good except the fallback is running as readonly and stay at RECOVERING.
What step did I missed?
My group configuration is (I followed the instruction from DigitalOcean help  page at  https://www.digitalocean.com/community/tutorials/how-to-configure-mysql-group-replication-on-ubuntu-16-04 ):
sync_binlog                    = 1 binlog_format                  =
ROW
loose-group_replication_start_on_boot = ON
loose-group_replication_ssl_mode = REQUIRED
loose-group_replication_recovery_use_ssl = 1
&quot;9dc4ae01-6664-437a-83f8-80546d58e025&quot;
loose-group_replication_ip_whitelist =
&quot;172.AAA.BBB.166,138.AAA.BBB.199&quot; loose-group_replication_group_seeds
= &quot;172.AAA.BBB.166:33061,138.AAA.BBB.199:33061&quot;
loose-group_replication_enforce_update_everywhere_checks = ON
&quot;138.AAA.BBB.199:33061&quot;
Below is MySQL log on first server:
2018-06-08T06:10:12.167400Z 0 [Warning] Plugin group_replication
reported: 'Members removed from the group: 138.AAA.BBB.199:3306'
2018-06-08T06:10:12.167475Z 0 [Note] Plugin group_replication
reported: 'Group membership changed to 172.AAA.BBB.166:3306 on view
15271181169364149:11.'
2018-06-08T06:11:59.032666Z 0 [Note] Plugin
group_replication reported: 'Members joined the group:
138.AAA.BBB.199:3306' 2018-06-08T06:11:59.032722Z 0 [Note] Plugin group_replication reported: 'Group membership changed to
172.AAA.BBB.166:3306, 138.AAA.BBB.199:3306 on view 15271181169364149:12.'
Below is the MySQL log on fallback server:
2018-06-11T09:22:57.490896Z 0 [Warning] option 'max_allowed_packet':
unsigned value 3221225472 adjusted to 1073741824
2018-06-11T09:22:57.490942Z 0 [Warning] The use of InnoDB is mandatory
since MySQL 5.7.
The former options like '--innodb=0/1/OFF/ON' or
'--skip-innodb' are ignored.
2018-06-11T09:22:57.491057Z 0 [Warning]
The syntax '--log_warnings/-W' is deprecated and will be removed in a
future release.
Please use '--log_error_verbosity' instead.
2018-06-11T09:22:57.491098Z 0 [Warning] TIMESTAMP with implicit
DEFAULT value is deprecated.
Please use
--explicit_defaults_for_timestamp server option (see documentation for more details).
2018-06-11T09:22:57.492972Z 0 [Note] /usr/sbin/mysqld
(mysqld 5.7.22-log) starting as process 31633 ...
2018-06-11T09:22:57.500063Z 0 [Warning] InnoDB: Using
innodb_locks_unsafe_for_binlog is DEPRECATED.
This option may be
removed in future releases.
Please use READ COMMITTED transaction
isolation level instead; Please refer to
http://dev.mysql.com/doc/refman/5.7/en/set-transaction.html
2018-06-11T09:22:57.500175Z 0 [Note] InnoDB: PUNCH HOLE support
available 2018-06-11T09:22:57.500191Z 0 [Note] InnoDB: Mutexes and
rw_locks use GCC atomic builtins 2018-06-11T09:22:57.500200Z 0 [Note]
InnoDB: Uses event mutexes 2018-06-11T09:22:57.500205Z 0 [Note]
InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
2018-06-11T09:22:57.500209Z 0 [Note] InnoDB: Compressed tables use
zlib 1.2.3 2018-06-11T09:22:57.500213Z 0 [Note] InnoDB: Using Linux
native AIO 2018-06-11T09:22:57.500430Z 0 [Note] InnoDB: Number of
pools: 1 2018-06-11T09:22:57.500575Z 0 [Note] InnoDB: Using CPU crc32
instructions 2018-06-11T09:22:57.501015Z 0 [ERROR] InnoDB: Failed to
create check sector file, errno:13 Please confirm O_DIRECT is
supported and remove the file /data/check_sector_size if it exists.
2018-06-11T09:22:57.502305Z 0 [Note] InnoDB: Initializing buffer pool,
total size = 4G, instances = 8, chunk size = 128M
2018-06-11T09:22:57.799065Z 0 [Note] InnoDB: Completed initialization
of buffer pool 2018-06-11T09:22:57.857325Z 0 [Note] InnoDB: If the
mysqld execution user is authorized, page cleaner thread priority can
be changed.
See the man page of setpriority().
2018-06-11T09:22:57.870317Z 0 [Note] InnoDB: Highest supported file
format is Barracuda.
2018-06-11T09:22:58.081570Z 0 [Note] InnoDB:
Creating shared tablespace for temporary tables
2018-06-11T09:22:58.081656Z 0 [Note] InnoDB: Setting file
'/data/databases/ibtmp1' size to 12 MB.
Physically writing the file
full; Please wait ... 2018-06-11T09:22:58.116190Z 0 [Note] InnoDB:
File '/data/databases/ibtmp1' size is now 12 MB.
2018-06-11T09:22:58.117279Z 0 [Note] InnoDB: 96 redo rollback
segment(s) found.
96 redo rollback segment(s) are active.
2018-06-11T09:22:58.117293Z 0 [Note] InnoDB: 32 non-redo rollback
segment(s) are active.
2018-06-11T09:22:58.117670Z 0 [Note] InnoDB:
Waiting for purge to start 2018-06-11T09:22:58.168094Z 0 [Note]
InnoDB: 5.7.22 started; log sequence number 51745666191
2018-06-11T09:22:58.168309Z 0 [Note] InnoDB: Loading buffer pool(s)
from /data/databases/ib_buffer_pool 2018-06-11T09:22:58.168558Z 0
[Note] Plugin 'FEDERATED' is disabled.
2018-06-11T09:22:58.183268Z 0
[Warning] CA certificate /etc/mysql/mysql-ssl/ca-cert.pem is self
signed.
2018-06-11T09:22:58.184615Z 0 [Note] Server hostname
(bind-address): '138.AAA.BBB.199'; port: 3306
2018-06-11T09:22:58.184636Z 0 [Note]   - '138.AAA.BBB.199' resolves to
'138.AAA.BBB.199'; 2018-06-11T09:22:58.184668Z 0 [Note] Server socket
created on IP: '138.AAA.BBB.199'.
2018-06-11T09:22:58.186203Z 0
[Warning] 'user' entry 'mysql.session@localhost' ignored in
--skip-name-resolve mode.
2018-06-11T09:22:58.186220Z 0 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve
mode.
2018-06-11T09:22:58.186238Z 0 [Warning] 'user' entry
'phpmadsys@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.186260Z 0 [Warning] 'user' entry
'phpmyadmin@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.186308Z 0 [Warning] 'db' entry 'performance_schema
mysql.session@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.186313Z 0 [Warning] 'db' entry 'sys
mysql.sys@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.186318Z 0 [Warning] 'db' entry 'phpmyadmin
phpmadsys@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.186322Z 0 [Warning] 'db' entry 'performance_schema
datadog@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.186327Z 0 [Warning] 'db' entry 'phpmyadmin
phpmyadmin@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.186340Z 0 [Warning] 'proxies_priv' entry '@
root@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.188628Z 0 [Warning] 'tables_priv' entry 'user
mysql.session@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.188649Z 0 [Warning] 'tables_priv' entry
'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
2018-06-11T09:22:58.192624Z 0 [Warning] Neither --relay-log nor
--relay-log-index were used; so replication may break when this MySQL server acts as a slave and has his hostname changed!!
Please use
'--relay-log=dvm02-relay-bin' to avoid this problem.
2018-06-11T09:22:58.206545Z 0 [Note] Event Scheduler: Loaded 0 events
2018-06-11T09:22:58.206745Z 0 [Note] /usr/sbin/mysqld: ready for
connections.
Version: '5.7.22-log'  socket:
'/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server
(GPL) 2018-06-11T09:22:58.207175Z 2 [Note] Plugin group_replication
reported: 'Group communication SSL configuration:
group_replication_ssl_mode: &quot;REQUIRED&quot;; server_key_file:
&quot;/etc/mysql/mysql-ssl/server-key.pem&quot;; server_cert_file:
&quot;/etc/mysql/mysql-ssl/server-cert.pem&quot;; client_key_file:
&quot;/etc/mysql/mysql-ssl/server-key.pem&quot;; client_cert_file:
&quot;/etc/mysql/mysql-ssl/server-cert.pem&quot;; ca_file:
&quot;/etc/mysql/mysql-ssl/ca-cert.pem&quot;; ca_path: &quot;&quot;; cipher: &quot;&quot;;
tls_version: &quot;TLSv1,TLSv1.1&quot;; crl_file: &quot;&quot;; crl_path: &quot;&quot;'
2018-06-11T09:22:58.207378Z 2 [Warning] Plugin group_replication
reported: '[GCS] Automatically adding IPv4 localhost address to the
whitelist.
It is mandatory that it is added.'
2018-06-11T09:22:58.207820Z 2 [Note] Plugin group_replication
reported: 'Initialized group communication with configuration:
group_replication_group_name: &quot;9dc4ae01-6664-437a-83f8-80546d58e025&quot;;
group_replication_local_address: &quot;138.AAA.BBB.199:33061&quot;;
group_replication_group_seeds:
&quot;172.AAA.BBB.166:33061,138.AAA.BBB.199:33061&quot;;
group_replication_bootstrap_group: false;
group_replication_poll_spin_loops: 0;
group_replication_compression_threshold: 1000000;
group_replication_ip_whitelist: &quot;172.AAA.BBB.166,138.AAA.BBB.199&quot;'
2018-06-11T09:22:58.207853Z 2 [Note] Plugin group_replication
reported: '[GCS] Configured number of attempts to join: 0'
2018-06-11T09:22:58.207859Z 2 [Note] Plugin group_replication
reported: '[GCS] Configured time between attempts to join: 5 seconds'
2018-06-11T09:22:58.207878Z 2 [Note] Plugin group_replication
reported: 'Member configuration: member_id: 2; member_uuid:
&quot;822868f9-52a0-11e8-aa0e-1e45f9551f27&quot;; single-primary mode: &quot;false&quot;;
group_replication_auto_increment_increment: 7; '
2018-06-11T09:22:58.209024Z 3 [Note] 'CHANGE MASTER TO FOR CHANNEL
'group_replication_applier' executed'.
Previous state
master_host='', master_port= 0, master_log_file='',
master_log_pos= 4, master_bind=''.
New state master_host='',
master_port= 0, master_log_file='', master_log_pos= 4, master_bind=''.
2018-06-11T09:22:58.216904Z 6 [Note] Slave SQL thread for channel
'group_replication_applier' initialized, starting replication in log
'FIRST' at position 0, relay log
'./dvm02-relay-bin-group_replication_applier.000071' position: 4
2018-06-11T09:22:58.216931Z 2 [Note] Plugin group_replication
reported: 'Group Replication applier module successfully initialized!'
2018-06-11T09:22:58.241357Z 0 [Note] Plugin group_replication
reported: 'XCom protocol version: 3' 2018-06-11T09:22:58.241397Z 0
[Note] Plugin group_replication reported: 'XCom initialized and ready
to accept incoming connections on port 33061'
2018-06-11T09:22:59.213826Z 0 [Note] InnoDB: Buffer pool(s) load
completed at 180611 11:22:59 2018-06-11T09:23:00.316791Z 0 [Note]
Plugin group_replication reported: 'Group membership changed to
172.AAA.BBB.166:3306, 138.AAA.BBB.199:3306 on view 15271181169364149:16.'
What is the best way/tool to monitor an EBS volume available space when mounted inside a Docker container?
I really need to monitor the available disk space in order to prevent crash because of no  space left on device .
Do you know of any tool that can monitor that, like datadog, newrelic grafana, prometheus or something opensource?
My scenario is that currently, I'm running my application as Daemon sets and want to integrate Datadog into my infrastructure.
As my understanding is that Daemon sets purpose is to make sure one pod of each set is ran on each node.
Here I wanted to point my application at datadog agent so it will feed data into it.
I've defined a  Service  of  Nodeport  type to expose the port of the agent.
I provided the service name in my application definition and it works.
For one node.
What happens now when I will have more nodes?
Will k8s be clever enough to route to the agent on the same nodes or there is a situation where a pod with my application might call the agent on a different node?
Is this a correct setup?
I want to monitor Undertow (mainly IO and worker threads) using statsd, I had in place tomcat and a lot of metrics were exported automatically to jmx but it seems not the case for Undertow.
I can see XNIO on JConsole for the application(Spring-boot app), but no metrics appear when I try to see them anywhere else.
(Graphite, Datadog)
I would like to use Datadog to monitor the queue length of some background jobs.
Basically  I need to know the name of the key that represents a queue in Sidekiq , so that I can monitor it as described here:
 https://docs.datadoghq.com/integrations/redisdb/
I've read  that the Sidekiq keys have the form  sidekiq:queue:myqueuename .
However I have tried to execute  KEYS *myqueuename*  and I can't find anything.
I have also tried to search  KEYS *sidekiq*  but I don't get anything.
If I search  KEYS *queue*  I get the key  queues  which represents a set with the names of the queues (e.g.
deliveries, default, low).
However those are only the names: I need the actual queues.
What is the key of a queue?
Is there a way to name containers when they start through ECS?
Wondering because I'm currently using Datadog to monitor the system usage and the containers are named something long etc like
ecs-datadog-agent-task-1-datadog-agent-c0a1f3e8d9e58dd5e901
would like to set my own name
I want to monitor quota level usage for openstack projects and I need to be able to monitor  current and max levels for networks, ports and routers  (from Python code).
Please note that I am talking about project-level access so the user performing the monitoring is not an open-stack admin.
I was able to successfully read current level and max level for nova metrics (compute) but for those related to neutron (networking) it seems that API and command line returns only max-limits and not current levels.
It very easy to test it yourself:
I should mention that the web interface (horizon) does already report correctly the Floating IPs.
It seems that it does not display any gauges for networks.
The solution should work with openstack  kilo (7) releases or newer.
I am very new to writing code.
I've been looking at every way I can find of finding a string in a text document and then returning part of the string on the following line.
Ideally with the end goal of putting this extracted string into an excel file but I'm no where near that step yet.
I've been playing around with a lot of different options and I can not for the life of me get it to work.
I feel like I'm close and it's killing me because I just can't figure out where I'm going wrong here.
Goal: to extract the name of the person who posted the job from the text below without knowing the person's name.
I know the string "Job posted by" will immediately preseed the name I'm looking for and I know " · " will immediately follow the name.
no where else in the text document do either of these surround strings appear.
my attempts at this so far are the following (my issue is that it appears to simply return the entire text document as opposed to just the name I'm looking for)
any and all help and ideas is enormously appreciated.
sample text in the file:
 9/2/2016 Application Security Engineer Job at Datadog in Greater New York City Area | LinkedIn
    60
 Home Profile
Job description
My Network Jobs
 Search for people, jobs, companies, and more...
Interests
 Advanced
 
Business Services

Go to Lynda.c
  Application Security Engineer
Datadog
Greater New York City Area
    Posted 15 days ago 93 views
1 alum works here
   Apply on company website
  We’re on a mission to bring sanity to cloud operations and we need you to build resilient and secure applications on our platform.
What you will do
Perform code and design reviews, contribute code that improves security throughout Datadog's products Educate your fellow engineers about security in code and infrastructure
Monitor production applications for anomalous activity
Prioritize and track application security issues across the company
    Help improve our security policies and processes
Job posted by
Ryan Elberg · 2nd
Head of Tech Talent Acquisition at Datadog Greater New York City Area
Send Inmail
It's a pretty simple question, really.
I want to report the number of running instances to datadog, along with a bunch of my other stats.
There's an irony to the fact that I search Google Web Search for how to do something in Google App Engine and get the crappiest possible result, every time: The Google App Engine documentation pages.
There are some not labeled corpus.
I extracted from it triples (OBJECT, RELATION, OBJECT).
For relation extraction I use Stanford OpenIE.
But I need only some of this triples.
For example, I need relation " funded ".
Text :
 Every startup needs a steady diet of funding to keep it strong and growing.
Datadog, a monitoring service that helps customers bring together data from across a variety of infrastructure and software is no exception.
Today it announced a massive $94.5 million Series D Round.
The company would not discuss valuation.
From this text i want to extract relation  (Datadog, announced, $94.5 million Round)
I have only one idea:
May be there are better approach?
May be I need labeled corpus(i haven't it)?
I want to write an DataDog Check to monitor some process like Puma, delayed_job etc, I can see there are ready plugins available for these for nagios and Sensu but not for DataDog, But can I write my own check/plugin for this services in datadog ?
or can I use existing Nagios/sensu plugins with DataDog ?
If yes How should I proceed ?
I want to integrate server monitoring system like DataDog that implements APM standards.
I want to achieve this without using the custom server like Express or Koa, just using the out-of-box NextJS server.
My NextJS server is only being used for pages, not APIs.
It would help if NextJS allows us to insert middleware for all requests.
However it doesn't look like that's an option.
Do you know a good way to do this?
I set up datadog trace client in my kubernetes cluster to monitor my deployed application.
It was working fine with the kubernetes version 1.15x but as soon as I upgraded the version to 1.16x, the service itself is not showing in the Datadog Dashboard.
Currently using:
Kubernetes 1.16.9
Datadog 0.52.0
When checked for agent status.
It is giving following exception :
This looks like a version issue to me.
If it is which Datadog version I need to use for monitoring?
Per  this spec  on github and these  helm instructions   I'm trying to upgrade our Helm installation of datadog using the following syntax:
However I'm getting the error below regardless of any attempt at altering the syntax of the  prometheus_url  value (putting the url in quotes, escaping the quotes, etc):
Error: UPGRADE FAILED: failed to create resource: ConfigMap in version "v1" cannot be handled as a ConfigMap: v1.ConfigMap.Data: ReadString: expects " or n, but found {, error found in #10 byte of ...|er.yaml":{"instances|..., bigger context ...|{"apiVersion":"v1","data":{"kube_scheduler.yaml":{"instances":[{"prometheus_url":"\" http://localhost| ...
If I add the  --dry-run --debug  flags I get the following yaml output:
The Yaml output appears to mesh with the integration as specified on this  github page .
I have a node.js app already deployed on Elastic Beanstalk.
The EC2 instance on which the node app is deployed is running Ubuntu 16.04.5 LTS.
I am trying to integrate Datadog APM with Elastic Beanstalk using datadog's config file in .ebextensions folder.
I am following the instructions given on their docs page( https://docs.datadoghq.com/integrations/amazon_elasticbeanstalk/#alternate-datadog-agent-configuration )
Even though I am following all the mentioned steps I keep getting the following error on AWS ELB.
My Datadog config file code:
Even after replacing the  initctl  with  systemctl  in the start &amp; stop scripts, I am still getting the same error.
Can't understand where I'm going wrong.
Please Help!
I'm just starting out using Apache Beam on Google Cloud Dataflow.
I have a project set up with a billing account.
The only things I plan on using this project for are:
1. dataflow - for all data processing
2. pubsub - for exporting stackdriver logs to be consumed by Datadog
Right now, as I write this, I am not currently running any dataflow jobs.
Looking at the past month, I see ~$15 in dataflow costs and ~$18 in Stackdriver Monitor API costs.
It looks as though Stackdriver Monitor API is close to a fixed $1.46/day.
I'm curious how to mitigate this.
I do not believe I want or need Stackdriver Monitoring.
Is it mandatory?
Further, while I feel I have nothing running, I see this over the past hour:
So I suppose the questions are these:
1. what are these calls?
2. is it possible to disable Stackdriver Monitoring for dataflow or otherwise mitigate the cost?
I am currently exporting Actuator metrics for my Spring Boot Webflux project to DataDog with 10 seconds interval.
I would like to add another exporter for one of our internal system that is not in the list of supported backends.
Looking at the implementation from  DataDogMeterRegistry  I came up with the following.
However this is not working since no logs are printed.
My question is how can I add and implement another MeterRegistry for Spring Boot Micrometer?
We're running NodeJS application inside docker container hosted on Amazon EC2 instance.
To
To enable Monitoring for Node.js app with Datadog we are using datadog-metrics library and integrate it with our application.
We basically require to save the below Javascript code into a file called example_app.js

 
 var metrics = require('datadog-metrics');
metrics.init({ **host: 'myhost', prefix: 'myapp.
'** });

function collectMemoryStats() {
    var memUsage = process.memoryUsage();
    metrics.gauge('memory.rss', memUsage.rss);
    metrics.gauge('memory.heapTotal', memUsage.heapTotal);
    metrics.gauge('memory.heapUsed', memUsage.heapUsed);
    metrics.increment('memory.statsReported');
}

setInterval(collectMemoryStats, 5000);
Although, we are able to successfully publish metrics to datadog but we're wondering if this can be automated.
We want build this into our docker image, hence require an automatic way to pick up the hostname, at the very least be able to use the docker hosts name if possible..Because till now we're manually specifying "myhost" and "myapp" values manually.
Any better way to fetch the AWS instance hostname value into %myhost?
We're implementing logging &amp; monitoring for a Vue/Node application which is using a REST Api.
Oftentimes the API returns 4xx reponses (401s, 404s) which are currently caught by Axios and returned as &quot;Errors&quot;.
These end up in our logging solutions (Datadog, Sentry) but dont bring much actionable points.
Should in general  status codes like these be considered Errors?
Are there any best practices for SPA logging and monitoring?
(couldn't find any resources)
I have a use case wherein I want to publish my spring boot API metrics to Datadog &amp; CloudWatch simultaneously
I have added the below dependencies to my pom
Main Application class
I have added all required properties in the  application.properties  as well.
I can see metrics are being published to both datadog &amp; CloudWatch with default metrics name  http.server.request 
But I want the metrics name for datadog to be different &amp; for this, I have added the below property as well
management.metrics.web.server.requests-metric-name = i.want.to.be.different
But this is changing the name for both CloudWatch &amp; datadog
My question is how can I change the default metrics name for datadog only or keep names different for both
1.16 deprecation notice:
I have about 10 helm charts that contain the old api versions - datadog, nginx-ingress and more.
I don't want to upgrade these different services.
are there any known work arounds?
I'm new to Kafka.
During study to kafka, I think monitoring consumer's lag is needed.
When I search from google and docs, I found few ways.
I just trying to get less pipeline steps.
What should be the simple way to visualize consumer's lag?
I want to see the remaining lag in near real-time from Kafka for a particular consumer group.
The closest thing I've done is run the  describe  script from Kafka binaries, but it's slow and unreliable.
We are trying to programmatically do this to perform some conditional logic within our ETL pipeline.
My first thought is to garner metrics within the consumer and publish over statsD to new relic or datadog then poll over HTTP.
This is something I would do long-term.
Is there a shorter-term, simpler approach to poll the consumer lag for a particular group?
I’m looking to enable JMX to allow datadog to monitor our java JBoss wildfly systems but keep hitting runtime errors
I have set up the standalone.xml with
And
As well as
Then in my startup.sh i have added
But this gives me
java.lang.IllegalStateException: The LogManager was not properly
  installed (you must set the "java.util.logging.manager" system
  property to "org.jboss.logmanage r.LogManager")
This seems to be fairly common if I look at both here and on google but there seem to be different solutions depending on the version of wildfly.
I think I need to do something like
Set at the start of the standalone.conf
And then
At the end.
But I still get errors “Could not load Logmanager "org.jboss.logmanager.LogManager"”
Any advice would be appreciated.
I am deploying my flask application in AWS Elasticbeanstalk and I want to add a command for running datadog tracing when executing entry-point command.
How can I do that?
This is the entry-point command to start my flask app in local machine:
This is how to add a command before that entry-point command (using datadog as example):
How to do the same in AWS elasticbeanstalk?
Seems like beanstalk is using apache + mod_wsgi to run python-flask application but I am not sure how to add a command before the entry-point command.
I've a Spring Boot REST application.
I use many Spring libraries as: Spring Data REST, HATEOAS, Spring JPA, Hibernate, Redis, ElasticSearch...
I want to track metrics of my application and I did a research to find the best tool to do it.
After have given a try to Micrometer+DataDog, becuase I already use ElasticSearch I did try  APM Java Agent  and I found quite impressive the amount of data I get in Kibana Dashboard.
I can see my endpoints and investigate where the time was spent (Mysql queries and others stuff).
I didn't try yet Micrometer+ElasticSearch, but from the documentation it seems to collect less data out of the box.
I'd like to know your advice about and what you think is the best tool to collect metric for an application in production.
Last month we had an outage caused by the AKS Scheduler going down.
Commands such as  kubectl  were still working but pods weren't starting.
When we contacted AKS, they eventually "restarted the API server" which resolved this issue.
It definitely makes me a little worried that we could lose something as critical as the scheduler and we have to call to ask Azure to fix it.
Azure has made the Control Plane opaque from within the cluster.
The API server, scheduler, and controller are not even listed as objects.
We are working on a simple healthcheck pod that would start up and send a ping to Datadog saying "I'm alive", however, I tend to think that Azure should be providing someway to monitor or view the health of these services.
Has anyone come up with a better method of monitoring these processes?
The app has the following containers
In the dev process many feature branches are created to add new features.
such as
I have an AWS EC2 instance per feature branches running docker engine V.18 and docker compose to build the and run the docker stack that compose the php app.
To save operation costs 1 AWS EC2 instance can have 3 feature branches at the same time.
I was thinking that there should be a custom docker-compose with special port mapping and docker image tag for each feature branch.
The goal of this configuration is to be able to test 3 feature branches and access the app through different ports while saving money.
I also thought about using  docker networks  by keeping the same ports and using an nginx to redirect traffic to the different docker network ports.
What recommendations do you give?
We have an app that was initially created in 2008 on Rails 2.3.
This app runs on top of JRuby version 1.7, in Ruby 1.8 compatibility mode.
This is a largish app with 350 controllers and 450 models.
We're (finally) upgrading the app to Rails 5.2, upgrading JRuby to latest version 9.2.8.0.
We're now noticing a significant performance regression compared to the old app, running on the same server and database.
Most requests are 30% to 200% slower than in the Rails 2.3 version.
The slowdown seems to be linear to the amount of data rendered.
I.e.
when we render a large table of data, the request may take 3000ms vs 1000ms on the old version, even when there is no DB access happening in the view.
When rendering a request with little data, the slowdown is much less pronounced.
For instance, Rails 2.3 will render a certain "heavy" page in 3 sec:
Completed in 2962ms (View: 1565, DB: 1384)
Compared to 7 sec for the new version:
Completed 200 OK in 7254ms (Views: 5808.5ms | ActiveRecord: 1389.3ms)
The DB access time is very similar, but the "view" time is much much higher.
We've tried running our app on MRI Ruby, and it's 3 times faster than on JRuby 9.
Unfortunately, we make extensive use of Java libraries in our code, so we can't easily switch to MRI.
Some questions:
Is JRuby still a viable option for Rails 5, is anyone using it with good performance?
I couldn't find a recommendation which JDK to run JRuby on.
We're now running on OpenJDK 8.
I've briefly tried OpenJDK 13 but that seemed even slower.
Is there a recommended version?
We're using these options:  JRUBY_OPTS=-J-Xmx4g -XX:ReservedCodeCacheSize=300M --server .
Any recommendations on other options that may improve performance?
We've used "Datadog" to produce flame-graphs of our requests, which confirms that most of the time is spent rendering views outside of data access, but we can't really tell what is costing time in the view.
We've tried attaching JVisualVM to the app in sampling mode, but it's very hard to get useful metrics out of this.
Is there a better way of profiling JRuby on Rails applications?
I want to use mongodb driver.But I get the following error:
go.mongodb.org/mongo-driver/vendor/github.com/DataDog/zstd
  exec: "gcc": executable file not found in %PATH%
I am trying to trace .NET application using Datadog .NET Tracer.
https://github.com/DataDog/dd-trace-dotnet/releases 
The application and the tracer are installed on Windows 2008R2SP1x64 Std with .NET Framework 4.6.1.
It fails to trace with the following warning messages:
.net tracer log:
"Failed to attach profiler: interface ICorProfilerInfo3 not found."
Windows Application Event log:
"NET Runtime version 2.0.50727.8800 - Failed to CoCreate profiler."
The requirement for the .net tracer is .NET CLR 4.5 and above.
My understanding of .NET is that CLR is a component of the framework, hence CLR version is same as framework version.
I'm trying to understand why .NET runtime version (2.0.50727.8800 according to Windows event log) is older than the framework (4.6.1 according to the Windows control panel).
I have a gauge in datadog and want to see the 'last' value.
But when I select  the last day  in the dashboard time-picker I see a different value than when I select  the last week .
My natural understanding is that 'last' gauge value should be the same regardless of how far back in time I go.
Am I missing some understanding of how gauges or datadog works?
I have a ResponseTimeMiddleware.cs responsible for getting response time metrics (I am using datadog) for every request made.
Which is tagged by controller and action names.
However when we hit the "connect/token" endpoint, the context.GetRouteData() is null, probably because identity server is doing it behind the scenes.
Is there a way I could get this information or some other unique information where I could tag with?
here's my code:
This is my Startup:
Is there a simple way it to report custom defined stats to our statsd / Datadog infrastructure from a Google Cloud Function written in Node.js?
Since it's a high-traffic Javascript Cloud Function, I'd like to avoid heavy initialization of additional libraries every time the cloud function is invoked.
Also, by custom stats I mean stats of our own definition (not boilerplate summary statistics via StackDriver or DataDog GCP integration).
Bundler is causing a build fail for my Heroku app.
It is a ruby on rails app and was working perfectly and deployed on Heroku.
I started getting  the following build error only after I installed the datadog agent.
The build error seemed easy enough to fix, but it has grown difficult.
After removing my Gemfile.lock and reinstalling to make sure bundler version was   2.0, I then started getting another build error.
This time it was that the bundler version my project requires is &lt; 2:
I updated the latest version on my system, like it suggests.
But then I run back into the first error that Bundler V2 is required for the Heroku build step.
This is my Gemfile.lock:
Here is my Gemfile if that helps
I'm really at a loss an appreciate your help.
ANY SUGGESTIONS APPRECIATED!
I've recently made some updates to a Ruby on Rails Heroku app that was working fine.
I tried updating the bundler version so that it could be configured with a datadog agent.
Now when I try to push to heroku master I get the following error:

 
 git push heroku master
Counting objects: 28, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (28/28), done.
Writing objects: 100% (28/28), 3.33 KiB | 1.66 MiB/s, done.
Total 28 (delta 19), reused 0 (delta 0)
remote: Compressing source files... done.
remote: Building source:
remote: 
remote: -----&gt; ActiveStorage Preview app detected
remote: -----&gt; Installing binary dependencies for ActiveStorage Preview
remote:        Downloading packages..
remote:        Installing packages.....
remote: -----&gt; Ruby app detected
remote: 
remote:  !
remote:  !
different prefix: "" and "/tmp/build_2f915e77052b7fa5cef9531ffdd277e6"
remote:  !
remote: /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/pathname.rb:522:in `relative_path_from': different prefix: "" and "/tmp/build_2f915e77052b7fa5cef9531ffdd277e6" (ArgumentError)
remote: 	from /tmp/d20190215-459-drze92/bundler-1.15.2/gems/bundler-1.15.2/lib/bundler/lockfile_parser.rb:98:in `rescue in initialize'
remote: 	from /tmp/d20190215-459-drze92/bundler-1.15.2/gems/bundler-1.15.2/lib/bundler/lockfile_parser.rb:61:in `initialize'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:130:in `new'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:130:in `block in parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:128:in `parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:112:in `lockfile_parser'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:74:in `specs'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:59:in `block in gem_version'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/helpers/bundler_wrapper.rb:58:in `gem_version'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/rails5.rb:9:in `block in use?'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/base.rb:48:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/rails5.rb:8:in `use?'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:17:in `block (2 levels) in detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `each'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:16:in `block in detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack.rb:13:in `detect'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/bin/support/ruby_compile:17:in `block in &lt;main&gt;'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:35:in `block in trace'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/vendor/ruby/heroku-18/lib/ruby/2.5.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/lib/language_pack/instrument.rb:35:in `trace'
remote: 	from /app/tmp/buildpacks/b7af5642714be4eddaa5f35e2b4c36176b839b4abcd9bfe57ee71c358d71152b4fd2cf925c5b6e6816adee359c4f0f966b663a7f8649b0729509d510091abc07/bin/support/ruby_compile:15:in `&lt;main&gt;'
remote:  !
Push rejected, failed to compile Ruby app.
remote: 
remote:  !
Push failed
remote: Verifying deploy...
remote: 
remote: !
Push rejected to codereader-backend.
remote: 
To https://git.heroku.com/codereader-backend.git
 !
[remote rejected] master -&gt; master (pre-receive hook declined)
error: failed to push some refs
Before this error I was getting an error about the my lockfile is unreadable.
I removed the lockfile and rebundled to no avail.That error is below.

 
 git push heroku master
Counting objects: 27, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (27/27), done.
Writing objects: 100% (27/27), 3.24 KiB | 1.62 MiB/s, done.
Total 27 (delta 18), reused 0 (delta 0)
remote: Compressing source files... done.
remote: Building source:
remote: 
remote: -----&gt; ActiveStorage Preview app detected
remote: -----&gt; Installing binary dependencies for ActiveStorage Preview
remote:        Downloading packages..
remote:        Installing packages.....
remote: -----&gt; Ruby app detected
remote: 
remote:  !
remote:  !
Your lockfile is unreadable.
Run `rm Gemfile.lock` and then `bundle install` to generate a new lockfile.
remote:  !
remote: /tmp/d20190215-478-1u5qan1/bundler-2.0.1/gems/bundler-2.0.1/lib/bundler/lockfile_parser.rb:98:in `rescue in initialize': Your lockfile is unreadable.
Run `rm Gemfile.lock` and then `bundle install` to generate a new lockfile.
(Bundler::LockfileError)
remote: 	from /tmp/d20190215-478-1u5qan1/bundler-2.0.1/gems/bundler-2.0.1/lib/bundler/lockfile_parser.rb:61:in `initialize'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:130:in `new'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:130:in `block in parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:128:in `parse_gemfile_lock'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:112:in `lockfile_parser'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:74:in `specs'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:59:in `block in gem_version'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:86:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/helpers/bundler_wrapper.rb:58:in `gem_version'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/rails5.rb:9:in `block in use?'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/base.rb:48:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/rails5.rb:8:in `use?'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:17:in `block (2 levels) in detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `each'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:16:in `block in detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack.rb:13:in `detect'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/bin/support/ruby_compile:17:in `block in &lt;main&gt;'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:35:in `block in trace'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:18:in `block (2 levels) in instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:40:in `yield_with_block_depth'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:17:in `block in instrument'
remote: 	from /tmp/tmp.f4TZaBjbzE/lib/ruby/2.4.0/benchmark.rb:308:in `realtime'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:16:in `instrument'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/lib/language_pack/instrument.rb:35:in `trace'
remote: 	from /app/tmp/buildpacks/19b20846a186a4065aa6710b809d0c465ff7a2235264402d06b5038f9562c56867a32c308dd2410d96108925a9f9fed74b5ef871dd6514a3a3a5485da3f442cf/bin/support/ruby_compile:15:in `&lt;main&gt;'
remote:  !
Push rejected, failed to compile Ruby app.
remote: 
remote:  !
Push failed
remote: Verifying deploy...
remote: 
remote: !
Push rejected to codereader-backend.
remote:
Any help is appreciated.I'm not sure what steps to take and there aren't any similar errors that I've found.
Thank you.
Is it possible to trigger alarm in zabbix or datadog based on output of bash command.
I.e.
I have a bash command that returns value and I want to trigger an alarm if value rises above some level.
I am investigating options for monitoring our installation in Swisscom's cloud-foundry.
My objectives are the following:
So far, I understand the options are the following (including some BUTs):
That is very useful for tracing / ad-hoc monitoring, but not very good for a serious infrastructure monitoring.
This can be deployed as an app to (as far as I understand) do the job in similar way, as the TOP cf plugin.
The problem is, that it requires registered client, so it can authenticate with the doppler endpoint.
For some reason, the top-cf-plugin does that automatically / in another way.
That can be for example done with  Datadog .
But it seems to also require a dedicated uaa client to register the Nozzle.
I would like to check, if somebody is (was) on the similar road, has some findings.
Eventually I would like to raise the following questions towards the swisscom community support:
In a AWS ECS cluster each cluster instance runs the ecs-agent [1] as a docker container.
Next to that container I run datadog-agent [2] also as a container.
The datadog-agent monitors all other containers and ship their logs to DataDog.
In order to have the log of each container tagged by name I've added a specific docker label [3] to each container with the respective name.
However, I'm not been able to add a docker label to the ecs-agent itself.
Is there a way to add custom docker labels to the ecs-agent container?
[1] -  https://github.com/aws/amazon-ecs-agent
[2] -  https://github.com/DataDog/datadog-agent
[3] -  https://docs.datadoghq.com/logs/log_collection/docker/
We are Moving from datadog to prometheus,
Datadog have google cloud metrics, for example:
gcp.gcp.instance.is_running 
gcp.gcp.project.quota.networks.limit
and more .. (all metrics starting with gcp)
i want to have those google cloud metrics using prometheus - 
is there an exporter to get them?couldn't find anything .
Is there any way to install exe/MSI agents in AWS EC2 instances in an automated way??
In specific, I am looking for a counterpart of Azure's Custom Script Extension.
[Free of cost]
Scenario:
I want to install BigFix and Datadog agents on 1000 Ec2 instances, this is a one time job, so I am not looking for any solution that involves Chef / Puppet, etc.,
I am looking for any API or any way to access Microsoft's Cloudyn service.
I want to extract data - Azure Storage Cost and utilization per month, this is available under the "Management Dashboard of Cloudyn" and want to integrate with Datadog.
I'm using Datadog and NewRelic to try and track down odd behavior that seems to happen at random times.
Recently I have noticed huge spikes in REDIS latency to my application in NewRelic.
I added Datadog to the Redis server and saw these spikes of commands/second going from around ~0.5-2k to over 40-60k!
Along with those is a spike in bandwidth and load, but only very mindor CPU changes.
When these were occurring, GoogleAnalytics (GA) was actually showing a rather slow day in comparison.
In fact, the overall app load today is about 2-3x higher than the day shown in the image below, but today has had perfect REDIS performance without any latency/commands spikes.
Could it be bots/crawlers hitting stale caches that are causing large chunks of data to be inserted at once?
My app heavily relies on an external API, which does occasionally spike in response time as well, but why would a slow API call cause slower redis calls or massive spikes in redis commands?
Understand the options to secure the docker.sock.
As in those articles, giving access to docker.sock is a risk.
However there could be cases where we need to deploy a pod such which needs to talk to docker daemon via the socket for monitoring or controlling.
For example  datadog  which mounts the socket via hostPath mount.
OpenShift requires explicit grant of SCC e.g.
hostaccess to the service account which runs the pod for the pod to use hostPath, but it is OpenShift proprietary.
I suppose SELinux can be used so that any pods who access the docker socker are required to have a certain label.
I would like to know if my understanding of SELinux label is valid, and what other options would be available.
volume-mounting the docker socket into a container is unsupported by Red Hat.
This means that while it is entirely possible to do so (as with any other volume mount), Red Hat is unable to assist with configurations using this setup, problems that arise because of this setup or the security implications/concerns surrounding this setup.
I'm pushing gunicorn metrics from multiple applications into datadog from the same host however I cannot find a way to group the statsd metrics using either a tag or proc_name.
Datadog gunicorn integration
https://app.datadoghq.com/account/settings#integrations/gunicorn
Datadog agent checks are being updated automatically with the  app:proc_name  tag.
I can use this to group and select the data for a specific service.
https://github.com/DataDog/dd-agent/blob/5.2.x/checks.d/gunicorn.py#L53
For the statsd metrics however, I do not see how to assign a tag or proc_name.
This is not being done automatically nor do I see a way to specify a tag.
https://github.com/benoitc/gunicorn/blob/19.6.0/gunicorn/instrument/statsd.py#L90
Datadog config:
Gunicorn config:
Any ideas on how this might be achieved?
Examples using notebooks:
In this example, I am able to select  app:service  in either the 'from' or 'avg by' drop downs.
For the metrics with the my_namespace prefix I am unable to reference the same application name.
Only host and environment related tags are available.
I have a squid proxy container on my local Docker for Mac (datadog/squid image).
Essentially I use this proxy so that app containers on my local docker and the browser pod (Selenium) on another host use the same network for testing (so that the remote browser can access the app host).
But with my current setup, when I run my tests the browser starts up on the remote host and then after a bit fails the test.
The message on the browser is  ERR_PROXY_CONNECTION_FAILED  right before it closes.
So I assume that there is an issue with my squid proxy config.
I use the default config and on the docker hub site it says
Please note that the stock configuration available with the container is set for local access, you may need to tweak it if your network scenario is different.
I'm not really sure how my network scenario is different.
What should I be looking into for more information?
Thanks!
I would like my pods in my minikube-vm to use the network that my docker-machine uses.
Inside the docker-machine I set up a proxy container(datadog/squid) for minikube to access, but I'm not sure exactly how to allow my minikube/pods to use this proxy.
Should I be setting the env vars HTTP_PROXY or is there something else I need to do?
Not sure what I should be looking into, any help would be appreciated, thanks!
For me it is very strange that jmeter does not bring response time for restcall.
These are all possibilites to be saved on jtl/csv file:
My question is either if response time is equal one of the data above or if I can manually calculate sum some values and get to it.
PS: The reason while I don't simply use Jmeter Response Time graph is due to the fact that I send data to Datadog (measurement tool) instead.
I followed a Youtube video by Chris Pettus called PostgreSQL Proficiency for Python People to edit some of my postgres.conf settings.
My server has 28 gigs of RAM and prior to making the changes, my system memory was averaging around 3GB.
Now it hovers around 10GB.
I am not having any issues right now, but I would like to understand the pros and cons of the changes I made.
I assume that there must be some tangible benefits of tripling the average memory being used in my system (measured with Datadog).
My server is used to perform ETL (Airflow) and hosts the database.
Airflow has a lot of connections but typically the files are pretty small (a few mb) which are processed with pandas, compared with the database to find new rows, and then loaded.
I have several applications running on a Glassfish application server (4.0).
I have recorded some statistics of the java memory usage with DataDog, so I am able to see the historic of the used heap memory along with the  initial  and  maximum  constant heap sizes.

 The image shows the initial (yellow), maximum (blue) and real (purple) heap values.
As you can see above, the real heap size is always bellow the initial heap value, so I'm planning to move these parameters to improve the server's performance, but I'm not sure if this is really necessary.
So, I have this doubts:
I guess this questions hold true talking of a tomcat, JBoss or any servlet-oriented server.
Any help will be gratefully received.
I recently watched the  OSCON Austin 2016 talk  on  "Detecting outliers and anomalies in realtime at Datadog"  by  Homin Lee  and found proper motivation to ask the following question.
Basically what I am trying to do is find anomalies in graphs that do not necessarily start at the same time ( t ) but are quite similar (in family) in shape.
Separated:
Combined:
As depicted in my (rough) concept drawing, given two similar frequency ( f ) functions of time, I want to line them up on top of each other on the basis of where each of their inflection points are.
One of the frequency graphs starts at  t=-2  and the other  t=5 .
They both have inflection points around  t_1=8.5  and  t_2=1.5 .
This is where I want to line them up.
Essentially, the image drawn should be the final output from my algorithm and listing any anomalies that trigger like say for the green curve, if  f=0.2  at  t_1=12 , then that should be an anomaly because it is not in family.
And as Homin Lee says it, the graph would not be "within the trained envelope."
Now I want to lay out what my particular approach would be and see if you think the same or have a better approach to develop this algorithm.
Before we choose which anomaly detection algorithm to use, we need to discuss how to prepare this data.
We will continue to use frequency-versus-time data for example purposes.
To prepare the data, we need to (1) find the inflection points, (2) scale the data so that the data all have the same time domain duration (i.e.,  12-5=7=7=5-(-2) ), and (3) find a way to match (line up) the times of each graph (i.e., 5 to -2, 6 to -1, and so on).
Once the data is prepared, now it is onto the algorithm.
For (robust) anomaly detection, I was thinking about using  one-class / multi-class  Support Vector Machines (SVM) because we are going to be training a huge set of graphs to form the "envelope."
This section is also open for suggestions.
As a moonshot thought, I would like to eventually be able to put all of the graphs onto one huge plot and point out the anomalies from there.
The problem is that there would be so many different time intervals.
So one solution would be to create a singular universal ( u ) time interval so that you don't have to deal with the differing intervals (e.g.,  t_1=5,9  would become  t_u=1,5  and same goes for  t_2 ).
So to recap, I am looking to analyze similar graphs on different time intervals for anomalies.
Find critical/key points (not necessarily inflection points), scale, plot graphs, and check for anomalies.
I have rambled on for long enough but if something does not make sense and you would like me to clarify or elaborate, let me know and I will.
Feel free to make suggestions, submit to me some code, and/or any other ideas or approaches that I did not necessary think of before.
Thank you.
P.S., sorry about the drawings; I tried my best.
:P
I'm using Datadog for Ansible.
I have a role which installs the Datadog package but doesn't run the datadog role automatically after the package installation.
Currently, we need in each project to call Datadog role manually.
Is it possible to call Datadog role in my role1 instead of having to write "datadog.datadog" everywhere after role1.
Precisely, can we execute a role after a task which is responsible to install this role ?
Thank you in advance :)
In DockerCloud I am trying to get my container to speak with the other container.
I believe the problem is the hostname not resolving (this is set in  /conf.d/kafka.yaml  shown below).
To get DockerCloud to have the two containers communicate, I have tried many variations including the full host-name  kafka-development-1  and  kafka-development-1.kafka , etc.
Within the container I run  ./etc/init.d/datadog-agent info  and receive:
SSH Into Docker Node:
I log into the containers to see their values, this is the  datadog-agent :
This is the  kafka container :
Datadog  conf.d/kafka.yaml :
Can anyone see what I am doing wrong?
I am implementing a recommendation engine in .Net C#, I am using Cassandra to store the data.
I am still new in using C*, just started using it 2 months ago.
At the moment I have only 2 nodes in my cluster (single DC), deployed in Azure DS2 VM (each has 7Gb RAM, 2 Cores).
I set  RF=2, CL=1  for both read and write.
I set the timeouts in yaml config file as below
I set lower read query timeout in client side (30 secs each).
The data stored in cassandra is user history, item counter, and recommended items data.
I created an API (stands in equinix DC) for my recommendation engine, its work is very simple, only reading all recommended_items Id from recommended_items table in C* everytime a user opens the website page.
It means that the query is very simple for each user :
When I did load testing for up to 500 users/threads, it was fine and very fast.
But when the online site calls API to read from C* table, I got read timeouts very often.
There were usually only less than 20 users at the same time though.
I monitor the cassandra nodes activity using DataDog and I found that only node #2 that keeps getting timeouts (the seed node is node #1, though what I understand is seed doesn't really matter except during bootstrapping step).
However, everytime the timeout happens, I tried to query using cqlsh in both nodes, and node #1 is the one that return
OperationTimeOut Exception.
I have been trying to find the main root of this issue.
Does that have anything to do with coordinator node being down ( I read this article ) ?
Or is that because I have only 2 nodes?
When the timeout happens (the webpage shows nothing), then I tried to refresh the page that calls the API, it will be loading for long time before showing nothing again (because of the timeout).
But surprisingly, I will get the log that all those requests were actually successful after few minutes even though the web page has been closed.
It's like the read request was still running even though the page has been closed.
The exception are like these (they didn't happen together) :
OR
Does anyone have any suggestion about my problem?
thank you.
output of cfstats .recommended_items
NODE #1
NODE #2
My name is Daniel, 
I'm a newcomer accountwise but a long time lurker.
I decided to learn Apache Cassandra for my next "lets write some code while the kids are sleeping" project.
What i'm writing is a neat little api that will do read and writes against a cassandra database.
I had a lot of the db layout figured out in mongodb, but for me it's time to move on and grow as a engineer :)
Mission:
I will collect metrics from the servers in my rack, an agent will send a payload of metrics every minute.
I have the api part pretty much figured out, will use JWT tokens signing the payloads.
The type of data i will store can be seen below.
cpuload, cpuusage, memusage, diskusage etc.
The part where i am confused with cassandra is how to write the actual model, i understand the storagengines sort of writes it all as a time serie
on disk for me making reads quite amazing.
i know anything i would whip together now would work for my lab since it's jsut 30 machines, 
but i'm trying to understand how these things are done properly and how it could be done for a real life scenario like server density, datadog , "insert your prefered server monitoring service".
:)
But how are you more experienced engineers designing a schema like this ?
Usage scenarios for the database:
Read the assets associated with ones userid
Generate monthly pdf reports showing uptime and such.
Should i insert the rows containing the full payload or am i better of inserting them per service basis: timeuid|cpuusage 
Per service row
All in one
In mongo i would preallocate the buckets, and also keep a quick read avg inside of the document.
So in the webgui i could simply show the avg stats for pre-defined time periods.
Examples for dumbasses are highly appreciated.
Hope you can decipher my rather poor english.
Just found this url in the SO suggestions:
 Cassandra data model for time series 
i guess that is something that applies to me aswell.
Sincerly
Daniel Olsson
I am using  ElasticBeanstalk with single docker container .
I am using DataDog(statsd client) for pushing metrics from the docker container.
I have a running datadog-agent which is technically a statsd client on the host machine.
The issue I am facing is to connect that client running at port 8125 from the container.
What I have tried is:
Thanks in Advance
We have been handling a rather peculiar issue with  clock offsets between VM's (Windows Server 2019) on Azure, that are hosted in the same region and datacenter, moreover in a VMSS .
Several facts regarding the issue, after doing some experiments in the last three months:
Would appreciate any ideas.
I am using cloudwatch subscription filter which automatically sends logs to elasticsearch aws and then I use Kibana from there.
The issue is that everyday cloudwatch creates a new indice due to which I have to manually create the new index pattern each day in kibana.
Accordingly I will have to create new monitors and alerts in kibana as well each day.
I have to automate this somehow.
Also if there is better option with which I can go forward would be great.
I know datadog is one good option.
I have setup a 4 broker Kafka cluster on AWS MSK (version 2.2.1).
I am monitoring the same through datadog (crawler setup from  https://docs.datadoghq.com/integrations/amazon_msk/ ).
Now as per my understanding, the fetch follower total time is the sum of the other metrics.
But you can see all of them &lt;1ms while fetch follower total time is over 200ms.
I have not really changed the default Kafka config much:
Can someone suggest a possible cause of this high follower fetch time and how to reduce it?
Any approches to identify the cause are also welcome.
I'm sending UDP packets (statsd) from pods on a host to  &lt;hostIP&gt;:8125 .
On the other end, a collector (datadog-agent using  hostPort ; one per host via DaemonSet) picks up the packets and does it's thing.
Generally this works fine, but if I ever delete + re-create the collector ( kubectl delete pod datadog-agent-xxxx ; new pod is started on same IP/port a few seconds later), traffic from  existing  client-sockets stop arriving at the collector (UDP sockets created  after  the pod-rescheduling works fine).
Re-starting just the agent inside the collector pod ( kubectl exec -it datadog-agent-xxxxx agent stop ; auto-restarts after ~30s) the same old traffic  does  show up.
So containers somehow must have an impact.
While UDP are (supposedly) stateless, something, somewhere is obviously keeping state around!?
Any ideas/pointers?
Each "client" pod has something like this in the deployment/pod:
On the collector (following  datadog's k8s docs ):
This happens on Kubernetes 1.12 on Google Kubernetes Engine.
I am looking for JMX metric(s) for Kafka Broker [Not more than 1 or 2, if possible] which at a high level can identify the health of the cluster?
I have referred to the list compiled by datadog and confluent, but couldn't find anything similar.
Absolutely love micrometer.
Running a spring boot JAX RS app.
Am using datadog registry and wanted to time some service methods.
I saw all rest end points are timed and detail sent - thats great.
However here is the issue I am facing.
If I have a GET call that takes a path param, for example  /employees/{empId} , And If I have a million users in the system, then I guess we will have 1 million tag combinations pushed to datadog.
So question is how to get around this.
In this case I am only interested in all of the GET calls and not a specific call to get a specific employee.
So how do I also tell micrometer/spring-boot to use the pattern rather than actual values.
(I am not using @Timed on the JAX-RS resource classes.
Is that the way to go ?)
My function scrapes my servers for the command and outputs something along the lines of  offset=1.3682  which  metrics_emit  uses to send to our metrics collector/visualizer, datadog.
What I need to do is strip off the  offset=  part because  metrics_emit  only wants the numerical value.
What would be the best way of stripping  offset=  as well as calling  strip()  on  i  so that it gets rid of all newlines and trailing/leading whitespaces?
We have a requeriment to monitor certain counters on our production applications.
Each application has it's own AppPool.
We know how to attach and monitor the performance counters, but the problem we face i sthat IIS worker processes are recycled, and thus, the instance names for those performance counters, invariably change.
Our operations and sysadmins use external tools to monitor several counters and metrics.
In order to include .NET CLR counters, we need to rely on the instanceId being stable, or else the monitoring needs to be setup again if the worker process dies or gets recycled.
Does anybody know a way around this limitation?
Maybe keeping the instanceIds stable (same PID!?)
or by hooking these external tools (Datadog, WhatsUp, PerfMon, etc) to something that doesn't change?
I am new to writing tests in java, and seem to be unable to test if a method of a class is called.
I am sending metrics to datadog, and want to test in the code if a function of another class was called.
It says I need to mock first, but I couldn't get it to work.
MetricRecorder.java
MetricRecorderTest.java
When I run the test I get this =  org.mockito.exceptions.misusing.NotAMockException: 
Argument passed to verify() is of type NonBlockingStatsDClient and is not a mock!
Any idea of how I should be testing if recordHistogramValue was called, and if so with what arguments?
I'm trying to send data to datadog using kamon.
My setup is the following:
I'm getting the following exception at akka startup:
I am using Ansible  Datadog role  and trying to install and configure datadog agents in target servers however, i am stuck at a point where i need to use host variables and update a section of the playbook using these variables.
The variable has got multiple values separated by a space.
I want to ensure that these values are added in the playbook based on the variable values.
Following example will help in understanding the requirement.
Playbook:
Here, the tag value AID is using a host variable with the same name i.e., AID and in some cases this host variable can have values like the following:
AID: 100 101 102 103
Is there a way that the while executing tag section of the playbook is parsed based on the variable values in following format.
I believe i cannot use templates for such requirements since the configurations are used under vars in the role.
Any suggests would be appreciated.
I'm trying to set up a Datadog PostgreSQL integration that requires a user with  pg_monitor  role and  SELECT  permission on  pg_stat_database  as described on their own  documentation .
My database is currently hosted on Heroku and it seems the default user doesn't have  SUPERUSER permissions because, when I try to apply the above role and permission to a &quot;monitor&quot; user I have the following error message:
ERROR:  must have admin option on role &quot;pg_monitor&quot;
So I'm looking for some way of:
Someone has ever faced this issue?
There is a way to handle this case?
Our infra for web application looks like this
Nodejs Web application -&gt; GraphQL + Nodejs as middleware (BE for FE) -&gt; Lot's of BE services in ROR -&gt; DB/ES etc etc
We have witness the whole middleware layer of GrpahQL+Nodejs gets latent whenever any of the multiple crucial BE service gets latent and request queuing starts happening.
When we tried to compare it with number of requests during the period it got latent it was &lt;1k request which is much lower than the claimed 10k concurrent request handling of nodejs.
Looking for pointers to debug this issue further.
Analysis done so far from our end:
We have a Rails 4.2 app and are currently using a shared cache across several apps.
Our memcached miss rate is pretty high (like 85% hits and 15% misses) but this is complicated by the fact that multiple apps are sharing the same memcached instance.
So we might be getting a high miss rate for a couple of critical cache processes (our DataDog data would support this).
Is it possible to specify a cache store on a fragement cache call like:
I think this is possible with object caching by doing something like:
Would there be other ways to untangle the hit / miss ratio of specific cache actions?
I set up a simple health check in nestjs using terminus.
In Datadog APM, I see dozens of failed fs.stat() calls for various paths under /dist
But the health check seems to complete fine after those calls.
Any idea why this is happening?
Let's say I have a daemonset running in k8s cluster, and the cluster has other deployments pods (e.g.
nginx webserver).
I would like the daemonset change/update config files in the deployment pods (e.g.
change nginx worker_processes or worker_connections in /etc/nginx/nginx.conf of the nginx pods from the daemonset).
This process is part of the automated or continuous performance tuning workflow that I would like to implement.
In short, I'm looking for ways to have the daemonset change config files in other pods depending on performance metric continuously.
I will have the daemonset continuously measure the performance or get performance metrics from another metric servers (Prometheus, new-relic, datadog, or others).
Depending on the performance measures, I want to take action (tune the pods) from the daemonset.
So the only problem I have from this workflow is how to access pods from the daemonset.
I appreciate any feedback or solutions on how to achieve this.
C++ and CMake newbie question regarding how to integrate a third-party library into my own code.
I'm trying to add Datadog metrics to C++ application.
The  officially-endorsed library  doesn't state how it can be integrated.
I imagine it should tell me how to import it like this:
This is my understanding of how to integrate a third-party library (don't you wish there is &quot;pip&quot; in C++?).
But the names in &lt;&gt; are not provided in the README.
I certainly don't have to do it like this as long as I can use CMake.
Any help is appreciated!
We have a consumer using librdkafka (via confluent-kafka-python) that we would like to monitor with DataDog.
We are trying to get  records-consumed-rate  which is a recommended thing to watch.
The problem is, I can't seem to find it in the librdkafka stats object.
Is there a way to get this metric?
librdkafka stats docs
Confluent docs showing that this indeed a thing
Datadog recommendation to watch this
I have reverse proxy on running k8s.
I want to log request and response headers log.
I did not manage to it.
Could anyone help me ?
It could be console log.
I have Datadog agent on my Cluster.
So can can get request logs from NGINX.
I have recently been creating a POC using the new  DataDog/Azure DevOps Integration .
The purpose of doing this is to aggregate all of my build/release logs, PR data, etc into DataDog to build insights, alerts, dashboards, etc.
The DataDog charts are very nice, but I would prefer to use Azure Log Analytics as this is where most of my company's log and metric data is aggregated already and the ability to correlate it would be helpful.
Note, I realize that Azure DevOps has Analytics charting and PowerBI integration, but I would like to use Log Analytics to store the metric and log data, if possible.
The Azure DevOps ServiceHooks do not have Azure Log Analytics as an option (see image below).
Maybe the trick is to push it to Azure Service Bus and then push it to Log Analytics?
I have looked at the  Azure DevOps Reporting documentation  and I didn't see anything obvious.
If anyone knows of any good blogs on pushing data from Azure DevOps to Log Analytics, I'd really appreciate it.
Unfortunately most of my searches come back with advice on how to use Azure DevOps to provision monitoring of external applications with App Insights and Log Analytic rather than the other way around.
I can imagine using a scheduled task calling the Azure DevOps API and pushing it into Log Analytics API, but that seems like the least elegant and most error prone solution.
Any thoughts on the best ways to monitor this data are appreciated.
Thanks!
I have a custom metrics which is pushed to the metrics endpoint, I use the metrics endpoint to push to datadog.
I wanted to know how to test DatadogMeterRegistry in spring boot application.
I found something in  GitHub  but it was dead-end
Scenario:
I have a remote server which is monitored (via DataDog) and sends out a warning when some anomaly is detected.
This warning can be fetched via a webhook.
Now I want to connect that webhook ( https://docs.datadoghq.com/integrations/webhooks/ ) with MS Teams (probably via Bot), to receive a warning.
Then I want to send a command back to the remote server to resolve the warning.
Technology:
MS Teams, Python flask/Django, remote server
Expected Results:
I can receive a warning from my remote server to MS Teams via a bot.
Then send a command back to the remote server.
My initial plan is doing this using Python Flask/Django but not tied to a specific language.
Environment:
Remote server is a LINUX based system.
we have a internal network that is used within our company, so might need to resolve a firewall problem potentially (idk whole lot about it tho).
Things I have tried:
I just want to see if this is possible or not, so i havent coded up any.
But I found some information relevant to our problem:
https://docs.datadoghq.com/integrations/webhooks/
https://docs.microsoft.com/en-us/microsoftteams/platform/bots/how-to/create-a-bot-for-teams
https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/what-are-webhooks-and-connectors
One last note, I am not also tied down to Teams bot.
Our company also uses Azure Devops, so that is another resource I can use to realize the solution.
Btw, this question was posted on  here  but was told to post on MSDN, but I couldn't find an appropriate forum.
Hence, I am posting on stack overflow instead
We recently started using SCDF on Kubernetes, and we are trying to workout the kinks.
One of thing things that i was'nt able to find is whether there is a way to affect logging format,for ex.
switch to using Json format.
Reason for this is simple, we are using Datadog as our logging platform, and with Json, you don't have to write custom log parsing rules.
With regular log format, you will endup with something like this
For example in Node.js container I do:
 throw new Error('lol');  or  console.error('lol'); 
But when I open container logs:  docker-compose logs -f  nodejs 
there are no any statuses or colors like all logs have info status.
I use Datadog to collect logs from container - it also mark all logs as 'info'.
I'm trying to monitor my celery queues and which tasks they are running.
The main idea is to get a better understanding of how everything works.
I'm fairly new to celery outside of calling  delay  or setting up a  periodic_task .
I'm running into a bit of a pickle, no pun indented and I'm using datadog to monitor some of this information.
What I setup is an  after_task_publish  function that I'd like to use to track the task and queue it was processed on.
I actually would like the worker as well.
I have extended my celery results so I can look things up in redis and see the queue.
Currently the tasks  _exec_options  queue is always  None .
Below is an example.
Now this I don't understand because I set the queue, and also know the job ran so it had to be queued.
I'm trying to figure out if my configuration of celery is not doing what I anticipate or if I am going about trying to find the queue and worker that processed the job in the wrong way.
Any direction would be appreciated.
I am currently setting up my microservice application and am currently in the phase of error tracking and logging.
I am unsure on what the best practices are when it comes to this.
I have done some research on services that provide this.
I feel there is a difference between application logging, like user actions and error tracking.
When it comes to error tracking I was planning on using a service like  Bugsnag  or  Sentry  which will allow me to follow a stack trace to pin down the error.
However when it comes to application logging I am debating whether to build that in house or also use a service like Datadog or Papertrail.
However it seems like Datadog and Papertrail help with lower level logs like performance, etc.
Stripe is a great example of clear logging.
You can see the chain of events for each user in an easy to read format.
To achieve a similar experience, I was thinking of creating a logging microservice which will be a consumer on a kafka broker and every microservice will produce a message when a certain function/user action is invoked and I can store in my db the user info and what action was taken.
Is this standard industry parctice?
Is there a service that can acheive this experience?
I'm running Postgres 11.
I have a table with 1.000.000 (1 million) rows and each row has a size of 40 bytes (it contains 5 columns).
That is equal to 40MB.
When I execute (directly executed on the DB via DBeaver, DataGrid ect.- not called via Node, Python ect.
):
it takes 40 secs first time (is this not very slow even for the first time).
The CREATE statement of my tables:
On 5 random tables: EXPLAIN (ANALYZE, BUFFERS) select * from [table_1...2,3,4,5]
When I add a LIMIT 1.000.000 to table_5 (it contains 1.7 million rows)
When I add a WHERE clause between 2 dates (I'm monitored the query below with DataDog software and the results are here (max.~ 31K rows/sec when fetching):  https://www.screencast.com/t/yV0k4ShrUwSd ):
All tables has an unique index on the c3 column.
The size of the database is like 500GB in total.
The server has 16 cores and 112GB M2 memory.
I have tried to optimize Postgres system variables - Like: WorkMem(1GB), shared_buffer(50GB), effective_cache_size (20GB) - But it doesn't seems to change anything (I know the settings has been applied - because I can see a big difference in the amount of idle memory the server has allocated).
I know the database is too big for all data to be in memory.
But is there anything I can do to boost the performance / speed of my query?
How do I get all the metrics including status codes and exceptions using micrometer and statsd with flavor datadog.
I am using maven dependency for micrometer-statsd and spring-boot actuator ?
I have added @Timed annotation according to spring boot actuator configuration to a controller.
But in the graphite I only see http.server.requests.max BUT no exceptions or status codes.
Can someone point out what config am I missing ?
I would like to create a file to which I can write as described in the  Datadog Datagram docs :
Everything that is written to that file should be – instead of being handled by Datadog and sent to them via the agent – written to a log file.
After executing the three lines above the log file should contain the following:
I thought that a named pipe and a background process that handles that would be perfect.
However, it does not work as expected and the background process never writes anything, even though writing seems to work.
I created the following script:
And the following systemd service:
The service is started correctly after executing  systemctl enable --now datadog-agent , however, as I said, nothing is ever being written to the log file.
This is very strange to me because opening two shell instances where I write the following in the first shell:
And then start sending data in the second shell prints the lines correctly.
I am looking for any tips or advice on how to troubleshoot our Elasticsearch cluster.
This cluster has been running flawlessly with only minor maintenance for a couple years.
We suddenly experienced an outage a week ago today.
I have been struggling to keep it running ever since (it is actually our dev environment, so not production, but our devs are impacted, and we are worried the same thing can happen in production).
The symptom is this:
We have three client nodes that coordinate requests with the data nodes.
As soon as there is any kind of traffic, I see constant Garbage Collection in the logs.
Example:
[2019-08-09T00:38:15,835][WARN ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1034] overhead, spent [694ms] collecting in the last [1s]
[2019-08-09T00:38:17,066][WARN ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1035] overhead, spent [693ms] collecting in the last [1.2s]
[2019-08-09T00:38:18,079][INFO ][o.e.m.j.JvmGcMonitorService] [client-vm0] [gc][1036] overhead, spent [352ms] collecting in the last [1s]
At some point the client loses communication with the master:
[2019-08-06T14:38:54,403][INFO ][o.e.d.z.ZenDiscovery ] [client-vm0] master_left [{master-vm1}{PZJChTgxT46h4YYOqMr2fg}{1G3fXiSMQ5auVH-i5RH10w}{10.0.0.11}{10.0.0.11:9300}{ml.machine_memory=30064300032, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true}], reason [failed to ping, tried [3] times, each with maximum [30s] timeout]
[2019-08-06T14:38:54,419][WARN ][o.e.d.z.ZenDiscovery ] [client-vm0] master left (reason = failed to ping, tried [3] times, each with maximum [30s] timeout), current nodes: nodes:…
[2019-08-06T14:38:54,434][INFO ][o.e.x.w.WatcherService ] [client-vm0] stopping watch service, reason [no master node]
It tries to find another master, but is unable to:
[2019-08-06T14:41:56,528][WARN ][o.e.d.z.ZenDiscovery ] [client-vm0] not enough master nodes discovered during pinging (found [[]], but needed  2 ), pinging again
During this time, the masters and all the data nodes are perfectly find.
I have usually seen the above when there is a large GC and the *  master  * loses contact with the *  client  * because it is too busy with GC to respond.
But in this case, the client can’t find the master.
Eventually the client suffers an Out Of Memory failure and the JVM crashes.
I am assuming that the memory issues, the GC and the crashing are all related, but I am having a problem figuring out what the cause is, and why so sudden.
Cluster details:
ElasticSearch
Version: 6.3.2
License: Open Source
Nodes:
Client (3) (D13_v2): 8 CPU; 56 GB RAM; HDD Drives
Master (3) (D4_v2): 8 CPU; 28 GB RAM; HDD Drives
Data (35) (DS13): 8 CPU; 56 GB RAM; SSD OS &amp; 3x1TB SSD data drives
Indexes:
Taxonomy:
Size: 1.45 GB
Shards: 2
Replicas: 1
Support:
Size: 365 GB
Shards: 30
Replicas: 1
NonSupport:
Size: 2 TB
Shards: 80
Replicas: 1
JVM
Java Version: (build 1.8.0_144-b01) Note: we stayed with this build as it was what had been running for most of the last year and we wanted to start from a good state.
ES_HEAP_SIZE: 28672m (roughly half available memory)
Note also that we have a current issue with field mapping explosion that has grown.
This may be a culprit that we are investigating.
If I shut off external access to the cluster, then everything is roses.
When I open back up again, the clients go down in minutes.
If I clear out all pending requests that I can see (we have a lot of queue based traffic), then it seems to be fine for some measure of time (~12 hours), but then it must reach some load where things fall over again.
I have tried modifying the heap size to try to adjust GC time.
I have reduced mappings in the taxonomy index, and that seemed to help.
My current questions:
I also find that I am unable to inspect the hprof crash dumps as they are just too huge (~45GB), so I can’t get any info there on why the JVM might be crashing.
I set up DataDog when I started the investigation, but it has a lot of data I don’t know how to interpret
Any advice would be appreciated.
Thanks,
~john
We're utilizing the  django-rest-framework  to create a RESTful API and using generic views or view sets to create the endpoint views.
There is no templating happening, all the frontend is in React.
However, upon watching the traces in Datadog, we're seeing that SOMETIMES (not every time), Jinja2 is rendering, causing a 500-800ms latency.
Does anyone have any clues to why this might be happening and how to turn it off?
I'm setting up datadog monitors/alerts and want to have alerts routed to slack or pagerduty depending on if the issue is in our production environment or not.
I've created multi-alert monitors that alert correctly, but I can't figure out how to make only ones where  environment.name  is equal to  prod  send an alert to pagerduty, and always send them to Slack.
I was hoping to be able to do something like the following in the alert message but haven't been able to figure out a syntax that works:
For now, I've found a work around of having two monitors that are duplicates of each other where one has is scoped to production only and alerts pagerduty only and the second is for all environments and alerts slack only.
However, I know this is going to become a maintenance nightmare as we grow and I'd like to know if there's a better solution.
So i need to figure out how to write the following path into regexp
/private/toolbox/*
I'm not sure how to do it because of the *
I have add the following 2 paths with no problem
/private/healthcheck
/private/datadog/dashboards
I'm using Datadog's  statsd client  to record the duration of a certain server response.
I used to pass in quite a few number of custom tags when  time -ing these responses.
So I'm in the process of reducing the number of custom tags.
However, the problem is that when I reduce the number of tags passed in, there is extra latency of server response, which isn't intuitive because I'm passing in fewer tags and the implementation hasn't changed.
According to Datadog and Etsy (which originally released  statsd ), these methods that record these metrics aren't blocking.
However, they must be using some extra threads to perform this.
What could be the issue?
Are there possible any side effects associated with using this client?
I have an application running under spring boot utilizing SMBJ to mount and read remote files, and it works perfectly.
However I am trying to set up some datadog reporting and trying to use JMX as a datasource for datadog...
TO do this I am staring the springboot jar with the following:
And once I do this, SMBJ no longer creates the mount.
If I remove these parameters the code works fine again and SMBJ is able to create/mount to the share, If I have them it simply times out trying to create the share.
I thought maybe it was the RMI hostname change, but removing just this this doesn't seem to fix it.
Can anyone offer any help on this?
Is SMBJ really dependent on the jmxremote settings?
It certainly seems to be..I have tried removing the overriding of the ports, so they go to their default ports as well, but this didn't fix it either.
Any help would be appreciated.
We need to separate logs generated by a REST web-service on a user-specific basis and eventually import these logs into an aggregation framework like Datadogs.com.
There are several ways to approach this and I’m interested in getting feedback before selecting an approach.
The basics would go something like this:
Depending on the stage
For development, use the NLOG File Logger, so the developer can simply “tail” the log file.
Use the variable in the nlog filename target as 
 &lt;target filename="/path/file-${var:userid}.log"/&gt;.
Or use the, it in the nlog target as:
 layout="${var:userid}-${OtherLayout} ", and have developers do a  tail -f masterFile.log | grep USERID.
In production switch to using the nlog JsonLayout, so a system like DataDog can read Time, Threadid, Userid and message data.
Use the JsonFormat and specify a userid attributes.
I see that the JsonFormat has supports for MappedDiagnosticsLogicalContext but I would prefer the simplicity of the ${var:xxx} format to specify the value.
Problems:
I’ve used the MappedDiagnosticContext in the past and then specified it in the target filename.
I just attempted to use the ${var} approach and it did not appear to work?
?
Concerns:
I like the filename target approach for development.
It should work well with 10-100 of users but not scale with 1000's of users.
Clearly with 1000’s of users we would need to close the log file after each line is written, as we don’t want to keep 1000’s of files open.
A major concern is threading.
It’s possible that each webservice is called multiple times by different users and all approaches require Nlog MappedDiagnosticContext or ${var} capability’s to be thread safe.
Is it?
Any issues to consider?
Eventually we would like to introduce some structured logging into the system, but the majority of the code base was built using standard logging techniques.
If the objects being logged in the structured logging included userid then much of this complexity could be avoided, but that would require a lot of work rewriting for structured logging.
Clearly, there is a lot to think about and I know I’m not the first to ponder this.
Your input will be appreciated.
I'm trying to monitor our postgresql DB and identify the 20 largest tables and than see when was the last vacuum and analyse took place.
I have this query that shows me the largest 20 schema name/relname which is good and that's what I was looking for:
I also have this query that shows me all the analysis I want to see with schema name and relname:
But I'm having a real hard time combining them together to one query that will show me when those analysis only for those 20 tables.
Once this is done I'm looking to view the results in some sort of a graphic view in datadog, so If anyone have a good idea how to run this query as a datadog posgres query it will be amazing as well.
I have a metric client that looks something like:
Then in my application I import it and use it
This publishes the log  Updating metric metric1  in datadog and I can see it.
But this will only publish the first instance.
Until I restart  service nginx restart , I will not get any more increments.
Update
So I have a  start.lua  in  /etc/nginx/conf.d/start.lua  that is:
And the nginx config is
If I were to copy/paste the metric code into  start.lua , then the metric is updated every time.
Why is this?
!
Update
I noticed this in the error logs:
This happens on the 2nd request to the nginx; the first time after restart, this is all fine ...
Update 2
This happens only if I have a  metrics  file and  require  it in my other.
So if I instantiate the  resty_dogstatsd  client inside the main lua file, then everything is fine ...
I have installed data dog agent on one of my virtual machines When I have altered my NSG so that all "Outbound-connections" are denied, I am still able to see "CPU metric" getting updated on Data dog dashboard.
I would like to know where this information is going from Azure to Datadog.
We have an application hosted on Heroku and logs are getting redirected to Sumologic.
I see some options can be passed to JVM where log files will be generated locally.
Question: Is there a way we can redirect these logs to cloud log analyzers like Splunk, SumoLogic, or Datadog?
I have configured the docker-daemon,and also added modified the auto_conf.
How should i pass the  %%host%%  variable?
changed the etcd.yaml
but when i try to do
sudo docker exec -it dd-agent /etc/init.d/datadog-agent configcheck
the collector logs show
I'm sending metrics from a C# web service to datadog.
I need to track the length of words that are being searched in an api call and display this in a histogram.
But datadog is averaging these values which is not what I want.
If one string is 1 character in length and another string is 10 characters in length it records a metric of 5.5, which isn't much use to me.
Ideally I would like a histogram graph over a time period e.g.
an hour, showing the number of instances of 1, 2, 3 etc.
that were recorded during that time period.
Is that possible in datadog?
This is the call I'm making in the code:
I have a python program that crunches a large dataset using Pandas.
It currently takes about 15 minute to complete.
I want to log (stdout &amp; send the metric to Datadog) about the progress of the task.
Is there a way to get the %-complete of the task (or a function)?
In the future, I might be dealing with larger datasets.
The Python task that I am doing is a simple grouping of a large pandas data frame.
Something like this:
here, the categoryList has about 20000 items, and df is a large data frame having (say) a 5 million rows.
I am not looking for anything fancy (like progress-bars..).
Just percentage complete value.
Any ideas?
Thanks!
I would like to detect bad/faulty aws instances using datadog's outlier detection.
Is that possible?
I'm trying to create an automatic failover scenario using datadog.
Any suggestions would be appreciated.
We're setting up monitoring for SphinxQL query_log in DataDog, and we'd like to understand what each value represents in the logging format.
We understand all of the values except for  conn , which we're not seeing an explicit definition, but are making an educated guess that it might be a connection id.
We'd like to know for sure.
The standard log format for Sphinx:
The SphinxQL logging format:
You can see the SphinxQL format adds a  conn  param after the query date, but the docs and referencing the standard log format do not make clear what  conn  is.
We think it's a connection id, but I'm hoping someone with more expert knowledge of Sphinx can help clarify.
I have terraform config as below, which I am using to bring up ECS cluster in various prod/uat/dev environments.
In our infra, we configure the  logConfiguration  of container definition, based on targeted environment.
if infra being brought up by developer and  env:dev  , we configure the  logConfiguration  to use  awslogs  as logging driver and its subsequent log driver options ( i.e awslogs-region, awslogs-group, awslogs-stream-prefix )
if infra being brought up by our continuous deployment pipeline with  env:prod or uat , we configure the  logConfiguration  to use  awsfirelens  logDriver and its subsequent options , we also add fluentbit container definition ( we use DataDog as logging / monitoring solutions, so we bring up the fluentbit container beside our service container to forward the logs to datadog endpoint )
As of today, below configuration is working out just fine.
however it has the code duplication of my service container definition for each log configuration use-case as I am using ternary operator to decide which container definition to generate based on env.
This makes my code un-necessary long and error prone to work with ( If I add/substract environment variables for my service, I have to do that in both the use-case.
If I change my service config in the future, I have to keep maintaining that in both the use-case etc etc. )
How can I improve above TF config, so that terraform will conditionally generates the container definition based on env: I passed in?
If I pass env:dev it will log to awslogs logging driver and its options, if I pass env:uat/prod , it will log to awsfirelens log driver and also add fluentbit container definition in task definition.
During years I used to work with SUMO.
I used to do things like:
And from there you would add time slices, to count on time basis.
And then you could also integrate the results with some nice charts like time series, pie chart.
All within seconds.
Now, I forced to use DataDog.
Imagine you have errors like:
I would like to count errors by place.
In this case:
Well I can't do it with Data Dog.
I hate it.
I read about Pipeline, Processors, Metrics.
I can't believe this is so difficult compared with SUMO.
I have a Pipeline, who parses my logs.
Then I can see all the fields being parsed.
But how can I create a metric from there?
My fields are strings, so I can't &quot;measure&quot; them.
I just need to count how many &quot;house&quot; errors happened.
And so on with some other strings.
In Laravel, the general way to debug performance is to use  Laravel Debugbar
What that doesn't include is things like:
With all of this, it's difficult to debug where the performance is being slowed down.
Does anyone have any tools or details on how to debug FULLY end to end and each little area?
Visual tools preferred.
Haven't looked as much into Xdebug, but I see tools like Datadog/Newrelic but haven't played with those much either.
Would be great to be able to debug it locally rather than in production environment.
I want to be able to see that from all my middleware, view composers, included partials that there's one particular function that's causing 200ms due to a query or inefficient piece of code and I haven't found anything that can do that easily without adding breakpoints or start/stop timers everywhere but the app is too big for that.
Any help appreciated!
Background of the issue :
We are using Magnolia CMS with customized UI.
As a first step of using Magnolia, we are migrating old content including documents to Magnolia.
Content migration is working fine.
Issue with DAM.
We have more than 50 GB of historical content to be stored in Magnolia.
Storing in DB became very expensive.
We decided to save them in File system.
Magnolia running in Kubernetes cluster as a service.
Problem: After we migrate dam from old system to magnolia, we are trying to publish asset from author to public instance.
After MAx 200 documents published, It is killing the pod.
In logs, we are not seeing anything.
Last message is Catalish.sh killed...
Could you please suggest if anyone come across such scenarios.
Best practices to implement dam and other configuration.
In Datadog, have seen that threadcounts are always more and GC is going crazy.
Thank you in advance.
For e.g.
tags:
When using above syntax not getting logs in Datadog for component2 only getting logs only for component1 so we want to parse logs for both the component i.e component1 and component2 so how would get it?
In my project haproxy is used.
But time to time I'm getting backend session limit exceed in haproxy when check with datadog graphs.
At the same time when check the frontend session count it also bit high than the normal time.
I need to know following things.
What is the connection between frontend sessions and user requests come to the frontend?
Single user getting one session for all the requests or multiple sessions?
For one frontend session backend going to create one session or multiple sessions?
If the frontend sessions are hight then can we say requests count to the haproxy is high?
If possible can anyone share a tutorial to clearly understand connections between user requests, frontend sessions and backend sessions.
Thank you.
I am using datadog with the agent, parsing logs in a file in json format.
The way I found to tag json attributes is to edit a pipeline configuration and do something like
But I have to do it for each individual tag key, which implies to know the list of tags in advance and maintain it over time.
It's not ideal.
I'd like to know if there is a way to log things in this format
And have a pipeline configuration taking all key/value pairs in the &quot;tags&quot; value of the log, and tag each one of them.
What I thought about was doing this
But datadog's documentation is not clear about native attributes and their names so I don't know what would be the target attribute name.
I'm using NestJS (with Express Server) for a project and trying to optimize the performance on some of the endpoints.
Using Datadog I noticed that about 83% of the response time of all endpoints is spent in an anonymous middleware.
Does anyone know what middleware this is and why it's taking this long?
I suspect that it has to do with the framework itself due to the similar unanswered question  here .
My company has a cluster that's already been monitoring by a datadog agent.
But my team needs a monitoring just for us.
I already looked on fluentd, prometheus and so on, but I cound't find an option for use a tool that I don't need to install in my namespace.
Does anyone know an option that I can collect the logs of my pods just in my namespace?
Like, up a pod for grafana and another for collect logs and send to grafana or something like that?
I´am trying to setup serilog for for a project, but have some trouble understanding how it should be setup according to best practice for production.
I have followed this  https://benfoster.io/blog/serilog-best-practices/  , but I have some questions regarding the section  Configuration .
There he says;
In .NET writing to Console is a blocking call and can have a
significant performance impact.
and he links to  https://weblog.west-wind.com/posts/2018/Dec/31/Dont-let-ASPNET-Core-Default-Console-Logging-Slow-your-App-down  .
Which again says that console is bad for perfomance and has this solution to the problem:
Which only adds console if in development.
My question is: If I´am not logging to console for production what should i do?
Should i write to file?
RollingFile?
What is the best perfomance vice and will it work on fargate?
My service is running on fargate with firelens sending logs to datadog.
I recently read an article regarding Tomcat architecture and a high level overview of its working and monitoring.
Key metrics for monitoring Tomcat - DataDog
In this article, it mentions Tomcat having a pool of worker threads per connector that can be configured.
It also mentions about Executors and how it is mainly a thread pool that is shared with multiple connectors.
I have some doubts regarding Spring Boot and its Embedded Tomcat Server
I would be grateful if someone could shed some light on the above.
In short, I just wanted to know if the server configuration via application.propeties is for an Executor or for the Connector specific pool of worker threads.
So here's the scenario..
I'm seeing higher network packets per second on the older 2 webservers, and lower on the newer one.
Datadog defines this metric as "The number of packets of data received by the interface".
The graph looks like this
To me this looks like there is higher throughput on the older servers.
Looking at the nginx requests per 24 hour period on the 3 boxes, I see about  12.5% increase on the new box .
This tells me that..
Can someone help me makes sense of this
I'm trying to deploy a build Jenking ANSIBLE and i receive this error message.
The target is to install Datadog Agent on Ubuntu ANSIBLE via playbook that i build.
the message i receive is the following:
10:39:16 [INFO] les credentials Conjur n'ont pas été récupérés : Could not find credentials entry with ID 'conjur_api_key'
and :
&lt;10.145.99.222  (0, b'', b'')
10:40:00 fatal: [uamudaxd07]: FAILED!
=  {
10:40:00     "changed": false,
10:40:00     "invocation": {
10:40:00         "module_args": {
10:40:00             "_original_basename": null,
10:40:00             "attributes": null,
10:40:00             "backup": false,
10:40:00             "checksum": null,
10:40:00             "content": null,
10:40:00             "delimiter": null,
10:40:00             "dest": "/tmp/datadoginstall/datadog-agent-base_1.0_all.deb",
10:40:00             "directory_mode": null,
10:40:00             "follow": false,
10:40:00             "force": true,
10:40:00             "group": null,
10:40:00             "local_follow": null,
10:40:00             "mode": null,
10:40:00             "owner": null,
10:40:00             "regexp": null,
10:40:00             "remote_src": true,
10:40:00             "selevel": null,
10:40:00             "serole": null,
10:40:00             "setype": null,
10:40:00             "seuser": null,
10:40:00             "src": "/S3binaries/binaires_mdw_dax/DATADOG_AGENT/Linux/datadog-agent-base_1.0_all.deb",
10:40:00             "unsafe_writes": null,
10:40:00             "validate": null
10:40:00         }
10:40:00     },
10:40:00     "msg": "Source /S3binaries/binaires_mdw_dax/DATADOG_AGENT/Linux/datadog-agent-base_1.0_all.deb not found"
10:40:00 }
10:40:00 
10:40:00 PLAY RECAP *********************************************************************
10:40:00 uamudaxd07                 : ok=4    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0 
10:40:00
is there someone to help me :)?
We have a custom Golang script to publish messages to PubSub.
We then use the same client to publish to upto 40 topics.
and then based on a certain condition publish to 1 of the topic.
Our publisher loop looks like this
We use 3000 Goroutines to publish messages to the topics and synchronously wait for messages to get acknowledged, that means there are at a time only 3000 in flight/waiting for acknowledgement at client.
Our current rate of publishing is close to 5K RPS but our latencies are as high as 30 seconds.
Below are the stats that I compiled from our Datadog dashboard.
When I wrote a small benchmark script to publish messages to a single topic the average latency was 147ms from the same machine.
I've tried to tweak the publisher settings for each topic but that did not help.
Now I have couple of question.
I'm trying to send logs from td-agent to Datadog using the below configuration.
My expectation is filtering some keywords and formating that logs with using CSV format type.
How can I do this?
I tried to grep and format plugin in the filter section as below but it doesn't work as expected.
The current and expected situation as below picture.
How can I solve this situation?
current
expected
I am trying to install datadog agent via runbook on multiple Azure Virtual Machine (VM), I have uploaded binaries on Blob from where I can download on my local computer (for testing, it is working fine), but when I am trying to connect to Azure Vm via  $Session = New-PSSession -ComputerName $vm  -Credential $cred , I am getting an error that Winrm server is having an issue.
Even winrm is running fine on that server.
Just I wanted to know is there any other way to download binaries on a remote VM and install on it via Powershell or runbook.
If there is an option, please suggest it.
we use kubenrnetes nginx ingress controller version 0.25.1 on aws eks (kubernetes version 1.13).
we enable opentracing as per the documentation and use Datadog to view the traces.
We have a general ingress rule to catch every path:
In the Datadog ui we see the nginx traces, however the "resource" column always shows "/" rather than the full path which is "/test" or "/ping".
If we create a separate ingress rule for each resource path, then we see the full path as expected (i.e.
"/test" or "//ping") but it is very inconvenient and tedious to create a ingress rule for each path.
is there any way we can see the full resource path in datadog UI without creating a separate ingress rule for each resource path?
I am in the process of migrating our Laravel application to EKS Kubernetes which is currently running on Docker however the response time is significantly slower.
The current response time is roughly (Docker): 350-450 ms 
The new response time is roughly (Kubernetes): 750-1100 ms
Notable Environment Differences:
APM Findings:
I am running DataDog which shows that a lot of time is being spent on Laravel, rather than DB or Redis which doesn't give me much to work with.
At this point, I am thinking it is infrastructure related rather than an issue with Laravel as the Docker environment already preforms (decently).
I am running this as an init container (which occurs every deployment or pod restart):
I am unsure where to start troubleshooting.
Any advice will be helpful.
I want to get a list of queries executed against my mysql instance, I also want to get list of executions counts for them and duration,
I can get these stats in something like datadog APM, but I would like to be able to run a query for them locally.
is there a table or schema I need to look at?
I have a Flink job which reads from Kafka (v0.9) and writes to Redis.
I want to monitor the  records-consumed-rate  and  records-lag-max  metrics emitted by Kafka which Flink should be able to forward.
In this case, I am forwarding to Datadog.
When I start the job with a parallelism of 1, I see this metric emitted just fine.
However, if I make the parallelism greater than 1, this metric is no longer forwarded.
The job is running when parallelism   1 because I can see the entries being written to Redis.
I'm running Flink (v1.6.2) on AWS EMR:
The parallelism is set by streamExecutionEnvironment.setParallelism().
Each Kafka Consumer is instantiated with the same group.id and a unique client.id.
The DD agent is running just fine on the cluster.
Many metrics are being emitted such as numberOfCompletedCheckpoints and upTime etc.
Is there any reason Flink would not be forwarding these metrics from Kafka if the parallelism is greater than 1?
Update: 
I also tried sending a custom DD metric ( counter.inc() ) from the Redis RichSinkFunction.
When the parallelism=1, the metric is sent fine.
When parallelism=7, the metric is not sent however it is being called (added a debug line).
So it seems its not limited to the forwarded metrics from Kafka.
I'm using Nginx and I want to keep track how many hits we get to what endpoint.
We have few services in our website, how we can track the number of hits each one gets (not only number of connections but to what path of the platform)?
That way for example we can see what each point of our API get's the most hits and to improve things there.
If there is a way to get this even further with origin of the request it will be great.
I've installed Datadog agent but didn't installed anything related to NGINX, there is better tool for this task?
Thanks!
I can't seem to configure my rails app properly.
I had a perfectly working rails app deployed on Heroku.
I added the datadog buildpack and then started getting the following build error:
After removing my Gemfile.lock and reinstalling and making sure bunder version was   2.0, I then started getting another build error.
This time it was that the bundler version my project needs is &lt; 2:
I'm not sure how to move forward debugging this error.
I don't know how to access the /tmp/build folder and I'm not sure what I would do if I did.
Please any direction or suggestions would be greatly appreciated.
I've already tried pushing to heroku master with various versions of bundler.
There is always a problem with the build.
I feel like there are multiple areas where bundler is required in the build and in the actual project, and that they require different versions, but I don't know how to find them.
Here is my Gemfile:
and here is my Gemfile.lock
I'd like to be able to find where the areas are in my code that are requiring different versions of bundler.
Or be able to circumvent the issue with a different gem.
If anyone can help me get the Heroku build to work I would be very grateful.
I wonder if this is possible to achieve in Datadog.
I have a data collected under 1 metric  entity.count  - now the data are being posted to Datadog with multiple tags, for example  entity.count.visits ,  entity.count.payment  and probably another 10 different tags.
I'm trying to create Datadog chart in a dashboard, which would display top 5 tags of the entity counts in a stacked bar chart.
I know about the option of adding more queries, but since I'm not sure what entities will be available in the future, I would like datadog to always just display dynamically the top 5 entities in the dashboard (Insted of me specifying in the queries what tag to display).
This is what I currently have (and it does the job, it's just not dynamic):
I've decided to use zstd library for compression in my code using the  Go wrapper from datadog .
My build command for app is (I build my app in gitlab-ci using  image: golang:1.10.2-alpine )
Which fails with
When I try to enable  CGO_ENABLED  (and keep the rest of the build commands unchanged) I end up with warnings:
Which doesn't seem ok as I want to have my build statically linked.
Do I have to firstly build zstd library and then build my app?
I created a sample service in Akka for testing Kamon + DataDog monitoring.
Here are dependencies which I added:
Here are plugins enabled in  build.sbt :
Then  application.conf :
Finally in the  Main  class I invoke:
On EC2 I installed datadog-agent for docker.
When I run the service container on the EC2 instance and then look into the DataDog interface I don't see any related metrics to akka, just a list of standard metrics like:  datadog.process.agent ,  docker.cpu.usage ,  system.io.await  etc
How to enable akka related metrics in case when an akka app is packaged into docker and deployed on EC2?
I've a 4 node kafka cluster in my production where we are using custom partitioner which does mod 64 of an id to determine the partition.
since last week, there has been imbalanced kafka messages_in rate on 1 of our nodes as can been seen in the graph attached .
The pink line shows the message in rate on kafka01 node and bluish yellow line shows the message in rate on all other 3 boxes .
I'm using datadog for monitoring and using the metric kafka.messages_in.rate .
Assuming that there has been no change in the id distribution , there should have been no change in distribution of message in rate .
Steps I've taken to debug the issue are
Requesting any help or areas/metrics one can look into to debug this anomaly.
For people who are searching about this in future
 https://mail-archives.apache.org/mod_mbox/kafka-users/201710.mbox/%3CCALaekbwkSKapqPwsyuAoHGiSnc1+3jF2wF+2FDZbAVx61E+c2w@mail.gmail.com%3E
I am working on a project where i need to display the database mssql server's performance metrics for example memory consumed/free, storage free space etc.
I have researched for this purpose and one thing came up was  DOGSTATSD .
Datadog  provides the library for .net project to get custom metrics but that was not the solution for me because the metrics works on datadog website.
I have to display the all (in graph or whatever suited) data, received from MSSQL SERVER.
There will be multiple servers/instances.
Is there a way to do that, our WebApp connected with multiple databases and we receive/display information.
I cannot use already available tools for the insights.
I got error with the latest heapster version:  v1.5.1 .
I've described in detail in this github issue link:  https://github.com/kubernetes/heapster/issues/1969
The error message:
Anybody knows how to solve it?
Perhaps someone who already successfully integrated the heapster to datadog statsd agent in Kubernetes?
Thanks before
Is there any way to send matrics graph over email for a specific period of time from AWS Cloudwatch or by using datadog.
I want to send ec2 system check matrics for a specific time period.
Thanks in advance.
Is there a way to alert / detect in elasticsearch when the primary shard and replica shard land up on the same data node ?
May be via datadog or any other way ?
Looking for an automatic way not manual way like monitoring through head plugin etc.
I am on a RHEL system and I would like to add the following parameters so that I can have my datadog + docker integration as described here ( https://github.com/DataDog/docker-dd-agent#cgroups ).
I need to set the following kernel parameters:
cgroup_enable=memory swapaccount=1
I was planning to use something like:
cgset -r cgroup_memory=enable
but I get the error  wrong -r  parameter (cgroup_enable=memory)
I am planning to setup a 80 nodes cassandra cluster (current version 2.1 but will upgrade to 3 in future).
I have gone though  http://graphite.readthedocs.io/en/latest/tools.html  which has list of tools that graphite supports.
I want to decide which tools to choose as listener and storage so that it could scale.
As a listener should i use the default carbon or should i choose graphite-ng ?
However as storage component, i am confused that whether default whisper is enough?
Or should I look at ohter option (like Influxdata,cynite or some rdms db (postgres/mysql))?
As gui component i have finalized to use grafana for better visulization.
I think datadog + grafana will work fine but datadog is not opensource.So Please suggest an opensource scalable up to 100 cassandra nodes alternative.
It seems like we ran into a OutOfMemoryError: Metaspace before actually running out of available memory for that pool.
More specifically, we appeared to hit that error as soon as the  committed  amount for that pool reached the maximum, instead of when the  used  amount did.
Here's the setup:
We have a Jenkins server running on Oracle Java 8 update 121, and have the following metaspace arguments  -XX:MetaspaceSize=10G -XX:MaxMetaspaceSize=10G .
We also have Datadog monitoring heap and non-heap pools.
We hit an issue where the Jenkins log indicated that some thread threw an OutOfMemoryError: Metaspace.
However, in Datadog at the time of incident, the amount of non-heap used is shown to be very low (graph below).
At first I thought Datadog might be measuring it wrong, but using jconsole I get matching results for current usage (I didn't have jconsole open at the time of incident).
My only other conclusion was that the error originated from trying to allocate more  committed  metaspace, even though there's still plenty of gap between that and the used amount.
Am I missing something about how these memory pools are supposed to work?
Note:  I'm perfectly aware that this is a pretty large metaspace to have to begin with, and that we likely have a classloader leak somewhere.
This is something we hit while trying to investigate that leak.
Does anyone know why we are experiencing on our kubernetes master node some system load peaks.
I thought that the master node is not doing anything except monitoring our agent nodes.
Each time we have a system load peak of 1.8-2 on our dual-core machine, I see in the kube-controller-manager log that the master tries to start 3 things:
Our kubernetes version is 1.4.6 and is created via the azure portal.
The system peaks can we see via datadog monitoring.
I'm using Datadog to collect metrics from Kafka running on my localhost.
When I run the -info command on my Datadog agent this is the error I get for Kafka.
Any ideas whats causing this?
I have some doubts regarding monitoring nexus OSS 3.0.1 server.
Can some please let me know the following:-
I have an application hosted in openshift.
Now I want figure out how many request can handle in order to check the speed and availability.
So my first attempt will be generate a multiple HTTP GET requests to my Rest Service(made in python and hosted in openshift).
My fear is can get my IP workplace banned regarding this looks like an attack.
In the other hand I see there are tools like  New Relic  or  DataDog  to check metrics, but I don't know if I can simulate http requests and then check the response times.
I finally wrote to Openshift support and they told me I can simulate http requests without worries.
I'm adding logging to my application, and was wondering what the correct way is to add a tag that may or may not have a value.
My code is
The  config  is a dict of configuration variables constructed from another class.
Is it okay to pass objects that might not have a value like the  sometimes_here  config element?
Or do I need to build the tags list separately like
Since there might potentially be even more optional elements in config, I'm hoping that the more compact code will be suitable, but I can't find anything in the Datadog documentation on what it would do about those tags.
We have the below Category processor pipeline for Lambdas such that when there is a log with (@error.kind:* or @error.message:*) , the logs will be defined as an error.
To test above Category process rule, I have create a simple Lambda called python-test as below
When I run this lambda with below test event, I don’t see the log messages being converted to error
Eg: In Datadog as you can see below,
it still remains as log ie.
it is not converted to error (ErrorType –error.kind is not set)
Can you please advise If I am missing something?
I'm trying to implement distributed tracing in my kotlin app using spring cloud sleuth.
I'm sending those data to the datadog.
Now I'm able to trace my logs but I want to add some extra data to spans.
Let's say I want to add info about user and be able to see it in datadog.
Am I right that span tags are good for it?
I'm sending the logs in json format to datadog but I cannot add tags here.
(traceId and spanId are injected).
Logback config:
gradle:
and to add the tag I'm trying
example log:
shouldn't be that 'user' injected into MDC and then into logs?
I'm trying to configure spring actuator metrics along with micrometer to be sent to Datadog stastd agent.
Still, I'd like to get them all sent with a tag, so that I can filter in my Datadog dashboard just my service metrics, and not considering other services metrics.
I've added:
to my service metrics configuration, but I can't see this tag value in Datadog dashboard.
I'm not seeing anything weird in app logs nor actuator logfile neither.
I have nothing else regarding metrics in my service, as I don't want to implement custom metrics, just want to use the one provided by actuator.
This is how the whole metrics configuration looks like:
Versions:
micrometer version: 1.6.4
actuator version: 2.4.3
spring version: 2.3.8
Any clue about what I could be missing to get the tag reaching Datadog?
Thanks!
I'm exploring the Open Telemetry Collector Project and how this could work with containerized .NET Core apps (or any other apps for that matter).
Currently we're using DynaTrace at the company I work for, which requires the DynaTrace 'OneAgent' agent to be installed on hosts.
The DynaTrace agent somehow hooks into the dotnet CLR and does bytecode/MSIL instrumentation.
Basically this approach allows us to capture APM data to DynaTrace without having to do any code changes in our apps whatsoever.
Contrast this with the Open Telemetry approach, which (as far as I can tell) requires additional (nuget) packages to be installed into the services we want instrumented.
In .NET Core land, I suspect this is using DiagnosticSource based instrumentation, which I would describe as a type of AOP.
That said, it's mostly automatic and integrated into various .NET libraries/frameworks such as ASP.NET, Entity Framework, etc; so the only code changes are; a) installing the Open Telemetry nuget packages, b) some basic Startup.cs configuration, and c) optionally adding additional spans if/when needed.
It's minimal code, but it's NOT no-code like the DynaTrace approach.
I'd also assume the granularity of spans to be a lot courser than bytecode/MSIL instrumentation approach used by DynaTrace.
WRT the Open Telemetry Collecter; I really like the fact that we can configure exporters to send instrumentation data to any supported third party monitoring solution (e.g.
DataDog, Elastic, Kafka, etc) without having to install any proprietary agents.
IMO this approach means we can more easily change the monitoring service(s) we might be using and therefore mitigates vendor lock.
I'm hoping I can find a way to get the best of both worlds;
Is this possible?
I was looking at how the DataDog agent works  here .
It looks like this might be a similar approach to DynaTrace.
For DataDog, it looks like you need to set some some CLR environment variables in the app container - presumably this extends the CLR to send bytecode/MSIL instrumentation data to the DataDog agent (running as a sidecar or something)...
Would we be able to use this DataDog approach, but instead of sending the instrumentation data to a DataDog agent, we send it instead to an Open Telemerty Collecter agent?
Technically the Open Telemerty Collecter agent could still send to DataDog if we wanted (i.e.
using the  DataDog exporter ), but at least we'd mitigate the vendor lock using this approach.
Thoughts...
Our team has a production Django app that's been experiencing latency spikes throughout the day.
Drilling into the requests during those time periods in Datadog,  the recurring theme is that the Django middleware is running very slowly , which you can see in the flame graph.
Compared to normal-latency periods, that's the main difference in performance, and this happens for all endpoints.
You can see from the Heroku metrics that it occurs in an almost regular pattern.
We haven't added any new middleware recently.
Has anyone seen this before, or have any guidance on what could be causing this issue with the middleware?
I'm using scala, sbt, sbt-native-package, and potentially sbt-java-agent to conditionally activate a datadog java agent at runtime w/ kubernetes.
By adding the  dd-java-agent  as a dependency and adding a script snippet, I'm able to activate datadog only when a specific env.
variable is set, but this is also adding the dd-java-agent to the classpath, which I'm trying to avoid:
Is there a way to have sbt manage the downloading of dd-java-agent.jar, include this jar in the  lib  directory (or a different directory if that's what it takes), but exclude from classpath?
I've tried using  sbt-java-agent  which puts the jar in a  dd-java-agent  directory and excludes the jar from the classpath, but I can not figure out how to wrap the  addJava  statement in an  if  check when using that plugin.
Thanks for any help you can provide!
I’m integrating Istio v1.8.0 with DataDog and their integrations works well, Im getting most of the metrics.
But, I’m not getting tcp related metrics for my services:
My services communicate using HTTP with each other, but I want to see tcp metrics as well for open connections to my envoy-proxy and other tcp related metrics.
Are there any metrics of this kind that I can use?
I don't see any here -  https://istio.io/latest/docs/reference/config/metrics/
I'm trying Migrating from DataDog to prometheus.
and this one query i'm having some difficulties with.
I'm unable to find the pct_change functionality available in DD in prometheus.
I tried using (A-B)/B or A/B but they don't work.
not appropriate results.
e.g.
But it's not correct.
The alert fires very often.
Any leads on this would be appreciated.
How can I write the PromQl alternative of DD?
We are using Datadog and AWS ECS.To collect ECS host network related metrices in  DataDog we are following below documentation
https://docs.datadoghq.com/network_performance_monitoring/installation/?tab=docker
I was able to add all mentioned parms in DataDog agent container definition except below
I tried to update same in my ecs-taskdefination cloudformation
but getting below error when deploying cloudformation
 Model validation failed (#/conatinerdefination/0/Privilaged: expected type: Boolean found: string)
We have an electron app that connects to our servers on AWS (ALB, Fargate, lambdas) its a meteor based app (ddp, socks underneath) for the past month we've seen lots of XHR error GET on our datadog logs, it seems it's not disrupting our service, but it's really polluting all of our logs as we get a lot of those, could it be its timing out due to ALB not letting the connection in?
it's odd as I stated our users haven't reported any connection issues and are able to use the app, we have ports 80, 443 open and redirect any calls from Http to Https, so not really sure what would be causing the error.
Do XHR sockjs are using a different port?
We assume it has to be something on our network but we haven't figured it out, we don't experience any of these errors when we use our app locally on development mode, only happens if we have the electron app running and we turn off our server instance, as the connection cannot be established.
Any insights would be great!
thanks!
I'm literarlly new to DataDog.
I have been assigned into a project where they have set up DataDog to log everything within the code.
After spending 1hr or so trying to do something like:
I concluded that to do a simple &quot;count by&quot;, I need to go over the process of creating a Processor, then a Metric.
Really is that cumbersome?
I miss so much SUMO Logic simplicity for this scenario.
In my company, we have Datadog dashboards and monitors which we often make changes to and thus we wish to have some version control.
After discussions it was decided that Terraform is the way to go with this.
I've successfully been able to manually back up our current Datadog infrastructure using Google's  Terraformer  (to import the infrastructure as resources) and Terraform's CLI commands to import the corresponding infrastructure's states.
It is also possible to make changes to the resources through commands such as  terraform plan  and  terraform apply .
The manual changes to the resources can then be backed up with Git.
Now to the issue I'm having.
We wish to automate the the back-up of our resources, that is, through making changes in the Datadog GUI and to have those changes be reflected in our resource files.
That means the other way around of what I've managed to accomplish above.
It is rather straight forward to do this manually, where one imports the infrastructure resources that have been changed using the GUI (using terraformer).
If &quot;conflicts&quot; arise, one needs to manually remove certain resources and re-import them which I've found difficult to automate.
My question is: is there a straight forward way of automating back-up of changes made &quot;remotely&quot; (e.g.
in the Datadog GUI) to resource files?
My research for Terraform CLI's for this purpose has lead me to nothing thus far.
Any suggestions are appreciated!
Thanks
Yesterday my JVM application was broken due to high CPU usage, when checking the root cause this is because GC stop the world.
The GC require more time to clean up the memory, this is because I introduce new LRU cache at the app.
So my question is:
Is there any way to track how big memory group by JVM class?
For example, I have 3 classes,  Foo ,  Bar ,  Baz .
I want to know how big memory used by each class at run time.
I have a Datadog account, so I plan to use tools to send those metrics to Datadog.
I'm trying to import a 3rd party library (datadog) for use with a glue shell script and I'm running into issues.
I've packaged the file as a .egg and given the path to it in the glue job, as instructed  here .
This ends up throwing an error saying zipimport.ZipImportError: not a Zip file: '/tmp/glue-python-libs/datadog.egg'.
When I try using a zip file instead, it throws ModuleNotFoundError: No module named 'datadog'.
How do I go about importing the library?
Currently have some task-based automation for ECS that run on a scheduled basis, however sometimes there is a need to run only run task or re-run tasks for only a certain kinds of tasks (for example sql tasks or datadog tasks).
I know this can be done via console, but it's inefficient.
Was thinking of a bash script that calls to start a task from a CLI.
Currently I know I can do this with the AWS CLI using '--task-definition', but it's not much better.
I don't usually write scripts, so I'm basically here to help with brainstorming.
I'm wondering if there is a way to make an API call to start tasks.
Would I need to type in the ARN every time?
Can I just list the tasks on the AWS CLI and have the exported to the script?
Would network-config need to be hard-coded?
Thanks!
I use  Laravel Horizon  to process jobs in my Laravel application with the help of Supervisor.
The whole setup runs on docker in ECS.
On the host node of ECS (managed by Fargate) I have datadog agents running on it, which will grab the stdout/stderr of the container and put it into datadog (so I can access the logs centrally).
What I noticed is that I am not getting any logs out of the jobs that is processing.
Even simple  print("test");  in the job code does not go to the stdout.
Here's my setup:
Dockerfile
The service definition for this in yaml looks like this:
And here's the content of my  horizon.conf :
Any idea what I might be missing?
What can't I get logs out of the jobs processed by horizon in docker via supervisor?
I've installed the datadog app on my linux vm but i can't seem to read the datadog.yaml agent file.
[ Error reading /etc/datadog-agent/datadog.yaml: Permission denied ]
My linux box is hosted on GCP, do i need to configure permissions?
I have a Spring Boot (2.2.6) application that uses Log4j2 (with Slf4j).
Log4j is configured to use the json layout and in the end I want to ingest the logs in Datadog.
For that the 'serviceName' is important as a field in the json.
Now according to the log4j docu ( https://logging.apache.org/log4j/2.x/manual/layouts.html#JSONLayout ) one can add a custom field with the 'KeyValuePair' tags and that works.
Unfortunately this breaks the normal structure of the spring logs.
Log4j2.xml config:
Log w/o custom field:
log w/ custom field:
Spring docu mentions how this might work with logback, but not log4j ( https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-custom-log-configuration , end of chapter)
I've searched but couldn't find anything useful.
Any ideas how i can add a custom field to the json log while still preserving all fields coming from Spring?
Thanks, 
Felix
I want to expose all metrics on the metrics endpoint but publish some of them to a remote meter registry.
For doing so, I have a SimpleMeterRegistry for the metrics endpoint and added a  MeterRegistryCustomizer  for the remote meter registry(Datadog) to add some  MeterFilter  to avoid specific metrics using  MeterFilter's DENY  function.
For example :
However, all jvm related metrics are visible in Datadog.
I tried  MeterFilterReply  but no use.
Please suggest how this can be achieved.
I have a climate control automation tool running with Home Assistant.
Hass supports many types of long-term db storage of it's entity states (sensor data etc), like datadog, influbdb, graphite etc.
So far I've tried influxdb and graphite.
I've been using grafana to visualize the data.
I want to store threshold values in the database, such as min/max temperature.
These temperatures can be set using an input slider on the hass UI.
Once set, these controls can be left for days, even weeks.
So there may be only one data point in the db for very long periods of time.
If I want to display these on grafana, they disappear from the time range being looked at pretty quickly, and grafana simply removes the entity from the graph.
Influx has a "use previous value", and graphite has a "keepLastValue" function that I thought I could use to pull the last value of the threshold from the db, but in both cases, the values must exist in the time range selected.
If the previous value for the control was days before the time range, too bad, so sad.
I thought this would be a very common requirement, but perhaps not.
Does anyone know a combination of database and dashboard that can display the last value for an entity even if said last value was recorded far out of the time range selected?
So I'm sending my Nginx access logs to Datadog (an APM solution).
My log format looks like this
I can extract the url from the  referrer  field and it looks like this
I only want  /foo/bar  though.
Is this something I have to modify in the  log_format ?
I saw an example from datadog docs where they're able to extract a url path attribute, but there's no example config.
I am trying to install the chart  stable/efs-provisioner  and I would like to apply an annotation so that the deployment is correctly tagged in datadog.
Datadog requires the  annotation :  ad.datadoghq.com/tags: '{"env": "staging"}'
I have tried various incantations of the following, but I keep getting the error below.
Error:
Tomcat getting hang with high load.
Tomcat threads DB connections are showing a sudden increase.
ex: tomcat threads reach 1000 from 200 within a minute.
normally tomcat thread shows 150 - 250 stable pattern
We have checked all application metrics and JVM metrics.
But cannot identifies the root cause for this sudden instability.
below is the list that we have checked.
App - tomcat thread, tomcat CPU, tomcat memory, server memory, server CPU
DB - CPU,process,connections,memory
JVM - GC calls
TomcatThreadGraph-Nagios -  https://imgur.com/NItgPjp
TomcatThreadGraph-DataDog -  https://imgur.com/4RH570B
I have  elastic-search cluster which hosts more than 15 indices, I have a Datadog integration which shows me the below view of my elastic-search cluster.
We have alert integration with DD(datadog) which gives us alert if overall CPU usage goes beyond 60% and also in our application we start getting alerts when  elasticsearch cluster is under stress  as in this case our response time increases beyond a configures threshold.
Now my problem is how to know which indices are consuming the ES cluster resources most, so that we can fine either throttle the request from those indices or optimize their requests.
Some things which we did:
So my problem is very simple and all I want some metric from DD or elastic which can easily tell me which indices are consuming the most resources on a elastic-search cluster.
I've started a new project, and am building my first micro(ish) service.
It has a  /health  endpoint, which currently just replies with HTTP status  200  and a  { "status": "OK" }  body.
My service relies on several other (external) services, and I was wondering if there's a standard I can follow for exposing information about the connection between my services and the external ones?
I'd like to not invent my own patterns here if there are industry best practices or standards I can follow.
Something like:
My service will be running in Kubernetes on GCP, if that helps.
We use Datadog for observability.
I need to identify in my infrastructure which hosts have tag1 and tag2 and tag3.
I'm new to datadog but it seems that when filtering I have to specify a value for a specific tag.
I also need to identify the inverse, list hosts that have tag1 but are missing tag2 OR tag3.
I have setup a dashboard for each of the tags, I seem to be limited by up 2 tags.
e.g filter by -  env:dev 
    group by -  tag1 , tag2
I would expect to be able to see what hosts have tag1 AND tag2 AND tag3
And the inverse -  what hosts do not have all 3 tags.
Our project is responsible for migrating data from one system to another.
We are going to run transformation, validation and migration scripts using Jenkins.
It's unclear for me how to aggregate logs from several Jobs or Pipelines in Jenkins.
How it can be done?
We'll rely on logs heavily to identify any issues found during validation etc.
In terms of our planned setup we'll have AWS EC2 instances + we can use Datadog (our company uses it).
Can we use Datadog for this purpose?
I've executed this query on production with load on the server.
It took 35 minutes according to the datadog.
this  table1  has around 100 million rows.
Is 35 minutes normal?
Is there any way I can execute such simple migrations (adding a nullable column) without locking the table?
I have created a windows cmd file that calls three independent bat files.
I want to create a windows task that calls this cmd file and runs every 5 minutes.
The problem is that this task runs perfectly fine only when I'm logged into the system.
But I'm unable to make this task continue to run "whether I'm logged in or not" .
I even asked my colleague to login to that machine and run this task under his account - it worked.
I created a local admin user on that machine, logged in as that user, tried to run this task - it did not work - the script waits forever while post_results.bat.
I even tried to schedule a jenkins job that basically does the same thing - it did not work - the jenkins job waits forever while post_results.bat (I killed the jenkins job after waiting for ~20 min).
Here is a summary of what these tasks are doing:
run_all.cmd
run_test.bat  - executes a jmeter script
post_results.bat  - calls a python script that posts the jmeter test results to datadog
post_jmeter_results_to_datadog.py  - uses the datadog python api to post metrics to datadog
clean.bat  - deletes the jmeter test result files
All I need is to be able to run this task every 5 minutes.
If anyone is able to see what I'm doing wrong and points that out...
I'd be really grateful.
We have a cookbook with multiple recipes where we select features.
In this case, it's Couchbase and we want to be able to have  data ,  query , and  index  nodes tagged in Datadog, but that's probably more than you need to know...
Anyhow, one or more features can be selected.
So, we have 3 recipes, one for each concern.
Each recipe adds the feature name to an array and then  include_recipe cookbook::default
With Chef 12, we could select multiple feature recipes and then it seemed to wait until all of them were processed to run the default cookbook, so it could aggregate the array and process all the chosen features together.
With Chef 13, it appears to be run immediately after the first feature recipe is processed, so that subsequent  include_recipe  are skipped.
As a workaround, of course, I've changed some of the logic, but finding details on this behavior change hasn't come up with anything.
Thanks for any help...
-H
I have a linux trusty on aws m4.xlarge so 4 CPU, 16 GB RAM.
It's running a java application on tomcat7 and oracle java 8.
Very frequently the app will hang and won't accept any other connection.
Status cake will report it as down since the response times out.
Datadog will show threads are maxed out.
But there is no increase in CPU (barely 10% of usage).
RAM usage remains unchanged during that period.
Only a tomcat restart fixes the problem temporarily(12h approx ).
So I have taken a thread dump and seen so many threads in a waiting state.
Since this is very new to me, I am blind even with the data.
I was hoping I could get help here and eventually master the art of ciphering a thread dump file.
I have attached it here and I have as well uploaded it to  fastthread.io and it says there is no problem .
I have also uploaded the full  threadump on zerobin
I would be very grateful if anyone here can shed some lights on this and I hope it will help others in the same situation.
Thanks in advance.
Having followed  Spring Boot Metrics documentation , I was able to set metrics logging for datadog easily.
The only remaining stuff is to set custom tags for my instances.
With Spring Boot, you can do it by registering a new bean:
However, I'm not able to register it in Grails 3.
Not in  resource.groovy  nor in application main class  Application.groovy .
Is there any way how to set this in Grails 3?
I try to use a wrapper Chef recipe to read Datadog API keys from an encrypted data bag and override node default attribute.
My confusion here is with  Chef::EncryptedDataBagItem.load method use.
I created an encrypted bag with name datadog with an item datadog_keys inside of it.
I would like to get api key and app key from inside of this data bag item.
So I'm using:
My question, this usage is it correct or should I use:
Chef::EncryptedDataBagItem.load("datadog_keys", "api_key")
or
Chef::EncryptedDataBagItem.load("datadog::datadog_keys", "api_key")
I have netcat listening for udp traffic on port 8125 in terminal 1
nc -ul 8125
and in terminal 2 I run the following (a test dogstatsd message for troubleshooting a datadog client connection):
I would expect to see  test_metric:1|c  show up in the output of terminal 1, but there is no output at all.
Can you help me understand why the udp message is not showing up and how to successfully send the udp message?
I'm running DataDog agent as a container within my AWS CoreOS instance.
This is done via running dd-agent as a container.
To automate this I have written a systemd unit for enabling and running data dog agent within AWS CoreOS instance.
But none of the metrics are being sent into the DataDog side.
But the Docker container is running without any issue.
Here is my Systemd unit file
EDIT - Adding More Info
Initially when I ran this on a single CoreOS instance I was able to see docker related metrics of the instance within DataDog dashboard.
Then I enabled this on multiple CoreOS AWS instances.
From that point on wards none of the metrics which are related to the CoreOS instances or Docker containers are not visible.
EDIT - Adding docker logs
I have three slaves (jmeter-servers) running on EC2 instances, and in one case – (1) JMeter GUI on a local laptop, on another – same test plan (2) running from a command line on yet another EC2 instance.
In case of GUI I can see all the aggregated numbers for Throughput, 99%, etc.
in – well, GUI.
I'm creating a jtl file with Aggregate Report listener.
From watching Datadog charts monitoring the application server parameters (CPU usage, memory, etc.)
I see that in case of a command line and everything on EC2 load is more than twice higher than when my local laptop is communicating with the jmeter-servers, meaning probably that the network becomes a bottleneck.
So I want to run everything on EC2.
But then – how do I get access to the same aggregated numbers when I'm running from the command line when all four machines are EC2 instances?
The huge jtl file contains records for each transaction, not the aggregated one line of the entire run result.
On an attempt to download that jtl from EC2 and open it in GUI on a local laptop it generates some error instead of showing aggregated data.
Am I using a wrong listener to get to the summary data?
(Tried Summary Report – it creates even larger jtl file, not the one line I'm looking for.)
I'm trying to get the version of program that is installed on a windows server and I want it as a variable inside the recipe.
Basically I'm trying to find the version and if it is not what I want it will be removed and the correct version of the program will be installed.
I can't figure out a way to get the version though.
The program I want the version for is the Datadog agent.
I have a requirement in my program to send metrics to datadog indefinitely (for continuous app monitoring in datadog).
The program runs for a while and exits with the error &quot;dial udp 127.0.0.1:18125: socket: too many open files&quot;.
The app names are read from a config.toml file
I'm trying to figure out how to take the results of "repadmin /syncall /d /e" and put the results into an if else statement.
I've considered trying to just look for the success string it outputs for the if and, but I'm wondering if there is a more official way to pull the status code?
So if successful use some built in PowerShell feature to know the status is successful.
I'm doing this so I can publish a metric to DataDog giving a pass or fail count for cross-site AD Replications.
Any ideas?
In Linux shell, how to use regex to filter output of other command.
like in cisco devices we use sh ver | b interface, which will dispaly info about int only.
my requirement is filter output of below command to display from "Dogstatsd (v 5.12.0)" and status date &amp; time.
So that i can use this o/p with certain criteria to write a script to auto restart the agent.
Status date: 2017-05-30 08:20:13 (17s ago)
  Pid: 7864
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/collector.log, syslog:/dev/log
Clocks
  ======
NTP offset: 0.018 s
    System UTC time: 2017-05-30 06:20:31.535928
Paths
  =====
conf.d: /etc/dd-agent/conf.d
    checks.d: /opt/datadog-agent/agent/checks.d
Hostnames
  =========
socket-hostname: adcd
    hostname: adcd
    socket-fqdn: adcd
Checks
  ======
apache (5.0)
    ---------------
      - instance #0 [OK]
      - Collected 12 metrics, 0 events &amp; 1 service check
network (5.0)
    ----------------
      - instance #0 [OK]
      - Collected 16 metrics, 0 events &amp; 0 service checks
directory (5.0)
    ------------------
      - instance #0 [OK]
      - Collected 17 metrics, 0 events &amp; 0 service checks
ntp (5.0)
    ------------
      - Collected 0 metrics, 0 events &amp; 0 service checks
disk (5.0)
    -------------
      - instance #0 [OK]
      - Collected 24 metrics, 0 events &amp; 0 service checks
Emitters
  ========
====================
Status date: 2017-05-30 08:20:24 (7s ago)
  Pid: 7859
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/dogstatsd.log, syslog:/dev/log
Flush count: 583466
  Packet Count: 333155
  Packets per second: 0.0
  Metric count: 1
  Event count: 0
  Service check count: 1
====================
Status date: 2017-05-30 08:20:29 (2s ago)
  Pid: 8868
  Platform: Linux-3.11.0-24-generic-x86_64-with-Ubuntu-13.10-saucy
  Python Version: 2.7.13, 64bit
  Logs: , /var/log/datadog/forwarder.log, syslog:/dev/log
Queue Size: 422 bytes
  Queue Length: 1
  Flush Count: 1102592
  Transactions received: 879956
  Transactions flushed: 879955
  Transactions rejected: 0
  API Key Status: API Key is valid
======================
Not running (port 8126)
root@adcd:~#
We have several Tomcat servers (in AWS) running under Debian, we have all of them instrumented with Cloudwatch metrics for overall performance (Memory, CPU and others).
We've detected that in a few of them we have "spikes" of either CPU or Memory utilization, and we'd like do detect what is actually clogging those resources.
As all the server runs is java based inside a Tomcat container, the logical would be to hook up some kind of JVM profiler and visually monitor the threads in it, but as we do have Cloudwatch alerts enabled when exceeding a certain threshold (for example CPU over 90%), we'd like to trigger some kind of automated stats collection to see what actual Java thread/code is the root cause of such consumption.
Is there any monitoring agent and/or performance collection tool that might help to diagnose those specific spikes and not needing to collect stats for an actually long running process?
We've already tried trial versions of New Relic, DataDog, and Dynatrace (the latest being the most useful, prohibitively expensive due to its business model not suitable for small companies.
), but these solutions gather EVERYTHING, not only required timing windows, as I've asked above...these could work but introduces quite an overhead to the servers if being used 100% time in production servers (where the problem arises and, not in pre-production ones.
).
Are there any npm modules/plugins exists so as integrate Elastic APM's RUM JS Agent with the application ?
By integration i mean good coupling with application so as to record all the things /events(such as route loading, all service requests, actions etc.)
and provide it to elastic search.
I am using the elastic apm java agent to publish tracing information to an elastic apm server.
I am setting the  enable_log_correlation  property to true which makes the span, trace and transaction ids available in the MDC and using logstash to capture all that in an elastic index.
The logs are visible in the Discover tab in elastic and tracing information is visible in the APM tab.
I have seen in some screenshots a link on the APM   Transactions page to  View Transaction in Discover  to be able the see all logs related to a single transaction.
This link is not showing up for me.
The span link  Show span in Discover  is showing up and it takes you to Discover and opens up the apm index for this span.
So two questions:
Thanks!
Kibana response is
APM server returns 503 - Internal Server Error, 
Having hard time identifying the root cause.
Is it ES queue is full or ran out of memory or cluster is not being setup correctly?
According to ES documentation:  https://www.elastic.co/guide/en/apm/server/master/common-problems.html#queue-full
A full queue generally means that the agents collect more data than
  APM server is able to process.
This might happen when APM Server is
  not configured properly for the size of your Elasticsearch cluster, or
  because your Elasticsearch cluster is underpowered or not configured
  properly for the given workload.
The queue can also fill up if Elasticsearch runs out of disk space.
Documentation doesn't help in identifying what could be the root cause.
How do we identify the root cause?
Restarting Kibana and Elasticsearch helps but that does not help to identify the root cause
I am currently doing a PoC to integrate Elastic APM into my spring application.
I was following this page :-  https://www.elastic.co/guide/en/apm/agent/java/1.x/setup-attach-api.html  
to programatically attach elastic-apm jar.
I have added the required jar into pom.xml but i am not getting how should i attach Elastic Apm (ElasticApmAttacher.attach())into my normal spring code.
Example given is for SpringBoot.
But my application is on Spring core ( spring-core, spring-web ..) with rest services exposed using Jax-Rs.
After adding elastic-apm-node on our backend server we receive the below error hundreds of times a day.
Can anyone suggest what might cause this?
The error we see is this:
I think we run node v10.x but I'm not absolutely sure.
The service is built on feathersJS which again is built on Express.
We start APM like this:
The dependencies listed in package.json are:
I'm using  elastic apm  to profiling my  NestJS  application and my apm agent is  elastic-apm-node .
My ORM is  typeOrm  and my database is  Oracle .
My problem is apm agent does not record database query spans and I can't see  database query spans  in kibana ui.
Can anyone help me?
We want to track our elixir phoenix app  using elastic apm.
But I could not find an apm agent from elastic.
Someone suggested to use  opentelemetry  along with exporter but I am unable to understand how to use that from the docs.
I want to track details like the new relic does like errors and all things.
Previously we used new relic for which there is an open source apm agent but now we want to switch to elastic.I am unable to understand how to use span in the app and how to handle multiple span and where to put them.
If anyone can help with that or provide alternate solution to use elastic apm it would be great.
I'd like to measure avg request-response time for my webserver.
Apm has  transaction.duration.us  and it seems this could be the metric I'm looking for.
But I coulnd't find the documentation what it is.
Where can I find the meaning of the variable?
I have a problem when I want to start the app, when i type  npm run server .
And the error information are like this :
Elastic APM agent is inactive due to configuration, what does it mean?
Maybe someone can help me, please...
This is the package.json :
{ 
    "name": "opbeans", 
    "version": "1.0.0", 
    "description": "The Opbeans inventory management system", 
    "main": "server.js", 
    "dependencies": { 
      &nbsp;&nbsp;&nbsp;"after-all-results": "^2.0.0", 
      &nbsp;&nbsp;&nbsp;"body-parser": "^1.15.2", 
      &nbsp;&nbsp;&nbsp;"concurrently": "^3.1.0", 
      &nbsp;&nbsp;&nbsp;"detect-port": "^1.2.2", 
      &nbsp;&nbsp;&nbsp;"elastic-apm-node": "^1.3.0", 
      &nbsp;&nbsp;&nbsp;"express": "^4.14.0", 
      &nbsp;&nbsp;&nbsp;"pg": "^6.1.2", 
      &nbsp;&nbsp;&nbsp;"redis": "^2.6.3", 
      &nbsp;&nbsp;&nbsp;"request": "^2.79.0", 
      &nbsp;&nbsp;&nbsp;"webpack": "^4.3.0", 
      &nbsp;&nbsp;&nbsp;"webpack-dev-server": "^3.1.1", 
      &nbsp;&nbsp;&nbsp;"workload": "^2.3.0" 
    }, 
    "devDependencies": { 
      &nbsp;&nbsp;&nbsp;"dotenv": "^4.0.0", 
      &nbsp;&nbsp;&nbsp;"react-dev-utils": "^5.0.0", 
      &nbsp;&nbsp;&nbsp;"react-scripts": "^1.1.1", 
      &nbsp;&nbsp;&nbsp;"standard": "^10.0.2" 
    }, 
    "scripts": { 
      &nbsp;&nbsp;&nbsp;"db-setup": "./db/setup.sh", 
      &nbsp;&nbsp;&nbsp;"test": "standard  .js server/ / .js db/ /*.js", 
      &nbsp;&nbsp;&nbsp;"server": "node server.js", 
      &nbsp;&nbsp;&nbsp;"client": "npm run start --prefix client/", 
      &nbsp;&nbsp;&nbsp;"client-build": "npm run build --prefix client/", 
      &nbsp;&nbsp;&nbsp;"client-install": "npm install --prefix client/", 
      &nbsp;&nbsp;&nbsp;"postinstall": "./client/build-client.sh", 
      &nbsp;&nbsp;&nbsp;"start": "concurrently \"npm run server\" \"npm run &nbsp;&nbsp;&nbsp;client\"", 
      &nbsp;&nbsp;&nbsp;"workload": "workload -f .workload.js" 
    }
im trying to install the Elastic APM with Elasticsearch, Kibana and the APM server as 3 services with docker-compose.
Now im getting confused on how to set the IPs in the app-server.yml file with the documentation  APM Server Configuration .
The file should look like this:
I tried to set ElasticsearchAddress to localhost or 127.0.0.1 but I always get errors like
 Failed to connect: Get http://127.0.0.1:9200: dial tcp 127.0.0.1:9200: getsockopt: connection refused  or  Failed to connect: Get http://localhost:9200: dial tcp [::1]:9200: connect: cannot assign requested address .
I also tried it with several other ips.
Does anyone know how to configure the app server correctly or are there any docker-compose files to do the installation correctly?
Thanks for ur help
I have downloaded elastic-apm-agent.jar on my local which is monitoring locally deployed spring boot micro service.
Now I want to add same jar to PCF.
I am using a TypeScript setup with webpack and babel and get the following error when trying to include  elastic-apm-node .
I have the settings in environment variables.
Errors:
Any idea how I can prevent this?
When using just TypeScript with Meteor I don't get the errors, so I think it is connected to Babel / Webpack.
I have setup of Elastic with APM server on single machine.
I've configured APM java agent to push traces to APM server on localhost.
Everything works fine with localhost configuration on Windows.
Now, I'm looking to run apm java agent for application running on different machine on the same network.
That is apm java agent on linux &amp; apm server running on windows machine.
Default APM-server listen to localhost.
I tried to change setting on apm-server.yml file with -
default is:
After making apm-server.yml change, process explorer show apm-server.exe process listening to IP- host-ip port- 8200 protocol- TCP.
But, still  http://host-ip:8200  is not accessible from other machine on network.
While on the same machine (windows)  http://localhost:8200  &amp;  http://host-ip:8200  works fine &amp; give below response.
Thanks for help.
Given this docker file to setup the backend services that includes: elasticsearch, apm-server, kibana, jaeger-collector, jaeger-agent, jaeger-query, grafana.
I am running Elastic APM with Opentracing from my Angular client:
I am encountering CORS issue:
My goal is to hook up Angular the elastic APM's opentracing client to the services inside docker.
There are some additional issues and documents that covers CORS for apm-server:
Distributed Tracing Guide
Config with RUM enabled
It looks like the config should work, since  Default value is set to ['*'], which allows everything.
I am quite new to Elastic APM, Kibana, Elasticsearch and APM in general and did not come across any information pointing towards my needs.
I set up the  elastic-apm[flask]  module and followed the tutorial.
In the Kibana dashboard I get information like response times and server name, but the fields for client.ip etc.
are empty.
I would like to track the IP addresses (more exactly where my website visitors come from).
So, how do I get the user's IP address into the client.ip field in Elastic APM?
I don't want to issue an  app.logger.debug  statement everytime a route is being requested.
I'm currently checking why the APM UI in Kibana doesn't output me the information when the timeframe is set to more than 24 hours.
When checking the config I did notice that we haven't had the Kibana endpoint set for the APM server.
Checking the APM logs I can't see an error but when going to the APM UI I can find this error in the Kibana logs:
From the output it looks like that Kibana can't find any server configuration but the setup was done successfully but I can't as well access the APM settings that were moved in Kibana (timeout with error 404).
Other queries or the query from the same indices work well without any increased delay.
The APM agent is the latest version of the Django (Python) agent.
And the resource metrics from clusters as well as status are really good, so the cluster should be powerful enough.
We have had as well testing deploy in Kubernetes and the services sometimes show up in the APM UI but it's turned off currently.
Environment and versions:
Elasticsearch deployed on GCP in Docker containers (4 nodes on different VMs + 1 VM with APM Server, Kibana, and Elasticsearch client node).
ES, Kibana, APM-Server versions: 7.3.1
I'm using Elastic APM, and want to find out how long the garbage collector has been running during a period of time.
This is to understand if the application is running out of memory, which seems more accurate than just checking heap used, as garbage collection could trigger when heap space is limited, and then free up a large amount.
Elastic APM will track  jvm.gc.time , which the Elastic site defines as:
The approximate accumulated collection elapsed time in milliseconds.
Source
I assumed this meant how much time has been spent garbage collecting since the application started.
My plan was to read this value periodically, and determine how much of the time interval was spent garbage collecting.
When I read this value two different times, it turns out the second, and  later  reading, is actually lower than the first.
First Reading
Second Reading
Can anyone help me understand what  jvm.gc.time  captures?
I am interested in using Elastic APM within an ASP.NET Core to instrument traces of a set of services which communicate over a mix of protocols (HTTP, SQS, SNS).
Despite reviewing the documentation, I am not clear how I can use the  Elastic APM Public API  to connect transactions to one another which occur outside of HTTP (HttpClient is automatically instrumented for trace by Elastic APM).
According to the documentation, I should be able to serialize the  CurrentTransaction.OutgoingDistributedTracingData  on the caller and then deserialize it to resume the transaction on the callee, but despite implementing this pattern in memory, my traces in Kibana are missing spans from all but the final transaction.
My implementation spike can be found on  Github .
I add ElasticAPM to my startup on AspNetCore 3.1
in my project, rest api services logs as transaction tab of kibana-apm.
but my background services dos not logged by apm agent &amp; only metrics tab work for me.
I have set up an elastic stack with elasticsearch, filebeat, kibana and apm server, and an spring-boot-application with the apm java agent and started my setup in a docker compose file.
I have enabled the dashboard and I can see traces about processes in the application.
But I cannot filter for container id, because there is no id.
How do I enable my stack apm server/apm agent to receive the metadata about container id, pod id and so on.
Where can I enable metadata for apm server / apm agent to receive the container id for instance.
My apm server can't connect to ES with the following log
I tried to 'reset' the index by the following command, it won't work either
I tried to setup a policy where apm data is deleted after 3 month, and I think I messed up the index setup.. (I can't remember what I did exactly)
How do I reset the index and start using apm again?
(It's a plus if I can retain the data, but I can sacrifice it)
I am trying to instrument our java webapp based on tomcat using Elastic APM.
Tomcat starts fine without the javaagent but refuses to start with it.
I do not see any logs in /var/log/tomcat.
The following is the log:
Has anyone faced this before?
Any ideas what I could do to fix it?
Getting error while integrating Elastic APM in the Node Express application.
Nodejs: v6.4.2,
app.js
package.json
Error:
You have triggered an unhandledRejection, you may have forgotten to catch a Promise rejection:
  Error: Can't set headers after they are sent.
at ServerResponse.OutgoingMessage.setHeader (_http_outgoing.js:335:11)
      at /home/ubuntu/depanels/server/api/user/user.controller.js:22:25
      at /home/ubuntu/depanels/server/auth/auth.service.js:149:28
      at _fulfilled (/home/ubuntu/depanels/node_modules/q/q.js:854:54)
      at self.promiseDispatch.done (/home/ubuntu/depanels/node_modules/q/q.js:883:30)
      at Promise.promise.promiseDispatch (/home/ubuntu/depanels/node_modules/q/q.js:816:13)
      at /home/ubuntu/depanels/node_modules/q/q.js:624:44
      at runSingle (/home/ubuntu/depanels/node_modules/q/q.js:137:13)
      at flush (/home/ubuntu/depanels/node_modules/q/q.js:125:13)
      at elasticAPMCallbackWrapper (/home/ubuntu/depanels/node_modules/elastic-apm-node/lib/instrumentation/index.js:191:27)
      at nextTickCallbackWith0Args (node.js:419:9)
      at process._tickDomainCallback [as _tickCallback] (node.js:389:13)
  Elastic APM HTTP error (404): 404 page not found
  Elastic APM HTTP error (404): 404 page not found
Please suggest where I am doing wrong?
I need to send span to elastic-apm after sending express response.
In my code example, i use setInterval.
First express response, after 1 second, i try use startspan, but i got error: Cannot read property 'end' of null
My elastic stack version is 7.6.2
I am using Elastic APM.
I find that mongoose integration is not working when used with GraphQL/Apollo Server somehow.
I wrote an apollo-server plugin like this to start/stop transactions:
It works, but I am missing spans from mongoose, when I enable trace, this is what I see:
Notice the spans created fine before and after those mongoose calls, but the mongoose calls seem to be unable to find the active transaction somehow.
no active transaction found - cannot build new span
I am adding the elastic-apm-node package to our nestjs backend.
I am using the graphql feature of nestjs.
Because of this, all requests are merged together as  /graphql  in elastic.
Is this how it is supposed to be?
I imagined that since apollo-server-express is supported by elastic-apm-node, which is also used by nestjs, it should be displaying it better.
Am I missing something?
UPDATE
The graphql feature is set up using the docs for nestjs:  https://docs.nestjs.com/graphql/quick-start  it is basically their recommended setup I am using.
Has anyone already tried to connect wso2 production with elastic apm server ?
I have done this
But i dont have the http request of the API in the APM in kibana
I am running a sample application jar on local system using elasticAPM agent.
Elastic APM show 2 different cpu stats (system/process).
Metrics explanation on official site says the same thing for both stats
 https://www.elastic.co/guide/en/apm/server/current/exported-fields-system.html
Please explain, Is the &quot;system cpu stats&quot; is of my system even when the agent is connected to application.jar only using java command?
If so, how can I check on elastic apm what else on my system in consuming cpu since only application is running during the load test.
java -javaagent:&lt;agent.jar&gt; -jar &lt;app.jar&gt;
The CPU usage shown below
Elastic APM uses a Java agent to collect application performance metrics and sends it to the Elastic APM server, which means it will use Java instrumentation for underlying metrics in JVM.
My question is:
Is there any vulnerability issue or security risk for using it?
I am very new to Elastic APM and not sure how it can support different frameworks.
I can see that from the documentation APM supports Spring Boot.
I have tested a Spring Boot application with the APM and it looks promising.
I was wondering if APM supports Spring Cloud Stream as well.
Spring Cloud Stream provides Event Driven Architecture by using Spring Boot and messaging middleware.
Middleware can be Kafka, RabbitMQ, etc.
Is there a way for me to track garbage collection of my Java application using Elastic APM and the associated Java APM agent?
I'm using Spring Boot, if that makes a difference.
Out-of-the-box I'm able to see the heap and non-heap memory utilization, but I'm not sure if there is also a way to view garbage collection.
When nesting spans in Elsatic APM through either the Opentracing API or Elastic APM's API.
Some spans are never recorded.
Using  import * as apm from '@elastic/apm-rum'; :
Using Elastic's OpenTracing API:
The behavior for spans are just as inconsistent.
It is unclear when a transaction begins or ends.
Some spans are translated into transactions, and nested spans may not be recorded.
If I declare a page wide transaction, Angular's  ngOnInit  can be recorded by a span, but other event hooks are never recorded.
I have tried variations of this.
Wrapping span in span, childOf, app-level span, individual instances of span.
I am trying out Elastic APM.
I used automatic setup with  apm-agent-attach-standalone.jar .
(I also used  -javaagent  flag (manual setup), and it worked fine)
Data was successfully recieved from the agents, and I used APM UI to monitor.
How do I detach this agent from the process?
What do I do if I want to stop this agent?
I'm looking to use Elastic APM to monitor my own custom events only (user action events) in Node.js
I can't find in the docs how to turn off all HTTP monitoring and also how to send a custom event.
They have custom span and custom transaction in the docs but I'm not sure when or how to use either for my use case.
For example in the docs they describe a custom transaction as follows:
I'm not looking to send an error, I'm looking to send a custom event so I'm not sure what's up with that.
I'm trying out Elastic APM.
I have successfully created a service with data flowing in.
I wanted to see if I can have multiple services.
Somehow, I ran into problems, so I wanted to delete some services.
However, I couldn't find a way to delete a service.
Question : How can I delete a service in APM?
Indexes related to APM :
Above contains the service that I want to remove.
I've seen some APM that only measures web applications which run on WAS.
Can Elastic APM meausure the performance of other applications like pure Java application and etc?
If not,  can I use  https://www.elastic.co/guide/en/apm/agent/java/1.x/public-api.html  (Public API) so that it can measure the performance of non web applications?
I will appreciate any advice.
Cheers.
I integrated  Elastic APM  as follows to my Vue.js App.
It logs successfully to Elastic APM.
However it is not showing the current page name in logs correctly.
It seems to always show the first page on which the App has been mounted in APM.
I would like to always see the current page to be linked to the corresponding APM events.
Any suggestions how to achieve this?
The documentation says to set  pageLoadTransactionName .
However it seems to not update this on route change:
 https://www.elastic.co/guide/en/apm/agent/rum-js/current/custom-transaction-name.html
App.js
I can't understand if Elastic APM for Java should capture logs from slf4j or it can track only exceptions?
I have Spring Boot service that uses slf4j but I can't find log entries in apm index.
Can anybody clarify the expectation?
Thank you
flushing due to time since last flush 9.060s   max_flush_time 9.060s
I 'm getting tone of those message in django debug.
I tried changing to their default setting
Still getting lots of logs.
How to turn this off?
I'm having trouble getting the APM "button" and dashboard to appear on the Kibana page.
Yes, there is the "Add APM" button which tells you what to do, but it doesn't seem to actually work.
Actually, this is not entirely true - I was able to get the APM "button" and corresponding dashboard "installed" in my Kibana view,  but I can't remember what I had to do to make that happen.
I believe that I have the various components (Elasticsearch, Kibana, APM server) installed and running.
The "check APM server status" button indicates that it's been set up properly.
If I click the "APM dashboard" button at the bottom of the page, it gives me a list of items, but I don't know what they are or whether they have anything to do with APM.
I am at a loss as to how to get APM to appear in Kibana.
Does anybody have any ideas?
UPDATE :
https://www.elastic.co/guide/en/apm/server/current/getting-started-apm-server.html
then
https://www.elastic.co/guide/en/apm/server/current/installing.html
then
https://www.elastic.co/guide/en/apm/server/current/apm-server-configuration.html
This seems to provide specific information I haven't been able to find elsewhere.
The usage of  apm-server setup &lt;flags&gt;  seems promising.
I'm not sure which flags (if any) I should use?
I have integrated  Elastic APM  to my Vue.js App accordingly to the documentation ( https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html )
In addition to the default events  page-load  and  route-change  I want to add custom transactions/spans for some button clicks.
I am stucked with checking if there is already an existing transaction start which I could use to add a custom span:
However getting the current transaction fails (first line).
I am trying to integrate Elastic APM and Sentry into my website using Buffalo.
The interesting files are as follows:
handlers/sentryHandler.go
handlers/elasticAPMHandler.go
actions/app.go
The problem I'm running into is if I have the Sentry/APM handlers at the top, then I get errors like  application.html: line 24: "showPagePath": unknown identifier .
However, if I move it to just before I set up the routes, then I get a no transaction found error.
So, I'm guessing that the handler wrappers are dropping the  buffalo.Context  information.
So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?
I am using Elastic APM agent ( https://www.elastic.co/guide/en/apm/agent/dotnet/current/index.html ) to instrument an ASP.NET MVC Application.
I added the nuget packages and added the module entry in the web.config.
I am able to get data in the Kibana APM tab and nicely shows the time spent by each call.
(see screenshot below).
Mu Question is: How can I drill down inside each of these calls to see where the time is spent in the stackstace?
Is there something I am missing?
We have newly introduced elastic APM to monitor application performance.
After the deployment, we have an issue with Kibana APM UI that doesn't show up any transactions for few services.
we are having data in the APM indice but doesn't transactions show up in the Kibana APM dashboard   UI we get Avg.response time N/A and Trans.per minute is 0.
APM-Server 7.6.1
Elastic Agent - 1.15.0
ELK Stack - 7.6.1
Please help me on this to identify the issue.
Thanks in advance.
I try to implement profile based application in Spring Boot and this works for Spring Boot clearly.
But when I try to implement it for elastic-search APM, it doesn't work.
According to  here : We can describe properties like with elastic.apm prefix:
but it doesn't work.
When I call it with elasticapm.properties, it works.
Do you have any suggestion?
We're using the .NET agent (1.4) and Elastic ECE 7.6.
Activating the APM server and instrumenting our application was quite easy.
Everything seems to work up until the point where it needs to show the code snippet related to a particular span.
Below an example of what I'm looking for from a Node application:
My question:
Is this not yet included in the .NET agent, or is there additional configuration necessary to get this working?
I have the Elastic Stack running with Docker.
I've also added an Angular 9 App via Docker and enabled CORS on Elastic.
I am trying to send logs from the Angular 9 app to APM (on port 9200), using the  APM Angular Package .
However I'm getting a 400 error.
I'm doing what the docs say, their NPM package and running:
But still getting the error in the screenshot  [Elastic APM] Failed sending events!
Error: "http://localhost:9200/intake/v2/rum/events HTTP status: 400"
Any advice on how to resolve this?
docker-compose.yml  file:
Elastic yml:
APM Server yml:
I am currently new to Elastic APM.
I am currently developing an application using spring-webflux and want to monitor my application using Elastic APM, but unfortunately, it's not working for me.
Dependecies
APM Java Agent Version -  1.8.0
Elastic search - 7.2.0
APM server - 7.2.0
Exception observed -
Could someone please suggest what I am missing ?
I am trying to check for the instrumentation of a function which runs asynchronously through the @Async annotation in spring boot.
I am using the following elastic APM library : 
  co.elastic.apm:elastic-apm-agent:1.9.0
As of now, with general configurations, I am able to see the instrumentation of API requests and scheduled jobs (@Scheduled annotation in spring boot), but I am not able to check the details of @Async annotated functions.
Is there any specific configuration that I need to do?
As far as I know, it is available in newrelic.
It would be great if someone can point to a documentation / example for the same in elastic APM.
Thanks in advance.
I am getting errors with ElasticAPM version 5.5.2 while running
Error: logging_config_func(self.LOGGING) File &quot;/usr/lib/python2.7/logging/config.py&quot;, line 795, in dictConfig dictConfigClass(config).configure() File &quot;/usr/lib/python2.7/logging/config.py&quot;, line 577, in configure '%r: %s' % (name, e)) ValueError: Unable to configure handler 'elasticapm': Cannot resolve 'elasticapm.contrib.django.handlers.LoggingHandler': No module named apps
Django: 1.6.11
Python: 2.7
ElastcAPM: 5.5.2
I have a JavaScript application and configured Elastic APM/RUM on that.
As soon as I start to interact with the application some metrics start sending to APM Server (page_load for example).
I want to know if it's possible to send a specific text message to APM Server.
Example:
I'm trying to inject myDebugMessage this way:
payload.myDebugMessage = debugMessage
See the code below:
I have a standalone JAVA application.
And have integrated it successfully with Elastic APM (+ElasticSearch +Kibana) for capturing telemetries.
Java Version:  8 - OpenJDK 
 Elastic Agent &amp; Library Version:  1.16 
 Elastic Search, APM and Kibana Version:  7.7.1
Below are the relevant JVM Options being used:
However some out of the box transactions from Quartz(which is being used in the application) are shown as expected.
Which should mean the integration with the Elastic APM Server is fine.
It appears to me, even though the transactions are being captured successfully, those are not reported(sent) to the APM Server
Refer some relevant apm logs:
I have a standalone Python application.
The python process is not using any framework.
And is a simple standalone python process.
This has been successfully integrated with Elastic APM (+ElasticSearch +Kibana) for capturing telemetries.
Python version:  3.7 
 elastic-apm python agent:  5.8.0 
 Elastic Search, APM and Kibana Version:  7.7.1 
 
As mentioned in the  official doc , I have used the following statements to start capturing metrics from my python process
But on Kibana, I am able to see only the following 3 system metrics (under 2 visualizations):
As per the  python code  analysis, as well as per what I have  read .
Elastic APM Agent collects other process related metrics like:
Refer the screenshot 
 
 
 
 Additionally, I expect the Elastic APM Python agent to collect other informations like :
Which are already available for Elastic APM Java agent.
Refer the screenshot
As of a few weeks ago we added filebeat, metricbeat and apm to our dotnet core application ran on our kubernetes cluster.
It works all nice and recently we discovered filebeat and metricbeat are able to write a different index upon several rules.
We wanted to do the same for APM, however  searching the documentation  we can't find any option to set the name of the index to write to.
Is this even possible, and if yes how is it configured?
I also tried finding the current name  apm-*  within the codebase but couldn't find any matches upon configuring it.
The problem which we'd like to fix is that every space in kibana gets to see the apm metrics of every application.
Certain applications shouldn't be within this space so therefore i thought a new  apm-application-*  index would do the trick...
Since it shouldn't be configured on the agent but instead in the cloud service console.
I'm having troubles to 'user-override' the settings to my likings.
The rules i want to have:
I see you can add  output.elasticsearch.indices  to make this happen:  Array of index selector rules supporting conditionals and formatted string.
I tried this by copying the same i had for metricbeat and updated it to use the apm syntax and came to the following 'user-override'
but when i use this setup it tells me:
Then i updated the example but came to the same conclusion as it was not valid either..
Currently spring boot micorservices are enabled in Elastic APM.
We can also trace at method level and DB queries are shown.
But Spring batch job(Spring boot based) does not show any method level details and Oracle transaction details.
Does anything needs to be explicitly configured in Elastic APM for Spring batch applications.
logstash 's elasticsearch output has option to turn off SSL verification
https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-ssl_certificate_verification
Is there a similar option for apm?
Elasticsearch is using self signed certificate, and apm is complaining when connecting to ES.
I have setup two Elastic APM servers which sends data to Elastic-search.
When I setup Elastic-search with single instance, Elastic APM server working fine responding 202 response.
When I setup Elastic-search cluster(3 master, 3 data nodes, 2 ingest, 2 coordinate nodes) , APM server server misbehaves responding interleaved 202 and 503
Using coordinate host in Elastic APM.
This link  https://www.elastic.co/guide/en/apm/server/master/common-problems.html#queue-full   has some info but I am unable to resolve issue.
Edited: 
Problem is one of Elastic APM server is not able to push data on ES cluster due to index.
But I am still don't know why index name is different on single ES node vs Cluster
Currently have Elastic Apm setup with:  app.UseAllElasticApm(Configuration);  which is working correctly.
I've just been trying to find a way to record exactly how many SQL Queries are run via Entity Framework for each transaction.
Ideally when viewing the Apm data in Kibana the metadata tab could just include an  EntityFramework.ExecutedSqlQueriesCount .
Currently on .Net Core 2.2.3
I have a react application with SSR implementation using node js Express.
I am trying to instrument the application with elastic APM.
Following  Elastic APM   docs I added below changes to index js file to start APM agent.
index.js :
Getting below message on local env console: APM Server transport error
(ECONNREFUSED): connect ECONNREFUSED 127.0.0.1:8200
On Kibana APM dashboard UI we get Avg.response time N/A and Trans.per minute is 0.
Please suggest how this can be debugged.
Reference Article:  Distributed Tracing in your Kibana with Node.JS
I am using elastic-apm with spring application to monitor API requests and track all SQL's executed for given endpoint.
The problem is give the amount of traffic elastic search is collecting huge magnitude of data and I would like to enable capturing span only for specific endpoints.
I tried using public api of elastic-apm  https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html 
I can customize a transaction and span but I couldn't find a way to enable/disable to specific endpoints.
I have tried this but no luck -
I have a very basic Django APM agent setup:
My APM server is up and running on localhost:8200.
But it seems my APM agent can't make a connection to the APM server.
Here is a part of my log file that I think cause the problem:
Here are my APM logs:
On my APM server, I'm not receiving any requests from my agent.
I checked the APM server log files.
I'm trying out the .Net agent in Elastic APM and I'm using a C# application which is created using a framework called ASP.net Boilerplate.
I've added the core libraries as mentioned in the documentation and added the settings in appsettings.json.
This enables the default instrumentation and I got traces in the APM visualized through Kibana.
Currently I've got a node.js application running and I publish a message to a RabbitMQ queue with the traceparent in the message payload.
The C# app reads the published message.
I need to create a transaction or span using this traceparent / trace id so that Kibana would show the trace among the distributed systems.
I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.
I've checked the Elastic APM agent documentation -&gt; Public API for information but couldnt find any information on this.
Is there a way?
Thanks.
I'm trying to figure out a way to add a custom implementation to the Elastic APM .Net Agent code.
Does anyone know how to set it up in Visual Studio 2019?
Any references that I could use to refer on setting it up.
Thanks.
I'm creating custom spans for outgoing requests for my java app.
It works and it's great ;)
But... when I compare my custom span on Kibana's APM board with other spans from springboot applications, which are created by elastic-apm-agent, then I can see that my spans are very low on additional details.
I would like to have at least the URL details included in my custom span.
The apm-agent-api doesn't allow this.
Is there a way to set additional details, like url, to custom span?
(I don't want to use labels for this)
Thanks
I want to install apm elastic for log errors and transactions in lumen.
I found a package for elastic called anik/elastic-apm-php but when I install with composer I see below error
How to use elastic log monitoring?
I am planning to deploy ELK to monitor my application running in AWS.
My apps are using AWS xray for trace data.
I am reading the doc about elastic APM to see how to ingest AWS xray to elasticsearch but I can't find any solution.
I have read the agent doc  https://www.elastic.co/guide/en/apm/agent/nodejs/3.x/intro.html  but xray is not listed as supported framework.
Does this mean I need to build a xray agent and send the trace data to APM server?
Or is there an easier way to do that?
I am using  https://github.com/elastic/apm-agent-nodejs  in a node application to send trace data to Elastic APM Server.
I will send trace data to APM server and I am able to view them via Kibana.
The trace id, start/end time is calculated by APM client or server.
How can I provide my own trace id and start/end time for each transaction and its spans?
I'm able to install elasticseach and kibana, both are up and running.
In Kibana dashboard APM server is setup, and indices are showing up.
I am getting the following error for APM-Agent when I trace the log.
ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Error trying to connect to APM Server.
Some details about SSL configurations corresponding the current connection are logged at INFO level.
ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Failed to handle event of type JSON_WRITER withthis error: connect timed out
APM-Agent Yaml File
ElasticSearch Yaml
Kibana Yaml
APM Server Yaml
I receive logs from opentelemetry-collector in Elastic APM
logs structure :
&quot;{Timestamp:HH:mm:ss} {Level:u3} trace.id={TraceId} transaction.id={SpanId}{NewLine}{Message:lj}{NewLine}{Exception}&quot;
example:
I tried use pipeline
My goal see logs in Elastic APM
I have the following simple code below to run APM with logs.
APM is firing stats and looks like it works great.
But the problems that the errors are not appearing anywhere in Kibana 
What am I missing?
Thanks
Elasticsearch,kibana and apm-server are installed in a ec2 server
I have installed automatic java agent attach to another server to track jenkins app
Agent is getting attached to the process but dynamic configuration options are not working
Apmagent directory: (command ls)
apm-agent-attach-standalone.jar elasticapm.properties
elasticapm.properties file
Attach Command:
sudo java -jar apm-agent-attach-standalone.jar --include '.
jenkins. '
-&gt;This doesn't pick configuration file but attached the agent
so i used below command to update
Log:
Query:
1.Which is the right way to use the configuration options in command line?
2.Do we need to create a log file or it will create if log_file is used..now its polluting the application log
I am implementing apm agent to sonarqube and I can connect to apm server using curl
curl -v &quot;&quot; returns 200
Command:
sudo java -jar apm-agent-attach-standalone.jar --include '.
sonar. '
---config config_file=elasticapm.properties
Command output:
It shows agent as attached
Logfile:
Query:
It shows access denied exception ... How to fix this?
Note: 
For server_url i tried using properties file, --config server_url, --args server_url which usually works
for the code below I wanted to create a new tab which would store all the external method calls and would be aggregated, but I am not getting any such results.
I was looking at APM tools.
Essentially Dynatrace and I could see that it also provides tracing capabilities that seem to be language agnostic and also without code modifications.
Where would jaeger/open tracing be a better option than a tool like dynatrace?
Yes, dynatrace (or others like Elastic APM) is capable of providing a lot more insight into an application other than tracing.
But just from tracing perspective...
What advantages or capabilities does jaeger have that are better than APM tooling or not available in APMs.
ONLY from the tracing perspective.
I'm using Kafka for a queue, with Node services producing and consuming messages to Kafka topics using  Kafka-Node .
I've been using a home-brewed distributed tracing solution, but now we are moving to the Elastic APM.
This seems to be tailored to HTTP servers, but how do I configure it to work with Kafka?
I want to be able to track transactions like the following: Service A sends an HTTP request to Service B, which produces it to Kafka Topic C, from which it is consumed by Service D, which puts some data into Kafka Topic E, from which it is consumed by Service B.
I have a very simple piece of code written in node.js which runs on Kubernetes and AWS.
The app just does POST/GET request to create and get data from other services.
service1-- service2- service3
Service1 get post request and call service2, service2 calls postgres DB (using sequlize) and create a new row and then call service3, service3 get data from the DB and returns the response to service2, service2 returns the response to service1.
Most of the times it works, but once in 4-5 attempts + concurrency, it dropped and I got a timeout.
the problem is that the service1 receives the response back (according to the logs and network traces) but it seems that the connection was dropped somewhere between the services and I got a timeout ( ESOCKETTIMEDOUT ).
Is it possible Kubernetes drops my connection?
I'm working on a corporate ASP.NET Core application that needs to reach out to a cloud resource (Elastic APM).
Unfortunately, our corporate proxy is swatting down the request before it can complete.
It's not my code that making the requests but code within a NuGet package, so I can't easily change how it's making the connection.
I'm hoping to use middleware to inject the proxy details before it goes out the door.
I've tried this below, but it doesn't seem to be working:
Long-term, I am going to whitelist the cloud resource.
But as I prototype this solution, I'd prefer not to go through that red tape...
I am using elastic apm agent for monitoring, I have to download the apm-agent.jar and included it in my start entry point like  java -javaagent:/path/to/apm-agent.jar app.jar .
The problem is I have to manually download the apm-agent.jar, is there a way that I can configure the apm agent in my Gradle dependencies?
and then refer to the path of the jar file that was downloaded by gradle in the Dockerfile?
What is the proper way of dependency management for jar files like java agent?
I'm using the  Elastic APM Agent  as a Java Agent to monitor usages of various methods in my spring boot microservice.
This all works fine and we're able to graph various metrics in Kibana.
Unfortunately what it doesn't do is consistently attach the same labels to all spans within a transaction e.g.
details of the user who made the original request.
To work around this I thought I could use ByteBuddy (which I've never used before) to wrap any usages of the APM Span class and attach that information (as it's readily available from ThreadLocal) to each instance.
I am however having issues accessing the Span class as it's in the APM Java Agent and with the following code I get the following logs where it appears that it's not able to find the Span class...
I've tried using ByteBuddy for my own classes and it all works without issue, but I'm getting very confused around which classloader has loaded what and how to point ByteBuddy at them.
What is the right way to configure/enable an Elastic APM agent in a Nuxtjs project?
I referred  this documentation  for a custom NodeJS app.
The key takeaway was:
It’s important that the agent is started before you require any other
modules in your Node.js application - i.e.
before http and before your
router etc.
I added the following snippet in nuxt.config.js, but the APM agent is not started or working.
I do not see any errors in the app logs.
Is there any other way to do this?
I monitor my jar using Elastic APM Agent, i run these commands manually :
Now , i want to pass these parameters using docker run , i create the image and i try with this command to pass these settings , but the application is not starting
any idea to resolve this ?
Thanks
I just created one application with springboot , used the Elastic APM attacher of APM tool.
When i run the apm attacher, it generates error exception as shown below.
Code used to generate the error:
But if I try to run the application commenting the line i.e //ElasticApmAttacher.attach(); from same code, it runs successfully
i am searching for solution but so far clueless.
Can someone please suggest how to resolve it
I wish to make a change to a standard singleton design pattern that follows the  System.Lazy&lt;T&gt;  design as described  here .
The change is to  Agent  in Elastic APM Agent which can be seen in GitHub  here .
This is the code broken down for brevity:
The clear problem with this is that if  Agent.Instance  is accessed before  Agent.Setup  is called, the  Foo  object in  Agent.Lazy  is instantiated with null ( _bar ) passed to its constructor.
Therefore, the expectation that the  Bar  object passed to  Setup  will be used for the underlying  Foo  will not be met.
The problem of course is that this is an antipattern as described  here , because this singleton is encapsulating global state.
As this link describes:
A singleton is a convenient way for accessing the service from anywhere in the application code.
The model quickly falls apart when the service not only provides access to operations but also encapsulates state, which affects how other code behaves.
Application configuration is a good example of this.
In the best case, the configuration is read once at the application start and does not change for the entire lifetime of the application.
However, different configuration can cause a method to return different results although no visible dependencies have changed, i.e.
the constructor and the method have been called with the same parameters.
This can become an even bigger problem if the singleton state can change at runtime, either by rereading the configuration file or by programmatic manipulation.
Such code can quickly become very difficult to reason with:
Without comments, an uninformed reader of the code above could not expect the values of  before  and  after  to be different, and could only explain it after looking into the implementation of the individual methods, which read and modify global state hidden in  Configuration  singleton.
The  article  advocates using DI to resolve this.
However, is there a simpler way to resolve this situation where DI is not possible or would involve too much of a refactor?
I would like to send some labels along with the stacktrace to the Elastic APM server from the client side.
Using the  @elastic/apm-rum  javascript agent, I'm calling  apm.addLabels({ ... })  and passing my object.
Looking at the request in my browser, the labels look like they are being sent properly.
Here's the relevant part of the request.
I am expecting that the  browser  and  os  labels show under the  Labels  tab in the error report, but instead I'm seeing this.
So my question as an elk novice is, what additional configuration do I need to get the labels in the report?
I installed latest version of ELK stack on Azure (7.0.1).
I have apm-server on kubernetes with this docker image:  docker.elastic.co/apm/apm-server:7.0.1
But, it's not connecting with elasticsearch server.
Error:
ERROR   pipeline/output.go:100  Failed to connect to backoff(elasticsearch( http://x.x.x.x:9200 )): Connection marked as failed because the onConnect callback failed: This Beat requires the default distribution of Elasticsearch.
Please upgrade to the default distribution of Elasticsearch from elastic.co, or downgrade to the oss-only distribution of beats
INFO    pipeline/output.go:93   Attempting to reconnect to backoff(elasticsearch( http://x.x.x.x:9200 )) with 11 reconnect attempt(s)
INFO    [publisher]     pipeline/retry.go:189   retryer: send unwait-signal to consumer
INFO    [publisher]     pipeline/retry.go:191     done
INFO    [publisher]     pipeline/retry.go:166   retryer: send wait signal to consumer
INFO    [publisher]     pipeline/retry.go:168     done
INFO    elasticsearch/client.go:734     Attempting to connect to Elasticsearch version 7.0.1
INFO    [request]       beater/common_handler.go:185    handled request {&quot;request_id&quot;: &quot;2e79d623-b8fb-4743-8b50-b516db256d5b&quot;, &quot;method&quot;: &quot;POST&quot;, &quot;URL&quot;: &quot;/intake/v2/events&quot;, &quot;content_length&quot;: -1, &quot;remote_address&quot;: &quot;10.0.11.11&quot;, &quot;user-agent&quot;: &quot;elastic-apm-node/2.11.0 elastic-apm-http-client/7.3.0&quot;, &quot;response_code&quot;: 202}
For logging in our microservice applications we simply log to stdout/console and the docker logging driver handles and redirects these logs somewhere e.g.
gelf/logstash, fluentd, etc.
Basically, we're following  12 factor  guidelines for logging.
This means that developers working on the application code don't need to know anything about the underlying logging solution (e.g.
Elasticsearch, Graylog, Splunk, etc) - it's entirely an ops/configuration concern.
In theory we should be able to change the underlying logging solution without any code changes.
I'd like something similar for traces and my research has led me to OpenTracing.
Developers shouldn't need to know the underlying tracing solution (e.g.
Jaeger, Zipkin, Elastic APM, etc) and as per logging; in theory we should be able to change the underlying tracing solution without any code changes.
I've successfully got a .NET core POC sending traces to Jaeger using the  opentracing/opentracing-csharp  and  jaegertracing/jaeger-client-csharp  libraries.
I'm still trying to fully get my head around OpenTracing, but I'm wondering if there's a way to send traces to an OpenTracing compliant API without having to take a hard dependency on a particular solution such as Jaeger (i.e.
the jaeger-client-csharp library).
Based on my understanding OpenTracing is just a standard.
Shouldn't I just be able to configure an OpenTracing endpoint with some sampling options without needing the jaeger-client-csharp library?
Or is it that the jaeger-client-csharp is not actually Jaeger specific and can actually send traces to any OpenTracing API?
Example configuration shown below, which uses jaeger client library:
I have the elk setup in a ec2 server.With Beats like metricbeat,filebeat,heartbeat.
I have setup the elastic apm for some applications like jenkins &amp; sonarqube.
Now In uptime I can see only few monitors like sonarqube and jenkins
Other application are missing..
When I see data from yesterday not available in elasticsearch for particular application
I have been using elastic APM for tracing a application.
I have a main Spring boot application and another spring based jar file(my own) which is included as a dependency in main Spring boot project.
When I add add custom context &amp; indexed labels from main Spring boot project , elastic APM console does shows up that trace data.
However , when I write some tracing code inside ( adding index label or adding custom context) inside that spring based project which is then included as a jar libaray inside my spring boot project , that is not shown up on console.
Does this mean I can only instrument only main project and not instrument included jar library&gt; I have configured packages for both main spring boot project as well included spring dependency.
Any help is highly appreciated.
ElasticApm.currentTransaction().setLabel(&quot;test1&quot;, &quot;test2&quot;) 
 ElasticApm.currentTransaction().addCustomContext(&quot;test3&quot;, &quot;test4&quot;)
Attached ELastic APM before starting spring boot app as :
 ElasticApmAttacher.attach(configMap);
I'm currently facing a very strange issue.
I did some optimizations in my queries, which improved quite a lot the overall performance for my Django application's GET requests.
However, I'm still facing a few very slow ones (1000ms or more).
Checking on Elastic APM, I noticed that for all those cases, there was a DB reconnection.
While it's obvious those requests would take more time, it's still takes WAY more time than the amount required for the reconnection itself.
I have set the DB_CONN_MAX_AGE to  None , therefore the connections themselves shouldn't be closed at all.
Which makes me think the reason for the disconnection itself could also indicate the cause for this.
The blue bars represent the amount of time a given transaction took.
The total amount of time for this particular request was 1599ms.
The DB reconnection took 100ms, the the queries about ~20ms.
Adding those up, gives a total time of 120ms.
I'm a bit clueless how to find out where the rest of the 1479ms.
I did some load tests locally, but couldn't reproduce this particular issue.
Of course is serializations, middlewares, authentication and other things that might add up some time to requests, but not nearly the 1479ms shown here.
It certainly related to the DB connection itself, or better yet, something that happens before that.
But I'm not sure how to diagnose it.
Specially being unable to reproduce this locally.
I'm open to any ideas that could lead to more information on how to solve this problem.
Or maybe someone had a similar experience and could share it with me?
I change some settings to elastic-apm.
https://www.elastic.co/guide/en/apm/server/current/configuration-process.html
I want to verify if the setting is actually changed.
but not sure how to check ..
Is there an endpoint where I can view the current configuration?
I am using Elastic Search in my MVC Application and getting en error when adding migration.
Flow is:
There is an warnin message like that
The type 'Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6' registered in the application config file as an IDbInterceptor not be loaded.
Make sure that the assembly-qualified name is used and that the assembly is available to the running application.
Running a .NET Core 3.1 API with an async Controller Method that runs multiple DatabaseRepository async methods.
I'm calling DatabaseRepository Tasks like this:
DatabaseRepository method all look like this:
What I expected is that most of these async methods will run concurrently, but that doesn't happen.
How do I know?
With Elastic APM.
This is what I see:
https://cdn.discordapp.com/attachments/195830344715337728/757569429683830795/unknown.png
What do I do wrong?
I have gke clusters, and I have elasticsearch deployments on elastic.co.
Now on my gke cluster I have network policies for each pod with egress and ingress rules.
My issue is that in order to use elastic APM I need to allow egress to my elastic deployment.
Anyone has an idea how to do that?
I am thinking either a list of IPs for elastic.co on the gcp instances to be able to whitelist them in my egress rules, or some kind of proxy between my gke cluster and elastic apm.
I know a solution can be to have a local elastic cluster on gcp, but I prefer not to go this way.
I have an Elastic APM-Server up and running and it has successfully established connection with Elasticsearch.
Then I installed an Elastic APM Go agent:
It returned the following:
Then I setup the  ELASTIC_APM_SERVER_URL  and  ELASTIC_APM_SERVICE_NAME :
However, I don't see the agent getting registered in the APM dashboard.
It isn't sending any data to the APM Server.
How do I make sure that the agent is running?
How do I check the agent log as to why it isn't able to connect to the APM server?
I am trying to run  Java APM agent on Kubernetes with Springboot 2.3.1.RELEASE
I get the following error
[elastic-apm-server-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler -  Failed to handle event of type METRICS with this error: / by zero
It works well if I run it on VM with same Java version
How I am using
APM Agent language and version : Java, elastic-apm-agent-1.17.0.jar,1.16.0.jar, 1.15.0.jar
Java version
More logs
I tried with previous agent versions 1.16.0 &amp; 1.15.0 but I still get the same error.
Can anyone please help me.
Thank you very much in advance
I'm working on a django project with postgres where table and field names are generated with
double quotes.
Anyone knows how can I disable this behavior?
[Model definition]
[Migtation]
[DDL Generated]
[DDL Expected]
[requirements]
We are using  Elastic APM  for monitoring our APIs.
It shows queries status and useful information about the queries.
I want to have the same information about the queries which are sent to Elasticsearch server.
I want to have information about queries, time, status code, etc.
Is there any plugin in Elastic stack that I can use for this purpose?
I'm using elastic apm for my go app during
 development i export env variable through the terminal and it's working.
but now i want to deploy the app so i need to read variables from the  .env file
explanation
i use  go.elastic.co/apm/module/apmhttp  at my app
and when  go.elastic.co/apm  read env variables, 
it can't see  ELASTIC_APM_SERVICE_NAME ,  ELASTIC_APM_SERVER_URL  or  ELASTIC_APM_SECRET_TOKEN 
that existed at my .env on my app files.
I'm trying out different approaches for microservices tracing (i'm mostly working with event-driven services using RabbitMQ).
What I'm testing:
Given the code
My questions / remarks / issues
To have this new span, I'm using  ScopedSpan sp = tracer.startScopedSpanWithParent("getUrlConnection", tracer.currentSpan().context());  and  sp.finish() .
The span is visible in ZipKin but it's not really appealing compared to only putting a  @NewSpan .
Am I missing something ?
Elasticsearch APM agent + API seems to handle this properly by just requiring the addition of  @CaptureTransaction  and  @CaptureSpan .
I know it's not perfect because it doesn't hook directly on the consumer call nor support tracing effectively with my use case.
But it also requires to add the agent.
Thank you :).
I have tried executing this docker command to setup Jaeger Agent and jaeger collector with elasticsearch.
but this command gives the below error.
How to configure Jaeger with ElasticSearch?
There is an existing Spring Boot app which is using SLF4J logger.
I decided to add the support of distributed tracing via standard  opentracing  API with Jaeger as the tracer.
It is really amazing how easy the initial setup is - all that is required is just adding two dependencies to the  pom.xml :
and providing the  Tracer  bean with the configuration:
All works like a charm - app requests are processed by Jaeger and spans are created:
However, in the span  Logs  there are only  preHandle  &amp;  afterCompletion  events with info about the class / method that were called during request execution (no logs produced by  slf4j  logger are collected) :
The question is if it is possible to configure the Tracer to pickup the logs produced by the app logger ( slf4j  in my case) so that all the application logs done via:  LOG.info  /  LOG.warn  /  LOG.error  etc.
would be also reflected in Jaeger
NOTE : I have figured out how to log to span  manually  via  opentracing  API e.g.
:
And do some  manual  manipulations with the  ERROR  tag for exception processing in filters e.g.
But, I'm still wondering if it is possible to configure the tracer to pickup the application logs  automatically :
UPDATE : I was able to add the application logs to the tracer by adding wrapper for the logger e.g.
However, so far I was not able to find opentracing configuration options that would allow to add the application logs to the tracer automatically by default.
Basically, it seems that it is expected that dev would add extra logs to tracer programmatically if needed.
Also, after investigating tracing more it appeared to be that normally  logging  and  tracing  are handled separately and adding all the application logs to the tracer is not a good idea (tracer should mainly include sample data and tags for request identification)
We're using Opentracing/Jaeger in Istio for tracing multiple Spring Boot/Spring Cloud based microservices.
I'm currently wondering if there's an option to enrich the tracing spans by providing information about executed query (i.e.
SQL statement)?
Tracing JDBC connection info is working fine using  opentracing-contrib/java-spring-cloud  but additional information is missing.
I know that e.g.
glowroot  is capable of tracing the statements itself but haven't found anything related to Opentracing or Jaeger.
Would be great if anybody could show some directions for research!
I want to use istio with existing jaeger tracing system in K8S, I began with installing jaeger system following  the official link  with cassandra as backend storage.
Then installed istio by  the helm way , but with only some selected components enabled:
Jaeger and istio are installed inside the same namespace  istio-sytem , after all done, all pods inside it looks like this:
Then I followed  the link  to deploy the bookinfo sample into another namespace  istio-play , which has label  istio-injection=enabled , but no matter how I flush the  productpage  page, there's no tracing data be filled into jaeger.
I guess maybe tracing spans are sent to jaeger by mixer, like the way istio do all other telementry stuff, so I  -set mixer.enabled=true , but unfortunately only some services like  istio-mixer  or  istio-telementry  are displayed.
Finally I cleaned up all the above installation and followed  this task  step by step, but the tracing data of bookinfo app are still not there.
My questions is: How indeed istio send tracing data to jaeger?
Does sidecar proxy send it directly to jaeger-collector( zipkin.istio-system:9411 ) like  how envoy does , or the data flows like this:  sidecar-proxy -&gt; mixer -&gt; jaeger-collector ?
And how could I debug how the data flow between all kinds of components inside the istio mesh?
Thanks for any help and info :-)
Update : I tried again by installing istio without helm:  kubectl -n istio-system apply -f install/kubernetes/istio-demo.yaml , this time everything works just fine, there must be something different between  kubectl way  and  helm way .
I instrumented a simple Spring-Boot application with Jaeger, but when I run the application within a Docker container with docker-compose, I can't see any traces in the Jaeger frontend.
I'm creating the tracer configuration by reading the properties from environment variables that I set in the docker-compose file.
This is how I create the tracer:
And this is my docker-compose file:
You can also find my project on  GitHub .
What am I doing wrong?
Created a deployment on  Google Kubernetes  using  jaegertracing/all-in-one  public image from  Docker Hub
Then, exposed the deployment with Service type as  LoadBalancer .
Now, launched the Jagger UI and it is working, but it do not show any service  except jagger-query .
I have deployed my .net web api application for testing on kubernetes.
My application has only one single web API which is successfully running on Google Kubernetes engine and exposed via load balancer service type.
The API has all the data hard-coded and do not call any other service or database.
Used following nuget packages in the project:
Jaeger -Version 0.2.2 
OpenTracing.Contrib.NetCore -Version 0.5.0
I have added following code in  Startup.cs  file of my API:
jaegerHost  environment variable is assigned the  IP  of the jaeger service created.
The question is how and where to make changes so that my application service gets available in the Jaeger UI, so that I can see it's traces.
I am stuck here, can anyone please help how to proceed ahead?
We are re-building our software platform using a microservice architecture approach.
Most of it using  Spring Boot  libraries, and for our entry points we are using  Spring Cloud Gateway  which can be easily integrated with  Jaeger  to have tracing in the platform.
We do that using the following Maven dependency:
And it is working pretty well, we can see the trace on the  Jaeger UI  web console.
The problem now is how to send the tracking information to the next inner service in the platform to have every service correlated.
Any ideas?
We have tried  to inject the tracking information as HTML Headers using a  GlobalFilter  but it is using the incoming request and we need to put them on the downstream request... any clue on how to override  HTTPClient  used underneath.
Tracing tasks in celery 4.1.1 using sample code.
Each worker runs:
When I first start celery and run tasks each worker gets a working tracer and there is a log output for each of:
Any task that runs after the initial gets the global tracer 
 from  Config.initialze_tracer  (which returns  None ) and a log warning  Jaeger tracer already initialized, skipping .
Watching tcpdump on the console shows that the UDP packets aren't being sent, I think I'm getting an uninitialized default tracer and it's using the noop reporter.
I've pored over the code in opentracing and jaeger_client and I can't find a canonical way around this.
I have a problem using the jaeger open tracing project within our microservice system.
The config I use is as below.
The origin of the trace is the same as this.
This works fine and is logged within the UI and with printed results below.
But then when I extract {'uber-trace-id': '2b55203a8773aa14:77cceca94f4cfe74:0:1'} within another service it says a new span has been created as expected, but nothing is logged within the UI.
Print result:
With both Format.HTTP_HEADERS and Format.TEXT_MAP it is the same result.
Does anyone know why nothing is logged in the UI and how to fix this?
Thanks in advance.
I am looking at open tracing implementations to trace the application to JaegerUI.
Our application Front end is angular, backend end is Asp.net Web api.
I am able to trace the webapi using Jaeger C# nuget packages.
However, I have not found a way/ npm package to trace the angular trace messages to Jaeger.
I understand there's a Jaeger npm package for node.js(server) but not for client javascript/typescript that runs in browser.
Could you let me know how can we implement opentracing with Jaeger at front end(Javascript/angular)- so that we can see the full span traced (front end to backend) in Jaeger
I installed the jaeger all in one in Docker with:
And below is the sample code on how I initialize the tracer and spans.
I get the logs in my console but it does not reflect in my Jaeger UI.
Could anyone please help me with this?
I'm trying to get a little example of Jaeger working using Node.js, but I can't get the Jaeger UI to display any data or show anything.
I have read this question:  uber/jaeger-client-node: backend wont receive data  but this hasn't helped in my case.
I'm running the Jaeger back end in a docker container using:
The code for my example is:
Any help as to what I am doing wrong would be much appreciated!
In a spring boot application (just one at the moment) I included jaeger by adding dependency  opentracing-spring-jaeger-web-starter  and the below beans
After starting Jaeger in docker  docker run -d --name jaeger -p 16686:16686 -p 6831:6831/udp jaegertracing/all-in-one:1.9  I get the traces.
I found now another dependency and read different tutorials which made me somehow unsure on what is the right way to use Jaeger with spring boot.
Which dependency would I use?
https://github.com/opentracing-contrib/java-spring-cloud
https://github.com/signalfx/tracing-examples/tree/master/jaeger-java-spring-boot-web
Following the  Jaeger documentation  possibly
would be enough!
?
Before trying Jaeger I used Zipkin which is very easy to integrate in Spring since there is a starter for sleuth.
The logs contain trace and span id's in separate fields so they can be searched for e.g.
in Kibana.
Jaeger does  not .
Can that be customized and if so - how?
Is it possibly to use Jaeger with Brave for instrumentation.
The project e.g.
includes  spring-cloud-starter-sleuth  as a dependency.
There are some conflicts with already existing beans.
Can Jaeger be used with brave at all?
I have added these fields in application.yml of microservices and dependency in pom.xml.Jaeger running on my local is abl to identify the services as well
I have deployed all my microservices on kubernetes.
Please help me in deploying jaeger on kubernetes.
UPDATE: 
I have reached this step.
I have a load balancer IP for jaeger-query.
But on which host and port will my microservice send the logs to  ?
?
So, I am trying to trace logs of my spring boot application with jaeger so what are the steps that should be perform if my application and jaeger is deploy on kubernetes.
I have successfully deployed jaeger and spring boot application 
now how will I configure jaeger in my service.
My services are not visible in the jaegar console.
I have added the following configuration to the yml:
Output og kubectl get service jaeger-query
I'm using jaeger with spring boot to trace a test application, sometimes I get some extra space or overlap that appears between spans in a single-threaded trace that takes up to 20ms.
I am confused about this extra space because there aren't any codes between these spans and I expected to see spans starting after each other.
Here are my output results.
We lately setup a jaeger server in order to trace all requests throughout our system.
The initial setup worked pretty nicely by simply adding the necessary spring (cloud) starter dependencies to our build files.
Each time, a request hits one of our servers, a new span is created and reported to the jaeger server which was setup by using the all-in-one docker image.
The most important dependencies are the following:
While spans are created on the server, the necessary headers are not forwarded to the feign clients.
According to the documentation, the addition of  opentracing-spring-cloud-feign-starter  dependency should to the trick, but so far, none of the feign clients worked.
I also added a breakpoint to the auto configure class provided by opentracing
and this method is invoked, when the application starts up.
There are also is also some information in the logs regarding the initialization of jaeger/opentracing:
I spent quite some time, reading into documentation and looking for examples how to configure a spring boot/cloud application correctly in order to work with feign clients but so far I had no luck.
Most examples out there use Springs' RestTemplate instead of Feign clients.
I would be very happy, if somebody could point me towards the right direction.
We are trying to go reactive with Webflux.
We are using Jaegar with Istio for instrumentation purposes.
Jaegar understands Spring MVC endpoints well, but don't seem to work at all for WebFlux.
I am looking for suggestions to make my webflux endpoints appear in Jaeger.
Thanks in advance.
I'm trying to set up a local k8s cluster and on  minikube  with installed  istio  and I have an issue with enabling distributed tracing with Jaeger.
I have 3 microservices  A -&gt; B -&gt; C .
I am propagating the all the headers that are needed:
But on Jaeger interface, I can only see the request to the service A and I cannot see the request going to service B.
I have logged the headers that are sent in the request.
Headers from service A:
Headers from service B:
So the  x-request-id ,  x-b3-traceid ,  x-b3-sampled , and  x-b3-spanid  mathces.
There are some headers that aren't set.
Also, I'm accessing service A via k8s Service IP of type LoadBalancer, not via ingress.
Don't know if this could be the issue.
UPD: I have setup istio gateway so now I'm accessing service  A  via istio gateway.
However the result is the same, I can see the trace for  gateway-&gt;A  but no any further tracing
After login to the Keycloak Jaeger(realm) client, the keycloak server doesn't navigate to the Jaeger UI path -  localhost:16686.
It seems keycloak verifies the user (see below code)
proxy.json
keycloak.json
We have selected to use Jaeger API is used for tracing.
There , we have setup the Jaeger locally using docker as mentioned below.
In the  ServletContextListener , we created new cofigurations like below.
Now this works fine and I can see the tracing in  http://localhost:16686
Problem :  
I want to make the Jager setup in an external environment and connect from another application server (application server is running on wildfly 10 docker under host mode).
In future that Jaeger instance may used by more than one server instances for tracings.
After looking at the source and various references as mentioned below I tried below options.
But it connects to local always.
I tried various ports like 5775, 6831, 6832 also but result was same.
Further I tried setting JAEGER_ENDPOINT and JAEGER_SAMPLER_MANAGER_HOST_PORT as environment variables also.
But failed.
In one  reference  I found that  "Jaeger client libraries expect jaeger-agent process to run locally on each host..." .
Is that means I can't use it in cebtrelized manner and I need to setup Jaeger in each application server instances?
Otherwise how to do that?
I am trying to integrate jaeger tracing in my java spring application.
I have added following code in the configuration file :
@Bean
    public io.opentracing.Tracer jaegerTracer(){
and used following docker command :
docker run -d -p 5775:5775/udp -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14268:14268 jaegertracing/all-in-one:latest
Still, I am not able to find my service in jaeger-ui
Upon hitting this url :
 http://localhost:5778/?service=pilot-tracking 
Output is :
tcollector error: tchannel error ErrCodeBadRequest: no handler for service "jaeger-collector" and method "SamplingManager::getSamplingStrategy"
Please help!
!
I can only find old and incomplete examples of using opentracing/jaeger with Kafka.
I want to run an example locally as a proof of concept - opentracing spans to kafka.
I managed to get some of this working, but on  jeager-query  service I keep getting:
I'm not sure if I need to use some sort of storage i.e.
cassandra too?
I'm trying to instrument a Spring Cloud RxJava sample app with Jaeger, and for some reason I'm failing!
I have a couple of other SpringCloud apps, like Hystrix, JDBC and JMS working fine with the tracing being reported to Jeager by just adding the maven dependency to it.
For RxJava, on the other hand, I can't figure it out why I'm not able to follow the same approach...
When I leave the App without a  Tracer @Bean , I don't get anything in Jaeger and I get this message:
All the other SpringCloud apps are working without the  Tracer @Bean , so I was expecting the same behavior for the RxJava...
The worst part is that whenever I add the  Tracer @Bean , the bean is initialized, but still no data is sent to Jaeger...
Not sure if it is related to this message:
Does anyone have any idea?
Do I need to set anything in the  application.properties ?
I'm posting below my  pom.xml  file:
Sample app committed here:  https://github.com/julianocosta89/rxjava-jeager
I am trying to trace in a front end app.
I am not be able to use  @opentelemetry/exporter-jaeger  since  I believe it is for Node.js back end app only .
So I am trying to use  @opentelemetry/exporter-collector .
First I tried to print the trace data in the browser console.
And
the code below succeed printing the trace data.
Now I want to forward them to Jaeger.
I am running  Jaeger all-in-one  by
Based on the  Jaeger port document , I might be able to use these two ports (if other ports work, that will be great too!
):
Then I further found  more info about this port :
Zipkin Formats (stable)
Jaeger Collector can also accept spans in several Zipkin data format,
namely JSON v1/v2 and Thrift.
The Collector needs to be configured to
enable Zipkin HTTP server, e.g.
on port 9411 used by Zipkin
collectors.
The server enables two endpoints that expect POST
requests:
I updated my codes to
However, I got bad request for both v1 and v2 endpoints without any response body returned
POST http://localhost:9411/api/v1/spans 400 (Bad Request)
POST http://localhost:9411/api/v2/spans 400 (Bad Request)
Any idea how can I make the request format correct?
Thanks
I think Andrew is right that I should use OpenTelemetry collector.
I also got some help from Valentin Marchaud and Deniz Gurkaynak
at Gitter.
Just add the link here for further people who meet same issue:
 https://gitter.im/open-telemetry/opentelemetry-node?at=5f3aa9481226fc21335ce61a
My final working solution posted at  https://stackoverflow.com/a/63489195/2000548
I am following all the instructions mentioned here:  https://github.com/opentracing-contrib/java-jdbc
I assumed that with these steps the traces related to JDBC operations will automatically start getting reported.
However, in the logs, I only see this below and nothing in Jaeger UI.
Can someone please show an example of how to achieve this?
I am using below versions:
opentracing-api-0.33.0
opentracing-jdbc-0.2.10
opentracing-util-0.33.0
jaeger-core-0.35.4
Here's the abstract that I have attempted.
Is there anything else I need to do?
And then this is the code around database call:
The database config is:
I would like to know what's the minimal configuration for a spring-boot app in terms of dependencies if I need to report traces to Jaeger in Istio.
I was expecting that by adding only
the traces will be present in Jaeger.
But it's true that the envoy can not correlate the requests that go to the service with the response if the headers with the tracing details are not in the response.
It seems that sleuth only propagates the headers when calling by RestTemplate or Feign or Spring Integration... but the headers are not there in the response for an API.
In order to make it work, I added this other dependency to the service
Or even propagating the headers manually I can see traces in Jaeger.
I am confused because it seems that sleuth should be doing this task without additional dependencies as I understood from @spencergibb in this video  https://youtu.be/AMJQO9zs2eo?t=1045
In case of Sleuth dependency is not enough.
What dependencies will be required?
I can see several dependencies that seem to do similar things like the previous one, like
Thanks in advance.
So I am exploring Jaeger for Tracing and I saw that we can directly send spans from the client to the collector in HTTP (PORT: 14268), if so then what is the advantage of using the jaeger agent.
When to go for the Jaeger Agent Approach and when to go with the direct HTTP Approach.
What is the disadvantage of going with the Direct approach to the collector
I've been trying to run basic shell commands, ls as an example, but any of them work.
So, I've tried to validate if the container has a bash enabled, and answers to similar posts say to run:
But any of them work (neither docker exec -it amazing_robinson ls).
This is the error:
The container is
We are using Jaeger with camel open tracing to get metrics, we are able to see all the latency metrics in Jaeger UI.
In Prometheus we are able to see few jaeger metrics request count, but the some metrics like latency we are not able to found in prometheus dashboard.
Prometheus dashborad
We are exposing metrics using below:
host:14269/metrics, host:16687/metrics
Can some one help me to get the jaeger service latency metrics in prometheus.
I was trying to implement the OpenTracing + Jaeger to my PHP project, following the "Get started" example there  https://github.com/jonahgeorge/jaeger-client-php
Tracer, spans and scopes have been created successfully, but Jaeger does not see my service.
There are my  .php  and  docker-compose.yml  files below:
I have distribute application that consists of several Go services.
Some of those use Kafka as data bus.
I was able trace down calls between services using  opentracing  with Jaeger.
I have problem plotting Kafka spans on graph, them appear as gaps.
Here is what i was able to do.
Initial spans been created by gRPC middleware.
Producer side:
Consumer side:
How should i modify this to plot span on graph when message was in Kafka?
I am trying to setup jaeger-collector on one server with jaeger-agent running in another server.
If I run the exe jaeger-all-in-one, everything works as expected (using in memory).
In order to see the options available with ES, i am not able to run a help command.
When I run jaeger-collector --help, it shows only cassandra related flags.
How do I check the elastic search specific details.
Now, my requirement is to specify and elastic search url.
I have set up the Environment variables SPAN_STORAGE_TYPES and ES_SERVER_URLS, but couldn't find how to run jaeger-collector.exe by asking it to take in these environment variables.
Thanks,
Minu
I would like to configure Jaeger in my Spring application.
Somehow I cannot find a proper way to do this.
Almost all Spring-Jaeger-related documentation is for Spring Boot where most of the properties are auto configured.
Here's my approach.
Maven dependency:
Spring config for Jaeger:
Jaeger is running locally in docker on port 6831.
Once my application starts, I noticed that application slows down considerably, I assume that is because of metrics logged heavily to console by LoggingReporter.
However, My Spring app does not show up in Jaeger UI.
In the beginning I would like to trace my REST endpoints.
Can someone point me in the right direction why my app is missing from UI and how I configure Jaeger properly?
Is there perhaps a sample project with Spring+Jaeger that does not rely on outdated Jaeger?
I have installed ISTIO using Helm .
I forgot to enable grafana, kiali and jaeger.
How can i enable all these above services after i have installed istio?
I had setup Jaeger in Azure Kubernetes Cluster in monitoring namespace and I deployed my container which is instrumented with jaeger client libraries in monitoring domain.
The service is up and running and I'm able to see the traces using actuator when I specify the :/actuator in the browser.
But the same microservice is not populating in the service dropdown in Jaeger UI.
Below are the files i'm using.
DemoOpentracingApplication.java
Why the instrumented service is not populating in Jaeger UI in Kubernetes?
I want to inject x-b3-traceid and x-b3-spanid in logs  with pattern as shown-
For zipkins, there are libraries available like
brave-context-log4j2 –
  ( https://github.com/openzipkin/brave/tree/master/context/log4j2 )
Spring cloud sleuth.
( https://cloud.spring.io/spring-cloud-sleuth/ )
How can I add that while using jaeger?
Jaeger provides  an all-in-one  configuration for a development setup of Jaeger that doesn't use tons of memory.
The instructions  show how to easily install this via:
However I manage my development environment using Helm.
Is there a Helm chart for this setup that I can use instead?
I really read many articles.
I figure out that need to just include a starters in spring boot )))
Can anyone sort it out: is Sleuth create MDC (Mapped Diagnostic Context)?
Is sleuth create a record's ID which used by Zipkin?
Can I see this ID in Kibana?
Or do I need to use zipkin API?
Are there best practice to use all of this together?
Is Jaeger substitute both Zipkin and Sleuth or how?
I'm trying to run  istioctl install istio-config.yaml  command within CodeBuild on AWS but I get this error:
error installer PersistentVolumeClaim &quot;istio-jaeger-pvc&quot; is invalid:
spec.resources.requests.storage: Forbidden: field can not be less than
previous value
even though I don't have the path  spec.resources.requests.storage  in my configuration file!
This is the content of my file:
and this is the whole log of the command:
This is more details about the pvc  istio-jaeger-pvc :
I'm trying to install Jaeger into my K8s cluster using the streaming strategy.
I need to use the existing Kafka cluster from my cloud provider.
It requires a username and password.
Jaeger documentation mentions only broker and topic:
How can I configure Kafka credentials in CRD?
-Thanks in advance!
In the Jaeger UI  http://localhost:16686/search , there is an option to upload JSON files for traces.
I wonder can we download the traces from Jaeger itself and use them in the future for finding performance issues?
How can we do that, I see no option to download traces from Jaeger Ui.
I am using the Jaeger Operator to deploy the Jaeger Query and Collector services to Kubernetes (K3S actually) along with an ElasticSearch instance for the storage backend.
The Jaeger Operator creates an Ingress instance for the Jaeger Query service but it assumes that all of your Jaeger Agents will also be running inside the Kubernetes cluster.
Unfortunately, that is not the case for me as some applications that I am tracing are not run within the cluster so I need my Jaeger Collector to be accessible from outside.
This Jaeger GitHub issue discusses a potential enhancement to the Jaeger Operator for this functionality  and it suggests creating your own Ingress outside of the Operator to expose the Jaeger Collector but doesn't go into details.
I also want to utilize gRPC for the communication between the Agent outside the cluster and the Collector in the cluster and  this article describes how to set up an Ingress for gRPC  (though it is not specific to Jaeger).
I used the  example ingress spec there , made some tweaks for my scenario, and deployed it to my cluster:
This creates an Ingress for me alongside the simple-prod-query ingress that is created by the Jaeger Operator:
Here are the services behind the ingress:
Unfortunately, my Jaeger Agent can't seem to speak to it still...
I am actually deploying my Jaeger Agent via docker-compose and as you can see here, I am configuring it to connect to jaeger-collector.my-container-dev:80:
I can see that something is wrong with the connection because when I hit the Jaeger Agent's Sampling Strategy service with an  HTTP GET to http://localhost:5778/sampling?service=myservice , I get an error back that says the following:
Is there something wrong with my Ingress spec?
No trace data seems to be making it from my Agent to the Collector and I get errors when hitting the Jaeger Agent Sampling Service.
Also, I find it a little strange that there is no IP Address listed in the  kubectl get ing  output but perhaps that is a red herring.
As mentioned above, I am using K3S which seems to use traefik for its ingress controller (as opposed to nginx).
I checked the logs for the traefik controller and I didn't see anything helpful there either.
I am prototyping the use of  Jaeger  in an ASP.NET Core (3.1) Web API using the  Jaeger C# Client  and I got it working with the  All in One approach they mention in their Getting Started documentation .
This works fine for initial prototyping but I also wanted to test with storing to an instance of ElasticSearch.
Luckily, I found  another Stack Overflow post about this which contains a docker-compose.yaml for deploying Elastic Search and all the Jaeger components  and I got that working after a few tweaks to the slightly outdated docker-compose ( details in my answer for that post ).
However, while digging through the Jaeger documentation, I found the  CLI Flags reference for the jaeger-all-in-one distribution  that seems to contradict itself.
First, it says
Jaeger all-in-one distribution with agent, collector and query.
Use with caution this version by default uses only in-memory database.
But then it also proceeds to say
jaeger-all-in-one can be used with these storage backends:
and then lists jager-all-in-one distribution CLI Flag details for:
So this implies that the Jaeger All in One distribution can be used with Elastic Search, etc.
I am guessing the initial comment about the all-in-one distribution only supporting an in-memory database applies to the  jaeger-all-in-one with memory  option and not the others as otherwise it doesn't make sense.
Can someone with Jaeger experience clarify?
I have used the following configuration to setup the Istio
and exposed the jaeger-query service as mentioned below
I couldn't see the below deployed application in Jaeger
and have deployed the application as mentioned below
I could access the service as shown below
I do know why the service is not listed in the Jaeger UI?
Please take a look at  https://github.com/winster/jaeger-trace-reactive/blob/master/src/main/java/com/example/demo/JaegerTraceReactiveApplication.java  (readme might help to understand the problem better  https://github.com/winster/jaeger-trace-reactive )
This is a spring boot application with opentracing-jaeger.
As per the doc, jaeger supports webflux and webclient.
But it has been noted that, the trace skips the reactive flow when there is a webclient call.
Is there a way to fix this?
I am doing some prototyping of Jaeger Tracing for an ASP.NET Core Web API and I am able to get it working using the  All in One instance of Jaeger described in the Getting Started documentation  and the following code in my  Startup.ConfigureServices()  method:
To use all this, you need to add a few packages to your project:
So this works OK and I get Traces with their Spans showing up in my Jaeger Search UI (http://localhost:16686/search) but it just shows the Trace with my service name (in this case &quot;MySuperCoolWebAPI&quot;) followed by &quot;HTTP GET&quot;:
This is not terribly useful to see &quot;HTTP GET&quot; there.
Instead, I want to see the Web API action name or something else that lets me know what kind of request this really is.
As you can see from my sample code above, I have tried setting the  HttpHandlerDiagnosticOptions.OperationNameResolver  but this only affects  HttpClient  calls I make from within my web service.
It does not seem to affect how the Trace/Span associated with the request I received is named.
I also tried setting the Span OperationName in my Web API Controller method using the GlobalTracer like this but this affects an inner span and NOT the main Trace/Span that shows up on the main Jaeger UI search results page:
Here you can see the first child Span has its name set to what I forced it to but the main level Span (the parent of the one I changed) is not affected:
Is there a way I can set the operation name of the main Span with the Jaeger C# Client?
Also, I am using .NET Core 3.1 in case that is relevant.
I have 2 services A and B. I'm calling an endpoint in B from A using Spring Integration with Spring Boot 2.1.4.
In service A logs:
In service  B logs:
Clearly traceId is different in service B.
I think due to this, I'm not seeing the service B span under service A span in JaegerUi.
Any idea what I'm doing wrong?
I have configured  JaegerGrpcSpanExporter  , so that it can export the created spans to Jaeger-Collector.
I don't want to export the spans to Jaeger-Agent.
I have written down below code.
when i change the port to 14250 i.e.
Jaeger-agent port spans are exported to UI but with 14268 I am not able to find any trace at Jaeger UI.
Do i need to change the above code?
I'm working on a POC and was able to integrate 2 microservices with JaegerUI.
Request to an endpoint in serviceA calls an endpoint in serviceB and returns a response.
I have used below dependencies:
spring.boot.version : 2.1.4.RELEASE
Spring autoconfiguration takes care of everything so just added the required properties:
I want to achieve the below:
Based on the answer in below SO question:
How to enrich Jaeger opentracing data with the application logs (produced by slf4j) for Spring Boot?
opentracing-spring-cloud-starter dependency should automatically take care of sending app logs to span in JaegerUI.
I have a log statement like below in serviceA:
logger.info(&quot;sending request to serviceB.&quot;);
But above log is not getting captured in corresponding span and not visible in JaegerUI.
Any suggestions on how to achieve the above scenarios are appreciated!
I'm using this library ( grpc-spring-boot-starter ) so I can have gRPC capabilities in a Spring Boot app.
I want to know how to properly integrate this with Istio + Jaeger tracing.
I need to know what are the needed dependencies for this to happen.
I have two (2) apps, one serves as the gRPC client and one serves as the gRPC server,
The expectation is that the trace between the gRPC client and the gRPC server must be reflected in Jaeger.
But it's not happening.
I am inside a Kubernetes cluster that has Istio.
What really happens is HTTP request -  Envoy sidecar -  gRPC Client's Spring Boot @RestController -  get the Headers from HTTP request -  copy those to gRPC call before making the call -  gRPC Service.
How can I make the gRPC client &lt;-  gRPC Service trace shown to Jaeger?
Are there any dependencies that needs to be imported?
Right now I have:
I also have done something like this to "propagate the headers":
But it doesn't seem to work..
I've added opentracing to my web app and am using the Jaeger all-in-one docker image as the collector.
I'm running docker on windows 10 (hyper-v) and am using the Jaeger java client.
When I test the web app locally on the host machine it sends traces successfully to the Jaeger collector docker instance.
However, when I run the web app in another docker container no traces are registered in the Jaeger UI.
I've tried both containers on the same docker network with no success.
The web app in the docker container can access other services in dockers contains such as a DB and an ETCD server with no issues.
I thought I might have the wrong ports but given the fact it works from the host environment I'm assuming these are correct and that it is a docker configuration issue.
I've also set  JAEGER_SAMPLER_TYPE environment variable to const and the JAEGER_SAMPLER_PARAM to 1 to ensure all traces are logged.
Edit  - when I look at the metrics it seems like the jaeger is receiving the spans.
Every call I make to the app increments this count by one.
I also tried the sample project  Hotrod  as suggested by Yuri Shkuro on a similar issue someone had.
Exact same result as above.
Metrics shows spans being received but nothing is displayed in UI.
Edit 2  - I've narrowed it down to happening in a hyper-v windows 10 VM.
Any help would be appreciated.
Thanks
192.168.0.15 is host machine
Setting up Jaeger tracer in Java
Unable to trace a services for springboot application on Jaeger UI(localhost:16686/search).
Here I am able to run an application successfully, but unable to get a services in jaeger ui(Except defalut one jaeger-query).
I used docker cmd to start the jaeger service,
Open the Jaeger UI on  http://localhost:16686/search
}
}
Image 1 
 Image 2
Github repository with demo:  https://github.com/pavolloffay/opentracing-java-examples .
Please help me to solve this.
Following Spring Cloud Sleuth's documentation I've set up a Spring Boot application with a Zipkin client:
Gradle config:
With this I start a Zipkin Server instance:
And I get traces in Zipkin.
So far so good.
Then I switch to a Jaeger server:
And without any change to my application I can see those traces in Jaeger.
Great.
Sleuth docs states:
15.1.
OpenTracing
Spring Cloud Sleuth is compatible with OpenTracing.
If you have
  OpenTracing on the classpath, we automatically register the
  OpenTracing Tracer bean.
If you wish to disable this, set
  spring.sleuth.opentracing.enabled to false
Following this I added the Open Tracing dependency:
During startup the  OpentracingAutoConfiguration  creates a  BraveTracer .
The point is I've placed a breakpoint in methods such as  scopeManager() ,  activeSpan() ,  activateSpan() ,  buildSpan()  from that  BraveTracer  and none of them is invoked during the execution of the application; the traces keep showing up in Jaeger though.
What am I missing here?
I am trying to implement Jaeger in the node js project.
I have deployed this node js project(using docker image) and Jaegaer in k8s (kubectl create -f  https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/all-in-one/jaeger-all-in-one-template.yml )
Both are working individually but traces are not visible in the service
I am trying to connect to jaeger collector which uses Kafka as intermediate buffer.
Here are my doubts could any one please point to some docs .
QUESTION
  1.
How to connect to collector by skipping agent and use kafka as intermediate buffer.Please provide me command or configuration
  2.
Whats the configuration for kafka to connect to particular host.When I tried to use below command its still pointing to localhost and failing
     docker run  -e SPAN_STORAGE_TYPE=kafka jaegertracing/jaeger-collector:1.17
I have Springboot Webflux main application that talks to other services using RSocket and I want to configure the stack so I want to see the traces looking like this:
As today I need to check each app to check the flow
I'm using implementation  'io.opentracing.contrib:opentracing-spring-jaeger-cloud-starter'  dependency
How does one integrate opentracing (jaeger) with RSocket?
I saw that for WebClient, you have autoconfiguration but found little or no instructions on how to do it besides  Tracking-Zipkin  page which I don't really know how to procced from that.
so I'm playing aroung with Jaeger and OpenTracing to trace the requests between my Spring Boot microservices.
I have setup all necessary configurations and added the dependency:
So far all works fine.
I see all the traces and spans in my Jaeger UI, no problem.
But now I have the challenge to add new spans to a specific trace, that is already finished.
Think of it like this.
A client calls one of the services and the tracing starts.
After the work is done, I see the trace in my Jaeger UI.
But now the invoking clients wants to add some additional tracing data to the specific trace.
Like tracing information from other service, that are not within the scope of my microservices.
I've added a filter so I can extract the trace id and send it to the client.
Now the client does a request containing the additional trace information and the trace id.
These informationa should then be added as an additional span in the already finished trace.
Now to my question.
Is there a way to create a span and add it to a trace with only having the trace id as a String?
I've tried Zipkin and I could just do:
That span could then be added by doing a POST request to my zipkin server on port 9411 which did the magic of adding this span to the trace with the given id.
Using OpenTracing I can do:
Unfortunately this approach needs the trace in form of a span to create the new span as a child of that trace.
Given the fact that I can only provide the trace id, I don't know how to get the needed span of that trace.
Do I really need to make a call to my the Jaeger query to get the trace span needed or is there another approach I haven't been thinking of?
Would really like to get some help on this.
Cheers!
I have a working Ambassador and a working Istio and I use the default Jaeger tracer in Istio which works fine.
Now I would like to make Ambassador report trace data to Istio's Jaeger.
Ambassador documentation suggests that Jaeger is supported with the Zipkin driver, but gives example only for usage with Zipkin.
https://www.getambassador.io/user-guide/with-istio/#tracing-integration
So I checked the ports of jaeger-collector service, and picked the http: jaeger-collector-http  14268/TCP
And modified the TracingService shown in the Ambassador docs:
But I cannot see trace data from Ambassador in Jaeger.
Does anyone have any experience on this topic?
I am trying to deploy the Jaeger Helm chart with Azure Cosmos DB acting as the Cassandra Storage Backend.
I have set up the CosmosDB and created a values file as below:
The command that I am using to deploy jaeger with the values file is:
However, on checking the pods, the collector and the query are in CrashLoopBackOff
Both the containers on running the describe command complain:
I am not sure what am I missing here?
I'm currently playing around with  Jaeger Query  and trying to access its content through the  API , which uses gRPC.
I'm not familiar with gRPC, but my understanding is that I need to use the Python gRPC compiler (grpcio_tools.protoc) on the relevant proto file to get useful Python definitions.
What I'm trying to do is find out ways to access Jaeger Query by API, without the frontend UI.
Currently, I'm very stuck on compiling the proto files.
Every time I try, I get dependency issues ( Import "fileNameHere" was not found or has errors.
).
The Jaeger  query.proto  file contains import references to files outside the repo.
Whilst I can find these and manually collect them, they also have dependencies.
I get the impression that following through and collecting each of these one by one is not how this was intended to be done.
Am I doing something wrong here?
The direct documentation through Jaeger is limited for this.
The below is my basic terminal session, before including any manually found files (which themselves have dependencies I would have to go and find the files for).
Thanks for any help.
I actually was trying to sample only the error traces in my application but i already have a probabilistic sampler parameter set in my application which samples the span at the beginning itself and the rest span follow the same pattern after then, i tried using force sampling option in jaeger but it doesnt seem to override the original decision made by the initial span of getting sampled or not.
Kindly help me out here.
I’ve started with instrumenting my gRPC service using go-gRPC-middleware.
I’ve got logs working using zap and metrics exposed for Prometheus.
Now that I’m trying to configure tracing using jaeger go client it requires me to add wrapper around metrics storage and logger.
I’m not sure I understand why those wrappers are required
https://github.com/jaegertracing/jaeger/blob/bf64373d1e690594fd8c279720faf32722cf1494/examples/hotrod/pkg/tracing/init.go#L46
In our application a Node.js front end talks to a Java Spring backend.
Everything is containerized and running in Kubernetes.
Some time ago we added support for Jaeger distribtued tracing across the front end and back end services.
Jaeger has been running fine until recently.
Our Elasticsearch cluster was out of date so we upgraded.
That mandated an upgrade of Jaeger--we ended up with the following bits:
Both of the opentracing libraries have a dependency on the version 0.35.1 of the Jaeger Java client.
Since upgrading, traces that are created on one side or the other seem to be fine.
But traces that span the boundary (i.e.
start on the Node.js front end and complete on the Java backend) generate errors in the jaeger-agent pod like this:
For these traces, the Jaeger UI shows us the spans that were created by the front end before invoking the backend API, but the child backend spans do not show up as you would expect.
What might cause this sort of processor error?
Language:  Java
Framework:  Spring boot
Tools:  Jaeger
I have done the following configuration for put whole trace on logs.
But at controller level log not shown a trace.
when hibernate query executed than after trace is put on logs(on service and repository level logs)
application.log
Reporter class
Appender class
Main spring boot class
Application.properties
pom.xml
Is there any way to put the std logs provided by the application and the errors to a span?
I know that I can send some logs with  span.LogKV()  or  span.LogFields()  but it makes code look bad while there are same logs with both application logger and span logger.
I'm looking for an automated way to put all logs to the corresponding span.
I am using a spring Cloud openFeign for making request from service#1 to service#2
When I use restTemplate I can correctly see 2 requests in jaeger tracing.
But when using openFeign I see only 1 request.
Is there any way of integrating jaeger and openFeign?
I found this:
 https://www.baeldung.com/spring-cloud-openfeign 
 https://github.com/OpenFeign/feign-opentracing
I've a simple Java application that I wanted to test tracing with Jaeger but encountered error.
maven dependency -
jaeger all-in-one -
Here is the code -
and I'm getting error -
Appreciate any help !
What are the advatages of the jaeger tracing with istio and without istio?
For example with istio it will reduce the latency for collecting the more traces
I am trying to get mongo logs in jaeger.
Basically I want my jaeger to show my mongo application errors.
What is the best method to do this?
I have tried using the maven repo- opentracing-mongo-driver (version 0.1.4)
and in my code I have configured it using -
But I am getting this error-
What is it that I am doing wrong?
How do I enable  Jaeger jdbc  tracing in  Quarkus ?
I've followed the  Quarkus  guides for  Opentracing  and didn't see any info about this.
I'm using  Quarkus  v0.21.2 with the following extensions:
And my code is just a basic Rest endpoint which calls my entity's Panache CRUD operation.
Any help is appreciated.
I've tried the following and it didn't work:
What I expect in  Jaeger  is, 2 spans for 1 trace, one for the REST call and another one for the  JDBC  call.
But what I see is just 1 span for the REST call.
I have developed a camel route with  Spring boot .
Now I want to trace the route using  jaeger .
I tried  this example  to trace the route using  camel-opentracing  component, but I am unable to get the traces to  jaeger .
I can only see it in the console.
One thing i am not clear is where to add the  jaeger  url?
Any working example will be helpful.
I'm currently trying to trace two Spring Boot (2.1.1) applications with Jaeger using  https://github.com/opentracing-contrib/java-spring-web
also tryed with no success
The tracing of the Spans for every single service / app works fine, but not over REST requests on a global level.
There is no dependency shown between the services like you can see in the image.
Shouldn't this work out of the box through the library?
Or do I have to implement some interceptors and request filters by my own and if so, how?
You can CHECKOUT a minimalistic project containing the problem   here
Btw: Jaeger runs as all-in-one via docker and works as expected
According to  https://quarkus.io/guides/opentracing-guide  all Jeager configuration is via JVM args (-DJAEGER_ENDPOINT...) but I'd like to use either  application.properties  or  microprofile-config.properties  to configure tracing.
I've tried the following but the only config that seems to be picked up by Quarkus is the service-name all other properties are ignored.
So, question is if it is possible to configure via config-files or this is not currently supported?
I have a kubernetes cluster on google cloud platform, and on it, I have a jaeger deployment via development setup of  jaeger-kubernetes templates  
because my purpose is setup  elasticsearch  like backend storage, due to this, I follow the jaeger-kubernetes github documentation with the following actions
Here are configured the URLs to access to  elasticsearch  server and username and password and ports
And here, there are configured the download of docker images of the elasticsearch service and their volume mounts.
And then, at this moment we have a elasticsearch service running over 9200 and 9300 ports
According to the  Jaeger architecture , the  jaeger-collector  and  jaeger-query  services require access to backend storage.
And so, these are my services running on my kubernetes cluster:
I execute it:
And I get the following edit entry:
Here ... do I need setup our own URLs to collector and query services, which will be connect wiht elasticsearch backend service?
How to can I setup the elasticsearch IP address or URLs here?
In the jaeger components, the query and collector need access to storage, but I don't know what is the elastic endpoint ...
Is this  server-urls: http://elasticsearch:9200  a correct endpoint?
I am starting in the kubernetes and DevOps world, and I appreciate if someone can help me in the concepts and point me in the right address in order to setup jaeger and elasticsearch as a backend storage.
I am having problems pointing a jaeger agent to a collector running in openshift.
I am able to browse my OCP collector endpoint doing this:
My jaeger agent Dockerfile currently looks like this
I get the expected result when i point my agent to a collector running locally per the first commented line.
I get the following error using the second uncommented CMD flag.
When i attempt the agent to the collector running on openshift, i get the error below
I am able to successfully curl the collector endpoint by doing this
I get the following error when i attempt to curl the endpoint this way:
I need help setting up a proper  --collector.host-port  flag that will connect to a collector running remotely behind an HTTPS protocol.
I'm struggling with setting up OpenTracing/Jaeger for a Spring Boot 2.0.2 application.
Starting from a working but very sample for Spring Boot 1.5.3 I moved on to Spring Boot 2.0.2 -- which properly sent the traces.
But the dependencies used there were ridiculously old (like 0.0.4 for opentracing-spring-web-autoconfigure, which is now available in 0.3.2).
So I migrated the application to the latest dependencies which resulted in no traces appearing anymore in Jaeger.
I've upload my tests to  https://gitlab.com/ceedee_/opentracing-spring-boot .
The branches are as follows:
Differences from 2. to 3. are as follows:
Does anyone have a clue what I'm doing wrong in order to properly put traces into Jaeger?
Hints on debugging OpenTracing/Jaeger are appreciated as well!
Best regards,
cd_
I have started Jaeger standalone binary on a Linux box and am trying to run Jaeger agent binary on Mac that tries to connect to the Jaeger collector of the standalone process.
However it keeps failing with "error":"tchannel error ErrCodeTimeout: timeout".
The problem is not with different OS versions as I get the same error when trying from another Linux box.
I used telnet to confirm that the collector port was open for connection.
The stack trace is below-
./cmd/agent/agent- --collector.host-port=172.xx.2.4:14267
{"level":"info","ts":1542954225.5485492,"caller":"tchannel/builder.go:94","msg":"Enabling service discovery","service":"jaeger-collector"}
{"level":"info","ts":1542954225.5489438,"caller":"peerlistmgr/peer_list_mgr.go:111","msg":"Registering active peer","peer":"172.xx.2.4:14267"}
{"level":"info","ts":1542954225.5502574,"caller":"agent/main.go:62","msg":"Starting agent"}
{"level":"info","ts":1542954226.5518098,"caller":"peerlistmgr/peer_list_mgr.go:157","msg":"Not enough connected peers","connected":0,"required":1}
{"level":"info","ts":1542954226.552439,"caller":"peerlistmgr/peer_list_mgr.go:166","msg":"Trying to connect to peer","host:port":"172.xx.2.4:14267"}
{"level":"error","ts":1542954226.8054206,"caller":"peerlistmgr/peer_list_mgr.go:171","msg":"Unable to connect","host:port":"172.xx.2.4:14267","connCheckTimeout":0.25,"error":"tchannel error ErrCodeTimeout: timeout","stacktrace":"github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr.(*PeerListManager).ensureConnections\n\t/Users/swarnim/go/src/github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr/peer_list_mgr.go:171\ngithub.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr.
(*PeerListManager).maintainConnections\n\t/Users/swarnim/go/src/github.com/jaegertracing/jaeger/pkg/discovery/peerlistmgr/peer_list_mgr.go:101"}
I'm trying setting up a spring application which use Jaeger/Prometheus.
I already configured Prometheus successfully by prometheus.yaml file, but I haven't understood how configure Jaeger target endpoint.
Must I create a new yaml file and specify into it the configuration?
If yes, with which syntax?
I am using  Zuul  as an api-gateway in a spring-cloud micro-service app, so that every access  to  api-gateway/some-service/a_route  is redirected to  /a_route  in a generic way (the discovery is backed by consul).
I am trying to use  Jaeger  to instrument this system.
And at this point I am using   opentracing-spring-web-autoconfigure , because I cannot upgrade my spring boot/cloud version easily (I am using1.4.5.RELEASE Camden.SR7).
So I just added the dependency, created the Jaeger tracer and redirect it to the docker all in one collector.
I have begin by instrumenting the gateway and It somewhat works =  It generate span on the gateway, but all the route are marked :
apigateway-service: GET
and there is no information concerning the forwarded route at this level, the full route itself is store in a tag : http.url 
" http://localhost:8080/collection-service/collections/projects/ "
To be useful I would prefer to have span named :
apigateway-service: GET collection-service/collections/projects/
Can this be configured somewhere ?
Our application consist of angular 6 for the UI and .netcore 2.0 for the back end, looking to implement tracing to it and so far opentracing seems the most prominent but I can't seem to find any good help documentations for .netcore 2.0 apps.
My java code:
My gradle dependencies:
This code works in localhost.
I have already passed the  JAEGER_AGENT_HOST  and  JAEGER_AGENT_PORT  env to the container.
And I can see the Jaeger Initialized log in remote:
Using the UDP Sender to send spans to the agent.
Using sender UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@27e16046, receiveBuf=null, receiveOffSet=-1, receiveLength=0))
  Using sender UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@27e16046, receiveBuf=null, receiveOffSet=-1, receiveLength=0))
  2018-08-16 13:24:32.809  INFO 1 --- [http-nio-8080-exec-1] io.jaegertracing.Configuration           : Initialized tracer=JaegerTracer(version=Java-0.30.4, serviceName=
But I can see it in Jaeger UI.
And I tried to use  tcpdump , I cannot find the udp package.
I am using Jaeger UI to display traces from my application.
It's work fine for me if both application an Jaeger are running on same server.
But I need to run my Jaeger collector on a different server.
I tried out with JAEGER_ENDPOINT, JAEGER_AGENT_HOST and JAEGER_AGENT_PORT, but it failed.
I don't know, whether my values setting for these variables is wrong or not.
Whether it required any configuration settings inside application code?
Can you provide me any documentation for this problem?
I am trying to setup Jaeger tracing for my micro service that is written in Node.js using Express.js.
I have added a simple get request handler in my express app and when I hit the endpoint via curl, I can see that a span is generated in logs, but I do not see the name of my service in Jaeger UI.
// server.js
// tracing.js
I see in logs:

2018-03-08T01:03:34.519134479Z INFO  Reporting span 9b88812951bcd52f:9b88812951bcd52f:0:1
I am new to jaeger and I am facing issues with finding the services list in the jaeger UI.
Below are the .yaml configurations I prepared to run jaeger with my spring boot app on Kubernetes using minikube locally.
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/production-elasticsearch/elasticsearch.yml --namespace=kube-system
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/jaeger-production-template.yml --namespace=kube-system
Created deployment for my spring boot app and jaeger agent to run on the same container
And the spring boot app service yaml
I am getting
No service dependencies found
Cannot find any information if Jaeger can be executed without docker?
Does a standalone jar exist, or will there be a release in the future for Jaeger like Zipkin has ?
Planning to use Jaeger for distributed tracing of our Application.
Need to use elasticsearch as db backend, rather than cassandra for Jaeger.
I have a very general question about jaeger (opentracing).
I set up Jaeger, and can find the spans and traces - where each originiated from, where it ends up, and so on.
However, I am curious how to use Jaeger 'well'.
I think Jaeger itself doesn't give much information, except for the fact I can check which server or api is the bottleneck.
The scenario I have in mind, is to get an alert for an error or warning from the logging system (probably it will be ElasticSearch), and get the trace id from it, and check the whole trace from Jaeger.
Any suggestions on how to use Jaeger 'well'?
i have been having some problem.
i have a k8s cluster up and running and wanted it to connect to jaeger.
followed this page(ReadMe.md)  https://github.com/jaegertracing/jaeger-operator?utm_source=thenewstack&amp;utm_medium=website&amp;utm_campaign=platform
i have the jaeger ui now but it not able to recognize the services.
can someone give any suggestions??
In the above link i didnt understand what this line means, &quot; You probably also want to download and customize the operator.yaml, setting the env var WATCH_NAMESPACE to have an empty value, so that it can watch for instances across all namespaces.
&quot;
kubectl logs -n observability deployment/jaeger-operator
time=&quot;2021-01-07T06:48:47Z&quot; level=info msg=Versions arch=amd64 identity=observability.jaeger-operator jaeger=1.21.0 jaeger-operator=v1.21.2 operator-sdk=v0.18.2 os=linux version=go1.14.12
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Consider running the operator in a cluster-wide scope for extra features&quot;
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Auto-detected the platform&quot; platform=kubernetes
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Auto-detected ingress api&quot; ingress-api=networking
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Automatically adjusted the 'es-provision' flag&quot; es-provision=no
time=&quot;2021-01-07T06:48:48Z&quot; level=info msg=&quot;Automatically adjusted the 'kafka-provision' flag&quot; kafka-provision=no
time=&quot;2021-01-07T06:48:50Z&quot; level=info msg=&quot;Install prometheus-operator in your cluster to create ServiceMonitor objects&quot; error=&quot;no ServiceMonitor registered with the API&quot;
I am using below example of OpenTracing, Jaeger
 https://github.com/yurishkuro/opentracing-tutorial/tree/master/csharp/src/lesson01
I have my Jaeger UI running for which I used the below command.
Below is my code which I tried from above github link
Above is the Class Library project.
Below is my console app where I am calling the TriggerTrace Method.
When that function is executed I would expect some traces in the Jaeger UI,
Below is the error I see, I do not see any traces in the UI,
Am I missing any configuration ?
I have a simple Spring Boot 2.x RestController with an endpoint performing certain remote calls as well as controller is also calling an Async method that in turn makes several remote HTTP calls.
I'm having opentracing-spring-jaeger-web-starter in classpath with tracing enabled.
If i invoke my REST endpoint, It creates a span for the endpoint call as well as remote calls that the controller is making synchronously.
However the remote calls made by Async method is getting reported in its own span.
Is this by design or is there a way to propagate some context information to the Async method to better group/relate the spans ?
I recently deployed the jaeger agent as a daemonset on my k8s cluster alongside a collector.
When trying to send spans to the agent using:
When looking at the application logs I see:
All nodes can access each other as the security group does not block ports between them, when using a sidecar agent the spans are sent without issue.
Replicate:
Deploy agent using:
Then deploy hotrod application:
I'm following this tutorial:  https://github.com/yurishkuro/opentracing-tutorial/tree/master/java/src/main/java/lesson03 .
What need to be set so that services running in different hosts can send the data to the same backend?
I am playing with quarkus and jaeger by opentracing integration.
After run the jaeger server and the  https://github.com/quarkusio/quarkus-quickstarts/tree/master/opentracing-quickstart  repo I found the traces at http://localhost:16686/search.
But I only found the Resource class, arguments, and Process name , but the &quot;Logs&quot; is not shown on trace detail expand.
The steps are easy:
1.Run jaeger server  docker run --rm=true --name erp_jaeger_server -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 -p 5775:5775/udp -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14268:14268 -p 9411:9411 jaegertracing/all-in-one:latest
clone the example repo and run it
  https://github.com/quarkusio/quarkus-quickstarts/tree/master/opentracing-quickstart 
(no further configuration)
run-&gt;  mvn quarkus:dev
visit  http://localhost:8080/hello/
5.Explore on jaeger ui 'http://localhost:16686/'
6.Found the traces Tags, and Process Details but detailes content Log.info('hello') is not shown
I was trying with @Slfj but i got the same result
Thanks in advance.
I have two endpoints, the first one uses Feign implementation:
Controller:
Feign Client:
the second, do the same thing, but uses RetroFit implementation:
Controller:
Retrofit Client:
Bean:
With Feign implementation, Jaeger UI shows me two steps: Controller Request and the Http Request [GET] to  http://viacep.com.br .
With Retrofit, Jaeger does not show the request to  http://viacep.com.br .
Only the controller step.
Why?
Is there some configuration that I can do at okHttpClient builder?
I can`t use feign in this project.
I am trying to instrument my program with jaeger-tracing (c++).
I was able to view my traces when I compliled the program with yaml-cpp version 0.5.3, but when I changed my yaml-cpp version to 0.6.x, I am unable to view my traces.
Dont know why its happening.
JaegerProgram Source code;
compiling command -  g++ -std=c++1z test.cpp -L /usr/local/lib/libyaml-cpp.a -ljaegertracing -lyaml-cpp
Yaml file
OS : ubuntu 18.04
jaegerTracing : master branch version
UPDATE
after little digging I found some fact, When I parse the above mention config file try to print the result I get the same value as written in config file, but when I parse same file using  yaml-cpp-0.6.x  the sampler.type is showing 'remote' and sampler.param to be '0.001' and when I manually change this change these values to be same as in  config.yaml  it has started showing traces.
The error is present in parsing the yaml file as I could clearly see different values is loaded as configuration.
Following  this  guide I I don't really see how to implement the operator with elasticsearch.
Ok, so I install the operator and after that follwing the  example which is with :
which is not supported by openshift as an api.
I just need to deploy jeager operator for with 1 elasticsearch, but this guide is quite confusing.
Does anyone know a quick and easy guide on how to do it?
I've found an example:  https://medium.com/velotio-perspectives/a-comprehensive-tutorial-to-implementing-opentracing-with-jaeger-a01752e1a8ce
I have a pretty large codebase and I really don't want to modify every function by adding a line like ' with tracer.start_span('booking') as span:'.
Is there any way to do it?
Thanks in advance.
I use Jaeger with Elasticsearch and I want to remove old indices.
I tried  jaeger-es-index-cleaner , see  Remove old data :
Remove old data
The historical data can be removed with the  jaeger-es-index-cleaner  that is also used for daily indices.
&lt;1&gt; Remove indices older than 14 days.
Log
I tried to delete all indices older than 2 days, but no indice was deleted:
Indices
If I list all indices with  http://localhost:9200/_cat/indices , I still see old indices:
Question
How to delete old indices of Jaeger from Elasticsearch?
I have configuration as  documentation  says
Collector produces error.
How I can configure collector to balance exporter for sending requests in different backends?
info    exporterhelper/queued_retry.go:276      Exporting failed.
Will retry the request after interval.
{&quot;component_kind&quot;: &quot;exporter&quot;, &quot;component_type&quot;: &quot;jaeger&quot;, &quot;component_name&quot;: &quot;jaeger&quot;, &quot;error&quot;: &quot;failed to push trace data via Jaeger exporter: rpc error: code = Unavailable desc = last connection error: connection error: desc = &quot;transport: Error while dialing dial tcp: address ipv4:firstHost:14250,secondHost:14250: too many colons in address&quot;&quot;, &quot;interval&quot;: &quot;30.456378855s&quot;}
I have been reading the documentation from Spring Cloud Sleuth and Zipkin, and I did not locate anything about how to show in Jaeger the logs came from Zipkin.
This is an example using the jaeger stack:
And this is an example using the same stack, but exporting to Jaeger using the Zipkin collector (port 9411 on Jaeger)
There is no  Logs  table.
Does anyone knows if would be possible show those logs there, like Jaeger implementation does?
I have a spring boot application with several microservices.
There are about 100+ different events.
And I wanted to see in convenient UI to see sequence of them.
I googled about Jaeger UI, ran it via docker container and everything works almost fine, except one important thing, events are not grouped, I just see multiple independent events.
I give 2 examples,
First: how I want it to see
Second: how I see.
Thanks for any suggestions.
networks:
axonnet:
driver: bridge
I am new to Jaeger and Kafka,
I am trying to use Kafka as intermediate buffer.
I am using OpenTelemetry to send data to Jaeger-Collector directly using  -Dotel.exporter.jaeger.endpoint .
Jaeger-Collector is deployed on Kubernetes and the Kafka is on another network but is accessible.
I can confirm that the traces are sent to Jaeger-collector.
On hitting the /metrics of collector and output tells me that spans were written successfully to Kafka.
jaeger_kafka_spans_written_total{status=&quot;success&quot;} 21
The Collector logs indicate what topic I am writing to
{&quot;Brokers&quot;:[&quot;myKafkaBroker......&quot;}},&quot;topic&quot;:&quot;tp6&quot;}
I want to get this (Span) data from Kafka Queue to ElasticSearch.
To do this I am starting the Jaeger Ingester as follows
docker run  -e &quot;SPAN_STORAGE_TYPE=elasticsearch&quot;  jaegertracing/jaeger-ingester:1.22 --kafka.consumer.topic=tp6 --kafka.consumer.brokers='myKafkaBroker'   --es.tls.skip-host-verify
But the container is stopped after fatal error
The elasticsearch and ingester are being run on the same machine using docker.
The elasticsearch is running on docker using
docker run -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot;ocker.elastic.co/elasticsearch/elasticsearch:7.11.2
I have disabled TLS so that shouldn't be a problem.
I am unable to get this to work.
　　I am trying to learn kubernetes recently.
I have already deployed jaeger (all-in-one) by istio on kubernetes, and everying works well.
Although I can see the trace information on the jaeger UI, I don't know how to extract these trace data by python.
I want to use these data to do root causes location of microservices.
I think there must be some API to access these data dicectly by python but I don't find it.
or can I access the cassandra using python to get these data?.
I am searching for a long time on net.
But no use.
Please help or try to give some ideas how to achieve this.
Getting this error when i startup jaeger allinone docker latest.
Not sure why this is - can anyone help here?
I am running this on Windows, Docker for desktop.
This is behind a corp proxy, if that's helpful.
This is the command i am using to startup
I'm new to Clickhouse.
I'm trying to read Jaeger logs from Kafka into Clickhouse db.
I have following Kafka messages format:
I was able to input traceID, spanID and Operation into Clickhouse using the following table:
But I failed to input tags.
Any idea which Clickhouse data type I should use for it?
I have a tornado application in which I use jaeger for tracing.
My problem is that jaeger keeps logging at the INFO level, tons of entries like this :
I've tried a bunch of configuration to try to remove these entries, but so far without luck.
My current jaeger logger configurations is:
How can I turn off, INFO logging for jaeger ?
Using jaeger to instrutment our HTTP API (nestJS application).
I would like to put an alert if span duration exceeds a threshold.
We are using elasticsearch as backend, so we could setup elasticsearch watcher, but I am wondering if jaeger eco-system bring a better solution?
I was trying to use Jaeger to trace some DAG execution (with some long tasks execution time) and I have some pretty good results in term of visualization &quot;post-execution&quot;.
I know this is not a common usage of Jaeger (by extension OpenTracing) but this is doing  almost  what I was looking for.
I am saying  almost  because I thought that a span would be displayed in the UI timeline as soon as it was &quot;started&quot; in the code.
But, as far as I understand, to be displayed in a trace, spans need to be complete and Jaeger is not yet able to store incomplete spans (I have seen some open PRs in GitHub).
My need is that I would like to do some real-time monitoring to know where are my bottlenecks on the DAG that I need to execute, and viewing those tasks execution in a single timeline as soon as they are started would have been awesome.
Do you know if such a behavior is possible with Jaeger or Zipkin to do some real-time traces rendering ?
Or if there is an open-source tool capable of doing that ?
Cheers !
I have a question regarding the Jaeger Operator which I hope someone could help me with.
I am deploying the Jaeger operator on k3s by simply adding the below template to my helm chart (templates/jaeger.yaml):
I have imported the Jaeger-Operator dependency in my Chart.yaml as below:
The operator deploys it's own ingress controller.
What changes do I need to make to my templates so that I can disable the deployment of ingress from Jaeger Operator and have it done through the ingress.yaml that I will define ?
B3 headers can be propagated using  zipkin.NewZipkinB3HTTPHeaderPropagator()
as explained here
Can uber-trace-id also be propagated along with this ?
uber-trace-id is the default format in jaeger but I need both uber-trace-id as well as Zipkin B3 headers
Specifically can we add more injectors and extractors like this
I'm trying to use Jaeger to manage tracing system.
Docker is running locally &quot;all-in-one&quot; image with application (on the same host) without any issues.
My question is how to configure jaeger agent on host1 that would send traces jaeger collector on another host2.
Host2 is configured with &quot;all-in-one&quot;.
I can see Jaeger UI on host2 but it doesn't seem getting any traces from host1.
Configure tracer:
Added environment variables in yaml file on host1:
Added jaeger image in yaml file on host2:
Any suggestions will be appreciated.
I'm new to the world of Opentelemetry and would like to send the  Spring-petclinic  instrumentation data to Jaeger which is running on my remote cloud system
Here is the bat file:
 java -javaagent:opentelemetry-javaagent-all.jar -Dotel.exporter=jaeger -Dotel.exporter.jaeger.endpoint=50.18.XXX.XX:14250 -Dotel.otlp.span.timeout=4000 -Dotel.jaeger.service.name=otel-ui -jar target/spring-petclinic-2.4.0.BUILD-SNAPSHOT.jar
When I run the bat file, I'm abe to open the petclinic app in browser (http://localhost:8080), I get the following error in the console:
 [opentelemetry.auto.trace 2021-01-06 17:22:21:008 +0530] [grpc-default-executor-1] WARN io.opentelemetry.exporter.otlp.OtlpGrpcSpanExporter - Failed to export spans.
Error message: UNAVAILABLE: io exception
How to resolve this issue?
Are there any other dependencies to be the added to the petclinic pom.xml or to the code?
Is there any way to load jaeger embeded ui with https instead of http.
from this : http://jaegerip/search?&amp;uiEmbed=v0
to this : https://jaegerip/search?&amp;uiEmbed=v0
I am trying to incorporate Jaeger and OpenTracing into my Class Library project in .Net core.
Most of the Jaeger documentation shows how to configure Jaeger for Web API's in Startup.cs file.
The below configuration works for .Net core API project
but I would like to configure the same for my Class LIbrary project.
Is it possible ?
I mean for example someone else uses my class library in their App I should be able to see the tracing.
The Jaeger configuration should be made in Class LIbarary.
I am getting the following exception while creating a span.
It works fine for Logging Exporter but for jaeger gives the following
At App startup I do
And at runtime
I'm running my Camel Quarkus service on Openshift where Jaeger is also installed.
The Jaeger agent is running as a daemon set.
I'm not getting any traces from my Camel service in the Jaeger UI using following properties:
I also have some Spring Boot services running in the same namespace and they work as they should.
Therefore, I think the Camel service is configured incorrectly.
Please help.
What am I missing?
Jaeger traces to spring-boot application are not able to capture traces for the DB calls made using spring-data.
All other calls like RESTTemplate are able to have the traces captured.
Using springboot version 2.2.2.RELEASE and added below jaeger dependencies,
Any additional dependencies are missing here?
I am deploying Jaeger using the  Jaeger Operator  and it seems to be working fine.
However, now I am trying to set up Prometheus metrics scraping (using the  Prometheus Operator ) but I am not seeing a  Service  in my cluster that exposes the metrics ports for the Jaeger Collector (port 14269) or Query services (port 16687) ( port number reference from the Jeager Monitoring documentation ).
The only relevant  Service  I see is  jaeger-operator-metrics :
I am able to set up a Prometheus  ServiceMonitor  to scrape metrics from this service but I am not sure if this includes the metrics that are normally gathered by the Collector and Query microservices or not...
I am guessing not as that would seem to violate the premise of microservices.
Is there some setting in the Jaeger Operator spec that I missed for exposing those metrics endpoints in the other components?
I have the problem that I cannot seem to get Grafana Tempo working with a Jaeger client.
Following their official docker-compose example everything should be straight forward:  https://github.com/grafana/tempo/tree/master/example/docker-compose
I've basically just adapted the official Jaeger python client example.
Since Grafana Tempo is running on the same machine the reporting_host is set to localhost and since the synthetic-load-generator uses port 14268 in the JAEGER_COLLECTOR_URL I'm using this as well.
I can query for traces generated by the synthetic-load-generator without a problem but I cannot seem to get it working with my script.
If I use e.g.
the all-in-one Jaeger container I can query for my traces.
As suggested here  https://grafana.com/docs/tempo/latest/getting-started/  I've also tried to use the &quot;Jaeger - Thrift Compact&quot; Protocol on port 6831 but it doesn't seem to work either.
Can somebody point me into the right direction what I might be doing wrong?
Thanks in advance!
!
Appreciate your support for the below issue as I built my demo as the below steps,
I built two microservices one by Django and the another by Go,
Django send HTTP request to Go service,
Jaeger tool is configured for UI tracing,
Django tracing and Go tracing are separated in Jaeger tool and I do not know the reason although I received Django parent trace id in the request header and it is normal to be all Django request tracing including Go tracing as one request tracing in Jaeger,
My Git repo:  https://github.com/OmarEltamasehy/django-gotracing-example
The below is Django code for calling Golang service
Golang service code
Jaeger UI output
this image refer to my inquiry why my request show Django tracing only without Go tracing for the same request
As the doc  https://www.jaegertracing.io/docs/1.19/troubleshooting/  says &quot;The logging reporter follows the sampling decision made by the sampler, meaning that if the span is logged, it should also reach the agent or collector.&quot;
But thats not happening, my gRPC server is logging the span -  &quot;{&quot;level&quot;:&quot;info&quot;,&quot;msg&quot;:&quot;Reporting span 1559209397c51d88:3217766e13b74f76:1559209397c51d88:1&quot;,&quot;time&quot;:&quot;2020-11-11T17:31:47Z&quot;}&quot;  , but I am not able to see the same span on jaeger-all-in-one UI.
The REST client span is projected onto the UI and is the parent of the trace.
Please help me to be able to project the grpc spans onto UI.
Referring to the link ( https://github.com/burkaa01/jaeger-tracing-kafka/tree/master/stream-app ), created stream pipeline with Jaeger enabled.
It is a springboot application but bean configurations are defined in spring xml file.
As part of stream topology, in transformation, while getting processorContext.headers(), i am getting error.
Stream pipeline works if jaeger is disabled.
Also it works if the beans are defined in annotations .
Referred JIRA,  https://issues.apache.org/jira/browse/KAFKA-4344 
 
Clueless on the issue, here is the error stack for reference
I have used the following configuration to setup the Istio
I want to access the services like grafana, prometheus, jaeger, kiali &amp; envoy externally - eg:  https://grafana.mycompany.com , how can I do it?
Update: 
I have tried below however it doesn't work
I'm trying to use the  Jaeger  package to send traces to Jaeger from a C# app.
There are no minimal examples in the jaeger-client-csharp documentation, but from what I read, I think this should work.
I have jaeger-all-in-one.exe running but when I run this code there's no sign of any new traces.
I've tried manually configuring samplers, senders, reporters, etc.
but nothing I tried worked.
What do I need to add to get my traces to appear in Jaeger?
I have installed Gitlab in AKS cluster, and I also installed Jaeger.
Both the applications are up and running.
I want to integrate GitLab with jaeger.
I searched for any documentation on how to do in AKS , but didnt find any.
Any suggestion is welcome.
I have question regarding  global.tracer.zipkin.address  while deploying istio.
I am using Jaeger, and have Jaeger Agents deployed in DaemonSet.
As I have each Jaeger Agents (on each nodes), Jaeger Collector, and Jaeger Query, I believe global.tracer.zipkin.address should be configured as Jaeger Agent host.
However, Agents are on each nodes, and I have hard time specifying the host.
How should I specify it?
Thanks in advance.
FYI) If I understood correctly, Jaeger Client will send the data to the Jaeger Agent via Envoy, and then to Jaeger Collector.
I am working with  Vert.X 3.9  and  Java 8  and I'm trying to implement  Opentrace  with  Jaeger .
I have an issue on sending method for the  Spans .
There is a  jaeger-collector  already working for another services (Not using Vert.X), but for some reason it is not receiving traces from the Vert.X app.
So far I noticed that the Sender is setted as  UdpSender  instead of  httpSender  althought I already set the env variable  JAEGER_ENDPOINT .
This is the Java code:
This code creates some logs that show the Tracer and Spam creation:
Regarding docs the sender should be httpSender, but here is udp
sender=UdpSender(udpTransport=ThriftUdpTransport(socket=java.net.DatagramSocket@55c10031, receiveBuf=null, receiveOffSet=-1, receiveLength=0)), closeEnqueueTimeout=1000)
These are the dependencies I am using:
I have a simple Node.js app the uses the Open Telemetry Jaeger exporter to send trace information into Jaeger.
It runs fine when I fire up the Jaeger as a Docker container then run the code from my machine's command line against  localhost .
However, when I try to run both the app and jaeger under Docker Compose within a Docker Compose network, the service registers and is apparent in the Jaeger UI, but the trace/span information never gets received into Jaeger.
Here is the code:  https://github.com/reselbob/simpletracing/tree/releases/v1.0
I attached a screenshot to demonstrate that the service seems to be registering but the spans are not getting through.
.
I am manually instrumenting code using jaeger, and have a question on how to instrument code that is generated automatically for me?
An example is when I try to instrument code that uses Spring's CrudRepository and MongoRepository.
Anyone have any ideas?
When I use opentelemetry's auto instrumentation javaagent jar located here,  https://github.com/open-telemetry/opentelemetry-java-instrumentation  it is able to trace the MongoRepository method that is a generated.
I am trying to set up Jaeger to collect traces from a spring boot application.
When my app starts up, I am getting this warning message
warn io.jaegertracing.internal.senders.SenderResolver - No sender factories available.
Using NoopSender, meaning that data will not be sent anywhere!
I use this method to get the jaeger tracer
I have manually instrumented the code, but no traces show up in the jaeger UI.
I have been stuck on this problem for a few days now and would appreciate any help given!
In my pom file, I have dependencies on jaeger-core and opentracing-api
I am required to run a jaeger-agent on a bare-metal server that doesn't have support for docker.
I have downloaded a jaeger-binary on it and am able to successfully accept binary traces on the udp port I specified in the config.
But I have the use-case where I'm only required to accept traces in binary format, and this means that I do NOT want to open the port for accepting compact thrifts.
Could anyone help me in achieving this (essentially, I should be able to open a minimal set of ports to run my agent).
I have a microservices-based application Running on Kubernetes.
The microservices are built using dropwizard framework.
I would like to enable tracing in order to track the requests and have a solution that can help debug stuff.
Basically, I know the implementation using Spring boot which is pretty straightforward.
but I'm wondering how it could be in dropwizard based application?
actually, Is this is possible?
Can someone share his experience with this topic?
And provide me with resources or examples of how I can do that?
Please make sure that I'm not using a service mesh.
I'm not able to get any tracing on Jaeger.
I did this configuration:
Should I keep the double quotes in the hostname and port ?
What is the correct port to use ?
I am using Spring Boot as Microservice, I am using Jaeger  for Monitor and troubleshoot transactions in complex distributed systems.
I could see all microservices Span with respect to given trace id/ call.
But When I am calling OSB layer service from Spring Boot Service, I could not OSB layer as a SPAN.
what could will be possible solution for this.
Thank You
I am trying to trace logs of my spring-boot-application with jaeger .
Both spring-boot microservice and jaeger are running on kubernetes ( local set-up on docker-desktop ) .
My services traces are not visible in jaeger UI .
The same spring boot microservice and Jaeger local set up (without kuberntes ) is working fine .
Below is the configuration in my application.properties to interact with jaeger-agent in kubernetes .
Below is my code :
https://github.com/anuragk3334/Spring-boot-and-Jaeger/tree/master/HelloWorld
Jaeger configuration is :
https://github.com/anuragk3334/Spring-boot-and-Jaeger/blob/master/HelloWorld/k8s/jaeger.yaml
I created an application with  spring-cloud-bus  (for the auto-refresh from spring-cloud-config-server) and  opentracing-jaeger .
Without  spring-cloud-bus , jaeger shows the application logs in traces.
But with  spring-cloud-bus , the logs are missing.
On debugging, the following details were found.
Please find a  sample application  with pre-configured settings here.
Can someone guide me, how to work around this issue?
Most of the integrations I have come across uses the java-agent to push the traces to a central collector and in turn one can view traces in Jaeger.
However in my case I can't use the java agent, hence I decided to go with the custom tracing api which seems fine and there are many examples for this.
By design my low latency application limits me from making any connections to external components/ports hence I am also trying to avoid pushing the traces/spans to the local Jaeger agent or Collector endpoint, rather have the traces logged via the LogReporter.
Beyond this I am wondering how to build a pipeline for pushing the trace logs in to Jaeger.
The logs themselves are in AWS cloudwatch as streams so I am thinking if I use a Serveless Lambda to subscribe and parse these trace log events then I could ship them myself to Jaeger using may be the HTTP /api/traces endpoint (not much details but read somewhere that this exists in some form).
At this point my question is if this is the right way or there is a better mechanism to achieve this.
As I have no idea if the traces themselves can be replayed in this fashion to the Collector.
Also not sure what format the endpoint accepts as I don't see much documentation or example around this.
The objective is for my application to &quot;not&quot; connect to any external monitoring infrastructure via push events so if there is any better way for Jaeger integration I would love to hear.
Also I am okay if any other API in the form of OpenTracing, OpenCensus or even the latest OpenTelemetry can help with this.
Thanks
I'm trying out OpenTracing Jaeger and have the following file  test.cpp :
And consider  config.yml  to be:
Now if I compile  test.cpp  with  g++ test.cpp -lopentracing -ljaegertracing -lyaml-cpp  and run  ./a.out config.yml , I get a
While if I compile with  g++ test.cpp -L /usr/local/lib -lopentracing -ljaegertracing -lyaml-cpp , I get a good
The contents of my  /usr/local/lib  are:
I'm using Jaeger in conjunction with a larger project involving ROS and get the same error even if I  add_compile_options(-L /usr/local/lib)  for  CMakeLists.txt  of the appropriate package; so, I wanted to better understand what the exact cause of the above error is, so hopefully that helps me to fix the one involved in ROS.
Thanks!
My query from the Jaeger ElasticSearc returns the following entry
My goal is to find entries which have tag[&quot;internal.span&quot;] &quot;zipkin&quot;.
Field &quot;tags&quot; is &quot;nested&quot;.
I am trying a query like the one below.
I do not get hits.
What do I miss?
In  jaeger-client-cpp  when I connect my  Tracer  variable to jaeger backend (I m using  jaeger-all-in-one  server) then upon successful connection  LOG INFO  message is shown telling me the connection is successful, but when connection is unsuccessful is just shows  LOG ERROR  message telling me that connection with server not successful.
So is there any way to check this programatically about the status of connection of  Tracer  with server.
OS-ubuntu 18.04 
 jaeger-client-cpp-v0.5.0
The  JaegerGrpcSpanExporter  of the Java OpenTelemetry API implements the export method with a grpc blocking stub ( CollectorServiceGrpc.CollectorServiceBlockingStub ).
In case of high latence or slowness on the collector side, isn't it dangerous to block the thread?
How to add method name to the log section in Jaeger.
I just included opentracing-spring-jaeger-cloud-starter in my spring boot application and it amazingly contextualized logs.
I can also see method name, but only in tags.
The problem is that when there are multiple methods invoked and if the logging is not explicitly referring method name, it is difficult (a lil bit) to trace it.
Currently level, logger, message and thread are present in the log section.
I want to add method name as well.
Does anyone know how to do it?
In case you want to see a sample application, this is one  https://github.com/winster/spring-tracer
Note that, I have already set a console pattern with method name.
But the log fields always remains the same.
We are using Multi cluster single plane Istio on GKE.
We are trying to get traces of all the remote cluster services into the main cluster tracing pod.
We have exposed the tracing pod via ILB in main cluster.
And created a headless zipkin service on remote cluster with ILB IP as endpoint.
As of now it's not working.
We have tried setting up &quot;remoteTelemetryAddress&quot; flag as well on remote clusters but still no result.
Istio Version: 1.5.5
GKE version: 1.16.8-gke.15
Network: Single VPC
I have a node.js application using RPC messages over thrift API to communicate with another server application.
I want to integrate distributed tracing, such as Jaeger or Zipkin, which traces the requests through these applications.
The problem is that every approach and example I found to this topic is given with HTTP requests, which I don't use.
The span context is added to the HTTP header with an inject function of the tracer.
On server-side, the extract function can be used to get these information, which was added to the header before.
Generally, this basic approach for this inter-process context propagation using RPC messages over thrift API should be the same, but I simply don't know how to start, because I cannot find any examples, recommended ways or best practises.
The only advice that this should be possible, I found on Zipkin's website (sub-item Thrift Tracing):
 https://zipkin.io/pages/instrumenting.html
I'm a newbie on Jaeger and would like to know if I could trace an end-to-end transaction with a parent span &amp; child ones like the one described below with polling from child components (no direct invocation from parent to child) and callback from childs to the parent component.
First let's describe a simplified view of what I'd like to do.
A solution made of several components exposes a REST API to submit transactions.
It synchronously returns a transaction Id after invocation and will callback the invoker upon completion or failure of the transaction.
So what I want to trace is the overall transaction from 1 to 3:
Under the hood:
I'm assuming that it will be possible to trace the entire transaction with Jaeger, but here are my questions:
Question:
Any hints &amp; tips will be welcome.
Thx.
Using the helm chart for Jaeger I see that it makes use of the cassandra subchart.
Looking at the documentation and config files it looks like by setting the provisionDataStore.cassandra override to false that the cassandra subchart shouldn't be getting installed.
However, when the override is set I can still see the cassandra service being installed on my cluster.
Anybody know why and how I can prevent cassandra service from being deployed to my cluster?
I was expecting that when I set the provisionDataStore.cassandra=false that I shouldn't see any cassandra services being deployed to my cluster.
This is what the requirements.yaml file looks like for the Jaeger helm chart:
dependencies:
  - name: cassandra
    version: ^0.13.1
    repository:  https://kubernetes-charts-incubator.storage.googleapis.com/ 
    condition: provisionDataStore.cassandra
  - name: elasticsearch
    version: ^7.5.1
    repository:  https://helm.elastic.co 
    condition: provisionDataStore.elasticsearch
I am using opentracing-spring-jaeger-web-starter in my spring boot project.
It create auto spans for all rest call and do tag with standard tags.
How can i add custom tags for rest call?
I have created a Helidon Microprofile quickstart project from helidon.io get started while configuring with Jaeger I am unable to find the Trace in Jaeger UI below are the steps which I have followed:
Created project using
Updated  pom.xml  with Jaeger dependencies
Updated GreetApplication
Updated /helidon-quickstart-mp/src/main/resources/META-INF/microprofile-config.properties
Executed mvn package and then  target&gt;java -jar helidon-quickstart-mp.jar
Now in my Jaeger UI I am unable to trace the running Service:
So how can I configure Jaeger UI to my helidon Microprofile project?
I'm trying to play around with Jaeger and open-tracing in my local k8s node (Docker for Mac) and having some trouble see traces in the UI.
I'm using the Jaeger operator and deployment annotations to inject the jaeger sidecar.
The Jaeger cr is configured to sample constantly every request.
Up until this point, everything seems to be fine but when I send some HTTP traffic to my pods (Through nginx-ingress) I can see it coming but can't find any traces in Jaeger UI.
From reading the documentation, these steps should've implicitly collect and send the traces.
Am I missing something?
I am trying to setup jaeger-all-in-one on one server.
If I run the exe jaeger-all-in-one, everything works as expected (using in-memory).
In order to see the options available with ES, I am not able to run a help command.
Now, my requirement is to specify an elastic search URL.
I have set up the environment variables  SPAN_STORAGE_TYPES  and  ES_SERVER_URLS , but couldn't find how to run jaeger-all-in-one.exe by asking it to take in these environment variables.
I have built a sample app to understand the trace and span using OpenTelemetry.
I want to see them in Jaeger UI.
How to set up Jaeger with my application which uses OpenTelemetry for tracing?
I have gone through Jaeger Documentation.
They have specified that how will Jaeger will work the HTTP request kind of scenario but if I want to get traces of Nservicebus's to publish/subscribe method then How will I get using Jaeger?
Is it possible?
Or Jaeger only works with HTTP requests?
Was trying to connect to jaeger using HTTP request using nodejs but the spans are not reaching the jaeger endpoint.
please help with this code snippet.,
Any help would be much appreciated!
I'm playing with JaegerTracing in Django using tutorial  https://github.com/contino/jaeger-django-docker-tutorial .
Now I don't know how to take out traceId from response headers because it's not there.
When finding traces in Jaeger UI it returns response with data (see also screenshot below):
I suspected it in response headers but it is not.
How can I do this?
Based on  this  and  this
How would I enable tracing for  reactive-sql-clients  ?
Now use  %dev.quarkus.datasource.url=vertx-reactive:postgresql://dev-db-server:5432/mydb  - it works, but no tracing support though.
I can see racing for my rest calls but not the db.
Tried to use  %dev.quarkus.datasource.url=vertx-reactive:tracing:postgresql://dev-db-server:5432/mydb
my deps:
I am planning to use  Jaeger  tracing in on my  Golang  server.
Everything is ok but I haven't found a way to handle  Jaeger  errors.
I want to catch, for example, connection error to  Jaeger  backend while sending trace and write it to  loggly .
Code example:
I am trying to deploy Istio Jaeger UI for distributed tracing.
Currently I am using kubectl port forwarding using the command  kubectl port-forward -n monitoring prometheus-prometheus-operator-prometheus-0 9090 .
But it runs on  http://localhost:port  So how can I do it in production?
Is there any other way to deploy in production.
And also how can I make it run on  https ?
We are able to get latency metrics of multiple microservices using Jaeger.
Currently Jaeger stores application metrics in elasticsearch.
My usecase is to get the latency of application from elasticsearch to prometheus.
Is there anyway to read the elasticsearch metrics of Jaeger?
I already used elasticsearch-prometheus-exporter which only exports cluster details of ES.
I am analysing at a very hight level how much effort would it be for jaeger integration in nodejs microservices.
Does it require code changes or only deployment.
and if code changes is required, is code changes needed in first service (i.e.
api-gateway) or all the services need to have code changes.
I would really appreciate if someone can give a rough idea of tasks and effort.
I have created a microservice based architecture using Spring Boot and deployed the application on Kubernetes/Istio platform.
The different microservices communicate with each other using either JMS (ActiveMQ) or REST API.
I am getting the tracing of REST communication on Istio's Jaeger but the JMS based communication is missing in Jaeger.
I am using ElasticSearch to store my application logs.
Is it possible to use the same ElasticSearch as a backend(DB) of Jaeger?
If yes then I will store tracing specific logs in ElasticSearch and query them  on Jaeger UI.
I'm setting up a proof of concept featuring two ASP.NET Core applications that are both instrumented with  Jaeger  to demonstrate how it can propagate a trace between services over the wire.
Both applications are being deployed to Azure App Services.
I'm using the  OpenTracing Contrib  package to automatically inject the Jaeger trace context into my inter-service traffic in the form of HTTP Headers (the package is hardcoded to use that form of transmission).
But it appears that those headers are going missing along the way, as the receiving application is unable to resume the tracing context.
Before deploying to Azure, I'm testing the applications locally with Docker Compose, and  with that setup the context propagation works fine .
It's only once the apps are in Azure that things break.
The applications communicate over HTTPS and I've disabled HSTS and HTTPS redirection in case that might be causing Azure to drop the headers, based on the answer in  this previous thread .
I've also tried running both applications in Azure Container Instances, and that seems to be a non-starter - it doesn't fix the context propagation and seems to introduce more bugs around span relationships.
The two applications are nearly identical in their setup, and differ only in the API endpoints they serve.
My CreateWebHostBuild from program.cs:
The contents of the AddJaeger extension method which is largely borrowed from  the Contrib sample :
My startup.cs configure method to show I'm not doing anything weird with the headers (the metrics extensions are for prometheus-net)
I expect any calls from one application to the other to propagate the active Jaeger trace context.
Instead, the two applications log their traces separately and no link can be discerned between them in the Jaeger UI.
Here's a screenshot of a trace that should have spanned both services, but instead only shows spans from the first service:
I have three services A, B, and C that communicate like so
I'm using OpenTracing and Jaeger for distributed tracing.
The problem is these services are in different languages, but I'm still trying to propagate the information that A is the parent span so that the span tree looks like this.
Right now, A, B, and C are being reported as individual traces with no causality relations.
All the examples I've seen involved propagating causality between different microservices in the same language and in the same project build.
None involved entirely separate services.
Unfortunately, I'm not able to use PyInstaller with  jaeger .
The problem is some sort of a thrift error between PyInstaller and  jaeger .
Like discussed  here .
Are they any workarounds or fixes?
I have tried it with python 3.6 and the newest jaeger-client.
There I get an Errno 2 -  Even I don't even use a Config file
The script runs as expected -  Spans are created and web server starts.
Only in the executable, it does not run.
And shows the following error:
The  Istio (version 1.0.6)  official document says:
We can access the Jaeger UI by the following action:
Kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath=’{.items[0].metadata.name}’) 16686:16686 &amp;
Then we can use  http://localhost:16686 .
But the localhost is a Linux machine, it doesn't have a browser.
I must open the browser on a remote machine.
How can I do this?
Thanks.
I've set up Jaeger with Opentracing in a Java environment and it works nicely with logging messages with spans and tracing.
But I am a bit stuck when it comes to catching and logging exceptions.
But this way does not format error logging in a good readable way.
I have looked around for information about this as it feels pretty obvious there should be as this is one of its components for logging.
But I have somehow never seen anything about this.
It is mostly about building and structuring spans.
Hope anyone can help me with this when it comes to capturing and logging exceptions.
I have two basic Springboot microservices and I am using Jaeger.
Lets say two services are  foo  and  bar .
I am able to send  User-Agent  header from foo to bar service using Tracing Baggage property.
From  foo  service, I will be calling  bar  service using  localhost:port  as of now.
The users will also send an  x-api-key  header in the request.
This header is not being forward from  foo  to  bar  service.
This is my code snippet,
On the logs of my  bar  service, it is receiving these headers,
 uberctx-user-agent  and  uberctx-x-api-key
I am not sure why  uber-ctx-*  is appended, I only want  x-api-key  header to be forwarded.
I run  ./jaeger-all-in-one --es.tags-as-fields.all=true --es.index-prefix=myteam.jaeger --es.server-urls=http://ip-server:9200
How add the variables  --es.tags-as-fields.all ,  --es.index-prefix  and  --es.server-urls  to the YAML config ?
Thanks!
I'm new to using Jaeger tracing system and have been trying to implement it for a flask based microservices architecture.
Below is my jaeger client config implemented in python:
I read somewhere that Sampling strategy is being used to sample the number of traces especially for the trace which doesn't have any metadata.
So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?
Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes.
I would like to understand this configuration spec more but not able to.
Describe the bug 
We have a container running with envoy sidecar proxy with service/deployment for port 443 using Istio's own example: sample/https/nginx.
We can curl the container to get nginx page just fine but see absolutely no traces in Jaeger for https calls.
We see HTTP calls in Jaeger as soon as we switch the port to 80 in deployment/service
Expected behavior 
We should see traces for both HTTP/HTTPS calls to the container.
Steps to reproduce the bug:
create nginx config:
Create nginx deployment :
curl -kv https://service-ip  gives 200
Version
Installation
Environment 
- Running this within AWS EKS
Cluster state 
- Attached 
 archite.tar.gz
Edit 1
yaml for service -  jaeger-query  :

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-10-02T02:32:23Z
  labels:
    app: jaeger
    chart: tracing-1.0.1
    heritage: Tiller
    jaeger-infra: jaeger-service
    release: istio
  name: jaeger-query
  namespace: istio-system
  resourceVersion: "5259733"
  selfLink: /api/v1/namespaces/istio-system/services/jaeger-query
  uid: 6513eded-c5eb-11e8-860c-12504ba0df7c
spec:
  clusterIP: 172.20.14.251
  ports:
  - name: query-http
    port: 16686
    protocol: TCP
    targetPort: 16686
  selector:
    app: jaeger
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
Deployment :  istio-tracing  :

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: 2018-10-02T02:32:23Z
  generation: 1
  labels:
    app: istio-tracing
    chart: tracing-1.0.1
    heritage: Tiller
    release: istio
  name: istio-tracing
  namespace: istio-system
  resourceVersion: "5259783"
  selfLink: /apis/extensions/v1beta1/namespaces/istio-system/deployments/istio-tracing
  uid: 65056099-c5eb-11e8-860c-12504ba0df7c
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: jaeger
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
        sidecar.istio.io/inject: "false"
      creationTimestamp: null
      labels:
        app: jaeger
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - ppc64le
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - s390x
            weight: 2
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
                - ppc64le
                - s390x
      containers:
      - env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: COLLECTOR_ZIPKIN_HTTP_PORT
          value: "9411"
        - name: MEMORY_MAX_TRACES
          value: "50000"
        image: docker.io/jaegertracing/all-in-one:1.5
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 16686
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: jaeger
        ports:
        - containerPort: 9411
          protocol: TCP
        - containerPort: 16686
          protocol: TCP
        - containerPort: 5775
          protocol: UDP
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 16686
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 10m
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: 2018-10-02T02:32:23Z
    lastUpdateTime: 2018-10-02T02:32:23Z
    message: Deployment has minimum availability.
reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: 2018-10-02T02:32:23Z
    lastUpdateTime: 2018-10-02T02:32:27Z
    message: ReplicaSet "istio-tracing-ff94688bb" has successfully progressed.
reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
does jaeger provide a way of querying the trace data without using the UI provided.
I'm aware that zipkin provides an API to directly access the trace data etc.
Use-case: i'm trying to use the trace data to pull together a custom report for internal purposes.
I could scrape the data from the UI but wondered if there was an easier way.
I was developing a Spring Boot application in which the loging is done by logback and Jaeger is integrated for instrumentation.
Myservice.java
logback.xml
pom.xml
The Jeager is properly connected to server and its getting the traces.
The problem is with the logback logs.
The traceId and spanId are not getting printed in the logs.
But I myself found a solution for that.
I added Spring Cloud Sleuth with my Spring Boot application.
Now all the trace information was available in the logback log.
But the problem is that Jaeger stopped registering traces to Jaeger server.
I tried Zipkin instead of Jaeger, but the same thing happened.
What's wrong with my application?
Is something wrong with the dependencies?
I have certain applications that run jaeger-client when I enable OpenTracing and start them.
First I start Jaeger collector using the command-
docker run -d -e   COLLECTOR_ZIPKIN_HTTP_PORT=9411   -p 5775:5775/udp   -p 6831:6831/udp   -p 6832:6832/udp   -p 5778:5778   -p 16686:16686   -p 14268:14268   -p 9411:9411   jaegertracing/all-in-one:latest
Then I start the applications like user-
start.sh user -apiserver=localhost:9900 -configfile=conf/configuration.json -traceroption enabled=true 
following which they become visible as enabled services  http://localhost:16686/api/services
The problem is that if I kill the Docker running the jaeger collector- 
systemctl stop docker
and later restart docker and jaegertracing/all-in-one, the services are no longer up at  http://localhost:16686/api/services
Does the jaeger client die on its own in absence of a Jaeger collector?
Does the Jaeger collector needs to be running before starting the Jaeger clients?
If so, how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?
I wasn't able to find any clear API in RegisterRoutes method of  https://github.com/jaegertracing/jaeger/blob/master/cmd/query/app/handler.go
I'm running a Spring-Boot application inside a docker container and want to instrument it with OpenTracing using the Jaeger client from Uber.
For the instrumentation I'm using the  OpenTracing Spring Web  library in combination with the  Jaeger  client.
The following code snippet configures the tracer in the application:
I can see the traces when I run the application (not inside a Docker container) and start Jaeger with the following command:
But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces.
After that I tried to declare the Jaeger docker container in the same docker-compose file and added a link from the  demo  service to the  jaeger  service:
But I still can't see any traces in the Jaeger client.
For hours I have tried different approaches but didn't make any progress so far, if somebody could help me out I would greatly appreciate it!
You can find my demo project on  GitHub .
I am hosting my application on GCP and I want to use stackdriver as my backend storage for trace spans with jaeger collectors.
I can't seem to find anything related to that.
In GCP I can find clearly that they support zipkin.
I am not sure what to do here.
Should I create some translation layer to push the data to stackdriver ?
Or is it supported somehow by the current zipkin connector ?
I truly wouldn't want to host my full tracing solution to avoid having to maintain it.
Can I run the Jaeger collector and somehow pass it to stackdriver ?
Thanks
From react application (App.js ) imported jaeger-client.
import jaegerClient from 'jaeger-client'
Got exception 'TypeError: _fs2.default.readFileSync is not a function' from following line of /node_modules/jaeger-client/dist/src/thrift.js:168
 
source: _fs2.default.readFileSync(_path2.default.join(__dirname, './jaeger-idl/thrift/jaeger.thrift'), 'ascii')
Trying to solve it.
Thanks for any help.
Complete package.json is like below
 
{
  "name": "calculator",
  "version": "0.1.0",
  "private": true,
  "homepage": "http://ahfarmer.github.io/calculator",
  "devDependencies": {
    "gh-pages": "^1.1.0",
    "react-scripts": "^1.0.17"
  },
  "dependencies": {
    "ajv": "^6.4.0",
    "ajv-keywords": "^3.1.0",
    "big.js": "^5.0.3",
    "bufferutil": "^3.0.3",
    "fs": "0.0.1-security",
    "github-fork-ribbon-css": "^0.2.1",
    "hexer": "^1.5.0",
    "jaeger-client": "^3.10.0",
    "react": "^16.2.0",
    "react-dom": "^16.2.0",
    "react-tracing": "^0.1.5",
    "thrift": "^0.11.0"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test --env=jsdom",
    "eject": "react-scripts eject",
     "deploy": "gh-pages -d build"
    },
    "eslintConfig": {
        "extends": "./node_modules/react-scripts/config/eslint.js"
    }
}
Forked from  https://github.com/ahfarmer/calculator  and I am trying to trace every user action ( button press ).
To test tracing from react.js application.
I'm currently looking into different openTracing Tracer-Implementations.
I want to use  uber/jaeger-client-node  but the backend won't receive my traces.
Here is what I did:
I started the all-in-one docker image:
 docker run -d -p5775:5775/udp -p16686:16686 jaegertracing/all-in-one:latest
Next, i wrote a simple example application:
 Gist
But when I go to Jaeger UI, nothing is shown about the example service.
What did I do wrong?
Thanks
I have not found a way to implement a Jaeger Open-Tracing framework implementation on the IBM Websphere Server platform.
All the examples I've seen point to environment variables to be set to specify  where  to communicate to a Jaeger collection endpoint.
I wanted to ask the community if anyone had experience with this.
I am trying to setup Jaeger using a CentOS base image instead of Alpine.
The agent, collector, and Cassandra containers all work fine except for the query container.
The Jaeger repository is  here .
After changing the base image to CentOS 7, commenting out the sections that apply to copying  ca-certificates.crt  and running  docker-compose , I get the following nil pointer error message when tailing the query container
panic: runtime error: invalid memory address or nil pointer dereference
I ran the makefile with the necessary flags to compile the code in the app directory.
Has anyone ever setup Jaeger using CentOS as a base image?
Below is the full stack error from the container
I'm trying to use  OpenTracing.Contrib.NetCore  with Serilog.
I need to send to Jaeger my custom logs.
Now, it works only when I use default logger factory  Microsoft.Extensions.Logging.ILoggerFactory
My Startup:
and somewhere in controller:
in a result I will able to see that log in Jaeger UI
But when I use Serilog, there are no any custom logs.
I've added  UseSerilog()  to  WebHostBuilder , and all custom logs I can see in console but not in Jaeger.
There is open issue in  github .
Could you please suggest how I can use Serilog with OpenTracing?
I have a Django web app served from Apache2 with mod_wsgi in docker containers running on a Kubernetes cluster in Google Cloud Platform, protected by Identity-Aware Proxy.
Everything is working great, but I want to send GCP Stackdriver traces for all requests without writing one for each view in my project.
I found middleware to handle this, using Opencensus.
I went through  this documentation , and was able to manually generate traces that exported to Stackdriver Trace in my project by specifying the  StackdriverExporter  and passing the  project_id  parameter as the Google Cloud Platform  Project Number  for my project.
Now to make this automatic for ALL requests, I followed the instructions to set up the middleware.
In settings.py, I added the module to  INSTALLED_APPS ,  MIDDLEWARE , and set up the  OPENCENSUS_TRACE  dictionary of options.
I also added the  OPENCENSUS_TRACE_PARAMS .
This works great with the default exporter 'opencensus.trace.exporters.print_exporter.PrintExporter', as I can see the Trace and Span information, including Trace ID and all details in my Apache2 web server logs.
However, I want to send these to my Stackdriver Trace processor for analysis.
I tried setting the  EXPORTER  parameter to  opencensus.trace.exporters.stackdriver_exporter.StackdriverExporter , which works when run manually from the shell, as long as you supply the project number.
When it is set up to use  StackdriverExporter , the web page will not respond load, the health check starts to fail, and ultimately the web page comes back with a 502 error, stating I should try again in 30 seconds (I believe the Identity-Aware Proxy is generating this error, once it detects the failed health check), but the server generates no errors, and there are no logs in access or errors for Apache2.
There is another dictionary in settings.py named  OPENCENSUS_TRACE_PARAMS , which I presume is needed to determine which project number the exporter should be using.
The example has  GCP_EXPORTER_PROJECT  set as  None , and  SERVICE_NAME  set as  'my_service' .
What options do I need to set to get the exporter to send back to Stackdriver instead of printing to logs?
Do you have any idea about how I can set this up?
settings.py
Here's an example (I prettified the format for readability) of the Apache2 log when it is set to use the  PrintExporter :
Thanks in advance for any tips, assistance, or troubleshooting advice!
Edit 2019-02-08 6:56 PM UTC:
I found this in the middleware:
The exporter is now named  StackdriverExporter , instead of  GoogleCloudExporter .
I set up a class in my app named  GoogleCloudExporter  that inherits  StackdriverExporter , and updated my settings.py to use  GoogleCloudExporter , but it didn't seem to work, I wonder if there is other code referencing these old naming schemes, possibly for the transport.
I'm searching the source code for clues...
This at least tells me I can get rid of the ZIPKIN and JAEGER param options, as this is determined on the  EXPORTER  param.
Edit 2019-02-08 11:58 PM UTC:
I scrapped Apache2 to isolate the problem and just set my docker image to use Django's built in webserver  CMD ["python", "/path/to/manage.py", "runserver", "0.0.0.0:80"]  and it works!
When I go to the site, it writes traces to Stackdriver Trace for each request, the Span name is the module and method being executed.
Somehow Apache2 is not being allowed to send these, but I can do so from the shell when running as root.
I'm adding Apache2 and mod-wsgi tags to the question, because I have a funny feeling this has to do with forking child processes in Apache2 and mod-WSGI.
Would it be the child process being unable to be created as apache2's child process is sandboxed, or could this be a permissions thing?
It seems strange, because it is just calling python modules, no external system OS binaries, that I am aware of.
Any other ideas would be greatly appreciated!
I am building a JSON validator from scratch, but I am quite stuck with the string part.
My hope was building a regex which would match the following sequence found on JSON.org:
My regex so far is:
It does match the criteria with a backslash following by a character and an empty string.
But I'm not sure how to use the UNICODE part.
Is there a regex to match any UNICODE character expert " or \ or control character?
And will it match a newline or horizontal tab?
The last question is because the regex match the string "\t", but not "    " (four spaces, but the idea is to be a tab).
Otherwise I will need to expand the regex with it, which is not a problem, but my guess is the horizontal tab is a UNICODE character.
Thanks to Jaeger Kor, I now have the following regex:
It appears to be correct, but is there any way to check for control characters or is this unneeded as they appear on the non-printable characters on regular-expressions.info?
The input to validate is always text from a textarea.
Update: the regex is as following in case anyone needs it:
Is there way to configure opentracing-spring-jaeger-cloud-starter to handle any other header than Uber-Trace-Id?
I have Traefik as an ingress in my kubernetes cluster.
Traefik can be configured to change traceContextHeaderName.
Default value is "uber-trace-id".
When I change it to some custom, there is no connection (I mean span connection) between services.
I believe that opentracing works only with Uber-Trace-Id.
Is there way to configure that?
I test this in minikube with Traefik as an ingress.
Then all requests go to spring-cloud-gateway and are propagate to services.
Thanks for help!
Is there a way to use Spring Cloud Sleuth with OpenTracing?
I want to connect Spring clients with Jaeger
I got the video to work on Android 4.0.3 (API 15) and up using  Jaeger 25's Html5Video plugin .
However, With the addition of Android 4.4 and its revised Chromium-based WebView, this plugin no longer suffices,  as mentioned by its awesome developer .
Now, I would much rather play these video's without any plugins anyway (which works perfectly on iOS...), so I went back to trying that.
Is was hoping that this Chromium-based webview would be friendlier with basic HTML5 playback, but instead, I just get the exact same error as before:  MediaPlayer(30579): Error (1,-2147483648) .
I've spent hours trying several different approaches, all to no avail.
I'll list a few things that I've tried below.
Please, does anyone have any clues to point me in the right direction?
I'm out of ideas...
Normal file reference using  file:///
Code:
Results in:
File reference using Phonegap's Filesystem API
Code:
Results in (contains that same MediaPlayer (1, -2147483648) error):
In a project I'm working on, I have a textbox where the user has to input his name.
To avoid the user from entering numbers I used the     jquery.limitkeypress.js  library written by  Brian Jaeger  and every thing was working perfectly until I tested the website in Internet Explorer 10.
In IE10, you can input all the letters you want, and you can not input number or weird symbols just as I wanted but when I type a space and then a letter, I see the letter print right to the space and then the space disappearing and the latter shifting to the left.
The weird thing is that if I wait like 30 seconds after typing the space to type the letter it works fine.
I have Kubernetes 1.17.5 and Istio 1.6.8 installed with demo profile.
And here is my test setup [nginx-ingress-controller] -&gt; [proxy&lt;-&gt;ServiceA] -&gt; [proxy&lt;-&gt;ServiceB]
When I'm sending requests to ingress controller I can see that ServiceA receives all required tracing headers from the proxy
Problem is  x-b3-sampled  is always set to 0 and no spans/traces are getting pushed to Jaeger
Few things I've tried
Here is the config I've tried to use
I am trying to make OpenTelemetry exporter to work with OpenTelemetry collector.
I found this  OpenTelemetry collector demo .
So I copied these four config files
to my app.
Also based on these two demos in open-telemetry/opentelemetry-js repo:
I came up with my version (sorry for a bit long, really hard to set up a minimum working version due to the lack of docs):
.env
docker-compose.yml
otel-agent-config.yaml
otel-collector-config.yaml
After running  docker-compose up -d , I can open Jaeger (http://localhost:16686) and Zipkin UI (http://localhost:9411).
And my  ConsoleSpanExporter  works in both web client and Express.js server.
However, I tried this OpenTelemetry exporter code in both client and server, I am still having issue to connect OpenTelemetry collector.
Please see my comment about URL inside of the code
Any idea?
Thanks
Input:
GCP, Kubernetes, java 11 spring boot 2 application
Container is started with memory limit 1.6GB.
Java application is limiting memory as well -XX:MaxRAMPercentage=80.0.
Under a "heavy" (not really) load - about 1 http request per 100 ms during about 4 hours application is killed by OOMKiller.
Internal diagnostic tools is showing that memory is far from limit:
However GCP tools is showing the following:
There is a suspicious that GCP is measuring something else?
POD contains only java app (+jaeger agent).
The odd thing that after restart GCP shows almost maximum memory usage instead of slowly growing if it was memory leak.
EDIT:
Docker file:
and run it with Kubernetes (extra details are ommited):
UPDATE 
according top command memory limit is also far from limit however CPU utilization became more then 100% before container is OOMKilled.
Is it possible that Kubernetes kills container that is trying to get more CPU then allowed?
UPDATE2
this pmap was called not far before OOMKilled.
5Gb?
Why top is not showing this?
Also not sure how to interpretate pmap command result
We are building a web-app using Micronaut ( v1.2.0 ) which will be deployed in a Kubernetes cluster (we are using Istio as the service-mesh).
We would like to instrument the critical method calls so that they can generate their own spans within a HTTP request span context.
For this we are using the Micronaut OpenTracing support and Jaeger integration.
The following dependencies are included in the  pom.xml
Have implemented Filter method with  @ContinueSpan  (also tried the same with  @NewSpan ) as shown below
The following is maintained in the  application-k8s.yml  (also have an  application.yml  with the same settings)
However we only see the trace entries that are generated by Istio (Envoy proxies) but we don't see the details of the method calls itself.
Any ideas as to what could be going wrong here?
docker pull jaegertracing/jaeger-agent:latest
Jaeger is just for illustration.
But my question is more generic.
The above command pulls the  latest  version of the  jaeger-agent  from docker-hub.
The docker-hub page for this is :  https://hub.docker.com/r/jaegertracing/jaeger-agent
My question is how do I find the actual version of  latest  ?
I looked in to the tags here, but there is not much info :
 https://hub.docker.com/r/jaegertracing/jaeger-agent/tags
Also I tried doing an  inspect  after pulling the image, but could not get necessary details.
docker image inspect jaegertracing/jaeger-agent:latest
Where can we get this information from ?
I am trying to integrate Jaeger tracing into K-Streams.
I was planning to add tracing to few of my most important pipelines and was wondering what would be a good way to pass traceid from one piepline to another?
Here is what I have so far - At the start of stream processing pipeline, I start a server span and save the traceid into a state store.
Later on, in a transform pipeline, I access the statestore and capture the  trace from the transform() method.
Is this a good way to handle tracing in stream processing?
I am trying to integrate a legacy system with microservice hosted on Red Hat OpenShift platform.
The service is a java app behind ingress gateway.
The legacy app passes unique operation identifier as a custom header  uniqueId .
The microservice leverages openshift service mesh support for Jaeger so I can pass tracing headers such as  x-b3-traceid  and see the request trace in Jaeger UI.
Unfortunately, the legacy app cannot be modified and won't send jaeger headers but  uniqueId  conforms jaeger rules and seems ok to be used for tracing.
I am trying to transform  uniqueId  into  x-b3-traceid  on an envoy filter.
The problem is that I can copy it to any other header, but cannot modify  x-b3-*  headers.
Istio keeps generating new set of  x-b3-*  headers no matter what I do in envoy filter.
See filter code below.
I tried different filter positions (on ingress gateway, on pod sidecar, before envoy.router, etc).
Seems nothing works.
Can anyone recommend how can I pass custom header as a traceId for service mesh's jaeger?
I can create a custom proxy service transforming one header with another but it looks redundant.
Is it possible to achieve that with service mesh only?
I've been using Spring Boot for a long time.
I'm working on Micronaut now.
I'm used to using Sleuth to print trace and span IDs automatically on logs.
What is the sleuth equivalent in Micronaut?
If there is no equivalent, how to print the trace and span IDs in Micronaut using Jaeger?
I want to inject secret values to Kubernetes crd.
For example, suppose I have Jaeger crd yaml file, and as the Elasticsearch server-url, password are secret values, I want them to be injected using Vault.
When using Deployment, I can inject the secrets using Vault secret, by first creating secrets and loading them from envs in container.
However, as crd cannot be done that way, I don't know how to inject the values from outside securely in code.
Any ideas?
I have installed istio using the official reference as on  Getting Started  page.
Below are the commands i used: 
 $ curl -L https://istio.io/downloadIstio | sh -
$ istioctl install --set profile=demo
$ kubectl label namespace default istio-injection=enabled
I ended up with below version of istio:
and my kubernetes version is:
Every thing seems fine until i verify the objects installed in  istio-system  namespace
As, you can see there are few missing components -
There are few  pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc.
These components were  available in 1.4.2.
In 1.4.2 installation I could see  grafana, jaeger, kiali, prometheus, zipkin dashboards.
But these are now  missing .
Example:
Is this expected behaviour in 1.7.2 or is my installation broken.
If the installation is broken, how else can i fix it.
After all I followed the instruction from the Starter Guide.
Im having some issues here with Opentracing and Jaegertracing when it comes to C#.
I have had this working before, but with Java projects.
So I start to wonder what Im missing when it comes to C# .NET Core web service.
This is my class to start my tracer to be used
Controller code that should report to the Jaeger agent and collector for show in the UI.
Startup.cs
But this is not working at all.
What am I missing here to get it work with a remote server?
I am using Spring Boot 2 Microservices with Spring Cloud Sleuth with the Dependency Management and Spring Cloud Version Greenwich.SR2.
My service is running in an Istio service mesh.
Sample policy of istio is set to 100 (pilot.traceSampling: 100.0).
To use distributed tracing in the mesh, the applications needs to forward HTTP headers like the X-B3-TraceId and X-B3-SpanID.
This is achieved by simply adding Sleuth.
All my HTTP request are are traced correctly.
The sidecar proxies of Istio (Envoy) send the traces to the Jaeger backend.
Sleuth is also supposed to work with Spring WebSocket.
But my incoming websocket requests do not get any trace or span id by sleuth; Logs look like [-,,,].
1.
Question: Why is Sleuth not working for websocket?
My WS-Config:
My clients are able to connect to my Service via Websocket.
I am implementing WebSocketHandler interface to handle WS messages.
To achieve that my WS connections are logged by Sleuth, I annotate the method that handles my connection with @NewSpan:
With this, Sleuth creates trace and spanId and also propagates them to the other Services, which are called via the restTemplate in this method.
But HTTP calls are not send to Jaeger.
The x-B3-Sampled Header is always set to 0 by the sidcar.
2 Question: Why are those traces not send to the tracing backend?
Thank you in advance!
I am using rabbitmq as one of the microservice and for that I want trace the rabbitmq spans,
I have used following dependencies for tracing the rabbitmq spans through opentracing,
I am getting only producer side spans for this microservice.
producer
I want to get end to end tracing for the request which passed through multiple micro services and one of them is  rabbitmq
like  microservice1==&gt;rabbitMQ(Producer)==&gt;Microservice2==&gt;rabbitMQ(Consumer)==&gt;Response Service
How can I achieve this kind of tracing in jaeger UI?
I have Kubernetes cluster with multiple Java services deployed to AWS with Istio.
I built a ServerInterceptor where I had the Istio needed B3 headers to the gRPC context.
I then implemented a ClientInterceptor where I parse those headers from the gRPC context and insert them into the outgoing headers.
I also implemented OpenTracing Server and Client Tracing Interceptors per:  https://github.com/opentracing-contrib/java-grpc
When I view the traces in the Jaeger UI, all I am seeing is spans reported by the sidecar.
They are properly linked as it gets called from service to service, but they are always of type "client."
I don't see any of the spans from when I call the database.
It seems the context is not transferred from the side car to my actual app.
Only sidecar to sidecar to sidecar, etc.
When I run the stack locally in a simple docker compose (without kubernetes and istio) configuration, I see the server spans being created and reported.
How do I get my Java gRPC server to expand on the spans created by the Istio side cars?
I am trying to set up distributed event tracing throughout out microservice architecture.
Here is some preamble about our architecture:
Traefik load balancer that forwards request to the appropriate backend service based on the route pathname.
Frontend application on a "catchall" route that is served whenever a route is not caught by another microservice.
Various backend services in node/dotnetcore listening on  /api/&lt;serviceName&gt;
traefik is setup with the  traceContextHeaderName  set to  "trace-id" .
How I imagine this would work is that the frontend application receives a header "trace-id" from the load balancer with a value that can be used to "link" the spans together for requests that are related.
Example scenario:
When a customer loads attempts to sign in, they make a request for the web application, receive and render the HTML/CSS/JS, then the subsequent requests to  /api/auth/login  can be POSTed with the login data and the value of the  "trace-id"  header supplied by traefik.
The backend service that handles the  /api/auth/login  endpoint can capture this  "trace-id"  header value and publish some spans to jaeger related to the work that it is doing to validate the user.
What is happening:
When the request is made for the frontend HTML, no  "trace-id"  header is received so any subsequent spans that are published are all considered individual traces and are not linked together.
traefik.toml:
I understand that StackOverflow is not a "code it for me" service.
I am looking for guidance on what could possibly be going wrong as I am new to distributed event tracing.
I have tried googling and searching for answers but I have come to a dead end.
Any help/suggestions on where to look would be greatly appreciated.
Please let me know if I am barking up the wrong tree, approaching this incorrectly, or if my understanding of how the  traceContextHeaderName  should work is incorrect.
Using MassTransit.RabbitMQ v5.3.2 and OpenTracing.Contrib.NetCore v0.5.0.
I'm able publish and consume events to RabbitMQ using MassTransit and I've got OpenTracing working with Jaeger, but I haven't managed to get my OpenTracing TraceIds propogated from my message publisher to my message consumer - The publisher and consumer traces have different TraceIds.
I've configured MassTransit with the following filter:
I'm not actually sure what the listener name should be, hence "test".
The  documentation  doesn't have an example for OpenTracing.
Anyways, this adds a 'Publishing Message' span to the active trace on the publish side, and automatically sets up a 'Consuming Message' trace on the consumer side; however they're separate traces.
How would I go about consolidating this into a single trace?
I could set a TraceId header using:
but then how would I configure my message consumer so that this is the root TraceId?
Interested to see how I might do this, or if there's a different approach...
Thanks!
I am testing Istio 1.1, but the collection of metrics is not working correctly.
I can not find what the problem is.
I followed  this tutorial  and I was able to verify all the steps without problems.
If I access prometheus I can see the log of some requests.
On the other hand, if I access Jaeger, I can not see any service (only 1 from Istio)
Grafana is also having some strange behavior, most of the graphs do not show data.
On  https://opentracing.io/  they state that opentracing API is:
A Vendor-neutral APIs and instrumentation for distributed tracing
Okay great but what does that actually mean in the context of an actual application?
What parts does this Opentracing API actually consist of, what is its purpose and how does it interact with other logging related systems like "zipkin" and "jaeger"
Is using  Opentracing API for Java   a requirement to be able to claim "My App supports" opentracing?
Is there one Opentracing protocol (e.g data send over the wire) or are they just saying opentracing is a middle layer which allows multiple other tracing frameworks to interoperate with each other?
Especially  this diagram  makes me think that.
I want to implement a jaeger installation with persistent storage using elasticsearch like backend on my Kubernetes cluster on Google cloud platform.
I am using the jaeger kubernetes templates and I am starting with elasticsearch  production setup .
I've downloaded and modified the  configmap.yml  file in order to change the password field value and the  elasticsearch.yml  file in order to fix the password value which I've changed.
My customized  .yml  files has stayed of this way:
configmap.yml
elasticsearch.yml
And then, I've created the kubernetes cluster configuration with the new password value from my machine to my KGE via  kubectl  command
And I've created the elasticsearch service via StatefulSet specialized pod  (also with the new password value) from my machine to my KGE via  kubectl  command
I can see that I have the elasticsearch service created on my GKE cluster
And I have the  elasticsearch-0  pod which have the docker container of elasticsearch service
But when I can detail my pod on KGE, I see that my pod have some warnings and is not healthy ...
I get the pod description detail and I get this warning
Here, some part of my entire output to  describe  command
I go to the container log section on GCP and I get the following:
And in the audit log section I can see something like this:
If I try with the original files and I change the password via KGE on GCP I get this error:
After that I've create a pod, is not possible update or perform some changes?
kubectl apply -f .....   ?
...
I suposse
How to can I change the elasticsearch password?
If I want configure a persistent volume claim on this pod, can I perform this before the  kubectl create -f command and my volume and mountPath will be created on container and KGE?
If somebody can point me in the correct address, their support will be highly appreciated.
I have made a trivial 3 tier services similar to the bookinfo app on the Istio site.
Everything seems to work fine, except for the tracing with zipkin or jaeger.
To clarify, I have 3 services S1, S2, S3, all pretty similar and trivial passing requests downstream and doing some work.
I can see S1 and S2 in the trace, but not S3.
I have narrowed this down a bit further, when i use Istio version 0.5.0, I can see S3 in the trace as well, but only after some time, however, with Istio version 0.5.1, I can only see S1 and S2 in the trace, even though the services are working properly and the calls are propagating down all the way to S3.
The only difference that I can see, which I am not sure if this is even an issue or not, is this output in istio-proxy for S3 using istio version 0.5.0, but not in 0.5.1
"GET /readiness HTTP/1.1" 200 - 0 39 1 1 "-" "kube-probe/1.9+" "0969a5a3-f6c0-9f8e-a449-d8617c3a5f9f" "10.X.X.18:8080" "127.0.0.1:8080"
I can add the exact yaml files if need.
Also, I am not sure if the tracing is supposed to be coming from istio-proxy as it shows in the istio docs, but in my case, I do not see istio-proxy but rather istio-ingress only.
So I'm pretty new to golang and i'm struggling to get a working example going of encrypting some text with openpgp and decrypting it again.
Here is what I have so far: ( https://gist.github.com/93750a142d3de4e8fdd2.git )
This is based off of  https://github.com/jyap808/jaeger
When I run it, it seems to partially work, but only outputs some of the characters of the original string... Changing the original string causes some very weird issues.
Clearly there is something I'm not understanding, so would appreciate any assistance given.
I am using opentelemetry api and sdk version 1.0.0 in python and Jaeger to see traces.
I have two services that communicates between each other and I can see traces for each service individually on Jaeger but spans are not nested (while they should).
This snippet show you what I do to propagate the trace between the services.
In previous opentelemetry versions (0.7b1), I could use directly  ctx_parent  without using  set_span_in_context  and it was working fine (I visualized nested spans on Jaeger), but unfortunately they removed the packages from pypi so I can not build anymore my project...
Thanks for any help !
I am trying to query the traces Cassandra table which is part of the Jaeger architecture.
As you can see the refs field is a list:
from the python code:
I am working on adding opentracing in our micro services, using Jaeger.
I have two GRPC server and one REST server.
The default opentracing with perfectly fine with both GRPC server and all the rest-grpc request are tracked under one parent span.
With Java GRPC I am able to add custom child spans and it appears in perfect hierarchy in the Jaeger UI.
But When I am trying to add same custom child in Go Lang, it is not added to the parent Rest Service span which has called the GRPC service.
Below is the golang code
I do not want to do the whole http headers extraction, as that is already taken care by GRPC library.
Even with java GRPC I do not do any extraction.
The scope manager that I use with opentracing is not available with go lang opentracing.
Thanks in advance!!
!
Cheers.
OS : ubuntu 18.04
ceph : octopus
jaeger : master
When I implement jaegertracer in the function that is responsibe for writing file to ceph via RGW, I am unable to upload my file Im getting this error
But when I remove my tracer from the code it uploads the file successfully
source code
When I remove the tracer it compiles fine again
I have activated  opentracing  on my Spring Boot micro-service application using  Jaeger  as a collector and it all works fine.
I manage to get a full trace of my calls from different components, it is very useful to understand the calls to the application.
Now, in the scope of performance testing, I need to generate statistics from the different readings.
That is e.g.
average time of traces during a time period or number of occurrences of a specific span.
Is there any tool to achieve that?
Is there a standard query language/api/tool to allow to extract big numbers of opentracing metrics?
I'm using the Open Tracing Python library for GRPC and am trying to build off of the example script here:  https://github.com/opentracing-contrib/python-grpc/blob/master/examples/trivial/trivial_client.py .
Once I have sent a request through the intercepted channel, how do I find the trace-id value for the request?
I want to use this to look at the traced data in the Jaeger UI.
I'm working with a few services in a kubernetes cluster.
I'm trying to implement telepresence to allow local debugging of code in the cluster, or potential changes in pull requests.
The services in the cluster are running SpringBoot REST services.
I have a simple test case of using curl to reach a REST endpoint running in the cluster.
I can reach it successfully without telepresence.
I'm on a Win7 laptop, running an Ubuntu VM with NAT networking.
I can run the telepresence command line, whose "--run" section runs "mvn spring-boot:run".
This defaults to proxy method "vpn-tcp".
The service appears to start up fine.
I can hit the service endpoint with "localhost:8080" successfully.
However, if I rerun the test case to reach the service in the cluster, it fails with a 502 (Bad Gateway).
When I run telepresence, I can watch it replace two pods running the springboot image with a single pod running the telepresence image.
I've looked at the detailed properties of the service and pods both before and after running telepresence, and I don't see any obvious issues in the minor differences.
If I then kill the telepresence process, it eventually restores the original pods and my test case works again.
Note that I'm currently doing this testing while connected to our corp network with VPN.
The instructions in the telepresence docs say to not mix "vpn-tcp" with another VPN.
I'm not sure if this is relevant.
The first time I tried this test, I was in the office, not on VPN, and I saw the same results.
I also tried changing the proxy method to "inject-tcp".
This resulted in the SpringBoot service failing to start, referring to a Jaeger client that couldn't connect to a server.
If it matters, here is the telepresence command that I'm executing (reverting inject-tcp change) and some initial output:
I'm looking for ideas to move forward from this.
I have a Java application built using Play 2.5x.
I am using AspectJ with Kamon to profile methods in my selected packages, and am reporting the execution details in Jaeger.
It works fine with synchronous methods but fails when it comes to reporting asynchronous ones.
For example, if I have a thenApply block which is executed after a future completes, then I cannot profile the lambda inside the thenApply in the current scheme of things.
I do not want to add any extra code to my Play app and want the aspect to take care of the profiling.
Any help will be greatly appreciated :)
I'm trying to crawl a specific page of a website ( https://www.johnlewis.com/jaeger-wool-check-knit-shift-dress-navy-check/p3767291 ) to get used to Scrapy and its features.
However, I can't get Scrapy to see the 'li' that contains the thumbnail images on the carousel.
My  parse  Function currently looks as follows:
No matter what Scrapy isn't "seeing" the li.
I've tried viewing the page in a scrapy shell to check Scrapy could see the images and they are showing up in the response for that (so I'm assuming Scrapy can definitely see the list/images in the list).
I've tried alternative lists and I've got a different list to work (as per the comment in the code).
My only thoughts are that the carousel may be loaded with JavaScript / AJAX but I can't be too sure.
I do know that the list class will change if it is the selected image from "li.thumbnail-slide" to "li.thumbnail-slide thumbnail-slide-active" however, I've tried the following in my script to no avail:
Nothing works.
Does anyone have any suggestions on what I may be doing wrong?
Or suggest any further reading that may help?
Thanks in advance!
EDIT:  I have edited in the output of the program.
The program calls for estimating a given value mu.
User gives a value of mu, and also provides four different numbers not equal to 1 (call them w, x, y, z).
The program then attempts to find an estimate of the mu value by using the de Jaeger formula.
If I enter values of 238,900 for mu, and w=14, x=102329, y=1936, z=13
then the value of estimate should be 239,103, and the error about .08%.
My code with the for loops works perfectly fine:
Output:
However, with the while loops, I am unable to replicate this.
Output:
I am creating an Excel sheet that will show when customers have paid and when they are late.
With the help of a friend, I was able to come up with a formula that will tell us if a customer is past their due date by highlighting the cell red.
As far as I've seen, there's no way to get conditional formatting to change the blank cell to say "LATE", so I was suggested to try out VBA.
I have been able to change the colors and add the words I need, but how to get VBA to check the dates like the conditional formatting formula?
Here is the formula and what I have in my VBA sheet:
Formula
VBA
I used some code I found online (thank you Rolf Jaeger from nullskull.com) for this so I'm not 100% if I need everything there, but it seems to be working fine, so I left it for now until I learn more about VBA.
I just started using OpenTracing with the Jaeger Cloud starter.
For this I added the folllowing dependency to my project:
The  io.opentracing.contrib.spring.integration.messaging.OpenTracingChannelInterceptor  of the  opentracing-spring-messaging  project adds the Scope (ThreadLocalScope) to the message header.
The Solace binder output message handler only supports header values that are instances of Serializable.
So it doesn't work out of the box.
I implemented an aspect around this message handler that looks like this (just parts of it):
With this hack it works.
What would be a proper way to avoid this hack?
Another implementation of TextMap?
Which one?
Update:
I found a way, but I'm not sure this is the right way:
}
After going through different blogs/docs, we came to know that we can skip agent and send spans to jaeger collector from client.
But I am thinking of skipping collector and trying to send spans from jaeger agent to UDP server like fluentD.
Is it possible ?
ANy pointers will be great help
I want to trace the whole project with Opencensus and Jaeger.
I added HTTP trace in entry services and add   stratspan  in middleware surrounded whol my services and this two-span called and show on Jaeger.
My problem is each service contain a lot of function and I want see a trace of all my functions but in this way not show overall service not shown each function.
I don't like add per function add one  stratspan .
I use  ctx context.Context  entry all my function but not different!
https://www.eclipse.org/che/docs/che-7/tracing-che/  has a number of  environment variables to set to enable tracing.
Is it possible to set these when working on OpenShift?
If so; where would the correct place be; and where will the Jaeger interface be available?
Thanks
I am getting the following error while creating a gateway for the sample bookinfo application
Internal error occurred: failed calling admission webhook
  "pilot.validation.istio.io": Post
   https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s :
  Address is not allowed
I have created a EKS poc cluster using two node-groups (each with two instances), one with t2.medium and another one is with t2.large type of instances in my dev AWS account using two subnets with /26 subnet with default VPC-CNI provided by EKS
But as the cluster is growing with multiple services running, I started facing issues of IPs not available (as per docs default vpc-cni driver treat pods as an EC2 instance)
to avoid same I followed following post to change networking from default to weave
https://medium.com/codeops/installing-weave-cni-on-aws-eks-51c2e6b7abc8
because of same I have resolved IPs unavailability issue,
Now after network reconfiguration from vpc-cni to weave
I am started getting above issue as per subject line for my service mesh configured using Istio
There are a couple of services running inside the mesh and also integrated kiali, prometheus, jaeger with the same.
I tried to have a look at Github ( https://github.com/istio/istio/issues/9998 ) and docs
( https://istio.io/docs/ops/setup/validation/ ), but could not get a proper valid answer.
Let me if anyone face this issue and have partial/full solution on this.
My application (hosted in a Kubernetes cluster with Istio installed) does NOT propagate distributed tracing headers (as described  here ).
My expectation is that istio-proxy should still generate a trace (consisting of a single call) that would be visible in Jaeger, even though of course the entire chain of calls would not be stitched together.
However, that doesn't appear to be the case, as I'm not seeing any calls to my application in Jaeger.
In attempt to troubleshoot I have tried the following:
I have enabled tracing in Mixer's configuration, and I can now see Mixer's activity in Jaeger UI (but no traces of calls to my application still).
I'm new to Istio, and it appears I have run out of option.
First off, is my expectation correct?
Am I supposed to be seeing traces - each consisting of a single call - in Jaeger UI when the application doesn't propagate distributed tracing headers?
If my expectation is correct, how can I troubleshoot further?
Can I somehow verify Envoy configuration and check that it's indeed tracing data to Mixer?
If my expectation is incorrect, can Istio's behavior be overridden so that I get what I need?
Thank you.
I log application tracing informations using Jaeger.
Do I need to use other log package again?
I've created an elasticsearch service to apply it like backend to jaeger tracing, using this  guide , all over Kubernetes GCP cluster.
I have the elasticsearch service:
And their respective pod called elasticsearch-0
I've enter to my pod configuration on GCP, and I can see that my elasticsearch-0 pod have limited resources:
And then, I want to assign it specific CPU request and CPU limit  according to the documentation , and then, I proceed to modufy the pod manifest, adding the following directives:
- cpu "2"  in the  args  section:
And I am including a  resources:requests  field in the container resource, in order to specify a request of 0.5 CPU and  I've include a  resources:limits  in order to specify a CPU limit of this way:
My complete pod manifest is this (See numerals 1,2,3,4 and 5 numerals commented with # symbol):
But when I apply my pod manifest file, I get the following output:
The complete output of my  kubectl apply  command is this:
How to can I modify my pod yaml file in order to assign it more resources and solve the  kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu request for container elasticsearch'  message?
What is the difference between  span.kind=server  and  span.kind=client in terms of OpenTracing?
How do I know which one to pick?
What does exactly it mean?
E.g.
there is my service Foo which is initially called by an external service Bar.
I start tracing on my Foo side and the logic is to call another service Buzz in my system to continue the flow chain.
I would rather assume that external service Bar is a  client , but I can't start my trace from there.
What would you suggest - start tracing as  client  in my service Foo and then just consider all following services as servers?
Setup: k8s microservices, jaeger metrics.
I already have some services in my k8s cluster and want to mantain them separately.
Examples:
Is it possible to use existing instances instead of creating istio-specific ones?
Can istio communicate with them or it's hardcoded?
I have deployed ingress-nginx helm chart ( 3.20.1 ,  https://github.com/kubernetes/ingress-nginx/tree/master/charts/ingress-nginx  ) into k8s cluster.
Some of the Ingresses configured for applications use basic auth - which works as expected.
I.e.
I can access some applications in the cluster without basic auth ( as configured )
and some applications require dedicated basic auth credentials ( as configured ).
But trying an unknown/unsupported URL/Path is handled awkward by default-backend.
Somehow the default backend is now requiring basic auth too ( as one of the ingress with basic-auth ) ?
How can I find out why/where this basic auth kicks in when it shouldn't ?
I do want default-backend to serve error pages for 404 without basic auth !
Currently deployed Ingresses:
E.g.
trying a simple curl on an unknown path ( where I would expect default backend to given 404 response ) returns data that is coming from my Prometheus endpoint being basic auth protected :
(The real hostname/ip has been obfuscated by me !)
or similar we trying on the other host I'll get 401 from something that looks like my Jaeger endpoint instead of default-backend 404:
I have a web application APPA that calls a webservice  master  that calls another webservice  slave .
To corelate the traces, I'm using opentelemetry.
When  master  calls  slave , I can see the traces in Jaeger, but when the web application calls  master  I see that only the web application is called and no traces are corelated with the webservice and I can see separatly the traces from the 2 webservies correlated:
but when I cann the web application, I see only the traces from the web application and not all the 3 components:
Here is what I am doing in the web application:
What am I doing wrong in corellating all 3 components?
I have a Apache Camel application with an  Aggregate EIP , which uses a  JdbcAggregationRepository  to persist the exchanges.
I also use the  OpenTracing component  with Jaeger to trace my application.
But after aggregation the parent span of the following span is wrong.
Jaeger UI
The span of route with direct  output  should be a child of one of the spans of route with direct  aggregate .
Spring Boot application
Logs
See also  Trace/Span Identity .
Research
If I remove  .to(file(&quot;d:/tmp/backup&quot;))  from my route, I also lose the trace ID.
The span for the route with direct  output  uses a new trace ID.
Therefore, it is seperated from the other spans.
Question
How can I preserve the parent span after an aggregation?
in a single application i can easily create (nested) spans but i am trying to trace http requests throughout multiple services and nothing i try with context propagation is working.
so maybe my setup is wrong.
can someone please explain the exact requirements for this in python?
so as far as i understand, in each microservice i have to setup my span exporter, the collector and trace providers.
after that is done in the classes that i want to instrument, i would just navigate to the block of code in my service that i wanna trace:
when i start my service the traces work perfectly and i can see them on my Jaeger UI but the spans are not aggregated under a single trace for that one individual http request.
I know there should be some context propagation in there to achieve this, but i can't find any way to get this done in python.
I'm using the W3C ContextTrace but i can't get it to work and i'm not really sure if i should setup a trace provider in every service or what is wrong exactly.
I've read a lot of documentation but i still don't know how to get this to work.
I'm attempting to configure the open telemetry collector in Kubernetes.
I took the jaeger all in one deployment which is here:  https://www.jaegertracing.io/docs/1.22/opentelemetry/  and ported it to kubernete running on my minikube.
The problem is I can't seem to get the open telemetry collector to receive the jaeger traces and send it to my proxy container.
My jaeger all in one app seems to be working in my minikube instance.
Traces are being sent through the hot rap app and I can view the traces in the jaeger UI.
My open telemetry collector looks like the following:
It doesn't seem that the open-tel collector is even receiving the jaeger traces.
The logs from the container are below..
Even when I send a ton of jaeger traces nothing ever seems to be received by the collector.
Is there a way to debug further or a configuration I'm missing?
Any help would be greatly appreciated.
I am trying to implement a solution to the heat equation (in 1-D) utilizing Python's Fast Fourier Transform (FFT).
The objective is an efficient numerical scheme to account for a time-dependent temperature specified at the boundary of a semi-infinite domain.
This is known in the heat-transfer literature as Duhamel's problem (see, e.g., Carslaw and Jaeger, 1959, sec.
2.5).
An exact solution is known for the forcing at the boundary specified as a function of time.
In the present case, I seek the response to a boundary history given as discrete data collected at a regular interval.
Taking the Fourier transform of the heat equation and the appropriate boundary conditions
( !
[equation] https://latex.codecogs.com/gif.download?T%280%2Ct%29%20%3D%20f%28t%29%3B%20%5Clim_%7Bx%20%5Crightarrow%20%5Cinfty%7D%20T%28x%2Ct%29%20%3D%200  ) and initial condition ( !
[equation] https://latex.codecogs.com/gif.download?T%28x%2C0%29%20%3D%200  ) with respect to time, the problem has a solution given by !
[equation] https://latex.codecogs.com/gif.download?%5Ctilde%7BT%7D%20%3D%20%5Ctilde%7Bf%7D%28%5Calpha%29%20%5Cexp%20%5B-%28i%20%5Calpha%20/%20%5Ckappa%29%5E%7B1/2%7D%20x%5D  , where the tilde indicates the Fourier transform of the function, alpha is the transform variable (frequency), and kappa is the thermal diffusivity.
So, the scheme is to take the FFT of the boundary data, multiply by
!
[equation] https://latex.codecogs.com/gif.download?%5Cexp%20%5B%20-%20%28i%20%5Calpha%20/%20%5Ckappa%29%5E%7B1/2%7D%20x%5D  , and invert (using IFFT) to obtain T.
I have constructed a benchmark problem with which to validate my coding.
In particular, I consider the forcing function !
[equation] https://latex.codecogs.com/gif.download?f%28t%29%20%3D%20a%20%5Cexp%20%5B%20-%20b%5E2%20%28t-t_0%29%5E2%5D  , where a, b, and t_0 are fixed constants.
This function can be put into the classical Duhamel solution and the result can be evaluated by numerical integration.
In addition, the FT of this function has a simple, closed form, which can be substituted into the solution given in the foregoing paragraph, and the FT of the solution can be inverted by direct numerical integration.
Results of these two independent evaluations are identical, verifying the FT solution given above.
I have attempted to code this problem for the above forcing function given by discrete data sampled at a regular interval of time.
I call Python's FFT routine to generate
the FT of the forcing function from the data.
Then, I multiply by the decaying exponential in x, and finally call IFFT to invert for the temperature history at a given fixed value of x.
The code returns values that exhibit some of the right qualitative behavior (e.g., delayed arrival of the peak temperature;  temperature tailing off at late time), but is way off both quantitatively (e.g., temperature much too high) and qualitatively (e.g., temperature at early time is not asymptotically zero).
I am clearly doing something wrong in my coding of this problem, but I have been unable to identify the issue.
A possible error may be found in the way I specify the frequency in the decaying exponential.
I would greatly appreciate any help that someone more familiar with the use of Python's FFT and IFFT can offer.
My code follows:
We have a Nodejs based microservices running in our on-prem kubernetes v1.19 with Istio v1.8.0.
What I would like to achieve is trace or display the external API calls in Kiali where we have Jaeger clients for each microservices and able to trace internal traffics.
But so far I could not able to trace any external API calls hits from any microservices.The only thing that I can see the traffic for proxy in Kiali's graph overview.
We have a cooperate proxy, and each container have env proxies set for both http_proxy, https_proxy.Any external service accessible via a cooperate proxy thus traffics should go through the our cooperate proxy first.
We have a secured gateway with TLS and we do not have egressgateway where only have istio-ingressgateway.
So is there anyway to trace external traffics likewise the internal traffics inside cluster?If yes what might be the missing thing?
Here are the ServiceEntries and VirtualServices that I created where I would like to use the retry feature as well the calls for proxy and externalAPI
I am trying to use OpenCensus and Linkerd.
Though Linkerd has an option to automatically provision OpenCensus and jaeger in its namespace, I don't want to use them.
Instead, I deployed them independently by myself under the namespace named 'ops'.
At the end (exactly 4th line from the last) of the the official  docs , it says,
Ensure the OpenCensus collector is injected with the Linkerd proxy.
What does this mean?
Should I inject linkerd sidecar into OpenCensus collector pod?
If so, why?
For example, let's say I've configured the default namespace like this.
my-opencensus-collector  is in  ops  namespace, so I put  .ops  at the end of its service name, resulting  my-opencensus-collector.ops:12345 .
And the dedicated service account for the OpenCensus collector exists in  ops  namespace, too.
In this case, should I put the namespace name at the end of service account name as well?
Which one would be right?
or
Thanks!
I am trying to send 2 different microservice data to my web service using open telemetry-javaagent ,one with jaeger exporter and the other with otlp, it looks like all the traces of jaeger are sent successfully and otlp are dropped as my web service only support HTTP protocol And javaagent has otlp grpc exporter.
I am not able to find any other agent which would exporter data using otlp in HTTP format.
are there any references through which I can get such an agent
Everyone,
I have the following inquiry:
Java 11 is used along with Spring-boot, Cassandra 3.4, JRPC and Jaeger.
All queries to the database are made solely by Spring, with annotation.
Thus, there are no further queries made, clusters or sessions in the code.
How to track Cassandra with Jaeger?
very new to .net and this seems like a simple questions.
Trying to get this code to work from the Jaeger website:  https://ocelot.readthedocs.io/en/latest/features/tracing.html
in the startup.cs file under the ConfigureServices.
I have added the proper references, but am getting an error with the:
Visual studio keeps complaining:
'Configuration' is a namespace but is used like a type
Any ideas on how to fix.
Here is what I have in startup so far:
I have a  Jaeger  running in a docker container in my local machine.
I've created a sample app which sends trace data to Jaeger.
When running from the IDE, the data is sent perfectly.
I've containerized my app, and now I'm deploying it as a container, but the communication only works when I use  --link jaeger  to link both containers (expected).
My question is:
Is there a way of adding the  --link  parameter within my Dockerfile, so then I don't need to specify it when running the  docker run  command?
I'm looking for the best strategy to deploy Jaegertracing on AWS.
My goal is to trace my Lambda functions (round about 10), which send the spans directly to the Collector over HTTP.
Therefore I don't need the Jaeger-Agent.
My question is now how I should deploy the jaeger-query and collector to AWS.
Should I rather deploy them via ECS or via an EKS-Cluster (Jaeger Operator) or is there another solution I didn't have mentioned yet?
What is the advantage of Kubernetes over ECS in this case?
Thanks in advance for your feedback!
I want to use  static target  for jaeger instead of linking to a dynamic target,
static target compiles fine but when I use it in my codebase, I see  undefined reference errors :
tried:
on  using **nm -uC libjaegertracing.a**  gives  hint for missing definitions , but I don't know why it is not getting included in the static file, while when I link to static target for example code(standalone) not with code I want to link it with, everything works fine.
I want to know how can I know and include missing library/definition so that I don't get these errors while trying to use a manually created target with libjaegertracing.a
also to verify whether I had the dependency library compiled to build static library, I used:
which gave the complied files as expected:
target that builds fine: [works]
for adding dependency:
for adding source files:
linking with example code:
with source code  [ where it doesn't work]
then:
TLDR : How can we ignore sending traces to jaeger (or zipkin) where the header is "get /**", and if that is not possible ignore sending traces where  "mvc.controller.class" is "RequestHttpRequestHandler"
We have a app which uses spring sleuth, in this app we have some requests sent to jaeger which result in get the header as "get /**" and the controller for these cases are "RequestHttpRequestHandler"
However it seems that we cant use a simply "**" path filter and after debugging the spring code a bit each specific pattern comes up in the skip section as follows
In my Rails app I'm trying to set up auto-instrumentation with SignalFx using  signalfx-ruby-tracing  gem which is using  jaeger-client-ruby .
In the output of  rails sever  I'm getting this error every 30 seconds or so:
ERROR -- : Failure while sending a batch of spans: "\x81" from
  ASCII-8BIT to UTF-8
which comes from  this part of code .
The traces are sent but I'd like to know why does this error happen because it might impact the quality of the monitoring.
Thanks!
My pods in my kubernetes cluster crashing after startup.
They are in a separate namespace (not default)
I am using microk8s and tilt
*2020-04-21 22:38:48.766  INFO 8 --- [           main] o.s.web.context.ContextLoader 
        : Root WebApplicationContext: initialization completed in 29903 ms
xargs: java: terminated by signal 9
[K8s EVENT: Pod com-config-6867587f48-9zsxs (ns: common-dev)] Back-off restarting failed container
[K8s EVENT: Pod com-config-6867587f48-9zsxs (ns: common-dev)] Back-off restarting failed container
warte auf debugger: n
Listening for transport dt_socket at address: 5005*
My microk8s.status
dashboard: enabled
dns: enabled
ingress: enabled
registry: enabled
storage: enabled
cilium: disabled
fluentd: disabled
gpu: disabled
helm: disabled
helm3: disabled
istio: disabled
jaeger: disabled
knative: disabled
kubeflow: disabled
linkerd: disabled
metallb: disabled
metrics-server: disabled
prometheus: disabled
rbac: disabled
Any suggestions why my pods don´t get a signal and Signal 9 comes up?
(Port-Forwarding is set, Firewall rules are set)
I have a c++ program which using Jaeger for tracing
I compile it using 
 g++ -std=c++1z test.cpp -I /usr/local/lib/ -ljaegertracing -lyaml-cpp  where
 /usr/local/lib/libyaml-cpp.a  is the installation path.
Error message -
I have installed  yaml-cpp-0.6.0  by downloading source code  .tar  version did  mkdir build , cd build , sudo make , sudo make install
I dont know why my compilation is failing.
I have  libyaml-cppd.so.0.6  in  yaml-cpp/build  directory and tried this path to compile but still it is failing.
OS - ubuntu 18.04
I'm trying to install the prometheus operator and inject using the sidecar.
Mutual TLS is turned on and works okay for Jaeger.
For the operator though we get a failure on the oper-admission job (see image).
I believe Istio is causing this as if I release prometheus-operator prior to istio or without istio it works okay, but then it isn't injected.
I've tried setting the following in the istio operator sidecar settings:
I've also tried to extend the readinessInitialDelaySeconds to 10s but still get the error.
Does anyone else have any ideas?
I am trying to use instrumentation in Go with Jaeger.
I am running the Jaeger backend with docker like this (as explained in  https://www.jaegertracing.io/docs/1.15/getting-started/ ):
However, after running the following Go code, I am not able to see spans in the Jaeger UI at  http://localhost:16686  and I am not sure what's wrong with this code?
I started from a similar piece of Python code and that is able to publish spans on the Jaeger UI.
I am digging in the docs here  https://godoc.org/github.com/uber/jaeger-client-go  and the ones for the open tracing project here  https://godoc.org/github.com/opentracing/opentracing-go  but I am a bit confused by the jargon and the library functions/methods.
I'm trying out the  Jaeger/OpenTracing tutorial  and finding that none of my changes to the HotROD application code have any effect.
The project structure is something like (abridged):
I start the application by running  go run main.go all .
It behaves as expected, the traces on Jaeger all match the screenshots on Medium.
I edit  services/config/config.go  to change the RouteWorkerPoolSize and MySQLGetDelay variables as directed.
Then stop the server and start it again with  go run main.go all
I'd expect these changes to be reflected in the newly running server, but they aren't.
The behaviour is the exact same as before.
It's like go is running the old code.
Am I misunderstanding something about  go run ?
Environment variables:
working directory:
Go version 1.12.6 running on Kubuntu 18.04
I am using Jaeger opentracing in an instrumented standalone non-spring java app.
Does opentacing/Jaeger expose any config or api or any other mechanism to disable it globally?
Which mechanism are you using to enable/disable opentracing if you are in the same boat?
I successfully added Apache Camel's  OpenTracing  component to my application.
I can see traces in Jaeger UI.
But the traces for the  RabbitMQ  component show only the exchange name without the routing key as operation name.
Because of my application uses only one exchange with different routing keys, I need to see the routing key as operation name in my traces.
Research
With  OpenTracing Spring RabbitMQ  I could expose another customized  RabbitMqSpanDecorator , see  Span decorator :
Note: you can customize your spans by declaring an overridden  RabbitMqSpanDecorator  bean.
(However, I coulnd't change the operation name with the  RabbitMqSpanDecorator  at all, because the operation name is hard coded to  producer  or  consumer .)
Unfortunately Apache Camel uses its own different implementation of a  RabbitmqSpanDecorator  to decorate spans.
I wrote a custom class by overiding Apache Camel's  RabbitmqSpanDecorator , but my custom class wasn't used.
Question
How can I change the operation name of a span with Apache Camel OpenTracing component for Apache Camel RabbitMQ component?
I have an application which is made up of multiple services built in proprietary language.
I want to collect traces and ingest into Jaeger or APM solution.
There is no instrumentation library.
However, these services produce traces in a proprietary format.
I want to convert these traces to OpenTelemetry traces and ingest into Jaeger or APM solution.
I started using OpenTelemetry-Java SDK.
Configured appropriate exporter.
I can see the traces in Jeager as expected.
BUT it shows only one service.
Whereas I want to depict two different services.
Is that possible using OpenTelemetry?
I am implementing service to service integration that using spring webflux.
Each microservice is isolated and running a different port.
I would like to see an end to end trace using jaeger.
The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.
Preference-service  is receiving the request and forwarding  customer-service .
Using sleuth and zipkin for collecting trace
Preference service code part
Preference service prop file
Customer-service code part
Customer service prop file
Jaeger is running in the docker-compose file
Preference service trace in Jaeger  
 Customer service trace in Jaeger
Tracing makes finding parts in code, worthwhile a developers time and attention, much easier.
For that reason, I attached  Jaeger  as tracer to a set of microservices inside Docker containers.
I use  Traefik  as ingress controller/ service-mesh to route and proxy requests.
The problem I am facing is, that something's wrong with the  tracing  config in Traefik.
Jaeger can not find the span context to connect the single/ service-dependend spans to a whole trace.
The following line appears in the logs:
The following snippets describe the  Docker Compose  setup.
This is a stripped down version of the traefik Container.
Traefik is set up using the  file provider  as base and Docker Compose labels on top of it:
We would like to use  OpenTelemetry  to collect information about our running servers.
But our primry focus is not so much about metrics per se, but actually more about the metadata.
So we would like to know which version of an application is installed (on QS, Prod, ...), which .Net assembly versions are used, which RabbitMQ messages are provided and which are consumed, REST Endpoints, gRPC-Endpoints, ...
I assume that all of the above are valid sources for OpenTelemetry metrics?
Can I use OpenTelemetry to create this sort of infrastructure registry?
Actual distributed traces and metrics are as of now actually of second concern.
What would be my GUI?
Zipkin, Jaeger, another tool?
My application extensively uses  CompletableFuture.supplyAsync(() -&gt; someService(context, args));  &amp; we rely on supplyAsync to use  ForkJoinPool.commonPool()  thread pool to get the service run in its own thread.
Is there a way to instrument someService call in open tracing without passing in a custom  Executor  as argument to supplyAsync() ?
I'm using spring and jaeger and have the below dependency
I have recently changed my Quarkus application from RestEasy to  Reactive Routes  to implement my HTTP endpoints.
My Quarkus app had OpenTracing enabled and it was working fine.
After changing the HTTP resource layer I can not see any trace in Jaeger.
After setting log level in DEBUG I can see my application is registered in Jaeger but I don't see any traceId or spanId in logs neither traces in Jaeger:
I'm using the latest version of Quarkus which is 1.9.2.Final.
Is it enabled OpenTracing when I'm using Reactive Routes?
Thanks for building this marvelous library and sample app.
I struggled with a lot of libraries but ended up using this one.
I have already started spreading the word in my circle!
I want to instrument Oatpp CRUD service using  Opentracing CPP  (with  Jaeger tracer  in place).
I have already  integrated  these into CRUD service and that is working well.
However, I want to call CRUD service from a Java app and extract the TextMapWriter so that the the traces from Java app and CRUD service could be correlated.
Do you mind suggesting some sample code to achieve this?
I know how to pass in required params from Java side.
Here's the sample Java code to inject the required headers in GET request to CRUD service.
This is only an abstract to give you an idea of how the injection is done.
I have put in certain &quot;imports&quot; to the class names so that you know where each reference is coming from.
}
I'm trying to set up a Jaeger datasource in grafana cloud.
In Jaeger datasource page - URL field is empty, my question is: Where can I find the required URL?
I have tried to write an imaginary URL, for test purpose, and when I clicked on 'Save &amp; Test' I've got no error feedback, but when I tried to pick Jaeger datasource in explore page I've got 'Failed to load services from Jaeger.
Failed to fetch'.
So I'm confused, can someone help me understand which URL should I use and where can I find it?
(I'm using grafana cloud)
Setting up jaeger tracing...So previously I had a binary executable (jaeger-agent) running on a Linux CentOS 8 box alongside a server side application.
The tracing spans were being sent to the jaeger-collector service (setup by kubernetes) on port 14250 and everything was working.
Then recently we had to reboot the jaeger tracing service due to a system crash.
Now things have stopped working and from the logs there's a &quot;504 Gateway Timeout&quot; and the agent can no longer communicate with the collector.
In AWS, we were running a kubernetes service &quot;jaeger-collector&quot; that conformed to the service here
 https://github.com/jaegertracing/jaeger-kubernetes/blob/master/jaeger-production-template.yml  with the only difference is that I'm using version 1.16.
There's no external IP with the service.
How do I use  curl  to test the communication with the jaeger-collector service?
Or do I need an external IP and perhaps that's the reason for the Gateway Timeout?
I tried using  curl  with the ClusterIP and that didn't seem to work.
I enabled sleuth in spring reactor:
In spring boot sample app declared and run my program:
Then I run jaeger and see traces coming from my app.
There are only traces with name &quot;async&quot;, and there are no tags with the key &quot;sample-key&quot;.
Help me please, how to assign tags to span in my flux?
I wanna integrate open telemetry to my node.js and I have a few questions about this project.
I am particularly interested in metrics and tracing Is it worth it to go for open telemetry or just get a Prometheus exporter and Zipkin/jaeger?
Also, I am a little bit confused about metrics in open telemetry for js.
There arent any default basic metrics that I can use?
I have created a custom exporter by Implementing the SpanExporter class of OpenTelemetry.Trace.Export in .NET Core.
I need to configure the opentelemetry traces to use this as an exporter.
I am not using any collector here I want to directly use this exporter in-process.
Earlier we were using Jaeger exporter as follows:
I need to use the custom exporter inplace of JaegerExporter now, how to configure this?
I am using Istio with tracing.
I have two REST services A1 which calls A2.
The A1 service will forward the headers received from the client to REST service A2.
However, I am not able to see the tracing in my Jaeger UI.
I am sending some random values in the header using curl to my service A1:
Another header parameters I tried also is:
However, I don't see any trace on Jaeger ui.
If just call a service A3 without passing any headers then I see that traced in the JaegerUI.
thanks
I'm using the Python opentracing module with an RPC (over HTTP) client.
At the moment I'm not interested in sending the tracing logs to an application like Jaeger - I just want to examine the span (and child spans) in the client when the RPC call returns.
So far I have this:
I found I had to use  MockTracer()  to get anything at all.
The base  Tracer()  class didn't seem to make any of the basic information ( start_time ,  finish_time ,  tags  etc) of the spans publicly accessible.
I can't currently figure out how to retrieve the updated span (in order to read any tags the server might have added) and any child spans created by the server from the results of the request.
(I'm also a bit puzzled about how the server will know what kind of child spans to create - obviously they need to be the same kind as the span that is passed in through the headers.)
In a nutshell, while reporting traces to a central server like Jaeger is useful, my purpose here is to have the RPC client print out all the server's tracing information.
(Not to say I don't want the traces in Jaeger as well but I'll deal with that once the client trace reporting is working.)
Based on the  Documentation  envoy is capable of generating and propagating the traces to the Jaeger service cluster.
It also states that
in order to fully take advantage of tracing, the application has to propagate trace headers that Envoy generates while making calls to other services.
So assuming if a client calls -&gt; service A -&gt; calls Service B, service A being proxied behind envoy.
If service A calls service B, this call from A to B would also have to go through envoy right.
So the traced Id that was originally generated by envoy when the client called Service A, wouldn't this be propagated to Service B.
Why does the application (Service A) need to forward these headers?
I have a project with microservices in kubernetes connected though rest swagger clients.
I want to make logging of all request and response payloads.
So that for each request there is an id and information about where it came from with full payload.
And for each service to service call as well.
Is it possible to do that with Istio?
There is distributed tracing tools: zipkin, jaeger.
But looks like they log only time.
Or better to handle it each app code internally?
I am using istio with version 1.3.5.
Is there any configuration to be set to allow istio-proxy to log traceId?
I am using jaeger tracing (wit zipkin protocol) being enabled.
There is one thing I want to accomplish by having traceId logging:
- log correlation in multiple services upstream.
Basically I can filter all logs by certain traceId.
My question about Istio in Kubernetes.
I have Istio sample rate of 1% and I have error which is not included in 1%.
Would I see in Jaeger trace for this error?
I kind of new to Kubernetes and Istio.
That's why can't tested on my own.
I have been playing with Istio's example of  Book Application  and I wonder would I see trace with error which not included in 1% of sample rate.
Configure Istio when installing with:
As result want to know can I see error which not included in sample rate.
If no, how I configure Istio to see it if possible?
I have installed Istio using the helm chart with the following settings:
When I check the services running in the cluster under the  istio-system  namespace I see multiple services around tracing.
Since Jaeger is the default setting, I was expecting to see only the  jaeger-collector .
It is not clear as to what the role of  jaeger-agent ,  tracing  and  zipkin  are, any ideas ?
,
I am facing difficulty in working with jaeger and Istio.
Can anyone please describe the steps that are to be followed in configuring jaeger and istio for any demo application.
I have tried a few blogs and sites but unfortunately, nothing worked for me.
if anyone could help me in this that would be great.
I have these containers running on my localhost
openzipkin/zipkin                   |   0.0.0.0:9410- 9410/tcp, 0.0.0.0:9412- 9411/tcp
omnition/opencensus-collector:0.1.9 |         0.0.0.0:1777- 1777/tcp, 0.0.0.0:8888- 8888/tcp, 0.0.0.0:9411- 9411/tcp, 0.0.0.0:32776- 55678/tcp, 0.0.0.0:55680- 55679/tcp   |
Trying to directly use the opencensus collector and my collector configuration looks like this
When I run this sample, collector logs have lots of errors
I am unable to use the collector to sends the traces from opencensus collector to the zipkin backend.
Also tried to use the jaeger backend to post the traces from collector, but I still see the same errors.
I'm studying the Opentracing Standard and reading the docs I didn't found the API default Endpoints that should be used by Tracer Providers (Jaeger, LightStep...).
Today I'm using Spring Cloud Sleuth to send metrics do Zipkin, and now I have the option to use Opentracing (brave), but How Spring Cloud Sleuth will know the correct API URL if Opentracing docs don't have a API URL standard.
i.e:  Jaeger and LightStep (both Opentracing providers) have different API URL.
I installed Istio with
I have a service that consumes external services, so I define the following egress rule.
But using Jaeger I can not see the traffic to the external service, and thus be able to detect problems in the network.
I'm forwarding the appropriate headers to the external service (x-request-id, x-b3-traceid, x-b3-spanid, b3-parentspanid, x-b3-sampled, x-b3-flags, x-ot-span-context)
Is this the correct behavior?
what is happening?
Can I only have statistics of internal calls?
How can I have statistics for egress traffic?
I have a Spring Boot app using OpenTracing and I would like to push its data to Prometheus, so I can query all metrics via Grafana (like in this tutorial  https://www.hawkular.org/blog/2017/06/26/opentracing-appmetrics.html ).
The problem is, I haven't found any consistent solution to do this, all the examples that I have found so far are outdated, deprecated or lacks documentation.
Ideally, I am looking for some solution which returns an instance of io.opentracing.Tracer, similar to what Jaeger does:
Best
I’m struggling with the last step of a configuration using MetalLB, Kubernetes, Istio on a bare-metal instance, and that is to have a web page returned from a service to the outside world via an Istio VirtualService route.
I’ve just updated the instance to
I’ll start with what does work.
All complementary services have been deployed and most are working:
I say most because since the upgrade to Istio 1.0.3 I've lost the telemetry from istio-ingressgateway in the Jaeger dashboard and I'm not sure how to bring it back.
I've dropped the pod and re-created to no-avail.
Outside of that, MetalLB and K8S appear to be working fine and the load-balancer is configured correctly (using ARP).
I can expose my deployment using:
it all works perfectly fine and I can hit the webpage from the external load balanced IP address (I deleted the exposed service after this).
If I create a K8S Service in the default namespace (I've tried multiple)
followed by a gateway and a route (VirtualService), the only response I get is a 404 outside of the mesh.
You'll see in the gateway I'm using the reserved word mesh but I've tried both that and naming the specific gateway.
I've also tried different match prefixes for specific URI and the port you can see below.
Gateway
VirtualService
I've double checked it's not the DNS playing up because I can go into the shell of the ingress-gateway either via busybox or using the K8S dashboard
http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/shell/istio-system/istio-ingressgateway-6bbdd58f8c-glzvx/?namespace=istio-system
and do both an
and
and both work successfully, so I know the ingress-gateway pod can see those.
The sidecars are set for auto-injection in both the default namespace and the istio-system namespace.
The logs for the ingress-gateway show the 404:
192.168.224.168:80 is the IP address of the gateway.
192.168.1.90:53960 is the IP address of my external client.
Any suggestions, I've tried hitting this from multiple angles for a couple of days now and I feel I'm just missing something simple.
Suggested logs to look at perhaps?
I am using Kubernetes for my project and I was using Helm to install Jaeger and Kafka in a simple way.
The problem is that regarding kafka, the zookeeper pods started correctly I have one pod that is "Pending" since the beginning.
I used this command  kubectl --namespace=default describe pod my-kafka-kafka-0  and got this information:
Warning  FailedScheduling  55m (x398 over 2h)   default-scheduler  pod has unbound PersistentVolumeClaims (repeated 2 times)
I have no idea how to solve this issue since I am new to Kubernetes and i thought that using Helm, deploying apps with it would be straightforward.
I am sorry if my question is not done in the correct way but it is also my first time here :)
Thank you in advance for your time and I would be really happy if someone could help me!
I'm trying the code for the game SquareChase shown in XNA 4.0 Game programming by Example by Jaeger.
The program is running through the VS2010 ide but the texture it is displaying is not the texture i drew.
I've checked the Content directory and that holds the correct texture so i dont know where the incorrect texture is coming from.
Can someone please help with this weird error?
I have got this code to work okay on another computer so i don't know what the problem is.
Thanks....
I'm in need of a rewrite rule for a site of mine.
I have the following type of URLS:
But I would like that to read:
This is what I think should work:
(it doesn't)
But then in addition, the  ?brand  parameter could actually appear as an  &amp;parameter  - e.g.
?word=clutch&amp;brand=TedBaker .
To summarise - I would like the brand parameter to be made into the final part of the static URL, preceded by the word 'by'.
I would like this to work regardless of where the brand parameter appears in the URL string.
Here is the current .htaccess file:
And here are all the URLs this needs to apply to:
Above need to be converted to /store/category/black-bags-by-jaeger/ or /store/category/black-bags-by-jaeger/?word=Cross%20Body (for 2nd and 3rd options).
I also have this:
 http://bag-saver.com/store/search/?brand=Jaeger  which needs to be converted to /store/search/bags-by-jaeger/.
I am exploring the various Tracing Systems.
I was looking into Light Step recently.
I have integrated my application for OpenTracing where I use the tracer from Light Step.
Now how can view the traces I am generating.
For example in Jaeger they had a ready to use docker image which can be used for quick demo.
Can somebody please help me here ?
I have parsed texts from several scientific pdf files.
All of these files contain a reference list at the end, where the authors and their publications are listed, + when and where they were released.
Also, there are cross-references in the text.
For example:
('1', ' I.  Altintas, C.  Berkley, E.  Jaeger, M.  Jones, B.  Lud-scher, and S.  Mock.
Kepler: An extensible system fordesign and execution of scientiﬁc workﬂows.
In In SS-DBM, pages 21–23, 2004. ')
and a different one from another text:
('1', ' G.  Antoniol, G.  Canfora, G.  Casazza, A.  DeLucia, and E.  Merlo,“Recovering Traceability Links between Code and Documentation,” IEEETrans.
Software Eng.
, vol.
28, no.
10, pp.
970-983, Oct.
2002.')
I was able to recognize both with the regex, which gives me 2 capturing groups besides the full-match:
The  first group  I can use to get the  number of the reference , to match with the cross-references in text
The  second  is the  everything else remaining , and i'd like to recognize the  author  and the  title of the publication  from it, regardless of the parsing format, if that's possible.
Later, I'd like to use these values to write separate  .txt  files with the author + title.txt name, and to append their cross-references found in the text to each file.
For this is what I have now for this:
It was giving UnicodeEncodeError when I was trying to create a file with mode ="a+", for a suggestion i changed it to bytes.
It's not giving me an UnicodeEncodeError, it's giving me another now:
f = open(author+".txt", "ab+")
OSError: [Errno 22] Invalid argument:  " A.  Yun chung Liu, “The Effect of Oversampling and Undersampling onClassifying Imbalanced Text Datasets,” master’s thesis,  http://www .
lans.
ece.
utexas.
edu/aliu/papers/aliu_ masters_thesis.
pdf, 2004.
J.  Cleland-Huang, R.  Settimi, X.  Zou, and P.  Solc, “The Detection andClassification of Non-Functional Requirements with Application to EarlyAspects,” Proc.
Requirements Eng.
Conf.
(RE ’06), pp.
36-45, 2006.
'.txt"
Currently the publication has everything else than the number from the references,  I'd like to switch that to author + title.txt , and I hope that would solve the error above as well.
I'd appreciate every suggestion for improvement!
I have configured InspectIT and a sample springboot application .
there is a request mapping which was configured to leak some memory .. trouble is InspectIT does not dig deep into the method calls .
the only level it goes down is doFilter-  service and then no method calls after that .. is this normal for inspectit ?
... somehow i would have expected it to dig deep down into the method calls
any help is appreciated
I'm looking for defining the regular expression on the HTTP Sensor so that as per the documentation &quot;Apply sensor regular expression on URI&quot; option is visible on the URI Aggregation view.
However, apart from the method definition i cannot see any field where i can define the regular expression for URI Transformation in UI of HTTP Sensor configuration (as shown in attached ss).
Also there is no &quot;http.cfg&quot; file present in the downloaded folders where we can do this out-of-the-box-instrumentation.
I am using InspectIT 1.9.3.107.
Could I get some more information or suggestions on this?
since i have already tried playing with http sensors and other trial n errors.
I am trying to install inspectIT in my docker container, however it seems as if they only package an installer that I must go through.
This seems rather difficult but I was thinking: is it possible to send commands to a console application?
for example if I start the installer it will get to a point like this
Can I send a command to it or script it in some way in order to install it on docker?
I have set up a Kafka &amp; Zookeeper cluster in Production.
I need to set up INSTANA APM Tool to monitor and manage the Production Kafka &amp; Zookeeper Cluster.
Did anyone  has ever used INSTANA to monitor Kafka .
Kindly share your thoughts on this.
I'm trying to integrate Instana into an application.
More specifically I'm trying to send errors from my Angular app to Instana.
I have my code 'working' so this is kind of a question about best practice.
Instana's  Backend Correlation documentation  defines functions in 'window' from what I understand.
I've set something similar to this in my index.html.
The issue I have is when I try to follow Instana's guide for  Angular 2+ Integration  regarding error tracking, where they call one of the methods that I can access from window.
The guide just straight up calls the function  ineum(...)  by itself.
When I try to do this, my project wouldn't compile.
My current fix is:  (&lt;any&gt;window).ineum('reportError', errorContext);  But I was looking at  another stack overflow question  where they accessed window differently in their javascript.
Sorry for my confusion as this may not be an actual issue because my code is 'working' but I just want clarification on this.
I'm not sure if I just didn't follow Instana's guides correctly but this was the best of what I could find.
I tried reaching out via their contact page but I haven't received a response quite yet.
The source map for CRA is enabled by default.
I have given Instana the permission to download source map from my application in production, but the errors reported are still compressed and uglified.
I guess the configuration has no effect.
Referring to  this doc .
When I do a curl for the source map from the terminal, it works.
My site is on HTTPS, but the doc says it makes an HTTP request.
Is that the root cause?
How to fix it?
Most APM solutions which analyze and store traces will sample traces to manage the volume of traces stored/analyzed (millions per day!
).
Instana claims to store and analyze all traces without sampling.
I have been doing some research around Instana, but do not see any whitepaper/blog post that hints at how they accomplish this.
Wondering if there is a whitepaper / seminar I have missed that may have hinted at this.
For example, Lightstep has a similar claim and does so through its satellite architecture - they provide a high level explanation of how they try and capture 100% of traces (temporarily).
Does the community here have any good hypothesis - high level - how Instana approaches the challenge?
I'm having some trouble monitoring my GitLab installation with Instana.
GitLab and the nginx shipped with it are running fine, only monitoring does not work.
Instana recognises the nginx but cannot get any information because it cannot pick up data from the /nginx_status location.
I added my additional configuration for /nginx_status to /etc/gitlab/gitlab.rb and installed it using gitlab-ctl reconfigure (just as the gitlab docs say).
And basically gitlab works fine and exposes the status page  http://git-test:9999/nginx_status .
My config file /var/opt/gitlab/nginx/conf/nginx.conf now has an additional include for /var/opt/gitlab/nginx/conf/nginx-status.conf at its end.
The file contents seem fine as well.
My problem now is that instana is not picking up the information exposed via /nginx_status stating that nginx needs configuration.
Status URL not found.
The nginx config file was parsed and no stub_status
  direction could be found.
This directive needs to be
  configured in order to gather nginx metrics.
The
  following snippet shows how to configure stub_status
  within an nginx config file.
To me it seems that instana is not following the include and hence not pickung up the configuration from /var/opt/gitlab/nginx/conf/nginx-status.conf.
So basically instana has no clue about the information it asks for.
Does anyone of you know how I can feed these status information to instana?
Thanks in advance guys and best regards.
Sebastian
trying to get receive informations via the Instana REST API.
Looks like that:
Getting back that error:
The matching Curl script (which I can't use) looks like that and works:
Any idea?
I'm trying to monitor a springboot microservice with instana on a docker swarm cluster.
the Microservice has 2 replicas per node on a 3 nodes cluster.
it is possible?
do i need to run instana agent image with docker run?
docker service?
any help will be appreciate.
Thanks
I am trying to integrate instana( https://www.instana.com ) with react native webview.
My app is a webapp which is being rendered inside webview.
My approach is to inject their javascript agent( https://www.instana.com/docs/website_monitoring/api ) to webview, but that doesn't seems to be working.
Any thoughts on this will be extremely helpful.
I tried installing instana agent using docker command and it works but I need it be installed using one liner command and when tried it gives error as below:
I tried on private wifi (with no proxy) but still the same.
Can anyone help on this error?
Thank you!
I'd like to start tracing my sequelize SQL calls using opentracing, but I'm having a hard time figuring out how.
I'd like to adapt this code to be more flexible so I can drop it into any sequelize project:  https://github.com/instana/nodejs-sensor/blob/626ab3c8258d4e91d42a61d79603532a921b35b4/packages/core/src/tracing/instrumentation/database/pg.js
I'm using Lightstep as a tracer, but I am using a raw tracer (not one that auto instruments) because I like the control.
Do you have suggestions on how I could add tracing to sequelize/postgres(pg)?
We are running a set of Java applications in docker containers on OpenShift.
On a regular basis we experience oom kills for our containers.
To analyse this issue we set up Instana and Grafana for monitoring.
In Grafana we have graphs for each of our containers showing memory metrics e.g.
JVM heap, memory.usage and memory.total_rss.
From these graphs we know that the heap as well as the memory.total_rss of our containers is pretty stable on a certain level over a week.
So we assume that we do not have a memory leak in our Java application.
However, the memeory.total is constantly increasing over the time and after a couple of days it goes beyond the configured memory limit of the docker container.
As far as we can see this doesn't cause Openshift to kill the container immediately but sooner or later it happens.
We increased the memory limit of all our containers and this seems to help since Openshift is not killing our containers that often anymore.
However we still see in Grafana that the memeory.total is exceeding the configures memory limit of our containers significantly after a couple of days (rss memory is fine).
To better understand Openshifts OOM killer, does anybody know which memory metric Openshift takes into account to decide if a container has to be killed or not?
Is the configured container memory limit related to the memory.usage or the memory.total_rss or something completely different?
Thanks for help in advance.
I am referring to this page:
https://www.instana.com/docs/setup_and_manage/host_agent/updates/#update-interval
Is there a way to pass mode and time from outside as environment variables or any other way beside logging into the pod and manually changing the files inside etc/instana/com.instana.agent.main.config.UpdateManager.cfg file?
Given a server APP that serves an Angular App to a web browser, an Apollo GraphQL server and a third-party tracking script by Instana.
If the tracking script is run in the browser, where do I need to set the Timing-Allow-Origin header in the response?
Does this need to point to the protocol + host of the third-party script?
I wanted to check the address that a pod uses to connect to a FTP server.
I wanted to test it by running  kubectl   -n cdol exec -it pod-namer  -- curl ipinfo.io/ip  but the connection is blocked by  an engress policy.
I know the CLUSTER-IP of the pod.
It has no EXTERNAL-IP.
I know the CLUSTER-IP of the service that pod uses.
I know the node the pod is running so I am able to check the network interfaces in Instana monitoring tool.
But all the IPs are private IPs, how can I know what IP other services sees when a pod is connecting to them?
I am running a Instana Agent on minishift.
I see these logs:
2020-05-26T03:10:58.763+00:00 | WARN  | nstana-sensor-scheduler-thread-1 | Kubernetes       | com.instana.sensor-kubernetes - 1.2.106 | Instana agent does not have permission to watch for statefulsets changes.
Kubernetes sensor will not work properly without this permission.
Please ensure proper permissions for  statefulsets  resource.
How to set proper permissions for statefulsets resource in Kubernete?
edit
I used 2 yaml files.
I used this yaml file to install operator ditto as is without any change:  https://github.com/instana/instana-agent-operator/releases/latest/download/instana-agent-operator.yaml
And I used this custom resource yaml file  https://github.com/instana/instana-agent-operator/blob/master/deploy/instana-agent.customresource.yaml  with just two changes: I added  size: 1  under spec key and replaced  replace-me  with proper agent key.
edit2
I used  oc api-resources  as that's how I created the project and deployed definition files.
Result can be found here -   https://pastebin.com/A6zRRReS
I have a  Spring Batch  application with  JpaPagingItemReader  (i modified it a bit) and 4 Jpa repositories to enrich  Model  which comes from JpaPagingItemReader.
My flow is:
All works great, but today i tried to run flow with big amount of data from database.
I generated 20 files ( 2.2 GB ).
But sometimes i got  OutOfMemory Java Heap  (I had 1Gb XMS, XSS), then i up it to 2 GB and all works good, but in Instana i see, that  Old gen Java memory  is always  900  in use after GC.
It is about 1.3-1.7Gb in use.
So i start to think, how can i optimize GC of Spring Data Jpa objects.
I think they are much time in memory.
When i select Model with  JpaPagingItemReader  i detach every Model (with  entityManager.detach ), but when i enrich  Model  with custom  Spring Data Jpa  requests i am not detaching results.
Maybe the problem in this and i should detach them?
I do not need to insert data to database, i need just to read it.
Or do i need to make page size less and select about  4000  per request?
I need to process  370 000  records from database and enrich them.
Why Cocoapods command gives this error in terminal?
Either it is the problem with my settings or I am using VMware?
Awaiting for your ideas &amp; tricks.
ERROR:  Could not find a valid gem 'pod' ( = 0) in any repository 
  ERROR:  Could not find a valid gem 'install' ( = 0) in any repository 
  ERROR:  Possible alternatives: installr, instant, instana, instacli, instapi
Background
I have a java server that is making an RPC call to a go server.
The java rpc client and go rpc server are instrumented with lightstep.
Everything about the trace looks normal except for where in the lightstep UI, the go rpc server span is placed.
The java span has ts 1493929521325 which is right before the request is sent to the go server.
The go rpc server has 2 timestamps: 1493929521326 is when it received the request and started the span, 1493929521336 is after it responded and finished the span.
Problem
I would expect the UI to have the go span horizontally to the immediate right of the java span.
Instead, it is far to the right.
The only possible cause I can think of is an incompatibility between v0.10.1 which java code is using and v0.9.1 which go is using.
Is this a possibility?
Do you have any thoughts on a possible cause?
The go code is essentially:
Before I was using  lightstep/opentelemetry-exporter-js , I can use my own exporters and Lightstep exporter at same time.
However, just saw lightstep/opentelemetry-exporter-js is deprecated and replaced by  lightstep/otel-launcher-node .
I checked the source code of it and the demo, it looks like it is a &quot;framework&quot; on top of OpenTelemetry.
Is it possible to simply use it as one of OpenTelemetry exporters?
I've inherited a legacy Spring3 application, and I'm trying to add Lightstep instrumentation to it.
I'm having trouble converting the instructions for manually configuration found here.
https://github.com/opentracing-contrib/java-spring-web
In short, I need to convert the code block below to the xml equivalent.
I've successfully created my Lightstep Tracer bean using the following dependencies.
I am trying to use  LightStep OpenTelemetry Launcher for Node.js  (which uses  OpenTelemetry Node SDK ) and  OpenTelemetry GraphQL Instrumentation  together.
However, I cannot see how to change the setup instruction for the GraphQL instrumentation so that they will work with the Node SDK.
The Node SDK does not expose a  NodeTracerProvider  instance I can pass to the  GraphQLInstrumentation  instance.
Has anyone gotten these to play together?
I'm using the latest milestone of spring-cloud-sleuth and I can't seem to get traces emitted through opentracing.
I have a  Tracer  bean defined and spring boot seems to acknowledge that, but no traces are being emitted.
Is there a way to check if spring-cloud-sleuth is aware of the Tracer bean?
update
I did see the merged documentation and have a  Tracer  instance on the bean, as defined below:
I'm not explicitly importing the OpenTracing APIs, because the LightStep tracer pulls that in transitively, but I can try doing that.
I've also explicitly enabled OpenTracing support in my application.yml file.
Couldn't see in  skywalking ui  data from  istio  metrics.
Using below guide to installed  istio  and  skywalking  backend to  Kubernetes :
 https://github.com/apache/skywalking/tree/master/docs/en/setup/istio .
Made all steps in the guide and deployed app.
Deployed app is example  book app  from  istio .
Could see traces in  Jaeger  and works good, but can't see  istio  metrics in  skywalking ui .
How I can provide metrics from  istio  to  skywalking ui  or what I need to configure to make metrics from  istio  come to  skywalking ui .
Mixer logs:
Istio policy:
somehow I have failed to connect to my  skywalking  backend, problem was with name and namespace but still getting error but now  skywalking  backend getting data but ui no updates.
Istio  telemetry:
rpc error: code = Unavailable desc = upstream connect error or disconnect/reset before headers.
reset reason: connection failure
I am using apache skywalking(7.0.0) in Kubernetes(v1.16.0) cluster to be my APM tool,but now I could not get service name in dashboard.
This is my collector config in Dockerfile:
and this is my dashboard UI:
Is something I am missing?
all data collected(Endpoint\Cache\Database\MQ) except the service.What should I do to make service data collect?
When I see the log output:
I am follow this  docs  to install skywalking using helm 3.2.1 :
but when I execute the second command:
and I create skywalking directory:
so what should I do to make it work?
This is I am trying follow:
I am now using skywalking as my apm, and now I  am configuring the address of my skywalking agent like this:
but it tells me this address is not correct.
Is skywalking agent having docker image?
What is the docker image address to use in kubernetes v1.16.0 cluster?
I am searching from internet and only find a skywalking  base image .
I am install skywalking in kubernetes cluster v1.16.8 using this command, follow  this :
all component runs fine, and I configurate the skywalking agent as sidecar in my pod like this:
but now the skywalking UI has no data, am I missing something to be configurated?
I am using skywalking 6.5.0 to monitor my apps in kubernetes cluster, this is my skywalking ui yaml config:
when the pod start, the log output like this:
I read the skywalking official issue and tell me because the mysql jdbc was GPL licence and SkyWalking is Apache license,so I must add the jdbc driver by myself, but how to add the jdbc driver jar into the image file?
I have no ideas.
I am having trouble finding any information about this in documentation.
In the config/application.yml file under  storage.elasticsearch7  I see various configuration options.
Is there a way to ensure that the indexes that get created are created using a given index template or ILM policy?
I am running the helm chart for the ELK stack and ES version 8.0.0-SNAPSHOT.
My goal is to just delete indexes from SW after 2 weeks so that my cluster doesn't run out of shards.
I have recently started exploring skywalking as APM tool.
I am interested in looking at the time spent by methods/functions at application layer.
Basically a instrumentation sort of thing for the JAVA application.
With Skywalking I just get 3 spans(methods) that have one root function and two DB execute functions.
I tried adding the property
But this dint work.
I could still see only 3 spans in the dashboard for the API being hit.
Under Profile feature I can get the thread stack.
But i am only interested with Hotspot methods.
Am i missing something in configuration?
I want classes starting with particular pattern to be instrumented and captured in trace.
How can I achieve this?
Or is there any other open source APM tool I can start with?
I am using kubernetes(v1.15.2) to manage my skywalking-ui(v6.5.0) apps,recently I found some app's not accessable but the pod is still running, I am not sure the app is works fine,there is no error output in pod's logs.But the pod status icon give tips:  the pod is in pending state .
Why the status not same in different places?The service is down now.How to avoid this situation or make the service recover automatic?
This is pod info:
I am using a inital container(k8s version:v1.15.2) to initial skywalking(6.5.0) jar file before container startup.But I could not found the file and directory the intial container create,this is my initial container define:
now the initial container execute success,I am check the log output like this:
now something I am confusing is where the directory locate?
where is the file I am copy?
I am login my container and do not find the jar file:
now I am starting my app to collection metrics data like this:
obviously it tell me could not fond the jar file error:
so what should I do to fix this?
I already searching from internet bu found no useful way to solve my situation.
jstat[option vmid[interval[s|ms][count]]]
i input jstat -gc 12285 get 12285 not found
so, why?
and how to fix it?
so, the vmid is 12285
2.but jstat -gc 12285 get "12285 not found"
I expect the output of the java JVM's heap status, including Eden, survivor, Old, Permian's status, and GCtime information
I am new to Stagemonitor, and I found that it is one of the best open source tool to get the performace metrics.
So, I tried to implement this for my tomcat server.
But I cannot get a clear documentation to implement and setup this.
The wiki provided in the github page is not enough to setup the application.
Can anyone guide me how to install this tool and make it a go.!
!
How does  Stagemonitor  compare over simple JMX metrics?
Unlike java-native JMX MBeans, Stagemonitor includes an agent that sits in your Java application, sending metrics and request traces at the central database, which can be Elasticsearch.
Since both ways can serve as input for an ELK Monitoring Stack (JMX see this blog  post ) what are the benefits of Stagemonitor?
I am trying to using stagemonitor for get metrics for different methods.
I used sample PetClinic application locally for get a idea.
I want to get metrics only what i need , not all of them is any possible to do that.
I change some code for my testing purpose
when i click test button in html it will go to this method and i can get the metrics form stagemonitor.If i not need to see this method metrics ,how to stop showing that in stagemonitor
@Felix
I trying to use browser-widget in example and i change code for testing.
This output I'm getting i don't want to see details about Testing.I only want to see some method i need, even other methods used i don't want to see about them.
is it possible  ?
?
I am trying stagemonitor with standalone applicaion.
And i get metrics for jmx reporter correctly.
I want to get that in browser-widget.
I saw config  stagemonitor.web.widget.enabled = true  will enable this.
But how to get dashbord.
Do we have to use grafana for this ?
I'm trying to get  www.stagemonitor.org  working with Grails.
I've created a sample project here:  https://github.com/jbwiv/teststagemonitor
I've added stagemonitor to grails-app/conf/BuildConfig.groovy as both a compile and runtime dependency.
It indeed gets installed to my maven directory after calling "grails refresh-dependencies":
I've also placed a stagemonitor.properties file in src/java, which at runtime gets moved to target/work/resources/stagemonitor.properties.
I believe I have that properties file configured properly.
I've installed the templates Grails uses for web.xml and modified to insure metadata-complete=true is not present in the generated web.xml.
However, after  grails run-app  and navigating to  http://localhost:8080/main/index , I get my index page as expected, but no stagemonitor icon to click and it appears no stagemonitor assets are included.
I would like to disable the browser widget in production release.
I would like to define the disabling the browser widget in  stagemonitor.properties .
Is there any java system properties like as " stagemonitor.browserwidget.activate=false "
Using a multi-tenant server is it possible to use these tools to get stats regarding specific clients performance usage.
The setup would be a MySQL DB which holds users belonging to organisations.
When the Java application is running all actions will be carried out by a User collection which has the Organisation ID variable.
Could this data then be used to work out how much CPU, memory, heap, processes etc are being used per Organisation?
Thanks
I am using stagemonitor for get metrics for different methods.
I'm using PetClinic application locally for get a idea and using browser-widget for testing.I find that we can track metrics using @Timed , @Metered annotations.
Is it possible to use them in the petclinic application and view them from browser-widget.
I have a Spring MVC web application running on tomcat.
I need to monitor my application for performance, log the time taken by each method call along with the values of the parameters.
I would need this logging for all the methods in all the controllers, services, util classes inside the application.
I have seen the question posted earlier here :  How to log the time taken by methods in Springframework?
As for the solutions proposed for that question., I have the following concerns in my case.
1) Using Spring AOP for logging - Closely matches the requirements but as far as I know it requires adding annotations to each and every method - would prefer to avoid changing current application.
2) Stagemonitor - Could not follow the installation instructions - it requires installation of docker which I could not because of OS limitation.
I am working on openSUSE 11.3 where as docker is available for openSUSE 12.3+
3) SpringInsight - It is a great tool and exactly matches my requirements.
But the problem is, it runs is vfabric-tc-server instance.
I tried setting up it on tomcat 7 using the steps mentioned by Daniel in  Using Spring Insight with Tomcat 6  but it did not workout as none of the jars in insight application has the class com.springsource.insight.collection.tcserver.ltw.TomcatWeavingInsightClassLoader which was supposed to be referred from server.xml.
Tried adding external jar but it did not work.
I'm wondering if there are any other tools which
-- will not require changing existing application MUCH - simple configuration should be acceptable.
-- will give method level performance monitoring .
-- strictly should not need to migrate the existing applications to other server.
Thanks in advance :)
I am working on Springboot REST microservice.
Facing issue while doing clear region on partition region type.
How to configure Gemfire cache so that my client should be able to add region to server while startup.
Also I am thinking of exposing an API to evict cache explicitly.
Is there a way we can do it?
We had Redis cache implemented now but we want to switch to gemfire as it offers nice GUI to track regions and data using PULSE.
Questions:
Code:
Could deploy Bosh and small footprint tanzu application service(tas) in Azure, without using the domains.All Vms are running.Can i access the ccapi and apps manager with the IP address instead of the api.SYSTEMDOMAIN?
This is a follow-up question of  How to implement HTTP request/reply when the response comes from a rabbitMQ reply queue using Spring Integration DSL?
.
We were able to build the Spring Integration application and the SCDF stream successfully locally.
We could send a http request to the rabbitMQ request queue which was bound to the SCDF stream rabbit source.
We could also receive the response back from the rabbitMQ response queue which was bound to the SCDF stream rabbit sink.
We have deployed the SCDF stream into PCF environment which had a binding of an internal rabbitMQ broker.
Now we need to specify the spring rabbitMQ connection information in the Spring Integration application properties - currently it's using the default localhost@5762, which is no longer valid.
Does anyone know how to get this rabbitMQ configuration properties?
We already checked the SCDF stream rabbit source/sink log files but couldn't find the information.
I know we probably need to check internally whoever set up the SCDF/rabbitMQ in PCF environment, but so far we haven't heard the answers from them.
Also, it appears we can have a different approach that binds both the SCDF stream and the integration application to a separate rabbitMQ instance (instead of using the existing one bundled with the SCDF configuration).
Is it a recommended solution?
Thanks,
I am autoscaling my application based on the HTTP throughput.
My question here is when it reaches min threshold it tries to reduce the instance created.
But during reducing the instance count if my instance is running or it is processing prev HTTP request.
In this case, it will wait till the processing completes or it forcibly reduces the instance count when reached threshold.
I'm trying to run E2E Kubernetes tests using Sonobuoy, but if one of the nodes has custom taints (NoSchedule) I'm getting
Kubernetes version is 1.12.
If I remove taints in my test cluster E2E tests passed successfully.
I've found that it was fixed in 1.17 version (see:  https://github.com/vmware-tanzu/sonobuoy/issues/599   https://github.com/kubernetes/kubernetes/issues/74282   https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.17.md  )
Is there a workaround to run e2e tests on production 1.12 Kubernetes cluster having node taints?
Except for waiting for 1.17 version and cluster upgrade of course.
Today I created a PCF Account (for testing purposes) and it will not let me do anything , it is telling me this:
&quot;We will no longer be accepting any new PWS account sign-ups after September 17, 2020.
Please do not hesitate to contact us with any questions.
If you are interested in an enterprise-grade service for hosting applications, Tanzu Application Service (TAS) offers this capability.
The VMware Team&quot;
Does this mean that I need to move my production applications from PCF to VMware Tanzu ?
Is PCF Dead?
So, I have followed the documentation in:
And I am still at a loss of how do I install Velero using its helm chart at  https://github.com/vmware-tanzu/helm-charts , because I cannot reconcile the helm chart based installation with what is documented in
 https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure
Here is what I have done so far:
Now I need to update the  values.yaml  file, but I am stuck at the credentials section.
I am unable to reconcile the  velero install  instructions given on  https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure  with what I see in the values.yaml file.
E.g.
I have created the  credentials-velero  file:
Of course, it does not contain the MSI credentials and the MSI name would be associated using the dedicated label.
The  doc  says clearly:
If you're using AAD Pod Identity, you now need to add the
aadpodidbinding=$IDENTITY_NAME label to the Velero pod(s), preferably
through the Deployment's pod template.
But how do I do it when I install Velero using a helm chart?
I have client application instrumented with Zipkin library with configuration in spring application.properties .
Maven dependency
The hawkular apm server console is reachable from the local machine.
However, when the rest api exposed in the client application is invoked, the zipkin trace is logged but they are not collected at the hawkular apm server.
I am not sure if its a configuration issue at client application, as the Hawkular APM UI is opening properly.
As per my understanding, Zipkin client can be integrated with Hawkular apm by simple replacing the hawkular url in place of zipkin server, but this does not seem to work.
Any suggestions on this, unfortunately I could not find any examples too.
Sleuth is not sending the trace information to Zipkin, even though Zipkin is running fine.
I am using Spring 1.5.8.RELEASE, spring cloud Dalston.SR4 and I have added the below dependencies in my microservices:
My Logs are always coming false:
[FOOMS,2e740f33c26e286d,2e740f33c26e286d,false]
My Zipkin dependencies are:
Why am I getting false instead of true in my slueth statements?
The traceId and SpanId are properly generated for all the calls though.
My Zipkin is running in port 9411
Does anybody know where the zipking examples are located ?
https://twitter.github.io/zipkin/Quickstart.html#super-quickstart 
 In the following I can read:
I have found the zipking-example on Maven Central only.
Not on Github.
1.2.1-rc24 
 It is still a bug in the documentation?
Documentation mentions zipkin-example which doesn't exist
I wanted to update my project Zipkin setup to Spring Boot 2.2.2.RELEASE and Spring Cloud Hoxton.RELEASE but it looks like simple jars update is not enough.
I thought the old setup (it was working fine for Spring Boot 2.1.5.RELEASE and Greenwich.SR2) would also work for Boot 2.2.2.RELEASE and Hoxton.RELEASE but it appears I still miss something here.
I'm getting the following exception ( java.lang.NoClassDefFoundError: zipkin2
/internal/Buffer$Writer ):
Can't see Zipkin Server when using Spring Initializer .
Has it been removed?
What is the alternative?
I have a Spring Boot app that I'm updating to 1.5.1.
It works great, until I add Sleuth and Zipkin to classpath
when these lines are present, I get
This is my dep.
management
I tried change to Dalston
but the errors get even stranger
and
am I missing something I haven't noticed yet?
Which exact dependencies and  application.yml  configuration are required for Spring Boot/Cloud Zipkin server (potentially Zipkin Stream server) to persist the tracing data using MySQL?
When I tried to integrate zipkin.
It threw this error
version:
Prerequisites: 
 Node.js  application 
 Opencensus  library 
 Zipkin Exporter  and local Zipkin service
app.js :
package.json :
Zipkin  server started locally with command:
after triggering  /service1  Zipkin Ui displays 2 spans for 2 different requests: 
first  /service1  incoming request that is configured in Node.js routers 
second  /external_service_2  is subsequent call to external service
Problem
The problem is that after triggering  /service1 : 
  1.
Zipkin UI displays 2 spans with same name  MyApplication (see image),  but expected 2 different span names
2.
As far Zipkin UI displays 2 spans with same name,  service dependencies page contains one Service only(see image)
We have been playing around with Brave(Java implementation of Zipkin) and successfully added tracing for REST and database calls.
We would like to also add RabbitMQ to the tracing and would like some thoughts from anyone who may have had similar experiences that they could share.
We have tried to find some stuff online but can't seem to find an interceptor we could add to our rabbit implementation.
Can you recommend anything?
Thanks in advance.
I read about zipkin, but from my understanding, zipkin is suitable for tracking history of network requests and time (via Finagle).
However, is it possible for me to use zipkin to track java method invocation time and location?
For example, I want to track how long it takes for  foobar()  to execute, and what are other methods internally called by  foobar()  and its execution time and so on.
I have multiservices application which is using Spring Cloud OpenFeign.
Now I have to use zipkin with that app.
I remember that when i had app without Feign I just added Sleuth and Zipkin starters dependencies and run zipkin server on port 9411.
After that Zipkin worked well..
But now, when i try same in my app with Feign i get error 500  "original request is required" .
I guess that Feign has some problems with headers when Sleuth add traces informations.
Can you help me fix this?
I'm new to zipkin and brave api for distribute tracing.
I've setup a zipkin server on my localhost listening on port 9411.
I've executed below function but there is no trace data show in my zipkin server.
Could someone point out what I'm missing?
I want to use zipkin to profile the internals of a traditional program.
I use the term "traditional", since AFAIK zipkin is for tracing in a microservice environment where one request gets computed by N sub-requests.
I would like to analyse the performance of my python program.
I would like to trace all python method calls and all linux syscalls which gets done.
How to trace the python method calls and linux syscalls to get the spans into zipkin?
Even if it is not feasible, I am interesting how this could be done.
I would like to learn how zipkin works.
So, we are using kafka queues internally for some microservices' communication, also zipkin for distributed tracing.
Would you suggest how to bring in kafka traces in zipkin server for debugability.
I came across the  brave-kafka-interceptor , but could not understand it with with kafka from the minimal example provided.
Is there any other example around, or something altogether different library is used.
I am using py_zipkin in my code.
And I can see the tracing result on the Zipkin UI.
But I don't know how to output the tracing results to a file with specified format, like a log file.
Here is a example of my code:
Small question about the possibility to integrate Zipkin with Prometheus.
Currently, we have a working Zipkin instance fully ready, with its web UI.
Zipkin is super cool, everything is fine.
We are able to have all micro services sending traces to Zipkin, and having Zipkin aggregating them.
We can also search the traces in the UI, etc, super cool.
On the other hand, we also have a very mature battle tested Prometheus Grafana, where container level metrics, application level metrics, and many other observations are already present in it.
Hence, currently, we have two places where we have to look at for production.
Our everything in one place Prometheus, and this super cool Zipkin.
I was wondering, would it be possible to have Prometheus as the back end, or some kind of Prometheus consuming Zipkin data to display in Grafana, so we truly have all in one place please?
Thank you
I am trying to add tracing on a Wildfly server (specifically Keycloak Docker image)
Following this document  https://docs.wildfly.org/19/Admin_Guide.html#MicroProfile_OpenTracing_SmallRye
I got as far as
But I can't get the next parts working to set it to point to zipkin:9411
The next command in the instructions failed
However, doing it using  /opt/jboss/startup-scripts/  also fails
Using @ehsavoie answer I got a bit further
but still does not log to zipkin which uses B3.
I also tried
When i run my spring boot application locally i always have trace information with exportable information = true
but when app running on AWS ECS in docker container i have always exportable false for all logs
except classpath  org.hibernate .
On this classpath exportable = true
I use below dependencies
And below properties :
Which may be the reason for different actions ?
Maybe log levels ?
base-url:  http://zipkin-server   must be reachable ?
i am running zipkin via docker with this command docker  run -d -p 9411:9411 openzipkin/zipkin  and accessing its server at  http://localhost:9411/zipkin/  
I am using playframework-2.4 i am not getting the service name in zipkin ui also trace data is now showing up it shows  0 of 0 services
here is my code 
application.conf
build.sbt
i am accessing  http://localhost:9000/direct-user/test1  it first then  http://localhost:9411  but trace data is not showing up 
is there any thing missing ?please help
I need to send spans through RabbitMQ to Zipkin.
I'm Using Spring-Cloud-Sleuth Edgware-SR5 version and SpringBoot 1.5.3.RELEASE versions.
With older Spring-cloud sleuth version (spring-cloud-stream-binder-rabbit -  v1.1.4.RELEASE) it was working fine.
When I try to start the service, I'm getting "  "AsyncReporter{RabbitMQSender{addresses=[localhost:5672], queue=zipkin}}.
Unable to establish connection to RabbitMQ server" error.
I have gone through the documentations, but I could't able to resolve this issue.
Gradle Configuration:
Application.yml:
Exception StackTrace:
Thanks and Regards
Suresh
I want to retrieve the TraceId of Zipkin, is there any method to get it ?
first i had a small issue with this class   brave.sampler.Sampler
could not import this class,  only imported when i added this dependency
and my big problem is, when i tried to use zipkin  for disturbed tracing, i added the required dependency but whenever i start the applications, it through an exception in start.
and this is the stack trace.
my pom.xml
i would someone to help me fix those issues, also i want to understand why this exception comes, and why the sampler class does not imported only when i add it's dependency, but i see in other projects codes there are no needs for the dependency.
I have enabled the distributed tracing using Zipkin tracer as mentioned in 
 https://github.com/openzipkin/brave  for the microservices.
I could see the service call info and time taken in the Zipkin server running in local machine.
I have usecase to capture the trace logs of every http requests alongside the JSON messages (with the Trace ,span and parent IDs) received by to Zipkin server from the client.
Zipkin server is started in debug mode to enable the logging, however the http requests are not logged in the Zipkin server logs.
Can someone throw some light please?
Trying to create zipkin server with the dependencies added in gradle as below,
Also,
i have added properties in both application.properties and bootstrap.properties files like,
application.properties
bootstrap.properties
Once i start the server and load the UI page i am getting error in UI as,
Can istio send tracing information to an external zipkin instance?
It seems like istio has hard coded zipkin address several places to expect it in the istio-system namespace(by referencing it without a namespace).
By default Spring Sleuth only sends 10% of requests to Zipkin.
By setting  spring.sleuth.sampler.percentage  you can increase the percentage.
Unfortunately it is stuck at 10% regardless of what value I set it to.
I have tried 1.0, 0.5, 1, 100.
Output from  /env
Regardless of the value, when I make multiple requests, only 10% make it to Zipkin.
We are using version Finchley.M8 of Spring Cloud and 2.0.0.RELEASE of Spring Boot.
Below are relevant POM settings.
Could this be a bug?
I want to load my Spring Cloud zipkin-server with elasticsearch.
I think, I tried almost everything I could.
but, It still running with in-memory.
(when I restart zipkin-server, all data is lost.)
I want to set up zipkin with elasticsearch.
Please tell me which exact  dependencies  and  applicartion.yml  or any other things needed.
I've enabled zipkin on my application and it works fine, I see the traces.
My application is using Consul service discovery, and I see a lot of traffic being traced in Zipkin.
Traces are like have names like "catalog-services_watch" and contain things like :
How can I disable these traces ?
I've tried the spring.sleuth.instrument.web.skipPattern parameter, but it's not working.
We have a socket based web app we currently developing using feathersJS, and we are currently leaning on using zipkin for performance tracking, but it seems that there's no instrumentation yet for socket based app, anyone have implemented Zipkin on socket based webapps?
or any alternatives  you recommend?
Thanks you so much.
I'm using these dependencies:
Is there a possibility to add the current active profile(s) to each log line?
This would make it possible to filter logs based on the profiles in Splunk/ELK/...
So instead of
it should log
EDIT: 
Based on Marcin's answer, I implemented it as follows:
application.yml
ProfileLogger.java
LogConfig.java
This prints logs like the following:
This is already good but not completely what I'm looking for yet.
I'd like to add the profile from the beginning -  even the "Started Application" should contain the profile - if possible.
Secondly, I'd like to move the  profiles  between  INFO  and  22481 .
One more question came up during implementation: In the linked implementation there is this statement:
does that mean you only send traces if log-level is set to TRACE?
If so, how could I improve logging to stdout with that approach (given a log-level of debug/info/warn)?
I think the log-pattern is overriden by Sleuth/Zipkin upon importing the dependencies and thus, local logging looks the same as tracing.
Eventually I'm interested in having the profile displayed in local stdout as well as in Zipkin.
EDIT 2:  With the help of Marcin, I have changed the pattern by introducing a  resources/logback-spring.xml  file containing these lines:
Note that you have to add a  bootstrap.yml  file too in order to have the application name correctly displayed.
Without a  bootstrap.yml  file, the above log-pattern just prints "bootstrap" as application name.
The  bootstrap.yml  just contains
in my case.
Everything else is configured in application-[profile].yml
Now everything works as desired:
I want to monitor some Windows Services with either perfino or Zipkin.
Does anyone know if that is possible?
Cheers.
Has anyone else encountered the following problem with using Zipkin &amp; Spring Cloud Sleuth?
Seems to be a problem posting out data to my localhost Zipkin server.
Is there any need to configure proxy settings on Zipkin?
I am using zipkin to track the requests across microservices.
One of my service is running jobs using a thread pool.
How do I transfer the zipkin header values to the threads?
is there a Zipkin wrapped thread pool/executor available?
I would like to implement tracing in my microservices architecture.
I am using Apache Kafka as message broker and I am not using Spring Framework.
Tracing is a new concept for me.
At first I wanted to create my own implementation, but now I would like to use existing libraries.
Brave looks like the one I will want to use.
I would like to know if there are some guides, examples or docs on how to do this.
Documentation on Github page is minimal, and I find it hard to start using Brave.
Or maybe there is better library with proper documentation, that is easier to use.
I will be looking at Apache HTrace because it looks promising.
Some getting started guides will be nice.
Did saw  this ...
But I'm not able to make it run, whatever I have tried, I get either still on localhost, either an exception on armeria bind ( I have stuff running on :8080) and server crash...
In short what I have tried (Windows Server 2016, so no Linux Docker containers ) with no avail.
Variation on these batch file commands:
SET &quot;SERVER_ADDRESS=xx.xx.xx.xx&quot; 
SET &quot;ZIPKIN_HOST=xx.xx.xx.xx&quot; 
java -jar zipkin-server-2.23.2-exec.jar --armeria.ports[0].port=9411 --&gt; armeria.ports[0].protocols[0]=http
It should be simple to run Zipkin on another ip, but I'm fighting.
Could you help me, maybe I'm missing something absolutely obvious...
But is a pretty common scenario, and is not that well documented.
We have around  10 Spring boot microservices , which communicate with each other via kafka.
The logs of each microservice are sent to Kibana, and in case of any errors, we have to sift through Kibana logs.
The good thing is:  at the start of any flow, a message-id is generated by one of our microservices, and that is propagated to all the others as part of the message transfer (which happens through kafka), so we can search for the message-id in the logs, and we can see the footprint of that flow across all our microservices.
The bad part:  having to sift through tons of logs to get a basic idea of where things broke and why.
So I was wondering if we can have some distributed tracing implemented, maybe through Zipkin (or some other open-tracing framework) that can work with the message-id that our ecosystem already produces, instead of generating a new one ?
Thank you for your time :)
Traces that should have been sent by dapr runtime to zipkin server somehow fails to reach it.
The situation is the following:
I'm using Docker Desktop on my Windows PC.
I have downloaded the sample from dapr repository ( https://github.com/dapr/samples/tree/master/hello-docker-compose ) which runs perfectly out of the box with  docker-compose up .
Then I've added Zipkin support as per dapr documentation:
When application runs, it should send traces to the server, but nothing is found in zipkin UI and logs.
Strange thing start to appear in the logs from  nodeapp-dapr_1  service:  error while reading spiffe id from client cert
Additional info - current dapr version used is 1.0.1.
I made sure that security (mtls) is disabled in config file.
I am very new to using OpenTelemetry and have just tried configuring it to send traces to my Zipkin server.
Unfortunately , after configuring the agent by specifying zipkin exporter details , I could see an exception in the console.
I used Petclic as sample spring boot and have followed the documentation here  https://github.com/open-telemetry/opentelemetry-java-instrumentation
Here is the command that I used to start spring-boot app(I have my zipkin server running at localhost:9411):
java -javaagent:opentelemetry-javaagent-all.jar -Dotel.exporter=zipkin -Dotel.exporter.zipkin.endpoint=localhost:9411 -jar spring-petclinic-2.4.2.jar
Exception in the console (It is trying to connect to gRpc exporter instead of Zipkins):
Please can you let me know what is wrong with this.
EDIT : 
I already tried passing -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin but was not successful.
I am just trying to start my spring boot app through eclipse by passing these params as jvm arguments.
Every tutorial on OpenTelemetery seems to be using Docker setup.
Please could someone help
I have a simple spring boot hello world application.
Trying to send data to the Zipkin collector.
But as per logs, it's trying to use  OtlpGrpcSpanExporter .
My application exposes a simple post rest API.
Following Opentelemetry
docs  https://opentelemetry.io/docs/java/getting_started/
[opentelemetry.auto.trace 2021-02-20 01:48:44:490 +0530] [grpc-default-executor-1] WARN io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter - Failed to export spans.
Error message: UNAVAILABLE: io exception
[opentelemetry.auto.trace 2021-02-20 01:49:14:106 +0530] [grpc-default-executor-2] WARN io.opentelemetry.exporter.otlp.metrics.OtlpGrpcMetricExporter - Failed to export metrics
io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
Please let me know if I have to change anything.
I'm using istio with ingress gateway, and added zipkin tracing.
All my apps are using spring boot with sleuth zipkin.
I've deployed 2 zipkin for testing
the spring boot configuration are pointing to the zipkin namespace, with always sampled configuration.
Problem is when I'm using ingress gateway, the trace id looks like request id and it does propagate to my sub systems.
But when I query to zipkin (deployed both in istio-system from istio documentation, and manually deployed to another namespace) the trace id are not present.
Interestingly when I do port-forward of my outer most system, and hit the spring boot with grpc, the trace id are being propagated to the sub systems, and it does show in the zipkin dashboard.
although the trace id are different when using ingressgateway and port-forward direct grpc call :
ingressgateway : 0672471566b9305f7dcaadecaf1a8c71
direct call : cdc337ec90b8c085
Thanks!
I am looking into distribution tracing tools.
Found there two very popular.
What are key differences between them ?
Which one would you recommend ?
Will you recommend other open source distributed tracking tool ?
Spring Cloud Sleuth is used for creating traceIds (Unique to request across services) and spanId (Same for one unit for work).
My idea is that Zipkin server is used to get collective visualization of these logs across service.
But I know and have used ELK stack which does necessarily the same function.
I mean we can group requests with the same traceId for visualising, using ELK stack.
But I do see people trying to implement distributed tracing with Sleuth, ELK along with Zipkin, as in these examples ( Link1 , Link2 ).
But why do we need Zipkin if there is already ELK for log collection and visualising?
Where I am missing?
I can't find a below  exportable  Span in Zipkin neither by it's traceId nor by spanId (some other spans appear, so Zipkin server seems to work)
I also can't find it's parent &quot;parentId&quot;:&quot;37eca1021fd5241c&quot; in Zipkin.
Where can be a problem?
How can I bite/debug it?
Possibly this span is in a flow, that was triggered by a rabbit message, not a rest request.
Spans from a trace that were triggered by http rest request are correctly visible in Zipkin.
But I can't find traces from flows triggered by rabbit message.
What problem could be here?
We have  slueth  in other microservices and we wants to send data to zipkin server for consolidated logging.I am trying to start my zipkin server.I am getting the following error:
I tried to use the sleuth and zipkin older versions, but getting conflicts and spring application is failing to start
We are not able to see Zipkin UI.
And  pom.xml  is like:
Any suggestions will be helpful.
I am trying to link three HTTP service hops in NodeJS together into a single Zipkin trace.
I have three services
The service  service-main  calls  service-hello , and  service-hello  needs to call  service-goodbye  to complete.
Zipkin can see these calls, but links them together as two separate traces.
( service-main  calling  service-hello , and  service-hello  calling  service-goodbye .
The services are implemented in  express , and the fetching happens via  node-fetch .
I create an instrumented service fetcher with code that looks like this
and I instrument express with code that looks like this
and finally, I create my tracer with code that looks like this
You can see the above code in context in  the following github repository .
I've done all this by cargo-culting the code samples from the zipkin github repositories, and I don't know enough about zipkin's implementation to diagnose this further.
How to I get zipkin to see the  service-main  -   service-hello  -   service-goodbye  call chain as a single trace?
I am working on micro service where micro services are communicating with each other.
I am using Zipkin with Sleuth and Apache Kafka as a message broker and running micro service and kafka using docker-compose.
I am using Spring Boot (2.2.6.RELEASE), Spring Cloud (Hoxton.SR3), Kafka Image(wurstmeister/kafka:2.12-2.4.1) and latest image of Zipkin.
When i try to spin the container, micro service is giving below error:
APPLICATION FAILED TO START
Description:
Parameter 2 of method reporter in org.springframework.cloud.sleuth.zipkin2.ZipkinAutoConfiguration &gt; required a bean of type 'zipkin2.reporter.Sender' that could not be found.
I have microservice running in AWS ECS and listens to AWS SQS messages.
I am using  zipkin-aws  to sent the traces to AWS Kinesis and collected in S3.
When there is any REST Operation, the traces are sent and collected in S3 perfectly.
But it doesnt capture the traces when the Microservice listens or send message to AWS queue.
Could anyone help in configuring zipkin to listen to SQS messages.
I have an application that is divided into a few microservices (using Spring Eureka project).
All the services are registered to Eureka Server - so that the communication between the services can be realized through a "Gateway API" (Eureka Server)
The logs produced by the services are reported to a Zipkin server that runs as a separate service.
All works as expected but when I go to the Eureka dashboard I am not able to see my Zipkin service since it is not registered with Eureka.
Question: Is it possible to register Zipkin with Eureka Server?
Here is my docker-compose file:
I am using the  Zipkin 5.6.11
I append  mysql  url:
but when I debug the project using spring boot application, it throws an Exception caused by not found the class TracingStatementInterceptor:
java.sql.SQLException: Unable to load statement interceptor
  'brave.mysql.TracingStatementInterceptor'.
Caused by: java.lang.ClassNotFoundException: brave.mysql.TracingStatementInterceptor
The dependency is
I have some distributed micro services written in Spring Boot and I am using RabbitMQ.
I want to track my requests.
Is there a possible way of tracking without using Spring Cloud or Sleuth
I want to send my spring boot log traces using Kafka to Zipkin server
I started my Kafka and Zipkin servers and I started a Kafka consumer which is listening to the topic Zipkin and I am able to see my logs here but when I open my Zipkin dashboard I can't find any traces
Spring boot version  :- 2.1.7.RELEASE 
 Spring Cloud version  :- Greenwich.SR2
when i make  spring.zipkin.sender.type=web  the traces are being shown in Zipkin dashboard 
Is there anything I'm missing here.
I have some services.
I want to trace those services using zipkin-go.
In every service, I am calling some of my other internal services or db calls.
I want to trace every activity like how much time it has taken to call internal services or db.
I have implemented using available tutorials on internet.
Below is my code:
I am getting my request traced but I am not able to trace what is happening inside the  uploadimage  controller.
Below is the screenshot of my zipkin UI:
I want to trace all the activities happening inside uploadimage controller.
What should I need to pass so that I can trace all.
I may have missed it, but how do save query parameters in zipkin?
My service is running:
I'm missing host too, but I suspect that is because I'm running services in docker containers.
update 8/13/2019
I make a call to  https://test-service.mydomain.com/api/conn/parallel2?repetitions=20&amp;delay=10000&amp;bypassTokenCache=true&amp;overrideTokenRefreshSeconds=10
Notice in the trace I'm missing host and query parameters.
Is there any way to record those values in zipkin?
I need to implement Zipkin tracing in one java-based service which is using Project Reactor Kafka for reactive streams and non-blocking IO operations.
I could not find any brave instrumentation library which supports reactive-Kafka.
The standard Kafka-client brave instrumentation:
https://github.com/openzipkin/brave/tree/master/instrumentation/kafka-clients
has no support for Reactive-Kafka.
Is there a library or repo which can help me with Zipkin tracing for reactive-Kafka in java?
I am creating a client sdk with retrofit calls to a service, packaged as a separate jar.
I have to include zipkin tracer/tracing in this jar so that any application using this jar to communicate with the service, have a separate span created automatically for every call to the service.
Is there a feasible solution to my problem?
I have been trying to solve the problem using the "io.zipkin.brave:brave-instrumentation-okhttp3" library.
I have also added the "org.springframework.cloud:spring-cloud-starter-sleuth" dependency so that the tracer-id is by-default generated.
But adding this jar to a project which uses kafka-streams and the "io.zipkin.brave:brave-instrumentation-kafka-streams" dependency, doesn't automatically initialses a new span.
What I expect is that applications using this jar have by-default a separate span for every retrofit call they make through this jar.
I'm trying to configure my spring boot app to log into a zipkin server.
The problem is that this server is protected by a proxy (with basic auth) and I cannot find any documentation describing how to configure authorization with spring-sleuth.
I have tried to use that kind of configuration :
But without success, logs indicating :
I have tried with curl and it works.
Has someone already succeed to configure authentication with spring-sleuth ?
I'm working on a spring boot application version 2.1.2 with below dependency
The application is not able to send the traces to Zipkin server which is running on Spring boot 1.5.
When I tried downgrading my application to Spring 1.5 it started sending traces to the Zipkin server.
Can someone please assist.
Am I missing any configuration for Spring boot 2.1?
Below are the dependency &amp; configuration for Spring cloud Sleuth and Starter Zipkin
Is there any CURL to get the latest available zipkin version and another CURL to get the currently running version in my local environment ?
I have to do a version comparison on currently running Zipkin version with related to the latest available.
I even walked through all the issues raised in github and couldn't find a solution.
I have Java application that is sending requests to Spring Boot applications.
I have implemented zipkin+sleuth on the Spring Boot applications and get traces.
Now I want to implement zipkin on the java application and to collect the traces.
How to implement that?
I need to use brave?
I have started zipkin-server and I can see the dashboard.
I have tested it with simple projects and it is okay.
But when I test it with my app I have a problem.
I have Spring Boot project that produce to kafka if the property for kafka is set on true in application.properties.
In my case it is always set to false and it is working correctly.
But when I added zipkin dependency it start to send to kafka.
And also I can not see my client app in the zipkin dashboard.
I am using Spring Boot 1.5.6.RELEASE version
This are my dependencies:
And this are my properties for zipkin and sleuth.
By adding the first 3 properties the application is not sending requests on the beggining, but it start after I send a request to my application.
With dependency spring-cloud-starter-zipkin, App should connect to zipkin server when sleuth triggered.
I did not started zipkin server, so it should throw a connection exception.
But nothing happened.
And when I start zipkin server, it can not receive anything.
pom.xml
App.java
application.properties
And logs
I want to implement Zipkin and Sleuth on vert.x application.
I have Zipkin server application with Spring Boot and Spring Boot Services and its okay.
I have added dependencies and setup them in application.properties and its working good.
But I have one vert.x application and I have found it difficult to implement the Zipkin there.
I have searched but I did not found some good example on how to implement it on vert.x application.
So, what I need to do to implement the Zipkin in the vert.x application?
How can I disable Istio to send spans to zipkin?
If I'm not wrong this is not a mixer adapter right?
It is something directly done from the pilot.
How can I disable it?
I'm adding Spring Cloud Sleuth with Zipkin to existing code to collect trace information and eventually log arbitrary messages.
Regular request spans are correctly sent to Zipkin:
However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans).
If I use  org.slf4j.Logger  to simply  LOG.info("something") , I see the  INFO  message in console output, with the  exportable  flag set to true:
Checking the traces in Zipkin, the span is correctly found, but the message used in the  LOG.info()  line is nowhere to be seen -- which suggests me that I'm doing something wrong here, or maybe it's just not supposed to work this way.
My sampling percentage is set to 100%.
Using the slf4j interface would be convenient because the existing code is already instrumented that way.
Is it possible?
If so, what could be a good way to implement it?
I am using spring boot and zipkin.
And using spring slueth to generate trace Id.
Is there a way I can generate this trace Id on my own?
Also I want to log only specific requests say with 500 error or response time   threshold, how to do this?
I have a system where we have 2 modules.
1) Module 1 is a webapp with multiple endpoints, deployed on Tomcat.
2) Module 2 is an executable jar,(not a web-app) which spins up 2 Kafka consumers (K1 and K2) listening to topic1 and topic2 respectively.
The web-app (Module 1) pushes messages to topic1.
K1 listens to topic1.It receives messages, processes them and pushes the processed messages to topic2.
K2 listens to topic2.
The messages are fully processed by K2 and do not propagate further.
There are multiple points where errors can occur in this flow.
I wanted to use Zipkin/ Jaegar to trace the entire flow, and also link the logs to the trace id, so that any issue can be easily and quickly investigated.
Can anyone suggest me the way to go forward?
I want to use ActiveMQ as a zipkin collector.
I have already used rabbitmq as collector, but my client is specific to using ActiveMQ.
Please let know if Zipkin supports ActiveMQ as collector and what are the configurations needed for that?
I have the microservice architecture of E-commerce website with all kinds of service containers and its corresponding database containers.
I failed to find a library to purely trace database running time for each request in like mongodb or mysql, so I used tcpdump to check the HTTP request roundtime at the port of service container.
The total communication time is always shorter than Zipkin's log time for that HTTP request.
So I am assuming that Zipkin include some service container process time in the tracing, which is not what I desire to have.
I plan to use tcpdump on database container and decode the package for tracing id, which could be a lot of hassle.
Why can't zipkin track this activity, or there might be some tools I can use?
I am trying to run zipkin with rabbitmq collector enabled, like this:
I can resolve tracing to an IP address and port 5672 is open.
A queue called zipkin has been created in RabbitMQ.
Here is the exception thrown:
Does anybody have any idea what I am doing wrong?
I am trying to integrate the Brave MySql Instrumentation into my Spring Boot 2.x service to automatically let its interceptor enrich my traces with spans concerning MySql-Queries.
The current Gradle-Dependencies are the following
I already configured Sleuth successfully to send traces concerning HTTP-Request to my Zipkin-Server and now I wanted to add some spans for each MySql-Query the service does.
The TracingConfiguration it this:
The Query-Interceptor works properly, but my problem now is that the spans are not added to the existing trace but each are added to a new one.
I guess its because of the creation of a new sender/reporter in the configuration, but I have not been able to reuse the existing one created by the Spring Boot Autoconfiguration.
That would moreover remove the necessity to redundantly define the Zipkin-Url (because it is already defined for Zipkin in my application.yml).
I already tried autowiring the Zipkin-Reporter to my Bean, but all I got is a  SpanReporter  - but the Brave-Tracer-Builder requries a  Reporter&lt;Span&gt;
Do you have any advice for me how to properly wire things up?
I am thinking of integrating WSO2 ESB and Zipkin following Open Tracing using a  Customize Statics Publisher implemented  by me.
The idea is enable statics and trace in WSO2 ESB.
Is there any other approach to to achieve this?
Is this approach correct?
The Collector Sampler today, samples the spans as received by the Zipkins Collector based on the rate.
But for a micro-services architecture where a single transaction traverses through multiple applications and spans, having a sampler logic to sample based on just the number of spans might cause us to loose a whole picture of a single transaction.
So we are looking for an ideal solution where the Zipkin (not the app's sleuth implementation) has options to sample the transactions based on trace(or a whole transaction with all its spans) and not based on individual spans.
And we expect this to be more asynchronous sampler technique.
Looking forward for your thoughts...
I'm looking for better graphs for the traces stored in zipkin.io database, wondering if anyone developed anything better than out of box zipkin UI..are there any alternate GUIs for Zipkin?
I have mircroservice environment based on spring-boot, where i am using zipkin server and discovery-server(eureka) and config-server.
Now i have a rest-microservice which sends logs to zipkin server and this microservice is required to resolve where is zipkin server using discovery-server.
following is zipkin configuration i have in my rest-microservice's application.properties(pulled from config-server).
here MTD-ZIPKIN-SERVER is zipkin-server name in discovery-server.
discovery-server dashboard.
but it does not try to resolve zipkin from discovery-server, instead it tries to connect directly using spring.zipkin.baseUrl, and i get below exception.
Dropped 1 spans due to ResourceAccessException(I/O error on POST request for " http://MTD-ZIPKIN-SERVER/api/v1/spans ":
  MTD-ZIPKIN-SERVER; nested exception is java.net.UnknownHostException:
  MTD-ZIPKIN-SERVER)
org.springframework.web.client.ResourceAccessException: I/O error on
  POST request for " http://MTD-ZIPKIN-SERVER/api/v1/spans ":
  MTD-ZIPKIN-SERVER; nested exception is java.net.UnknownHostException:
  MTD-ZIPKIN-SERVER     at
  org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:666)
    at
  org.springframework.web.client.RestTemplate.execute(RestTemplate.java:628)
    at
  org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:590)
    at
  org.springframework.cloud.sleuth.zipkin.RestTemplateSender.post(RestTemplateSender.java:73)
    at
  org.springframework.cloud.sleuth.zipkin.RestTemplateSender.sendSpans(RestTemplateSender.java:46)
    at
  zipkin.reporter.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:245)
    at
  zipkin.reporter.AsyncReporter$Builder.lambda$build$0(AsyncReporter.java:166)
    at zipkin.reporter.AsyncReporter$Builder$$Lambda$1.run(Unknown
  Source)   at java.lang.Thread.run(Thread.java:745) Caused by:
  java.net.UnknownHostException: MTD-ZIPKIN-SERVER  at
  java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
if i provide exact zipkin url in property spring.zipkin.baseUrl like below
then my rest-microservice is able to connect to zipkin-server.
My goal here is to read zipkin-server location from discovery-srever.
What am i doing wrong?
Do i need to add some zipkin enabling annotation on my spring-boot rest-microservice?
For some reason Zipkin is using the Consul discovery name instead of the base spring.application.name property.
But I want it to use the non-randomized application name (so myservice instead of myservice-67gg8d368).
If I set the Zipkin property  zipkin.service.name  then Consul throws errors saying it cannot find the service.
I'm unsure why the two are even sharing properties and not just adhering to their own.
I'd like the service to use it's base application name because otherwise Zipkin is hard to use, as it lists every new container as a completely new service, making it very difficult to see over time how code changes have changed timing.
UPDATE:
This is the error I get in my logs if I set the zipkin.service.name
I am having problem persisting the zipkin data.
I did not get any error message.
So I am sharing configuration to get a help.
I can see my logs in zipkin UI, but not able to persist in elastic search.
My zipkin-service pom file shared below.
And my properties looks like this
I am currently using zipkin and I am trying to build a python script to extract the data saved and process them differently.
zipkin logs and UI are working fine.
I have done this :
I have used wget and request and both are giving me the result below:
but If I copy paste the URL used in the request or wget in a web browser the result is shown
any idea how to extract the data in JSON or any other format for a Zipkin server?
Thanks
So I'm having zipkin gathering my data inside kubernetes from other services.
I'm having nginx ingress controller defined to expose my services and all works nice.
As zipkin is admin thing I'd love to have it behind some security ie.
basic auth.
If I add 3 lines marked as "#problematic lines - start" and "#problematic lines - stop" below my zipkin front is no longer visible and I get 503.
It's created with  https://github.com/kubernetes/ingress/tree/master/examples/auth/basic/nginx 
and no difficult things here.
I'm not sure if it's not about possible infulence but I used  https://github.com/kubernetes/ingress/blob/master/controllers/nginx/rootfs/etc/nginx/nginx.conf  file as template for my nginx ingress controller as I needed to modify some CORS rules.
I see there part:
but I don't see result in:  kubectl exec nginx-ingress-controller-lalala-lalala -n kube-system cat /etc/nginx/nginx.conf | grep auth .
Due to this my guess is that I need to add some annotation to make this  {{ if $location.BasicDigestAuth.Secured }}  part work.
Unfortunately I cannot find anything about it.
I saw the services had send the trace to zipkin server, and also found the trace result list in zipkin ui, when I clicked one of them to see the trace detail and timeline diagram, but there had no diagram just some duration as the title and json data is ok.
Is there something I can do ?
zipkin
On the official Zipkin repository  README  I see how to configure Zipkin with either Cassandra, MySql or Elastic Search.
However in my current job, we'd like to make use of Zipkin, but we are limited to Microsoft SQL Server as the only supported database.
AFAIK Zipkin works with MySql via standard JDBC driver, therefore I think it would be possible to make it work with SqlServer as well.
Am I right?
If so, how can I configure it to work over it?
I recently upgraded my project from Spring Boot 1.4.1, Spring Cloud Sleuth 1.1.0, Spring Cloud Zipkin 1.1.0 to Spring Boot 1.5.3, Spring Cloud Sleuth 1.2.0, Spring Cloud Zipkin 1.2.0.
Read that with the latest version of Spring Cloud Sleuth, they had added "error" tags which will get reported to Zipkin automatically in case of any exceptions.
I have a @ControllerAdvice class extending ResponseEntityExceptionHandler for custom exception handling.
I was able to report errors to the Tracer and visualize the same in Zipkin when using the old versions (Spring Boot 1.4.1, Spring Cloud Sleuth 1.1.0, Spring Cloud Zipkin 1.1.0) using the below method:
After I upgraded, this doesn't seem to work and spring cloud sleuth's default error reporting was also not happening.
Only after commenting out the @ControllerAdvice and letting Spring Boot's default ErrorController to handle the exceptions, I was able to visualize the errors in Zipkin.
However, we need the custom exception handling to format the error response in a standard way with error codes across all our PaaS services.
Is there a way to do this?
Should I use any other Sleuth objects to achieve this?
I am not able to start the Spring Cloud ZipKin Server, It is giving below mentioned exception.
BeanCreationException: Cannot create binder factory, no  META-INF/spring.binders  resources found on the classpath
Below are my maven dependencies -
Also my application startup class looks as below.
Any help is highly appreciated.
I have a distributed system, where a client needs information from multiple sources.
Is there any support for marking parallel processed spans for the same trace in Brave (Java implementation of Zipkin framework)?
Currently, before sending a message I call clientRequestInterceptor.handle(...) and after receiving response clientResponseInterceptor.handle(...), but there is only one instance, so only one span is recorded.
P.S.
I found the following project on GitHub that specified that Brave only supports one level of nested client call:  https://github.com/leigu/brave-tracer-example .
Maybe the same is valid for parallel client calls.
The infrastructure of system used by my company consists of different application running on a  ActiveMQ .
I want to know how would I setup  Zipkin  to trace the communication between different applications under  ActiveMQ ?
Thanks
Please,  notice  that this question can seem a duplicate of  this one , but it's not the case.
Below I include my rational
I'm trying to add Sleuth/Zipkin trace to my project.
For this I have followed this  tutorial .
My project is already using RabbitMQ for communication among the different microservices working fine.
My problem is that I'm able to get the traces fine when I use the web connection, but I get an unable to connect error when I try to communicate using RabbitMQ.
As commented in the first line, my problem does not seem to be related to rabbitmq host itself, because it is up and running, and providing service to my microservices.
Sure that I missing something in the configuration, but I cannot find it (I have also checked  this post ,  this  and  this .
My files (I have removed not relevant part):
Notice that is started with profile 'docker', so host for rabbit is rabbitmq
I have been searching if there is a way to instrument winstonJS with zipkin-js related data for each log.
However, I have not been able to find something.
I saw some example where it mentioned to configure winston as follows
However, the formatter is not available in the latest version of winston.
I was trying out to configure winston as
But the fields are never received.
Also, when using express and configuring the winstonjs with
My implementation is available at:  https://github.com/vtapadia/sample-node-app/blob/zipkin-tracing/src/config/logger.ts
I just want my normal logging to have the tracing information auto injected.
Is it possible to achieve and can someone point to an example for this.
Sending traces from an existing instrumented Spring Boot application to  honeycomb-opentracing-proxy  is failing with the following error in the proxy console:
Spring Boot Version: 2.1.3.RELEASE
Spring Cloud Sleuth Version: 2.1.1.RELEASE
Running the open tracing proxy with the following docker command:
From reading the documentation  here  the honeycomb-opentracing-proxy only supports v1 of the JSON API so I have explicitly set that in spring cloud config as this appears to default to v2.
application.properties
Any help would be greatly appreciated
I am using Zipkin for distributed tracing.
I have added zipkin-storage-mysql dependency in order to save the traces in MySQL DB.
When I query ZIPKIN_SPANS table, I don't find the 16 char trace id in TRACE_ID Colum that I use in order to load the trace on zipkin UI.
for ex:  localhost:9411/traces/4bcdd0bd5d2f70c0
Please help me understand how can I figure it out.
Also, How can I add a new column to the table for associating an application-specific id with it
Spring boot application with below configuration :
Required zipkin tracing feature.
But my application not listing on zipkin server used proper jar also as below
Spring Doc says
Spring Cloud Sleuth is compatible with OpenTracing.
If you have OpenTracing on the classpath, we automatically register the OpenTracing Tracer bean .
If you wish to disable this, set spring.sleuth.opentracing.enabled to false
I have the below dependency in my POM.
But, I get the following print out it the logs when I try to print the trace and span information :  tracer: NoopTracer
Why am I getting a NopTracer?
Why isn't Brave being registered automatically as promised?
Am I doing something wrong?
I am using
Finchley.SR2
My Sampler looks like that:
In Eureka everything seems well:
But my problem is that in Zipkin I can't see services at all.
I found only debug logs, I have no errors:
Question is simple.
Why can't I see anything in Zipkin?
This samples are written by  Josh Long.
I was working on an Ecommerce microservice-based application.
HTTP request structure: frontend -  order -  shipping -  rabbitmq -  queue-master.
The shipping container get the trace info from order but it doesn't pass along to rabbitmq.
Is there any configuration I should do?
I thought using "spring-cloud-starter-zipkin" will help me handle it effortlessly.
(tcpdump) shipping get post request with trace info:
shipping send out to rabbitmq missing tracing data.
EDITED:
Some code about post shipping request: (shipment is just an object with id and name)
I found that the bottleneck of zipkin is collector and API, are these two components stateless so i can deploy multi collectors and multi API?
I want to deploy zipkin in kubernetes.
I have zipkin server running as spring boot app.
I have exported jar to docker container.
My dockerfile looks like:
I have one mysql container.
Got this from official docker hub.
when I link my zipkin container to mysql using this command :
I am getting refused connection exception
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure
Although if I run my jar from host OS then it is able to connect mysql container and I can see spans stored in mysql db
My application.yml is :
How can I connect java docker container to mysql docker container?
I have already explored this  link.
I am getting this error when I try to run the unitTest in my spring boot application.
I notice that I only get this error when I use this version for  spring-cloud-dependencies :
but if I use this previous version:
everything works as I expect
What can I do in order to avoid this error with the last version?
The unit tests are extended this class in order to mock the SpanAccessor
}
I'm trying to integrate sleuth into a Spring Boot application so that it will talk to a zipkin server for tracing, but I'm not having much luck.
I've followed a few tutorials ( link to tutorial ) and have no problems getting them to talk to zipkin, but it's not translating well to my application, and I'm not sure where to look.
essentially, in the build.gradle file, to the dependencies section, I added:
In the controller, I added these two beans:
and, I added these to the application.properties file:
When I do all that in the demos, they send traces to my Zipkin host at localhost:4911 just fine (For the time being, I'm just running the quickstart jar file).
When I do all that in my application, I see that I have sleuth log entries with strings like:
so, I know that Sleuth is working.
When I run a demo app with the zipkin server application shut off, the application looks like it's working fine, but, reasonably enough, the log files show a big old ConnectionRefused stack trace.
When I try the same experiment with my application, I see no stack trace in my application logs, and the application also ran just fine.
Outside of my larger application, I can't reproduce my problem, and I don't know what else to share with you.
Anyone have any suggestions about where to start?
TL;DR: 
I want to persist data in ElasticSearch, how i can do this?
I have zipkin and Kafka and ElasticSearch.
Kafka as transport for traces.
When i send trace to Kafka, i got it in zipkin UI, it is persistent in memory.
I want to persist all traces in ES 5.0 for some time and when zipkin starts or when i search traces i want to search in ES or load trace from ES at start time of zipkin.
I start zipkin like this integrated with Kafka:
Here is  description of zipkin-storage/elasticsearch-http :
This is output:
But in ES in index zipkin there are no data.
Springboot app.
I'm using Brave v4 and trying not to use the brave-core module for when it is deprecated in the future.
With Brave v3 it was easy to pass around the current span as it was kept within the thread and handled by the Brave class.
I'm using the async-http-client client and I've created request and response filters which propagate to and from the header as well as starting and submitting spans.
This all works as expected.
The async-http-client pool is wired up at startup with the listeners attached, the listeners receive a TracingImpl which is just a wrapper for the Tracer class so that the listeners can do the submitting etc (well the spans inside can start/finish themselves)
My problem is for example: a request comes into the controller, I extract the Span from the request and now I want to use the async-http-client to make another request which would be a child of the one coming in.
I'm unsure how I should get the Span object I now have in my controller to the async-http-client object to it.
Any ideas or help would be greatly appreciated.
Thanks
I am working on implementing distributed tracing for a micro-services application.
In zipkin UI I could not able to get child URL paths.
For parent call whole URL is being captured like &quot;post/cart/create&quot;.
But for child calls only &quot;get&quot; and &quot;post&quot;.
Not showing entire URL for child calls.
How to fix this?
Need help.
The data in Zipkin_spans table is as shown above.
My calling service Code:
Dependecies that i am using are
This problem seems to be inconsistent.
Sometimes it shows entire URL and sometimes not.
Only after testing multiple times I came to know about this.
I have even tested with spring-cloud- starter -zipkin.
But it is also not showing any promising result.
So if entire URL is necessary then shouldn't I be using zipkin at all?
suggest any other tracing mechanisms for microservices using spring.
Small question on how to pass around the original trace id  traceId  (and not X-B3-TraceId ) please.
The entire flow is a call between a client ClientA to three other micro services (4 in total).
ClientA (team A) -&gt; Service B1 (team B) -&gt; Service B2 (team B) -&gt; Service C (team C)
Team A owns the client, the originator of the call, team C owns the last service.
My group, team B owns two out of the four micro services.
All subsequent micro services just perform computation and aggregate the response of each other back to the caller, ClientA.
Our team, B, had two micro services, hence we were the first out of the four to use Spring Cloud Sleuth and Zipkin.
Very happy, we could see out of the four services, in our services only, the traceId.
This helped us in debugging, timing calls, etc, very happy.
Since everyone saw benefits, now ClientA and ServiceC (the two others we do not own) also integrated with Sleuth-Zipkin.
Unfortunately, it seems we are not able to &quot;chain everything&quot;.
🤯
We looked at each others traceId, and saw something like:
Hence, we are now all very puzzled.
As we thought we would see something like:
Since for the two servicesB1 and B2, we did not do anything in particular, out of the box, we could see same traceId between the two, we thought, out of the box, we would be able to see between the four services.
May I ask, is my understanding not correct?
What did I miss please?
Do I have to do anything special, if yes, what, in order to ask ClientA to send me its traceId?
Or maybe ClientA is already sending it, I am not getting it properly?
(same applies to ServiceC)
Thank you for your help.
spring-boot 2.3.4-RELEASE
Error is shown after adding the zipkin dependencies to a project and running with
Cannot figure out the error causing it.
Other spring-boot apps in the project are running fine with the same zipkin configs and application.yaml.
Any help to figure out how to debug this would be helpful.
java.lang.IllegalStateException: Error processing condition on org.springframework.cloud.sleuth.zipkin2.ZipkinBackwardsCompatibilityAutoConfiguration$BackwardsCompatibilityConfiguration
at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:60) ~[spring-boot-autoconfigure-2.3.4.RELEASE.jar:2.3.4.RELEASE]
pom.xml
application.yaml
I am using zipkin tracing in node.js application and I am getting  this  error.
How to resolve this error?
Thanks in advance...
Small question as I am not able to see  X-Span-Export  in my logs please.
I have an app, which is Java based, with Springboot 2.4.2, with Sleuth and Zipkin.
In my log4j2.xml, I configured such:
However, in my logs, I am only able to see:
I was expecting to see a &quot;true&quot; or &quot;false&quot;, like  [myservice,336a9463f46cf034,18bffe81e7500eae,true]  however, it is not here.
May I know what is the issue please?
Thank you
I'm running a Spring Boot app using:
I've declared the  spring-cloud-starter-zipkin  and  spring-cloud-starter-openfeign  dependencies, and have configured my app to point to a Zipkin server.
Its a pretty vanilla setup and configuration (I also declare the  spring-cloud-starter-netflix-ribbon  and  spring-cloud-starter-kubernetes-all  dependencies o allow Spring Feign to use k8s service discovery).
My app declares a  @SpringFeign  annotated interface with a method to call to a remote service  S .
So generally zipkin is getting spans from my app (for e.g.
incoming REST calls) and B3 headers are being propagated via HTTP to the service  S  being called through feign.
But zipkin does not report a span from my app representing the Feign call to S.
Is that something that should &quot;just happen&quot;, or am I missing a piece of the puzzle?
I can e.g.
add  @NewSpan  to the feign interface method, but that doesn't give me HTTP details for the request/response as span tags.
And I rather not do that if this is supposed to work out of the box.
In my Spring Boot application I use spring-cloud-starter-sleuth (Version Hoxton.SR10) for tracing.
It is (still) a monolithic application so I widely use the  @NewSpan  annotation for creating new spans.
In my development environment I also use spring-cloud-starter-zipkin, which works great.
But on the servers of our customers I don't have access to any Zipkin server nor am allowed to install one.
Is there any possibility to save the data Spring is sending to Zipkin and import that to my local Zipkin server?
Solution thanks to Marcin's inspiration:
Implement  convertByteArrayToList  and  saveToFile  your own, because my solution depends on custom libraries.
i am new to  opencensus  and i am trying this  quick-start-example  here i want to export the traces to zipkin for that i added the exporter code and zipkin is running on docker
traces are not exporting to zipkin here is my example code
i have tried to search by trace id as well but it did not show up
I am using the Cassandra 3 storage type for Zipkin, but I can't find the option for it to clear the older data.
I am looking for something like  MEM_MAX_SPANS=1000000  as specified in the  README.md .
In ElasticSearch I can set up a rule to clean up older records, maybe there is something like that in Cassandra?
Question regarding Spring and Zipkin/Open tracing, in particularly, traces where an app in the middle does not have Zipkin/Open Tracing please.
Setup:
Step1: A client, front end app, without Zipkin is making a call to my-micro-service-A-with-zipkin.
Step2: my-micro-service-A-with-zipkin will then call my-micro-service-B-with-zipkin
Step3: my-micro-service-B-with-zipkin will then call  third-party-service-C-without-zipkin
Step4:  third-party-service-C-without-zipkin  will then call my-micro-service-D-with-zipkin
Step5: After all those calls, my-micro-service-A-with-zipkin will return the response to the client.
A bit surprised, when searching for the trace, I could only find the call my-micro-service-A-with-zipkin -&gt; my-micro-service-B-with-zipkin, and  none of the other  calls.
Basically, just one step our of the 5.
Question:
Is this expected please?
I understand third-party-service-C-without-zipkin does not have Zipkin, but the for me, this is kinda defeating the purpose.
In this example scenario, I will never be able to see further traces other than A to B (Step2) ?
Also, is there a way to remediate to this?
The third party is a third party API, out of our control, we cannot just go and say &quot;please install Zipkin/open tracing, it will be helpful to everyone&quot;.
Really wondering how to achieve the correct traces in this described scenario.
Thank you
Question regarding Spring Webflux 2.4.2+ project, with Spring Cloud Ilford 2020.0.0 please.
In my small Spring Webflux app, I am currently using
I see the traces fine in m Zipkin server, very happy.
But I was also expecting to be able to get some metrics, such as
However, while I am seeing the traces in Zipkin server, and &quot;some&quot; zipkin metrics, such as  zipkin_reporter_spans_total  I am not getting Zipkin related metrics at all.
May I ask how to get above mentioned metrics please?
Thank you
Can I query a json file with the dependencies suing zipkin API?
like with traces:
Seems like tags aren't working with vert.x zipkin integration.
Below is the sample code snippet
We use vert.x application as api gateway which takes requests and forward the requests to other services.
On the server side, Tracing.currentTracer().currentSpan() always gives a null object.
Why is the span going out of context here before the request is forwarded down to other service?
Is there any bug in the vert.z zipkin code?
Use Case :
Using vert.x application as an API gateway for all incoming traffic.
Want to sample incoming requests through vert.x
Sampling option inside ZipkinTracingOptions is hard coded to Sampler.ALWAYS_SAMPLE
I feel, we should provide the application an ability to supply Sampler from outside so that all sampling options supported by Brave can be used in ZipkinTracingOptions
Does vert.x supports any other Sampling Option or does it intend to do that in future?
I see active work going on for vert.x integration with zipkin.
https://github.com/eclipse-vertx/vertx-tracing
Will it be backward compatible with vert.x 3.8.x version?
If yes, can you share tentative timelines for the same?
I have set up a docker image of  Zipkin  to enable distributed tracing in my organization.
The information needs to be available for specific users, Can I add some user authentication in front of the UI, without protecting the current flow of span information?
Can anyone tell me, is there any way to  integrate Zipkin (if possible Sleuth also) for non-spring boot projects ?
I am trying to integrate Zipkin for my traditional spring application.
It is not a microservice.
How could I do this?
Any suggestions, please?
Thanks.
I am new to distributed logging and I need help around the propagation of extra fields across Http Request and Messaging Request.
Currently, I am able to propagate the traceId and spanId, but I need to pass  correlationId  to be propagated across all the microservices.
logback.xml
I am a bit curious about how to pass the correlation id to other services.
I have integrated spring cloud sleuth (2.2.5.RELEASE) and Zipkin in my spring boot application.
I am trying to exclude certain url pattern from being traced and show up in Zipkin.
I have this in my spring boot application.properties file
But still I see that traceId is being generated for url /api/v1/hello and I also can visualize this request in Zipkin.
I have also tried spring.sleuth.web.additional-skip-pattern to achieve this but that also doesn't help.
How  can I achieve this?
Thanks in advance.
I have to configure two separate Kafka brokers (as on example below) and this is working just fine - I have my writes and reads on different Kafkas.
But also I need to configure zipkin+sleuth in application - and only way I can do this is via adding:
As soon, as  spring.kafka.bootstrap-servers  added, it started to override  kafka2.environment.spring.cloud.stream.kafka.binder.brokers  - so application just trying to write to kafka on dev1-stage.dub, instead of dev2-stage.dub.
How can I prevent this overriding?
Or how I should rework configuration to support both sets of kafka brokers along with zipkin?
I am trying to deploy zipkin within k8s.
I am using elasticsearch (version 6.8.8) as storage.
The deployment works fine and the server starts.
However, I only can access the server with a port-forward.
$ kubectl -n ns-zipkin port-forward zipkin-bdcf7f78b-shd9p 8888:9411
After that I can access  http://localhost:8888/zipkin/
What could be the reason?
Already the deployment of the service does not get an endpoint (see  in output below) which I would expect.
deployment.yaml
$ kubectl -n ns-zipkin describe deployment zipkin
service.yaml
$ kubectl -n ns-zipkin describe service zipkin
ingress.yaml
$ kubectl -n ns-zipkin describe ingress
I'm using openzipkin/zipkin:2.21.5 run on okd openshift with some basic default conf (possibly docker run -d -p 9411:9411 openzipkin/zipkin - but I'm not sure, it was installed by admin).
After about one minute I can't go into details of trace from search page to which I could go before moment.
Maybe Zipkin server is running out of memory and deletes old spans - are there any statistics about it?
Maybe it has some default number of detained traces in some circular buffer, if so, how can I increase it?
How can it be checked and configured?
Where can I find any documentation about Zipkin server configuration?
https://hub.docker.com/r/openzipkin/zipkin  states mysql is deprecated,  elasticsearch  link is broken, in-memory is not for prod, where can I find a doc for production configuration?
How can I force a Zipkin span to be exportable?
In below code spans are sometimes exportable, sometimes not in a non repeatable manner.
It seems to me that if I comment first scopedSpan, than second manually created spanInScope is exportable, but how can first scopedSpan prevent second spanInScope from being exportable?
How do they interfere?
We have  spring-cloud-starter-sleuth  and  spring-cloud-starter-zipkin  dependencies in in other microservices and we are able to see the traces on the  zipkin  dashboard.
We also want to show the custom messages on zipkin i.e whatever log messages we are logging in our controller methods.
we want to show additional log data on zipkin.
Is there any way to show these  logger.info  messages on the zipkin dashboard?
Also, is there any way to customize the format of messages being sent to zipkin?
we want to show message in something like  %d{yyyy-MMM-dd HH:mm:ss.SSS} %5p [%X{X-B3-TraceId:-},%X{X-B3-SpanId:-}] [%thread] %logger{15} - %replace(%msg %ex){'[\\r\\n]+', '\\n'}%nopex%n  format.
I tried in Windows 10 machine to coonect RabbitMQ (3.6.11 version installed with Erlang 20) to ZipKin, but I got the following error:
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Unsatisfied dependency expressed through method 'armeriaServer' parameter 4; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'serverConfigurator' defined in zipkin2.server.internal.ZipkinHttpConfiguration: Unsatisfied dependency expressed through method 'serverConfigurator' parameter 2; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:797) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:538) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:893) ~[spring-beans-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:879) ~[spring-context-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:551) ~[spring-context-5.2.7.RELEASE.jar!/:5.2.7.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758) ~[spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:140) [spring-boot-2.3.1.RELEASE.jar!/:2.3.1.RELEASE]
at zipkin.server.ZipkinServer.main(ZipkinServer.java:54) [classes!/:?]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?
:1.8.0_251]
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?
:1.8.0_251]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?
:1.8.0_251]
at java.lang.reflect.Method.invoke(Unknown Source) ~[?
:1.8.0_251]
at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:49) [zipkin-server-2.21.5-exec.jar:?]
at org.springframework.boot.loader.Launcher.launch(Launcher.java:109) [zipkin-server-2.21.5-exec.jar:?]
at org.springframework.boot.loader.Launcher.launch(Launcher.java:58) [zipkin-server-2.21.5-exec.jar:?]
at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:88) [zipkin-server-2.21.5-exec.jar:?]
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'serverConfigurator' defined in zipkin2.server.internal.ZipkinHttpConfiguration: Unsatisfied dependency expressed through method 'serverConfigurator' parameter 2; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'zipkin2.server.internal.health.ZipkinHealthController': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitMq' defined in zipkin2.server.internal.rabbitmq.ZipkinRabbitMQCollectorConfiguration: Invocation of init method failed; nested exception is java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
Caused by: java.io.UncheckedIOException: Unable to establish connection to RabbitMQ server: Connection refused: connect
Caused by: java.net.ConnectException: Connection refused: connect
I’m trying to implement an async one way span in zipkin.
Effectively 2 different processes share a span and need to capture between them.
Per the documentation , I am using the ‘cs’ and ‘sr’ annotations on the span, however don’t see the duration being captured accurately.
Brave version: io.zipkin.brave:brave:5.12.3
Zipkin server version:2.21.4
Using it as an in memory storage currently.
This is what I’m currently doing:
Client Side:
Tracer.tospan(spancontext).
Annotate(cs).start().flush()
Server Side:
Tracer.joinSpan(spancontext).
Annotate(sr).start().flush()
The spans Json shows 2 spans of kind Client and server.
Their time stamps are accurate and they have no duration.
In the trace graph I expect to see the span having duration as the difference between the timestamp of the client and server spans.
However I don’t.
I do see the expected  duration under the ‘server start’ annotation on the right pane as the ‘relative time’.
Is there anyway to see this as a duration on the span itself?
Is this expected behaviour or am I missing anything?
Trying to integrate zipkin and brave in my cxf application, application is having spring xml configuration.
reference page  https://cxf.apache.org/docs/using-openzipkin-brave.html  is quite outdated and does not include new implementation of different libraries.
Would appreciate any doc or reference for integrating same.
cxf-version
I am following the tutorial for creating tracing application  zipkin  and  sleuth  but I am having some trouble.
I cannot create a span.
The problem is that the method does not exist.
Also I cannot find the import for the tracer.
This is what I am trying to do:
Why is the implementation above not working?
I want to integrate spring sleuth zipkin server with elastic server.
Can anyone please help how to integrate?
This is specifically for Zipkin's Elastic Search storage connector.
Which does not do the index that you can use Curator.
Is there a way of  automatically  removing old traces and have that as part of the ElasticSearch configuration (rather than building yet another service or cron job)  Since I am using it for a development server I just need it wiped every hour or so.
I am new to zipkin server.
I am trying to run a zipkin-server-2.12.9-exec on linux server facing the below exception.
2020-03-09 15:36:28.796  WARN 1685 --- [           main] s.c.a.AnnotationConfigApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.linecorp.armeria.server.Server]: Factory method 'armeriaServer' threw exception; nested exception is java.util.concurrent.CompletionException: java.lang.IllegalStateException: Armeria server failed to start
2020-03-09 15:36:28.805  INFO 1685 --- [           main] ConditionEvaluationReportLoggingListener :
Error starting ApplicationContext.
To display the conditions report re-run your application with 'debug' enabled.
2020-03-09 15:36:28.806 ERROR 1685 --- [           main] o.s.b.SpringApplication                  : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'armeriaServer' defined in com.linecorp.armeria.spring.ArmeriaAutoConfiguration: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.linecorp.armeria.server.Server]: Factory method 'armeriaServer' threw exception; nested exception is java.util.concurrent.CompletionException: java.lang.IllegalStateException: Armeria server failed to start
I want to use stackdriver trace as a back end for distributed tracing in istio.
I installed Docker on the VM of GCP, and run the image of zipkin-gcp.
Then, according to the official documentation, I configured istio to send spans to this VM.
However, no trace was displayed in the stackdriver trace.
To isolate the problem, I stopped zipkin-gcp and checked if packets were being sent with tcpdump.
As a result, it was found that nothing was sent.
I have confirmed port 9411 connectivity from the kubernetes cluster to the VM.
How do I send a trace to an external zipkin server with istio?
I am trying to configure Istio control Plane to use zipkin as tracing backend, but I can't.
In their docs, they state that in order to do this, I just have to pass the following parameters when installing Istio:
--set values.tracing.enabled=true  and  --set values.tracing.provider=zipkin .
My problem is that I have installed Istio manually.
I found the parameter  provider: jaeger  in the  Configmap   istio-sidecar-injector , and made the change, then killed the control plane so it would be re-deployed with zipkin, but didn't work.
Does anyone know what object/s should I manipulate to get zipkin?
pom.xml
class
Please tell me how to solve the issue?
I have implemented different microservice where internal communication is taking place by camel and rabbitMQ with different exchange and queue.
I'm using  camel-zipkin  for tracing, and  log4j2  for logging.
Ex:
I am using Springboot  1.5.12.RELEASE  and  Camel  2.21.5 version   with the following dependencies (of camel):
I need suggestion for my two problems
My Questions are :
Sample Code :
Pom.xml
SpringBoot Application
RabbitMQConfig
Controller adding message to queue
RouteBuilder
MessageActivity
SmsActivity
Output
While creating a Zipkin Server with Spring Boot(v2), I am facing Whitelabel Error Page. 
        "
Whitelabel Error Page 
        This application has no explicit mapping for /error, so you are seeing this as a fallback.
Wed Oct 30 11:21:35 IST 2019
        There was an unexpected error (type=Not Found, status=404).
No message available 
        " 
And also while i run the application in spring boot, i get: 
        "  Cannot find template location: classpath:/templates/ (please add some templates or check your 
        Thymeleaf configuration) "
Please help me to resolve this
In the logs, Zipkin status is coming as true but I can not see it in the Zipkin UI.
The same things are working for the zuul but not for the other microservices.
dependencies
properties:
The only difference between zuul and this microservice is, it is using the spring cloud stream as well.
Can it be a reason?
I try to setup zipkin, elasticsearch, prometheus and grafana with docker-compose.yml 
When I run dockers, see in the log:
dependencies_zipkin | 19/09/30 14:37:09 ERROR NetworkClient: Node [172.28.0.2:9200] failed (java.net.ConnectException: Connection refused (Connection refused)); no other nodes left - aborting...
I'm on MacOS X with docker 2.1.0.3
the content of my docker-compose.yml is this one:
When I connect to localhost:9200, I see that elasticsearch is working fine and on port 9411, zipkin is deployed but I have the error:
ERROR: cannot load service names: server error (Service Unavailable)(due to the network error
In the log, I have this information:
105 ^[[35mdependencies_zipkin |^[[0m 19/09/30 14:45:20 ERROR NetworkClient: Node [172.28.0.2:9200] failed (java.net.ConnectException: Connection refused (Connection refused)); no     other nodes left - aborting...
and this one
^[[31mzipkin          |^[[0m java.lang.IllegalStateException: couldn't connect any of [Endpoint{storage:80, ipAddr=172.28.0.2, weight=1000}]
Any idea?
UPDATE
by using mysql it is working fine, so the problem is at the level of elastic search.
I tried alsoo by using
"STORAGE_PORT_9200_TCP_ADDR=127.0.0.1"
but the issue still occurs.
UPDATE
As mention is the solution gave by Brian, I have to use:
ES_HOSTS= http://storage:9300
the key is on port, I was using the port 9200
The error disappear between zipkin and es but still occurs between es and zipkin-dependencies.
I use zipking for testing with curl post.
Examples for post  https://zipkin.io/zipkin-api/#/default/post_spans
503 Service Unavailable
zipkin in docker, logs in container:
2019-07-24 07:05:42.383 WARN 1 --- [orker-epoll-2-5]
  z.s.i.BodyIsExceptionMessage : Unexpected error handling request.
com.linecorp.armeria.common.stream.AbortedStreamException: null
I also tried example:
but I do not sees in web ui.
What is the corresponding command for “STORAGE_TYPE=mysql MYSQL_USER=root java -jar zipkin.jar” under cmd?
I succeeded in Ubuntu, but the cmd in Windows has never been successful.
My spring boot application has some problem.
Zipkin and jdbc can not coexist together.
It is normal to have only one zipkin or jdbc.
Maven dependency:
Exception:
Nested exception is:
org.springframework.beans.factory.BeanCreationNotAllowedException: Error creating bean with name 'eurekaClientConfigBean': Singleton bean creation not allowed while singletons of this factory are in destruction (Do not request a bean from a BeanFactory in a destroy method implementation!)
Trying to add Zipkin mysql tracing instrumentation by including the following library in a Spring Boot Application
And appending the following to my jdbc connection string:
Existing DB connection is using a c3po connection pool configured as follows:
but running into issue starting service when attempting to acquire connections from the pool:
We are in process to keep all monitoring and logging stuff outside of AKS.
we got some success with Azure log analytics as well.
I am checking if Azure log analytics provide any feature similar to zipkin.
i.e.
providing trace of REST API.
I hope you can help me.
Let's use for example this very simple code
Like you can see this microservice only forwards messages from one kafka-topic to another.
I also want to send this data to zipkin to see the duration of the messages or something like that.
Maybe I've seen the solution and don't get it but I realy have looked for a solution but didn't find one.
You are my last hope.
I have seen the brave api but I don't realy understand how to use it for kafka.
I
I am new to sleuth and zipkin.
I have logged some messages and sleuth is appending trace id and space id for those messages.
I am using zipkin to visualize it.
I am able to see timings at different microservices.
Can we see logger messages(we put at different microservices) in zipkin UI by trace id?
I am new to distributed tracing and trying to use the example explained in the video  https://www.youtube.com/watch?v=CFLZJSwbYI0
In short this has following
Now when I run zipkin server using command
java -jar zipkin.jar
and accesing zipkin at url
     http://localhost:9411/zipkin
So far everything works fine.
Now I am starting zipkin-client/service which is running at port 8081/8082
After this i accessed zipkin url ( http://localhost:9411/zipkin ) but it's broken now.
I am wondering why starting other service on port 8081/8081 is causing zipkin server to stop responding.
Any kind of help is much appreciated!!
!
I am upgrading an application to Spring Boot 2.1.3 (from 1.5.x), and I am facing an issue at startup time.
Below block can't be bound properly :
I am getting this error :
A bit before I am getting a WARN log announcing the issue :
I am trying to follow in debug, and I end up pretty deep in Spring Boot internals 
 in  org.springframework.boot.context.properties.bind.Binder  .
I have a similar app with more or less same version for which it works just fine.
I am trying to find a difference, compare the execution flows, but not finding anything obvious.
In IntelliJ, I get the auto-completion so I know my yaml is formatted properly : the "web" value is proposed to me.
Any idea of how to investigate this kind of issue ?
I found there is an old issue  Sleuth/Zipkin tracing with @ControllerAdvice , but I meet the same problem with the latest version(spring-cloud-starter-zipkin:2.1.0.RELEASE), I debug it and find that the error is null, so zipkin just guess with statuscode.
I have to throw the exception again to make zipkin notify the exception
error is null
zipkin result
ControllerAdvice
throw the exception again, it works
In the micro-services based architecture , I have a services which helps me to fetch order details .
Internally order details fetches - customer details , delivery details , product details .
We have all services developed and the architecture is established.
No we export everything to zipkin with the sampler rate of 100%.
So it includes - INFO level logs and error level logs also , currently it is useful but we already have separate mechanism for ERROR level logs.
so , i just want to skip the zipkin logging for ERROR level log and  send only INFO level logs to zipkin
I tried searching through, but could not get any help, i am newbie to micro services
any help is appreciated , thank you
I created a spring boot 2.1.2 basic web app using the initializr tool.
The app starts fine and responds to a hello world kinda request.
When I then attempted to add zipkin and sleuth, I now get an error.
Process finished with exit code 0
Dependencies look like this
I tried to go down to spring boot version 2.0.4, which is the next down available through maven, however then the Jersey package started bucking.
Is there a way to get zipkin and sleuth to work with spring boot 2.1.2?
Swagger seem to have a similar issue.
I want to write the zipkin trace data from the recorder to a file using NodeJS in a format which zipkin ui supports so that i can import the file into the zipkin ui later and do analysis.
Actually I have a microservice architecture as follows
So I have 4 microservices and for every microservice I send a notification to zipkin when starts and finish it's objective.
I have to monitor my product to make sure all requested checkouts will have
zipkin as a tracking system already own all this information cause it keep the checkout track from the very beginning until the end, I'm wondering how I can query at zipkin all checkouts that have been processed by  JAVA REST API  microservice and didn't be processed by at least one of the others ( PAYMENT GATEWAY ,  SALE CREATOR  and  EMAIL NOTIFIER
How can I query on zipkin which checkouts haven't been processed by all others microservices after  REST API ?
Hi everyone!
I have "ELK" (6.4.2) working perfectly with filebeat, metricbeat, packetbeat and winlogbeat in CentOS 7 x86_64 (Kernel 3.10.0-862.11.6.el7.x86_64).
I'm trying to integrate zipkin + elk (see  https://logz.io/blog/zipkin-elk/ ), but Elasticsearch does not create indices with Kibana.
When trying to create the indices in Kibana, the process does not end.
(Follow logs below).
I suspect the zipkin connection drivers are not compatible with elk 6.4.2.
Has anyone had the same problem and has a "light at the end of the tunnel"?
Tks for all!
Java version:
Zipkin startup:
Error log in Elasticsearch:
Requirement is to export traces for requests that matches url pattern to zipkin from apps.
I got to know that there are options in sleuth properties to exclude traces from exporting.
But my case is the opposite of it.
Include traces for exporting for only specified url patterns.
I was trying to have a custom httpSampler and mentioned my logic to export the trace based on url patterns.
But it did not work as expected.
Any samples available on the same, would really be helpful?
Thanks much.
I have the following problem: i need to send traces to Zipkin via Kafka using Sleuth.
Based on what i read in the documentation( https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka ) all i need to do is to add  spring-kafka  dependency:
But i can't really see there to which kafka bootstrap server should i send or to which topic(is there some default topic that i should know about?
).
I have the following problem: i have added numerous services in Zipkin but now i want to remove some of them.
I keep the data in memory so no persistency involved there.
Is there any way to delete a service name from Zipkin's service name dropdown list?
I have two Microservices (Spring boot application) .
For tracing I am using  &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;  along with zipkin.
Service A is producer and  send message using RabbitMQ broker.
On other hand Service B is the consumer, their is   @RabbitListener  .
I want to exchange the traceId(with span details) from service A to Service B.
I have seen the  example  (using brave) but unable to integrate zipkin with rabbitMQ and trace propogation.
Can Any One please help me how to acheive this ?Any complete step-by-step and simple example?
In spring initializer i couldn't find following dependencies  zipkin ui ,  zipkin stream ,  stream rabbit .I know it was available but i don't know why they've deprecated those dependencies.
Are there any other alternatives dependencies spring initializer provide?
We have a spring-boot application (spring-boot-starter-parent-2.0.0.RELEASE) using spring-cloud-starter-zipkin for writing "spans" to zipkin.
We use spring-integration too (through spring-boot-starter-integration) and we have added an integration flow with a PollableChannel to be used within a poller:
Since adding this configuration, we are having an "asynch" span every second.
It seems this span comes from the @Poller, checking whether there are items in the queue.
I'd like to know how to control this span.
Is it possible to disable?
Specially if there are no items.
Thanks in advance!
I am looking for a Tracing tool for my spring web-mvc application and i ended up with using Brave-zipkin[ https://github.com/openzipkin/brave-webmvc-example/tree/master/webmvc3] .
Everything looks fine for me except that, in the given example jetty server deploys the application twice; one for FrontEnd and another for Backend(using two profiles).
Whereas my project uses tomcat-server.
Can anyone help me how to use this same tool for deploying in Tomcat-server and start the application without using profiles?
or please suggest any other open source tool for tracing simple monolithic spring-web-mvc application (not spring-boot) and i should be able to see the spans and dependency (eg controllerClass- serviceClass- repositoryClass just like we see under dependency tab of openzipkin web page:  http://localhost:9411/zipkin/dependency/ )
I am using zipkin to do distributed tracing of my microservice architecture.
I have created zipkin server by adding zipkin server dependency and @EnableZipkinServer Annotation.
Now is there a way I can add password protection to my zipkin web interface?
I am using Zipkin with Spring Sleuth to display traces.
When I use it locally,  http://localhost:9411/zipkin/dependency/  displays a nicely created graph of dependencies within the eco-system.
Sometimes, backends from outside that eco-system get called and those are not displayed in that graph.
Is it possible to annotate a call (let's assume RestTemplate and Feign clients) to such an external system so Zipkin would actually draw that dependency?
If it's possible, what do I have to do?
This would be my baseline of code:
Somewhere I would like to type  httpbin  so this call gets drawn in the dependency-graph of Zipkin.
Thank you!
//  Edit  based on current solution
I'm using Spring Cloud Finchley and added the following line before restTemplate's call:
I simply inject  SpanCustomizer  in this class.
The Span is sent to Zipkin and I see the tag is set:
Unfortunately, it is not drawn in the dependencies-view.
Is there anything else I need to configure, maybe in Zipkin rather than in Sleuth?
I have the following setup :
There used to be a  StreamEnvironmentPostProcessor  that did the job of adding the trace headers to the kafka bindings when I included the  spring-cloud-sleuth-stream  dependendy in the past.
But the doc clearly states now  :
Note: spring-cloud-sleuth-stream is deprecated and incompatible with these destinations
What should I do to make this work properly now ?
Add the headers to the bindings configuration myself ?
Or is there something I'm missing ?
in the documentation it is explained that your services have to resend a set of headers to enable pilot/Zipkin to correlate the information.
But who generates the first headers and set its values?
The Istio Ingress controller?
How can I configure it, enable/disable it?
Thank you.
I have done all the possible matches and mix-up of dependency and still not able to record traces in zipkin ans store it in MYSQL using RabbitMQ.
Still i can see the trace and span id's  in console and nothing beyond this.
Someone please take a look at the code in github from below location.
Github code:  https://github.com/javayp/distributed-tracing-1
I'm testing zipkin to spring boot integration but im facing error like below.
The error seems to happen when it tries to send message to zipkin server
here is my pom.xml file.
it may be have a problem in version.
and this is my application.yml file
im running zipkin server in same machine with different port
any guide or information are welcomed!
In our cluster, we have set up a Zipkin collector for Stackdriver Trace ( like this ) so we can trace our apps.
I am running the simple  JavaScript web example  that is offered.
It works correctly when I configure the app to send the traces to the collector that is running in the cluster (in  recorder.js ).
However, when I want to inspect the traces in Stackdriver Trace, something seems to be going wrong:
The  HTTP Method  column is empty, and the  URI  column seems to show the HTTP method.
How can I make these columns display the correct information?
Let me know if I need to add more information.
I have a heterogeneous(Java, php, python, C#.Net) micro-service system which was written by several teams.
All communication happens over HTTP connections.
I objective is to use Zipkin to trace the path of execution and identify the slowest services and start profiling them using (VisualVM, dotTrace).
I've heard that Zipkin supports HTTP connectivity for tracing.
How would I go about doing this?
Is Zipkin even the right approach?
I'm looking for direction and some Java examples to get started.
Is there a http format I could use or do I need to use multiple (Wingtips, ZipkinTracerModule,Brave) libraries?
Thanks.
In the zipkin web ui, when the request url is  http://10.19.138.169:9411/zipkin/api/v1/trace/ae60bd175a61e820
I find the return response is 
[
    {
        "traceId": "ae60bd175a61e820",
        "id": "ae60bd175a61e820",
        "name": "client",
        "timestamp": 1511858133224433,
        "duration": 508444,
        "binaryAnnotations": [
            {
                "key": "lc",
                "value": "",
                "endpoint": {
                    "serviceName": "monitor-gw-http-c",
                    "ipv4": "10.19.138.169"
                }
            }
        ]
    },
    {
        "traceId": "ae60bd175a61e820",
        "id": "19d69c3e93bc9040",
        "name": "post",
        "parentId": "ae60bd175a61e820",
        "timestamp": 1511858133239803,
        "duration": 490921,
        "annotations": [
            {
                "timestamp": 1511858133239803,
                "value": "cs",
                "endpoint": {
                    "serviceName": "monitor-gw-http-c",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "timestamp": 1511858133383290,
                "value": "sr",
                "endpoint": {
                    "serviceName": "monitor-gw-web",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "timestamp": 1511858133609368,
                "value": "ss",
                "endpoint": {
                    "serviceName": "monitor-gw-web",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "timestamp": 1511858133730724,
                "value": "cr",
                "endpoint": {
                    "serviceName": "monitor-gw-http-c",
                    "ipv4": "10.19.138.169"
                }
            }
        ],
        "binaryAnnotations": [
            {
                "key": "ca",
                "value": true,
                "endpoint": {
                    "serviceName": "",
                    "ipv4": "127.0.0.1",
                    "port": 43928
                }
            },
            {
                "key": "http.path",
                "value": "/security/gateway",
                "endpoint": {
                    "serviceName": "monitor-gw-web",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "key": "http.path",
                "value": "/security/gateway",
                "endpoint": {
                    "serviceName": "monitor-gw-http-c",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "key": "sa",
                "value": true,
                "endpoint": {
                    "serviceName": "",
                    "ipv4": "127.0.0.1",
                    "port": 8090
                }
            }
        ]
    },
    {
        "traceId": "ae60bd175a61e820",
        "id": "16eefe087852af41",
        "name": "ennmonitorsecuritygatewayserver/put",
        "parentId": "19d69c3e93bc9040",
        "timestamp": 1511858133393425,
        "duration": 212916,
        "annotations": [
            {
                "timestamp": 1511858133393425,
                "value": "cs",
                "endpoint": {
                    "serviceName": "monitor-gw-web",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "timestamp": 1511858133588237,
                "value": "sr",
                "endpoint": {
                    "serviceName": "monitor-gw-s",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "timestamp": 1511858133593907,
                "value": "ss",
                "endpoint": {
                    "serviceName": "monitor-gw-s",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "timestamp": 1511858133606341,
                "value": "cr",
                "endpoint": {
                    "serviceName": "monitor-gw-web",
                    "ipv4": "10.19.138.169"
                }
            }
        ]
    },
    {
        "traceId": "ae60bd175a61e820",
        "id": "8ef78f0edefe3a4b",
        "name": "data enqueue",
        "parentId": "16eefe087852af41",
        "timestamp": 1511858133592958,
        "duration": 129,
        "binaryAnnotations": [
            {
                "key": "lc",
                "value": "",
                "endpoint": {
                    "serviceName": "monitor-gw-s",
                    "ipv4": "10.19.138.169"
                }
            }
        ]
    },
    {
        "traceId": "ae60bd175a61e820",
        "id": "97c637bcc891b86a",
        "name": "data dequeue, send to kafka",
        "parentId": "16eefe087852af41",
        "timestamp": 1511858133593147,
        "duration": 2416,
        "binaryAnnotations": [
            {
                "key": "lc",
                "value": "",
                "endpoint": {
                    "serviceName": "monitor-gw-s",
                    "ipv4": "10.19.138.169"
                }
            }
        ]
    },
    {
        "traceId": "ae60bd175a61e820",
        "id": "f193c7f4193f2879",
        "name": "",
        "parentId": "16eefe087852af41",
        "timestamp": 1511858133594113,
        "duration": 7575,
        "annotations": [
            {
                "timestamp": 1511858133594113,
                "value": "ms",
                "endpoint": {
                    "serviceName": "monitor-gw-s",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "timestamp": 1511858133601688,
                "value": "ws",
                "endpoint": {
                    "serviceName": "monitor-gw-s",
                    "ipv4": "10.19.138.169"
                }
            }
        ],
        "binaryAnnotations": [
            {
                "key": "kafka.topic",
                "value": "rdkafka",
                "endpoint": {
                    "serviceName": "monitor-gw-s",
                    "ipv4": "10.19.138.169"
                }
            }
        ]
    },
    {
        "traceId": "ae60bd175a61e820",
        "id": "54a3f6268df0aaee",
        "name": "",
        "parentId": "f193c7f4193f2879",
        "timestamp": 1511858133600067,
        "duration": 5,
        "annotations": [
            {
                "timestamp": 1511858133600067,
                "value": "wr",
                "endpoint": {
                    "serviceName": "monitor-kafka-consumer",
                    "ipv4": "10.19.138.169"
                }
            },
            {
                "timestamp": 1511858133600072,
                "value": "mr",
                "endpoint": {
                    "serviceName": "monitor-kafka-consumer",
                    "ipv4": "10.19.138.169"
                }
            }
        ],
        "binaryAnnotations": [
            {
                "key": "kafka.topic",
                "value": "rdkafka",
                "endpoint": {
                    "serviceName": "monitor-kafka-consumer",
                    "ipv4": "10.19.138.169"
                }
            }
        ]
    }
]
It can easy to find that there are 8 spans.
When I use the api to get the trace with the same traceId
        ElasticsearchStorage storage = ElasticsearchStorage.newBuilder()
                .hosts(Arrays.asList(" http://10.19.138.169:9200 ")).build();
I get 
[
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "16eefe087852af41",
    "id": "97c637bcc891b86a",
    "name": "data dequeue, send to kafka",
    "timestamp": 1511858133593147,
    "duration": 2416,
    "localEndpoint": {
      "serviceName": "monitor-gw-s",
      "ipv4": "10.19.138.169"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "id": "ae60bd175a61e820",
    "name": "client",
    "timestamp": 1511858133224433,
    "duration": 508444,
    "localEndpoint": {
      "serviceName": "monitor-gw-http-c",
      "ipv4": "10.19.138.169"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "f193c7f4193f2879",
    "id": "54a3f6268df0aaee",
    "kind": "CONSUMER",
    "timestamp": 1511858133600067,
    "duration": 5,
    "localEndpoint": {
      "serviceName": "monitor-kafka-consumer",
      "ipv4": "10.19.138.169"
    },
    "tags": {
      "kafka.topic": "rdkafka"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "ae60bd175a61e820",
    "id": "19d69c3e93bc9040",
    "kind": "SERVER",
    "name": "post",
    "timestamp": 1511858133383290,
    "duration": 226078,
    "localEndpoint": {
      "serviceName": "monitor-gw-web",
      "ipv4": "10.19.138.169"
    },
    "remoteEndpoint": {
      "ipv4": "127.0.0.1",
      "port": 43928
    },
    "tags": {
      "http.path": "/security/gateway"
    },
    "shared": true
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "19d69c3e93bc9040",
    "id": "16eefe087852af41",
    "kind": "CLIENT",
    "name": "ennmonitorsecuritygatewayserver/put",
    "timestamp": 1511858133393425,
    "duration": 212916,
    "localEndpoint": {
      "serviceName": "monitor-gw-web",
      "ipv4": "10.19.138.169"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "19d69c3e93bc9040",
    "id": "16eefe087852af41",
    "kind": "SERVER",
    "name": "ennmonitorsecuritygatewayserver/put",
    "timestamp": 1511858133588237,
    "duration": 5670,
    "localEndpoint": {
      "serviceName": "monitor-gw-s",
      "ipv4": "10.19.138.169"
    },
    "shared": true
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "16eefe087852af41",
    "id": "f193c7f4193f2879",
    "kind": "PRODUCER",
    "timestamp": 1511858133594113,
    "duration": 7575,
    "localEndpoint": {
      "serviceName": "monitor-gw-s",
      "ipv4": "10.19.138.169"
    },
    "tags": {
      "kafka.topic": "rdkafka"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "ae60bd175a61e820",
    "id": "19d69c3e93bc9040",
    "kind": "CLIENT",
    "name": "post",
    "timestamp": 1511858133239803,
    "duration": 490921,
    "localEndpoint": {
      "serviceName": "monitor-gw-http-c",
      "ipv4": "10.19.138.169"
    },
    "remoteEndpoint": {
      "ipv4": "127.0.0.1",
      "port": 8090
    },
    "tags": {
      "http.path": "/security/gateway"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "ae60bd175a61e820",
    "id": "19d69c3e93bc9040",
    "kind": "SERVER",
    "name": "post",
    "timestamp": 1511858133383290,
    "duration": 226078,
    "localEndpoint": {
      "serviceName": "monitor-gw-web",
      "ipv4": "10.19.138.169"
    },
    "remoteEndpoint": {
      "ipv4": "127.0.0.1",
      "port": 43928
    },
    "tags": {
      "http.path": "/security/gateway"
    },
    "shared": true
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "19d69c3e93bc9040",
    "id": "16eefe087852af41",
    "kind": "SERVER",
    "name": "ennmonitorsecuritygatewayserver/put",
    "timestamp": 1511858133588237,
    "duration": 5670,
    "localEndpoint": {
      "serviceName": "monitor-gw-s",
      "ipv4": "10.19.138.169"
    },
    "shared": true
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "16eefe087852af41",
    "id": "97c637bcc891b86a",
    "name": "data dequeue, send to kafka",
    "timestamp": 1511858133593147,
    "duration": 2416,
    "localEndpoint": {
      "serviceName": "monitor-gw-s",
      "ipv4": "10.19.138.169"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "16eefe087852af41",
    "id": "f193c7f4193f2879",
    "kind": "PRODUCER",
    "timestamp": 1511858133594113,
    "duration": 7575,
    "localEndpoint": {
      "serviceName": "monitor-gw-s",
      "ipv4": "10.19.138.169"
    },
    "tags": {
      "kafka.topic": "rdkafka"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "19d69c3e93bc9040",
    "id": "16eefe087852af41",
    "kind": "CLIENT",
    "name": "ennmonitorsecuritygatewayserver/put",
    "timestamp": 1511858133393425,
    "duration": 212916,
    "localEndpoint": {
      "serviceName": "monitor-gw-web",
      "ipv4": "10.19.138.169"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "ae60bd175a61e820",
    "id": "19d69c3e93bc9040",
    "kind": "CLIENT",
    "name": "post",
    "timestamp": 1511858133239803,
    "duration": 490921,
    "localEndpoint": {
      "serviceName": "monitor-gw-http-c",
      "ipv4": "10.19.138.169"
    },
    "remoteEndpoint": {
      "ipv4": "127.0.0.1",
      "port": 8090
    },
    "tags": {
      "http.path": "/security/gateway"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "id": "ae60bd175a61e820",
    "name": "client",
    "timestamp": 1511858133224433,
    "duration": 508444,
    "localEndpoint": {
      "serviceName": "monitor-gw-http-c",
      "ipv4": "10.19.138.169"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "f193c7f4193f2879",
    "id": "54a3f6268df0aaee",
    "kind": "CONSUMER",
    "timestamp": 1511858133600067,
    "duration": 5,
    "localEndpoint": {
      "serviceName": "monitor-kafka-consumer",
      "ipv4": "10.19.138.169"
    },
    "tags": {
      "kafka.topic": "rdkafka"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "16eefe087852af41",
    "id": "8ef78f0edefe3a4b",
    "name": "data enqueue",
    "timestamp": 1511858133592958,
    "duration": 129,
    "localEndpoint": {
      "serviceName": "monitor-gw-s",
      "ipv4": "10.19.138.169"
    }
  },
  {
    "traceId": "ae60bd175a61e820",
    "parentId": "16eefe087852af41",
    "id": "8ef78f0edefe3a4b",
    "name": "data enqueue",
    "timestamp": 1511858133592958,
    "duration": 129,
    "localEndpoint": {
      "serviceName": "monitor-gw-s",
      "ipv4": "10.19.138.169"
    }
  }
]
It is easy to find that there are 18 spans.
It seems that some of spans are merged in the web request, I want to know to where is the cource code deal this.
Thanks!
I'm using the camel-zipkin component to trace a request that flows between two different services:
service-a: Camel application running on Spring Boot, acting as a simple HTTP proxy (for the purposes of this proof of concept).
Zipkin support provided by the camel-zipkin module.
Route:
service-b: Spring Boot application with a REST controller.
Zipkin support provided by the spring-cloud-starter-kubernetes-zipkin module from Spring Cloud.
When I make a request to service-a, I see part of the trace in Zipkin: I see the client request from service-a, and I see the server request in service-b, as well as the spans I've added there to instrument various parts of the request-path.
However, I don't see the server request from the Camel portion, including the additional two seconds caused by the delay I've put in the route.
Tracing the camel-zipkin code, I've realized that the server request will only be traced if there is already a trace ID header, due to this line:
 https://github.com/apache/camel/blob/c6c02ff92a536e78f7ed1b9dd550d6531e852cee/components/camel-zipkin/src/main/java/org/apache/camel/zipkin/ZipkinTracer.java#L753
With this knowledge, I am able to get the entire trace as expected if I manually provide my own tracing headers (X-B3-TraceId, X-B3-Sampled, and X-B3-SpanId).
However, I would like to be able to start a trace even if the client doesn't specify one.
Based on my reading of the camel-zipkin code, I think I can create a PR that will induce my desired behavior.
Before I do that, though, I want to verify a couple of things:
Thanks!
I have a client application with multiple channels as SOURCE/SINK.
I want to send logs to Zipkin server.
According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP.
At client side:
Q1.
Is there an automatic configuration for zipkin rabbit binding in such scenario?
If not, what is default channel name of zipkin SOURCE channel?
Q2.
Do I need to configure defaultSampler to AlwaysSampler()?
At Server side:
Q1.
Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using:
 wget -O zipkin.jar 'https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec' 
...as stated on  https://zipkin.io/pages/quickstart.html  ?
Q2.
How do I configure zipkin SINK channel to destination?
Spring boot version: 1.5.9.RELEASE
Spring cloud version: Edgware.RELEASE
Following the link  Istio/Distributed tracing , I can get the tracing working with zipkin.
Currently in order for the client/caller to know about the x-request-id (in case no id is sent, zipkin creates one), he 
needs to send it as a part of the request.
This gives him the ability of trace the request.
All works well.
However, I am thinking maybe it is not a good idea for the client to send the x-request-id to avoid issues of constraints/duplication.
It would be good if it is possible that at the istio level, one should be able to modify the response headers and send the x-request-id back.
I am not finding such capabilities for istio at present.
If there is a way to achieve this, please let me know.
When I use:
The error log is:
But when I delete the dependency, it runs normally.
I can't find the reason, I don't know why it's going to be a problem.
this is my fegin interface
I'm using the feign interface  like this
this is my parent pom.xml config
I tried to modify the POM file, like:
still error.
How can I solve this?
I can see spans being recorded in the Zipkin UI from the following Spring Boot controller code:
and the log output looks like:
but the traces are independent in the UI.
I would expect the two calls to Google and Facebook urls to be nested concurrently under the call to the  /concurrent1  endpoint.
I suspect that it's due to the thread that coroutines are executed on being different to the one in which the spring application is started but I have no idea how to move forward with Spring Sleuth at this point!
I am using Spring cloud Zipkin to trace calls with sample percentage 0.4.
I am not using any persistent storage like MySQL or Cassandra.
Could you please let me know how to set data retention period in Zipkin server e.g.
I want to check only 6 hours/1 day data.
Or if I can set max span count
everyone
I am trying to use Zipkin to trace services in OpenStack.
I know it is a huge project for me.
So I wonder if there is an open source library for Zipkin tracing OpenStack.
I think I searched it before and if my mind does not cheat me, there is one presentation (only slices) for this.
However, I can not find it.
Can someone help with it?
I know there is the library, osprofiler, for tracing OpenStack, while the example of API seems unclear to me.
Could you please give me a more detailed or even a complete example, maybe like Zipkin  https://github.com/openzipkin/pyramid_zipkin-example
I do not mean it is not helpful.
It seems I still have to find the RESTful request point in OpenStack, for example creating an instance may trigger one service to request neutron for networking, and I may have to locate the front end code and add a tracing code.
If using py_zipkin, I can add decorator @zipkin_span(some params) before it.
The problem is it is tough for me to find the front end of these services like Nova, neutron, cinder and so on.
It seems osprofiler does the same thing.
My understanding is highly likely wrong, and I appreciate who can help with it.
By the way, I do not intend to trace a big project like OpenStack.
I intend to trace a RESTful-like or RPC system with Zipkin to collect the information to analyze.
Unfortunately, I have found a middle-size open source project.
So I choose OpenStack.
If you could provide me something else, that will be very helpful.
:)
Thank you very much.
I've been roaming the depths of the internet but I find myself unsatisfied by the examples I've found so far.
Can someone point me or, show me, a good starting point to integrate zipkin tracing with jaxrs clients and amqp clients?
My scenario is quite simple and I'd expect this task to be trivial tbh.
We have a micro services based architecture and it's time we start tracing our requests and have global perspective of our inter service dependencies and what the requests actually look like (we do have metrics but I want more!)
.
The communication is done via jax-rs auto generated clients and we use rabbit template for messaging.
I've seen brave integrations with jaxrs but they are a bit simplistic.
My zipkin server is a spring boot mini app using stream-rabbit, so zipkin data is sent using rabbitmq.
Thanks in advance.
I am raising this query as a result of seeing another query with no satisfactory answer (and reading the advice to not ask another question in an answer or comment).
That reference is  Enabling Sleuth slows requests down (a lot)
My issue is similar.
I am not using Feign.
I am using the following:
I am using spring-cloud-sleuth, logback and zipkin.
When i remove the zipkin pom reference
the performance is very quick.
But when I put it back, the performance is very poor.
Changing my log level from  INFO  to  DEBUG  in the logging changes the non zipkin calls from 701ms to 1051ms.
Adding zipkin http changes that timing from 1051ms to around 53 seconds.
In the code, i make one call to my service which in turn makes 303 calls to an arango database (via its REST interface).
I am using spring.cloud.starter-sleuth and logstash-logback-encoder.
By themselves there is no performance issue, it is only when i add the spring-cloud-starter-zipkin that the performance degrades.
I am running zipkin on my local pc in a docker instance, and running my service out of eclipse as a standalone app (spring boot).
Arango is on my local pc running as a service.
Unit test methods of my controller (using mockito to mock the mvc calls) that usually take 0.3 to 0.5 seconds each without zipkin, end up taking ~16seconds each when zipkin  is  enabled.
The Zipkin UI reports all the calls and the sum of the calls is the roughly the full time of the call as reported by Postman.
The Application class looks like this at the top:
The end of the log file looks like the following without the zipkin reference:
And with the zipkin reference it looks like this:
some logs removed to fit in the 30k character limit
I have zipkin server running and when i hit the zipkin client rest api end point i am getting the error below
2017-06-10 23:16:08.782  INFO [product,d650504f4f922a51,d650504f4f922a51,true] 5676 --- [ix-ProductAPI-3] com.accenture.api.ProductAPI             : List all Product
Hibernate: select product0_.prod_id as prod_id1_0_, product0_.prod_description as prod_des2_0_ from product product0_
2017-06-10 23:16:09.801  WARN [product,,,] 5676 --- [ender@698c7814)] z.r.AsyncReporter$BoundedAsyncReporter   : Dropped 1 spans due to HttpClientErrorException(404 null)
org.springframework.web.client.HttpClientErrorException: 404 null
    at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:63) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:700) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:653) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:628) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:590) ~[spring-web-4.3.8.RELEASE.jar:4.3.8.RELEASE]
    at org.springframework.cloud.sleuth.zipkin.RestTemplateSender.post(RestTemplateSender.java:73) ~[spring-cloud-sleuth-zipkin-1.2.0.RELEASE.jar:1.2.0.RELEASE]
    at org.springframework.cloud.sleuth.zipkin.RestTemplateSender.sendSpans(RestTemplateSender.java:46) ~[spring-cloud-sleuth-zipkin-1.2.0.RELEASE.jar:1.2.0.RELEASE]
    at zipkin.reporter.AsyncReporter$BoundedAsyncReporter.flush(AsyncReporter.java:228) [zipkin-reporter-0.6.12.jar:na]
    at zipkin.reporter.AsyncReporter$Builder.lambda$build$0(AsyncReporter.java:153) [zipkin-reporter-0.6.12.jar:na]
    at zipkin.reporter.AsyncReporter$Builder$$Lambda$1.run(Unknown Source) [zipkin-reporter-0.6.12.jar:na]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
we are using elasticsearch as the storage of zipkin, the dependency are as follows:
now, trace log was storaged in elasticsearch, and we can see trance spans in the zipkin ui,but cannot to see span details or find trace via traceid,with the exception of :
I am trying to create a tracer, then a span from the tracer.
Performing work.
Then closing the span.
I would expect that the close action will call spanReporter.report, which should post the data to an available Zipkin server (default localhost)
However, that is not happening.
Code below.
I noticed that I was using NeverSampler.
Changed it to AlwaysSampler (for now).
I was also using NoOpSpanReporter as the default SpanReporter (which does nothing).
I want to change it to a ZipkinSpanReporter.
(Or something else).
This is where I am stuck.
Questions:
Now the code snippet which uses the above:
Note: 
This particular project does not have any http calls in any of its services.
Its a data processing project.
@SpringBootApplication has not been used.
(All zipkin examples use it though).
Is it necessary?
Hi  Hoping somebody can help am trying to get a very basic implementation of zipkin working to get to grips with distributed tracing.
I am using the spring boot to do this but cannot seem to get it to work.
Nothing appears in the zipkin UI when I try to find traces for a my service.
I have got 2 deployments as follows:
My spring boot app which I am wanting to log:
pom.xml
With these dependencies I do get a connect error because of the rabbit mq dependency.
I had to include this becuase I got a META-INF/spring binders error.
Wasnt really sure how to get around this other than putting the dependency in.
My application.class
When I run this aplication and call my endpoint I can see through the logging that it should be sending it to zipkin.
This are my full logs.
Again I get the rabbitmq exception but not sure why i actually need this.
Can zipkin not work without it
My full logs:
The 2nd application I deployed is my zipkin client / UI
Pom .xml
My application.class
application.properties
Running the zipkin server it starts up ok but nothing is shown in the trace logs excpet for an error
When I go to the client I do get this error though
Any help is appreciated here
Thanks in advance
I am using the Kubernetes to deploy and trace data from application using zipkin.
I am facing issue in replacing MySQL with Elasticsearch since I am not able to get the idea.
Even the replacement is done on command line basis, using STORAGE_TYPE="Elasticsearch" but how that can be done through kubernetes?
I am able to run the container from docker imgaes but is there any way to replace through deployment?
I have several services.
I am instrumenting them using Zipkin.
In each module, in build.gradle is added a dependency to Zipkin:
In each module, in application.properties file are following settings:
I call a specific endpoint that use other 3 modules, in total are 4 modules.
Entire setup is on my laptop.
I realized that Zipkin introduces a lot of overhead.
I used Mozilla to compare the results.
The small values are when Zipkin does not record the requests and the big value is when Zipkin records.

Do you have any idea why there are so much overhead?
Thank you.
We're looking to implement Zipkin in our stack.
As I look into Zipkin it makes sense to me to extend the Zipkin system to handle generic flags as well.
Observations:
Conclusion:
I have a question regarding Zipkin with elasticsearch storage.
After updating spring-cloud-sleuth to  1.1.1.RELEASE (because we updated spring boot from 1.3.8 to 1.4.4 and spring cloud from Brixton.SR6 to Camden.SR4) we also updated  zipkin-storage-elasticsearch  and  zipkin-autoconfigure-storage-elasticsearch  to version  1.16.2  in the pom.xml of our zipkin service.
We are using elasticsearch version  2.4.1 .
When we boot up the service we get an 
 error on zipkin's ui  and a stacktrace:
Zipkin was working with spring-cloud-sleuth  1.0.0.RELEASE , zipkin-storage-elasticsearch, zipkin-autoconfigure-storage-elasticsearch  1.7.0  and elasticsearch  2.3.5 .
What am I missing here?
Which versions should work together?
Can be Zipkin used to instrument a console/classic application?
I mean I have a method foo() and I want to know how much time it took.
Does Zipkin can be used just for applications that communicates over http protocol?
Thanks
I am facing an issue where in the ZipKin UI is failing to load in the traces from MySQL.
It is giving me below mentioned error on UI -
Error executing query: SQL [select distinct  zipkin_spans .
trace_id 
  from  zipkin_spans  join  zipkin_annotations  on
  ( zipkin_spans .
trace_id  =  zipkin_annotations .
trace_id  and
   zipkin_spans .
id  =  zipkin_annotations .
span_id ) where
  ( zipkin_spans .
start_ts  between ?
and ?
and
   zipkin_annotations .
endpoint_service_name  = ?)
order by
   zipkin_spans .
start_ts  desc limit ?
]; Expression #1 of ORDER BY
  clause is not in SELECT list, references column
  'zipkin.zipkin_spans.start_ts' which is not in SELECT list; this is
  incompatible with DISTINCT
I see below exception on ZipKin Server -
My ZipKin Server Configuration is a below -
When I query the MySQL schema, I can see the record being populated in "zipkin.zipkin_spans and zipkin.zipkin_annotations" table.
But when I try to load the Zipkin UI, it give me above error on UI.
Any help is highly appreciated.
I have created a spring-boot application which publishes zipkin logs to a zipkin consumer.
But the Zipkin consumer(another spring boot application) is behind some authentication filters which check for several parameters/headers in the request before allowing.
How to I use my own custom HttpClient to publish my messages from the producer in this case?
I would like to create my own basic, minimalistic library used for distributed tracing with Zipkin.
I will be sending traces via HTTP and nothing more fancy.
My question is if there is any more information about this topic than in the Zipkin docs and the source code of Zipkin and Brave?
I would like not to rely on Spring Framework.
Spring Cloud Sleuth works very well, but my services are not built using Spring.
Do you know any resources?
Or do you have any ideas where to start doing this?
We are using finagle stack and taught of adding zipkin for tracing our micro-services.
I am able to see our tracing happening but parent finishes before the child.
I have already opened an issue here: 
 https://github.com/openzipkin/docker-zipkin/issues/100
Any help would be really appreciated.
I do have several services interacting with each other, and all of them, sending traces to openzipkin (  https://github.com/openzipkin/docker-zipkin  ).
While i can see the system behaviour in detail , looks like the 'dependencies' tab does not display anything at all.
The trace i check has 6 services, 21 spans and 43 spans, and i believe something should appear.
I'm using latest ( 1.40.1 ) docker-zipkin, with cassandra as storage, and 
just connecting to the cassandra instance, can see there's no entry in the dependencies 'table'.
why ?
Thanks
I am trying to install  zipkin , after following the steps given ( https://github.com/twitter/zipkin/blob/master/doc/install.md ), when I access  http://localhost:8080/  on the web browser, instead of the zipkin UI, it gives,
and gives out an error on the query terminal.
Can please anyone help me how to resolve this ?
?
I am working in the server side of an application and I am getting the tracer id in the form of X-b3 headers from the client side in the request object.
Now how can I create a child span for that tracer using these x-b3 headers?
I have some tools running in my kubernetes cluster (ELK, zipkin,..) and i want to know in which namespace to place them, for example i have fluentd which is a daemonset running in kube-system namespace so should i place elasticsearch in the same namespace or put them together in a custom namespace so they can reach each other, i just want to know what is the best practice to do it
Im trying to implement distributed tracing using Spring, RabbitMQ, Sleuth and Zipkin.
So I added the dependencies:
And configured  sleuth  and  zipkin  in my  bootstrap.yml :
So starting my services and making some rest calls I get this in the logs:
For now it looks good.
Sleuth added the tracing ID's.
Calling the Zipkin UI I can see that the service names where added:
But there are no tracing informations at all:
So im wondering what im missing in my configuration.
EDIT
Turned out there are tracing informations arriving in zipkin.
I can use the search bar in the top right corner to search for tracing id's directly:
I will then get:
So the question is why is there nothing in the overview or queryable via the trace lookup?
We are using Istio/Zipkin as a tracing system on our server to add the dynamic headers through Istio sidecar proxy for tracing the request later on using Zipkin.
Is there anyway we could disable istio for a certain request.
Problem is, we are working with JMS queues, and when JMS listener tries to listen to a certain queue, it sees the headers like x-request-id added by the istio dynamically and it gives an error (as it'll accept header keys only in camelCase or with underScore or $).
We can change header keys added by istio, so we want either istio to not to add headers in some specific requests (the one queue is making).
I've searched google but couldn't find anything about it.
Following is the error message we are getting:
I am using spring Boot Version 1.5.14.RELEASE with spring cloud sleuth zipkin.
If I return a ResponseEntity setting its HttpStatus as BAD_REQUEST then I see the trace highlighted in Blue color.
Is there a way to highlight the trace in Red color for a bad request with ResponseEntity object?
I explicitly threw a custom Exception for bad requests and saw the zipkin trace highlighted in Red color in Zipkin UI.
But I don't want to do this as I am returning a body in ResponseEntity.
I expect the Zipkin trace to be highlighted in Red color as it is a bad request but the actual color is Blue.
Actual Trace
Expected Trace
I'm using spring-cloud's sleuth with zipkin with kafka.
here is my pom.xml configure.
and ,the blew is my application.yaml:
and then, I start the spring boot application,try to access by browsers,but I can't find the spring cloud send sleuth  to kafka.
So I try to debug the zipkin2.reporter.AsyncReporter, and find that every time the [flush] method return when go to line 265 which is "if (!bundler.isReady() &amp;&amp; !closed.get()) return;"  the code show as below.
I know it means bufferFull is false.but why every time it is not bufferFull even if I try invoke the URL frequently?
thank you for attention.
i know this is a very general question but i simply don't know how to start.
I have spring-boot applications which serve a thrift API via HTTP.
The same or another spring-boot app is using the thrift-client of another application to communicate.
my goal is to trace the communication path with zipkin.
i could imagine, i need to somehow intercept incoming and outcoming http-calls with the application-type x-thrift but simply have no idea how to do this and properly integrate with zipkin libraries.
any hint how to start on this is highly appreciated, thanks a lot in advance
I am new in spring-cloud.My projects are about spring-cloud-config, spring-cloud-eureka, spring-cloud-zipkin,I am running projects locally and it is normal.When I put my projects in Ubuntu, the project 'spring-cloud-zipkin' runs incorrectly.
The error is about 'Exception in thread "main" java.lang.ClassNotFoundException', thanks for your answer very much.
enter image description here
and my code:
enter image description here
enter image description here
in the pom.xml:
 enter image description here
I need to use Zipkn Serve to trace my spring boot application.Here is my configurations of application.yml
But the spans not being created in Zipkin.I have added all the required dependencies to my service's pom file.
and the zipkin service's pom file.
I am using Zipkin Slueth with Spring boot.
Now my zipkin is working fine in normal case but when I spawn 3 new threads from main thread, it generate different traces and not 1 trace.
So i am unable to see the complete request.
Same starts working if I do everything in main thread?
My Pom for including dpendencies
Properties
My spring cloud version is Dalton.SR5
So slueth sends traces to zipkin auto.
This is all I have configured for zipkin.
Can i use rxjava schedules hook?
How?
I am unable to use it?
SpringCloud version:Dalston.SR1,
rabbitMQ version:3.6.10,ElasticSearch version:6.2.4
There was nothing unusual when I use MySQL as a storage.
Now I use ElasticSearch.I can't find any services.
I lost something?
here is the picture:
application.properties
pom.xml
zipkin is a tool for tracing request as well as tracking the span of time a service took to process the request useful in multi-service projects it doesnt require much effort for setting up u just have to add zipkin dependency in your services and define a sampler bean.
add the following dependency in project
compile group: 'org.springframework.cloud', name: 'spring-cloud-starter-zipkin', version: '1.3.2.RELEASE'
add sampler bean inside ur project
`
add above bean when u want only fraction of ur requests traces to send to zipkin else define a bean
add 
 spring.zipkin.base-url=localhost:9411  in ur properties file and host the zipkin server on the same port defined above.
but if u r using api-gateway for accessing zipkin (in case of deplyment in cloud ) or inside proxy u may face the issue of broken ui elements when accessing thru gateway in this case im  using zuul with propertis as:
zuul.routes.zipkin.path=/zipkin/*
zuul.routes.zipkin.url=http://localhost:9411
First I want to integrate  zipkin  +  rabbitmq  into my project.
So my  pom.xml  is below:
So after I add this.
I can't not invoke my controller.
But if the controller
in the same package with the Application, can the controller be invoked?
I'm following  this  guide, with Zipkin.
I have 3 microservices involed,  A -&gt; B -&gt; C , I'm propagating headers from A to B and from B to C.
But in the Zipkin dashboard I only see entries for  A -&gt; B  and  B -&gt; C , not  A -&gt; B -&gt; C .
Those are the headers:
I can see that in B  x-b3-parentspanid  is null and I guess that's wrong, but the other are working I think...how is it possible?
EDIT:
added code snippets to show headers propagation
A -&gt; B  propagation:
...
B -&gt; C  propagation:
...
I'm working with JMS and queues (Azure queues) for the first time.
I'm required to make a queue where Rubi server would write some data and Java would read it from queue and will do further executions.
This process is working fine locally on my machine.
I've created a REST endpoint which is writing data in the queue and once data is written in the queue, the listener would take over and read the data and execute.
When we deploy it to Azure the error I can see in logs which is not letting the Queues start is
Zipkin is also present on the Azure server as a distributed tracing system and I guess this  x-request-id  is related to Zipkin which is creating the problem.
I've searched Google for the issue but couldn't understand why its happening.
Following is detailed error message:
Spring Boot Cloud Disovery Question,  Problem with Eureka hostname after docker upgrade on windows 10.
(Note: docker is not hosting spring services, just mariadb, rabbitmq, and zipkin)
Summary
Everything worked fine until the docker update today, after the docker upgrade
Eureka returns  "host.docker.internal"    as the hostname for my development box (machine hosting the spring boot cloud services)
This has worked fine until the docker updgrade on windows 10 today.
Any guidance on this one?
------------------------------ Details ----------------------------
"
---------------- Versions of spring ----------
I am using windows 10 enterprise for java development.
I use docker-compose to host mariadb, zipkin, and rabbitmq in my dev env on my windows 10 box
I have a multi-project gradle build with 8 spring boot cloud services
One of the services is a spring cloud discovery service hosting Eureke
The other spring cloud services are eureka clients.
Until today, everything worked
1) Eureka spring boot cloud services are started first
2) Other spring boot cloud services that are clients of the eclipse startup, register and query the spring cloud discovery client code to obtain the URL of the other services
Today,  The latest docker for windows 10 was pushed out, and I installed it (I have been developing this app through several other docker updates).
I updated docker, did a reboot.
After the reboot,  The Eureka server is returning  "host.docker.internal"  as the hostname in the URL  instead of  http:/mymachinename:8080
I don't have the network data before the upgrade, but now it is
My client application.properties file is:
Server application.property file
We are trying to implement Spring cloud sleuth with Zipkin in our project and wanted to know if Spring cloud sleuth will support DB calls with Spring data JPA.
I want to trace the time taken for DB calls just like service calls
When I make a service call with RestTemplate, that gets sent to zipkin and I am able to see that on the dashboard
But DB interactions with Spring data jpa is not getting displayed in Zipkin
While Zipkin sdk is available for Node.js, I'm looking for auto-instrumentation like Spring Cloud Sleuth in Node.js app.
Is there a module or framework for it in Node.js?
What I mean by auto-instrumentation above is that in Java I don't have to write code to instrument servlets/filters/rest clients with Zipkin.
Sleuth automatically does that.
While Zipkin instrumentation seems manual in Node.js.
i am using python flask in my application.
I want to change the header before each request in order to add information for zipkin distributed tracing.
My current code looks like:
Unfortunately, this is not working as the method  append  does not exist.
What is the right approach?
Best regards
Martin
I have a Spring Boot 2.0.0 REST service where I'm trying to enable Sleuth and Zipkin to send traces to my localhost Zipkin server.
The app worked fine unti I add the two dependencies  spring-cloud-starter-sleuth  and  spring-cloud-sleuth-zipkin  to my pom.xml.
Once I did that, I'm now getting a compilation error:
Project build error: Non-resolvable import POM: Could not find artifact io.zipkin.brave:brave-bom:pom:4.16.3-SNAPSHOT
I've ensured it's not a corrupt Maven package issue by deleting my .m2 folder and updating (twice).
Why am I getting this error and how can I fix it?
This is my pom.xml:
I have developed several micro services using spring cloud Netflix stack (Eureka, Zuul, Zipkin, config server etc.)
Is there any open source / free solution available to spin up/down microservices instances.
e.g.
if CPU usage   90% for 3 consecutive checking it will add one more instance.
I try to add a distributed tracing in my microservices (under Kubernetes in Azure).
I added the dependencies in the parent pom.xml :
I use 1.4.1 and CAMDEN.SR4 because fabric8 kubeflix doesn't support newer versions.
I forced 1.1.3.RELEASE to try newest sleuth version to see if it was a bug in older version of sleuth.
I use this configuration of logback-spring.xml :
And here is my application.yml :
The zipkin URL is a Kubernetes services exposing the Zipkin server (Spring boot app with @EnableZipkinServer)
I then call a first service (services-1) with this code :
which produces these logs :
As you can see it calls the services-i18n-2 service with a RestTemplate, which produces these logs :
As you can see the traceId in service-2 (e0c6495a0a598cff) is different from the service-1 (eaf3dbcb2f92091b).
And in service-2 the traceId is the same as the spanId.
Questions :
FYI, I have Hystrix in the dependencies and I have removed the @HystrixCommand to be sure it was not a problem with Hystrix creating a new traceId at each HTTP call.
I have implemented a Distributed Transaction Logging library with Tree like Structure as mention in Google Dapper( http://research.google.com/pubs/pub36356.html ) and eBay CAL Transaction Logging Framework( http://devopsdotcom.files.wordpress.com/2012/11/screen-shot-2012-11-11-at-10-06-39-am.png ).
Log Format
GUID HEX NUMBER FORMAT
What I would like to do is to integrate this format with Kibana UI and when user want to search and click on on TRACE_GUID it will show something similar to Distributed CALL graph which show where the time was spent.
Here is UI  http://twitter.github.io/zipkin/ .
This will be great.
I am not UI developer if some can point me how to do this that will be great.
Also I would like to know how I can index elastic search payload data so user specify some expression like in payload (duration   1000) then, Elastic Search will bring all the loglines that satisfy condition.
Also, I would like to index Payload as Name=Value pair so user can query  (key3=value2 or key4 =  exception ) some sort of regular expression.
Please let me know if this can be achieved.
Any help pointer would be great..
Thanks,
Bhavesh
I'm going to design distributed system with Scala and Akka.
I want to aggregate tracing messages from a cluster and have possibility to view them in some kind of UI.
Is Zipkin the best solution, or Flume(+some wrapper?
), or something else?
We are setting up microservice framework.
We use following stack for distributed tracing.
Following is how the configuration is done
In  gradle.build  (or pom.xml) following starter dependencies added
Add one AlwaysSampler bean
If we have  kafka  running, things work automatically.
But if kafka is not running, server does not start - this is mostly the case for development environment.
If I want to stop this, I have to comment out all the code mentioned here (as we use starter dependency with spring boot, it automatically configures as I understand).
Can we just make some changes in properties (or yaml) files so that I don't need to go and comment out all these code?
Or probably another way to disable this without doing some commenting, etc.
I am using spring cloud slueth with zipkin in spring boot to trace the services calls.
My spring cloud version is Edgware.RELEASE
Now I when I tried to trace my facade layer which uses rxjava, it creates 12 traces for a single request?
What should I do ?
I mean i want just 1 trace should be generated for 1 request of facade layer(facade layer do parallel calls using rxjava).
I am using slueth with http and have not change any property if properties file
I have added these 2 dependencies:
This I have added in properties file:
i am trying to evaluate zipkin to enable distributed tracing capability for all our micro-service.
Below are versions in my setup.
Spring-boot version:  1.5.7.RELEASE
spring-cloud version:
 Camden.SR6
zipkin version :  2.2.1
Configuration for seluth in  application.properties
spring.sleuth.sampler.percentage=1.0
spring.sleuth.web.skipPattern=(^cleanup.
|.+favicon. )
And i created the ZipkinSpanReporter bean as below.
Note that I have setup the Eureka server as all micro services and even zipkin server registerred with Eureka server so that the Zipkin client can resolve zipkin server via eureka
What I have observered is that the zipkin client (book) is not reporting all spans back to zipkin server when I checked the zipkin.
Some are reported, almost of spans are dropped
I have enabled the logging for
below are logging info:
But I could not be able to find the traceId which is logged in  book.log  file from zipkin console
Could you please explain why many spans are not reported to zipkin server?
Thanks in advance.
My application is a spring-rabbitmq based application(neither spring-cloud nor spring-boot), requests were received from one queue and sent responses to another queue.
I want to use brave to trace the system by injecting Zipkin headers before sending messages and extracting Zipkin headers right after receiving messages.
The problem is  In step3 of the following scenario, how can I get span1 before sending message?
Scenario:
Code snippet before sending message:
In the above code,  Span currentSpan = tracer.currentSpan();  , the  currentSpan  is always  null .
Code snippet after receiving message:
Brave configuration code:
Following are my references:
https://github.com/openzipkin/brave/tree/master/brave#one-way-tracing
https://github.com/openzipkin/brave/blob/master/brave/src/test/java/brave/features/async/OneWaySpanTest.java
https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30
I’m working on a system of microservices, implemented in Scala with Finagle and Thrift as the platform.
As there are a few services that nobody touched for a while, I need to find out if they are used at all anymore (or rather, which parts are not used anymore).
For that, IMHO a simple invocation count for each method would suffice (since the service was started, or possibly in the last 24h).
As far as I see, the Finagle/Thrift integration does not bring something like this built-in, at least not exposed in the admin panel.
So what would be the most clever way to do this?
Just add a filter that counts the invocations and exposes them via the admin interface?
Or would Zipkin (possibly with custom code) help here?
I have a web service written in scala and built on top of twitter finagle RPC system.
Now we are hitting some performance issues.
We have external API components and database layer.
I am planning of installing Zipkin in order to have a service level tracing system.
This will allow me to know where the bottleneck is at the service level.
I am wondering though if there are framework out there to monitor the performance inside my application layer.
The application is a suite of filters that are applied consecutively to my data and I would like to know which filter take time to compute.
I heard about JVM profiling but it seems a little overkill for what I want to do.
What would you recommend ?
Thanks for your help.
I'm making a sequential request using Feign Builder.
There are no x-b3-traceid,x-b3-spanid .. in the title of the request.
That's why the log my last client appears on the zipkin.
I use spring  boot 2.4.2 , spring cloud 2020.0.0 , feign-core 10.10.1 , feign-okhttp 10.10.1.
I have tried spring-cloud-openfeign and i achieved wanted result.
But i don't want to use this lib.
There are requests  when using Feign Builder and Rest Template in here.
I dont' see same log at zipkin.
My Client1 App.
I am sending request http://localhost:8082/
This is yml of my client1 app.
I use same yml conf on other client apps which client2 and clint3.
Only changes port and app name.
This is my Feign at Client2 app.
Here impl of ClientFeign2.
This is my Feign at Client3 app.
Here impl of Client3 Feign.
pom.xml from client3 and i use  client3 at client2/pom and the same as client1.
diff-feign-rest-image
feign-zipkin-img
feign-request-img
rstlet-rest-tmplte-img
rest-zipkin-img
Trying to read version from properties file but getting below exception
Below is the  pom.xml  and  properties  file
I'm comparing different tracing backend using OpenCensus.
I already have the simple OpenCensus.io python samples running fine using Zipkin and Azure Monitor.
Now I'm trying to test using GCP's Stackdriver...
I have set up the test code from Opencensus
 https://opencensus.io/exporters/supported-exporters/python/stackdriver/  as follows:
I have set the environment variable for  GCP_PROJECT_ID  and also have my key file path for my service account JSON file set in  GOOGLE_APPLICATION_CREDENTIALS .
The service account has the  &quot;Cloud trace agent&quot;  role.
My code runs through with no errors but I can't see any info appearing in the GCP console under traces or in the monitoring dashboard.
Am I missing something?
Environment notes:
I'm testing this from my local Windows machine using Python 3.7.2
We have problem with propagation of  traceId  in requests which are called by spring oauth2 module.
For instance consider authorization and resource server.
In resource server we have spring security configuration to ensure get rsa public key from authorization server with following property:
When I call controller of resource server with jwt token, I can see in zipkin traces from resource server and authorization server as well, but there is no traceId propagation from resource server to authorization server.
First record is calling rest api to get resources, and second record is produced call to authorization server to find out public RSA key.
We use istio to use distributed tracing.
Our microservices sometimes need to hit external APIs, which usually communicate over https.
To measure the exact performance of the whole system, we want to trace the communication when hitting an external API.
However, distributed tracing requires access to the header of the request, but https does not allow access because the header is encrypted.
For confirmation, I deployed bookinfo on GKE with istio enabled, entered the productpage container of the productpage pod, and executed the following command.
Only http communication was displayed on zipkin.
Is it possible to get a series of traces, including APIs that use external https?
I'm using Kubernetes on Azure cloud, and I have installed zipkin.
I already install nginx ingress, and if I use the following host rule, it works fine:
But this is not what I want.
What I want is something like hostname.com/zipkin.
I tried with this, but I got a 404 error:
What do I have to do?
Edit:
I tried to add the host and after doing a describe command i get this
EDIT:
I solved my issue adding a rewrite rule annotation
I have two services(S1, S2) in a chain.
I call with CURL(or Postman) S1, and S1 sends request to S2.
S1 has -  spring.sleuth.sampler.probability: 0.1 
S2 has -   spring.sleuth.sampler.probability: 0.5
I don't understand how the system will behave.
If i send 100 requests:
or
or
I'm trying to generate zipkin trace id from nginx in order to be able to trace from nginx to applications.
To achieve this, I want to find out how to generate 16 random bytes to be used for X-B3-SpanId since $request_id generates 32 bytes (which can be used for X-B3-TraceId).
I am trying to deploy to ibm-cloud a node web application with Angular 2, node.js,express and mongo.
My app works fine without adding the socket.io require line:
However, every time i try to add the socket.io and execute the container, the console logs (for each request):
Error: Can't set headers after they are sent.
at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
    at SendStream.send (/app/node_modules/send/index.js:618:10)
    at onstat (/app/node_modules/send/index.js:730:10)
    at FSReqWrap.oncomplete (fs.js:153:5)
  Error: Can't set headers after they are sent.
at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
     at SendStream.send (/app/node_modules/send/index.js:618:10)
     at onstat (/app/node_modules/send/index.js:730:10)
     at FSReqWrap.oncomplete (fs.js:153:5)
  Error: Can't set headers after they are sent.
at SendStream.headersAlreadySent 
  (/app/node_modules/send/index.js:390:13)
     at SendStream.send (/app/node_modules/send/index.js:618:10)
     at onstat (/app/node_modules/send/index.js:730:10)
     at FSReqWrap.oncomplete (fs.js:153:5)
This is my code
        // Uncomment following to enable zipkin tracing, tailor to fit your network configuration:
        // var appzip = require('appmetrics-zipkin')({
        //     host: 'localhost',
        //     port: 9411,
        //     serviceName:'frontend'
        // });
Thank you very much!.
I have a bunch of micro-services hosted on AWS.
I am using StatsD, Graphite and Grafana to monitor them.
Now I want to expand it to monitor the queues (SQS) through which these micro-services are talking to each other.
How can I leverage Graphite/ Grafana to do this?
Or a better approach if there aint any support/ plugin for the same.
Thanks :)
PS : If it's gotta be Zipkin, please tell me they can co-exist or is there a catch to using multiple tracers.
I have a spring boot microservice: Zuul-api-gateway-server, and I am trying to implement a Zipkin server listening to rabbitmq for logging messages within the microservice.
I have added the following dependencies to this microservice:
I have started the Zipkin server using the following commands:
SET RABBIT_URI=amqp://localhost
java -jar zipkin.jar
I then try to start up the microservice however I get the following error:
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'rabbitListenerContainerFactory' defined in class path resource [org/springframework/boot/autoconfigure/amqp/RabbitAnnotationDrivenConfiguration.class]: Initialization of bean failed; nested exception is java.lang.NullPointerException
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:584) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:846) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:863) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
      at com.shopping.sandbox.netflixzuulapigatewayserver.NetflixZuulApiGatewayServerApplication.main(NetflixZuulApiGatewayServerApplication.java:16) [classes/:na]
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_171]
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_171]
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_171]
      at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_171]
      at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) [spring-boot-devtools-2.1.1.RELEASE.jar:2.1.1.RELEASE]
  Caused by: java.lang.NullPointerException: null
      at org.springframework.amqp.rabbit.config.AbstractRabbitListenerContainerFactory.getAdviceChain(AbstractRabbitListenerContainerFactory.java:198) ~[spring-rabbit-2.1.2.RELEASE.jar:2.1.2.RELEASE]
      at brave.spring.rabbit.SpringRabbitTracing.decorateSimpleRabbitListenerContainerFactory(SpringRabbitTracing.java:170) ~[brave-instrumentation-spring-rabbit-5.4.4.jar:na]
      at org.springframework.cloud.sleuth.instrument.messaging.SleuthRabbitBeanPostProcessor.postProcessBeforeInitialization(TraceMessagingAutoConfiguration.java:186) ~[spring-cloud-sleuth-core-2.1.0.M2.jar:2.1.0.M2]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:419) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1737) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
      ... 20 common frames omitted
We have an application and where we are tracing the whole logs using a custom correlation header i.e.
X-CID with a value as for example, 615b7eea-6d4c-4efb-9431-fcbba084ea3f.
Recently we have integrated the Spring Sleuth with the train version as Greenwich.BUILD-SNAPSHOT in our application and using brave's Span and tracer to create trace and span ad sending it to Zipkin.
What we are looking for a mechanism where we can use or set the CID value as a traceId rather than using the auto generated one from Sleuth.
The rational behind is, we would like to make it uniform CID value to use or search in Zipkin UI rather than two different values i.e.
CID value and Spring Sleuth traceId to trace the complete API calls.
Please note that, we would like to reuse the X-CID from request which is already supplied in the request header.
Are there any APIs to override this behavior or any alternate way to achieve this ?
TLDR :
More details :
I'm trying to create a custom Sleuth instrumenting service that can also work when no tracing is available.
Here is a really simplified version of the service interface :
I then create an AutoConfiguration (as this code is in a separate module, used by many projects) :
The idea is really to have a kind of Noop version of the service when no Tracer is available.
I then declare the the AutoConfiguration in a spring.factories file as usual.
This is the application I want to run :
For all of this I want to use Zipkin, with a KafkaSpanReporter.
So my application.properties look like this :
and my (truncated) pom.xml like this :
When I try to run this code I get an error :
If I look at the configuration report I see :
This is weird, because there is a  @EnableConfigurationProperties(KafkaProperties.class)  on  KafkaAutoConfiguration  and the configuration report clearly shows :
What is even weirder is that if I remove the  @AutoConfigureAfter(name = "org.springframework.cloud.sleuth.autoconfig.TraceAutoConfiguration")  in my AutoConfiguration, the service starts all right =  but I get the  NoTracedConfiguration  flavour of my bean, so  Tracer  is probably not configured yet.
What can I do to fix this problem ?
I'm working on the microservices project that is the dockerized Spring Cloud Netflix project and contains 3 microservices except for some Netflix services which are turbine,zipkin,discovery,configserver etc , yet.
(Its just working on locally now..)
Soon, I decided to deploy my project to a cloud provider with an orchestration tool.
After some researches, I decided to use Kuberenetes.
But, both of Spring Cloud Netflix and Kubernetes have some solutions for distributed systems: service discovery, load balancing, fault-tolerance, etc..
In that case, using Netflix libs.
seem unnecessary with Kubernetes.
I read  this  and  this .
I think Spring Cloud Kubernetes looks like a workaround solution.
So my questions are :
I'm experiencing issues scaling my app with multiple requests.
Each request sends an ask to an actor, which then spawns other actors.
This is fine, however, under load(5+ asks at once), the  ask  takes a massive amount of time to deliver the message to the target actor.
The original design was to bulkhead requests evenly, but this is causing a bottleneck.
Example:
In this picture, the  ask  is sent right after the query plan resolver.
However, there is a multi-second gap when the Actor receives this message.
This is only experienced under load(5+ requests/sec).
I first thought this was a starvation issue.
Design:
Each planner-executor is a seperate instance for each request.
It spawns a new 'Request Acceptor' actor each time(it logs 'requesting score' when it receives a message).
I'm a bit stumped by this.
From these tests it does not look like a thread starvation issue.
Back at square one, I have no idea why the message takes longer and longer to deliver the more concurrent requests I make.
The Zipkin trace before reaching this point does not degrade with more requests until it reaches the  ask  here.
Before then, the server is able to handle multiple steps to e.g veify the request, talk to the db, and then finally go inside the planner-executor.
So I doubt the application itself is running out of cpu time.
I have a Kubernetes's and spring boot's env variables conflict error.
Details is as follows:
When creating my zipkin server pod, I need to set env variable  RABBITMQ_HOST=http://172.16.100.83，RABBITMQ_PORT=5672 .
Initially I define zipkin_pod.yaml as follows:
With this configuration, when I do command
The console throws error:
so I modified the last line of zipkin_pod.yaml file as follows: Or use brutal force to make port number as int.
Then pod is successfully created, but spring getProperties throws exception.
When I check logs:
My question is how to let kubernetes understand the port number as int, while not breaking spring boot convert rule from string to int?
because spring boot could not convert  !
!31503 to int 31503 .
I am using Spring Cloud for Creating Microservice Architecture.
I was using the below feature from the Spring Cloud
Now Lets say if I have 100 microservices, then we need 100 servers to maintain each microservices.
So I thought of using Kubernetes to solve this issue by  deploying each microservices in a separate docker container, so now since Kubernetes takes care of microserivice health check, autoscaling, load-balancing so do I need to again use Ribbon, Eureka and Zuul.
Can anyone please help me on this
I got a  Spring Boot  application making use of  Spring Sleuth  for tracing inter-service calls.
Within that application a  ScheduledExecutorService  exists that performs http requests in a loop (pseudo-code below):
If I now have a look at the traces produced by Sleuth and stored in  Zipkin  I can see that all http calls are associated to a single Trace.
Most likely because the trace context is handed over during the call to  ScheduledExecutorService::submit .
How can I clear the current trace before starting the next iteration so that each http call will result in a new detached/orphaned trace?
I should say I'm really impressed with the simplicity and usefulness of spring-cloud-sleuth and zipkin.
However, I'm working on a POC for which I'm considering reactive toolkits.
Vertx 3 is the first item in my list to try (with spring cloud ecosystem).
I'm wondering if Sleuth log tracing would work in a reactive context as I guess it relies on ThreadLocals to pass around the context?
Keen to understand where Sleuth would stand in a reactive environment.
I use Spring Cloud Feign and Sleuth with a Zipkin server.
My problem is that when I enable Sleuth, then any simple request takes at least 600ms.
Note that for tests purposes, I've set the sampler percentage of Sleuth at 1.
Can I do something to improve that?
Here some log of a request which takes 25ms without Sleuth and 700ms with Sleuth.
(user calls /teams which calls /cities):
I'm trying to get an openzipkin server running in a k8s cluster, starting with testing in a minikube.
I'm beginner with k8s config, but here's what I've done so far:
What I think I'm doing is starting a new pod and deploying the zipkin image, then exposing the Web UI at port 9411 via zipkin-http.
After doing this:
Then I run the kubectl proxy so I can access the Web UI from my browser:
Now if I browse to  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json  I get the config file contents:
But if I browse to the root at  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/  I receive an error:
The config.json it's attempting to load is the one at :9411/config.json.
The request to load /config.json comes from a JS file that was loaded by the html in the root page.
Since it looks like I can get to the json file directly from both inside and outside the cluster, I'm confused as to why the JS file isn't able to load it.
What am I doing wrong here?
Thanks!
I am using  zipkin-go-opentracing , which is an implementation of the  opentracing  API for zipkin in go.
For (reasons) I need to get the traceId from a span.
So the question:
given a opentracing.Span, how do I get the TraceId?
Everything I've tried has given me some kind of type assertion error.
Thanks,
I am running mysql and zipkin in kubernetes.
Zipkin is failing to connect to mysql database.
I've checked environment variables and all variables are set up properly.
Exception
mysql-deployment.yaml
mysql-service.yaml
zipkin-deployment.yaml
zipkin-service.yaml
secret.yaml
I think, I need to change Charset in zipkin.
But, I have no way to change it other than build another image myself.
I have recently started exploring sping boot 2.
I am uisng logaback for logging purpose.
For distributed logging tracing, I wanted to use sping boot sleuth starter.
But with below dependency in pom.xml without zipkin integration its not adding traceid in logs.Also added spring.application.name property in application.properties file.
Am I missing anything here?
I am using Spring Cloud Sleuth and Zipkin (via HTTP), by adding spring-cloud-starter-zipkin version 2.0.0.M6 to my dependencies (based on Spring Boot 2.0.0.RC1 and Spring Cloud Finchley M6).
I am using @Newspan annotation to mark a child span around some (expensive) operation.
When the span information is sent to Zipkin, I notice that the timestamp and duration of the child span are missing.
This leads to a strange rendering on Zipking side.
However, when I create the child span by calling tracer#newChild, it works as expected.
Am I missing something?
Would this be an issue with Sleuth 2.0.0.M6?
When I run the same code using Spring Boot 1.5.9 and Spring Cloud Edgware SR2, it behaves as expected.
Here's the JSON received on Zipkin side.
The span named "child-span-with-annotation" is the one created using @NewSpan, whereas the span "childspanwithnewchild" is created using tracer#newChild.

 
 [
  {
    "traceId": "b1c2636366c919be",
    "id": "b1c2636366c919be",
    "name": "get",
    "timestamp": 1518495271073166,
    "duration": 862032,
    "annotations": [
      {
        "timestamp": 1518495271073166,
        "value": "sr",
        "endpoint": {
          "serviceName": "sample-sleuth-app",
          "ipv4": "---.---.---.---"
        }
      },
      {
        "timestamp": 1518495271935198,
        "value": "ss",
        "endpoint": {
          "serviceName": "sample-sleuth-app",
          "ipv4": "---.---.---.---"
        }
      }
    ],
    "binaryAnnotations": [
      {
        "key": "ca",
        "value": true,
        "endpoint": {
          "serviceName": "",
          "ipv6": "::1",
          "port": 51982
        }
      },
      {
        "key": "http.path",
        "value": "/hello",
        "endpoint": {
          "serviceName": "sample-sleuth-app",
          "ipv4": "---.---.---.---"
        }
      },
      {
        "key": "mvc.controller.class",
        "value": "MyRestController",
        "endpoint": {
          "serviceName": "sample-sleuth-app",
          "ipv4": "---.---.---.---"
        }
      },
      {
        "key": "mvc.controller.method",
        "value": "sayHello",
        "endpoint": {
          "serviceName": "sample-sleuth-app",
          "ipv4": "---.---.---.---"
        }
      }
    ]
  },
  {
    "traceId": "b1c2636366c919be",
    "id": "14be7ac6eafb0e01",
    "name": "child-span-with-annotation",
    "parentId": "b1c2636366c919be",
    "binaryAnnotations": [
      {
        "key": "class",
        "value": "MyService",
        "endpoint": {
          "serviceName": "sample-sleuth-app",
          "ipv4": "---.---.---.---"
        }
      },
      {
        "key": "method",
        "value": "expensiveOperation1",
        "endpoint": {
          "serviceName": "sample-sleuth-app",
          "ipv4": "---.---.---.---"
        }
      }
    ]
  },
  {
    "traceId": "b1c2636366c919be",
    "id": "b34a4f910f27fdb4",
    "name": "childspanwithnewchild",
    "parentId": "b1c2636366c919be",
    "timestamp": 1518495271479040,
    "duration": 453747,
    "binaryAnnotations": [
      {
        "key": "lc",
        "value": "",
        "endpoint": {
          "serviceName": "sample-sleuth-app",
          "ipv4": "---.---.---.---"
        }
      }
    ]
  }
]
Can someone provide a delete query to delete older records(older than 5 days)for the below mysql tables with its reference tables?
(All reference tables records also need to deleted otherwise crashes my application)
https://github.com/openzipkin/zipkin/blob/master/zipkin-storage/mysql/src/main/resources/mysql.sql
I have no clue to write a coordinated delete query (deleting multiple tables in same query)
Small question regarding some actuator endpoints returning 404 please.
I have a web app, based on Webflux 2.4.2, and for testing this issue only, I am using
Actuator is working, because a curl will get the response for /health /metrics and other endpoints.
However, for those endpoints  /auditevents /httptrace /integrationgraph /sessions , I am not able to get anything, besides a http 404.
[05/Feb/2021:13:00:18 +0000] &quot;OPTIONS /auditevents HTTP/1.1&quot; 404 141 55 ms
Those are really the only endpoints returning 404, still do not know why.
Don't want to spam with one question same question per endpoint.
All other actuator endpoints are fine.
Thank you
I have a reactive application using Spring Webflux.
I have used sleuth annotations like  @NewSpan  to create spans, but I am getting warning like
I know  Flux.subscribe  is a final method so proxies are not generated correctly, but I can still see those spans on Zipkin.
I need to know what are the implications of this warning.
And how can I avoid this?
Related to  https://github.com/openzipkin/zipkin/pull/3239  , we came across some (maybe) odd behaviour and i wanted to know if below test works as expected or not:
Basically we wanted to disable all  TRACE  requests, and set  pathPrefix(&quot;/&quot;)  to achieve this.
But for some reason the  OPTIONS  call to  /something  gets trapped in the same path.
If i remove the route decorator things work as expected.
I am new to Microservices.
(Learning phase).
I have a question.
We deploy microservices at cloud.
(e.g.
AWS).
Cloud already provide load balancing and logs.
And We also implement Load Balancing(Ribbon) and logs(Rabbit MQ and Zipkin) in Spring Boot.
What is the difference in these two implementation?
Do we need both?
Can some answer these questions.
Thanks in advance.
I deploy zipkin in docker (zipkin-server-2.21.7-exec.jar) and I connect with rabbit in docker.
I'm using Eureka in docker to register microservices.
When I run one this microservices this error compare
APPLICATION FAILED TO START
Description:
Parameter 2 of method reporter in org.springframework.cloud.sleuth.zipkin2.ZipkinAutoConfiguration required a bean of type 'zipkin2.reporter.Sender' that could not be found.
The following candidates were found but could not be injected:
Bean method 'rabbitSender' in 'ZipkinRabbitSenderConfiguration' not loaded because @ConditionalOnBean (types: org.springframework.amqp.rabbit.connection.CachingConnectionFactory; SearchStrategy: all) did not find any beans of type org.springframework.amqp.rabbit.connection.CachingConnectionFactory
Bean method 'restTemplateSender' in 'ZipkinRestTemplateSenderConfiguration' not loaded because ZipkinSender org.springframework.cloud.sleuth.zipkin2.sender.ZipkinRestTemplateSenderConfiguration rabbit sender type
Action:
Consider revisiting the entries above or defining a bean of type 'zipkin2.reporter.Sender' in your configuration.
I use this properties
spring.zipkin.sender.type=rabbit
spring.zipkin.base-url=http://zipkin-server:9411/
We have an EKS cluster in AWS and i am using istio as service mesh in my cluster.
We are using istio only for injecting the sidecar into applications and to trace the application traffic through zipkin.
To access the application from outside we are not using istio-ingressgateway instead we are using ALB &amp; ELBs
So my problem is I am not getting any traces to zipkin / kiali when i am accessing my application through AWS LBs.
Do i have to use istio-ingressgateway to record the traces in zipkin and view in kiali or is there a way to get traces using ALB/ELB as a loadbalancer?
Running a deployment with three pod.
The only one pod container I can connect is zipkin with  kubectl exec -it my-api-XXX -- /bin/bash .
If I want to access the my-api container using  kubectl exec -it my-api-XXX -c &lt;my-api container ID&gt; -- /bin/bash .
It report a error show the container is not in that pod.
Error from server (BadRequest): container my-api_containerID is not valid for pod my-api-XXX
I am new to K8s and I am trying to migrate my service (which currently utilizes docker-compose.yml) to k8s.
My service
deploys zipkin and elasticsearch
and these can be accessed at  'localhost:9411'  and  'localhost:9200'  respectively.
The most commonly used solution I found online was 'kompose' and I tried to run,
2.
Once I finish this, I run kubectl get pods and I can see my deployments, but elasticsearch and zipkin are no more responsive on their respective localhost ports.
Ouput of  'kubectl get pods'
Output of  'docker ps'
Output of  curl http://localhost:9200
Can someone tell me why this is happening and how to debug?
We are building event-driven microservices using Spring Cloud Stream(with Kafka binder) and looking at options for tracing Micorservices that are not exposed as http end point.
Please suggest.
I understand that using Sleuth will automatically add trace and span id to logs if it is over http.
Documentation is not clear for using it with Spring Cloud Stream -  https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth#_messaging
Found an example but not sure whether it is the right approach -  https://github.com/bjedrzejewski/food-order-publisher/blob/zipkin-example/src/main/java/com/e4developer/foodorderpublisher/FoodOrderController.java
Thanks
I want to send RabbitMQ messages tracking event to Zipkin with using spring cloud sleuth, After many research I found some configuration added recently to spring in order to manage it you can find in  here , But unfortunately there is not any  documentation that explains how can we configure it, I tried many ways but I couldn't send tracking events to Zipkin.
Please advice
I need to set the traceId with an existing Id (we have created some kind of correlation-id from the main origin app) into brave tracer.
I don't want to use the Spring Sleuth/brave created one as  I want to make it consistent throughout my different micro-services.
I am able to create traces and span and able to send all details into Zipkin.
My sample snippet:
I am using: Spring Cloud 'Greenwich.BUILD-SNAPSHOT' and brave.
The whole purpose is to search using correlationId rather than traceId in zipkin ui.
In our Spring Boot application (2.0.4.RELEASE), we use Zipkin to integrate distributed tracing.
When creating the integration manually with a 10% sampling rate, meaning with a  @Configuration  like this:
our application has a 50 percentile performance of about 19ms and a 99.9 percentile of about 90ms at around 10 requests per second.
When integrating Sleuth 2.0.2.RELEASE instead like this in gradle:
the performance drops massively to a p50 of 49ms and a p999 of 120ms.
I tried disabling the different parts of the Sleuth integration ( spring.sleuth.async.enabled ,  spring.sleuth.reactor.enabled , etc.
).
Disabling all these integrations brings the performance to p50: 25ms, p999: 103 ms.  Just having Sleuth adds about 15-25% of overhead.
It turns out that the one thing with the significant impact is setting  spring.sleuth.log.slf4j.enabled  to  false .
If all other integrations are enabled, but this is disabled, the performance stays within the Sleuth overhead mentioned above, although nothing is logged.
So my question is:
Is there a way to avoid the overhead by Sleuth (compared to "manual" tracing) and especially the one done by the SLF4J integration?
we are trying to make parallel calls to different recipient using scatter-gather and it works fine.
But the second recipient flow is not starting unless the first one is complete(traced in Zipkin).
is there is a way to make all recipients async.. very similar to split-aggregate with executor channel.
flow2(),flow3(),flow4() methods are methods with  InterationFlow  as return type.
sample code  flow2()  :
I'm learning about microservices and I need spring cloud cli for this test project.
I have installed spring boot cli (extracted and added to path) version Spring CLI v2.0.3.RELEASE.
I have installed the spring cloud cli plugin (spring-cloud-cli:1.3.1.RELEASE), and verify it with checking its version.
I would like to execute [spring cloud eureka configserver zipkin]
but I'm getting:
File ........\.m2\repository\org\springframework\cloud\launcher\spring-cloud-launcher-deployer\1.3.3.BUILD-SNAPSHOT\spring-cloud-launcher-deployer-1.3.3.BUILD-SNAPSHOT.jar must exist
I am using rxjava 2 with spring boot 2 and spring cloud finchley.rc2.
Now i am tracing the requests with sleuth / zipkin.
But i am getting one issue that IO threads are reusing the olds requests traceids.
When I am doing this, traceids generated are fine(same for all parallel calls in same request )
But when I am doing this, it is not working properly.
I am assuming since io() threads are reused, thats why they are using old data since they are old threads?
But how can i force them to use the trace id for current main thread?
How can i fix this?
I cant move the webflux as i have lot of code writtenin rxjava?
Currently, I am working on my company's microservices solution which uses  Spring Cloud Edgware.SR1 .
This solution includes the following main flavors:
 api-gateway(Zuul) ,  service-discovery(Eureka) ,  uaa ,  zipkin-server  and  business logic  services.
I am trying to provide a good tracing for all requests in our system.
In Zipkin UI I can see a trace for the request that starts in api-gateway, going through uaa to our business logic service.
But requests to the Eureka are missing in the trace.
Instead, there is a separate trace with a single span for  service-discovery  endpoint  http://eureka/apps/** .
I had the same issue with  uaa  request to  http://user/  endpoint and solved it by adding  TraceRestTemplateInterceptor  to our Oauth2 client.
However, I found it difficult to override  EurekaHttpClient  and add the mentioned interceptor.
Is there any other way to propagate trace id to Eureka Clients?
I have created a spring boot application with spring cloud sleuth.
For POC purposes, I used zipkin on 
my local machine and I am able to instrument a external service which is not instrumented by creating 
manual span.
I reffered below link.
https://cloud.spring.io/spring-cloud-sleuth/1.2.x/multi/multi__customizations.html
Now, When I move to PCF environment, then I am unable to collect proper custom spans.
PCF metrics always shows parent span and service with total time taken.
Could anyone please let me know where I am going wrong.
Zipkin Output:-
PCF Metrics:-
UPDATE 
screen shot for Zipkin with @NewSpan.
PCF metrics screen shot without call hierachy
I have been trying to get Spring boot 2.0 and Spring Cloud Slueth 2.x (POM= Finchley.M6) working, but no avail.
I have a  service1  calls  service2  and  service3 .
I see that a new  traceId  is created whenever a request is received in  service1  but not passed to  Service2  and  Service3  instead a new  traceid  is being created every time on  Service2  and  Service3 .
Is this anyhow related to  this defect  ?
NOTE: I don't need zipkin support and I need sleuth for distributed tracing and will be using Splunk as log aggregater.
Source Code :  https://github.com/trmsmy/springboot-cloud-examples/tree/springboot2
I am currently running Spring Cloud Edgware.SR2.
I am migrating services from RabbitMQ to Kafka, and at this point I don't see any Zipkin traces when I run kafka-console-consumer.sh on the zipkin topic (i.e.,  kafka-console-consumer.sh --new-consumer --bootstrap-server localhost:9092 --topic zipkin --from-beginning ).
As a result, I of course don't see any trace information in the Zipkin UI.
Following are the dependencies I have as part of the producer service:
These are the dependency overrides I had to make after pulling in the  spring-cloud-stream-binder-kafka11  dependency per the instructions  at the bottom of the Spring Cloud Stream project page .
I also took a look at the instructions for  Sleuth with Zipkin via RabbitMQ or Kafka , and I think I have that part correct.
The documentation states  If you want Sleuth over RabbitMQ add the spring-cloud-starter-zipkin and spring-rabbit dependencies.
It specifically mentions  spring-cloud-starter-zipkin  is needed for RabbitMQ, but I added it even though I'm using Kafka since it didn't work without this dependency either.
Any ideas on what I'm missing or have configured incorrectly to capture Sleuth traces and send them to the Zipkin server using Kafka?
I'm trying to trace services in Openstack Mitaka using osprofiler, but i'm having some issues.
It seems it's not possible to trace nova service in Mitaka using osprofiler (correct me if i'm wrong).
So i was thinking of using Zipkin.
Can anyone tell me if Zipkin integrates with openstack mitaka?
I have a Java Spring-Cloud-based microservice integrated with RabbitMQ using Spring Boot Starter AMQP (extract from a  pom.xml  below):
Now I would like to connect this service to the Zipkin monitoring using Sleuth.
According to the  documentation , when AMQP support is enabled, Sleuth sends all its data through a RabbitMQ queue.
For some reason I would like to disable this default behaviour and send data via HTTP.
Probably there is one magic property which I cannot find.
Do you know how I can force my application to send Sleuth-related data via HTTP to a Zipkin Server (also a Spring Boot application with  @EnableZipkinServer  annotation)?
In addition I would like to mention that after removing the AMQP support everything works fine, i.e.
Zipkin receives tracing data via HTTP.
Moreover, setting both  spring.zipkin.collector.http.enabled: true  and  spring.zipkin.collector.amqp.enabled: false  (and  spring.zipkin.collector.rabbitmq.enabled: false ) does not help.
I'm using  Spring Cloud Stream  binder from the Edgware release to send Kafka messages.
I'm also using  Spring Sleuth  with  Zipkin .
Spring embeds headers into the Kafka message using a custom class  EmbeddedHeaderUtils .
This causes a problem for some non-Spring consumers of the message who would have to deal with this custom decoding.
My Question:  Is there a way to configure Spring with a custom encoder/decoder for message headers (e.g.
plain JSON)?
Or possibly use Kafka Headers?
Ideally any custom implementation needs to work with Spring Sleuth and Zipkin.
I've been having a look at the latest Finchley release to see if Kafka headers will be supported but not sure about that.
Does ELK stack provide micro service and network latency monitoring in kibana?
Zipkin  provides details bout service request and service response duration.
In behind ELS stack should trace span events:
cs - Client Sent
sr - Server Received
ss - Server Sent
cr - Client Received
I'm trying to create a Zipkin 1.31.1 server using Spring Boot 1.3.5.RELEASE to build a fat executable JAR with with Tomcat 8.0.33 embedded in it.
This is failing with the following error message:
as described in  Spring Boot Enable Async Supported Like in web.xml  even with the suggested fix.
After setting breakpoints in the debugger, I found that the problem is the same as described in
How to Make LogbackValve async Supported
which wasn't answered and ultimately had the following improvement request created:
ch.qos.logback.access.tomcat.LogbackValve is not async-supported
Does anyone have any recommendations how I can workaround this issue?
I need help either:
OR
Any help you can provide would be much appreciated.
Thanks!
I'm debugging calls made from my express app to another micro-service on my network.
I'm receiving 401 errors and I need to get full raw http logs to give to my security team for analysis.
I'm looking for some advice on tracking HTTP calls from a micro-service I have deployed on Pivotal Cloud Foundry.
I've been doing some research and ran across tools like Zipkin and OpenTracing etc.. but those appear to be more about debugging latency and probably do not show HTTP logs.
I've also tried using Morgan/Winston modules but they do not track internal calls.
Morgan is currently what I'm using to log out the basic HTTP codes but it doesn't pick up on my calls from inside my app either, just the ones made to the app itself from the browser.
I need to get the full raw HTTP request to assist the security team.
I'm using the default logging output with morgan (STDOUT).
I've console logged the headers to see the headers but would like to get them out in a slightly more readable format.
Does Spring cloud sleuth support WebserviceTemplate?
I mean - I have a service which makes 2 service calls - One using RestTemplate and another using Webservicetemplate.
The Rest call is getting displayed in Zipkin and the Soap call using Webservicetemplate is not.
Do I have to add @NewSpan to all my soap calls ?
Is it not automatically done like Resttemplate?
I am using spring-amqp via spring-integration.
By the way, when using spring-cloud-sleuth-zipkin, the following error occurred.
spring-cloud-starter-zipkin:1.0.9 
spring-integration-core:4.2.9 
spring-amqp:1.5.6
In Brave (zipkin tracer), we attach state read by interceptors by controlling the Dispatcher's ExecutorService.
It works like this..
This works for synchronous requests (since they don't use the dispatcher thread anyway) and normal asynchronous requests.
It doesn't work when there are more in-flight connections than permitted, because asynchronous requests are pushed into a ready queue before they are executed.
The ready queue is not processed by the calling thread of the request, so the re-attach won't work.
When an interceptor runs on a delayed request, it cannot see its calling span which breaks tracing.
I was thinking that maybe the application interceptor might not have this problem.
If the host/connection limit was enforced (via the mentioned queue) at the network level, I would be able to coordinate the state by dual-registering an interceptor.
This interceptor could re-attach the state without relying on thread locals by mapping application/network level requests.
Sadly, this doesn't work because the host/connection limit is enforced (via the mentioned queue) at the application level, so I'm stumped.
I would like to be able to trace requests especially when they are backlogged.
Any ideas?
Hats off to brianm for finding this problem, btw
I've successfully used Zipkin with Hadoop Htrace in 2.6.0 x32, on Ubuntu 14.04.
Now I want to use it with Hadoop 2.7.3., but I can't even enable Htrace tracing with this hadoop version.
The setup for HTrace in 2.6.0 is different from 2.7.3, as it can be seen here- 2.6.0  and here- 2.7.3 .
In 2.6.0 I'd have this line in the namenode log file :
I have nothing like that in 2.7.3 Namenode log file.
Because of not having success with Zipkin, I tried to use the LocalFileSpanReceiver as described in the online tutorial:
The /var/log/hadoop/ exists, with 777 rights on it, but nothing...
The TracingFsShell example compiles and runs with the following modification:
As it can be found in the source code of hadoop in  hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/SpanReceiverHost.java  although the online tutorial does not use that method signature.
(Source  diff )
The environment is the same for both Hadoop versions, java 1.7.
Also, hadoop is compiled from source, as the Ubuntu 14.04 is x32 bit.
Hadoop is deployed in fully-distributed mode, using lxc containers.
core-site.xml  for Zipkin ( Zipkin params  here ):
In a microservice environment I see two main benefits from tracing requests through all microservice instances over an entire business process.
With  Zipkin  there is a tool, which addresses the first issue.
But how can tracing be used to unveil failures in your microservice landscape?
I definitely want to trace all error afflicted spans, but not each request, where nothing went wrong.
As mentioned  here  a custom Sampler could be used.
Alternatively, you may register your own Sampler bean definition and programmatically make the decision which requests should be sampled.
You can make more intelligent choices about which things to trace, for example, by ignoring successful requests, perhaps checking whether some component is in an error state, or really anything else.
So I tried to implement that, but it doesn't work or I used it wrong.
So, as the blog post suggested I registered my own Sampler:
And in my controller I create a new Span, which is being tagged as an error if an exception raises
Now, this doesn't work.
If I request /calc/a the method Sampler.isSampled(Span) is being called before the Controller method throws a NumberFormatException.
This means, when isSampled() checks the Span, it has no tags yet.
And the Sampler method is not being called again later in the process.
Only if I open the Sampler and allow every span to be sampled, I see my tagged error-span later on in Zipkin.
In this case Sampler.isSampled(Span) was called only 1 time but HttpZipkinSpanReporter.report(Span) was executed 3 times.
So what would the use case look like, to transmit only traces, which have error spans ?
Is this even a correct way to tag a span with an arbitrary "error_" tag ?
Update : I have pushed the code to  my repo  so people can take a look there to see what may be going wrong.
Edit : I'm almost sure it's the client code NOT POSTing any stats to the server, but neither guides below explain how should this be enabled: is there a configuration setting that I am missing?
I have been following the quick starts on both  OpenZipkin  and  Spring Sleuth : I have a running Zipkin server from  docker-zipkin  using the  docker-compose  and Cassandra as the backend:
I have created and run the  Spring Sleuth sample app  and it seems to be configured correctly to trace calls:
The logs seem to show that the traces ought to be logged:
However, the UI does not show any traces at all, no matter what I do.
The weird thing is that the  localhost:9411/trace  does show a bunch of traces (they seem to be mostly from Zipkin itself) but there are none from the  zipkin-demo  app.
I believe this due to the demo app not sending the traces to the host, but I'm just using  Spring's example app , so what can I be doing wrong?
I would like to store the spring cloud sleuth messages that come in my zipkin server (ala @EnableZipkinStreamServer) into a NoSql store.
I know that the original zipkin impl used cassandra which would work for me but I am curious about say MongoDb or CouchBase.
I looked in the documents ( http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_zipkin_consumer ) and saw that you can use spring-boot-starter-jdbc for this and it will write into the MySQL DB instance that you define.
I don't see an API/SPI for putting this into anything else.
Is there one and what is it.
I can implement the NoSql portion myself.
This is a continuation of the  shopping cart sample , where we have an external API that allows checkout from a shopping cart.
To recap, we have a flow where we create an empty shopping, add line item(s) and finally checkout.
All the operations above, happen as enrichments through HTTP calls to an external service.
We would like to add line items concurrently (as part of the add line items) call.
Our current configuration looks like this:
To add line items in parallel, we are using an executor channel.
However, they still seem to be getting processed sequentially when seen in zipkin:
What are we doing wrong?
The source for the whole project is on  github  for reference.
Thanks!
I've had a search over the internet but can't seem to find any straightforward instructions on how to use the Thrift protocol from behind a proxy.
To give you a bit of background - we have a Zipkin instance setup ( https://github.com/twitter/zipkin ) that uses a Cassandra instance ( http://cassandra.apache.org/ ) to store Zipkin traces.
Our intention is to negotiate over the thrift protocol to a collector that is then responsible for writing traces to Cassandra.
What conditions have to be in place for us to negotiate successfully via our corporate proxy?
Do we just have to set certain proxy properties when trying to negotiate or do we have to set something else up that allows this negotiation to happen?
Any help people can give in this direction with regards to resources and/or an answer would be greatly appreciated.
I am trying to install Zipkin on CentOS.
When I try to run  bin/collector , I get the following errors:
I have installed Java 7 and Scala.
Note: These errors are from a second run of bin/collector.
The first run downloaded libraries, compiled the scala files and then displayed the erorrs, however they were the same errors.
While the  apollo-opentracing  node library allows Apollo Server to create trace/span information for forwarding to e.g.
Zipkin, it only seems to work for GraphQL mutations and queries.
I would like to trace Graphql subscriptions, which includes:
I'm having difficulty identifying suitable callbacks (or other mechanisms) to help achieve all of these things; if anyone has any advice or pointers that can offer me that would be most appreciated.
Can anyone explain how to troubleshoot whether the tracingservice we configured for ambassador-gateway is working or not, we actually created the tracingService for our ambassador-gateway service as mentioned here  https://www.getambassador.io/docs/latest/topics/running/services/tracing-service/  .
Later we restarted the ambassador-gateway pod and checked the logs, we found these
we haven't found any other info regarding tracing service apart from the above two logs, we had sent some requests to ambassador-gateway ,the related traces were not showing up in the zipkin ui we configured , can some please help me how to troubleshoot this issue?
TracingServiceConfiguration:
Let's say I have a NodeJS program that has  two  separate instances of an express server running.
I've been able to instrument a program like this via  open telemetry , and have my spans sent/exported successfully to Zipkin.
All I needed to do is/was add code like the following to the start of my program.
and make sure that the express and http plugins were/are installed
This all works great -- except for  one thing .
Open Telemetry sees both my express services running as the same  service-main  service.
When I instrumented these services  directly  with Zipkin -- I would add the Zipkin middleware to each running express server
Each tracer could be instantiated with its own service name, which allowed each service to have its individual name and show up as a different service in Zipkin.

( /main ,  /hello , and  /goobye  are all service via a different express service in the above URL)
Is this sort of thing (instrumenting two services in one program) possible with Open Telemetry?
Or would I need to separate these two services out into separate programs in order to have each services have an individual name?
This question is less about solving a particular problem, and more about understanding the semantics of Open Telemetry.
I'm trying to run Zipkin on Kubernetes cluster.
This is my Deployment file:
when I call  /health endpoint from other deployment  inside of my cluster with pod's ip, I can access to it and this is my output:
but when I try to access with service name, I can't.
this is the output of  kubectl describe svc zipkin
and:
    kubectl logs zipkin-9898fcf7f-2nmkx
can anybody help me?
I have a very big problem I'm struggling with for 3 days.
I use docker swarm on the remote server.
20 microservices are in the same network NetA and stack StackA.
Now I want to add Zipkin and Sleuth to my microservices to trace all requests.
All microservices are made by docker-compose file that looks like:
Now the question is - HOW to ADD Zipkin Server?
I've added Zipkin server from Docker Hub Image.
NOW my ZIpkin Service is:
- in a separate network ZIPN
- in a separate stack ZIPST
What should I do to send data do Zipkin by all my microservices?
What URL should i send in properties file:
spring.zipkin.base-url= http://zipkinserver_network_zipkin_server:9411/
Should it be maybe:
-container name (like my_zipkin_server) - but I use swarm so container name changes dynamically?
- network name?
I added an additional network NetA to my Zipkin container but it didn't solved my problem - there are no traces in my Zipkin UI.
Please help me, I spent 4 days with this problem without any success.
Zipkin server should be in a separate Stack because will be used by different applications.
There is only ONE case when Zipkin works: when I set Zipkin container name:
I am using spring boot in a project and currently exploring the logging behaviour.
For that I'm using the zipkin service.
I have exported my logs to a json file using proper logback.xml:
Is there a way so that I could insert a jsonObject in my message part of the log.
Something like:
I have tried searching a way extensively but to no avail.
Is it even possible?
I am wondering if I can use BAM and CEP to monitor requests from client, and even find the bottleneck of the service.
I found zipkin, a project that could do this, but the base of my application is WSO2, I don't want to get other projects from scratch.
We are going to have a high load on telemetry service.
I'm looking for solutions, which be able to scale collector and backend(zipkin)
There is   solution for scaling zipkin.
Seems simple - just use internal balancing
But, I can't find examples for using multiple openTelemetry collectors.
There is no troubles to run several instances of collector, but how can I say &quot;myApp&quot; to balance beetwin them?
There is no such option in exptorters.
What is the right way to scale such system?
i want to implement distributed tracing(using sleuth-otel) in microservice and it should export the trace details to Google cloud tracing Api(not zipkin).
application structure is like this,
service1 -&gt; pubsub -&gt; service2 -&gt; pubsub -&gt; service3
i tried the following
A) In service1,
my dobut in this is,
=&gt; why i did't see the application name that i mentioned in application.properties.
Is there any other thing i missed?
=&gt; i also want to change this default traceId with my own value.
is it possible?
if yes what should i do for that?
(sample code pls)
B) in service2
doubt
=&gt; why i did't get the same traceId logged in service1 here?
(it doesn't log spanId aswell)
=&gt; How to get the same traceId in service2?
=&gt; To export the traces to google cloud trace API, this dependency is enough or do i need to add any config or anything?
Micronaut has a zipkin tracing library where you can easily override the zipkin server http endpoint like this:
Now by default it adds  DEFAULT_PATH = &quot;/api/v2/spans&quot;  to it, but for new relic the entire path should be like:
I tried with Bean replacement and factories, but I just cannot find a proper clean solution.
The only solution I have found is by copying the entire  public final class HttpClientSender extends Sender  class  as  public final class NewRelicSender extends Sender  and just modifying the constructor:
It does work, but I feel like there is a better way to do this.
I am copying almost 300 lines of code to replace 1.
What would be the Micronaut way of doing this?
All,
I am trying to integrate OpenTelemetry collector with Prometheus in a .Net application.
I am using OTLP Exporter in the application and running OpenTelemetry Collector as a windows service on same system.
Please find the below 'config.yaml' I am using for collector
Here is the 'prometheus.yml'
I don't see the metrics exported to Prometheus.
I am creating some counters programatically in the application and looking for the same in Prometheus.
Please note, I tried to export traces to Zipkin and it worked fine.
Any help is greatly appreciated.
Prometheus UI target health is showing 'UP' as below.
I have 4 spring-boot applications (A, B, C and D).
The lifecycle of a transaction is as follows :
Now for a single transaction I would expect a single trace across all four application, but instead I get
I would have uploaded the pictures to show the zipkin spans, but for some reason I am not able to do so.
All the above applications are Spring boot applications and they utilize spring-cloud-sleuth for producing transactions traces.
I am relying on spring boot's autoconfiguration and these are the properties that I have set in all the applications:
I am not able to understand what's exactly happening here.
Why the spans are scattered across 2 traces and not one?
I am using spring-boot 2.3.3 and spring-cloud-dependencies Hoxton.SR8.
I have a beginner question with Docker Compose.
I am trying to extend the  docker-compose-slim.yml  example file  from  Zipkin GitHub repository .
I need to change it so that it can include a simple FastAPI app that I have written.
Unfortunately, I cannot make them connect to each other.
FastAPI gets rejected when it attempts to send a POST request to the Zipkin container, even though they are both connected to the same network with explicit links and port mapping defined in the YAML file.
However, I am able to connect to both of them from the host, however.
Could you please tell me what I have done wrong?
Here is the error message:
Here is the Docker Compose YAML file:
Here is the Dockerfile:
First, we've thought that problem is in the incompatible version of Brave and Zipkin.
We have fixed it and now it's exactly like in the snippet above, but the problem remain.
While debugging we've found out that the problem only appears when in Braves class  ThreadLocalCurrentTraceContext  is creating a new Span in method
with  local.get()  which return not null value.
local  is  final ThreadLocal&lt;TraceContext&gt;  variable and it only works when get method returns null value.
Any idea how could we solve this problem or other possibilities where can be problem?
Thanks.
my spring boot microservices run currently with Spring-Boot 2.2.9.RELEASE
I also still use
dependencies.
Now I wanted switch to Spring Boot 2.3.2.RELEASE.
I can compile the code without any errors.
After start the service I see this
INFO: HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
log message.
There are now errors.
And then the deployment process stopped.
Nothing happens.
At the moment I have no idea why it's not running.
Has someone some tip?
I am trying to use Spring Cloud CLI to start eureka, configserver and zipkin with this command:
but I get this message:
It seems like the CLI can't find some packages in the maven repos.
Is there a solution for this?
I have 3 spring boot applications ,  spring-cloud-gateway  ,  spring-boot-resource-server  and  spring-boot-authentication-server .
My spring-boot-resource-server is protected via Oauth.So whenever an api is requested , it is first authenticated by authentication server using ( check_token )  enpoint.
So now,i can see the request flow from gateway to resource server in zipkin as single flow
Request-&gt; Gateway -&gt; Resource Server
but the ( check_token ) request shows up as a separate request.
Resource Server -&gt; Auth Server
If my understanding is right it should be a part of the previous request
Request-&gt; Gateway -&gt; Resource Server -&gt;  Auth Server
Am i missing something ?
I searched on the web and found 2 results :
From the above link i think i need to register TraceFilter before my OauthFilter, but i don't seem to find any Trace filter in current release.
I am using  Greenwich.SR3  in all 3 applications.
Update
I have printed filter chain if it helps.
update 2 :
Updated Spring Cloud to Hoxton.SR4
Sample Application
I am creating a zipkin server, and i have seen tutorial which annotated the boostrap class with @EnableZipkinServer and others with @EnableZipkinStreamServer.
I am having a hard time understand what is difference between these two.
Are they interchangeable?
I have Spring cloud sleuth(and also zipkin) configured in my application.
I have a controller which calls service, which in turn calls repository and finally database.
The setup works fine, Sleuth is generating span id's and It is visible in zipkin as well.
I wanted to try creating span's across multiple internal beans and methods.
I came across  Managing Spans with Annotations .
This does not seem to work.
When I use any of the annotations mentioned here like  @NewSpan  or  @ContinueSpan , Autowiring stops working.
My service class which is autowired in Controller is  null .
If I remove these annotations everything works again.
I am using.
spring-boot 2.2.5.RELEASE
spring-cloud.version Hoxton.SR3 
I have these dependencies in my pom
Here is a sample code
And my Service class is like
My guess is, Spring-Aop has something to do with it.
Any idea?
I have been working on spring cloud sleuth to push traces to zipkin, We are pushing traces to Kafka with the help of  spring-cloud-starter-stream-kafka  and  spring-cloud-sleuth-stream
Below are the dependencies I have added to my application
And my application property just have kafka broker which is by default pushing traces to  sleuth  topic  spring.cloud.stream.kafka.binder.brokers=locahost:9092
Everything is working fine as of now  but I am not able to figure out if we can start the application if Kafka is down ?
From the logs I see application starts, but since it will be not able to connect to kafka we will be not able to access any of the endpoints (Added some bits of logs below).
Looking for this solution because pushing traces is not the primary function of my application.
We use a lot of Finagle  Filter s and  Service s in our code.
However, we don't support Zipkin in our infrastructure.
At times there's a need to  trace  an incoming request across the chain of Filters/Services, especially in the face of concurrent requests.
What's the most non-intrusive way to get such a functionality?
Goal: 
Would be great if Finagle itself provided an additional  apply  method like so:
Then the subclasses could opt-in to using this method instead.
The last argument being the identifier that every Filter/Service in the chain could "append to".
Example:
The goal is to have something that can uniquely trace/log the series of filter/services effectively.
How may I go about by extending Finagle via traits/monkey-patching?
Or is this not doable?
Added dependency for cloud sleuth and zipkin dependency of version below in pom.xml
Simple spring boot class.
Should i add filter to set new fields?
resultant LOG is no showing the newly added property :

does something i am missing ..?
I have a system that distributing tasks with queues.
I have N services that communicate with each other with N queues.
My system diagram looks like this:
As you can see, the outcome results performance depends on multiple services.
So, diagnose failures (or performance issues) is hard task - it's very hard to find which part of the chain was the problem.
I was looking for a tool that will provide me a map of all the processing traces.
Something like this:
With a visual diagram like this, I can find which part of the chain was the problem.
Easily.
So, I found a few solutions that aim to solve this problem:
Unfortunately, it looks very difficult to:
Am I wondering if I can have an easier solution for this simple problem?
I am using spring-cloud-sleuth-zipkin and spring-cloud-starter-sleuth as dependency.
When i call a netflix FeignClient call TraceId changes...
When i call TesterClient the traceId changes?
How could i preserve the same traceId?
I want to pass the span across multiple threads and want to get trace.
I have different worker threads.
while running I am getting  You may have forgotten to close or detach null
pom.xml
WorkerSpan1.java
WorkerSpan2.java
here I am passing span because tracer.getCurrentSpan() is returning null.
and my controller
TestController.java
While running I am not getting trace in zipkin and getting warning:
I also want to know how to pass trace information across different threads.
in my microservices architecture application I'd like to add Sleuth and Zipkin server (image from Docker Hub).
Everything works fine locally - each microservice sends data to Zipkin server.
The problem is more complicated when I deployed all microservices on the server  -Zipkin Web UI is empty - no traces.
In application.properties we can explicitly set the url to the Zipkin server:
spring.zipkin.base-url:  http://10.0.44.1:9411/
I thought that all containers can communicate each other in different stacks/networks but it's not true.
What should I do to publish my Zipkin server/container to be availabe for all containers from all stacks/networks?
Is there a possibility to do this using Portainer?
Thank you in advance
I and my team is stuck in a very silly case of HttpClientErrorException let me first give overview of my work scenario.
I have a micro-service stack with following things:
In scheduler I am using spring Scheduler with cron
The purpose of code in prepareDataForSync is to obtain data about each hotel and check current state and if any change is deducted pass it on to third party.
Now here comes the real problem:
I make a rest service call to obtain list of hotels from my scheduler:
All these services are running in docker environment with each service having own container and communicating via docker networking.
Now when I start the services including Hotel and Scheduler, everything work fine for few hours but then I get following exception in my logs and service is no more syncing with third party.
At line 1159 I have a service call only:
I checked logs in corresponding Hotel service, logs there displays the service request was received and data was collected and written to response stream but as the exception shows I never received any response and get this Exception.
Logs from Hotel service:
If I restart the Scheduler service it starts working again, but again in few hours i have same issue.
As a workaround currently I have set up a cron on the server to restart the service every 2 hours but it is really a bad workaround, I can't rely on this in the production and need to get to the root of problem.
I have have googled and tried to go through any HttpClientErrorException based question, but nothing made sense to me.
Please let me know if more information is required from my end.
EDIT:
Docker Stats Output:
0b7d20c5a566        schedular            0.12%               1.365GiB / 31.41GiB   4.34%               0B / 0B             3.27MB / 0B         64
TOP Output inside container
i have s basic spring boot/cloud application based on
But i need spring-cloud-sleuth-zipkin with at least 1.3.x.
By importing 1.3.5.RELEASE i get a strange error.
It seems like the same dependency creates a convergence.
Is that easily solavble?
I have Spring Cloud Sleuth (2.0.2.RELEASE) working within a (partially) reactive class in some web based request/response system.
The code is something like this:
As there are quite many responses coming in all the time, this method is being called several hundreds of times for one single request.
I suspect that this call with the  .publishOn  leads to hundreds and thousands of  async  spans in Zipkin (see attached screenshot).
At least I assume that the spans are from that because it is what I understand from  the documentation
So my first question would be:
How can I associate a name for such async threads?
I don't have a place to put  @SpanName  here.
As a follow up, is there any way to NOT collect these spans?
I don't need them, they fill up our Zipkin storage, but I also don't want to disable reactive or Sleuth in general since it is needed in other places ...
Screenshot from Zipkin
I'm learning how to track my distributed processes through all the microservices.
I've been playing with Sleuth, Zipkin and different microservices, and it works fantastic!
But when I try to do the same in a project interacting between the different dependencies I can not create the same behavior.
This image show how currently is working different microservices.
This is the diagram of microservices:
And this image show how works an application with dependencies.
This is the diagram of application with dependencies:
I wonder, is it possible to create the same behavior using dependencies as with microservices?
I am developing Microservices specifically  "zipkin-service" .
I have taken a reference from  link:  https://www.baeldung.com/tracing-services-with-zipkin .
Could anyone please guide what's the issue ?
When I added below dependencies, then I get the
Error:
Project build error: 'dependencies.dependency.version' for io.zipkin.java:zipkin-autoconfigure-ui:jar is missing.
pom.xml
ZipkinServiceApplication.java
I am running  open-zipkin  with docker-compose for testing purposes.
The end goal is to have open-zipkin running so that I can successfully do a  curl localhost:9411/prometheus  on the zipkin container itself and view prometheus metrics.
How can I expose the metrics like this?
For reference the zipkin portion of the  docker-compose file looks like this:
I am using spring boot in a project and currently exploring the logging behaviour.
For that I'm using the zipkin service.
My  logback.xml  is as follows:
Now for what I have understood, to log in json, you need a custom logger implementation which I have done as follows:
My message class is:
Now in my controller class I'm using my custom logger as:
My logs end up in kibana as:
Now instead of this I would want my  message  part of the logs as a  json  entry.
Where am I going wrong?
I have tried searching a way extensively but to no avail.
Is it even possible?
I am using Docker Toolbox 1803 under Windows 10 64bit.
I am trying to run  docker zipkin  in my local system, and got failure when ran the following command in Toolbox GuickStart Terminal.
docker-compose -f docker-compose.yml  -f docker-compose-cassandra.yml up
The following line of docker-compose.yml caused the problem.
The failure info is like following:
UPDATE : I have set env variable  COMPOSE_CONVERT_WINDOWS_PATHS=1 , it still does not work.
Update : Switched to Docker for Windows and WSL2.
The experience is great enough, but sadly I have to give up my VirutalBox.
I am using spring cloud finchley.rc2 with spring boot version 2 along with sleuth and zipkin.
I have a facade layer which uses reactor project.
Facade calls services parallel and each service store some tracing info in rabbit mq.
Issue is I am seeing in zipkin some spans like
How can i stop such traces from being captured
I have a virtual machine centos(ver 7.4) on win10 machine, I do not use AWS, Google cloud service, nor Azure.
I put master and node in one machine.
My original problem domain have 5 components, I configure them as ClusterIP, so they could communicate with each other(eureka, config,api,uaa,zipkin).
Now I only need api talk outside.
But for short, I make two components for convenience (api and eureka).
But now, api needs to receive from outside of cluster.
So that I configure ingress.
When ingress, I need to configure rbac.
I put my yaml file here with error message.
eureka_pod.yaml
eureka_svc.yaml
api_pod.yaml:
api_svc.yaml:
ingress_nginx_role_rb.yaml:
nginx_default_backend.yaml:
ingress_nginx_ctl.yaml:
ingress_nginx_res.yaml:
when I try  172.16.100.88:31080/uaa/login , (my virtual machine current IP is  172.16.100.88 ) it says following connection problme:
I check ingress-nginx pod, it seems request has not yet reached nginx.
When I login in to pod gearbox-rack-api-gateway, I could see clearly it redirects to the page I expected.
so there must be some configuration wrong in my yaml files.
=================================================================
In my virtual machine, I type  telnet localhost 31080 , rejected.
but  telnet -6 localhost 31080  succeed.
And  netstat -anp | less  find 31080 binding kube-proxy.
I put  sysctl -q -w net.ipv6.conf.all.disable_ipv6=1  and  sysctl -w net.ipv6.conf.default.disable_ipv6=1  in my starting script, but got same result.
==============================================================
Yesterday question about Ipv6 is stupid.
I misconfigured /etc/hosts.
Now I have telnet localhost 31080 work, but when I do curl  http://localhost:31080/uaa/login , it hangs there for long time.
So pod is listening.
When I issue the command curl  http://localhost:31080/uaa/login , at the same time, I check several pods' log.
Log has shown no error and has no log to say the port 31080 has been sent request.
I checked ingress-nginx pod logs: I paste some here.
My own wild guess is that ingress nginx pod's namespace is kube-system, nginx service's name space is default, my-ingress's namespace is default.
nginx-default-backend's name space is kube-system.
Whether cross namespace traffic is forbidden.
Experts what kind of logs do you need?
=======================================================
After define all ingress controller, ingress resources as default namespace, now I did get move further.
Now nginx redirect my  http request to https:  request.
How to disable this feature will make my ingress working wholly.
I notice ingress-nginx receive the request, shown as below:
Is there a way to customize the Span inject and extractor for spring cloud sleuth 2?
In the documentation of the version 1.2 i found a way that is not available on the new version(2).
I think is because now its use Zipkin brave to take care of Span, right?
https://cloud.spring.io/spring-cloud-sleuth/1.2.x/multi/multi__customizations.html#_example
I tried to back to use the stable version(1.3.3) of spring cloud sleuth, but when i use the bom for the project its make conflict in the spring boot version that i am using(2.0).
Its compactible with the spring boot version 2?
I am using the spring cloud sleuth to make trace of services on my company, but i have a version of tracing on others services that is not compactible with the opentracing headers, so i want to change the headers of http messages to make the new services compactibles with the current tracing headers that i have in the others components.
Thanks
I have setup a demo project using spring-cloud-stream with RabbitMQ-binders and spring-cloud-sleuth.
I have a scheduled spring-cloud-stream source:
and then a middle tier, similar to the final sink layer looking like:
I nicely see the TraceId and SpanId automagically passed through the RabbitMQ queues across all processes down to the sink process in the logfiles:
so at this point the final sink tier wants to (explicitly) signal, that this trace (instance of my business process) is  finish ed here.
How do I signal that the whole Trace has finished?
I only found  sleuthTracer.currentSpan().finish();  but this only finishes the Span ... not explicitly signalling, that the whole trace is finalized here.
Did I miss something?
(quite new to zipkin, brave and sleuth)
there's a microservice  with spring-boot 1.5  which uses the  Feign  to communicate with others services, also there's  spring-cloud-starter-zipkin  which wrapped all calls through the Feign and sends tracing to zipkin server.
The thing is i don't wanna wrap all calls and trace them, there're only several most important to do that.
How can i exclude some calls(methods) with Feign from tracing or exlude some whole Feign client(interface)?
I am trying to use feature from  Spring Cloud  (ex: Feign or Zipkin Client) in a  Spring Boot  micro-service.
Whenever I introduce the Spring Cloud dependencies into the pom.xml I get the following error at startup:
Below is a sample pom.xml causing this.
I am currently on  Spring Boot 2.0.0.RELEASE  and  Spring Cloud Finchley.M8 .
What am I doing wrong?
Should I switch to another version of Spring Cloud?
UPDATE : It's not me who's doing it wrong, even  Spring Initializr  projects demonstrate this issue.
To repro:
pom.xml:
I also want to trace network latency from  App -&gt; Service1 -&gt; App -&gt; Service2 .
Spring sleuth works perferct to find latency between Sleuth aware services say  services1-&gt;service2  as I can see CS,SR tags in Zipkin.
Now I also want to track network Latency between devices and other area which we are hopping but those are not Sleuth aware services.
How can I do that.
Any pointers would be appreciated .
From the Docs,
That means that the current span has Trace-Id set to X, Span-Id set to
D. It also has emitted  Client Sent event .
Are there any specific headers which needs to be sent from App to Server?
I know Sleuth does it out of the box using Rest template.
How can I do the same thing from Apps or other non sleuth services.
I am trying to integrate my Application with Spring sleuth.
I am able to do a successfull integration and I can see spans getting exported to Zipkin.
I am exporting zipkin over http.
Spring boot version - 1.5.10.RELEASE 
Sleuth - 1.3.2.RELEASE 
Cloud- Edgware.SR2
But now I need to do this in a more controlled way as application is already running in production and people are scared about the overhead which sleuth can have by adding @NewSpan on the methods.
I need to decide on runtime wether the Trace should be added or not (Not talking about exporting).
Like for actuator trace is not getting added at all.
I assume this will have no overhead on the application.
Putting  X-B3-Sampled = 0  is not exporting but adding tracing information.
Something like skipPattern property but at runtime.
Always export the trace if service exceeds a certain threshold or in case of Exception.
If I am not exporting Spans to zipkin then will there be any overhead by tracing information?
What about this solution ?
I guess this will work in sampling specific request at runtime.
We are trying to add tracing to micro services so it can viewed in the google Stackdriver UI.
We are using Java Springboot apps deployed into Kubernetes containers, each microservices communicates over http.
We’ve seen that there is Sleuth and Zipkin which if we move our RestTemplate to a bean will work.
However we don’t really want to have to deploy a zipkin pod in each of our containers or create new zipkin collector pods.
Ideally we would like to get this working using just the google cloud tracing sdk with using sleuth/zipkin.
Playing around with the sdk we are able to get data into Stackdriver using the google cloud grpc library which just sends the data directly from the application into Stackdriver.
The problem we have now is that we can send the trace id to a downstream micro service but we cannot seem to find a way to create a new span on the same trace id, it always creates a new one.
I can’t seem to find any documentation on how to do this.
Surely what we are doing is what this library was build for?
Any pointers help on this would be great.
Adding a bit more info......
I cannot supply actual code because this is my problem, I can't actually find what I want to do.
Let me try to explain with a bit of code/pseudo code.
So lets assume this scenario, I have 3 microservices, A, B and C.
Hope this makes sense in what I'm trying to do.
I want to check latencies of RPC every day about CakePHP Application each endpoints running in GKE cluster.
I found it is possible using  php google client  or  zipkin server  by reading documents , but I don't know how easy to introduce to our app though both seem tough for me.
In addition, I'm concerned about GKE cluster configuration has StackDriver Trace option though our cluster it sets disabled.Can we trace span if it sets enable?
Could you give some advices?
im trying to connect my Spring Boot app to a Cassandra 2.2.8 cluster on EC2 instances (2 nodes).
my use is tracing with Sleuth and Zipkin.
when the tracing start, the driver always point to localhost :
com.datastax.driver.core.Cluster : New Cassandra host localhost/127.0.0.1:9042 added
this is my application.properties
and this is my pom.xml :
I have got problem running my js application in browser.
I have created client-server application, and I am using  Zipkin  to trace communications between them.
This is the client that uses Node.js  require() :
I am using Browserify to bundle dependencies for use in the browser.
I run  browserify CujojsClient.js -o bundle.js  to create bundle for the browser.
If I run browserify with  --node --ignore-missing  options everything works well in Node.js, but when I run the bundle in the browser (Firefox 45.3.0 on windows) I only got:
This is the problematic part:
My  index.html :
I have added sleuth/zipkin into my project.
I'm using logback, and by default I get very well formatted logs in my console and files as well.
I'm also using a logstash appender, and when I look at how kibana presents those logs - I'm not satisfied at all.
Here are details:
pom.xml:
My logstash appender:
And this is what I see in kibana:
The only thing that is resolved by the log pattern is the application name.
Am I missing some configuration?
Or maybe there's something wrong in my logstash appender?
Tracing information do not propagate over kafka messages due to the method SleuthKafkaAspect.wrapProducerFactory() is not triggered.
On the producer side, the message is correctly sent and the tracing information is correctly logged.
On consumer side, instead a new traceId and spanId is created.
The following two logging lines show different values for traceId,spanId (and parentId):
In first instance, using Krafdrop and also debugging, I verified that the message header doesn't contains any tracing information.
After that, I figured out that the method SleuthKafkaAspect.wrapProducerFactory() is never triggered, instead on consumer side the method SleuthKafkaAspect.anyConsumerFactory() is.
The libraries versions used are the following:
The kakfa client library version is 2.4.1 is due to a version downgrade related to production bug on 2.5.1 version of kafka client that increase the cpu usage.
I also tried to use the following libraries versions combination with no success:
We migrated our project to a different spring boot version, from 2.3.0.RELEASE to 2.3.7.RELEASE.
Before everthing was working correctly.
Below the old libraries versions:
We also introduced a log42/log4j (before it was slf4j with logback).
Below the related libraries:
The properties configured are the following:
The configuration class for the ProducerFactory creation is the following:
My spring boot application class:
Here you can find the output of command &quot;mvn dependency:tree&quot;
 mvn_dependency_tree.txt
Within next few months 6th edition of Spring in Action is going to be published.
It is said, it will not contain 3 chapters from 5th edition i.e.
Circut Breaker,
Eureka Service-Client Discovery, Eureka Server-Client Configuration.
Instead of this, it will include changes made in Spring Boot 2.4.
I have
alread heard that Circut Breaker (Hystrix) is outdated, but I wonder what about
rest, especially omitted chapters ?
I noticed that I can not choose ribbon in newest(2.4.3) Spring Boot version, zipkin also differs from earlies ones.
What is alternative for ribbon in newest version?
I am using  spring-cloud-starter-sleuth:3.0.1  and  spring-cloud-sleuth-zipkin:3.0.1  to generate  traceId  and  spanId  in log file.
I was able to get those in logs using  2.2.7.RELEASE  version.
I have tried using  logback  but not able to have with  3.0.1  version.
As per 3.0.1 documentation, they have removed Legacy MDC entries but brave  spanId  &amp;  traceId  are there.
Dependency hierarchy:
traceId &amp; spanId are not generated in log:
I have tried to see this request's tracing on zipkin and able to see it with traceid and spanid:
Can anyone help me to get traceid and spanid in log file using logback/log4j?
I have a microservices based software architecture.
There is a php application which orchestrates the communication among microservices and the application's whole logic.
I need to simulate the communication between microservices as a graph.
There will be edges with weights , which will represent the affinities between microservices.
I am searching for a tool in order to collect all messages and their size.
I have read that there are distibuted tracing systems like Zipkin which i have already deployed, and could accomplish this task.
But, i cannot find how to collect the messages i want.
This is the php library i used for the instrumentation of my app
[https://github.com/openzipkin/zipkin-php]
Any ideas about other tools or how to use Zipkin differently to achieve my goal?
I am using Spring Cloud Sleuth to send spans to zipkin when a Spring Boot application sends a message (to RabbitMQ).
I would like to customize the information sent to zipkin to include some extra tags that are populated from certain headers of the outgoing Message e.g.
the  myCustomTag  below.
Is it possible to do this using Sleuth/brave?
It feels like a messaging equivalent of registering (e.g.)
a bean of type  brave.http.HttpRequestParser  but I couldn't see an obvious way forward.
I use the latest Spring Boot 2.4.0.m3 to setup Spring Sleuth.
On the Zipkin, the data outflow from a source is directed to &quot;broker&quot;.
The following is the code snap:
where fooService is org.springframework.cloud.stream.messaging.Source data type.
The sink is connected to &quot;Kafka&quot;.
And there isn't a connection between the &quot;broker&quot; and &quot;Kafka&quot;.
Also, the sink has a data outflow to &quot;Kafka&quot;.
And the following is a code snap:
To my eyes, there are two errors in the picture.
One is the label of &quot;broker&quot;.
It should be &quot;Kafka&quot; as well.
And the sink should have inflow data from Kafka instead of outflow.
Is some additional configuration needed to resolve those two errors?
We are working on an IOT platform, which ingests many device parameter
values (time series) every second from may devices.
Once ingested the
each JSON (batch of multiple parameter values captured at a particular
instance) What is the best way to track the JSON as it flows through
many microservices down stream in an event driven way?
We use spring boot technology predominantly and all the services are
containerised.
Eg: Option 1 - Is associating UUID to each object and then updating
the states idempotently in Redis as each microservice processes it
ideal?
Problem is each microservice will be tied to Redis now and we
have seen performance of Redis going down as number api calls to Redis
increase as it is single threaded (We can scale this out though).
Option 2 - Zipkin?
Note: We use Kafka/RabbitMQ to process the messages in a distributed
way as you mentioned here.
My question is about a strategy to track
each of this message and its status (to enable replay if needed to
attain only once delivery).
Let's say a message1 is being by processed
by Service A, Service B, Service C. Now we are having issues to track
if the message failed getting processed at Service B or Service C as
we get a lot of messages
I have the zipkin deployment and service below as you can see zipkin is located under monitoring namespace the, i have an env variable called  ZIPKIN_URL  in each of my pods which are running under default namespace, this varibale takes this URL  http://zipkin:9411/api/v2/spans  but since zipkin is running in another namespace i tried this :
http://zipkin.monitoring.svc.cluster.local:9411/api/v2/spans
i also tried this format :
http://zipkin.monitoring:9411/api/v2/spans
but when i check the logs of my pods, i see  connection refused exception
when i exec into one of my pods and try curl  http://zipkin.tools.svc.cluster.local:9411/api/v2/spans
its shows me  Mandatory parameter is missing: serviceNameroot
Here is zipkin resource :
I have adapted the sample from Micronaut Users Guide V1.2.10 chapter Tracing Annotations.
My code looks that like:
The question is, why the call for method doubleName is not logged to zipkin (at least it does not show up in the zipkin GUI).
The REST call to address() is logged.
Do only REST calls get logged and no local method calls?
Actually I don't think that's that the case because the Users Guides sample tells that this should work.
Any ideas?
I am using  Sleuth 2.1.3.
I want  to add  a custom "trace  ID" as "correlation id" with alpha numeric  value and want to spit in logs  with  spanid  and  parent id.
If i use below implementation for creating new custom trace id.
does it  get  printed in logs ?
I tried  below implementation   but   does not see any custom trace in log 
 https://github.com/openzipkin/zipkin-aws/blob/release-0.11.2/brave-propagation-aws/src/main/java/brave/propagation/aws/AWSPropagation.java
I tried with above code  from  https://cloud.spring.io/spring-cloud-sleuth/reference/html/#propagation  but didnt see any custom trace id in log
I have 2 services which are exchanging events via Kafka.
First service packs necessary tracing info (headers which are set by brave: traceId, spanId and so on) right in message payload.
Consumer of the second service retrieve this information and create appropriate consumer span.
I can see the whole tracing including both services in Zipkin.
But I can't see appropriate tracing information in logs on consumer side.
On the producer side all is ok.
The code of consumer side (as the tracings are sending to zipkin I think the most of configuration is ok, but the tracing information just don't propagate to log context...):
What can you advice?
Maybe I should attach something else but it seems like it is pretty much the only code I have around sleuth and brave.
I'm using Spring cloud stream binder kafka, Edgware.SR4 release.
I have set custom headers to a message payload and published it but i can't see those headers in consumer end.
I have used Message object to bind payload and headers.
I have tried adding the property spring.cloud.stream.kafka.binder.headers but it did not work
Producer:
Application.yml
MessageChannelConstants.java
SampleMessageChannels.java
SampleEventPublisher.java
Consumer:
application.yml
MessageChannelConstants.java
SampleMessageChannels.java
SampleEventListener.java
Below is the Exception I got,
Note: I am using spring cloud sleuth and zipkin dependency as well.
We are running a java trading application and have around 50 orders per second.
When an order comes in, it jumps between services and we want to measure latency inside every service and between services by an external service which should gather all data with timestamps and produce distributions with percentiles.
We want to measure latency for every order in order to find issues and explain them to our members if they have latency-related questions.
The issue we are facing is a framework to choose to propagate orders from every service to another service with timestamps attached to calculate and produce latencies.
Given the flow of orders we have, what will be the most promising approach for us?
We looked into Zipkin, it also supports gRPC - does it fit our use-case?
Any other recommendations?
p.s.
we cannot use the transport we use for the business logic as we are going to get rid of it soon.
I am new to node js and was trying to integrate zipkins with my node APi using appmetrics-zipkin npm package.
Zipkin works fine except when there are multiple http calls in async  parallel method , it gives trace of only the first http call which was finished...I need trace for all the API calls in async parallel......Please help
I have a dependency  https://mvnrepository.com/artifact/io.zipkin.reporter2/zipkin-sender-okhttp3/2.7.14  declared as
This dependency in it's pom has parent dependency with pom file that declares dependency like this:
project.groupid equals to io.zipkin.reporter2 and project.version equals to 2.7.14.
So maven should import dependency  &lt;artifactId&gt;zipkin-reporter&lt;/artifactId&gt;  with version equals to 2.7.14.
BUT  it imports versions 2.2.0.
I don't declare this dependency anywhere else and other dependencies don't have this dependency as transative.
I tried reinstall maven, reclone project from git, invalidate cache and restart IDEA, delete .m2 folder - nothing worked.
The weird thing is that I have other project that is using the very same dependency ( &lt;artifactId&gt;zipkin-sender-okhttp3&lt;/artifactId&gt; ) and all versions are in order how they should be.
Any ideas how I can fix it?
Edit: mvn dependency:tree output (I edited out sensitive info): `
`
All modules have version 2.7.7 as they should be but one module with name  ...-deployment has versions 2.2.0.
It doesn't set explicitly there.
I have a distributed process which runs across two different servers (A and B) and I get two different log files A.log and B.log, I need this merged into a single 
file.
I have referred to following links but I am unable to get a merged file from the same:
Is there something that I am missing?
Edit: I need the logs in something along these lines
I am using spring-cloud-sleuth:2.0.1.RELEASE with Spring Webflux.
The doc talks about logging trace, span, etc using MDC.
It also talks about sending traces to Zipkin via HTTP.
I am interested in logging the trace information in more elaborate way.
With every log statement, I want to emit the zipkin traces in the JSON format - very close to what's depicted here:  https://zipkin.io/pages/data_model.html
What is the best way to accomplish this in sleuth?
I'm using  spring-cloud-sleuth  and zipkin.
In the producer, it worked.I can see message in kafka topic   see image .
but in the consumer,some exception occur.
solve by this issue .
but next exception cannot solve.
project infomation
pom.xml
application.yml
main class
I am facing some errors in a spring boot project where I am using spring integration to connect to RabbitMQ.
I am doing the configuration for RabbitMQ in XML files like this:
But I am creating two of each component.
How to set the primaries ones?
Now the problem comes here, I was using this version for spring cloud:
And everything was working fine, but if I update the version to:
This error is coming:
And the error comes because of this dependency:
If I remove this dependency the error is not coming.
You can find an example project to reproduce this scenario.
In the pom file you'll see this:
https://github.com/fjmpaez911/spring-integration-zipkin-cloud
So I need to know how to set a primary configuration for RabbitMQ and in addition I think that could be an issue because this error only comes if I use this version  Edgware.RELEASE
Am I missing something?
I have multiple services, some of which use Hystrix's HystrixObservableCommand to call other services and others use HystrixCommand.
How do I pass on traceIds from the calling service to the Observables in HystrixObservableCommand and also have them be passed on if the fallback is called?
All services are using grpc-java.
Sample code that I have:
WorldCommand.java
I am using Zipkin grpc tracing and MDCCurrentTraceContext to print the traceId and spanId in the logs.
Both the log entries in the WorldCommand do not print out the trace and span ids, they are called on RxIoScheduler thread.
EDIT
Added ConcurrencyStrategy as suggested by Mike.
HelloService calls two services World and Team.
The WorldCommand is a HystrixObservableCommand, the TeamCommand is a HystrixCommand.
PreservableContext class
The log in PreservableContexts and CustomHystrixConcurrencyStrategy never get printed.
I am registering the startegy when I start the HelloServer.
EDIT 2
Updated how the Observables are set up:
I have a weird problem now, the calls to TeamCommand and WorldCommand doesn't complete as in this code is never executed:
Also, if there is a fallback, the hystrix-timer threads doesn't have the MDC anymore.
I have 2 very simple spring-cloud-stream applications.
Service3, the message producer, sends messages to Service4, the consumer, through the binder-kafka.
And I use spring-cloud-sleuth to trace the spans among them.
But only the spans in Service3 are available in zipkin server.
No span shows for Service4.
Service3
Service4
Servic4 (message consumer) is not traced
What did I miss?
I am trying to trace HTTP calls made through  Async RestTemplate  from a Spring Boot Application.
I have a ZipKin instance running locally to which the microservices in question point to.
I could see spans recorded at every service in ZipKin UI, however I am not able to see the trace covering all the spans.
With  RestTemplate  the trace is recorded as normal.
i.e.
I am able to see end-to-end via the UI.
Any pointers will help,
Thanks in advance.
I would like to know if it is possible to modify  iron-ajax  somehow to use  cujojs-rest  to perform all the requests.
I would like to use cujojs-rest zipkin instrumentation for tracing in my app.
Here is an example app using cujojs-rest zipkin instrumentation to generate trace data for Zipkin:  wingtips-cujojs-spark-example
So let's say I have got code like this:
I would like to achieve the same in  iron-ajax
I am using  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_adding_to_the_project  for adding Spring Cloud Sleuth without the Zipkin integration
But in one of the microserives out of three,  does not show the spanid, and token id in logs  after adding dependency for all services (all are http request services, though there are couple of more services which require JMS - on which I need to work)
Service1
Service2
Service3
Experts, Please suggest what can be done to see the effect in Service2
I am using Spring Cloud Sleuth + Zipkin to have an insight of the service timing and behaviour.
The only downside I have found is: when there are several instances of every microservice I haven't found a way to determine which instance Zipkin information is referring to, since it identifies them all by its service name (which is the same for all).
Is there a way to configure Sleuth to add service-instance dinstintion in Zipkin?
I am using a script from the following paper (Zipkin, E.F., Royle, J.A., Dawson, D.K., Bates, S., 2010.
Multi-species occurrence models to evaluate the effects of conservation and management actions.
Biological Conservation 143, 479-484) to estimate bird species occupancy.
One of my variables in the detection estimate (the K loop in the code below) is Wind, which is a categorical variable, with levels 1-6.
I have attempted to use the  dcat  function in OpenBUGS which what I hope is an uniformative prior (beta(1,1)), but OpenBUGS fails with error:
Which, when I remove the line  b3[i] ~ dcat(p[i,])#WIND Data  does not happen.
Any advise on how to specify dcat properly, or how to code categorical variables for WinBUGS/OpenBUGS would be greatly appreciated!
what are the best practices in tracing of spring boot 2 microservice applications?
I found some 2 years old tutorials where tracing server was as another spring boot application with following dependencies:
and push traces with following configuration:
and
Is this solution still actual and suitable for production or should we configure standalone docker image of zipkin instead of spring boot app and connect it to ELK stack with logs?
What do you recommended?
It will be great if you can provide some example what is recommended approach to handle it in.
Thank you in advice.
i use zipkin with kafka
and work log:
and loop log...
Can anyone tell me what is the reason?
kafka-server: 0.11.0
kafka-client: 1.0.1
zipkin: 2.10.4
I'm developing event-driven Microservices which I use Java and Scala.
I used Spring Sleuth and Zipkin for request tracing with Java services, can I use Spring Sleuth with Scala?
if not how can I generate trace id and span id in Scala to be sent to Zipkin.
In spring and spring-boot there is a lot of "magic" that happens just by annotating methods and classes.
For learning purposes and do-it-yourself stuff I would be interested to have a look at them and so wondering how to find the "magic code" that an annotation "causes" ...
is there a "cook-book" on how to find the implementing code of Annotations inside spring jars?
We have integrated NiFi within our product suits.
We would like to track a user request by "Trace Id", which spans across different components.
Do let me know whether NiFi have some capability to support something similar to ZipKin.
Thanks
Senthil.
I suggest to look into Gartners magic quadrant and get dynaTrace since it has negligible overhead , less than 1% in production under load.
Full Disclosure: I currently work for AppDynamics.
AppDynamics was designed from the ground up for high volume production environments but works equally well in both prod and non-prod.
It's currently running in production in some of the worlds largest mission critical application environments at Netflix, Exact Target, Edmunds, and many others.
Here are a few quotes from existing customers…
"It's like a profiler that you can run in production" -- Leonid Igolnik, Taleo
"We found that the overhead was negligible" -- Jacob Marcus, Care.com
"We wanted a monitoring solution that wouldn't impact our production runway" -- John Martin, Edmunds
AppDynamics overhead is extremely low but I suggest you test it and see for yourself.
You can download and use it for free from the AppDynamics website.
Good luck in your search for the right APM tool.
Yes it will if the application is sensitive to extra GC cycles caused by call stack sampling.
The impact will depend on the number of threads and typical call stack depth.
This is not specific to AppDynamics, other call stack sampling solutions such as NewRelic and VisualVM Sampler will have a similar impact.
http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_Good_APM_vs_AppDynamics_Bad_APM.pdf
http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_vs_AppDynamics.pdf
There are a number of assumptions made by a vendor but the following are most common:
TRANSLATION: We assume you have a slow performing database backend.
TRANSLATION: We assume that you already know your performance hotspots.
TRANSLATION: We assume that you will not notice tricks used to hide our overhead.
TRANSLATION: We assume that you know little about performance engineering.
And my favorite (5) is the restriction within a vendors software license on the publication of benchmark results.
TRANSLATION: We assume that you blindly accept our claims – unquestionably.
Appdynamics will not slow down your system significant, I was on a usermeeting and they said that they always try to be under 2% cpu usage, thats nothing compared to what you get from them.
They are working with samples per time, so if you have 10 requests per second or 100, they will still take the same amout of your cpu / bandwith / whatever.
The way that these products generally work is by doing bytecode injection / function interposition / monkey-patching on commonly used libraries and methods.
For instance, you might hook into JDBC query methods, servlet base classes, and HTTP client libraries.
When a request enters the application, track all the important methods/calls it makes, and log them in some way.
Take the data and crunch it into analytics, charts, and alerts.
On top of that, you can start to add in statistical profiling or other options.
The tricky things are tracking requests across process boundaries and dealing with the volume of performance data you'll gather.
(I work on this problem at AppNeta)
One thing to check out is Twitter Zipkin ( https://github.com/twitter/zipkin ), doesn't support much and pretty early-stage but interesting project.
Both AppDynamics and New Relic use Standard BCI to monitor the common interfaces (entry and exit points) developers use to build applications (e.g.
Servlet, struts, SOAP, JMS, JDBC, ...).
This provides a basic skeleton of code execution (call graphs) with timing information which represents less than 5% of code that is executed.
The secret is to then uncover the timing of the remaining 95% code execution during slowdowns without incurring too much overhead in a production JVM.
AppDynamics uses a combination of in-memory agent analytics and Java API calls to then extract the remaining code execution in real-time.
This means no custom instrumentation is required or explicit declaration of what classes/methods you want the monitoring solution to instrument.
AppDynamics data collection is very different to that of New Relic.
For example, with AppDynamics you can get a complete distributed call graph across multiple JVMs for a specific user request, rather than say an aggregate of requests.
BCI is a commodity these days, the difference is in the analytics and algorithms used by vendors that trigger diagnostics/call graph information so you end up with the right visibility at the right time to solve problems.
Steve.
That error means that gradle is unable to resolve the dependency on  com.appdynamics:appdynamics-runtime .
The easiest way to fix this problem is to use the AppDynamics libraries from maven central rather than the  adeum-maven-repo  directory.
You can do that by editing your top level gradle file to look like this:
Then your project-level gradle file would look like:
Note that I have removed the references to  adeum-maven-repo , and changed the version numbers on the AppDynamics artifacts to refer to them as they exist in maven central.
Once you've done this, you no longer need  adeum-maven-repo  in your project, since gradle is now downloading these dependencies automatically.
JAVA_OPTS="$JAVA_OPTS -Djboss.modules.system.pkgs=org.jboss.byteman,com.singularity,org"
If you do not initialize the JVM, the installation throws a "class not found" exception.
-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/p:
/jboss-logmanager-.jar
JDK9 and above, -Xbootclasspath/p option has been removed; use -Xbootclasspath/a instead.
-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/a:
/jboss-logmanager-.jar
https://docs.appdynamics.com/display/PRO45/JBoss+and+Wildfly+Startup+Settings#JBossandWildflyStartupSettings-InitializetheJVM
A great place to ask this question would be on the AppDynamics discussion forums so that AppDynamics support can answer you directly...  http://appsphere.appdynamics.com/t5/Discussions/ct-p/Discussions
I'm guessing there is a permissions issue somewhere.
You should copy all files to run the agent, not just javaagent.jar.
This is a thread about it.
http://appsphere.appdynamics.com/t5/Lite-for-Java/Keep-getting-a-quot-Invalid-Agent-Installation-Directory-quot-on/td-p/1151
The definition can be found in AppDynamics docs:  Slow and Stalled Transactions
By default AppDynamics considers a slow transaction one that lasts longer than 3 times the standard deviation for the last two hours, a very slow transaction 4 times the standard deviation for the last two hours, and a stalled transaction one that lasts longer than 300 deviations above the average for the last two hours.
Of course, you can modify the default rules by providing your own:  Configure Thresholds
Even it is not the desired behaviour, if filtering by URL and specifying only createNewActivity worked for me (as long as there are no other REST URLs matching this).
Generally, monitoring tools cannot record method-level data continuously, because they have to operate at a much lower level of overhead compared to profiling tools.
They focus on "business transactions" that show you high-level performance measurements with associated semantic information, such as the processing of an order in your web shop.
Method level data only comes in when these business transactions are too slow.
The monitoring tool will then start sampling the executing thread and show you a call tree or hot spots.
However, you will not get this information for the entire VM for a continuous interval like you're used to from a profiler.
You mentioned JProfiler, so if you are already familiar with that tool, you might be interested in  perfino  as a monitoring solution.
It shows you samples on the method level and has cross-over functionality into profiling with the native JVMTI interface.
It allows you to do  full sampling of the entire JVM  for a selected amount of time and look at the results in the JProfiler GUI.
Disclaimer: My company develops JProfiler and perfino.
Information I got from another website.
Application Insights (AI) is a very simplistic APM tool today.
It does do transaction tracing, it has very limited code and runtime diagnostics, and it doesn’t go very deep into analyzing the browser performance.
Application Insights is much more like Google Analytics than like a typical APM tool.
We could install the AppDynamics extension use Azure portal or Kudu tool( https://functionAppname.scm.azurewebsites.net/ ).
Azure Portal:
Kudu UI
After installation
I have used Crashlytics and Google Analytics together without any issue.
All the logging is done in a background process so I don't think you will notice any speed degradation but the app is technically doing more work so there is some sort of performance hit.
I haven't seen any issues with the crashlog.
Analytics libraries just write the crashlogs to a file and then send them the next time the user opens your app.
They are not affecting how the actually crashes are handled by the operating system so there shouldn't be any issue with them conflicting.
First  : I would strongly suggest removing the unused one (or the one you don't prefer) from your code.
For reasons, like : 
1.
It will increase project size which in turn will increase your bundle size.
2.
Messy code.
3.
There is no point checking two different analytics.
4.
While third person is understanding the code, he would waste his time in understanding which will lead to confusion.
I might be missing other reasons.
Second  : To answer your question, it should work fine.
I did the same in one of my projects, where initially I was using  Hockey Crash reporting .
But then client asked to use  Crashlytics .
I didn't remove Hockey SDK immediately.
Though this worked fine and both reported the issues, but soon I removed Hockey SDK from the code.
There are integration tools available between influxdb/AppDynamics and grafana/AppDynamics.
https://github.com/Appdynamics/MetricMover
https://grafana.com/plugins/dlopes7-appdynamics-datasource/installation ).
There's nothing that integrates between Prometheus and AppDynamics at the moment
I'm not sure there will be one going forward, seeing how they are competing in the same space from different vantage points (Open Source vs Enterprise)
If you want to monitor only your worker processes of IIS and Standalone Service, You can use CLR Crash event on your policy configuration.
AppDynamics automaticaly creates CLR Crash Events If your IIS or Standalone Service are crashed.
You can find the details of CLR Crash Events:
 https://docs.appdynamics.com/display/PRO45/Monitor+CLR+Crashes
Also, Sample policy configuration: 
 Policy Configuration Screen
AppDynamics have a fork of contrib project  here  where there is an exporter but it isn't clear if it has been finished
there is a Beta program now running for using AppDynamics with the OpenTelemetry Collector.
More details are available in the public docs:  https://docs.appdynamics.com/display/PRO21/Ingest+OpenTelemetry+Trace+Data
Looks like you create the metric, then edit a dashboard, then click on a widget -  add metric -  (browse, but choose "Individual Nodes" instead of JMX, then select your metric.
voila.
The best place for you to get this question answered would be on the AppDynamics community discussion boards.
Here's a link for you...  http://community.appdynamics.com/t5/Forums-Community-AppDynamics/ct-p/Discussions
The AppDynamics documentation site is also a great resource and you don't even need a login to access them...  http://docs.appdynamics.com/
Installation instructions for the controller can be found in:
 http://docs.appdynamics.com/display/PRO14S/Install+the+Controller 
Assuming you are using App Server Agent for Java, installation instructions for that and the machine agent can be found in the above site on the Left Nav table of contents.
The controller is the management server, the app server agent monitors the JVM and the machine agent collects system metrics (such as CPU, Memory, Disk &amp; Network).
Very minimal configuration is required.
From what I understand from their documentation, AppD do not have a way to capture heap dumps.
They suggest using Memory Leak detection feature in such scenarios.
On a different note, I know we can get thread dumps which maybe helpful in some cases (Agents -  Request Agent Log Files)
Heap dumps in appdynamics can be taken for JRockit JVM by the below method(Note : This does not work for IBM JVM)
p0 – Path to generate heap dump(/path/dump.hprof)
p1 -  True – GC before Heap dump ; False - No GC before heap dump
Note : If you want heap dump to be generated in the case of out of memory give
p0 : HeapDumpOnOutOfMemoryError
Also note that these values will be lost on JVM restart.
Data retrieval by APM tools is done in several ways, each one with its pros and cons
Bytecode injection  (for both Java and .NET) is one technique, which is somewhat intrusive but allows you to get data from places the application owner (or even 3rd party frameworks) did not intend to allow.
Native function interception  is similar to bytecode injection, but allows you to intercept unmanaged code
Application plugins  - some applications (e.g.
Apache, IIS) give access to monitoring and application information via a well-documented APIs and plugin architecture
Network sniffing  allows you to see all the communication to/from the monitored machine
OS specific un/documented APIs  - just like application plugins, but for the Windows/*nix
Disclaimer : I work for Correlsense, provider of APM software SharePath, which uses all of the above methods to give you complete end-to-end transaction visibility.
I've tried add multi dex without giving any minimum or maximum of method count per dex file wise.I've tried with simply just adding multidex and able to build.And Yes!!
I am able to build app too.
major change is in  afterEvaluate  &amp;  incremental true  in  dexoption .
build.gradle
application's parent gradle
If above thing have still issue just check your dependecies hierarchy if any other extra dependecies are added (Based on your  build.gradle    packagingOptions  there should be some other dependecies there).Not sure but it may possible because of internal library conflicts its not proceeding further to create dexfile or build.
Let me know if anything
You can use the REST API of the Controller described here  https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs
To access the REST API Browser, in a Web browser, go to:
this will give you a nice swagger UI description of the available resources.
Alternatively you can create reports directly out of the box in AppDynamics via the  Dashboards &amp; Reports  section.
First you need to setup the AppDynamics Controller (should be hosted on a separate  machine), then you need java agents on the machine with the application.
You need to wire the application with the Java agent, e.g.
like this
Now the Java agent sends application information to the controller.
Take a look at the documentation.
https://docs.appdynamics.com/display/PRO43/Getting+Started
The application will automatically be available in AppDynamics and you can see the dashboard.
You don't need to host the controller, you can get a SAAS account here.
https://www.appdynamics.com/free-trial/  .
Once you get a SAAS account lookup the account name and account key in the welcome email and use the instructions in the email to add JVM args to your app as shown.
Don't forget to set the account name and account access key args: -Dappdynamics.agent.accountName -Dappdynamics.agent.accountAccessKey .
See  4.3.x Documentation⇒POJO Entry Points⇒Monitor Java Interface Static and Default Methods :
Note that another Java language feature introduced in Java 8, lambda method interfaces, are not supported by the AppDynamics Java Agent.
It’s possible that this is due to technical difficulties with  JDK-8145964  as you suspect.
But I’d also point out that this kind of Instrumentation would be questionable.
It’s not this JRE generated class that implements any specific behavior, it’s the invoked target method.
Support for Lambda expressions has been in the product since 4.1 which shipped in 2015 I believe.
That being said we are always enhancing support.
These do have some limitations after initialization of the classes using them (dynamic instrumentation limits).
The product should support them, we have added some additional capabilities and features for Lambda expressions in our next major release.
Have you tried to contact help @ appdynamics.com
Looks like it was not mentioned in release notes, but instead  Support Advisory 56039  was raised.
They indeed mention JDK-8145964 as a reason for removing the support.
I got this too...
I was running from command-line as a non-root user:
I added the shell expand(-x) switch and log to the command(s) like so:
If we tail the last bit of that log you get, this response in debug mode:
and the script checkLibaio.sh isn't left there... so you cannot figure it out easily.
I also have a RedHat variant with the packages installed:
Strangely enough I have one VM from the same image that will install the distribution just fine, and one that will not, so on the broken install (where I really want to install this).
I ran another command from the expanded view of the install.log, which was a really long JVM command line.
Anyways I got it to work and then made a looping script to retrieve the file (Because AppD for some reason removes the check script before you can look at it).
The script is as follows:
I you run this script like me on the faulty platform what you will discover is that your version of Linux has both:
and
installed.
To work around this you should temporarily make a name change to one of these two package manager executables so it cannot be found (by your shell environment).
Most common here will be that you are running a RedHat variant where someone chose to install dpkg (For who knows what reason).
If so desired remove that package and the install should be successful.
As stated on the AppD docs regarding  Kubernetes and AppDynamics APM
Install a Standalone Machine Agent (1) in a Kubernetes node.
Install an APM Agent (2) inside each container in a pod you want to monitor.
The Standalone Machine Agent then collects hardware metrics for each monitored container, as well as Machine and Server metrics for the host (3), and forwards the metrics to the Controller.
ContainerID and UniqueHostID can be taken from  /proc/self/cgroup
ContainerID  cat /proc/self/cgroup | awk -F '/' '{print $NF}'  | head -n 1
UniqueHostID  sed -rn '1s#.
*/##; 1s/(.{12}).
*/\1/p' /proc/self/cgroup
Thanks for the reply to my question.
The .NET agent in most APM tools works the same way which leverages the profiler APIs in the .NET SDK and allows for data collection and also callbacks and other interception.
Most of the tools also use performance counter data, and other sources aside from inside the .NET runtime.
This allows you to do several things similar to Java in terms of data collection.
ref:  https://docs.microsoft.com/en-us/visualstudio/profiling/walkthrough-using-profiler-apis?view=vs-2017
http://www.blong.com/conferences/dcon2003/internals/profiling.htm
The solution was adding "appdynamics" to the "externals" in the Webpack configuration:  https://webpack.js.org/configuration/externals/
This allows AppDynamics to use the default Node.js require import.
The most important thing to understand about this error is the meaning of this line:
Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
SSL certificates work by establishing a certificate chain, or a hierarchy of trust.
For example, if I go to  https://www.google.com  and look at their cert, this is what I see:
There is the google cert, which sits on their servers/CDN, then an intermediate cert which also sits on their servers/CDN, then a trusted root CA cert which is in the  client  keystore and is implicitly trusted.
So when someone browses to google, b/c they have the root CA cert and have trusted it, the browser (client) will trust that the server is actually who they say they are and will establish a secure connection to the site.
So getting back to your error, whatever CA issued the server cert being used by RabbitMQ, the monitor is not recognizing it as trusted.
To troubleshoot this error, here are the things to do:
There are a couple other gotchas to watch out for:
This issue was solved here:  https://community.appdynamics.com/t5/Java-Java-Agent-Installation-JVM/monitor-URL-page-availability/td-p/40904
Summary:
Turns out if you go to configure -  instrumentation -  JMX tab then voila, you can now delete metrics and modify/edit them.
But nowhere else.
Odd.
I've instrumented  org.apache.ode.bpel.engine.BpelProcess.handleJobDetails and it is showing me transactions going through Carbon out to the backends.
It is a bit hard to completely answer your question and solve the issue with the provided information.
However, I hope my questions below help you to get on the right track.
1.)
After making the configuration change, did you also restart the AppDynamics.AgentCoordinator_WindowsService?
Without restarting it the new configuration will not be applied to the agent itself.
2.)
Also important is your windows service hosting any OOTB support entry point like WCF, ASP.NET MVC4 WebAPI, web services etc.?
If not, you need to setup a custom entry point.
If you  check out the AppDynamics documentation and search for'POCO Entry Points' you should get onto the right track
3.)
In case No.1 &amp; No.2 did not do the trick, could you please attach the config.xml file for review?
Or directly reach out to the AppDynamics customer success team.
Kind regards,
Theo
Disclaimer: I work for AppDynamics as part of the Customer Success team.
Just to add to you answer and to clarify a little.
Until release 3.7.7 the .NET agent from AppDynamics used the web.config (for IIS based application), App.config for standalone and windows services or the global.config plus some environment variables to configure the agent.
With the 3.7.8 or later release we replaced this with a cleaner truly singly configuration file approach.
The configuration file is called config.xml and located in the %ProgramData%\AppDynamics... directory.
For any version after 3.7.8 all settings have to be in the config.xml.
You really should take this up with AppDynamics support by filing a ticket or posting in the support forums...  http://www.appdynamics.com/support/#helptab
There are many things you should analyse.
It depends on what your LR portal uses most.
You may want to analyse web content , user, groups, calnder, theme related services.
I would ask support on this if you can't get it to work.
Here is the configuration for custom exit and entry points in the product:  https://docs.appdynamics.com/display/PRO14S/Configure+Custom+Exit+Points
Love to help you out here, but there are no details about the error, please email help@appdynamics.com for assistance.
I assume this is C#, but wouldn't know based on this.
AppDynamics supports many languages and technologies.
For whom it might be of interest I have found a workaround and more details about this issue.
This occurs only in the following scenario:
If you use the MsBuild 14 that is installed along Microsoft Visual Studio 2015 RC then this issue does not occur anymore.
From my first findings there is an issue in ServiceStack's way of caching the endpoints and wrapping the execute method using Linq but I don't understand why this happens only when AppDynamics agent is installed.
@mythz if you want to dig deeper into this issue I'm available to help but with the above solution everything is ok.
Hope this helps
There are lots of APIs available out of the box, you can find the docs at  https://docs.appdynamics.com/  not everything has an API.
You should find the user management as part of the configuration API here :  https://docs.appdynamics.com/display/PRO42/Configuration+API
They do different things.
ELK will give you log aggregation that you can add in other functionality.
Appdynamics is great for real time monitoring and profiling.
I think it depends on what you're going for.
Logging a distributed system and capturing error messages in one place might be very helpful with ELK.
Not just that, but ELK can be used in a number of other ways.
Elasticsearch can be used stand alone as a search engine or data cache.
TL;DR  It depends on what you're doing.
Maybe yes...maybe no...
On that particular server, we have the .NET agent running as well.
After shutting off the .NET agent, we were no longer having this issue with 1-2 hour gaps in the metric browser.
Apparently, there is some conflict in having multiple machine agents installed on the same server.
You need both unit tests and integration tests.
Unit tests should not use in database or File, ect.
I like to use Spring profiles for my tests.
For instance, if I have a profile called integeration_test.
(I'm using xml) then in your context do something like:  &lt;beans profile="test"&gt;TODO &lt;/beans&gt;  And configure your data-source in there.
I know there are ways to rollback all your transactions after running a test, but I like this better.
Just don't delete everything in your real database haha, could even put some safety code in the clearDatabase to make sure that doesn't happen.
For performance testing you will really need to figure out what you want to achieve, and what is meaningful to display.
If you have a specific question about performance testing you can ask that, otherwise it is too broader topic.
Maybe you can make a mini-webapp which does performance testing for you and has the results exposed as URL requests for displaying in HTML.
Really just depends on how much effort you are willing to spend on it and what you want to test.
Once the agent is attached you can use the  AppDynamics Database Queries Window
Reflection (inherently  TypeVisitor  and  TypeToken  classes) is always costly in Java, try not using it.
Rendering time seems OK.
There can be thousand reasons for high latency in an application, but you only gave this much information so that's about the best answer you can get.
That's the beauty of APM is you don't need to deal with logging to get performance data.
APM tools instrument the applications themselves regardless of what the code does (logging, generating metrics, etc).
AppDynamics can collect log data, and provide similar capabilities to what a log analytics tool can do, but it's of less value than the transaction tracing and instrumentation you get.
Your application has to be built in a supported language (Java, .NET, PHP, Python, C++, Node.js) and if it's web or mobile based you can also collect client side performance data and unify between both frontend and backend.
If you have questions just reach out and I can answer them for you.
Good luck!
You basically need the AppDynamics Controller and a AppDynamics Machine-Agent which will be installed on the machine to monitor.
In the Machine-Agent configuration you set the URI of the controller and the agent starts to report machine metrics to the controller.
Then you can configure alarms, see metrics, create dashboards, etc.
I can deliver you more information if you want, but as Jonah Kowall said, take a look at the documentation as well  AppDynamics Machine Agent Doc
Typically, you don't need to change anything in your code to capture JMX metrics except your Java Beans have to fulfill the Management Beans (MBeans) requirements you have to enable JMX monitoring in each Java process that should be monitored and the monitored system runs on Java 1.5 or later.
See also  here  and  here .
Then you can navigate to  Tiers &amp; Nodes -&gt; Select a tier -&gt; JMX tab
I am not exactly sure what the problem is, but you can share a Dashboard and open this shared Dashboard URL and make the Browser fullscreen.
Thats how we do it and it works perfectly (on AppDynamics controller version 4.2).
We use the 'revolver tab' add-on for the browser to switch between desktops.
Try adding the line below in the crx-quickstart/conf/sling.properties:
org.osgi.framework.bootdelegation=com.singularity.*,com.yourkit.
*, ${org.apache.sling.launcher.bootdelegation}
You can build multiple baselines within AppDynamics if you'd like to.
The thresholds should be auto calculated off deviation from baseline.
This makes it so you don't need to configure them manually.
If you want to do SLA tracking, Business iQ (analytics) can do this very well.
We also are building additional features around SLA use cases we can share.
Feel free to email me or support for a hand.
I think everything you need is described  here  
Basically you need to do the following:
probably in your case it is
via CURL it looks like this
You can find more information about the AppDynamics API  here
Inside the widget settings make sure you select the "Stack Areas or Columns" checkbox.
This works in every version of AppD I've used including 4.3.0.2.
SQL 2005 is supported, but this was a bug which was introduced in version 4.3.0.
There is currently a diagnostic patch for this issue for supported customers.
The fix should be in the next patch level once we isolate the issue.
If you'd like support just email help@appdynamics.com and they can assist.
Thanks.
In order to capture this data follow these steps:
Pointer, you can also adjust the time periods in the metric browser to adjust the time.
Hope this is helpful.
Couple of things:
I think the general URL format for app dynamics applications are (notice the '#'):
Also, I think the requests.get method needs an additional parameter for the 'account'.
For instance, my auth format looks like:
I am able to get a right response code back with this config.
Let me know if this works for you.
You could also use native python code for more control:
example:
If you prefer it in JSON simply specify it in the request.
You can get severity information by appending &amp;severities=INFO,WARN,ERROR to your URL.
So your url must be like :  http://:/controller/rest/applications//business-transactions?output=JSON&amp;severities=INFO,WARN,ERROR
Severity is associated with specific events/entities in AppDynamics.
Based on your API call I can see that you are trying to retrieve information about Business Transactions (BTs).
Severity param is not associated with BTs.
e.g.
You can pull Severity for Health rule violations in AppDynamics by making the following API call:
http:///controller/rest/applications//problems/healthrule-violations
Result:
You can find further information about using AppD controller API in the following documentation pages:
https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs
https://docs.appdynamics.com/display/PRO42/Alert+and+Respond+API
The Issue is resolved.
The controller-info of Machine Agent need not to have any Application,Node and Tier name.
It should include unique host id which should be same as controller-info of JavaAgent.
It looks like you can use environment variables to configure the python appdynamics agent as well.
Open up your repl
For the usual configuration values (APP_NAME, TIER_NAME, NODE_NAME, etc) you can configure them via the environment variables.
You just need to prefix them with 'APPD_'.
For for APP_NAME it would be:
You can configure the python agent in your code like so:
Alternatively, you can pass in the location of your appdynamics.cfg file.
That is to say, setting environment variables is not enough.
Then you need to manually start the proxy (after you  appd.init ) by running
 pyagent proxy start
The agent configuration from your code will be automatically used by the proxy.
For a full list of config keys see the  setting docs
I managed to define just environment variables without changing application code.
Note that variable name for controller host is APPD_CONTROLLER_HOST.
You can also pass command line parameters to the process.
Yes, it does transaction tracking for every intra-component call across languages.
Without code changes.
When there is a slow transaction detail down to code level will show up in snapshots, this is what you'd use for diagnostics.
Depending on your language you can also configure data collectors which do runtime instrumentation of custom data from the code.
These show up on every call, and you can turn them into metrics too.
Once again no code changes.
Zipkin only does tracing.
APM tools like Appdynamics do other monitoring (browser, mobile, database, server, network).
Code-level diagnostics with automated overhead controls and limiters.
Don't forget log analytics and transaction analytics.
It also collects metrics.
There is a lot more to APM than just tracing, which is what Zipkin does.
You could do this with a stack of 20 open source tools, but you'd have to deal with disjointed UIs and data models not to mention the work associated with keeping them all working.
It's actually more the other way around - Crittercism does crash logging, network monitoring, and performance timing whereas AppDynamics is more focused on server monitoring.
Both products essentially do the same thing, if anything AppDynamics has advanced beyond Crittercism (now called Apteligent).
They had the lead when the mAPM market started, but now there is not much innovating happening, especially after they sold the company to VMware earlier this year.
Here are the mRUM docs for AppD:  https://docs.appdynamics.com/display/PRO44/Mobile+Real+User+Monitoring  some of the more advanced features in AppD you will not find in Appteligent would be things like screen recording, breadcrumb/navigation for every click and interaction.
The most valuable feature, of course, is the measurement outside of a straight mobile use case.
AppD does measure and ties together the mobile transaction with the backend, server (and network), Docker containers, AWS or other cloud providers.
Additionally, it monitors performance and usage of browsers.
Disclosure: I used to be the lead for these products at Gartner, and have been working at AppDynamics for the last 3 years.
For Apple platforms I recommend you to avoid any third party crash log frameworks and use  Xcode crashes organizer.
It seems that you schould be able to define your custom  Asynchronous Transaction Demarcator  as described in:   https://docs.appdynamics.com/display/PRO44/Asynchronous+Transaction+Demarcators
which will point to the last method of Runnable that you passes to the Executor.
Then according to the documentation all you need is to attach the Demarcator to your Business Transaction and it will collect the asynchronous call.
The first step is to add a username and password in the etc/users.properties file.
For most purposes, it is ok to just 
use the default settings provided out of the box.
For this, just uncomment the following line:
Then, you must bypass credential checks on BrokeViewMBean by adding it to the whitelist ACL configuration.
You can do so by replacing this line:
with this:
In addition to being the correct way, it also enables several different configuration options (eg: port, listen address, etc) by just changing the file org.apache.karaf.management.cfg on broker's etc directory.
Please keep in mind that JMX access is made through a different JMX connector root in this case: it uses  karaf-root  instead of  jmxrmi , which was previously used in the older method.
It also uses port 1099 by default, instead of 1616.
Therefore, the uri should be
From my current knowledge: no, AppDynamics doesn't support OpenTracing yet.
Usually, APM vendors have their own OpenTracing tracers build off the official specification and then get them listed at  http://opentracing.io .
But as of this writing there is no mention of any AppDynamics Tracers at  https://opentracing.io/docs/supported-tracers/  nor  https://github.com/opentracing-contrib/meta .
Full disclosure: I work for Instana, a competitor that does support OpenTracing.
No AppD doesn't support OpenTracing at this time.
The question is why do you want this functionality when you can already extract custom data from transactions dynamically with most AppDynamics agents?
Do you really want to hardcode your APM tool into your software application?
AppDynamics is building a unique way to support OpenTracing, which is currently in testing, but the approach is not by hardcoding libraries into the code.
If you'd like to learn more please reach out to support, or you can contact me directly as I work for AppDynamics.
Thanks.
You need to add:
into   wrapper.conf  file.
https://docs.appdynamics.com/display/PRO45/Tanuki+Service+Wrapper+Settings
You can export the dashboard and you will get the json version of your dashboard.
(The export button is located top of the your dashboard page)
This json file can be easily editable with any editor and you can replace the Application and/or other properties which contain on of your dashboards.
https://docs.appdynamics.com/display/PRO45/Import+and+Export+Custom+Dashboards+and+Templates+Using+the+UI
Due to its proprietary nature i don't think the internals are available for anyone to view or discuss.
Below link might give an idea of how the product works at a high level.
https://www.appdynamics.com/product/how-it-works/
You are on the right track, but you are not saying that you are having errors or showing the.
Yet, based on your experience to date, I am sure you already know that PowerShell provides cmdlets for working with XML.
See them using ---
or get other XML modules from the MS PowerShellGallery.com using ---
--- and install the one(s) that fit your needed goals.
And of course there are lot's of examples and videos on the topic.
Searching for 'PowerShell working with XML', gives you plenty of hits.
PowerShell Data Basics: XML
For the most part, what you will find is very similar to what you already have posted.
Yet, you say you want add nodes / elements, but that is not in your post, so, something like the below should help.
Or even just using the  WebAdministration module  on the IIS server directly.
You should check out the AppDynamics agent installation PowerShell Extension:  https://www.appdynamics.com/community/exchange/extension/dotnet-agent-installation-with-remote-management-powershell-extension/
It should be able to handle what you are trying to do without having to generate the xml manually, check the pdf for advanced options and read about the Add-IISApplicationMonitoring cmdlet
What it can’t do is the newer stuff like “multiple business application” and “custom node name” configuration.
(Which you can’t do in the install wizard either)
AppDynamics itself powerful enough to provide all sort of information, However if you still want actuator data to be captured and displayed then you may need to use AppD extension.
Please refer below official link to AppDyanmics Exchange.
https://www.appdynamics.com/community/exchange/
if the relevant is not available you may need write your own.
I found this solution  https://github.com/Appdynamics/flowmap-builder
Seems to be working so far.
Doesn't rely (directly) on APIs but it works!
For anyone who is looking for an answer here.
This is resolved by unchecking the Extended  tracking option for kafka in AppDynamics Console.
The option will be there in Automatic Backend Discovery.
Hope this helps.
Disable default Kafka configuration on "Backend Detection" page
Define a new "Custom Exit Point" with the last class and method which you see on the call graphs before Kafka exit call
Don't forget to click on "Is High Volume" checkbox.
If we take the example of an ECommerce Application:
Business Transactions  are Checkout, Landing Page, Add to Cart etc.
which are known by every end user of the application.
These business transactions cover all the method executions, database calls, web service calls etc.
Service End Points  are the sub calls(method call or web service call) execute inside of the Business Transactions.
Such as "Check Inventory" service which is executed in Checkout and Add to Cart transactions as well.
Information Points  are the key business or technical metric counts, such as Checkout amount, Add to Cart item count.
Service End Points and Information Points only give you performance metrics but Business Transactions also give you full code visibility with "call graphs"
Also, there are some limitations like max 200 Business Transaction on the default but you can change these rules.
While configuring the BTs &amp; SEs, you must focus on the needs of AppDynamics users.
If you configure AppDynamics for mostly business teams, I can use BTs like I described above.
But If you target the Dev and Ops teams, you can configure your BTs based on method or service calls.
There is no only single approach on BT &amp; SE configuration.
You must shape that with the needs of your AppDynamics users.
Configuration- Instrumentation- Transaction Detection- Add
On the "Split Transactions Using Request Data" section you must choose " Specific URI Segments "
Segment Numbers: 1,2,4
In your case transaction name will be "/data/scenario/job"
Sample Configuration:
On analyzing the heap dump taken during system idle state, we only see various WebAppClassLoaders holding instances of different library classes.
This pattern is also explained in official blogs of APM experts like  Plumbr  and  Datadog  as a sign of healthy JVM where regular GC activity is occurring and they explain that it means none of the objects will stay in memory forever.
From Plumbr blog:
Seeing the following pattern is a confirmation that the JVM at question is definitely not leaking memory.
The reason for the double-sawtooth pattern is that the JVM needs to allocate memory on the heap as new objects are created as a part of the normal program execution.
Most of these objects are short-lived and quickly become garbage.
These short-lived objects are collected by a collector called “Minor GC” and represent the small drops on the sawteeth.
The reason you don't see the browser name in HTTP protocol is because there is no browser.
The protocol is a transport level protocol which means it simulates the traffic of the browser without running the actual browser.
This allows the protocol to simulate many more virtual users than a client level protocol such as TruClient.
EDIT: There is no dedicated API and you must use the user agent.
Please refer to this article for more details:  https://www.appdynamics.com/blog/engineering/how-to-use-appdynamics-with-loadrunner-for-load-testing/
I'd try setting &quot;Use data from last&quot; to  60 minutes .
In Criteria-tab use &quot;Single Metric&quot; with  Sum  of &quot;Calls per Minute&quot; and define your threshold.
The Drop-off rate is the percentage of user sessions which reach the specific page and then end their session (they do not reach any further pages as part of the session and therefore &quot;drop-off&quot; the map).
This can be seen in the example (image) in the  documentation
Yes - you can do this using ADQL / Analytics searches (assuming you have this licensed).
Without specifics I can only give you a general guide:
Generally the workflow for sending reports from AppDynamics is as follows:
Note: There is a sample custom dashboard .json available here to get you started:  https://community.appdynamics.com/t5/Knowledge-Base/Sample-Custom-Dashboard-for-Business-Transaction-Report/ta-p/21264
Note: There are already many &quot;Standard Reports&quot; which could make things easier depending on use case.
Note: If you instead want to export data and then analyse with your own tooling, then see the docs on Public API's available here:  https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs
All currently available AppDynamics APIs are documented here:  https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs
The main page includes a summary of what is available.
Business transactions should accomplish this.
If you want to report on each web service you can build a report or custom dashboard.
If you need more assistance just email help@appdynamics.com
AFAIK the critical point for AppDynamics (or profilers like that) it is essential to find an entry point.
Usually the prefered way is to have an Servlet "Endpoint" that starts a threat and can be followed.
For the scenario you are describing this wouldn't work as it's missing the "trigger" to start the following.
Most likely you'll need to build your own app-dynamics monitoring extension for it.
By default much of the Apache stuff is excluded.
Try adding Call Graph Settings (Configure    Instrumentation    Call Graph Settings), to include specific transports, like org.apache.camel.component.file.
* in the Specific sub-packages / classes from the Excluded Packages to be included in call graphs section.
Do not include org.apache.camel.
* as it will instrument all the camel code which is very expensive.
You may want to do it at first to detect what you want to watch, but make sure to change it back.
Edit AppServerAgent\conf\app-agent-config.xml:
From the Controller web site:
Configure    Instrumentation    Call Graph Settings
Add Always Shown Package/Class: org.apache.camel.
*
Servers    App Servers    {tiername}    {nodename}    Agents
App Server Agent
Configure
Use Custom Configuration
find-entry-points: true
Can I ask if your trial is beyond 15 days?
According to their site the trial only includes monitoring for one agent beyond that time - that would explain why only your first agent delivers data.
Other tools in that space, e.g: Dynatrace free trial - gives you a bit more time if that's what you need and also allow you to extend the free trial if you need more time
You have gone beyond the 15 day pro trial and you are using the free product.
If you want to buy the product you can get an extension on the trial, if you do not want to buy the product it will not trace across JVMs.
Not sure if you already have resolved this, but: SNI is SQL Server Network Interface, and the mentioned method exists in most ADO.NET full call stacks that wait for data from SQL Server.
This is regardless of whether the higher-level implementation is EF, raw ADO.NET or whatever.
I'm not sure which metric or signal AppDynamics uses to capture the completion of a stored procedure execution, but you could be seeing this kind of behavior if your stored procedure completes relatively fast, but transmitting the query result from the server to your client takes a while.
Without knowing more about your infrastructure, it is very hard to help further.
If the problem still persists, I would recommend running the same query in SQL Server Management studio with SET STATISTICS TIME ON and "Include Client Statistics" switched to on.
Perhaps those numbers would give you an idea on whether data transfer is actually the problem.
In my case it was indeed, as Jouni mentions, very slow transmitting of the query results.
I use Automapper to prepare data for sending to client.
So, it's unclear what exact property caused the load but to be sure I've cut all compound ones I don't need to show on client side.
(I originally needed a collection to show in grid on client side.)
The execution became very fast.
I've came across similar issue - it turns out that SqlDataReader.Dispose can get stuck for very lengthy time if you break early from large select.
see:  https://github.com/dotnet/corefx/issues/29181
please see my reply in 
 G1GC long pause with initial-mark
your every setting should has a solid reason to be here...and unfortunately some of them don't have e.g.
-XX:+UseBiasedLocking (used for combination of tenured and young generation GCs, but G1GC is capable to handle both) -XX:+DisableExplicitGC (its unpredictable, in my experience its never restrict explicit gc calls)
please use/tweak accordingly below mentioned settings to get optimum results, I'm giving you baseline to move forward, hope so these settings will work for you:
 Java G1 garbage collection in production
We recorded this bug on 1.7._u06 and upgraded to  1.7.0_21-b11  just a couple of days ago and we haven't seen any Full GC's since upgrade, so it seems like this bug was fixed in JVM.
The code cache memory profiles look much nicer now too in the profiler.
In the past, this problem used to be a daily one, one to more times per day.
If the situation changes, I will report back.
Until then, I consider this problem solved with the upgrade.
Your query profile shows that the "Query end" time is very large.
This may be caused by a very (too) large  query cache .
Every time you perform an update statement (INSERT, DELETE, UPDATE), the query cache must be updated (every query that reads from the updated tables is invalidated).
I got in touch with RDS engineers from amazon and they gave me the solution.
Such a high latency was due to a very low performing storage type.
Indeed, I was using the default 5GB SSD (which they call GP2) which gives 3 IOPS per GB of storage, resulting in 15 IOPS when my application required about 50 IOPS or even more.
Therefore, they suggested me to change the storage type to  Magnetic  which provides 100 IOPS as baseline.
Moreover, I've also been able to decrease the instance type because the bottleneck was only the disk.
The migration took about 3h due to the very low performance of the source disk (GP2).
Hope it may help someone out there!
We never were able to properly solve the issue but at some point it vanished.
If I'm not wrong one the client change the appdynamics configuration / removed it which seemed to have solved the issue.
did you take a look to this google group ticket issue?
https://groups.google.com/forum/#!topic/hazelcast/ivk6hzk2YwA
Here in particular looks the reason of the issue.
https://github.com/hazelcast/hazelcast/issues/553
Have you tried to Minify your code?
Minifying unneccesary characters from your code without removing any functionality.
This could help speed up the download times.
Take a look at the following link:  http://www.htmlgoodies.com/beyond/reference/7-tips-to-make-your-websites-load-faster.html
Ill take a look at serving assets from webroot
From the book  (emphasis added):
It’s a well known fact that  serving assets through PHP is guaranteed to be slower than serving those assets without invoking PHP .
And while the core team has taken steps to make plugin and theme asset serving as fast as possible, there may be situations where more performance is required.
In these situations it’s recommended that you either symlink or copy out plugin/theme assets to directories in app/webroot with paths matching those used by CakePHP.
Depending on many factors, &quot;slower&quot; can be anywhere between barely-noticeable to barely-usable.
This advice is not version specific, and pretty much always applies.
To make assets load faster, let the webserver take care of them for you.
Yes, it is possible for the behaviour of GCs to change over time due to JIT optimisation.
One example is 'Escape Analysis' which has been enabled by default since Java 6u23.
This type of optimisation can prevent some objects from being created in the heap and therefore not require garbage collection at all.
For more information see  Java 7's HotSpot Performance Enhancements .
It is really difficult to find the exact cause of your problem without more information.
But I can try to answer to your question : 
 Can the OS block the garbage collection ?
It is very unlikely than your OS blocks the thread garbage collector and let the other threads run.
You should not investigate that way.
Can the OS block the JVM ?
Yes it perflecty can and it do it a lot, but so fast than you think that the processes are all running at the same time.
jvm is a process like the other and his under the control of the OS.
You have to check the cpu used by the application when it hangs (with monitoring on the server not in the jvm).
If it is very low then I see 2 causes (but there are more) :
In theory, YES, it can.
But it practice, it never should.
In most Java virtual machines, application threads are not the only threads that are running.
Apart from the application threads, there are compilation threads, finalizer threads, garbage collection threads, and some more.
Scheduling decisions for allocating CPU cores to these threads and other threads from other programs running on the machine are based on many parameters (thread priorities, their last execution time, etc), which try be fair to all threads.
So, in practice no thread in the system should be waiting for CPU allocation for an unreasonably long time and the operating system should not block any thread for an unlimited amount of time.
There is minimal activity that the garbage collection threads (and other VM threads) need to do.
They need to check periodically to see if a garbage collection is needed.
Even if the application threads are all suspended, there could be other VM threads, such as the JIT compiler thread or the finalizer thread, that do work and ,hence, allocate objects and trigger garbage collection.
This is particularly true for meta-circular JVM that implement VM threads in Java and not in a C/C++;
Moreover, most modern JVM use a generational garbage collector (A garbage collector that partitions the heap into separate spaces and puts objects with different ages in different parts of the heap) This means as objects get older and older, they need to be moved to other older spaces.
Hence, even if there is no need to collect objects, a generational garbage collector may move objects from one space to another.
Of course the details of each garbage collector in different from JVM to JVM.
To put more salt on the injury, some JVMs support more than one type of garbage collector.
But seeing a minimal garbage collection activity in an idle application is no surprise.
Does your OS have swapping enabled.
I've noticed HUGE problems with Java once it fills up all the ram on an OS with swapping enabled--it will actually devistate windows systems, effictevly locking them up and causing a reboot.
My theory is this:
At first it doesn't effect the system much, but if you try to launch an app that wants a bunch of memory it can take a really long time, and your system just keeps degrading.
Multiple large VMs can make this worse, I run 3 or 4 huge ones and my system now starts to sieze when I get over 60-70% RAM usage.
This is conjecture but it describes the behavior I've seen after days of testing.
The effect is that all the swapping seems to "Prevent" gc.
More accurately the OS is spending most of the GC time swapping which makes it look like it's hanging doing nothing during GC.
A fix--set -Xmx to a lower value, drop it until you allow enough room to avoid swapping.
This has always fixed my problem, if it doesn't fix yours then I'm wrong about the cause of your problem :)
One major GC per minute doesn't at all seem troublesome.
Usually it takes about half a second, so that's 1/120th of your overall CPU usage.
It is also quite natural that heavy application load results in more memory allocation.
Apparently you are allocating some objects that live on for a while (could be caching).
My conclusion: the demonstrated GC behavior is not a proof that there is something wrong with your application's memory allocation.
I have looked more carefully at your diagrams (unfortunately they are quite difficult to read).
You don't have one GC per min; you have 60  seconds  of major GC per minute, which would mean it's happening all the time.
That  does  look like major trouble; in fact in those conditions you usually get an OOME due to &quot;GC time percentage threshold crossed&quot;.
Note that the CMS collector which you are using is actually slower than the default one; its advantage is only that it doesn't &quot;stop the world&quot; as much.
It may not be the best choice for you.
But you do look to either have a memory leak, or a general issue in program design.
Are you always waiting for GC to take care of removing unused references?
Is there some places in your application that you know a reference to a heavy weight object won't be used anymore from that point, but it is not manually nulled?
Perhaps setting such heavy weight objects to null manually at the right places could prevent them growing till they hit the end of the heap....
When JVM reaches heap maximum size, GC is called more frequently to prevent OOM exception.
Normal program execution should not happen at such circumstances.
When a new object is allocated and JVM cant get enough of free space, GC is called.
This might postpone object allocation process and thus slowdown overall performance.
Garbage collection happens not concurrently in this case and you do not benefit from CMS collector.
Try to play with  CMSInitiatingOccupancyFraction , its default value is about 90%.
Setting this parameter to lower values, will force garbage collection before application reaches heap maximum.
Thus GC will work in parallel with application not clashing with it.
Have a look at article  Starting a Concurrent Collection Cycle .
Looking at the  source code  of  UTF8JsonGenerator._flushBuffer() , there is no indication of  LockSupport.parkNanos() .
So it has probably been inlined by the JIT compiler from  OutputStream.write() .
My guess is it's the place where – for your application – Tomcat typically waits until the client has accepted all the output (expect for the last piece that fits into the typical connection buffer size) before it can close the connection.
We have had bad experience with slow clients in the past.
Until they have retrieved all the output, they block a thread in Tomcat.
And blocking a few dozens threads in Tomcat seriously reduces the throughput of a busy web app.
Increasing the number of threads isn't the best option as the blocked threads also occupy a considerable amount of memory.
So what you want is that Tomcat can handle a request as quickly as possible and then move on to the next request.
We have solved the problem by configuring our reverse proxy, which we always had in front of Tomcat, to immediately consume all output from Tomcat and deliver it to the client at the client's speed.
The reverse proxy is very efficient at handling slow clients.
In our case, we have used  nginx .
We also looked at  Apache httpd .
But at the time, it wasn't capable of doing it.
Additional Note
Clients that unexpectedly disconnect also look like slow clients to the server as it takes some time until it has been fully established that the connection is broken.
You could use Javamelody, but you have to:
Generate war file from your play framework web
Edit web.xml in war file (http://code.google.com/p/javamelody/wiki/UserGuide?tm=6)
You need to set Multi-Release to true in the jar's MANIFEST.MF.
In the assembly plugin you should be able to do that by adding
to the configuration section of your assembly configuration.
You could also use the jar plugin to create your jar.
For that you would do
Use Stackify Prefix for this kind of things:
https://stackify.com/prefix/
View SQL queries: Including SQL parameters, affected records and how long it took to download the result set.
use tracer plugin where u can import the information in csv/html/xml.This is for jvisualvm
 http://visualvm.java.net/plugins.html
Start the application and run jconsole on the PID.
While its running look at the heap in the console.
When it near maxes get a heap dump.
Download Eclipse MAT and parse the heap dump.
If you notice the retained heap size is vastly less then the actual binary file parse the heap dump with -keep_unreachable_objects being set.
If the latter is true and you are doing a full GC often you probably have some kind of leak going on.
Keep in mind when I say leak I don't mean a leak where the GC cannot retain memory, rather some how you are building large objects and making them unreachable often enough to cause the GC to consume a lot of CPU time.
If you were seeing true memory leaks you would see GC Over head reached errors
You can get  POD_NAME  and  POD_NAMESPACE  passing them as environment variables via  fieldRef .
EDIT :  Added example env  REFERENCE_EXAMPLE  to show how to reference variables.
Thanks to  this  answer for pointing out the  $()  interpolation.
You can reference  supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP  as mentioned in the documentation  here .
However,  CLUSTERNAME  is not a standard property available.
According to this  PR #22043 , the  CLUSTERNAME  should be injected to the  .metadata  field if using GCE.
Otherwise, you'll have to specific the  CLUSTERNAME  manually in the  .metadata  field  and then use  fieldRef  to inject it as an environment variable.
Below format helped me, suggested by ewok2030 and Praveen.
Only one thing to make sure that the variable should be declared before they are used as JAVA_OPTS.
containers:
Are you sure you're not getting the FileNotFound when trying to do getInputStream on the connection later (or in your real code in case this is a simplified example)?
Since HttpUrlConnection implements http protocol and 4xx codes are error codes trying to call getInputStream generates FNF.
Instead you should call getErrorStream.
I ran your code (without the auth part) and I don't get any FilenotFoundException testing on url's that return 404.
So in this case HttpUrlConnection correctly implements the http protocol and appdynamics correctly catches the error.
I'm facing the same issue with jersey wrapping the FnF in a UniformResourceException but after some analyzing it's actually either jersey that should provide ways of checking the status code before returning output or correctly use httpurlconnection, and in our case - the webservice should not return 404 for requests that yields no found results but rather an empty collection.
Not sure whether it would be the cause of memory leaks, but your function definitely has a ton of unnecessary cruft that could be cleaned up with Bluebird's  Promise.mapSeries()  and other Bluebird helpers.
Doing so may very well help cut down on memory leaks.
doSomething  function reduced to 8 lines:
After this operations
will be
and
will be
Since you are a Spring Framework user, consider using  Spring AMQP .
The  RabbitTemplate  uses cached channels over a single connection with the channel being checked out of the cache (and returned) for each operation.
The default cache size is 1, so it generally needs to be configured for an environment like yours.
I'm leaning towards it being an issue with the number of channels and the channel cache size.
Does anyone know if there's a limit on the number of channels on a queue?
It seems like specifying connections rather than channels might help here.
If anyone has any information it would really help,  getting stuck for time :)
I think you should use  @PersistenceContext  annotation to obtain  EntityManager  from Spring context and  @Transactional  annotation to drive your transactions.
This way you won't need to worry about closing Hibernate sessions manually, and number of connections will not increase until there are  too many connections .
The problem did not lay in the sessions but in the connection pool, sessions where being managed correctly, but the connection pool in the JDBC layer wasn't closing the connections.
These are the things that I did to fix them.
JDBC context configuration
1.- Changed the JDBC connection factory from tomcat's old BasicDataSourceFactory to tomcat's new DataSourceFactory
2.-  Tuned the JDBC settings based on this article: 
 http://www.tomcatexpert.com/blog/2010/04/01/configuring-jdbc-pool-high-concurrency
Session factory xml configuration
3.- Deleted this line from the session factory configuration:
This is how my configuration ended up:
JDBC context configuration
Session factory xml configuration
Hey there, Just crawl in to this URL.
https://help.ubuntu.com/community/EnvironmentVariables
It will better help you.The above URL gives all informations about
  Environment Variable Ubuntu.
ANd the above POST is updated in 3 Jan
  2014.
Put the environment variables into the global /etc/environment file:
...
...
Execute "source /etc/environment" in every shell where you want the variables to be updated:
Check that it works:
Here is a other link from mkyong
Here is some info on environment variables, setting your path and where to install things that I hope is useful for you to get your environment setup.
.bashrc : is specific for the  bash  shell.
.profile : is used by several shells, and was originally used by the bourne shell (from memory).
.profile  might not be loaded by bash if there is a  .bashrc  present.
Some shells read it only if there is no shell specific configuration present.
If you happen to be using a different shell, you will need to look at how best to configure environment variables for that shell.
Note that adding to the above files only effect the user that you set them for, since they live in  /home/username/ .
Also remember to source the file again, or reload the shell so that your settings take effect.
You can achieve this with something like  source .bashrc  after you edit it at the command line to avoid having to restart or reopen terminal.
If you would like to set system wide variables, you can do that in  /etc/environment .
If you would like to execute java / ant / maven, etc.
from the command line, or enable applications that require the  PATH  environment variable to be set correctly to work, you will also need to add the  ./bin  directories to your PATH.
Depending on your preference regarding system wide or user specific path settings:
etc.
in the relevant file.
A side point and entirely optional, the correct place to install java, ant, maven, etc if not from .deb's would be in  /opt , according to the  FilesystemHierarchyStandard
You might check out Spring Insight.
Spring-insight source, design and alternatives
http://www.springsource.org/insight
Java Melody  might be relevant for you.
This is an issue with the AppDynamics plugin and gradle plugin tools 3.6.x.
You can see this link:  https://community.appdynamics.com/t5/End-User-Monitoring-EUM/AppDynamic-EUM-setup-for-Android-Cordova-project/td-p/38864
I have downgrade gradle plugin tools to 3.5.x and solve it
You can create a cron job to run from Monday to Saturday, here for each hour:
And another to cover the interval you want on Sunday, here one by one hour from 10:00 AM to 03:00 PM:
Looks like httprequest plugin does not support uploading zip file.
This is my observation.
I think upload will use  Content-Type: multipart/form-data .
But httpRequest plugin is not supporting this type.
However it supports  APPLICATION_OCTETSTREAM(ContentType.APPLICATION_OCTET_STREAM)
Could you post output from your curl?
Following Code worked for me :
It turns out the code I was given was completely wrong for angular 2 implementation.
The code they gave me is for running on the web server's side with node js.
Since angular 2 is an SPA that runs on the browser, it would never work.
I did some research and found this example application that I added a few tweaks to:  https://github.com/derrekyoung/appd-sampleapp-angular2
in your appdynamics.cfg, change
to
I am not exactly sure what the problem is, but I just tried it with AppDynamics Controller Version 4.2 to share a custom dashboard and open the shared Link in a different browser and I didn't need to put in any credentials.
Maybe AppDynamics Version 3.9 had a bug (assuming you used version 3.9 according to the docs link you posted)
Here is what I did:
CloudForms is not trying to substitute OpenShift UI, but complement it, giving you more information about the environment and providing additional capabilities on top of OPenShift.
You can see information about what is being done and demos in videos:
https://www.youtube.com/watch?v=FVLo9Nc_10E&amp;list=PLQAAGwo9CYO-4tQsnC6oWgzhPNOykOOMd&amp;index=15
And you can find the presentations here
 http://www.slideshare.net/ManageIQ/presentations
Just follow this istructions:
Clone a local copy of this project with
git clone  https://github.com/Appdynamics/ECommerce-Android
Open Android Studio and Import Project (select app/build.gradle)
Android Studio will ask you to build the project with gradle.
Gradle will use the  build.gradle  inside the project.
The version of gradle is inside the  gradle/wrapper/gradle-wrapper.properties  file:
Check for example the file inside the  app :
You need put a directory called "adeum-maven-repo" in your project setup .
https://docs.appdynamics.com/display/PRO39/Instrument+an+Android+Application#InstrumentanAndroidApplication-SetupforGradle
Whether this is an issue or not is really up to you to decide.
On the one hand, a full GC that runs for one second every 10 minutes is not a significant issue for throughput.
On the other hand, the full GC is probably going to dramatically reduce response times during that those one second windows.
But that may not be important or even relevant to your application.
The thing I would be worried about is whether your load testing is a realistic test.
The application appears to need 4Gb of heap space under test, but is it also going to need that in a real-world use?
I'd be worried that a memory leak might show up when it is deployed into production.
Or that the load in your load testing is causing the application's in-memory caching to reach a steady state in that won't be reproduced in production.
As a general rule, it is a bad thing for the heap to be running close to full, so an increase in the heap is probably advisable.
Your application's performance doesn't appear to be suffering, but you could be "on the cusp".
I thought that once old gen utilization is above certain level, GC will try to free/compact old gen area and if nothing is freed it would either fail with OOM and start crazy full GC cycles just before it.
I suspect that the monitoring reports might be misleading you.
If the full GC cycles were  really  not reclaiming anything, I'd expect the behaviour to be different.
Try turning on JVM GC log message, and see what they tell you about the amount of memory that the full GC cycles are managing to reclaim.
My answer with links got deleted by another SO user so I'm listing the steps here.
First uninstall using this
$ npm uninstall -g strong-cli
$ npm uninstall -g loopback-sdk-angular-cli
and then install
npm install -g strongloop
You can now run slc strongops
and let us know how it goes.
I created a Linux init.d Daemon script which you can use to run your app with slc as service:
 https://gist.github.com/gurdotan/23311a236fc65dc212da
Might be useful to some of you.
You should try Compuware's dynaTrace
i don't think that you can do full-featured profiling of distributed request across number of JVM's - AppDynamics from what i can remember understands the EE stuff - like calling DB, EJB, RMI, or remote webservice - however it still works in scope of JVM.
Isn't it suffient in your case just to use java profiler (like yourkit, jprofiler)?
you can try 24x7monitoring
https://code.google.com/p/monitor-24x7/
it provides method level monitoring, SQL queries, business transactions...
Did you try the free version of AppDynamics.
It's called AppDynamics LITE.
You can take a look also to EXTRAHOP free version.
Maybe it is good enough for your needs.
Also you can try using a SaaS solutions such as NewRelic or Boundary.
They have free accounts that could also be good enough for your needs.
Finally if you want to monitor the performance of any specific JAVA application, you can use  http://www.moskito.org/ .
It's totaly FREE.
can you tell me the methodology for deriving those stats?
In most cases, this behavior comes down to poor code implementation in a custom portlet or a JVM configuration issue and more likely the latter.
Take a look at the  Ehcache Garbage Collection Tuning Documentation .
When you run  jstat , how do the garbage collection times look?
I realize it's been a while, but I'll contribute anyway as it may help other people too.
In my case, importing Instrumentation in iOS caused this error; it seems to be a problem in the latest version of @appdynamics/react-native-agent (version 20.7.0 as of writing).
I instead initialized AppDynamics in native code (in the AppDelegate.m file), as follows:
For more info, check the iOS guide:
 https://docs.appdynamics.com/display/PRO45/Instrument+an+iOS+Application
Additionally, I avoided importing AppDynamics in javascript by requiring it in runtime, in Android only.
There's a lot of different questions here, and some of them don't have answers without more information about your specific setup, but I'll try to give you a good overview.
Why Tracing?
You've already intuited that there are a lot of similarities between &quot;APM&quot; and &quot;tracing&quot; - the differences are fairly minimal.
Distributed Tracing is a superset of capabilities marketed as APM (application performance monitoring) and RUM (real user monitoring), as it allows you to capture performance information about the work being done in your services to handle a single, logical request both at a per-service level, and at the level of an entire request (or transaction) from client to DB and back.
Trace data, like other forms of telemetry, can be aggregated and analyzed in different ways - for example, unsampled trace data can be used to generate RED (rate, error, duration) metrics for a given API endpoint or function call.
Conventionally, trace data is annotated (tagged) with properties about a request or the underlying infrastructure handling a request (things like a customer identifier, or the host name of the server handling a request, or the DB partition being accessed for a given query) that allows for powerful exploratory queries in a tool like Jaeger or a commercial tracing tool.
Sampling
The overall performance impact of generating traces varies.
In general, tracing libraries are designed to be fairly lightweight - although there are a lot of factors that influence this overhead, such as the amount of attributes on a span, the log events attached to it, and the request rate of a service.
Companies like Google will aggressively sample due to their scale, but to be honest, sampling is more beneficial to consider from a long-term storage perspective rather than an up-front overhead perspective.
While the additional overhead per-request to create a span and transmit it to your tracing backend might be small, the cost to store trace data over time can quickly become prohibitive.
In addition, most traces from most systems aren't terribly interesting.
This is why dynamic and tail-based sampling approaches have become more popular.
These systems move the sampling decision from an individual service layer to some external process, such as the  OpenTelemetry Collector , which can analyze an entire trace and determine if it should be sampled in or out based on user-defined criteria.
You could, for example, ensure that any trace where an error occurred is sampled in, while 'baseline' traces are sampled at a rate of 1%, in order to preserve important error information while giving you an idea of steady-state performance.
Proprietary APM vs. OSS
One important distinction between something like AppDynamics or New Relic and tools like Jaeger is that Jaeger does not rely on proprietary instrumentation agents in order to generate trace data.
Jaeger supports OpenTelemetry, allowing you to use open source tools like the  OpenTelemetry Java Automatic Instrumentation  libraries, which will automatically generate spans for many popular Java frameworks and libraries, such as Spring.
In addition, since OpenTelemetry is available in multiple languages with a shared data format and trace context format, you can guarantee that your traces will work properly in a polyglot environment (so, if you have Node.JS or Golang services in addition to your Java services, you could use OpenTelemetry for each language, and trace context propagation would work seamlessly between all of them).
Even more advantageous, though, is that your instrumentation is decoupled from a specific vendor or tool.
You can instrument your service with OpenTelemetry and then send data to one - or more - analysis tools, both commercial and open source.
This frees you from vendor lock-in, and allows you to select the best tool for the job.
If you'd like to learn more about OpenTelemetry, observability, and other topics I wrote a longer series that you can find  here  (look for the other 'OpenTelemetry 101' posts).
Windows 10 64-bit.
Powershell 5.
Does not require admin privileges.
How to quickly and efficiently extract text from a large logfile from input1 to input2 using powershell 5?
Sample logfile below.
For testing purposes copy your logfile to the desktop and name it logfile.txt
What is the default text editor in Windows Server 2012 R2 Standard 64-bit?
See line 42.
What program do you have associated with .txt files?
See line 42.
logfile.txt:
Results of running script on logfile.txt:
Powershell in four hours at Youtube
Parse and extract text with powershell at Bing
How to quickly and efficiently extract text from a large logfile from line x to line y using powershell 5?
How to quickly and efficiently extract text from a large logfile from date1 to date2 using powershell 5?
curl's  --user  command line option sets up HTTP authentication for the request ( username:password ).
In the case of AppDynamics' Configuration API (which is a subset of their Controller API),  user1@customer1:secret  are your account credentials in the format documented  here :
You should not be retrieving any embedded resources in your load test, you need to limit the scope of the embedded resources scanning to  your application under test domain only
Any external domains like  googleapis  or   appdynamics  must be excluded via  URLs must match  input (lives at "Advanced" tab of the  HTTP Request  sampler, or even better  HTTP Request Defaults )
More information:  Excluding Domains from the Load Test
The problem with this approach is you'll have to manually do that each time.
I would highly recommend just configuring your app server to automatically load the AppDynamics agent.
Another option is using the universal agent, which does auto-attach:  https://docs.appdynamics.com/display/PRO43/Install+the+Universal+Agent  Doing this one off attach is never really a good idea, as you'll have to get the PID each time.
The error indicates that you are probably not running the attach as the same user the JVM is running under, but it could also be permissions or something else as well, hence I would use the methods that work all the time :)
Use Dependency Injection instead of creating the actual WCF endpoints and passing them around.
Then mocking them up is trivial.
You would then use the interface and let DI take care of the rest!
The best way to logging the requests to an external logging provider.
Check out  http://docs.cloudfoundry.org/devguide/services/log-management-thirdparty-svc.html .
You can actually log to any http endpoint that supports POST.
You can use Splunk to calculate your response time and requests per second.
The logs that come out of that are in real time and are streamed to your logging endpoint.
It contains information about the requests as well as log messages from your app.
ex.
There are basic stats available when you run  cf app &lt;app-name&gt; .
These include the memory, cpu and disk utilization of your app.
You can also access these via the REST api, documented here.
https://s3.amazonaws.com/cc-api-docs/41397913/apps/get_detailed_stats_for_a_started_app.html
That's not going to help with requests per second or response time though.
@jsloyer's solution would work for that, or you could use an APM like NewRelic, which will give you a wealth of data for virtually nothing.
Yes this is pretty easy to use.
You can use Logstash as the ingesting engine, you just need the correct parser.
Check out  http://scottfrederick.cfapps.io/blog/2014/02/20/cloud-foundry-and-logstash  for the parser and config for ingesting cloud foundry logs.
I was playing around with this before and it worked quite well.
Let me know if you have any issues.
From checking out the code for Istio -  https://github.com/istio  - this appears to be an application with components written in Go and C++ which are deployed to containers using Kubernetes.
This would mean that for monitoring you should be looking at:
You have you work cut out for you here - both SDK's would require changes to the code being ran to get visibility.
It's hard to say what's wrong without seeing your Jenkins job and JMeter  Thread Group  configuration.
In order to apply external settings in JMeter you need to define threads, ramp-up and the number of iterations using JMeter Properties via  __P() function  like:
Once done you will be able to  override the values of these properties using  -J  command line argument , for example in Jenkins:
This way you will be able to pass whatever number of virtual users/iterations without having to change your script.
More information:  Apache JMeter Properties Customization Guide
just to make it clear, a  live connection  is (in the world of Power BI 😉) a connection to either a Power BI dataset or a SSAS tabular model.
I think what you are looking for is  DirectQuery , but it is currently not supported for URL GET commands.
MS docs - Power BI data sources
To get realtime data you need to use one of the supported sources.
maybe AppDynamics supports direct DB access.
DirectQuery is supported by SQL databases.
Another way is to offload the data to a supported source eg.
SQL-db or CDS-service and then connect your pbi to that source.
The correct and hard way is to amend the SELinux boolean on Tomcat directories to allow Tomcat amend files created by other users.
Read  here .
The easy and dirty solution is to  start-up process includes a step for renaming yesterdays AppDynamics logs  as user  sysXYZ  .
And that way you avoid the problem.
Use  su - sysXYZ &lt;script&gt;  command or  sudo -iu sysXYZ &lt;script&gt;  command or  sudo -t &lt;Tomcat role&gt; &lt;script&gt;
Good luck.
Add  CheckedParameter=false  in the connection string to fix the issue.
I found my answer on the Flow Force online help ( https://manual.altova.com/flowforceserver/flowforceserver/ )
The Flow Force is deployed as two servers, which in a window env, can be started and stopped as windows services (can be found via "Control Panel" "Administrative Tools" Services).
With this information, I can monitor them via NAGIOS.
One possibility is that you've got a cached execution plan which works fine for most parameter values, or combination of parameter values, but which fails badly for certain values/combinations.
You can try adding a non-filtering predicate such as  1 = 1  to your WHERE clause.
I've read  but haven't tested that this can be used to force a hard parse, but it may be that you need to change the value (e.g.
1 = 1 ,  2 = 2 ,  3 = 3 , etc) for each execution of your query.
Here's a recursive solution that yields key "paths".
The  (*keys, k)  syntax is available in Python versions  = 3.5, you can also use  keys + (k,)
As a little modifcation to @Jon Clements code, this is what gives me what I need.
Try this:-
Application Performance Management(APM)
In simpler terms:
APM monitors the speed at which transactions are performed both by
  end-users and by the systems and network infrastructure that support a
  software application, providing an end-to-end overview of potential
  bottlenecks and service interruptions.
In pragmatic terms, this typically involves the use of a suite of software tools—or a single integrated SaaS or on-premises tool—to view and diagnose an application’s speed, reliability, and other performance metrics in order to maintain an optimal level of service.
Here is
 Wiki description.
You don't need a machine agent extension (and additional custom metrics) to capture if a JVM goes down.
The machine agent delivers these kind of information out of the box.
You have to start the java process with the app server agent and associate the application, that is instrumented with the machine agent.
Now you can go to the  application dashboard -&gt; Events tab  and check for the  JVM Crash Event .
See also  here  and  here .
I'm not familiar with the AppDynamics output.
I assume that's a cumulative view of Threads and their sleep times.
So Threads get reused and so the sleep times add up.
In some cases, a Thread gets a connection directly, without any waiting and in another call the Thread has to wait until the connection can be provided.
The wait duration depends on when a connection becomes available, or the wait limit is hit.
Your screenshot shows a Thread, which waited  172ms .
Assuming the  sleep  is only called within the Jedis/Redis invocation path, the Thread waited  172ms  in total to get a connection.
Another Thread, which waited  530ms  looks to me as if the first attempt to get a connection wasn't successful (which explains the first  500ms ) and on a second attempt, it had to wait for  30ms .
It could also be that it waited 2x for  265ms .
Sidenote:
1000+ connections could severely limit scalability.
Spring Data Redis also supports other drivers which don't require pooling but work with fewer connections (see  Spring Data Redis Connectors  and  here ).
You are getting this error as you have not provided the  values required by app to work.
You need to add the values in  PreferenceConstants  .
Did you get it working?
Actually, you know.
You are trying to connect to server and app key is the one generated at server and has to be implemented at end application.
Since this is a template, there is no server allocated by the code writer to test and so you don't have app key.
Try exploring IoT frameworks by IBM any other or even you can try open source, Kaa.
It will be worth if want explore this kind of cloud dependent apps.
This sounds like you open a transaction and get a database connection always and only after check cache content.
But since you have opened the transactional context, it will be closed, issuing the commit, as no exceptions happened.
You most likely need to move the checking of the cache outside of the transactional context.
Of course, confirming this depends on your application code not included in the question.
The transaction demarcation related invokes on the  Connection  will happen nevertheless.
You could use something like Spring's  LazyConnectionDataSourceProxy  (doc'ed  here ) to avoid having these sent when they are not required.
The  asadmin  manual page says the following:
For the Windows operating system in single mode, a backslash is
  required to escape the colon and the backslash characters.
So try the following:
See also:
In theory, yes: if Resource Manager has been enabled it could be the case that different Resource Manager plans have such an impact but experience shows that this feature is seldom used.
In practive this kind of difference can have many cause:-
The first thing to look at database level is something similar to Statspack report (or AWR if licensing allows) to compare database configuration and activity.
And don't forget that application performance is not only database performance it depends also on application server, network and front-end.
I think this should do what you want.
I'm afraid it's not tested, but the principle is this:
1) if the Set-Cookie header starts with ADRUM, then set the env variable ADRUM_cookie.
2) if the ADRUM_cookie env variable is /not/ set, edit the Set-Cookie header.
If we are talking about "run tool-get result" the best option -  Java Mission Control .
It's free in test environment.
You need to pay only for some features in production.
It's much better than old VisualVM.
You can write a data to file using  Flight Recorder .
You can setup start point and duration.
You just need to start your application like this:
-XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:StartFlightRecording=duration=60s,filename=myrecording.jfr
I think what you are wanting is info around instrumentation of the front end of an Angular SPA.
Please see documentation here:  https://docs.appdynamics.com/display/PRO21/Monitor+Single-Page+Applications  - Angular is mentioned as being available using SPA2.
(Note for the front end we use Javascript BRUM for monitoring, general docs are here:  https://docs.appdynamics.com/display/PRO21/Browser+Monitoring  - this is a seperate part of the product than the APM used to monitor backends)
Since some packages are not working with different OS configurations, you need to setup a new build agent.
Deploy an agent on Windows
It's possible to solve this by to changing the query by using zero interpolation.
You can put ".fill(zero)" behind your query in the json or choose the option from the UI.
EDIT:
You're right, the interpolation is not working when no data is available.
I had the same problem in the end.
Support of Datadog said it isn't possible to show zero when there is no data for a metric.
Now there is a feature request made for it.
It would be nice if more people will request for this feature, so it will be prioritized.
Nonetheless I have tried to create a workaround by adding a second metric that always has data as a second query and added a formula ((b - b) + a) that negates the second query, but when there is data in the intended query it's show in the graph.
This will result in a zero line when there is no data available.
Scenario without data:
The only problem is that when you have a data in the intended query, it looks ugly and the zero line is gone.
As you see in the following screenshot.
Scenario with data:
Conclusion: 
The workaround is not perfect, but it will work for some situation.
For example, filling up query values with zero instead of (no data).
I hope this is a bit better answer to the problem.
how about  the "default" function ?
so  default(sum:foo.bar{hello:world} by {baz}, 0)  or some such?
There is now a  default_zero()  function that can be used in Datadog by modifying through JSON directly.
The  default_zero()  function does what you're looking for.
You can type it in manually, as  stephenlechner suggests .
There's another way I found:
When you save the graph and reopen, you'll see that things have been reshuffled a little, and you'll see a "default 0" section tagged onto the end of the metric's definition.
Metrics queries now support wildcards.
Example 1: Getting all the requests with a status tag starting with  2 :
 http.server.requests.count{status:2*}
Example 1: Getting all the requests with a service tag ending with  mongo :
 http.server.requests.count{service:*mongo}
Example 3 (advanced): Getting all the requests with a service tag starting with  blob  and ending with  postgres :
 http.server.requests.count{service:blob*,service:*postgres} 
 (this will match  service:blob-foo-postgres  and  service:blob_bar_postgres  but not  service:my_name_postgres )
I've finally found a dropwizzard module that integrates this library with datadog:  metrics-datadog
I've created a Spring configuration class that creates and initializes this Reporter using properties of my YAML.
Just insert this dependency in your pom:
Add this configuration to your YAML:
and add this configuration class to your project:
If JMX is an option for you, you may use the  JMX dropwizrd reporter  combined with  java datalog integration
It seems that Spring Boot 2.x added several monitoring system into its metrics.
DataDog is one of them supported by  micrometer.io .
See reference documentation:  https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-newrelic
For Spring Boot 1.x, you can use back-ported package:
compile 'io.micrometer:micrometer-spring-legacy:latest.release'
Confirmed on IRC (#datadog on freenode) that:
Datadog doesn't support multiple Y-axis at this time.
If the two axis have the same units but different degrees (10 vs 10 million), then using a non-linear scale such as  log  might provide what you need:
https://help.datadoghq.com/hc/en-us/articles/203038729-Is-it-possible-to-adjust-the-y-axis-for-my-graphs-
They do allow dual y-axis now
https://docs.datadoghq.com/dashboards/widgets/timeseries/#y-axis-controls
Introducing dual Y-axis for time series widgets.
The time series widget on dashboards now support dual y-axis, making it easier than ever to compare two sets of data on a single graph.
By removing the need to create separate graphs, your dashboards can show even more valuable information viewable at a glance.
If  dd-agent  listens on  localhost  it can receive data only from localhost (127.0.0.1).
Try to change the  dd-agent  host to  0.0.0.0  instead of  localhost .
We are using  docker-dd-agent  and it works OOTB.
You will need to set  non_local_traffic: yes  in your  /etc/dd-agent/datadog.conf  file.
Otherwise the agent will reject metrics from containers.
After setting, you will need to restart the agent for the change to take effect:  sudo /etc/init.d/datadog-agent restart  or  sudo service datadog-agent restart
The  docker-dd-agent  image enables  non_local_traffic: yes  by default.
You don't actually want to use the IP of the host in this case.
If you're running the docker dd-agent, there are two environment variables you can tap into:
statsd.connect(DOGSTATSD_PORT_8125_UDP_ADDR, DOGSTATSD_PORT_8125_UDP_PORT)
That should do the trick.
If not, you should be able to find the relevant info to your problem in  this section of the Datadog docs .
Also, I should point out that the only Python library that Datadog shows in their docs is  datadogpy .
I asked DataDog support, and apparently as of January 2020 this is not possible, but is a feature request in their backlog.
I know this is not a great answer to the question but if I hear that this changes, I will update my answer.
There doesn't appear to be a Crashalytics direct integration yet.
Datadog maintains mobile SDKs that can be included in mobile clients to produce metrics and logs to the platform.
For iOS Logs, see here:  https://docs.datadoghq.com/logs/log_collection/ios/
For Android Logs:  https://docs.datadoghq.com/logs/log_collection/android/
There's also a public Android SDK for Real User Monitoring, which can be read here:  https://docs.datadoghq.com/real_user_monitoring/android/
And the announcement, with a link for the private beta for iOS signup here:  https://www.datadoghq.com/blog/datadog-mobile-rum/
After speaking to the support team at DataDog, I managed to find out the following information relating to what the no_pod pods were.
Our Kubernetes check is getting the list of containers from the Kubernetes API, which exposes aggregated data.
In the metric explorer configuration here, you can see a couple of containers named /docker and / that are getting picked up along with the other containers.
Metrics with pod_name:no_pod that come from container_name:/ and container_name:/docker  are just metrics aggregated across multiple containers.
(So it makes sense that these are the highest values in your graphs.)
If you don't want your graphs to show these aggregated container metrics though, you can clone the dashboard and then exclude these pods from the query.
To do so, on the cloned dashboard, just edit the query in the JSON tab, and in the tag scope, add !pod_name:no_pod.
So it appears that these pods are the docker and root level containers running outside of the cluster and will always display unless you want to filter them out specifically which I now do.
Many thanks to the support guys at DataDog for looking into the issue for me and giving me a great explanation as to what the pods were and essentially confirming that I can just safely filter these out and not worry about them.
The free text editor you have in the screenshot is for metric queries.
Events in graphs are added as overlay to show when events happened over time.
There is no widget, as of now, that shows a single value for the number of times an event occurred.
But you can use the event timeline widget, which will show a timeline of events grouped by status and bucketed over a defined period of time.
See below:
Well I realized that the  query value  only works with metrics, so to create a counter we can emit metrics with  value: 1  and then count them with the  rollup(sum, 60)  function.
dog.emit_point('some.event.name', 1)
sum:some.event.name{*}.rollup(sum, 60)
The main thing to understand here is that DataDog does not retrieve all the points for a given timeframe.
Actually as  McCloud  says,  for a given time range we do not return more than 350 points , which is very important to have in mind when you create a counter.
When you query value from a metric in a timeframe, DataDog return a group of points that represents the real stored points, not all the points; the level of how those points are represented (as I understand) is called granurallity, and what you do with this  rollup  function is to define how those points are going to represent the real points, which in this case is going to be using the  sum  function.
I hope this helps somebody, I'm still learning about it.
Regards
In Ruby on the client, I use:
I then have a dashboard with a
 Query Value 
widget, set to &quot;take the [Sum] value from the displayed timeframe&quot;.
I tested this and it seems to give the right numbers.
The
 .as_count() 
seems key.
Answering just in case someone will spot this question via Google.
You cannot.
StatsD protocol do not define tags or comments at all, so there is no possibility for that.
You need to use different library like  Statix  for that.
Yes it is possible to emit metrics to DataDog from a AWS Lambda function.
If you were using node.js you could use  https://www.npmjs.com/package/datadog-metrics  to emit metrics to the API.
It supports counters, gauges and histograms.
You just need to pass in your app/api key as environment variables.
Matt
The easier way is using this library:  https://github.com/marceloboeira/aws-lambda-datadog
It has runtime no dependencies, doesn't require authentication and reports everything to cloud-watch too.
You can read more about it here:  https://www.datadoghq.com/blog/how-to-monitor-lambda-functions/
In most cases, the datadog agent retrieves metrics from an integration by connecting to a URL endpoint.
This would be the case for services such as nginx, mysql etc.
This means that you can run just one datadog agent on the host, and configure it to listen to URL endpoints of services exposed from each container.
For example, assuming a mysql docker container is run with the following command:
You can instruct the agent running on the host to connect to the container IP in the  mysql.yaml  agent configuration:
Varnish is slightly different as the agent retrieves metrics using the  varnishstat  binary.
According to the example template:
In order to support monitoring a Varnish instance which is running as a Docker container we need to wrap commands (varnishstat) with scripts which perform a docker exec on the running container.
To do this, on the host, create a wrapper script for the container:
Then specify the script location in the  varnish.yaml  agent configuration:
There are 2 relevant options in  /etc/dd-agent/conf.d/docker_daemon.yaml :
collect_disk_stats 
If you use devicemapper-backed storage (which is default in ECS but not in vanilla Docker or Kubernetes), docker.data.
* and docker.metadata.
* statistics should do what you are looking for.
collect_container_size 
A generic way, using the docker API but virtually running df in every container.
This enables the docker.container.
* metrics.
See more here:
 https://help.datadoghq.com/hc/en-us/articles/115001786703-How-to-report-host-disk-metrics-when-dd-agent-runs-in-a-docker-container-
and here:
 https://github.com/DataDog/docker-dd-agent/blob/master/conf.d/docker_daemon.yaml#L46
In your question, I looked for your purpose in terms of using the port 8125 or 8126 ports.
8125 port is used for stasd metrics, and 8126 is used for APM (trace) data.
So if you want to use 8125 the important thing to do is having  non_local_traffic : yes .
So there must be another problem which I don't know yet.
But if your purpose is using APM/trace port: 8126 is only bound to localhost by default.
You should make it listen to any network interface by the  bind_host: 0.0.0.0  configuration.
Currently, it will refuse the requests from your containers since they are not coming from localhost.
I had a similar problem and this page helped me:  https://github.com/DataDog/ansible-datadog/issues/149
UPDATED ANSWER:
Still yes.
Docs for new Dashboard endpoint  here .
ORIGINAL ANSWER:
Yes.
Docs for screenboards  here .
Docs for timeboards  here .
I doubt the Datadog agent will ever work on App Services web app as you do not have access to the running host, it was designed for VMs.
Have you tried this  https://www.datadoghq.com/blog/azure-monitoring-enhancements/  ?
They say they support AppServices
To respond to your comment on wanting custom metrics, this is still possible without the agent at the same location.
After installing the nuget package of datadog called statsdclient you can then configure it to send the custom metrics to an agent located elsewhere.
Example below:
I have written a app service extension for sending Datadog APM metrics with .NET core and provided instructions for how to set it up here:  https://github.com/payscale/datadog-app-service-extension
Let me know if you have any questions or if this doesn't apply to your situation.
Logs from App Services can also be sent to Blob storage and forwarded from there via an Azure Function.
Unlike traces and custom metrics from App Services, this does not require a VM running the agent.
Docs and code for the Function are available here:
https://github.com/DataDog/datadog-serverless-functions/tree/master/azure/blobs_logs_monitoring
If you want to use DataDog for logging from Azure Function of App Service you can use Serilog and DataDog Sink to the log files:
Full source code and required NuGet packages  are here:
You can deploy the Datadog agent in a container / instance that you manage and the configure it according to  these instructions  to gather metrics from the remote ElasticSearch cluster that is hosted on Elastic Cloud.
You need to create a  conf.yaml  file in the  elastic.d/  directory and provide the required information (Elasticsearch endpoint/URL, username, password, port, etc) for the agent to be able to connect to the cluster.
You may find a sample configuration file  here .
As George Tseres mentioned above, the way I had to get this working was to set up collection on a separate instance (through docker) and then to configure it to read the specific Elastic Cloud instances.
I ended up making this:  https://github.com/crwang/datadog-elasticsearch , building that docker image, and then pushing it up to AWS ECR.
Then, I spun up a Fargate service / task to run the container.
I also set it to run locally with  docker-compose  as a test.
These system.io metrics are reported from a  system agent check  that uses  iostat  under the hood.
According to the  iostat manpage  one of the metrics  %util  (reported as  system.io.util  within Datadog) seems to do the job:
%util: Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device).
Device saturation occurs when this value is close to 100%.
You can create a monitor, as a multi alert on host/device, when this metric is over 90 on the last 30 minutes on average, here is a current screenshot of such an example:
Of course one can also monitor other iostat metrics to identify other I/O performance failure modes.
This can be achieved in two ways.
If you only want this logic to applied on this one graph, you can divide metric either using the UI editor and clicking advanced or using the JSON editor:
UI Editor:  http://cl.ly/1c0K2O3P1E2K
Or JSON editor:  https://help.datadoghq.com/hc/en-us/articles/203764925-How-do-I-use-arithmetic-for-my-time-series-data-
Alternatively, you can use the Metric Summary page to edit this metric's metadata and alter this metric's unit throughout the application as seen here:  http://cl.ly/2x0Z290w2I3V
https://app.datadoghq.com/metric/summary
Hope this helps.
Also, you can reach out to support@datadoghq.com if you run into any other issues in the future.
So, while trying to debug this I deleted the deployment + dameonset and service and recreated it.
Afterwards it worked....
Have you seen the  Discovering Services  docs?
I'd recommend using DNS for service discovery rather than environment variables since environment variables require services to come up in a particular order.
There are two error in your code:
Once done, your code will run flawlessly.
So from that log line, it appears as though  this  try  is excepting  in the library's  hostname.py .
So either...
(A) The  hostname line  is where it's excepting, and (weirdly) the
library requires that a  hostname  option be set in your
 datadog.conf  file.
Maybe worth setting a hostname there if you
haven't already.
Or,
(B) The  get_config() line  is where it's excepting, and so the
library isn't able to correctly identify the configuration file
location (or access it, possibly related to permissions).
Based on
the directory structure in your question, I think you're working on
an OSX / mac environment, which means the library will be using the
function  _mac_config_path()  in  config.py  to try to identify the
configuration path, which from  this line in the function  would
make it  seem  as though the library were looking for the
configuration file in  ~/.datadog-agent/agent/datadog.conf  instead
of the appropriate  ~/.datadog-agent/datadog.conf .
Which might be a
legitimate bug...
So if I were you, and if all this seemed right, I'd try adding a  hostname in the  datadog.conf  to see if that helped, and if it didn't, then I'd try making a  ~/.datadog-agent/agent/  directory and copying your  datadog.conf  file there as well, just to see if that got things working.
This answer assumes you are working in an OSX / mac environment, and will likely not be correct otherwise.
If either (A) or (B) are the case, then that's a problem with the library and should be updated--it would be kind of you to open an issue on  the library itself  to bring this up so the Datadog team that supports that library can be made aware.
I suspect not many people end up this library in OSX / mac environments to begin with, so that could explain all this.
You installed the Datadog agent &amp; trace agent on a Mac, listening on localhost.
You installed the flask application and ddtrace library in a docker container on a linux vm sending traffic to localhost.
Those two localhosts are describing two different machines.
The easiest option is going to be running both the Agent &amp; flask app on the Mac, or running both in docker.
The latter is most similar to an eventual production deployment.
Do that.
According to the documentation, this can be achieved using following properties in telegraf.conf:
https://docs.influxdata.com/telegraf/v1.12/administration/configuration/#measurement-filtering
where  namepass  defines pattern list of points which will be emitted.
Datadog has two agents.
And yes for DogStatsD the node agents need to be deployed as Daemonset.
Here is the the deployment manifest for  cluster agent  and  node agent .
Yes, the following should work:
tag_one:(A OR B)
Unfortunately the query syntax is slightly different in different contexts, I find, so I don't know if that will solve your particular problem.
If you want to remove the warning, you can try adding  none  and  shm  to the  excluded_filesystems  in disk.yaml.
This file should exist or be created in the Agent's conf.d directory.
Otherwise, you'll find more options  here .
If you are looking to exclude the logs from the agent within the platform you can look at excluding the agent ( doc )
Found the answer here:  https://github.com/DataDog/datadog-agent/issues/3329
The field is  mount_point_blacklist
In Datadog Logs, there's a difference between the Tags associated with the execution environment, and Attributes set on Log entry content.
From  this section in the docs :
Context  refers to the infrastructure and application context in which the log has been generated.
Information is gathered from tags—whether automatically attached (host name, container name, log file name, serverless function name, etc.
)—or added through custom tags (team in charge, environment, application version, etc.)
on the log by the Datadog Agent or Log Forwarder.
And looking into the  source for the browser SDK , we can see:
This shows us that the  tags  query string parameter being submitted is being calculated based on configuration, and only provides a small amount of user-configurable entries, like  env ,  service  - these were released very recently in version 1.11.5 -  here's the change  introducing them.
So you may not be able to set  tags  for a specific log entry - rather you can set  attributes  per log entry, like in the example you shared, which is setting  Attributes  for the logger instance as a whole.
Attributes are part of the log  Content  - which will be viewable in the body of the log entry.
Yes, this is confusing since the function used is named  addContext / setContext  - and these don't set the same thing as the documentation's "Context" - rather they modify the Attributes that are associated with the log entry.
In that event, you may want to have either custom logger instances that provide specific attributes for that logger, or add context inline to the log entry, like this:
Here's the docs  on this approach which show what other default attributes are being set per log entry.
The Monitor section of Datadog now includes a "Monitor status" page for each of the monitor you define (for example the URL monitoring you already have).
On this page, you can see by group/scope the monitor history and it shows you the uptime for that monitor.
More to read about this new feature  here
It's not yet available as a "report" but that's a good idea!
The exception you are seeing is related to the IO system metrics collection and has nothing to do with your custom dogstream parser.
If you look at the stack trace it says that it wasn't able to apply the  _parse_linux2  function.
To troubleshoot that further you should take a look at the output of
which is the command launched by the agent.
Feel free to open a bug on the agent GitHub repository.
References:
Just a few items to note to get this working:

dogstatsd = Statsd.new('MY_API_KEY')
This line of code is trying to use your API key to establish a statsD connection, but this should actually be trying to establish the statsD connection via the statsD port currently configured on your Agent as seen here:

Create a stats instance.
statsd = Statsd.new('localhost', 8125)
The easiest way to get your custom metrics into Datadog is to send them to DogStatsD, a metrics aggregation server bundled with the Datadog Agent (in versions 3.0 and above).
DogStatsD implements the StatsD protocol, along with a few extensions for special Datadog features.
http://docs.datadoghq.com/guides/dogstatsd/
If you would not like to deploy an Agent on the host running the RoR application, you can utilize DogAPI gem:
https://github.com/DataDog/dogapi-rb
Which has additional documentation to get this custom metrics submitted:
If you have additional questions, please reach out to support@datadoghq.com
This Datadog blog should guide you on how to build a monitor.
https://www.datadoghq.com/blog/monitoring-cassandra-with-datadog/
There is indeed a  template Cassandra dashboard  in Datadog (where I work) that should appear as soon as you enable the integration.
This dash has a mix of Cassandra-specific metrics (e.g.
cache hit rates), plus metrics from the host (e.g.
CPU).
You can select a particular host or subset of hosts to make those host-level metrics more meaningful by  changing the scope of the dashboard , and the graphs will re-render on the fly.
You can also clone and modify the dashboard as you wish by clicking the gear icon in the upper right.
This dash should provide a good starting point for monitoring Cassandra, but we have an even better template dashboard in the works.
I'll update this answer as soon as it's released.
In the meantime, the blog post shared by John KVS should help you to identify key metrics that you might want to monitor.
Duplicate the definition of  event.sent  in  event.failed .
As soon as you restart the agent, any  sent  event will be seen as  sent   and   failed .
After a minute or so you revert the definition of  failed  to the proper definition, but you now have a few (admittedly bogus) failed event in datadog, allowing you to use the metric.
On your datadog dashboard, edit the relevant metric, and instead of using the graphical interface (tab  edit ) got to the  JSON  tab, where you can manually enter your metric name (probably a slightly updated cut &amp; paste of your  sent  metric), no matter if an event exists or not.
Datadog monitors evaluate every minute, I think.
So in your  sum(last_30m){X}  example, every minute, the monitor would sum the values of  {X}  over the last 30 minutes, and if that value was above whatever threshold you set, then it would trigger an alert.
Same thing for  sum(last_1h){X} , but every minute it would evaluate the sum over the last 1 hour.
The first step will be to make sure you have the  datadog agent running , and that the APM component of it is running and ready to receive trace data from your applications ( this option in your datadog.conf , which must be set to "true").
Second, you'll want to install the appropriate library(ies) for the languages your applications are written in.
You can find them all listed in your datadog account on this page:  https://app.datadoghq.com/apm/docs
Third, once the trace libraries are installed, you'll want to add trace integrations for the tools you're interested in collecting APM data on, and the instructions for those will be found in each library's docs.
(E.g,  Python ,  Ruby , and  Go )
The integraitons will be a fairly quick way to get pretty granular spans on where your applications have higher latency, errors, etc.
If from there you'd like to go even further, each library's docs also have instructions on how you can write your own custom tracing functions to expose more info on your custom applications--that's a little more work, but is fairly straight-forward.
You'll probably want to add those bit-by-bit as you go.
Then you'd be all set, I think.
You'll be tracing services, resources to get the latency, request-count, and error-count of your application requests, and you can drill down to the flame-graphs to further understand what requests spend the most time where in your applications.
Looking back now, seems like they made some recent changes to the setup process that makes it even easier to get the web framework and database integrations added if you're using Python.
They've even got a command line tool in their  get-started section now .
Hope this helps!
And reach out to their support team (support@datadoghq.com) if you run into issues along the way--they're always happy to lend a hand.
For now I see only two possibilities:
Use GCP custom metrics
 https://cloud.google.com/monitoring/custom-metrics/creating-metrics 
and datadog integration with GCP
 https://www.datadoghq.com/product/integrations/#cat-google-cloud
Use datadog statsd client, java one -
 https://github.com/DataDog/java-dogstatsd-client  so you can deploy
datadog agent on GCP and connect through it.
Sample with kubernetes.
https://docs.datadoghq.com/tracing/setup/kubernetes/#deploy-agent-daemonset
datadog deployment.yaml for kubernetes
Currently I'm investigating this so I'm not sure how to do this, yet.
It has support to CURL means you can make REST API calls.
Try using some Http libraries like  HttpURLConnection  in java to make those POST requests.
I don't think we need a java based SDK for that as you can frame your own SDK on top of REST api's.
There are two ways to access  dd-trace agent  on host from a container:
1.
Only on  &lt;HOST_IP&gt;:8126 , if docker container is started in a bridge network:
dd-trace agent  should be bound to  &lt;HOST_IP&gt;  or  0.0.0.0  (which includes  &lt;HOST_IP&gt; ).
2.
On  &lt;HOST_IP&gt;:8126  (if  dd-trace agent  is bound to  &lt;HOST_IP&gt;  or  0.0.0.0 ) and  localhost:8126 , if docker container is started in the host network:
As you already try to reach  dd-trace agent  on  localhost:8126 , so the second way is the best solution.
First step is to install the DataDog agent on the server in which you are running your application:
https://docs.datadoghq.com/agent/
You then need to enable the  DogStatsD  service in the DataDog agent:
https://docs.datadoghq.com/developers/dogstatsd/
After that, you can send metrics to the  statsd  agent using any Go library that connects to  statsd .
For example:
https://github.com/DataDog/datadog-go
https://github.com/go-kit/kit/tree/master/metrics/statsd
Here's an example program sending some counts using the first library:
Here is a convenience wrapper for DD.
It uses ENV vars to configure it, but it's a nice utility to package out common DD calls once you have the agent running in the background.
I figured out how to do this using the datadog api  https://docs.datadoghq.com/api/?lang=python#post-timeseries-points .
The following python script takes in the jtl file (jmeter results) and posts the transaction name, response time, and status (pass/fail) to datadog.
You can actually just use:
Datadog's monitor templating feature will fill in the blank and forward to the relevant channel as long as you whitelisted it in the integrations tile for Slack.
Starts to get messy, but you could nest two "does not" conditional variables, like this:
AFAIK it is not possible at the Moment to use micrometer to send events to datadog.
Micrometer states that  "Micrometer is not a distributed tracing system or an event logger. "
on its section "1.
Purpose" on its  concepts page .
This doc will show you a comprehensive list  of all integrations that involve log collection.
Some of these include other common log shippers, which can also be used to forward logs to Datadog.
Among these you'd find...
That said, you  can still just use the Datadog agent to collect logs only  (they want you to collect everything with their agent, that's why they warn you against collecting just their logs).
If you want to collect logs from docker containers, the Datadog agent is an easy way to do that, and it has the benefit of adding lots of relevant docker-metadata as tags to your logs.
( Docker log collection instructions here .)
If you don't want to do that, I'd look at Fluentd first on the list above -- it has a good reputation for containerized log collection, promotes JSON log formatting (for easier processing), and scales reasonably well.
You need to tell Datadog to pull custom metric.
Go to AWS integrations configuration page (Integrations side menu -  Integrations -  Amazon Web Services).
You will see a list of services to integrate with, custom metrics is the last option on list.
Make sure it's ticked.
Takes a while for Datadog to actually start pulling the metric.
Do you have the ability to add some parameters in the logs sent.
From  the documentation  you should be able to inject the trace id into your logs in a way that Datadog will interpret them.
You can also look at a parser to extract the trace id and span id from the raw log.
This documentation  should help you out on that.
From the documentation, if you don't have JSON logs, you need to include  dd.trace_id  and  dd.span_id  in your formatter:
If your logs are raw formatted, update your formatter to include
   dd.trace_id  and  dd.span_id  in your logger configuration:
So if you add  %X{dd.trace_id:-0} %X{ dd.span_id:-0} , it should work.
A better way would be to use a sidecar container with a logging agent, it won't increase the load on the API server.
Reference:  https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent
Datadog agent looks like doesn't support /suggest running as a sidecar ( https://github.com/DataDog/datadog-agent/issues/2203#issuecomment-416180642 )
I suggest looking at using other logging agent and pointing the backend to datadog.
Some options are:
Datadog supports them
Is that sample text formatted properly?
The final entity object is missing a  ]  from the end.
entity=[HttpEntity.Strict application/json {"type":"text","extract": "text", "field2":"text2","duration": 451 }
should be
entity=[HttpEntity.Strict application/json {"type":"text","extract": "text", "field2":"text2","duration": 451 }]
I'm going to continue these instructions assuming that was a typo and the entity field actually ends with  ] .
If it doesn't, I think you need to fix the underlying log to be formatted properly and close out the bracket.
Instead of just skipping the entire log and only parsing out that json bit, I decided to parse the entire thing and show what would look good as a final result.
So the first thing we need to do is pull out that set of key/value pairs after the request object:
Example Input:  thread-191555 app.main - [cid: 2cacd6f9-546d-41ew-a7ce-d5d41b39eb8f, uid: e6ffc3b0-2f39-44f7-85b6-1abf5f9ad970] Request: protocol=[HTTP/1.0] method=[POST] path=[/metrics] headers=[Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache] entity=[HttpEntity.Strict application/json {"type":"text","extract": "text", "field2":"text2","duration": 451 }]
Grok parser rule:  app_log thread-%{integer:thread} %{notSpace:file} - \[%{data::keyvalue(": ")}\] Request: %{data:request:keyvalue("=","","[]")}
Result:
Notice how we use the keyvalue parser with a quoting string of  [] , that allows us to easily pull out everything from the request object.
Now the goal is to pull out the details from that entity field inside of the request object.
With Grok parsers you can specify a specific attribute to parse further.
So in that same pipeline we'll add another grok parser processor, right after our first
And then configure the advanced options section to run on  request.entity , since that is what we called the attribute
Example Input:  HttpEntity.Strict application/json {"type":"text","extract": "text", "field2":"text2","duration": 451 }
Grok Parser Rule:  entity_rule %{notSpace:request.entity.class} %{notSpace:request.entity.media_type} %{data:request.entity.json:json}
Result:
Now when we look at the final parsed log it has everything we need broken out:
Also just because it was really simple, I threw in a third grok processor for the headers chunk as well (the advanced settings are set to parse from  request.headers ):
Example Input:  Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache
Grok Parser Rule:  headers_rule %{data:request.headers:keyvalue(": ", "/)(; :")}
Result:
The only tricky bit here is that I had to define a characterWhiteList of  /)(; : .
Mostly to handle all those special characters are in the  User-Agent  field.
References :
Just the documentation and some guess &amp; checking in my personal Datadog account.
https://docs.datadoghq.com/logs/processing/parsing/?tab=matcher#key-value-or-logfmt
When installing Datadog in your K8s Cluster, you install a  Node Logging Agent  as a Daemonset with various volume mounts on the hosting nodes.
Among other things, this gives Datadog access to the Pod logs at /var/log/pods and the container logs at /var/lib/docker/containers.
Kubernetes and the underlying Docker engine will only include output from stdout and stderror in those two locations (see  here  for more information).
Everything that is written by containers to log files residing inside the containers, will be invisible to K8s, unless more configuration is applied to extract that data, e.g.
by applying the  side care container pattern .
So, to get things working in your setup,  configure logback to log to stdout rather than /var/app/logs/myapp.log
Also, if you don't use APM there is no need to instrument your code with the datadog.jar and do all that tracing setup (setting up ports etc).
You need to tell Datadog that you're interested in that content by creating a facet from the field.
Click a log message, mouse over the attribute name, click the gear on the left, then Create facet for @...
For logs indexed after you create the facet, you can search with  @fieldName:text* , where  fieldName  is the name of your field.
You'll need to re-hydrate (reprocess) earlier logs to make them searchable.
You won't need to create a facet if you use fields from the  standard attributes list .
The error message itself is not a good fit to be defined as a facet.
If you are using JSON and want the main message (say from a  msg  json field) to be searchable in the Datadog  content  field.
Instead of making
facet for  msg , you can define a &quot;Message Remapper&quot; in the log configuration to map it to the  Content .
And then you can do wildcard searches.
log config screenshot
After reading this documentation,  https://docs.datadoghq.com/tracing/setup_overview/setup/ruby/#resque , did you try making the options a hash with curly braces surrounding?
Options is specified as being a hash.
So everything after  c.use :resque,   should be a  hash .
You can use the Agent's info command to see if the check is reporting correctly:
https://help.datadoghq.com/hc/en-us/articles/203764635
If the Agent Status shows the check is reporting correctly but your metrics are still not reporting you can contact support@datadoghq.com and they can troubleshoot this issue further.
Hope this helps!
This is not correct graph to detect correct resource limit.
You graph shows CPU usage of your app in the cluster, but resource limit is per pod (container).
We (and you as well) don't know from the graph how many containers were up and running.
You can determinate right CPU limit from the container CPU usage graph(s).
You will need Datadog-Docker integration:
Please be aware that Kubernetes relies on Heapster to report metrics,
  rather than the cgroup file directly.
The collection interval for
  Heapster is unknown which can lead to innacurate time-related data,
  such as CPU usage.
If you require more precise metrics, we recommend
  using the Datadog-Docker Integration.
Then it will depends how Datadog measure CPU utilization per container.
If container CPU utilization has max 100%, then 100% CPU container utilization ~ 1000m ~ 1.
I recommend you to read how and when cgroup limits CPU -  https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html
You will need a deep knowledge to set proper CPU limits.
If you don't need to prioritize any container, then IMHO the best practice is to set 1 ( resources.requests.cpu ) for all your containers - they will have always equal CPU times.
I am not familiar with the products and libraries that you are using, but there is an open source library  MgntUtils  that can extract full or filtered stacktrace from exception as a String.
Since you mentioned that you can pass the text (i.e.
String) this library may help you.
Here are the links to  MgntUtils  library:
I hope this helps
You need to pass  Content-Type  as a header with the request, as shown  in the docs
Response:
Your data is also not formatted according to the docs (there should be no  dash  field at the top level, for starters).
The format you're using to send the data does not comply with the  documentation  and your call fails to complete.
The call would work if you change your  $data  to:
Agreed that the error returned by the API is not really helpful, filed an issue in Datadog to correct that  behavior and return the proper error code and not a 500 (it's actually a 500 and you can see it by printing  curl_getinfo($ch)  after you executed your curl session).
Why not use dogstatsd instead of threadstats?
If you're already running the dd-agent on your node in a way that's reachable by your containers, you can use the  datadog.statsd.increment()  method instead to send the metric over statsd to the agent, and from there it would get forwarded to your datadog account.
Dogstatsd has the benefits of being more straight-forward and of being somewhat easier to troublehsoot, at least with debug-level logging.
Threadstats sometimes has the benefit of not requiring a dd-agent, but it does very little (if any) error logging, so makes it difficult to troubleshoot cases like these.
If you went the dogstatsd route, you'd use the following code:
And from there you'd find your metric metadata with the "rate" type and with an interval of "10", and you could use the "as_count" function to translate the values to counts.
In the python script I was initializing with an api key:
And sending some events
When I changed it to initialize like this, it started working with the dd-agent:
I didn't need the link command (--link dogstatsd:dogstastd).
With that setup it now works in the staging environment, but not in production.
:/
Maybe the  Datadog events-post api endpoint ?
Or their  metrics-post endpoint ?
Although, it's not altogether clear to me what is meant by "send this record only to Datadog".
If you use the log setup for Lambda log parsing that is described in  Datadog's AWS Lambda integration tile , that should be another perfectly good way to get the data collected into Datadog, but I think it may only collect those logs as metrics instead of events.
I found the answer thanks to  @tqr_aupa_atleti  and the support team from Datadog.
On the Datadog dashboard panel, I had to click Metrics -  Summary and look for my metric.
I looked at the tags and I could figure out it was a custom metric form my company that uses data from Amplitude.
However if you look at this metric carefully it appears to be
  calculating these percentiles on a short range (not sure what) and for
  each tuple of the tags that exist.
The short range that you have noticed is actually the flush interval which defaults to 10 seconds.
As per  this  article on histogram metric by datadog,
It aggregates the values that are sent during the flush interval
  (usually defaults to 10 seconds).
So if you send 20 values for a
  metric during the flush interval, it'll give you the aggregation of
  those values for the flush interval
For your query -
Ideally what I'd like to get is a 95th percentile of the ResponseTime
  metric over all the tags (maybe I filter it down by 1 or 2 and have a
  couple of different graphs) but over the last week or so.
Is there an
  easy way to do this?
as per my reading of the datadog docs, there isn't a way to get this done at the moment.
It might be a good idea to check with datadog support regarding this.
More details  here .
I think what you want is to add the  &quot;exact_match: false&quot;  option, like so:
This should match on any process whose path+name  include  the search string you provide.
Alternatively, if you only want it to match on the name of the process, you'll want to set the search_string to be the exact name of the process that's running (so whatever is given as the name when you run a  ps | grep &quot;ecommerce-order&quot; , which in your case seems to be  ecommerce-order-0.0.1-SNAPSHOT.jar )
I am on the same boat.
I found this link:  datadog instrumentation .
So, currently (20.11.2017) there are not agents for C#.
Only Go, python and ruby are available.
(Disclosure: I'm a software developer at Datadog.)
Datadog's open-source .NET Tracer is currently (2019-02-11) in open beta.
It supports manual and automatic instrumentation and  OpenTracing .
For a list of currently supported languages/frameworks, see the  updated documentation .
Happy tracing!
Looks like I found the problem -  https://github.com/DataDog/jenkins-datadog-plugin/issues/101
current Datadog version 0.6.1. has a bug , after change the Jenkins main config ( any change , not related to Datadog configuration) it stop works.
I downgrade it to 0.5.7 and it works OK
Not possible today, but that is in Datadog's plans for development.
What you can do as a workaround, though, is add a link to your logs explorer with the query that triggered the monitor alert, so you can get a quick reference to what were the logs that triggered it.
This link, for example, would quickly scope you to the error logs over the last 15 minutes:  https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc
And markdown is supported, so you can keep your monitor messages prettier without long links in the messages.
Like so:
 [Check the error logs here](https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc)
It looks to me that these are Datadog specific configuration parameters.
So you first need to install the Datadog app to your Slack workspace, which you find on the Slack App Directory.
Here is how the process is described in the official documentation:
Installation
The Slack integration is installed via its integration tile in the
  Datadog application.
Configuration
Source:  official documentation  from Datadog
This might be a problem that  other people are running into too .
kubelet is no longer listening on the ReadOnlyPort in newer Kubernetes versions, and the port is being deprecated.
Samuel Cormier-Iijima reports that the issue can be solved by adding adding  KUBELET_EXTRA_ARGS=--read-only-port=10255  in  /etc/default/kubelet  on the node host.
It is in fact possible to send an alert if a metric shows the same value for a fix period of time.
You can do this by using the diff() function to your query to produce delta values from consecutive delta points and then apply the abs() function to take absolute values of these deltas.
To do this we use the Arithmetic functions available, which can be applied using the '+' button to your query in UI.
For alert conditions in the metric monitor itself, configure as follows:
Select threshold alert
Set the “Trigger when the metric is…” dropdown selector to below or equal to
Set the “Alert Threshold” field to 0 (zero)
This configuration will trigger an alert event when no change in value has been registered over the selected timeframe.
Here is a link to datadog article:  https://docs.datadoghq.com/monitors/faq/how-can-i-configure-a-metric-monitor-to-alert-on-no-change-in-value/
By introducing space in datadog.json.j2 template definition .i.e.
and running deployment again I got the working config as below
However I am not able to understand the behaviour if anyone could help me understand it
it appears to me like you're missing a couple environment vars in your docker-compose  datadog  service configuration.
And also the volume that adds the registry for tailing the logs from the docker socket.
Maybe try something like this if you haven't?
from there, if you still end up with trouble, you may want to reach out to support@datadoghq.com to ask for help.
They're pretty quick to reply.
Installed it as "Run with Admin rights" and it fixed the issue.
For me, I had to manually give the  ddagentuser  account read access to the file
C:\ProgramData\Datadog\auth_token
Have you tried specifying dependencies with  go mod  yet?
I faced the same issue and finally can solved by generating  go.mod  file with these command,
Here  is the details explanation.
I think the disconnect here is that the pattern needs to be in the Jboss log manager, and then they're encoded to JSON.
Have you tried puttinng  %X{dd.trace_id:-0} %X{dd.span_id:-0}  into your Jboss logging pattern?
If not, I'd also recommend opening a ticket at support@datadoghq.com and we can work this through with you.
I’m not sure if it is a recent addition, but the Datadog public API supports configuring Log Archives:  https://docs.datadoghq.com/api/latest/logs-archives/
You can also use tools like Terraform to configure them:  https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/logs_archive  (it uses the Datadog API internally)
If you are trying to connect to an HTTPS URL for Datadog ( https://app.datadoghq.com  in your example), then you will need to set the  https.proxyHost  system property for it to have effect -  http.proxyHost  is for HTTP URLs[1].
These are system-wide settings that will be used by the default  HttpSender  ( HttpUrlConnectionSender ) if a  Proxy  is not passed to its constructor.
The micrometer doc says
But I dont understand what it means?
should I replace this url with my
  proxy url or is there any specific uri pattern with the proxy?
This is referring to a different kind of proxy that you would configure to receive your Datadog traffic on your internal network, and it would forward it to Datadog outside of your network.
If you are using an HTTP proxy then you should use the system properties or an  HttpSender  with your HTTP proxy configured (e.g.
an  HttpUrlConnectionSender  and passing a  Proxy  to its constructor).
You can configure a custom  HttpSender  with a  DatadogMeterRegistry  using its  Builder .
If you expose this as a  Bean  in a  @Configuration  class, Spring Boot will use it in its auto-configuration.
For example:
[1]  https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html
That's all handled in the  message  attribute with  conditional logic variables .
If, for example, you define your  message  value to be this...
... then ...
got this resolved with below
rule1  %{ipv4:network.client.ip}\s+-\s+%{word:user}\s+\[%{date(&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;):date}\]\s+\&quot;%{word:http.module}\s+\/v1\/resources\/+%{word:onemds.module}\?+%{data:onemds.params:keyvalue(&quot;=&quot;,&quot;/:&quot;,&quot;&quot;,&quot;:&amp;&quot;)}
was never expecting the delimiter that can take 2 characters above  :&amp;
this helped to remove  rs:  for all except the first one.
not an elegant approach but it worked for my use case.
Found the answer:
The metric  kubernetes.kubelet.volume.stats.used_bytes  will allow you to get the disk usage on PersistentVolumes.
This can be achieved using the tag  persistentvolumeclaim  on this metric.
You can view this metric and tag combination in your account here:  https://app.datadoghq.com/metric/summary?filter=kubernetes.kubelet.volume.stats.used_bytes&amp;metric=kubernetes.kubelet.volume.stats.used_bytes
Documentation on this metric can be found here:  https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubelet
This answer doesn't solve the problem, because postgres is not running in the cluster, it's running in Azure.
I'll leave it up since it might be interesting, but I posted another answer for the actually environment setup.
For containerized setups, it's not usually recommended to set up a configmap or try giving the agent a yaml file.
Instead the recommended configuration is to put annotations on the postgres pod:  https://docs.datadoghq.com/integrations/postgres/?tab=containerized#containerized .
This concept of placing the config on the application pod, not with the datadog agent, is called autodiscovery.
This blog post does a good job explaining the benefits of this solution:  https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/#autodiscovery
Here is a picture diagram showing how the agent goes out to the pods on the same node and would pull the configuration from them:
To configure this, you'd take each of the sections of the yaml config, convert them to json, and set them as annotations on the postgres manifest.
An example of how to set up pod annotations is provided for redis, apache, and http here:  https://docs.datadoghq.com/agent/kubernetes/integrations/?tab=kubernetes#examples
For your scenario I would do something like:
notice how the folder name  postgres.d/conf.yaml  maps to the  check_names  annotation, the  init_configs  section maps to  init_configs  annotation, etc.
For the section on custom metrics, since I personally am more familiar with the yaml config, and it's easier to just fill out, I'll usually go to a yaml to json converter, and copy the json from there
A key thing to notice for all those configs is that I never set the hostname.
That is automatically discovered by the agent as it scans through containers.
However you may have set  my-postgres-host.com  because this postgres instance is not actually running in your kubernetes cluster, and is instead living on its own, and not in a container.
If that is the case, I would recommend trying to just put the agent on the postgres node directly, all the yaml stuff you've done would work just fine, if that db and the agent are both directly on the vm.
It looks like the question has been updated to say that this postgres db you are trying to monitor is not actually running the the cluster.
And you are not able to put an agent directly on the postgres server since it's a managed service in Azure, so you don't have access to the underlying host.
In those situations it is common to have a random datadog agent on some other host set up the postgres integration anyway, but instead of having  host: localhost  in the yaml config, put the hostname you would put to access the db externally.
In your example it was  host: my-postgres-host.com .
This provides all the same benefits of the normal integration (except you won't have cpu/disk/resource metrics available obviously)
This is all fine and makes sense, but what if all of the agents you have installed are the agents in the kubernetes daemonset you created?
You don't have any hosts directly on VMs to run this check.
But we definitely don't recommend configuring the daemonset to run this check directly.
If you did, that would mean you are collecting duplicate metrics from that one postgres db in every single node in your cluster.
Since every agent is a copy, they'd each be running the same check on the same db you define.
Luckily I notice that you are running the Datadog Cluster Agent.
This is a separate Datadog tool that is deployed as a single service once per cluster, instead of a daemonset running once per node.
It is possible to have the cluster agent configured to run 'cluster level' checks.
Perfect for things like databases, message queues, or http checks.
The basic idea is that (in addition to it's other jobs) the cluster agent will also schedule checks.
the DCA (datadog cluster agent) will choose one agent from the daemonset to run the check, and if that node agent pod dies, the DCA will find a new one to run the cluster check.
Here are the docs on how to set up the DCA to run cluster checks:  https://docs.datadoghq.com/agent/cluster_agent/clusterchecks/#how-it-works
To configure it you would enable some flags, and give the DCA the yaml file you created with a config map, or just mounting the file directly.
The DCA will pass along that config to whichever node agent it chooses to run the check.
Answering my own question.
The DataDog logging page has a Configuration section.
On that page the &quot;Pre processing for JSON logs&quot; section allows you to specify alternate property names for a few of the major log message properties.
If you add @m to the Message attributes section and @l to the Status attributes section you will correctly ingest JSON messages from the  RenderedCompactJsonFormatter  formatter.
If you add RenderedMessage and Level respectively you will correctly ingest  JsonFormatter(renderMessage: true)  formatter.
You can specify multiple attributes in each section, so you can simultaneously support both formats.
After few days of research and follow up with datadog support team, I am able to get the APM logs on datadog portal.
Below is my  docker-compose.yml  file configuration,  I believe it helps someone in future
The  Dockerfile  for my python long running application
Please note, on the requirements.txt file I have   ddtrace  package listed
In the  instructions  it refers to http://ccloudexporter_ccloud_exporter_1:2112/metrics in the open metrics file, but in my setup docker-compose gave the ccloud exporter container the name ccloud_ccloud_exporter_1.
To prevent this I added &quot;container_name: ccloud_ccloud_exporter_1&quot; in the docker compose file and used &quot;http://ccloud_ccloud_exporter_1:2112/metrics&quot; as prometheus url in the openmetrics file.
Tracing for requests coming from the browser are handled by RUM and not APM.
I can't find the documentation for it but there is a configuration option on the  browser SDK  to allow tracing to specific endpoints.
The tracing context will automatically be propagated to APM if enabled on the server side, and both the browser and server spans will appear in the same trace.
Usually metrics exposed to  /actuator/metrics  are sent to the metrics system like datadog.
You can try to check what exactly gets sent to datadog by examining the source code of  DatadogMeterRegistry
Put a breakpoint in the publish method and see what gets sent, or, alternatively set the logger of the class to &quot;trace&quot; so that it will print the information that gets sent to the datadog (line 131 in the linked source code).
Another possible direction to check is usage of filters (see  MeterFilter ) that can filter out some metrics.
This did the trick : Thanks to @MarkBramnik
Try this
You might be able to get this if you...
If you start having trouble as you start setting it up, you may want to reach out to support@datadoghq.com to get their assistance.
this seem to be working:
 -@userId:*?
*  do not forget the minus at the start.
Either of the  count_not_null()  or  count_nonzero()  functions should get you where you want.
If graph your metric, grouped by your tag, and then apply one of those functions, it should return the count of unique tag values under that tag key.
So in your case:
count_not_null(sum:your.metric.name{*} by {file_name})
And it works with multiple group-by tags too, so if you had separate tags for  file_name  and  directory  then you could use this same approach to graph the count of unique  combinations  of these tag values, or the count of unique combinations of  directory + file_name :
count_not_null(your.metric.name{*} by {file_name,directory})
Yes, it is possible.
You can do that in a  processing pipeline  with a grok parser, but you'll want to configure which attribute the grok parser applies to in the advanced settings ( docs here ).
(By default grok parsers apply to the "message" attribute, but you can configure them to parse any attribute.)
In this case, you'd set the  Extract From  field to  requestUri .
The  Helper Rules  section is not necessary for this.
And then in the main  Define Parsing Rules  section, you'll plug in a rule similar to this:
or even further
What about using the template variables  doc ?
You could select:
Then you'll be able to replace your  {name:$flavor-db-master}  with  {$Name}
Otherwise, if you actually wants the value of the template variable you have to use  $flavor.value .
I advise to use a not widget to check the actual behavior.
EDIT:
This kind of setup is not the recommended.
It would be better to set two tags on your database:
You would then have a unique selection of tags,  env:dev,dbname:db1-master .
It would then be easy to have a query such as:
You can do this in a processing pipeline with 2 steps:
If there are other queries/patterns you want to use to determine the log level/status, you can add multiple rules to the  Category Processor  in (1), and you can map the  level  value to  info/warn/error  and any other relevant status value.
You mostly have to wait for it all to fill in over time.
Metric timestamps cannot be more than 10 minutes in the future or more than 1 hour in the past.
https://docs.datadoghq.com/developers/metrics/
I got the answer to the second question.
Now, I can get all tables from one database that I specified.
All I needed to do; relation_regex: '.
*' and disabled relation_name.
Answer to the first question I got from datadog is that there is no way to monitor all the DBs without listing them individually.
They may change this in future, but for now we have to add blocks for each and every database that we want to monitor
This works for me:
What you are doing is correct only, however, the common mistake is not following the below.
This library MUST be imported and initialized before any instrumented
  module.
When using a transpiler, you MUST import and initialize the
  tracer library in an external file and then import that file as a
  whole when building your application.
This prevents hoisting and
  ensures that the tracer library gets imported and initialized before
  importing any other instrumented module.
Basically, you cannot have  require(any instrumented lib)  (e.g.
http, express, etc) before calling init() tracing function.
https://docs.datadoghq.com/tracing/setup/nodejs/
It's probably too late, but it may be useful to others.
You can set tags by using the environment variables of the application container (not the agent container) by using DD_TAGS.
Apparently there is a datadog/agent:latest-jmx that should be used that contains the java image...
I just missed it in the docs.
I discussed this with Datadog support, and they confirmed that the  awslogs  logging driver prevents the Datadog agent container from accessing a container's logs.
Since  awslogs  is currently the only logging driver available to tasks using the Fargate launch type, getting logs into Datadog will require another method.
Since the  awslogs  logging driver emits logs to CloudWatch, one method that I have used is to create a subscription to stream those log groups to Datadog's Lambda function as configured  here .
You can do that from the  Lambda side  using CloudWatch logs as the trigger, or from the CloudWatch Logs side, by clicking  Actions    Stream to AWS Lambda .
I chose the Lambda option because it was quick and easy and required no code changes to our applications (since we are still in the evaluation stage).
Datadog support advised me that it was necessary to modify the Lambda function in order to attribute logs to the corresponding service:
In  this block , modify it to something like:
According to Datadog support:
I had to make further modifications to set the value of the  syslog.
*  keys in a way that made sense for our applications, but it works great.
I found the solution: the user  datadog  didnt have permission to read connections that wasnt form him.
So it was just getting a single row.
I gave permissions for that user to read  pg_stat_activity
It looks like you created  some  policy, but not the policy of required type.
When you create the role for Datadog, you have to choose a very specific role type:
Select Another AWS account for the Role Type.
and then create a policy for that role.
Also, don't forget to
Check off Require external ID
You shouldn't have any problems as long as you follow the guideline step by step:  https://docs.datadoghq.com/integrations/amazon_web_services/
I had this problem, when I tried to both use the role-assumption role as an assumption role on the  assume_role_policy , as well as trying to attach it.
Once I got rid of the aws_iam_policy that I created with the role-assumption policy doc as well as the role-policy attachment, it worked.
Hope this helps.
One of Dockers main features is portability and it makes sense to bind datadog into that environment.
That way they are packaged and deployed together and you don't have the overhead of installing datadog manually everywhere you choose to deploy.
What they are also implying is that you should use  docker-compose  and turn your application / docker container into an multi-container Docker application, running your image(s) alongside the docker agent.
Thus you will not need to write/build/run/manage a container via Dockerfile, but rather add the agent image to your  docker-compose.yml  along with its configuration.
Starting your multi-container application will still be easy via:
Its really convenient and gives you additional features like their  autodiscovery  service.
Just use a find me Twimlet.
Enter up to 10 numbers and a timeout between moving on to the next number.
Twilio will do the rest.
https://www.twilio.com/labs/twimlets/findme
If you are looking for a more full featured paid solution I'd recommend PagerDuty.
DataDog has an integration for PagerDuty.
Any monitor that gets triggered that mentions  @pagerduty-myteamname (as example) in the monitor message will cause PagerDuty to page the on call person.
If that person does not acknowledge the page you can configure it to go through list of people to contact next until it is acknowledged by someone.
The role arn:aws:iam::xxxxxxxxxx:role/DatadogAWSIntegrationRole also has to have permission to assume the role on the other account.
You'll have to update the DatadogAWSIntegrationRole on the primary account to include:
I suspect you're probably looking to query the event stream which is where all alerts from monitors can be found.
The docs at  https://docs.datadoghq.com/api/#events-get-all  are a pretty good starting place.
You'll want to query this endpoint with the proper source and tags, but this should be a starting point.
If this doesn't quite work, I'd recommend looking at pulling the details from the monitor as shown here:   https://docs.datadoghq.com/api/#monitor-get-details .
This may be a second option if you're unable to get the information you're looking for from the event stream.
The "ReadTimeout: HTTPConnectionPool" error can be corrected by adding a timeout parameter under instances in the elasticsearch.yaml
Frank,
Your use case follows the standard "custom metric" submission that is common within Datadog.
Using one of the supported libraries:
http://docs.datadoghq.com/libraries/#java
You can leverage the statsD port of an Agent running on your host to submit these custom metrics:
http://docs.datadoghq.com/guides/dogstatsd/
You will want to install the Agent on either the host running this function or point your statsD connection towards an accepting host:
http://docs.datadoghq.com/guides/basic_agent_usage/
There are additional docs found here that should help you understand how custom metrics work in Datadog:
https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-
https://help.datadoghq.com/hc/en-us/articles/203765485-How-do-I-submit-custom-metrics-What-s-their-overhead-
Usually when troubleshooting custom metric submissions, we try to implement some form of local printing/logging to ensure the statsD connection is being made and that the custom function is being called and submitted.
Once you can confirm the metric is being sent to the Agent, use the Metric Summary page to check for the custom metric:
https://app.datadoghq.com/metric/summary
If all else fails, reach out to Datadog at support@datadoghq.com
It is very much possible, just use the alias property of the  attribute  filter:
Verify if the datadog package is installed in your environment.
You can do this with this command:
If it's not installed, you can install it with this command:
You could tag your metrics with the name or ID of the agent it is collecting metrics from (if you aren't already).
Then in Datadog you could write a query that groups by the agent ID and applies a count_not_null function:  https://docs.datadoghq.com/dashboards/functions/count/
This basically hijacks a random metric to extract the unique count of agents reporting that metric to assume the total count of agents.
You wouldn't be able to easily group by queue though, so idk if it would be a good solution to your use case.
Your idea around using gauges sounds good to me.
You can send a new metric called something like  myagent.running  which sends a value of 1 for each of your agents and does a sum of all gauges in order to get a count.
That is actually how the metric  datadog.agent.running  is implemented:  https://docs.datadoghq.com/integrations/agent_metrics/#metrics
After testing different queries I found that running this query groups the results by query statements and will return the count of each.
In Datadogs mysql configuration I added tags for the query statement and database name.
Now since they are grouped, I can see information per different statement in datadog.
Datadog’s IIS integration queries the Web Service performance counters automatically and sends the results to Datadog.
The Web Service performance counter class collects information from the World Wide Web Publishing Service.
You can enable the IIS integration by creating a configuration file either manually or through the Datadog Agent GUI.
To create a configuration file through the GUI, navigate to the “Checks” tab, choose “Manage Checks,” and select the iis check from the “Add a Check” menu.
You can also manually create a conf.yaml file in C:\ProgramData\Datadog\conf.d\iis.d.
There is a sites attribute in the conf.yaml file.
This attribute represents the IIS site you want to monitor.
You only need to delete the sites you want to exclude.
More information you can refer to this link:  IIS monitoring with Datadog .
If you are using Datadog's .NET Tracer, you can set  DD_TRACE_ENABLED=false  in the  appSettings  section of the  web.config  file ( docs ).
For example:
Another option is to deploy a  datadog.json  file ( docs ) in the root of your app that contains:
(Disclaimer: I work at Datadog)
Update: I found this bit of code in the tracing client repo:
https://github.com/DataDog/dd-trace-js/issues/1249
maybe it would help
Old message:
Never mind.
seems like my solution is only for express, graphql doesn't support that property
You probably want to just modify the validateStatus property in the http module:
Callback function to determine if there was an error.
It should take a status code as its only parameter and return true for success or false for errors
https://datadoghq.dev/dd-trace-js/interfaces/plugins.http.html#validatestatus
As an example you should be able to mark 403s as not be errors with something like this:
Unfortunately, that doesn't seem to be a setting you can directly control at this time.
The reasoning is that a given timeseries could be split by tag, so setting a single color for a timeseries split by tag would amount to multiple entires of the same color, and that wouldn't make sense.
To support the semantic meaning, I've often used the following settings:
I'm not sure I know what distinction there is of Error vs Critical in your definition, but using these palettes has proven useful for my team.
If you're looking for a specific widget to change color based on value - so if the number exceeds a threshold - take a look at the  Query Value Widget , as that can be customized to change color based on the current value.
Alternately, if you have a Monitor already set for the timeseries, use the  Alert Value Widget  to show the current status, with less configuration, since the thresholds are managed in the Monitor's definition.
You can't set a color per line, but you can set the color per query.
If you edit the graph using json, that field  requests.style.palette  is exposed and you can just try typing in whatever color you want there.
https://imgur.com/VrZZl72
If you want to have one time series that is green for hits, and one that is red for errors, you just make two metric queries, and then color one green and one red.
https://imgur.com/AHGi1Hk
That endpoint is used to send events to Datadog, if your one account is sending them to the logs product, it is most likely because that account has a special flag that converts all your incoming events into logs.
This is common for users that use the Security Monitoring product.
I would recommend reaching out to support@datadoghq.com to see if this flag is enabled in that one account.
And if you want to replicate that behavior in another account, you can ask them to enable that feature.
In Datadog, create an API key in Integrations, APIs.
Give the API key a name.
In NLog.config create a target.
The url is either datadoghq.com or datadoghq.eu (for europe).
Now create a rule to write to the target and you are done!
All of the parameters can be configured to become columns in Datadog, and/or facets to select on.
I am using a date parameter so that the date matches other logs, rather than displaying the built in date.
The datadog agent you deployed has no power to run scripts or take action.
It is purely a monitoring/data collection tool.
However one of the things your monitors in the Datadog application can do is trigger events when they go into an alert state.
There are  lots of integrations : creating a ticket in Jira, posting a message to Slack, triggering an SNS topic.
What I recommend you try to do, is to create some kind of job or script that can be triggered externally, like a lambda function, or a jenkins job, or anything really.
When the monitor goes off you can use a  webhook  to trigger that script to do whatever you define.
Here is a blog post showing  how twilio sent out a text message by connecting their api to a webhook .
In these kinds of checks, the response order matters, since the columns returned from the DB are going to be mapped back to the names specified in the YAML.
Reading the error message:
Error: postgres:953578488181a512 | (postgres.py:398) | non-numeric value  cldtx  for metric column  active_connections  of metric_prefix  postgresql
We can see that the value of  cldtx  is being returned for the  active_connections  column, which in the YAML is declared as a gauge, and this is a string.
The fix should be straightforward, by reordering the YAML, like so:
Alternately, if you want to keep the YAML ordered, change the query to:
So I ended up just looking at the tests in the datadog terraform provider and noticing the query format they are testing.
It seems you need to specify a time range and also add in a comparison threshold that matches your critical alert threshold.
That was what was missing.
It is not possible no.
Confirmed with DD support.
Actually it is possible, but you need to put every json log into quotes (some prefix before each log will also work), so that Datadog agent will consider this as a 'text'.
I.e.
log.json  file should contain quoted logs:
After that, in Datadog Logs Configuration you need to add a pipeline with Grok parser filter  json  (see filter tab in  Matcher and Filter ):
This allowed me to perform full text search thru all fields in my json logs and automatically parse all json fields as attributes.
P.S.
This solution was provided by Datadog support 2 years ago.
And seems they are working on solution to allow full text search for JSON logs.
It looks from the above snippets that the  combined  Morgan format is sent directly sent to Winston, and then parsed within a log pipeline in Datadog.
Since the  combined  format doesn't include the body and there is no built-in token for it, you would have to use a custom format with your own tokens and then update your pipeline accordingly.
For example, to create a custom format in Morgan that includes the status code and the body:
You can also create a token to achieve the same result with a simpler format definition:
You can find the documentation for custom Morgan formats  here , creating tokens  here , and Datadog log pipeline parsing  here .
Hope this helps!
Maybe you can ask them to add it by opening a feature request:
 https://github.com/DataDog/documentation/issues/new/choose
but it looks like I'm missing an important part here.
That was the point.
The lambda itself has not much todo with particular  statusCodes .
So I either may log each status code and let datadog parse it accordingly.
Or, that's the solution I went for, I can leverage API-Gateway for monitoring status codes per lambda.
You can use most any of the common open source log shippers to send server logs to Datadog without using the Datadog agent, for example  fluentd .
But there can be several benefits to using the Datadog agent to collect server logs, such as:
There are other ways to collect logs in Datadog, among those is the  HTTP API .
Since this API uses a POST method, I bet you could configure Datadog's  webhook integration  to generate log events from Datadog events and alerts.
That said, before you go through the trouble of doing this, if you have a use-case or reason you're interested in doing this, you may want to reach out to Datadog support to see if they have some features coming / in beta that would get you what you want without the extra work on your end.
(What  is  your use-case?
I'm curious)
Sounds like some of your  http.server.requests.count  metric values do not have any  status  tag, so when you group by the  status  tag, those are being aggregaed with a value of  n/a .
If it is intentional/expected that this metric would have values without the  status  tag and you just want to ignored those metric values, then you can use the  exclude_null()  function to remove that tag grouping from your graph (docs  here ).
If it is not intentional/expected that this metric would have values without the  status  tag, then you probably want to reach out to support@datadoghq.com to get that looked into.
Just add this in the startup.cs.
I'm very sorry for this late answer.
After exchanging a bunch of emails with the Datadog support guys, it turned out that the solution is straightforward:
The  Dispose()  method force the sinks to gracefully close up and send the logs stored in cache.
Please note that the logs do not appear instantly in the Datadog console: allow a few seconds (to some minutes) for their systems to process the logs you send.
You might want to use datadog service-check which is also can be sent by the StatsDClient, and then add it to your monitor/dashboard page.
https://docs.datadoghq.com/developers/service_checks/
Another option is to use  Datadog's Java / JMX integration  to capture the health data that's exposed over JMX -- this can probably give you up/down health data, and can certainly give you a lot more granular health metrics too.
Well, apparently you can  -@facet:*
Didn't specify it in my question because it was not important, but what I really needed was a way to either  filter by a specific facet value, or get logs without said facet
The following works for me:
Spring is scanning your classpath that seems incomplete.
Maybe it is related to Spring's class loading mechanisms.
The  class in question  exists and seems to be part of the agent.
Possibly, you are using an outdated version of the agent.
I'm not particularly familiar with this Datadog JSON format but the general pattern I would propose here has multiple steps:
Here's a basic example of that to show the general principle.
It will need more work to capture all of the details you included in your initial example.
The separate normalization step to build  local.screenboard  here isn't strictly necessary: you could instead put the same sort of normalization expressions (using  try  to set defaults for things that aren't set) directly inside the  resource "datadog_screenboard"  block arguments if you wanted.
I prefer to treat normalization as a separate step because then this leaves a clear definition in the configuration for what we're expecting to find in the JSON and what default values we'll use for optional items, separate from defining how that result is then mapped onto the physical  datadog_screenboard  resource.
I wasn't able to test the example above because I don't have a Datadog account.
I'm sorry if there are minor typos/mistakes in it that lead to errors.
My hope was to show the general principle of mapping from a serialized data file to a resource rather than to give a ready-to-use solution, so I hope the above includes enough examples of different situations that you can see how to extend it for the remaining Datadog JSON features you want to support in this module.
If this JSON format is a interchange format formally documented by Datadog, it could make sense for Terraform's Datadog provider to have the option of accepting a single JSON string in this format as configuration, for easier exporting.
That may require changes to the Datadog provider itself, which is beyond what I can answer here but might be worth raising in the GitHub issues for that provider to streamline this use-case.
I solved this by adding 'squashfs' to the list of filesystem types to be ignored by the datadog agent.
Create a file  /etc/datadog-agent/conf.d/disk.d/conf.yaml :
Restart datadog agent ( systemctl restart datadog-agent ).
If you have an ever increasing counter, you can use the a function called  rate .
You'll be able to select it with the  +  on the query line.
With that you'll be able to have a rate of increase per seconds, minutes or hours.
If you are looking to get a difference between the same metric but at another point in the past, you have a function called  timeshift  that could also help.
This is also accessible with the small  +  on the right of the query line.
Finally, if you are looking at comparing two different metrics, you  have a button called  Advanced  that will enable you to write more complex queries such as a difference between two metrics.
I believe you are looking for  clusterName :
 https://github.com/helm/charts/blob/master/stable/datadog/values.yaml#L75
You can add it in your  values.yaml  under the  datadog  section like this:
refer below command
I am not a fan of helm, but you can accomplish this in 2 ways:
via env vars: make use of  DD_AC_EXCLUDE  variable to exclude the Redis containers: eg  DD_AC_EXCLUDE=name:prefix-redis
via a config map: mount an empty config map in  /etc/datadog-agent/conf.d/redisdb.d/ , below is an example where I renamed the  auto_conf.yaml  to  auto_conf.yaml.example .
alter the daemonset/deployment object:
On the bottom of the infrastructure list, you should see a link called "JSON API permalink".
If you  query it , this should give you a JSON of all your hosts with their agent version.
You can then query it with a quick Python script.
Your metrics should have a common prefix like  myapp.metric1 ,  myapp.metric2 , etc.
Then you can disable all metrics and enable explicitly all  myapp.
*  metrics like so:
application.properties:
the  management.metrics.enable.&lt;your_custom_prefix&gt;  will enable all  &lt;your_custome_prefix&gt;.
*  metrics.
If you want to enable some of the built-in core metrics again, for example reenabling  jvm.
* , you can do:
I've created a sample project  in github  that disables core metrics, enables custom metrics, and  jvm.
*  metrics and sends to Datadog.
Not sure I fully grasp the issue.
Here are some steps to collect your traces:
Just in case, more info on Open Tracing  here
Seems like there is a typo in your command.
DD_DOGSTATD_NON_LOCAL_TRAFFIC   is used instead of  DD_DOGSTATSD_NON_LOCAL_TRAFFIC
I usually used the below command for testing with Datadog:
Have you tried  composite monitors ?
You should be able to combine your low CPU Credit monitor with another monitor that looks at events from RDS.
Two monitors such as:
A composite monitor: A &amp;&amp; !B
(I hope my example makes sense)
so, I had to install a Datadog agent on ec2 instance and configure it to be able to access all mysql dbs and collect the mysql performance schema metrics.
And surely with correct security groups for the ec2.
I followed this  docs  and  this  and contacted the support.
If the Windows OS is D drive, the setting is installed in  D:\ProgramData\Datadog .
Copying it to  C:\ProgramData\Datadog  will work, but I submitted an improvement request to Datadog Support.
Yes it does have this functionality in the Audit Association entity.
The entity stored with the blame_id in the Audit Log entity contains information regarding the user.
The one with source_id contains information regarding the entity itself and thus the ID of the entity in the  fk  field.
Once the datadog is properly installed on your server, you can use the custom metric feature to let datadog read your query result into a custom metric and then use that metric to create a dashboard.
You can find more on custom metric on datadog  here
They work with yaml file so be cautious with the formatting of the yaml file that will hold your custom metric.
It looks like Datadog uses zstd compression in order to compress its data before sending it:  https://github.com/DataDog/datadog-agent/blob/972c4caf3e6bc7fa877c4a761122aef88e748b48/pkg/util/compression/zlib.go
This is what I've got so far.
Everything but the  source .
Need to use category-processor  https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor
Example:
I didn't do enough research before posting this question, but the answer for anyone else looking.
You are using the DataDog configuration for the commercial  k6 Cloud service  ( k6 cloud ), not locally run k6 tests ( k6 run ).
test_run_id  is a concept in the cloud service, though it's also easy to emulate locally as a way to distinguish between test runs.
For local tests, you should enable the DataDog output by running k6 with  k6 run --out datadog script.js .
I assume you did that, otherwise you wouldn't see any metrics in DataDog.
Then, you can use the  tags  option  to inject a unique extra tag for all metrics generated by a particular k6 run, so you can differentiate them in DataDog.
For example:
Of course, you can choose any  key=value  combination, you are not restricted to  test_run_id .
Nothing is wrong there.
DataDog conceals that the Kafka integration uses Dogstatsd under the hood.
When  use_dogstatsd: 'true  within /etc/datadog-agent/datadog.yaml is set, metrics do appear in DataDog webUI.
If that option is not set the default Broker data is available via JMXFetch using  sudo -u dd-agent datadog-agent status  as also via  sudo -u dd-agent datadog-agent check kafka  but not in the webUI.
Based on the  doc  you can decide which one is good for your use case.
Stackdriver is detailed as &quot;Monitoring, logging, and diagnostics for applications on Google Cloud Platform and AWS&quot;.
Google Stackdriver provides powerful monitoring, logging, and diagnostics.
It equips you with insight into the health, performance, and availability of cloud-powered applications, enabling you to find and fix issues faster.
Grafana can be classified as a tool in the &quot;Monitoring Tools&quot; category, while Stackdriver is grouped under &quot;Cloud Monitoring&quot;.
Answering my own question this might be helpful for others
I had to set
inside  /etc/datadog-agent/datadog.yaml  then I've created  python.d/conf.yaml  with the following configs
Restart the agent with
You can see your logs in the dashboard logs panel
Window option available... Go to integration and  click on agents..
There is an option available windows ( left side ).. Click on windows then u get link..... Just copy the link and paste it on ur windows Server.... Then datadog starts monitoring...
It takes 5 minutes to monitor
The MeterRegistry already has implemented how to send custom metrics (posting to DataDog) See the code  https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java#L133
It appears like you are trying to add common tags to each metric.
Perhaps you shouldn't implement your own DataDog registry, and instead use the provided one to send the metrics and set the common tags via config:
I don't believe there is any way to graph the historical behavior of the SLI from an SLO.
The closest you could get would be to measure the underlying metric, so if you had  good events / bad events  you could display that percentage.
But the calculation of how often that percentage is above or below a certain threshold would not be possible.
I recommend reaching out to support@datadoghq.com to let them know it's a feature you're interested in.
They might be able to provide some updates.
Not sure if this will work but you can give it a try..:
{{^is_exact_match a.value b.value }}
@my@mail.com
Alert 2 hosts has passed the threshold
{{/is_exact_match}}
same value - ignore - do nothing
The problem is that you probably might get 2 alerts at the same time...
It turns out that trace id can be set via HTTP endpoint  https://docs.datadoghq.com/api/v1/tracing/#send-traces .
There doesn't seem to be an option for sending traces to the agent directly.
This can still be useful if the performance penalty of making HTTP calls is not a concern, i.e., if you are not working on a real-time system.
Datadog keeps the logs for a period of time according to the billing plan you've selected:  https://www.datadoghq.com/pricing/#section-log  If you choose the 7 day plan, logs will be dropped from Datadog after 7 days.
The default plan seems to be 15 days, but there are other options between 3-60 days.
I've solved it now by verifying the status via code and by adding tags to the metrics:
This way I can filter in my dashboard for  occurrence:first  only.
To make sure things are clear, you have a metric called  myService.errorType  with a tag  entity .
This metric is a counter that will increase every time an entity is in error.
You will then use this metric query:
When you speak about UUID, it seems that the cardinality is small (here you show 3).
Which means that every hour you will have small amount of UUID available.
In that case, adding UUID to the metric tags is not as critical as user ID, timestamp, etc.
which have a limitless number of options.
I would invite you to add this uuid tag, and check the cardinality in the  metric summary page  to ensure it works.
Then to get the number of UUID concerned by errors, you can use something like:
Finally, as an alternative, if the cardinality of UUID can go through the roof, I would invite you to work with logs or work with Christopher's solution which seems to limit the cardinality increase as well.
You may need to set the environment variable  DD_APM_NON_LOCAL_TRAFFIC=true  in your datadog agent container.
Ref:  https://docs.datadoghq.com/agent/docker/apm/?tab=linux#docker-apm-agent-environment-variables
Allowing 'triggered' (but not 'recovery') monitor notifications is not a configurable option for the integration on either Opsgenie or Datadog.
You  can , however, control this within the Datadog Monitor message body where you reference opsgenie
Lorem ipsum dolor sit amet @opsgenie-oncall @slack-somechannel
You can wrap the opsgenie reference within the message body with conditional tags (datadog actually calls them variables) documented here:
 https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables 
Then the message body might look something like this:
Lorem ipsum dolor sit amet {{#is_alert}}@opsgenie-oncall{{/is_alert}} @slack-somechannel
Now the alert to opsgenie will only occur on the 'trigger' of the monitor but not on 'recovery'
If you are sending metrics to an actual StatsD server, then tags are not supported by the protocol.
You would need to instead send the metrics to the Datadog agent's DogStatsD endpoint which extends StatsD with additional features such as tags.
You can find more information about DogStatsD  here .
If you are already using the DogStatsD endpoint, then I would suspect an incompatibility with the  node-statsd  library.
The library has not been updated for 6 years and it's possible that something changed since then, causing it to no longer work.
In that case I would recommend switching to a more recent DogStatsD client that is still maintained such as  hot-shots .
Hope this helps!
There's a 2-click path from Slack that should already do this for you out-of-the-box.
The slack notification gives you a link to the alert event in your Datadog account (click-1), and from the alert event, towards the bottom you'll find a series of links to other relevant places, one of those is "Related Logs" (click-2).
That brings you to the Log Explorer scoped to the relevant time period of the alert, and scoped to the tags of whatever it was that was alerted on (so presumably the logs you're looking for).
If you want to add a link of this sort as something you can configure in the alert message, that sounds like something you should reach out to support@datadoghq.com for to ask Datadog to implement it.
In the end we solved the problem dynamically building the url for the logs:
There's a Prometheus endpoint for Tibco EMS:
https://community.tibco.com/wiki/statistics-logger-tibco-enterprise-message-service#toc-15
I think you can then add the prometheus integration to your datadog agent to send the data do datadog, and build your own dashboard for that:
https://docs.datadoghq.com/integrations/prometheus/#data-collected
I had something similar issue and chose the first option, but i won't say it is from terraform perspective (since i also had lack in experience in terraform).
The first hierarchy was more reasonable in segregation aspects, plus would be easier to add/remove/update organizations by demand.
Answer from Datadog Support to this:
Thanks again for reaching out to Datadog!
From looking further into this, there does not seem to be a way we can package the JDBC  driver with the Datadog Agent.
I understand that this is not desirable as you would prefer to use a standard image but I believe the best way to have these bundled together would be to have a custom image for your deployment.
Apologies for any inconveniences that this may cause.
Yes, you can configure widgets to exclude results by tags.
You can do this by applying a tag prepended with a  !
to signify "not".
So in your case, you can set up your widget scoped over  importance:ignore  and then hit the little  &lt;/&gt;  button on the right to expose the underlying query, and sneak a  !
in front to make it  !importance:ignore .
This doc has a nice example  (although it's for notebooks, it works the same in dashboards as well).
After talking with Datadog support, it seems like this is a known issue.
Thanks for your patience while we looked into this issue.
We we're currently investigating this along with PoolExecutors and will reach out with updates.
Right now it looks like those child spans within the async call lose context, so they appear disconnected.
The workaround for now is to pass in the parent's context.
Add this line just before calling the thread pool executor.
Then pass that context to the function that gets run in the threadpool:
And use it to create a span inside the function like this:
The complete example looks like this:
This will produce a result that looks like this:
First  you'll want to make sure your logs are well structured (which you can control in  Datadog's processing pipelines ).
Effectively you'll want to parse out the "code" values into some "error code" attribute.
If your log events are in this format...
...Then all you need is a fairly simple grok parser rule, thanks to the "json" filter function.
Something like this would get you where you want (note the  %{data::json}  part, that's what parses the in-log JSON).
Once you've configured this, your logs will also have an attribute called "error.code" with a value of  2001  or  1001  or whatever.
Second  you'll want to  create a facet  for that new  error.code  attribute so that you can  make toplist / timeseries / etc.
graphs grouped out by your "error code" facet .
No, you cannot install the Datadog agent on a Snowflake host.
We use our separate job scheduling system to monitor Snowflake by running queries (e.g.
checks on SYSTEM$CLUSTERING_DEPTH, aggregate queries against the QUERY_HISTORY for timing, etc) via the JDBC connector then relaying the results to our monitoring stack (similar to how Datadog agent would work.)
Point both of those at a service like  httpbin  to see how they differ.
Requests'  data  option for POST requests  generates form-encoded data  by default, while  curl  passes the JSON string through directly.
You can manually encode your payload as a JSON string:
or if you have Requests version 2.4.2 or later you can use the  json  parameter to have your  dict  converted to JSON automatically:
One solution would be to setup in  logs &gt; configuration &gt; pipelines  a  category processor  to add a new attribute that could be made searchable.
edit: 19th Nov 2019
Step 1:
Add grok parser to extract sign:
rule:  detect_dollar .*%{regex("[$]+"):dollarSign}.
*
Step 2:
Then you can setup a category processor as indicated above.
This could look for the attribute  @dollarSign:$  and set the attribute  hasDollarSign  to True and set it to false otherwise.
Step 3:
Create a facet on  dollarSign  attribute.
Following logs can then be searched for.
For logs with no  $
For logs with  $
You can do the same with the  hasDollarSign  attribute and set it as a facet.
I've tried with this query which is similar to yours:
And this seems to give me a count by  dbinstanceidentifier  results.
Do you have more information to provide?
Maybe an event list and a monitor result screenshot?
Side note : I also use this for Kafka (just as a reference) but it should not be required in your case:
The main issue is that you would have to mount the volume in both the app container and the agent container in order to make it available.
It also means you have to find a place to store the log file before it gets picked up by the agent.
Doing this for every container could become difficult to maintain and time consuming.
An alternative approach would be to instead send the logs to  stdout  and let the agent collect them with the Docker integration.
Since you configured  logsConfigContainerCollectAll  to  true , the agent is already configured to collect the logs from every container output, so configuring Winston to output to  stdout  should just work.
See:  https://docs.datadoghq.com/agent/docker/log/
To support rochdev comment, here are a few code snippets to help out (if you do not opt in for the STDOUT method which should be simpler).
This is only to mount the right volume inside the container agent.
On your app deployment, add:
And on your agent daemonset:
At first glance, this seems to be a use case more suitable for the APM part of Datadog which would measure the execution time and could be  instrumented  to measure the execution time of the smaller functions (if it does not picked up this data automatically).
You can then create some nice charts with  Trace Search and Analytics .
You could also use a custom metric with tags such as  opsize:large  and  opsize:small  which would represent the execution time (a gauge).
You can find more details  here .
At the moment, the log module of Datadog does not seem to support the calculation you expect to see.
However, the two solutions above and the related logs can be made visible in a dashboard side by side.
Have you tried using the  start  and  end  tags with a 24 hour window?
As I posted the issue on GitHub, they give an answer the issue was in source code for dd-trace-php and they will fix and new release.
https://github.com/DataDog/dd-trace-php/issues/334
Below response of DatDog in github:
Ah now this is much more clear, thanks for sharing.
This is a known problem that we are currently and actively working on.
As I cannot commit to that, the fix will be probably come out with the next release.
At an higher level, the cause is an issue we have in some specific cases while invoking private/protected methods and parent::* invocations.
In the meantime, if you are still interested in testing/using the other integrations, the only thing I can recommend is to disable the pdo integration:  fastcgi_param DD_INTEGRATIONS_DISABLED pdo .
Again, the fix to this is currently in development and will be released very soon.
Use the Query Value widget.
It can only show a single value, which is the average for the current time window that has been chosen.
Maybe you could mod the process check to also tag the process number metric by PID ( this is probly where you'd change that ).
That way you could group your monitor by your pid tag and the no-data alerts would tell you when the pid switched.
But this would also alert on expected pid changes, so maybe you'd have to schedule downtimes too aggressively for this to be a good idea?
Maybe monitoring some crash logs with  their Log Management tool  would be a better approach?
Lambda is serverless.
Datadog agent is for the host.
While running lambda you have absolutely no control over the host as you are not managing it.
Hence, You can monitor application running on lambda using datadog integration of lambda for the different application.
You may follow below link for AWS Integration of datadog.
Ref:  https://docs.datadoghq.com/integrations/amazon_lambda/
You can monitor a database from a different host as long as the host the agent is running on has access.
So for this section in the config file:
https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/data/conf.yaml.example#L4
instead of using  localhost  you can set the IP.
This will allow you to monitor your database without adding a new agent.
Then you would just follow the normal  postrgres setup  and you will have access to all the metrics and  service checks  including  postgres.can_connect  which is probably what you care about.
It seems to be an intentional "feature"  https://github.com/kamon-io/kamon-datadog/issues/19  introduced in 1.x.
They have chosen approach to put service name in tag instead.
Even though datadog is being run from the same machine, it is setting up a separate server on your machine.
Because of that, it sounds like the datadog agent doesn't have access to your z:/ driver.
Try to put the "TaskResults" folder in your root directory (when running from datadog - where the mycheck.yaml file is) and change the path accordingly.
If this works and you still want to have a common drive to be able to share files from your computer to datadog's agent, you have to find a way to mount a drive\folder to the agent.
They probably have a way to do that in the  documentation
The solution to this is to create a file share on the network drive and use that path instead of the full network drive path.
May be obvious to some but it didn't occur to me right away since the normal Python code worked without any issue outside of Datadog.
So instead of:
use
Have a look at the documentation for  Dogstream .
It allows you to send metrics to datadog from log files (including summarised metrics).
You may need to write a custom parser for any data that is not in the datadog canonical format in order for datadog to recognize the data.
Checkout the example  here .
This sounds like a bug.
It is possible the Datadog exporter is running in a non-daemon thread.
The JVM views non-daemon threads as application critical work.
So essentially the JVM thinks it shouldn't shutdown until the non-daemon thread finishes.
In the case of the Datadog exporter thread, that probably won't happen.
To verify there are non-daemon threads, use  jstack  to generate a thread dump.
(command:  jstack &lt;pid&gt; ) or dump all threads in your  close  method:
An example thread dump output is below.
Notice the word 'daemon' on the first line:
Gauge metric  types will do the job here given that your query does not run more than once within 10 seconds.
If that is not the case, go for  count metric
The flush interval in datadog by default is 10 seconds, if you use a  gauge metric  and the metric is reported more than once in a flush interval, datadog agent only sends the last value ignoring the previous ones.
For  count metric  in contrast, the agent sums up all the values reported in the flush interval.
More details about flush interval  here .
The best metric type would be a  histogram  metric.
This will take multiple values, and pre-aggregate them within a flush window, so you will be able to get things like min/max/sum/avg and various percentiles.
If you run multiple times within a flush window:
As I have mentioned in the comment, you are affected by  two pass model .
You should remove the keys in the resource added to the end of the chef run or triggered by the DD cookbook resources invoked as the last one in the run.
However, it may not work with all versions of DD cookbook.
From few DD cookbook versions, it is possible to store keys in node's run state which is not written to the Chef server.
The above example is preferred solution to your issue.
I noticed that the API key of Datadog changes everytime whenever in close the site and open a new instance.
so after entering new API key , the issue was solved
Two approaches that may work:
It looks like flink has an  HTTP connector  to send metrics to Datadog, which at first glance looks to send over the Datadog metrics API instead of dogstatsd.
Dogstatsd is not very different from statsd otherwise, so it's often easy to modify statsd libraries to work for dogstatsd.
This project on GitHub  seems to be such a project, and may come in handy.
Well, you  could  use the Datadog API to script the creation of 20 unique dashboards that all share the same content, but with different hosts.
This is the part of the API docs that would help (with examples!)
for Timeboards , and this one  for Screenboards .
That said, I'd personally find 20 dashboards a bit cluttered / unwieldy in my own Datadog account.
Instead, if it was me, I'd try to (A) find clever uses of dashboard template variables (on e.g, cluster tags, host tags, etc.
), or (B) group out by each host tag and apply  the "top()" function  in some way so that I'd be able to see just the most extreme-value hosts.
But that's certainly up to you :)
Your answer appears to be there in the text -- you're missing a Python package.
Try running  sudo pip install psutil , then restarting the agent.
Can you add your agent version, OS and version, and how you installed the agent to your text as well?
It looks like you're also using a  very  old version of the agent (it's up to 5.17.
* for a number of OS's) so there may be better package bundling or critical updates since v. 4.4.0.
Try installing a newer version as well.
Please find the required
In the code you posted:
This seems to be creating an empty client object.
Shouldn't you be creating a client with your keys using  datadog.NewClient("...", "...")  as in the first code snippet you posted?
Also, you should check the error returned as that will give you more hints to troubleshoot the issue:
`
The solution:
docker container 0
#!/bin/sh 
    java -Djava.util.logging.config.file=logging.properties -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=9998 -Dcom.sun.management.jmxremote.port=9998 -Djava.rmi.server.hostname=$HOSTNAME -Dcom.sun.management.jmxremote.host=$HOSTNAME -Dcom.sun.management.jmxremote.local.only=false -jar /app/my-streams.jar
docker container 1
way more stuff was done that is available from from stack overflow posts.
but the above fixes the metrics finding error from datadog-agent.
Here is how to run each component:
docker container 0 
* my-streams 
* spin up dependent services in tab 
** mvn clean package docker:build 
** docker-compose up
docker container 1 
* docker build -t dd-agent-my-streams .
* docker run -v /var/run/docker.sock:/var/run/docker.sock:ro   -v /proc/:/host/proc/:ro   -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro -e LOG_LEVEL=DEBUG -e SD_BACKEND=docker --network=mystreams_default
ssh into docker container 1 to verify if metrics work 
* docker ps // to find the name of the container to log into 
* docker exec -it  /bin/bash 
root@904e6561cc97:/# service datadog-agent configcheck 
root@904e6561cc97:/# service datadog-agent jmx list_everything 
root@904e6561cc97:/# service datadog-agent jmx collect
I think what you actually want is the metrics-query API endpoint?
http://docs.datadoghq.com/api/#metrics-query
There are also a few Node.JS libraries that may be able to handle this kind of metric querying for you:  http://docs.datadoghq.com/libraries/#community-node
The recommendation of:
"Please don't include endlessly growing tags in your metrics, like timestamps or user ids.
Please limit each metric to 1000 tags."
Is more of a warning against using infinitely expanding values as they can drastically increase your custom metric usage.
As mentioned in the following article:
https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-
"By default customers are allotted 100 custom metrics per host across their entire infrastructure rather than on a per-host basis.
For example if you were licensed for 3 hosts, you would have 300 custom metrics by default - these 300 metrics may be divided equally amongst each individual host, or all 300 metrics could be sent from a single host."
You will want to keep in mind when configuring your metrics/tags of your current allotment of custom metrics and any billing implications that may have.
That said, if having these tags is important to your team please reach out to support@datadoghq.com and we can sync up with the Sales Team to determine what is best for your team and use case.
There is a preview feature that allows you to graph your SNAT port usage and allocation, see:
https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics#how-do-i-check-my-snat-port-usage-and-allocation
You can look in to other datadog kube metrics like kubernetes.replicas.available / total to alert if no of available - total &lt; 0.
Same can be done or for daemonset pods also there is a specific metric exposed.
[Datadog docs-kube metrics][1]
[1]: https://docs.datadoghq.com/agent/kubernetes/data_collected/
An issue with IE11 is fixed in v1.26.1
See the fix here:  [RUMF-791] prevent IE11 performance entry error #633
Datadog's Ruby library keeps this info on the struct  Datadog.tracer.active_correlation .
You can call  Datadog.tracer.active_correlation.trace_id  to grab the trace ID.
You probably want to be using the MySQL integration, and configure the 'custom queries' option:  https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries
You can follow those instructions after you configure the base integration  https://docs.datadoghq.com/integrations/mysql/#pagetitle  (This will give you a lot of use metrics in addition to the custom queries you want to run)
As you mentioned, DogStatsD is a library you can import to whatever script or application in order to submit metrics.
But it really isn't a common practice in the slightest to modify the underlying code of your database.
So instead it makes more sense to externally run a query on the database, take those results, and send them to datadog.
You could totally write a python script or something to do this.
However the Datadog agent already has this capability built in, so it's probably easier to just use that.
I am also just assuming SQL refers to MySQL, there are other integration for things like SQL Server, and PostgreSQL, and pretty much every implementation of sql.
And the same pattern applies where you would configure the integration, and then add an extra line to the config file where you have the check run your queries.
There is  dogapi  which wraps the entire Datadog API and should be able to do the above use case, probably using a mix of  metric.query ,  infrastructure.search ,  search.query  and  monitor.getAll .
For example, to get the list of monitors, it would look something like this:
Please keep in mind that I didn't test the above code.
If you need something that is not in the library, it should also be fairly easy to wrap the API directly since it's a simple HTTP endpoint.
I hope this helps!
For each web application that you want to configure with a different  Datadog APM service name , you need to set the environment variable  DD_SERVICE_NAME .
If they're all running under the same IIS process, that's not possible.
In IIS there's a feature named  Application Pool , which can be used to isolate multiple web applications by running them under different processes.
The first thing you need to do is to create a separate application pool for each web application.
Once you're done with that, you can set a different  DD_SERVICE_NAME  for each application pool.
The  command  to set an environment variable scoped to a specific application pool is
where  MyAppPool  is the name of the application pool, and  my-service  is the service name that you want to use for the Datadog APM.
After running the above command, you have to restart IIS for the changes to take effect:
Starting with version 1.0 of Datadog's .NET Tracer, you can set most settings in your application's  app.config / web.config  file.
For example, to set  DD_SERVICE_NAME :
[Disclaimer: I am a Datadog employee]
Maybe you could use the Datadog respective dashboard apis to update your metric names in all dashboards?
Should theoretically be not too complicated to script out.
https://docs.datadoghq.com/api/?lang=python#screenboards 
 https://docs.datadoghq.com/api/?lang=python#timeboards
Use the  requests  library its a lot simpler
Generate a request header like this
Send the request like this
While searching through this  other issue , I found that all that is needed to fix this issue is to specify the API key and the application key within the URL.
Consider the following.
Yes, kind of.
It's possible to show single value on a dashboard (just use "Query Value" visualization), but it must be based on some metric reported to Datadog.
This is how it looks like:
It only applies to produce requests.
First you may need to download the MSI file:
The actual powershell command for installation (with extra optional arguments included as arguments):
It's been a while since i've done this (8 months or so?
), so it could be outdated, but it used to work :).
Note, if you're running this from a remote provisioning script, you'll probly have to schedule this to be executed not-remotely so that the installation command can be run with heightened permissions, which i believe is required.
And you  may  need to make sure the computer is plugged into the power source (i remember hitting some infuriating issue where that was an arbitrary requirement for Windows scheduled tasks to run, and Windows didn't allow me to configure around that).
is your  activemq_58.yaml  all in one line like that?
You probably want it to be more like this:
There are a variety of issues here.
1.
You've misconfigured the scope formats.
(metrics.scope.operator)
For one the configuration doesn't make sense since you specify "metrics.scope.operator" multiple times; only the last config entry is honored.
Second, and more importantly, you have misunderstood for scope formats are used for.
Scope formats configure which context information (like the ID of the task) is included in the reported metric's name.
By setting it to a constant ("latency") you've told Flink to not include anything.
As a result, the numRecordsIn metrics for every operator is reported as "latency.numRecordsIn".
I suggest to just remove your scope configuration.
2.
You've misconfigured the Datadog Tags
I do not understand what you were trying to do with your tags configuration.
The tags configuration option can only be used to provide  global  tags, i.e.
tags that are attached to every single metrics, like "Flink".
By  default  every metric that the Datadog reports has tags attached to it for every available scope variable available.
So, if you have an operator name A, then the numRecordsIn metric will be reported with a tag "operator_name:A".
Again, I would suggest to just remove your configuration.
For the container agent, you'll want to run  sudo docker exec -it dd-agent /etc/init.d/datadog-agent status  from your unix based box.
If, however, you are using the alpine image the command is:  docker exec -it dd-agent /opt/datadog-agent/bin/agent status  (different path).
More here in this KB from Datadog:  https://help.datadoghq.com/hc/en-us/articles/203764635-Agent-Status-and-Information
I had issues with it too.
The agent is most likely sending a value for 100 incorrectly as 10000 instead, as that's the highest I've seen it go.
To make it usable, ensure the graph type is 'line', click "Advanced" on the metric, and make the equation  a / 100 .
I didn't find any guidance on this either in my search, but this seems to go along with what I've seen logged locally into a Windows box and watching the performance counters locally.
Just should set hostname for  org.coursera.metrics.datadog.DatadogReporter.Builder :
Set is almost never the right custom metric type to use.
It will send a count of the number of unique items per a given tag.
The underlying items details will be stripped from the metric, meaning that from one time slice to the next, you will have no idea that actual true number of items over time.
For example
Your time series to datadog will report  3 , and then  2 .
But because the underlying device info is stripped you have no idea how to combine that 2 and 3 if you to zoom out in time and roll up the numbers to show 1 data point per minute.
It could be any number from 3 to 5, but the Datadog backend has no idea.
(even though we know that across those 30 seconds there were 4 unique values total)
Plus even if it was accurate somehow, you can't create an alert of it or notify anyone, because you won't know which device is having issues if you see a spike of devices in the 60 second bucket.
So let's go through other metric options.
The only metric types that are ever worth using are usually  distributions  or  gauges , or [counts].
A gauge metric is just a measurement of the latency at a point in time, it's usually good for things like CPU or Memory of a computer, or temperature in a room.
Numbers that are impossible to actually collect all dat a points for so you just take measurements every 10 seconds, or every minute, or however often you never to get an idea of the behavior.
A count metric is more exact, it's the number of things that happened.
Usually good for number of requests to a server, or number of files processed.
Even something like the amount of bytes flowing through something, although that usually is treated like a gauge by most people.
Distributions are good for when you want to create a gauge metric, but you need detailed measurements for every single event that happens.
For example a web server is handling hundreds of requests per second and we need to know the latency metrics of that server.
It's not possible to send a latency metric for every request as a gauge.
Gauges have a built in limit of 1 data point per second (in Datadog).
Anything more sent in a 1 second interval gets dropped.
But we need stats for every request, so a distribution will summarize the data, it keep a running count, min, max, average, and optionally several percentiles (p50, p75, p99).
I haven't seen many good use cases for metric types outside of those 3.
For your scenario, it seems like you would want to be sending a distribution metric for that device interval.
So device 1 sends a value of 10.14 and device 3 sends a value of 2.3 and so on.
Then you can use a  distribution widget  in a dashboard to show the number of devices for each interval bucket.
Of course make sure you tag each metric by the device that is generating the metric.
I was able to do that by using this api call:  https://docs.datadoghq.com/api/?lang=python#get-a-screenboard 
and then get it as a son file, which can be passed to cloud formation later.
Actually what you need to do is create a generic method.
Now when you are hitting the different different end points just call the updateCounter method will will capture your metric with the specific name of your route.
For example you have route like add and subtract
Then call the update counter method with metric name add and subtract.
You can leverage Datadog's Agent to collect metrics via a JMX connection.
There is documentation found here:
http://docs.datadoghq.com/integrations/java/
https://www.datadoghq.com/blog/monitoring-jmx-metrics-with-datadog/
https://help.datadoghq.com/hc/en-us/articles/204501525-Custom-JMX-Integration-s-
https://help.datadoghq.com/hc/en-us/articles/207525586-View-jmx-data-in-jConsole-and-set-up-your-jmx-yaml-to-collect-them
That should help you get setup and collecting the necessary metrics exposed by your JMX port.
That said, if you encounter any issues reach out to support@datadoghq.com and they can assist you.
What you have is a nil pointer dereference.
(Unless you are using package  unsafe , which you probably shouldn't touch, so I'm assuming you're not.)
It looks like the  e  argument to  func (c *Client) Event(e *Event) error  is  nil  when called from  github.com/some/path/server/http.go:86 .
Thanks to a comment from @twotwotwo, I think I figured this out.
In this line
I wrote the following program to demonstrate to myself how different function signatures appear in a stack trace:
When  bam  is called, which acts on  *Y  but has no arguments or return value, the output contains:
When  foo  is called, which acts on  *Y  and takes a  *X  as argument, but has no return value, the output contains:
When  bar  is called, which acts on  *Y , takes a  *X  as argument, and returns a  *Y , the output contains:
When  baz  is called, which acts on  *Y , takes  *X  as argument, and returns an  error  (which is an interface), the output contains:
Think differently :)
Do bind a nginx-server (vhost) on port 10080  in addition  - that server does offer the status location and what you need.
Server on 80/443 is also there and ONLY that one is bound/exposed to host ( exposed to the outer world ).
Since datadog is part of your docker-network / service network, it can still access 10080 in the internal network, but nobody else from the outer network.
Bulletproof, easy - no strings attached.
Since we are running the service through  docker-compose  and our issue being we don't know the IP of the agent.
So the simple solution is to know the IP before starting.
And that means assigning our agent a specific IP
Here is a update  docker-compose  to do that
Now you can do two possible things
You can listen only on  172.25.0.101  which is accessible only container running on agent network.
Also you can add  allow 172.25.0.100  to only allow the agent container to be able to access this.
There are two (easier) ways to go about it.
First one is  docker-compose  but since I already have a setup running since 2 years which doesn't use docker-compose, I went for the 2nd way.
Second way is  Allow  Directive with a range of IPs.
Eg:
I am not security expert, but mostly  192.168.
*  IP range is for local networks, not sure about  172.18.
*  range though.
To get more idea about this IP range thing and CIDR stuff, refer below links
 http://nginx.org/en/docs/http/ngx_http_access_module.html
https://www.ripe.net/about-us/press-centre/understanding-ip-addressing
As the err info said dh key is too small, a larger one might help.
Replace the default dh512.pem file with dh4096.pem
sudo wget "https://git.openssl.org/gitweb/?p=openssl.git;a=blob_plain;f=apps/dh4096.pem" -O dh4096.pem
Ref:  http://www.alexrhino.net/jekyll/update/2015/07/14/dh-params-test-fail.html
This is actually a lot harder than it seems.
Representing big integers in JavaScript can be done using the  BigInt  data type (by suffixing the number with  n ), which is fairly widely supported at this point.
This would make your object look like this:
The problem presents itself in the JSON serialization, as there is currently no support for the serialization of  BigInt  objects.
And when it comes to JSON serialization, your options for customization are very limited:
So the only option that I can find is to (at least partially) implement your own JSON serialization mechanism.
This is a  very  poor man's implementation that calls  toString()  for object properties that are of type  BigInt , and delegates to  JSON.stringify()  otherwise:

 
 const o = {
  "span_id": 16956440953342013954n,
  "trace_id": 13756071592735822010n
};

const stringify = (o) =&gt; '{'
  + Object.entries(o).reduce((a, [k, v]) =&gt; ([
      ...a, 
      `"${k}": ${typeof v === 'bigint' ?
v.toString() : JSON.stringify(v)}`
    ])).join(', ')
  + '}';

console.log(stringify(o));
Note that the above will not work correctly in a number of cases, most prominently nested objects and arrays.
If I were to do this for real-world usage, I would probably base myself on  Douglas Crockford's JSON implementation .
It should be sufficient to add an additional case around  this line :
You could try to use  clinic  in order to debug and profile the app.
pretty good tool for nodeJS.
You could user  node-memwatch  to detect where is memory leak.
It also might be a known issue, here is the  link  with a similar issue.
You are on the right path.
The guide I'm about to link to begins by following a similar approach to the one you've taken.
I'll link to the section that talks about monitoring memory in real time, which is available when you  Record allocation timeline  in chrome://inspect
https://marmelab.com/blog/2018/04/03/how-to-track-and-fix-memory-leak-with-nodejs.html#watching-memory-allocation-in-real-time
This question is quite old, however, still might be useful for new users of Google Cloud.
In 'Metrics Explorer' in Google Cloud Console there is an option to write a query with MQL (click  Query Editor  button).
MQL supports expressions which are described in detail  here .
The simplest example for dividing one metric by another would look like this:
It is probably caused by a regression between .NET Core 2.2 and .NET Core 3.0
Apparently it will be fixed in version 3.1.7
Just starting the process causes the memory leak on linux, because of a non released handle
Issue has been tracked here  https://github.com/dotnet/runtime/issues/36661
The problem is that the advice class will be loaded on the system class loader as a part of the agent whereas the actual application code is loaded on a sub-class loader that is not visible to the system class loader.
This situation does not change if you load your agent on the boot loader either.
Therefore, the agent cannot load the  HttpServletRequest  class which is part of the uber-jar.
This is a typical problem with agents and Byte Buddy has a standard way to circumvent it by using a  Transformer.ForAdvice  instance instead of using the  Advice  class directly.
Byte Buddy then creates a virtual class loader hierarchy that considers classes represented by both class loaders.
Update : The problem is that you are calling down to your interceptor that is defined in the system class loader where the class in question is not available.
The annotated code will be inlined but the invoked method will not.
If you copy-pasted the code into the annotated method, the behavior is as you'd expect it.
Byte Buddy uses the annotated code as template and reuses a lot of information emitted by javac to guarantee a speedy conversion.
Therefore, the library cannot simply copy the method and should rather feed the entire method body to javac.
The reason for this error is because apache is listening to port 80 on IPv4 &amp; IPv6.
This will explicitly tell apache to listen to IPv4.
In apache config change:
Listen 80
to
Listen 0.0.0.0:80
Make sure the file is being copied in to your docker container and being used in apache.
Or add an extra step in the Dockerfile:
&amp;&amp; sed -i 's/^Listen 80$/Listen 0.0.0.0:80/' /etc/apache2/httpd.conf
Containers are about isolation so in container "localhost" means inside container  so ddtrace-test cannot find ddagent inside his container.
You have 2 ways to fix that:
I'm guessing you're talking about "metrics" instead of matrix!
On the Producer, you have  kafka.producer:type=producer-metrics,client-id="{client-id}" .
That metric has 2 interesting attributes:
request-latency-avg: The average request latency in ms
request-latency-max: The maximum request latency in ms
On the broker side, there are a few metrics you want to check to investigate your issue:
Request total time: Total time Kafka took to process the request.
kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce
In case this is high, you can check the break down metrics:
These are all listed in the metrics recommended to monitor list in the Kafka documentation:  http://kafka.apache.org/documentation/#monitoring
Some preliminary Google searching lands me on  https://github.com/kubernetes/kubernetes/pull/42717  by way of  https://github.com/kubernetes/kubernetes/issues/24657 .
It looks like the pull request was merged in time to be in Kubernetes 1.7.
This should mean that you can use the Downward API to expose  status.hostIP  as an environment variable ( https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ ) or a file in a volume ( https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/ ).
Your application would then need to read the environment variable or file to get the value of the actual host IP address.
If you agent is written by yourself, you can open and listen on a Unix domain socket and let the other pod send data through it.
If not, you can write a small data proxy that listens on a Unix socket for data.
On the other end, by sharing a pod with the daemon, you can easily send data to the local container
I use the exact same setup,  dd-agent  running as a DaemonSet in my kubernetes cluster.
Using the same port mapping you commented  here , you can just send metrics to the hostname of the node an application is running on.
You can add the node name to the pods environment using the downward api in your pod spec:
Then, you can just open an UDP connection to  ${NODE_NAME}:8125  to connect to the datadog agent.
There are a number of different ways you could handle this:
These are just some of your options.
Since the Airflow webserver is just a Flask app you can really expose metrics in whatever way you see fit.
As I understand, you can monitor running tasks in DAGs using DataDog, refer the integration with Airflow  docs
You may refer metrics via DogStatD  docs .
Also, look at this  page  would be useful to understand what to monitor.
E.g., the metrics as below:
HikariCP is a connection pool and JDBC is the API for managing a connection.
So it can be thought that Spring thinks about separating connection-pool-manager metrics from connection metrics.
You need to set the  aws_ecs_task_definition 's  network_mode  to  awsvpc  if you are defining the  network_configuration  of the service that uses that task definition.
This is mentioned in the  documentation for the  network_configuration  parameter of the  aws_ecs_service  resource :
network_configuration  - (Optional) The network configuration for the
  service.
This parameter is required for task definitions that use the
   awsvpc  network mode to receive their own Elastic Network Interface,
  and it is not supported for other network modes.
In your case you've added the  network_mode  parameter to the  container  definition instead of the  task  definition (a task is a collection of n containers and are grouped together to share some resources).
The  container definition schema  doesn't allow for a  network_mode  parameter.
You can use a conditional with count to override if a resource is to be created.
The example below will only create the resource when the variable environment is not = production.
If Count = 0 then the resource won't be created,
Regards,
I don't think there is something that does this for you automatically.
You have to reset the counter yourself at each reporting interval.
Something like this should work:
Actually, It is quite simple.
This is called  Packaging namespace packages .
https://packaging.python.org/guides/packaging-namespace-packages/
All you need is to separate all packages to sub - packages and after install it with a namespace.
2 questions that'll be helpful:
Now some clarification on where I think you're going wrong here:
I suggest tracing the problematic query to see what cassandra was doing.
https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tracing_r.html
Open cql shell, type  TRACING ON  and execute your query.
If everything seems fine, there is a chance that this problem happens occasionally, in which case I'd suggest tracing the queries using nodetool settraceprobablilty for some time, until you manage to catch the problem.
You enable it on each node separately using  nodetool settraceprobability &lt;param&gt;  where param is the probability (between 0 and 1) that the query will get traced.
Careful: this WILL cause increased load, so start with a very low number and go up.
If this problem is occasional there is a chance that this might be caused by long garbage collections, in which case you need to analyse the GC logs.
Check how long your GC's are.
edit: just to be clear, if this problem is caused by GC's you will NOT see it with tracing.
So first check your GC's, and if its not the problem then move on to tracing.
Here you can find how to add a new webhook to your mandrill account:  https://mandrillapp.com/api/docs/webhooks.php.html#method=add
tha main thing here is this:
 $url = 'http://example/webhook-url'; 
this is your webhook URL what will process the data sent by mandrill and forward the information to Datadog.
and this is a description about what mandrill will send to your webhook URL:  http://help.mandrill.com/entries/21738186-Introduction-to-Webhooks
a listener for webhooks is nothing else then a website/app which triggers an action if a request comes in.
Usually you keep it secret or secure it with (http basic) authentication.
E.g.
create a website called  http://yourdomain.com/hooklistener.php .
You can then call it with HTTP POST or GET and pass some data like hooklistener.php?event=triggerDataDog or with POST and send data along with the body.
You then run a script or anything you want to process that event.
A "listener" is just any URL that you host where you can receive data that is posted to it.
Keep in mind, since you mentioned Zapier, you can set up a trigger that receives the webhook data - in this case the listener URL is provided by Zapier, and you can then send that data into any application (or even post to another webhook).
Using Zapier is nice because it doesn't require you to write the listener code that receives the hook data and does something with it.
I believe that Amazon actually offers a service that would accomplish your goal -  CloudWatch   (pricing) .
I'm going to take your points one by one.
Note that I haven't actually  used  it before, but the documentation is fairly clear.
One server had high CPU for over 5 mins when the alert should be triggered after 1 minute
It looks like CloudWatch can be configured to send an alert (which I'll get to) after one minute of a condition being met:
One can actually set conditions for many other metrics as well - this is what I see on one of my instances, and I think that detailed monitoring (I use free), might have even more:
What else is out there that can do the same job and  will also integrate with Pager Duty?
I'm assuming you're talking about  this .
It turns out the Pager Duty has a  helpful guide  just for integrating CloudWatch.
How nice!
Here's the pricing page , as you would probably like to parse it instead of me telling you.
I'll give a brief overview, though:
You don't want basic monitoring, as it only gives you metrics once per five minutes (which you've indicated is unacceptable.)
Instead, you want detailed monitoring (once every minute).
For an EC2 instance, the price for detailed monitoring is $3.50 per instance  per month .
Additionally, every alarm you make is $0.10 per month.
This is actually very cheap if compared to  CopperEgg's pricing  - $70/mo versus  maybe  $30 per month for 9 instances and copious amounts of alarms.
In reality, you'll probably be paying more like $10/mo.
Pager Duty's tutorial suggests you use SNS, which is another cost.
The good thing:  it's dirt cheap .
$0.60 per million notifications.
If you ever get above a dollar in a year for SNS, you need to perform some serious reliability improvements on your servers.
You're not just limited to Amazon's pre-packaged metrics!
You can actually send custom metrics (time it took to complete a cronjob, whatever) to Cloudwatch via a PUT request.
Quite handy.
Submit Custom Metrics generated by your own applications (or by AWS resources not mentioned above) and have them monitored by Amazon CloudWatch.
You can submit these metrics to Amazon CloudWatch via a simple Put API request.
(from  here )
So all in all: CloudWatch is quite cheap, can do 1-minute frequency stats, and will integrate with Pager Duty.
In short Server Density is a monitoring tool that will monitor all the relevant server metrics.
You can take a look at this page  where it’s all described .
One server had high CPU for over 5 mins when the alert should be triggered after 1 minute
Server Density’s open source agent collects and posts the data to their server every minute and you can decide yourself when that alert should be triggered.
In the alert below you can see that the alert will alert 1 person after 1 minute and then repeatedly alert every 5 minutes.
There is a lot of other metrics that you can alert on too.
What else is out there that can do the same job and will also integrate with Pager Duty?
Server Density also integrates with PagerDuty.
The only thing you need to do is to  generate an api key at PagerDuty  and then provide that in the settings.
Just provide the API key in the settings and you can then in check pagerduty as one of the alert recipients.
You can find the  pricing page here .
I’ll give you a brief overview of it.
The pricing starts at $10 for one server plus one web check and then get’s cheaper per server the more servers you add.
Everything will be monitored once every minute and there is no fees added for the amount of alerts added or triggered, even if that is an SMS to your phone number.
The cost is slightly more expensive than the Cloudwatch example, but the support is good.
If you used copperegg before they have a  migration tool  too.
Server Density allows you to monitor all the things!
Then only thing you need to do is to send us custom metrics which you can do with a plugin written by yourself or by someone else.
I have to say that the graphs that Server Density provides is somewhat akin to eye candy too.
Most other monitoring solutions I’ve seen out there have quite dull dashboards.
It will do the job for you.
Not as cheap as CloudWatch, but doesn’t lock you in into AWS.
It’ll give you 1 minute frequency metrics and integrate with pagerduty + a lot more stuff.
Reference
https://people.apache.org/~dkulp/camel/eventnotifier-to-log-details-about-all-sent-exchanges.html
Update
The  Exchange.CREATED_TIMESTAMP  is no longer stored as exchange property, but you should use the  getCreated  method on Exchange.
If you put in ECS Task Definition (sample from json version, but in UI also possible to setup), you should be able to configure container logs:
Since your question says  is there a way to inspect inside/after each task completes  - I'm assuming you haven't tried this celery-result-backend stuff.
So you could check out this feature which is provided by Celery itself :  Celery-Result-Backend / Task-result-Backend  .
It is very useful for storing results of your celery tasks.
Read through this =   https://docs.celeryproject.org/en/stable/userguide/configuration.html#task-result-backend-settings
Once you get an idea of how to setup this result-backend, Search for  result_extended  key (in the same link) to be able to add  queue-names  in your task return values.
Number of options are available - Like you can setup these results to go to any of these :
I have made use of this  Result-Backend  feature with  Elasticsearch  and this how my task results are stored :
It is just a matter of adding few configurations in  settings.py  file as per your requirements.
Worked really well for my application.
And I have a weekly cron that clears only  successful results  of tasks - since we don't need the results anymore - and I can see only  failed results   (like the one in image).
These were main keys for my requirement :  task_track_started  and  task_acks_late  along with  result_backend
Try  recover  to catch all panics and log them.
Without that, it'll write the panic msg to stderr:
You can override the default  TracingInstrumentation  with your own implementation.
It will be picked automatically due to the @ConditionalOnMissingBean annotation in the  GraphQLInstrumentationAutoConfiguration  class.
Here is a simple example that adds two custom metrics:  graphql.counter.query.success  and  graphql.counter.query.error :
My application.yaml, just in case:
I'm using spring-boot-starter-parent:2.2.2.RELEASE, graphql-spring-boot-starter:6.0.0
I hope it helps.
Avg CPU usage may not give better view.
Check if max CPU utilization is getting around 100%.
If so, you may need to optimize on ES side.
You should understand how cluster autoscaler works.
It is responsible  only  for adding or removing nodes.
It is not responsible for creating or destroying pods.
So in your case cluster autoscaler is not doing anything because it's useless.
Even if you add one more node - there will be still a requirement to run DaemonSet pods on nodes where is not enough CPU.
That's why it is not adding nodes.
What you should do is to manually remove some pods from occupied nodes.
Then it will be able to schedule DaemonSet pods.
Alternatively you can reduce CPU requests of Datadog to, for example, 100m or 50m.
This should be enough to start those pods.
You can add priorityClassName to point to a high priority PriorityClass to your DaemonSet.
Kubernetes will then remove other pods in order to run the DaemonSet's pods.
If that results in unschedulable pods, cluster-autoscaler should add a node to schedule them on.
See  the docs  (Most examples based on that) (For some pre-1.14 versions, the apiVersion is likely a beta (1.11-1.13) or alpha version (1.8 - 1.10) instead)
Apply it to your workload
Here are two ways that work:
1.
(Drill down to  .services , remember it as  $services  for later use, get the list of keys, and select the ones such that the corresponding value in  $services  has a  build  key).
2.
(Drill down to  .services , convert to a list of  {"key": ..., "value": ...}  objects, select the ones where the  .value  has a  build  key, and return the  .key  for each).
The second is probably more idiomatic jq, but the first provides an interesting way to think about the problem as well.
Here's a third approach, notable for being oblivious to the upper reaches:
Have you tried to mock the  datalog  module inside your function  test ?
As long as your other scripts are not running concurrently with your test, this may work.
That way the mock itself will be set only when the function is called, instead of being set in your script scope.
You could use  unittest.mock.patch .
If you are using pytest you can do the same with the  monkeypatch  fixture.
I have used many of solutions you mentioned.
Splunk is good but it becomes really expensive if you have huge amount of data.
You could have always used Cloudwatch Logs but it doesn't give you so much on visual part..
I will recommend ELK (ElasticSearch, Logstash, Kibana) stack.
It is a very standard solution; in which logs are stored in Elastic Search.
Kibana is used for visualization of logs.
This works in almost real time.
If you have very specific dashboards; then you can always create custom dashboards using some front end technologies like AngularJS etc.
but if visual part is really huge and very flexible then I feel ELK is better.
ELK (ElasticSearch, Logstash, Kibana) stack is a really good solution for what you are looking for, but in some cases ELK is not going to be able to get some metrics, in this case you have some solutions like create your own  beat  program to get the information or use another program to gather this metrics like Apache NiFi.
You can use AWS CloudWatch, create a log stream for each of your application or service.
Define your custom metrics, create a dashboard and alert.
It's not limited to AWS things; you can use CloudWatch log agent for On-premises services or software on your local network.
For more information read the following article by Jeff Barr
https://aws.amazon.com/blogs/aws/cloudwatch-log-service/
and
https://aws.amazon.com/blogs/aws/improvements-to-cloudwatch-logs-dashboards/
FYI: we already monitor a lot of application and service inside and outside of AWS by CloudWatch, and it works like a charm.
It seems normal because  Ansible  does not refer to Python virtual environment in your case:
In  virtualenv  non-installed packages are initialized from real system environment.
So you can achieve it by setting up  Ansible  within  virtualenv
Have a look at this example:
After installation of  Ansible  in  virtualenv
Ansible  refers to the paths of Python virtual environment:
ps: Need to deactivate and activate again the  virtualenv  once to load the  Ansible  from virtual environment after the installation.
To get you started: Create a timelion expression:
What you are looking for is achievable using Visual Builder visualization
See  https://www.elastic.co/guide/en/kibana/6.1/time-series-visualizations.html
Aggregate  Max  or  Avg  on  system.cpu.total.pct
Group By  Terms  By  beat.hostname.keyword
The visualization will show CPU usage in % for all hosts sending metrics to your cluster.
If you add more hosts those will show up too!
This is just to illustrate @ben5556's answer with an image.
NOTE:  The "Term" is  beat.hostname .
sorry for the delay.
From your error log I can't see any issues on recovery being thrown but I either don't see any connection attempts.
I wonder if you have some issues with data in the group replication relay logs...
I suggest you open a bug if the problem still persists.
As a workaround you can try to reset the applier channel before "START GROUP_REPLICATION"
RESET SLAVE ALL FOR CHANNEL "group_replication_applier";
The telegraf/influxdb/grafana stack can monitor space left on disc.
Kapacitor can also be added if you want alerts.
If you want to specify a limit, you have to use a dedicated partition / mount point or a btrfs subvolume with quotas.
Another option is to make cron job to clean up unused docker images, unused docker volume, and exited docker container.
I use this method myself.
A better approach than using DaemonSets to run your application would be to use a Deployment so that you don't tie your application to the number of nodes in your cluster.
You can then deploy the datadog agent image as a DaemonSet with a set  spec.template.spec.affinity  that selects nodes with a pod of your application running.
This will make sure you have a datadog agent in every node where your application runs.
Another option is to deploy the datadog agent container in the same pod as your application container.
In this case you can reach the agent through localhost and scale together, but might end up with more than an agent per node, hence my preference for a DaemonSet with an affinity.
My team ran it as a daemon set for the purposes of collecting node metrics, but only exposed it as a normal cluster IP service for the purposes of programmatically sending it data from other apps in the cluster.
You don't need to expose it on a node port unless you need to access it from outside the cluster and don't have a service-aware load balancer like an ingress controller.
(That would be quite a strange use case, so chances are you don't need to expose it on a node port.)
is the name of the default queue.
As you state, it is "queue:$NAME" but namespaces (if you use them (please don't)) will also prefix the key.
You can not set a name on the instances of docker that manages amazon.
The namespaces it uses are to be able to handle the scaling of the service.
Think that if you write the name and then the service you ask for more than one instance of your application, amazon could not instantiate it on the same node.
I hope the explanation has served.
No, there is not a way to control the name used for the container in Amazon ECS.
ECS picks a random name designed to avoid conflicts (since names must be unique in Docker; you can't have two containers with the same name) and you can see the code  here .
However, ECS does give you a few things that might be able to help you.
There are automatically-assigned Docker labels for the task ARN, the container name in your task definition, the task definition family, the task definition revision, and the cluster; see  here .
Additionally, you can assign your own custom Docker labels through the task definition.
I just had some difficulties to determine what is exactly your second separator.
you text example shows '·', but when I checked what is just after 'Elberg" and before '2nd...', I found 4 characters : code 32 (space), code 194 (¬), code 183 (∑), code 32 (space).
In the script bellow, I have used the code 194. it works when I cut/paste your text example into a file.
Here is the script :
Note : if the text does not contain "Job posted by ", then myAuthor is ''.
You had the right idea to use  AppleScript's text item delimiters , but the way you tried to extract the name was giving you trouble.
First, though, I'll go through some things you can do to improve your script:
There's no need to break the file contents into lines; AppleScript can operate on entire paragraphs or more, if desired.
Removing these unnecessary steps (and adding new ones to make it work on the entire file) shrinks the script considerably:
This right here is what's giving you wrong output:
This is incorrect.
It's not the  last  word you want; that's the last word of the file!
To extract the poster of the job listing, change it to the following:
Due to AppleScript's weird Unicode handling, for whatever reason the dot (·) that separates the name from the other text is converted to "¬∑" when run though the script.
So, we look for "¬" instead.
Some last code fixes:
Some of your variable names use  the_snake_case , while others use  theCamelCase .
It's generally a good idea to use one convention or another, so I fixed that, too.
I assumed you wanted that dollar sign in the output for whatever reason, so I kept it in.
If you don't want it, just replace  set output to "$ "  with  set output to "" .
So, your final, working script looks like this:
I agree it's hard to find, the closest one I can find is this
Return the number of instances that are set for the given module
  version.
This is only valid for fixed modules, an error will be raised for automatically-scaled modules.
Support for automatically-scaled modules may be supported in the future.
https://cloud.google.com/appengine/docs/python/refdocs/google.appengine.api.modules.modules
Btw you can also have monitoring from the StackDriver which has metric for total instance
You should also be able to use the recently GA'd App Engine Admin API to figure this out.
The nice thing about the admin API is that it's going to work for both standard and flexible:
 https://cloud.google.com/appengine/docs/admin-api/
Here's the endpoint that returns all of the instances for a given service/version:
https://cloud.google.com/appengine/docs/admin-api/reference/rest/v1/apps.services.versions.instances/list
Depending on the language you're using, there's usually a nice wrapper in the form of a "Google API client" + language library.
Hope this helps!
If you're trying to collect stats, you might want to use the  Stackdriver Monitoring API  to collect the timeseries values that Google has already aggregated.
In particular, the list of  App Engine Metrics is here .
For example,  system/instance_count  is the metric indicating the number of instances App Engine is running.
I hate it when SO questions only end up with partial answers, so here's a complete, working example.
If you paste it into your interactive console, it should work for you.
(Don't forget to set the  versionsId  to whatever your default app version is.
If you know how I can get it to use the default version, please post a comment.
'default', '*', 'any', etc.
all no da workie.)
Strictly achieved by trial and error:
The easiest way to proceed is to create a custom check.
You can read up on this here:  http://docs.datadoghq.com/guides/agent_checks/ .
There isn't a way to take a pre-existing Nagios or Sensu plugin and have it work as is with Datadog, but looking at one of the delayed_job plugins on Github, looks like it should be pretty easy to convert to a Datadog check.
If you have any issues, reach out to support either via email or #datadog on IRC.
This was a issue with deployed DataDog daemonset for me:
What I did to resolve:
Check daemonset if it exists or not:
Edit the datadog daemonset:
In the opened yaml, add
Add this in  env:  tag for all places.
For me there were 4 places which are having DD tags in the yaml.
Save and close it.
The daemonset will restart.
And the application will start getting traced.
If you are using the Helm chart, you can overwrite on the values:
hey!
Sorry in advance if my answer isn't correct because  I'm a complete newby  in kuber and helm and I can't make sure that it will help, but maybe it helps.
So, the problem, as I can understand, in the resulting  ConfigMap  configuration.
From my expirience, I faced the same with the following config:
And I could solve it only by surrounding with quotes all the values:
Per Yuri's suggestion, I found the culprit, and this is how (thanks to Google Support for walking me through this):
This showed me a graph making it clear just about all of my requests were coming from a credential named  datadog-metrics-collection , a service account I'd set up previously to collect GCP metrics and emit to Datadog.
Considering the answer posted and question, If we think we do not need Stackdriver monitoring, we can disable stackdriver monitoring API using bellow steps:
In addition you can view Stackdriver usage by billing account and also can estimate cost using Stackdriver pricing calculator [a] [b].
View Stackdriver usage by billing account:
4.Select Group By   SKU.
This menu might be hidden; you can access it by clicking Show 
  Filters.
You can also select just one or some of these SKUs if you don't want to group your usage data.
Note: If your usage of any of these SKUs is 0, they don't appear in the Group By   SKU pull-down menu.
For example, who use only the Cloud console might never generate API requests, so Monitoring API Requests doesn't appear in the list.
Use the Stackdriver pricing calculator [b]:
[a]  https://cloud.google.com/stackdriver/estimating-bills#billing-acct-usage
[b]  https://cloud.google.com/products/calculator/#tab=google-stackdriver
You need to  start  the publishing.
Compare with the  LoggingMeterRegistry
In your constructor something like:
Why not try?
It will return the docker container's hostname.
If you haven't set a hostname explicitly, using something like  docker run -h hostname image command  then it will return the docker host's hostname.
Alternatively, you could do this using a deployment tool like puppet, ansible, etc.
and template the file when you deploy the container.
See  HTTP response status codes
The categories are generally:
So 4XX errors are errors, but they indicate the client is likely at fault.
E.g.
The user went to a page or user agent made a request to a page that does not exist.
The server responds with 404 because &quot;Everything on my end is fine, but that page isn't real.&quot;
Is it an error?
Sure.
Could you potentially identify issues (e.g.
typos in links, missing pages, misspellings, malformed API requests, etc..) by routing these to your logs?
Sure.
Are you obligated to take action on it?
Not if you don't want to.
You're probably best to determine why you feel they are not actionable.
Most likely are actionable.
Micrometer uses  MeterFilter s registered with a  MeterRegistry  to modified the meters that are registered.
The modifications include the ability to map a meter's ID to something different.
In Spring Boot, you can use a  MeterRegistryCustomizer  bean to add a  MeterFilter  to a registry.
You can use generics to work with a registry of a specific type, for example  MeterRegistryCustomizer&lt;DatadogMeterRegistry&gt;  for a customizer that is only interested in customizing the Datadog registry.
Putting this together, you can map the ID of the  http.server.request  meter to  i.want.to.be.different  using the following bean:
There are some options you should consider:
don't update anything and just stick to Kubernetes 1.15 (not recommended as it is 4 main versions behind the latest one)
git clone  your repo and change  apiVersion  to  apps/v1  in all your resources
use  kubectl convert  in order to change the  apiVersion , for example:  kubectl convert -f deployment.yaml --output-version apps/v1
It is worth to mention that stuff gets deprecated for a reason and it is strongly not recommended to stick to old ways if they are not supported anymore.
If you are doing the setup in an organisation, datadog or prometheus is probably the way to go.
You can capture other Kafka related metrics as well.
These agents also have integrations with many other tools beside Kafka and will be a good common choice for monitoring.
If you are just doing it for personal POC type of a project and you just want to  view  the lag, I find CMAK very useful ( https://github.com/yahoo/CMAK ).
This does  not  have historical data, but provides a good  current  visual state of Kafka cluster including lag.
For cluster wide metrics you can use kafka_exporter ( https://github.com/danielqsj/kafka_exporter ) which exposes some very useful cluster metrics(including consumer lag) and is easy to integrate with prometheus and visualize using grafana.
Burrow is extremely effective and specialised in monitoring consumer lag.Burrow is good at caliberating consumer offset and more importantly validate if the lag is malicious or not.
It has integrations with pagerduty so that the alerts are pushed to the necessary parties.
https://community.cloudera.com/t5/Community-Articles/Monitoring-Kafka-with-Burrow-Part-1/ta-p/245987
What burrow has:
If you are looking for quick solution you can deploy burrow followed by the burrow front end  https://github.com/GeneralMills/BurrowUI
You could use the Java Admin Kafka API quite easily to expose this over command line or as an HTTP call quite quickly with Spring Boot (could avoid any metrics for each app individually since Admin API could do it for any group).
See link for an example:  https://gquintana.github.io/2020/01/16/Retrieving-Kafka-lag.html
Code taken from example above.
Get all groups:
Get highest offset in a set of partitions:
High level on connecting them together to get lag:
Use the supplied jconsole.sh script in bin, don't try and build up the classpath by hand.
You also need to use the custom service url.
See the docs for details
You can run datadog tracing on AWS Elastic Beanstalk with Flask by configure tracing manually as defined  here :
Custom metrics are on the roadmap for the APM agent, but we're still working on the exact schedule.
In the meantime you could either use the  JMX config options  of the agent with custom JMX key properties.
Or use the Elasticsearch output of Micrometer.
Maybe just change the Micrometer output as an interim solution and potentially switch to custom APM metrics once they are available?
There's also the option to get metrics with  Metricbeat from JMX / Jolokia , but that sounds like an even bigger change and not really a long-term upside.
One straight forward way I can think of in this case is to use the  .env  file for your docker-compose.
docker-compose.yaml  file will look something like this
.env  file for each stack will look something like this
and
for different projects.
Note:
You're making things harder than they have to be.
Your app is containerized- use a container system.
ECS is  very  easy to get going with.
It's a json file that defines your deployment- basically analogous to docker-compose (they actually supported compose files at some point, not sure if that feature stayed around).
You can deploy an arbitrary number of services with different container images.
We like to use a terraform module with the image tag as a parameter, but easy enough to write a shell script or whatever.
Since you're trying to save money, create a single application load balancer.
each app gets a hostname, and each container gets a subpath.
For short lived feature branch deployments, you can even deploy on Fargate and not have an ongoing server cost.
It turns out the solution involved capabilities from docker-compose.
In docker docs the concept is called  Multiple Isolated environments on a single host
to achieve this:
I used an .env file with so many env vars.
The main one is  CONTAINER_IMAGE_TAG  that defines the git branch ID to identify the stack.
A separate docker-compose-dev file defines ports, image tags, extra metadata that is dev related
Finally the use of  --project-name  in the docker-compose command allows to have different stacks.
an example docker-compose Bash function that uses the docker-compose command
The separation should be done in the image tags, container names, network names, volume names and project name.
The issue is that your library depends on  gcc  to run.
If you are running in a container, you can try two options:
You could also need  musl-dev  package, but you should try without it first.
Since MacOS and most Linux distros come with GCC, I guess you could be using Windows.
In this case, you need to install  MinGW .
I know this is old but I ran into this problem too, About Alexey answer, on windows, you should install MinGW and add the path to win environment.
You should follow  this .
In case MinGW did not work, you can install  this  one which worked perfectly for me on windows.
I had the same error and installing the .NET Framework 4.6.1 SDK ( https://dotnet.microsoft.com/download/visual-studio-sdks ) and restarting the Datadog Agent solved the problem
Use  context.Request.Path  conditionally if your  routeData  is null.
It is the closest I can think of since Identity Server 4 middleware has internal routing logic for the standard OAuth protocol routes.
After a few more days of research, discovered that:
The errors you are getting are coming from the remote computer, that is, the Heroku dyno.
You can't follow the instructions in the warning (to update bundler) as you can't run arbitrary instructions on their servers.
Heroku only support limited " carefully curated " versions of bundler.
Normally when the bundler versions don't match it just gives a warning, not an error, so you can  potentially  just ignore it.
Personally I like to eliminate warnings (or supress them if elimination isn't possible) so that when new warnings pop up I am more likely to notice them and deal with them.
That being said, I was not able to "downgrade" my Gemfile.lock from 2.0.1 to 1.15.2.
I had to first delete Gemfile.lock and then recreate it (presumably there are potentially breaking changes across these major versions).
I suspect this is the second problem you encountered.
The best way around these warnings/errors is to match your local version of Bundler to Heroku's carefully curated version.
That page above links to another page with the currently supported versions: 
 https://devcenter.heroku.com/articles/ruby-support#libraries
As of today that's version 2.0.1 for Gemfile.locks bundled with 2.x and 1.15.2 for everything else.
Could be merge conflicts in the Gemfile.lock.
Try running  bundle install  locally and see if it works before committing and pushing to heroku.
add bash script as userparameters in zabbix-agent.
Since it requires admin permissions, we can not give out UAA clients for the firehose.
However, there are different ways to get metrics in context of a user.
CF API
You can obtain basic metrics of a specific app by polling the CF API:
 https://apidocs.cloudfoundry.org/5.0.0/apps/get_detailed_stats_for_a_started_app.html
However, since you have to poll (and for each app), it's not the recommended way.
Metrics in syslog drain
CF allows devs to forward their logs to syslog drains; in more recent versions, CF also sends metrics to this syslog drain (see  https://docs.cloudfoundry.org/devguide/deploy-apps/streaming-logs.html#container-metrics ).
For example, you could use Swisscom's Elasticsearch service to store these metrics and then analyze it using Kibana.
Metrics using loggregator (firehose)
The firehose allows streaming logs to clients for two types of roles:
Streaming  all  logs to admins (which requires a UAA client with admin permissions) and streaming  app  logs and metrics to devs with permissions in the app's space.
This is also what the  cf logs  command uses.
cf top  also works this way  (it enumerates all apps and streams the logs of each app).
However, you will find out that most open source tools that leverage the firehose only work in admin mode, since they're written for the platform operator.
Of course you also have the possibility to monitor your app by instrumenting it (white box approach), for example by configuring Spring actuator in a Spring boot app or by including an agent of your favourite APM vendor (Dynatrace, AppDynamics, ...)
I guess this is the most common approach; we've seen a lot of teams having success by instrumenting their applications.
Especially since advanced monitoring anyway requires you to create your own metrics as the firehose provided cpu/memory metrics are not that powerful in a microservice world.
However, option 2. would be worth a try as well, especially since the ELK's stack metric support is getting better and better.
How are you spinning up ecs-agent container?
What is docker run command?.
Did you try like below?.
Yes, you can pass a script to the instance that will be executed on the first boot (but not thereafter).
It is often referred to as a  User Data script .
See:
If you wish to install  after  the instance has started, use the  AWS Systems Manager Run Command .
The simplest way to get access to data in Cloudyn is to configure a report and schedule data to be pushed to a storage account.
From there, you can use standard storage account APIs to access the data.
Instead of using Cloudyn APIs, however, I would recommend using the  Cost Management Query API  for aggregated cost/usage data or  UsageDetails API  for raw usage.
If you want to secure access to docker socket,  this docker documents  is a good start.
Spoke with Datadog support.
Very helpful but the short answer is that there is currently no option to add additional tags to specify the specific proc_name in the individual  gunicorn.yaml  file.
As a workaround to enable grouping we enabled unique prefixes for each application but the trade-off is that the metrics are no longer sharing the same namespace.
I've submitted a new feature request on the Github project which will hopefully be considered.
Response time is in  time  field.
There is an additional metric  latency  which provides time to first byte.
See:
You might also want to read :
Shared buffers are used for postgres memory cache (at a lower level closer to postgres as compared to OS cache).
Setting it to 7gb means that pg will cache to 7gb of data.
So if you are doing a lot of full table scans or (recursive) CTEs that may improve performance.
Note that  postgres  master process will allocate this entire amount at database startup, which is why you are seeing your OS use 10GB of ram now.
work_mem  is memory used for sorts and  each  concurrent sort allocates a bucket of this size.
Therefore this is only bounded by  max_connections  * concurrent sorts, so effectively it is  only  bounded by the sort complexity of your queries, so increasing this poses the most risk to system stability.
(That is, if you have a single query that the query planner executes with 8 merge sorts, you will use 8* work_mem  every time the query is executed).
maintenance_work_mem  is the memory used by  VACUUM  and friends (including  ALTER TABLE ADD FOREIGN KEY !
Increasing this may increase VACUUM speed.
wal_buffers  has no benefit beyond 16MB, which is the largest WAL chunk the server will write at one time.
This can help with slow write i/o.
See also:  https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server
It depends.
Also, your image doesn't display as of my answer.
If your machines are super memory hungry and you are an individual without unlimited income, I think your approach would be fine.
I would recommend a slightly higher arbitrary percentage to start with, such as 50%, to provide a bit of wiggle room.
Continue to analyze the memory usage and adjust your maximum accordingly.
I don't see any reason to set memory usage below default.
Otherwise, you can be much more gratuitous and provide 100-200% extra memory, in case your application experiences sudden heavy load.
As you can see  here , Ansible provides role dependecies.
You may create in  Datadog.datadog  role new directory named meta with main.yml file.
In  meta/main.yml  write
After that, when you call  Datadog.datadog  role, Ansible will run  role1  automatically before  Datadog.datadog  role.
If you create another role named  Datadog.datadog1  with the same  meta/main.yml  file and call roles  Datadog.datadog  and  Datadog.datadog1 , then Ansible will run  role1  only once, before running Datadogs roles.
For your data model, I would suggest adding   time   as a clustering column:
Use descending order to keep the latest metrics first.
You can then query using the LIMIT clause to get the most recent hour:
Or day:
Depending upon how long you plan to keep the data, you may want to add a  column for year, month, or days to the table to limit your partition size.
For example, if you wish to keep data for 3 months,  a  month  column can be added to partition your keys by id and month:
If you keep data for several years, use year + month or a date value.
Regarding your final question, about separate tables or a single table.
Cassandra supports sparse columns, so you can make multiple inserts in a common table for each metric without updating any data.
However, it's always faster to write just once per row.
You may need separate tables if you have to query for different metrics by an alternative key.
For example, query for disk usage by id and disk name.
You'd need a separate table or a materialized view to support that query pattern.
Finally, your schema defines an  assetid , but this isn't defined in your primary key so with your current schema you can't query using assetid.
Try
Finally, found the solution by examining services logs.
w32time service was configured as &quot;manual start&quot; instead of delay-auto or auto.
And though &quot;Set time automatically&quot; was &quot;On&quot;, clock synchronization only happened after starting the service and resyncing.
So I've changed the Startup type of the Windows time service from Manual to Automatic (Delayed Start) using the following command:
&amp;sc.exe config w32time start= delayed-auto
Disabled Time Synchronization (service task) in Task Scheduler.
Disable-ScheduledTask -TaskName &quot;SynchronizeTime&quot; -TaskPath &quot;\Microsoft\Windows\Time Synchronization&quot;
Time Skews are being fixed automatically and occur less often.
Typical work flow will look like this (there are other methods)
This can be achieved in multiple ways, easiest is to create a template with index pattern , alias and mapping.
Example: Any new index created matching the pattern  staff-*  will be assigned with given mapping and attached to alias  staff   and we can query  staff  instead of individual indexes and setup alerts.
We can use cwl--aws-containerinsights-eks-cluster-for-test-host to run queries.
Note: If unsure of mapping, we can remove mapping section.
FetchFollower acting like &quot;long poll&quot; request.
It waits till it gets  replica.fetch.min.bytes  data for replication or  replica.fetch.wait.max.ms  timeout (which is by default 500ms).
So it's basically ok, it's just means that most of FetchFollower requests are waiting for data
This is likely related to this  issue in the portmap plugin .
The current working theory is that a conntrack entry is created when the client pod reaches out for the UDP host port, and that entry becomes stale when the server pod is deleted, but it's not deleted, so clients keep hitting it, essentially blackholing the traffic.
You can try removing the conntrack entry with something like  conntrack -D -p udp --dport 8125  on one of the impacted host.
If that solves the issue then that was the root cause of your problem.
This workaround described in the GitHub issue should mitigate the issue until a fix is merged:
You can add an initContainer to the server's pod to run the conntrack command when it starts:
Some things I'd consider indicative of the health of the cluster are as follows:
Offline/Under Replicated Partitions : This is a good indicator as to whether all the nodes in a cluster are even online.
If one goes offline, you will almost certainly see some under-replication, and if several are offline, you might even see some offline partitions.
Active Controller : If this keeps changing, then it means that the cluster is potentially unstable.
The controller should not change regularly; if it does, then something is wrong with your cluster.
Bytes In/Out : These show that your cluster is able to send and receive data.
If these are lower than you'd expect, then it might imply that the cluster is undergoing some sort of network issue which would possibly impact the cluster health.
Hope this helps!
.strip()  for removing whitespace characters.
.replace('offset=', '')  for removing that string.
You should be able to chain them too.
How to extract the numeric value appeared after  offset= ?
Why i prefer regular expression?
Because even if the string contains other keywords, regular expression will extract the numeric value which appeared after the  offset=  expression.
For example, check for the following cases with my given example.
How to remove leading and trailing whitespace characters?
will remove all the leading and trailing whitespace characters such as \n, \r, \t, \f, space.
For more flexibility use the following
Reference: see this SO  answer .
The straightforward way is:
For example:
Example:
Up to you if you need to convert the output.
Since it looks like  StatsDClient  is an interface of some kind, it would make your testing effort easier to simply inject this dependency into your object.
Even if you're not using an IoC container like Spring or Guice, you can still somewhat control this simply by passing an instance of it in through the constructor.
This will make your testing simpler since all you realistically need to do is mock the object passed in during test.
Right now, the reason it's failing is because you're  new ing up the instance, and Mockito (in this current configuration) isn't equipped to mock the newed instance.
In all honesty, this set up will make testing simpler to conduct, and you should only need your client configured in one area.
You are getting things wrong here.
You don't use a  mocking  framework to test your "class under test".
You use the mocking framework to create  mocked  objects; which you then pass to your "class under test" within a test case.
Then your "code under test" calls methods on the mocked object; and by controlling returned values (or by verifying what happens to your mock); that is how you write your testcases.
So, your testcase for a MetricRecorder doesn't mock a MetricRecorder; it should mock the StatsDClient class; and as Makoto suggests; use  dependency  injection to put an object of that class into MetricRecorder.
Besides: basically writing "test-able" code is something that needs to be practiced.
I wholeheartedly recommend you to watch these  videos  if you are serious about getting in this business.
All of them; really (worth each second!
).
Kamon was being built for Java 1.7 by default.
Now, it will support 1.6
As told by @TRW in the comments, using this should do the trick:
I had to open a ticket asking the Heroku CS team to apply the &quot;pg_monitor&quot; role to my user.
They've granted the role and now everything is working fine
Maybe it's a complicated idea but I think you can make your own cache store wrapper that decides which cache store to use if I understand your question correctly.
When calling the cache method, it eventually calls  read_fragment  and  write_fragment  on your controller  https://apidock.com/rails/AbstractController/Caching/Fragments/read_fragment  and those methods call  cache_store.read  and  cache_store.write .
Then you could have a custom cache store class with custom  read  and  write  method that, depending on an option, delegates the read and write to real cache stores.
Then you use it like...
I'm not sure if that's what you are asking sorry.
Easy solution is to fetch this library directly and do  add_subdirectory .
But this requires cmake &gt;= 3.11.
Create dir  cmake  and file  cmake/cpp-datadogstatsd.cmake
cpp-datadogstatsd.cmake :
Then, include this cmake file, and link  DataDogStatsD_static  to your lib/exe:
As you have limited requirements, you could achieve this without a bot.
MS Teams has income and outgoing webhooks.
You could create a  Incoming webhook  inside a Teams channel.
It provides an URL which you could use inside the monitoring remote server and POST the message in JSON format to the webhook url.
It will be posted in teams channel like below
For sending message back to the server you need to configure the  Outgoing webhook  in the channel.
Spring Cloud Data Flow and Skipper servers are  Spring Boot  applications and hence you can configure/customize logging system based on your requirements.
Here are some of the references to configure logging system for a Spring Boot app:
docker logs  and similar just collect the stdout and stderr streams from the main process running inside the container.
There's not a "log level" associated with that, though some systems might treat or highlight the two streams differently.
As a basic example, you could run
The resulting file listing isn't especially "error" or "debug" level.
The production-oriented setups I'm used to include the log level in log messages (in a Node context, I've used the  Winston  logging library), and then use a tool like  fluentd  to collect and parse those messages.
Make sure  CreatedDate  is indexed.
Make sure  CreatedDate  is using the  date  column type .
This will be more efficient on storage (just 4 bytes), performance, and you can use all the built in  date formatting  and  functions .
Avoid  select *  and only select the columns you need.
Use  YYYY-MM-DD  ISO 8601 format .
This has nothing to do with performance, but it will avoid a lot of ambiguity.
The real problem is likely that you have thousands of tables with which you regularly make unions of hundreds of tables.
This indicates a need to redesign your schema to simplify your queries and get better performance.
Unions and date change checks suggest a lot of redundancy.
Perhaps you've partitioned your tables by date.
Postgres has its own built in  table partitioning  which might help.
Without more detail that's all I can say.
Perhaps ask another question about your schema.
Without seeing  EXPLAIN (ANALYZE, BUFFERS) , all we can do is speculate.
But we can do some pretty good speculation.
Cluster the tables on the index on CreatedDate.
This will allow the data to be accessed more sequentially, allowing more read-ahead (but this might not help much for some kinds of storage).
If the tables have high write load, they may not stay clustered and so you would have recluster them occasionally.
If they are static, this could be a one-time event.
Get more RAM.
If you want to perform as if all the data was in memory, then get all the data into memory.
Get faster storage, like top-notch SSD.
It isn't as fast as RAM, but much faster than HDD.
The answer to the question is found in the comments to it.
Hence, this question should not go unanswered.
The code from the question works as expected, however, the path where the named pipe resides is a special path and this is the reason why the data that is being sent to it never reaches the script.
The corresponding special casing in Bash for instance can be found in  redir.c .
The solution to the problem is to use a real UDP server on that port:
It turns out that someone had turned on a scheduled job that was sending a super expensive query that was supposed to be a singleton onto the query queue every 5 min.
The query takes 20 min to run, so eventually the system bogs down and falls over.
Apparently this was / is an issue with  rpy2 , which was a dependency of our project.
It was being imported by a utility module that was imported on startup.
This caused it to be called on every single request to our REST API endpoints.
Putting the import inside the actual function that was using it fixed this issue.
What you want is either the  is_match  or  is_exact_match  conditional variable, which are  documented here  (with examples).
The idea is that you can nest your messages  and notifications  in conditional logic arguments so that only when the monitor alerts/warns/resolves, or only when the evaluated tag scope matches certain conditions, will certain messages or notification channels be part of the alert.
So in your case you want your message to include something like this:
{{#is_exact_match "environment.name" "prod"}}
Add special prod message here
and @pagerduty or @pagerduty-foo
{{/is_exact_match}}
Add message that should always show up here
and @slack-bar
In this case, only when the "environment" tag's value is "prod" will the bracketed content be included (which includes the pagerduty notification).
The non-bracketed part will always be included (which includes the slack notification).
(?
:\/private\/toolbox\/)(.+)  ought to match your route path, capturing the wildcard as the first group:
I cannot speak to that RegExp's performance, however.
I can't speak specifically for the Java implementation, but in the CSharp client, the ability to send this data to Datadog is done to 127.0.0.1 via UDP port 8125.
It's on the same thread as your executing code and not asynchronous.
The whole effort by your process is finished once the UDP message is sent - it's fired and immediately forgotten.
The thread overhead you mention occurs in the separate Datadog agent process which is listening on the other end of UDP 8125, and has it's own thread pool and ability to buffer some data before sending up to Datadog's servers.
Do you have additional information that shows this behavior?
Based on what I know, this doesn't sound like a side effect of the Datadog/StatsD stuff.
I found the answer on Datadog's help forum:  "How to graph percentiles in Datadog" .
So the gist is that the latency itself didn't go up, but aggregating over multiple streams (where each stream corresponds to each custom tag) caused the graph to display a different shape.
Just use  NLog.MappedDiagnosticsLogicalContext.Set("userid", "someValue")  together with  ${mdlc:item=userid}  where needed.
See also  http://github.com/NLog/NLog/wiki/MDLC-Layout-Renderer
MappedDiagnosticsLogicalContext  uses  CallContext  (and  AsyncLocal  on NetCore) which are thread-safe.
Settings will also support async Task and follow to the chained tasks, but if scheduling a Time-callback based on a user-request, then the Timer-callback will not see the userid.
You should avoid changing  LogManager.Configuration.Variable  at runtime, they are global for all concurrent requests, and might get lost during configuration-reload (If autoreload configured).
If anyone will need the answer, this is how I did this.
It shows the biggest tables that were not last vacuumed in that past 2 weeks, but limits the list for 20 results.
If you want to you can change LIMIT 20 or delete it for shorter/longer list.
Also change pg.last_autovacuum for analyze or anything else from pg.stat table you want to check and also 2 week can be changed to whatever time period you want.
In datadog under postgres.yaml I added this:
Then I added this as a top-list inside a dashboard.
You can also make it as an alert and sum up the counts inside the metrics and decide what's your limit that about it you want to start vacuuming, although it's just recommended to run it regularly and not just when it's just too big, that's why we use this only as a list in our dashboard.
To get eyes only.
Datadog, like OMS and other monitoring software uses the Azure VM agent to steam the information.
Once this agent is installed on the system we are able to gather the info needed.
The VM agent is not something that goes out over the internet like other connections.
Hence, you should still see the reporting available.
Rather, it should be a direct connection from the Hyper-V manager and the VM itself.
This therefore, bypassing any NSG rules you would have in place.
I have installed data dog agent on one of my virtual machines
Datadog agent will collect system metrics and  forward  to Datadog.
Datadog agent works like this:
Also you can try to perform a network capture on your Azure VM, then we are able to find the detailed of the agent behavior.
Here is the network capture in my test VM:
We can find that  Datadog agent forward over HTTPS(443) to Datadog HQ .
After you deny port 443 in NSG outbound rules, the datadog will not get your metrics:
More information about datadog agent, please refer to this official  article .
Anything printed to STDOUT will be sent to your logging addons like SumoLogic.
The options you've shown should take care of that.
The mechanism that addons like SumoLogic use is call  Log Drains , and you can tap into that your self to get your log stream over HTTP.
hmm, so you're trying to use autodiscovery to find which container the dd-agent should be running the etcd check on?
and you're using the auto_conf files approach?
And there, you're wondering how to apply the  %%host%%  template variable?
If that's what you're interested in, I think you'll want to add it into your  etcd.yaml  on the  url  line, as shown in  the example file  like so:
When submitting histograms via dogstatsD you should be automatically creating 5 metrics as shown here:
dog.histogram(...)
Usage: Used to track the statistical distribution of a set of values over a statsd flush period.
Actually submits as multiple metrics:
Additional details on metric types and their submission sources can be found here:
https://help.datadoghq.com/hc/en-us/articles/206955236-Metric-types-in-Datadog
It appears for your use case  metric.count  would be the closest match for calculating the total length of your word.
Once selected, you can make use of the  as_count()  modifier which will calculate the total count rather than the average over the flushing period.
More information on this use case can be found here:
https://help.datadoghq.com/hc/en-us/articles/204271195-Why-is-a-counter-metric-being-displayed-as-a-decimal-value-
If you find yourself still running into any issues with this submission feel free to reach out to support@datadoghq.com
You can modify the following according to your needs.
What this basically does, is that it prevents  print()  from writing the default end character ( end='' ) and at the same time, it write a carriage return ( '\r' ) before anything else.
In simple terms, you are overwriting the previous  print()  statement.
the naive solution would be to just use the total amount of rows in your dataset and the index your are at, then calculate the progress:
This will only be somewhat reliable if every row takes around the same time to complete.
Because you have a large dataset, it might average out over time, but if some rows take a millisecond, and another takes 10 minutes, the percentage will be garbage.
Also consider rounding the percentage to one decimal:
Printing for every row might slow your task down significantly so consider this improvement:
There are, of course, also modules for this:
progressbar
progress
You could use Datadog's Outlier detection to identify instances which exhibit behavior outside the normal for it's peer set.
As an example, you could create an outlier detection monitor:
http://docs.datadoghq.com/guides/outliers/#alerts
Which would be scoped to a system metric like  aws.ec2.cpuutilization  and be alerted if any host spiked abnormally or had very low utilization in comparison to its group.
There are some additional blog posts which discuss the use of the algorithms that can be found here:
https://www.datadoghq.com/blog/introducing-outlier-detection-in-datadog/
https://www.datadoghq.com/blog/outlier-detection-algorithms-at-datadog/
https://www.datadoghq.com/blog/scaling-outlier-algorithms/
That said, if you find yourself needing additional assistance with outlier detection you can always reach out to the Support team at support@datadoghq.com or by using the internal support features found here:
https://app.datadoghq.com/help
Hope this helps!
I have found that  Blackfire  is doing the trick.
Seems to be relatively easy to install and can run it free locally.
Datadog Tags are generally strings, but also support &quot;key:value&quot; strings, which is most useful, since then the  key  can act as a dimension.
There's no support that I know of that allows for a single key with multiple values, so I don't think Datadog will support the syntax you're attempting.
You  may  want to try:
in your config.
General reference here:  https://docs.datadoghq.com/getting_started/tagging/
You're welcome to go look through the source code yourself , but generally my comment is correct.
Nest binds all route handlers and enhancers (guards, interceptors, pipes, and filters) as a large anonymous function, in a very abstract way (does the same thing for Fastify as far as I can tell).
You can use Fluentd as a  daemonset  on your cluster.
see this repo and docker images -&gt;  fluent/fluentd-docker-image
and use this filter to add  Kubernetes metadata  to every log collected by Fluentd and then use a grep filter to exclude logs that are not in your namespaces.
something like this:
did u try bulk publish?
publish(topic,[]Message{1,2,3,4,.....})
I believe your requirement can be accomplished using cmdlet  Invoke-AzVMRunCommand  /  Invoke-AzureRmVMRunCommand  or  Set-AzVMCustomScriptExtension  /  Set-AzureRmVMCustomScriptExtension .
Related scripts can be found  here  and  here .
Just FYI,  this  and  this  are actual references for the above information.
Hope this update helps!
This is possible by adding the annotation below to nginx ingress:
See full answer at  https://github.com/DataDog/dd-opentracing-cpp/issues/118
Turn on the "general log" and have it write to a file.
Wait a finite amount of time.
Then use  pt-query-digest  to summarize the results.
Turn off the general log before it fills up disk.
The slowlog (with a small value in  long_query_time ) is more useful for finding naughty queries.
The issue was the size of the HTTPRequest was to large the higher the parallelism which makes sense.
I was getting back "Request Entity Too Large" however the exception wasn't logging out correctly so I missed it.
It seems that the Flink  DatadogHttpReporter  does not take the size of the request into consideration when building it.
I modified the Reporter to limit the number of metrics per request to 1000.
Now the metrics are showing up just fine.
This becomes pretty easy with Datadog's Log Management product -- you can measure lots of things by endpoint, including hits, unique client-ip count, latency (if you add response time to your nginx logs).
More info on the setup and these use cases  in this blogpost .
Documentation on the logs part of the nginx integration  here .
This is definitely possible, but you will want to change your tag setup a little.
You want to take advantage of  key:value  syntax with your tags, so that you can group out the tags by their common  key .
So in your case, instead of tagging by  entity.count.payment , you would want to tag by  entity.count:payment  or better yet  entity:payment .
That way you can write one query of your metric and use the  group by  functionality on the shared  entity  tag key to see it's values for all the different  entity  tags.
From there, you can use the  top  function to always see just the top n values, whether that be  payment  or  cart  or  visit  etc.
This doc here about tags  is definitely worth a read!
Tags can make graphing and monitoring much easier and more scalable.
It is necessary to  add AspectJ weaver as Java Agent  when you're starting your Akka aplication:  -javaagent:aspectjweaver.jar
You can add the following settings in your project SBT configuration:
So AspectJ weaver JAR will be copied to  ./lib_managed/jars/org.aspectj/aspectjweaver/aspectjweaver-[aspectJWeaverV].jar  in your project root.
Then you can refer this JAR in your Dockerfile:
Few things to debug
You can easily get all needed data via querying dmv and other resources inside SQL Server.
Good start is  here .
I have 35 Cassandra nodes (different clusters) monitored without any problems with graphite + carbon + whisper + grafana.
But i have to tell that re-configuring collection and aggregations windows with whisper is a pain.
There's many alternatives today for this job, you can use influxdb (+ telegraf) stack for example.
Also with datadog you don't need grafana, they're also a visualizing platform.
I've worked with it some time ago, but they have some misleading names for some metrics in their plugin, and some metrics were just missing.
As a pros for this platform, it's really easy to install and use.
We have a cassandra cluster of 36 nodes in production right now (we had 51 but migrated the instance type since then so we need less C* servers now), monitored using a single graphite server.
We are also saving data for 30 days but in a 60s resolution.
We excluded the internode metrics (e.g.
open connections from a to b) because of the scaling of the metric count, but keep all other.
This totals to ~510k metrics, each whisper file being ~500kb in size =  ~250GB.
iostat tells me, that we have write peaks to ~70k writes/s.
This all is done on a single AWS i3.2xlarge instance which include 1.9TB nvme instance storage and 61GB of RAM.
To fully utilize the power of the this disk type we increased the number of carbon caches.
The cpu usage is very low (&lt;20%) and so is the iowait (&lt;1%).
I guess we could get away with a less beefy machine, but this gives us a lot of headroom for growing the cluster and we are constantly adding new servers.
For the monitoring: Be prepared that AWS will terminate these machines more often than others, so backup and restore are more likely a regular operation.
I hope this little insight helped you.
It looks like that you have not set your JMX_PORT for kafka from where your datadog agent can listen information abouot the metrics.
Restart your Kafka with the following additional key/value pair parameter:
'JMX_PORT=9999'
$ JMX_PORT=9999 ./kafka-server-start.sh ../config/server.properties
This error essentially means that the Datadog Agent is unable to connect to the Kafka instance to retrieve metrics from the exposed mBeans over the RMI protocol.
This error can be resolved by including the following JVM (Java Virtual Machine) arguments when starting the Kafka instance (required for Producer, Consumer, and Broker as they are all separate Java instances)
please
Please read this article
Nexus 3.0.1 exposes authenticated access to metrics using  http://metrics.dropwizard.io/3.1.0/manual/servlets/ 
You have these endpoints available for different purposes:
 
        {host:port}/service/metrics/healthcheck
        {host:port}/service/metrics/data
        {host:port}/service/metrics/ping
        {host:port}/service/metrics/threads
I recall the default behavior being that each gear can handle 16 concurrent connections, then auto-scaling would kick in and you would get a new gear.
Therefore I would think it makes sense to start by testing that a gear works well with 16 users at once.
If not, then you can  change the scaling policy  to what works best for you application.
BlazeMeter  is a tool that could probably help with creating the connections.
They mention 100,000 concurrent users on that main page so I don't think you have to worry about getting banned for this sort of test.
spring.sleuth.baggage.correlation-fields  automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.
I suppose you use Sleuth out of the box (uses Brave):
The  spring.sleuth.baggage.correlation-fields  property automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.
Also, using  MDCScopeDecorator , you can set the baggage values to Slf4j’s MDC programmatically, you can see how to do it in  Sleuth docs :
We figured this out in the comments, I'm posting an answer that summarizes it all up: it seems the root cause was using different versions of different spring-boot modules.
It is a good rule of thumb to not define the versions yourself but use BOMs and let them define the versions for you, e.g.
see:  spring-boot-dependencies .
This way you will use the compatible (and tested) versions.
management.metrics.tags.your-tag  is the way to add tags to all of your metrics.
A good way to check this is looking at  /actuator/metrics .
I ended up going with the  sbt-javaagent  plugin to avoid extra code to exclude the agent jar from the classpath, which the plugin handles automatically.
The trick/hack was to filter out the default  addJava -javaagent  line the  sbt-javaagent  plugin adds automatically , and then appending a new script snippent to only enable the javaagent when a certain env.
variable is set.
You can use simple  jcmd  command line tool
As an example of running this on my simple Clojure application:
As shown in the documentation in  your link , WHL files are also supported.
It says:
You might already have one or more Python libraries packaged as an .egg or a .whl file.
There is a .whl file available for the DataDog python library here:  https://pypi.org/project/datadog/#files .
You might try downloading that file, uploading it to your S3 bucket, and using that as your Python library for your Glue job.
You might be more successful using that than trying to build your own .egg file.
The AWS API calls to start a task are:
StartTask :
Starts a new task from the specified task definition on the specified container instance or instances.
RunTask :
Starts a new task using the specified task definition.
You can allow Amazon ECS to place tasks for you, or you can customize how Amazon ECS places tasks using placement constraints and placement strategies.
Since this is AWS API calls, there are equivalent calls in CLI and SDK.
A colleaque of mine informed me that, since we're using docker, we can by-pass supervisor and just run the horizon artisan command directly as the entrypoint of the container.
So, I removed all things related to supervisor and my service yaml is simplified to the following and the logs are coming into datadog:
you can open it by navigating to the directory it is in, and then typing
You need root permissions to view the file as far as I know.
Replace  key  and  value  to what you want to use.
In my case  key  is "testKey" and  value  is "testValue"
it it my full sample code and xml configuration info.
code
log4j2.xml
output
You are configuring the filter on a new  StatsdMeterRegistry .
When using a  MeterRegistryCustomizer  you need to operate on the registry that was passed in.
Since the customizer will be used against all registries, you also would need to add an if statement to only filter against the registry you want filtered.
It doesn't have Grafana support yet (coming in a week or 2) but Questdb supports traditional SQL on a Time Series database and might be able to do what you want.
https://questdb.io  or  https://github.com/questdb  on GitHub.
Datadog can process logs through their pipeline fitering feature
https://docs.datadoghq.com/logs/processing/pipelines/
If you already have an attribute that contains the url, a really easy way to do this would be to use the  processing pipelines  and add a processor of the " url parser " type to these logs.
You just plug in the attribute that contains the url and an attribute path that you'd like to contain all the outputs from it (usually  http.url_details ), and then all new logs will get the extra url parsing applied.
If your logs have the "source:nginx" applied to them (configured in the log shipper), then you'll already have an out-of-the-box Nginx processing pipeline that Datadog has for structuring standard Nginx syntax logs.
You can clone that and then just add your new url parser there.
Or, if your syntax is similar to the standard syntax, you can just modify their default suggested parsers (in the cloned pipeline).
In any case, it'd be worth looking at that default pipeline for inspiration for other valuable things to do beyond url parsing.
Assuming you'd like the output to look like the following:
you need to escape the  {  and and use  \"  instead of  \' :
Unfortuanetly I can not propose an exact solution/workaround to you but you might have a look at the following documentations/API's:
Indices Stats API
Cluster Stats API
Nodes Stats API
The cpu usage is not included in the exported fields but maybe you can derive a high cpu usage behaviour from the other fields.
I hope I could help you in some way.
You can reference  this doc  to find where the default logging path is for Jenkins depending on your OS.
(For linux, it's  /var/log/jenkins/jenkins.log  if you don't configure it to be something else.
Then as long as your  Datadog agent  is v6+ you can use the Datadog agent to tail your jenkins.log file by following  this doc .
Specifically, you'd add this line to your  dadatod.yaml :
and add this content to any old  conf.yaml  file nested in your  conf.d/  directory, such as  conf.d/jenkins.d/conf.yaml :
Then the agent will tail your log file as it's written to, and will forward it to your Datadog account so you can query, graph, and monitor on your log data there.
Once you have the logs coming in, you may want to write a  processing pipeline  to get the critical attributes parsed out, but that would be material for a new question :) .
This command will only take a split second.
You must have spent 35 minutes waitung for the  ACCESS EXCLUSIVE  lock on the table to be granted (all the while blocking any transaction unfortunate enough to be queued behind you).
You probably have a problem with long transactions.
Normally they should be as short as possible, otherwise they hold locks for a long time and also keep  VACUUM  from cleaning up dead row versions.
The lock is necessary, but is should not pose a problem with a well behaved database workload.
You can create one PowerShell script to execute your Batch scripts remotely.
And Even you can schedule your PowerShell script using Windows Task Scheduler which will run as per your settings.
Rubber Duck.
Turns out because we changed  .set  to  .default , we lost the ability to have the variables properly set during the first run.
.normal  will do it for us.
Lots of threads are in WAITING state, and it's absolutely ok for them.
For example, there are thread which have the following stack trace:
This only means threads are waiting for any tasks to do.
However, other stacks do not look good.
Those threads are waiting for connection to be free in the pool.
C3P0 is a pool of database connections.
Instead of creating a new connection every time, they are cached in the pool.
Upon closing, the connection itself is not closed, but only returned to the pool.
So, if hibernate for some reason (or other user) do not close connection after releasing it, then pool can get exhausted.
In order to resolve an issue, you have to find out why some connections are not closed after using.
Try to look at your code to do this.
The other option is to temporarily go without C3P0 (pooling).
This is not forever, but at least you can check whether this guess is right.
In Grails 3, You should put the below code to  grails-app/conf/spring/resources.groovy :
Neither, you want something like this I think:
Also putting the key into node attributes like that is very unsafe and kind of defeats the point of encrypted bags since node attributes are all written back to the Chef Server and so the key will be sent unencrypted.
I still don't know why it makes a difference, but adding the  -4  option made it work
Here's the man page on the option:
-4      Forces nc to use IPv4 addresses only.
Problem in this case is not running scripts via JMeter GUI.
Instead it is related to network.
I had a similar distributed setup in EC2-environment and I successfully executed heavy load tests in GUI mode.
In my case, all my JMeter (master/slaves) were running on EC2 instances (windows environment).
So, I will recommend you to setup your  JMeter   (Master)  on EC2 and run scripts via GUI mode.
If you still want to run in command line mode then you simply need to pass command to create jtl file while the script runs on command line.
Later on you can use this JTL to generate any JMeter report as per requirement.
For more details check..
Jmeter - Run .jmx file through command line and get the summary report in a excel
jmeter -n -t /path/to/your/test.jmx  -l /path/to/results/file.jtl
Please refer to Dmitri answer in following question to reduce JTL size.
How can we control size of JTL file while running test from Non GUI Mode
Before implementing the code, you need to look around in Widows "registry" using "regedit" and find the exact registry key value for the software.
Below example shows, how to fetch the version number of "internet explorer".
Also recommended to have basic knowledge on Ruby array and hash, to understand the code
I've used registry_key_XXXXX Chef methods.
Note: Registry key entry may differ for Windows 32bit and 64bit
The problem is your  sendData()  function.
This function is called in your for loop and has the following line:
This line will create a new DataDog client, which uses a Unix socket.
This explains your error message.
With every iteration of your loop, a new socket is &quot;allocated&quot;.
After a sufficient amount of loops no sockets can be opened, resulting in:
socket: too many open files
To fix this you should create the client only once and pass it to your method as parameter.
The variable  $LASTEXITCODE  will give you the exit code of the last native command (executable) that was run.
I'm not familiar with the program, but I've had to solve a problem like this before.
I'm making the assumption that you're always going to start the output you want with a line 'Dogstatsd', and always end with several equals signs.
Based on that, you could script out your output like this:
We get the values defining the length of the file, the length until we hit the first line you want, the length where the output ends, and trim accordingly.
I would strongly suggest to setup a pre-production environment and run load tests (with tools like  JMeter ) in conjunction with server-side monitoring.
Tomcat backends can be monitored using the JMX protocol.
You have 2 solutions :
Like always, free software costs nothing but your time, and paid software gets you straight to the issue in exchanges for some pennies.
As it is reported on the official site it could be released in the future.
Some of the features we are very excited to provide in the near future
  include distributed tracing and providing framework-specific
  information (e.g., route change times) for some of the frontend
  frameworks such as React, Angular, Vue.js, etc.
https://www.elastic.co/blog/elastic-apm-rum-js-agent-is-generally-available
For now you can rely on Elastic APM RUM JS Agent using JS tags:
https://www.elastic.co/guide/en/apm/agent/js-base/3.x/getting-started.html#using-script-tags
You can try to increase:
Please take a look on documentation:  Tune APM Server
You can attach your ElasticApmAttacher.attach() in the Spring Application main class
For a SpringBootApplication packaged as a war file, and deployed to Tomcat server, this can be added to the configure method
Below code might help:
unfortunately oracle is not supported by elastic apm agent.
you should wrap your  oracleQueryRunner  in order to start and end agent spans manually.
put this code in your  main.ts  file:
It's true, there isn't an Elixir agent for Elastic APM - you can upvote  this issue  to get the topic more attention.
As you discovered, you can use the OpenTelemetry in the meantime.
To do that, run the OpenTelemetry contrib collector (otel collector) and configure it to export to  elastic  - there is a full explanation  in the docs  along with this sample configuration:
In your application, configure the tracer use the  opentelemetry exporter .
At that point you'll have a tracer in your application sending traces to the otel collector.
From there, traces will be exported to the Elastic Stack via APM Server.
In summary:  your app -&gt; otel collector -&gt; apm-server -&gt; elasticsearch
The  Erlang/Elixir Agent Docs  have sample code for starting and decorating spans.
transaction.duration.us  should indeed be what you're looking for.
It's the duration in microseconds as an integer.
Divide it by 1000 to get milliseconds, or by 1'000'000 to get seconds.
https://www.elastic.co/guide/en/apm/server/7.9/exported-fields-apm-transaction.html#_duration_2
When you bring up containers using compose, each container has its own networking stack (so they can each talk to themselves on  localhost , but they need an ip address or dns name to talk to a different container!
).
Compose by default connects each of the containers to a default network and gives each a dns name with the name of the service.
If your compose file looks like
A process in the  apm  container could access elasticsearch at  http://elasticsearch:9200
If you are starting all the services with single docker compose file, the app-server.yaml should have the value like this
 
output:
  elasticsearch:
    hosts: elasticsearch:9200
 
The "hosts: elasticsearch:9200" should be service name of the elasticsearch you mentioned in the docker-compose.
Like in the followiing

    version: '2'
    services:
      elasticsearch:
          image: elasticsearch:latest
Answer to the Errors - I have custom resole.extensions in the  webpack.config.js :
That was missing the default  .json :
Now only the warnings are left:
I addressed them to the developer:  https://github.com/elastic/apm-agent-nodejs/issues/1154
To listen on  0.0.0.0  try:
Try using a configuration:
where the file apm-server/config/apm-server.yml has your config content:
Note the rum.allow_origins option that you can configure to resolve the CORS issue.
https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html
you have to modify your elastic APM python agent code.
In general, you can add labels to your span example
also, you can add directly to span object.
You will get the ip from request object flask
request.remote_addr   set this to the desired key.
more details of APIs on elastic APM python agent can be found here -  https://www.elastic.co/guide/en/apm/agent/python/current/api.html
Thanks
These metrics come directly from  java.lang.management.GarbageCollectorMXBean .
The value of the  jvm.gc.time  metric is taken from  GarbageCollectorMXBean.getCollectionTime , which is indeed accumulating since the process started.
Assuming you're looking at metrics from a single JVM, there are a couple of possible reasons for why the value would appear to have gone backwards:
If the process had restarted (which I expect you would know about anyway), the metrics documents in Elasticsearch would have different values for the field  agent.ephemeral_id .
The more likely answer is that you're seeing values for two different memory managers/GC generations, in which case the metrics documents in Elasticsearch would have different values for the field  labels.name .
You don't end  trans1  and  trans2 .
Just put these 2 lines to the point where these end, and everything should show up fine:
There is the  CaptureTransaction , which is a convenient method that can wrap you any code and makes sure the transaction is ended and all exceptions are captured - so you use that method and it does "everything" for you.
Then there is the  StartTransaction  method - this is the one you use in your code -, which starts the transaction and does not do anything else.
The advantage here is that you get an  ITransaction  instance which you can use wherever and whenever you want.
But in this case you need to call  .End()  on it manually once the transaction (aka the code you want to capture) is executed.
Same with  CaptureSpan  and  StartSpan .
So you used  CaptureSpan  for your spans, so those where ended automatically when the lambda with  Task.Delay  finished, on the other hand you started your transactions with  StartTransaction  but only called  .End()  on  trans3  and not on the 2 other transactions.
There is some explanation with a demo  here  - sample code of that demo is  here .
Currently background services are not captured out of the box.
What you can do is to use the  Public Agent API  and with a little bit of an additional code you can capture those also as transactions.
Something like this in the background service:
I run via docker-compose elasticsearch, apm, kibana and tomcat application in docker.
In apm- -transaction-  index exist this meta information:  container.id .
And in apm- -metrics-  index this information is also stored.
Try to look at json structure at Discover tab by index pattern "apm-*"
enter image description here
did you try to give permissions to folder /opt/elastic ?
It's hard to say without knowing exactly how you're using NestJS and GraphQL together, and without knowing which parts of Apollo Server that NestJS itself uses.
Seeing a small sample of what  I am using the graphql feature of nestjs  means would be useful.
Here's a few datapoints that might help you narrow things down.
Also, opening  an issue  in the Agent repository or  a question in their forums  might get more of the right eyes on this.
The Elastic APM instrumentation for Apollo Server works by wrapping the  runHttpQuery  function of the  apollo-server-core  module, and  marking the transaction with  trans._graphqlRoute .
When the agent  sees this  _graphqlRoute  property , it runs some code that will set a default name for the transaction
In your application, either the  _graphqlRoute  property isn't getting set, or the renaming code above is doing something weird, or something about NestJS comes along and renames the transaction after it's been named with the above code.
Knowing more specifically  what  you're doing would help folks narrow in on your problems.
The metrics shown in Kibana are sent by the APM agent that like you said has limited access to your environment.
It basically says anything that is collected by the JVM running your JAR.
If you want to get further visibility into the CPU details of your local environment then you must augment your setup using  Elastic MetricBeats  that ships O.S level details about your machine that sees beyond what the JVM can see.
In the presentation below I show how to configure logs, metrics, and APM altogether.
https://www.youtube.com/watch?v=aXbg9pZCjpk
currently,we run apm-agent for java application, but after 1 day, server cpu has over 100% and java application killed by system
I am not familiar with what is Elastic APM, but if it says it supports Spring Boot, then it means it supports any spring-boot-based framework which Spring Cloud Stream is.
The  JVM GC metrics tracked  right now are  jvm.gc.alloc ,  jvm.gc.time , and  jvm.gc.count .
If you are looking for additional ones, which ones would those be?
And could you  open an issue with the details .
Please import from saved objects option -  https://github.com/elastic/apm-contrib/blob/master/apm-agent-java/dashboards/java_metrics_dashboard_7.x.json
To include spans into the transactions you should start the spans from the transaction object

 
 ...
var span = transaction.startSpan('My custom span')
...
And ending the parent transaction object all the nested spans will be also ended in cascade
https://www.elastic.co/guide/en/apm/agent/js-base/4.x/transaction-api.html#transaction-start-span
You can start your application with argument  active=false .
C:\Users\dsm\Documents&gt;java -jar apm-agent-attach-1.9.0-standalone.jar --pid 16832 --args 'service_name=test;server_urls=http://localhost:8200;active=false'
You simply need to change your query to this:
The accepted answer no longer works, you can use the following
The key features of APM agents is normally in their framework integrations.
The Java APM agent is mostly focussed on web frameworks — see the list of  supported technologies .
But you already mentioned the  public API  — if you manually instrument your code with that, you will still be able to use it.
It just doesn't automatically understand the framework and you need to help it with that.
Alternatively, if your tool supports OpenTracing then you could use the  OpenTracing bridge  for that.
When routing to a different subpage you have to set the Route Name manually.
You can achieve this via a filter on the 'change-route' type.
See  apm.addFilter()  
docs:  https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#apm-add-filter
Something like this should work:
Another way to do it would be to observe for transaction events which is fired whenever transaction starts and ends.
API docs -  https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#observe
The agent also supports frameworks such as Vue.js, React and Angular out of the box, so the above code should not be necessary.
Vue docs -  https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html
Logging is separate from APM / tracing, but can be integrated.
https://github.com/elastic/ecs-logging-java  is a curated logging library that will also correlate the trace IDs, so you can tie both together.
Keep using SLF4J and just add the right logging backend.
The output can then be picked up by Filebeat (as described in the repository) and you're ready to go.
Elastic-apm-agent-java automatically capture exceptions when you use slf4j implementation  Logger#error("message", Throwable) .
More information you can find  here
Try  this  official docker-compose set up:
The Elastic RUM agent has support for  click user interactions , therefore you shouldn't need to manually start these type of transactions.
Regarding the failure in your code the correct API call is  getCurrentTransaction  and not  currentTransaction .
Hope this helps.
So, I'm guessing that the handler wrappers are dropping the buffalo.Context information.
That's correct.
The problem is that  buffalo.WrapHandler  ( Source ) throws away all of the context other than the underlying  http.Request / http.Response :
So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?
I can see two options:
There's an open issue in the Elastic APM agent for the latter option:  elastic/apm#39 .
There are basically 2 ways the agent captures things:
In a typical ASP.NET Classic MVC application the agent has auto instrumentation for outgoing HTTP calls with  HttpClient , Database calls with EF6 (Make sure to  add the interceptor ) ( SqlClient  support is already work-in-progress, hopefully released soon).
So unless you have any of these within those requests, the agent won't capture things out of the box.
If you want to capture more things, currently the way to go is to place some agent specific code - so basically manual code instrumentation - into your application and use the public agent API.
Finally noticed issue.
application API is returning service name as Bootstrap because.
Id is not set so it is using default value
The Id can be set like this
You can try the method described here  disscuss-elastic , via ElasticApmAttacher#attach(map of properties).
Is this not yet included in the .NET agent, or is there additional configuration necessary to get this working?
This is not yet included in the .NET Agent unfortunately.
WebFlux can run on Servlet containers with support for the Servlet 3.1 Non-Blocking IO API as well as on other async runtimes such as  Netty  and Undertow.
Each Spring Boot web application includes an embedded web server.
For reactive stack applications, the  spring-boot-starter-webflux  includes Reactor Netty by default I guess.
And it does not include  Servlet API  (Netty is non-Servlet runtime), but it looks like your  Elastic APM  expects this API to be present.
Try to use  spring-boot-starter-tomcat  instead of Netty.
When  switching to a different HTTP server , you need to exclude the default dependencies in addition to including the one you need.
Here is an example:
Tomcat dependency brings Servlet API.
Perhaps it will resolve your issue.
Looks like there is no support from elastic for WebFlux yet
Check here  https://github.com/elastic/apm-agent-java/issues/60
They are currently working on it, but there is not a date to be ready yet
I got the answer after posting the same on  Elastic Support Forum .
It was a very prompt response.
This was not a problem from Elastic APM side, and was more of a silly problem from my side.
Refer the  discussion  to find the problem and solution.
This question was cross-posted to discuss.elastic.co, and you can see the answer that was provided there:  https://discuss.elastic.co/t/elastic-apm-python-system-metrics-dont-show-process-related-metrics-like-memory-on-kibana/240531/2?u=basepi
In your ES Cloud console, you need to Edit the cluster configuration, scroll to the APM section and then click &quot;User override settings&quot;.
In there you can override the target index by adding the following property:
Note that if you change this setting, you also need to modify the corresponding index template to match the new index name.
currently elastic-apm-agent support natively Quartz framework(since 1.8).
If you use it, instrumentation should work.
But you should add your packages to  application_packages .
It would be good if you can share mini-demo project.
And I can reproduce your problem locally.
Information from  supported-technologies-details
All of the Elastic APM agents, with the exception of the RUM JavaScript agent, have a  verify_server_cert  configuration variable.
You can set this to  false  to disable server TLS certificate verification.
I am using docker-compose
and the following option doesn't work.
I think its a bug for apm..
When I use same option in apm-server.yml it works fine.
One thing you can use is the  Filter API  for this.
With that you have access to all transactions and spans before they are sent to the APM Server.
You can't run through all the spans on a given transaction, so you need some tweaking - for this I use a  Dictionary  in my sample.
Couple of thing here:
Looks like it can be done using drop_event processor in api-server.yml.
and in code set custom context:
Commenting out  'SERVER_URL': '127.0.0.1:8200'  solved the problem.
I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.
Yes, this is possible and there is an API for it.
This part of the documentation  explains it.
So you'll need to do this when you start your transaction - I imagine in your scenario this will be when you read a message from RabbitMQ.
When you  start the transaction  there is an optional parameter called  distributedTracingData  - if you pass it, then the transaction will reuse the traceid which you passed through RabbitMQ, and this way the new transaction will be part of the whole trace.
If you don't pass this parameter, a new traceid will be generated and a new trace will be started.
Another comment that may help: you pass the trace id into the method where you start the transaction and each span will inherit this trace id within a transaction - so you control this on the transaction level and accordingly you don't pass it into a span.
Here is a small code snippet on how this would look:
@gregkalapos, again thank you for the information.
I checked how to acquire the neccessary trace information as in  node.js agent documentation  and when I debugged noticed that it was the trace id.
Next in the C# consumer end I placed a code snippet as mentioned in the  .Net agent  and gave it a run.
Kibana displayed the transactions from two different services in a single trace as I hoped it would.
Initially asked this questions because Visual Studio did not show the source as expected in the editor.
So what I did was clicked Build after right clicking on the .sln (solution) then noticed that necessary version of .Net SDK was not there and version of MSBuild related to it.
Once I updated Visual Studio the sources were visible.
Hope this would help if someone faced a similar situation.
Using labels should be the best way to add custom details to the transaction/span but you can also use the  addCustomContext()  method:
https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-add-custom-context
That error indicates the agent can't connect to apm-server.
SERVER_URL  should be  ELASTIC_APM_SERVER_URL  in the apm-agent-container env.
Thanks for the reply, I'm able to connect apm-server with the agent, but in kibana dashboard, I'm getting &quot; No data has been received from agents yet&quot; .
My application is running fine
Good job so far.
Your pipeline is almost good, however, the grok pattern needs some fixing and you have some orphan curly braces.
Here is a working example:
Response:
Just note that the exact date is missing so the @timestamp field resolve to January 1st this year.
Try to specify the  config_file  using the following notation:
-Delastic.apm.config_file=elasticapm.properties
The attacher can create the log file depending on the settings configured during startup.
See the [1] current code for a better understanding.
[1]  https://github.com/elastic/apm-agent-java/blob/0465d479430172c3e745afd2ef5b62a3da6b60aa/apm-agent-attach-cli/src/main/java/co/elastic/apm/attach/AgentAttacher.java#L79
Do you mean that you need new "Transaction type"?
If yes, so you should set  type  annotation parameter.
But @CaptureTranscation annotation work  in case :
I worked with the Elastic APM team, who had just rolled out this package:  https://www.npmjs.com/package/elastic-apm-node
The directions are pretty self-explanatory, works like a charm.
Hard to tell without debugging but since some connections are getting dropped when you add more load + concurrency it's likely that you need more replicas on your  Kubernetes deployments  and possibly adjusts the  Resources  on your container pod specs.
If this turns out to be the case you can also configure an  HPA  (Horizontal Pod Autoscaler) to handle your load.
Are you not building a custom Dockerfile and you could just add it there (using wget or curl probably)?
If you really want a build dependency,  https://search.maven.org/artifact/co.elastic.apm/elastic-apm-agent/1.7.0/jar  should be what you want.
PS: IMO it's a feature that this is only a runtime dependency and you can just add, remove, change it independently of your application; unless you want to do some custom instrumentation.
If the objective is to attach labels to a transaction over multiple spans then using the  public APIs from the Elastic APM for Java  is a better choice instead of instrumenting the JVM with ByteBuddy.
You will have much more freedom to do what you want to do without relying on a hacking.
FYI, the Elastic APM agent for Java already instrument the JVM with additional bytecode so what you are doing may get even more confusing because of this.
Alternatively, you can also use the  OpenTracing Bridge  to set labels in a transaction.
Disclaimer: This answer is a stub for now.
You should first try to explore a more canonical way of doing things like Ricardo suggested.
If for some reason that does not work, then we could explore ways to instrument your agent class - not so much because I think it is a good idea but because it is technically interesting.
Basically, we would have to find out if maybe the class you want to instrument was already loaded before your ByteBuddy agent gets active.
Then you would have to use class retransformation rather than redefinition.
You would have to make sure the advice you apply can do its job without the need to change the class structure with regard to method signatures and fields.
You would also need to make sure that the advice and ByteBuddy are visible to the other agent's classloader, e.g.
by putting both on the boot class path.
But let's not get ahead of ourselves.
Explore Ricardo's ideas first, please.
Based on what I've seen it looks like there isn't a &quot;right&quot; way to do this with the stock  nuxt  command line application.
The problem seems to be that while  nuxt.config.js  is the first time a user has a chance to add some javascript, that the  nuxt  command line application bootstraps the Node's HTTP frameworks before this config file is  required .
This means the elastic agent (or any APM agent) doesn't have a chance to hook into the modules.
The  current recommendations  from the Nuxt team appears to be
Invoke  nuxt  manually via  -r
Skip  nuxt  and  use NuxtJS programmatically  as a middleware in your framework of choice
Based on Alan Storm answer (from Nuxt team) I made it work but with a little modification:
actually there are many reasons why your app not starting depending on how you setup and configured your ELK stack , but for me I did the following and it's working fine :
create image from this Dockerfile:
run the created image :
The exception comes from the constructur of the  Kernel32  class which is a class of the Maven coordinate  net.java.dev.jna:jna-platform  which itself depends on  net.java.dev.jna:jna .
It seems to me like you have to incompatible versions of those dependencies on the class path.
I assume that you use version 4 of JNA core and version 5 of JNA platform.
Upgrade the first or downgrade the latter and the error should disappear.
Well, as an option, you can use something like that
It seems that you are using the oss distribution of elasticsearch but the defaut version of apm.
upgrade the elasticsearch cluster to the default disto or use this oss apm docker image: docker.elastic.co/apm/apm-server-oss:7.0.1
OpenTracing   is a set of standard APIs that consistently model and describe the behavior of distributed systems )
OpenTracing did not describe how to collect, report, store or represent the data of interrelated traces and spans.
It is implementation details (such as  jaeger  or  wavefront ).
jaeger-client-csharp is very jaeger-specific.
But there is one exception, called  zipkin  which in turns is not fully OpenTracing compliant, even it has similar terms.
If you are OK with  opentracing-contrib/csharp-netcore  (hope you are using this library) then if you want to achieve "no code change" (in target microservice) in order to configure tracing subsystem, you should use some plug-in model.
Good news that aspnetcore has concept of  hosted startup assemblies , which allow you to configure tracing system.
So, you can have some library called  JaegerStartup  where you will implement IHostedStartup like follows:
When you decide to switch the tracing system - you need to create another library, which can be loaded automatically, and target microservice code will not be touched.
The best way to troubleshoot what is going on is to check if the events from Heartbeat are being collected.
The Uptime application only displays events from Heartbeat, and therefore — this is the Beat that you need to check.
First, check the connectivity of Heartbeat and the configured output:
Secondly, check if the events are being generated.
You can check this by commenting out your existing output (Likely Elasticsearc/Elastic Cloud) and enabling either the  Console  output or the  File  output.
Then start your Metricbeat and check if events are being generated.
If they are, then it might be something with the backend side of things; maybe Elasticsearch is rejecting the documents sent and refusing to index them.
Apropos, Elastic is implementing a native  Jenkins  plugin that allows you to observe your CI pipeline using OpenTelemetry compatible backends such as  Elastic APM .
You can learn more about this plugin  here .
The only way to get configuration is to check apm-server.yml in your instance, but if you want to check your agent configuration you can use Agent Configuration API, for more information check  https://www.elastic.co/guide/en/apm/server/current/agent-configuration-api.html .
I can by pass it when I comment entityframework-&gt;Interceptor node in web.config file
And I can continue after uncomment it
About the possibility of using some kind of Proxy between your gke cluster and elastic apm.
You can check the following link [1], to see if it can fit your necessities.
[1]  https://cloud.google.com/vpc/docs/special-configurations#proxyvm
Since you didn't mention it above: did you instrument a Go application?
The Elastic APM Go &quot;Agent&quot; is a package which you use to instrument your application source code.
It is not an independent process, but runs within your application.
So, first (if you haven't already) instrument your application.
See  https://www.elastic.co/guide/en/apm/agent/go/current/getting-started.html#instrumenting-source
Here's an example web server using  Echo , and the  apmechov4  instrumentation module:
If you run that and send some requests to  http://localhost:8080/hello/world , you should soon see requests in the APM app in Kibana.
If you still don't see anything in Kibana, you can follow  https://www.elastic.co/guide/en/apm/agent/go/current/troubleshooting.html#agent-logging  to enable logging.
Here's what you can expect to see if the agent is able to successfully send data to the server:
If on the other hand the server is inaccessible, you would see something like this:
Sorry my bad, the server urls weren't correctly passed to docker.
This is by design in Django, and it is intentionally designed in this way.
this is a parametrized way.
suppose someone has a column name with spaces like  test column name  then think what would happen.
it will lead to some unwanted errors, so don't change the underlying logic of the framework.
Thanks @BjarniRagnarsson, The upper case letters was making this behavior of framework as @Sanjay mentioned.
Solution:
For a high-level overview type of information, have a look at  Elastic Stack Monitoring .
If you want to look at any monitoring in more detail, have a look at the  monitoring APIs themselves .
If you want to log this sort of information, you should set thresholds  for your Elasticsearch slow log .
If you want to index and then view data from the slow log,  you can always use Filebeat to ingest that slow log data back into Elasticsearch .
You're calling one method from another.
Spring is creating a proxy around your method.
If you call one method from another from the same class then you're not going through the proxy.
Extract the method annotated with new span to a separate class and it will work fine.
After searching a solution for some time, I found a docker-compose.yml file which had the Jaeger Query,Agent,collector and Elasticsearch configurations.
docker-compose.yml
The docker-compose.yml file installs the elasticsearch, Jaeger collector,query and agent.
Install docker and docker compose first
 https://docs.docker.com/compose/install/#install-compose
Then, execute these commands in order
start all the docker containers - Jaeger agent,collector,query and elasticsearch.
sudo docker start container-id
access -   http://localhost:16686/
If Jaeger needs to be set up in Kubernetes cluster as a helm chart, one can use this:  https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger 
It can delploy either Elasticsearch or Cassandara as a storage backend.
Which is just a matter of right value being passed in to the chart:
This section shows the helm command as an example:
 https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger#installing-the-chart-using-a-new-elasticsearch-cluster
If you would like to deploy the Jaeger with Elasticsearch and Kibana to quickly validate and check the stack e.g.
in kind or Minikube, the following snippet may help you.
For people who are using OpenTelemetry, Jaeger, and Elasticsearch, here is the way.
Note the image being used are  jaegertracing/jaeger-opentelemetry-collector  and  jaegertracing/jaeger-opentelemetry-agent .
Then just need
Reference:  https://github.com/jaegertracing/jaeger/blob/master/crossdock/jaeger-opentelemetry-docker-compose.yml
As I mentioned in my comment on the OP's first answer above, I was getting an error when running the docker-compose exactly as given:
Error: unknown flag: --collector.host-port
I think this CLI flag has been deprecated by the Jaeger folks since that answer was written.
So I poked around in the jaeger-agent documentation a bit:
And I got this to work with a couple of small modifications:
The updated docker-compose.yaml:
https://github.com/opentracing-contrib/java-spring-cloud  project automatically sends standard logging to the active span.
Just add the following dependency to your pom.xml
Or use this  https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core  starter if you want only logging integration.
Then I use  opentracing-spring-jaeger-cloud-starter
I got just one line in console with current trace and span
i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - my_method
Then I use  spring-cloud-starter-sleuth
I got the Trace and Spans like [my-service,90e1114e35c897d6,90e1114e35c897d6,false] in each line and it's helpfull for filebeat in ELK
How could I get the same log in console using opentracing-spring-jaeger-cloud-starter ?
my opentracing config
Here is what I did to make jdbc related logs from Logback (Slf4j) write into Jaeger server:
Beginning with Logback config (logback-spring.xml):
Here is my appender:
I was facing similar issue.
ConnectionInfo was getting traced but not the SQL statements.
In my case, I had to enable traceWithActiveSpanOnly=true.
For e.g, : jdbc:tracing:h2:mem:test?traceWithActiveSpanOnly=true
After that the statements started getting traced.
Check the documentation of opentracing java-jdbc module here
Based on my experience and reading online, I found this interesting line in Istio  mixer faq
Mixer trace generation is controlled by command-line flags: trace_zipkin_url, trace_jaeger_url, and trace_log_spans.
If any of those flag values are set, trace data will be written directly to those locations.
If no tracing options are provided, Mixer will not generate any application-level trace information.
Also, if you go deep into mixer  helm chart , you will find traces of Zipkin and Jaeger signifying that it’s mixer that is passing trace info to Jaeger.
I also got confused which reading this line in one of the articles
Istio injects a sidecar proxy (Envoy) in the pod in which your application container is running.
This sidecar proxy transparently intercepts (iptables magic) all network traffic going in and out of your application.
Because of this interception, the sidecar proxy is in a unique position to automatically trace all network requests (HTTP/1.1, HTTP/2.0 &amp; gRPC).
On Istio mixer documentation, The Envoy sidecar logically calls Mixer before each request to perform precondition checks, and after each request to report telemetry.
The sidecar has local caching such that a large percentage of precondition checks can be performed from cache.
Additionally, the sidecar buffers outgoing telemetry such that it only calls Mixer infrequently.
Update:  You can enable tracing to understand what happens to a request in Istio and also the role of mixer and envoy.
Read more information  here
I found the solution to my problem, in case anybody is facing similar issues.
I was missing the environment variable  JAEGER_SAMPLER_MANAGER_HOST_PORT , which is necessary if the (default) remote controlled sampler is used for tracing.
This is the working docker-compose file:
For anyone finding themselves on this page looking at a simialr issue for jaeger opentracing in .net core below is a working docker compose snippet and working code in Startup.cs.
The piece I was missing was getting jaeger to read the environment variables from docker-compose as by default it was trying to send the traces over udp on localhost.
Add following nuget packages to your api's.
Then inside your Startup.cs Configure method, you will need to configure jaeger and opentracing as below.
The jaeger-agent service should look like
Don't use IP, use FQDN.
First, try to hardcode value for  jaegerHost
where  jaeger-agent  - service name,  jaeger  - namespace of service
Also, you should create  jaeger-collector  service
Are you closing the tracer and the scope?
If you are using a version before 0.32.0, you should manually call  tracer.close()  before your process terminates, otherwise the spans in the buffer might not get dispatched.
As for the scope, it's common to wrap it in a try-with-resources statement:
You might also want to check the OpenTracing tutorial at  https://github.com/yurishkuro/opentracing-tutorial  or the Katacoda-based version at  https://www.katacoda.com/courses/opentracing
-- EDIT
and is deployed on a different hostname and port
Then you do need to tell the tracer where to send the traces.
Either export the  JAEGER_ENDPOINT  environment variable, pointing to a collector endpoint, or set  JAEGER_AGENT_HOST / JAEGER_AGENT_PORT , with the location of the agent.
You can check the available environment variables for your client on the following URL:  https://www.jaegertracing.io/docs/1.7/client-features/
You need to add some more properties to your config options.
For reporter deployed on localhost and local sampler strategy :
Replace  localhost  by server or route name to target another host for Jeager runtime.
This link ( https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/ ) provides the details of how to enable jaeger traces.
The simplest way to enable jaeger to spring-boot application is add the dependency and the required properties.
Dependency:
Example Properties
Answering to your question about dependencies it is explained here in Dependencies section ( https://github.com/opentracing-contrib/java-spring-jaeger ):
The opentracing-spring-jaeger-web-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-web-starter This means that by including it, simple web Spring Boot microservices include all the necessary dependencies to instrument Web requests / responses and send traces to Jaeger.
The opentracing-spring-jaeger-cloud-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-cloud-starter This means that by including it, all parts of the Spring Cloud stack supported by Opentracing will be instrumented
And by the way:
same as:
You can use  Jaeger Operator  to deploy Jaeger on kubernetes.The Jaeger Operator is an implementation of a Kubernetes Operator.
Operators are pieces of software that ease the operational complexity of running another piece of software.
More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application
Follow this link for steps to deploy JAEGER on kubernetes .
https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/
make following changes in application.properties
You can use this link for a better understanding -  https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/
You add  opentracing-spring-jaeger-starter   library  into the project which simply contains the code needed to provide a Jaeger implementation of the OpenTracing's  io.opentracing.Tracer  interface.
Since you have the jaeger deployed in Kubernetes and exposed it via a loadbalancer service you can use the loadbalancer IP and port to connect to it from outside the Kubernetes cluster.
So the solution that works for me is -
I have made the following changes in my application.properties file of application
You should config:
This is why tracing is a useful tool, it often reveals issues like this that you wouldn't suspect otherwise.
If your application is using async framework, these gaps may indicate execution waiting on available threads.
Or your application may be CPU throttled during and between the spans.
You cannot really explain the gaps from the trace itself, but you surely do have them.
Time to whip out the profiler.
Turns out that  Feign  clients are  currently not supported  or to be precise, the spring startes do not configure the Feign clients accordingly.
If you want to use Jaeger with your Feign clients, you have to provide an integration of your own.
In my experience so far, the jaeger community is a lesser supportive one, thus you have to gain such knowledge on your own, which in my opinion is a big downside and you should probably consider, using an alternative.
In some cases it might be necessary to explicitly expose the  Feign Client  in the Spring configuration, in order to get the traceId propagated.
This can be done easily by adding the following into one of your configuration classes
The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT!
Jaegar has the ability to collect Zipkin spans:
https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin
You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.
The above will send Zipkin spans to  http://localhost:9411  by default.
You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.
spring.zipkin.base-url= http://your-jaegar-server:9411
Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.
In the log4j2.xml file, all you have to mention is
[%X]
You can find the sample code here:
https://github.com/anoophp777/spring-webflux-jaegar-log4j2
Some web frameworks return empty string if a non-existent header is queried.
I have seen this in Spring Boot and KoaJS.
If any of the tracing headers is not sent by Istio, this header logic causes us to send empty string for those non-existent headers which breaks tracing.
My suggestion is after getting the values for headers filter out the ones with empty string as their values and propogate the remaining ones.
Also, do you get an Error Message?
If so, please post it.
BINGO!
We need to setup the ReporterConfigurations as below.
previously my ones were default ones that's why it always connected to local.
Even better, you can create the Configuration from Environment as below providing the environment variables as below
You can provide this when you run the docker container
-e JAVA_OPTS="
" ....
Step 1 : First we need to configure remote host address and port.
Step 2 : configure sender configuration and pass the remote host and port in                           withAgentHost, withAgentPort.
SenderConfiguration senderConfig = Configuration.SenderConfiguration.fromEnv()
                    .withAgentHost(JAEGER_HOST)
                    .withAgentPort(JAEGER_PORT);
Step 3 : Pass sender configuration in reporter configuration
Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv()
                .withLogSpans(true)
                .withSender(senderConfig);
This seems a bit old and it's a bit hard to tell what's wrong.
My first guess would be the sampling strategy, as Jaeger samples one trace in one thousand, but looks like you did set it.

JAEGER_SAMPLER_TYPE=const
JAEGER_SAMPLER_PARAM=1
I would recommend you start by using a simple  Configuration.fromEnv().getTracer()  to get your tracer.
Then, control it via env vars, probably setting  JAEGER_REPORTER_LOG_SPANS  to  true .
With this option, you should be able to see in the logs whenever Jaeger emits a span.
You can also set the  --log-level=debug  option to the agent and collector (or all-in-one, in your case) to see when these components receive a span from a client.
You can see that jaeger-query configuration includes: SPAN_STORAGE_TYPE: &quot;kafka&quot;
The error indicates that a kafka client used by jaeger-query to store spans in Kafka cannot in fact reach Kafka, and therefore the jaeger storage factory fails to initialize.
This can be either because Kafka failed to start (did you check)?
Or a misconfig of the network in your docker.
I was missing a lot of information.
I managed to get it working:
Update :
I got the same exception ( Tracer bean is not configured!.. )
when I use your version of spring cloud jaeger dependencies.
This is irrespective of the  RxJava  package.
I think you can directly use  opentracing-spring-jaeger-cloud-starter  which is a combination of  opentracing-spring-cloud-starter  and  opentracing-spring-jaeger-starter .
Read  this details  for java spring jaeger.
The opentracing-spring-jaeger-cloud-starter starter is convenience
starter that includes both opentracing-spring-jaeger-starter and
opentracing-spring-cloud-starter This means that by including it, all
parts of the Spring Cloud stack supported by Opentracing will be
instrumented
Note :
Maybe RxJava tracing won't work without registering the tracer using the decorators provided by  opentracing-contrib .
Please see the working app  here .
I have followed  this spring guide  for Reactive Restful webservice and  jaeger  worked with below  pom.xml  without any  Tracer bean  -
Thing is, you should use opentelemetry collector if you use opentelemetry exporter.
Pls see schema in attachment
Also I created a gist, which will help you to setup
pls see
https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f
(just tune the values, export to jaeger-all-in-one instead of separate + cassandra, etc)
From the official FAQ ( https://www.jaegertracing.io/docs/latest/faq/#do-i-need-to-run-jaeger-agent ):
jaeger-agent  is not always necessary.
Jaeger client libraries can be configured to export trace data directly to  jaeger-collector .
However, the following are the reasons why running  jaeger-agent  is recommended:
If you check the your  base image  it from scratch.
So there is no  Bash, ash  as the image is from scratch so it will only cotnain  hotrod-linux .
To get sh or bash in such cases you need to use multi-stage Dockerfile, you can use the base image in Dockerfile and then copy the binaries from the base image in multi-stage in Dockerfile.
Here you go
so now you can build and test and you will able to run command inside container using docker exec, here is the example
https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/Dockerfile
As I can see hotrod image was built from scratch image.
And from the docker hub:
"an explicitly empty image, especially for building images "FROM
  scratch"...
"This image is most useful in the context of building base images
  (such as debian and busybox) or super minimal images (that contain
  only a single binary and whatever it requires, such as hello-world)."
https://hub.docker.com/_/scratch
So, I think there is not bash inside this image
'reporting_host' must be not 'localhost', but 'jaeger', just as service in docker-compose.yml called.
'reporting_host' =&gt; 'jaeger',
Also, I needed to add  $tracer-&gt;flush();  after all, it closes all the entities and does sending via UDP behind the scenes.
So, answering my own question.
Jaeger does not support cross system spans.
Every sub-system is responsible for its own span in the whole system.
For reference, check this answer  https://github.com/opentracing/specification/issues/143
If anyone else would like to set up Jaeger in spring project, here's what I did:
Add dependencies to pom:
Set up you web.xml to register new tracing filter  tracingFilter  to intercept REST API:
Register jaeger tracer in spring mvc:
Set up the  tracingFilter  bean we described in web.xml:
Finally define jaeger tracer spring configuration:
I have got following gradle dependencies working,
Following tracer bean configuration,
And then the spans can be recorded as
Here is the working example you can refer  https://github.com/krushnatkhawale/jaeger-with-spring-boot-web-app
So did you install direct  or created a yaml from the templates ?
I would run the command you used to install but with template function and then add the options for jaeger,Kiali and grafana.
Here is  howto : from official repository.
you need to update  values.yaml .
and turn on  grafana,  kiali and jaeger.
For example with kiali change:
to
than rebuild the Helm dependencies:
than upgrade your istio inside kubernetes:
that's it, hope it was helpful
Scalabilty is dependent on sampling frequency and volumes.
The agent supports adaptive sampling which is a feedback loop from the collector to your instrumented app.
You can statically define this up front in your instrumentation but you lose the adaptive features.
It is possible to bypass agent all together and send metrics directly to collector.
Just define variable JAEGER_ENDPOINT in your app running environment.
This behaviour is documented but buried down in the Jager git repo:
https://github.com/jaegertracing/jaeger-client-java/blob/master/jaeger-core/README.md
Would a single agent colocated with a single collector be possible in a Jaeger deployment?
It's possible, and that's how the  "all-in-one"  image works.
Would it be advisable?
Depends on your architecture.
If you don't expect your Jaeger infra to grow, using the all-in-one is easier from the maintenance perspective.
If you need your Jaeger infra to be highly available, then you probably want to place your agents closer to your instrumented applications than to your collector and scale the collectors separately.
More about the Jaeger Agent is discussed in the following blog posts:
Running Jaeger Agent on bare metal 
 Deployment strategies for the Jaeger Agent
Is it possible to skip the agent altogether and submit spans directly to the collector over HTTP?
For some clients (Java, NodeJS and C#), yes.
Look for the  JAEGER_ENDPOINT  option .
You can turn on the debugging in the client by setting the option  JAEGER_REPORTER_LOG_SPANS  to true (or use the related option in the  ReporterConfiguration , as it seems that's how you are using it).
https://www.jaegertracing.io/docs/1.8/client-features/
Once you confirm the traces are being generated and sent to the agent, set the log-level in the agent to  debug :

docker run -p... jaegertracing/jaeger-agent:1.8 --log-level=debug
If you don't see anything in the logs there indicating that the agent received a span (or span batch), then you might need to configure your client with the Agent's address ( JAEGER_AGENT_HOST  and  JAEGER_AGENT_PORT , or related options in the  Configuration  object).
You mentioned that you are deploying in Azure AKS, so, I guess that the agent isn't available at  localhost , which is the default location where the client sends the spans.
Typically, the agent would be deployed as a sidecar in such a scenario:
https://github.com/jaegertracing/jaeger-kubernetes#deploying-the-agent-as-sidecar
The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT!
Jaegar has the ability to collect Zipkin spans.
https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin
You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.
The above will send Zipkin spans to http://localhost:9411 by default.
You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.
Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.
In the  log4j2.xml  file, all you have to mention is
I'll be uploading a working example of this approach into my GitHub and sharing the link.
EDIT 1:
You can find the sample code here:
https://github.com/anoophp777/spring-webflux-jaegar-log4j2
The Jaeger helm chart is now available  here .
You need to add the helm repo first using the following:
This can be installed with:
Yes you can, and I have shown that numerous times during my presentations ( https://toomuchcoding.com/talks ) and we describe it extensively in the documentation ( https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/ ).
Sleuth will set up your logging pattern which you can then parse and visualize using the ELK stack.
Sleuth takes care of tracing context propagation and can send the spans to a span storage (e.g.
Zipkin or Jaeger).
Sleuth does take care of updating the MDC for you.
Please always read the documentation and the project page before filing a question
The solution for this one is to simply increase the memory size in the  istio-config.yaml  file.
in my case, I'm updating the PVC and it looks like it's already filled with data and decreasing it wasn't an option for istio, so I increased it in the config file instead:
Based on following example from  jaeger docs :
and on example  cli falgs :
I infere that you should be able to do the following:
Notice that I split the cli options with the dot and added it as a nested fields in yaml.
Do the same to other parameters by analogy.
See the answer on this  jaeger issue , you will need to query elastic search or the source where the data is stored.
Alternatively, you should raise an issue on  jaeger-ui  detailing your case.
When you open an individual trace in the Jaeger UI, there is a View dropdown in the top right corner.
One of the options is to view/download the given trace as a JSON file.
You can also programmatically query the Jaeger query service via JSON/Protobuf API, but those endpoints will not result in a data format that you can load back into the UI.
https://www.jaegertracing.io/docs/latest/apis/#trace-retrieval-apis
OK, I figured out the issue here which may be obvious to those with more expertise.
The guide I linked to above that describes how to make an Ingress spec for gRPC  is specific to NGINX.
Meanwhile, I am using K3S which came out of the box with Traefik as the Ingress Controller.
Therefore, the annotations I used in my Ingress spec had no affect:
So I found  another Stack Overflow post discussing Traefik and gRPC  and modified my original Ingress spec above a bit to include the annotations mentioned there:
These are the changes I made:
Hopefully this helps someone else running into this same confusion.
It's not clear in the documentation, but I managed to get it working by providing the  SPAN_STORAGE_TYPE  and the respective connection details to allow the jaeger components to talk to the storage running outside of the all-in-one container.
For instance, I'm running elasticsearch on my Mac, so I used the following command to run all-in-one:
According to istio  documentation
To see trace data, you must send requests to your service.
The number of requests depends on Istio’s sampling rate.
You set this rate when you install Istio.
The default sampling rate is 1%.
You need to send at least 100 requests before the first trace is visible.
Could you try to send at least 100 requests and check if it works?
If you wan't to change the default sampling rate then there is istio  documentation  about that.
Customizing Trace sampling
The sampling rate option can be used to control what percentage of requests get reported to your tracing system.
This should be configured depending upon your traffic in the mesh and the amount of tracing data you want to collect.
The default rate is 1%.
To modify the default random sampling to 50, add the following option to your tracing.yaml file.
The sampling rate should be in the range of 0.0 to 100.0 with a precision of 0.01.
For example, to trace 5 requests out of every 10000, use 0.05 as the value here.
After digging around in the OpenTracing C# .NET Core source ( https://github.com/opentracing-contrib/csharp-netcore ) I figured out how to override the top level Span.OperationName.
I had to update my  Startup.ConfigureServices()  call to  services.AddOpenTracing()  to the following:
I have resolved it after configuring port as 14250 as JaegerGrpcSpanExporter internally uses grpc port which has been configured to 14250 for jaeger-collector
I was studying Opentracing and Jeager and I've used this tutorial to get familiar with the basic possibilities:
 https://github.com/yurishkuro/opentracing-tutorial/tree/master/java
If you take a look in the case 1 (Hello World), it explains how to &quot; Annotate the Trace with Tags and Logs &quot;.
That would answer your questions 1, 2 and 3, as with that you can add all the info that you would like within spans and logs.
Here is a snippet from the repository (but I'd recommend checking there, as it has a more detailed explanation):
In this case  helloTo  is a variable containing a name, to whom the app will say hello.
It would create a span tag called hello-to with the value that is coming from the execution.
Below we have an example for the logs case, where the whole  helloStr  message is added to the logs:
Regarding the last question, that would be easier, you can use the Jaeger UI to search for the trace you would like, there is a field for that on the top left corner:
There you go.
There are various overloaded methods as follows
I want to add some fields to span tags so that it's easy to search in JaegerUI.
Jaeger API provides  log  method to log multiple fields that needs to be added to a map, the method signature is as follows,
Span log(Map&lt;String, ?&gt; fields);
eg:
spanId and traceId are stored in JaegerSpanContext class, which can be obtained from context() method of Span class.
There is a search box in the navigation bar of Jaeger UI where you can search traces by trace ID.
What you did is for http 1.x, and it doesn't work for http2/grpc.
Please dive into grpc impl in springboot doc.
Thanks Yuri.
Yes it was a clock issue.
Although the host machine (VM) updated its clock on every unpause, docker for windows did not.
The timezones were correct for all containers but the times were all out by the exact same amount.
This must be the docker internal clock that seems to only get updated once on launch and not at the launch of every new container.
Although all container clocks were out by the same amount, the windows host machine was correct.
The messages were arriving but the times/dates were outside the time frame window the UI was displaying.
If I set a custom date range I'm sure they would appear.
The containers must have been out by a few days with the continual stopping and starting and not by just a few hours.
Time drift in docker is a known issue on Mac and Windows OS.
Check the date/time in a docker container using this (apologies in advance)
or calculate the drift...
Unfortunately there doesnt seem to be a better solution than occasionally restarting the container.
The problem is that your Jaeger collector is not accessible from outside docker network host as you specified in your docker command.
This would only work if your spring boot application is deployed on the host network too.
Try to run Jaeger as follows:
It should trigger.
Have you tried looking at the logs being generated by your pods?
In my case I got the following
ERROR Failed to flush spans in reporter: error sending spans over UDP:
  Error: getaddrinfo ENOTFOUND  http://jaeger-agent , packet size: 984,
  bytes sent: undefined
Changing it to jaeger-agent worked for me.
Also if it helps I have declared this under my jaeger image in docker-compose.yml:
The answer here is to install istio with  --set values.global.tracer.zipkin.address  as provided in  istio documentation
And
Use the original TracingService  setting: service: "zipkin.istio-system:9411"  as Donato Szilagyi confirmed in comments.
Great!
It works.
And this time I used the original TracingService setting: service: "zipkin.istio-system:9411" – Donato Szilagy
Not sure, but it seems you miss setting the TLS for Cassandra Storage in Azure Cosmos DB.
When you use the Cassandra client to connect the Cassandra Storage in Azure Cosmos DB, it will give out the time out error, but if you enable the SSL, the connection works well.
So I think you can try to enable the TLS for Cassandra in your values.yaml following the steps in the Github which provided.
A colleague of mine provided the answer...
It was hidden in the Makefile, which hadn't worked for me as I don't use Golang (and it had been more complex than just installing Golang and running it, but I digress...).
The following .sh will do the trick.
This assumes the query.proto file is a subdirectory from the same location as the script below, under model/proto/api_v2/ (as it appears in the main Jaeger repo).
This will definitely generate the needed Python file, but it will still be missing dependencies.
Jaeger clients implement so-called  head-based sampling , where a sampling decision is made at the root of the call tree and propagated down the tree along with the trace context.
This is done to guarantee consistent sampling of all spans of a given trace (or none of them), because we don't want to make the coin flip at every node and end up with partial/broken traces.
Implementing on-error sampling in the head-based sampling system is not really possible.
Imaging that your service is calling service A, which returns successfully, and then service B, which returns an error.
Let's assume the root of the trace was not sampled (because otherwise you'd catch the error normally).
That means by the time you know of an error from B, the whole sub-tree at A has been already executed and all spans discarded because of the earlier decision not to sample.
The sub-tree at B has also finished executing.
The only thing you can sample at this point is the spans in the current service.
You could also implement a reverse propagation of the sampling decision via response to your caller.
So in the best case you could end up with a sub-branch of the whole trace sampled, and possible future branches if the trace continues from above (e.g.
via retries).
But you can never capture the full trace, and sometimes the reason B failed was because A (successfully) returned some data that caused the error later.
Note that reverse propagation is not supported by the OpenTracing or OpenTelemetry today, but it has been discussed in the last meetings of the W3C Trace Context working group.
The alternative way to implement sampling is with  tail-based sampling , a technique employed by some of the commercial vendors today, such as Lightstep, DataDog.
It is also on the roadmap for Jaeger (we're working on it right now at Uber).
With tail-based sampling 100% of spans are captured from the application, but only stored in memory in a collection tier, until the full trace is gathered and a sampling decision is made.
The decision making code has a lot more information now, including errors, unusual latencies, etc.
If we decide to sample the trace, only then it goes to disk storage, otherwise we evict it from memory, so that we only need to keep spans in memory for a few seconds on average.
Tail-based sampling imposes heavier performance penalty on the traced applications because 100% of traffic needs to be profiled by tracing instrumentation.
You can read more about head-based and tail-based sampling either in Chapter 3 of my book ( https://www.shkuro.com/books/2019-mastering-distributed-tracing/ ) or in the awesome paper  "So, you want to trace your distributed system?
Key design insights from years of practical experience"  by Raja R. Sambasivan, Rodrigo Fonseca, Ilari Shafer, Gregory R. Ganger ( http://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf ).
You can bind it to metrics and logging frameworks, but you don't have to.
You can simply just call  cfg.NewTracer() , like in this example:
Source:  https://github.com/jaegertracing/jaeger-client-go/blob/3585cc566102e0ea2225177423e3fcc3d2e5fd7a/config/example_test.go#L88-L105
Check the Jaeger Go Client readme for more information on the metrics/logging integration:  https://github.com/jaegertracing/jaeger-client-go
Jaeger clients are designed to have a minimum set of dependencies.
We don't know if your application is using Prometheus metrics or Zap logger.
This is why  jaeger-client-go  (as well as many other Jaeger clients in other languages) provide two lightweight interfaces for a Logger and MetricsFactory that can be implemented for a specific logs/metrics backend that your application is using.
Of course, the bindings for Prometheus and Zap are already implemented in the  jaeger-lib  and can be included optionally.
It looks like you have different versions of opentracing.
The spring-starter-jaeger version 2.x upgrade the version of opentracing, so you might have introduced this breaking changes when you upgraded the dependency version.
Unfortunatelly, the reporter interface is used to report FINISHED spans, it is invoked on JaegerSpan.finish.
I presume this is why it does not appear in logs.
If you are using spring boot with auto configuration, the logs printed using log4j will be instrumented and sent automatically in the span.
In Go this is not very straightforward, and largely depends on the logging library you use and the interface it provides.
One example is implemented in the  HotROD demo  in the Jaeger repository and it is described in the  blog post accompanying the demo .
It uses  go.uber.org/zap  logging library underneath and allows to write log statements accepting the Context argument:
Behind the scenes  logger.For(ctx)  captures the current tracing spans and adds all log statements to that span, in addition to sending then to  stdout  as usual.
To my knowledge,  go.uber.org/zap  does not yet support this mode of logging natively, and therefore requires a wrapper.
If you use another logging library, than on the high level this is what needs to happen:
spring-cloud-openfeign  since is from the spring-cloud family should be instrumented automatically once you add  opentracing-spring-jaeger-cloud-starter  the  as stated  here .
But sometimes (depending on how you create your feign client bean) you need to explicitly expose the bean to the spring context, so that the autoconfiguration can instrument your Feign Client.
Something like this:
it is kotlin but you can adapt.
This was due to the helidon dependency.
https://helidon.io/docs/latest/#/guides/03_quickstart-mp
Also I had to upgrade  opentracing-api  version to  0.33.0
If you are already using Istio in the deployment, then enabling tracing in it will provide more complete picture of request processing, such as accounting for the time spent in the network between the proxies.
You also don't need to have full tracing instrumentation in your services as long as they pass through certain headers, then Istio can still provide a pretty accurate picture of the traces (but you cannot capture any business specific data in the traces).
Traces generated by Istio will have standardized span names that you can use to reason about the SLAs across the whole infrastructure, whereas explicit tracing instrumentation inside the services can often use different naming schemes, especially when services are written in different languages and using different frameworks.
For the best of both worlds, I would recommend adding instrumentation inside the services for full fidelity, and also enabling tracing in Istio to capture full picture of request execution (and all network latencies).
You most likely have a mismatch with your OpenTracing libraries.
It looks like your Servlet Filter integration (io.opentracing.contrib.web.servlet.filter.TracingFilter) is making use of a method that doesn't exist.
You can use opentracing  java-jdbc  extension  it will works in Quarkus (I didn't test the native mode).
You need to use the version 0.0.12 as the latest one is based on Opentracing 0.33 but Quarkus use the version 0.31.
Add the dependency to your pom.xml:
Update your application.properties to use the opentracing-jdbc driver, the following are for a Postgres database:
You will then saw the SQL queries in Jaeger as spans.
What I eventually did is create a JaegerTraces and annotated with Bean
Apache Camel doesn't provide an implementation of  OpenTracing , so you have to add also an implementation to your dependencies.
For example  Jaeger .
Maven POM:
Also you have to enable OpenTracing for Apache Camel on your Spring Boot application class, see  Spring Boot :
If you are using Spring Boot then you can add the  camel-opentracing-starter  dependency, and turn on OpenTracing by annotating the main class with  @CamelOpenTracing .
The Tracer will be implicitly obtained from the camel context’s Registry, or the ServiceLoader, unless a Tracer bean has been defined by the application.
Spring Boot application class:
Could you try using a more recent version of Jaeger:  https://www.jaegertracing.io/docs/latest/getting-started/#all-in-one  - actually 1.11 is now out, so could try that.
The problem is that you are using  RestTemplate template = new RestTemplate();  to get an instance of the  RestTemplate  to make a REST call.
Doing that means that Opentracing cannot instrument the call to add necessary HTTP headers.
Please consider using  @Autowired RestTemplate restTemplate
While doing  mvnDebug quarkus:dev  (without  jvm.args ) and placing a breakpoint  here , I see that you all your params are being passed except  quarkus.jaeger.sampler.parameter  which is wrong.
It should be  quarkus.jaeger.sampler.param
When you are accessing the service from the pod in the  same namespace  you can use just the service name.
Example:
If you are accessing the service from the pod in the  different namespace  you should also specify the namespace.
Example:
To check in what namespace the service is located, use the following command:
Note : Changing ConfigMap does not apply it to deployment instantly.
Usually, you need to restart all pods in the deployment to apply new ConfigMap values.
There is no rolling-restart functionality at the moment, but you can use the following command as a workaround: 
 (replace deployment name and pod name with the real ones)
I don't think it's possible at the moment and you should definitely ask for this feature in the mailing list, Gitter or GitHub issue.
The current assumption is that a clear TChannel connection can be made between the agent and collector(s), all being part of the same trusted network.
If you are using the Java, Node or C# client, my recommendation in your situation is to have your Jaeger Client to talk directly to the collector.
Look for the env var  JAEGER_ENDPOINT  in the  Client Features  documentation page.
The problem was, that the Report instance used a NoopSender -- thus ignoring the connection settings.
Using
in your POM will provide an appropriate Sender to the SenderFactory used by Jaeger's SenderResolver::resolve method.
This solved my problem.
The Jaeger tracer (the part that runs along with your application) will send spans via UDP to an agent running on localhost by default.
If your agent is somewhere else, set the  JAEGER_AGENT_HOST / JAEGER_AGENT_PORT  env vars accordingly.
If you don't want an agent running on localhost and want to access a Jaeger Collector directly via HTTP, then set the  JAEGER_ENDPOINT  env var.
More info about these env vars can be found in the  documentation  or here:
 https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core#configuration-via-environment
No,  it cannot , but it wouldn't hurt to open an issue there with this suggestion.
There is several components which works together and can fully satisfy your requirement.
Common  opentracing library , consisted of abstract layer for span, tracer, injectors and extractors, etc.
Official  jaeger-client-csharp .
Full list of clients can be found  here , which implement  opentracing abstraction layer  mentioned earlier.
The final piece is the  OpenTracing API for .NET , which is glue between  opentracing library  and  DiagnosticSource  concept in dotnet.
Actually, the final library has  sample  which uses jaeger csharp implementation of ITracer and configure it as default GlobalTracer.
At the rest in your Startup.cs, you will end up with something like from that sample (services is IServiceCollection):
I resolved this.
It related to the sample rate.
After I configured the  JAEGER_SAMPLER_TYPE  and  JAEGER_SAMPLER_PARAM , I can see the data.
In server 2 , Install jaeger
In server 1, set these environment variables.
Change the tracer registration application code as below in server 1, so that it will get the configurations from the environment variables.
Hope this works!
Check this link for integrating elasticsearch as the persistence storage backend so that the traces will not remove once the Jaeger instance is stopped.
How to configure Jaeger with elasticsearch?
I finally figured this out after trying out different combinations.
This is happening because Jaeger agent is not receiving any UDP packets from my application.
You need to tell the tracer where to send UDP packets, which in this case is  docker-machine ip  
I added:
and then I was able to see my services in Jaeger UI.
Service graph data must be generated in Jaeger.
Currently it's possible with via a Spark job here:  https://github.com/jaegertracing/spark-dependencies
The Downloads page ( https://www.jaegertracing.io/download/ ) lists both the Docker images and the raw binaries built for various platforms (Linux, macOS, windows).
You can also build binaries from source.
Just to add to Yuris answer, you can also download the source from github -  Github - Jaeger  This is useful for diagnosing issues, or just getting a better understanding of how it all works.
I have run both the released apps and custom versions on both windows and linux servers without issues.
For windows I would recommend running as a service using Nssm.
Nssm details
Elastic search works fine for this.
And Kibana allows you to build nice aggregated views of the traffic.
A recommendation from my experience is to use the  --es.tags-as-fields.dot-replacement  option and specify a character.
This flattens the data structure.
Its very useful because ElasticSearch/Kibana struggle with the tags data as an array.
I had the same problem.
Found this page that explains how to configure Thrift sender:  https://github.com/jaegertracing/jaeger-client-csharp/blob/master/src/Senders/Jaeger.Senders.Thrift/README.md
The C# tutorial does not mention it though ...
And here is my InitTracer().
Works fine with Jaeger launched from binary:
I solved it by using this library instead  https://github.com/opentracing-contrib/java-spring-cloud
It seem to have an option to enable or disable different instrumentation feature.
Read about  opentracing.spring.cloud.async.enabled  for more info.
Looks like your DaemonSet misses the  hostNetwork  property, to be able to listen on the node IP.
You can check that article for further info:  https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677
You have two options:
For (2), you can pass the environment variable to you applications:
Additional references:
By default, OpenTracing doesn't log automatically into span logs, only important messages that Jaeger feels it needs to be logged and is needed for tracing would be there :).
The idea is to separate responsibilities between Tracing and Log management,  Check this GitHub discussion .
An alternative would be to use centralized log management and print traceId &amp; spanId into your logs for troubleshooting and correlating logs and tracing.
As iabughosh said, the main focus on jaeger is traceability, monitoring and performance, not for logging.
Anyway, i found that using the @traced Bean injection you can insert a tag into the current span, that will be printed on Jaeger UI.
this example will added the first 4 lines of an excpetion to the Tags seccion.
(I use it on my global ExceptionHandler to add more info about the error):
}
and you will see the little stacktrace at JaegerUI.
Hope helps
Did you install the operator on openshift using the instructions listed:  https://github.com/jaegertracing/jaeger-operator#openshift  ?
Did the operator start up ok, if you not were there errors in the log?
The  Creating a new Jaeger Instance  section starts with a link to some examples, including  simple-prod.yaml , which creates a Jaeger instance that uses an Elasticsearch cluster at the specified URL.
You simply run:
It doesn't work in golang grpc client.
I used openTelemetry  load balancing  Another option - use kubernetes to balance requests to backends.
I realized that I had got into a completely wrong direction.
I thought that I have to access the backend storage to get the trace data, which actually make the problem much more complex.
I got the answer from github discussion and here is the address  https://github.com/jaegertracing/jaeger/discussions/2876#discussioncomment-477176
Can you paste the Collector config file?
It seems you are using the gRPC protocol and it's not supported on the system where the collector is running.
https://github.com/open-telemetry/opentelemetry-collector/blob/master/exporter/otlpexporter/README.md
gRPC port isn't enabled in your jaeger instance.
You can try a docker-compose file like this
And you can connect to it without problems
Remove your dependencies and use the following one that will include also the instrumentation you need
I figured it out... the Jaeger Operator doesn't create a  Service  exposing the metrics endpoints.
These endpoints are just exposed via the pods for the Collector and Query components.
An example from the Collector pod spec:
Note the  admin-http  port there.
So to get the Prometheus Operator to scrape these metrics, I created a  PodMonitor  which covers both the Collector and Query components because both of them have the  labels/app: jaeger  and  admin-http  ports defined:
referring to the documentation provided in below link helped to resolve the issue.
java.lang.IllegalStateException: This should not happen as headers() should only be called while a record is processed
I got it working as mentioned below
I would assume that it may not be the right way of exposing the services.
Instead
This is the simplest working example that I was able to find.
Here is a more realistic example that builds the tracer from a configuration.
GitLab Helm charts support tracing, and you can configure it with:
For more details refer : https://docs.gitlab.com/charts/charts/globals.html#tracing
According to  istio  documentation:
Consult the   Jaeger documentation   to
get started.
No special changes are needed for Jaeger to work with
Istio.
Once Jaeger is installed, you will need to point Istio proxies to send
traces to the deployment.
This can be configured with   --set values.global.tracer.zipkin.address=&lt;jaeger-collector-address&gt;:9411 
at installation time.
See the
 ProxyConfig.Tracing 
for advanced configuration such as TLS settings.
Istio documentation states to use jaeger collector address in  global.tracer.zipkin.address .
As for the Jaeger agent host, according to  Jaeger  Operator documentation:
&lt;9&gt; By default, the operator assumes that agents are deployed as
sidecars within the target pods.
Specifying the strategy as
“DaemonSet” changes that and makes the operator deploy the agent as
DaemonSet.
Note that your tracer client will probably have to override
the “JAEGER_AGENT_HOST” environment variable to use the node’s IP.
Your tracer client will then most likely need to be told where the agent is located.
This is usually done by setting the environment variable   JAEGER_AGENT_HOST   to the value of the Kubernetes node’s IP, for example:
Solved by adding dependency in pom file on jaeger-thrift.
I enabled instrumentation on the services using those two dependencies:
And, I used jaeger-client to configure the tracer using environment variables:
Getting a Tracer Instance:
Finally, in the dropwizard application, you have to register the tracer like so
You need to keep double quotes as it is.
An issue has been identified similar to this [1] and has been fixed recently.
Can you try to get the latest WUM updated API Manager 3.1.0 and try enabling Jaeger open tracing?
Alternatively, this issue will not occur when using &quot;localhost&quot; as the hostname.
[1]  https://github.com/wso2/product-apim/issues/7940
Run Jager using the docker image as follows.
Then add the following config to the deployment.toml.
Side Note: For zipkin you can use the following.
I'm not sure if what you're looking for exists today per se, but you could accomplish this with OpenTelemetry by writing traces through the LogReporter then using a serverless function to read the cloudwatch stream and send it to Jaeger (or to an OpenTelemetry Collector that sends them to Jaeger).
You could also write a custom plugin for the OpenTelemetry Collector that read a cloudwatch stream into OTLP then exported it to any endpoint supported by the collector.
You are missing the configuration of Jaeger address.
Since you did not provided it, it is trying to connect to the default one, which is TCP protocol,  127.0.0.1  and port 5778.
Check for details the configuration section  here .
You just need to make use of  tags.value  instead of  value  in your match query.
Below query should help:
if it throws error then it is unable to reach host, this method is costly I agree but this is the only viable solution I find
so I give it a try and my answer to the question above are:
Q1) yes it is possible (congrats to the jaeger team, package pretty easy to grasp with a good documentation)
Q2) I did struggle a bit with this one and thanks to  https://github.com/CHOMNANP/jaeger-js-text-map-demo  I implemented a solution by adding a "textCarrier" with a ref.
to the span context formatted as "FORMAT_TEXT_MAP" to the message Component 1 was publishing towards Component 2.
Code snipper in C1 on the first API invocation
followed by this part when sending the msg on redis:
The getTextCarrierBySpanObject function is coming from  https://github.com/CHOMNANP/jaeger-js-text-map-demo
Code snippet in C2 receiving the msg from redis
I tested with version 1.13
So all in all a pretty convincing prototyping exercise with Jaeger, will most probably pilot it now on a real project before trying it in production.
According to  https://helm.sh/docs/chart_template_guide/control_structures/  a string is converted to a boolean of True.
So even a string of false would get evaluated as a Boolean of True by Helm.
I was using Spinnaker which handles all overrides as a string unless the "Raw Overrides" box is checked.
If that box is checked than it converts the string to primitives where applicable.
My issue was that even though I was overriding with a value of false, Spinnaker would pass that as a string to Helm which would then evaluate that as True.
The solution was to check the "Raw Overrides" box in Spinnaker.
You can set a tag to Span creating a new custom Span
or retrieving the current active Span
Here is a working example:  https://github.com/jobinesh/cloud-native-applications/tree/master/helidon-example-mp-jaeger .
See if that helps you.
If you are interested, see the details captured here:  https://www.jobinesh.com/2020/04/tracing-api-calls-in-your-helidon.html
You need to enable open tracing in nginx ingress controller.
To enable the instrumentation we must enable OpenTracing in the configuration ConfigMap:
To enable or disable instrumentation for a single Ingress, use the enable-opentracing annotation:
You must also set the host to use when uploading traces:
https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/
Are you connecting it to only elasticsearch or stack like ELK/EFK?.
I had tried but we cannot configure ELK in jeager-all-one.exe alone in windows without docker.You can do it by running Jeager-collector, Jeager agent and Jeager query individually by mentioning configurations related to ELK.
In Jeager collector and Jeager query you need to set up variables  SPAN_STORAGE_TYPES  and  ES_SERVER_URLS .
To start a jaeger container:
Then you should by able to access to the Jaeger UI at  http://localhost:16686
Once you've have a Jaeger up and running, you need to configure a Jaeger exporter to forward spans to Jaeger.
This will depends of the language used.
Here  is the straightforward documentation to do so in python.
Not out of the box, you have to plug a behaviour into NSB that uses open telemetry
 https://github.com/open-telemetry/opentelemetry-dotnet 
You will have to write custom code.
Plus you can do push metrics as well as shown in our app insights, Prometheus and other samples.
Let's continue the conversation in our support channels?
Not sure if you are still looking for a solution for this.
You should be able to do this currently using the  NServiceBus.Extensions.Diagnostics.OpenTelemetry  package from  nuget .
This is built by Jimmy Bogard and instruments NServiceBus with the required support for  Open Telemetry .
The source for this is available  here .
You can connect this to any backend of your choice that supports  Open Telemetry  including but not limited to  Jaeger  and  Zipkin .
Additionally, here is an  example  that shows this in action.
Got it!
We need to enable sampling strategy to reach the collector endpoint.
Found out how.
I just added one single line of code into tracing.py of django_opentracing lib:
And the result:
I see..
I thought  @Traced  will be somehow propagated to my db-services/repositories.
No, I have to put it explicitly:
That fixes the issue.
According to the documentation  Remotely Accessing Telemetry Addons .
There are different ways how to acces telemetry.
The Recommended way is to create Secure acces using https instead of http.
Note for both methods:
This option covers securing the transport layer only.
You should also configure the telemetry addons to require authentication when exposing them externally.
Please note that jaeger itself doesn't support authentication methods  github  and workaround using Apache httpd server  here .
With your recruitments you can use Gateways (SDS)  with self-signed certificates :
a .)
Make sure your that during istio instalation youe have enabled SDS at ingress gateway  --set gateways.istio-ingressgateway.sds.enabled=true  and  --set tracing.enabled=true  for tacing purposes.
b .)
Create self signed certificates for testing purposes you can use this  example and repository .
c .)
Please follow  Generate client and server certificates and keys   and  Configure a TLS ingress gateway using SDS .
Create Virtualservice and Gateway:
Hope this help
The prometheus-es-exporter provides a way to create metrics using queries.
For further details you can check  prometheus-es-exporter#query-metrics
Great question and a very popular one too.
In short, yes, code changes are required.
Not just in one service but in all the services that a request will go through.
You need to instrument all services to get continuous traces that will be able to tell you the story of a request as it travels through the system.
I believe you can reuse Elasticsearch for multiple purposes - each would use a different set of indices, so separation is good.
from:  https://www.jaegertracing.io/docs/1.11/deployment/  :
Collectors require a persistent storage backend.
Cassandra and Elasticsearch are the primary supported storage backends
Tying the networking all together, a docker-compose example:
 How to configure Jaeger with elasticsearch?
While this isn't exactly what you asked, it sounds like what you're trying to achieve is seeing tracing for your JMS calls in Jaegar.
If that is the case, you could use an OpenTracing tracing solution for JMS or ActiveMQ to report tracing data directly to Jaegar.
Here's one potential solution I found with a quick google.
There may be others.
https://github.com/opentracing-contrib/java-jms
Maybe you should check whether the application services which you set up in a hurry are both in the same azure resource group as the VM running the Jaeger all-in-one instance, otherwise the second application might not be able to communicate with the Jaeger instance at all.
It doesn't really matter in which language your individual microservices are written, you should see them all in the same trace.
Given that you are seeing three traces instead of one trace with three spans, it appears that the context propagation isn't working.
Check your HTTP client in your nodejs services, they should perform the  "inject" operation .
Your service "B" and "C" should then perform the "extract" operation.
If you haven't yet, check  Yuri Shkuro's OpenTracing Tutorial .
The lesson 3 is about the context propagation, including the inject and extract operations.
I'm not quite sure how it works in the NodeJS world, but in Java, it should be sufficient to have the  opentracing-contrib/java-web-servlet-filter  instrumentation library in your classpath, as it would register the necessary pieces in the right hooks and make the trace context available for each incoming HTTP request.
It seems that PyInstaller can't resolve  jaeger_client  import.
So an easy way is to just edit your spec file and add the whole  jaeger_client  library as a  Tree  class:
And generate your executable with  pyinstaller script.spec .
You can create a  NodePort  service using the  app: jaeger  selector to expose the UI outside the cluster.
kubectl port-forward  command default is expose to  localhost  network only, try to add  --address 0.0.0.0
see  kubectl command reference
There are several ways of doing this.
The  port-forward  works fine on Google Cloud Shell.
If you are using GKE, then I strongly recommend using Cloud Shell, and  port-forward  as it is the easiest way.
On other clouds, I don't know.
What is suggesting Stefan would work.
You can edit the jaeger service with  kubectl edit svc jaeger-query , then change the type of the service from  ClusterIP  to  NodePort .
Finally, you can access the service with  NODE_IP:PORT  (any node).
If you do  kubectl get svc , you will see the new port assigned to the service.
Note: You might need to open a firewall rule for that port.
You can also make the service type  LoadBalancer , if you have a control plane to set up an external IP address.
This would be a more expensive solution, but you would have a dedicated external IP address for your service.
There are more ways, but I would say these are the appropriate ones.
This issue looks more to have to do with Java it self then either Opentracing and Jaeger.
as  ex.getStackTrace()  is more of the problem.
As it should be more like
Problem solved.
Setting a baggage item is  not  the same as setting an HTTP header.
You should use your HTTP client (not shown in your example) to set the HTTP header.
Baggage items might or not be available as individual HTTP headers: it's a detail implementation of the underlying tracer, such as Jaeger's.
Each "dot" would be a new child node in the YAML file, like:
Make sure to run the process with the env var SPAN_STORAGE_TYPE set to elasticsearch, like:
(as seen on  https://github.com/jaegertracing/jaeger/issues/1299 )
So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?
Using the sampler type as  const  with  1  as the value means that you are sampling everything.
Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes.
I would like to understand this configuration spec more but not able to.
There are several things that might be happening.
You might not be closing spans, for instance.
I recommend reading the following two blog posts to try to understand what might be happening:
Help!
Something is wrong with my Jaeger installation!
The life of a span
Your best chance is to get the data from whatever storage the Jaeger collector is using (Cassandra, Elastic.)
( https://www.jaegertracing.io/docs/1.6/deployment/  )
My suggestion is to store in Elastic and use Kibana to accomplish what you need.
Old topic but the current version of Jaeger Query UI is a single page app and has an underlying API that allows the same queries capabilities as the UI.
You're using  Camden  release train with boot  2.0  and Sleuth  2.0 .
That's completely incompatible.
Please generate a project from start.spring.io from scratch, please don't put any versions manually for spring cloud projects, and please try again.
Try using  Finchley  release train instead of  Camden
The problem is that if I kill the Docker running the jaeger collector- systemctl stop docker and later restart docker and jaegertracing/all-in-one, the services are no longer up at  http://localhost:16686/api/services
That's because you are using the in-memory storage.
If you stop and start the container, the storage is reset, so, you'll effectively lose your data.
For production purposes, you should use a backing storage like Cassandra or Elasticsearch.
Does the Jaeger collector needs to be running before starting the Jaeger clients?
No, but spans reported by clients when the collector isn't available might get dropped.
Note that clients will send spans to the agent by default, and will not contact the collector directly.
So, if the agent isn't available, spans might get dropped as well.
how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?
Use the configuration option  --memory.max-traces .
With this option, older traces will get overwritten by new ones once this limit is reached.
But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces
That's because the Jaeger client will, by default, send the spans via UDP to an agent at  localhost .
When your application is running in a Docker container, your  localhost  there is the container itself, so that the spans are lost.
As you are linking the Jaeger container with your application, you may want to get it solved by exporting the env var  JAEGER_AGENT_HOST  to  jaeger .
Turns out I don't need neither Zipkin nor Jaeger to have my traces on Stackdriver.
All that is needed is a deployment of  zipkin-collector  and a service to point to it and all my traces are now reporting as expected on GCP Stackdriver.
Not jaeger, able to send traces to zipkin server, using zipkin-simple.
Related code is in repository  https://github.com/debmalya/calculator
node-jaeger-client currently doesn't run in the browser.
There is ongoing  work  to make jaeger-client browser friendly.
This issue:  readFileSync is not a function  contains relevant information to why you're seeing the error message.
Essentially, you're trying to run jaeger-client (a nodejs library) using react-scripts which doesn't contain the modules that jaeger-client needs.
There are two issues here.
One is that your code sets the port for Jaeger client to 5775.
This port expects a different data model than what Node.js client sends, you can remove the  agentHost  and  agentPort  parameters and rely on defaults.
The second issue is that you're running the Docker image without exposing the required UDP port.
The correct command is shown in the  documentation , as of today it should be this (one long line):
Liberty does not have Open Tracing Tracer implementation for Jaeger yet.
We have a sample Tracer implementation for Zipkin.
You can find it at  https://github.com/WASdev/sample.opentracing.zipkintracer .
Jaegar claims it is backward compatible with Zipkin by accepting spans in Zipkin formats over HTTP.
Feel free to open a RFE at  https://developer.ibm.com/wasdev/help/submit-rfe/
This is most likely caused by the static assets  not  being included in the binary.
You can try that out by running the binary you compiled.
Instead of compiling on your own, a better approach would be to get the official binaries from the releases page and build your Docker container using that.
https://github.com/jaegertracing/jaeger/releases/latest
This is a limitation in the Serilog logger factory implementation; in particular, Serilog currently ignores added providers and assumes that Serilog Sinks will replace them instead.
So, the solutions is implementaion a simple  WriteTo.OpenTracing()  method to connect Serilog directly to  OpenTracing
I had this problem while using gunicorn with gevent as the worker class.
To resolve and get cloud traces working the solution was to monkey patch grpc like so
See  https://github.com/grpc/grpc/issues/4629#issuecomment-376962677
For your exact question create a character class
And then you can just add * on the end to get 0 or unlimited number of them or alternatively 1 or an unlimited number with +
or
Also there is this below, found at  https://regex101.com/  under the library tab when searching for json
This should match any valid json, you can also test it at the website above
EDIT:
Link to the regex
No, there is no out-of-the-box possibility to change the HTTP header name.
However, you can enable B3 header propagation with  opentracing.jaeger.enable-b3-propagation=true .
To configure Traefik to send the trace data as B3 headers, see  https://github.com/containous/traefik/blob/master/docs/content/observability/tracing/jaeger.md#propagation .
traceContextHeaderName  should also be configured as  X-B3-TraceId  then.
There's an ongoing discussion over here -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/599  .
In general we don't explicitly use the OpenTracing API but we are Zipkin compatible in terms of header propagation.
You can also manipulate the header names as you wish so if any sort of library you're using requires other header names for span / trace etc.
then you can set it yourself as you want to.
Spring Sleuth is now OpenTracing compatible.
All you have to do is use OpenTracing Jars in your class path.
You can then use Sleuth-Zipkin to send instrumentation data to Jaeger's Zipkin collector.
This way you achieve everything you want with minimal configuration.
You can use my sample program as an example here:
https://github.com/anoophp777/spring-webflux-jaegar-log4j2
It looks like this is not currently possible with Cordova 3.4 when attempting to read a video file out of the application assets.
See  https://issues.apache.org/jira/browse/CB-6079
It is possible to read the file if its copied to a directory outside the application assets, or the file is stored remotely.
But not in the app assets folder any longer.
I have a similar issue - my application has a welcome screen with a short video explaining the application (~300k) which I cannot play out of the APK itself.
jquery.limitkeypress.js has some issue with ie, I recommend you to use a more powerful library.
http://github.com/RobinHerbots/jquery.inputmask
With this library you can use something like this:
It works perfectly on ie.
:)
Sorry i have not updated that plugin in a few years but...
jquery.limitkeypress  now works with IE9+ there was an issue with how the selection was determined.
IE11 killed support for their document.selection but they kept the document.setSelectionRange which i was using to test what browser was being used...
IE9 enabled document.selectionStart and document.selectionEnd so i now check directly what browser version of IE peoples are using...
I added this to check for IE version:
So my selection functions now look like this:
After few days of digging I've figured it out.
Problem is in the format of the  x-request-id  header that nginx ingress controller uses.
Envoy proxy expects it to be an UUID (e.g.
x-request-id: 3e21578f-cd04-9246-aa50-67188d790051 ) but ingrex controller passes it as a non-formatted random string ( x-request-id: 60e82827a270070cfbda38c6f30f478a ).
When I pass properly formatted x-request-id header in the request to ingress controller its getting passed down to envoy proxy and request is getting sampled as expected.
I also tried to remove
x-request-id header from the request from ingress controller to ServiceA with a simple EnvoyFilter.
And it also works as expected.
Envoy proxy generates a new x-request-id and request is getting traced.
The demo you tried is using older configuration and opencensus which should be replaced with otlp receiver.
Having said that this is a working example
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node/docker 
So I'm copying the files from there:
docker-compose.yaml
collector-config.yaml
prometheus.yaml
This should work fine with opentelemetry-js ver.
0.10.2
Default port for traces is 55680 and for metrics 55681
The link I posted previously - you will always find there the latest up to date working example:
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node 
And for web example you can use the same docker and see all working examples here:
 https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/tracer-web/
Thank you sooo much for @BObecny's help!
This is a complement of @BObecny's answer.
Since I am more interested in integrating with Jaeger.
So here is the config to set up with all Jaeger, Zipkin, Prometheus.
And now it works on both front end and back end.
First both front end and back end use same exporter code:
docker-compose.yaml
collector-config.yaml
prometheus.yaml
Per the log file, there are more than 10,000 started threads.
That's  a lot  even if we don't look at the less that 2 CPUs/cores reserved for the container (limits.cpu = request.cpu = 1600 millicores).
Each thread, and its stack, is allocated in memory separate from the heap.
It is quite possible that the large number of started threads is the cause for the OOM problem.
The JVM is started with the Native Memory Tracking related options ( -XX:NativeMemoryTracking=detail, -XX:+UnlockDiagnosticVMOptions, -XX:+PrintNMTStatistics)  that could help to see the memory usage, including what's consumed by those threads.
This doc  could be a starting point for Java 11.
In any case, it would be highly recommended to  not  have that many threads started.
E.g.
use a pool, start and stop them when not needed anymore...
There are two reasons a container is OOM Killed: Container Quota and System Quota.
OOM Killer  only  triggers with memory related issues.
If your system is far from being out of memory, there is probably a limit in your container.
To your process inside the pod, the pod resource limit is like the whole system being OOM.
Also, it's worth checking the Resource Requests because by default they are not set.
Requests must be less than or equal to container limits.
That means that containers could be overcommitted on nodes and killed by  OOMK if multiple containers are using more memory than their respective requests at the same time.
In my case the issue was with debugger component that is located in CMD line of Docker file
After removal application stopped leaking.
But disappeared only native memory leak.
As later investigated there also was heap memory leak induced by jaegger tracer component (luckily here we have much more tools).
After its removal application became stable.
I don't know if those components were leaky by itself or with combination with other components but fact is that now it is stable.
Istio have this feature called  Distributed Tracing , which enables users to track requests in mesh that is distributed across multiple services.
This can be used to visualize request latency, serialization and parallelism.
For this to work Istio uses  Envoy Proxy - Tracing  feature.
You can deploy  Bookinfo Application  and see how  Trace context propagation  works.
If you have the same issue explained in this ticket, you need to wait for the next release of micronaut or use the workaround mentioned by micronaut guys there.
https://github.com/micronaut-projects/micronaut-core/issues/2209
latest  is just a tag like any other -- you will want  docker image inspect , which will give you information about the other tags on your image.
In the case of  jaegertracing/jaeger-agent:latest , it doesn't look this image has any other tags, so it's probable that this image is tracking something like the master branch of a source control repository, i.e., it doesn't correspond to a published version at all.
As @max-gasner mentioned, it's common for  latest  to be tracking the  master  branch of a git repository.
This allows the engineers to quickly build and test images before they are released and version tagged.
This is one of the reasons why it's not recommended to ever use  latest  tags for anything critical where you need reproducibility.
jaegertracing/jaeger-agent:latest  doesn't have any other tags so the only way to determine which "version" of  latest  you are using is to look at the digest.
This uniquely identifies the image build.
Tags actually resolve to digests.
So when a new image is built with the  latest  tag, that tag will then resolve to the digest of the new image.
DockerHub only shows the short version.
You can inspect the full digest like this:
There are similar ideas in this zipkin/brave repo by @jeqo.
https://github.com/jeqo/brave/tree/kafka-streams-processor/instrumentation/kafka-streams
There also seems to be something available in opentracing-contrib repo but it seems to only at trace producer/consumer level.
https://github.com/opentracing-contrib/java-kafka-client/tree/master/opentracing-kafka-streams
As, you can see there are few missing components - There are few pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc.
These components were available in 1.4.2.
These components where merged with version 1.5 into one service named  istiod .
See:  https://istio.io/latest/blog/2020/istiod/
In 1.4.2 installation I could see grafana, jaeger, kiali, prometheus, zipkin dashboards.
But these are now missing.
These AddonComponents must be installed manually and are not part of  istioctl  since version 1.7.
See:  https://istio.io/latest/blog/2020/addon-rework/
So your installation is not broken.
It's just a lot has changed since 1.4.
I would suggest to go through the release announcements to read about all changes:  https://istio.io/latest/news/releases/
Iv finally found the solution.
It seemed to have to do with how the reporter is started up.
Anyhow, I changed my tracer class to this.
I know there are several inactive variables here right now.
Will see if they still can be of use some how.
But none is needed right now to get it rolling.
Hope this might help someone else trying to get the .NET Core working properly together with a remote Jeagertracing server.
I use  io.opentracing.contrib:opentracing-spring-rabbitmq-starter:3.0.0 .
You can find some documentation  here .
With this mechanism I achieved exactly what you asked for.
I found 2 important things to make sure:
Hopefully you find this helpful.
Ugh.
I am an idiot.
Here is what was going wrong for anyone else who might be stuck something like this:
The frontend application  is  receiving a header, I was just looking in the wrong place.
The request comes from the load balancer to the node frontend microservice which sends its response to the browser.
I was checking the browser for the header, but the node frontend microservice was not forwarding this header to the browser.
If anyone is interested; I ended up solving this by creating some publish and consume MassTransit middleware to do the trace propagation via trace injection and extraction respectively.
I've put the solution up on GitHub -  https://github.com/yesmarket/MassTransit.OpenTracing
Still interested to hear if there's a better way of doing this...
In istio 1.1, the default sampling rate is 1%, so you need to send at least 100 requests before the first trace is visible.
This can be configured through the  pilot.traceSampling  option.
OpenTracing is the API that  your  code will interact with directly.
Basically, your application would be "instrumented" using the OpenTracing API and a concrete tracer (like Jaeger or Brave/Zipkin) would capture the data and send it somewhere.
This allows your application to use a neutral API throughout your code so that you can change from one provider to another without having to change your entire code base.
Another way to think about it is that OpenTracing is like SLF4J in the Java logging world, whereas Jaeger and Zipkin are the concrete implementations, such as Log4j in the Java logging world.
Trace context propagation might be missing.
https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-context-propagation
I cannot test your code, but the only thing i can think off, is that the order of execution is wrong.
You first make a string, then you make it Base64, then you encrypt it.
Now you undo the Base64 and afterwards you decrypt the encoded string.
These last two must be swapped.
A reminder that  security is unusually treacherous territory , and if there's a way to call on other well-tested code even more of your toplevel task than just what Go's OpenPGP package is handling for you, consider it.
It's good that at least low-level details are outsourced to  openpgp  because they're nasty and so so easy to get wrong.
But tiny mistakes at any level can make crypto features worse than useless; if there's a way to write less security-critical code, that's one of the best things anyone can do for security.
On the specific question: you have to  Close()  the writer to get everything flushed out (a trait OpenPGP's writer shares with, say,  compress/gzip 's).
Unrelated changes: the way you're printing things is a better fit  log.Println , which just lets you pass a bunch of values you want printed with spaces in between (like, say, Python  print ), rather than needing format specifiers like  "%s"  or  "%d" .
(The "EXTRA" in your initial output is what Go's  Printf  emits when you pass more things than you had format specifiers for.)
It's also best practice to check errors (I dropped  if err != nil s where I saw a need, but inelegantly and without much thought, and I may not have gotten all the calls) and to run  go fmt  on your code.
Again,  I can't testify to the seaworthiness of this code or anything like that.
But now it round-trips all the text.
I wound up with:
You could try with  StartSpanFromContext , inside your gRPC handlers:
As the documentation of  otgrpc.OpenTracingServerInterceptor  says:
[...] the server Span will be embedded in the context.Context for the
application-specific gRPC handler(s) to access.
If we look at the function implementation:

 Edit : Given the above, you probably can omit this code:
The issue was related to  yaml  file parsing
Jaeger has a UI to look at your data, but no tools to create statistics.
However all your data is being stored in a DB of your choice.
Storing it in e.g.
Elasticsearch gives you a powerful query language to look at the data as well as many other tools that integrate with it.
Correct me, if I'm wrong.
If you mean how to find the trace-id on the server side, you can try to access the OpenTracing span by  get_active_span .
The trace-id, I suppose, should be one of the tags in it.
I had missed a key piece of documentation.
In order to get a trace ID, you have to create a span on the client side.
This span will have the trace ID that can be used to examine data in the Jaeger UI.
The span has to be added into the GRPC messages via an  ActiveSpanSource  instance.
Of course, you could switch the ordering of the  with  statements so that the span is created after the GRPC channel.
That part doesn't make any difference.
You assumption is correct, the elements are there, but not exactly where you think they are.
To easily check if an element is part of the response html and not being loaded by javascript I normally recommend using a  browser plugin to disable javascript .
If you want the images, they are still part of the html response, you can get them with:
the main image appears separately:
Hope that helps you.
You haven't initialized the variables for the next few iterations.
You need to reinitialize the variables used for while loop's condition check outside their respective while loops.
i.e
Similarly do it for the while loops which use variables  c  &amp;  d .
Daniel's answer is correct  : the 
structure of the  while  loop should be:
It may not be possible to input text with conditional formatting, but you can change the font color.
A solution could be to put the word "LATE in the specified cell(s) beforehand and set the font-color equal to the background-color, which makes the word invisable.
When the condition (formula) evaluates true, the new format will change the font-color and the word LATE appears.
No VBA requiered.
On the other hand: wouldn't a simple if-formula be better?
Something like:
If you wish you can then change the background with conditional formating
There's really not many options for other than starting a span in each function you'd like to instrument:
If your functions have a common call signature, or you can coalesce your function into a common call signature, you can write a wrapper.
Examples of this can be seen in http  "middleware" .
Consider the http.Handler, you could write a  decorator  for your functions that handles the span lifecycle:
A similar pattern could be applied by  embedding  structs.
Is it possible to set these when working on OpenShift?
Yes, you can configure it for the Che master of your installation.
OpenShift is the Saas Che offering
As a user of che.openshift.io you can't leverage from tracing capabilities of Che at this moment.
This 'appears' to be related to the switch from AWS CNI to weave.
CNI uses the IP range of your VPC while weave uses its own address range (for pods), so there may be remaining iptables rules from AWS CNI, for example.
Internal error occurred: failed calling admission webhook "pilot.validation.istio.io": Post  https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s : Address is not allowed
The message above implies that whatever address  istio-galley.istio-system.svc  resolves to, internally in your K8s cluster, is not a valid IP address.
So I would also try to see what that resolves to.
(It may be related to  coreDNS ).
You can also try the following  these steps ;
Basically, (quoted)
Furthermore, you can try reinstalling everything from the beginning using weave.
Hope it helps!
OpenTracing is a framework for Distributed Tracing.
As such, it is more about performance monitoring and observability than logging (what NLog is about).
OpenTracing allows you to manually instrument your code to generate traces with relevant spans containing information about code execution in your app.
This includes annotating spans with errors and arbitrary keys and values, which you  could  use instead of logging.
However, that's not the same as dedicated structured logging.
Here's an article/guide on how to work with the limit-ranger and its default values [1]
[1] https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b
The  span.kind=server  tag denotes an entry span, e.g.
a span created in the local code in response to an external request.
Likewise,  span.kind=client  denotes an exit span, e.g.
a call made from the local code to another server.
In your example, the span generated for Foo is a  span.kind=server  and the span recording the call to Buzz is a  span.kind=client .
Kubernetes provides quite a big variety of Networking and Load Balancing features from the box.
However, the idea to simplify and extend the functionality  of  Istio sidecars  is a good choice as they are used for automatic injection into the Pods in order to proxy the traffic between internal Kubernetes services.
You can implement  sidecars  manually or automatically.
If you choose the manual way, make sure to add the appropriate parameter under Pod's annotation field:
Automatic  sidecar  injection requires  Mutating Webhook admission controller , available since Kubernetes version 1.9 released, therefore  sidecars  can be integrated for Pod's creation process as well.
Get yourself familiar with this  Article  to shed light on using different monitoring and traffic management tools in Istio.
Yes - it is possible to use external services with istio.
You can disable grafana and prometheus just by setting proper flags in values.yaml of istio helm chart (grafana.enabled=false, etc).
You can check  kyma-project  project to see how istio is integrated with prometheus-operator, grafana deployment with custom dashboards, and custom jaeger deployment.
From your list only certmanager is missing.
I am not sure why Istio doesn't automatically trace your calls to external APIs.
Perhaps it requires an egress gateway to be used, I'm not sure.
Note also that Istio creates traces for http(s) traffic, not TCP.
However, this is something you can still do programmatically.
You can use any of the  Jaeger client libraries  to augment&quot;the traces already created by Envoy by appending your own spans.
To do so, you need first to extract the trace context from the HTTP headers of the incoming request (assuming that your external API calls are consecutive to an incoming request), and then create a new span as child of that previous span context.
A good idea would be to use  OpenTracing semantic conventions  when you tag your new span.
Tools like Kiali will be able to leverage some information if it follows this convention.
I've found this blog post that explains how to do it with the nodejs jaeger client:  https://rhonabwy.com/2019/01/06/adding-tracing-with-jaeger-to-an-express-application/
Yes, the OpenCensus collector should be injected with the Linkerd proxy because the proxies themselves send the span info using mTLS.
With mTLS, the sending (client) and receiving (server) sides of the request must present certificates to each other in to  verify  that identities to each other in a way that validates that the identity was issued by the same trusted source.
The Linkerd service mesh is made up of the control plane and the data plane.
The control plane is a set of services that run within the cluster to implement the features of the service mesh.
Mutual TLS (mTLS) is one of those features and is implemented by the  linkerd-identity  component of the control plane.
The data plane is comprised of any number of the Linkerd proxies which are injected into the services in the application, like the OpenCensus collector.
Whenever a proxy is started within a pod, it sends a certificate signing request to the  linkerd-identity  component and receives a certificate in return.
So, when the Linkerd proxies in the control plane send the spans to the collector, they authenticate themselves with those certificates, which must be verified by the proxy injected into the OpenCensus collector Pod.
This ensures that all traffic, even distributed traces, are sent securely within the cluster.
In your case, you should suffix the service account with the namespace.
By default, Linkerd will use the Pod namespace, so if the service account doesn't exist in the Pod namespace, then the configuration will be invalid.
The  logic  has a function that checks for a namespace in the annotation name and appends it, if it exists:
So, this one is correct:
you are using System.Configuration namespace which causes ambiguity.
i would suggest remove the using System.Configuration.
And try specifying fully qualified name for the Configuration.
visual studio would suggest possible candidates (press Ctrl .
on the Class name you want to qualify) provided you have added all required references in project already.
If you have a
then the C# compiler gets confused with  Configuration  and thinks it refers to the namespace  System.Configuration .
You can solve it by using the explicit namespace  Jaeger :
There is no possibility of doing it in the Dockerfile if you want to keep two separate image.
How should you know in advance the name/id of the container you're going to link ?
Below are two solutions :
I recommend you using  netwoking , by creating:
and then run with --network=&quot;network&quot;
using docker-compose with  network  and link to each other
example:
-I  is used for  include  paths.
Use  -L  for library paths:
It also looks like you've linked with a shared library  libyaml-cppd.so  - not the static library  libyaml-cpp.a .
I don't recognize the  d  in  libyaml-cppd.so  though.
I'd check if that's really the library you built.
libyaml-cpp  will be built as a static library by default ( libyaml-cpp.a ) and on a 64 bit machine, it will probably default to being installed in  /usr/local/lib64 .
You are only allowed to do very limited things in  namespace std .
Adding new functions/classes are not allowed (unless as template specializations including user defined types) - so remove  namespace std { ... }  around your program.
Also.
the  main  function should be in the global namespace.
The reason it's not found by the linker is because you put it in a namespace ( std ).
UPDATE : The issue is resolved follow this link exaclty  https://github.com/jaegertracing/jaeger-client-cpp/issues/162#issuecomment-565892473  (use thrift version 0.11 or 0.11+)
Fisrt of all according to istio  documentation  Prometheus is used as default observation operator in istio mesh by default:
The  default Istio metrics  are defined by a set of configuration artifacts that ship with Istio and are exported to  Prometheus  by default.
Operators are free to modify the shape and content of these metrics, as well as to change their collection mechanism, to meet their individual monitoring needs.
So by having istio injected prometheus operator You end up with two Prometheus operators in Your istio mesh.
Secondly, when you enforce Mutual TLS in Your istio mesh every connection has to be secure ( TLS ).
And as You mentioned it works when there is no istio injection.
So the most likely cause is that the readiness probe fails because it is using  HTTP  protocol which is insecure (plain text) and this is one of the reason why You would get  503  error.
If you really need prometheus operator within istio mesh, this could be fixed by creating  DestinationRule  to  Disable  tls mode just for the readiness probe.
Example:
Note: Make sure to modify it so that it matches Your namespaces and hosts.
Also there could be some other prometheus collisions within mesh.
The other solution would be not to have prometheus istio injected in the first place.
You can disable istio injection in prometheus namespace by using the following commands:
I was able to publish some spans so I could see them on  http://localhost:16686
This is the updated main function:
go run  compiles and runs the named main Go package.
Only  go build  or  go install  would compile the packages named by the import paths, along with their dependencies,
I guess if you set sampler to 0 in the configuration then no traces will be captured.
https://github.com/jaegertracing/jaeger-client-java#testing
But it's specific to Jaeger.
Otherwise you can use NoopTracer like  Tracer tracer = NoopTracerFactory.create();   Maven
You can set the service name in the code as follows:
Based on some of the hints provided by Opentelemetry Java community, created two providers which indeed creates two services.
Reusing same exporter, so that multiple connections to the backend can be avoided.
I will learn more about  reusing exporter to create two or more provides in the same application in coming days.
Thank you to amazing Opentelemetry java community - @John Watson (jkwatson), Bogdan Drutu, Rupinder Singh, Anuraag Agrawal.
Based on this:
The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.
it seems that the tracing information is not propagated across services.
You can check this by looking into the HTTP headers and check the  traceId .
In order to make this work the  traceId  should be the same across the requests.
You should see the same  traceId  in the logs too.
The documentation gives you some pointers how to troubleshoot this:
I'm not 100% sure what the problem is you're experiencing, but here's some things to consider.
According to  this post on the Traefik forums , that message you're seeing is  debug  level because it's not something you should be worried about.
It's just logging that no trace context was found, so a new one will be created.
That second part is not in the message, but apparently that's what happens.
You should check to see if you're getting data appearing in Jaeger.
If you are, that message is probably nothing to worry.
If you are getting data in Jaeger, but it's not connected, that will be because Traefik can only only work with trace context that is already in inbound requests, but it can't add trace context to outbound requests.
Within your application, you'll need to implement trace propagation so that your outbound requests include the trace context that was received as part of the incoming request.
Without doing that, every request will be sent without trace context and will start a new trace when it is received at the next Traefik ingress point.
The problem actually was with the  traceContextHeaderName .
Sadly I can not tell exactly what the problem was as the  git diff  only shows that nothing changed around traefik and jaeger at the point where I fixed it.
I assume config got &quot;stuck&quot; somehow.
I tracked down the  related lines in source , but as I am no Go-Dev, I can only guess if there's a bug.
What I did was to switch back to  uber-trace-id , which magically &quot;fixed&quot; it.
After I ran some traces and connected another service (node, npm  jaeger-client  with  process.env.TRACER_STATE_HEADER_NAME  set to an equal value), I switched back to  traefik-trace-id  and things worked.
To my knowledge, the design of  ForkJoinPool.commonPool()  makes it impossible to actually replace that pool programmatically with an instrumented version.
So the only workaround is to do it via bytecode manipulation.
The  OpenTelemetry Java Automatic Instrumentation  libraries perform a lot of magic to be able to take care of correctly propagating context through async/concurrency primitives, you may want to give them a try.
Tracing is enabled by default for JAX-RS endpoints only, not for reactive routes at the moment.
You can activate tracing by annotating your route with  @org.eclipse.microprofile.opentracing.Traced .
Yes, adding @Traced enable to activate tracing on reactive routes.
Unfortunately, using both JAX-RS reactive and reactive routes bugs the tracing on event-loop threads used by JAX-RS reactive endpoint when they get executed.
I only started Quarkus 2 days ago so i don't really the reason of this behavior (and whether it's normal or it's a bug), but obviously switching between two completely mess up the tracing.
Here is an example to easily reproduce it:
Here is a screenshot that show the issue
As you can see, as soon as the JAX-RS resource is it and executed on one of the two threads available, it &quot;corrupts&quot; it, messing the trace_id reported (i don't know if it's the generation or the reporting on logs that is broken) on logs for the next calls of the reactive route.
This does not happen on the JAX-RS resource, as you can notice on the screenshot as well.
So it seems to be related to reactive routes only.
Another point here is the fact that JAX-RS Reactive resources are incorrectly reported on Jaeger.
(with a mention to a missing root span) Not sure if it's related to the issue but that's also another annoying point.
I'm thinking to completely remove the JAX-RS Reactive endpoint and replace them by normal reactive route to eliminate this bug.
I would appreciate if someone with more experience than me could verify this or tell me what i did wrong :)
EDIT 1: I added a route filter with priority 500 to clear the MDC and the bug is still there, so definitely not coming from MDC.
EDIT 2: I opened a  bug report  on Quarkus
EDIT 3: It seems related to how both implementations works (thread locals versus context propagation in actor based context)
So, unless JAX-RS reactive resources are marked @Blocking (and get executed in a separated thread pool), JAX-RS reactive and Vertx reactive routes are incompatible when it comes to tracing (but also probably the same for MDC related informations since MDC is also thread related)
I tried applying your file on a Kubernetes 1.16 cluster, and there are a couple of issues with it:
The .spec.selector field defines how the Deployment finds which Pods to manage.
It seems like you are applying something that is super old.
Kubernetes notes the below in its doc, and so I wonder if this used to work on older versions of Kubernetes where selectors were defaulted.
As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the .spec.template.
The pod selector will no longer be defaulted when left empty.
It seems like you should take a new approach-- in looking around, I found a couple of good tutorials  here  and  here , and Jaeger themselves offer a similar approach  here .
They all make use of  Kubernetes Operators .
A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user
I don't know what you mean by  &quot;So previously I had a binary executable (jaeger-agent) running on a Linux CentOS 8 box alongside a server side application&quot;
The file you are applying looks like it  deploys the agent as a daemonset , which means the agent is run as a pod on each node of your cluster.
If it is running in your k8's cluster, then  this is how I normally approach troubleshooting kubernetes services .
If it is running outside of your cluster entirely, then you need to make sure the Service it talks to is exposed outside of the cluster probably using type LoadBalancer.
Metrics in OpenTelemetry are currently undergoing continued development and refinement, so they aren't necessarily available for each language yet (see  this tag in the OpenTelemetry JS repo  for an example of metric instruments that aren't up to date yet with spec), but once they are, I'd expect for metrics to be added to the existing node/web instrumentation packages.
That said, I would still advise you to try out OpenTelemetry for traces at this point, as it's pretty stable for tracing.
You can use a Prometheus client to export metrics separately, and once OpenTelemetry metrics are fully supported in the JS library, switch over to that.
There are a few different reasons why You could be experiencing this issue.
From  istio  documentation:
Since Istio 1.0.3, the sampling rate for tracing has been reduced to 1% in the   default    configuration profile .
This means that only 1 out of 100 trace instances captured by Istio will be reported to the tracing backend.
The sampling rate in the   demo   profile is still set to 100%.
See   this section   for more information on how to set the sampling rate.
If you still do not see any trace data, please confirm that your ports conform to the Istio   port naming conventions   and that the appropriate container port is exposed (via pod spec, for example) to enable traffic capture by the sidecar proxy (Envoy).
If you only see trace data associated with the egress proxy, but not the ingress proxy, it may still be related to the Istio   port naming conventions .
Starting with   Istio 1.3   the protocol for   outbound   traffic is automatically detected.
Hope it helps.
The short answer is "you can't."
My question was based on a very fundamental misunderstanding of what opentracing does.
Tracing context is only propagated downstream, not upstream.
From the same discussion thread:
On the wire propagation is only meant to carry "span context", which
  is a small set of ID fields and possible baggage.
Returning the whole
  trace as part of the request is not a use case that was considered.
and
The trace collection is meant to be asynchronous and out of process.
So my understanding is now thus:
Each individual software component creates its own tracing data, bundles it up, and sends it off to the tracing server (e.g.
Jaeger).
Each software component  must  be configured to use the same tracing provider and the same tracing server - an RPC client cannot tell an RPC server that for a particular trace it should use the Jaeger tracing provider and a Jaeger server at such-and-such an address.
(At least, the opentracing standard doesn't provide a way to do this.)
The tracing information injected into a RPC request by the client allows the RPC server to embed a 'parent' ID field into the tracing information.
It's then the responsibility of the tracer (e.g.
Jaeger) to figure out the relationships between the various traces it has received from various software components by matching up ID codes embedded in them.
So what I wanted to do is not a use case considered by opentracing and is not possible.
My interpretation of this is that We need to keep in mind that communication between services needs to support forwarding/&quot;passing along&quot; the trace ID's so that the tracing works correctly.
So it warns us against situations where:
Client calls -&gt; Service A #using http request with trace ID in header.
Service A -&gt; Service B #using tcp request that does not support headers and the trace ID header is lost.
This situation could break or limit tracing functionality.
On the other hand If we have situation where:
Client calls -&gt; Service A #using http request with trace ID in header.
Service A -&gt; Service B #using http request the trace ID is forwarded to service B.
This situation allows for the trace ID header to be present in both connections so the tracing can be logged and then viewed in tracing service dashboard.
Then We can explore the path taken by the request and view the latency incurred at each hop.
Hope it helps.
When doing tracing in a service mesh, behind proxies, the traceID generated upon the initial client call is propagated automatically only so long as the call goes from proxy- proxy.
So:
To get around this, Microservice A just needs to know which headers represent the traceIDs, how to append into it, and some state to make sure it makes it to outgoing requests.
Then you'll get a full transaction chain.
Without the service propagating the headers, tracing basically gives you each path that ends in a microservice.
Still useful, but not as complete of a picture.
By default, you don't have a logging system on Istio.
I mean, besides the native logging of Kubernetes.
Zipkin and Jaeger are tracing systems, meaning for latency, not for logging.
You can definitely get this info through Istio components, but you will have to set it up first.
I found  this  articles; in Istio website about how to collect logs.
I would say  Fluentd  +  Elasticsearch  would give you something as powerful as you need.
Unfortunately I don't have any examples.
According to envoy proxy documentation for envoy  v1.12.0  used by istio  1.3 :
Envoy provides the capability for reporting tracing information regarding communications between services in the mesh.
However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests.
Whichever tracing provider is being used, the service should propagate the   x-request-id   to enable logging across the invoked services to be correlated.
The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood.
This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests.
This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace.
Alternatively the trace context can be manually propagated by the service:
When using the LightStep tracer, Envoy relies on the service to propagate the 
   x-ot-span-context 
  HTTP header while sending HTTP requests to other services.
When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( 
   x-b3-traceid ,
   x-b3-spanid ,
   x-b3-parentspanid ,
   x-b3-sampled ,
  and 
   x-b3-flags ).
The 
   x-b3-sampled 
  header can also be supplied by an external client to either enable or
  disable tracing for a particular request.
In addition, the single 
   b3 
  header propagation format is supported, which is a more compressed
  format.
When using the Datadog tracer, Envoy relies on the service to propagate the Datadog-specific HTTP headers ( 
   x-datadog-trace-id ,
   x-datadog-parent-id ,
   x-datadog-sampling-priority ).
TLDR: traceId headers need to be manually added to B3 HTTP headers.
Additional information:  https://github.com/openzipkin/b3-propagation
If you have sampling rate set to 1% then error will be seen in Jaeger once it occurs 100 times.
This is mentioned at  Distributed Tracing - Jaeger :
To see trace data, you must send requests to your service.
The number of requests depends on Istio’s sampling rate.
You set this rate when you install Istio.
The default sampling rate is 1%.
You need to send at least 100 requests before the first trace is visible.
To send a 100 requests to the   productpage   service, use the following command:
$ for i in  `seq 1 100`;  do  curl -s -o /dev/null http://$GATEWAY_URL/productpage;  done
If you are not seeing the error in the current sample, I would advice make the sample higher.
You can read about  Tracing context propagation  which is being done by  Envoy .
Envoy automatically sends spans to tracing collectors
Alternatively the trace context can be manually propagated by the service:
Just mentioning beforehand (you might already know) that a Kubernetes Service is not a "service" as in a piece of code.
It is a way for Kubernetes components &amp; deployments to communicate with one another through an interface which always stays the same, regardless of how many pods or servers there are.
When Istio deploys it's tracing mechanism, it deploys modular parts so it can deploy them independently, and also scale them independently, very much like micro-services.
Generally a Kubernetes deployed utility will be deployed as a few parts which make up the bigger picture.
For instance in your case:
jaeger-agent - This is the components which will collect all the traffic and tracing from your nodes.
jaeger-collector - This is the place where all of the jaeger-agents will push the logs and traces they find on the node, and the collector will aggregate these as a trace may span multiple nodes.
tracing - might be the component which injects the tracing ID's into network traffic for the agent to watch.
zipkin - could be the UI which allows debugging with traces, or replaying requests etc.
The above might not be absolutely correct, but I hope you get the idea of why multiple parts would be deployed.
In the same way we deploy mysql, and our containers separately, Kubernetes projects are generally deployed as a set of deployments or pods.
To complement @christiaan-vermeulen's answer: the  tracing  service is Jaeger's UI (jaeger-query) so that the same URL can be used for alternative backends, whereas the Zipkin service is a convenience service, allowing applications using Zipkin tracers (like Brave) to send data to Jaeger without requiring complex changes.
If you look closely, the Zipkin service is backed by the jaeger-collector as well.
I hope you have followed the official documentation of the jager with istio.
If you are using the helm chart make the following changes required.
Export the dashboard via Kube port-forward or ingress.
Official Documentation.
https://istio.io/docs/tasks/telemetry/distributed-tracing/jaeger/
NOTE: The important thing by default jaeger will trace something like 0.1% request i.e.
1 request out of 100 so put a lot of requests only then you can see a trace in UI.
I had a wrong opencensus collector configuration.
The docker container network cannot see port 9411 as it was on the host network.
I was able to fix the issue after noticing this misconfiguration.
OpenTracing does not define the concrete data model, how the data should be collected, nor how it should be transported.
As such, there's no specification for the endpoint.
This allows implementations like Jaeger to use non-HTTP transport by default when sending data from the client (tracer) to the backend, by sending UDP packets to an intermediate "Jaeger Agent".
Given that the base model is pretty much similar among implementations, it's common to have tracing solutions to support each other's endpoints.
For instance, Jaeger is able to expose an endpoint with  Zipkin compatibility .
Based on your question, I think you might be interested in the OpenTelemetry project, a successor to the OpenTracing project, as the result of a merge with the OpenCensus project.
OpenTelemetry provides its own tracer and is able to "receive" data in several formats (including Jaeger), and "export" to several backends.
Assuming that your services are  defined in Istio’s internal service registry.
If not please configure it according to instruction  service-defining .
In HTTPS all the HTTP-related information like method, URL path, response code, is encrypted so Istio  cannot  see and cannot monitor that information for HTTPS.
If you need to monitor HTTP-related information in access to external HTTPS services, you may want to let your applications issue HTTP requests and configure Istio to perform TLS origination.
First you have to  redefine  your ServiceEntry and create VirtualService  to rewrite the HTTP request port and add a DestinationRule to perform TLS origination.
The VirtualService redirects HTTP requests on port 80 to port 443 where the corresponding DestinationRule then performs the TLS origination.
Unlike the previous ServiceEntry, this time the protocol on port 443 is HTTP, instead of HTTPS, because clients will only send HTTP requests and Istio will upgrade the connection to HTTPS.
I hope it helps.
Note that tracing data (spans) are not the same as "metrics", although there could be some overlap in some cases.
I recommend the following blog post on what is the purpose of each, including logging:
https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html
That said, there is the OpenTracing library mentioned in the blog post you linked, called  opentracing-contrib/java-metrics .
It allows you to pick specific spans and record them as data points (metrics).
It works as a decorator of a concrete tracer, so, your spans would reach a concrete backend like Jaeger and, additionally, create data points based on the configured spans.
The data points are then reported via  Micrometer , which can be configured to expose this data in Prometheus format.
The problem is, I haven't found any consistent solution to do this, all the examples that I have found so far are outdated, deprecated or lacks documentation.
Please, open an issue on the  java-metrics  repository with the problems you are facing.
Just to close this question out for the solution to the problem in my instance.
The mistake in configuration started all the way back in the Kubernetes cluster initialisation.
I had applied:
the pod-network-cidr using the same address range as the local LAN on which the Kubernetes installation was deployed i.e.
the desktop for the Ubuntu host used the same IP subnet as what I'd assigned the container network.
For the most part, everything operated fine as detailed above, until the Istio proxy was trying to route packets from an external load-balancer IP address to an internal IP address which happened to be on the same subnet.
Project Calico with Kubernetes seemed to be able to cope with it as that's effectively Layer 3/4 policy but Istio had a problem with it a L7 (even though it was sitting on Calico underneath).
The solution was to tear down my entire Kubernetes deployment.
I was paranoid and went so far as to uninstall Kubernetes and deploy again and redeploy with a pod network in the 172 range which wasn't anything to do with my local lan.
I also made the same changes in the Project Calico configuration file to match pod networks.
After that change, everything worked as expected.
I suspect that in a more public configuration where your cluster was directly attached to a BGP router as opposed to using MetalLB with an L2 configuration as a subset of your LAN wouldn't exhibit this issue either.
I've documented it more in this post:
Microservices: .Net, Linux, Kubernetes and Istio make a powerful combination
Actually, there is no error.
The code was changing the color of a yellow texture to a red tint inline.
Try this : (if you can gives us all the variants of the url it would be better)
Unlike Jaeger, LightStep is a commercial SaaS offering.
If you wanted to try out their service, you'd need to contact their sales team.
Managed to create the desired capturing groups:
Then I could write out the files, it looks correct as for these few occurences.
You need to add instrumentation rules for your application to "dig deeper".
The  doFilter  and  service  methods are instrumented by default as part of the HTTP instrumentation profile.
In addition to the HTTP profile, by default inspectIT instruments SQL statement executions, other entry points into your application (like JMS), the places where external HTTP/JMS calls are done and Java Executors.
There are some other common instrumentation profiles (Hibernate, SQL parameters, Spring, etc), but you need to enable them by yourself.
Usually, you would start with the default settings and then, in addition, add instrumentation rules related to your application.
Disclaimer: I work for Instana.
There is not much to setup.
Instana provides out-of-the-box support for Kafka and Zookeeper nodes.
So all you need to do is to install the Instana agent on the server(s) you want to monitor.
It will automatically detect your Kafka and Zookeeper installations and start reporting metrics for them to your tenant unit.
If you don't have a tenant unit yet, you can register for a free trial at  https://www.instana.com/trial/  or contact Sales.
If you need additional help, I suggest to open a ticket at  https://instana.zendesk.com  to get dedicated support.
Instana has a  demo application  that shows to do this.
To summarize the parts that you would need:
The combination of these two steps will make TypeScript aware of the function.
Now you can use  ineum  just like any other global.
Instana will use the same protocol to make the sourcemap request.
The documentation example uses http, but it will work with https the same way.
The most likely reason for your problem is that the sourcemap is not readable from the public internet.
In your case, the sourcemap file requires http session authentication and redirects to a login page.
You could remove the location /nginx_status in that server, and add a new server section like this:
That endpoint requires a POST, it appears you are using GET.
Hence method not allowed.
Instana offers an agent tailored to React native, which simplifies the integration.
The  React native agent  is different than the one used for website monitoring.
You can get started with React native monitoring by creating a mobile app within Instana's user interface under  Websites &amp; Mobile Apps -&gt; Mobile Apps .
For the React native agent you can find  dedicated documentation and installation instructions  on Instana's documentation site.
For further questions and support, I suggest leveraging  Instana's support portal .
the Instana repository has been upgraded to support Disco Dingo as well.
To whoever removed his/her answer: It was a correct answer.
I don't know why you deleted it.
Anyhow, I am posting again in case someone stumbles here.
You can control frequency and time by using  INSTANA_AGENT_UPDATES_FREQUENCY  and  INSTANA_AGENT_UPDATES_TIME  environment variables.
Updating  mode  via env variable is still unknown at this point.
Look at this page for more info:  https://www.instana.com/docs/setup_and_manage/host_agent/on/docker/#updates-and-version-pinning
Most agent settings that one may want to change quickly are available as environment variables, see  https://www.instana.com/docs/setup_and_manage/host_agent/on/docker .
For example, setting the mode via environment variable is supported as well with  INSTANA_AGENT_MODE , see e.g.,  https://hub.docker.com/r/instana/agent .
The valid values are:
On Kubernetes, it is also of course possible to use a ConfigMap to override files in the agent container.
Solved.
Added flags to my run configuration, and increase XMS and XMX twice.
pod install  is  cocoapods  command, not part of  ruby  or  gem .
The error means there is no  pod  or  install  package in  ruby  package repository.
After writing a proper  Podfile  in your Xcode project dir, just run  pod install .
[Disclaimer: I work at  LightStep]
Sorry you're having trouble getting Java and Go to play well together.
I suspect this is caused by time-correction being enabled in Java but not being used in Go.
You can disable time correction in Java using the  withClockSkewCorrection(boolean clockCorrection)  
option to turn off clockCorrection when passing in options to the LightStep tracer
Here is the updated  README  and a link to the  option code
If you contact us via the [Support] button in LightStep, we should be able to get you sorted out.
Please send us a note so that we can confirm that this is solved for you.
We'll start monitoring SO more carefully so that we catch these things earlier.
Thanks and happy tracing!
Will
lightstep-opentelemetry-launcher-node  basically bundles the required things for you for easier configuration so this is not an exporter.
If you were to simply replace the &quot;LightstepExporter&quot; with &quot;OpenTelemetry Collector Exporter&quot; in your code you can simply do this
The default  YOUR_DIGETS_URL  from  lightstep/otel-launcher-node  is  https://ingest.lightstep.com:443/api/v2/otel/trace
If you go to the latest snapshot documentation (or milestone) and you search for the word OpenTracing, you would get your answer.
It's here  https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html#_opentracing
Spring Cloud Sleuth is compatible with OpenTracing.
If you have OpenTracing on the classpath, we automatically register the OpenTracing Tracer bean.
If you wish to disable this, set  spring.sleuth.opentracing.enabled  to false
So it's enough to just have OpenTracing on the classpath and Sleuth will work out of the box
Explanation how to set up  skywalking  properly:
 https://github.com/apache/skywalking/issues/3589#issuecomment-543268029
It is the dashboard default time filter value problem, the time range did not contains data:
change the time start and end to having collection data area.
Finally I build the side car image by myself:
this is the docker file:
how to add the jdbc driver jar into the image file?
One way would be an  initContainer:  and then artificially inject the jdbc driver via  -Xbootclasspath
a similar, although slightly riskier way, is to find a path that is already on the classpath of the image, and attempt to volume mount the jar path into that directory
All of this seems kind of moot given that your image looks like one that is custom built, and therefore the correct action is to update the  Dockerfile  for it to download the jar at build time
I created a lifecycle that performs the delete action after a set time, and then I added this configuration to the skywalking application.yml under  storage.elasticsearch7 :
SW creates index templates, and now I see that this is part of the template, and indeed the indexes have this sw-policy attached.
So you can see this more clearly in the output.
The pod is Running but the Ready flag is false meaning the container is up but is failing the Readiness Probe.
So you should go throught this document first
https://kubernetes.io/docs/concepts/storage/volumes/#hostpath
use  hostPath  as sample
You need reference it for both init container and normal container.
Most of the metrics stagemonitor collects are not available via JMX.
For example, response time statistics grouped by the endpoint of your application.
Also, stagemonitor is much more than just metrics.
It is also a profiler, you can use to see which methods caused a request to be slow.
Further more, it can (soon) do distributed tracing which helps you to analyze and debug latency problems in a microservice environment by correlating related requests.
It also offers you a Kibana dashboard you can use to drill into the requests your application serves to find out about causes of errors or latency.
Another use case is lightweight web analytics to identify which devices and operating systems your customers use to access your site.
Two possible solutions.
You can have a button 'hide', that will hide the metrics using some javascript code.
Or in the same button you do the following:
It doesn't appear to be compatible with Grails.
If you enable logging
you'll see a bunch of error stacktraces that appear to imply that the way they're using Javassist to wire in tracing code isn't compatible with Groovy and/or the AST transformations that Grails uses:
Finally I found out how to disable the browser widget.
Set:
You can see more information about it  here .
MySQL does not provide anything more than how much data each tenant has.
That can be found in  information_schema .
If you need CPU/IO/etc., you need to set up multiple instances of MySQL in VMs or cgroups and have the OS / VM-manager provide the data.
This will cost extra RAM, so it may not be worth it.
I'm afraid this is currently not possible.
However, stagemonitor offers a "Custom Metrics" dashboard for Grafana.
To see the metrics locally, currently the only way is to enable periodic logging of all metrics.
Stagemonitor now features a in browser widget that is automatically injected in your web page.
You don't need any infrastructure or docker for this and the configuration and set up is easy.
For more information, visit  http://www.stagemonitor.org/ .
This is how you enable the widget:  https://github.com/stagemonitor/stagemonitor/wiki/Step-2%3A-Log-Only-Monitoring-In-Browser-Widget#in-browser-widget .
The problem(s) (as noted in  GEODE-788 ,  GEODE-7665 ,  GEODE-7666 ,  GEODE-7670 ,  GEODE-7672  and  GEODE-7676 ) is, is that GemFire/Geode does not support  Region.clear()  for  PARTITION   Regions  (yet).
When you declare the  @CacheEvent(allEntries = true)  annotation/attribute on your managed application component, for example...
This in effect calls  Region.clear()  (see  here ).
This behavior works for  REPLICATE  and  LOCAL   Regions , however not for  PARTITION   Regions , given the numerous GemFire/Geode problems.
It is currently a WIP, though.
There was (partly, still is) an intention in Spring Data for Apache Geode &amp; VMware Tanzu (Pivotal) GemFire to handle cache clear operations.
https://jira.spring.io/browse/DATAGEODE-265
However, this is on hold until the above GEODE JIRA tickets get sorted out.
The short answer is no.
You really, really want to have DNS set up properly.
Here's the long answer that is more nuanced.
All requests to your foundation go through the Gorouter.
Gorouter will take the incoming request, look at the  Host  header and use that to determine where to send the request.
This happens the same for system services like CAPI and UAA as it does for apps you deploy to the foundation.
DNS is a requirement because of the  Host  header.
A browser trying to access CAPI or an application on your foundation is going to set the  Host  header based on the DNS entry you type into your browser's address bar.
The cf CLI is going to do the same thing.
There are some ways to work around this:
If you are strictly using a client like  curl  where you can set the  Host  header to arbitrary values.
In that way, you could set the host header to  api.system_domain  and at the same time connect to the IP address of your foundation.
That's not a very elegant way to use CF though.
You can manually set entries in your /etc/hosts` (or similar on Windows).
This is basically a way to override DNS resolution and supply your own custom IP.
You would need to do this for  uaa.system_domain ,  login.system_domain ,  api.system_domain  and any host names you want to use for apps deployed to your foundation, like  my-super-cool-app.apps_domain .
These should all point to the IP of the load balancer that's in front of your pool of Gorouters.
If you add enough entries into  /etc/hosts  you can make the cf CLI work.
I have done this on occasion to bypass the load balancer layer for troubleshooting purposes.
Where this won't work is on systems where you can't edit  /etc/hosts , like customers or external users of software running on your foundation or if you're trying to deploy apps on your foundation that talk to each other using routes on CF (because you can't edit  /etc/hosts  in the container).
Like if you have  app-a.apps_domain  and  app-b.apps_domain  and  app-a  needs to talk to  app-b .
That won't work because you have no DNS resolution for  apps_domain .
You can probably make app-to-app communication work if you are able to use container-to-container networking and the  apps.internal  domain though.
The resolution for that domain is provided by Bosh DNS.
You have to be aware of this difference though when deploying your apps and map routes on the  apps.internal  domain, as well as setting network policy to allow traffic to flow between the two.
Anyway, there might be other hiccups.
This is just off the top of my head.
You can see it's a lot better if you can set up DNS.
The most easy way to achieve a portable solution is a service like  xip.io  that will work out of the box.
I have setup and run a lot of PoCs that way, when wildcard DNS was something that enterprise IT was still oblivious about.
It works like this (excerpt from their site):
What is xip.io?
xip.io is a magic domain name that provides wildcard DNS
for any IP address.
Say your LAN IP address is 10.0.0.1.
Using xip.io,
mysite.10.0.0.1.xip.io   resolves to   10.0.0.1
foo.bar.10.0.0.1.xip.io   resolves to   10.0.0.1
...and so on.
You can use these domains to access virtual
hosts on your development web server from devices on your
local network, like iPads, iPhones, and other computers.
No configuration required!
It is unclear whether you're using the SCDF tile or the SCDF OSS (via  manfest.yml ) on PCF.
Suppose you're using the OSS, AFA.
In that case, you are providing the right RMQ service-instance configuration (that you pre-created) in the  manifest.yml , then SCDF would automatically propagate that RMQ service instance and bind it to the apps it is deploying to your ORG/Space.
You don't need to muck around with connection credentials manually.
On the other hand, if you are using the SCDF Tile, the SCDF service broker will auto-create the RMQ SI and automatically bind it to the apps it deploys.
In summary, there's no reason to manually pass the connection credentials or pack them as application properties inside your apps.
You can automate all this provided you're configuring all this correctly.
&quot; In this case it will wait till the processing completes or it
forcibly reduces the instance count when reached threshold.
&quot;
Answer: 
No, the App Autoscaler will not force anything, after the decision cycle, it will prepare the instance to be escalated-down (shutdown), so the intention is to avoid lose requests or data during this process.
Please, take a look into the documentation below, it will help you to understand better the App Autoscaler mechanism.
How App Autoscaler Determines When to Scale:
Every 35 seconds, App Autoscaler makes a decision about whether to
scale up, scale down, or keep the same number of instances.
To make a scaling decision, App Autoscaler averages the values of a
given metric for the most recent 120 seconds.
The following diagram provides an example of how App Autoscaler makes scaling decisions:
Reference: 
 VMWare Tanzu App Autoscaler documentation
VMWare Tanzu is the former Pivotal Cloud Foundry (PCF).
I have the same question and as far as I understood from  App Container Lifecycle  it’s up to your app to gracefully shutdown but that might not be possible in given 10 seconds as some processes might take longer.
Shutdown 
CF requests a shutdown of your app instance in the following scenarios:
When a user runs  cf scale , cf stop, cf push, cf delete, or cf restart-app-instance
As a result of a system event, such as the replacement procedure during Diego Cell evacuation or when an app instance stops because of a failed health check probe
To shut down the app, CF sends the app process in the container a SIGTERM.
By default, the process has ten seconds to shut down gracefully.
If the process has not exited after ten seconds, CF sends a SIGKILL.
By default, apps must finish their in-flight jobs within ten seconds of receiving the SIGTERM before CF terminates the app with a SIGKILL.
For instance, a web app must finish processing existing requests  and stop accepting new requests.
Note: One exception to the cases mentioned above is when monit restarts a crashed Diego Cell rep or Garden server.
In this case, CF immediately stops the apps that are still running using SIGKILL.
I think there's a workaround for kubernetes versions prior to 1.17.
On  kubernetes version v1.16  you can run Sonobuoy (Sonobuoy version v0.16.1 or higher) with providing the test framework flag:  --allowed-not-ready-nodes=1
And on  kubernetes version prior to v1.16  it was more complicated.
I haven't tested this but according to docs:
Pivotal Web Services is not the same as Pivotal Cloud Foundry.
Pivotal Web Services has been sunset, yes.
Tanzu Application Service is VMware's enterprise solution that is, if you want to think about it this way, a self-hosted Pivotal Web Services (this is a gross understatement, but works for this situation).
Are you looking to test Cloud Foundry for its suitability?
Add the AzureIdentity and AzureIdentityBinding roles to cluster as mentioned in docs.
once done, use the selector defined AzureIdentityBinding as label while deploying helm chart.
Check for the actual syntax for podLabels using with --set in helm install command.
Or you can clone the charts and make changes to values.yaml below and install it from local charts.
https://github.com/vmware-tanzu/helm-charts/blob/04615803a7d976ce15a6eeb4b1bd6a1cfb9a02c5/charts/velero/values.yaml#L27
Just for help:
 https://medium.com/@kimvisscher/using-aad-pod-identity-in-an-aks-cluster-117c08565692
It seems that you are struggling with how to format the  secretContents  section of the values.yaml file.
If that is so, take a look at a recent update to the documentation.
It lays out and documents exactly how to format it:
https://github.com/vmware-tanzu/helm-charts/blob/1ea07c7fb9c7ba910ba52801c536a6f8dcee096d/charts/velero/values.yaml#L219-L232
I found that I need to add a sampler percentage.
By default zero percentage of the samples are sent and that is why the sleuth was not sending anything to zipkin.
when I added  spring.sleuth.sampler.percentage=1.0  in the properties files, it started working.
If you are exporting all the span data to Zipkin, sampler can be installed  by creating a bean definition in the Spring boot main class
For the latest version of cloud dependencies  &lt;version&gt;Finchley.SR2&lt;/version&gt; 
The correct property to send traces to zipkin is:  spring.sleuth.sampler.probability=1.0 
Which has changed from percentage to probability.
It is a very long time ago, but it looks like it was moved here:
http://zipkin.io/pages/quickstart
Found multiple language examples at  github .
If you need basic setup steps:  https://zipkin.io/ 
Integrated zipkin with spring boot 2 and mysql 
 Steps 
 example 
Here is sample
Lately I have been trying the same and couldn't find that option in initializer.
I am just posting this if anyone encounters the same issues and lands on this page.
You can refer below sample GitHub project which is consists of four micro services ( zipkin server, client, rest service, and Eureka ) using Edgware release with latest version of sleuth.
 Sample Zipkin Server/Client
Zipkin Server is not part of Spring initializers.
You have to use the official release of the Zipkin server
https://github.com/openzipkin/zipkin#quick-start
And custom servers are not supported anymore meaning you can't use  @EnableZipkinServer  anymore since 2.7
https://github.com/openzipkin/zipkin#quick-start
I can't explain the error that you got with Dalston.BUILD-SNAPSHOT, but the error with Camden.SR4 is because it's not compatible with Spring Boot 1.5.
I'd recommend upgrading to Camden.SR5  which is compatible with Spring Boot 1.5 .
Even I got this error while setting up my project.
I was using Spring boot 1.5.8 with the Brixton.SR6 release.
However, when I consulted the site  http://projects.spring.io/spring-cloud/  I got to know the issue and I updated my dependency to Dalston.SR4 and then the application started working.
The official documentation was helpful, but I think it didn't include all the dependencies explicitly (at least as of now).
I had to do some extra research for samples to get all the required dependencies and configuration together.
I wanted to share it, because I believe it could be helpful for someone else.
Spring Boot version:   1.4.0.RELEASE
Spring Cloud version:   Brixton.SR4
POM:
Java:
application.yml:
References:
https://cloud.spring.io/spring-cloud-sleuth/
Your application starts in Tomcat Server but Zipkin use another server but i dont know that name , i include this code to spring boot starter web dependecy for ignore tomcat server
This worked for me try it
I have tested this with the official  opencensus-node  example at github.
Zipkin UI displays 2 spans with same name MyApplication(see image),
  but expected 2 different span names
Just to be clear,  MyApplication  is the service name you set in your app.js, and the span names are those which you selected on the image  /service1 ,  /service1 ,  /external_service_2 .
I think this is the intended behavior, you got one service ( MyApplication ), a root span ( /service1 ) and a child span ( /external_service_2 ).
If you got multiple services connected to the same Zipkin server then you'll have multiple names for the services.
From the Zipkin's  documentation :
Span
A set of Annotations and BinaryAnnotations that correspond to a particular RPC.
Spans contain identifying information such as traceId, spanId, parentId, and RPC name.
Trace
A set of spans that share a single root span.
Traces are built by collecting all Spans that share a traceId.
The spans are then arranged in a tree based on spanId and parentId thus providing an overview of the path a request takes through the system.
As far Zipkin UI displays 2 spans with same name, service dependencies
  page contains one Service only(see image)
Again, this is the intended behavior, since you got only one service and the external request you made goes through it.
If you mean the framed names on the first image, at the top it shows only the root span you clicked on the previous screen.
However, you can write custom span names after a little change in your code.
From the  tracing documentation  (with your code):
Now you can use  tracer.startRootSpan , I used it in the express sample with a request:
A span must be closed.
For more information, check the  test file  of the tracer.
The best way to ask for a feature is using github issues.
To add a new transport such as RabbitMQ, you'd have to affect Brave (reporter) and Zipkin (collector)
https://github.com/openzipkin/zipkin/issues 
 https://github.com/openzipkin/brave/issues
It's not suitable, Zipkin is about tracing in distributed systems.
I would say you would want something like a profiler, such as  Visual VM ,  - free and included with the JDK, or  YourKit .
Other profilers are available.
No, it  is not suitable at all .
Why?
Because the architecture of Zipkin have multiple levels of indirection: you have to send your spans into collector service and even if collector is running on your own machine there is a huge overhead -- imagine you are traveling from two neighbour cities somewhere in Europe and somebody proposes you instead of going directly from A to B, fly from A to New York, USA and back to B.
That is simply ridiculous.
As for the tools that may suit your needs: VisualVM and Yourkit mentioned by @Jonathan are good for looking at average situation in your program -- if you need to carefully inspect low level paths in your program  InTrace  might be a better choice.
Zipkin is foremost intended to provide observability into a complex distributed network of services (aka Microservice Architecture).
It wasn't intended to support just a single application.
So a profiler can often be a better way to go, particularly if you have an urgent issue to diagnose.
That said, most profilers will only provide realtime data, where Zipkin persists, allowing for analysis over large datasets.
If Zipkin is already in your stack, it's not a bad idea to enrich higher level Zipkin spans (ie a complete REST requests) with specific method calls as child-spans, particularly those that do I/O.
This way you can drill down within Zipkin to more quickly determine bottlenecks.
Otherwise you'd have to first look at the Zipkin span for high-level bottlenecks, then separately look at other logs or run a profiler, which is inefficient.
** If it's already in your stack **
It's hard to tell without more information.
But it can be related to  incompatible libraries .
Can you post your dependencies?
In case you are using  older version  of okhttpclient with  latest  spring cloud:greenwich it can cause this issue.
I'm using  Greenwich.RELEASE  with  okhttpclient:10.2.0  which works without problems
Use the below dependency Management for spring-boot to download the suitable versions for cloud version
I am using Java 10, cloud.version is  Finchley.SR2  and sprinb-boot:2.2.0 and spring-cloud-starter-openfeign :2.1.2.RELEASE.
and this combination worked for me to fix the issue.
Acctual problem was 10.x.x feign-core  was not working only  and io.github.openfeign:feign-core:jar:9.7.0:compile was working.
I faced this problem using java 11, springboot 2.3.0.RELEASE, and spring-cloud version Greenwich.RELEASE.
Adding the following dependences saved me:
Hope this helps someone.
It seems to be a timing issue.
If we add some delay, for instance, between children spans execution like
In between
Then we get to see spans:
I've faced something like this before when Zipkin dropped spans I was (mistakenly) assigning wrong timestamps to.
For reference and ease of reproduction: I've setup a  project  for reproducing this issue / "fix".
In zipkin lingo, what you are asking about is often called "local spans" or "local tracing", basically an operation that neither originated, nor resulted in a remote call.
I'm not aware of anything at the syscall level, but many tracers support explicit instrumentation of function calls.
For example, using  py_zipkin 
 
@zipkin_span(service_name='my_service', span_name='some_function')
def some_function(a, b):
    return do_stuff(a, b)
Besides explicit instrumentation like this, one could also export data to zipkin.
For example, one could convert trace data that is made in another tool to  zipkin's json format .
This probably doesn't answer your question, but I hope it helps.
The easiest way to get it working is to use the Micrometer library and configure the micrometer to send this data to the Zipkin server.
Enabling metrics using micrometer is very simple, you need to just add a micrometer-core and spring-cloud-starter-zipkin libraries.
See this tutorial for details about configuration and code  https://www.baeldung.com/tracing-services-with-zipkin
Micrometer will report consumer/producer metrics to Zipkin
If you search for zipkin grafana you'll get this as one of the first results  https://grafana.com/docs/grafana/latest/features/datasources/zipkin/
You need to reload after adding the subsystem:
This jboss-cli script should enable opentracing before starting the server properly.
I'm not sure how/when you can execute that with keycloack image
Connection refused: connect
Simply means that RabbitMQ is not running on  localhost:5672  (which is the default if you don't provide a host/port, or addresses, for it in your  application.yml ).
If service 2 is getting traceId from service 1, you can take the traceId from requestHeader in your java code.
Otherwise sleuth generate a new traceId in service 2.
To get the trace Id In java
Just do
Hello you can also get the x-b3-traceid header information from the request, I created a Util class for that -   https://gist.github.com/walterwhites/067dd635986e564aafdb5ac559073b0f
Then in your main class you just need to call
after many searches, i found that there are a version conflicts between the dependencies.
and thanks for  vladimir-vagaytsev
so, i see that  spring-cloud-starter-sleuth  imported as a different version.
to fix it i have added   sleuth.version  to properties  in pom.xml like so.
then in dependency management we need to specify the version like so
after then remove unused dependencies build and run.
This class comes from zipkin-2.
You can try adding this dependency.
Hello seeing your screenshot maybe you are using a spring boot 2.x version, I had the same problem with spring boot 2.0.3 with Finchley.RELEASE.
I found that Zipkin custom Server are not any more supported and deprecated for this reason it is not possible to use @EnableZipkinServer in Spring Cloud code and you have the ui but not the server side configured, api endpoint and so on.
form the Zipkin base code:
the code is available on  official repo of Zipkin 
I solve the my problem using the official docker image with a compose
How you can see i use the streaming version.
it for me work
I hope that is can help you
try with this:
I believe you should be able to as long as you use the fully qualified domain name.
For instance,  zipkin.mynamespace.svc.cluster.local .
Ok we found the problem and also a work around.
It looks like all the documentation out there is wrong, at least for the version of Spring Cloud Sleuth we are using.
The correct property is not  spring.sleuth.sampler.percentage .
The correct property is  spring.sleuth.sampler.probability
And here is a workaround we found right before noticing that the property was wrong.
Here are some official documentation from Spring Cloud that contain the wrong property.
https://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M5/single/spring-cloud-sleuth.html
https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html
Here is the source code that is being used and it is using  probability  not  percentage .
https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/sampler/SamplerProperties.java
Creating custom zipkin servers is an unsupported configuration, but if you must all of the configuration options are documented in the project readme:  https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md
For the dependencies part, the most important one is  zipkin-autoconfigure-storage-elasticsearch-http , here's an full maven pom.xml example:
For the configuration part, you will need the following in you  application.yml :
I configured zipkin to use ES as a data storage on top of kubernetes.
If it fits your requirement feel free to download and use  https://github.com/handysofts/zipkin-on-kubernetes
I found that these traces are actually generated by  https://github.com/spring-cloud/spring-cloud-consul/blob/master/spring-cloud-consul-discovery/src/main/java/org/springframework/cloud/consul/discovery/ConsulCatalogWatch.java
And since this is a class annotated with @scheduled , this Sleuth aspect applies :
https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/TraceSchedulingAspect.java
And therefore, the property to control the skipped regexp is not spring.sleuth.instrument.web.skipPattern , but spring.sleuth.instrument.
scheduled .skip-pattern
Of course - you have to just provide your own logging format (e.g.
via   logging.pattern.level  - check  https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html  for more info).
Then you have to register your own  SpanLogger  bean implementation where you will take care of adding the value of a spring profile via MDC  (you can take a look at this as an example  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/log/Slf4jSpanLogger.java  )
UPDATE:
There's another solution for more complex approach that seems much easier than rewriting Sleuth classes.
You can try the  logback-spring.xml  way like here -  https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/service1/src/main/resources/logback-spring.xml#L5-L11  .
I'm resolving the application name there so maybe you could do the same with active profiles and won't need to write any code?
As long as you have the ability to specify VM parameters, you can add the monitoring agent, regardless of the whether the JVM is started as a Windows service.
For perfino, that VM parameter is
please ensure config your zipkin sever correctly in your spring boot config file.
just like this:
And add below config in your zipkin client spring boot config file:
We have a  LazyTraceExecutor  that you can use -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/async/LazyTraceExecutor.java  .
There are a bunch of ways to answer this, but I'll answer it from the "one-way" perspective.
The short answer though, is I think you have to roll your own right now!
While Kafka can be used in many ways, it can be used as a transport for unidirectional single producer single consumer messages.
This action is similar to normal one-way RPC, where you have a request, but no response.
In Zipkin, an RPC span is usually request-response.
For example, you see timing of the client sending to the server, and also the way back to the client.
One-way is where you leave out the other side.
The span starts with a "cs" (client send) and ends with a "sr" (server received).
Mapping this to Kafka, you would mark client sent when you produce the message and server received when the consumer receives it.
The trick to Kafka is that there is no nice place to stuff the trace context.
That's because unlike a lot of messaging systems, there are no headers in a Kafka message.
Without a trace context, you don't know which trace (or span for that matter) you are completing!
The "hack" approach is to stuff trace identifiers as the message key.
A less hacky way would be to coordinate a body wrapper which you can nest the trace context into.
Here's an example of the former:
https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30
I meet the same problem too.Here is my solution, a less hacky way as above said.
you must implements MyHttpServerRequest and MyRequest.It is easy,you just return something a span need,such as uri,header,method.
This is a rough and ugly code example,just offer an idea.
server.address=&lt;ip&gt;  does not work?
java -jar zipkin.jar --server.address=192.168.0.7
If it's not working you can add a property file and connects to it when the server starts:
java -jar zipkin.jar --spring.config.location=./application.properties
in application.properties:
I'm not entirely sure if that's what you mean, but you can use Jeager  https://www.jaegertracing.io/   which checks if trace-id already exist in the invocation metadata and in it generate child trace id.
Based on all trace ids call diagrams are generated
In your command please try the following -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin
i had the same issue, but i solved with this jvm arguments:
-Dotel.traces.exporter=zipkin -Dotel.metrics.exporter=none -Dotel.exporter.zipkin.endpoint=http://localhost:9411/api/v2/spans
Maybe the error is on zipkin.endpoint, try to write the entire url.
Regards,
Marco
I get the same problem and below command did the trick.
I checked the source code.
It looks the property name has been changed:
https://github.com/open-telemetry/opentelemetry-java/blob/14ace1ec32dbb194b8990763beb3ab6935849547/sdk-extensions/autoconfigure/src/main/java/io/opentelemetry/sdk/autoconfigure/TracerProviderConfiguration.java#L43
Getting a handle on the distributed tracing space can be a bit confusing.
Here's a quick summary...
Open Source Tracers
There are a number of popular open source tracers, which is where Zipkin sits:
Commercial Tracers
There are also a lot of vendors offering commercial monitoring/observability tools which are either centred around or include distributed tracing:
Standardisation Efforts
Alongside all these products are numerous attempts at creating standards around distributed tracing.
These typically start by creating a standard API for the trace-recording side of the architecture, and sometimes extend to become prescriptive about the content of traces or even the wire format.
This is where OpenTracing fits in.
So it is not a tracing solution itself, but an API that can be implemented by the trace recording SDKs of multiple tracers, allowing you to swap between vendors more easily.
The most common standards are:
Note that the first two in the list have been abandoned, with their contributors joining forces to create the third one together.
[1]
[1]  https://opensource.googleblog.com/2019/05/opentelemetry-merger-of-opencensus-and.html
I was using it with default storage which is discouraged in production use, it can handle only small amount of data and can be treated only as a demo version.
What helped a little was setting
spring.sleuth.sampler.probability: 0.01
-- by default it logs all spans.
You should create application.properties file and after that you should add the following
Your application.properties :
Your main class :
Your Pom.xml :
Your zipkin port :
It looks like it is related to  https://github.com/openzipkin/zipkin-js/pull/498 , could you try with zipkin-context-cls@0.19.2-alpha.7 and change  ctxImpl  into  ctxImpl = new CLSContext('zipkin', true); ?
The problem ended up not being on Zipkin's end, but instead in how I was instrumenting the express server.
I had added the zipkin middleware  after  my call to  app.get .
Express executes middlwares in order and makes no distinction between middleware for a named route vs. something added via  app.use .
Doing things like this
Gave me the result I was looking for.
The reason behind error was that i forgot to add the kafka dependency in pom.xml
After adding the dependency, error was gone.
According to Sleuth documentation, AWS SQS is &quot;natively&quot; supported only on the consumer's side:
https://docs.spring.io/spring-cloud-sleuth/docs/current-SNAPSHOT/reference/html/#spring-cloud-aws-messaging-sqs
In order to add seamless tracing over AWS SQS I resorted to Brave SQS instrumentation (aka SqsMessageTracing) and had to add another dependency:
and have the following configuration:
This is just because I didn't want to do the SQS producer instrumentation myself nor add the tracing headers programmatically.
Little reference for the Brave instrumentation can be found here:
https://github.com/spring-cloud/spring-cloud-sleuth/issues/1550#issuecomment-589686583
My SQS message producer looks like this:
FINAL NOTE
Not required but I also excluded the
Since it performs an AWS environment configuration scan at application startup which wasn't required for me (and also raised a lengthy error log)
I think you must have found your answer by now.
But I am posting this for future reference.
Take a look at this  Github issue , it basically explains everything and provides a few workarounds.
According to  this  Spring Cloud Sleuth is the only tracer that supports messaging.
Brave is the library spring cloud sleuth is built on, therefore you could make it work without sleuth:  https://github.com/openzipkin/brave
Just to clarify though, Sleuth doesn't force you to use any of the rest of the spring-cloud components.
It is  spring-cloud  because it is one of the "cloud native" spring technologies
ok I finally realized whats the mistake that I have done when I was starting my zipkin server with this command
but I was not specifying my Zipkin server where my Kafka is running so when I did
it worked
I am new to zipkin and golang, If you want to trace internal process, then you can create span from context
example: say you have api called Login, inside login you might perform database operation or any other operations
On my project, we generated the spans manually before sending the events.
var span = tracing.tracer().nextSpanWithParent(req -&gt; true,
Void.class, ctx.get(Span.class).context());
span.name(&quot;yourSpanName&quot;).start();
return sendEventPublisher.doOnError(span::error).doOnTerminate(span::finish);
This way, we also link the span to the publisher lifecycle as we had problems with webflux sharing spans between threads.
Basically, we create a span and link it to the parent context created by Spring for the request (either from an incoming B3 HTTP header, or generated if absent).
&quot;ctx&quot; is the subscriber context here.
This also implied to tell sleuth not to generate the spans for async operations in application.properties:
spring.sleuth.async.enabled=false
For basic authentication, the username and password are required to be sent as part of the HTTP Header  Authorization .
The header value is computed as Base64 encoding of the string  username:password .So if the username is  abcd  and password is  1234 , the header will look something like this (Chatset used: UTF-8).
Authorization: Basic YWJjZDoxMjM0
Sleuth cloud project provides  ZipkinRestTemplateCustomizer  to configure the  RestTemplate  used to communicate with the Zipkin server.
Refer to the documentation for the same: 
 https://cloud.spring.io/spring-cloud-sleuth/reference/html/#sending-spans-to-zipkin
Note: Base64 encoding is reversible and hence Basic auth credentials are not secured.
HTTPS communication should be used along with Basic Authentication.
I got same problem.
Seem Spring boot  2.1.2.RELEASE  not work with Zipkin.
Please upgrade to Spring boot version &gt;  2.1.2.RELEASE .
For those who could come across with a same scenario like this,
github  has given  APIs  to get details on the repository tag set of each project release as a json object ( https://api.github.com/repos/openzipkin/zipkin/tags  ).
So that can be used to get the latest version of zipkin.
To get the currently running version of my system, zipkin has given an actuator/info end point ( http://localhost:9411/actuator/info ).
Yes.
You have to use Brave.
In fact, Spring cloud sleuth (V2) uses Brave under the hood.
Check the brave web-mvc example to get started.
https://github.com/openzipkin/brave-webmvc-example
Try to change all properties with this:
spring.sleuth.sampler.percentage=1.0 is Edgware
so you need that
baseUrl by default is localhost
When I change the project root log level to "debug", I saw some error report from zipkin.
Then I realized that the zipkin server I used was very very old.
And the zipkin API call returned 404.
When I updated my zipkin server to latest version.
It worked.
According to  the section titled Cleanup in the Istio docs :
However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans).
If I use org.slf4j.Logger to simply LOG.info("something"), I see the INFO message in console output, with the exportable flag set to true:
You can't send logs to Zipkin.
You can send log statements to ELK.
You can check out the sample  https://github.com/marcingrzejszczak/vagrant-elk-box  that has a vagrant box with ELK, uses Sleuth for log correlation and uses ELK to visualize the logs
To log only request with particular error you can add the log in your exception mapper where you are handling those error.
To show the log for error response you can set like below,
and set
You can add the trace id in your logback.xml
"request_id":
                {"trace_id":"%X{X-B3-TraceId}","span_id":"%X{X-B3-SpanId}","parent_span_id":"%X{X-B3-ParentSpanId}"},
I found the solution I think.
I changed it to this:
RABBIT_URI=amqp://user:password@tracing:5672
Please use latest snapshots.
Sleuth in latest snapshots uses brave internally so integration will be extremely simple.
This feature is available in edgware release train.
That corresponds to version 1.3.x of sleuth
The Zipkin UI is making an AJAX request to the API in order to retrieve the data that is displayed.
You can find the API definition for zipkin at:
https://github.com/openzipkin/zipkin-api
I believe you are looking for the URL:  http://zipkin.iamplus.xyz/api/v1/traces
From there you will get the traces matching your filter
I have the same config running on my ingress 9.0-beta.11.
I guess it's just a misconfiguration.
First I'll recommend you to not change the template and use the default values and just change when the basic-auth works.
What the logs of ingress show to you?
Did you create the basic-auth file in the same namespace of the ingress resource?
The issue got fixed -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/585  .
In the upcoming releases 1.1.5 and 1.2.1 it should work
Spring Cloud Zipkin Stream is using Spring Cloud Stream underneath.
You need to provide how do you want to send the spans to Zipkin - thus you need a binder.
One possible binder is the RabbitMQ binder.
Check out this:  https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/zipkin-server/build.gradle#L6
It seems that Brave does not support this.
An issue has been reported on their GitHub page.
https://github.com/openzipkin/brave/issues/166
I don't know much about ActiveMQ but you need to pass the zipkin trace information along.
Review the  ActiveMQ Collector  section in this doc 
 https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md
I just set up Zipkin tracing for a stack that includes RabbitMQ.
I added the parent_span_id, and span_id to the message header before the message is placed on the queue.
Then the applications that read the messages get the trace information from the header.
And if you need more help, I recommend jumping on IRC #zipkin.
I was not able to reproduce your issue with the  spring-cloud-sleuth-sample-zipkin  app (it worked to me), here's what I did:
A few pointers to troubleshoot this:
Try to make it work using the  sample  and try to bring the working example closer to your app (by adding dependencies) and see what is the difference between the two and where will it break.
If you can create a minimal sample app (e.g.
: based on the zipkin sample) that reproduces the issue, please feel free to create an issue on GH:  https://github.com/spring-cloud/spring-cloud-sleuth  and tag me ( @jonatan-ivanov ), I can take a look.
Finally I found it.
I had 2 problemas
1 - I was using zipkin-slim docker image for my zip container.
This image doesn't contain the rabbitmq collector  rabbitmq collector .
I have replaces by standar zipkin image
2 - I do not know why, but the connection from sleuth/zipkin to RabbitMQ is not retrying (I will investigate further).
So, if I was in a hurry and test very early (when RabbitMQ is not yet available) it fails, and not retries.
My docker-compose relevant sections now are like this:
Thanks again to  Jonatan Ivanov  for helping me!
probably best to have the issue you raised in github vs cross posting.
it is a bug  https://github.com/honeycombio/honeycomb-opentracing-proxy/issues/37
There are 2 entries in mysql zipkin_spans table
Example
32 character hex trace id  5ec92d0240cd9dee0421f4763e9f674f  displayed in zipkin ui corresponds to
trace_id_high = 6830039797584469486 in mysql  (5EC92D0240CD9DEE -  upper 16 hex character)
id = 297787839077115727 in mysql  (421F4763E9F674F -  lower 16 hex charecter)
After continue efforts and going throgh core api of spring boot application I got my solution:)
Root cause of my issue is below :
MY application using Spring boot RabitMQ integration and due that
zipkin taking 1st prefrance to RabitMQ sender and my trace are ignored
my zipkin server.
So use below configration is anyone has same issue to avoid lot painless efforts , even we are not getting in logs of server root cause of it
*---
I was using Finchley.SR2 train of releases.
Once I upgraded to the latest Spring Boot and Spring Cloud versions, the issue fixed itself.
I removed the opentracing-spring-cloud-starter dependency and am now just using
Check your configuration file and make sure the baseUrl is given properly here
OK!
I see now the problem!
So, you say that your HTTP request has these tracing headers:  X-B3-TraceId ,  X-B3-SpanId ,  X-B3-Sampled ,  X-Span-Name ,  X-B3-ParentSpanId .
Then you have this code:
And that's absolutely natural that your tracing header are not transferred to the RabbitMQ: there is just no those headers to send.
I believe that you can extract those headers in this  @RequestMapping  method and populate them to the AMQP message before sending.
See  org.springframework.amqp.core.MessageBuilder .
I also think that Spring Cloud Sleuth should have some mechanism to obtain those headers, e.g.
Tracer.currentSpan() :  http://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.RC1/single/spring-cloud-sleuth.html#_current_span
Yes, they are both stateless.
You can deploy them using whatever horizontal-scalability construct is available to you.
When connecting to the mysql container while using links, you need to use the container name as a hostname.
Change the connection string to:
And when starting the zipkin container, set the env variable:
Why are you mocking a span?
This makes absolutely no sense.
Also a Span is never a bean.
You already create a normal span via a builder and you should leave that.
Assuming that you have set up the Boot context property and  you want to mock out  tracer  bean you should do the following
*sigh, so it turns out, that someone had turned off zipking tracing in a properties file, for no good reason.
*sigh
You are trying to run 2 different applications.
To run the  zipkin  application with with ElasticSearch and Kafka you will need to run it with both sets of environment variables:
Once you have the  zipkin  server running with ES, then you can use your second command to generate the data for the dependency graph view
At the moment, there's no replacement for the "thread binder" apis.
There will be in the coming months.
This is indeed needed to renovate existing instrumentation.
Until then, you can re-use thread binders via TracerAdapter or use a different in-process propagation library.
The following link includes a working example  https://github.com/openzipkin/brave/tree/master/brave#upgrading-from-brave-3
It was removed in Sleuth 3.0, though it seems the docs were not updated, I'm going to update the docs soon.
To fix the rest with your logs, you can check the logging config  here , the  log integration in the docs  and  this answer .
This should be done out of the box:  https://docs.spring.io/spring-cloud-sleuth/docs/2.2.7.RELEASE/reference/html/#feign
You can take a look at the feign sample (you need to go back in the history, currently it is for 3.x):  https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-feign
In order to see if propagation works, look into the outgoing request, it should contain the tracing-related headers.
You could create your own  SpanHandler  bean that takes the  FinishedSpan , converts into JSON and stores it somewhere on your drive.
Then you could just iterate over jsons and upload them to the Zipkin server
No, the tracing SPI will not be backported to Vert.x 3.
I would recommend to check out  Migrate from Vert.x 3 to Vert.x 4 :
When­ever pos­si­ble, Vert.x 4 APIs have been made avail­able in
Vert.x 3 with a dep­re­ca­tion of the old API, giv­ing the
op­por­tu­nity to im­prove a Vert.x 3 ap­pli­ca­tion with a bet­ter
API while al­low­ing the ap­pli­ca­tion to be ready for a Vert.x 4
mi­gra­tion.
In other words, one of the Vert.x 4 goals was to minimize the upgrading effort.
You should use e.g.
openzipkin Brave project or Opentelemetry projects directly.
Sleuth works only with boot based projects
I think what you call correlationId is in fact the traceId, if you are new to distributed tracing, I highly recommend reading the docs of  spring-cloud-sleuth , the  introduction  section will give you a basic understanding while the  propagation  will tell you well, how your fields are propagated across services.
I also recommend this talk:  Distributed Tracing: Latency Analysis for Your Microservices - Grzejszczak, Krishna .
To answer your exact questions:
How the correlation id will be passed to Kafka messages?
Kafka has headers, I assume the fields are propagated through Kafka headers.
How the correlation id will be passed to Http requests?
Through HTTP Headers.
Is it possible to use existing tracedId from other service?
Not just possible, Sleuth does this for you out of the box.
If there is a traceId in the incoming request/message/event/etc.
Sleuth will not create a new one but it will use it (see the docs I linked above).
Your service is expecting following labels on pod:
Although it looks like you have only one label on zipkin pods:
Label selector uses logical AND (&amp;&amp;), and this means that all labels specified must be on pod to match it.
The following worked.
Sorry, I cannot provide info on all details since I don't know them :( Maybe somebody else can.
deployment.yaml
service.yaml
ingress.yaml
It's because of sampling.
Please create a bean of sampler type whose value can be Sampler.ALWAYS or set the probability property to 1.0
Hi I just resolved this issue ..
Step1.verify C:\Programfiles\Err(version) folder including bin folder is created or not.
Otherwise try to download and install Erlang again.
reinstall RabbitMQ and try connecting Zipkin.
Make sure Erlang version and RabbitMQ version is compatible.
Step2.Check ERLANG_HOME is set to proper location in environment variables.
at this point if RabbitMQ windows installer pointing to any old erlang version installed earlier  try to install RabitMQ windows manually
follow the steps mentioned in below link for manual installation
https://www.rabbitmq.com/install-windows-manual.html
That's an old implementation.
Below I have modified your code to work:
For more information check this link:  https://gist.github.com/marcingrzejszczak/d3c15a0c11dda71970e42c513c9c0e09
From  zipkin docs :
There is no support for TTL through this SpanStore.
It is recommended instead to use Elastic Curator to remove indices older than the point you are interested in.
I had the same problem and solved it by:
Open windows task manger and kill all java instances java.exe or javaw.exe
I have finally figured out what could be the cause of this issue:
The install option:
--set values.tracing.provider=zipkin --set values.global.tracer.zipkin.address
requires  &lt;zipkin-collector-service&gt;.&lt;zipkin-collector-namespace&gt;:9411  according to  istio  documentation.
While You have just IP address and port of external server.
This most likely means that the install option requires existing name that is in istio service mesh registry.
So if Your zipkin collector is outside cluster We need to add  ServiceEntry ,  VirtualService  and maybe  DestinationRule  and so the external service can be used within mesh.
You can follow  istio  documentation to see how to create these objects for external service.
Here  is another guide.
After that We need to update the tracer address value with the  VirtualService  as an endpoint.
Hope this helps.
By using the following  commands  I was able to generate the manifests using  istioctl  with parameters You mentioned:
Then compared them to see differences made with those parameter modifications.
You can try to manually modify those applied settings or apply it to Your cluster.
Istioctl I used to generate these manifests:
Hope it helps.
Still with using 2.2.0 parent, I still face the whitelable error.
I will check on this latter but by changing the pom defination the Zipkin server work
And in zipkinserverapplication we need the @Enablezipkinserver
Hi if your spring cloud app target is a Spring Boot 2.x base I suggest to do not try to use the @EnableZipkinServer because it is not a raccomanded way as the java doc suggest:
form the Zipkin base code:
I spoke for personal experience with spring boot application 2.x family.
The my solution,for development, was use a docker compose like below(consider that my application use spring cloud stream in order to push on zipkin the tracing information:
On the other hands if your target is a spring boot 1.5.x you can use the legacy embedded zipkin server like below:
POM:
Application:
package it.valeriovaudi.emarket;
both solution works for me, spring boot 1.5.x was the base spring boot version for my master thesis and spring boot 2.x version for a my personal distributed system project that I use every day for my persona family budget management.
I hope that this can help you
For more details can read the document  https://www.baeldung.com/spring-boot-actuators#boot-2x-actuator
I have a working project with spring cloud stream and zipkin using the following configuration (maybe you should set the sender.type):
Hope this can help.
The problem lies in your  ES_HOSTS  variable, from the docs  here :
So you will need:  ES_HOSTS=http://storage:9200
Finally I have this file:
Main differences are the usage of
"ES_HOSTS=elasticsearch:9300"
instead of
"ES_HOSTS=storage:9300"
and in the dependencies configuration I add the entrypoint in dependencies:
entrypoint: crond -f
  This one is really the key to not have the exception when I start docker-compose.
To solve this issue, I check the this project:  https://github.com/openzipkin/docker-zipkin
The remaining question is: why do I need to use entrypoint: crond -f
I found examples from:
 https://github.com/openzipkin/zipkin/tree/master/zipkin-lens/testdata
It works well.
Set the datasource setting in the application.yml file of the application as follows,
You can add the zipkin attribute to POM.xml
Problems can occur due to Spring's auto configuration property.
Therefore, modify the datasource setting as follows and modify the datasource configuration related source to make the application work normally.
It looks like the trace is triggering an old-fashioned inverted-lock-order deadlock freezing threads that are attempting to acquire new Connections.
The last of the three deadlocked threads is  trying to get a lock on some singleton or bean .
It has already passed through and presumably acquired a lock on a  GenericScope .
The other two threads are  trying to acquire a lock on a  GenericScope , which presumably the first thread has.
An unexpected reentrance from the  zipkin  code into spring is generating a deadlock.
c3p0  has a fixed-size thread pool that notices when all its threads (just 3 here,  c3p0 's default) are persistently frozen, then (pretty correctly in this case) declares a deadlock and replaces the blocked threads in hopes of recovering.
Does c3p0 recover?
Is this a rare or frequent deadlock?
There's not much you can easily do to prevent this deadlock, I think either you'll have to tolerate it or do without the instrumentation.
Application Insights users would also be able to leverage the distributed tracing offered through Zipkin by instrumenting their services using existing libraries.
To use the Application Insights back-end store, configure your Zipkin server instance to use the Application Insights  plug-in .
This integration makes monitoring and debugging your overall end-to-end applications much easier.
Once you have the data in Application Insights, you can always perform  cross-resource log queries  between Application Insights and Log Analytics.
Additional Documentation Reference -
Zipkin to Application Insights Module
Zipkin-Azure
Send Log Data to Azure Monitor with HTTP Data Collector API (public preview)
Hope the above information helps.
No you can't.
You can use tools like Elasticsearch Logstash Kibana to visualize it.
You can go to my repo  https://github.com/marcingrzejszczak/docker-elk  and run  ./   getReadyForConference.sh , it will start docker containers with the ELK stack, run the apps, curl the request to the apps so that you can then check them in ELK.
Alright, so after a few hours struggling, I made some progress, and now the app starts - even though the root cause of the issue is not fully clear to me at this time.
Below are my findings :
one strange thing I noticed : if I change the  sender.type  from  web  to  rabbit , then the application starts with no error.
I also found this Spring Boot  issue report , very similar to mine, that was pointing at a JDK bug.
And indeed, upgrading from  jdk1.8.0_25  to  jdk1.8.0_201  .
Finally, I also found that if I was using  jdk1.8.0_25  and wasn't providing the  sender.type  at all, then the app was also starting with no issue.
For some reason, in the other app that I have and that works, I am able to use  jdk1.8.0_25  and  sender.type: web
If anyone has a methodology to figure out this kind of issue quickly, don't hesitate to add it in the comment or edit this answer.
It makes perfect sense that it's  null .
That's because YOU control the way what happens with the caught exception.
In your case, nothing, cause you swallow that exception.
If you want to do sth better, just add the error tag manually via the  SpanCustomizer .
That way you'll add the exception to the given span.
It will then automatically get closed and reported to Zipkin (you can do sth else than  ex.toString()  of course.
Zipkin  is a solution for distributed tracing.
Specifically it allows to track latency problems in distributed system.
Also it's a greate tool for debugging/investigating problems in your application.
So by definition it requires to collect successful and failed traces.
However  traces  have nothing to do with logging.
Assuming you mean controlling the logging level of Zipkin server, then you can just set it using  --logging.level.zipkin2=INFO .
I don't understand the problem.
You don't send logs to Zipkin.
You send spans to Zipkin.
Zipkin has nothing to do with logs.
Seems to work once I added the Web package.
Though I don't recall it being needed previously.
Zipkin currently supports four types of backend storage to store spans in-memory, MySQl, ElasticSearch, Cassandra.
Although for production it is recommended to use ES or Cassandra.
The other two can be used for learning and understanding.
Traces stored in the in-memory is ephemeral and won't be available after the restart.
In the zipkin UI there is an option to see the trace and download it, which can be used to view at a later point in time.
If you still have further questions drop in to the zipkin  gitter  channel.
we also use use zipkin but can't query with zipkin as elk.
we can just click on each services which are display on zipkin and get more info as below image.
Zipkin is not a business transaction tracking system and it should not be used that way because it is not built for this purpose.
There are other tools which are built specifically to cater the needs to business operations which you must consider.
P.S.
I am a Zipkin contributor.
This is not an answer to how achieve this with zipkin but yes for the whole problem.
If you have a  transaction that didn't complete it's steps then you probably have two of following issues
Some microservice failed to deliver the event to the next one and didn't figure it out
You have to make sure delivery at least once here, using Kafka you have to wait until message get flushed to the server for example
The destiny microservice received the message and is not processing it
You have to make sure you application is processing what it's supposed to,  you can monitor the database if the transactions are there or use some tool like LinkedIn burrow to monitor your Kafka message group if you are integrating by using Kafka.
Conclusion is, instead to try monitor all the thing once it looks like creating specialist monitors for every step will be more assertive and simple to develop.
Here is the related issue:
https://github.com/openzipkin/zipkin/issues/1939
I opened a issue on the zipkin github, a theme already being treated as a bug.
Initial thread:
 https://github.com/openzipkin/zipkin/issues/2218#issuecomment-432876510
Bug track:
 https://github.com/openzipkin/zipkin/issues/2219
Tks for all!
You have to use  spring.sleuth.web.skipPattern
sample you will get here  https://www.baeldung.com/tracing-services-with-zipkin
I think to remove the service names from zipkin you have to Re-deploy the zipkin service
You will need to remove the spring.zipkin.base-url property from the corresponding applications to remove it from zipkin server list.
finally got working after spring verison updated to  5.x 
It already have  Brave Instrument for zipkin trace
If you read the docs or any information starting from edgware you would see that we've removed that support.
You should use native zipkin rabbit / kafka dependencies.
Everything is there in the docs.
If it comes from the  @Scheduled  method then you can use  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/SleuthSchedulingProperties.java#L38  ( spring.sleuth.scheduled.skipPattern ) to find the thread and disable it.
If you say its name is  async  then it means that it comes from a  TraceRunnable  or  TraceCallable .
That can be problematic to get rid off.
You can file an issue in Sleuth to allow  SpanAdjuster  to actually not send spans to Zipkin (by for example returning  null ).
You can also try to disable async at all  spring.sleuth.async.enabled .
If you're not using any other features of async that should not interfere.
Brave will work regardless of the server that you choose to use.
Remove the jetty configuration from the pom file and use the Tomcat.
If you still have trouble or want to know more about zipkin/brave connect to the community via the gitter channel.
P.S.
I contribute to OpenZipkin (Zipkin)
The UI cannot be password protected without also password protecting the API endpoints, including the one you would send your spans to.
PS: The  @EnableZipkinServer  annotation has been deprecated
EDGWARE
Have you read the documentation?
If you use Spring Cloud Sleuth in Edgware version if you read the Sleuth section you would find this piece of the documentation  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_custom_sa_tag_in_zipkin
Let me copy that for you
54.5 Custom SA tag in Zipkin Sometimes you want to create a manual Span that will wrap a call to an external service which is not
  instrumented.
What you can do is to create a span with the
  peer.service tag that will contain a value of the service that you
  want to call.
Below you can see an example of a call to Redis that is
  wrapped in such a span.
[Important]   Important Remember not to add both peer.service tag and
  the SA tag!
You have to add only peer.service.
FINCHLEY
The  SA  tag will not work for Finchley.
You have to do it in the following manner using the  remoteEndpoint  on the span.
That was a bug in Spring Cloud Sleuth in Edgware.
The Stream Kafka Binder in Edgware required explicit passing of headers that should get propagated.
The side effect of adding  sleuth-stream  on the classpath was exactly that feature.
By fixing the  https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005  issue we're adding back the missing feature to core.
This is not ported to Finchley since Stream Kafka Binder in Finchley passes all headers by default.
The workaround for Edgware is to pass a list of headers in the following manner:
The Istio sidecar proxy (Envoy) generates the first headers.
According to  https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#x-request-id :
Envoy will generate an x-request-id header for all external origin requests (the header is sanitized).
It will also generate an x-request-id header for internal requests that do not already have one.
You've mixed almost everything you could have mixed.
On the app side you're using both the deprecated zipkin server and the deprecated client.
On the server side you're using deprecated zipkin server.
My suggestion is that you go through the documentation  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_spring_cloud_sleuth  and read that the  stream servers  are deprecated and you should use the openzipkin zipkin server with rabbitmq support ( https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq ).
On the consumer side use  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka  .
It really is as simple as that.
Also don't forget to turn on the sampling percentage to 1.0
Just add below, it need to be working,
Yeah,you should use different libraries for different languages.
Brave for Java,Zipkin4net for C# and so on.
For more details,you can visit Zipkin official site:  Zipkin Existing instrumentations .
Then all you shoud do is following the librarie guide.
Have fun!
The first request uses v1 of the Zipkin api while the second uses v2 (see  https://github.com/openzipkin/zipkin/issues/1499  for the v2 specification).
Spans are broken up by kind (SERVER and CLIENT) instead of having client receive, server receive, client send, and server send annotations (hence why there are more spans).
I have a client application with multiple channels as SOURCE/SINK.
I want to send logs to Zipkin server.
Zipkin is not a tool to store logs
According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP.
No - you need the  sleuth-stream  dependency on the client side and the  zipkin-stream  dependency on the server side (which got deprecated and you should start using the inbuilt rabbitmq support from Zipkin).
At client side: Q1.
Is there an automatic configuration for zipkin rabbit binding in such scenario?
If not, what is default channel name of zipkin SOURCE channel?
Yes, there is.
The channel is  sleuth
Q2.
Do I need to configure defaultSampler to AlwaysSampler()?
No, you have the  PercentageBasedSampler  (I'm pretty sure it's written in the docs).
You can tweak its values.
At Server side: Q1.
Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using: wget -O zipkin.jar ' https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec ' ...as stated on  https://zipkin.io/pages/quickstart.html  ?
You should do the wget.
If you want to use the legacy stream support then you should create a zipkin server yourself.
Q2.
How do I configure zipkin SINK channel to destination?
If you're using the legacy zipkin stream app then it's automatically configured to point to proper destination.
You can tweak the destination as you please in the standard way Spring Cloud Stream supports it.
I'm not sure I fully understand your question but I can elaborate a bit on how istio works with respect to tracing:
Tracing means identifying every span or node that is part of the original request, so typically an Id is generated by the istio-ingress and your application should  propagate it  so each istio-proxy can capture and forward that information to istio-mixer which then lets you use Zipkin or Jaeger to visualize it.
Istio can't know when you make outcalls from your application for which original request it was for unless you do copy the headers.
Does that help/makes sense ?
It looks a incompatibility between version in my opinion, something is overridden when you inject the spring-cloud-starter-zipkin dependency
What i don't understand from your question is:
Do you need this dependency "spring-cloud-starter-zipkin", are you using it?
If no obviously just put it out of the pom, if yes, check which version are you using:
mvn dependency:tree and try to align spring-cloud-starter-zipkin with the Spring version you are using.
Playing a bit with the version of your artifacts you will find the solution.
Hope it helped.
I use &quot;TraceCallable&quot; class from &quot; spring-cloud-sleuth &quot; lib to solve it in my code.
My code example is:
Attention: This solution does works for logging purposes, but does not work for other Sleuth features like instrumenting RestTemplates to send the tracing headers to other services.
So unfortunately, this is not a fully working solution.
:(
Some time after adopting @Baca's solution, I discovered that Kotlin Coroutines offer direct integration with slf4j, which is what Spring Sleuth builds on.
Sleuth adds properties  X-B3-TraceId ,  traceId ,  X-B3-SpanId , and  spanId  to the thread's MDC.
You can retain the parent thread's MDC for a coroutine with the code shown below.
The coroutine framework will take care of restoring the MDC context on the worker thread whenever the coroutine is executed/resumed.
This is the easiest solution I could discover so far.
:)
The launch method takes an optional CoroutineContext and the coroutine-slf4j integration implements the MDCContext.
This class captures the calling thread's MDC context (creates a copy) and uses that for the coroutine execution.
Add this dependency to your build.gradle:
Project:  https://github.com/Kotlin/kotlinx.coroutines/tree/master/integration/kotlinx-coroutines-slf4j 
Documentation:  https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-slf4j/index.html
With Spring boot  Dalston.SR3  (which uses open zipkin 1.28) you can achieve this by setting property  zipkin.storage.mem.max-spans=xxx  This will limit the number of spans and discard old ones.
pom.xml
The best way to trace OpenStack project is to use Osprofiler library.
If you just want to understand the workflow or just know about the types of calls being made inside OpenStack then Osprofiler is the best and easiest way to get the trace.
Now Osprofiler is an accepted OpenStack project and the official project to get traces for OpenStack.
Instead of having to go through the whole code and adding instrumentation points near HTTP request or RPC calls, osprofiler is already integrated in all of the main projects of OpenStack (Nova, Neutron, Keystone, Glance etc..).
You just have to enable osprofiler in the configuration files of each project in OpenStack to get a trace of that particular project.
You can go through this link -  https://docs.openstack.org/osprofiler/latest/
Enabling of Osprofiler in the configuration files can be done by adding these lines at the end of the configuration file (nova.conf or neutron.conf) :
The connection_string parameter indicates the collector (where the trace information is stored).
By default it uses Ceilometer.
You can actually redirect the trace information to other collectors like Elasticsearch by changing the connection_string parameter in the conf file to the elasticsearch server.
This is by far the easiest way to get a trace in OpenStack with just minimal effort.
After some discussion with Marcin Grzejszczak and Adrien Cole (zipkin and sleuth creators/active developers) I ended up creating a Jersey filter that acts as bridge between sleuth and brave.
Regarding AMQP integration, added a new @StreamListener with a conditional for zipkin format spans (using headers).
Sending messages to the sleuth exchange with zipkin format will then be valid and consumed by the listener.
For javascript (zipkin-js), I ended up creating a new AMQP Logger that sends zipkin spans to a determined exchange.
If someone ends up reading this and needs more detail, you're welcome to reach out to me.
Take a look at Sampling interval in the docs :
In distributed tracing the data volumes can be very high so sampling can be important (you usually don’t need to export all spans to get a good picture of what is happening).
Spring Cloud Sleuth has a Sampler strategy that you can implement to take control of the sampling algorithm.
Samplers do not stop span (correlation) ids from being generated, but they do prevent the tags and events being attached and exported.
By default you get a strategy that continues to trace if a span is already active, but new ones are always marked as non-exportable.
If all your apps run with this sampler you will see traces in logs, but not in any remote store.
For testing the default is often enough, and it probably is all you need if you are only using the logs (e.g.
with an ELK aggregator).
If you are exporting span data to Zipkin or Spring Cloud Stream, there is also an AlwaysSampler that exports everything and a PercentageBasedSampler that samples a fixed fraction of spans.
http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling
You are blending Zipkin autoconfigure version 1.2 with Zipkin 1.26.
This results in a version missmatch.
The problem is casued by two reasons.
First:just as Rafael Winterhalter said the version dismatched; second: you are lacking dependency for zipkin.
Finally i fixed the problem by adding the whole 'io.zipkin.java:zipkin" library.
Here is my final pom file with the storage type of elasticsearch:
Appears that a sleuth span is not the same as a Zipkin span.
Hence, in the above code there is no way to instantiate a default tracer with zipkin span reporter.
I converted the sleuth span into a zipkin span and then reported it to zipkin.
The class to convert it is available in spring-cloud-sleuth-stream.
I used pretty much the same class with some tweaks.
Here you have a very basic example of Sleuth &amp; HTTP communication.
https://github.com/openzipkin/sleuth-webmvc-example  You can set your dependencies in a similar manner and everything should work fine.
In your example you've got Stream but I don't think you're using it so it's better to remove it.
As M.Deinum said remove  stream  and  stream-rabbit  dependencies what if you do not need some AMQP server to store the trace message.
or
config the AMQP(rabbitMQ in your code) from application-configuration(both) and add  zipkin-stream  &amp;  stream-rabbit  in  zipkin-server  side, so this time your app( zipkin-client ) will not direct connect with  zipkin-server  
and it will be:
You may define all needed params via ENV options.
Here is a cmd for running zipkin in docker:
All these params can be defined in Deployment (see  Expose Pod Information to Containers Through Environment Variables )
1.You should check if your Zipkin Server is on.
2.You should check if the Span transfering is async.
In HTTP,Zipkin uses in-band transfer,all the information carried in HTTP headers.The cost time of generating Span is about 200 nanosecond.
The TL;DR; is that  B3 propagation  was initially designed for fixed size data: carrying data ancillary to tracing isn't in scope, and for this reason any solution that extends B3 in such a fashion wouldn't be compatible with existing code.
So, that means any solution like this will be an extension which means custom handling in the  instrumented apps  which are the things passing headers around.
The server won't care as it never sees these headers anyway.
Ways people usually integrate other things like flags with zipkin is to add a tag aka binary annotation including its value (usually in the root span).
This would allow you to query or retrieve these offline, but it doesn't address in-flight lookups from applications.
Let's say that instead of using an intermediary like linkerd, or a platform-specific propagated context, we want to dispatch the responsibility to the tracing layer.
Firstly, what sort of data could work alright?
The easiest is something set-once (like zipkin's trace id).
Anything set and propagated without mutating it is the least mechanics.
Next in difficulty is appending new entries mid-stream, and most difficult is mutating/merging entries.
Let's assume this is for inbound flags which never change through the request/trace tree.
We see a header when processing trace data, we store it and forward it downstream.
If this value doesn't need to be read by the tracing system, it is easiest, as it is largely a transport/propagation concern.
For example, maybe other middleware read that header and it is only a "side job" we are adding to the tracer to remember certain things to pass along.
If this was done in a single header, it would be less code than a pattern in each of the places this would be to added.
It would be even less code if the flags could be encoded in a number, however unrealistic that may be.
There are libraries with apis to manipulate the propagated context manually, for example,  "baggage" from brownsys  and OpenTracing (of which some libraries support zipkin).
The former aims to be a generic layer for any instrumentation (ex monitoring, chargeback, tracing etc) and the latter is specific to tracing.
OpenTracing has defines abstract types like  injector and extractor  which could be customized to carry other fields.
However, you still would need a concrete implementation (which knows your header format etc) in order to do this.
Unless you want applications to read this data, it would need to be a secret detail of that implementation (specifically the trace context).
Certain zipkin-specific libraries like Spring Cloud Sleuth and Brave have means to  customize how headers are parsed , to support variants of B3 or new or site-specific trace formats.
Not all support this at the moment, but I would expect this type of feature to become more common.
This means you may need to do some surgery in order to support all platforms you may need to support.
So long story short is that there are some libraries which are pluggable with regards to propagation, and those will be easiest to modify to support this use case.
Some code will be needed regardless as B3 doesn't currently define an expression like this.
In general it is better to use the zipkin's http variant of Elasticsearch as it cannot conflict with Spring Boot's elasticsearch library versions.
I would set everything in zipkin's group id to latest (currently 1.21.0 which is Spring Boot 1.4.x) and use zipkin-autoconfigure-storage-elasticsearch-http (plus.. the one you are using  will be dropped )
Make sure your es hosts is specified in url syntax ex.
http://host1:9200
Zipkin generates traces and communicates them back to a Zipkin server.
The latter action is typically performed via HTTP but Zipkin is agnostic to how a span is started and ended.
If you want to measure the execution time of a single method, you would normally create a local span that is nested inside a server span.
Zipkin is a distributrd tracing tool for discovering machine-to-machine interaction which is why spans often cover HTTP.
If you want to measure execution time of a method, a tool like metrics might be more suited.
This was an issue with MySQL 5.7 and more recently resolved.
You can try latest Zipkin.
You'd have to implement your own ZipkinSpanReporter that would look more or less like  https://github.com/spring-cloud/spring-cloud-sleuth/blob/v1.0.8.RELEASE/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java  .
In the next version of Sleuth you will be able to register a bean of ZipkinSpanReporter that can you a custom version of a publisher -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/1.0.x/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java
Instrumenting a library is something that sometimes folks have to do for one reason or another.
There are several tracer libraries in Java but the salient points about creating a tracer are either on the website, or issues on the website.
http://zipkin.io/pages/instrumenting.html 
 https://github.com/openzipkin/openzipkin.github.io/issues/11
OpenTracing also has some nice fundamentals to look at  http://opentracing.io/
This isn't a one-answer type of question, in my experience, as you'll learn you'll need to address other things that tracer libraries address out-of-box.
For that reason I'd highly suggest joining gitter so that you
can have a dialogue through your journey  https://gitter.im/openzipkin/zipkin
I was recording wrong annotation i.e client instead of server.
Just a simple change did the trick.
Trace.traceService("Function1","Test")
Sample working Zipkin example:  https://gist.github.com/AkhilJ876/3e38757c28d43924f296dd2d147c0bd9#file-zipkintracing_example-L34
Same problem here with the docker images using Cassandra (1.40.1, 1.40.2, 1.1.4).
This is a problem specific to using Cassandra as the storage tier.
Mysql and the in-memory storage generate the dependency graph on-demand as expected.
There are references to the following project to generate the Cassandra graph data for the UI to display.
This looks to be superseded by ongoing work mentioned here
If storage type is other than inline storage, for zipkin dependencies graph you have to start separate cron job/scheduler which reads the storage database and builds the graph.
Because zipkin dependencies is separate spark job .
For reference :  https://github.com/openzipkin/docker-zipkin-dependencies
I have used zipkin with elastic search as storage type.
I will share the steps for setting up the zipkin dependencies with elastic search and cron job for the same:
Other solution is to start a separate service and run the cron job
  using docker
Steps to get the latest zipkin-dependencies jar try running given
  command on teminal
you will get jar file at above mention directory
Dockerfile
entry.sh
script.sh
crontab.txt
This is due to not having an instance of the query server running.
I'm in the middle of a re-write that'll simplify all of this.
Until then, you need to spin up a query server.
I would say you can have fluentd in multiple namespaces and Elasticsearch in one namespace and fluentd can discover Elasticsearch via K8s internal DNS A/AAAA record e.g.
elasticsearch.${namespace}.svc.cluster.local .
I don't have any link to the best practice, but I would show you a practice I saw from the community.
If you are not familiar with configuring K8s cluster, I recommend to deploy ELK by Helm.
It will save you a lot of time and give you enough configuration options.
https://github.com/helm/charts/tree/master/stable/elastic-stack .
Install your ELK helm release on a  separate  namespace, for example:  logging .
Install fluentd in any namespaces in your cluster and configure elasticsearch host  https://github.com/helm/charts/tree/master/stable/fluentd-elasticsearch
I used an Aspect and from the returned ResponseEntity object, decided whether or not to programatically add an error tag to span.
With this tag, zipkin will identify and highlight the trace in red colour.
Below is the code snippet to add error tag to span.
i think i found a suitable way to do this.
After further thinking my idea went into the direction of using annotations and aspects for intercepting HTTP requests from/to thrifts http client which seems to be quite some work.
after further search, i found this library for spring-boot serving exactly my needs:
 https://github.com/aatarasoff/spring-thrift-starter
I dont kown can you see my pic and I put my code:
pom.xml:
ZipkinApplication.java:
The error:
I can say your YAML has some bad indentation and things are not in the right sections even.
Otherwise though, you are trying to run Zipkin in an unsupported configuration.
Please check out our quickstart documentation:  https://zipkin.io/pages/quickstart.html
There are 2 approaches to this
Looking at your yml file you have added
which means your approach is 2.
But then in your pom, you have added  zipkin-server  and  zipkin-autoconfigure-ui  dependencies which is not required.
I will try to separate both setups
1.
To Start Zipkin server with SpringBootApplication
pom.xml
application.properties
Application.java
2.
To Start Zipkin server as a standalone and use SpringBootApplication as Zipkin Client
Start Zipkin server
pom.xml
application.properties
Edit 1:
@EnableZipkinServer  is deprecated and unsupported as per Brian Devins's comment.
So, please go through the  doc  for more detail info.
You're using an ancient version of Spring CLoud.
Please upgrade to latest Edgware.
The RxJava support is very basic so we suggest that you use Project Reactor.
To do that just migrate to Finchley and it should work out of the box with WebFlux.
You're using an ancient version of Sleuth, can you please upgrade?
Why do you provide Zipkin's version manually?
Also as far as I see you're using the Sleuth's Zipkin server (that is deprecated in Edgware and removed in Finchley).
My suggestion is that you stop using the Sleuth's Stream server (you can read more about this here  https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka ).
1) In order not to pick versions by yourself it’s much better if you add the dependency management via the Spring BOM
2) Add the dependency to spring-cloud-starter-zipkin - that way all dependent dependencies will be downloaded
3) To automatically configure rabbit, simply add the spring-rabbit dependency
the simplest solution i have found for solving broken ui for zipkin thru gateway
is by changing following property of zipkin-server-shared.yml file inside zipkin server
zipkin:
  ui:
   base-path: /zipkin
 
change above property to
zipkin:
  ui:
   base-path: /api/tracing/zipkin
and change ur zuul path to following
 zuul.routes.zipkin.path=/api/tracing/*
and than access zipkin using follwing url
https://gatewayhost:port/api/tracing/zipkin/
give attention to small details in config and dont forget to put trailing "/" after zipkin  in url
It has nothing to do with Spring Cloud Sleuth or Zipkin.
@SpringBootApplication automatically does @ComponentScan so all @RestController classes will get registered as beans if they are in the same package as your @SpringBootApplication annotated class or if it's in the child packages.
Please read and try to understand how Spring Boot works by reading this chapter of the docs -  https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-using-springbootapplication-annotation.html
Object.keys(headers).filter((key) =&gt; TRACING_HEADERS.includes(key)).map((key) =&gt; headers[key])  returns an  array .
What you want is:
I'm pretty sure this isn't an istio / distributed tracing issue ;-)
b3-propagation of x-b3-parentspanid ( https://github.com/openzipkin/b3-propagation ) can be configured in your application.yml by adding:
Details of error (Java stack trace) would be really useful here.
By error message I assume, you are using  qpid JMS client , that is performing check of message properties' names.
These names can contain only characters, that are valid  Java identifier characters .
In string 'queue-name' there is a '-' character, that is not Java identifier.
To fix, you need to change 'queue-name' into something with valid characters, for example 'queue_name' (with underscore), or 'queueName' (camel case).
Section 3.5.1 of the JMS 2 specification states this about message properties:
Property names must obey the rules for a message selector identifier.
See
  Section 3.8 “Message selection” for more information.
In regards to identifiers, section 3.8.1.1 states, in part:
An identifier is an unlimited-length character sequence that must begin with a Java identifier start character; all following characters must be Java identifier part characters.
An identifier start character is any character for which the method  Character.isJavaIdentifierStart  returns  true .
This includes '_' and '$'.
An identifier part character is any character for which the method   Character.isJavaIdentifierPart  returns  true .
If you pass the character  -  into either  Character.isJavaIdentifierStart  or  Character.isJavaIdentifierPart  the return value is  false .
In other words,  the  -  character in the name of a message property violates the JMS specification  and therefore will cause an error.
From the error message its obvious that you are using qpid JMS client for communication through queues.
qpid client won’t allow any keys which violates java variable naming convention e.g.
you won’t be able to send x-request-id in a queue’s header
which qpid jms client is consuming as it’ll throw error.
You need to take care of istio/zipkin to not to add certain headers (id you don’t need them actually) with the queue when its trying to communicate on azure bus.
So you have to disable the istio/zipkin libraries  to intercept the request for queues so that request to/from queue can be made without headers.
This will fix the issue.
It the application.properties file for each eureka client ,  I added/changed
------------------ client
-------------------- eureka server application.property--------------------
I was facing a similar issue where the eureka server was registering the services at host.docker.internal instead of localhost.
The issue in my case was an altered host file at location C:\Windows\System32\Drivers\etc\hosts.
I deleted all the lines in the host file and saved it using npp with admin privilege.
Restart the server post this change.
Looks like 'Docker Desktop' was changing the hostfile.
"message": "Connection refused: no further information: host.docker.internal in eureka gateway error
Resolution:
check ping host.docker.internal
response is some ip addresses apart form local host i,e 127.0.0.1
remove the C:\Windows\System32\Drivers\etc\hosts.file entries , make it empty
then restart eureka and your microservice instance.
also will find the message like below in the log this ensures you are registered in eureka
DiscoveryClient_BEER-SERVICE/DESKTOP-G2AIGG1:beer-service:
splitting the above log message which denotes discovery client 
BEER-SERVICE is my service and 
DESKTOP-G2AIGG1 is my pc name
beer-service is the service registered.
I was also facing the same issue, when I was loadbalancing my restTemplate.
Something like this
This is because of the ribbon client.
So, without making any changes in the host file, when i deleted this code and made use of  RestTemplateBuilder  to get restTemplate, everything was working fine.
Code Example:
You can try this approach as well.
Thanks for the tip on the host file on windows
I found that docker adds aliases  in the host file for host.docker.internal and gateway.docker.internal.
I am guessing that Eureka does a host lookup from the IP and host.docker.internal is returned.
I am not an expert at hosts files, but I added an alias for my actual host name to my IP (note: we use static ip's).
After doing this, docker did not change my host file on reboot and the reverse lookup of the ip-&gt;host now returns my machine name instead of host.docker.internal
Solution for Window 10:
You don't have to remove all the lines from hosts files.
Just comment this if exists (#192.168.1.4 host.docker.internal) (as we use this when playing with docker)
And paste this (127.0.0.1   host.docker.internal)
It worked for me.
You can use the new Dalston feature of using annotations on Spring Data repositories.
You can check out this for more info  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_managing_spans_with_annotations
This is really strange because you are using latest relase and in the GitHub spring-cloud-sleuth depends to  &lt;brave.version&gt;4.17.2&lt;/brave.version&gt; .
And I think 4.16.3-SNAPSHOT version is not exists in the maven repo.
(just checked 2.0.0.M8 depends to this version)
If you change to  &lt;sleuth.version&gt;2.0.0.M7&lt;/sleuth.version&gt;  it does find the required dependencies.
https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/pom.xml
The M8 for sleuth was broken.
That issue will be fixed in M9.
You can use M8 but you have to explicitly change the brave version to some release one.
For 1, 2, 3 it was because I was doing a new RestTemplate.
The doc says :
You have to register RestTemplate as a bean so that the interceptors will get injected.
If you create a RestTemplate instance with a new keyword then the instrumentation WILL NOT work.
So RTFM for myself, and this solved my 3 first problems :
The first step to good searching in elasticsearch is to create fields from your data.
With logs, logstash is the proper tool.
The grok{} filter uses patterns (existing or user-defined regexps) to split the input into fields.
You would need to make sure that it was mapped to an integer (e.g.
%{INT:duration:int} in your pattern).
You could then query elasticsearch for "duration: 1000" to get the results.
Elasticsearch uses the lucene query engine, so you can find sample queries based on that.
Zipkin is the best solution.
--zipkin developer
EDIT  - Ok ok, here's a serious answer:
Zipkin is a distributed tracing system developed by Twitter because our service-oriented-architecture is so goddamned big that it's often hard to understand WTF is happening in any given request.
Seriously, here's a visualization in Zipkin of all the services dependencies at twitter:
Is your platform this intense?
You should use zipkin.
Did I mention it's one of the best scaling systems I've ever seen?
It has zero problem keeping up with twitter-level load, and that might be important to you if you're that big.
What's that you say?
You're not as big as twitter?
You only have three services: a web frontend, some kind of middleware, and your database backend?
Maybe zipkin is a bit overkill for you.
We've done some work to make it a bit easier to setup, but really my job isn't to make zipkin easy for you, it's to make zipkin awesome for Twitter.
Still, if you plan on scaling scala, the twitter stack with Finagle etc is insanely good.
Don't let all the evangelists from Typesafe fool you.
Their stack has some serious deficiencies when you try to deploy it in massive-scale architectures.
But again, our job isn't to tell you how good our stack is, or even help you use it.
It's to make our stack awesome.
You can add the following setting on your properties key to disable zipkin,  source .
Better yet, create separate development properties (like  application-dev.properties ) to avoid changing above setting everytime you want to run in your machine:  https://stackoverflow.com/a/34846351/4504053
Most likely your code is broken.
You can check out the  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/ZipkinAutoConfiguration.java  class where for Edgware we've added load balanced zipkin server resolution.
Is it correct that you are using the code example from the baeldung tutorial?
( http://www.baeldung.com/tracing-services-with-zipkin  - 3.2.
Spring Config)
I think there is a mistake with line 34 and 35 (the closing curly brace).
I've fixed the problem by modifing the method like this:
Maybe this helps someone or maybe someone from baeldung reads this and could verify and correct the code example.
;)
Problem solved.
tracer.withSpanInScope(clientSpan)  would do the work.
Note that,  withSpanInScope(...)  has not been called before sending messages .
Some people use zipkin to identify dead services, but probably metrics/stats would be the better route if you are trying to break down and report by thrift method.
Well before starting digging into JVM stuff or setting up all the infrastructure needed by Zipkin you could simply start by measuring some application-level metrics.
You could try the library  metrics  via this  scala api .
Basically you manually set up counters and gauges at specific points of your application that will help you diagnose your bottleneck problem.
The problem might be related to the fact that you're creating the Feign builder manually via  Feign.builder()  factory method.
We're unable to instrument that call.
You should create a bean (via  SleuthFeignBuilder.builder ) and inject that into your code.
Dependencies are resolved before plugins are executed.
So the properties you read with the properties-maven-plugin are not available in the  &lt;dependencies&gt;  section.
If you want to set a dependency version by property, this property must be set inside the POM, on the command line or in the  settings.xml .
You are using an old version of the plugin ( 1.0-alpha-2 ), update it to the  latest   1.0.0 .
Then make sure that the file  version.properties  is in the folder  C:\Workspace .
Anyway, with the latest version of the plugin you should get a proper error message if it can't find the file.
One more suggestion:  spring-cloud-starter-zipkin  belongs to the  org.springframework.cloud  group which follows another version.
The suggested way to declare that dependency is like the following:
I was able to setup OpenCensus in GCP (in an instance on the project) by following the steps  mentioned here .
To set it up quickly, here are the commands I ran in a brand new Ubuntu instance
You should use  egress-gateway .
When all external calls go to the gateway, istio can get the metadata and does some tracing works.
There are many advantages when using ingress/egress gateway:
Based on  envoy documentation  it doesn't support https tracing.
The tracing configuration specifies global settings for the HTTP tracer used by Envoy.
The configuration is defined by the Bootstrap tracing field.
Envoy may support other tracers in the future, but right now the HTTP tracer is the only one supported.
And this post on  stackoverflow
HTTPS (HTTP over SSL) sends all HTTP content over a SSL tunel, so HTTP content and headers are encrypted as well.
I have even tried to reproduce that, but like in your case zipkin worked only for http.
Based on that I would say it's not possible to use zipkin to track https.
It's because you haven't mentioned the  host  here:
First, previous answer is wrong, you don't need to specify  host  it is not mandatory unless you want to set up a DNS.
Second, the backend  zipkin  requires the  /zipkin  URI to respond right?
If this is the case, then the rewrite annotation is removing the URI.
So you would need to change your yaml like this to pass  /zipkin  to your backend.
Just to clarify the OP problem.
There are different  ingress Controllers
Note:
When you create an ingress, you should annotate each ingress with the appropriate ingress.class to indicate which ingress controller should be used if more than one exists within your cluster.
If you do not define a class, your cloud provider may use a default ingress controller.
Ideally, all ingress controllers should fulfill this specification, but the various ingress controllers operate slightly differently.
Using this annotation:
It looks like you are using NGINX Ingress Controller provided by nginxinc.
You can find more information about  Rewrites Support  for NGINX Ingress Controller provided by  nginxinc here .
example:
It's different from the kubernetes community at  kubernetes/ingress-nginx repo .
Different ingress controllers have different configs and annotations.
So for this example:
Test it:
If S1 sends a request to S2 it's the S1 that sets the sampler value and S2 just continues the result.
In other words S1 will export to zipkin 100 * 0.1 = 10 requests, and S2 will export 100 * 0.1 = 10 requests.
S1 makes a decision and S2 will continue it.
FYI, this worked after generating dummy X-B3-SpanId; it works as long as X-B3-TraceId is unique.
e.g.
I know this is old but I have just had exactly the same problem, and have just worked out it's being caused by the appmetrics libraries.
Once I figure out how to get it working I'll update this.
EDIT:
OK managed to get it working with appmetrics-dash.
You need to use monitor() instead of attach() and move the monitor to the end of your routes as so.
I have investigated appmetrics-prometheus and it only has an attach() at this stage so can't be used:
Alright, so I'm going to answer this based on what you said here:
Or a better approach  if there aint any support/ plugin for the same.
The way that I do it us through  Prometheus , in combination with  cloudwatch_exporter , and  alertmanager .
The configuration for  cloudwatch_exporter  to monitor SQS is going to be something like (this is only two metrics, you'll need to add more based on what you're looking to monitor):
You'll then need to configure prometheus to scrape the  cloudwatch_exporter  endpoint at an interval, for ex what I do:
You would then configure  alertmanager  to alert based on those scraped metrics; I do not alert on those metrics so I cannot give you an example.
But, to give you an idea how of this architecture, a diagram is below:
If you need to use something like  statsd  you can use  statsd_exporter .
And, just in-case you were wondering, yes  Grafana supports prometheus .
As there is a bug in Spring AMQP, which will be fixed in Release 2.1.3 
 Issue link
For a tempory fix, you can enable retry properties to create advice chain.
Hope this resolves your problem.
I had this same issue, changing Spring Boot version to 2.1.0.RELEASE did the trick for me.
You should try it too.
There must be something wrong with  RabbitMQ in Spring Boot version 2.1.1.RELEASE.
add build.gradle
apply plugin: 'org.springframework.boot'
springBootVersion=2.1.3.RELEASE
springCloudVersion=Greenwich.RELEASE
Finally got it to work removing  @AutoConfigureAfter ,  @CondtionnalOnBean  and  @ConditionnalOnMissingBean , using instead  @ConditionalOnClass ,  @ConditionnalOnMissingClass  and reproducing other  @Conditionnals  from  TraceAutoConfiguration .
Not great, but at least working.
I think that Christian Posta article you refer to is very good.
As he says, you can deal with the most common use-cases with the out of the box Kubernetes solutions for discovery (kub dns), load-balancing (with Services) and edge services/gateway (Ingress).
As Christian also points out, if you need to dynamically discover services by actively querying rather than knowing what you are looking for then Spring Cloud Kubernetes can be better than going directly to Kubernetes Apis.
If you need to refresh your app from a config change and see it update quickly without going through a rolling update (which would be needed if you were mounting the configmap as a volume) then Spring cloud Kubernetes config client could be of value.
The ribbon integration could also be of value if you need client-side load-balancing.
So you could start out without Spring Cloud Kubernetes and add parts of it if and when you find that it would help.
I think it is better to think of the project as adding extra options and conveniences rather than alternatives to Kubernetes-native solutions.
It is also worth noting that you can deploy a Netflix stack app to Kubernetes (including using Zuul and eureka) and there isn't necessarily anything wrong with that.
It has the advantage that you can work with it outside Kubernetes and it might be more convenient for your particular team if it's Java team.
The main downside is that the Netflix stack is very tied to Java, whereas Kubernetes is language neutral.
We had this very similar issue with Akka.
We observed huge delay in ask pattern to deliver messages to the target actor on peek load.
Most of these issues are related to heap memory consumption and not because of usages of dispatchers.
Finally we fixed these issues by tuning some of the below configuration and changes.
1) Make sure you stop entities/actors which are no longer required.
If its a persistent actor then you can always bring it back when you need it.
Refer :  https://doc.akka.io/docs/akka/current/cluster-sharding.html#passivation
2) If you are using cluster sharding then check the akka.cluster.sharding.state-store-mode.
By changing this to persistence we gained 50% more TPS.
3) Minimize your log entries (set it to info level).
4) Tune your logs to publish messages frequently to your logging system.
Update the batch size, batch count and interval accordingly.
So that the memory is freed.
In our case huge heap memory is used for buffering the log messages and send in bulk.
If the interval is more then you may fill your heap memory and that affects the performance (more GC activity required).
5) Run blocking operations on a separate dispatcher.
6) Use custom serializers (protobuf) and avoid JavaSerializer.
7) Add the below JAVA_OPTS to your jar
export JAVA_OPTS="$JAVA_OPTS -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 -Djava.security.egd=file:/dev/./urandom"
The main thing is XX:MaxRAMFraction=2 which will utilize more than 60% of available memory.
By default its 4 means your application will use only one fourth of the available memory, which might not be sufficient.
Refer :  https://blog.csanchez.org/2017/05/31/running-a-jvm-in-a-container-without-getting-killed/
Regards,
Vinoth
As @Bal Chua and @Pär Nilsson mentioned, for environmental variables you can use only string variables because Linux environmental variables can be only strings.
So, if you use yaml, you need to place value into quotes to force Kubernetes to use string.
For example:
Even when you use Spring Cloud, 100 services do NOT mean 100 servers.
In Spring Cloud the packaging unit is Spring Boot application and a single server may host many such Spring Boot applications.
If you want, you can containerize the Spring Boot applications and other Spring Cloud infrastructure support components.
But that is not Kubernetes.
If you move to Kubernetes, you don't need the infrastructure support services like Zuul, Ribbon etc.
because Kubernetes has its own components for service discovery, gateway, load balancer etc.
In Kubernetes, the packaging unit is Docker images and one or more Docker containers can be put inside one pod which is the minimal scaling unit.
So, Kubernetes has a different set of components to manage the Microservices.
Kubernetes is a different platform than Spring cloud.
Both have the same objectives.
However, Kubernetes has some additional features like self healing, auto-scaling, rolling updates, compute resource management, deployments etc.
Just to add to saptarshi basu's answer, you might want to look at  https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes  as it walks through the comparison and asks which responsibilities you might want to be handled by which components when using Spring cloud on kubernetes
If you're using Sleuth 2.0 you can call on the  Tracer  a method to create a new trace.
In the older version of sleuth I guess what I'd do is to use an executor that is  NOT  a bean.
That way you would lose the trace and it would get restarted at some point (by rest template or sth like that).
Thanks for the kind words!
In Sleuth Edgware we will support Reactor -  https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-reactor  and in Sleuth Finchley we will support reactor and webflux  https://github.com/spring-cloud/spring-cloud-sleuth/blob/2.0.x/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceWebFluxAutoConfiguration.java .
In other words it's already possible to use Sleuth in the reactive context.
It seems like you are using  Sleuth with Zipkin via HTTP .
You can try the  Sleuth with Zipkin via Spring Cloud Stream  approach.
I haven't done the benchmark myself, but it should improve the performance in theory.
Please see the documentation at:  https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_spring_cloud_stream
I wonder what kind of a benchmarking method you have picked.
Which version of Sleuth are you using?
Also is this one single benchmark that you're doing?
Is it on your computer?
Has the JVM gotten heated up?
Are there any other processes executed?
Doing benchmarking is not that easy... You can use tools like JMH to do it better.
BTW try turning off the DEBUG logging level and check the results again.
We are performing benchmark tests of Sleuth and from what we see when adding Sleuth the latency gets increased by around 20 ms. Definitely not 600 ms.
I belive you had problem with: org.springframework.cloud.sleuth.zipkin.ServerPropertiesEndpointLocator.
It uses InetUtils.findFirstNonLoopbackAddress() to determine instance address.
Method is called synchronously when each span is close (in ZipkinSpanListener#convert).
The workaround is to create custom org.springframework.cloud.sleuth.zipkin.EndpointLocator.
You can use something like that:
And combine it with one of existing EndpointLocators.
You can find them in: org.springframework.cloud.sleuth.zipkin.ZipkinAutoConfiguration.
This issue is already fixed in sleuth 2.X.X.
Where: org.springframework.cloud.sleuth.zipkin2.DefaultEndpointLocator caches server address:
The web app is trying to access  config.json  at root (accessing as  /config.json  vs just  config.json  ) - that is  http://localhost:8001/config.json  .
This would obviously be wrong as it should be  http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json
There is a very simple solution for this - just run:
Now just go to  http://localhost:9411  and the UI should be up (tried and verified.)
You can get the name of the pod by doing  kubectl get pods
PS:  kubectl proxy  is generally meant to access the Kubernetes API, and  kube port-forward  is the right tool in this case.
I'm not sure this is the right way to do it but this should normally works
I have seen this issue a lot.
From my experience the most common cause is, that the base64 string was encoded on the commandline using  echo '$mypw' | base64  which will create newlines in the encoded string.
You need to use the  -n  switch to echo:  echo -n '$mypw' | base64 .
What logging framework are you using?
I was using log4j2 in my project.
It started working when I removed log4j2 and left it to the default logging of spring-cloud-sleuth.
It's a bug -  https://github.com/spring-cloud/spring-cloud-sleuth/issues/855  .
I've fixed it ATM.
A workaround is to start it manually either in each method that uses  @NewSpan  by calling  start()  method on current span (that doesn't scale too nicely)
You can also create a bean of  SpanCreator  (you can check the fixed version here  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/annotation/DefaultSpanCreator.java )
Notice the  .start()  at the end of the method.
Try this
I hope this will help you.
According to  Spring Boot Reference Docs  :
To enable  /httptrace  in the actuator, then you have to create a bean of   InMemoryHttpTraceRepository  class in the custom  @Configuration  class which provides the trace of the request and response.
To enable  /auditevents  in the actuator, then you have to create a bean of  InMemoryAuditEventRepository  class in the custom  @Configuration  class which exposes audit events information.
To enable  /integrationgraph  in actuator, you have to add  spring-integration-core dependency  in the pom.xml (as per documentation) :
or if you are having a spring-boot project, then add this :
/actuator/sessions  are by-default enabled.
But still you can add this explicitly to check the behaviour.
Add this in application.properties.
Thanks Jorg Heymans for the question.
Yeah, it's a bug and should be fixed by  https://github.com/line/armeria/pull/3120 
Thank you!
Ribbon is a client side load balancer which means there is no any other hop in between your client and service.
Basically you keep and maintain a list of service on your client.
In AWS load balancer case you need to make another hop in between the client and server.
Both have advanges and disadvantages.
Former has the advantage of not having any dependency to any specific external solution.
Basically with ribbon and service discovery like eureka you can deploy your product to any cloud provider or on-premise setup without additional effort.
Latter has advantage of not needing an extra component of service discovery or keeping the cache of service list on client.
But it has that additional hop which might be an issue if you are trying to run an very high-load system.
Although I don't have much experience with AWS CloudWatch what I know is it helps you to collect logs to a central place from different AWS components.
And that is what you are trying to do with your solution.
kubectl exec -it "pod-name" -c "container-name" -n "namespace"
Here only the container name is needed.
In your case it will be:
kubectl exec -it my-api-XXX -c my-api  -- /bin/bash
You can exec to Zipkin because  exec  is taking zipkin as the default container.
It is solved now; all I had to do was port forwarding.
Thanks,
By default you service is exposed as  ClusterIP , in this case your service will be accessible from within your cluster.
You can use port forwarding " With this connection in place you can use your local workstation to debug your application that is running in the pod " as described in the answer above.
Another approach is to use other  "service types"  like  NodePort .
You can find more information here  Publishing services (ServiceTypes)
Sleuth will do the same for messaging by using message headers to propagate  span id, trace id  and other relevant information.
It does so by registering special channel interceptor.
The configuration you're referring to is for the instrumentation of messaging systems, not for sending traces to zipkin using a messaging system.
You should look at this  auto-configuration , and especially this  sender config .
What you want to do has also been documented here:  https://cloud.spring.io/spring-cloud-sleuth/2.0.x/single/spring-cloud-sleuth.html#_sleuth_with_zipkin_over_rabbitmq_or_kafka
You should only need to add  spring-cloud-starter-zipkin  and  spring-rabbit  to your dependencies.
If you want to change the default queue (which is  zipkin ), then you'll need to add  spring.zipkin.rabbitmq.queue  to your properties.
You will need your own  PropagationFactory  implementation.
Here is the default one:  https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/B3Propagation.java
You can create a bean and sleuth should use that instead of this one.
More specifically you will need an implementation with a custom  TraceContext.Extractor&lt;C&gt;  implementation.
This can then pull the trace ID from your header, and add return the appropriate  TraceContext .
Then it can pass it along using the normal headers.
If you'd like to use the same correlation header when sending downstream then you will also have to implement  TraceContext.Injector&lt;C&gt; .
The option is to disable Slf4j integration as you mentioned.
When a new span / scope is created, we go through Slf4j to put data in MDC and it takes time unfortunately.
Disabling that will save it.
This is indeed possible with the mentioned  executor channel .
All you recipient flows must really start from the  ExecutorChannel .
In your case you have to modify all of them to something like this:
Pay attention to the  IntegrationFlows.from(MessageChannels.executor(taskExexecutor())) .
That's exactly how you can make each sub-flow async.
UPDATE
For the older Spring Integration version without  IntegrationFlow  improvement for the sub-flows we can do like this:
This is similar to what you show in the comment above.
Works for me with 1.4.0.RELEASE (2.0.0.RELEASE isn't out yet, but should be soon).
You probably have a bad jar file in your local maven cache (e.g.
the one that it complains about).
You have to provide a different logging pattern to make it work with PCF Metrics AFAIR.
You need the parent span to be present in logs.
Set the property  logging.pattern.level: "%clr(%5p) %clr([${spring.application.name:-},%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-B3-ParentSpanId:-},%X{X-Span-Export:-}]){yellow}" .
Check this example:  https://github.com/pivotal-cf/pcf-metrics-trace-example-spring
PCF metrics does  not support custom spans, it only shows the respomse time distribution span that corresponds to http request routed by goRouter.
It was a bug that got fixed with this commit -  https://github.com/spring-cloud/spring-cloud-sleuth/commit/d7a0747907f4ab7201f67e7d0c762a324fbe0668  .
Please check out the latest snapshots
Why are you setting the values of dependencies manually?
Please use the Edgware.SR2 BOM.
You have to add the kafka dependency, ensure that rabbit is not on the classpath.
If you have both kafka and rabbit on the classpath you need to set the  spring.zipkin.sender.type=kafka
UPDATE:
As we describe in the documentation, the Sleuth Stream support is deprecated in Edgware and removed in FInchley.
If you've decided to go with the new approach of using native Zipkin messaging support, then you have to use the Zipkin Server with Kafka as described here  https://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10  .
Let me copy part of the docs here
The following configuration points apply apply when  KAFKA_BOOTSTRAP_SERVERS  or
 zipkin.collector.kafka.bootstrap-servers  is set.
They can be configured by setting an environment
variable or by setting a java system property using the  -Dproperty.name=value  command line
argument.
Some settings correspond to "New Consumer Configs" in
 Kafka documentation .
Environment Variable | Property | New Consumer Config | Description
KAFKA_BOOTSTRAP_SERVERS  |  zipkin.collector.kafka.bootstrap-servers  | bootstrap.servers | Comma-separated list of brokers, ex.
127.0.0.1:9092.
No default
KAFKA_GROUP_ID  |  zipkin.collector.kafka.group-id  | group.id | The consumer group this process is consuming on behalf of.
Defaults to  zipkin
KAFKA_TOPIC  |  zipkin.collector.kafka.topic  | N/A | Comma-separated list of topics that zipkin spans will be consumed from.
Defaults to  zipkin
KAFKA_STREAMS  |  zipkin.collector.kafka.streams  | N/A | Count of threads consuming the topic.
Defaults to  1
OpenStack does not have Zipkin as an inbuilt tracer.
Hence OSProfiler was adopted as a standard project for tracing in OpenStack.
As far as i can see from the documentation, Nova should have OSProfiler support for Mitaka.
Although i have not used OSProfiler with Mitaka, I have worked with OSProfiler with Newton and subsequent releases.
You can post the issue that you are facing so that it will be easier to debug.
If you're using Edgware release train, just set  spring.zipkin.sender.type=web .
That way you force the HTTP based span sending
Embedded headers are not pluggable, but you can disable them with  ...producer.header-mode=raw .
With Ditmars (1.3.x) you can use the kafka11 artifact, which supports native headers - you have to override a bunch of dependencies (kafka-clients, SK, SIK and kafka itself if you are using the  KafkaEmbedded  broker for testing.
See  the relesae notes ).
There's  a discussion on Gitter  about overriding the versions.
spring-kafka 1.3.x natively uses 0.11 but 1.3.1 and higher (1.3.2 is current) also supports the 1.0.0 client.
Elmhurst (2.0) - currently in milestones - uses SK 2.1.0 which natively uses 1.0.0 kafka.
We implemented this on our microservices platform
A lot of the logging is done by pushing requests onto a RabbitMQ queue and then getting logstash to consume that.
Other data is obtained via filebeat transmitting the logs to logstash
Both the logs and the RabbitMQ data has the id attached so can be correlated
An alternative approach would be to build instrumentation into each microservice that specifically monitored latency and then record that directly into logstash
You might like to read  https://medium.com/devopslinks/how-to-monitor-the-sre-golden-signals-1391cadc7524  for a general guide to essential monitoring that is applicable to microservices
I figured out how to disable the bean that was injecting LogbackAccess.
This resolved the issue so that Zipkin is now accepting requests.
To log internal HTTP request sent from a Node.js server, you can create a Proxy Node.js server and log all requests there using  Morgan .
First, define 3 constants (or read from your project config file):
Second, When your Node.js server is launched, start the logger server at the same time if  ENABLE_LOGGER  is true.
The logger server only do one thing: log the request and forward it to the real API server using  request  module.
You can use  Morgan  to provide more readable format.
Third, in your Node.js server, send API request to logger server when  ENABLE_LOGGER  is true, and send API directly to the real server when  ENABLE_LOGGER  is false.
No - we haven't added any instrumentation around Webservicetemplate.
You'd have to add an interceptor similar to the one we add for RestTemplate.
You'd have to pass all the tracing headers to the request so that the other side can properly parse it.
We have an internal OkHttpClient wrapper implementing Call.Factory which adds an initial interceptor:
to solve this problem.
It is not transparent, however, so may not be good for Brave.
It works fine, because in practice once a client is configured, you only really use the  Call.Factory  interface :-)
The  Ctx  bit is just the context with stuff we want to propagate, we can do it implicit or explicit, hence the extra method to explicitly take it.
Thanks for trying out HTrace!
Sorry that the version issue is such a pain right now.
It is much easier to configure HTrace with the version in cloudera's CDH5.5 distribution of Hadoop and later.
There is a good description of how to do it here:  http://blog.cloudera.com/blog/2015/12/new-in-cloudera-labs-apache-htrace-incubating/   If you want to stick with an Apache release of the source code rather than a vendor release, try Hadoop 3.0.0-alpha1.
http://hadoop.apache.org/releases.html
The HTrace libraries shippped in Hadoop 2.6 and 2.7 are very old... we never backported HTrace 4.x to those branches.
They were stability branches, so new features like tracing was out of scope.
There is some functionality there, but not much.
I recommend using the newer HTrace 4.x library which is actively developed.
The HTrace 4.x branch also has a stable API, so hopefully breakage will be minimized in the future.
Exactly, in the code, I see the configuration key's prefix is  dfs.htrace , not the  hadoop.htrace .
And in dfsclient, it's  dfs.client.htrace .
You can change the prefix to  dfs.htrace , then restart the cluster and it take effect.
The code is in class  org.apache.hadoop.tracing.SpanReceiverHost .
Hope this help!
The sampling decision is taken for a trace.
That means that when the first request comes in and the span is created you have to take a decision.
You don't have any tags / baggage at that point so you must not depend on the contents of tags to take this decision.
That's a wrong approach.
You are taking a very custom approach.
If you want to go that way (which is not recommended) you can create a custom implementation of a  SpanReporter  -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/SpanReporter.java#L30  .
SpanReporter  is the one that is sending spans to zipkin.
You can create an implementation that will wrap an existing  SpanReporter  implementation and will delegate the execution to it only when some values of tags match.
But from my perspective it doesn't sound right.
If I'm not mistaken (and I guess I'm not) no wonder that you're not sending the Spans to Zipkin cause you didn't add the Zipkin dependency.
Check the  Sleuth with Zipkin via HTTP  section of the docs:  http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html  .
This config worked for me in 1 of my application:
Enabling property might do the trick!
First of all the main feature of Spring Integration is  MessageChannel , but it still isn't clear to me why people are missing  .channel()  operator in between endpoint definitions.
I mean that for your case it must be like:
Now about your particular problem.
Look,  ContentEnricher  ( .enrich() ) is request-reply component:  http://docs.spring.io/spring-integration/reference/html/messaging-transformation-chapter.html#payload-enricher .
Therefore it sends request to its  requestChannel  and waits for reply.
And it is done independently of the  requestChannel  type.
I raw Java we can demonstrate such a behavior with this code snippet:
where you should see that  ADD_LINE_ITEM_CHANNEL  as an  ExecutorChannel  doesn't have much value because we are blocked within loop for the reply anyway.
A  .split()  does exactly similar loop, but since by default it is with the  DirectChannel , an iteration is done in the same thread.
Therefore each next item waits for the reply for the previous.
That's why you definitely should parallel exactly as an input for the  .enrich() , just after  .split() .
The Apache Thrift TSocketTransport (almost certainly what you are using) uses TCP on a configurable port.
Cassandra usually uses port 9160 for thrift.
When using Thrift/TCP no HTTP setup is necessary.
Just open 9160 (and any other ports your custom thrift servers may be listening on).
Though you can use Thrift over HTTP, Thrift is RPC, not REST, so proxy caching will cause problems, the client needs a direct comm channel with the server.
If you do need to access a thrift service via a proxy, something like this would work:
https://github.com/totally/thrift_goodies/blob/master/transport.py
You can kill the kerberos stuff if you don't need that.
i get the same messages but still be able to collect messages and view them with the web service.
I dont know why the [error] prefix is in front of it but if you read the chars behind you see INF/DEB and so on...
It stays for INFO and DEBUG.
Greets
yes, you can have multiple express running in the same node process (thats how clustering works in node as well)
but you will need to have them running on different ports.
;
It is possible to create two separate tracer providers.
Only one of them will be the global tracer provider, which the API will use if you call API methods.
You can't use the plugins in this configuration, which means you will have to manually instrument your application.
If this is a use-case which is important to you, I suggest you create an issue on the github repo.
This indicates that the pod is not ready, hence the service will not add that pod's IP in the list of endpoints.
Check the readiness probe of your pod by describing it and debug the issue that's making it non-ready.
Once this if fixed, you'll start seeing some endpoints populated when you describe the service &amp; that will enable you to access the service by DNS name.
First specify your own overlay network (see bottom of code below) and use it for your services.
Then in your compose file for your other services like ZIpkin, add the  backbone  network to its list.
Eg:
Note that outside of the first compose file, you'll need to prefix the  project  name for your network.
Unless you set the environment variable  COMPOSE_PROJECT_NAME  it will be the name of the directory that the compose file is in.
Do a  docker network ls  to find out the full name of the network to use.
OK, so I solved my problem using only docker-compose files.
By Portainer I pasted my second docker-compose file in Stack section (I creaated new Stack):
Now we should use 'my_name_zipkin' name to communicate with this service.
Service name is the name we should use to communicate between containers.
So in properties file I set:
The slf4j API only takes  String  as the input to the  info ,  debug ,  warn ,  error  messages.
What you could do is create your own JsonLogger wrapper, which takes a normal  Logger  (maybe wraps around it), which you could include at the top of your classes like:
private static final JsonLogger logger = new JsonLogger(LoggerFactory.getLogger(MyClass.class)) ;
You can then use Jackson, GSON or your favourite object to JSON mapper inside your JsonLogger so that you could do what you want.
It can then offer the  info ,  debug ,  warn ,  error  methods like a normal logger.
You can also create your own  JsonLoggerFactory  which encapsulates this for you so that the line to include in each class is more concise.
Yes, you can use BAM/CEP for this.
If you need real time monitoring you can use CEP and you can use BAM for batch process.
From BAM 2.4.0 onwards, CEP features have been added inside BAM also hence you can use BAM and do real time analytics.
What type of services are involved with your scenario?
Depends on this you can use already existing data publisher or write new data publisher for BAM/CEP to publish your request details.
For example if you are having chain of axis2 webservice calls for a request from client, and you want to monitor where the bottle neck/more time consumed, then you may use the service stats publishing, and monitor the average time take to process the message which will help you to see where the actual delay has been introduced.
For this you can use existing service statistics publisher feature.
Also BAM will allow you to create your own dashboard to visualize, hence you can customize the dashboard.
Also with BAM 2.4.0 we have introduced notifications feature also which you can define some threshold value and configure to send notification if that cross that threshold value.
I contributed to Micronaut and submitted a PR, which is now merged.
Pull request
So it was application B which was not passing the header along.
Turns out that the queue uri had a property  targetClient  which was set to 1.
The uri is something like
Now I am not an IBM MQ expert by far, but the  documentation  states that setting this property to 1 means that  Messages do not contain an MQRFH2 header.
I toggled it to 0 and voila, all spans fall into place.
You must tell the containers the network &quot;foo_network&quot;.
The External flag says that the containers are not accessible from outside.
Of course you don't have to bet, but I thought as an example it might be quite good.
And because of the &quot;links&quot; function look here  Link
I think I found the problem.
Than I used the Exceutor in my code:
With this configuration the service was not starting correctly.
Now the @Bean(&quot;threadPoolTaskExecutor&quot;) configuration is removed and I'm using only @Async.
But why it's not working with Spring Boot Starter 2.3.x?
And there was no error message in  the log.
Zipkin is a Spring-Boot-based project, the @EnableZipkinServer is not a Spring Cloud annotation.
It’s an annotation that’spart of the Zipkin project.
This often confuses people who are new to the Spring Cloud Sleuth and Zipkin, because the Spring Cloud team did write the @EnableZipkinStreamServer annotation as part of Spring Cloud Sleuth.
The @EnableZipkinStreamServer annotation simplifies the use of Zipkin with RabbitMQ and Kafka.
Advantages of  @EnableZipkinServer is simplicity in setup.
With the @EnableZipkinStream server you need to set up and configure the services being traced and the Zipkin server to publish/listen to RabbitMQ or Kafka for tracing data.The advantage of the @EnableZipkinStreamServer annotation is that you can continue to collect trace data even if the Zipkin server is unavailable.
This is because the trace messages will accumulate the trace data on a message queue until the Zipkin server is available for processing the records.
If you use the @EnableZipkinServer annotation and the Zipkin server is unavailable,the trace data that would have been sent by the service(s) to Zipkin will be lost.
Please don't use field injection, use constructor injection.
Also new span there doesn't make sense cause you already have a new span created by the framework.
Your versions are wrong.
Please don't set the versions by yourself, please use the Spring Cloud BOM (spring-cloud-dependencies) dependency management like presented below
Also - it's enough for you to add the starters.
You've added a starter in a given version and then you've added the core dependency in another one, that makes no sense.
Last thing - versions 1.x are deprecated and no longer maintained.
The current version is 2.2.0.RELEASE and release train version is Hoxton.RELEASE
This can be done using Finagle's  Contexts .
Contexts give you access to request-scoped state, such as a request’s deadline, throughout the logical life of a request without requiring them to be explicitly passed
Unfortunately the best answer to this issue is to upgrade to the latest version of Sleuth where we've migrated to Brave as an internal tracer and fixed a lot of issues.
You can connect your existing container to another network
The error-code implies an error on the other end - 400-errors are not located on your end.
Have you tried dumping the response (including headers)?
Also, did you try to re-authenticate, perhaps reset cookings, etc?
Did you contact the other end?
How did they respond to it?
Add a dependencyManagement entry for io.zipkin.zipkin2:zipkin:2.7.1
For me or anybody finding this thread:
Solved it by upgrading from Camden to Edgware which contains 1.3.5 (and resolving everything around that switch).
You can create your own custom  SpanAdjuster  that will modify the span name.
You can also use  FinishedSpanHandler  to operate on finished spans to tweak them.
Yes, when you create a span you can set the service name.
Just call  newSpan.remoteServiceName(...)
Taking the input of @MarcinGrzejszczak as reference, I resolved using a custom span:
Where  tracer  is an autowired object from  Trace :
Both classes are in  brave  package
Result:
If you want to take a look at the implementation in more detail, here is the sample:  https://github.com/juanca87/sample-traceability-microservices
add this under your  section at the end of your pom.xml.
you may need to add for all the dependencies.
Maybe, I didn't get your question right, but with almost your  docker-compose.yaml  file:
prometheus  metrics are available on  localhost:9411/metrics  both inside container and on host system:
I finally got it working.
I just changed the logstash config file and added:
The filter part was missing earlier.
Can you follow the guidelines described here  https://stackoverflow.com/help/how-to-ask  and the next question you ask, ask it with more details?
E.g.
I have no idea how exactly you use Sleuth?
Anyways I'll try to answer...
You can create a  SpanAdjuster  bean, that will analyze the span information (e.g.
span tags) and basing on that information you will change the sampling decision so as not to send it to Zipkin.
Another option is to wrap the default span reporter in a similar logic.
Yet another option is to verify what kind of a thread it is that is creating this span and toggle it off (assuming that it's a  @Scheduled  method) -  https://cloud.spring.io/spring-cloud-static/Finchley.RC2/single/spring-cloud.html#__literal_scheduled_literal_annotated_methods
name: tcp -  protocol: TCP ?
I tried to back to use the stable version(1.3.3) of spring cloud sleuth, but when i use the bom for the project its make conflict in the spring boot version that i am using(2.0).
Its compactible with the spring boot version 2?
You can't use the Sleuth 1.3 with Boot 2.0.
I am using the spring cloud sleuth to make trace of services on my company, but i have a version of tracing on others services that is not compactible with the opentracing headers, so i want to change the headers of http messages to make the new services compactibles with the current tracing headers that i have in the others components.
Yeah, that's the Brave change.
For http you can define your own parses.
https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceHttpAutoConfiguration.java#L57-L68  .

@Autowired HttpClientParser clientParser;
    @Autowired HttpServerParser serverParser;
    @Autowired @ClientSampler HttpSampler clientSampler;
    @Autowired(required = false) @ServerSampler HttpSampler serverSampler;
These are the samplers that you can register.
For messaging you'd have to create your own version of the global channel interceptor - like the one we define here -  https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/messaging/TraceSpringIntegrationAutoConfiguration.java#L49-L53  .
If that's not acceptable for you, go ahead and file an issue in Sleuth so we can discuss it there.
There's no concept of a trace finishing in zipkin.
A span within a trace can start and finish.
We don't start and stop spans on different hosts, so unfinished spans probably are accidental.
You can chat more here if you like  https://gitter.im/spring-cloud/spring-cloud-sleuth
In Sleuth  1.3.x  you can create a custom  SpanReporter  that, before sending a span to Zipkin, would analyze the URL and would not report that span.
In Sleuth  2.0.x  you can create a custom  HttpSampler  for the client side (with name  sleuthClientSampler )
Turns out the issue was a corrupt Maven package.
Deleting my  .m2\repository  folder and running  mvn spring-boot:run  to downloaded dependencies and run my app resolved the issue.
Please read this page from Zipkin -  https://zipkin.io/pages/instrumenting.html  .
It's all written there how things should work.
HTTP Tracing
HTTP headers are used to pass along trace information.
The B3 portion of the header is so named for the original name of Zipkin: BigBrotherBird.
Also please check the B3 specification page -  https://github.com/openzipkin/b3-propagation
people are scared about the overhead which sleuth can have by adding @NewSpan on the methods.
Do they have any information about the overhead?
Have they turned it on and the application started to lag significantly?
What are they scared of?
Is this a high-frequency trading application that you're doing where every microsecond counts?
I need to decide on runtime whether the Trace should be added or not (Not talking about exporting).
Like for actuator trace is not getting added at all.
I assume this will have no overhead on the application.
Putting X-B3-Sampled = 0 is not exporting but adding tracing information.
Something like skipPattern property but at runtime.
I don't think that's possible.
The instrumentation is set up by adding interceptors, aspects etc.
They are started upon application initialization.
Always export the trace if service exceeds a certain threshold or in case of Exception.
With the new Brave tracer instrumentation (Sleuth 2.0.0) you will be able to do it in a much easier way.
Prior to this version you would have to implement your own version of a  SpanReporter  that verifies the tags (if it contains an  error  tag), and if that's the case send it to zipkin, otherwise not.
If I am not exporting Spans to zipkin then will there be any overhead by tracing information?
Yes, there is cause you need to pass tracing data.
However, the overhead is small.
The code you've provided is not related to Sleuth but opentracing.
In Sleuth you would call  Tracer.createSpan("name")  and that way a child span od your current trace would be created.
I've also managed to get it working by using just the cloud trace api by doing this before I create a span.
Not sure if there is a negative of doing this.
I succeeded to send gcp's trace api in php client via REST.
It can see trace set by php client parameters , but my endpoint for trace api has stopped though I don't know why.Maybe ,it is not still supported well because the document have many ambiguous expression so, I realized watching server response by BigQuery with fluentd and DataStudio and it seem best solution because auto span can be set by table name with yyyymmdd and we can watch arbitrary metrics with custom query or calculation field.
Zipkin's connection to cassandra is independent from the normal spring setup.
We use some very specific setup.
you'll want to set properties in the namespace of zipkin.storage.cassandra
https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/resources/zipkin-server-shared.yml#L40
As the documentation suggests , you need to create a  ProducerFactory  bean if you want to use your own  KafkaTemplate :
The Spring Cloud project is moving to their own solutions.
Ribbon is replaced by the Spring Cloud Load Balancer, Hysterix by the Spring Cloud Circuit Breaker, Zuul by the Spring Cloud Gateway.
This is a good read, including examples, about this topic:  https://piotrminkowski.com/2020/05/01/a-new-era-of-spring-cloud/
Sleuth does this for you by default in 3.x too:  https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-log-integration
You can break this functionality by misconfiguring your log pattern or  logging.pattern.level  or your classpath.
What I would suggest is going to  https://start.spring.io , generate a new project using sleuth and web/webflux, write a single controller and check the logs (do not create any log config file, just leave everything on default).
Let me add to this thread my three bits.
Speaking of Envoy, yes, when attached to your application it adds a lot of useful features from observability bucket, e.g.
network level statistics and tracing.
Here is the question, have you considered running your legacy apps inside service mesh, like Istio ?.
Istio simplifies deployment and configuration of Envoy for you.
It injects sidecar container (istio-proxy, in fact Envoy instance) to your Pod application, and gives you these extra features like a set of service metrics out of the box*.
Example: Stats produced by Envoy in Prometheus format, like  istio_request_bytes  are visualized in Kiali Metrics dashboard for inbound traffic as  request_size  (check screenshot)
*as mentioned by @David Kruk, you still needs to have Prometheus server deployed in your cluster to be able to pull these metrics to Kiali dashboards.
You can learn more about Istio  here .
There is also a dedicated section on how to  visualize metrics  collected by Istio (e.g.
request size).
You can use a  TagValueResolver  or a  MessageSpanCustomizer , see the docs for the details.
Better approach will be using Kafka instead of Redis.
Create a topic for every microservice &amp; keep moving the packet from
one topic to another after processing.
Keep appending the results to same object and keep moving it down the line, untill every micro-service has processed it.
What you have is correct, your issue is likely not DNS.
You can confirm by doing just a DNS lookup and comparing that to the IP of the Service.
The  doubleName  method is private.
Micronaut cannot apply AOP annotations (like  ContinueSpan  to private methods.
Are these methods on a bean ( Singleton , etc.)?
I have found the span annotations only get applied on beans properly.
I had to refactor some of my code to create beans from  Factory s or such.
You've passed in the  B3Propagation.FACTORY  as the implementation of the propagation factory so you're explicitly stating that you want the default B3 headers.
You've said that you want some other field that is alphanumeric to be also propagated.
Then in a log parsing tool you can define that you want to use your custom field as the trace id, but it doesn't mean that the deafult X-B3-TraceId field will be changed.
If you want to use your custom field as trace id that Sleuth understands, you need to change the logging format and implement a different propagation factory bean.
One  of the  way which   worked for me is 
using ExtraFieldPropagation
and adding those   keys in sleuth  properties  under   propagation-keys
and  whitelisted-keys
sample code 
  '  @Autowired Tracer tracer;
With Edgware (SCSt Ditmars), you have to specify which headers will be transferred.
See Kafka Binder Properties .
This is because Edgware was based on Kafka before it supported headers natively and we encode the headers into the payload.
spring.cloud.stream.kafka.binder.headers
The list of custom headers that will be transported by the binder.
Default: empty.
You should also be sure to upgrade spring-kafka to 1.3.9.RELEASE and kafka-clients to 0.11.0.2.
Preferably, though, upgrade to Finchley or Greemwich.
Those versions support headers natively.
Well, without seeing any code, I could only give you a sample of how you should achieve this.
So an http call, for example if you use node-fetch or axios will return a promise.
To wait for promises paralelly, you can do the following:
Note that I use fetch API here, provided in node by the node-fetch package.
Fetch returns a  Promise .
Then I call  Promise.all(promises)  where  promises  is a  Promise  array.
You can then do whatever you would like to do with the 3 responses and your requests were made paralelly.
Hope this helps, good luck!
You have
This means that there is a  &lt;dependencyManagement&gt;  entry in your POM or your Parent POM that sets the version to  2.2.0 .
You can have s custom span reporter that before sending spans to zipkin will dump the span as a json structure to logs.
UPDATE:
With this PR merged  https://github.com/spring-cloud/spring-cloud-sleuth/pull/1068 , in 2.1.0 you'll have an easy way to implement your own MDC entries
Usage of Sleuth Stream is deprecated.
Please use the  zipkin  starter, add the  Kafka  dependency and set things as presented here  https://cloud.spring.io/spring-cloud-static/Edgware.SR1/multi/multi__introduction.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka
fran, in Edgware.RELEASE the  &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;  will resolve Zipkin 2 dependencies try using  &lt;artifactId&gt; spring-cloud-starter-zipkin-legacy&lt;/artifactId&gt;  instead
To define the primary connection factory for RabbitMQ in XML files, you can do something like this:
I do not have much knowledge with hysterix, but if you are trying to pass some contextual info like trace IDs around, then io.grpc.Context is the correct class to use.
You would need to call  context.withValue  to create a new context with the traceID.
In the places where you want the data, you need to attach the context.
Also be sure to detach the context when done, which I do not see happening in your snippet.
You need to use ...
HystrixPlugins.getInstance().registerConcurrencyStrategy(...)
... to register a custom  HystrixConcurrencyStrategy  that uses your own  Callable  ...
... that applies context preservation around the circuit ...
... via is a helper class capable of preserving the Zipkin context ...
... and allowing an easy method of adding other contexts you may wish to preserve e.g.
MDC, SecurityContext etc ...
This is a guess.
Kafka has no concept of message headers (where spans are stored).
SCSt therefore has to embed message headers in the payload.
The current version requires you to &quot;opt-in&quot; which headers you want transported in this way.
Documentation here .
spring.cloud.stream.kafka.binder.headers
The list of custom headers that will be transported by the binder.
Default: empty.
Unfortunately, patterns are not currently supported, you have to list the headers individually.
We are  considering adding support for patterns  and/or transporting all headers by default.
Finally, I found 2 issues related with my applications.
1.
The application with @EnalbeZipkinStreamServer could not be traced.
This looks like a by design.
2.
If kafka is used as the binder, the applications should specify the headers as the following:
I'm not sure this is what you are expecting, you can add this dependancy in  pom.xml  if you are using maven:
and a  AlwaysSampler @Bean  in your  SpringBootApplication  class
This will help you to sample your inputs in zipkin all time.
The best thing to do would be to show your sample project.
Another is to check if you don't have a custom logback.xml or any other type of logging configuration that breaks the current set up (most likely you do cause I can see that the pattern is different).
So you have an ip and port of your app so that could give you a hint.
Also if you want a custom span of yours to have that information, then it's enough to add a custom tag to it.
Actually you can always call the  tracer.addTag("key", "value")  to put the additional information that you need.
I am not sure about the  dcat  distribution itself, but your error may be because you have:
however when you specify  p  in the model, it is a three dimensional array  p[j,k,i].
I think you need:
Note, the  i  is the last index for  p , and the two commas.
Hope his helps...
You can use spring cloud sleuth.
Please check the documentation for examples of using elk stack to harvest the logs.
The zipkin server can be fetched as a standalone jar, you don't need to create your custom version
It is indeed a cluster problem.
There is a problem with the  __consumer_offsets  topic data of kafka.
It is good to restart kafka after deleting.
I have no knowledge of it being impossible.
Maybe you should first try doing it and then asking a question?
Also, if for some reason it turns out you can't use it, then if you just google  zipkin scala  you'll see things like  https://github.com/lloydmeta/zipkin-futures  ,  https://github.com/bizreach/play-zipkin-tracing  etc.
You'll find all of it on GitHub.
Spring Web annotations
Spring framework annotations
More spring framework annotations
You can use Apache NiFi's built-in provenance capabilities to trace how a given flow went through the system.
https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#data_provenance
