toolname,answer_id,question_id,comment_count,creation_date,is_accepted,last_activity_date,owner_reputation,owner_id,score,body
AppDynamics,13532221,13524580,1,"2012/11/23, 17:48:01",False,"2012/11/23, 17:48:01",17,1847969,0,"<p>I suggest to look into Gartners magic quadrant and get dynaTrace since it has negligible overhead , less than 1% in production under load.</p>
"
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74,1854219,5,"<p>Full Disclosure: I currently work for AppDynamics.</p>

<p>AppDynamics was designed from the ground up for high volume production environments but works equally well in both prod and non-prod. It's currently running in production in some of the worlds largest mission critical application environments at Netflix, Exact Target, Edmunds, and many others. Here are a few quotes from existing customers…</p>

<p>""It's like a profiler that you can run in production"" -- Leonid Igolnik, Taleo</p>

<p>""We found that the overhead was negligible"" -- Jacob Marcus, Care.com</p>

<p>""We wanted a monitoring solution that wouldn't impact our production runway"" -- John Martin, Edmunds</p>

<p>AppDynamics overhead is extremely low but I suggest you test it and see for yourself. You can download and use it for free from the AppDynamics website. Good luck in your search for the right APM tool.</p>
"
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207,153728,1,"<p>Yes it will if the application is sensitive to extra GC cycles caused by call stack sampling. The impact will depend on the number of threads and typical call stack depth. This is not specific to AppDynamics, other call stack sampling solutions such as NewRelic and VisualVM Sampler will have a similar impact.</p>

<p><a href=""http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_Good_APM_vs_AppDynamics_Bad_APM.pdf"" rel=""nofollow"">http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_Good_APM_vs_AppDynamics_Bad_APM.pdf</a></p>

<p><a href=""http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_vs_AppDynamics.pdf"" rel=""nofollow"">http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_vs_AppDynamics.pdf</a></p>
"
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207,153728,4,"<p>There are a number of assumptions made by a vendor but the following are most common: </p>

<ol>
<li>We assume your application request processing time is sufficiently high to dwarf our significant overhead. </li>
</ol>

<p>TRANSLATION: We assume you have a slow performing database backend. </p>

<ol>
<li>We assume that instrumentation is applied to a very limited section of the code source to lessen the impact of our significant overhead. </li>
</ol>

<p>TRANSLATION: We assume that you already know your performance hotspots. </p>

<ol>
<li>We assume that there is large amount of under utilized processing capacity to offload our significant overhead. </li>
</ol>

<p>TRANSLATION: We assume that you will not notice tricks used to hide our overhead. </p>

<ol>
<li>We assume that it is impossible to realistically and reliably measure our significant overhead. </li>
</ol>

<p>TRANSLATION: We assume that you know little about performance engineering. </p>

<p>And my favorite (5) is the restriction within a vendors software license on the publication of benchmark results. </p>

<p>TRANSLATION: We assume that you blindly accept our claims – unquestionably.</p>
"
AppDynamics,19518863,13524580,0,"2013/10/22, 16:10:28",False,"2013/10/22, 16:10:28",3374,2508411,1,"<p>Appdynamics will not slow down your system significant, I was on a usermeeting and they said that they always try to be under 2% cpu usage, thats nothing compared to what you get from them.
They are working with samples per time, so if you have 10 requests per second or 100, they will still take the same amout of your cpu / bandwith / whatever. </p>
"
AppDynamics,17416710,17412297,0,"2013/07/02, 05:56:59",True,"2013/07/02, 05:56:59",415,621964,6,"<p>The way that these products generally work is by doing bytecode injection / function interposition / monkey-patching on commonly used libraries and methods.  For instance, you might hook into JDBC query methods, servlet base classes, and HTTP client libraries.  When a request enters the application, track all the important methods/calls it makes, and log them in some way.  Take the data and crunch it into analytics, charts, and alerts.</p>

<p>On top of that, you can start to add in statistical profiling or other options.</p>

<p>The tricky things are tracking requests across process boundaries and dealing with the volume of performance data you'll gather. (I work on this problem at AppNeta)</p>

<p>One thing to check out is Twitter Zipkin (<a href=""https://github.com/twitter/zipkin"">https://github.com/twitter/zipkin</a>), doesn't support much and pretty early-stage but interesting project.</p>
"
AppDynamics,17418261,17412297,0,"2013/07/02, 08:52:04",False,"2013/07/02, 08:52:04",101,1792169,10,"<p>Both AppDynamics and New Relic use Standard BCI to monitor the common interfaces (entry and exit points) developers use to build applications (e.g. Servlet, struts, SOAP, JMS, JDBC, ...). This provides a basic skeleton of code execution (call graphs) with timing information which represents less than 5% of code that is executed.</p>

<p>The secret is to then uncover the timing of the remaining 95% code execution during slowdowns without incurring too much overhead in a production JVM. AppDynamics uses a combination of in-memory agent analytics and Java API calls to then extract the remaining code execution in real-time. This means no custom instrumentation is required or explicit declaration of what classes/methods you want the monitoring solution to instrument.</p>

<p>AppDynamics data collection is very different to that of New Relic. For example, with AppDynamics you can get a complete distributed call graph across multiple JVMs for a specific user request, rather than say an aggregate of requests.</p>

<p>BCI is a commodity these days, the difference is in the analytics and algorithms used by vendors that trigger diagnostics/call graph information so you end up with the right visibility at the right time to solve problems.</p>

<p>Steve.</p>
"
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86,5051173,7,"<p>That error means that gradle is unable to resolve the dependency on <code>com.appdynamics:appdynamics-runtime</code>. The easiest way to fix this problem is to use the AppDynamics libraries from maven central rather than the <code>adeum-maven-repo</code> directory. You can do that by editing your top level gradle file to look like this:</p>

<pre><code>buildscript {
    configurations.classpath.resolutionStrategy.force('com.android.tools.build:gradle:1.2.3')
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath 'com.android.tools.build:gradle:1.2.3'
        classpath 'com.appdynamics:appdynamics-gradle-plugin:4.+'
    }
}

allprojects {
    repositories {
        mavenCentral()
    }
}
</code></pre>

<p>Then your project-level gradle file would look like:</p>

<pre><code>apply plugin: 'adeum'

repositories {
    flatDir {
        dirs 'lib'
    }
}

dependencies {
    compile 'com.appdynamics:appdynamics-runtime:4.+'
}
</code></pre>

<p>Note that I have removed the references to <code>adeum-maven-repo</code>, and changed the version numbers on the AppDynamics artifacts to refer to them as they exist in maven central. Once you've done this, you no longer need <code>adeum-maven-repo</code> in your project, since gradle is now downloading these dependencies automatically.</p>
"
AppDynamics,58801626,56138542,0,"2019/11/11, 14:45:04",False,"2019/11/11, 14:45:04",26,12355048,0,"<ol>
<li>To install the Java Agent on JBoss EAP or JBoss Wildfly, you need to initialize the JVM. Run the following parameter:</li>
</ol>

<p>JAVA_OPTS=""$JAVA_OPTS -Djboss.modules.system.pkgs=org.jboss.byteman,com.singularity,org""
If you do not initialize the JVM, the installation throws a ""class not found"" exception. </p>

<ol start=""2"">
<li>Add the following to the end of the standalone.conf file in the JAVA_OPTS section.</li>
</ol>

<p>-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/p:
/jboss-logmanager-.jar</p>

<p>JDK9 and above, -Xbootclasspath/p option has been removed; use -Xbootclasspath/a instead.</p>

<p>-Djava.util.logging.manager=org.jboss.logmanager.LogManager -Xbootclasspath/a:
/jboss-logmanager-.jar</p>

<ol start=""3"">
<li>You can find the all installation process on the latest installation document:</li>
</ol>

<p><a href=""https://docs.appdynamics.com/display/PRO45/JBoss+and+Wildfly+Startup+Settings#JBossandWildflyStartupSettings-InitializetheJVM"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO45/JBoss+and+Wildfly+Startup+Settings#JBossandWildflyStartupSettings-InitializetheJVM</a></p>
"
AppDynamics,16171037,16161856,0,"2013/04/23, 16:48:17",False,"2013/04/23, 16:48:17",11,2047636,1,"<p>A great place to ask this question would be on the AppDynamics discussion forums so that AppDynamics support can answer you directly... <a href=""http://appsphere.appdynamics.com/t5/Discussions/ct-p/Discussions"" rel=""nofollow"">http://appsphere.appdynamics.com/t5/Discussions/ct-p/Discussions</a></p>

<p>I'm guessing there is a permissions issue somewhere.</p>
"
AppDynamics,18347965,16161856,0,"2013/08/21, 05:35:52",False,"2013/08/21, 05:35:52",17506,524588,1,"<p>You should copy all files to run the agent, not just javaagent.jar. </p>

<p>This is a thread about it. <a href=""http://appsphere.appdynamics.com/t5/Lite-for-Java/Keep-getting-a-quot-Invalid-Agent-Installation-Directory-quot-on/td-p/1151"" rel=""nofollow"">http://appsphere.appdynamics.com/t5/Lite-for-Java/Keep-getting-a-quot-Invalid-Agent-Installation-Directory-quot-on/td-p/1151</a></p>
"
AppDynamics,26749167,25891322,0,"2014/11/05, 05:23:23",True,"2014/11/05, 05:23:23",5502,376340,4,"<p>The definition can be found in AppDynamics docs: <a href=""https://docs.appdynamics.com/display/PRO39/Troubleshoot+Slow+Response+Times#TroubleshootSlowResponseTimes-SlowandStalledTransactions"" rel=""nofollow"">Slow and Stalled Transactions</a></p>

<blockquote>
  <p>By default AppDynamics considers a slow transaction one that lasts longer than 3 times the standard deviation for the last two hours, a very slow transaction 4 times the standard deviation for the last two hours, and a stalled transaction one that lasts longer than 300 deviations above the average for the last two hours.</p>
</blockquote>

<p>Of course, you can modify the default rules by providing your own: <a href=""https://docs.appdynamics.com/display/PRO39/Configure+Thresholds"" rel=""nofollow"">Configure Thresholds</a></p>
"
AppDynamics,29123098,29122786,0,"2015/03/18, 15:14:41",False,"2015/03/18, 15:14:41",505,4382794,0,"<p>Even it is not the desired behaviour, if filtering by URL and specifying only createNewActivity worked for me (as long as there are no other REST URLs matching this).</p>
"
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504,936832,2,"<p>Generally, monitoring tools cannot record method-level data continuously, because they have to operate at a much lower level of overhead compared to profiling tools. They focus on ""business transactions"" that show you high-level performance measurements with associated semantic information, such as the processing of an order in your web shop.</p>

<p>Method level data only comes in when these business transactions are too slow. The monitoring tool will then start sampling the executing thread and show you a call tree or hot spots. However, you will not get this information for the entire VM for a continuous interval like you're used to from a profiler.</p>

<p>You mentioned JProfiler, so if you are already familiar with that tool, you might be interested in <a href=""http://www.ej-technologies.com/products/perfino/overview.html"" rel=""nofollow noreferrer"">perfino</a> as a monitoring solution. It shows you samples on the method level and has cross-over functionality into profiling with the native JVMTI interface. It allows you to do <a href=""http://resources.ej-technologies.com/perfino/help/doc/main/profiling.html"" rel=""nofollow noreferrer"">full sampling of the entire JVM</a> for a selected amount of time and look at the results in the JProfiler GUI.</p>

<p><a href=""https://i.stack.imgur.com/40ZV9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/40ZV9.png"" alt=""enter image description here""></a></p>

<p>Disclaimer: My company develops JProfiler and perfino.</p>
"
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207,992280,-7,"<p>Information I got from another website.</p>

<p>Application Insights (AI) is a very simplistic APM tool today. It does do transaction tracing, it has very limited code and runtime diagnostics, and it doesn’t go very deep into analyzing the browser performance. Application Insights is much more like Google Analytics than like a typical APM tool.</p>

<ul>
<li>AI does not support mobile apps, AppDynamics does Android and iOS</li>
<li>AI only supports Java, .NET, and node.js while AppDynamics supports many
additional languages and technologies.</li>
<li>AI requires code changes to do additional metric capture from the
code. AppDynamics has do this on the fly without code change on most
languages.</li>
<li>AI doesn’t so transaction distributed tracing, it has a simple data
collection model similar to what New Relic does. This makes
troubleshooting much harder in complex apps. If your app is simple
it’s not required.</li>
<li>AI lacks transaction scoring and baselining, you must set manual
thresholds. AppDynamics does this for every metric and every business
transaction.</li>
<li>AI doesn’t monitor databases or data stores. AppDynamics does both.</li>
<li>AI is SaaS only, while AppDynamics can be deployed on premises or
SaaS.</li>
</ul>
"
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298,7005159,1,"<p>We could install the AppDynamics extension use Azure portal or Kudu tool(<a href=""https://functionAppname.scm.azurewebsites.net/"" rel=""nofollow noreferrer"">https://functionAppname.scm.azurewebsites.net/</a>).</p>

<p><strong>Azure Portal:</strong></p>

<p><a href=""https://i.stack.imgur.com/9Ge0J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Ge0J.png"" alt=""enter image description here""></a></p>

<p><strong>Kudu UI</strong></p>

<p><a href=""https://i.stack.imgur.com/HIWHy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HIWHy.png"" alt=""enter image description here""></a></p>

<p><strong>After installation</strong></p>

<p><a href=""https://i.stack.imgur.com/i0BpP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i0BpP.png"" alt=""enter image description here""></a></p>
"
AppDynamics,48631895,48631700,0,"2018/02/05, 23:36:59",False,"2018/02/05, 23:36:59",1274,7716468,0,"<p>I have used Crashlytics and Google Analytics together without any issue. All the logging is done in a background process so I don't think you will notice any speed degradation but the app is technically doing more work so there is some sort of performance hit. </p>

<p>I haven't seen any issues with the crashlog. Analytics libraries just write the crashlogs to a file and then send them the next time the user opens your app. They are not affecting how the actually crashes are handled by the operating system so there shouldn't be any issue with them conflicting.</p>
"
AppDynamics,48631903,48631700,0,"2018/02/05, 23:37:27",True,"2018/02/05, 23:53:27",12972,541786,4,"<p><strong>First</strong> : I would strongly suggest removing the unused one (or the one you don't prefer) from your code. For reasons, like :<br>
1. It will increase project size which in turn will increase your bundle size.<br>
2. Messy code.<br>
3. There is no point checking two different analytics.<br>
4. While third person is understanding the code, he would waste his time in understanding which will lead to confusion.  </p>

<p>I might be missing other reasons.  </p>

<p><strong>Second</strong> : To answer your question, it should work fine. I did the same in one of my projects, where initially I was using <strong>Hockey Crash reporting</strong>. But then client asked to use <strong>Crashlytics</strong>. I didn't remove Hockey SDK immediately. Though this worked fine and both reported the issues, but soon I removed Hockey SDK from the code.</p>
"
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182,1622880,2,"<p>There are integration tools available between influxdb/AppDynamics and grafana/AppDynamics.</p>

<p><a href=""https://github.com/Appdynamics/MetricMover"" rel=""nofollow noreferrer"">https://github.com/Appdynamics/MetricMover</a></p>

<p><a href=""https://grafana.com/plugins/dlopes7-appdynamics-datasource/installation"" rel=""nofollow noreferrer"">https://grafana.com/plugins/dlopes7-appdynamics-datasource/installation</a>).</p>

<p>There's nothing that integrates between Prometheus and AppDynamics at the moment</p>

<p>I'm not sure there will be one going forward, seeing how they are competing in the same space from different vantage points (Open Source vs Enterprise)</p>
"
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26,12355048,0,"<p>If you want to monitor only your worker processes of IIS and Standalone Service, You can use CLR Crash event on your policy configuration.</p>

<p>AppDynamics automaticaly creates CLR Crash Events If your IIS or Standalone Service are crashed.</p>

<p>You can find the details of CLR Crash Events:
<a href=""https://docs.appdynamics.com/display/PRO45/Monitor+CLR+Crashes"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO45/Monitor+CLR+Crashes</a></p>

<p>Also, Sample policy configuration: 
<a href=""https://i.stack.imgur.com/mloax.png"" rel=""nofollow noreferrer"">Policy Configuration Screen</a></p>
"
AppDynamics,65946975,65946794,2,"2021/01/29, 01:58:56",False,"2021/01/29, 01:58:56",483,4895267,0,"<p>AppDynamics have a fork of contrib project <a href=""https://github.com/Appdynamics/opentelemetry-collector-contrib"" rel=""nofollow noreferrer"">here</a> where there is an exporter but it isn't clear if it has been finished</p>
"
AppDynamics,66421315,65946794,0,"2021/03/01, 13:29:48",True,"2021/03/01, 13:29:48",3170,1237595,1,"<p>there is a Beta program now running for using AppDynamics with the OpenTelemetry Collector.</p>
<p>More details are available in the public docs: <a href=""https://docs.appdynamics.com/display/PRO21/Ingest+OpenTelemetry+Trace+Data"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Ingest+OpenTelemetry+Trace+Data</a></p>
"
AppDynamics,13182788,13182423,0,"2012/11/01, 19:52:02",True,"2012/11/01, 19:52:02",49920,32453,1,"<p>Looks like you create the metric, then edit a dashboard, then click on a widget -> add metric -> (browse, but choose ""Individual Nodes"" instead of JMX, then select your metric. voila.</p>
"
AppDynamics,24808336,24797525,2,"2014/07/17, 18:58:39",False,"2014/07/17, 18:58:39",11,2047636,0,"<p>The best place for you to get this question answered would be on the AppDynamics community discussion boards. Here's a link for you... <a href=""http://community.appdynamics.com/t5/Forums-Community-AppDynamics/ct-p/Discussions"" rel=""nofollow"">http://community.appdynamics.com/t5/Forums-Community-AppDynamics/ct-p/Discussions</a></p>

<p>The AppDynamics documentation site is also a great resource and you don't even need a login to access them... <a href=""http://docs.appdynamics.com/"" rel=""nofollow"">http://docs.appdynamics.com/</a></p>
"
AppDynamics,24809185,24797525,3,"2014/07/17, 19:45:22",False,"2014/07/17, 19:45:22",66,3850108,0,"<ol>
<li><p>Installation instructions for the controller can be found in:
<a href=""http://docs.appdynamics.com/display/PRO14S/Install+the+Controller"" rel=""nofollow"">http://docs.appdynamics.com/display/PRO14S/Install+the+Controller</a>
Assuming you are using App Server Agent for Java, installation instructions for that and the machine agent can be found in the above site on the Left Nav table of contents.</p></li>
<li><p>The controller is the management server, the app server agent monitors the JVM and the machine agent collects system metrics (such as CPU, Memory, Disk &amp; Network).</p></li>
<li><p>Very minimal configuration is required.</p></li>
</ol>
"
AppDynamics,30294344,29858961,0,"2015/05/18, 05:39:08",True,"2015/05/18, 05:39:08",16,4910351,0,"<p>From what I understand from their documentation, AppD do not have a way to capture heap dumps. They suggest using Memory Leak detection feature in such scenarios.
On a different note, I know we can get thread dumps which maybe helpful in some cases (Agents -> Request Agent Log Files)</p>
"
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51,3269857,3,"<p>Heap dumps in appdynamics can be taken for JRockit JVM by the below method(Note : This does not work for IBM JVM)</p>

<ul>
<li>Select you node in appdynamics for which the heap dump has to be taken.</li>
<li>Select the tab ""JMX""</li>
<li>Go to ""MBean Browser""</li>
<li>Go to ""com.sun.management"" >> ""HotSpotDiagnostic"" from the left pane </li>
<li>On the right window under operations for dumpHeap click on the thunderbolt sign to invoke action.</li>
<li>A new dialogue box opens up in which you have to fill the p0 and p1 text boxes as follows</li>
</ul>

<p>p0 – Path to generate heap dump(/path/dump.hprof)</p>

<p>p1 -  True – GC before Heap dump ; False - No GC before heap dump</p>

<ul>
<li>Click on ""invoke""</li>
</ul>

<p>Note : If you want heap dump to be generated in the case of out of memory give </p>

<p>p0 : HeapDumpOnOutOfMemoryError</p>

<p>Also note that these values will be lost on JVM restart.</p>
"
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66,1355536,3,"<p>Data retrieval by APM tools is done in several ways, each one with its pros and cons</p>

<ul>
<li><p><em>Bytecode injection</em> (for both Java and .NET) is one technique, which is somewhat intrusive but allows you to get data from places the application owner (or even 3rd party frameworks) did not intend to allow.</p></li>
<li><p><em>Native function interception</em> is similar to bytecode injection, but allows you to intercept unmanaged code</p></li>
<li><p><em>Application plugins</em> - some applications (e.g. Apache, IIS) give access to monitoring and application information via a well-documented APIs and plugin architecture</p></li>
<li><p><em>Network sniffing</em> allows you to see all the communication to/from the monitored machine</p></li>
<li><p><em>OS specific un/documented APIs</em> - just like application plugins, but for the Windows/*nix</p></li>
</ul>

<p>Disclaimer : I work for Correlsense, provider of APM software SharePath, which uses all of the above methods to give you complete end-to-end transaction visibility.</p>
"
AppDynamics,42301899,41933094,0,"2017/02/17, 17:40:40",False,"2017/02/17, 17:48:24",4877,1140237,0,"<p>I've tried add multi dex without giving any minimum or maximum of method count per dex file wise.I've tried with simply just adding multidex and able to build.And Yes!! I am able to build app too.</p>

<p>major change is in <code>afterEvaluate</code> &amp; <code>incremental true</code> in <code>dexoption</code>.</p>

<p><strong>build.gradle</strong> </p>

<pre><code>android {
    compileSdkVersion 23
    buildToolsVersion '23.0.3'    
    defaultConfig {
        minSdkVersion 14
        targetSdkVersion 18
        renderscriptTargetApi 18
        //renderscriptSupportModeEnabled true
        // To enable MultiPle dex
        multiDexEnabled true
    }
    dexOptions {
        // Option to handle multi dex
        incremental true
        javaMaxHeapSize ""4g"" // try with 2g as it worked for my case its working in both cases
        // here 4g i got this thing from https://groups.google.com/forum/#!topic/adt-dev/P_TLBTyFWVY
    }
    packagingOptions {

        exclude 'META-INF/LICENSE.txt'
        exclude 'META-INF/DEPENDENCIES'
        exclude 'META-INF/LICENSE'
        exclude 'META-INF/NOTICE'

    }
    signingConfigs {
        Release {
            keyAlias 'helloworld'
            keyPassword 'helloworld'
            storeFile file('../helloworld')
            storePassword 'helloworld'
        }
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'),
                    'proguard-project.txt'
        }
        debug {
            signingConfig signingConfigs.Release
        }
    }
}


dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    testCompile 'junit:junit:4.12'

      compile 'com.android.support:multidex:1.0.1'
    compile 'com.android.support:appcompat-v7:23.3.0'
    compile 'com.android.support:design:23.3.0'

    compile 'com.google.android.gms:play-services-maps:8.4.0'
    compile 'com.google.android.gms:play-services-ads:8.4.0'
    compile 'com.google.android.gms:play-services-analytics:8.4.0'
    compile 'com.google.code.gson:gson:2.6'    
    compile 'com.github.bumptech.glide:glide:3.7.0'
    compile 'com.github.bumptech.glide:okhttp3-integration:1.4.0@aar'
    compile 'com.squareup.okhttp3:okhttp:3.2.0'    
    compile 'com.appdynamics:appdynamics-runtime:4.+'
}
afterEvaluate {
    tasks.matching {
        it.name.startsWith('dex')
    }.each { dx -&gt;
        if (dx.additionalParameters == null) {
            dx.additionalParameters = []
        }
        dx.additionalParameters += '--multi-dex' // enable multidex without giving minimum or maximum limit
    }
}
</code></pre>

<p>application's parent gradle</p>

<pre><code>// Top-level build file where you can add configuration options common to all sub-projects/modules.

buildscript {
    repositories {
        jcenter()
    }
    dependencies {
        classpath 'com.android.tools.build:gradle:2.2.3'
        classpath 'com.appdynamics:appdynamics-gradle-plugin:4.+'
        // NOTE: Do not place your application dependencies here; they belong
        // in the individual module build.gradle files
    }
}

allprojects {
    repositories {
        jcenter()
    }
}

task clean(type: Delete) {
    delete rootProject.buildDir
}
</code></pre>

<p>If above thing have still issue just check your dependecies hierarchy if any other extra dependecies are added (Based on your <code>build.gradle</code>  <code>packagingOptions</code> there should be some other dependecies there).Not sure but it may possible because of internal library conflicts its not proceeding further to create dexfile or build.</p>

<p>Let me know if anything</p>
"
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324,7220665,3,"<p>You can use the REST API of the Controller described here <a href=""https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs</a></p>

<p>To access the REST API Browser, in a Web browser, go to: </p>

<pre><code>https://&lt;controller_host&gt;:&lt;primary_port&gt;/api-docs/index.html
</code></pre>

<p>this will give you a nice swagger UI description of the available resources.</p>

<p>Alternatively you can create reports directly out of the box in AppDynamics via the <code>Dashboards &amp; Reports</code> section.</p>
"
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324,7220665,1,"<p>First you need to setup the AppDynamics Controller (should be hosted on a separate  machine), then you need java agents on the machine with the application. 
<a href=""https://i.stack.imgur.com/b4r01.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b4r01.png"" alt=""enter image description here""></a></p>

<p>You need to wire the application with the Java agent, e.g. like this</p>

<pre><code>/usr/bin/java
-Dappdynamics.controller.hostName=""my-appdynamics-controller-url""
-Dappdynamics.controller.port=""8090"" \
-Dappdynamics.agent.applicationName=""MyAppName""
-javaagent:/appdynamics/appserver-agent/javaagent.jar
-Dappdynamics.agent.tierName=""my-tier-name""
-Dappdynamics.agent.nodeName=""my-node-name""
-jar myapp.jar
</code></pre>

<p>Now the Java agent sends application information to the controller. Take a look at the documentation. <a href=""https://docs.appdynamics.com/display/PRO43/Getting+Started"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO43/Getting+Started</a></p>

<p>The application will automatically be available in AppDynamics and you can see the dashboard.</p>
"
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68,1394813,0,"<p>You don't need to host the controller, you can get a SAAS account here. <a href=""https://www.appdynamics.com/free-trial/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/free-trial/</a> . </p>

<p>Once you get a SAAS account lookup the account name and account key in the welcome email and use the instructions in the email to add JVM args to your app as shown. Don't forget to set the account name and account access key args: -Dappdynamics.agent.accountName -Dappdynamics.agent.accountAccessKey .</p>
"
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035,2711488,2,"<p>See <a href=""https://docs.appdynamics.com/display/PRO43/Monitor+Java+Interface+Static+and+Default+Methods"" rel=""nofollow noreferrer"">4.3.x Documentation⇒POJO Entry Points⇒Monitor Java Interface Static and Default Methods</a>:</p>

<blockquote>
  <p>Note that another Java language feature introduced in Java 8, lambda method interfaces, are not supported by the AppDynamics Java Agent.</p>
</blockquote>

<p>It’s possible that this is due to technical difficulties with <a href=""https://bugs.openjdk.java.net/browse/JDK-8145964"" rel=""nofollow noreferrer"">JDK-8145964</a> as you suspect. But I’d also point out that this kind of Instrumentation would be questionable. It’s not this JRE generated class that implements any specific behavior, it’s the invoked target method.</p>
"
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474,4682632,0,"<p>Support for Lambda expressions has been in the product since 4.1 which shipped in 2015 I believe. That being said we are always enhancing support. These do have some limitations after initialization of the classes using them (dynamic instrumentation limits). The product should support them, we have added some additional capabilities and features for Lambda expressions in our next major release. Have you tried to contact help @ appdynamics.com</p>
"
AppDynamics,49033394,49004859,0,"2018/02/28, 17:45:13",False,"2018/02/28, 17:45:13",9111,4647853,0,"<p>Looks like it was not mentioned in release notes, but instead <a href=""https://docs.appdynamics.com/display/PAA/Support+Advisory+56039%3A+Errors+triggered+by+lambda+constructs"" rel=""nofollow noreferrer"">Support Advisory 56039</a> was raised. They indeed mention JDK-8145964 as a reason for removing the support.</p>
"
AppDynamics,50713654,50713335,2,"2018/06/06, 09:34:40",False,"2018/06/06, 09:34:40",44,2480398,1,"<p>I got this too... I was running from command-line as a non-root user:</p>

<pre><code>./platform-setup-x64-linux-4.4.3.10393.sh -q -varfile /appd/home/Install/response.varfile
</code></pre>

<p>I added the shell expand(-x) switch and log to the command(s) like so:</p>

<pre><code> bash -x ./platform-setup-x64-linux-4.4.3.10393.sh -q -varfile /appd/home/Install/response.varfile &gt; install.log 2&gt;&amp;1
</code></pre>

<p>If we tail the last bit of that log you get, this response in debug mode:</p>

<pre><code>Verifying if the libaio package is installed. /opt/appdynamics/platform/installer/checkLibaio.sh
Required libaio package is not found. For instructions on installing
the missing package, refer to https://docs.appdynamics.com/display/PRO44/Enterprise+Console+Requirements
</code></pre>

<p>and the script checkLibaio.sh isn't left there... so you cannot figure it out easily. I also have a RedHat variant with the packages installed:</p>

<pre><code>rpm -qa | grep libaio
libaio-0.3.109-13.el7.x86_64
</code></pre>

<p>Strangely enough I have one VM from the same image that will install the distribution just fine, and one that will not, so on the broken install (where I really want to install this). I ran another command from the expanded view of the install.log, which was a really long JVM command line. Anyways I got it to work and then made a looping script to retrieve the file (Because AppD for some reason removes the check script before you can look at it). The script is as follows:</p>

<pre><code>#!/bin/sh

# Script used to check if the machine has libaio on it or not. 

cat /dev/null &gt; /opt/appdynamics/platform/installer/.libaio_status
chmod 777 /opt/appdynamics/platform/installer/.libaio_status

# Check if the dpkg or rpm command exists before running it.
command -v dpkg &gt;/dev/null 2&gt;&amp;1
OUT=$?
if [ $OUT -eq 0 ];
then
    if [ `dpkg -l | grep -i libaio* | wc -l` -gt 0 ];
    then
        echo SUCCESS &gt;&gt; /opt/appdynamics/platform/installer/.libaio_status
        exit 0
    fi
else
    command -v rpm &gt;/dev/null 2&gt;&amp;1
    OUT=$?
    if [ $OUT -eq 0 ];
    then
        if [ `rpm -qa | grep -i libaio* | wc -l` -gt 0 ];
        then
            echo SUCCESS &gt;&gt; /opt/appdynamics/platform/installer/.libaio_status
            exit 0
        fi
    fi
fi
echo FAILURE &gt;&gt; /opt/appdynamics/platform/installer/.libaio_status
exit 1
</code></pre>

<p>I you run this script like me on the faulty platform what you will discover is that your version of Linux has both:</p>

<pre><code>dpkg
</code></pre>

<p>and </p>

<pre><code>rpm 
</code></pre>

<p>installed. To work around this you should temporarily make a name change to one of these two package manager executables so it cannot be found (by your shell environment). </p>

<p>Most common here will be that you are running a RedHat variant where someone chose to install dpkg (For who knows what reason). If so desired remove that package and the install should be successful.</p>
"
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049,3156333,2,"<p>As stated on the AppD docs regarding <a href=""https://docs.appdynamics.com/display/CLOUD/Kubernetes+and+AppDynamics+APM"" rel=""nofollow noreferrer"">Kubernetes and AppDynamics APM</a></p>
<p><a href=""https://i.stack.imgur.com/qieEb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qieEb.jpg"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>Install a Standalone Machine Agent (1) in a Kubernetes node.</p>
<p>Install an APM Agent (2) inside each container in a pod you want to monitor.</p>
<p>The Standalone Machine Agent then collects hardware metrics for each monitored container, as well as Machine and Server metrics for the host (3), and forwards the metrics to the Controller.</p>
</blockquote>
<p>ContainerID and UniqueHostID can be taken from <code>/proc/self/cgroup</code></p>
<blockquote>
<p>ContainerID <code>cat /proc/self/cgroup | awk -F '/' '{print $NF}'  | head -n 1</code></p>
<p>UniqueHostID <code>sed -rn '1s#.*/##; 1s/(.{12}).*/\1/p' /proc/self/cgroup</code></p>
</blockquote>
"
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474,4682632,2,"<p>Thanks for the reply to my question. The .NET agent in most APM tools works the same way which leverages the profiler APIs in the .NET SDK and allows for data collection and also callbacks and other interception. Most of the tools also use performance counter data, and other sources aside from inside the .NET runtime. This allows you to do several things similar to Java in terms of data collection. </p>

<p>ref: <a href=""https://docs.microsoft.com/en-us/visualstudio/profiling/walkthrough-using-profiler-apis?view=vs-2017"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/visualstudio/profiling/walkthrough-using-profiler-apis?view=vs-2017</a></p>

<p><a href=""http://www.blong.com/conferences/dcon2003/internals/profiling.htm"" rel=""nofollow noreferrer"">http://www.blong.com/conferences/dcon2003/internals/profiling.htm</a></p>
"
AppDynamics,55150379,55048208,0,"2019/03/13, 22:07:48",True,"2019/03/13, 22:07:48",41,2625630,0,"<p>The solution was adding ""appdynamics"" to the ""externals"" in the Webpack configuration: <a href=""https://webpack.js.org/configuration/externals/"" rel=""nofollow noreferrer"">https://webpack.js.org/configuration/externals/</a></p>

<p>This allows AppDynamics to use the default Node.js require import.</p>
"
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955,284480,1,"<p>The most important thing to understand about this error is the meaning of this line: </p>

<p><code>Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target</code></p>

<p>SSL certificates work by establishing a certificate chain, or a hierarchy of trust.  For example, if I go to <a href=""https://www.google.com"" rel=""nofollow noreferrer"">https://www.google.com</a> and look at their cert, this is what I see:</p>

<p><a href=""https://i.stack.imgur.com/K6GBG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K6GBG.png"" alt=""Google cert chain""></a></p>

<p>There is the google cert, which sits on their servers/CDN, then an intermediate cert which also sits on their servers/CDN, then a trusted root CA cert which is in the <strong>client</strong> keystore and is implicitly trusted.  So when someone browses to google, b/c they have the root CA cert and have trusted it, the browser (client) will trust that the server is actually who they say they are and will establish a secure connection to the site.</p>

<p>So getting back to your error, whatever CA issued the server cert being used by RabbitMQ, the monitor is not recognizing it as trusted.  To troubleshoot this error, here are the things to do:</p>

<ul>
<li>Look at the server cert and ensure it can be validated.  openssl works well for this; run: <code>openssl s_client -connect 127.0.0.1:15672 -showcerts</code> and look at the cert chain.</li>
<li>Validate the root CA is trusted by your java keystore.  You can view these certs with the <code>keystore</code> tool: <code>keytool -list -v -keystore &lt;/path/to/keystore&gt; -storepass &lt;pass&gt;</code>.  Ensure the cert listed above is in the keystore.</li>
</ul>

<p>There are a couple other gotchas to watch out for:</p>

<ul>
<li>What keystore java is using is not always obvious.  The jdk has a default keystore, and each app can use its own keystore, like you are doing above.  Ensure you know what keystore is being used.  Although it will add lots of logging, is can be helpful to add <code>-Djavax.net.debug=all</code> to the command line.</li>
<li>Beware adding individual server certs to the keystore.  This will work, until the server cert expires.  Much better to depend on trusted CA certs, which are generally maintained at the platform level.  Adding individual certs is generally considered an anti-pattern.</li>
</ul>
"
AppDynamics,66101093,63647198,0,"2021/02/08, 13:51:32",False,"2021/02/08, 13:51:32",3170,1237595,0,"<p>This issue was solved here: <a href=""https://community.appdynamics.com/t5/Java-Java-Agent-Installation-JVM/monitor-URL-page-availability/td-p/40904"" rel=""nofollow noreferrer"">https://community.appdynamics.com/t5/Java-Java-Agent-Installation-JVM/monitor-URL-page-availability/td-p/40904</a></p>
<p>Summary:</p>
<ul>
<li>Question did not contain full config.yml contents</li>
<li>Lots of default config needed commenting out as below</li>
</ul>
<pre>
    # Patterns to be matched, multiple patterns(to be matched) can be configured for a given site
     #matchPatterns:
     #- name: 
     #  type: 
     #  pattern: 

   #- name:     
   #  url:      
   #  authType: 

  # Basic Authentication with password encryption
   #- name:       
   #  url:        
   #  username:   
   #  password:   
   #  encryptionKey: """"
   #  encryptedPassword: """"
   #  authType: 

   #- name:     
   #  url:      

  #NTLM Auth Sample Configuration
   #- name:     
   #  url:      
   #  username: 
   #  password: 
   #  authType: 

     # Client Cert Auth Sample Configuration
   #- name:         
   #  url:          
   #  password:     
   #  authType:     
   #  keyStoreType: 
   #  keyStorePath: 
   #  keyStorePassword: 
   #  trustStorePath: 
   #  trustStorePassword: 

     #POST request sample configuration
   #- name:     
   #  url:      
   #  username:
   #  password:
   #  connectTimeout: 60000
   #  usePreemptiveAuth: 
   #  method:   
   #  headers:
   #    Content-Type: application/json
   #  requestPayloadFile: src/test/resources/conf/postrequestPayloadFile
   #  matchPatterns:
   #    - name:       Error
   #      type:       substring
   #      pattern:    Error 400

     #Proxy Configuration
   #- name:     
   #  url:      
   #  groupName: 
   #  proxyConfig:
   #    host: """"
   #    port: 
   #    username: """"
   #    password: """"
</pre>
"
AppDynamics,13198174,13198172,0,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",49920,32453,0,"<p>Turns out if you go to configure -> instrumentation -> JMX tab then voila, you can now delete metrics and modify/edit them. But nowhere else. Odd.</p>
"
AppDynamics,18914810,18895293,0,"2013/09/20, 13:44:18",False,"2013/09/20, 13:44:18",3,2795488,0,"<p>I've instrumented  org.apache.ode.bpel.engine.BpelProcess.handleJobDetails and it is showing me transactions going through Carbon out to the backends. </p>
"
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1,2908075,0,"<p>It is a bit hard to completely answer your question and solve the issue with the provided information. However, I hope my questions below help you to get on the right track.</p>

<p>1.) After making the configuration change, did you also restart the AppDynamics.AgentCoordinator_WindowsService? Without restarting it the new configuration will not be applied to the agent itself.</p>

<p>2.) Also important is your windows service hosting any OOTB support entry point like WCF, ASP.NET MVC4 WebAPI, web services etc.? If not, you need to setup a custom entry point. If you  check out the AppDynamics documentation and search for'POCO Entry Points' you should get onto the right track</p>

<p>3.) In case No.1 &amp; No.2 did not do the trick, could you please attach the config.xml file for review? Or directly reach out to the AppDynamics customer success team.</p>

<p>Kind regards,
Theo</p>

<p>Disclaimer: I work for AppDynamics as part of the Customer Success team.</p>
"
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1,2908075,0,"<p>Just to add to you answer and to clarify a little. Until release 3.7.7 the .NET agent from AppDynamics used the web.config (for IIS based application), App.config for standalone and windows services or the global.config plus some environment variables to configure the agent.</p>

<p>With the 3.7.8 or later release we replaced this with a cleaner truly singly configuration file approach. The configuration file is called config.xml and located in the %ProgramData%\AppDynamics... directory. For any version after 3.7.8 all settings have to be in the config.xml.</p>
"
AppDynamics,19599763,19518737,0,"2013/10/26, 00:32:59",False,"2013/10/26, 00:32:59",11,2047636,0,"<p>You really should take this up with AppDynamics support by filing a ticket or posting in the support forums... <a href=""http://www.appdynamics.com/support/#helptab"" rel=""nofollow"">http://www.appdynamics.com/support/#helptab</a></p>
"
AppDynamics,28696479,28692916,2,"2015/02/24, 14:58:43",False,"2015/02/24, 14:58:43",72,3641322,-1,"<p>There are many things you should analyse. It depends on what your LR portal uses most. You may want to analyse web content , user, groups, calnder, theme related services.</p>
"
AppDynamics,30730071,30706344,0,"2015/06/09, 13:57:33",False,"2015/06/09, 13:57:33",474,4682632,0,"<p>I would ask support on this if you can't get it to work. Here is the configuration for custom exit and entry points in the product: <a href=""https://docs.appdynamics.com/display/PRO14S/Configure+Custom+Exit+Points"" rel=""nofollow"">https://docs.appdynamics.com/display/PRO14S/Configure+Custom+Exit+Points</a></p>
"
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474,4682632,0,"<p>Love to help you out here, but there are no details about the error, please email help@appdynamics.com for assistance. I assume this is C#, but wouldn't know based on this. AppDynamics supports many languages and technologies. </p>
"
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117,2384622,1,"<p>For whom it might be of interest I have found a workaround and more details about this issue. This occurs only in the following scenario:</p>

<ol>
<li>AppDynamics agent is installed and running</li>
<li>ServiceStack API is compiled using the MSBuild from c:\Windows\Microsoft.NET\Framework64\v4.0.30319\MsBuild.exe</li>
</ol>

<p>If you use the MsBuild 14 that is installed along Microsoft Visual Studio 2015 RC then this issue does not occur anymore. From my first findings there is an issue in ServiceStack's way of caching the endpoints and wrapping the execute method using Linq but I don't understand why this happens only when AppDynamics agent is installed.</p>

<p>@mythz if you want to dig deeper into this issue I'm available to help but with the above solution everything is ok.</p>

<p>Hope this helps</p>
"
AppDynamics,38459237,38454851,0,"2016/07/19, 15:55:16",False,"2016/07/19, 15:55:16",474,4682632,0,"<p>There are lots of APIs available out of the box, you can find the docs at <a href=""https://docs.appdynamics.com/"" rel=""nofollow"">https://docs.appdynamics.com/</a> not everything has an API. You should find the user management as part of the configuration API here : <a href=""https://docs.appdynamics.com/display/PRO42/Configuration+API"" rel=""nofollow"">https://docs.appdynamics.com/display/PRO42/Configuration+API</a> </p>
"
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247,1232692,1,"<p>They do different things.  ELK will give you log aggregation that you can add in other functionality.  Appdynamics is great for real time monitoring and profiling. I think it depends on what you're going for.  Logging a distributed system and capturing error messages in one place might be very helpful with ELK.  Not just that, but ELK can be used in a number of other ways.  Elasticsearch can be used stand alone as a search engine or data cache.</p>

<p>TL;DR  It depends on what you're doing.  Maybe yes...maybe no...</p>
"
AppDynamics,39715544,39652512,0,"2016/09/27, 06:32:35",True,"2016/09/27, 06:32:35",506,5006681,0,"<p>On that particular server, we have the .NET agent running as well. After shutting off the .NET agent, we were no longer having this issue with 1-2 hour gaps in the metric browser. Apparently, there is some conflict in having multiple machine agents installed on the same server.</p>
"
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633,4033292,0,"<p>You need both unit tests and integration tests. Unit tests should not use in database or File, ect. I like to use Spring profiles for my tests. For instance, if I have a profile called integeration_test. </p>

<pre><code>@ActiveProfiles(""integeration_test"")
@ContextConfiguration(locations = {
        ""classpath:you-context.xml""})
@RunWith(SpringJUnit4ClassRunner.class)
public abstract class DaoTest
{

    @Autowired
    protected DataSource dataSource;

    // delete all your stuff here
    protected void clearDatabase()
    {
        JdbcTemplate jdbc = new JdbcTemplate(dataSource);
        jdbc.execute(""delete table"");
    }

    @Before
    public final void init()
    {
        clearDatabase();
    }

    @After
    public final void cleanup()
    {
        clearDatabase();
    }

}
</code></pre>

<p>(I'm using xml) then in your context do something like: <code>&lt;beans profile=""test""&gt;TODO &lt;/beans&gt;</code> And configure your data-source in there.</p>

<p>I know there are ways to rollback all your transactions after running a test, but I like this better. Just don't delete everything in your real database haha, could even put some safety code in the clearDatabase to make sure that doesn't happen.</p>

<p>For performance testing you will really need to figure out what you want to achieve, and what is meaningful to display. If you have a specific question about performance testing you can ask that, otherwise it is too broader topic.</p>

<p>Maybe you can make a mini-webapp which does performance testing for you and has the results exposed as URL requests for displaying in HTML. Really just depends on how much effort you are willing to spend on it and what you want to test.</p>
"
AppDynamics,64280827,40452664,0,"2020/10/09, 16:13:06",False,"2020/10/09, 16:13:06",556,1086679,0,"<p>Once the agent is attached you can use the <a href=""https://docs.appdynamics.com/display/PRO45/Database+Queries+Window"" rel=""nofollow noreferrer"">AppDynamics Database Queries Window</a></p>
"
AppDynamics,41757511,41755184,1,"2017/01/20, 08:38:17",True,"2017/01/20, 08:38:17",2681,506813,1,"<p>Reflection (inherently <code>TypeVisitor</code> and <code>TypeToken</code> classes) is always costly in Java, try not using it. Rendering time seems OK. There can be thousand reasons for high latency in an application, but you only gave this much information so that's about the best answer you can get. </p>
"
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474,4682632,1,"<p>That's the beauty of APM is you don't need to deal with logging to get performance data. APM tools instrument the applications themselves regardless of what the code does (logging, generating metrics, etc). AppDynamics can collect log data, and provide similar capabilities to what a log analytics tool can do, but it's of less value than the transaction tracing and instrumentation you get. Your application has to be built in a supported language (Java, .NET, PHP, Python, C++, Node.js) and if it's web or mobile based you can also collect client side performance data and unify between both frontend and backend. If you have questions just reach out and I can answer them for you. Good luck!</p>
"
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324,7220665,1,"<p>You basically need the AppDynamics Controller and a AppDynamics Machine-Agent which will be installed on the machine to monitor. In the Machine-Agent configuration you set the URI of the controller and the agent starts to report machine metrics to the controller. Then you can configure alarms, see metrics, create dashboards, etc. I can deliver you more information if you want, but as Jonah Kowall said, take a look at the documentation as well <a href=""https://docs.appdynamics.com/display/PRO42/Standalone+Machine+Agent"" rel=""nofollow noreferrer"">AppDynamics Machine Agent Doc</a></p>
"
AppDynamics,43040838,42274447,0,"2017/03/27, 10:58:07",False,"2017/03/27, 10:58:07",324,7220665,1,"<p>Typically, you don't need to change anything in your code to capture JMX metrics except your Java Beans have to fulfill the Management Beans (MBeans) requirements you have to enable JMX monitoring in each Java process that should be monitored and the monitored system runs on Java 1.5 or later. See also <a href=""https://docs.appdynamics.com/display/PRO42/Monitor+JMX+MBeans"" rel=""nofollow noreferrer"">here</a> and <a href=""https://docs.appdynamics.com/display/PRO42/Monitor+JVMs"" rel=""nofollow noreferrer"">here</a>. Then you can navigate to <code>Tiers &amp; Nodes -&gt; Select a tier -&gt; JMX tab</code></p>
"
AppDynamics,42996563,42361990,0,"2017/03/24, 12:05:58",True,"2017/06/07, 09:23:41",324,7220665,2,"<p>I am not exactly sure what the problem is, but you can share a Dashboard and open this shared Dashboard URL and make the Browser fullscreen. Thats how we do it and it works perfectly (on AppDynamics controller version 4.2).</p>

<pre><code>Toolbar --&gt; Dashboard &amp; Reports --&gt; Chose a dashboard --&gt; 
Switch to edit mode --&gt; Actions --&gt; Share Dashboard --&gt; Action (again) --&gt; 
Open Shared URL in new window --&gt; Make Browser fullscreen
</code></pre>

<p>We use the 'revolver tab' add-on for the browser to switch between desktops.</p>
"
AppDynamics,52864552,42719255,0,"2018/10/18, 01:37:57",False,"2018/10/18, 01:37:57",1,10521093,0,"<p>Try adding the line below in the crx-quickstart/conf/sling.properties:</p>

<p><code>org.osgi.framework.bootdelegation=com.singularity.*,com.yourkit.*, ${org.apache.sling.launcher.bootdelegation}</code></p>
"
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474,4682632,1,"<p>You can build multiple baselines within AppDynamics if you'd like to. The thresholds should be auto calculated off deviation from baseline. This makes it so you don't need to configure them manually. If you want to do SLA tracking, Business iQ (analytics) can do this very well. We also are building additional features around SLA use cases we can share. </p>

<p>Feel free to email me or support for a hand. </p>
"
AppDynamics,43535736,43496426,0,"2017/04/21, 09:47:35",False,"2017/04/21, 09:47:35",324,7220665,0,"<p>I think everything you need is described <a href=""https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs"" rel=""nofollow noreferrer"">here</a> 
Basically you need to do the following:</p>

<pre><code>&lt;your_username&gt;@&lt;your_accountname&gt;:&lt;your_password&gt;
</code></pre>

<p>probably in your case it is</p>

<pre><code>&lt;your_username&gt;@customer1:&lt;your_password&gt;
</code></pre>

<p>via CURL it looks like this</p>

<pre><code>curl --user user1@customer1:secret http://demo.appdynamics.com/controller/rest/applications?output=JSON
[
    {
    ""description"": """",
    ""id"": 5,
    ""name"": ""ECommerce_E2E""
  },
    {
    ""description"": """",
    ""id"": 8,
    ""name"": ""ECommerce_E2E-Fulfillment""
  },
]
</code></pre>

<p>You can find more information about the AppDynamics API <a href=""https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs"" rel=""nofollow noreferrer"">here</a></p>
"
AppDynamics,44338324,44269450,0,"2017/06/03, 01:35:15",True,"2017/06/03, 01:35:15",68,1394813,1,"<p>Inside the widget settings make sure you select the ""Stack Areas or Columns"" checkbox. This works in every version of AppD I've used including 4.3.0.2.</p>
"
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474,4682632,1,"<p>SQL 2005 is supported, but this was a bug which was introduced in version 4.3.0. There is currently a diagnostic patch for this issue for supported customers. The fix should be in the next patch level once we isolate the issue. If you'd like support just email help@appdynamics.com and they can assist. Thanks.</p>
"
AppDynamics,44606002,44569260,1,"2017/06/17, 17:54:45",False,"2017/06/17, 17:54:45",474,4682632,1,"<p>In order to capture this data follow these steps:</p>

<ul>
<li>Login to AppDynamics=</li>
<li>Select the application you wish to get these metrics from</li>
<li>Go into Metric Browser (left nav)</li>
<li>Go into overall application performance, in this case number of slow, very slow, stall, errors</li>
<li>For each of these right click on the metric and ""copy rest url""</li>
<li>Use those in your code to pull the data</li>
</ul>

<p>Pointer, you can also adjust the time periods in the metric browser to adjust the time. </p>

<p>Hope this is helpful.</p>
"
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676,7761024,0,"<p>Couple of things:</p>

<p>I think the general URL format for app dynamics applications are (notice the '#'):</p>

<pre><code>url ='http://10.201.51.40:8090/controller/#/rest/applications?output=JSON'
</code></pre>

<p>Also, I think the requests.get method needs an additional parameter for the 'account'. For instance, my auth format looks like:</p>

<pre><code>auth = (_username + '@' + _account, _password)
</code></pre>

<p>I am able to get a right response code back with this config. Let me know if this works for you.</p>
"
AppDynamics,50472163,44776999,0,"2018/05/22, 19:09:38",False,"2018/05/22, 21:41:45",6,8195498,-1,"<p><strong>You could also use native python code for more control:</strong></p>

<p>example:</p>

<pre><code>import os
import sys
import urllib2
import base64

# if you have a proxy else comment out this line
proxy = urllib2.ProxyHandler({'https': 'proxy:port'}) 

opener = urllib2.build_opener(proxy)
urllib2.install_opener(opener)

username = ""YOUR APPD REST API USER NAME""
password = ""YOUR APPD REST API PASSWORD""

#Enter your request
request = urllib2.Request(""https://yourappdendpoint/controller/rest/applications/141/events?time-range-type=BEFORE_NOW&amp;duration-in-mins=5&amp;event-types=ERROR,APPLICATION_ERROR,DIAGNOSTIC_SESSION&amp;severities=ERROR"")

base64string = base64.encodestring('%s:%s' % (username, password)).replace('\n', '')
request.add_header(""Authorization"", ""Basic %s"" % base64string)
response = urllib2.urlopen(request)

html = response.read()
</code></pre>

<h1>This will get you the response and you can parse the XML as needed.</h1>

<p>If you prefer it in JSON simply specify it in the request.</p>
"
AppDynamics,44805282,44805162,1,"2017/06/28, 17:23:30",False,"2017/06/28, 17:23:30",333,2534221,0,"<p>You can get severity information by appending &amp;severities=INFO,WARN,ERROR to your URL.</p>

<p>So your url must be like : <a href=""http://:/controller/rest/applications//business-transactions?output=JSON&amp;severities=INFO,WARN,ERROR"" rel=""nofollow noreferrer"">http://:/controller/rest/applications//business-transactions?output=JSON&amp;severities=INFO,WARN,ERROR</a></p>
"
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11,8349426,1,"<p>Severity is associated with specific events/entities in AppDynamics. Based on your API call I can see that you are trying to retrieve information about Business Transactions (BTs). Severity param is not associated with BTs.</p>

<p>e.g. You can pull Severity for Health rule violations in AppDynamics by making the following API call:
http:///controller/rest/applications//problems/healthrule-violations
Result:</p>

<pre><code>&lt;policy-violations&gt;&lt;policy-violation&gt;
  &lt;id&gt;266&lt;/id&gt;
  &lt;name&gt;CPU utilization is too high&lt;/name&gt;
  &lt;startTimeInMillis&gt;1452630655000&lt;/startTimeInMillis&gt;
  &lt;detectedTimeInMillis&gt;0&lt;/detectedTimeInMillis&gt;
  &lt;endTimeInMillis&gt;1452630715000&lt;/endTimeInMillis&gt;
  &lt;incidentStatus&gt;RESOLVED&lt;/incidentStatus&gt;
  **&lt;severity&gt;WARNING&lt;/severity&gt;**
  &lt;triggeredEntityDefinition&gt;
    &lt;entityType&gt;POLICY&lt;/entityType&gt;
    &lt;entityId&gt;30&lt;/entityId&gt;
    &lt;name&gt;CPU utilization is too high&lt;/name&gt;
  &lt;/triggeredEntityDefinition&gt;
....
</code></pre>

<p>You can find further information about using AppD controller API in the following documentation pages:</p>

<p><a href=""https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs</a></p>

<p><a href=""https://docs.appdynamics.com/display/PRO42/Alert+and+Respond+API"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO42/Alert+and+Respond+API</a></p>
"
AppDynamics,44960915,44939278,0,"2017/07/07, 03:51:06",False,"2017/07/07, 03:51:06",7,1858160,0,"<p>The Issue is resolved. 
The controller-info of Machine Agent need not to have any Application,Node and Tier name. It should include unique host id which should be same as controller-info of JavaAgent. </p>
"
AppDynamics,46592166,46591986,0,"2017/10/05, 21:07:15",False,"2017/10/05, 21:07:15",938,1640095,1,"<p>It looks like you can use environment variables to configure the python appdynamics agent as well.</p>

<pre><code>pip install appdynamics
</code></pre>

<p>Open up your repl</p>

<pre><code>from appdynamics import config
help(config)
</code></pre>

<p>For the usual configuration values (APP_NAME, TIER_NAME, NODE_NAME, etc) you can configure them via the environment variables.  You just need to prefix them with 'APPD_'.  For for APP_NAME it would be:</p>

<pre><code>export APPD_APP_NAME=MY_SUPER_COOL_APP_NAME
</code></pre>
"
AppDynamics,47352287,46591986,0,"2017/11/17, 15:57:08",False,"2017/11/17, 15:57:08",1248,2339348,0,"<p>You can configure the python agent in your code like so: </p>

<pre><code>from appdynamics.agent import api as appd

appd.init(environ={'APPD_APP_NAME': 'appname',
                   'APPD_TIER_NAME': 'yourtier',
                   'APPD_NODE_NAME': 'yournode'})
</code></pre>

<p>Alternatively, you can pass in the location of your appdynamics.cfg file. That is to say, setting environment variables is not enough. </p>

<p>Then you need to manually start the proxy (after you <code>appd.init</code>) by running
<code>pyagent proxy start</code></p>

<p>The agent configuration from your code will be automatically used by the proxy.</p>

<p>For a full list of config keys see the <a href=""https://docs.appdynamics.com/display/PRO43/Python+Agent+Settings"" rel=""nofollow noreferrer"">setting docs</a></p>
"
AppDynamics,53704958,46591986,0,"2018/12/10, 13:41:43",False,"2018/12/10, 13:41:43",13,4981560,0,"<p>I managed to define just environment variables without changing application code. Note that variable name for controller host is APPD_CONTROLLER_HOST. You can also pass command line parameters to the process.</p>
"
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474,4682632,0,"<p>Yes, it does transaction tracking for every intra-component call across languages. Without code changes. When there is a slow transaction detail down to code level will show up in snapshots, this is what you'd use for diagnostics. Depending on your language you can also configure data collectors which do runtime instrumentation of custom data from the code. These show up on every call, and you can turn them into metrics too. Once again no code changes.</p>
"
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474,4682632,3,"<p>Zipkin only does tracing. APM tools like Appdynamics do other monitoring (browser, mobile, database, server, network). Code-level diagnostics with automated overhead controls and limiters. Don't forget log analytics and transaction analytics. It also collects metrics. </p>

<p>There is a lot more to APM than just tracing, which is what Zipkin does. You could do this with a stack of 20 open source tools, but you'd have to deal with disjointed UIs and data models not to mention the work associated with keeping them all working.</p>
"
AppDynamics,47382243,47378215,1,"2017/11/19, 23:34:58",False,"2017/11/19, 23:34:58",127,7526329,-2,"<p>It's actually more the other way around - Crittercism does crash logging, network monitoring, and performance timing whereas AppDynamics is more focused on server monitoring.</p>
"
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474,4682632,0,"<p>Both products essentially do the same thing, if anything AppDynamics has advanced beyond Crittercism (now called Apteligent). They had the lead when the mAPM market started, but now there is not much innovating happening, especially after they sold the company to VMware earlier this year. </p>

<p>Here are the mRUM docs for AppD: <a href=""https://docs.appdynamics.com/display/PRO44/Mobile+Real+User+Monitoring"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO44/Mobile+Real+User+Monitoring</a> some of the more advanced features in AppD you will not find in Appteligent would be things like screen recording, breadcrumb/navigation for every click and interaction. The most valuable feature, of course, is the measurement outside of a straight mobile use case. AppD does measure and ties together the mobile transaction with the backend, server (and network), Docker containers, AWS or other cloud providers. Additionally, it monitors performance and usage of browsers. </p>

<p>Disclosure: I used to be the lead for these products at Gartner, and have been working at AppDynamics for the last 3 years. </p>
"
AppDynamics,50738776,47378215,0,"2018/06/07, 13:19:23",False,"2018/06/07, 13:25:10",2304,1162044,0,"<p>For Apple platforms I recommend you to avoid any third party crash log frameworks and use <a href=""https://help.apple.com/xcode/mac/8.0/#/dev861f46ea8"" rel=""nofollow noreferrer"">Xcode crashes organizer.</a></p>

<ul>
<li>Your app can be smaller and faster</li>
<li>There's no setup</li>
<li>You will be able to see what line of code caused crash</li>
<li>Better privacy for your users as it's opt-in</li>
<li>It's free</li>
</ul>
"
AppDynamics,52641688,49509430,0,"2018/10/04, 10:52:28",False,"2018/10/04, 10:52:28",21,3348861,0,"<p>It seems that you schould be able to define your custom <strong>Asynchronous Transaction Demarcator</strong> as described in:<br/> <a href=""https://docs.appdynamics.com/display/PRO44/Asynchronous+Transaction+Demarcators"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO44/Asynchronous+Transaction+Demarcators</a></p>

<p>which will point to the last method of Runnable that you passes to the Executor. Then according to the documentation all you need is to attach the Demarcator to your Business Transaction and it will collect the asynchronous call.</p>
"
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918,1047788,0,"<p>The first step is to add a username and password in the etc/users.properties file. For most purposes, it is ok to just 
use the default settings provided out of the box. For this, just uncomment the following line:</p>

<pre><code>admin=admin,admin,manager,viewer,Operator, Maintainer, Deployer, Auditor, Administrator, SuperUser
</code></pre>

<p>Then, you must bypass credential checks on BrokeViewMBean by adding it to the whitelist ACL configuration. You can do so by replacing this line:</p>

<pre><code>org.apache.activemq.Broker;getBrokerVersion=bypass
</code></pre>

<p>with this:</p>

<pre><code>org.apache.activemq.Broker=bypass
</code></pre>

<p>In addition to being the correct way, it also enables several different configuration options (eg: port, listen address, etc) by just changing the file org.apache.karaf.management.cfg on broker's etc directory. </p>

<p>Please keep in mind that JMX access is made through a different JMX connector root in this case: it uses <code>karaf-root</code> instead of <code>jmxrmi</code>, which was previously used in the older method. It also uses port 1099 by default, instead of 1616.</p>

<p>Therefore, the uri should be </p>

<pre><code>service:jmx:rmi:///jndi/rmi://&lt;host&gt;:&lt;port&gt;/karaf-root
</code></pre>
"
AppDynamics,52869901,52825100,0,"2018/10/18, 11:25:36",False,"2018/10/18, 11:25:36",294930,208809,3,"<p>From my current knowledge: no, AppDynamics doesn't support OpenTracing yet. </p>

<p>Usually, APM vendors have their own OpenTracing tracers build off the official specification and then get them listed at <a href=""http://opentracing.io"" rel=""nofollow noreferrer"">http://opentracing.io</a>. But as of this writing there is no mention of any AppDynamics Tracers at <a href=""https://opentracing.io/docs/supported-tracers/"" rel=""nofollow noreferrer"">https://opentracing.io/docs/supported-tracers/</a> nor <a href=""https://github.com/opentracing-contrib/meta"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/meta</a>.</p>

<p><em>Full disclosure: I work for Instana, a competitor that does support OpenTracing.</em></p>
"
AppDynamics,52989775,52825100,0,"2018/10/25, 15:53:20",False,"2018/10/25, 15:53:20",474,4682632,0,"<p>No AppD doesn't support OpenTracing at this time. The question is why do you want this functionality when you can already extract custom data from transactions dynamically with most AppDynamics agents? Do you really want to hardcode your APM tool into your software application?</p>

<p>AppDynamics is building a unique way to support OpenTracing, which is currently in testing, but the approach is not by hardcoding libraries into the code. If you'd like to learn more please reach out to support, or you can contact me directly as I work for AppDynamics.</p>

<p>Thanks. </p>
"
AppDynamics,58802450,54003615,0,"2019/11/11, 15:36:01",False,"2019/11/11, 15:36:01",26,12355048,0,"<p>You need to add:</p>

<pre><code>wrapper.java.additional.6=-javaagent:C://agent//javaagent.jar
</code></pre>

<p>into  <code>wrapper.conf</code> file.</p>

<p><a href=""https://docs.appdynamics.com/display/PRO45/Tanuki+Service+Wrapper+Settings"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO45/Tanuki+Service+Wrapper+Settings</a></p>
"
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26,12355048,0,"<p>You can export the dashboard and you will get the json version of your dashboard.(The export button is located top of the your dashboard page)</p>

<p>This json file can be easily editable with any editor and you can replace the Application and/or other properties which contain on of your dashboards.</p>

<p><a href=""https://docs.appdynamics.com/display/PRO45/Import+and+Export+Custom+Dashboards+and+Templates+Using+the+UI"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO45/Import+and+Export+Custom+Dashboards+and+Templates+Using+the+UI</a></p>
"
AppDynamics,55811490,55332537,1,"2019/04/23, 15:42:30",False,"2019/04/23, 15:42:30",53,2309810,0,"<p>Due to its proprietary nature i don't think the internals are available for anyone to view or discuss.</p>

<p>Below link might give an idea of how the product works at a high level.</p>

<p><a href=""https://www.appdynamics.com/product/how-it-works/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/product/how-it-works/</a></p>
"
AppDynamics,55700067,55697936,0,"2019/04/16, 06:18:36",False,"2019/04/16, 06:18:36",11138,9132707,0,"<p>You are on the right track, but you are not saying that you are having errors or showing the. Yet, based on your experience to date, I am sure you already know that PowerShell provides cmdlets for working with XML. </p>

<p>See them using --- </p>

<pre><code>Get-Command -Name '*xml*' | ft -a  
</code></pre>

<p>or get other XML modules from the MS PowerShellGallery.com using --- </p>

<pre><code>Find-Module -Name '*xml*' | ft -a 
</code></pre>

<p>--- and install the one(s) that fit your needed goals.</p>

<p>And of course there are lot's of examples and videos on the topic. Searching for 'PowerShell working with XML', gives you plenty of hits.</p>

<p><a href=""https://www.red-gate.com/simple-talk/sysadmin/powershell/powershell-data-basics-xml/"" rel=""nofollow noreferrer"">PowerShell Data Basics: XML</a></p>

<p>For the most part, what you will find is very similar to what you already have posted. Yet, you say you want add nodes / elements, but that is not in your post, so, something like the below should help.</p>

<pre><code>$xml = [xml](Get-Content C:\file.xml)
$elem = $xml.Configuration.ConfigSections.AppendChild($xml.CreateNode([System.Xml.XmlNodeType]::Element,'section',$null))
$elem.SetAttribute('name','something')
$xml.Save('C:\file.xml')
</code></pre>

<p>Or even just using the <a href=""https://docs.microsoft.com/en-us/powershell/module/webadministration/"" rel=""nofollow noreferrer"">WebAdministration module</a> on the IIS server directly.</p>
"
AppDynamics,55709010,55697936,0,"2019/04/16, 16:11:04",False,"2019/04/16, 16:11:04",101,2586735,0,"<p>You should check out the AppDynamics agent installation PowerShell Extension: <a href=""https://www.appdynamics.com/community/exchange/extension/dotnet-agent-installation-with-remote-management-powershell-extension/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/community/exchange/extension/dotnet-agent-installation-with-remote-management-powershell-extension/</a></p>

<p>It should be able to handle what you are trying to do without having to generate the xml manually, check the pdf for advanced options and read about the Add-IISApplicationMonitoring cmdlet</p>

<p>What it can’t do is the newer stuff like “multiple business application” and “custom node name” configuration. (Which you can’t do in the install wizard either)</p>
"
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53,2309810,0,"<p>AppDynamics itself powerful enough to provide all sort of information, However if you still want actuator data to be captured and displayed then you may need to use AppD extension. Please refer below official link to AppDyanmics Exchange.</p>

<p><a href=""https://www.appdynamics.com/community/exchange/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/community/exchange/</a></p>

<p>if the relevant is not available you may need write your own.</p>
"
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21,5846608,0,"<p>I found this solution <a href=""https://github.com/Appdynamics/flowmap-builder"" rel=""nofollow noreferrer"">https://github.com/Appdynamics/flowmap-builder</a> </p>

<p>Seems to be working so far. Doesn't rely (directly) on APIs but it works!</p>
"
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41,7611518,0,"<p>For anyone who is looking for an answer here. This is resolved by unchecking the Extended  tracking option for kafka in AppDynamics Console. The option will be there in Automatic Backend Discovery. Hope this helps.</p>
"
AppDynamics,58800924,57958059,0,"2019/11/11, 13:57:56",False,"2019/11/11, 13:57:56",26,12355048,0,"<ol>
<li><p>Disable default Kafka configuration on ""Backend Detection"" page</p></li>
<li><p>Define a new ""Custom Exit Point"" with the last class and method which you see on the call graphs before Kafka exit call</p></li>
<li><p>Don't forget to click on ""Is High Volume"" checkbox.</p></li>
</ol>
"
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26,12355048,1,"<p>If we take the example of an ECommerce Application:</p>

<p><strong>Business Transactions</strong> are Checkout, Landing Page, Add to Cart etc. which are known by every end user of the application. These business transactions cover all the method executions, database calls, web service calls etc.</p>

<p><strong>Service End Points</strong> are the sub calls(method call or web service call) execute inside of the Business Transactions. Such as ""Check Inventory"" service which is executed in Checkout and Add to Cart transactions as well.</p>

<p><strong>Information Points</strong> are the key business or technical metric counts, such as Checkout amount, Add to Cart item count.</p>

<p>Service End Points and Information Points only give you performance metrics but Business Transactions also give you full code visibility with ""call graphs""</p>

<p>Also, there are some limitations like max 200 Business Transaction on the default but you can change these rules.</p>

<p>While configuring the BTs &amp; SEs, you must focus on the needs of AppDynamics users. If you configure AppDynamics for mostly business teams, I can use BTs like I described above. But If you target the Dev and Ops teams, you can configure your BTs based on method or service calls.</p>

<p>There is no only single approach on BT &amp; SE configuration. You must shape that with the needs of your AppDynamics users.</p>
"
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26,12355048,0,"<p>Configuration->Instrumentation->Transaction Detection->Add</p>

<p>On the ""Split Transactions Using Request Data"" section you must choose ""<em>Specific URI Segments</em>""
Segment Numbers: 1,2,4</p>

<p>In your case transaction name will be ""/data/scenario/job""</p>

<p>Sample Configuration:</p>

<p><a href=""https://i.stack.imgur.com/tYguT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tYguT.png"" alt=""enter image description here""></a></p>
"
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11,2131858,0,"<p>On analyzing the heap dump taken during system idle state, we only see various WebAppClassLoaders holding instances of different library classes.</p>

<p>This pattern is also explained in official blogs of APM experts like <a href=""https://plumbr.io/blog/memory-leaks/memory-leaks-fallacies-and-misconceptions"" rel=""nofollow noreferrer"">Plumbr</a> and <a href=""https://www.datadoghq.com/blog/tomcat-architecture-and-performance/#jvm-memory-usage"" rel=""nofollow noreferrer"">Datadog</a> as a sign of healthy JVM where regular GC activity is occurring and they explain that it means none of the objects will stay in memory forever.</p>

<p><br>From Plumbr blog:</p>

<blockquote>
  <p>Seeing the following pattern is a confirmation that the JVM at question is definitely not leaking memory. 
  The reason for the double-sawtooth pattern is that the JVM needs to allocate memory on the heap as new objects are created as a part of the normal program execution. Most of these objects are short-lived and quickly become garbage. These short-lived objects are collected by a collector called “Minor GC” and represent the small drops on the sawteeth.</p>
</blockquote>
"
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705,1501456,0,"<p>The reason you don't see the browser name in HTTP protocol is because there is no browser. The protocol is a transport level protocol which means it simulates the traffic of the browser without running the actual browser. This allows the protocol to simulate many more virtual users than a client level protocol such as TruClient. </p>

<p>EDIT: There is no dedicated API and you must use the user agent. Please refer to this article for more details: <a href=""https://www.appdynamics.com/blog/engineering/how-to-use-appdynamics-with-loadrunner-for-load-testing/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/blog/engineering/how-to-use-appdynamics-with-loadrunner-for-load-testing/</a></p>
"
AppDynamics,62775806,61730657,0,"2020/07/07, 15:55:48",False,"2020/07/07, 15:55:48",1,11862547,0,"<p>I'd try setting &quot;Use data from last&quot; to <strong>60 minutes</strong>. In Criteria-tab use &quot;Single Metric&quot; with <strong>Sum</strong> of &quot;Calls per Minute&quot; and define your threshold.</p>
"
AppDynamics,65580996,63517278,0,"2021/01/05, 16:39:58",False,"2021/01/05, 16:39:58",3170,1237595,0,"<p>The Drop-off rate is the percentage of user sessions which reach the specific page and then end their session (they do not reach any further pages as part of the session and therefore &quot;drop-off&quot; the map).</p>
<p>This can be seen in the example (image) in the <a href=""https://docs.appdynamics.com/display/PRO45/Experience+Journey+Map"" rel=""nofollow noreferrer"">documentation</a></p>
"
AppDynamics,66421794,65134789,0,"2021/03/01, 14:02:52",False,"2021/03/01, 14:02:52",3170,1237595,0,"<p>Yes - you can do this using ADQL / Analytics searches (assuming you have this licensed).</p>
<p>Without specifics I can only give you a general guide:</p>
<ul>
<li>Ensure your API / Applications are instrumented - so response timings are captured</li>
<li>Enable Transaction Analytics (for the relevant Business Transactions / Application)</li>
<li>In Analytics use ADQL (<a href=""https://docs.appdynamics.com/display/PRO21/ADQL+Reference"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/ADQL+Reference</a>) to actually do the Apdex calculation - and create an analytics metric from this per Business Transaction / Application (<a href=""https://docs.appdynamics.com/display/PRO21/Create+Analytics+Metrics+From+Scheduled+Queries"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Create+Analytics+Metrics+From+Scheduled+Queries</a>)</li>
<li>The metric can then be used in any Dashboards / Reports / Health Rule etc or exported using the Analytics API (<a href=""https://docs.appdynamics.com/display/PRO21/Analytics+Events+API"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Analytics+Events+API</a>)</li>
</ul>
"
AppDynamics,66623443,65710328,0,"2021/03/14, 12:32:34",False,"2021/03/20, 18:19:18",3170,1237595,0,"<p>Generally the workflow for sending reports from AppDynamics is as follows:</p>
<ol>
<li>Create a Custom Dashboard - <a href=""https://docs.appdynamics.com/display/PRO21/Custom+Dashboards"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Custom+Dashboards</a></li>
<li>Add Widgets to your Dashboard containing the tables, charts etc that you want to see in your report - <a href=""https://docs.appdynamics.com/display/PRO21/Widgets"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Widgets</a></li>
<li>Now you can send a report (one-off or scheduled) by configuring a report based off the Dashboard you have created, including setting the time range of the report - <a href=""https://docs.appdynamics.com/display/PRO21/Reports"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Reports</a></li>
</ol>
<p>Note: There is a sample custom dashboard .json available here to get you started: <a href=""https://community.appdynamics.com/t5/Knowledge-Base/Sample-Custom-Dashboard-for-Business-Transaction-Report/ta-p/21264"" rel=""nofollow noreferrer"">https://community.appdynamics.com/t5/Knowledge-Base/Sample-Custom-Dashboard-for-Business-Transaction-Report/ta-p/21264</a></p>
<p>Note: There are already many &quot;Standard Reports&quot; which could make things easier depending on use case.</p>
<p>Note: If you instead want to export data and then analyse with your own tooling, then see the docs on Public API's available here: <a href=""https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs</a></p>
"
AppDynamics,66421156,66096667,0,"2021/03/01, 13:19:27",False,"2021/03/01, 13:19:27",3170,1237595,0,"<p>All currently available AppDynamics APIs are documented here: <a href=""https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs</a></p>
<p>The main page includes a summary of what is available.</p>
"
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474,4682632,0,"<p>Love to help you out here, but there are no details about the error, please email help@appdynamics.com for assistance. I assume this is C#, but wouldn't know based on this. AppDynamics supports many languages and technologies. </p>
"
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117,2384622,1,"<p>For whom it might be of interest I have found a workaround and more details about this issue. This occurs only in the following scenario:</p>

<ol>
<li>AppDynamics agent is installed and running</li>
<li>ServiceStack API is compiled using the MSBuild from c:\Windows\Microsoft.NET\Framework64\v4.0.30319\MsBuild.exe</li>
</ol>

<p>If you use the MsBuild 14 that is installed along Microsoft Visual Studio 2015 RC then this issue does not occur anymore. From my first findings there is an issue in ServiceStack's way of caching the endpoints and wrapping the execute method using Linq but I don't understand why this happens only when AppDynamics agent is installed.</p>

<p>@mythz if you want to dig deeper into this issue I'm available to help but with the above solution everything is ok.</p>

<p>Hope this helps</p>
"
AppDynamics,31367542,31358408,0,"2015/07/12, 15:01:07",False,"2015/07/12, 15:01:07",474,4682632,0,"<p>Business transactions should accomplish this. If you want to report on each web service you can build a report or custom dashboard. If you need more assistance just email help@appdynamics.com</p>
"
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170,1242093,0,"<p>AFAIK the critical point for AppDynamics (or profilers like that) it is essential to find an entry point. Usually the prefered way is to have an Servlet ""Endpoint"" that starts a threat and can be followed. 
For the scenario you are describing this wouldn't work as it's missing the ""trigger"" to start the following. Most likely you'll need to build your own app-dynamics monitoring extension for it. </p>
"
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26,2378795,0,"<p>By default much of the Apache stuff is excluded.  Try adding Call Graph Settings (Configure >> Instrumentation >> Call Graph Settings), to include specific transports, like org.apache.camel.component.file.* in the Specific sub-packages / classes from the Excluded Packages to be included in call graphs section.  Do not include org.apache.camel.* as it will instrument all the camel code which is very expensive.  You may want to do it at first to detect what you want to watch, but make sure to change it back.  </p>
"
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26,2378795,0,"<p>Edit AppServerAgent\conf\app-agent-config.xml:</p>

<pre><code>--under--&gt;
&lt;app-agent-configuration&gt;
    &lt;agent-services&gt;
        &lt;agent-service name=""TransactionMonitoringService"" enable=""true""&gt;
            &lt;configuration-properties&gt;
--add--&gt;        &lt;property name=""enable-async-correlation-for"" value=""camel""/&gt;
</code></pre>

<p>From the Controller web site:</p>

<p>Configure >> Instrumentation >> Call Graph Settings
Add Always Shown Package/Class: org.apache.camel.*</p>

<p>Servers >> App Servers >> {tiername} >> {nodename} >> Agents
App Server Agent
Configure
Use Custom Configuration
find-entry-points: true</p>
"
AppDynamics,29370213,29346979,0,"2015/03/31, 16:51:25",True,"2015/03/31, 16:51:25",602,3453371,0,"<p>Can I ask if your trial is beyond 15 days? According to their site the trial only includes monitoring for one agent beyond that time - that would explain why only your first agent delivers data. Other tools in that space, e.g: Dynatrace free trial - gives you a bit more time if that's what you need and also allow you to extend the free trial if you need more time</p>
"
AppDynamics,30553280,29346979,0,"2015/05/31, 05:34:32",False,"2015/05/31, 05:34:32",474,4682632,0,"<p>You have gone beyond the 15 day pro trial and you are using the free product. If you want to buy the product you can get an extension on the trial, if you do not want to buy the product it will not trace across JVMs. </p>
"
AppDynamics,16543274,14993309,2,"2013/05/14, 15:33:59",True,"2013/05/14, 15:33:59",1970,131903,15,"<p>Not sure if you already have resolved this, but: SNI is SQL Server Network Interface, and the mentioned method exists in most ADO.NET full call stacks that wait for data from SQL Server. This is regardless of whether the higher-level implementation is EF, raw ADO.NET or whatever.</p>

<p>I'm not sure which metric or signal AppDynamics uses to capture the completion of a stored procedure execution, but you could be seeing this kind of behavior if your stored procedure completes relatively fast, but transmitting the query result from the server to your client takes a while. </p>

<p>Without knowing more about your infrastructure, it is very hard to help further. If the problem still persists, I would recommend running the same query in SQL Server Management studio with SET STATISTICS TIME ON and ""Include Client Statistics"" switched to on. Perhaps those numbers would give you an idea on whether data transfer is actually the problem.</p>
"
AppDynamics,42025178,14993309,0,"2017/02/03, 15:12:58",False,"2017/02/03, 15:12:58",2131,585819,1,"<p>In my case it was indeed, as Jouni mentions, very slow transmitting of the query results. I use Automapper to prepare data for sending to client. So, it's unclear what exact property caused the load but to be sure I've cut all compound ones I don't need to show on client side. (I originally needed a collection to show in grid on client side.) The execution became very fast.</p>
"
AppDynamics,49904327,14993309,0,"2018/04/18, 19:08:53",False,"2018/04/18, 19:08:53",1707,2308106,0,"<p>I've came across similar issue - it turns out that SqlDataReader.Dispose can get stuck for very lengthy time if you break early from large select.</p>

<p>see: <a href=""https://github.com/dotnet/corefx/issues/29181"" rel=""nofollow noreferrer"">https://github.com/dotnet/corefx/issues/29181</a></p>
"
AppDynamics,14983064,14330329,3,"2013/02/20, 17:05:14",False,"2016/08/20, 12:41:24",424,2091508,7,"<p>please see my reply in 
<a href=""https://stackoverflow.com/questions/14051323/g1gc-long-pause-with-initial-mark/14982456#14982456"">G1GC long pause with initial-mark</a></p>

<p><em>your every setting should has a solid reason to be here...and unfortunately some of them don't have e.g.</em> <code>-XX:+UseBiasedLocking (used for combination of tenured and young generation GCs, but G1GC is capable to handle both) -XX:+DisableExplicitGC (its unpredictable, in my experience its never restrict explicit gc calls)</code></p>

<p>please use/tweak accordingly below mentioned settings to get optimum results, I'm giving you baseline to move forward, hope so these settings will work for you:
<a href=""https://stackoverflow.com/questions/2254041/java-g1-garbage-collection-in-production/14983318#14983318"">Java G1 garbage collection in production</a></p>
"
AppDynamics,16782833,14330329,1,"2013/05/28, 05:08:52",True,"2013/05/28, 05:08:52",1275,1335717,5,"<p>We recorded this bug on 1.7._u06 and upgraded to  1.7.0_21-b11  just a couple of days ago and we haven't seen any Full GC's since upgrade, so it seems like this bug was fixed in JVM. The code cache memory profiles look much nicer now too in the profiler. In the past, this problem used to be a daily one, one to more times per day.</p>

<p>If the situation changes, I will report back. Until then, I consider this problem solved with the upgrade.</p>
"
AppDynamics,28458957,28436858,1,"2015/02/11, 18:07:23",False,"2015/02/11, 18:13:31",49324,168493,0,"<p>Your query profile shows that the ""Query end"" time is very large. This may be caused by a very (too) large <a href=""http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_query_cache_size"" rel=""nofollow"">query cache</a>. Every time you perform an update statement (INSERT, DELETE, UPDATE), the query cache must be updated (every query that reads from the updated tables is invalidated).</p>
"
AppDynamics,28481902,28436858,1,"2015/02/12, 17:54:03",True,"2018/08/13, 11:49:47",525,3132445,20,"<p>I got in touch with RDS engineers from amazon and they gave me the solution.
Such a high latency was due to a very low performing storage type. Indeed, I was using the default 5GB SSD (which they call GP2) which gives 3 IOPS per GB of storage, resulting in 15 IOPS when my application required about 50 IOPS or even more. </p>

<p>Therefore, they suggested me to change the storage type to <code>Magnetic</code> which provides 100 IOPS as baseline. Moreover, I've also been able to decrease the instance type because the bottleneck was only the disk.</p>

<p>The migration took about 3h due to the very low performance of the source disk (GP2).</p>

<p>Hope it may help someone out there!</p>
"
AppDynamics,49953075,35971200,0,"2018/04/21, 09:34:37",True,"2018/04/21, 09:34:37",639,812093,0,"<p>We never were able to properly solve the issue but at some point it vanished. If I'm not wrong one the client change the appdynamics configuration / removed it which seemed to have solved the issue. </p>
"
AppDynamics,28937115,24261110,0,"2015/03/09, 09:38:07",False,"2015/03/09, 09:38:07",10216,854207,0,"<p>did you take a look to this google group ticket issue?</p>

<p><a href=""https://groups.google.com/forum/#!topic/hazelcast/ivk6hzk2YwA"" rel=""nofollow"">https://groups.google.com/forum/#!topic/hazelcast/ivk6hzk2YwA</a></p>

<p>Here in particular looks the reason of the issue.</p>

<p><a href=""https://github.com/hazelcast/hazelcast/issues/553"" rel=""nofollow"">https://github.com/hazelcast/hazelcast/issues/553</a></p>
"
AppDynamics,13532221,13524580,1,"2012/11/23, 17:48:01",False,"2012/11/23, 17:48:01",17,1847969,0,"<p>I suggest to look into Gartners magic quadrant and get dynaTrace since it has negligible overhead , less than 1% in production under load.</p>
"
AppDynamics,13570380,13524580,1,"2012/11/26, 19:52:11",True,"2012/11/26, 19:52:11",74,1854219,5,"<p>Full Disclosure: I currently work for AppDynamics.</p>

<p>AppDynamics was designed from the ground up for high volume production environments but works equally well in both prod and non-prod. It's currently running in production in some of the worlds largest mission critical application environments at Netflix, Exact Target, Edmunds, and many others. Here are a few quotes from existing customers…</p>

<p>""It's like a profiler that you can run in production"" -- Leonid Igolnik, Taleo</p>

<p>""We found that the overhead was negligible"" -- Jacob Marcus, Care.com</p>

<p>""We wanted a monitoring solution that wouldn't impact our production runway"" -- John Martin, Edmunds</p>

<p>AppDynamics overhead is extremely low but I suggest you test it and see for yourself. You can download and use it for free from the AppDynamics website. Good luck in your search for the right APM tool.</p>
"
AppDynamics,13806782,13524580,0,"2012/12/10, 20:18:05",False,"2012/12/10, 20:18:05",207,153728,1,"<p>Yes it will if the application is sensitive to extra GC cycles caused by call stack sampling. The impact will depend on the number of threads and typical call stack depth. This is not specific to AppDynamics, other call stack sampling solutions such as NewRelic and VisualVM Sampler will have a similar impact.</p>

<p><a href=""http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_Good_APM_vs_AppDynamics_Bad_APM.pdf"" rel=""nofollow"">http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_Good_APM_vs_AppDynamics_Bad_APM.pdf</a></p>

<p><a href=""http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_vs_AppDynamics.pdf"" rel=""nofollow"">http://www.jinspired.com/wp-content/uploads/2011/11/JXInsightOpenCore_vs_AppDynamics.pdf</a></p>
"
AppDynamics,13820531,13524580,0,"2012/12/11, 14:32:58",False,"2012/12/11, 14:32:58",207,153728,4,"<p>There are a number of assumptions made by a vendor but the following are most common: </p>

<ol>
<li>We assume your application request processing time is sufficiently high to dwarf our significant overhead. </li>
</ol>

<p>TRANSLATION: We assume you have a slow performing database backend. </p>

<ol>
<li>We assume that instrumentation is applied to a very limited section of the code source to lessen the impact of our significant overhead. </li>
</ol>

<p>TRANSLATION: We assume that you already know your performance hotspots. </p>

<ol>
<li>We assume that there is large amount of under utilized processing capacity to offload our significant overhead. </li>
</ol>

<p>TRANSLATION: We assume that you will not notice tricks used to hide our overhead. </p>

<ol>
<li>We assume that it is impossible to realistically and reliably measure our significant overhead. </li>
</ol>

<p>TRANSLATION: We assume that you know little about performance engineering. </p>

<p>And my favorite (5) is the restriction within a vendors software license on the publication of benchmark results. </p>

<p>TRANSLATION: We assume that you blindly accept our claims – unquestionably.</p>
"
AppDynamics,19518863,13524580,0,"2013/10/22, 16:10:28",False,"2013/10/22, 16:10:28",3374,2508411,1,"<p>Appdynamics will not slow down your system significant, I was on a usermeeting and they said that they always try to be under 2% cpu usage, thats nothing compared to what you get from them.
They are working with samples per time, so if you have 10 requests per second or 100, they will still take the same amout of your cpu / bandwith / whatever. </p>
"
AppDynamics,32718584,32717415,1,"2015/09/22, 16:44:26",False,"2015/09/22, 16:44:26",nan,nan,0,"<p>Have you tried to Minify your code? Minifying unneccesary characters from your code without removing any functionality. This could help speed up the download times. </p>

<p>Take a look at the following link: <a href=""http://www.htmlgoodies.com/beyond/reference/7-tips-to-make-your-websites-load-faster.html"" rel=""nofollow"">http://www.htmlgoodies.com/beyond/reference/7-tips-to-make-your-websites-load-faster.html</a></p>
"
AppDynamics,32719029,32717415,0,"2015/09/22, 17:02:56",False,"2015/09/22, 17:18:53",55753,761202,5,"<blockquote>
<p>Ill take a look at serving assets from webroot</p>
</blockquote>
<h2>Always put public assets in the webroot.</h2>
<p><a href=""http://book.cakephp.org/2.0/en/views/themes.html#increasing-performance-of-plugin-and-theme-assets"" rel=""nofollow noreferrer"">From the book</a> (emphasis added):</p>
<blockquote>
<p>It’s a well known fact that <strong>serving assets through PHP is guaranteed to be slower than serving those assets without invoking PHP</strong>. And while the core team has taken steps to make plugin and theme asset serving as fast as possible, there may be situations where more performance is required. In these situations it’s recommended that you either symlink or copy out plugin/theme assets to directories in app/webroot with paths matching those used by CakePHP.</p>
<ul>
<li>app/Plugin/DebugKit/webroot/js/my_file.js becomes app/webroot/debug_kit/js/my_file.js</li>
<li>app/View/Themed/Navy/webroot/css/navy.css becomes app/webroot/theme/Navy/css/navy.css</li>
</ul>
</blockquote>
<p>Depending on many factors, &quot;slower&quot; can be anywhere between barely-noticeable to barely-usable.</p>
<p>This advice is not version specific, and pretty much always applies. To make assets load faster, let the webserver take care of them for you.</p>
"
AppDynamics,21035147,20975043,0,"2014/01/10, 04:24:32",False,"2016/07/23, 21:57:52",1025,1777388,0,"<p>Yes, it is possible for the behaviour of GCs to change over time due to JIT optimisation. One example is 'Escape Analysis' which has been enabled by default since Java 6u23. This type of optimisation can prevent some objects from being created in the heap and therefore not require garbage collection at all.</p>

<p>For more information see <a href=""http://docs.oracle.com/javase/7/docs/technotes/guides/vm/performance-enhancements-7.html#escapeAnalysis"" rel=""nofollow"">Java 7's HotSpot Performance Enhancements</a>.</p>
"
AppDynamics,16108561,16090702,0,"2013/04/19, 18:49:57",False,"2013/04/19, 18:49:57",625,2298085,1,"<p>It is really difficult to find the exact cause of your problem without more information.</p>

<p>But I can try to answer to your question :<br>
<em>Can the OS block the garbage collection ?</em> <br>
It is very unlikely than your OS blocks the thread garbage collector and let the other threads run. You should not investigate that way.</p>

<p><em>Can the OS block the JVM ?</em><br>
Yes it perflecty can and it do it a lot, but so fast than you think that the processes are all running at the same time.<br> jvm is a process like the other and his under the control of the OS. You have to check the cpu used by the application when it hangs (with monitoring on the server not in the jvm). If it is very low then I see 2 causes (but there are more) :<br></p>

<ul>
<li>Your server doesn't have enough RAM and is swapping (RAM &lt;-> disk), process becomes extremely slow. In this case cpu will be high on the server but low for the jvm<br></li>
<li>Another process or server grabs the resources and your application or server receive nothing. Check the priority on CentOs.</li>
</ul>
"
AppDynamics,16108770,16090702,0,"2013/04/19, 18:59:28",False,"2013/04/19, 18:59:28",243,2096203,0,"<p>In theory, YES, it can. But it practice, it never should.</p>

<p>In most Java virtual machines, application threads are not the only threads that are running. Apart from the application threads, there are compilation threads, finalizer threads, garbage collection threads, and some more. Scheduling decisions for allocating CPU cores to these threads and other threads from other programs running on the machine are based on many parameters (thread priorities, their last execution time, etc), which try be fair to all threads. So, in practice no thread in the system should be waiting for CPU allocation for an unreasonably long time and the operating system should not block any thread for an unlimited amount of time.</p>

<p>There is minimal activity that the garbage collection threads (and other VM threads) need to do. They need to check periodically to see if a garbage collection is needed. Even if the application threads are all suspended, there could be other VM threads, such as the JIT compiler thread or the finalizer thread, that do work and ,hence, allocate objects and trigger garbage collection. This is particularly true for meta-circular JVM that implement VM threads in Java and not in a C/C++;</p>

<p>Moreover, most modern JVM use a generational garbage collector (A garbage collector that partitions the heap into separate spaces and puts objects with different ages in different parts of the heap) This means as objects get older and older, they need to be moved to other older spaces. Hence, even if there is no need to collect objects, a generational garbage collector may move objects from one space to another.</p>

<p>Of course the details of each garbage collector in different from JVM to JVM. To put more salt on the injury, some JVMs support more than one type of garbage collector. But seeing a minimal garbage collection activity in an idle application is no surprise. </p>
"
AppDynamics,16108782,16090702,3,"2013/04/19, 19:00:01",True,"2013/04/19, 19:32:49",59821,12943,1,"<p>Does your OS have swapping enabled.</p>

<p>I've noticed HUGE problems with Java once it fills up all the ram on an OS with swapping enabled--it will actually devistate windows systems, effictevly locking them up and causing a reboot.</p>

<p>My theory is this:</p>

<ul>
<li>The OS ram gets near full.</li>
<li>The OS requests memory back from Java.</li>
<li>This Triggers Java into a full GC to attempt to release memory.</li>
<li>The full GC touches nearly every piece of the VMs memory, even items that have been swapped out.</li>
<li>The system tries to swap data back into memory for the VM (on a system that is already out of ram)</li>
<li>This keeps snowballing.</li>
</ul>

<p>At first it doesn't effect the system much, but if you try to launch an app that wants a bunch of memory it can take a really long time, and your system just keeps degrading.</p>

<p>Multiple large VMs can make this worse, I run 3 or 4 huge ones and my system now starts to sieze when I get over 60-70% RAM usage.</p>

<p>This is conjecture but it describes the behavior I've seen after days of testing.</p>

<p>The effect is that all the swapping seems to ""Prevent"" gc.  More accurately the OS is spending most of the GC time swapping which makes it look like it's hanging doing nothing during GC.</p>

<p>A fix--set -Xmx to a lower value, drop it until you allow enough room to avoid swapping.  This has always fixed my problem, if it doesn't fix yours then I'm wrong about the cause of your problem :)</p>
"
AppDynamics,18276354,18276208,5,"2013/08/16, 18:00:12",True,"2013/08/16, 18:39:33",177788,1103872,3,"<p>One major GC per minute doesn't at all seem troublesome. Usually it takes about half a second, so that's 1/120th of your overall CPU usage. It is also quite natural that heavy application load results in more memory allocation. Apparently you are allocating some objects that live on for a while (could be caching).</p>
<p>My conclusion: the demonstrated GC behavior is not a proof that there is something wrong with your application's memory allocation.</p>
<h2>UPDATE</h2>
<p>I have looked more carefully at your diagrams (unfortunately they are quite difficult to read). You don't have one GC per min; you have 60 <em>seconds</em> of major GC per minute, which would mean it's happening all the time. That <em>does</em> look like major trouble; in fact in those conditions you usually get an OOME due to &quot;GC time percentage threshold crossed&quot;. Note that the CMS collector which you are using is actually slower than the default one; its advantage is only that it doesn't &quot;stop the world&quot; as much. It may not be the best choice for you. But you do look to either have a memory leak, or a general issue in program design.</p>
"
AppDynamics,18276716,18276208,0,"2013/08/16, 18:16:46",False,"2013/08/16, 18:16:46",470,2623382,1,"<p>Are you always waiting for GC to take care of removing unused references? Is there some places in your application that you know a reference to a heavy weight object won't be used anymore from that point, but it is not manually nulled? Perhaps setting such heavy weight objects to null manually at the right places could prevent them growing till they hit the end of the heap....</p>
"
AppDynamics,18277366,18276208,0,"2013/08/16, 18:49:24",False,"2013/08/16, 18:56:19",4061,2031799,2,"<p>When JVM reaches heap maximum size, GC is called more frequently to prevent OOM exception. Normal program execution should not happen at such circumstances. When a new object is allocated and JVM cant get enough of free space, GC is called. This might postpone object allocation process and thus slowdown overall performance. Garbage collection happens not concurrently in this case and you do not benefit from CMS collector. Try to play with <code>CMSInitiatingOccupancyFraction</code>, its default value is about 90%. Setting this parameter to lower values, will force garbage collection before application reaches heap maximum. Thus GC will work in parallel with application not clashing with it. Have a look at article <a href=""http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#cms.starting_a_cycle"" rel=""nofollow"">Starting a Concurrent Collection Cycle</a>.</p>
"
AppDynamics,53270016,53269696,2,"2018/11/12, 22:59:13",True,"2018/11/12, 23:07:57",64495,413337,2,"<p>Looking at the <a href=""https://github.com/FasterXML/jackson-core/blob/master/src/main/java/com/fasterxml/jackson/core/json/UTF8JsonGenerator.java#L2104"" rel=""nofollow noreferrer"">source code</a> of <code>UTF8JsonGenerator._flushBuffer()</code>, there is no indication of <code>LockSupport.parkNanos()</code>. So it has probably been inlined by the JIT compiler from <code>OutputStream.write()</code>.</p>

<p>My guess is it's the place where – for your application – Tomcat typically waits until the client has accepted all the output (expect for the last piece that fits into the typical connection buffer size) before it can close the connection.</p>

<p>We have had bad experience with slow clients in the past. Until they have retrieved all the output, they block a thread in Tomcat. And blocking a few dozens threads in Tomcat seriously reduces the throughput of a busy web app.</p>

<p>Increasing the number of threads isn't the best option as the blocked threads also occupy a considerable amount of memory. So what you want is that Tomcat can handle a request as quickly as possible and then move on to the next request.</p>

<p>We have solved the problem by configuring our reverse proxy, which we always had in front of Tomcat, to immediately consume all output from Tomcat and deliver it to the client at the client's speed. The reverse proxy is very efficient at handling slow clients.</p>

<p>In our case, we have used <em>nginx</em>. We also looked at <em>Apache httpd</em>. But at the time, it wasn't capable of doing it.</p>

<p><em>Additional Note</em></p>

<p>Clients that unexpectedly disconnect also look like slow clients to the server as it takes some time until it has been fully established that the connection is broken.</p>
"
AppDynamics,7413670,7409112,7,"2011/09/14, 11:52:55",False,"2011/09/15, 05:34:03",706,795321,1,"<p>You could use Javamelody, but you have to:</p>

<ol>
<li><p>Generate war file from your play framework web</p></li>
<li><p>Edit web.xml in war file (http://code.google.com/p/javamelody/wiki/UserGuide?tm=6)</p></li>
<li>Deploy it in container server such as tomcat</li>
</ol>
"
AppDynamics,31063347,31001196,2,"2015/06/26, 04:13:54",True,"2015/06/26, 04:13:54",86,5051173,7,"<p>That error means that gradle is unable to resolve the dependency on <code>com.appdynamics:appdynamics-runtime</code>. The easiest way to fix this problem is to use the AppDynamics libraries from maven central rather than the <code>adeum-maven-repo</code> directory. You can do that by editing your top level gradle file to look like this:</p>

<pre><code>buildscript {
    configurations.classpath.resolutionStrategy.force('com.android.tools.build:gradle:1.2.3')
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath 'com.android.tools.build:gradle:1.2.3'
        classpath 'com.appdynamics:appdynamics-gradle-plugin:4.+'
    }
}

allprojects {
    repositories {
        mavenCentral()
    }
}
</code></pre>

<p>Then your project-level gradle file would look like:</p>

<pre><code>apply plugin: 'adeum'

repositories {
    flatDir {
        dirs 'lib'
    }
}

dependencies {
    compile 'com.appdynamics:appdynamics-runtime:4.+'
}
</code></pre>

<p>Note that I have removed the references to <code>adeum-maven-repo</code>, and changed the version numbers on the AppDynamics artifacts to refer to them as they exist in maven central. Once you've done this, you no longer need <code>adeum-maven-repo</code> in your project, since gradle is now downloading these dependencies automatically.</p>
"
AppDynamics,60742827,60741008,0,"2020/03/18, 17:37:23",False,"2020/03/18, 17:37:23",6820,1709216,7,"<p>You need to set Multi-Release to true in the jar's MANIFEST.MF. In the assembly plugin you should be able to do that by adding</p>

<pre><code>      &lt;archive&gt;
        &lt;manifestEntries&gt;
          &lt;Multi-Release&gt;true&lt;/Multi-Release&gt;
        &lt;/manifestEntries&gt;
      &lt;/archive&gt;
</code></pre>

<p>to the configuration section of your assembly configuration. </p>

<p>You could also use the jar plugin to create your jar. For that you would do</p>

<pre><code>&lt;plugin&gt;
  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
  &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
  &lt;configuration&gt;
    &lt;archive&gt;
        &lt;manifestEntries&gt;
            &lt;Multi-Release&gt;true&lt;/Multi-Release&gt;
        &lt;/manifestEntries&gt;
    &lt;/archive&gt;
    &lt;finalName&gt;mr-jar-demo.jar&lt;/finalName&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>
"
AppDynamics,43779475,43771883,0,"2017/05/04, 12:49:36",False,"2017/05/04, 12:49:36",383,2305216,2,"<p>Use Stackify Prefix for this kind of things:</p>

<p><a href=""https://stackify.com/prefix/"" rel=""nofollow noreferrer"">https://stackify.com/prefix/</a></p>

<blockquote>
  <p>View SQL queries: Including SQL parameters, affected records and how long it took to download the result set.</p>
</blockquote>
"
AppDynamics,22447659,19772523,0,"2014/03/17, 07:40:09",True,"2014/03/17, 07:40:09",391,1801817,2,"<p>use tracer plugin where u can import the information in csv/html/xml.This is for jvisualvm
<a href=""http://visualvm.java.net/plugins.html"" rel=""nofollow"">http://visualvm.java.net/plugins.html</a></p>
"
AppDynamics,6588384,6588296,1,"2011/07/05, 23:19:08",True,"2011/07/05, 23:19:08",37594,192444,3,"<p>Start the application and run jconsole on the PID.  While its running look at the heap in the console.  When it near maxes get a heap dump.  Download Eclipse MAT and parse the heap dump.  If you notice the retained heap size is vastly less then the actual binary file parse the heap dump with -keep_unreachable_objects being set.  </p>

<p>If the latter is true and you are doing a full GC often you probably have some kind of leak going on.  Keep in mind when I say leak I don't mean a leak where the GC cannot retain memory, rather some how you are building large objects and making them unreachable often enough to cause the GC to consume a lot of CPU time.</p>

<p>If you were seeing true memory leaks you would see GC Over head reached errors</p>
"
AppDynamics,58803167,53397807,1,"2019/11/11, 16:20:49",False,"2019/11/11, 17:18:30",26,12355048,0,"<p>If you want to monitor only your worker processes of IIS and Standalone Service, You can use CLR Crash event on your policy configuration.</p>

<p>AppDynamics automaticaly creates CLR Crash Events If your IIS or Standalone Service are crashed.</p>

<p>You can find the details of CLR Crash Events:
<a href=""https://docs.appdynamics.com/display/PRO45/Monitor+CLR+Crashes"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO45/Monitor+CLR+Crashes</a></p>

<p>Also, Sample policy configuration: 
<a href=""https://i.stack.imgur.com/mloax.png"" rel=""nofollow noreferrer"">Policy Configuration Screen</a></p>
"
AppDynamics,47278626,47278327,1,"2017/11/14, 07:56:39",True,"2017/11/14, 07:56:39",22298,7005159,1,"<p>We could install the AppDynamics extension use Azure portal or Kudu tool(<a href=""https://functionAppname.scm.azurewebsites.net/"" rel=""nofollow noreferrer"">https://functionAppname.scm.azurewebsites.net/</a>).</p>

<p><strong>Azure Portal:</strong></p>

<p><a href=""https://i.stack.imgur.com/9Ge0J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Ge0J.png"" alt=""enter image description here""></a></p>

<p><strong>Kudu UI</strong></p>

<p><a href=""https://i.stack.imgur.com/HIWHy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HIWHy.png"" alt=""enter image description here""></a></p>

<p><strong>After installation</strong></p>

<p><a href=""https://i.stack.imgur.com/i0BpP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i0BpP.png"" alt=""enter image description here""></a></p>
"
AppDynamics,16171037,16161856,0,"2013/04/23, 16:48:17",False,"2013/04/23, 16:48:17",11,2047636,1,"<p>A great place to ask this question would be on the AppDynamics discussion forums so that AppDynamics support can answer you directly... <a href=""http://appsphere.appdynamics.com/t5/Discussions/ct-p/Discussions"" rel=""nofollow"">http://appsphere.appdynamics.com/t5/Discussions/ct-p/Discussions</a></p>

<p>I'm guessing there is a permissions issue somewhere.</p>
"
AppDynamics,18347965,16161856,0,"2013/08/21, 05:35:52",False,"2013/08/21, 05:35:52",17506,524588,1,"<p>You should copy all files to run the agent, not just javaagent.jar. </p>

<p>This is a thread about it. <a href=""http://appsphere.appdynamics.com/t5/Lite-for-Java/Keep-getting-a-quot-Invalid-Agent-Installation-Directory-quot-on/td-p/1151"" rel=""nofollow"">http://appsphere.appdynamics.com/t5/Lite-for-Java/Keep-getting-a-quot-Invalid-Agent-Installation-Directory-quot-on/td-p/1151</a></p>
"
AppDynamics,33141659,33134494,1,"2015/10/15, 09:47:16",True,"2015/10/15, 09:52:42",42504,936832,2,"<p>Generally, monitoring tools cannot record method-level data continuously, because they have to operate at a much lower level of overhead compared to profiling tools. They focus on ""business transactions"" that show you high-level performance measurements with associated semantic information, such as the processing of an order in your web shop.</p>

<p>Method level data only comes in when these business transactions are too slow. The monitoring tool will then start sampling the executing thread and show you a call tree or hot spots. However, you will not get this information for the entire VM for a continuous interval like you're used to from a profiler.</p>

<p>You mentioned JProfiler, so if you are already familiar with that tool, you might be interested in <a href=""http://www.ej-technologies.com/products/perfino/overview.html"" rel=""nofollow noreferrer"">perfino</a> as a monitoring solution. It shows you samples on the method level and has cross-over functionality into profiling with the native JVMTI interface. It allows you to do <a href=""http://resources.ej-technologies.com/perfino/help/doc/main/profiling.html"" rel=""nofollow noreferrer"">full sampling of the entire JVM</a> for a selected amount of time and look at the results in the JProfiler GUI.</p>

<p><a href=""https://i.stack.imgur.com/40ZV9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/40ZV9.png"" alt=""enter image description here""></a></p>

<p>Disclaimer: My company develops JProfiler and perfino.</p>
"
AppDynamics,53074197,52994676,0,"2018/10/31, 01:18:54",True,"2018/10/31, 01:18:54",1182,1622880,2,"<p>There are integration tools available between influxdb/AppDynamics and grafana/AppDynamics.</p>

<p><a href=""https://github.com/Appdynamics/MetricMover"" rel=""nofollow noreferrer"">https://github.com/Appdynamics/MetricMover</a></p>

<p><a href=""https://grafana.com/plugins/dlopes7-appdynamics-datasource/installation"" rel=""nofollow noreferrer"">https://grafana.com/plugins/dlopes7-appdynamics-datasource/installation</a>).</p>

<p>There's nothing that integrates between Prometheus and AppDynamics at the moment</p>

<p>I'm not sure there will be one going forward, seeing how they are competing in the same space from different vantage points (Open Source vs Enterprise)</p>
"
AppDynamics,42376943,41130233,1,"2017/02/21, 22:06:15",True,"2017/02/21, 22:06:15",207,992280,-7,"<p>Information I got from another website.</p>

<p>Application Insights (AI) is a very simplistic APM tool today. It does do transaction tracing, it has very limited code and runtime diagnostics, and it doesn’t go very deep into analyzing the browser performance. Application Insights is much more like Google Analytics than like a typical APM tool.</p>

<ul>
<li>AI does not support mobile apps, AppDynamics does Android and iOS</li>
<li>AI only supports Java, .NET, and node.js while AppDynamics supports many
additional languages and technologies.</li>
<li>AI requires code changes to do additional metric capture from the
code. AppDynamics has do this on the fly without code change on most
languages.</li>
<li>AI doesn’t so transaction distributed tracing, it has a simple data
collection model similar to what New Relic does. This makes
troubleshooting much harder in complex apps. If your app is simple
it’s not required.</li>
<li>AI lacks transaction scoring and baselining, you must set manual
thresholds. AppDynamics does this for every metric and every business
transaction.</li>
<li>AI doesn’t monitor databases or data stores. AppDynamics does both.</li>
<li>AI is SaaS only, while AppDynamics can be deployed on premises or
SaaS.</li>
</ul>
"
AppDynamics,65946975,65946794,2,"2021/01/29, 01:58:56",False,"2021/01/29, 01:58:56",483,4895267,0,"<p>AppDynamics have a fork of contrib project <a href=""https://github.com/Appdynamics/opentelemetry-collector-contrib"" rel=""nofollow noreferrer"">here</a> where there is an exporter but it isn't clear if it has been finished</p>
"
AppDynamics,66421315,65946794,0,"2021/03/01, 13:29:48",True,"2021/03/01, 13:29:48",3170,1237595,1,"<p>there is a Beta program now running for using AppDynamics with the OpenTelemetry Collector.</p>
<p>More details are available in the public docs: <a href=""https://docs.appdynamics.com/display/PRO21/Ingest+OpenTelemetry+Trace+Data"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Ingest+OpenTelemetry+Trace+Data</a></p>
"
AppDynamics,26749167,25891322,0,"2014/11/05, 05:23:23",True,"2014/11/05, 05:23:23",5502,376340,4,"<p>The definition can be found in AppDynamics docs: <a href=""https://docs.appdynamics.com/display/PRO39/Troubleshoot+Slow+Response+Times#TroubleshootSlowResponseTimes-SlowandStalledTransactions"" rel=""nofollow"">Slow and Stalled Transactions</a></p>

<blockquote>
  <p>By default AppDynamics considers a slow transaction one that lasts longer than 3 times the standard deviation for the last two hours, a very slow transaction 4 times the standard deviation for the last two hours, and a stalled transaction one that lasts longer than 300 deviations above the average for the last two hours.</p>
</blockquote>

<p>Of course, you can modify the default rules by providing your own: <a href=""https://docs.appdynamics.com/display/PRO39/Configure+Thresholds"" rel=""nofollow"">Configure Thresholds</a></p>
"
AppDynamics,29123098,29122786,0,"2015/03/18, 15:14:41",False,"2015/03/18, 15:14:41",505,4382794,0,"<p>Even it is not the desired behaviour, if filtering by URL and specifying only createNewActivity worked for me (as long as there are no other REST URLs matching this).</p>
"
AppDynamics,52681154,52680807,5,"2018/10/06, 19:38:02",False,"2018/10/08, 13:21:09",465,6645806,4,"<p>You can get <code>POD_NAME</code> and <code>POD_NAMESPACE</code> passing them as environment variables via <code>fieldRef</code>. </p>

<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: test-env
spec:
  containers:
    - name: test-container
      image: my-test-image:latest
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: MY_POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: REFERENCE_EXAMPLE
          value: ""/$(MY_NODE_NAME)/$(MY_POD_NAMESPACE)/$(MY_POD_NAME)/data.log""
  restartPolicy: Never
</code></pre>

<p><strong>EDIT</strong>: <em>Added example env <code>REFERENCE_EXAMPLE</code> to show how to reference variables. Thanks to <a href=""https://stackoverflow.com/questions/49582349/kubernetes-how-to-refer-to-one-environment-variable-from-another"">this</a> answer for pointing out the <code>$()</code> interpolation.</em></p>

<p>You can reference <code>supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP</code> as mentioned in the documentation <a href=""https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#envvarsource-v1-core"" rel=""nofollow noreferrer"">here</a>.</p>

<p>However, <code>CLUSTERNAME</code> is not a standard property available. According to this <a href=""https://github.com/kubernetes/kubernetes/pull/22043"" rel=""nofollow noreferrer"">PR #22043</a>, the <code>CLUSTERNAME</code> should be injected to the <code>.metadata</code> field if using GCE.</p>

<p>Otherwise, you'll have to specific the <code>CLUSTERNAME</code> manually in the <code>.metadata</code> field  and then use <code>fieldRef</code> to inject it as an environment variable.</p>
"
AppDynamics,52696997,52680807,0,"2018/10/08, 10:02:32",False,"2018/10/08, 10:08:15",415,6220162,1,"<p>Below format helped me, suggested by ewok2030 and Praveen.
Only one thing to make sure that the variable should be declared before they are used as JAVA_OPTS.</p>

<p>containers:</p>

<pre><code>   - env:

    - name: APPD_NODE_NAME
       valueFrom: 
        fieldRef:
          fieldPath: spec.nodeName
    - name: APPD_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: APP_POD_NAME
      valueFrom: 
        fieldRef:
          fieldPath: metadata.name
    - name: JAVA_OPTS
      value: -Xmx712m -Xms556m -Dpdp.logging.level=WARN -Dappdynamics.agent.nodeName=$(APPD_NODE_NAME)-$(APPD_POD_NAMESPACE)-$(APP_POD_NAME)
</code></pre>
"
AppDynamics,60428806,50355471,1,"2020/02/27, 10:11:48",False,"2020/02/27, 10:11:48",85,2574522,0,"<p>Are you sure you're not getting the FileNotFound when trying to do getInputStream on the connection later (or in your real code in case this is a simplified example)? Since HttpUrlConnection implements http protocol and 4xx codes are error codes trying to call getInputStream generates FNF. Instead you should call getErrorStream. </p>

<p>I ran your code (without the auth part) and I don't get any FilenotFoundException testing on url's that return 404.</p>

<p>So in this case HttpUrlConnection correctly implements the http protocol and appdynamics correctly catches the error. 
I'm facing the same issue with jersey wrapping the FnF in a UniformResourceException but after some analyzing it's actually either jersey that should provide ways of checking the status code before returning output or correctly use httpurlconnection, and in our case - the webservice should not return 404 for requests that yields no found results but rather an empty collection.</p>

<pre><code>import java.io.IOException;
import java.net.HttpURLConnection;
import java.net.MalformedURLException;
import java.net.URL;
import java.net.URLConnection;

public class Connection {

    public boolean connect(URL url, String requestType) {
        HttpURLConnection connection = null;
        try {
            URLConnection urlConnection = url.openConnection();
            if (urlConnection instanceof HttpURLConnection) {
                connection = (HttpURLConnection) urlConnection;
                connection.setRequestMethod(requestType);
            }
            if (connection == null) {
                return false;
            }
            connection.connect();
            System.out.println(connection.getResponseCode());
            return connection.getResponseCode() == HttpURLConnection.HTTP_OK;
        } catch (IOException e) {
            System.out.println(""E"" + e.getMessage());
            return false;
        }
    }

    public static void main(String args[])
    {
        Connection con = new Connection();
        try{
        con.connect(new URL(""http://www.google.com/asdfasdfsd""), ""GET"");   
    } catch(MalformedURLException mfe)
    {

    }
    }
}
</code></pre>
"
AppDynamics,47301009,47300483,0,"2017/11/15, 08:46:45",False,"2017/11/15, 19:23:51",89858,1945651,-1,"<p>Not sure whether it would be the cause of memory leaks, but your function definitely has a ton of unnecessary cruft that could be cleaned up with Bluebird's <code>Promise.mapSeries()</code> and other Bluebird helpers. </p>

<p>Doing so may very well help cut down on memory leaks.</p>

<p><code>doSomething</code> function reduced to 8 lines:</p>

<pre><code>function doSomething(){
    return Promise.delay(1000)  // &lt;-- specify time
        .return(dataArray)
        .mapSeries(function (el) {
            return executeFunction({ user: el.user });
        })
        .filter(function (result) { 
            return result !== null; 
        });
}
</code></pre>
"
AppDynamics,38628646,38600280,0,"2016/07/28, 09:30:52",False,"2016/07/28, 09:30:52",9374,5493302,2,"<ul>
<li>Write a wrapper called <code>appdynamics.coffee</code></li>
<li>Compile this wrapper to <code>.js</code></li>
<li>Replace <code>server.js</code> with <code>appdynamics.js</code> and <code>server.coffee</code> with <code>appdynamics.coffee</code></li>
</ul>

<p>After this operations</p>

<pre><code>{
  ""scripts"": {
    ""start"": ""node server.js""
  }
}
</code></pre>

<p>will be </p>

<pre><code>{
  ""scripts"": {
    ""start"": ""node appdynamics.js""
  }
}
</code></pre>

<p>and</p>

<pre><code>{
  ""scripts"": {
    ""start"": ""coffee server.coffee""
  }
}
</code></pre>

<p>will be</p>

<pre><code>{
  ""scripts"": {
    ""start"": ""coffee appdynamics.coffee""
  }
}
</code></pre>
"
AppDynamics,29033676,29023156,4,"2015/03/13, 15:37:34",True,"2015/03/13, 15:37:34",129048,1240763,3,"<p>Since you are a Spring Framework user, consider using <a href=""http://projects.spring.io/spring-amqp/"" rel=""nofollow"">Spring AMQP</a>. The <code>RabbitTemplate</code> uses cached channels over a single connection with the channel being checked out of the cache (and returned) for each operation. The default cache size is 1, so it generally needs to be configured for an environment like yours.</p>
"
AppDynamics,26594982,26555066,0,"2014/10/27, 21:06:16",False,"2014/10/27, 21:06:16",41,702498,2,"<p>I'm leaning towards it being an issue with the number of channels and the channel cache size. Does anyone know if there's a limit on the number of channels on a queue? </p>

<p>It seems like specifying connections rather than channels might help here. </p>

<p>If anyone has any information it would really help,  getting stuck for time :) </p>
"
AppDynamics,23488652,23480106,2,"2014/05/06, 10:35:58",False,"2014/05/06, 10:35:58",670,2080975,1,"<p>I think you should use <code>@PersistenceContext</code> annotation to obtain <code>EntityManager</code> from Spring context and <code>@Transactional</code> annotation to drive your transactions. This way you won't need to worry about closing Hibernate sessions manually, and number of connections will not increase until there are <em>too many connections</em>.</p>
"
AppDynamics,23682727,23480106,1,"2014/05/15, 18:33:56",True,"2014/05/15, 18:33:56",962,360903,4,"<p>The problem did not lay in the sessions but in the connection pool, sessions where being managed correctly, but the connection pool in the JDBC layer wasn't closing the connections.</p>

<p>These are the things that I did to fix them.</p>

<p><strong>JDBC context configuration</strong></p>

<p>1.- Changed the JDBC connection factory from tomcat's old BasicDataSourceFactory to tomcat's new DataSourceFactory</p>

<p>2.-  Tuned the JDBC settings based on this article: 
<a href=""http://www.tomcatexpert.com/blog/2010/04/01/configuring-jdbc-pool-high-concurrency"" rel=""nofollow"">http://www.tomcatexpert.com/blog/2010/04/01/configuring-jdbc-pool-high-concurrency</a></p>

<p><strong>Session factory xml configuration</strong></p>

<p>3.- Deleted this line from the session factory configuration:</p>

<pre><code>&lt;prop key=""hibernate.max_fetch_depth""&gt;1&lt;/prop&gt;
</code></pre>

<p>This is how my configuration ended up:</p>

<p><strong>JDBC context configuration</strong></p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;Context path=""/"" override=""true"" reloadable=""false"" swallowOutput=""false""&gt;
    &lt;Resource name=""jdbc/catWDB"" auth=""Container"" type=""javax.sql.DataSource""
              driverClassName=""com.mysql.jdbc.Driver""
              factory=""org.apache.tomcat.jdbc.pool.DataSourceFactory""
              url=""@URL@""
              username=""@USERNAME@""
              password=""@PASSWORD@""
              maxActive=""200""
              maxIdle=""50""
              minIdle=""10""
              suspectTimeout=""60""
              timeBetweenEvictionRunsMillis=""30000""
              minEvictableIdleTimeMillis=""60000""
              validationQuery=""select 1""
              validationInterval=""30000""
              testOnBorrow=""true""
              removeAbandoned=""true""
              removeAbandonedTimeout=""60""
              abandonWhenPercentageFull=""10""
              maxWait=""10000""
              jdbcInterceptors=""ResetAbandonedTimer;StatementFinalizer""
            /&gt;
&lt;/Context&gt;
</code></pre>

<p><strong>Session factory xml configuration</strong></p>

<pre><code>&lt;bean id=""sessionFactory"" class=""org.springframework.orm.hibernate3.LocalSessionFactoryBean""&gt;
        &lt;property name=""dataSource""&gt;
            &lt;ref local=""dataSource""/&gt;
        &lt;/property&gt;
        &lt;property name=""configLocation"" value=""classpath:hibernate.cfg.xml"" /&gt;
        &lt;property name=""hibernateProperties""&gt;
            &lt;props&gt;
                &lt;prop key=""hibernate.dialect""&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt;
                &lt;prop key=""hibernate.show_sql""&gt;false&lt;/prop&gt;
                &lt;prop key=""hibernate.max_fetch_depth""&gt;3&lt;/prop&gt;
            &lt;/props&gt;
        &lt;/property&gt;
&lt;/bean&gt;
</code></pre>
"
AppDynamics,23100111,23099992,2,"2014/04/16, 07:39:32",False,"2014/04/16, 07:39:32",755,3535576,0,"<blockquote>
  <p>Hey there, Just crawl in to this URL.</p>
</blockquote>

<p><strong><a href=""https://help.ubuntu.com/community/EnvironmentVariables"" rel=""nofollow"">https://help.ubuntu.com/community/EnvironmentVariables</a></strong></p>

<blockquote>
  <p>It will better help you.The above URL gives all informations about
  Environment Variable Ubuntu. ANd the above POST is updated in 3 Jan
  2014.</p>
</blockquote>
"
AppDynamics,23100276,23099992,0,"2014/04/16, 07:54:30",False,"2014/04/16, 07:54:30",6589,3145373,4,"<p>Put the environment variables into the global /etc/environment file:</p>

<p>...</p>

<pre><code>JAVA_HOME=/usr/lib/jvm/java-1.7.0
</code></pre>

<p>...
Execute ""source /etc/environment"" in every shell where you want the variables to be updated:</p>

<pre><code>$ source /etc/environment
</code></pre>

<p>Check that it works:</p>

<pre><code>$ echo $JAVA_HOME
$ /usr/lib/jvm/java-1.7.0
</code></pre>

<p><a href=""http://www.mkyong.com/linux/how-to-set-environment-variable-in-ubuntu/"" rel=""nofollow"">Here is a other link from mkyong</a></p>
"
AppDynamics,23100418,23099992,0,"2014/04/16, 08:06:09",False,"2014/04/16, 08:06:09",526,1494927,2,"<p>Here is some info on environment variables, setting your path and where to install things that I hope is useful for you to get your environment setup. </p>

<h2>User specific environment variables</h2>

<p><strong>.bashrc</strong>: is specific for the <em>bash</em> shell.</p>

<p><strong>.profile</strong>: is used by several shells, and was originally used by the bourne shell (from memory). <code>.profile</code> might not be loaded by bash if there is a <code>.bashrc</code> present. Some shells read it only if there is no shell specific configuration present. </p>

<p>If you happen to be using a different shell, you will need to look at how best to configure environment variables for that shell.</p>

<p>Note that adding to the above files only effect the user that you set them for, since they live in <code>/home/username/</code>.</p>

<p>Also remember to source the file again, or reload the shell so that your settings take effect. You can achieve this with something like <code>source .bashrc</code> after you edit it at the command line to avoid having to restart or reopen terminal. </p>

<h2>System wide environment variables</h2>

<p>If you would like to set system wide variables, you can do that in <code>/etc/environment</code>. </p>

<h2>Updating your PATH</h2>

<p>If you would like to execute java / ant / maven, etc. from the command line, or enable applications that require the <code>PATH</code> environment variable to be set correctly to work, you will also need to add the <code>./bin</code> directories to your PATH. Depending on your preference regarding system wide or user specific path settings:</p>

<pre><code>export PATH=$PATH:$JAVA_HOME/bin:$ANT_HOME/bin 
</code></pre>

<p>etc. in the relevant file. </p>

<h2>Filesystem Hierarchy Standard</h2>

<p>A side point and entirely optional, the correct place to install java, ant, maven, etc if not from .deb's would be in <code>/opt</code>, according to the <a href=""https://wiki.debian.org/FilesystemHierarchyStandard"" rel=""nofollow"">FilesystemHierarchyStandard</a></p>
"
AppDynamics,11917597,11916710,1,"2012/08/11, 23:56:01",False,"2012/08/11, 23:56:01",2045,1469663,0,"<p>You might check out Spring Insight.</p>

<p><a href=""https://stackoverflow.com/questions/11817807/spring-insight-source-design-and-alternatives"">Spring-insight source, design and alternatives</a></p>

<p><a href=""http://www.springsource.org/insight"" rel=""nofollow noreferrer"">http://www.springsource.org/insight</a></p>
"
AppDynamics,12173401,11916710,0,"2012/08/29, 11:08:08",False,"2012/08/29, 11:08:08",275412,40342,0,"<p><a href=""https://code.google.com/p/javamelody/"" rel=""nofollow"">Java Melody</a> might be relevant for you.</p>
"
AppDynamics,54083177,54046191,0,"2019/01/08, 01:13:19",True,"2019/01/08, 01:13:19",474,4682632,2,"<p>Thanks for the reply to my question. The .NET agent in most APM tools works the same way which leverages the profiler APIs in the .NET SDK and allows for data collection and also callbacks and other interception. Most of the tools also use performance counter data, and other sources aside from inside the .NET runtime. This allows you to do several things similar to Java in terms of data collection. </p>

<p>ref: <a href=""https://docs.microsoft.com/en-us/visualstudio/profiling/walkthrough-using-profiler-apis?view=vs-2017"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/visualstudio/profiling/walkthrough-using-profiler-apis?view=vs-2017</a></p>

<p><a href=""http://www.blong.com/conferences/dcon2003/internals/profiling.htm"" rel=""nofollow noreferrer"">http://www.blong.com/conferences/dcon2003/internals/profiling.htm</a></p>
"
AppDynamics,58210199,58208802,0,"2019/10/03, 01:37:31",True,"2019/10/03, 01:37:31",955,284480,1,"<p>The most important thing to understand about this error is the meaning of this line: </p>

<p><code>Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target</code></p>

<p>SSL certificates work by establishing a certificate chain, or a hierarchy of trust.  For example, if I go to <a href=""https://www.google.com"" rel=""nofollow noreferrer"">https://www.google.com</a> and look at their cert, this is what I see:</p>

<p><a href=""https://i.stack.imgur.com/K6GBG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K6GBG.png"" alt=""Google cert chain""></a></p>

<p>There is the google cert, which sits on their servers/CDN, then an intermediate cert which also sits on their servers/CDN, then a trusted root CA cert which is in the <strong>client</strong> keystore and is implicitly trusted.  So when someone browses to google, b/c they have the root CA cert and have trusted it, the browser (client) will trust that the server is actually who they say they are and will establish a secure connection to the site.</p>

<p>So getting back to your error, whatever CA issued the server cert being used by RabbitMQ, the monitor is not recognizing it as trusted.  To troubleshoot this error, here are the things to do:</p>

<ul>
<li>Look at the server cert and ensure it can be validated.  openssl works well for this; run: <code>openssl s_client -connect 127.0.0.1:15672 -showcerts</code> and look at the cert chain.</li>
<li>Validate the root CA is trusted by your java keystore.  You can view these certs with the <code>keystore</code> tool: <code>keytool -list -v -keystore &lt;/path/to/keystore&gt; -storepass &lt;pass&gt;</code>.  Ensure the cert listed above is in the keystore.</li>
</ul>

<p>There are a couple other gotchas to watch out for:</p>

<ul>
<li>What keystore java is using is not always obvious.  The jdk has a default keystore, and each app can use its own keystore, like you are doing above.  Ensure you know what keystore is being used.  Although it will add lots of logging, is can be helpful to add <code>-Djavax.net.debug=all</code> to the command line.</li>
<li>Beware adding individual server certs to the keystore.  This will work, until the server cert expires.  Much better to depend on trusted CA certs, which are generally maintained at the platform level.  Adding individual certs is generally considered an anti-pattern.</li>
</ul>
"
AppDynamics,43318354,43315501,1,"2017/04/10, 11:34:41",True,"2017/04/10, 11:34:41",324,7220665,3,"<p>You can use the REST API of the Controller described here <a href=""https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO42/Using+the+Controller+APIs</a></p>

<p>To access the REST API Browser, in a Web browser, go to: </p>

<pre><code>https://&lt;controller_host&gt;:&lt;primary_port&gt;/api-docs/index.html
</code></pre>

<p>this will give you a nice swagger UI description of the available resources.</p>

<p>Alternatively you can create reports directly out of the box in AppDynamics via the <code>Dashboards &amp; Reports</code> section.</p>
"
AppDynamics,55150379,55048208,0,"2019/03/13, 22:07:48",True,"2019/03/13, 22:07:48",41,2625630,0,"<p>The solution was adding ""appdynamics"" to the ""externals"" in the Webpack configuration: <a href=""https://webpack.js.org/configuration/externals/"" rel=""nofollow noreferrer"">https://webpack.js.org/configuration/externals/</a></p>

<p>This allows AppDynamics to use the default Node.js require import.</p>
"
AppDynamics,44308592,44283136,0,"2017/06/01, 16:15:26",False,"2017/06/01, 20:52:24",324,7220665,1,"<p>First you need to setup the AppDynamics Controller (should be hosted on a separate  machine), then you need java agents on the machine with the application. 
<a href=""https://i.stack.imgur.com/b4r01.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b4r01.png"" alt=""enter image description here""></a></p>

<p>You need to wire the application with the Java agent, e.g. like this</p>

<pre><code>/usr/bin/java
-Dappdynamics.controller.hostName=""my-appdynamics-controller-url""
-Dappdynamics.controller.port=""8090"" \
-Dappdynamics.agent.applicationName=""MyAppName""
-javaagent:/appdynamics/appserver-agent/javaagent.jar
-Dappdynamics.agent.tierName=""my-tier-name""
-Dappdynamics.agent.nodeName=""my-node-name""
-jar myapp.jar
</code></pre>

<p>Now the Java agent sends application information to the controller. Take a look at the documentation. <a href=""https://docs.appdynamics.com/display/PRO43/Getting+Started"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO43/Getting+Started</a></p>

<p>The application will automatically be available in AppDynamics and you can see the dashboard.</p>
"
AppDynamics,44338258,44283136,0,"2017/06/03, 01:27:31",False,"2017/06/03, 01:27:31",68,1394813,0,"<p>You don't need to host the controller, you can get a SAAS account here. <a href=""https://www.appdynamics.com/free-trial/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/free-trial/</a> . </p>

<p>Once you get a SAAS account lookup the account name and account key in the welcome email and use the instructions in the email to add JVM args to your app as shown. Don't forget to set the account name and account access key args: -Dappdynamics.agent.accountName -Dappdynamics.agent.accountAccessKey .</p>
"
AppDynamics,33780274,33210833,1,"2015/11/18, 14:36:17",True,"2015/11/18, 14:36:17",66,1355536,3,"<p>Data retrieval by APM tools is done in several ways, each one with its pros and cons</p>

<ul>
<li><p><em>Bytecode injection</em> (for both Java and .NET) is one technique, which is somewhat intrusive but allows you to get data from places the application owner (or even 3rd party frameworks) did not intend to allow.</p></li>
<li><p><em>Native function interception</em> is similar to bytecode injection, but allows you to intercept unmanaged code</p></li>
<li><p><em>Application plugins</em> - some applications (e.g. Apache, IIS) give access to monitoring and application information via a well-documented APIs and plugin architecture</p></li>
<li><p><em>Network sniffing</em> allows you to see all the communication to/from the monitored machine</p></li>
<li><p><em>OS specific un/documented APIs</em> - just like application plugins, but for the Windows/*nix</p></li>
</ul>

<p>Disclaimer : I work for Correlsense, provider of APM software SharePath, which uses all of the above methods to give you complete end-to-end transaction visibility.</p>
"
AppDynamics,66101093,63647198,0,"2021/02/08, 13:51:32",False,"2021/02/08, 13:51:32",3170,1237595,0,"<p>This issue was solved here: <a href=""https://community.appdynamics.com/t5/Java-Java-Agent-Installation-JVM/monitor-URL-page-availability/td-p/40904"" rel=""nofollow noreferrer"">https://community.appdynamics.com/t5/Java-Java-Agent-Installation-JVM/monitor-URL-page-availability/td-p/40904</a></p>
<p>Summary:</p>
<ul>
<li>Question did not contain full config.yml contents</li>
<li>Lots of default config needed commenting out as below</li>
</ul>
<pre>
    # Patterns to be matched, multiple patterns(to be matched) can be configured for a given site
     #matchPatterns:
     #- name: 
     #  type: 
     #  pattern: 

   #- name:     
   #  url:      
   #  authType: 

  # Basic Authentication with password encryption
   #- name:       
   #  url:        
   #  username:   
   #  password:   
   #  encryptionKey: """"
   #  encryptedPassword: """"
   #  authType: 

   #- name:     
   #  url:      

  #NTLM Auth Sample Configuration
   #- name:     
   #  url:      
   #  username: 
   #  password: 
   #  authType: 

     # Client Cert Auth Sample Configuration
   #- name:         
   #  url:          
   #  password:     
   #  authType:     
   #  keyStoreType: 
   #  keyStorePath: 
   #  keyStorePassword: 
   #  trustStorePath: 
   #  trustStorePassword: 

     #POST request sample configuration
   #- name:     
   #  url:      
   #  username:
   #  password:
   #  connectTimeout: 60000
   #  usePreemptiveAuth: 
   #  method:   
   #  headers:
   #    Content-Type: application/json
   #  requestPayloadFile: src/test/resources/conf/postrequestPayloadFile
   #  matchPatterns:
   #    - name:       Error
   #      type:       substring
   #      pattern:    Error 400

     #Proxy Configuration
   #- name:     
   #  url:      
   #  groupName: 
   #  proxyConfig:
   #    host: """"
   #    port: 
   #    username: """"
   #    password: """"
</pre>
"
AppDynamics,49007036,49004859,0,"2018/02/27, 12:58:41",False,"2018/02/27, 12:58:41",239035,2711488,2,"<p>See <a href=""https://docs.appdynamics.com/display/PRO43/Monitor+Java+Interface+Static+and+Default+Methods"" rel=""nofollow noreferrer"">4.3.x Documentation⇒POJO Entry Points⇒Monitor Java Interface Static and Default Methods</a>:</p>

<blockquote>
  <p>Note that another Java language feature introduced in Java 8, lambda method interfaces, are not supported by the AppDynamics Java Agent.</p>
</blockquote>

<p>It’s possible that this is due to technical difficulties with <a href=""https://bugs.openjdk.java.net/browse/JDK-8145964"" rel=""nofollow noreferrer"">JDK-8145964</a> as you suspect. But I’d also point out that this kind of Instrumentation would be questionable. It’s not this JRE generated class that implements any specific behavior, it’s the invoked target method.</p>
"
AppDynamics,49029706,49004859,1,"2018/02/28, 14:33:14",False,"2018/02/28, 14:33:14",474,4682632,0,"<p>Support for Lambda expressions has been in the product since 4.1 which shipped in 2015 I believe. That being said we are always enhancing support. These do have some limitations after initialization of the classes using them (dynamic instrumentation limits). The product should support them, we have added some additional capabilities and features for Lambda expressions in our next major release. Have you tried to contact help @ appdynamics.com</p>
"
AppDynamics,49033394,49004859,0,"2018/02/28, 17:45:13",False,"2018/02/28, 17:45:13",9111,4647853,0,"<p>Looks like it was not mentioned in release notes, but instead <a href=""https://docs.appdynamics.com/display/PAA/Support+Advisory+56039%3A+Errors+triggered+by+lambda+constructs"" rel=""nofollow noreferrer"">Support Advisory 56039</a> was raised. They indeed mention JDK-8145964 as a reason for removing the support.</p>
"
AppDynamics,13182788,13182423,0,"2012/11/01, 19:52:02",True,"2012/11/01, 19:52:02",49920,32453,1,"<p>Looks like you create the metric, then edit a dashboard, then click on a widget -> add metric -> (browse, but choose ""Individual Nodes"" instead of JMX, then select your metric. voila.</p>
"
AppDynamics,24808336,24797525,2,"2014/07/17, 18:58:39",False,"2014/07/17, 18:58:39",11,2047636,0,"<p>The best place for you to get this question answered would be on the AppDynamics community discussion boards. Here's a link for you... <a href=""http://community.appdynamics.com/t5/Forums-Community-AppDynamics/ct-p/Discussions"" rel=""nofollow"">http://community.appdynamics.com/t5/Forums-Community-AppDynamics/ct-p/Discussions</a></p>

<p>The AppDynamics documentation site is also a great resource and you don't even need a login to access them... <a href=""http://docs.appdynamics.com/"" rel=""nofollow"">http://docs.appdynamics.com/</a></p>
"
AppDynamics,24809185,24797525,3,"2014/07/17, 19:45:22",False,"2014/07/17, 19:45:22",66,3850108,0,"<ol>
<li><p>Installation instructions for the controller can be found in:
<a href=""http://docs.appdynamics.com/display/PRO14S/Install+the+Controller"" rel=""nofollow"">http://docs.appdynamics.com/display/PRO14S/Install+the+Controller</a>
Assuming you are using App Server Agent for Java, installation instructions for that and the machine agent can be found in the above site on the Left Nav table of contents.</p></li>
<li><p>The controller is the management server, the app server agent monitors the JVM and the machine agent collects system metrics (such as CPU, Memory, Disk &amp; Network).</p></li>
<li><p>Very minimal configuration is required.</p></li>
</ol>
"
AppDynamics,30294344,29858961,0,"2015/05/18, 05:39:08",True,"2015/05/18, 05:39:08",16,4910351,0,"<p>From what I understand from their documentation, AppD do not have a way to capture heap dumps. They suggest using Memory Leak detection feature in such scenarios.
On a different note, I know we can get thread dumps which maybe helpful in some cases (Agents -> Request Agent Log Files)</p>
"
AppDynamics,31398932,29858961,0,"2015/07/14, 09:12:20",False,"2015/07/14, 09:12:20",51,3269857,3,"<p>Heap dumps in appdynamics can be taken for JRockit JVM by the below method(Note : This does not work for IBM JVM)</p>

<ul>
<li>Select you node in appdynamics for which the heap dump has to be taken.</li>
<li>Select the tab ""JMX""</li>
<li>Go to ""MBean Browser""</li>
<li>Go to ""com.sun.management"" >> ""HotSpotDiagnostic"" from the left pane </li>
<li>On the right window under operations for dumpHeap click on the thunderbolt sign to invoke action.</li>
<li>A new dialogue box opens up in which you have to fill the p0 and p1 text boxes as follows</li>
</ul>

<p>p0 – Path to generate heap dump(/path/dump.hprof)</p>

<p>p1 -  True – GC before Heap dump ; False - No GC before heap dump</p>

<ul>
<li>Click on ""invoke""</li>
</ul>

<p>Note : If you want heap dump to be generated in the case of out of memory give </p>

<p>p0 : HeapDumpOnOutOfMemoryError</p>

<p>Also note that these values will be lost on JVM restart.</p>
"
AppDynamics,53539438,53537738,0,"2018/11/29, 14:47:54",False,"2018/11/29, 14:47:54",6049,3156333,2,"<p>As stated on the AppD docs regarding <a href=""https://docs.appdynamics.com/display/CLOUD/Kubernetes+and+AppDynamics+APM"" rel=""nofollow noreferrer"">Kubernetes and AppDynamics APM</a></p>
<p><a href=""https://i.stack.imgur.com/qieEb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qieEb.jpg"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>Install a Standalone Machine Agent (1) in a Kubernetes node.</p>
<p>Install an APM Agent (2) inside each container in a pod you want to monitor.</p>
<p>The Standalone Machine Agent then collects hardware metrics for each monitored container, as well as Machine and Server metrics for the host (3), and forwards the metrics to the Controller.</p>
</blockquote>
<p>ContainerID and UniqueHostID can be taken from <code>/proc/self/cgroup</code></p>
<blockquote>
<p>ContainerID <code>cat /proc/self/cgroup | awk -F '/' '{print $NF}'  | head -n 1</code></p>
<p>UniqueHostID <code>sed -rn '1s#.*/##; 1s/(.{12}).*/\1/p' /proc/self/cgroup</code></p>
</blockquote>
"
AppDynamics,61748113,60872992,0,"2020/05/12, 12:22:07",True,"2020/05/12, 12:33:22",56,9641196,1,"<p>This is an issue with the AppDynamics plugin and gradle plugin tools 3.6.x. You can see this link: <a href=""https://community.appdynamics.com/t5/End-User-Monitoring-EUM/AppDynamic-EUM-setup-for-Android-Cordova-project/td-p/38864"" rel=""nofollow noreferrer"">https://community.appdynamics.com/t5/End-User-Monitoring-EUM/AppDynamic-EUM-setup-for-Android-Cordova-project/td-p/38864</a></p>

<p>I have downgrade gradle plugin tools to 3.5.x and solve it</p>
"
AppDynamics,59252943,58873513,0,"2019/12/09, 18:27:29",False,"2019/12/09, 18:27:29",392,2801554,0,"<p>You can create a cron job to run from Monday to Saturday, here for each hour:</p>

<pre><code>0 * * * mon-sat
</code></pre>

<p>And another to cover the interval you want on Sunday, here one by one hour from 10:00 AM to 03:00 PM:</p>

<pre><code>0 10-15/1 * * sun
</code></pre>
"
AppDynamics,50644421,50618877,0,"2018/06/01, 16:20:18",False,"2018/06/01, 16:20:18",23,9436164,0,"<p>Looks like httprequest plugin does not support uploading zip file. This is my observation.</p>
"
AppDynamics,50857929,50618877,0,"2018/06/14, 15:47:37",False,"2018/06/14, 15:47:37",61,6101424,0,"<p>I think upload will use <code>Content-Type: multipart/form-data</code>. But httpRequest plugin is not supporting this type. However it supports <code>APPLICATION_OCTETSTREAM(ContentType.APPLICATION_OCTET_STREAM)</code></p>

<p>Could you post output from your curl?</p>
"
AppDynamics,57322375,50618877,3,"2019/08/02, 11:13:00",False,"2019/08/02, 11:13:00",177,277547,1,"<p>Following Code worked for me : </p>

<pre><code>def response =  httpRequest(acceptType: 'APPLICATION_JSON', contentType: 'APPLICATION_ZIP',
                   customHeaders  : [[name: ""authorization"" , value : ""${authToken}""],[name: 'x-username' , value: 'admin']],
                   httpMode: 'POST', ignoreSslErrors: true, 
                   multipartName: '&lt;fileName&gt;', timeout: 900,
                   responseHandle: 'NONE', uploadFile: ""&lt;filePath&gt;"", 
                   url: ""${url}"")
</code></pre>
"
AppDynamics,47084941,47024920,0,"2017/11/02, 23:29:17",True,"2017/11/02, 23:29:17",190,550621,1,"<p>It turns out the code I was given was completely wrong for angular 2 implementation.  The code they gave me is for running on the web server's side with node js.  Since angular 2 is an SPA that runs on the browser, it would never work.
I did some research and found this example application that I added a few tweaks to: <a href=""https://github.com/derrekyoung/appd-sampleapp-angular2"" rel=""nofollow noreferrer"">https://github.com/derrekyoung/appd-sampleapp-angular2</a></p>
"
AppDynamics,47319681,45650349,0,"2017/11/16, 02:37:15",False,"2017/11/16, 02:37:15",4583,756095,1,"<p>in your appdynamics.cfg, change</p>

<pre><code>ssl = (on)
</code></pre>

<p>to</p>

<pre><code>ssl = on
</code></pre>
"
AppDynamics,43387027,41357203,0,"2017/04/13, 11:20:58",False,"2017/04/13, 11:20:58",324,7220665,0,"<p>I am not exactly sure what the problem is, but I just tried it with AppDynamics Controller Version 4.2 to share a custom dashboard and open the shared Link in a different browser and I didn't need to put in any credentials. Maybe AppDynamics Version 3.9 had a bug (assuming you used version 3.9 according to the docs link you posted)</p>

<p>Here is what I did:</p>

<pre><code>Toolbar --&gt; Dashboard &amp; Reports --&gt; Chose a dashboard --&gt; Switch to edit mode --&gt; 
Actions --&gt; Share Dashboard --&gt; Action (again) --&gt; Open Shared URL in new window --&gt; 
Make Browser full screen
</code></pre>
"
AppDynamics,38639601,38635466,3,"2016/07/28, 17:40:52",False,"2016/07/28, 17:40:52",211,5400918,1,"<p>CloudForms is not trying to substitute OpenShift UI, but complement it, giving you more information about the environment and providing additional capabilities on top of OPenShift.</p>

<p>You can see information about what is being done and demos in videos:</p>

<p><a href=""https://www.youtube.com/watch?v=FVLo9Nc_10E&amp;list=PLQAAGwo9CYO-4tQsnC6oWgzhPNOykOOMd&amp;index=15"" rel=""nofollow"">https://www.youtube.com/watch?v=FVLo9Nc_10E&amp;list=PLQAAGwo9CYO-4tQsnC6oWgzhPNOykOOMd&amp;index=15</a></p>

<p>And you can find the presentations here
<a href=""http://www.slideshare.net/ManageIQ/presentations"" rel=""nofollow"">http://www.slideshare.net/ManageIQ/presentations</a></p>
"
AppDynamics,30225687,30191131,1,"2015/05/14, 00:43:25",True,"2015/05/14, 00:43:25",185615,2016562,0,"<p>Just follow this istructions:</p>
<blockquote>
<p>Clone a local copy of this project with
git clone <a href=""https://github.com/Appdynamics/ECommerce-Android"" rel=""nofollow noreferrer"">https://github.com/Appdynamics/ECommerce-Android</a></p>
<p>Open Android Studio and Import Project (select app/build.gradle)</p>
</blockquote>
<p>Android Studio will ask you to build the project with gradle.
Gradle will use the <code>build.gradle</code> inside the project.</p>
<p>The version of gradle is inside the <code>gradle/wrapper/gradle-wrapper.properties</code> file:</p>
<pre><code>distributionUrl=https\://services.gradle.org/distributions/gradle-2.2.1-all.zi
</code></pre>
<p>Check for example the file inside the <a href=""https://github.com/Appdynamics/ECommerce-Android/blob/master/gradle/wrapper/gradle-wrapper.properties"" rel=""nofollow noreferrer"">app</a>:</p>
"
AppDynamics,30268298,30191131,0,"2015/05/15, 23:34:56",False,"2015/05/15, 23:34:56",166,4226112,0,"<p>You need put a directory called ""adeum-maven-repo"" in your project setup .</p>

<p><a href=""https://docs.appdynamics.com/display/PRO39/Instrument+an+Android+Application#InstrumentanAndroidApplication-SetupforGradle"" rel=""nofollow"">https://docs.appdynamics.com/display/PRO39/Instrument+an+Android+Application#InstrumentanAndroidApplication-SetupforGradle</a></p>
"
AppDynamics,27420289,27415368,0,"2014/12/11, 12:14:49",True,"2014/12/11, 12:20:03",628370,139985,1,"<p>Whether this is an issue or not is really up to you to decide.</p>

<ul>
<li><p>On the one hand, a full GC that runs for one second every 10 minutes is not a significant issue for throughput.</p></li>
<li><p>On the other hand, the full GC is probably going to dramatically reduce response times during that those one second windows.  But that may not be important or even relevant to your application.</p></li>
</ul>

<p>The thing I would be worried about is whether your load testing is a realistic test.  The application appears to need 4Gb of heap space under test, but is it also going to need that in a real-world use?  I'd be worried that a memory leak might show up when it is deployed into production.  Or that the load in your load testing is causing the application's in-memory caching to reach a steady state in that won't be reproduced in production. </p>

<p>As a general rule, it is a bad thing for the heap to be running close to full, so an increase in the heap is probably advisable.  Your application's performance doesn't appear to be suffering, but you could be ""on the cusp"".</p>

<hr>

<blockquote>
  <p>I thought that once old gen utilization is above certain level, GC will try to free/compact old gen area and if nothing is freed it would either fail with OOM and start crazy full GC cycles just before it. </p>
</blockquote>

<p>I suspect that the monitoring reports might be misleading you.  If the full GC cycles were <em>really</em> not reclaiming anything, I'd expect the behaviour to be different.</p>

<p>Try turning on JVM GC log message, and see what they tell you about the amount of memory that the full GC cycles are managing to reclaim.</p>
"
AppDynamics,25416702,25134646,0,"2014/08/21, 03:50:57",True,"2014/08/21, 03:50:57",525,2479518,1,"<p>My answer with links got deleted by another SO user so I'm listing the steps here. </p>

<p>First uninstall using this      </p>

<p>$ npm uninstall -g strong-cli
$ npm uninstall -g loopback-sdk-angular-cli</p>

<p>and then install</p>

<p>npm install -g strongloop</p>

<p>You can now run slc strongops </p>

<p>and let us know how it goes.</p>
"
AppDynamics,27635551,25134646,0,"2014/12/24, 12:38:57",False,"2014/12/24, 12:38:57",924,427300,1,"<p>I created a Linux init.d Daemon script which you can use to run your app with slc as service:
<a href=""https://gist.github.com/gurdotan/23311a236fc65dc212da"" rel=""nofollow"">https://gist.github.com/gurdotan/23311a236fc65dc212da</a></p>

<p>Might be useful to some of you.</p>
"
AppDynamics,19326222,19191781,1,"2013/10/11, 22:56:19",False,"2013/10/11, 22:56:19",11,2872398,1,"<p>You should try Compuware's dynaTrace</p>
"
AppDynamics,11687150,11687049,8,"2012/07/27, 14:47:42",True,"2012/07/27, 14:47:42",2990,1554255,3,"<p>i don't think that you can do full-featured profiling of distributed request across number of JVM's - AppDynamics from what i can remember understands the EE stuff - like calling DB, EJB, RMI, or remote webservice - however it still works in scope of JVM.</p>

<p>Isn't it suffient in your case just to use java profiler (like yourkit, jprofiler)?</p>
"
AppDynamics,20809292,11687049,0,"2013/12/28, 01:37:03",False,"2013/12/28, 01:37:03",4378,341191,1,"<p>you can try 24x7monitoring</p>

<p><a href=""https://code.google.com/p/monitor-24x7/"" rel=""nofollow"">https://code.google.com/p/monitor-24x7/</a></p>

<p>it provides method level monitoring, SQL queries, business transactions...</p>
"
AppDynamics,21438317,11687049,0,"2014/01/29, 19:29:28",False,"2014/01/29, 19:29:28",31,3249955,3,"<p>Did you try the free version of AppDynamics. It's called AppDynamics LITE.
You can take a look also to EXTRAHOP free version. Maybe it is good enough for your needs.</p>

<p>Also you can try using a SaaS solutions such as NewRelic or Boundary. They have free accounts that could also be good enough for your needs.</p>

<p>Finally if you want to monitor the performance of any specific JAVA application, you can use <a href=""http://www.moskito.org/"" rel=""nofollow"">http://www.moskito.org/</a>. It's totaly FREE.</p>
"
AppDynamics,7264935,7077885,0,"2011/09/01, 03:19:24",False,"2011/09/01, 03:19:24",251,914642,0,"<p>can you tell me the methodology for deriving those stats?  In most cases, this behavior comes down to poor code implementation in a custom portlet or a JVM configuration issue and more likely the latter.  Take a look at the <a href=""http://ehcache.org/documentation/user-guide/garbage-collection"" rel=""nofollow"">Ehcache Garbage Collection Tuning Documentation</a>.  When you run <code>jstat</code>, how do the garbage collection times look?</p>
"
AppDynamics,44606002,44569260,1,"2017/06/17, 17:54:45",False,"2017/06/17, 17:54:45",474,4682632,1,"<p>In order to capture this data follow these steps:</p>

<ul>
<li>Login to AppDynamics=</li>
<li>Select the application you wish to get these metrics from</li>
<li>Go into Metric Browser (left nav)</li>
<li>Go into overall application performance, in this case number of slow, very slow, stall, errors</li>
<li>For each of these right click on the metric and ""copy rest url""</li>
<li>Use those in your code to pull the data</li>
</ul>

<p>Pointer, you can also adjust the time periods in the metric browser to adjust the time. </p>

<p>Hope this is helpful.</p>
"
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21,5846608,0,"<p>I found this solution <a href=""https://github.com/Appdynamics/flowmap-builder"" rel=""nofollow noreferrer"">https://github.com/Appdynamics/flowmap-builder</a> </p>

<p>Seems to be working so far. Doesn't rely (directly) on APIs but it works!</p>
"
AppDynamics,60129360,60127980,4,"2020/02/08, 19:36:49",False,"2020/02/09, 10:51:22",2705,1501456,0,"<p>The reason you don't see the browser name in HTTP protocol is because there is no browser. The protocol is a transport level protocol which means it simulates the traffic of the browser without running the actual browser. This allows the protocol to simulate many more virtual users than a client level protocol such as TruClient. </p>

<p>EDIT: There is no dedicated API and you must use the user agent. Please refer to this article for more details: <a href=""https://www.appdynamics.com/blog/engineering/how-to-use-appdynamics-with-loadrunner-for-load-testing/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/blog/engineering/how-to-use-appdynamics-with-loadrunner-for-load-testing/</a></p>
"
AppDynamics,58010383,57958059,0,"2019/09/19, 14:50:45",True,"2019/09/19, 14:50:45",41,7611518,0,"<p>For anyone who is looking for an answer here. This is resolved by unchecking the Extended  tracking option for kafka in AppDynamics Console. The option will be there in Automatic Backend Discovery. Hope this helps.</p>
"
AppDynamics,58800924,57958059,0,"2019/11/11, 13:57:56",False,"2019/11/11, 13:57:56",26,12355048,0,"<ol>
<li><p>Disable default Kafka configuration on ""Backend Detection"" page</p></li>
<li><p>Define a new ""Custom Exit Point"" with the last class and method which you see on the call graphs before Kafka exit call</p></li>
<li><p>Don't forget to click on ""Is High Volume"" checkbox.</p></li>
</ol>
"
AppDynamics,44805282,44805162,1,"2017/06/28, 17:23:30",False,"2017/06/28, 17:23:30",333,2534221,0,"<p>You can get severity information by appending &amp;severities=INFO,WARN,ERROR to your URL.</p>

<p>So your url must be like : <a href=""http://:/controller/rest/applications//business-transactions?output=JSON&amp;severities=INFO,WARN,ERROR"" rel=""nofollow noreferrer"">http://:/controller/rest/applications//business-transactions?output=JSON&amp;severities=INFO,WARN,ERROR</a></p>
"
AppDynamics,45253438,44805162,0,"2017/07/22, 13:24:47",False,"2017/07/22, 13:24:47",11,8349426,1,"<p>Severity is associated with specific events/entities in AppDynamics. Based on your API call I can see that you are trying to retrieve information about Business Transactions (BTs). Severity param is not associated with BTs.</p>

<p>e.g. You can pull Severity for Health rule violations in AppDynamics by making the following API call:
http:///controller/rest/applications//problems/healthrule-violations
Result:</p>

<pre><code>&lt;policy-violations&gt;&lt;policy-violation&gt;
  &lt;id&gt;266&lt;/id&gt;
  &lt;name&gt;CPU utilization is too high&lt;/name&gt;
  &lt;startTimeInMillis&gt;1452630655000&lt;/startTimeInMillis&gt;
  &lt;detectedTimeInMillis&gt;0&lt;/detectedTimeInMillis&gt;
  &lt;endTimeInMillis&gt;1452630715000&lt;/endTimeInMillis&gt;
  &lt;incidentStatus&gt;RESOLVED&lt;/incidentStatus&gt;
  **&lt;severity&gt;WARNING&lt;/severity&gt;**
  &lt;triggeredEntityDefinition&gt;
    &lt;entityType&gt;POLICY&lt;/entityType&gt;
    &lt;entityId&gt;30&lt;/entityId&gt;
    &lt;name&gt;CPU utilization is too high&lt;/name&gt;
  &lt;/triggeredEntityDefinition&gt;
....
</code></pre>

<p>You can find further information about using AppD controller API in the following documentation pages:</p>

<p><a href=""https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO42/AppDynamics+APIs</a></p>

<p><a href=""https://docs.appdynamics.com/display/PRO42/Alert+and+Respond+API"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO42/Alert+and+Respond+API</a></p>
"
AppDynamics,47149088,47129599,0,"2017/11/07, 04:33:37",False,"2017/11/07, 04:33:37",474,4682632,0,"<p>Yes, it does transaction tracking for every intra-component call across languages. Without code changes. When there is a slow transaction detail down to code level will show up in snapshots, this is what you'd use for diagnostics. Depending on your language you can also configure data collectors which do runtime instrumentation of custom data from the code. These show up on every call, and you can turn them into metrics too. Once again no code changes.</p>
"
AppDynamics,55810979,55787843,0,"2019/04/23, 15:12:33",False,"2019/04/23, 15:24:27",53,2309810,0,"<p>AppDynamics itself powerful enough to provide all sort of information, However if you still want actuator data to be captured and displayed then you may need to use AppD extension. Please refer below official link to AppDyanmics Exchange.</p>

<p><a href=""https://www.appdynamics.com/community/exchange/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/community/exchange/</a></p>

<p>if the relevant is not available you may need write your own.</p>
"
AppDynamics,44960915,44939278,0,"2017/07/07, 03:51:06",False,"2017/07/07, 03:51:06",7,1858160,0,"<p>The Issue is resolved. 
The controller-info of Machine Agent need not to have any Application,Node and Tier name. It should include unique host id which should be same as controller-info of JavaAgent. </p>
"
AppDynamics,64358686,63590062,0,"2020/10/14, 20:36:25",False,"2020/10/14, 20:36:25",1,14450632,0,"<p>I realize it's been a while, but I'll contribute anyway as it may help other people too.</p>
<p>In my case, importing Instrumentation in iOS caused this error; it seems to be a problem in the latest version of @appdynamics/react-native-agent (version 20.7.0 as of writing).</p>
<p>I instead initialized AppDynamics in native code (in the AppDelegate.m file), as follows:</p>
<pre class=""lang-objectivec prettyprint-override""><code>#import &lt;ADEUMInstrumentation/ADEumInstrumentation.h&gt;

...

- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions {
  
  ...

  ADEumAgentConfiguration *adeumAgentConfig = [[ADEumAgentConfiguration alloc] initWithAppKey:@&quot;YOUR IOS KEY&quot;];

  // AppDynamics swizzles some methods for making requests and may mess up other libraries; disable automatic instrumentation if it causes problems to you.
  adeumAgentConfig.enableAutoInstrument = NO;

  // Initialize AppDynamics
  [ADEumInstrumentation initWithConfiguration:adeumAgentConfig];

  // Leaving screnshots enabled may cause lag during touches; block screenshots if you experience that.
  [ADEumInstrumentation blockScreenshots];

  ...

  return YES;
}
</code></pre>
<p>For more info, check the iOS guide:
<a href=""https://docs.appdynamics.com/display/PRO45/Instrument+an+iOS+Application"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO45/Instrument+an+iOS+Application</a></p>
<p>Additionally, I avoided importing AppDynamics in javascript by requiring it in runtime, in Android only.</p>
<pre class=""lang-js prettyprint-override""><code>if (Platform.OS === 'android') {
    const { Instrumentation } = require('@appdynamics/react-native-agent');
    const appKey = 'YOUR ANDROID KEY';
    console.log(`Starting AppDynamics with key ${appKey}.`);
    Instrumentation.start({
      appKey,
    });
  }
</code></pre>
"
AppDynamics,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213,7933630,1,"<p>There's a lot of different questions here, and some of them don't have answers without more information about your specific setup, but I'll try to give you a good overview.</p>
<p><strong>Why Tracing?</strong></p>
<p>You've already intuited that there are a lot of similarities between &quot;APM&quot; and &quot;tracing&quot; - the differences are fairly minimal. Distributed Tracing is a superset of capabilities marketed as APM (application performance monitoring) and RUM (real user monitoring), as it allows you to capture performance information about the work being done in your services to handle a single, logical request both at a per-service level, and at the level of an entire request (or transaction) from client to DB and back.</p>
<p>Trace data, like other forms of telemetry, can be aggregated and analyzed in different ways - for example, unsampled trace data can be used to generate RED (rate, error, duration) metrics for a given API endpoint or function call. Conventionally, trace data is annotated (tagged) with properties about a request or the underlying infrastructure handling a request (things like a customer identifier, or the host name of the server handling a request, or the DB partition being accessed for a given query) that allows for powerful exploratory queries in a tool like Jaeger or a commercial tracing tool.</p>
<p><strong>Sampling</strong></p>
<p>The overall performance impact of generating traces varies. In general, tracing libraries are designed to be fairly lightweight - although there are a lot of factors that influence this overhead, such as the amount of attributes on a span, the log events attached to it, and the request rate of a service. Companies like Google will aggressively sample due to their scale, but to be honest, sampling is more beneficial to consider from a long-term storage perspective rather than an up-front overhead perspective.</p>
<p>While the additional overhead per-request to create a span and transmit it to your tracing backend might be small, the cost to store trace data over time can quickly become prohibitive. In addition, most traces from most systems aren't terribly interesting. This is why dynamic and tail-based sampling approaches have become more popular. These systems move the sampling decision from an individual service layer to some external process, such as the <a href=""https://github.com/open-telemetry/opentelemetry-collector"" rel=""nofollow noreferrer"">OpenTelemetry Collector</a>, which can analyze an entire trace and determine if it should be sampled in or out based on user-defined criteria. You could, for example, ensure that any trace where an error occurred is sampled in, while 'baseline' traces are sampled at a rate of 1%, in order to preserve important error information while giving you an idea of steady-state performance.</p>
<p><strong>Proprietary APM vs. OSS</strong></p>
<p>One important distinction between something like AppDynamics or New Relic and tools like Jaeger is that Jaeger does not rely on proprietary instrumentation agents in order to generate trace data. Jaeger supports OpenTelemetry, allowing you to use open source tools like the <a href=""https://github.com/open-telemetry/opentelemetry-java-instrumentation"" rel=""nofollow noreferrer"">OpenTelemetry Java Automatic Instrumentation</a> libraries, which will automatically generate spans for many popular Java frameworks and libraries, such as Spring. In addition, since OpenTelemetry is available in multiple languages with a shared data format and trace context format, you can guarantee that your traces will work properly in a polyglot environment (so, if you have Node.JS or Golang services in addition to your Java services, you could use OpenTelemetry for each language, and trace context propagation would work seamlessly between all of them).</p>
<p>Even more advantageous, though, is that your instrumentation is decoupled from a specific vendor or tool. You can instrument your service with OpenTelemetry and then send data to one - or more - analysis tools, both commercial and open source. This frees you from vendor lock-in, and allows you to select the best tool for the job.</p>
<p>If you'd like to learn more about OpenTelemetry, observability, and other topics I wrote a longer series that you can find <a href=""https://dev.to/lightstep/opentelemetry-101-what-is-observability-44m"" rel=""nofollow noreferrer"">here</a> (look for the other 'OpenTelemetry 101' posts).</p>
"
AppDynamics,59198597,59191709,1,"2019/12/05, 17:40:40",False,"2019/12/09, 14:27:54",722,8826818,0,"<p>Windows 10 64-bit. Powershell 5. Does not require admin privileges.</p>

<p>How to quickly and efficiently extract text from a large logfile from input1 to input2 using powershell 5?</p>

<p>Sample logfile below.</p>

<p>For testing purposes copy your logfile to the desktop and name it logfile.txt </p>

<p>What is the default text editor in Windows Server 2012 R2 Standard 64-bit?  See line 42.
What program do you have associated with .txt files?  See line 42.</p>

<pre><code># Get Start Time
$startDTM = (Get-Date)
$zstart = Read-Host -prompt '
Enter your start date in ""10 Nov"" format (w/o quotes). Start date must be earlier than stop date.'
$zstop = Read-Host -prompt 'Enter your stop date in ""13 Nov"" format (w/o quotes).  Stop date must be later than start date.'
# $zstart = '10 Nov'
# $zstop = '13 Nov'
$zstart= Select-String $zstart ""$env:userprofile\Desktop\logfile.txt"" | Select-Object -ExpandProperty LineNumber
$zstop= Select-String $zstop ""$env:userprofile\Desktop\logfile.txt"" | Select-Object -ExpandProperty LineNumber
$AppLog = gc $env:userprofile\Desktop\logfile.txt 
$i = 0
$array = @()
foreach ($line in $AppLog){
foreach-object { $i++ }
if (($i -ge $zstart) -and ($i -le $zstop))
{$array += $line}}
$array | Out-File -encoding ascii -filepath $env:userprofile\Desktop\logfile-edited.txt 
# begin get file size 
$y = (Get-ChildItem ""$env:userprofile\Desktop\logfile.txt"" | Measure-Object -property length -sum)
$y = [System.Math]::Round(($y.Sum /1KB),2) 
$z = (Get-ChildItem ""$env:userprofile\Desktop\logfile-edited.txt"" | Measure-Object -property length -sum)
$z = [System.Math]::Round(($z.Sum /1KB),2) 
#   end get file size 
# get Stop Time
$endDTM = (Get-Date) 
Write-Host ""
Extracted $z KB from $y KB. The extraction took $(($endDTM-$startDTM).totalseconds) seconds""
start-process -wait notepad $env:userprofile\Desktop\logfile-edited.txt
remove-item $env:userprofile\Desktop\logfile-edited.txt 
exit 
</code></pre>

<p>logfile.txt:</p>

<pre><code>[AD Thread-Metric Reporter1] 09 Nov 2019 14:48:32,899 ERROR ManagedMonitorDelegate - Error sending metrics
com.singularity.ee.agent.commonservices.metricgeneration.metrics.MetricSendException: Connection back off limitation
at com.singularity.ee.agent.commonservices.metricgeneration.AMetricSubscriber.publish(AMetricSubscriber.java:350)
at com.singularity.ee.agent.commonservices.metricgeneration.MetricReporter.run(MetricReporter.java:113)
at com.singularity.ee.util.javaspecific.scheduler.AgentScheduledExecutorServiceImpl$SafeRunnable.run ]

[AD Thread-Metric Reporter1] 10 Nov 2019 14:47:32,899 ERROR ManagedMonitorDelegate - Error sending metrics
com.singularity.ee.agent.commonservices.metricgeneration.metrics.MetricSendException: Connection back off limitation
at com.singularity.ee.agent.commonservices.metricgeneration.AMetricSubscriber.publish(AMetricSubscriber.java:350)
at com.singularity.ee.agent.commonservices.metricgeneration.MetricReporter.run(MetricReporter.java:113)
at com.singularity.ee.util.javaspecific.scheduler.AgentScheduledExecutorServiceImpl$SafeRunnable.run ]

[AD Thread-Metric Reporter1] 11 Nov 2019 14:46:32,899 ERROR ManagedMonitorDelegate - Error sending metrics
com.singularity.ee.agent.commonservices.metricgeneration.metrics.MetricSendException: Connection back off limitation
at com.singularity.ee.agent.commonservices.metricgeneration.AMetricSubscriber.publish(AMetricSubscriber.java:350)
at com.singularity.ee.agent.commonservices.metricgeneration.MetricReporter.run(MetricReporter.java:113)
at com.singularity.ee.util.javaspecific.scheduler.AgentScheduledExecutorServiceImpl$SafeRunnable.run ]

[extension-scheduler-pool-5] 13 Nov 2019 18:45:40,634  INFO ReportMetricsConfigSupplier - Basic metrics will be collected 
[extension-scheduler-pool-8] 14 Nov 2019 18:47:18,650  INFO ReportMetricsConfigSupplier - Basic metrics will be collected
</code></pre>

<p>Results of running script on logfile.txt: </p>

<pre><code>[AD Thread-Metric Reporter1] 10 Nov 2019 14:47:32,899 ERROR ManagedMonitorDelegate - Error sending metrics
com.singularity.ee.agent.commonservices.metricgeneration.metrics.MetricSendException: Connection back off limitation
at com.singularity.ee.agent.commonservices.metricgeneration.AMetricSubscriber.publish(AMetricSubscriber.java:350)
at com.singularity.ee.agent.commonservices.metricgeneration.MetricReporter.run(MetricReporter.java:113)
at com.singularity.ee.util.javaspecific.scheduler.AgentScheduledExecutorServiceImpl$SafeRunnable.run ]

[AD Thread-Metric Reporter1] 11 Nov 2019 14:46:32,899 ERROR ManagedMonitorDelegate - Error sending metrics
com.singularity.ee.agent.commonservices.metricgeneration.metrics.MetricSendException: Connection back off limitation
at com.singularity.ee.agent.commonservices.metricgeneration.AMetricSubscriber.publish(AMetricSubscriber.java:350)
at com.singularity.ee.agent.commonservices.metricgeneration.MetricReporter.run(MetricReporter.java:113)
at com.singularity.ee.util.javaspecific.scheduler.AgentScheduledExecutorServiceImpl$SafeRunnable.run ]

[extension-scheduler-pool-5] 13 Nov 2019 18:45:40,634  INFO ReportMetricsConfigSupplier - Basic metrics will be collected 
</code></pre>

<p><a href=""https://www.youtube.com/watch?v=plqpmZruOYk"" rel=""nofollow noreferrer"">Powershell in four hours at Youtube</a></p>

<p><a href=""https://www.bing.com/search?q=Parse%20and%20extract%20text%20with%20powershell"" rel=""nofollow noreferrer"">Parse and extract text with powershell at Bing</a></p>

<p>How to quickly and efficiently extract text from a large logfile from line x to line y using powershell 5?</p>

<p>How to quickly and efficiently extract text from a large logfile from date1 to date2 using powershell 5?</p>
"
AppDynamics,55505955,55505087,0,"2019/04/04, 02:58:03",True,"2019/04/04, 02:58:03",1220,796375,2,"<p>curl's <code>--user</code> command line option sets up HTTP authentication for the request (<code>username:password</code>).</p>

<p>In the case of AppDynamics' Configuration API (which is a subset of their Controller API), <code>user1@customer1:secret</code> are your account credentials in the format documented <a href=""https://docs.appdynamics.com/display/PRO44/Using+the+Controller+APIs#UsingtheControllerAPIs-Authentication"" rel=""nofollow noreferrer"">here</a>:</p>

<ul>
<li><code>customer1</code> is the AppDynamics tenant account name</li>
<li><code>user1</code> is a user in that account</li>
<li><code>secret</code> is the password</li>
</ul>
"
AppDynamics,55002450,54993328,1,"2019/03/05, 14:04:18",False,"2019/03/05, 14:04:18",116702,2897748,0,"<p>You should not be retrieving any embedded resources in your load test, you need to limit the scope of the embedded resources scanning to <strong>your application under test domain only</strong></p>

<p>Any external domains like <code>googleapis</code> or  <code>appdynamics</code> must be excluded via <strong>URLs must match</strong> input (lives at ""Advanced"" tab of the <a href=""https://jmeter.apache.org/usermanual/component_reference.html#HTTP_Request"" rel=""nofollow noreferrer"">HTTP Request</a> sampler, or even better <a href=""https://jmeter.apache.org/usermanual/component_reference.html#HTTP_Request_Defaults"" rel=""nofollow noreferrer"">HTTP Request Defaults</a>)</p>

<p><a href=""https://i.stack.imgur.com/Ef5Km.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ef5Km.png"" alt=""enter image description here""></a></p>

<p>More information: <a href=""https://guide.blazemeter.com/hc/en-us/articles/206732519-Excluding-Domains-from-the-Load-Test"" rel=""nofollow noreferrer"">Excluding Domains from the Load Test</a></p>
"
AppDynamics,44071970,44038577,0,"2017/05/19, 16:57:39",False,"2017/05/19, 16:57:39",474,4682632,0,"<p>The problem with this approach is you'll have to manually do that each time. I would highly recommend just configuring your app server to automatically load the AppDynamics agent. Another option is using the universal agent, which does auto-attach: <a href=""https://docs.appdynamics.com/display/PRO43/Install+the+Universal+Agent"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO43/Install+the+Universal+Agent</a> Doing this one off attach is never really a good idea, as you'll have to get the PID each time. </p>

<p>The error indicates that you are probably not running the attach as the same user the JVM is running under, but it could also be permissions or something else as well, hence I would use the methods that work all the time :)</p>
"
AppDynamics,39731234,39730780,3,"2016/09/27, 20:49:55",False,"2016/09/27, 20:49:55",6408,5004822,0,"<p>Use Dependency Injection instead of creating the actual WCF endpoints and passing them around. Then mocking them up is trivial. You would then use the interface and let DI take care of the rest!</p>
"
AppDynamics,26850353,26778994,2,"2014/11/10, 20:22:08",False,"2014/11/10, 20:22:08",4817,3817025,0,"<p>The best way to logging the requests to an external logging provider.  Check out <a href=""http://docs.cloudfoundry.org/devguide/services/log-management-thirdparty-svc.html"" rel=""nofollow"">http://docs.cloudfoundry.org/devguide/services/log-management-thirdparty-svc.html</a>.  You can actually log to any http endpoint that supports POST.  You can use Splunk to calculate your response time and requests per second.  The logs that come out of that are in real time and are streamed to your logging endpoint.  It contains information about the requests as well as log messages from your app.</p>

<p>ex.</p>

<pre><code>2014-11-10T11:12:47.97-0500 [App/0]   OUT GET / 304 5ms
2014-11-10T11:12:48.36-0500 [App/0]   OUT GET /favicon.ico 404 0ms
</code></pre>
"
AppDynamics,27005164,26778994,2,"2014/11/19, 00:15:20",False,"2014/11/19, 00:15:20",10201,1585136,0,"<p>There are basic stats available when you run <code>cf app &lt;app-name&gt;</code>.  These include the memory, cpu and disk utilization of your app.  You can also access these via the REST api, documented here.</p>

<p><a href=""https://s3.amazonaws.com/cc-api-docs/41397913/apps/get_detailed_stats_for_a_started_app.html"" rel=""nofollow"">https://s3.amazonaws.com/cc-api-docs/41397913/apps/get_detailed_stats_for_a_started_app.html</a></p>

<p>That's not going to help with requests per second or response time though.  @jsloyer's solution would work for that, or you could use an APM like NewRelic, which will give you a wealth of data for virtually nothing.</p>
"
AppDynamics,27018574,26778994,1,"2014/11/19, 15:53:11",True,"2014/11/19, 15:53:11",4817,3817025,1,"<p>Yes this is pretty easy to use.  You can use Logstash as the ingesting engine, you just need the correct parser.  Check out <a href=""http://scottfrederick.cfapps.io/blog/2014/02/20/cloud-foundry-and-logstash"" rel=""nofollow"">http://scottfrederick.cfapps.io/blog/2014/02/20/cloud-foundry-and-logstash</a> for the parser and config for ingesting cloud foundry logs.  I was playing around with this before and it worked quite well.  Let me know if you have any issues.</p>
"
AppDynamics,29093613,29083565,0,"2015/03/17, 09:37:21",True,"2015/03/17, 09:37:21",5170,1242093,0,"<p>AFAIK the critical point for AppDynamics (or profilers like that) it is essential to find an entry point. Usually the prefered way is to have an Servlet ""Endpoint"" that starts a threat and can be followed. 
For the scenario you are describing this wouldn't work as it's missing the ""trigger"" to start the following. Most likely you'll need to build your own app-dynamics monitoring extension for it. </p>
"
AppDynamics,29100312,29083565,2,"2015/03/17, 15:27:06",False,"2015/03/17, 15:27:06",26,2378795,0,"<p>By default much of the Apache stuff is excluded.  Try adding Call Graph Settings (Configure >> Instrumentation >> Call Graph Settings), to include specific transports, like org.apache.camel.component.file.* in the Specific sub-packages / classes from the Excluded Packages to be included in call graphs section.  Do not include org.apache.camel.* as it will instrument all the camel code which is very expensive.  You may want to do it at first to detect what you want to watch, but make sure to change it back.  </p>
"
AppDynamics,29398487,29083565,1,"2015/04/01, 21:48:07",False,"2015/04/01, 21:48:07",26,2378795,0,"<p>Edit AppServerAgent\conf\app-agent-config.xml:</p>

<pre><code>--under--&gt;
&lt;app-agent-configuration&gt;
    &lt;agent-services&gt;
        &lt;agent-service name=""TransactionMonitoringService"" enable=""true""&gt;
            &lt;configuration-properties&gt;
--add--&gt;        &lt;property name=""enable-async-correlation-for"" value=""camel""/&gt;
</code></pre>

<p>From the Controller web site:</p>

<p>Configure >> Instrumentation >> Call Graph Settings
Add Always Shown Package/Class: org.apache.camel.*</p>

<p>Servers >> App Servers >> {tiername} >> {nodename} >> Agents
App Server Agent
Configure
Use Custom Configuration
find-entry-points: true</p>
"
AppDynamics,31367542,31358408,0,"2015/07/12, 15:01:07",False,"2015/07/12, 15:01:07",474,4682632,0,"<p>Business transactions should accomplish this. If you want to report on each web service you can build a report or custom dashboard. If you need more assistance just email help@appdynamics.com</p>
"
AppDynamics,31233300,31219797,0,"2015/07/05, 20:54:05",False,"2015/07/05, 20:54:05",474,4682632,0,"<p>Love to help you out here, but there are no details about the error, please email help@appdynamics.com for assistance. I assume this is C#, but wouldn't know based on this. AppDynamics supports many languages and technologies. </p>
"
AppDynamics,31309166,31219797,0,"2015/07/09, 08:49:40",True,"2015/07/09, 08:49:40",117,2384622,1,"<p>For whom it might be of interest I have found a workaround and more details about this issue. This occurs only in the following scenario:</p>

<ol>
<li>AppDynamics agent is installed and running</li>
<li>ServiceStack API is compiled using the MSBuild from c:\Windows\Microsoft.NET\Framework64\v4.0.30319\MsBuild.exe</li>
</ol>

<p>If you use the MsBuild 14 that is installed along Microsoft Visual Studio 2015 RC then this issue does not occur anymore. From my first findings there is an issue in ServiceStack's way of caching the endpoints and wrapping the execute method using Linq but I don't understand why this happens only when AppDynamics agent is installed.</p>

<p>@mythz if you want to dig deeper into this issue I'm available to help but with the above solution everything is ok.</p>

<p>Hope this helps</p>
"
AppDynamics,42982843,42950752,0,"2017/03/23, 19:21:05",True,"2017/03/23, 19:21:05",474,4682632,1,"<p>You can build multiple baselines within AppDynamics if you'd like to. The thresholds should be auto calculated off deviation from baseline. This makes it so you don't need to configure them manually. If you want to do SLA tracking, Business iQ (analytics) can do this very well. We also are building additional features around SLA use cases we can share. </p>

<p>Feel free to email me or support for a hand. </p>
"
AppDynamics,13198174,13198172,0,"2012/11/02, 17:21:28",True,"2018/05/25, 01:03:27",49920,32453,0,"<p>Turns out if you go to configure -> instrumentation -> JMX tab then voila, you can now delete metrics and modify/edit them. But nowhere else. Odd.</p>
"
AppDynamics,44338324,44269450,0,"2017/06/03, 01:35:15",True,"2017/06/03, 01:35:15",68,1394813,1,"<p>Inside the widget settings make sure you select the ""Stack Areas or Columns"" checkbox. This works in every version of AppD I've used including 4.3.0.2.</p>
"
AppDynamics,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474,4682632,3,"<p>Zipkin only does tracing. APM tools like Appdynamics do other monitoring (browser, mobile, database, server, network). Code-level diagnostics with automated overhead controls and limiters. Don't forget log analytics and transaction analytics. It also collects metrics. </p>

<p>There is a lot more to APM than just tracing, which is what Zipkin does. You could do this with a stack of 20 open source tools, but you'd have to deal with disjointed UIs and data models not to mention the work associated with keeping them all working.</p>
"
AppDynamics,18914810,18895293,0,"2013/09/20, 13:44:18",False,"2013/09/20, 13:44:18",3,2795488,0,"<p>I've instrumented  org.apache.ode.bpel.engine.BpelProcess.handleJobDetails and it is showing me transactions going through Carbon out to the backends. </p>
"
AppDynamics,19523960,19518737,4,"2013/10/22, 19:58:09",False,"2013/10/22, 19:58:09",1,2908075,0,"<p>It is a bit hard to completely answer your question and solve the issue with the provided information. However, I hope my questions below help you to get on the right track.</p>

<p>1.) After making the configuration change, did you also restart the AppDynamics.AgentCoordinator_WindowsService? Without restarting it the new configuration will not be applied to the agent itself.</p>

<p>2.) Also important is your windows service hosting any OOTB support entry point like WCF, ASP.NET MVC4 WebAPI, web services etc.? If not, you need to setup a custom entry point. If you  check out the AppDynamics documentation and search for'POCO Entry Points' you should get onto the right track</p>

<p>3.) In case No.1 &amp; No.2 did not do the trick, could you please attach the config.xml file for review? Or directly reach out to the AppDynamics customer success team.</p>

<p>Kind regards,
Theo</p>

<p>Disclaimer: I work for AppDynamics as part of the Customer Success team.</p>
"
AppDynamics,19524167,19518737,3,"2013/10/22, 20:08:31",False,"2013/10/22, 20:08:31",1,2908075,0,"<p>Just to add to you answer and to clarify a little. Until release 3.7.7 the .NET agent from AppDynamics used the web.config (for IIS based application), App.config for standalone and windows services or the global.config plus some environment variables to configure the agent.</p>

<p>With the 3.7.8 or later release we replaced this with a cleaner truly singly configuration file approach. The configuration file is called config.xml and located in the %ProgramData%\AppDynamics... directory. For any version after 3.7.8 all settings have to be in the config.xml.</p>
"
AppDynamics,19599763,19518737,0,"2013/10/26, 00:32:59",False,"2013/10/26, 00:32:59",11,2047636,0,"<p>You really should take this up with AppDynamics support by filing a ticket or posting in the support forums... <a href=""http://www.appdynamics.com/support/#helptab"" rel=""nofollow"">http://www.appdynamics.com/support/#helptab</a></p>
"
AppDynamics,58800607,58644710,2,"2019/11/11, 13:37:12",False,"2019/11/11, 13:55:20",26,12355048,0,"<p>Configuration->Instrumentation->Transaction Detection->Add</p>

<p>On the ""Split Transactions Using Request Data"" section you must choose ""<em>Specific URI Segments</em>""
Segment Numbers: 1,2,4</p>

<p>In your case transaction name will be ""/data/scenario/job""</p>

<p>Sample Configuration:</p>

<p><a href=""https://i.stack.imgur.com/tYguT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tYguT.png"" alt=""enter image description here""></a></p>
"
AppDynamics,42996563,42361990,0,"2017/03/24, 12:05:58",True,"2017/06/07, 09:23:41",324,7220665,2,"<p>I am not exactly sure what the problem is, but you can share a Dashboard and open this shared Dashboard URL and make the Browser fullscreen. Thats how we do it and it works perfectly (on AppDynamics controller version 4.2).</p>

<pre><code>Toolbar --&gt; Dashboard &amp; Reports --&gt; Chose a dashboard --&gt; 
Switch to edit mode --&gt; Actions --&gt; Share Dashboard --&gt; Action (again) --&gt; 
Open Shared URL in new window --&gt; Make Browser fullscreen
</code></pre>

<p>We use the 'revolver tab' add-on for the browser to switch between desktops.</p>
"
AppDynamics,50270677,50218209,3,"2018/05/10, 13:22:05",False,"2018/05/10, 13:22:05",16918,1047788,0,"<p>The first step is to add a username and password in the etc/users.properties file. For most purposes, it is ok to just 
use the default settings provided out of the box. For this, just uncomment the following line:</p>

<pre><code>admin=admin,admin,manager,viewer,Operator, Maintainer, Deployer, Auditor, Administrator, SuperUser
</code></pre>

<p>Then, you must bypass credential checks on BrokeViewMBean by adding it to the whitelist ACL configuration. You can do so by replacing this line:</p>

<pre><code>org.apache.activemq.Broker;getBrokerVersion=bypass
</code></pre>

<p>with this:</p>

<pre><code>org.apache.activemq.Broker=bypass
</code></pre>

<p>In addition to being the correct way, it also enables several different configuration options (eg: port, listen address, etc) by just changing the file org.apache.karaf.management.cfg on broker's etc directory. </p>

<p>Please keep in mind that JMX access is made through a different JMX connector root in this case: it uses <code>karaf-root</code> instead of <code>jmxrmi</code>, which was previously used in the older method. It also uses port 1099 by default, instead of 1616.</p>

<p>Therefore, the uri should be </p>

<pre><code>service:jmx:rmi:///jndi/rmi://&lt;host&gt;:&lt;port&gt;/karaf-root
</code></pre>
"
AppDynamics,40455823,40452664,0,"2016/11/07, 01:40:56",False,"2016/11/07, 01:40:56",5633,4033292,0,"<p>You need both unit tests and integration tests. Unit tests should not use in database or File, ect. I like to use Spring profiles for my tests. For instance, if I have a profile called integeration_test. </p>

<pre><code>@ActiveProfiles(""integeration_test"")
@ContextConfiguration(locations = {
        ""classpath:you-context.xml""})
@RunWith(SpringJUnit4ClassRunner.class)
public abstract class DaoTest
{

    @Autowired
    protected DataSource dataSource;

    // delete all your stuff here
    protected void clearDatabase()
    {
        JdbcTemplate jdbc = new JdbcTemplate(dataSource);
        jdbc.execute(""delete table"");
    }

    @Before
    public final void init()
    {
        clearDatabase();
    }

    @After
    public final void cleanup()
    {
        clearDatabase();
    }

}
</code></pre>

<p>(I'm using xml) then in your context do something like: <code>&lt;beans profile=""test""&gt;TODO &lt;/beans&gt;</code> And configure your data-source in there.</p>

<p>I know there are ways to rollback all your transactions after running a test, but I like this better. Just don't delete everything in your real database haha, could even put some safety code in the clearDatabase to make sure that doesn't happen.</p>

<p>For performance testing you will really need to figure out what you want to achieve, and what is meaningful to display. If you have a specific question about performance testing you can ask that, otherwise it is too broader topic.</p>

<p>Maybe you can make a mini-webapp which does performance testing for you and has the results exposed as URL requests for displaying in HTML. Really just depends on how much effort you are willing to spend on it and what you want to test.</p>
"
AppDynamics,64280827,40452664,0,"2020/10/09, 16:13:06",False,"2020/10/09, 16:13:06",556,1086679,0,"<p>Once the agent is attached you can use the <a href=""https://docs.appdynamics.com/display/PRO45/Database+Queries+Window"" rel=""nofollow noreferrer"">AppDynamics Database Queries Window</a></p>
"
AppDynamics,66421794,65134789,0,"2021/03/01, 14:02:52",False,"2021/03/01, 14:02:52",3170,1237595,0,"<p>Yes - you can do this using ADQL / Analytics searches (assuming you have this licensed).</p>
<p>Without specifics I can only give you a general guide:</p>
<ul>
<li>Ensure your API / Applications are instrumented - so response timings are captured</li>
<li>Enable Transaction Analytics (for the relevant Business Transactions / Application)</li>
<li>In Analytics use ADQL (<a href=""https://docs.appdynamics.com/display/PRO21/ADQL+Reference"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/ADQL+Reference</a>) to actually do the Apdex calculation - and create an analytics metric from this per Business Transaction / Application (<a href=""https://docs.appdynamics.com/display/PRO21/Create+Analytics+Metrics+From+Scheduled+Queries"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Create+Analytics+Metrics+From+Scheduled+Queries</a>)</li>
<li>The metric can then be used in any Dashboards / Reports / Health Rule etc or exported using the Analytics API (<a href=""https://docs.appdynamics.com/display/PRO21/Analytics+Events+API"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Analytics+Events+API</a>)</li>
</ul>
"
AppDynamics,58801167,57977882,0,"2019/11/11, 14:16:20",True,"2019/11/11, 14:16:20",26,12355048,1,"<p>If we take the example of an ECommerce Application:</p>

<p><strong>Business Transactions</strong> are Checkout, Landing Page, Add to Cart etc. which are known by every end user of the application. These business transactions cover all the method executions, database calls, web service calls etc.</p>

<p><strong>Service End Points</strong> are the sub calls(method call or web service call) execute inside of the Business Transactions. Such as ""Check Inventory"" service which is executed in Checkout and Add to Cart transactions as well.</p>

<p><strong>Information Points</strong> are the key business or technical metric counts, such as Checkout amount, Add to Cart item count.</p>

<p>Service End Points and Information Points only give you performance metrics but Business Transactions also give you full code visibility with ""call graphs""</p>

<p>Also, there are some limitations like max 200 Business Transaction on the default but you can change these rules.</p>

<p>While configuring the BTs &amp; SEs, you must focus on the needs of AppDynamics users. If you configure AppDynamics for mostly business teams, I can use BTs like I described above. But If you target the Dev and Ops teams, you can configure your BTs based on method or service calls.</p>

<p>There is no only single approach on BT &amp; SE configuration. You must shape that with the needs of your AppDynamics users.</p>
"
AppDynamics,41773005,41764607,2,"2017/01/20, 23:58:40",False,"2017/01/20, 23:58:40",474,4682632,1,"<p>That's the beauty of APM is you don't need to deal with logging to get performance data. APM tools instrument the applications themselves regardless of what the code does (logging, generating metrics, etc). AppDynamics can collect log data, and provide similar capabilities to what a log analytics tool can do, but it's of less value than the transaction tracing and instrumentation you get. Your application has to be built in a supported language (Java, .NET, PHP, Python, C++, Node.js) and if it's web or mobile based you can also collect client side performance data and unify between both frontend and backend. If you have questions just reach out and I can answer them for you. Good luck!</p>
"
AppDynamics,42998075,41764607,0,"2017/03/24, 13:14:03",False,"2017/03/24, 13:14:03",324,7220665,1,"<p>You basically need the AppDynamics Controller and a AppDynamics Machine-Agent which will be installed on the machine to monitor. In the Machine-Agent configuration you set the URI of the controller and the agent starts to report machine metrics to the controller. Then you can configure alarms, see metrics, create dashboards, etc. I can deliver you more information if you want, but as Jonah Kowall said, take a look at the documentation as well <a href=""https://docs.appdynamics.com/display/PRO42/Standalone+Machine+Agent"" rel=""nofollow noreferrer"">AppDynamics Machine Agent Doc</a></p>
"
AppDynamics,44411981,44391470,1,"2017/06/07, 14:55:46",True,"2017/06/07, 14:55:46",474,4682632,1,"<p>SQL 2005 is supported, but this was a bug which was introduced in version 4.3.0. There is currently a diagnostic patch for this issue for supported customers. The fix should be in the next patch level once we isolate the issue. If you'd like support just email help@appdynamics.com and they can assist. Thanks.</p>
"
AppDynamics,55811490,55332537,1,"2019/04/23, 15:42:30",False,"2019/04/23, 15:42:30",53,2309810,0,"<p>Due to its proprietary nature i don't think the internals are available for anyone to view or discuss.</p>

<p>Below link might give an idea of how the product works at a high level.</p>

<p><a href=""https://www.appdynamics.com/product/how-it-works/"" rel=""nofollow noreferrer"">https://www.appdynamics.com/product/how-it-works/</a></p>
"
AppDynamics,67137278,66872969,0,"2021/04/17, 13:46:07",False,"2021/04/17, 13:46:07",3170,1237595,0,"<p>From checking out the code for Istio - <a href=""https://github.com/istio"" rel=""nofollow noreferrer"">https://github.com/istio</a> - this appears to be an application with components written in Go and C++ which are deployed to containers using Kubernetes.</p>
<p>This would mean that for monitoring you should be looking at:</p>
<ul>
<li>Go SDK - <a href=""https://docs.appdynamics.com/display/PRO21/Go+SDK"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Go+SDK</a> - To instrument Go elements</li>
<li>C/C++ SDK - <a href=""https://docs.appdynamics.com/display/PRO21/CCPP+SDK"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/CCPP+SDK</a> - To instrument C++ elements</li>
<li>Cluster Agent - <a href=""https://docs.appdynamics.com/display/PRO21/Monitor+Kubernetes+with+the+Cluster+Agent"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Monitor+Kubernetes+with+the+Cluster+Agent</a> - to monitor the containers</li>
</ul>
<p>You have you work cut out for you here - both SDK's would require changes to the code being ran to get visibility.</p>
"
AppDynamics,52864552,42719255,0,"2018/10/18, 01:37:57",False,"2018/10/18, 01:37:57",1,10521093,0,"<p>Try adding the line below in the crx-quickstart/conf/sling.properties:</p>

<p><code>org.osgi.framework.bootdelegation=com.singularity.*,com.yourkit.*, ${org.apache.sling.launcher.bootdelegation}</code></p>
"
AppDynamics,58802165,55234879,0,"2019/11/11, 15:18:20",False,"2019/11/11, 15:18:20",26,12355048,0,"<p>You can export the dashboard and you will get the json version of your dashboard.(The export button is located top of the your dashboard page)</p>

<p>This json file can be easily editable with any editor and you can replace the Application and/or other properties which contain on of your dashboards.</p>

<p><a href=""https://docs.appdynamics.com/display/PRO45/Import+and+Export+Custom+Dashboards+and+Templates+Using+the+UI"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO45/Import+and+Export+Custom+Dashboards+and+Templates+Using+the+UI</a></p>
"
AppDynamics,28696479,28692916,2,"2015/02/24, 14:58:43",False,"2015/02/24, 14:58:43",72,3641322,-1,"<p>There are many things you should analyse. It depends on what your LR portal uses most. You may want to analyse web content , user, groups, calnder, theme related services.</p>
"
AppDynamics,60203721,60082614,0,"2020/02/13, 10:51:30",False,"2020/02/13, 10:51:30",11,2131858,0,"<p>On analyzing the heap dump taken during system idle state, we only see various WebAppClassLoaders holding instances of different library classes.</p>

<p>This pattern is also explained in official blogs of APM experts like <a href=""https://plumbr.io/blog/memory-leaks/memory-leaks-fallacies-and-misconceptions"" rel=""nofollow noreferrer"">Plumbr</a> and <a href=""https://www.datadoghq.com/blog/tomcat-architecture-and-performance/#jvm-memory-usage"" rel=""nofollow noreferrer"">Datadog</a> as a sign of healthy JVM where regular GC activity is occurring and they explain that it means none of the objects will stay in memory forever.</p>

<p><br>From Plumbr blog:</p>

<blockquote>
  <p>Seeing the following pattern is a confirmation that the JVM at question is definitely not leaking memory. 
  The reason for the double-sawtooth pattern is that the JVM needs to allocate memory on the heap as new objects are created as a part of the normal program execution. Most of these objects are short-lived and quickly become garbage. These short-lived objects are collected by a collector called “Minor GC” and represent the small drops on the sawteeth.</p>
</blockquote>
"
AppDynamics,52641688,49509430,0,"2018/10/04, 10:52:28",False,"2018/10/04, 10:52:28",21,3348861,0,"<p>It seems that you schould be able to define your custom <strong>Asynchronous Transaction Demarcator</strong> as described in:<br/> <a href=""https://docs.appdynamics.com/display/PRO44/Asynchronous+Transaction+Demarcators"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO44/Asynchronous+Transaction+Demarcators</a></p>

<p>which will point to the last method of Runnable that you passes to the Executor. Then according to the documentation all you need is to attach the Demarcator to your Business Transaction and it will collect the asynchronous call.</p>
"
AppDynamics,45979323,44776999,0,"2017/08/31, 13:33:52",False,"2017/08/31, 13:33:52",676,7761024,0,"<p>Couple of things:</p>

<p>I think the general URL format for app dynamics applications are (notice the '#'):</p>

<pre><code>url ='http://10.201.51.40:8090/controller/#/rest/applications?output=JSON'
</code></pre>

<p>Also, I think the requests.get method needs an additional parameter for the 'account'. For instance, my auth format looks like:</p>

<pre><code>auth = (_username + '@' + _account, _password)
</code></pre>

<p>I am able to get a right response code back with this config. Let me know if this works for you.</p>
"
AppDynamics,50472163,44776999,0,"2018/05/22, 19:09:38",False,"2018/05/22, 21:41:45",6,8195498,-1,"<p><strong>You could also use native python code for more control:</strong></p>

<p>example:</p>

<pre><code>import os
import sys
import urllib2
import base64

# if you have a proxy else comment out this line
proxy = urllib2.ProxyHandler({'https': 'proxy:port'}) 

opener = urllib2.build_opener(proxy)
urllib2.install_opener(opener)

username = ""YOUR APPD REST API USER NAME""
password = ""YOUR APPD REST API PASSWORD""

#Enter your request
request = urllib2.Request(""https://yourappdendpoint/controller/rest/applications/141/events?time-range-type=BEFORE_NOW&amp;duration-in-mins=5&amp;event-types=ERROR,APPLICATION_ERROR,DIAGNOSTIC_SESSION&amp;severities=ERROR"")

base64string = base64.encodestring('%s:%s' % (username, password)).replace('\n', '')
request.add_header(""Authorization"", ""Basic %s"" % base64string)
response = urllib2.urlopen(request)

html = response.read()
</code></pre>

<h1>This will get you the response and you can parse the XML as needed.</h1>

<p>If you prefer it in JSON simply specify it in the request.</p>
"
AppDynamics,39897499,39540605,0,"2016/10/06, 16:34:01",False,"2016/10/06, 16:34:01",1247,1232692,1,"<p>They do different things.  ELK will give you log aggregation that you can add in other functionality.  Appdynamics is great for real time monitoring and profiling. I think it depends on what you're going for.  Logging a distributed system and capturing error messages in one place might be very helpful with ELK.  Not just that, but ELK can be used in a number of other ways.  Elasticsearch can be used stand alone as a search engine or data cache.</p>

<p>TL;DR  It depends on what you're doing.  Maybe yes...maybe no...</p>
"
AppDynamics,47382243,47378215,1,"2017/11/19, 23:34:58",False,"2017/11/19, 23:34:58",127,7526329,-2,"<p>It's actually more the other way around - Crittercism does crash logging, network monitoring, and performance timing whereas AppDynamics is more focused on server monitoring.</p>
"
AppDynamics,47398459,47378215,0,"2017/11/20, 20:08:39",True,"2017/11/20, 20:08:39",474,4682632,0,"<p>Both products essentially do the same thing, if anything AppDynamics has advanced beyond Crittercism (now called Apteligent). They had the lead when the mAPM market started, but now there is not much innovating happening, especially after they sold the company to VMware earlier this year. </p>

<p>Here are the mRUM docs for AppD: <a href=""https://docs.appdynamics.com/display/PRO44/Mobile+Real+User+Monitoring"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO44/Mobile+Real+User+Monitoring</a> some of the more advanced features in AppD you will not find in Appteligent would be things like screen recording, breadcrumb/navigation for every click and interaction. The most valuable feature, of course, is the measurement outside of a straight mobile use case. AppD does measure and ties together the mobile transaction with the backend, server (and network), Docker containers, AWS or other cloud providers. Additionally, it monitors performance and usage of browsers. </p>

<p>Disclosure: I used to be the lead for these products at Gartner, and have been working at AppDynamics for the last 3 years. </p>
"
AppDynamics,50738776,47378215,0,"2018/06/07, 13:19:23",False,"2018/06/07, 13:25:10",2304,1162044,0,"<p>For Apple platforms I recommend you to avoid any third party crash log frameworks and use <a href=""https://help.apple.com/xcode/mac/8.0/#/dev861f46ea8"" rel=""nofollow noreferrer"">Xcode crashes organizer.</a></p>

<ul>
<li>Your app can be smaller and faster</li>
<li>There's no setup</li>
<li>You will be able to see what line of code caused crash</li>
<li>Better privacy for your users as it's opt-in</li>
<li>It's free</li>
</ul>
"
AppDynamics,66421156,66096667,0,"2021/03/01, 13:19:27",False,"2021/03/01, 13:19:27",3170,1237595,0,"<p>All currently available AppDynamics APIs are documented here: <a href=""https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/AppDynamics+APIs</a></p>
<p>The main page includes a summary of what is available.</p>
"
AppDynamics,38459237,38454851,0,"2016/07/19, 15:55:16",False,"2016/07/19, 15:55:16",474,4682632,0,"<p>There are lots of APIs available out of the box, you can find the docs at <a href=""https://docs.appdynamics.com/"" rel=""nofollow"">https://docs.appdynamics.com/</a> not everything has an API. You should find the user management as part of the configuration API here : <a href=""https://docs.appdynamics.com/display/PRO42/Configuration+API"" rel=""nofollow"">https://docs.appdynamics.com/display/PRO42/Configuration+API</a> </p>
"
AppDynamics,65580996,63517278,0,"2021/01/05, 16:39:58",False,"2021/01/05, 16:39:58",3170,1237595,0,"<p>The Drop-off rate is the percentage of user sessions which reach the specific page and then end their session (they do not reach any further pages as part of the session and therefore &quot;drop-off&quot; the map).</p>
<p>This can be seen in the example (image) in the <a href=""https://docs.appdynamics.com/display/PRO45/Experience+Journey+Map"" rel=""nofollow noreferrer"">documentation</a></p>
"
AppDynamics,57925017,57912322,0,"2019/09/13, 16:56:48",False,"2019/09/13, 16:56:48",21,5846608,0,"<p>I found this solution <a href=""https://github.com/Appdynamics/flowmap-builder"" rel=""nofollow noreferrer"">https://github.com/Appdynamics/flowmap-builder</a> </p>

<p>Seems to be working so far. Doesn't rely (directly) on APIs but it works!</p>
"
AppDynamics,65073399,65072887,0,"2020/11/30, 14:14:21",False,"2020/11/30, 14:14:21",116702,2897748,0,"<p>It's hard to say what's wrong without seeing your Jenkins job and JMeter <a href=""https://jmeter.apache.org/usermanual/component_reference.html#Thread_Group"" rel=""nofollow noreferrer"">Thread Group</a> configuration.</p>
<p>In order to apply external settings in JMeter you need to define threads, ramp-up and the number of iterations using JMeter Properties via <a href=""https://jmeter.apache.org/usermanual/functions.html#__P"" rel=""nofollow noreferrer"">__P() function</a> like:</p>
<p><a href=""https://i.stack.imgur.com/1sYOx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1sYOx.png"" alt=""enter image description here"" /></a></p>
<p>Once done you will be able to <a href=""https://jmeter.apache.org/usermanual/get-started.html#override"" rel=""nofollow noreferrer"">override the values of these properties using <code>-J</code> command line argument</a>, for example in Jenkins:</p>
<pre><code>jmeter -Jthreads=1 -Jrampup=1 -Jloops=40 -n -t test.jmx -l result.jtl
</code></pre>
<p><a href=""https://i.stack.imgur.com/73inz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/73inz.png"" alt=""enter image description here"" /></a></p>
<p>This way you will be able to pass whatever number of virtual users/iterations without having to change your script.</p>
<p>More information: <a href=""https://www.blazemeter.com/blog/apache-jmeter-properties-customization"" rel=""nofollow noreferrer"">Apache JMeter Properties Customization Guide</a></p>
"
AppDynamics,64909534,64881926,0,"2020/11/19, 12:13:48",False,"2020/11/19, 12:13:48",615,2319439,0,"<p>just to make it clear, a <code>live connection</code> is (in the world of Power BI 😉) a connection to either a Power BI dataset or a SSAS tabular model.<br />
I think what you are looking for is <code>DirectQuery</code>, but it is currently not supported for URL GET commands.<br />
<a href=""https://docs.microsoft.com/en-us/power-bi/connect-data/power-bi-data-sources"" rel=""nofollow noreferrer"">MS docs - Power BI data sources</a></p>
<p>To get realtime data you need to use one of the supported sources. maybe AppDynamics supports direct DB access. DirectQuery is supported by SQL databases.<br />
Another way is to offload the data to a supported source eg. SQL-db or CDS-service and then connect your pbi to that source.</p>
"
AppDynamics,64443455,64442861,0,"2020/10/20, 13:40:45",True,"2020/10/20, 13:49:24",2274,6266192,1,"<p>The correct and hard way is to amend the SELinux boolean on Tomcat directories to allow Tomcat amend files created by other users.
Read <a href=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/chap-security-enhanced_linux-selinux_contexts"" rel=""nofollow noreferrer"">here</a>.</p>
<p>The easy and dirty solution is to <code>start-up process includes a step for renaming yesterdays AppDynamics logs</code> as user <code>sysXYZ</code> . And that way you avoid the problem.</p>
<p>Use <code>su - sysXYZ &lt;script&gt;</code> command or <code>sudo -iu sysXYZ &lt;script&gt;</code> command or <code>sudo -t &lt;Tomcat role&gt; &lt;script&gt;</code></p>
<p>Good luck.</p>
"
AppDynamics,55963734,55948183,0,"2019/05/03, 08:23:29",False,"2019/05/03, 08:23:29",4861,192923,0,"<p>Add <code>CheckedParameter=false</code> in the connection string to fix the issue. </p>
"
AppDynamics,55058358,55057999,0,"2019/03/08, 09:09:07",False,"2019/03/08, 09:09:07",129,6226761,1,"<pre><code>@Aspect
@Component
public class FunctionTraceAspect {
    @Before(""execution(* some.package.*.*.*(..))"")
    public void appendUserContext(JoinPoint joinPoint) {
        System.out.println(""TRACE: "" + joinPoint.getSignature());
   }
}
</code></pre>
"
AppDynamics,53293120,53271297,0,"2018/11/14, 06:16:21",False,"2018/11/14, 06:16:21",342,1394897,0,"<p>I found my answer on the Flow Force online help (<a href=""https://manual.altova.com/flowforceserver/flowforceserver/"" rel=""nofollow noreferrer"">https://manual.altova.com/flowforceserver/flowforceserver/</a>)</p>

<p>The Flow Force is deployed as two servers, which in a window env, can be started and stopped as windows services (can be found via ""Control Panel"">""Administrative Tools"">Services). With this information, I can monitor them via NAGIOS.</p>
"
AppDynamics,51469112,51468443,0,"2018/07/22, 23:45:36",False,"2018/07/22, 23:45:36",45031,213136,0,"<p>One possibility is that you've got a cached execution plan which works fine for most parameter values, or combination of parameter values, but which fails badly for certain values/combinations. You can try adding a non-filtering predicate such as <code>1 = 1</code> to your WHERE clause. <a href=""https://oracle-randolf.blogspot.com/2009/02/how-to-force-hard-parse.html"" rel=""nofollow noreferrer"">I've read</a> but haven't tested that this can be used to force a hard parse, but it may be that you need to change the value (e.g. <code>1 = 1</code>, <code>2 = 2</code>, <code>3 = 3</code>, etc) for each execution of your query.</p>
"
AppDynamics,48974838,48974684,0,"2018/02/25, 16:57:04",False,"2018/02/25, 17:12:50",49358,6779307,1,"<p>Here's a recursive solution that yields key ""paths"".  The <code>(*keys, k)</code> syntax is available in Python versions >= 3.5, you can also use <code>keys + (k,)</code></p>

<pre><code>from collections.abc import Collection

def find(dictionary, value, keys=()):
    for k, v in dictionary.items():
        path_to = (*keys, k)
        if v == value:
            yield path_to
        elif isinstance(v, dict):
            yield from find(v, value, path_to)
        elif isinstance(v, Collection): # might make more sense to use Container, I'm not sure
            if value in v:
                yield path_to


print(list(find(metric_app_mapper, 'art')))
# [('appdynamics', 'avgresptime')]
</code></pre>
"
AppDynamics,48983005,48974684,0,"2018/02/26, 08:59:10",False,"2018/02/26, 08:59:10",2251,5550284,0,"<p>As a little modifcation to @Jon Clements code, this is what gives me what I need.</p>

<pre><code>interpreted_tool = [[k1, k2] for k1, v1 in metric_app_mapper.items() for k2, v2 in v1.items() for v3 in v2 if v3 == metric]
</code></pre>
"
AppDynamics,48983245,48974684,0,"2018/02/26, 09:16:56",False,"2018/02/26, 09:16:56",1513,7352806,0,"<p><strong>Try this:-</strong></p>

<pre><code>for i,j in metric_app_mapper.items():
    for k,l in j.items():
        if ""art"" in l:
            print(k,i)
</code></pre>
"
AppDynamics,47754925,47754856,0,"2017/12/11, 16:20:02",True,"2017/12/11, 16:20:02",5644,9908141,1,"<p><strong>Application Performance Management(APM)</strong></p>

<p>In simpler terms:</p>

<blockquote>
  <p>APM monitors the speed at which transactions are performed both by
  end-users and by the systems and network infrastructure that support a
  software application, providing an end-to-end overview of potential
  bottlenecks and service interruptions.</p>
</blockquote>

<p>In pragmatic terms, this typically involves the use of a suite of software tools—or a single integrated SaaS or on-premises tool—to view and diagnose an application’s speed, reliability, and other performance metrics in order to maintain an optimal level of service. Here is
<a href=""https://en.wikipedia.org/wiki/Application_performance_management"" rel=""nofollow noreferrer"">Wiki description.</a></p>
"
AppDynamics,43064247,42076632,0,"2017/03/28, 11:21:22",False,"2017/03/28, 11:21:22",324,7220665,0,"<p>You don't need a machine agent extension (and additional custom metrics) to capture if a JVM goes down. The machine agent delivers these kind of information out of the box. You have to start the java process with the app server agent and associate the application, that is instrumented with the machine agent. Now you can go to the <code>application dashboard -&gt; Events tab</code> and check for the <code>JVM Crash Event</code>. See also <a href=""https://docs.appdynamics.com/display/PRO42/Monitor+JVMs"" rel=""nofollow noreferrer"">here</a> and <a href=""https://docs.appdynamics.com/display/PRO42/JVM+Crash+Guard"" rel=""nofollow noreferrer"">here</a>.</p>
"
AppDynamics,36874297,36871506,0,"2016/04/26, 22:41:42",False,"2016/04/26, 22:41:42",14666,2067527,1,"<p>I'm not familiar with the AppDynamics output. I assume that's a cumulative view of Threads and their sleep times. So Threads get reused and so the sleep times add up. In some cases, a Thread gets a connection directly, without any waiting and in another call the Thread has to wait until the connection can be provided. The wait duration depends on when a connection becomes available, or the wait limit is hit. </p>

<h3>Let's have a practical example:</h3>

<p>Your screenshot shows a Thread, which waited <code>172ms</code>. Assuming the <code>sleep</code> is only called within the Jedis/Redis invocation path, the Thread waited <code>172ms</code> in total to get a connection.</p>

<p>Another Thread, which waited <code>530ms</code> looks to me as if the first attempt to get a connection wasn't successful (which explains the first <code>500ms</code>) and on a second attempt, it had to wait for <code>30ms</code>. It could also be that it waited 2x for <code>265ms</code>.</p>

<p><em>Sidenote:</em></p>

<p>1000+ connections could severely limit scalability. Spring Data Redis also supports other drivers which don't require pooling but work with fewer connections (see <a href=""http://docs.spring.io/spring-data/redis/docs/current/reference/html/#redis:connectors:lettuce"" rel=""nofollow"">Spring Data Redis Connectors</a> and <a href=""https://github.com/mp911de/lettuce"" rel=""nofollow"">here</a>).</p>
"
AppDynamics,36460104,36459842,7,"2016/04/06, 22:01:39",False,"2016/04/06, 22:01:39",6806,807193,0,"<p>You are getting this error as you have not provided the  values required by app to work. You need to add the values in <code>PreferenceConstants</code> .</p>

<pre><code>public final class PreferenceConstants {
    public final static String END_POINT_URL = """"; //Put appropriate value here
    public static final String EUM_APP_KEY = """"; //Put appropriate value here
    public static final String EUM_COLLECTOR_URL = """"; //Put desired value here
}
</code></pre>
"
AppDynamics,38212864,36459842,0,"2016/07/06, 00:16:07",False,"2016/07/06, 01:16:50",4169,6146338,0,"<p>Did you get it working? Actually, you know. You are trying to connect to server and app key is the one generated at server and has to be implemented at end application. Since this is a template, there is no server allocated by the code writer to test and so you don't have app key. Try exploring IoT frameworks by IBM any other or even you can try open source, Kaa. It will be worth if want explore this kind of cloud dependent apps.</p>
"
AppDynamics,34131925,34103927,2,"2015/12/07, 12:58:09",False,"2015/12/07, 12:58:09",11383,18591,0,"<p>This sounds like you open a transaction and get a database connection always and only after check cache content.</p>

<p>But since you have opened the transactional context, it will be closed, issuing the commit, as no exceptions happened.</p>

<p>You most likely need to move the checking of the cache outside of the transactional context.</p>

<p>Of course, confirming this depends on your application code not included in the question.</p>
"
AppDynamics,34137555,34103927,0,"2015/12/07, 17:44:30",True,"2015/12/07, 17:44:30",1220,526781,1,"<p>The transaction demarcation related invokes on the <code>Connection</code> will happen nevertheless. You could use something like Spring's <code>LazyConnectionDataSourceProxy</code> (doc'ed <a href=""http://docs.spring.io/spring/docs/4.2.4.BUILD-SNAPSHOT/javadoc-api/org/springframework/jdbc/datasource/LazyConnectionDataSourceProxy.html"" rel=""nofollow"">here</a>) to avoid having these sent when they are not required.</p>
"
AppDynamics,28040996,28027105,4,"2015/01/20, 10:48:34",False,"2015/01/20, 10:48:34",13083,1880810,0,"<p>The <code>asadmin</code> manual page says the following:</p>

<blockquote>
  <p>For the Windows operating system in single mode, a backslash is
  required to escape the colon and the backslash characters.</p>
</blockquote>

<p>So try the following:</p>

<pre><code>-javaagent:C\:\\AppServerAgent\\javaagent.jar
</code></pre>

<p><strong>See also:</strong> </p>

<ul>
<li><a href=""http://docs.oracle.com/cd/E18930_01/html/821-2416/gepzd.html"" rel=""nofollow"">Oracle GlassFish Server 3.1 Administration Guide - Administering JVM Options </a></li>
</ul>
"
AppDynamics,29370213,29346979,0,"2015/03/31, 16:51:25",True,"2015/03/31, 16:51:25",602,3453371,0,"<p>Can I ask if your trial is beyond 15 days? According to their site the trial only includes monitoring for one agent beyond that time - that would explain why only your first agent delivers data. Other tools in that space, e.g: Dynatrace free trial - gives you a bit more time if that's what you need and also allow you to extend the free trial if you need more time</p>
"
AppDynamics,30553280,29346979,0,"2015/05/31, 05:34:32",False,"2015/05/31, 05:34:32",474,4682632,0,"<p>You have gone beyond the 15 day pro trial and you are using the free product. If you want to buy the product you can get an extension on the trial, if you do not want to buy the product it will not trace across JVMs. </p>
"
AppDynamics,61510109,61509776,0,"2020/04/29, 22:29:43",False,"2020/04/29, 22:29:43",6328,10726850,1,"<p>In theory, yes: if Resource Manager has been enabled it could be the case that different Resource Manager plans have such an impact but experience shows that this feature is seldom used. </p>

<p>In practive this kind of difference can have many cause:-</p>

<ul>
<li>different SQL statements run</li>
<li>data is different</li>
<li>database statistics differences</li>
<li>different database configuration</li>
<li>different hardware</li>
<li>etc.</li>
</ul>

<p>The first thing to look at database level is something similar to Statspack report (or AWR if licensing allows) to compare database configuration and activity.</p>

<p>And don't forget that application performance is not only database performance it depends also on application server, network and front-end.</p>
"
AppDynamics,37659288,37658175,0,"2016/06/06, 16:52:28",False,"2016/06/06, 17:01:30",5857,3641043,0,"<p>I think this should do what you want. I'm afraid it's not tested, but the principle is this:</p>

<p>1) if the Set-Cookie header starts with ADRUM, then set the env variable ADRUM_cookie.</p>

<p>2) if the ADRUM_cookie env variable is /not/ set, edit the Set-Cookie header.</p>

<pre><code>SetEnvIf Set-Cookie ^ADRUM ADRUM_cookie
Header edit Set-Cookie ^(.*)$ $1;HttpOnly;Secure env=!ADRUM_cookie
</code></pre>
"
AppDynamics,32768918,32768813,10,"2015/09/24, 22:00:00",True,"2016/02/03, 19:13:19",3381,868947,1,"<p>If we are talking about ""run tool-get result"" the best option - <a href=""http://www.oracle.com/technetwork/java/javaseproducts/mission-control/java-mission-control-1998576.html"" rel=""nofollow"">Java Mission Control</a>. It's free in test environment. You need to pay only for some features in production. It's much better than old VisualVM.</p>

<p>You can write a data to file using <a href=""http://docs.oracle.com/javacomponents/jmc-5-5/jfr-runtime-guide/run.htm#CHDIDCHG"" rel=""nofollow"">Flight Recorder</a>. You can setup start point and duration. You just need to start your application like this:</p>

<p><code>-XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:StartFlightRecording=duration=60s,filename=myrecording.jfr</code></p>
"
AppDynamics,67107284,66992681,0,"2021/04/15, 14:03:49",False,"2021/04/15, 14:03:49",3170,1237595,0,"<p>I think what you are wanting is info around instrumentation of the front end of an Angular SPA.</p>
<p>Please see documentation here: <a href=""https://docs.appdynamics.com/display/PRO21/Monitor+Single-Page+Applications"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Monitor+Single-Page+Applications</a> - Angular is mentioned as being available using SPA2.</p>
<p>(Note for the front end we use Javascript BRUM for monitoring, general docs are here: <a href=""https://docs.appdynamics.com/display/PRO21/Browser+Monitoring"" rel=""nofollow noreferrer"">https://docs.appdynamics.com/display/PRO21/Browser+Monitoring</a> - this is a seperate part of the product than the APM used to monitor backends)</p>
"
AppDynamics,50694003,50682773,2,"2018/06/05, 10:03:15",False,"2018/06/05, 10:03:15",30936,6587650,0,"<p>Since some packages are not working with different OS configurations, you need to setup a new build agent.</p>

<p><a href=""https://docs.microsoft.com/en-us/vsts/pipelines/agents/v2-windows?view=vsts"" rel=""nofollow noreferrer"">Deploy an agent on Windows</a></p>
"
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107,4772912,2,"<p>It's possible to solve this by to changing the query by using zero interpolation. You can put "".fill(zero)"" behind your query in the json or choose the option from the UI.
<a href=""https://i.stack.imgur.com/OgTNl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OgTNl.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT:</strong></p>

<p>You're right, the interpolation is not working when no data is available. I had the same problem in the end. Support of Datadog said it isn't possible to show zero when there is no data for a metric. Now there is a feature request made for it. It would be nice if more people will request for this feature, so it will be prioritized. </p>

<p>Nonetheless I have tried to create a workaround by adding a second metric that always has data as a second query and added a formula ((b - b) + a) that negates the second query, but when there is data in the intended query it's show in the graph. This will result in a zero line when there is no data available.</p>

<p><strong>Scenario without data:</strong>
<a href=""https://i.stack.imgur.com/cGykP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cGykP.png"" alt=""GraphWithNoData""></a></p>

<p>The only problem is that when you have a data in the intended query, it looks ugly and the zero line is gone. As you see in the following screenshot.</p>

<p><strong>Scenario with data:</strong>
<a href=""https://i.stack.imgur.com/8Tcux.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Tcux.png"" alt=""GraphWithData""></a></p>

<p><strong>Conclusion:</strong>
The workaround is not perfect, but it will work for some situation. For example, filling up query values with zero instead of (no data). I hope this is a bit better answer to the problem.</p>
"
Datadog,53345774,49671175,0,"2018/11/16, 23:38:15",False,"2018/11/16, 23:38:15",1371,5540166,21,"<p>how about <a href=""https://docs.datadoghq.com/graphing/functions/interpolation/#default"" rel=""noreferrer"">the ""default"" function</a>?</p>

<p>so <code>default(sum:foo.bar{hello:world} by {baz}, 0)</code> or some such?</p>
"
Datadog,58276947,49671175,0,"2019/10/07, 23:49:14",False,"2020/08/28, 05:49:42",452,1655072,6,"<p>There is now a <a href=""https://docs.datadoghq.com/dashboards/functions/interpolation/#default-zero"" rel=""nofollow noreferrer"">default_zero()</a> function that can be used in Datadog by modifying through JSON directly.</p>
"
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308,3141234,7,"<p>The <code>default_zero()</code> function does what you're looking for. You can type it in manually, as <a href=""https://stackoverflow.com/a/53345774/3141234"">stephenlechner suggests</a>.</p>

<p>There's another way I found:</p>

<ol>
<li>Click ""Advanced"" in the bottom right: <a href=""https://i.stack.imgur.com/Xc7T5.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xc7T5.png"" alt=""enter image description here""></a></li>
<li>Enter <code>default(a, 0)</code> as the formula, and disable the visibility of metric <code>a</code>: <a href=""https://i.stack.imgur.com/MZeFc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MZeFc.png"" alt=""enter image description here""></a></li>
</ol>

<p>When you save the graph and reopen, you'll see that things have been reshuffled a little, and you'll see a ""default 0"" section tagged onto the end of the metric's definition.</p>

<p><a href=""https://i.stack.imgur.com/t3bFh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/t3bFh.png"" alt=""enter image description here""></a></p>
"
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336,7243426,0,"<p>Metrics queries now support wildcards.</p>
<p>Example 1: Getting all the requests with a status tag starting with <code>2</code>:
<code>http.server.requests.count{status:2*}</code></p>
<p>Example 1: Getting all the requests with a service tag ending with <code>mongo</code>:
<code>http.server.requests.count{service:*mongo}</code></p>
<p>Example 3 (advanced): Getting all the requests with a service tag starting with <code>blob</code> and ending with <code>postgres</code>:
<code>http.server.requests.count{service:blob*,service:*postgres}</code>
<em>(this will match <code>service:blob-foo-postgres</code> and <code>service:blob_bar_postgres</code> but not <code>service:my_name_postgres</code>)</em></p>
"
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984,750117,11,"<p>I've finally found a dropwizzard module that integrates this library with datadog: <a href=""https://github.com/coursera/metrics-datadog"" rel=""noreferrer"">metrics-datadog</a></p>

<p>I've created a Spring configuration class that creates and initializes this Reporter using properties of my YAML.</p>

<p>Just insert this dependency in your pom:</p>

<pre><code>    &lt;!-- Send metrics to Datadog --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.coursera&lt;/groupId&gt;
        &lt;artifactId&gt;dropwizard-metrics-datadog&lt;/artifactId&gt;
        &lt;version&gt;1.1.3&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Add this configuration to your YAML:</p>

<pre><code>yourapp:
  metrics:
    apiKey: &lt;your API key&gt;
    host: &lt;your host&gt;
    period: 10
    enabled: true
</code></pre>

<p>and add this configuration class to your project:</p>

<pre><code>/**
 * This bean will create and configure a DatadogReporter that will be in charge of sending
 * all the metrics collected by Spring Boot actuator system to Datadog.
 *     
 * @see https://www.datadoghq.com/
 * @author jfcorugedo
 *
 */
@Configuration
@ConfigurationProperties(""yourapp.metrics"")
public class DatadogReporterConfig {

  private static final Logger LOGGER = LoggerFactory.getLogger(DatadogReporterConfig.class);

  /** Datadog API key used to authenticate every request to Datadog API */
  private String apiKey;

  /** Logical name associated to all the events send by this application */
  private String host;

  /** Time, in seconds, between every call to Datadog API. The lower this value the more information will be send to Datadog */
  private long period;

  /** This flag enables or disables the datadog reporter */
  private boolean enabled = false;

  @Bean
  @Autowired
  public DatadogReporter datadogReporter(MetricRegistry registry) {

      DatadogReporter reporter = null;
      if(enabled) {
          reporter = enableDatadogMetrics(registry);
      } else {
          if(LOGGER.isWarnEnabled()) {
              LOGGER.info(""Datadog reporter is disabled. To turn on this feature just set 'rJavaServer.metrics.enabled:true' in your config file (property or YAML)"");
          }
      }

      return reporter;
  }

  private DatadogReporter enableDatadogMetrics(MetricRegistry registry) {

      if(LOGGER.isInfoEnabled()) {
          LOGGER.info(""Initializing Datadog reporter using [ host: {}, period(seconds):{}, api-key:{} ]"", getHost(), getPeriod(), getApiKey());
      }

      EnumSet&lt;Expansion&gt; expansions = DatadogReporter.Expansion.ALL;
      HttpTransport httpTransport = new HttpTransport
                                .Builder()
                                .withApiKey(getApiKey())
                                .build();

      DatadogReporter reporter = DatadogReporter.forRegistry(registry)
        .withHost(getHost())
        .withTransport(httpTransport)
        .withExpansions(expansions)
        .build();

      reporter.start(getPeriod(), TimeUnit.SECONDS);

      if(LOGGER.isInfoEnabled()) {
          LOGGER.info(""Datadog reporter successfully initialized"");
      }

      return reporter;
  }

  /**
   * @return Datadog API key used to authenticate every request to Datadog API
   */
  public String getApiKey() {
      return apiKey;
  }

  /**
   * @param apiKey Datadog API key used to authenticate every request to Datadog API
   */
  public void setApiKey(String apiKey) {
      this.apiKey = apiKey;
  }

  /**
   * @return Logical name associated to all the events send by this application
   */
  public String getHost() {
      return host;
  }

  /**
   * @param host Logical name associated to all the events send by this application
   */
  public void setHost(String host) {
      this.host = host;
  }

  /**
   * @return Time, in seconds, between every call to Datadog API. The lower this value the more information will be send to Datadog
   */
  public long getPeriod() {
      return period;
  }

  /**
   * @param period Time, in seconds, between every call to Datadog API. The lower this value the more information will be send to Datadog
   */
  public void setPeriod(long period) {
      this.period = period;
  }

  /**
   * @return true if DatadogReporter is enabled in this application
   */
  public boolean isEnabled() {
      return enabled;
  }

  /**
   * This flag enables or disables the datadog reporter.
   * This flag is only read during initialization, subsequent changes on this value will no take effect 
   * @param enabled
   */
  public void setEnabled(boolean enabled) {
      this.enabled = enabled;
  }
}
</code></pre>
"
Datadog,34400755,34398692,3,"2015/12/21, 18:57:48",False,"2015/12/21, 18:57:48",2073,3239981,2,"<p>If JMX is an option for you, you may use the <a href=""https://dropwizard.github.io/metrics/3.1.0/getting-started/#reporting-via-jmx"" rel=""nofollow"">JMX dropwizrd reporter</a> combined with <a href=""http://docs.datadoghq.com/integrations/java/"" rel=""nofollow"">java datalog integration</a></p>
"
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355,607038,6,"<p>It seems that Spring Boot 2.x added several monitoring system into its metrics. DataDog is one of them supported by <a href=""http://micrometer.io/"" rel=""noreferrer"">micrometer.io</a>. See reference documentation: <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-newrelic"" rel=""noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-newrelic</a></p>

<p>For Spring Boot 1.x, you can use back-ported package:</p>

<p><code>compile 'io.micrometer:micrometer-spring-legacy:latest.release'</code></p>
"
Datadog,37013010,37010163,1,"2016/05/03, 22:41:22",True,"2016/05/03, 22:41:22",12342,496289,10,"<p>Confirmed on IRC (#datadog on freenode) that:</p>

<blockquote>
  <p>Datadog doesn't support multiple Y-axis at this time.</p>
</blockquote>
"
Datadog,41817123,37010163,0,"2017/01/24, 00:52:07",False,"2017/01/24, 00:52:07",15978,770425,0,"<p>If the two axis have the same units but different degrees (10 vs 10 million), then using a non-linear scale such as <code>log</code> might provide what you need:</p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/203038729-Is-it-possible-to-adjust-the-y-axis-for-my-graphs-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/203038729-Is-it-possible-to-adjust-the-y-axis-for-my-graphs-</a></p>
"
Datadog,64150659,37010163,2,"2020/10/01, 10:13:05",False,"2020/10/01, 10:13:05",399,1619555,3,"<p>They do allow dual y-axis now</p>
<p><a href=""https://docs.datadoghq.com/dashboards/widgets/timeseries/#y-axis-controls"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/dashboards/widgets/timeseries/#y-axis-controls</a></p>
<blockquote>
<p>Introducing dual Y-axis for time series widgets.
The time series widget on dashboards now support dual y-axis, making it easier than ever to compare two sets of data on a single graph. By removing the need to create separate graphs, your dashboards can show even more valuable information viewable at a glance.</p>
</blockquote>
"
Datadog,35113402,35111147,2,"2016/01/31, 13:39:25",False,"2016/01/31, 13:39:25",4968,19892,3,"<p>If <code>dd-agent</code> listens on <code>localhost</code> it can receive data only from localhost (127.0.0.1). Try to change the <code>dd-agent</code> host to <code>0.0.0.0</code> instead of <code>localhost</code>.</p>

<p>We are using <a href=""https://github.com/DataDog/docker-dd-agent"" rel=""nofollow"">docker-dd-agent</a> and it works OOTB.</p>
"
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111,5868112,11,"<p>You will need to set <code>non_local_traffic: yes</code> in your <code>/etc/dd-agent/datadog.conf</code> file. Otherwise the agent will reject metrics from containers.</p>

<p>After setting, you will need to restart the agent for the change to take effect: <code>sudo /etc/init.d/datadog-agent restart</code> or <code>sudo service datadog-agent restart</code></p>

<p>The <a href=""https://github.com/DataDog/docker-dd-agent"" rel=""noreferrer"">docker-dd-agent</a> image enables <code>non_local_traffic: yes</code> by default.</p>
"
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296,166816,0,"<p>You don't actually want to use the IP of the host in this case. If you're running the docker dd-agent, there are two environment variables you can tap into:</p>

<p><code>statsd.connect(DOGSTATSD_PORT_8125_UDP_ADDR, DOGSTATSD_PORT_8125_UDP_PORT)</code></p>

<p>That should do the trick. If not, you should be able to find the relevant info to your problem in <a href=""https://github.com/DataDog/docker-dd-agent#dogstatsd-from-the-host"" rel=""nofollow"">this section of the Datadog docs</a>.</p>

<p>Also, I should point out that the only Python library that Datadog shows in their docs is <a href=""https://github.com/DataDog/datadogpy"" rel=""nofollow"">datadogpy</a>.</p>
"
Datadog,59791963,57918254,0,"2020/01/17, 19:12:16",True,"2020/01/17, 19:12:16",10672,75801,10,"<p>I asked DataDog support, and apparently as of January 2020 this is not possible, but is a feature request in their backlog. I know this is not a great answer to the question but if I hear that this changes, I will update my answer.</p>
"
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493,244037,0,"<p>There doesn't appear to be a Crashalytics direct integration yet.</p>
<p>Datadog maintains mobile SDKs that can be included in mobile clients to produce metrics and logs to the platform.</p>
<p>For iOS Logs, see here: <a href=""https://docs.datadoghq.com/logs/log_collection/ios/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/ios/</a></p>
<p>For Android Logs: <a href=""https://docs.datadoghq.com/logs/log_collection/android/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/android/</a></p>
<p>There's also a public Android SDK for Real User Monitoring, which can be read here: <a href=""https://docs.datadoghq.com/real_user_monitoring/android/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/real_user_monitoring/android/</a></p>
<p>And the announcement, with a link for the private beta for iOS signup here: <a href=""https://www.datadoghq.com/blog/datadog-mobile-rum/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/datadog-mobile-rum/</a></p>
"
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199,2525626,4,"<p>After speaking to the support team at DataDog, I managed to find out the following information relating to what the no_pod pods were.</p>

<blockquote>
  <p>Our Kubernetes check is getting the list of containers from the Kubernetes API, which exposes aggregated data. In the metric explorer configuration here, you can see a couple of containers named /docker and / that are getting picked up along with the other containers. Metrics with pod_name:no_pod that come from container_name:/ and container_name:/docker  are just metrics aggregated across multiple containers. (So it makes sense that these are the highest values in your graphs.) If you don't want your graphs to show these aggregated container metrics though, you can clone the dashboard and then exclude these pods from the query. To do so, on the cloned dashboard, just edit the query in the JSON tab, and in the tag scope, add !pod_name:no_pod.</p>
</blockquote>

<p>So it appears that these pods are the docker and root level containers running outside of the cluster and will always display unless you want to filter them out specifically which I now do.</p>

<p>Many thanks to the support guys at DataDog for looking into the issue for me and giving me a great explanation as to what the pods were and essentially confirming that I can just safely filter these out and not worry about them.</p>
"
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676,1230594,0,"<p>The free text editor you have in the screenshot is for metric queries. Events in graphs are added as overlay to show when events happened over time.</p>

<p>There is no widget, as of now, that shows a single value for the number of times an event occurred. But you can use the event timeline widget, which will show a timeline of events grouped by status and bucketed over a defined period of time. See below:</p>

<p><a href=""https://i.stack.imgur.com/6nyJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6nyJS.png"" alt=""enter image description here""></a></p>
"
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929,2599875,3,"<p>Well I realized that the <strong>query value</strong> only works with metrics, so to create a counter we can emit metrics with <code>value: 1</code> and then count them with the <code>rollup(sum, 60)</code> function.</p>

<p><code>dog.emit_point('some.event.name', 1)
</code></p>

<p><code>sum:some.event.name{*}.rollup(sum, 60)</code></p>

<p>The main thing to understand here is that DataDog does not retrieve all the points for a given timeframe. Actually as <a href=""https://help.datadoghq.com/hc/en-us/articles/204526615-What-is-the-rollup-function"" rel=""nofollow noreferrer"">McCloud</a> says, <em>for a given time range we do not return more than 350 points</em>, which is very important to have in mind when you create a counter.</p>

<p>When you query value from a metric in a timeframe, DataDog return a group of points that represents the real stored points, not all the points; the level of how those points are represented (as I understand) is called granurallity, and what you do with this <code>rollup</code> function is to define how those points are going to represent the real points, which in this case is going to be using the <code>sum</code> function. </p>

<p>I hope this helps somebody, I'm still learning about it. Regards</p>
"
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66,2625807,0,"<p>In Ruby on the client, I use:</p>
<pre class=""lang-rb prettyprint-override""><code>Datadog::Statsd.new(hostname, port)
datadog.increment(metric_name, tags: tag_list)
</code></pre>
<p>I then have a dashboard with a
<a href=""https://docs.datadoghq.com/dashboards/widgets/query_value/"" rel=""nofollow noreferrer"">Query Value</a>
widget, set to &quot;take the [Sum] value from the displayed timeframe&quot;.</p>
<pre class=""lang-sh prettyprint-override""><code>sum:some.event.name{*}.as_count()
</code></pre>
<p>I tested this and it seems to give the right numbers.  The
<a href=""https://docs.datadoghq.com/monitors/guide/as-count-in-monitor-evaluations/"" rel=""nofollow noreferrer"">.as_count()</a>
seems key.</p>
"
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407,1017941,1,"<p>Answering just in case someone will spot this question via Google.</p>

<p>You cannot. StatsD protocol do not define tags or comments at all, so there is no possibility for that. You need to use different library like <a href=""https://github.com/lexmag/statix"" rel=""nofollow noreferrer"">Statix</a> for that.</p>
"
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1,1976300,-1,"<p>Yes it is possible to emit metrics to DataDog from a AWS Lambda function.</p>

<p>If you were using node.js you could use <a href=""https://www.npmjs.com/package/datadog-metrics"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/datadog-metrics</a> to emit metrics to the API.  It supports counters, gauges and histograms.  You just need to pass in your app/api key as environment variables.</p>

<p>Matt</p>
"
Datadog,54256464,42588899,0,"2019/01/18, 16:57:13",False,"2019/01/18, 16:57:13",809,1647226,0,"<p>The easier way is using this library: <a href=""https://github.com/marceloboeira/aws-lambda-datadog"" rel=""nofollow noreferrer"">https://github.com/marceloboeira/aws-lambda-datadog</a></p>

<p>It has runtime no dependencies, doesn't require authentication and reports everything to cloud-watch too. You can read more about it here: <a href=""https://www.datadoghq.com/blog/how-to-monitor-lambda-functions/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/how-to-monitor-lambda-functions/</a></p>
"
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474,10109833,0,"<p>In most cases, the datadog agent retrieves metrics from an integration by connecting to a URL endpoint. This would be the case for services such as nginx, mysql etc. </p>

<p>This means that you can run just one datadog agent on the host, and configure it to listen to URL endpoints of services exposed from each container.</p>

<p>For example, assuming a mysql docker container is run with the following command:</p>

<pre><code>docker run -d \
  --name mysql \
  -p 3306:3306 \
  -e MYSQL_ROOT_PASSWORD=secret \
  -e MYSQL_DATABASE=mySchema \
  mysql
</code></pre>

<p>You can instruct the agent running on the host to connect to the container IP in the <code>mysql.yaml</code> agent configuration:</p>

<pre><code>init_config:

instances:
- server: &lt;container IP&gt;
    user: datadog
    pass: secret

    tags:
        - optional_tag1
        - optional_tag2
    options:
</code></pre>

<p>Varnish is slightly different as the agent retrieves metrics using the <code>varnishstat</code> binary. According to the example template:</p>

<blockquote>
  <p>In order to support monitoring a Varnish instance which is running as a Docker container we need to wrap commands (varnishstat) with scripts which perform a docker exec on the running container.</p>
</blockquote>

<p>To do this, on the host, create a wrapper script for the container:</p>

<pre><code>echo ""/usr/bin/docker exec varnish_container_name varnishstat ""$@"""" &gt; /home/myuser/docker_varnish
</code></pre>

<p>Then specify the script location in the <code>varnish.yaml</code> agent configuration:</p>

<pre><code>init_config:

instances:
    - varnishstat: /home/myuser/docker_varnish
</code></pre>
"
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992,902415,2,"<p>There are 2 relevant options in <code>/etc/dd-agent/conf.d/docker_daemon.yaml</code>:</p>

<ul>
<li><p><strong>collect_disk_stats</strong><br>
If you use devicemapper-backed storage (which is default in ECS but not in vanilla Docker or Kubernetes), docker.data.* and docker.metadata.* statistics should do what you are looking for.</p></li>
<li><p><strong>collect_container_size</strong><br>
A generic way, using the docker API but virtually running df in every container. This enables the docker.container.* metrics.</p></li>
</ul>

<p>See more here:
<a href=""https://help.datadoghq.com/hc/en-us/articles/115001786703-How-to-report-host-disk-metrics-when-dd-agent-runs-in-a-docker-container-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/115001786703-How-to-report-host-disk-metrics-when-dd-agent-runs-in-a-docker-container-</a></p>

<p>and here:
<a href=""https://github.com/DataDog/docker-dd-agent/blob/master/conf.d/docker_daemon.yaml#L46"" rel=""nofollow noreferrer"">https://github.com/DataDog/docker-dd-agent/blob/master/conf.d/docker_daemon.yaml#L46</a></p>
"
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11,11431260,1,"<p>In your question, I looked for your purpose in terms of using the port 8125 or 8126 ports. 8125 port is used for stasd metrics, and 8126 is used for APM (trace) data.</p>

<p>So if you want to use 8125 the important thing to do is having <code>non_local_traffic : yes</code>. So there must be another problem which I don't know yet.</p>

<p>But if your purpose is using APM/trace port: 8126 is only bound to localhost by default. You should make it listen to any network interface by the <code>bind_host: 0.0.0.0</code> configuration. Currently, it will refuse the requests from your containers since they are not coming from localhost.</p>

<p>I had a similar problem and this page helped me: <a href=""https://github.com/DataDog/ansible-datadog/issues/149"" rel=""nofollow noreferrer"">https://github.com/DataDog/ansible-datadog/issues/149</a></p>
"
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371,5540166,5,"<p><strong>UPDATED ANSWER:</strong></p>

<p>Still yes. </p>

<p>Docs for new Dashboard endpoint <a href=""https://docs.datadoghq.com/api/?lang=bash#dashboards"" rel=""nofollow noreferrer"">here</a>.</p>

<hr>

<p><strong>ORIGINAL ANSWER:</strong></p>

<p>Yes.</p>

<p>Docs for screenboards <a href=""https://docs.datadoghq.com/api/?lang=python#screenboards"" rel=""nofollow noreferrer"">here</a>. </p>

<p>Docs for timeboards <a href=""https://docs.datadoghq.com/api/?lang=python#timeboards"" rel=""nofollow noreferrer"">here</a>. </p>
"
Datadog,54194264,53880368,2,"2019/01/15, 09:19:00",True,"2019/01/15, 09:19:00",747,2461574,1,"<p>I doubt the Datadog agent will ever work on App Services web app as you do not have access to the running host, it was designed for VMs.
Have you tried this <a href=""https://www.datadoghq.com/blog/azure-monitoring-enhancements/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/azure-monitoring-enhancements/</a> ? They say they support AppServices</p>
"
Datadog,55897902,53880368,2,"2019/04/29, 09:16:35",False,"2019/04/29, 09:16:35",27,5404159,0,"<p>To respond to your comment on wanting custom metrics, this is still possible without the agent at the same location. After installing the nuget package of datadog called statsdclient you can then configure it to send the custom metrics to an agent located elsewhere. Example below: </p>

<pre><code>using StatsdClient;

var dogstatsdConfig = new StatsdConfig
{
    StatsdServerName = ""127.0.0.1"", // Optional if DD_AGENT_HOST environment variable set
    StatsdPort = 8125, // Optional; If not present takes the DD_DOGSTATSD_PORT environment variable value, else default is 8125
    Prefix = ""myTestApp"", // Optional; by default no prefix will be prepended
    ConstantTags = new string[1] { ""myTag:myTestAppje"" } // Optional
};

StatsdClient.DogStatsd.Configure(dogstatsdConfig);
StatsdClient.DogStatsd.Increment(""fakeVisitorCountByTwo"", 2); //Custom metric itself
</code></pre>
"
Datadog,56799311,53880368,0,"2019/06/28, 01:59:38",False,"2019/06/28, 01:59:38",55,11661354,0,"<p>I have written a app service extension for sending Datadog APM metrics with .NET core and provided instructions for how to set it up here: <a href=""https://github.com/payscale/datadog-app-service-extension"" rel=""nofollow noreferrer"">https://github.com/payscale/datadog-app-service-extension</a></p>

<p>Let me know if you have any questions or if this doesn't apply to your situation.</p>
"
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11,11713986,1,"<p>Logs from App Services can also be sent to Blob storage and forwarded from there via an Azure Function. Unlike traces and custom metrics from App Services, this does not require a VM running the agent. Docs and code for the Function are available here:</p>

<p><a href=""https://github.com/DataDog/datadog-serverless-functions/tree/master/azure/blobs_logs_monitoring"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-serverless-functions/tree/master/azure/blobs_logs_monitoring</a></p>
"
Datadog,58550213,53880368,0,"2019/10/25, 02:03:41",False,"2019/10/25, 02:03:41",1630,1246590,1,"<p>If you want to use DataDog for logging from Azure Function of App Service you can use Serilog and DataDog Sink to the log files:</p>

<pre><code>        services
            .AddLogging(loggingBuilder =&gt;
                loggingBuilder.AddSerilog(
                    new LoggerConfiguration()
                        .WriteTo.DatadogLogs(
                            apiKey: ""REPLACE - DataDog API Key"",
                            host: Environment.MachineName,
                            source: ""REPLACE - Log-Source"",
                            service: GetServiceName(),
                            configuration: new DatadogConfiguration(),
                            logLevel: LogEventLevel.Infomation
                        )
                        .CreateLogger())
            );
</code></pre>

<p><a href=""https://druss.co/2019/10/logging-datadog-azure-function-serilog/"" rel=""nofollow noreferrer"">Full source code and required NuGet packages</a> are here: </p>
"
Datadog,61118441,59500017,0,"2020/04/09, 12:53:48",False,"2020/04/09, 12:53:48",385,1068531,0,"<p>You can deploy the Datadog agent in a container / instance that you manage and the configure it according to <a href=""https://docs.datadoghq.com/integrations/elastic/"" rel=""nofollow noreferrer"">these instructions</a> to gather metrics from the remote ElasticSearch cluster that is hosted on Elastic Cloud. You need to create a <code>conf.yaml</code> file in the <code>elastic.d/</code> directory and provide the required information (Elasticsearch endpoint/URL, username, password, port, etc) for the agent to be able to connect to the cluster. You may find a sample configuration file <a href=""https://github.com/DataDog/integrations-core/blob/master/elastic/datadog_checks/elastic/data/conf.yaml.example"" rel=""nofollow noreferrer"">here</a>.</p>
"
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287,466390,0,"<p>As George Tseres mentioned above, the way I had to get this working was to set up collection on a separate instance (through docker) and then to configure it to read the specific Elastic Cloud instances.</p>

<p>I ended up making this: <a href=""https://github.com/crwang/datadog-elasticsearch"" rel=""nofollow noreferrer"">https://github.com/crwang/datadog-elasticsearch</a>, building that docker image, and then pushing it up to AWS ECR.</p>

<p>Then, I spun up a Fargate service / task to run the container.</p>

<p>I also set it to run locally with <code>docker-compose</code> as a test.</p>
"
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455,4676932,10,"<p>These system.io metrics are reported from a <a href=""https://github.com/DataDog/dd-agent/blob/5.6.3/checks/system/unix.py#L115-L133"" rel=""noreferrer"">system agent check</a> that uses <code>iostat</code> under the hood.</p>

<p>According to the <a href=""http://linux.die.net/man/1/iostat"" rel=""noreferrer"">iostat manpage</a> one of the metrics <code>%util</code> (reported as <code>system.io.util</code> within Datadog) seems to do the job:</p>

<blockquote>
  <p>%util: Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%.</p>
</blockquote>

<p>You can create a monitor, as a multi alert on host/device, when this metric is over 90 on the last 30 minutes on average, here is a current screenshot of such an example:</p>

<p><a href=""https://i.stack.imgur.com/bxVvc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bxVvc.png"" alt=""example monitor in Datadog""></a></p>

<p>Of course one can also monitor other iostat metrics to identify other I/O performance failure modes.</p>
"
Datadog,35560762,35554988,2,"2016/02/22, 20:06:38",True,"2016/02/22, 20:06:38",261,4172512,10,"<p>This can be achieved in two ways.</p>

<p>If you only want this logic to applied on this one graph, you can divide metric either using the UI editor and clicking advanced or using the JSON editor:</p>

<p>UI Editor: <a href=""http://cl.ly/1c0K2O3P1E2K"" rel=""noreferrer"">http://cl.ly/1c0K2O3P1E2K</a></p>

<p>Or JSON editor: <a href=""https://help.datadoghq.com/hc/en-us/articles/203764925-How-do-I-use-arithmetic-for-my-time-series-data-"" rel=""noreferrer"">https://help.datadoghq.com/hc/en-us/articles/203764925-How-do-I-use-arithmetic-for-my-time-series-data-</a></p>

<p>Alternatively, you can use the Metric Summary page to edit this metric's metadata and alter this metric's unit throughout the application as seen here: <a href=""http://cl.ly/2x0Z290w2I3V"" rel=""noreferrer"">http://cl.ly/2x0Z290w2I3V</a></p>

<p><a href=""https://app.datadoghq.com/metric/summary"" rel=""noreferrer"">https://app.datadoghq.com/metric/summary</a></p>

<p>Hope this helps.  Also, you can reach out to support@datadoghq.com if you run into any other issues in the future.</p>
"
Datadog,42481675,42441120,3,"2017/02/27, 11:09:46",True,"2017/02/27, 11:09:46",6657,86,1,"<p>So, while trying to debug this I deleted the deployment + dameonset and service and recreated it. Afterwards it worked....</p>
"
Datadog,44038527,42441120,0,"2017/05/18, 07:21:01",False,"2017/05/18, 07:21:01",9,7901388,0,"<p>Have you seen the <a href=""https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services"" rel=""nofollow noreferrer"">Discovering Services</a> docs? I'd recommend using DNS for service discovery rather than environment variables since environment variables require services to come up in a particular order.</p>
"
Datadog,43669352,42538664,1,"2017/04/28, 02:11:48",False,"2017/04/28, 02:11:48",162,2254902,2,"<p>There are two error in your code:</p>

<ol>
<li>The <code>type</code> used is wrong. It should be <code>service check</code> instead of <code>metric alert</code>.</li>
<li>You need to enclose <code>process.up</code> in a pair of <code>''</code>. </li>
</ol>

<p>Once done, your code will run flawlessly.</p>
"
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371,5540166,5,"<p>So from that log line, it appears as though <a href=""https://github.com/DataDog/datadogpy/blob/v0.6.x/datadog/util/hostname.py#L51"" rel=""noreferrer"">this <code>try</code> is excepting</a> in the library's <code>hostname.py</code>. So either...</p>

<ul>
<li><p>(A) The <a href=""https://github.com/DataDog/datadogpy/blob/v0.6.x/datadog/util/hostname.py#L53"" rel=""noreferrer"">hostname line</a> is where it's excepting, and (weirdly) the
library requires that a <code>hostname</code> option be set in your
<code>datadog.conf</code> file. Maybe worth setting a hostname there if you
haven't already. Or,</p></li>
<li><p>(B) The <a href=""https://github.com/DataDog/datadogpy/blob/v0.6.x/datadog/util/hostname.py#L52"" rel=""noreferrer"">get_config() line</a> is where it's excepting, and so the
library isn't able to correctly identify the configuration file
location (or access it, possibly related to permissions). Based on
the directory structure in your question, I think you're working on
an OSX / mac environment, which means the library will be using the
function <code>_mac_config_path()</code> in <code>config.py</code> to try to identify the
configuration path, which from <a href=""https://github.com/DataDog/datadogpy/blob/v0.6.x/datadog/util/config.py#L83"" rel=""noreferrer"">this line in the function</a> would
make it <em>seem</em> as though the library were looking for the
configuration file in <code>~/.datadog-agent/agent/datadog.conf</code> instead
of the appropriate <code>~/.datadog-agent/datadog.conf</code>. Which might be a
legitimate bug...</p></li>
</ul>

<p>So if I were you, and if all this seemed right, I'd try adding a <a href=""https://github.com/DataDog/dd-agent/blob/5.13.x/datadog.conf.example#L25"" rel=""noreferrer"">hostname in the <code>datadog.conf</code></a> to see if that helped, and if it didn't, then I'd try making a <code>~/.datadog-agent/agent/</code> directory and copying your <code>datadog.conf</code> file there as well, just to see if that got things working.</p>

<p>This answer assumes you are working in an OSX / mac environment, and will likely not be correct otherwise. </p>

<p>If either (A) or (B) are the case, then that's a problem with the library and should be updated--it would be kind of you to open an issue on <a href=""https://github.com/DataDog/datadogpy"" rel=""noreferrer"">the library itself</a> to bring this up so the Datadog team that supports that library can be made aware. I suspect not many people end up this library in OSX / mac environments to begin with, so that could explain all this. </p>
"
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301,2708541,0,"<p>You installed the Datadog agent &amp; trace agent on a Mac, listening on localhost. </p>

<p>You installed the flask application and ddtrace library in a docker container on a linux vm sending traffic to localhost. </p>

<p>Those two localhosts are describing two different machines. The easiest option is going to be running both the Agent &amp; flask app on the Mac, or running both in docker. The latter is most similar to an eventual production deployment. Do that. </p>
"
Datadog,58681226,58476988,0,"2019/11/03, 16:41:58",True,"2019/11/03, 16:41:58",2206,712543,3,"<p>According to the documentation, this can be achieved using following properties in telegraf.conf:</p>

<pre><code>[[outputs.datadog]]
apikey = ""&lt;datadog api key&gt;"" # required.
namepass = [""metric_1"",""metric_2""...etc.]
</code></pre>

<p><a href=""https://docs.influxdata.com/telegraf/v1.12/administration/configuration/#measurement-filtering"" rel=""nofollow noreferrer"">https://docs.influxdata.com/telegraf/v1.12/administration/configuration/#measurement-filtering</a></p>

<p>where <strong>namepass</strong> defines pattern list of points which will be emitted.</p>
"
Datadog,60213083,60064427,1,"2020/02/13, 19:02:26",True,"2020/02/14, 09:17:50",27576,1839482,3,"<p><a href=""https://i.stack.imgur.com/MNeRh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MNeRh.png"" alt=""enter image description here""></a>Datadog has two agents.</p>

<ol>
<li>Cluster agent which is a proxy between Kubernetes API Server and Datadog node agents. The cluster agent is deployed as deployment to one of the kubernetes node.</li>
<li>Node agents which is deployed in each and every Kubernetes node as Daemonset.</li>
</ol>

<p>And yes for DogStatsD the node agents need to be deployed as Daemonset.</p>

<p>Here is the the deployment manifest for <a href=""https://github.com/DataDog/datadog-agent/blob/master/Dockerfiles/manifests/cluster-agent/cluster-agent.yaml"" rel=""nofollow noreferrer"">cluster agent</a> and <a href=""https://github.com/DataDog/datadog-agent/blob/master/Dockerfiles/manifests/agent.yaml"" rel=""nofollow noreferrer"">node agent</a>.</p>
"
Datadog,64168376,60290897,0,"2020/10/02, 11:24:58",True,"2020/10/02, 11:24:58",347,1485995,0,"<p>Yes, the following should work:</p>
<p>tag_one:(A OR B)</p>
<p>Unfortunately the query syntax is slightly different in different contexts, I find, so I don't know if that will solve your particular problem.</p>
"
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336,7243426,1,"<p>If you want to remove the warning, you can try adding <code>none</code> and <code>shm</code> to the <code>excluded_filesystems</code> in disk.yaml.  This file should exist or be created in the Agent's conf.d directory.</p>

<p>Otherwise, you'll find more options <a href=""https://github.com/DataDog/dd-agent/issues/2932"" rel=""nofollow noreferrer"">here</a>.</p>

<p>If you are looking to exclude the logs from the agent within the platform you can look at excluding the agent (<a href=""https://docs.datadoghq.com/agent/autodiscovery/management/?tab=containerizedagent#exclude-containers"" rel=""nofollow noreferrer"">doc</a>)</p>
"
Datadog,60677166,60662893,0,"2020/03/13, 22:56:50",True,"2020/03/13, 22:56:50",787,1363715,2,"<p>Found the answer here: <a href=""https://github.com/DataDog/datadog-agent/issues/3329"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-agent/issues/3329</a></p>

<p>The field is <code>mount_point_blacklist</code></p>
"
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493,244037,4,"<p>In Datadog Logs, there's a difference between the Tags associated with the execution environment, and Attributes set on Log entry content.</p>

<p>From <a href=""https://docs.datadoghq.com/logs/explorer/#log-structured-information"" rel=""nofollow noreferrer"">this section in the docs</a>:</p>

<blockquote>
  <p><strong>Context</strong> refers to the infrastructure and application context in which the log has been generated. Information is gathered from tags—whether automatically attached (host name, container name, log file name, serverless function name, etc.)—or added through custom tags (team in charge, environment, application version, etc.) on the log by the Datadog Agent or Log Forwarder.</p>
</blockquote>

<p>And looking into the <a href=""https://github.com/DataDog/browser-sdk/blob/723100bf032b6340a0002f51f6cd8af0d84467bd/packages/core/src/configuration.ts#L144-L153"" rel=""nofollow noreferrer"">source for the browser SDK</a>, we can see: </p>

<pre class=""lang-js prettyprint-override""><code>...
  const tags =
    `sdk_version:${conf.sdkVersion}` +
    `${conf.env ? `,env:${conf.env}` : ''}` +
    `${conf.service ? `,service:${conf.service}` : ''}` +
    `${conf.version ? `,version:${conf.version}` : ''}`
  const datadogHost = `${type}-http-intake.logs.${domain}`
  const host = conf.proxyHost ? conf.proxyHost : datadogHost
  const proxyParameter = conf.proxyHost ? `ddhost=${datadogHost}&amp;` : ''

  return `https://${host}/v1/input/${conf.clientToken}?${proxyParameter}ddsource=${source || 'browser'}&amp;ddtags=${tags}`
...
</code></pre>

<p>This shows us that the <code>tags</code> query string parameter being submitted is being calculated based on configuration, and only provides a small amount of user-configurable entries, like <code>env</code>, <code>service</code> - these were released very recently in version 1.11.5 - <a href=""https://github.com/DataDog/browser-sdk/pull/392"" rel=""nofollow noreferrer"">here's the change</a> introducing them.</p>

<p>So you may not be able to set <strong>tags</strong> for a specific log entry - rather you can set <strong>attributes</strong> per log entry, like in the example you shared, which is setting <strong>Attributes</strong> for the logger instance as a whole.
Attributes are part of the log <strong>Content</strong> - which will be viewable in the body of the log entry. </p>

<p>Yes, this is confusing since the function used is named <code>addContext</code>/<code>setContext</code> - and these don't set the same thing as the documentation's ""Context"" - rather they modify the Attributes that are associated with the log entry.</p>

<p>In that event, you may want to have either custom logger instances that provide specific attributes for that logger, or add context inline to the log entry, like this:</p>

<pre class=""lang-js prettyprint-override""><code>DD_LOGS.logger.info('Page Viewed', { referrer: document.referrer });
</code></pre>

<p><a href=""https://docs.datadoghq.com/logs/log_collection/javascript/?tab=bundle#send-a-custom-log-entry"" rel=""nofollow noreferrer"">Here's the docs</a> on this approach which show what other default attributes are being set per log entry.</p>
"
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455,4676932,1,"<p>The Monitor section of Datadog now includes a ""Monitor status"" page for each of the monitor you define (for example the URL monitoring you already have). On this page, you can see by group/scope the monitor history and it shows you the uptime for that monitor.</p>

<p>More to read about this new feature <a href=""https://www.datadoghq.com/blog/monitor-alert-status/"" rel=""nofollow"">here</a></p>

<p>It's not yet available as a ""report"" but that's a good idea!</p>
"
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455,4676932,2,"<p>The exception you are seeing is related to the IO system metrics collection and has nothing to do with your custom dogstream parser.</p>

<p>If you look at the stack trace it says that it wasn't able to apply the <code>_parse_linux2</code> function. To troubleshoot that further you should take a look at the output of </p>

<pre><code>/opt/datadog-agent/embedded/bin/iostat -d 1 2 -x -k
</code></pre>

<p>which is the command launched by the agent. Feel free to open a bug on the agent GitHub repository.</p>

<p>References:</p>

<ul>
<li><a href=""https://github.com/DataDog/dd-agent/blob/5.3.2/checks/system/unix.py#L184-L185"" rel=""nofollow"">https://github.com/DataDog/dd-agent/blob/5.3.2/checks/system/unix.py#L184-L185</a></li>
<li><a href=""https://github.com/DataDog/dd-agent/blob/5.3.2/checks/system/unix.py#L264-L284"" rel=""nofollow"">https://github.com/DataDog/dd-agent/blob/5.3.2/checks/system/unix.py#L264-L284</a></li>
</ul>
"
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261,4172512,4,"<p>Just a few items to note to get this working:</p>

<p><code>
dogstatsd = Statsd.new('MY_API_KEY')
</code></p>

<p>This line of code is trying to use your API key to establish a statsD connection, but this should actually be trying to establish the statsD connection via the statsD port currently configured on your Agent as seen here:</p>

<p><code>
Create a stats instance.
statsd = Statsd.new('localhost', 8125)
</code></p>

<blockquote>
  <p>The easiest way to get your custom metrics into Datadog is to send them to DogStatsD, a metrics aggregation server bundled with the Datadog Agent (in versions 3.0 and above). DogStatsD implements the StatsD protocol, along with a few extensions for special Datadog features.</p>
</blockquote>

<p><a href=""http://docs.datadoghq.com/guides/dogstatsd/"" rel=""nofollow"">http://docs.datadoghq.com/guides/dogstatsd/</a></p>

<p>If you would not like to deploy an Agent on the host running the RoR application, you can utilize DogAPI gem:</p>

<p><a href=""https://github.com/DataDog/dogapi-rb"" rel=""nofollow"">https://github.com/DataDog/dogapi-rb</a></p>

<p>Which has additional documentation to get this custom metrics submitted:</p>

<pre><code>require 'rubygems'
require 'dogapi'

api_key = ""abcdef123456""

dog = Dogapi::Client.new(api_key)

dog.emit_point('some.metric.name', 50.0, :host =&gt; ""my_host"", :device =&gt; ""my_device"")
</code></pre>

<p>If you have additional questions, please reach out to support@datadoghq.com</p>
"
Datadog,41144060,41142456,0,"2016/12/14, 15:38:50",True,"2016/12/14, 15:38:50",101,6197827,2,"<p>This Datadog blog should guide you on how to build a monitor.</p>

<p><a href=""https://www.datadoghq.com/blog/monitoring-cassandra-with-datadog/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/monitoring-cassandra-with-datadog/</a></p>
"
Datadog,41149100,41142456,0,"2016/12/14, 19:56:34",False,"2016/12/14, 19:56:34",41,7297928,4,"<p>There is indeed a <a href=""https://app.datadoghq.com/dash/integration/cassandra"" rel=""nofollow noreferrer"">template Cassandra dashboard</a> in Datadog (where I work) that should appear as soon as you enable the integration. This dash has a mix of Cassandra-specific metrics (e.g. cache hit rates), plus metrics from the host (e.g. CPU). You can select a particular host or subset of hosts to make those host-level metrics more meaningful by <a href=""https://i.stack.imgur.com/yqO7Z.png"" rel=""nofollow noreferrer"">changing the scope of the dashboard</a>, and the graphs will re-render on the fly. You can also clone and modify the dashboard as you wish by clicking the gear icon in the upper right.</p>

<p>This dash should provide a good starting point for monitoring Cassandra, but we have an even better template dashboard in the works. I'll update this answer as soon as it's released. In the meantime, the blog post shared by John KVS should help you to identify key metrics that you might want to monitor.</p>
"
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981,2473382,4,"<h1>Solution 1, dirty:</h1>

<p>Duplicate the definition of <code>event.sent</code> in <code>event.failed</code>. As soon as you restart the agent, any <code>sent</code> event will be seen as <code>sent</code> <strong>and</strong> <code>failed</code>. After a minute or so you revert the definition of <code>failed</code> to the proper definition, but you now have a few (admittedly bogus) failed event in datadog, allowing you to use the metric.</p>

<h1>Solution 2, clean:</h1>

<p>On your datadog dashboard, edit the relevant metric, and instead of using the graphical interface (tab <code>edit</code>) got to the <code>JSON</code> tab, where you can manually enter your metric name (probably a slightly updated cut &amp; paste of your <code>sent</code> metric), no matter if an event exists or not.</p>
"
Datadog,43699066,43146756,0,"2017/04/29, 21:21:56",False,"2017/04/29, 21:21:56",1371,5540166,0,"<p>Datadog monitors evaluate every minute, I think. So in your <code>sum(last_30m){X}</code> example, every minute, the monitor would sum the values of <code>{X}</code> over the last 30 minutes, and if that value was above whatever threshold you set, then it would trigger an alert. Same thing for <code>sum(last_1h){X}</code>, but every minute it would evaluate the sum over the last 1 hour.</p>
"
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371,5540166,1,"<ol>
<li><p>The first step will be to make sure you have the <a href=""https://app.datadoghq.com/account/settings#agent"" rel=""nofollow noreferrer"">datadog agent running</a>, and that the APM component of it is running and ready to receive trace data from your applications (<a href=""https://github.com/DataDog/dd-agent/blob/5.13.x/datadog.conf.example#L28"" rel=""nofollow noreferrer"">this option in your datadog.conf</a>, which must be set to ""true"").</p></li>
<li><p>Second, you'll want to install the appropriate library(ies) for the languages your applications are written in. You can find them all listed in your datadog account on this page: <a href=""https://app.datadoghq.com/apm/docs"" rel=""nofollow noreferrer"">https://app.datadoghq.com/apm/docs</a></p></li>
<li><p>Third, once the trace libraries are installed, you'll want to add trace integrations for the tools you're interested in collecting APM data on, and the instructions for those will be found in each library's docs. (E.g, <a href=""http://pypi.datadoghq.com/trace/docs/"" rel=""nofollow noreferrer"">Python</a>, <a href=""http://www.rubydoc.info/github/DataDog/dd-trace-rb/"" rel=""nofollow noreferrer"">Ruby</a>, and <a href=""https://godoc.org/github.com/DataDog/dd-trace-go/tracer"" rel=""nofollow noreferrer"">Go</a>)</p></li>
</ol>

<p>The integraitons will be a fairly quick way to get pretty granular spans on where your applications have higher latency, errors, etc. If from there you'd like to go even further, each library's docs also have instructions on how you can write your own custom tracing functions to expose more info on your custom applications--that's a little more work, but is fairly straight-forward. You'll probably want to add those bit-by-bit as you go.</p>

<p>Then you'd be all set, I think. You'll be tracing services, resources to get the latency, request-count, and error-count of your application requests, and you can drill down to the flame-graphs to further understand what requests spend the most time where in your applications.</p>

<p>Looking back now, seems like they made some recent changes to the setup process that makes it even easier to get the web framework and database integrations added if you're using Python. They've even got a command line tool in their <a href=""http://pypi.datadoghq.com/trace/docs/#get-started"" rel=""nofollow noreferrer"">get-started section now</a>.</p>

<p>Hope this helps! And reach out to their support team (support@datadoghq.com) if you run into issues along the way--they're always happy to lend a hand.</p>
"
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11,7120456,0,"<p>For now I see only two possibilities:</p>

<ol>
<li>Use GCP custom metrics using client from google-cloud-clients/google-cloud-monitoring and stackdriver integration with Datadog</li>
<li>Use datadog agent deployed in the cloud and connect to it using Datadog StatsD client (Java, Python, Go)</li>
</ol>

<hr>

<ol>
<li><p>Use GCP custom metrics
<a href=""https://cloud.google.com/monitoring/custom-metrics/creating-metrics"" rel=""nofollow noreferrer"">https://cloud.google.com/monitoring/custom-metrics/creating-metrics</a>
and datadog integration with GCP
<a href=""https://www.datadoghq.com/product/integrations/#cat-google-cloud"" rel=""nofollow noreferrer"">https://www.datadoghq.com/product/integrations/#cat-google-cloud</a></p>

<pre class=""lang-java prettyprint-override""><code>final MetricServiceClient client = MetricServiceClient.create();
ProjectName name = ProjectName.of(projectId);

MetricDescriptor descriptor = MetricDescriptor.newBuilder()
    .setType(metricType)
    .setDescription(""This is a simple example of a custom metric."")
    .setMetricKind(MetricDescriptor.MetricKind.GAUGE)
    .setValueType(MetricDescriptor.ValueType.DOUBLE)
    .build();

CreateMetricDescriptorRequest request = CreateMetricDescriptorRequest.newBuilder()
    .setName(name.toString())
    .setMetricDescriptor(descriptor)
    .build();

client.createMetricDescriptor(request);
</code></pre></li>
<li><p>Use datadog statsd client, java one -
<a href=""https://github.com/DataDog/java-dogstatsd-client"" rel=""nofollow noreferrer"">https://github.com/DataDog/java-dogstatsd-client</a> so you can deploy
datadog agent on GCP and connect through it. Sample with kubernetes.
<a href=""https://docs.datadoghq.com/tracing/setup/kubernetes/#deploy-agent-daemonset"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/setup/kubernetes/#deploy-agent-daemonset</a></p>

<pre class=""lang-java prettyprint-override""><code>import com.timgroup.statsd.ServiceCheck;
import com.timgroup.statsd.StatsDClient;
import com.timgroup.statsd.NonBlockingStatsDClient;

public class Foo {

  private static final StatsDClient statsd = new NonBlockingStatsDClient(
    ""my.prefix"",                          /* prefix to any stats; may be null or empty string */
    ""statsd-host"",                        /* common case: localhost */
    8125,                                 /* port */
    new String[] {""tag:value""}            /* Datadog extension: Constant tags, always applied */
  );

  public static final void main(String[] args) {
    statsd.incrementCounter(""foo"");
    statsd.recordGaugeValue(""bar"", 100);
    statsd.recordGaugeValue(""baz"", 0.01); /* DataDog extension: support for floating-point gauges */
    statsd.recordHistogramValue(""qux"", 15);     /* DataDog extension: histograms */
    statsd.recordHistogramValue(""qux"", 15.5);   /* ...also floating-point */
    statsd.recordDistributionValue(""qux"", 15);     /* DataDog extension: global distributions */
    statsd.recordDistributionValue(""qux"", 15.5);   /* ...also floating-point */

    ServiceCheck sc = ServiceCheck
          .builder()
          .withName(""my.check.name"")
          .withStatus(ServiceCheck.Status.OK)
          .build();
    statsd.serviceCheck(sc); /* Datadog extension: send service check status */

    /* Compatibility note: Unlike upstream statsd, DataDog expects execution times to be a
     * floating-point value in seconds, not a millisecond value. This library
     * does the conversion from ms to fractional seconds.
     */
    statsd.recordExecutionTime(""bag"", 25, ""cluster:foo""); /* DataDog extension: cluster tag */
  }
}
</code></pre>

<p>datadog deployment.yaml for kubernetes</p>

<pre class=""lang-yaml prettyprint-override""><code>apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: datadog-agent
spec:
  template:
    metadata:
      labels:
        app: datadog-agent
      name: datadog-agent
    spec:
      serviceAccountName: datadog-agent
      containers:
      - image: datadog/agent:latest
        imagePullPolicy: Always
        name: datadog-agent
        ports:
          - containerPort: 8125
            # hostPort: 8125
            name: dogstatsdport
            protocol: UDP
          - containerPort: 8126
            # hostPort: 8126
            name: traceport
            protocol: TCP
        env:
          - name: DD_APM_ENABLED
            value: ""true""
          - name: DD_API_KEY
            value: ""&lt;YOUR_API_KEY&gt;""
          - name: DD_COLLECT_KUBERNETES_EVENTS
            value: ""true""
          - name: DD_LEADER_ELECTION
            value: ""true""
          - name: KUBERNETES
            value: ""yes""
          - name: DD_KUBERNETES_KUBELET_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
        resources:
          requests:
            memory: ""256Mi""
            cpu: ""200m""
          limits:
            memory: ""256Mi""
            cpu: ""200m""
        volumeMounts:
          - name: dockersocket
            mountPath: /var/run/docker.sock
          - name: procdir
            mountPath: /host/proc
            readOnly: true
          - name: cgroups
            mountPath: /host/sys/fs/cgroup
            readOnly: true
        livenessProbe:
          exec:
            command:
            - ./probe.sh
          initialDelaySeconds: 15
          periodSeconds: 5
      volumes:
        - hostPath:
            path: /var/run/docker.sock
          name: dockersocket
        - hostPath:
            path: /proc
          name: procdir
        - hostPath:
            path: /sys/fs/cgroup
          name: cgroups
</code></pre></li>
</ol>

<p>Currently I'm investigating this so I'm not sure how to do this, yet. </p>
"
Datadog,49867829,49690040,0,"2018/04/17, 02:53:52",True,"2018/04/17, 02:53:52",356,2750290,1,"<p>It has support to CURL means you can make REST API calls. Try using some Http libraries like <strong>HttpURLConnection</strong> in java to make those POST requests. </p>

<p>I don't think we need a java based SDK for that as you can frame your own SDK on top of REST api's.</p>
"
Datadog,49706349,49699969,1,"2018/04/07, 13:26:10",False,"2018/04/07, 13:26:10",17842,7862821,0,"<p>There are two ways to access <code>dd-trace agent</code> on host from a container:</p>

<p><strong>1.</strong> Only on <code>&lt;HOST_IP&gt;:8126</code>, if docker container is started in a bridge network:</p>

<pre><code>docker run -d &lt;image_name&gt;
</code></pre>

<p><code>dd-trace agent</code> should be bound to <code>&lt;HOST_IP&gt;</code> or <code>0.0.0.0</code> (which includes <code>&lt;HOST_IP&gt;</code>).</p>

<p><strong>2.</strong> On <code>&lt;HOST_IP&gt;:8126</code> (if <code>dd-trace agent</code> is bound to <code>&lt;HOST_IP&gt;</code> or <code>0.0.0.0</code>) and <code>localhost:8126</code>, if docker container is started in the host network:</p>

<pre><code>docker run --network host -d &lt;image_name&gt;
</code></pre>

<p>As you already try to reach <code>dd-trace agent</code> on <code>localhost:8126</code>, so the second way is the best solution.</p>
"
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159,4907630,2,"<p>First step is to install the DataDog agent on the server in which you are running your application:</p>

<p><a href=""https://docs.datadoghq.com/agent/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/</a></p>

<p>You then need to enable the <code>DogStatsD</code> service in the DataDog agent:</p>

<p><a href=""https://docs.datadoghq.com/developers/dogstatsd/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/developers/dogstatsd/</a></p>

<p>After that, you can send metrics to the <code>statsd</code> agent using any Go library that connects to <code>statsd</code>.</p>

<p>For example:</p>

<p><a href=""https://github.com/DataDog/datadog-go"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-go</a></p>

<p><a href=""https://github.com/go-kit/kit/tree/master/metrics/statsd"" rel=""nofollow noreferrer"">https://github.com/go-kit/kit/tree/master/metrics/statsd</a></p>

<p>Here's an example program sending some counts using the first library:</p>

<pre><code>import (
    ""github.com/DataDog/datadog-go/statsd""
    ""log""
)

func main() {
    // Create the client
    c, err := statsd.New(""127.0.0.1:8125"")
    if err != nil {
        log.Fatal(err)
    }
    // Prefix every metric with the app name
    c.Namespace = ""myapp.""
    // Count two events
    err = c.Count(""my_counter"", 2, nil, 1)
    if err != nil {
        log.Fatal(err)
    }
    // Close the client
    err = c.Close()
    if err != nil {
        log.Fatal(err)
    }
}
</code></pre>
"
Datadog,51145348,51124961,0,"2018/07/03, 04:00:56",False,"2018/07/03, 04:00:56",3174,4639336,0,"<p>Here is a convenience wrapper for DD. It uses ENV vars to configure it, but it's a nice utility to package out common DD calls once you have the agent running in the background.</p>

<pre><code>package datadog

import (
    ""errors""
    ""log""
    ""os""
    ""regexp""
    ""sort""
    ""sync""
    ""time""

    ""github.com/DataDog/datadog-go/statsd""
)

var (
    mu     sync.RWMutex
    client = newClient()

    GlobalTags Tags = Tags{}
)

// Sometimes, the connection to datadog can fail, but because UDP is connectionless, we don't get insight into
// those failures. This loop just makes sure that once a minute, the client is refreshed.
func init() {
    go func() {
        for range time.Tick(time.Minute) {
            c := newClient()
            if c != nil {
                mu.Lock()
                client = c
                mu.Unlock()
            }
        }
    }()
}

func newClient() *statsd.Client {
    hp := os.Getenv(""DOGSTATSD_HOST_PORT"")
    if hp == """" {
        hp = ""127.0.0.1:8125""
    }

    c, err := statsd.New(hp)
    if err != nil {
        log.Println(""stat/datadog"", ""Could not create a datadog statsd client."", ""error"", err)
    }

    return c
}

type Tags map[string]string

func (tags Tags) StringSlice() []string {
    var stringSlice []string
    for k, v := range tags {
        if k != """" &amp;&amp; v != """" {
            stringSlice = append(stringSlice, formatName(k)+"":""+formatName(v))
        }
    }
    sort.Strings(stringSlice)
    return stringSlice
}

func mergeTags(tagsSlice []Tags) Tags {
    merged := Tags{}
    for k, v := range GlobalTags {
        merged[formatName(k)] = formatName(v)
    }
    for _, tags := range tagsSlice {
        for k, v := range tags {
            merged[formatName(k)] = formatName(v)
        }
    }
    return merged
}

func Gauge(name string, value float64, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Gauge(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

func Count(name string, value int64, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Count(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

func Histogram(name string, value float64, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Histogram(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

func Incr(name string, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Incr(formatName(name), mergeTags(tags).StringSlice(), 1)
}

func Decr(name string, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Decr(formatName(name), mergeTags(tags).StringSlice(), 1)
}

func Set(name string, value string, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Set(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

func Timing(name string, value time.Duration, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Timing(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

// Datadog allows '.', '_' and alphas only.
// If we don't validate this here then the datadog error logs can fill up disk really quickly
var nameRegex = regexp.MustCompile(`[^\._a-zA-Z0-9]+`)

func formatName(name string) string {
    return nameRegex.ReplaceAllString(name, ""_"")
}
</code></pre>
"
Datadog,52615071,51975736,0,"2018/10/02, 22:23:00",True,"2018/10/04, 04:20:38",899,4386440,1,"<p>I figured out how to do this using the datadog api <a href=""https://docs.datadoghq.com/api/?lang=python#post-timeseries-points"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/?lang=python#post-timeseries-points</a>. The following python script takes in the jtl file (jmeter results) and posts the transaction name, response time, and status (pass/fail) to datadog. </p>

<pre><code>#!/usr/bin/env python3
import sys
import pandas as pd
from datadog import initialize, api

options = {
    'api_key': '&lt;API_KEY&gt;',
    'app_key': '&lt;APPLICATION_KEY&gt;'
}
metrics = []

def get_current_metric(timestamp, label, elapsed, success):
    metric = {}
    metric.update({'metric': 'jmeter'})
    metric.update({'points': [(timestamp, elapsed)]})
    curtags = {}
    curtags.update({'testcase': label})
    curtags.update({'success': success})
    metric.update({'tags': curtags})
    return metric

initialize(**options)

jtl_file = sys.argv[1]
df = pd.read_csv(jtl_file)

for index, row in df.iterrows():
    timestamp = row['timeStamp']/1000
    label = row['label']
    elapsed = row['elapsed']
    success = str(row['success'])
    metric = get_current_metric(timestamp, label, elapsed, success)
    metrics.append(metric)

api.Metric.send(metrics)
</code></pre>
"
Datadog,52828393,52828258,1,"2018/10/16, 08:20:06",False,"2018/10/16, 08:44:48",21,10510929,2,"<p>You can actually just use:</p>

<pre><code>EOF
@slack-datadog-{{environment.name}}
EOF
</code></pre>

<p>Datadog's monitor templating feature will fill in the blank and forward to the relevant channel as long as you whitelisted it in the integrations tile for Slack.</p>
"
Datadog,61282340,52828258,0,"2020/04/18, 02:32:29",False,"2020/04/18, 02:32:29",5547,296829,1,"<p>Starts to get messy, but you could nest two ""does not"" conditional variables, like this:</p>

<pre><code> message = &lt;&lt;EOF
{{#is_match ""environment.name"" ""production""}}
   {{#is_alert}} @slack-datadog-production {{/is_alert}}
{{/is_match}}
{{#is_match ""environment.name"" ""uat""}}
   {{#is_alert}} @slack-datadog-uat {{/is_alert}}
{{/is_match}}

{{^is_match ""environment.name"" ""production""}}
   {{^is_match ""environment.name"" ""uat""}}
      {{#is_alert}} @slack-datadog {{/is_alert}}
   {{/is_match}}
{{/is_match}}
EOF
</code></pre>
"
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23,6286219,0,"<p>AFAIK it is not possible at the Moment to use micrometer to send events to datadog. Micrometer states that <em>""Micrometer is not a distributed tracing system or an event logger.</em>"" on its section ""1. Purpose"" on its <a href=""https://micrometer.io/docs/concepts"" rel=""nofollow noreferrer"">concepts page</a>.</p>
"
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371,5540166,1,"<p><a href=""https://docs.datadoghq.com/integrations/#cat-log-collection"" rel=""nofollow noreferrer"">This doc will show you a comprehensive list</a> of all integrations that involve log collection. Some of these include other common log shippers, which can also be used to forward logs to Datadog. Among these you'd find...</p>

<ul>
<li><a href=""https://docs.datadoghq.com/integrations/fluentd/#log-collection"" rel=""nofollow noreferrer"">Fluentd</a></li>
<li><a href=""https://docs.datadoghq.com/integrations/logstash/#log-collection"" rel=""nofollow noreferrer"">Logstash</a></li>
<li><a href=""https://docs.datadoghq.com/integrations/rsyslog/?tab=datadogussite"" rel=""nofollow noreferrer"">Rsyslog</a> (for linux)</li>
<li><a href=""https://docs.datadoghq.com/integrations/syslog_ng/?tab=datadogussite"" rel=""nofollow noreferrer"">Syslog-ng</a> (for linux, windows)</li>
<li><a href=""https://docs.datadoghq.com/integrations/nxlog/?tab=datadogussite"" rel=""nofollow noreferrer"">nxlog</a> (for windows)</li>
</ul>

<p>That said, you <a href=""https://docs.datadoghq.com/agent/faq/the-datadog-agent-for-logs-or-traces-only/?tab=logs"" rel=""nofollow noreferrer"">can still just use the Datadog agent to collect logs only</a> (they want you to collect everything with their agent, that's why they warn you against collecting just their logs).</p>

<p>If you want to collect logs from docker containers, the Datadog agent is an easy way to do that, and it has the benefit of adding lots of relevant docker-metadata as tags to your logs. (<a href=""https://docs.datadoghq.com/logs/log_collection/docker/?tab=containerinstallation"" rel=""nofollow noreferrer"">Docker log collection instructions here</a>.)</p>

<p>If you don't want to do that, I'd look at Fluentd first on the list above -- it has a good reputation for containerized log collection, promotes JSON log formatting (for easier processing), and scales reasonably well. </p>
"
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375,1803990,2,"<p>You need to tell Datadog to pull custom metric. Go to AWS integrations configuration page (Integrations side menu -> Integrations -> Amazon Web Services).</p>

<p>You will see a list of services to integrate with, custom metrics is the last option on list. Make sure it's ticked. Takes a while for Datadog to actually start pulling the metric.</p>

<p><a href=""https://i.stack.imgur.com/1rdOY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1rdOY.png"" alt=""enter image description here""></a></p>
"
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336,7243426,1,"<p>Do you have the ability to add some parameters in the logs sent. From <a href=""https://docs.datadoghq.com/tracing/advanced/connect_logs_and_traces/?tab=python#manual-trace-id-injection"" rel=""nofollow noreferrer"">the documentation</a> you should be able to inject the trace id into your logs in a way that Datadog will interpret them.</p>

<p>You can also look at a parser to extract the trace id and span id from the raw log. <a href=""https://docs.datadoghq.com/tracing/faq/why-cant-i-see-my-correlated-logs-in-the-trace-id-panel/?tab=withlogintegration"" rel=""nofollow noreferrer"">This documentation</a> should help you out on that.</p>
"
Datadog,61929834,58505214,1,"2020/05/21, 10:55:37",False,"2020/05/21, 10:55:37",8464,85348,1,"<p>From the documentation, if you don't have JSON logs, you need to include <code>dd.trace_id</code> and <code>dd.span_id</code> in your formatter: </p>

<blockquote>
  <p>If your logs are raw formatted, update your formatter to include
  <code>dd.trace_id</code> and <code>dd.span_id</code> in your logger configuration:</p>

<pre><code>&lt;Pattern&gt;""%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L -
%X{dd.trace_id:-0} %X{ dd.span_id:-0} - %m%n""&lt;/Pattern&gt; ```
</code></pre>
</blockquote>

<p>So if you add <code>%X{dd.trace_id:-0} %X{ dd.span_id:-0}</code>, it should work.</p>
"
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172,3514300,1,"<p>A better way would be to use a sidecar container with a logging agent, it won't increase the load on the API server.</p>

<p><a href=""https://i.stack.imgur.com/AE8K9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AE8K9.png"" alt=""enter image description here""></a></p>

<p>Reference: <a href=""https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent</a></p>

<p>Datadog agent looks like doesn't support /suggest running as a sidecar (<a href=""https://github.com/DataDog/datadog-agent/issues/2203#issuecomment-416180642"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-agent/issues/2203#issuecomment-416180642</a>)</p>

<p>I suggest looking at using other logging agent and pointing the backend to datadog.</p>

<p>Some options are:</p>

<ul>
<li>fluentd: <a href=""https://blog.powerupcloud.com/kubernetes-pod-management-using-fluentd-as-a-sidecar-container-and-prestop-lifecycle-hook-part-iv-428b5f4f7fc7"" rel=""nofollow noreferrer"">https://blog.powerupcloud.com/kubernetes-pod-management-using-fluentd-as-a-sidecar-container-and-prestop-lifecycle-hook-part-iv-428b5f4f7fc7</a></li>
<li>fluentd-bit: <a href=""https://github.com/leahnp/fluentbit-sidecar"" rel=""nofollow noreferrer"">https://github.com/leahnp/fluentbit-sidecar</a></li>
<li>filebeat: <a href=""https://www.elastic.co/beats/filebeat"" rel=""nofollow noreferrer"">https://www.elastic.co/beats/filebeat</a></li>
</ul>

<p>Datadog supports them</p>

<ul>
<li><a href=""https://docs.datadoghq.com/integrations/fluentd/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/fluentd/</a></li>
<li><a href=""https://docs.datadoghq.com/integrations/filebeat/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/filebeat/</a></li>
</ul>
"
Datadog,62096791,62092243,2,"2020/05/30, 04:32:35",True,"2020/05/30, 04:32:35",156,2676108,3,"<p>Is that sample text formatted properly? The final entity object is missing a <code>]</code> from the end.</p>

<p><code>entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }</code></p>

<p>should be </p>

<p><code>entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }]</code></p>

<p>I'm going to continue these instructions assuming that was a typo and the entity field actually ends with <code>]</code>. If it doesn't, I think you need to fix the underlying log to be formatted properly and close out the bracket.</p>

<hr>

<p>Instead of just skipping the entire log and only parsing out that json bit, I decided to parse the entire thing and show what would look good as a final result. So the first thing we need to do is pull out that set of key/value pairs after the request object:</p>

<p>Example Input: <code>thread-191555 app.main - [cid: 2cacd6f9-546d-41ew-a7ce-d5d41b39eb8f, uid: e6ffc3b0-2f39-44f7-85b6-1abf5f9ad970] Request: protocol=[HTTP/1.0] method=[POST] path=[/metrics] headers=[Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache] entity=[HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }]</code></p>

<p>Grok parser rule: <code>app_log thread-%{integer:thread} %{notSpace:file} - \[%{data::keyvalue("": "")}\] Request: %{data:request:keyvalue(""="","""",""[]"")}</code></p>

<p>Result: </p>

<pre><code>{
  ""thread"": 191555,
  ""file"": ""app.main"",
  ""cid"": ""2cacd6f9-546d-41ew-a7ce-d5d41b39eb8f"",
  ""uid"": ""e6ffc3b0-2f39-44f7-85b6-1abf5f9ad970"",
  ""request"": {
    ""protocol"": ""HTTP/1.0"",
    ""method"": ""POST"",
    ""path"": ""/metrics"",
    ""headers"": ""Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache"",
    ""entity"": ""HttpEntity.Strict application/json {\""type\"":\""text\"",\""extract\"": \""text\"", \""field2\"":\""text2\"",\""duration\"": 451 }""
  }
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/mVgn1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mVgn1.png"" alt=""app log parser""></a></p>

<p>Notice how we use the keyvalue parser with a quoting string of <code>[]</code>, that allows us to easily pull out everything from the request object.</p>

<hr>

<p>Now the goal is to pull out the details from that entity field inside of the request object. With Grok parsers you can specify a specific attribute to parse further.</p>

<p>So in that same pipeline we'll add another grok parser processor, right after our first</p>

<p><a href=""https://i.stack.imgur.com/5dpOf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5dpOf.png"" alt=""enter image description here""></a></p>

<p>And then configure the advanced options section to run on <code>request.entity</code>, since that is what we called the attribute</p>

<p><a href=""https://i.stack.imgur.com/Jm8S8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jm8S8.png"" alt=""enter image description here""></a></p>

<p>Example Input: <code>HttpEntity.Strict application/json {""type"":""text"",""extract"": ""text"", ""field2"":""text2"",""duration"": 451 }</code></p>

<p>Grok Parser Rule: <code>entity_rule %{notSpace:request.entity.class} %{notSpace:request.entity.media_type} %{data:request.entity.json:json}</code></p>

<p>Result: </p>

<pre><code>{
  ""request"": {
    ""entity"": {
      ""class"": ""HttpEntity.Strict"",
      ""media_type"": ""application/json"",
      ""json"": {
        ""duration"": 451,
        ""extract"": ""text"",
        ""type"": ""text"",
        ""field2"": ""text2""
      }
    }
  }
}
</code></pre>

<p>Now when we look at the final parsed log it has everything we need broken out:</p>

<p><a href=""https://i.stack.imgur.com/H6cF8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H6cF8.png"" alt=""enter image description here""></a></p>

<hr>

<p>Also just because it was really simple, I threw in a third grok processor for the headers chunk as well (the advanced settings are set to parse from <code>request.headers</code>):</p>

<p>Example Input: <code>Timeout-Access: &lt;function1&gt;, Remote-Address: 192.168.0.1:37936, Host: app:5000, Connection: close, X-Real-Ip: 192.168.1.1, X-Forwarded-For: 192.168.1.1, Authorization: ***, Accept: application/json, text/plain, */*, Referer: https://google.com, Accept-Language: cs-CZ, Accept-Encoding: gzip, deflate, User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko, Cache-Control: no-cache</code></p>

<p>Grok Parser Rule: <code>headers_rule %{data:request.headers:keyvalue("": "", ""/)(; :"")}</code></p>

<p>Result:</p>

<pre><code>{
  ""request"": {
    ""headers"": {
      ""Timeout-Access"": ""function1"",
      ""Remote-Address"": ""192.168.0.1:37936"",
      ""Host"": ""app:5000"",
      ""Connection"": ""close"",
      ""X-Real-Ip"": ""192.168.1.1"",
      ""X-Forwarded-For"": ""192.168.1.1"",
      ""Accept"": ""application/json"",
      ""Referer"": ""https://google.com"",
      ""Accept-Language"": ""cs-CZ"",
      ""Accept-Encoding"": ""gzip"",
      ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko"",
      ""Cache-Control"": ""no-cache""
    }
  }
}
</code></pre>

<p>The only tricky bit here is that I had to define a characterWhiteList of <code>/)(; :</code>. Mostly to handle all those special characters are in the <code>User-Agent</code> field. </p>

<hr>

<p><strong>References</strong>:</p>

<p>Just the documentation and some guess &amp; checking in my personal Datadog account.</p>

<p><a href=""https://docs.datadoghq.com/logs/processing/parsing/?tab=matcher#key-value-or-logfmt"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/processing/parsing/?tab=matcher#key-value-or-logfmt</a></p>
"
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138,2385808,2,"<p>When installing Datadog in your K8s Cluster, you install a <a href=""https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-node-logging-agent"" rel=""nofollow noreferrer"">Node Logging Agent</a> as a Daemonset with various volume mounts on the hosting nodes. Among other things, this gives Datadog access to the Pod logs at /var/log/pods and the container logs at /var/lib/docker/containers.</p>
<p>Kubernetes and the underlying Docker engine will only include output from stdout and stderror in those two locations (see <a href=""https://kubernetes.io/docs/concepts/cluster-administration/logging/#logging-at-the-node-level"" rel=""nofollow noreferrer"">here</a> for more information). Everything that is written by containers to log files residing inside the containers, will be invisible to K8s, unless more configuration is applied to extract that data, e.g. by applying the <a href=""https://kubernetes.io/docs/concepts/cluster-administration/logging/#streaming-sidecar-container"" rel=""nofollow noreferrer"">side care container pattern</a>.</p>
<p>So, to get things working in your setup, <strong>configure logback to log to stdout rather than /var/app/logs/myapp.log</strong></p>
<p>Also, if you don't use APM there is no need to instrument your code with the datadog.jar and do all that tracing setup (setting up ports etc).</p>
"
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852,1601506,2,"<p>You need to tell Datadog that you're interested in that content by creating a facet from the field. Click a log message, mouse over the attribute name, click the gear on the left, then Create facet for @...</p>
<p>For logs indexed after you create the facet, you can search with <code>@fieldName:text*</code>, where <code>fieldName</code> is the name of your field. You'll need to re-hydrate (reprocess) earlier logs to make them searchable.</p>
<p>You won't need to create a facet if you use fields from the <a href=""https://docs.datadoghq.com/logs/processing/attributes_naming_convention/#default-standard-attribute-list"" rel=""nofollow noreferrer"">standard attributes list</a>.</p>
"
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11,6062224,1,"<p>The error message itself is not a good fit to be defined as a facet.</p>
<p>If you are using JSON and want the main message (say from a <code>msg</code> json field) to be searchable in the Datadog <code>content</code> field. Instead of making
facet for <code>msg</code>, you can define a &quot;Message Remapper&quot; in the log configuration to map it to the <code>Content</code>. And then you can do wildcard searches.</p>
<p><a href=""https://i.stack.imgur.com/or0ld.png"" rel=""nofollow noreferrer"">log config screenshot</a></p>
"
Datadog,66307900,64126426,0,"2021/02/22, 00:39:04",False,"2021/02/22, 00:44:46",1272,1563480,0,"<p>After reading this documentation, <a href=""https://docs.datadoghq.com/tracing/setup_overview/setup/ruby/#resque"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/setup_overview/setup/ruby/#resque</a>, did you try making the options a hash with curly braces surrounding? Options is specified as being a hash. So everything after <code>c.use :resque, </code> should be a <strong>hash</strong>.</p>
"
Datadog,37795216,37188932,0,"2016/06/13, 19:53:12",False,"2016/06/13, 19:53:12",261,4172512,2,"<p>You can use the Agent's info command to see if the check is reporting correctly:</p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/203764635"" rel=""nofollow"">https://help.datadoghq.com/hc/en-us/articles/203764635</a></p>

<p>If the Agent Status shows the check is reporting correctly but your metrics are still not reporting you can contact support@datadoghq.com and they can troubleshoot this issue further.</p>

<p>Hope this helps!</p>
"
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334,3348604,1,"<p>This is not correct graph to detect correct resource limit. You graph shows CPU usage of your app in the cluster, but resource limit is per pod (container). We (and you as well) don't know from the graph how many containers were up and running. You can determinate right CPU limit from the container CPU usage graph(s). You will need Datadog-Docker integration:</p>

<blockquote>
  <p>Please be aware that Kubernetes relies on Heapster to report metrics,
  rather than the cgroup file directly. The collection interval for
  Heapster is unknown which can lead to innacurate time-related data,
  such as CPU usage. If you require more precise metrics, we recommend
  using the Datadog-Docker Integration.</p>
</blockquote>

<p>Then it will depends how Datadog measure CPU utilization per container. If container CPU utilization has max 100%, then 100% CPU container utilization ~ 1000m ~ 1. </p>

<p>I recommend you to read how and when cgroup limits CPU - <a href=""https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html"" rel=""nofollow"">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html</a></p>

<p>You will need a deep knowledge to set proper CPU limits. If you don't need to prioritize any container, then IMHO the best practice is to set 1 (<code>resources.requests.cpu</code>) for all your containers - they will have always equal CPU times.</p>
"
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213,5802417,-1,"<p>I am not familiar with the products and libraries that you are using, but there is an open source library <strong>MgntUtils</strong> that can extract full or filtered stacktrace from exception as a String. Since you mentioned that you can pass the text (i.e. String) this library may help you. Here are the links to <strong>MgntUtils</strong> library: </p>

<ol>
<li><a href=""https://www.linkedin.com/pulse/open-source-java-library-some-useful-utilities-michael-gantman?trk=pulse_spock-articles"" rel=""nofollow"">https://www.linkedin.com/pulse/open-source-java-library-some-useful-utilities-michael-gantman?trk=pulse_spock-articles</a> Detailed article that explains what utilities are available in <strong>MgntUtils</strong> library and how to use them</li>
<li><a href=""http://search.maven.org/#search%7Cga%7C1%7Cmichaelgantman"" rel=""nofollow"">http://search.maven.org/#search%7Cga%7C1%7Cmichaelgantman</a> - Here you can get it as Maven resource (also library, source and Javadoc available as a separate resource)</li>
<li>Github: <a href=""https://github.com/michaelgantman/Mgnt"" rel=""nofollow"">https://github.com/michaelgantman/Mgnt</a> - Here you can get this library as a git project as well as Jars for library itself, sources and javadoc (each in separate jar)</li>
</ol>

<p>I hope this helps</p>
"
Datadog,41968804,41964906,0,"2017/02/01, 00:39:07",True,"2017/02/01, 00:39:07",26,4313641,1,"<p>You need to pass <code>Content-Type</code> as a header with the request, as shown <a href=""http://docs.datadoghq.com/api/?lang=console#timeboards"" rel=""nofollow noreferrer"">in the docs</a></p>

<pre><code>$ curl -X POST -H ""Content-type: application/json"" 'https://app.datadoghq.com/api/v1/dash?api_key=&lt;key&gt;&amp;application_key=&lt;key&gt;' -d '{""dash"":{""title"":""Foo"",""description"":""bar"",""graphs"":[]}}'
</code></pre>

<p>Response:</p>

<pre><code>{""errors"": [""The parameter 'title' is required""]}
</code></pre>

<p>Your data is also not formatted according to the docs (there should be no <code>dash</code> field at the top level, for starters).</p>
"
Datadog,42839492,42815252,1,"2017/03/16, 18:18:52",True,"2017/03/16, 18:35:39",455,4676932,3,"<p>The format you're using to send the data does not comply with the <a href=""http://docs.datadoghq.com/api/?lang=console#tags-update"" rel=""nofollow noreferrer"">documentation</a> and your call fails to complete.</p>

<p>The call would work if you change your <code>$data</code> to:</p>

<pre><code>$data = [ 'tags' =&gt; ['env:prod'] ];
</code></pre>

<p>Agreed that the error returned by the API is not really helpful, filed an issue in Datadog to correct that  behavior and return the proper error code and not a 500 (it's actually a 500 and you can see it by printing <code>curl_getinfo($ch)</code> after you executed your curl session).</p>
"
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371,5540166,2,"<p>Why not use dogstatsd instead of threadstats? If you're already running the dd-agent on your node in a way that's reachable by your containers, you can use the <code>datadog.statsd.increment()</code> method instead to send the metric over statsd to the agent, and from there it would get forwarded to your datadog account. </p>

<p>Dogstatsd has the benefits of being more straight-forward and of being somewhat easier to troublehsoot, at least with debug-level logging. Threadstats sometimes has the benefit of not requiring a dd-agent, but it does very little (if any) error logging, so makes it difficult to troubleshoot cases like these.</p>

<p>If you went the dogstatsd route, you'd use the following code:</p>

<pre><code>from datadog import initialize
from datadog import statsd
statsd.increment('api.request_count', tags=['environment:' + environment])
</code></pre>

<p>And from there you'd find your metric metadata with the ""rate"" type and with an interval of ""10"", and you could use the ""as_count"" function to translate the values to counts.</p>
"
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644,4279006,0,"<p>In the python script I was initializing with an api key:</p>

<pre><code>from datadog import api
from datadog import initialize
from datadog import statsd
options = {
    'api_key':'#######'
}

initialize(**options)
</code></pre>

<p>And sending some events</p>

<pre><code>api.Event.create(title=title, text=text, tags=tags)
</code></pre>

<p>When I changed it to initialize like this, it started working with the dd-agent:</p>

<pre><code>initialize(statsd_use_default_route=True)
</code></pre>

<p>I didn't need the link command (--link dogstatsd:dogstastd).</p>

<p>With that setup it now works in the staging environment, but not in production. :/</p>
"
Datadog,45014280,44996259,0,"2017/07/10, 17:09:28",False,"2017/07/10, 17:09:28",1371,5540166,0,"<p>Maybe the <a href=""http://docs.datadoghq.com/api/#events-post"" rel=""nofollow noreferrer"">Datadog events-post api endpoint</a>? Or their <a href=""http://docs.datadoghq.com/api/#metrics-post"" rel=""nofollow noreferrer"">metrics-post endpoint</a>?</p>

<p>Although, it's not altogether clear to me what is meant by ""send this record only to Datadog"". If you use the log setup for Lambda log parsing that is described in <a href=""https://app.datadoghq.com/account/settings#integrations/amazon_lambda"" rel=""nofollow noreferrer"">Datadog's AWS Lambda integration tile</a>, that should be another perfectly good way to get the data collected into Datadog, but I think it may only collect those logs as metrics instead of events. </p>
"
Datadog,46122641,46122135,0,"2017/09/08, 21:34:13",True,"2017/09/08, 22:02:50",1269,3294286,1,"<p>I found the answer thanks to <a href=""https://stackoverflow.com/users/6815223/tqr-aupa-atleti"">@tqr_aupa_atleti</a> and the support team from Datadog.</p>

<p>On the Datadog dashboard panel, I had to click Metrics -> Summary and look for my metric. I looked at the tags and I could figure out it was a custom metric form my company that uses data from Amplitude.</p>
"
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939,2506172,2,"<blockquote>
  <p>However if you look at this metric carefully it appears to be
  calculating these percentiles on a short range (not sure what) and for
  each tuple of the tags that exist.</p>
</blockquote>

<p>The short range that you have noticed is actually the flush interval which defaults to 10 seconds. As per <a href=""https://help.datadoghq.com/hc/en-us/articles/205638045-What-is-the-histogram-metric-type-"" rel=""nofollow noreferrer"" title=""this"">this</a> article on histogram metric by datadog, </p>

<blockquote>
  <p>It aggregates the values that are sent during the flush interval
  (usually defaults to 10 seconds). So if you send 20 values for a
  metric during the flush interval, it'll give you the aggregation of
  those values for the flush interval</p>
</blockquote>

<p>For your query - </p>

<blockquote>
  <p>Ideally what I'd like to get is a 95th percentile of the ResponseTime
  metric over all the tags (maybe I filter it down by 1 or 2 and have a
  couple of different graphs) but over the last week or so. Is there an
  easy way to do this?</p>
</blockquote>

<p>as per my reading of the datadog docs, there isn't a way to get this done at the moment. It might be a good idea to check with datadog support regarding this.</p>

<p>More details <a href=""https://help.datadoghq.com/hc/en-us/articles/211545826-Why-histogram-stats-are-all-the-same-inaccurate-Characteristics-of-Datadog-histograms-"" rel=""nofollow noreferrer"" title=""here"">here</a>.</p>
"
Datadog,46827281,46826873,0,"2017/10/19, 12:56:44",True,"2017/10/19, 14:14:51",1371,5540166,3,"<p>I think what you want is to add the <a href=""https://github.com/DataDog/integrations-core/blob/5.18.x/process/conf.yaml.example#L21-L22"" rel=""nofollow noreferrer"">&quot;exact_match: false&quot;</a> option, like so:</p>
<pre><code>init_config:

instances:
  - name: ecommerce-order
    search_string: ['ecommerce-order']
    exact_match: False
    tags:
      - env:dev
</code></pre>
<p>This should match on any process whose path+name <em>include</em> the search string you provide.</p>
<p>Alternatively, if you only want it to match on the name of the process, you'll want to set the search_string to be the exact name of the process that's running (so whatever is given as the name when you run a <code>ps | grep &quot;ecommerce-order&quot;</code>, which in your case seems to be <code>ecommerce-order-0.0.1-SNAPSHOT.jar</code>)</p>
"
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629,1970882,1,"<p>I am on the same boat. I found this link: <a href=""https://docs.datadoghq.com/tracing/#instrument-your-application"" rel=""nofollow noreferrer"">datadog instrumentation</a>.
So, currently (20.11.2017) there are not agents for C#. Only Go, python and ruby are available.</p>

<p><a href=""https://i.stack.imgur.com/GoVcW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GoVcW.png"" alt=""screenshot 20.11.2017""></a></p>
"
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566,24231,0,"<p>(Disclosure: I'm a software developer at Datadog.)</p>

<p>Datadog's open-source .NET Tracer is currently (2019-02-11) in open beta. It supports manual and automatic instrumentation and <a href=""https://opentracing.io/guides/csharp/"" rel=""nofollow noreferrer"">OpenTracing</a>.</p>

<ul>
<li><a href=""https://github.com/DataDog/dd-trace-dotnet"" rel=""nofollow noreferrer"">GitHub</a></li>
<li><a href=""https://docs.datadoghq.com/tracing/languages/dotnet"" rel=""nofollow noreferrer"">Official docs</a></li>
</ul>

<p>For a list of currently supported languages/frameworks, see the <a href=""https://docs.datadoghq.com/tracing/languages/"" rel=""nofollow noreferrer"">updated documentation</a>.</p>

<p>Happy tracing!</p>
"
Datadog,47769427,47739117,1,"2017/12/12, 11:41:56",True,"2017/12/12, 11:41:56",2084,5156990,2,"<p>Looks like I found the problem - <a href=""https://github.com/DataDog/jenkins-datadog-plugin/issues/101"" rel=""nofollow noreferrer"">https://github.com/DataDog/jenkins-datadog-plugin/issues/101</a></p>

<p>current Datadog version 0.6.1. has a bug , after change the Jenkins main config ( any change , not related to Datadog configuration) it stop works.</p>

<p><strong>I downgrade it to 0.5.7 and it works OK</strong></p>
"
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371,5540166,0,"<p>Not possible today, but that is in Datadog's plans for development.</p>

<p>What you can do as a workaround, though, is add a link to your logs explorer with the query that triggered the monitor alert, so you can get a quick reference to what were the logs that triggered it. </p>

<p>This link, for example, would quickly scope you to the error logs over the last 15 minutes: <code>https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc</code></p>

<p>And markdown is supported, so you can keep your monitor messages prettier without long links in the messages. Like so:
<code>[Check the error logs here](https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc)</code></p>
"
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418,4379151,0,"<p>It looks to me that these are Datadog specific configuration parameters. So you first need to install the Datadog app to your Slack workspace, which you find on the Slack App Directory.</p>

<p>Here is how the process is described in the official documentation:</p>

<blockquote>
  <p><strong>Installation</strong></p>
  
  <p>The Slack integration is installed via its integration tile in the
  Datadog application.</p>
  
  <p><strong>Configuration</strong></p>
  
  <ol>
  <li>In your Slack account go to the Applications page and search for
  Datadog.</li>
  <li>Click Install, followed by Add Integration.</li>
  <li>Copy the Slack Service Hook and paste in the service hook field for
  Slack in Datadog.</li>
  <li>Add the channels you want to be able to post to.</li>
  <li>If you would like to be notified for every comment on a graph, tick
  the check box “Transfer all user comments” by each channel. If left
  unchecked (default) you will need to use the @slack-channel_name
  syntax for comments to be posted to slack.</li>
  </ol>
</blockquote>

<p>Source: <a href=""https://docs.datadoghq.com/integrations/slack/"" rel=""nofollow noreferrer"">official documentation</a> from Datadog</p>
"
Datadog,53290808,52891518,0,"2018/11/14, 01:03:15",False,"2018/11/14, 01:03:15",51,6111276,1,"<p>This might be a problem that <a href=""https://github.com/DataDog/integrations-core/issues/1829"" rel=""nofollow noreferrer"">other people are running into too</a>. kubelet is no longer listening on the ReadOnlyPort in newer Kubernetes versions, and the port is being deprecated. Samuel Cormier-Iijima reports that the issue can be solved by adding adding <code>KUBELET_EXTRA_ARGS=--read-only-port=10255</code> in <code>/etc/default/kubelet</code> on the node host.</p>
"
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113,4742614,1,"<p>It is in fact possible to send an alert if a metric shows the same value for a fix period of time. 
You can do this by using the diff() function to your query to produce delta values from consecutive delta points and then apply the abs() function to take absolute values of these deltas.</p>

<p>To do this we use the Arithmetic functions available, which can be applied using the '+' button to your query in UI. </p>

<p>For alert conditions in the metric monitor itself, configure as follows:
Select threshold alert
Set the “Trigger when the metric is…” dropdown selector to below or equal to
Set the “Alert Threshold” field to 0 (zero)</p>

<p>This configuration will trigger an alert event when no change in value has been registered over the selected timeframe.</p>

<p>Here is a link to datadog article: <a href=""https://docs.datadoghq.com/monitors/faq/how-can-i-configure-a-metric-monitor-to-alert-on-no-change-in-value/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/monitors/faq/how-can-i-configure-a-metric-monitor-to-alert-on-no-change-in-value/</a></p>
"
Datadog,54813734,54811591,0,"2019/02/21, 20:18:42",False,"2019/02/21, 20:18:42",790,1958107,1,"<p>By introducing space in datadog.json.j2 template definition .i.e.</p>

<pre><code> [{""source"":""{{ sourcea }}""{{ ',' }} ""service"":""{{ serviceb }}""}] (space at start)
</code></pre>

<p>and running deployment again I got the working config as below </p>

<pre><code>template:
    metadata:
      annotations:
        ad.datadoghq.com/yps.logs: ' [{""source"":""test"", ""service"":""test""}]'
</code></pre>

<p>However I am not able to understand the behaviour if anyone could help me understand it</p>
"
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371,5540166,0,"<p>it appears to me like you're missing a couple environment vars in your docker-compose <code>datadog</code> service configuration. And also the volume that adds the registry for tailing the logs from the docker socket. Maybe try something like this if you haven't?</p>

<pre><code>  # agent section
  datadog:
    build: datadog
    links:
     - redis # ensures that redis is a host that the container can find
     - web # ensures that the web app can send metrics
    environment:
     - DD_API_KEY=34f-------63c
     - DD_LOGS_ENABLED=true
     - DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL=true
     - DD_AC_EXCLUDE=""name:datadog-agent""
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock:ro
     - /proc/:/host/proc/:ro
     - /sys/fs/cgroup/:/host/sys/fs/cgroup:ro
     - /opt/datadog-agent/run:/opt/datadog-agent/run:rw
</code></pre>

<p>from there, if you still end up with trouble, you may want to reach out to support@datadoghq.com to ask for help. They're pretty quick to reply. </p>
"
Datadog,55806500,55632833,0,"2019/04/23, 10:46:35",True,"2019/04/23, 10:46:35",410,5254815,0,"<p>Installed it as ""Run with Admin rights"" and it fixed the issue.</p>
"
Datadog,57805506,55632833,0,"2019/09/05, 15:28:28",False,"2019/09/05, 15:28:28",971,3271599,0,"<p>For me, I had to manually give the <code>ddagentuser</code> account read access to the file</p>

<blockquote>
  <p>C:\ProgramData\Datadog\auth_token</p>
</blockquote>
"
Datadog,59403759,58599000,0,"2019/12/19, 07:30:11",False,"2019/12/19, 09:15:25",1,12559838,0,"<p>Have you tried specifying dependencies with <strong>go mod</strong> yet?
I faced the same issue and finally can solved by generating <strong>go.mod</strong> file with these command,</p>

<pre><code>GO111MODULE=on go mod init

GO111MODULE=on go mod tidy
</code></pre>

<p><a href=""https://cloud.google.com/functions/docs/writing/specifying-dependencies-go"" rel=""nofollow noreferrer"">Here</a> is the details explanation.</p>
"
Datadog,61108577,61091978,0,"2020/04/08, 22:31:05",False,"2020/04/08, 22:31:05",61,6722990,2,"<p>I think the disconnect here is that the pattern needs to be in the Jboss log manager, and then they're encoded to JSON.</p>

<p>Have you tried puttinng <code>%X{dd.trace_id:-0} %X{dd.span_id:-0}</code> into your Jboss logging pattern? </p>

<p>If not, I'd also recommend opening a ticket at support@datadoghq.com and we can work this through with you.</p>
"
Datadog,66962237,61092487,0,"2021/04/06, 06:12:46",False,"2021/04/06, 06:12:46",733,1219379,0,"<p>I’m not sure if it is a recent addition, but the Datadog public API supports configuring Log Archives: <a href=""https://docs.datadoghq.com/api/latest/logs-archives/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/latest/logs-archives/</a></p>
<p>You can also use tools like Terraform to configure them: <a href=""https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/logs_archive"" rel=""nofollow noreferrer"">https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/logs_archive</a> (it uses the Datadog API internally)</p>
"
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408,3754710,1,"<p>If you are trying to connect to an HTTPS URL for Datadog (<code>https://app.datadoghq.com</code> in your example), then you will need to set the <code>https.proxyHost</code> system property for it to have effect - <code>http.proxyHost</code> is for HTTP URLs[1]. These are system-wide settings that will be used by the default <code>HttpSender</code> (<code>HttpUrlConnectionSender</code>) if a <code>Proxy</code> is not passed to its constructor.</p>

<blockquote>
  <p>The micrometer doc says</p>

<pre><code>management.metrics.export.datadog.uri=https://app.datadoghq.com # URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Datadog, you can define the location of the proxy with this.
</code></pre>
  
  <p>But I dont understand what it means? should I replace this url with my
  proxy url or is there any specific uri pattern with the proxy?</p>
</blockquote>

<p>This is referring to a different kind of proxy that you would configure to receive your Datadog traffic on your internal network, and it would forward it to Datadog outside of your network. If you are using an HTTP proxy then you should use the system properties or an <code>HttpSender</code> with your HTTP proxy configured (e.g. an <code>HttpUrlConnectionSender</code> and passing a <code>Proxy</code> to its constructor).</p>

<p>You can configure a custom <code>HttpSender</code> with a <code>DatadogMeterRegistry</code> using its <code>Builder</code>. If you expose this as a <code>Bean</code> in a <code>@Configuration</code> class, Spring Boot will use it in its auto-configuration. For example:</p>

<pre><code>@Bean
public DatadogMeterRegistry datadogMeterRegistry(DatadogConfig config, Clock clock) {
    HttpSender httpSender = new HttpUrlConnectionSender(config.connectTimeout(), config.readTimeout(), new Proxy(Proxy.Type.HTTP, new InetSocketAddress(""myproxy"", 8080)));
    return DatadogMeterRegistry.builder(config).clock(clock).httpClient(httpSender).build();
}
</code></pre>

<hr>

<p>[1] <a href=""https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html</a></p>
"
Datadog,62798229,62787931,3,"2020/07/08, 18:23:57",True,"2020/07/08, 18:23:57",1371,5540166,2,"<p>That's all handled in the <code>message</code> attribute with <a href=""https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables"" rel=""nofollow noreferrer"">conditional logic variables</a>.</p>
<p>If, for example, you define your <code>message</code> value to be this...</p>
<pre><code>{{#is_alert}}
High Priority @pagerduty
{{/is_alert}}

{{#is_warning}}
Medium Priority @slack-mychannel
{{/is_warning}}

You should reference [this dashboard](mydashboardlink) to see how bad this is and follow [these steps](myrunbook) to resolve the situation.
</code></pre>
<p>... then ...</p>
<ol>
<li>when your monitor crosses the alert threshold, it will include &quot;High Priority&quot; in the message and it will send a notification to your pagerduty integration,</li>
<li>when it crosses the warn threshold it will include &quot;Medium Priority&quot; and send a notification to your slack channel &quot;mychannel&quot;, and</li>
<li>the &quot;you should reference&quot; part with interesting links would always show up.</li>
</ol>
"
Datadog,63907036,63859370,0,"2020/09/15, 20:27:53",False,"2020/09/16, 01:09:03",31,12517930,0,"<p>got this resolved with below</p>
<p>rule1 <code>%{ipv4:network.client.ip}\s+-\s+%{word:user}\s+\[%{date(&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;):date}\]\s+\&quot;%{word:http.module}\s+\/v1\/resources\/+%{word:onemds.module}\?+%{data:onemds.params:keyvalue(&quot;=&quot;,&quot;/:&quot;,&quot;&quot;,&quot;:&amp;&quot;)}</code></p>
<p>was never expecting the delimiter that can take 2 characters above <code>:&amp;</code></p>
<p>this helped to remove <strong>rs:</strong> for all except the first one. not an elegant approach but it worked for my use case.</p>
"
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23,9053059,1,"<p>Found the answer:</p>
<p>The metric <code>kubernetes.kubelet.volume.stats.used_bytes</code> will allow you to get the disk usage on PersistentVolumes. This can be achieved using the tag <code>persistentvolumeclaim</code> on this metric.</p>
<p>You can view this metric and tag combination in your account here: <a href=""https://app.datadoghq.com/metric/summary?filter=kubernetes.kubelet.volume.stats.used_bytes&amp;metric=kubernetes.kubelet.volume.stats.used_bytes"" rel=""nofollow noreferrer"">https://app.datadoghq.com/metric/summary?filter=kubernetes.kubelet.volume.stats.used_bytes&amp;metric=kubernetes.kubelet.volume.stats.used_bytes</a></p>
<p>Documentation on this metric can be found here: <a href=""https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubelet"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubelet</a></p>
"
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156,2676108,1,"<p>This answer doesn't solve the problem, because postgres is not running in the cluster, it's running in Azure. I'll leave it up since it might be interesting, but I posted another answer for the actually environment setup.</p>
<hr />
<p>For containerized setups, it's not usually recommended to set up a configmap or try giving the agent a yaml file. Instead the recommended configuration is to put annotations on the postgres pod: <a href=""https://docs.datadoghq.com/integrations/postgres/?tab=containerized#containerized"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/postgres/?tab=containerized#containerized</a>.</p>
<p>This concept of placing the config on the application pod, not with the datadog agent, is called autodiscovery. This blog post does a good job explaining the benefits of this solution: <a href=""https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/#autodiscovery"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/#autodiscovery</a></p>
<p>Here is a picture diagram showing how the agent goes out to the pods on the same node and would pull the configuration from them:</p>
<p><a href=""https://i.stack.imgur.com/XIZkM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XIZkM.png"" alt=""kubernetes and daemonset agent diagram"" /></a></p>
<p>To configure this, you'd take each of the sections of the yaml config, convert them to json, and set them as annotations on the postgres manifest. An example of how to set up pod annotations is provided for redis, apache, and http here: <a href=""https://docs.datadoghq.com/agent/kubernetes/integrations/?tab=kubernetes#examples"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/kubernetes/integrations/?tab=kubernetes#examples</a></p>
<p>For your scenario I would do something like:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: mypostgres
  annotations:
    ad.datadoghq.com/mypostgres.check_names: '[&quot;postgres&quot;]'
    ad.datadoghq.com/mypostgres.init_configs: '[{}]'
    ad.datadoghq.com/mypostgres.instances: |
      [
        {
          &quot;host&quot;:&quot;%%host%%&quot;, 
          &quot;port&quot;:5432,
          &quot;username&quot;:&quot;my-user&quot;,
          &quot;password&quot;:&quot;some-password&quot;
        }
      ]
  labels:
    name: mypostgres
spec:
  containers:
    - name: mypostgres
      image: postgres:latest
      ports:
        - containerPort: 5432
</code></pre>
<p>notice how the folder name <code>postgres.d/conf.yaml</code> maps to the <code>check_names</code> annotation, the <code>init_configs</code> section maps to <code>init_configs</code> annotation, etc.</p>
<hr />
<p>For the section on custom metrics, since I personally am more familiar with the yaml config, and it's easier to just fill out, I'll usually go to a yaml to json converter, and copy the json from there</p>
<p><a href=""https://i.stack.imgur.com/qkU0W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qkU0W.png"" alt=""yaml to json converter screeenshot"" /></a></p>
<pre><code>metadata:
  name: mypostgres
  annotations:
    ad.datadoghq.com/mypostgres.instances: |
      [
        {
          &quot;host&quot;: &quot;%%host%%&quot;,
          &quot;port&quot;: 5432,
          &quot;username&quot;: &quot;my-user&quot;,
          &quot;password&quot;: &quot;some-password&quot;,
          &quot;dbname&quot;: &quot;some-database&quot;,
          &quot;ssl&quot;: true,
          &quot;tags&quot;: [
            &quot;some_tag&quot;
          ],
          &quot;custom_queries&quot;: [
            {
              &quot;metric_prefix&quot;: &quot;some.prefix&quot;,
              &quot;query&quot;: &quot;SELECT COUNT(*) FROM bla WHERE timestamp &gt; NOW() - INTERVAL '1 hour';&quot;,
              &quot;columns&quot;: [
                {
                  &quot;name&quot;: &quot;countLastHour&quot;,
                  &quot;type&quot;: &quot;count&quot;
                }
              ]
            }
          ]
        }
      ]
</code></pre>
<hr />
<p>A key thing to notice for all those configs is that I never set the hostname. That is automatically discovered by the agent as it scans through containers.</p>
<p>However you may have set <code>my-postgres-host.com</code> because this postgres instance is not actually running in your kubernetes cluster, and is instead living on its own, and not in a container. If that is the case, I would recommend trying to just put the agent on the postgres node directly, all the yaml stuff you've done would work just fine, if that db and the agent are both directly on the vm.</p>
"
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156,2676108,1,"<p>It looks like the question has been updated to say that this postgres db you are trying to monitor is not actually running the the cluster. And you are not able to put an agent directly on the postgres server since it's a managed service in Azure, so you don't have access to the underlying host.</p>
<p>In those situations it is common to have a random datadog agent on some other host set up the postgres integration anyway, but instead of having <code>host: localhost</code> in the yaml config, put the hostname you would put to access the db externally. In your example it was <code>host: my-postgres-host.com</code>. This provides all the same benefits of the normal integration (except you won't have cpu/disk/resource metrics available obviously)</p>
<p>This is all fine and makes sense, but what if all of the agents you have installed are the agents in the kubernetes daemonset you created? You don't have any hosts directly on VMs to run this check. But we definitely don't recommend configuring the daemonset to run this check directly. If you did, that would mean you are collecting duplicate metrics from that one postgres db in every single node in your cluster. Since every agent is a copy, they'd each be running the same check on the same db you define.</p>
<p><a href=""https://i.stack.imgur.com/TLiQh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TLiQh.png"" alt=""enter image description here"" /></a></p>
<p>Luckily I notice that you are running the Datadog Cluster Agent. This is a separate Datadog tool that is deployed as a single service once per cluster, instead of a daemonset running once per node. It is possible to have the cluster agent configured to run 'cluster level' checks. Perfect for things like databases, message queues, or http checks.</p>
<p>The basic idea is that (in addition to it's other jobs) the cluster agent will also schedule checks. the DCA (datadog cluster agent) will choose one agent from the daemonset to run the check, and if that node agent pod dies, the DCA will find a new one to run the cluster check.</p>
<p><a href=""https://i.stack.imgur.com/dEH0y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dEH0y.png"" alt=""cluster agent scheduling node agent to query postgres"" /></a></p>
<p>Here are the docs on how to set up the DCA to run cluster checks: <a href=""https://docs.datadoghq.com/agent/cluster_agent/clusterchecks/#how-it-works"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/cluster_agent/clusterchecks/#how-it-works</a></p>
<p>To configure it you would enable some flags, and give the DCA the yaml file you created with a config map, or just mounting the file directly. The DCA will pass along that config to whichever node agent it chooses to run the check.</p>
"
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878,180368,1,"<p>Answering my own question.</p>
<p>The DataDog logging page has a Configuration section.  On that page the &quot;Pre processing for JSON logs&quot; section allows you to specify alternate property names for a few of the major log message properties.  If you add @m to the Message attributes section and @l to the Status attributes section you will correctly ingest JSON messages from the <code>RenderedCompactJsonFormatter</code> formatter.  If you add RenderedMessage and Level respectively you will correctly ingest <code>JsonFormatter(renderMessage: true)</code> formatter.  You can specify multiple attributes in each section, so you can simultaneously support both formats.</p>
"
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383,2032722,1,"<p>After few days of research and follow up with datadog support team, I am able to get the APM logs on datadog portal.</p>
<blockquote>
<p>Below is my <code>docker-compose.yml</code> file configuration,  I believe it helps someone in future</p>
</blockquote>
<pre><code>version: &quot;3&quot;
services:
  web:
    build: web
    command: ddtrace-run python standalone_api.py 
    volumes:
      - .:/usr/src/app
    depends_on: 
      datadog-agent:
        condition: service_healthy         
    image: pythonbusinessservice:ICDNew
    ports: 
     - 5000:5000
    environment:     
    - DATADOG_HOST=datadog-agent
    - DD_TRACE_AGENT_PORT=8126
    - DD_AGENT_HOST=datadog-agent
  datadog-agent:
    build: datadog
    image: gcr.io/datadoghq/agent:latest
    ports: 
     - 8126:8126          
    environment: 
     - DD_API_KEY=9e3rfg*****************adf3
     - DD_SITE=datadoghq.com
     - DD_HOSTNAME=pythonbusinessservice
     - DD_TAGS=env:dev      
     - DD_APM_ENABLED=true
     - DD_APM_NON_LOCAL_TRAFFIC=true
     - DD_DOGSTATSD_NON_LOCAL_TRAFFIC=true     
     - DD_SERVICE=pythonbusinessservice 
     - DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL=true 
     - DD_CONTAINER_EXCLUDE=&quot;name:datadog-agent&quot;      
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock
     - /proc/:/host/proc/:ro
     - /opt/datadog-agent/run:/opt/datadog-agent/run:rw
     - /sys/fs/cgroup:/host/sys/fs/cgroup:ro
</code></pre>
<blockquote>
<p>The <code>Dockerfile</code> for my python long running application</p>
</blockquote>
<pre><code>FROM python:3.7

COPY . /app
WORKDIR /app

RUN pip install -r requirements.txt

CMD [&quot;ddtrace-run python&quot;, &quot;/app/standalone_api.py&quot;]
</code></pre>
<p>Please note, on the requirements.txt file I have  <code>ddtrace</code> package listed</p>
"
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334,3348604,1,"<p>This is not correct graph to detect correct resource limit. You graph shows CPU usage of your app in the cluster, but resource limit is per pod (container). We (and you as well) don't know from the graph how many containers were up and running. You can determinate right CPU limit from the container CPU usage graph(s). You will need Datadog-Docker integration:</p>

<blockquote>
  <p>Please be aware that Kubernetes relies on Heapster to report metrics,
  rather than the cgroup file directly. The collection interval for
  Heapster is unknown which can lead to innacurate time-related data,
  such as CPU usage. If you require more precise metrics, we recommend
  using the Datadog-Docker Integration.</p>
</blockquote>

<p>Then it will depends how Datadog measure CPU utilization per container. If container CPU utilization has max 100%, then 100% CPU container utilization ~ 1000m ~ 1. </p>

<p>I recommend you to read how and when cgroup limits CPU - <a href=""https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html"" rel=""nofollow"">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html</a></p>

<p>You will need a deep knowledge to set proper CPU limits. If you don't need to prioritize any container, then IMHO the best practice is to set 1 (<code>resources.requests.cpu</code>) for all your containers - they will have always equal CPU times.</p>
"
Datadog,66329196,66172172,1,"2021/02/23, 10:00:35",False,"2021/02/23, 10:00:35",871,7779815,0,"<p>In the <a href=""https://medium.com/ni-tech-talk/monitoring-confluent-cloud-kafka-with-datadog-natural-intelligence-e0fed5df535"" rel=""nofollow noreferrer"">instructions</a> it refers to http://ccloudexporter_ccloud_exporter_1:2112/metrics in the open metrics file, but in my setup docker-compose gave the ccloud exporter container the name ccloud_ccloud_exporter_1.
To prevent this I added &quot;container_name: ccloud_ccloud_exporter_1&quot; in the docker compose file and used &quot;http://ccloud_ccloud_exporter_1:2112/metrics&quot; as prometheus url in the openmetrics file.</p>
"
Datadog,64973799,64929616,0,"2020/11/23, 20:02:47",True,"2020/11/23, 20:08:08",251,5477963,0,"<p>Tracing for requests coming from the browser are handled by RUM and not APM. I can't find the documentation for it but there is a configuration option on the <a href=""https://github.com/DataDog/browser-sdk/blob/701a0f81b303fad50d9f16e724af8143ab0fffe7/packages/core/src/domain/configuration.ts#L7"" rel=""nofollow noreferrer"">browser SDK</a> to allow tracing to specific endpoints. The tracing context will automatically be propagated to APM if enabled on the server side, and both the browser and server spans will appear in the same trace.</p>
"
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563,605153,2,"<p>Usually metrics exposed to <code>/actuator/metrics</code> are sent to the metrics system like datadog.</p>
<p>You can try to check what exactly gets sent to datadog by examining the source code of <a href=""https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java"" rel=""nofollow noreferrer"">DatadogMeterRegistry</a></p>
<p>Put a breakpoint in the publish method and see what gets sent, or, alternatively set the logger of the class to &quot;trace&quot; so that it will print the information that gets sent to the datadog (line 131 in the linked source code).</p>
<p>Another possible direction to check is usage of filters (see <a href=""https://www.javadoc.io/doc/io.micrometer/micrometer-core/1.0.3/io/micrometer/core/instrument/config/MeterFilter.html"" rel=""nofollow noreferrer"">MeterFilter</a>) that can filter out some metrics.</p>
"
Datadog,63835825,63814864,0,"2020/09/10, 21:39:35",True,"2020/09/10, 22:25:47",205,8534030,2,"<p>This did the trick : Thanks to @MarkBramnik</p>
<pre><code>    @Bean
    @Primary
    CompositeMeterRegistry compositeMeterRegistry(DatadogMeterRegistry datadogMeterRegistry, LoggingMeterRegistry loggingMeterRegistry) {
        CompositeMeterRegistry compositeMeterRegistry = new CompositeMeterRegistry();
        compositeMeterRegistry.add(datadogMeterRegistry);
        compositeMeterRegistry.add(loggingMeterRegistry);
        return compositeMeterRegistry;
    }
</code></pre>
"
Datadog,65920537,63314162,2,"2021/01/27, 15:50:52",True,"2021/01/27, 15:50:52",48,8161041,1,"<p>Try this</p>
<pre><code>&lt;encoder class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;&gt;
        &lt;customFields&gt;
            {&quot;service&quot;:&quot;ServiceName&quot;,&quot;ddsource&quot;:&quot;java&quot;}
        &lt;/customFields&gt;...
&lt;/encoder&gt;
</code></pre>
"
Datadog,62545412,62545185,0,"2020/06/24, 02:43:20",False,"2020/06/24, 02:43:20",1371,5540166,1,"<p>You might be able to get this if you...</p>
<ol>
<li>Start creating a metric type monitor</li>
<li>To the far right of the metric definition, select &quot;advanced&quot;</li>
<li>Select &quot;Add Query&quot;</li>
<li>Input your metrics</li>
<li>In the field called &quot;Express these queries as:&quot;, input <code>(a-b)/b</code> or some such</li>
<li>Trigger when the metric is <code>above or equal to</code> the threshold <code>in total</code> during the last <code>24 hours</code></li>
<li>Set Alert threshold &gt;= <code>0.05</code></li>
</ol>
<p>If you start having trouble as you start setting it up, you may want to reach out to support@datadoghq.com to get their assistance.</p>
"
Datadog,61524549,61521576,0,"2020/04/30, 16:28:56",False,"2020/04/30, 16:28:56",1824,944768,3,"<p>this seem to be working:
<code>-@userId:*?*</code> do not forget the minus at the start.</p>
"
Datadog,61128911,61108009,2,"2020/04/09, 22:36:58",False,"2020/04/15, 21:52:15",1371,5540166,1,"<p>Either of the <a href=""https://docs.datadoghq.com/dashboards/functions/count/#count-not-null"" rel=""nofollow noreferrer""><code>count_not_null()</code></a> or <a href=""https://docs.datadoghq.com/dashboards/functions/count/#count-non-zero"" rel=""nofollow noreferrer""><code>count_nonzero()</code></a> functions should get you where you want.</p>

<p>If graph your metric, grouped by your tag, and then apply one of those functions, it should return the count of unique tag values under that tag key. So in your case:</p>

<p><code>count_not_null(sum:your.metric.name{*} by {file_name})</code></p>

<p>And it works with multiple group-by tags too, so if you had separate tags for <code>file_name</code> and <code>directory</code> then you could use this same approach to graph the count of unique <em>combinations</em> of these tag values, or the count of unique combinations of <code>directory</code>+<code>file_name</code>:</p>

<p><code>count_not_null(your.metric.name{*} by {file_name,directory})</code></p>
"
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371,5540166,0,"<p>Yes, it is possible. You can do that in a <a href=""https://app.datadoghq.com/logs/pipelines"" rel=""nofollow noreferrer"">processing pipeline</a> with a grok parser, but you'll want to configure which attribute the grok parser applies to in the advanced settings (<a href=""https://docs.datadoghq.com/logs/processing/parsing/?tab=matcher#advanced-settings"" rel=""nofollow noreferrer"">docs here</a>). (By default grok parsers apply to the ""message"" attribute, but you can configure them to parse any attribute.)</p>

<p>In this case, you'd set the <code>Extract From</code> field to <code>requestUri</code>. The <code>Helper Rules</code> section is not necessary for this. And then in the main <code>Define Parsing Rules</code> section, you'll plug in a rule similar to this:</p>

<pre><code>parse_customer_id \/customers\/%{notSpace:customerId}\/users
</code></pre>

<p>or even further</p>

<pre><code>parse_customer_id \/%{notSpace}\/%{notSpace:customerId}\/%{notSpace}
</code></pre>
"
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336,7243426,1,"<p>What about using the template variables <a href=""https://docs.datadoghq.com/dashboards/template_variables/#pagetitle"" rel=""nofollow noreferrer"">doc</a>? </p>

<p>You could select: </p>

<ul>
<li>Name: Name, </li>
<li>Tag or Attribute: name, </li>
<li>Default Value: dev-db-master</li>
</ul>

<p>Then you'll be able to replace your <code>{name:$flavor-db-master}</code> with <code>{$Name}</code></p>

<p>Otherwise, if you actually wants the value of the template variable you have to use <code>$flavor.value</code>. I advise to use a not widget to check the actual behavior.</p>

<p>EDIT:</p>

<p>This kind of setup is not the recommended. It would be better to set two tags on your database: </p>

<ul>
<li><code>env:dev</code> or <code>env:prod</code> </li>
<li><code>dbname:db1-master</code> or <code>dbname:db2-master</code>. </li>
</ul>

<p>You would then have a unique selection of tags, <code>env:dev,dbname:db1-master</code>. It would then be easy to have a query such as:</p>

<pre><code>""q"": ""avg:aws.rds.bin_log_disk_usage{$Env,dbname:db1-master}""
</code></pre>
"
Datadog,59359866,59357130,0,"2019/12/16, 17:42:43",True,"2019/12/16, 17:42:43",1371,5540166,2,"<p>You can do this in a processing pipeline with 2 steps:</p>

<ol>
<li>Set up a <a href=""https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor"" rel=""nofollow noreferrer""><code>Category Processor</code></a> with a rule that matches on the query <code>Service is running</code> and applies a new attribute to those logs with a value of <code>level:info</code></li>
<li>Set up a <a href=""https://docs.datadoghq.com/logs/processing/processors/?tab=ui#log-status-remapper"" rel=""nofollow noreferrer""><code>Status Remapper</code></a> to take the status from the attribute called <code>level</code></li>
</ol>

<p>If there are other queries/patterns you want to use to determine the log level/status, you can add multiple rules to the <code>Category Processor</code> in (1), and you can map the <code>level</code> value to <code>info/warn/error</code> and any other relevant status value. </p>
"
Datadog,58760072,58725596,0,"2019/11/08, 05:46:30",False,"2019/11/08, 05:46:30",156,2676108,1,"<p>You mostly have to wait for it all to fill in over time.</p>

<blockquote>
  <p>Metric timestamps cannot be more than 10 minutes in the future or more than 1 hour in the past.</p>
</blockquote>

<p><a href=""https://docs.datadoghq.com/developers/metrics/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/developers/metrics/</a></p>
"
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107,10913713,1,"<p>I got the answer to the second question. Now, I can get all tables from one database that I specified. All I needed to do; relation_regex: '.*' and disabled relation_name.</p>

<p>Answer to the first question I got from datadog is that there is no way to monitor all the DBs without listing them individually. They may change this in future, but for now we have to add blocks for each and every database that we want to monitor</p>
"
Datadog,61927265,56382266,0,"2020/05/21, 07:25:17",False,"2020/05/21, 07:25:17",728,2218580,0,"<p>This works for me:</p>

<pre><code>const { createLogger, format, transports } = require('winston')
const { combine, timestamp, json } = format

function dataDogLogger(options) 
{
    const logger = createLogger({
        exitOnError: false,
        format: combine(
            // add a timestamp to all logs
            timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),
            json()
        ),
        transports: [ ...whatever you need here... ]
    })

    return logger
}

module.exports = dataDogLogger
</code></pre>
"
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059,8763847,4,"<p>What you are doing is correct only, however, the common mistake is not following the below.</p>

<blockquote>
  <p>This library MUST be imported and initialized before any instrumented
  module. When using a transpiler, you MUST import and initialize the
  tracer library in an external file and then import that file as a
  whole when building your application. This prevents hoisting and
  ensures that the tracer library gets imported and initialized before
  importing any other instrumented module.</p>
</blockquote>

<p>Basically, you cannot have <code>require(any instrumented lib)</code> (e.g. http, express, etc) before calling init() tracing function. </p>

<p><a href=""https://docs.datadoghq.com/tracing/setup/nodejs/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/setup/nodejs/</a></p>
"
Datadog,53585633,52176297,0,"2018/12/03, 01:28:11",False,"2018/12/03, 01:28:11",41,1970447,4,"<p>It's probably too late, but it may be useful to others.
You can set tags by using the environment variables of the application container (not the agent container) by using DD_TAGS.</p>
"
Datadog,51933718,51866333,0,"2018/08/20, 18:11:48",True,"2018/08/20, 18:11:48",3212,282172,2,"<p>Apparently there is a datadog/agent:latest-jmx that should be used that contains the java image... I just missed it in the docs.</p>
"
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46,9872725,3,"<p>I discussed this with Datadog support, and they confirmed that the <code>awslogs</code> logging driver prevents the Datadog agent container from accessing a container's logs. Since <code>awslogs</code> is currently the only logging driver available to tasks using the Fargate launch type, getting logs into Datadog will require another method.</p>

<p>Since the <code>awslogs</code> logging driver emits logs to CloudWatch, one method that I have used is to create a subscription to stream those log groups to Datadog's Lambda function as configured <a href=""https://docs.datadoghq.com/integrations/amazon_web_services/#log-collection"" rel=""nofollow noreferrer"">here</a>.  You can do that from the <a href=""https://docs.datadoghq.com/integrations/amazon_web_services/#manually-set-up-triggers"" rel=""nofollow noreferrer"">Lambda side</a> using CloudWatch logs as the trigger, or from the CloudWatch Logs side, by clicking <code>Actions</code>> <code>Stream to AWS Lambda</code>.</p>

<p>I chose the Lambda option because it was quick and easy and required no code changes to our applications (since we are still in the evaluation stage).  Datadog support advised me that it was necessary to modify the Lambda function in order to attribute logs to the corresponding service:</p>

<p>In <a href=""https://github.com/DataDog/dd-aws-lambda-functions/blob/master/Log/lambda_function.py#L168-L179"" rel=""nofollow noreferrer"">this block</a>, modify it to something like:</p>

<pre><code>structured_line = merge_dicts(log, {
    ""syslog.hostname"": logs[""logStream""],
    ""syslog.path"": logs[""logGroup""],
    ""syslog.appname"": logs[""logGroup""],
    ""aws"": {
        ""awslogs"": {
            ""logGroup"": logs[""logGroup""],
            ""logStream"": logs[""logStream""],
            ""owner"": logs[""owner""]
        }
    }
})
</code></pre>

<p>According to Datadog support:</p>

<ol>
<li><code>syslog.appname</code> needs to match an existing APM service in order to correlate logs to the service.</li>
<li>This solution is not fully supported at the moment and they are working on documenting this more thoroughly.</li>
</ol>

<p>I had to make further modifications to set the value of the <code>syslog.*</code> keys in a way that made sense for our applications, but it works great.</p>
"
Datadog,51324294,51323878,0,"2018/07/13, 14:39:40",False,"2018/07/13, 14:39:40",61,9763778,1,"<p>I found the solution: the user <code>datadog</code> didnt have permission to read connections that wasnt form him. So it was just getting a single row.</p>

<p>I gave permissions for that user to read <code>pg_stat_activity</code></p>
"
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647,1958151,2,"<p>It looks like you created <em>some</em> policy, but not the policy of required type. When you create the role for Datadog, you have to choose a very specific role type:</p>

<blockquote>
  <p>Select Another AWS account for the Role Type.</p>
</blockquote>

<p>and then create a policy for that role. Also, don't forget to</p>

<blockquote>
  <p>Check off Require external ID</p>
</blockquote>

<p>You shouldn't have any problems as long as you follow the guideline step by step: <a href=""https://docs.datadoghq.com/integrations/amazon_web_services/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/amazon_web_services/</a></p>
"
Datadog,61044468,50792558,1,"2020/04/05, 17:51:15",False,"2020/04/05, 17:51:15",1,12211192,0,"<p>I had this problem, when I tried to both use the role-assumption role as an assumption role on the <code>assume_role_policy</code>, as well as trying to attach it.</p>

<p>Once I got rid of the aws_iam_policy that I created with the role-assumption policy doc as well as the role-policy attachment, it worked.</p>

<p>Hope this helps.</p>
"
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141,8721010,1,"<p>One of Dockers main features is portability and it makes sense to bind datadog into that environment. That way they are packaged and deployed together and you don't have the overhead of installing datadog manually everywhere you choose to deploy.</p>

<p>What they are also implying is that you should use <strong><a href=""https://docs.docker.com/compose/overview/"" rel=""nofollow noreferrer"">docker-compose</a></strong> and turn your application / docker container into an multi-container Docker application, running your image(s) alongside the docker agent. Thus you will not need to write/build/run/manage a container via Dockerfile, but rather add the agent image to your <strong>docker-compose.yml</strong> along with its configuration. Starting your multi-container application will still be easy via:</p>

<pre><code>docker-compose up
</code></pre>

<p>Its really convenient and gives you additional features like their <a href=""https://docs.datadoghq.com/agent/autodiscovery/?tab=docker"" rel=""nofollow noreferrer"">autodiscovery</a> service.</p>
"
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158,7147666,2,"<p>Just use a find me Twimlet. Enter up to 10 numbers and a timeout between moving on to the next number. Twilio will do the rest.</p>

<p><a href=""https://www.twilio.com/labs/twimlets/findme"" rel=""nofollow noreferrer"">https://www.twilio.com/labs/twimlets/findme</a></p>
"
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150,1394755,1,"<p>If you are looking for a more full featured paid solution I'd recommend PagerDuty. DataDog has an integration for PagerDuty. Any monitor that gets triggered that mentions <code>@pagerduty-myteamname</code>(as example) in the monitor message will cause PagerDuty to page the on call person.  If that person does not acknowledge the page you can configure it to go through list of people to contact next until it is acknowledged by someone.</p>
"
Datadog,46202190,45695083,0,"2017/09/13, 19:05:55",True,"2017/09/13, 19:05:55",421,2016045,4,"<p>The role arn:aws:iam::xxxxxxxxxx:role/DatadogAWSIntegrationRole also has to have permission to assume the role on the other account.</p>

<p>You'll have to update the DatadogAWSIntegrationRole on the primary account to include:</p>

<pre><code>{
""Version"": ""2012-10-17"",
""Statement"": [
                ...
                {
                    ""Effect"": ""Allow"",
                    ""Action"": ""sts:AssumeRole"",
                    ""Resource"": ""arn:aws:iam::xxxxxxxxxxxx:role/AssumedRoleForDataDogInOtherAccount""
                }
            ]
}
</code></pre>
"
Datadog,45438610,45438114,9,"2017/08/01, 16:00:52",True,"2017/08/01, 16:00:52",36,3185251,2,"<p>I suspect you're probably looking to query the event stream which is where all alerts from monitors can be found.  The docs at <a href=""https://docs.datadoghq.com/api/#events-get-all"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/#events-get-all</a> are a pretty good starting place.  </p>

<p>You'll want to query this endpoint with the proper source and tags, but this should be a starting point.  If this doesn't quite work, I'd recommend looking at pulling the details from the monitor as shown here:  <a href=""https://docs.datadoghq.com/api/#monitor-get-details"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/#monitor-get-details</a>. This may be a second option if you're unable to get the information you're looking for from the event stream.</p>
"
Datadog,45124573,45104434,1,"2017/07/16, 06:03:15",True,"2017/07/16, 06:03:15",1171,6826691,2,"<p>The ""ReadTimeout: HTTPConnectionPool"" error can be corrected by adding a timeout parameter under instances in the elasticsearch.yaml</p>

<pre><code> timeout: 8
</code></pre>
"
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261,4172512,1,"<p>Frank,</p>

<p>Your use case follows the standard ""custom metric"" submission that is common within Datadog.  Using one of the supported libraries:</p>

<p><a href=""http://docs.datadoghq.com/libraries/#java"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/libraries/#java</a></p>

<p>You can leverage the statsD port of an Agent running on your host to submit these custom metrics:</p>

<p><a href=""http://docs.datadoghq.com/guides/dogstatsd/"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/guides/dogstatsd/</a></p>

<p>You will want to install the Agent on either the host running this function or point your statsD connection towards an accepting host:</p>

<p><a href=""http://docs.datadoghq.com/guides/basic_agent_usage/"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/guides/basic_agent_usage/</a></p>

<p>There are additional docs found here that should help you understand how custom metrics work in Datadog:</p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-</a></p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/203765485-How-do-I-submit-custom-metrics-What-s-their-overhead-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/203765485-How-do-I-submit-custom-metrics-What-s-their-overhead-</a></p>

<p>Usually when troubleshooting custom metric submissions, we try to implement some form of local printing/logging to ensure the statsD connection is being made and that the custom function is being called and submitted.  Once you can confirm the metric is being sent to the Agent, use the Metric Summary page to check for the custom metric:</p>

<p><a href=""https://app.datadoghq.com/metric/summary"" rel=""nofollow noreferrer"">https://app.datadoghq.com/metric/summary</a></p>

<p>If all else fails, reach out to Datadog at support@datadoghq.com</p>
"
Datadog,41220782,40933155,0,"2016/12/19, 12:42:07",True,"2016/12/19, 12:42:07",1981,2473382,0,"<p>It is very much possible, just use the alias property of the <a href=""http://docs.datadoghq.com/integrations/java/#the-attribute-filter"" rel=""nofollow noreferrer"">attribute</a> filter:</p>

<pre><code>- include:
    domain: data
    attribute:
      success:
        alias: jmx.com.abc.reporting.successCount
      error:
        alias: jmx.com.abc.reporting.errorCount
</code></pre>
"
Datadog,39235805,39235163,0,"2016/08/30, 22:38:33",True,"2016/08/30, 22:40:17",496,2050873,6,"<p>Verify if the datadog package is installed in your environment.</p>

<p>You can do this with this command: </p>

<pre><code>$ pip freeze | grep datadog
</code></pre>

<p>If it's not installed, you can install it with this command:</p>

<pre><code>$ pip install datadog
</code></pre>
"
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156,2676108,1,"<p>You could tag your metrics with the name or ID of the agent it is collecting metrics from (if you aren't already). Then in Datadog you could write a query that groups by the agent ID and applies a count_not_null function: <a href=""https://docs.datadoghq.com/dashboards/functions/count/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/dashboards/functions/count/</a></p>
<p>This basically hijacks a random metric to extract the unique count of agents reporting that metric to assume the total count of agents. You wouldn't be able to easily group by queue though, so idk if it would be a good solution to your use case.</p>
<hr />
<p>Your idea around using gauges sounds good to me. You can send a new metric called something like <code>myagent.running</code> which sends a value of 1 for each of your agents and does a sum of all gauges in order to get a count. That is actually how the metric <code>datadog.agent.running</code> is implemented: <a href=""https://docs.datadoghq.com/integrations/agent_metrics/#metrics"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/agent_metrics/#metrics</a></p>
"
Datadog,66632382,66606637,0,"2021/03/15, 06:26:59",True,"2021/03/15, 06:26:59",902,10709519,0,"<p>After testing different queries I found that running this query groups the results by query statements and will return the count of each.</p>
<pre><code>SELECT COUNT(*) as Query_Count, DB, INFO as Query FROM 
INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query' AND TIME &gt; (5 * 60 * 1000) 
GROUP BY QUERY;
</code></pre>
<p>In Datadogs mysql configuration I added tags for the query statement and database name. Now since they are grouped, I can see information per different statement in datadog.</p>
<pre><code>    custom_queries:
      - query: SELECT COUNT(*) as Query_Count, DB, INFO as Query FROM 
        INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query' GROUP BY QUERY;
        columns:
         - name: mysql.processlist.Slow_Query.Query_Count
           type: count
         - name: mysql.processlist.Slow_Query.DB
           type: tag
         - name: mysql.processlist.Slow_Query.Query
           type: tag
</code></pre>
"
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249,13336642,0,"<p>Datadog’s IIS integration queries the Web Service performance counters automatically and sends the results to Datadog. The Web Service performance counter class collects information from the World Wide Web Publishing Service.</p>
<p>You can enable the IIS integration by creating a configuration file either manually or through the Datadog Agent GUI. To create a configuration file through the GUI, navigate to the “Checks” tab, choose “Manage Checks,” and select the iis check from the “Add a Check” menu. You can also manually create a conf.yaml file in C:\ProgramData\Datadog\conf.d\iis.d.</p>
<p>There is a sites attribute in the conf.yaml file. This attribute represents the IIS site you want to monitor. You only need to delete the sites you want to exclude.</p>
<p>More information you can refer to this link: <a href=""https://www.datadoghq.com/blog/iis-monitoring-datadog/"" rel=""nofollow noreferrer"">IIS monitoring with Datadog</a>.</p>
"
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566,24231,0,"<p>If you are using Datadog's .NET Tracer, you can set <code>DD_TRACE_ENABLED=false</code> in the <code>appSettings</code> section of the <code>web.config</code> file (<a href=""https://docs.datadoghq.com/tracing/setup_overview/setup/dotnet-framework/?tab=webconfig#configuration"" rel=""nofollow noreferrer"">docs</a>). For example:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;configuration&gt;
  &lt;appSettings&gt;
    &lt;add key=&quot;DD_TRACE_ENABLED&quot; value=&quot;false&quot;/&gt;
  &lt;/appSettings&gt;
&lt;/configuration&gt;
</code></pre>
<p>Another option is to deploy a <code>datadog.json</code> file (<a href=""https://docs.datadoghq.com/tracing/setup_overview/setup/dotnet-framework/?tab=jsonfile#configuration"" rel=""nofollow noreferrer"">docs</a>) in the root of your app that contains:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;DD_TRACE_ENABLED&quot;: &quot;false&quot;
}
</code></pre>
<p>(Disclaimer: I work at Datadog)</p>
"
Datadog,66357392,66316616,2,"2021/02/24, 21:12:06",False,"2021/02/25, 00:49:28",156,2676108,0,"<p>Update: I found this bit of code in the tracing client repo:</p>
<pre><code>tracer.use('graphql', {
  hooks: {
    execute: (span, args, res) =&gt; {
        if (res?.errors?.[0]?.status === 403) { // assuming &quot;status&quot; is a number
            span?.setTag('error', null) // remove any error set by the tracer
        }
    }
  }
})
</code></pre>
<p><a href=""https://github.com/DataDog/dd-trace-js/issues/1249"" rel=""nofollow noreferrer"">https://github.com/DataDog/dd-trace-js/issues/1249</a></p>
<p>maybe it would help</p>
<hr />
<p>Old message:</p>
<p>Never mind. seems like my solution is only for express, graphql doesn't support that property</p>
<p><s>You probably want to just modify the validateStatus property in the http module:</p>
<blockquote>
<p>Callback function to determine if there was an error. It should take a status code as its only parameter and return true for success or false for errors</p>
</blockquote>
<p><a href=""https://datadoghq.dev/dd-trace-js/interfaces/plugins.http.html#validatestatus"" rel=""nofollow noreferrer"">https://datadoghq.dev/dd-trace-js/interfaces/plugins.http.html#validatestatus</a></p>
<p>As an example you should be able to mark 403s as not be errors with something like this:</p>
<pre><code>const tracer = require('dd-trace').init();
tracer.use('express', {
  validateStatus: code =&gt; code &lt; 400 &amp;&amp; code != 403
})
</code></pre>
</s>
"
Datadog,66299116,66282520,0,"2021/02/21, 06:18:23",False,"2021/02/21, 06:18:23",493,244037,0,"<p>Unfortunately, that doesn't seem to be a setting you can directly control at this time.
The reasoning is that a given timeseries could be split by tag, so setting a single color for a timeseries split by tag would amount to multiple entires of the same color, and that wouldn't make sense.</p>
<p>To support the semantic meaning, I've often used the following settings:</p>
<ul>
<li>Error: Warm (first timeseries is red)</li>
<li>Warn: Orange (it's orange, and nothing rhymes with orange)</li>
<li>Normal: Cool</li>
</ul>
<p>I'm not sure I know what distinction there is of Error vs Critical in your definition, but using these palettes has proven useful for my team.</p>
<p>If you're looking for a specific widget to change color based on value - so if the number exceeds a threshold - take a look at the <a href=""https://docs.datadoghq.com/dashboards/widgets/query_value/"" rel=""nofollow noreferrer"">Query Value Widget</a>, as that can be customized to change color based on the current value.</p>
<p>Alternately, if you have a Monitor already set for the timeseries, use the <a href=""https://docs.datadoghq.com/dashboards/widgets/alert_value/"" rel=""nofollow noreferrer"">Alert Value Widget</a> to show the current status, with less configuration, since the thresholds are managed in the Monitor's definition.</p>
"
Datadog,66356118,66282520,0,"2021/02/24, 19:43:52",False,"2021/02/24, 19:43:52",156,2676108,0,"<p>You can't set a color per line, but you can set the color per query.</p>
<p>If you edit the graph using json, that field <code>requests.style.palette</code> is exposed and you can just try typing in whatever color you want there.</p>
<p><a href=""https://imgur.com/VrZZl72"" rel=""nofollow noreferrer"">https://imgur.com/VrZZl72</a></p>
<p>If you want to have one time series that is green for hits, and one that is red for errors, you just make two metric queries, and then color one green and one red.</p>
<p><a href=""https://imgur.com/AHGi1Hk"" rel=""nofollow noreferrer"">https://imgur.com/AHGi1Hk</a></p>
"
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156,2676108,0,"<p>That endpoint is used to send events to Datadog, if your one account is sending them to the logs product, it is most likely because that account has a special flag that converts all your incoming events into logs. This is common for users that use the Security Monitoring product.</p>
<p>I would recommend reaching out to support@datadoghq.com to see if this flag is enabled in that one account. And if you want to replicate that behavior in another account, you can ask them to enable that feature.</p>
"
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072,252340,2,"<p>In Datadog, create an API key in Integrations, APIs. Give the API key a name.</p>
<p>In NLog.config create a target. The url is either datadoghq.com or datadoghq.eu (for europe).</p>
<pre><code> &lt;target xsi:type=&quot;WebService&quot;
        name=&quot;datadog&quot;
        url=&quot;https://http-intake.logs.datadoghq.com/v1/input&quot;
        encoding=&quot;utf-8&quot;
        protocol=&quot;JsonPost&quot;
        preAuthenticate=&quot;false&quot; &gt;
        &lt;parameter name='date' type='System.String' layout='${longdate}'/&gt; 
        &lt;parameter name='ipaddress' type='System.String' layout='${aspnet-request-ip}'/&gt;
        &lt;parameter name=&quot;userid&quot; type='System.String' layout=&quot;${aspnet-User-Identity}&quot; /&gt;
        &lt;parameter name=&quot;level&quot;  type='System.String'  layout=&quot;${level:upperCase=true}&quot;/&gt;
        &lt;parameter name=&quot;version&quot; type='System.String'  layout=&quot;${configsetting:name=VersionSettings.Version:default=?}&quot; /&gt;
        &lt;parameter name=&quot;threadid&quot;  type='System.String' layout=&quot;${threadid}&quot; /&gt;
        &lt;parameter name=&quot;controller&quot;  type='System.String' layout=&quot;${aspnet-mvc-controller}&quot; /&gt;
        &lt;parameter name=&quot;class&quot; type='System.String'  layout=&quot;${callsite:className=True:includeNamespace=False:fileName=False:includeSourcePath=False:methodName=True:cleanNamesOfAnonymousDelegates=True:cleanNamesOfAsyncContinuations=True}&quot; /&gt;
        &lt;parameter name=&quot;message&quot; type='System.String'  layout=&quot;${message}&quot; /&gt;
        &lt;parameter name=&quot;elapsed&quot;  type='System.String' layout =&quot;${event-properties:item=elapsed}&quot; /&gt;
        &lt;parameter name=&quot;service&quot;  type='System.String' layout=&quot;${configsetting:name=Nlog.Component}&quot; /&gt;
        &lt;parameter name=&quot;hostname&quot;  type='System.String' layout=&quot;${configsetting:name=Nlog.HostName}&quot; /&gt;
        &lt;parameter name=&quot;exception&quot;  type='System.String' layout=&quot;${exception:format=ToString}&quot; /&gt;
        &lt;parameter name=&quot;ddsource&quot;  type='System.String' layout=&quot;csharp&quot; /&gt;
        &lt;header name=&quot;DD-API-KEY&quot; layout=&quot;${configsetting:name=Nlog.datadog}&quot;/&gt;
        &lt;header name=&quot;Content-Type&quot; layout=&quot;application/json&quot;/&gt;
&lt;/target&gt;       
</code></pre>
<p>Now create a rule to write to the target and you are done!</p>
<p>All of the parameters can be configured to become columns in Datadog, and/or facets to select on. I am using a date parameter so that the date matches other logs, rather than displaying the built in date.</p>
"
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156,2676108,0,"<p>The datadog agent you deployed has no power to run scripts or take action. It is purely a monitoring/data collection tool.</p>
<hr />
<p>However one of the things your monitors in the Datadog application can do is trigger events when they go into an alert state. There are <a href=""https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#integrations"" rel=""nofollow noreferrer"">lots of integrations</a>: creating a ticket in Jira, posting a message to Slack, triggering an SNS topic.</p>
<p>What I recommend you try to do, is to create some kind of job or script that can be triggered externally, like a lambda function, or a jenkins job, or anything really. When the monitor goes off you can use a <a href=""https://docs.datadoghq.com/integrations/webhooks/"" rel=""nofollow noreferrer"">webhook</a> to trigger that script to do whatever you define. Here is a blog post showing <a href=""https://www.datadoghq.com/blog/send-alerts-sms-customizable-webhooks-twilio/"" rel=""nofollow noreferrer"">how twilio sent out a text message by connecting their api to a webhook</a>.</p>
"
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493,244037,0,"<p>In these kinds of checks, the response order matters, since the columns returned from the DB are going to be mapped back to the names specified in the YAML.</p>
<p>Reading the error message:</p>
<blockquote>
<p>Error: postgres:953578488181a512 | (postgres.py:398) | non-numeric value <code>cldtx</code> for metric column <code>active_connections</code> of metric_prefix <code>postgresql</code></p>
</blockquote>
<p>We can see that the value of <code>cldtx</code> is being returned for the <code>active_connections</code> column, which in the YAML is declared as a gauge, and this is a string.</p>
<p>The fix should be straightforward, by reordering the YAML, like so:</p>
<pre class=""lang-yaml prettyprint-override""><code>...
     columns:
       - name: db_name
         type: tag
       - name: active_connections
         type: gauge
</code></pre>
<p>Alternately, if you want to keep the YAML ordered, change the query to:</p>
<pre class=""lang-yaml prettyprint-override""><code>...
     query: SELECT count(pid) as active_connections, datname as db_name FROM pg_stat_activity where state = 'active' group by db_name;
...
</code></pre>
"
Datadog,63912117,63888511,0,"2020/09/16, 05:05:14",False,"2020/09/16, 05:05:14",2509,1190203,1,"<p>So I ended up just looking at the tests in the datadog terraform provider and noticing the query format they are testing.</p>
<pre><code>  query = &quot;avg(last_30m):avg:gcp.pubsub.subscription.num_undelivered_messages{project_id:${var.project_name},subscription_id:{project_id:terraform_gcp_test} &gt; 2&quot;
</code></pre>
<p>It seems you need to specify a time range and also add in a comparison threshold that matches your critical alert threshold. That was what was missing.</p>
"
Datadog,63779195,63779194,0,"2020/09/07, 17:06:52",True,"2020/09/07, 17:06:52",95,285601,0,"<p>It is not possible no. Confirmed with DD support.</p>
"
Datadog,64995876,63779194,0,"2020/11/25, 00:36:55",False,"2020/12/02, 18:50:18",906,3758005,1,"<p>Actually it is possible, but you need to put every json log into quotes (some prefix before each log will also work), so that Datadog agent will consider this as a 'text'. I.e. <code>log.json</code> file should contain quoted logs:</p>
<pre><code>'{&quot;key1&quot;: &quot;value1&quot;}'
'{&quot;key1&quot;: &quot;value2&quot;}'
'{&quot;key1&quot;: &quot;value3&quot;}'
</code></pre>
<p>After that, in Datadog Logs Configuration you need to add a pipeline with Grok parser filter <a href=""https://docs.datadoghq.com/logs/processing/parsing/?tab=filter#matcher-and-filter"" rel=""nofollow noreferrer"">json</a> (see filter tab in <strong>Matcher and Filter</strong>):</p>
<p><a href=""https://i.stack.imgur.com/gRQs4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gRQs4.png"" alt=""Grok parser"" /></a></p>
<p>This allowed me to perform full text search thru all fields in my json logs and automatically parse all json fields as attributes.</p>
<p>P.S. This solution was provided by Datadog support 2 years ago. And seems they are working on solution to allow full text search for JSON logs.</p>
"
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251,5477963,1,"<p>It looks from the above snippets that the <code>combined</code> Morgan format is sent directly sent to Winston, and then parsed within a log pipeline in Datadog. Since the <code>combined</code> format doesn't include the body and there is no built-in token for it, you would have to use a custom format with your own tokens and then update your pipeline accordingly.</p>
<p>For example, to create a custom format in Morgan that includes the status code and the body:</p>
<pre class=""lang-js prettyprint-override""><code>morgan((tokens, req, res) =&gt; [
  tokens.status(req, res),
  req.body // assuming body-parser middleware is used
].join(' '))
</code></pre>
<p>You can also create a token to achieve the same result with a simpler format definition:</p>
<pre class=""lang-js prettyprint-override""><code>morgan.token('body', (req, res) =&gt; req.body
morgan(':status :body')
</code></pre>
<p>You can find the documentation for custom Morgan formats <a href=""http://expressjs.com/en/resources/middleware/morgan.html#using-format-string-of-predefined-tokens"" rel=""nofollow noreferrer"">here</a>, creating tokens <a href=""http://expressjs.com/en/resources/middleware/morgan.html#creating-new-tokens"" rel=""nofollow noreferrer"">here</a>, and Datadog log pipeline parsing <a href=""https://docs.datadoghq.com/logs/processing/parsing/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Hope this helps!</p>
"
Datadog,62228894,62094698,0,"2020/06/06, 10:54:35",False,"2020/06/06, 10:54:35",21,7314273,0,"<p>Maybe you can ask them to add it by opening a feature request:
<a href=""https://github.com/DataDog/documentation/issues/new/choose"" rel=""nofollow noreferrer"">https://github.com/DataDog/documentation/issues/new/choose</a></p>
"
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771,842302,0,"<blockquote>
  <p>but it looks like I'm missing an important part here.</p>
</blockquote>

<p>That was the point. The lambda itself has not much todo with particular <code>statusCodes</code>. So I either may log each status code and let datadog parse it accordingly. </p>

<p>Or, that's the solution I went for, I can leverage API-Gateway for monitoring status codes per lambda.</p>
"
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371,5540166,2,"<p>You can use most any of the common open source log shippers to send server logs to Datadog without using the Datadog agent, for example <a href=""https://docs.datadoghq.com/integrations/fluentd/"" rel=""nofollow noreferrer"">fluentd</a>. But there can be several benefits to using the Datadog agent to collect server logs, such as:</p>

<ul>
<li>If you are using the Datadog agent for other monitoring data already, it saves you having to run/manage more software for log collection</li>
<li>It's the safest way to make sure you get all the right tags applied to both your logs and other monitoring data (like metrics, traces, etc.) for better correlation of data when you're investigating stuff. </li>
</ul>

<p>There are other ways to collect logs in Datadog, among those is the <a href=""https://docs.datadoghq.com/api/v1/logs/#get-a-list-of-logs"" rel=""nofollow noreferrer"">HTTP API</a>. Since this API uses a POST method, I bet you could configure Datadog's <a href=""https://docs.datadoghq.com/integrations/webhooks/"" rel=""nofollow noreferrer"">webhook integration</a> to generate log events from Datadog events and alerts. That said, before you go through the trouble of doing this, if you have a use-case or reason you're interested in doing this, you may want to reach out to Datadog support to see if they have some features coming / in beta that would get you what you want without the extra work on your end. (What <em>is</em> your use-case? I'm curious)</p>
"
Datadog,61513379,61511271,1,"2020/04/30, 02:25:23",True,"2020/04/30, 02:25:23",1371,5540166,1,"<p>Sounds like some of your <code>http.server.requests.count</code> metric values do not have any <code>status</code> tag, so when you group by the <code>status</code> tag, those are being aggregaed with a value of <code>n/a</code>. </p>

<p>If it is intentional/expected that this metric would have values without the <code>status</code> tag and you just want to ignored those metric values, then you can use the <code>exclude_null()</code> function to remove that tag grouping from your graph (docs <a href=""https://docs.datadoghq.com/dashboards/functions/beta/#exclude-null"" rel=""nofollow noreferrer"">here</a>). </p>

<p>If it is not intentional/expected that this metric would have values without the <code>status</code> tag, then you probably want to reach out to support@datadoghq.com to get that looked into. </p>
"
Datadog,62812732,61405563,0,"2020/07/09, 13:23:09",False,"2020/07/09, 13:23:09",877,1570636,0,"<p>Just add this in the startup.cs.</p>
<pre><code>var config = new DatadogConfiguration(&quot;https://http-intake.logs.datadoghq.eu&quot;);
        Serilog.Log.Logger = new LoggerConfiguration()
       .WriteTo.DatadogLogs(&quot;&lt;Datadog_API_KEY&gt;&quot;, configuration: config)
       .CreateLogger();
// Call this from anywhere in the app
Serilog.Log.Logger.Information(&quot;This is a test from session-start&quot;);
Serilog.Log.Logger.Error(&quot;This is a test from session-start&quot;);
</code></pre>
"
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365,5666309,0,"<p>I'm very sorry for this late answer.</p>
<p>After exchanging a bunch of emails with the Datadog support guys, it turned out that the solution is straightforward:</p>
<pre><code>[OneTimeTearDown]
public virtual void Cleanup()
{
    logger.Dispose();
}
</code></pre>
<p>The <code>Dispose()</code> method force the sinks to gracefully close up and send the logs stored in cache.</p>
<p>Please note that the logs do not appear instantly in the Datadog console: allow a few seconds (to some minutes) for their systems to process the logs you send.</p>
"
Datadog,61225485,61191225,0,"2020/04/15, 12:32:12",False,"2020/04/15, 12:32:12",21,7314273,1,"<p>You might want to use datadog service-check which is also can be sent by the StatsDClient, and then add it to your monitor/dashboard page.
<a href=""https://docs.datadoghq.com/developers/service_checks/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/developers/service_checks/</a></p>
"
Datadog,61232317,61191225,0,"2020/04/15, 18:18:06",False,"2020/04/15, 18:18:06",1371,5540166,0,"<p>Another option is to use <a href=""https://docs.datadoghq.com/integrations/java/?tab=host"" rel=""nofollow noreferrer"">Datadog's Java / JMX integration</a> to capture the health data that's exposed over JMX -- this can probably give you up/down health data, and can certainly give you a lot more granular health metrics too. </p>
"
Datadog,60829558,60828990,1,"2020/03/24, 12:51:21",True,"2020/03/24, 18:07:20",3151,4409319,3,"<p>Well, apparently you can <code>-@facet:*</code></p>

<p>Didn't specify it in my question because it was not important, but what I really needed was a way to either <strong>filter by a specific facet value, or get logs without said facet</strong></p>

<p>The following works for me:</p>

<pre><code>-@facet:* OR @facet:specificvalue
</code></pre>
"
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971,1237575,0,"<p>Spring is scanning your classpath that seems incomplete. Maybe it is related to Spring's class loading mechanisms. The <a href=""https://github.com/DataDog/dd-trace-java/blob/7df5cfe3e3f1710d8840522232f18496dc6be530/dd-java-agent/instrumentation/spring-scheduling-3.1/src/main/java/datadog/trace/instrumentation/springscheduling/SpringSchedulingInstrumentation.java"" rel=""nofollow noreferrer"">class in question</a> exists and seems to be part of the agent.</p>

<p>Possibly, you are using an outdated version of the agent.</p>
"
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832,281848,1,"<p>I'm not particularly familiar with this Datadog JSON format but the general pattern I would propose here has multiple steps:</p>

<ul>
<li>Decode the serialized data into a normal Terraform value. In this case that would be using <a href=""https://www.terraform.io/docs/configuration/functions/jsondecode.html"" rel=""nofollow noreferrer""><code>jsondecode</code></a>, because the data is JSON-serialized.</li>
<li>Transform and normalize that raw data into a consistent shape that is more convenient to use in a declarative Terraform configuration. This will usually involve at least one <a href=""https://www.terraform.io/docs/configuration/locals.html"" rel=""nofollow noreferrer"">named local value</a> containing an expression that uses <a href=""https://www.terraform.io/docs/configuration/expressions.html#for-expressions"" rel=""nofollow noreferrer""><code>for</code> expressions</a> and <a href=""https://www.terraform.io/docs/configuration/functions/try.html"" rel=""nofollow noreferrer"">the <code>try</code> function</a>, along with the type conversion functions, to try to force the raw data into a more consistent shape.</li>
<li>Use the transformed/normalized result with Terraform's resource and block repetition constructs (resource <code>for_each</code> and <code>dynamic</code> blocks) to describe how the data maps onto physical resource types.</li>
</ul>

<p>Here's a basic example of that to show the general principle. It will need more work to capture all of the details you included in your initial example.</p>

<pre><code>variable ""datadog_json"" {
  type = string
}

locals {
  raw = jsondecode(var.datadog_json)
  screenboard = {
    title       = local.raw.title
    description = try(local.raw.description, tostring(null))
    widgets = [
      for w in local.raw.widgets : {
        type   = w.definition.type

        title       = w.definition.title
        title_size  = try(w.definition.title_size, 16)
        title_align = try(w.definition.title_align, ""center"")

        x      = try(w.definition.x, tonumber(null))
        y      = try(w.definition.y, tonumber(null))
        width  = try(w.definition.x, tonumber(null))
        height = try(w.definition.y, tonumber(null))

        requests = [
          for r in w.definition.requests : {
            q            = r.q
            display_type = r.display_type
            style        = tomap(try(r.style, {}))
          }
        ]
      }
    ]
  }
}

resource ""datadog_screenboard"" ""acceptance_test"" {
  title       = local.screenboard.title
  description = local.screenboard.description
  read_only   = true

  dynamic ""widget"" {
    for_each = local.screenboard.widgets
    content {
      type = widget.value.type

      title       = widget.value.title
      title_size  = widget.value.title_size
      title_align = widget.value.title_align

      x      = widget.value.x
      y      = widget.value.y
      width  = widget.value.width
      height = widget.value.height

      tile_def {
        viz = widget.value.type

        dynamic ""request"" {
          for_each = widget.value.requests
          content {
            q            = request.value.q
            display_type = request.value.display_type
            style        = request.value.style
          }
        }
      }
    }
  }
}
</code></pre>

<p>The separate normalization step to build <code>local.screenboard</code> here isn't strictly necessary: you could instead put the same sort of normalization expressions (using <code>try</code> to set defaults for things that aren't set) directly inside the <code>resource ""datadog_screenboard""</code> block arguments if you wanted. I prefer to treat normalization as a separate step because then this leaves a clear definition in the configuration for what we're expecting to find in the JSON and what default values we'll use for optional items, separate from defining how that result is then mapped onto the physical <code>datadog_screenboard</code> resource.</p>

<p>I wasn't able to test the example above because I don't have a Datadog account. I'm sorry if there are minor typos/mistakes in it that lead to errors. My hope was to show the general principle of mapping from a serialized data file to a resource rather than to give a ready-to-use solution, so I hope the above includes enough examples of different situations that you can see how to extend it for the remaining Datadog JSON features you want to support in this module.</p>

<hr>

<p>If this JSON format is a interchange format formally documented by Datadog, it could make sense for Terraform's Datadog provider to have the option of accepting a single JSON string in this format as configuration, for easier exporting. That may require changes to the Datadog provider itself, which is beyond what I can answer here but might be worth raising in the GitHub issues for that provider to streamline this use-case.</p>
"
Datadog,64437131,60326963,0,"2020/10/20, 04:12:43",False,"2020/10/20, 04:12:43",11,7000092,0,"<p>I solved this by adding 'squashfs' to the list of filesystem types to be ignored by the datadog agent.</p>
<p>Create a file <code>/etc/datadog-agent/conf.d/disk.d/conf.yaml</code>:</p>
<pre><code>init_config:
    file_system_global_blacklist:
      - iso9660$
      - squashfs

instances:
  - use_mount: false

</code></pre>
<p>Restart datadog agent (<code>systemctl restart datadog-agent</code>).</p>
"
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336,7243426,1,"<p>If you have an ever increasing counter, you can use the a function called <a href=""https://docs.datadoghq.com/dashboards/functions/rate/"" rel=""nofollow noreferrer"">rate</a>. You'll be able to select it with the <code>+</code> on the query line. With that you'll be able to have a rate of increase per seconds, minutes or hours.</p>

<p>If you are looking to get a difference between the same metric but at another point in the past, you have a function called <a href=""https://docs.datadoghq.com/dashboards/functions/timeshift/"" rel=""nofollow noreferrer"">timeshift</a> that could also help. This is also accessible with the small <code>+</code> on the right of the query line.</p>

<p>Finally, if you are looking at comparing two different metrics, you  have a button called <a href=""https://docs.datadoghq.com/dashboards/querying/#arithmetic-between-two-metrics"" rel=""nofollow noreferrer"">Advanced</a> that will enable you to write more complex queries such as a difference between two metrics.</p>
"
Datadog,59177979,59177043,0,"2019/12/04, 16:10:33",False,"2019/12/04, 16:10:33",807,1506396,2,"<p>I believe you are looking for <code>clusterName</code>:
<a href=""https://github.com/helm/charts/blob/master/stable/datadog/values.yaml#L75"" rel=""nofollow noreferrer"">https://github.com/helm/charts/blob/master/stable/datadog/values.yaml#L75</a></p>

<p>You can add it in your <code>values.yaml</code> under the <code>datadog</code> section like this:</p>

<pre><code>datadog:
  clusterName: myexamplename
</code></pre>
"
Datadog,59178230,59177043,0,"2019/12/04, 16:24:18",False,"2019/12/04, 16:24:18",9281,3270785,2,"<p>refer below command</p>

<pre><code>helm install --name datadog-monitoring \
    --set datadog.apiKey=&lt;DATADOG_API_KEY&gt; \
    --set datadog.appKey=&lt;DATADOG_APP_KEY \
    --set clusterAgent.enabled=true \
    --set clusterAgent.metricsProvider.enabled=true \
    --set datadog.clusterName=&lt;CLUSTER_NAME&gt; \
    stable/datadog
</code></pre>
"
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111,5054074,0,"<p>I am not a fan of helm, but you can accomplish this in 2 ways:</p>

<ul>
<li><p>via env vars: make use of <code>DD_AC_EXCLUDE</code> variable to exclude the Redis containers: eg <code>DD_AC_EXCLUDE=name:prefix-redis</code></p></li>
<li><p>via a config map: mount an empty config map in <code>/etc/datadog-agent/conf.d/redisdb.d/</code>, below is an example where I renamed the <code>auto_conf.yaml</code> to <code>auto_conf.yaml.example</code>.</p></li>
</ul>

<pre><code>apiVersion: v1
data:
  auto_conf.yaml.example: |
    ad_identifiers:
      - redis    init_config:    instances:
        ## @param host - string - required
        ## Enter the host to connect to.
        #
      - host: ""%%host%%""        ## @param port - integer - required
        ## Enter the port of the host to connect to.
        #
        port: ""6379""
  conf.yaml.example: |
    init_config:    instances:        ## @param host - string - required
        ## Enter the host to connect to.
        # [removed content]
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: redisdb-d
</code></pre>

<p>alter the daemonset/deployment object: </p>

<pre><code>[....]
        volumeMounts:
        - name: redisdb-d
          mountPath: /etc/datadog-agent/conf.d/redisdb.d
[...]
      volumes:
      - name: redisdb-d
        configMap:
          name: redisdb-d

[...]
</code></pre>
"
Datadog,58695216,58694140,1,"2019/11/04, 16:13:25",False,"2019/11/04, 16:13:25",336,7243426,1,"<p>On the bottom of the infrastructure list, you should see a link called ""JSON API permalink"". If you <a href=""https://app.datadoghq.com/reports/v2/overview"" rel=""nofollow noreferrer"">query it</a>, this should give you a JSON of all your hosts with their agent version. You can then query it with a quick Python script.</p>
"
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987,90580,0,"<p>Your metrics should have a common prefix like <code>myapp.metric1</code>, <code>myapp.metric2</code>, etc. 
Then you can disable all metrics and enable explicitly all <code>myapp.*</code> metrics like so:</p>

<p>application.properties:</p>

<pre><code>management.metrics.enable.all=false
management.metrics.enable.myapp=true
</code></pre>

<p>the <code>management.metrics.enable.&lt;your_custom_prefix&gt;</code> will enable all <code>&lt;your_custome_prefix&gt;.*</code> metrics. </p>

<p>If you want to enable some of the built-in core metrics again, for example reenabling <code>jvm.*</code>, you can do: </p>

<pre><code>management.metrics.enable.all=false
management.metrics.enable.myapp=true
management.metrics.enable.jvm=true
</code></pre>

<p>I've created a sample project <a href=""https://github.com/ecerulm/spring-boot-app-with-metrics"" rel=""nofollow noreferrer"">in github</a> that disables core metrics, enables custom metrics, and <code>jvm.*</code> metrics and sends to Datadog. </p>
"
Datadog,58689932,58607248,0,"2019/11/04, 10:54:36",False,"2019/11/04, 10:54:36",336,7243426,0,"<p>Not sure I fully grasp the issue. Here are some steps to collect your traces:</p>

<ol>
<li>Enable trace collection on Kubernetes and open relevant port (8126) <a href=""https://docs.datadoghq.com/agent/kubernetes/daemonset_setup/?tab=k8sfile#apm-and-distributed-tracing"" rel=""nofollow noreferrer"">doc</a></li>
<li>Configure your app to send traces to the right container. Here is an example to adapt based on your situation. <a href=""https://docs.datadoghq.com/tracing/setup/java/"" rel=""nofollow noreferrer"">doc on java
instrumentation</a></li>
</ol>

<pre><code>        env:
          - name: DD_AGENT_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
</code></pre>

<p>Just in case, more info on Open Tracing <a href=""https://docs.datadoghq.com/tracing/advanced/opentracing/?tab=java"" rel=""nofollow noreferrer"">here</a></p>
"
Datadog,58708338,58579323,0,"2019/11/05, 11:33:57",False,"2021/02/05, 00:01:35",4059,8763847,1,"<p>Seems like there is a typo in your command. <code>DD_DOGSTATD_NON_LOCAL_TRAFFIC </code> is used instead of <code>DD_DOGSTATSD_NON_LOCAL_TRAFFIC</code></p>
<p>I usually used the below command for testing with Datadog:</p>
<pre><code>DOCKER_CONTENT_TRUST=1 docker run -d \
    --name dd-agent \
    -v /var/run/docker.sock:/var/run/docker.sock:ro \
    -v /proc/:/host/proc/:ro \
    -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \
    -e DD_API_KEY=&lt;api-key&gt; \
    -e DD_DOGSTATSD_NON_LOCAL_TRAFFIC=&quot;true&quot; \
    -p 8125:8125/udp \
    -p 8126:8126/tcp \
    datadog/agent:latest
</code></pre>
"
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336,7243426,1,"<p>Have you tried <a href=""https://docs.datadoghq.com/monitors/monitor_types/composite/#pagetitle"" rel=""nofollow noreferrer"">composite monitors</a>? You should be able to combine your low CPU Credit monitor with another monitor that looks at events from RDS.</p>

<p>Two monitors such as:</p>

<ul>
<li>A: CPU Credit &lt; 10</li>
<li>B: Number of event received about RDS creation > 1 in past hour</li>
</ul>

<p>A composite monitor: A &amp;&amp; !B</p>

<p>(I hope my example makes sense)</p>
"
Datadog,53935650,53222715,0,"2018/12/26, 20:21:57",True,"2018/12/26, 20:21:57",347,2650254,0,"<p>so, I had to install a Datadog agent on ec2 instance and configure it to be able to access all mysql dbs and collect the mysql performance schema metrics. And surely with correct security groups for the ec2.
I followed this <a href=""https://docs.datadoghq.com/integrations/amazon_rds/#native-database-integration-1"" rel=""nofollow noreferrer"">docs</a> and <a href=""https://www.datadoghq.com/blog/monitoring-rds-mysql-performance-metrics/"" rel=""nofollow noreferrer"">this</a> and contacted the support.</p>
"
Datadog,51871692,51707255,1,"2018/08/16, 10:24:11",True,"2018/08/16, 10:24:11",76,4481158,0,"<p>If the Windows OS is D drive, the setting is installed in <code>D:\ProgramData\Datadog</code>.
Copying it to <code>C:\ProgramData\Datadog</code> will work, but I submitted an improvement request to Datadog Support.</p>
"
Datadog,50785165,47436429,0,"2018/06/10, 18:22:21",True,"2018/06/10, 18:22:21",518,3605831,0,"<p>Yes it does have this functionality in the Audit Association entity. The entity stored with the blame_id in the Audit Log entity contains information regarding the user. The one with source_id contains information regarding the entity itself and thus the ID of the entity in the <code>fk</code> field.</p>
"
Datadog,47294550,41578245,0,"2017/11/14, 22:22:41",False,"2017/11/14, 22:22:41",51,2454643,0,"<p>Once the datadog is properly installed on your server, you can use the custom metric feature to let datadog read your query result into a custom metric and then use that metric to create a dashboard.</p>

<p>You can find more on custom metric on datadog <a href=""https://help.datadoghq.com/hc/en-us/articles/208385813-Postgres-custom-metric-collection-explained"" rel=""nofollow noreferrer"">here</a></p>

<p>They work with yaml file so be cautious with the formatting of the yaml file that will hold your custom metric.</p>
"
Datadog,66880008,66875990,0,"2021/03/31, 03:15:15",False,"2021/03/31, 03:15:15",3,1467883,0,"<p>It looks like Datadog uses zstd compression in order to compress its data before sending it: <a href=""https://github.com/DataDog/datadog-agent/blob/972c4caf3e6bc7fa877c4a761122aef88e748b48/pkg/util/compression/zlib.go"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-agent/blob/972c4caf3e6bc7fa877c4a761122aef88e748b48/pkg/util/compression/zlib.go</a></p>
"
Datadog,66345892,66326300,0,"2021/02/24, 08:42:37",False,"2021/02/24, 08:55:54",3000,1335245,0,"<p>This is what I've got so far. Everything but the <code>source</code>.</p>
<p><a href=""https://i.stack.imgur.com/WrrVs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WrrVs.png"" alt=""Sending env, hostname, and service to datadog"" /></a></p>
<pre class=""lang-java prettyprint-override""><code>import ch.qos.logback.classic.LoggerContext;

// Add context to logs
LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory();
lc.setPackagingDataEnabled(true);
lc.putProperty(&quot;source&quot;, &quot;java&quot;);
lc.putProperty(&quot;service&quot;, &quot;thing-doer&quot;);
lc.putProperty(&quot;host&quot;, &quot;prd1.do.things&quot;));
lc.putProperty(&quot;env&quot;, &quot;production&quot;);
</code></pre>
"
Datadog,65834472,65829133,0,"2021/01/21, 21:29:00",False,"2021/01/21, 21:29:00",17806,119790,0,"<p>Need to use category-processor <a href=""https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor</a></p>
<p>Example:
<a href=""https://i.stack.imgur.com/Oicit.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oicit.png"" alt=""enter image description here"" /></a></p>
<p>I didn't do enough research before posting this question, but the answer for anyone else looking.</p>
"
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756,9629802,0,"<p>You are using the DataDog configuration for the commercial <a href=""https://k6.io/cloud"" rel=""nofollow noreferrer"">k6 Cloud service</a> (<code>k6 cloud</code>), not locally run k6 tests (<code>k6 run</code>). <code>test_run_id</code> is a concept in the cloud service, though it's also easy to emulate locally as a way to distinguish between test runs.</p>
<p>For local tests, you should enable the DataDog output by running k6 with <code>k6 run --out datadog script.js</code>. I assume you did that, otherwise you wouldn't see any metrics in DataDog.</p>
<p>Then, you can use the <a href=""https://k6.io/docs/using-k6/options#tags"" rel=""nofollow noreferrer""><code>tags</code> option</a> to inject a unique extra tag for all metrics generated by a particular k6 run, so you can differentiate them in DataDog. For example:</p>
<pre><code>k6 run --out datadog --tag test_run_id=1 script.js
k6 run --out datadog --tag test_run_id=2 script.js
k6 run --out datadog --tag test_run_id=3 script.js
...
</code></pre>
<p>Of course, you can choose any <code>key=value</code> combination, you are not restricted to <code>test_run_id</code>.</p>
"
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56,10960667,1,"<p>Nothing is wrong there. DataDog conceals that the Kafka integration uses Dogstatsd under the hood. When <code>use_dogstatsd: 'true</code> within /etc/datadog-agent/datadog.yaml is set, metrics do appear in DataDog webUI. If that option is not set the default Broker data is available via JMXFetch using <code>sudo -u dd-agent datadog-agent status</code> as also via <code>sudo -u dd-agent datadog-agent check kafka</code> but not in the webUI.</p>
"
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530,11866104,0,"<p>Based on the <a href=""https://stackshare.io/stackups/grafana-vs-stackdriver"" rel=""nofollow noreferrer"">doc</a> you can decide which one is good for your use case. Stackdriver is detailed as &quot;Monitoring, logging, and diagnostics for applications on Google Cloud Platform and AWS&quot;. Google Stackdriver provides powerful monitoring, logging, and diagnostics. It equips you with insight into the health, performance, and availability of cloud-powered applications, enabling you to find and fix issues faster.</p>
<p>Grafana can be classified as a tool in the &quot;Monitoring Tools&quot; category, while Stackdriver is grouped under &quot;Cloud Monitoring&quot;.</p>
"
Datadog,65631654,65629898,0,"2021/01/08, 17:15:24",False,"2021/01/08, 17:15:24",678,11584728,0,"<p>Answering my own question this might be helpful for others</p>
<p>I had to set</p>
<pre><code>logs_enabled: true
</code></pre>
<p>inside <code>/etc/datadog-agent/datadog.yaml</code> then I've created <code>python.d/conf.yaml</code> with the following configs</p>
<pre><code>init_config:

instances:

##Log section
logs:

  - type: file
    path: &quot;&lt;PATH_TO_PYTHON_LOG&gt;.log&quot;
    service: &quot;&lt;YOUR_APPLICATION&gt;&quot;
    source: python
    sourcecategory: sourcecode
    # For multiline logs, if they start by the date with the format yyyy-mm-dd uncomment the following processing rule
    #log_processing_rules:
    #  - type: multi_line
    #    name: new_log_start_with_date
    #    pattern: \d{4}\-(0?[1-9]|1[012])\-(0?[1-9]|[12][0-9]|3[01])
</code></pre>
<p>Restart the agent with</p>
<pre><code>sudo service datadog-agent restart
</code></pre>
<p>You can see your logs in the dashboard logs panel</p>
"
Datadog,65403420,65320127,0,"2020/12/22, 06:52:25",False,"2020/12/22, 06:52:25",1,14869489,0,"<p>Window option available... Go to integration and  click on agents.. There is an option available windows ( left side ).. Click on windows then u get link..... Just copy the link and paste it on ur windows Server.... Then datadog starts monitoring... It takes 5 minutes to monitor</p>
"
Datadog,64776832,64772196,0,"2020/11/10, 23:11:22",False,"2020/11/10, 23:11:22",11561,970308,0,"<p>The MeterRegistry already has implemented how to send custom metrics (posting to DataDog) See the code <a href=""https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java#L133"" rel=""nofollow noreferrer"">https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java#L133</a></p>
<p>It appears like you are trying to add common tags to each metric. Perhaps you shouldn't implement your own DataDog registry, and instead use the provided one to send the metrics and set the common tags via config:</p>
<pre><code>registry.config().commonTags(Arrays.asList(
  Tag.of(MetricConstants.TENANT_MONIKER, tenant.getTenantMoniker()), 
  Tag.of(MetricConstants.STACK_NAME, stackName)
));
</code></pre>
"
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156,2676108,1,"<p>I don't believe there is any way to graph the historical behavior of the SLI from an SLO.</p>
<p>The closest you could get would be to measure the underlying metric, so if you had <code>good events</code>/<code>bad events</code> you could display that percentage. But the calculation of how often that percentage is above or below a certain threshold would not be possible.</p>
<p>I recommend reaching out to support@datadoghq.com to let them know it's a feature you're interested in. They might be able to provide some updates.</p>
"
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21,7314273,0,"<p>Not sure if this will work but you can give it a try..:</p>
<ol>
<li>create 2 instances of the same monitor mentioned about</li>
<li>create a composite monitor based on them both</li>
<li>trigger the composite only when a.value is not the same as b.value</li>
</ol>
<hr />
<p>{{^is_exact_match a.value b.value }}</p>
<p>@my@mail.com
Alert 2 hosts has passed the threshold</p>
<p>{{/is_exact_match}}</p>
<p>same value - ignore - do nothing</p>
<hr />
<p>The problem is that you probably might get 2 alerts at the same time...</p>
"
Datadog,64396016,64345103,0,"2020/10/16, 23:28:57",True,"2020/10/16, 23:28:57",318,1253272,0,"<p>It turns out that trace id can be set via HTTP endpoint <a href=""https://docs.datadoghq.com/api/v1/tracing/#send-traces"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/v1/tracing/#send-traces</a>. There doesn't seem to be an option for sending traces to the agent directly.</p>
<p>This can still be useful if the performance penalty of making HTTP calls is not a concern, i.e., if you are not working on a real-time system.</p>
"
Datadog,63971375,63909996,0,"2020/09/19, 20:06:40",True,"2020/09/19, 20:06:40",3852,1601506,1,"<p>Datadog keeps the logs for a period of time according to the billing plan you've selected: <a href=""https://www.datadoghq.com/pricing/#section-log"" rel=""nofollow noreferrer"">https://www.datadoghq.com/pricing/#section-log</a> If you choose the 7 day plan, logs will be dropped from Datadog after 7 days.</p>
<p>The default plan seems to be 15 days, but there are other options between 3-60 days.</p>
"
Datadog,63668170,63603260,0,"2020/08/31, 12:25:38",True,"2020/08/31, 12:25:38",2771,842302,0,"<p>I've solved it now by verifying the status via code and by adding tags to the metrics:</p>
<ul>
<li><code>occurrence:first</code></li>
<li><code>subsequent</code></li>
</ul>
<p>This way I can filter in my dashboard for <code>occurrence:first</code> only.</p>
"
Datadog,63682403,63603260,0,"2020/09/01, 09:21:17",False,"2020/09/01, 09:21:17",336,7243426,0,"<p>To make sure things are clear, you have a metric called <code>myService.errorType</code> with a tag <code>entity</code>. This metric is a counter that will increase every time an entity is in error. You will then use this metric query:</p>
<pre><code>sum:myService.errorType{*} by {entity}
</code></pre>
<p>When you speak about UUID, it seems that the cardinality is small (here you show 3). Which means that every hour you will have small amount of UUID available. In that case, adding UUID to the metric tags is not as critical as user ID, timestamp, etc. which have a limitless number of options.</p>
<p>I would invite you to add this uuid tag, and check the cardinality in the <a href=""https://app.datadoghq.com/metric/summary"" rel=""nofollow noreferrer"">metric summary page</a> to ensure it works.</p>
<p>Then to get the number of UUID concerned by errors, you can use something like:</p>
<pre><code>count_not_null(sum:myService.errorType{*} by {uuid})
</code></pre>
<p>Finally, as an alternative, if the cardinality of UUID can go through the roof, I would invite you to work with logs or work with Christopher's solution which seems to limit the cardinality increase as well.</p>
"
Datadog,64256562,63599025,0,"2020/10/08, 08:55:02",False,"2020/10/08, 08:55:02",537,3134333,0,"<p>You may need to set the environment variable <code>DD_APM_NON_LOCAL_TRAFFIC=true</code> in your datadog agent container.</p>
<p>Ref: <a href=""https://docs.datadoghq.com/agent/docker/apm/?tab=linux#docker-apm-agent-environment-variables"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/docker/apm/?tab=linux#docker-apm-agent-environment-variables</a></p>
"
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797,2135,0,"<p>Allowing 'triggered' (but not 'recovery') monitor notifications is not a configurable option for the integration on either Opsgenie or Datadog.</p>
<p>You <em>can</em>, however, control this within the Datadog Monitor message body where you reference opsgenie</p>
<blockquote>
<p>Lorem ipsum dolor sit amet @opsgenie-oncall @slack-somechannel</p>
</blockquote>
<p>You can wrap the opsgenie reference within the message body with conditional tags (datadog actually calls them variables) documented here:
<a href=""https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables</a>
Then the message body might look something like this:</p>
<blockquote>
<p>Lorem ipsum dolor sit amet {{#is_alert}}@opsgenie-oncall{{/is_alert}} @slack-somechannel</p>
</blockquote>
<p>Now the alert to opsgenie will only occur on the 'trigger' of the monitor but not on 'recovery'</p>
"
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251,5477963,1,"<p>If you are sending metrics to an actual StatsD server, then tags are not supported by the protocol. You would need to instead send the metrics to the Datadog agent's DogStatsD endpoint which extends StatsD with additional features such as tags. You can find more information about DogStatsD <a href=""https://docs.datadoghq.com/developers/dogstatsd/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>If you are already using the DogStatsD endpoint, then I would suspect an incompatibility with the <code>node-statsd</code> library. The library has not been updated for 6 years and it's possible that something changed since then, causing it to no longer work. In that case I would recommend switching to a more recent DogStatsD client that is still maintained such as <a href=""https://github.com/brightcove/hot-shots"" rel=""nofollow noreferrer"">hot-shots</a>.</p>
<p>Hope this helps!</p>
"
Datadog,62476112,62474317,0,"2020/06/19, 21:03:23",False,"2020/06/19, 21:03:23",1371,5540166,0,"<p>There's a 2-click path from Slack that should already do this for you out-of-the-box. The slack notification gives you a link to the alert event in your Datadog account (click-1), and from the alert event, towards the bottom you'll find a series of links to other relevant places, one of those is ""Related Logs"" (click-2).</p>

<p>That brings you to the Log Explorer scoped to the relevant time period of the alert, and scoped to the tags of whatever it was that was alerted on (so presumably the logs you're looking for).</p>

<p>If you want to add a link of this sort as something you can configure in the alert message, that sounds like something you should reach out to support@datadoghq.com for to ask Datadog to implement it. </p>
"
Datadog,62672749,62474317,0,"2020/07/01, 11:29:39",False,"2020/07/01, 11:29:39",1,3327131,0,"<p>In the end we solved the problem dynamically building the url for the logs:</p>
<pre><code>https://app.datadoghq.com/logs?cols=core_host%2Ccore_service&amp;index=&amp;live=true&amp;messageDisplay=inline&amp;query=service%3Aregistration+env%3A{{environment.name}}&amp;stream_sort=desc&amp;to_ts={{last_triggered_at_epoch}}
</code></pre>
"
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179,1032890,0,"<p>There's a Prometheus endpoint for Tibco EMS:</p>

<p><a href=""https://community.tibco.com/wiki/statistics-logger-tibco-enterprise-message-service#toc-15"" rel=""nofollow noreferrer"">https://community.tibco.com/wiki/statistics-logger-tibco-enterprise-message-service#toc-15</a></p>

<p>I think you can then add the prometheus integration to your datadog agent to send the data do datadog, and build your own dashboard for that:</p>

<p><a href=""https://docs.datadoghq.com/integrations/prometheus/#data-collected"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/prometheus/#data-collected</a></p>
"
Datadog,62228982,62221212,3,"2020/06/06, 11:04:38",False,"2020/06/06, 11:04:38",21,7314273,1,"<p>I had something similar issue and chose the first option, but i won't say it is from terraform perspective (since i also had lack in experience in terraform). 
The first hierarchy was more reasonable in segregation aspects, plus would be easier to add/remove/update organizations by demand.</p>
"
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744,10300113,0,"<p>Answer from Datadog Support to this:</p>

<blockquote>
  <p>Thanks again for reaching out to Datadog!</p>
  
  <p>From looking further into this, there does not seem to be a way we can package the JDBC  driver with the Datadog Agent. I understand that this is not desirable as you would prefer to use a standard image but I believe the best way to have these bundled together would be to have a custom image for your deployment.</p>
  
  <p>Apologies for any inconveniences that this may cause. </p>
</blockquote>
"
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371,5540166,3,"<p>Yes, you can configure widgets to exclude results by tags. You can do this by applying a tag prepended with a <code>!</code> to signify ""not"". </p>

<p>So in your case, you can set up your widget scoped over <code>importance:ignore</code> and then hit the little <code>&lt;/&gt;</code> button on the right to expose the underlying query, and sneak a <code>!</code> in front to make it <code>!importance:ignore</code>. </p>

<p><a href=""https://docs.datadoghq.com/tagging/using_tags/?tab=assignment#notebooks"" rel=""nofollow noreferrer"">This doc has a nice example</a> (although it's for notebooks, it works the same in dashboards as well). </p>
"
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447,554481,0,"<p>After talking with Datadog support, it seems like this is a known issue.</p>

<blockquote>
  <p>Thanks for your patience while we looked into this issue. We we're currently investigating this along with PoolExecutors and will reach out with updates. Right now it looks like those child spans within the async call lose context, so they appear disconnected.</p>
</blockquote>

<p>The workaround for now is to pass in the parent's context. Add this line just before calling the thread pool executor.</p>

<pre><code>current_context = tracer.get_call_context()
</code></pre>

<p>Then pass that context to the function that gets run in the threadpool:</p>

<pre><code>perform_work(
    input_value=input_value,
    parent_context=current_context
)

</code></pre>

<p>And use it to create a span inside the function like this:</p>

<pre><code>span = tracer.start_span('do_something', child_of=parent_context)
seconds = random()
time.sleep(seconds)
span.finish()
</code></pre>

<p>The complete example looks like this:</p>

<pre><code>from concurrent.futures import ThreadPoolExecutor
from ddtrace import tracer
from random import random
import time


def perform_work(input_value, parent_context=None):
    span = tracer.start_span('do_something', child_of=parent_context)
    seconds = random()
    time.sleep(seconds)
    span.finish()
    return input_value ** 2


def sync_work(input_values):
    with tracer.trace('sync_work') as _:
        results = []
        for input_value in input_values:
            result = perform_work(input_value=input_value)
            results.append(result)
        return results


def async_work(input_values):
    with tracer.trace('async_work') as _:
        current_context = tracer.get_call_context()
        thread_pool = ThreadPoolExecutor(max_workers=10)
        futures = thread_pool.map(
            lambda input_value:
            perform_work(
                input_value=input_value,
                parent_context=current_context
            ),
            input_values
        )
        results = list(futures)
        return results


@tracer.wrap(service='ddtrace-example')
def start_work():
    input_values = list(range(15))
    sync_results = sync_work(input_values=input_values)
    print(sync_results)
    async_results = async_work(input_values=input_values)
    print(async_results)


if __name__ == '__main__':
    start_work()
</code></pre>

<p>This will produce a result that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/oYa2M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oYa2M.png"" alt=""enter image description here""></a></p>
"
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371,5540166,0,"<p><strong>First</strong> you'll want to make sure your logs are well structured (which you can control in <a href=""https://docs.datadoghq.com/logs/processing/"" rel=""nofollow noreferrer"">Datadog's processing pipelines</a>). Effectively you'll want to parse out the ""code"" values into some ""error code"" attribute. </p>

<p>If your log events are in this format...</p>

<pre><code>2020-01-01 12:10:10 myservername - Server Error {""error"":{""code"":1001,""type"":""MATCH"",""message"":""Invoke failed: Failed""}}
</code></pre>

<p>...Then all you need is a fairly simple grok parser rule, thanks to the ""json"" filter function. Something like this would get you where you want (note the <code>%{data::json}</code> part, that's what parses the in-log JSON). </p>

<pre><code>myrulename %{date(""yyyy-mm-dd' 'HH:MM:ss""):timestamp} %{notSpace:hostname} - Server Error %{data::json}
</code></pre>

<p>Once you've configured this, your logs will also have an attribute called ""error.code"" with a value of <code>2001</code> or <code>1001</code> or whatever. </p>

<p><strong>Second</strong> you'll want to <a href=""https://docs.datadoghq.com/logs/explorer/?tab=logsearch#setup"" rel=""nofollow noreferrer"">create a facet</a> for that new <code>error.code</code> attribute so that you can <a href=""https://docs.datadoghq.com/logs/explorer/analytics/?tab=timeseries"" rel=""nofollow noreferrer"">make toplist / timeseries / etc. graphs grouped out by your ""error code"" facet</a>.</p>
"
Datadog,60307539,60216953,0,"2020/02/19, 21:07:06",False,"2020/02/19, 21:07:06",571,2019978,0,"<p>No, you cannot install the Datadog agent on a Snowflake host.</p>

<p>We use our separate job scheduling system to monitor Snowflake by running queries (e.g. checks on SYSTEM$CLUSTERING_DEPTH, aggregate queries against the QUERY_HISTORY for timing, etc) via the JDBC connector then relaying the results to our monitoring stack (similar to how Datadog agent would work.)</p>
"
Datadog,59096255,59092413,0,"2019/11/28, 23:19:23",True,"2019/11/28, 23:19:23",91617,354577,1,"<p>Point both of those at a service like <a href=""https://httpbin.org/"" rel=""nofollow noreferrer"">httpbin</a> to see how they differ.</p>

<p>Requests' <code>data</code> option for POST requests <a href=""https://2.python-requests.org/en/master/user/quickstart/#more-complicated-post-requests"" rel=""nofollow noreferrer"">generates form-encoded data</a> by default, while <code>curl</code> passes the JSON string through directly. You can manually encode your payload as a JSON string:</p>

<pre class=""lang-py prettyprint-override""><code>import json

response = requests.post(..., data=json.dumps(data))
#                                  ^^^^^^^^^^
</code></pre>

<p>or if you have Requests version 2.4.2 or later you can use the <code>json</code> parameter to have your <code>dict</code> converted to JSON automatically:</p>

<pre class=""lang-py prettyprint-override""><code>response = requests.post(..., json=data)
#                             ^^^^
</code></pre>
"
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336,7243426,0,"<p>One solution would be to setup in <code>logs &gt; configuration &gt; pipelines</code> a <a href=""https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor"" rel=""nofollow noreferrer"">category processor</a> to add a new attribute that could be made searchable.</p>

<p><a href=""https://i.stack.imgur.com/0l7ZJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0l7ZJ.png"" alt=""category processor example""></a></p>

<hr>

<p><em>edit: 19th Nov 2019</em></p>

<p>Step 1:</p>

<p>Add grok parser to extract sign:</p>

<p>rule: <code>detect_dollar .*%{regex(""[$]+""):dollarSign}.*</code>
<a href=""https://i.stack.imgur.com/aJwhy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aJwhy.png"" alt=""enter image description here""></a></p>

<p>Step 2:</p>

<p>Then you can setup a category processor as indicated above. This could look for the attribute <code>@dollarSign:$</code> and set the attribute <code>hasDollarSign</code> to True and set it to false otherwise.</p>

<p><a href=""https://i.stack.imgur.com/wmivb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wmivb.png"" alt=""enter image description here""></a></p>

<p>Step 3:</p>

<p>Create a facet on <code>dollarSign</code> attribute. Following logs can then be searched for.</p>

<p>For logs with no <code>$</code></p>

<pre><code>-@dollarSign:$
</code></pre>

<p>For logs with <code>$</code></p>

<pre><code>@dollarSign:$
</code></pre>

<p>You can do the same with the <code>hasDollarSign</code> attribute and set it as a facet.</p>
"
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336,7243426,1,"<p>I've tried with this query which is similar to yours: </p>

<pre><code>events('sources:rds priority:all tags:event_source:db-instance').by('dbinstanceidentifier').rollup('count').last('1d') &gt; 1
</code></pre>

<p><a href=""https://i.stack.imgur.com/mO06q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mO06q.png"" alt=""enter image description here""></a></p>

<p>And this seems to give me a count by <code>dbinstanceidentifier</code> results.</p>

<p>Do you have more information to provide? Maybe an event list and a monitor result screenshot?</p>

<p><a href=""https://i.stack.imgur.com/quSOj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/quSOj.png"" alt=""enter image description here""></a></p>
"
Datadog,58689757,58630932,1,"2019/11/04, 10:41:42",True,"2019/11/04, 11:05:42",336,7243426,0,"<ol>
<li>First, I would check what metrics are being stored in Datadog with the <a href=""https://docs.datadoghq.com/api/?lang=python#query-timeseries-points"" rel=""nofollow noreferrer"">API</a></li>
<li>Second, I would check the type of metric you are sending <code>https://app.datadoghq.com/metric/summary?metric=&lt;my-metric&gt;</code> Not sure if a counter type can have decimal values, maybe a gauge would be more appropriate.</li>
<li>Finally, to display a value you can use a query value widget such as the one in the snippet below. Make sure to:

<ul>
<li>Take the last value (here aggregator: last)</li>
<li>Select the number of decimals of interest (here precision: 3)</li>
<li>Select the right space aggregation if you receive this metric from multiple places (i.e. multiple tags) (here avg:my_metric{*})</li>
</ul></li>
</ol>

<pre><code>{
  ""viz"": ""query_value"",
  ""requests"": [
    {
      ""q"": ""avg:nginx.logs.request.count{*}.as_count()"",
      ""type"": null,
      ""style"": {
        ""palette"": ""dog_classic"",
        ""type"": ""solid"",
        ""width"": ""normal""
      },
      ""aggregator"": ""last"",
      ""conditional_formats"": [
        {
          ""comparator"": ""&gt;"",
          ""palette"": ""white_on_red"",
          ""value"": null
        },
        {
          ""comparator"": ""&gt;="",
          ""palette"": ""white_on_yellow"",
          ""value"": null
        },
        {
          ""comparator"": ""&lt;"",
          ""palette"": ""white_on_green"",
          ""value"": null
        }
      ]
    }
  ],
  ""autoscale"": true,
  ""precision"": ""3""
}
</code></pre>

<p><strong>Side note</strong>: I also use this for Kafka (just as a reference) but it should not be required in your case:</p>

<pre><code>ENTRYPOINT [""java"",""-javaagent:dd-java-agent.jar"",""-Ddd.agent.host=localhost"",""-Ddd.jmxfetch.statsd.host=localhost"",""-Ddd.trace.global.tags=env:kafka"",""-Ddd.agent.port=8126"",""-Ddd.service.name=KafkaProducer"",""-Ddd.logs.injection=true"",""-Ddd.trace.analytics.enabled=true"",""-Ddd.kafka.analytics.enabled=true"",""-Ddd.servlet.analytics.enabled=true"",""-Ddd.spring-web.analytics.enabled=true"",""-jar"",""target/KafkaConsumer-0.0.1-SNAPSHOT.jar""]
</code></pre>
"
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251,5477963,2,"<p>The main issue is that you would have to mount the volume in both the app container and the agent container in order to make it available. It also means you have to find a place to store the log file before it gets picked up by the agent. Doing this for every container could become difficult to maintain and time consuming.</p>

<p>An alternative approach would be to instead send the logs to <code>stdout</code> and let the agent collect them with the Docker integration. Since you configured <code>logsConfigContainerCollectAll</code> to <code>true</code>, the agent is already configured to collect the logs from every container output, so configuring Winston to output to <code>stdout</code> should just work.</p>

<p>See: <a href=""https://docs.datadoghq.com/agent/docker/log/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/docker/log/</a></p>
"
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336,7243426,2,"<p>To support rochdev comment, here are a few code snippets to help out (if you do not opt in for the STDOUT method which should be simpler). This is only to mount the right volume inside the container agent.</p>

<p>On your app deployment, add:</p>

<pre><code>   spec:
      containers:
      - name: your_nodejs_app
        ...
        volumeMounts:
          - name: abc
            mountPath: /app/logs
      volumes:
        - hostPath:
            path: /app/logs
          name: abc
</code></pre>

<p>And on your agent daemonset:</p>

<pre><code>   spec:
      containers:
      - image: datadog/agent
        ...
        volumeMounts:
          ...
          - name: plop
            mountPath: /app/logs
      volumes:
        ...
        - hostPath:
            path: /app/logs/
          name: plop
</code></pre>
"
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336,7243426,0,"<p>At first glance, this seems to be a use case more suitable for the APM part of Datadog which would measure the execution time and could be <a href=""https://docs.datadoghq.com/tracing/advanced/adding_metadata_to_spans/?tab=java"" rel=""nofollow noreferrer"">instrumented</a> to measure the execution time of the smaller functions (if it does not picked up this data automatically). You can then create some nice charts with <a href=""https://docs.datadoghq.com/tracing/trace_search_and_analytics/?tab=java"" rel=""nofollow noreferrer"">Trace Search and Analytics</a>.</p>

<p>You could also use a custom metric with tags such as <code>opsize:large</code> and <code>opsize:small</code> which would represent the execution time (a gauge). You can find more details <a href=""https://docs.datadoghq.com/developers/metrics/custom_metrics/#pagetitle"" rel=""nofollow noreferrer"">here</a>.</p>

<p>At the moment, the log module of Datadog does not seem to support the calculation you expect to see. However, the two solutions above and the related logs can be made visible in a dashboard side by side.</p>
"
Datadog,55956522,55953321,1,"2019/05/02, 19:18:34",False,"2019/05/02, 19:18:34",2574,4162641,0,"<p>Have you tried using the <code>start</code> and <code>end</code> tags with a 24 hour window?</p>
"
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9,11043786,0,"<p>As I posted the issue on GitHub, they give an answer the issue was in source code for dd-trace-php and they will fix and new release.
<a href=""https://github.com/DataDog/dd-trace-php/issues/334"" rel=""nofollow noreferrer"">https://github.com/DataDog/dd-trace-php/issues/334</a></p>

<p>Below response of DatDog in github:</p>

<p>Ah now this is much more clear, thanks for sharing. This is a known problem that we are currently and actively working on. As I cannot commit to that, the fix will be probably come out with the next release.</p>

<p>At an higher level, the cause is an issue we have in some specific cases while invoking private/protected methods and parent::* invocations.</p>

<p>In the meantime, if you are still interested in testing/using the other integrations, the only thing I can recommend is to disable the pdo integration: <strong>fastcgi_param DD_INTEGRATIONS_DISABLED pdo</strong>.</p>

<p>Again, the fix to this is currently in development and will be released very soon.</p>
"
Datadog,53624609,53459133,0,"2018/12/05, 05:10:35",True,"2018/12/05, 05:10:35",3223,43973,1,"<p>Use the Query Value widget. It can only show a single value, which is the average for the current time window that has been chosen.</p>
"
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371,5540166,1,"<p>Maybe you could mod the process check to also tag the process number metric by PID (<a href=""https://github.com/DataDog/integrations-core/blob/6.6.0/process/datadog_checks/process/process.py#L415-L418"" rel=""nofollow noreferrer"">this is probly where you'd change that</a>). That way you could group your monitor by your pid tag and the no-data alerts would tell you when the pid switched. </p>

<p>But this would also alert on expected pid changes, so maybe you'd have to schedule downtimes too aggressively for this to be a good idea?</p>

<p>Maybe monitoring some crash logs with <a href=""https://app.datadoghq.com/logs"" rel=""nofollow noreferrer"">their Log Management tool</a> would be a better approach?</p>
"
Datadog,53232465,53231545,0,"2018/11/09, 21:54:22",False,"2018/11/09, 21:54:22",1710,2779323,2,"<p>Lambda is serverless. Datadog agent is for the host. While running lambda you have absolutely no control over the host as you are not managing it.</p>

<p>Hence, You can monitor application running on lambda using datadog integration of lambda for the different application.</p>

<p>You may follow below link for AWS Integration of datadog. </p>

<p>Ref: <a href=""https://docs.datadoghq.com/integrations/amazon_lambda/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/amazon_lambda/</a></p>
"
Datadog,53400457,53226600,0,"2018/11/20, 21:44:45",False,"2018/11/20, 21:44:45",156,2676108,0,"<p>You can monitor a database from a different host as long as the host the agent is running on has access. So for this section in the config file:</p>

<pre><code>instances:
  - host: localhost
    port: 5432
</code></pre>

<p><a href=""https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/data/conf.yaml.example#L4"" rel=""nofollow noreferrer"">https://github.com/DataDog/integrations-core/blob/master/postgres/datadog_checks/postgres/data/conf.yaml.example#L4</a></p>

<p>instead of using <code>localhost</code> you can set the IP. This will allow you to monitor your database without adding a new agent. Then you would just follow the normal <a href=""https://docs.datadoghq.com/integrations/postgres/#prepare-postgres"" rel=""nofollow noreferrer"">postrgres setup</a> and you will have access to all the metrics and <a href=""https://docs.datadoghq.com/integrations/postgres/#service-checks"" rel=""nofollow noreferrer"">service checks</a> including <code>postgres.can_connect</code> which is probably what you care about.</p>
"
Datadog,53114821,52311463,0,"2018/11/02, 10:10:01",False,"2018/11/02, 10:10:01",1976,43842,1,"<p>It seems to be an intentional ""feature"" <a href=""https://github.com/kamon-io/kamon-datadog/issues/19"" rel=""nofollow noreferrer"">https://github.com/kamon-io/kamon-datadog/issues/19</a> introduced in 1.x. They have chosen approach to put service name in tag instead.</p>
"
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978,7217896,0,"<p>Even though datadog is being run from the same machine, it is setting up a separate server on your machine. Because of that, it sounds like the datadog agent doesn't have access to your z:/ driver.</p>

<p>Try to put the ""TaskResults"" folder in your root directory (when running from datadog - where the mycheck.yaml file is) and change the path accordingly.</p>

<p>If this works and you still want to have a common drive to be able to share files from your computer to datadog's agent, you have to find a way to mount a drive\folder to the agent. They probably have a way to do that in the <a href=""https://docs.datadoghq.com/agent/"" rel=""nofollow noreferrer"">documentation</a></p>
"
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410,831608,0,"<p>The solution to this is to create a file share on the network drive and use that path instead of the full network drive path.
May be obvious to some but it didn't occur to me right away since the normal Python code worked without any issue outside of Datadog.</p>

<p>So instead of: </p>

<pre><code>init_config:
taskResultLocation: ""Z:/TaskResults""
</code></pre>

<p>use</p>

<pre><code>init_config:
taskResultLocation: '//FileShareName/d/TaskResults'
</code></pre>
"
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474,10109833,0,"<p>Have a look at the documentation for <a href=""https://docs.datadoghq.com/agent/faq/dogstream/"" rel=""nofollow noreferrer"">Dogstream</a>. It allows you to send metrics to datadog from log files (including summarised metrics).</p>

<p>You may need to write a custom parser for any data that is not in the datadog canonical format in order for datadog to recognize the data. Checkout the example <a href=""https://docs.datadoghq.com/agent/faq/dogstream/#example-for-metrics-collecting"" rel=""nofollow noreferrer"">here</a>.</p>
"
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561,970308,0,"<p>This sounds like a bug. It is possible the Datadog exporter is running in a non-daemon thread. The JVM views non-daemon threads as application critical work. </p>

<p>So essentially the JVM thinks it shouldn't shutdown until the non-daemon thread finishes. In the case of the Datadog exporter thread, that probably won't happen.</p>

<p>To verify there are non-daemon threads, use <code>jstack</code> to generate a thread dump. (command: <code>jstack &lt;pid&gt;</code>) or dump all threads in your <code>close</code> method:</p>

<pre><code>ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();
for (ThreadInfo ti : threadMxBean.dumpAllThreads(true, true)) {
  System.out.print(ti.toString());
}
</code></pre>

<p>An example thread dump output is below. Notice the word 'daemon' on the first line:</p>

<pre><code>""pool-1-thread-1"" #13 prio=5 os_prio=31 tid=0x00007fe885aa5000 nid=0xa907 waiting on condition [0x000070000d67b000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000006c07e9720&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
"
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939,2506172,3,"<p><a href=""https://docs.datadoghq.com/developers/metrics/#gauges"" rel=""nofollow noreferrer"" title=""Gauge metric"">Gauge metric</a> types will do the job here given that your query does not run more than once within 10 seconds. If that is not the case, go for <a href=""https://docs.datadoghq.com/developers/metrics/#count"" rel=""nofollow noreferrer"" title=""count metric"">count metric</a> </p>

<p>The flush interval in datadog by default is 10 seconds, if you use a <strong>gauge metric</strong> and the metric is reported more than once in a flush interval, datadog agent only sends the last value ignoring the previous ones. For <strong>count metric</strong> in contrast, the agent sums up all the values reported in the flush interval.</p>

<p>More details about flush interval <a href=""https://help.datadoghq.com/hc/en-us/articles/211545826-Why-histogram-stats-are-all-the-same-inaccurate-Characteristics-of-Datadog-histograms-"" rel=""nofollow noreferrer"" title=""here"">here</a>.</p>
"
Datadog,65533013,49381672,0,"2021/01/01, 21:35:13",False,"2021/01/01, 21:35:13",1718,555329,0,"<p>The best metric type would be a <code>histogram</code> metric.  This will take multiple values, and pre-aggregate them within a flush window, so you will be able to get things like min/max/sum/avg and various percentiles.</p>
<p>If you run multiple times within a flush window:</p>
<ul>
<li><code>count</code> would combine multiple values together, so you would lose the individual numbers, meaning you couldn't easily tell between the process returning a lot of documents, or it returning only a few, but being called a lot</li>
<li><code>gauge</code>, as mentioned in @narayan's answer, would only keep the latest, making it harder to get thins like the max/min count.</li>
</ul>
"
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365,4894399,0,"<p>As I have mentioned in the comment, you are affected by <a href=""https://coderanger.net/two-pass/"" rel=""nofollow noreferrer"">two pass model</a>. You should remove the keys in the resource added to the end of the chef run or triggered by the DD cookbook resources invoked as the last one in the run.</p>

<pre><code>ruby_block ""clean datadog api attributes"" do
  block do
    node.rm(""datadog"", ""api_key"")
    ....
  end
  subscribes :create, ""template[&lt;some dd template using api keys&gt;]"", :immediately
end
</code></pre>

<p>However, it may not work with all versions of DD cookbook. From few DD cookbook versions, it is possible to store keys in node's run state which is not written to the Chef server.</p>

<pre><code>node.run_state[""datadog""] = {
  ""api_key""         =&gt; datadog[""api_key""],
  ""application_key"" =&gt; datadog[""application_key""]
}
</code></pre>

<p>The above example is preferred solution to your issue.</p>
"
Datadog,47003630,47003531,0,"2017/10/29, 20:17:50",True,"2017/10/29, 20:17:50",2010,6020610,0,"<p>I noticed that the API key of Datadog changes everytime whenever in close the site and open a new instance. so after entering new API key , the issue was solved</p>

<p><a href=""https://i.stack.imgur.com/Jaiam.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jaiam.png"" alt=""enter image description here""></a></p>
"
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371,5540166,2,"<p>Two approaches that may work:</p>

<ol>
<li><p>It looks like flink has an <a href=""https://ci.apache.org/projects/flink/flink-docs-release-1.3/monitoring/metrics.html#datadog-orgapacheflinkmetricsdatadogdatadoghttpreporter"" rel=""nofollow noreferrer"">HTTP connector</a> to send metrics to Datadog, which at first glance looks to send over the Datadog metrics API instead of dogstatsd. </p></li>
<li><p>Dogstatsd is not very different from statsd otherwise, so it's often easy to modify statsd libraries to work for dogstatsd. <a href=""https://github.com/drivetribe/flink-metrics-datadog-statsd/blob/master/README.md"" rel=""nofollow noreferrer"">This project on GitHub</a> seems to be such a project, and may come in handy.</p></li>
</ol>
"
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371,5540166,2,"<p>Well, you <em>could</em> use the Datadog API to script the creation of 20 unique dashboards that all share the same content, but with different hosts. This is the part of the API docs that would help (with examples!) <a href=""https://docs.datadoghq.com/api/#timeboards"" rel=""nofollow noreferrer"">for Timeboards</a>, and this one <a href=""https://docs.datadoghq.com/api/#screenboards"" rel=""nofollow noreferrer"">for Screenboards</a>. </p>

<p>That said, I'd personally find 20 dashboards a bit cluttered / unwieldy in my own Datadog account. Instead, if it was me, I'd try to (A) find clever uses of dashboard template variables (on e.g, cluster tags, host tags, etc.), or (B) group out by each host tag and apply <a href=""https://help.datadoghq.com/hc/en-us/articles/204972439-There-are-too-many-lines-on-my-graph-can-I-only-display-the-most-important-ones-"" rel=""nofollow noreferrer"">the ""top()"" function</a> in some way so that I'd be able to see just the most extreme-value hosts. But that's certainly up to you :)</p>
"
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148,6815223,0,"<p>Your answer appears to be there in the text -- you're missing a Python package. Try running <code>sudo pip install psutil</code>, then restarting the agent. Can you add your agent version, OS and version, and how you installed the agent to your text as well? It looks like you're also using a <em>very</em> old version of the agent (it's up to 5.17.* for a number of OS's) so there may be better package bundling or critical updates since v. 4.4.0. Try installing a newer version as well.</p>
"
Datadog,45996096,45974396,8,"2017/09/01, 11:27:23",False,"2017/09/01, 12:04:48",1,8547108,-1,"<p>Please find the required</p>

<pre>
[root@mudcsftpup01 ~]# sudo pip install psutil
Requirement already satisfied: psutil in /usr/lib64/python2.7/site-packages
</pre>
"
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159,4907630,1,"<p>In the code you posted:</p>

<pre><code>c := datadog.Client{}
</code></pre>

<p>This seems to be creating an empty client object.</p>

<p>Shouldn't you be creating a client with your keys using <code>datadog.NewClient(""..."", ""..."")</code> as in the first code snippet you posted?</p>

<pre><code>c := datadog.NewClient(""..."", ""..."")
</code></pre>

<p>Also, you should check the error returned as that will give you more hints to troubleshoot the issue:</p>

<pre><code>_, err := c.PostEvent(&amp;e)
if err != nil {
  log.Fatalf(""fatal: %s\n"", err)
}
</code></pre>

<p>`</p>
"
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1,6688988,0,"<p>The solution:</p>

<ul>
<li><p>docker container 0</p>

<ul>
<li>running the application that is outputting the metrics</li>
<li>create a bash script within the application that is outputting the metrics.  </li>
<li>within the script use the set the value of the docker container $HOSTNAME environment variable to the jmxremote.host and the rmi.server.hostname.  </li>
</ul>

<p>#!/bin/sh<br>
    java -Djava.util.logging.config.file=logging.properties -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=9998 -Dcom.sun.management.jmxremote.port=9998 -Djava.rmi.server.hostname=$HOSTNAME -Dcom.sun.management.jmxremote.host=$HOSTNAME -Dcom.sun.management.jmxremote.local.only=false -jar /app/my-streams.jar</p>

<ul>
<li>remember to set chmod +x </li>
<li>set the dockerfile CMD to run the script above like so:<br>
CMD[""./""]</li>
</ul></li>
<li><p>docker container 1  </p>

<ul>
<li>the container running the datadog agent</li>
<li>configure the jmx.yaml file as mentioned above in the question. just set the host to the application name</li>
</ul></li>
<li><p>way more stuff was done that is available from from stack overflow posts. but the above fixes the metrics finding error from datadog-agent.</p></li>
</ul>

<hr>

<p>Here is how to run each component:  </p>

<p>docker container 0<br>
* my-streams<br>
* spin up dependent services in tab<br>
** mvn clean package docker:build<br>
** docker-compose up  </p>

<ul>
<li>another tab spin up my-streams-app<br>
** docker kill my-streams-app<br>
** docker rm my-streams-app<br>
** docker run -d --name my-streams-app -p 9998:9998 --
network=mystreams_default quay.io/myimage/my-streams  </li>
</ul>

<p>docker container 1<br>
* docker build -t dd-agent-my-streams .<br>
* docker run -v /var/run/docker.sock:/var/run/docker.sock:ro   -v /proc/:/host/proc/:ro   -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro -e LOG_LEVEL=DEBUG -e SD_BACKEND=docker --network=mystreams_default  </p>

<p>ssh into docker container 1 to verify if metrics work<br>
* docker ps // to find the name of the container to log into<br>
* docker exec -it  /bin/bash<br>
root@904e6561cc97:/# service datadog-agent configcheck<br>
root@904e6561cc97:/# service datadog-agent jmx list_everything<br>
root@904e6561cc97:/# service datadog-agent jmx collect</p>
"
Datadog,44336405,44335719,4,"2017/06/02, 22:48:19",False,"2017/06/02, 22:48:19",1371,5540166,1,"<p>I think what you actually want is the metrics-query API endpoint? <a href=""http://docs.datadoghq.com/api/#metrics-query"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/api/#metrics-query</a></p>

<p>There are also a few Node.JS libraries that may be able to handle this kind of metric querying for you: <a href=""http://docs.datadoghq.com/libraries/#community-node"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/libraries/#community-node</a></p>
"
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261,4172512,2,"<p>The recommendation of:</p>

<blockquote>
  <p>""Please don't include endlessly growing tags in your metrics, like timestamps or user ids. Please limit each metric to 1000 tags.""</p>
</blockquote>

<p>Is more of a warning against using infinitely expanding values as they can drastically increase your custom metric usage.  As mentioned in the following article:</p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-</a></p>

<blockquote>
  <p>""By default customers are allotted 100 custom metrics per host across their entire infrastructure rather than on a per-host basis. For example if you were licensed for 3 hosts, you would have 300 custom metrics by default - these 300 metrics may be divided equally amongst each individual host, or all 300 metrics could be sent from a single host.""</p>
</blockquote>

<p>You will want to keep in mind when configuring your metrics/tags of your current allotment of custom metrics and any billing implications that may have.  That said, if having these tags is important to your team please reach out to support@datadoghq.com and we can sync up with the Sales Team to determine what is best for your team and use case.</p>
"
Datadog,66357269,66354474,1,"2021/02/24, 21:03:12",False,"2021/02/24, 21:03:12",60467,86611,0,"<p>There is a preview feature that allows you to graph your SNAT port usage and allocation, see:</p>
<p><a href=""https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics#how-do-i-check-my-snat-port-usage-and-allocation"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics#how-do-i-check-my-snat-port-usage-and-allocation</a></p>
"
Datadog,65396545,65396200,1,"2020/12/21, 18:34:57",False,"2020/12/21, 18:34:57",64,6668901,1,"<p>You can look in to other datadog kube metrics like kubernetes.replicas.available / total to alert if no of available - total &lt; 0. Same can be done or for daemonset pods also there is a specific metric exposed. [Datadog docs-kube metrics][1]
[1]: https://docs.datadoghq.com/agent/kubernetes/data_collected/</p>
"
Datadog,64825884,64720852,0,"2020/11/13, 20:07:25",True,"2020/11/24, 23:46:04",177,872145,0,"<p>An issue with IE11 is fixed in v1.26.1
See the fix here: <a href=""https://github.com/DataDog/browser-sdk/pull/633/files"" rel=""nofollow noreferrer"">[RUMF-791] prevent IE11 performance entry error #633</a></p>
"
Datadog,66445632,64611252,0,"2021/03/02, 21:08:29",False,"2021/03/02, 21:08:29",2834,444794,0,"<p>Datadog's Ruby library keeps this info on the struct <code>Datadog.tracer.active_correlation</code>.</p>
<p>You can call <code>Datadog.tracer.active_correlation.trace_id</code> to grab the trace ID.</p>
"
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156,2676108,0,"<p>You probably want to be using the MySQL integration, and configure the 'custom queries' option: <a href=""https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries</a></p>

<p>You can follow those instructions after you configure the base integration <a href=""https://docs.datadoghq.com/integrations/mysql/#pagetitle"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/mysql/#pagetitle</a> (This will give you a lot of use metrics in addition to the custom queries you want to run)</p>

<p>As you mentioned, DogStatsD is a library you can import to whatever script or application in order to submit metrics. But it really isn't a common practice in the slightest to modify the underlying code of your database. So instead it makes more sense to externally run a query on the database, take those results, and send them to datadog. You could totally write a python script or something to do this. However the Datadog agent already has this capability built in, so it's probably easier to just use that.</p>

<hr>

<p>I am also just assuming SQL refers to MySQL, there are other integration for things like SQL Server, and PostgreSQL, and pretty much every implementation of sql. And the same pattern applies where you would configure the integration, and then add an extra line to the config file where you have the check run your queries.</p>
"
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251,5477963,0,"<p>There is <a href=""https://www.npmjs.com/package/dogapi"" rel=""nofollow noreferrer"">dogapi</a> which wraps the entire Datadog API and should be able to do the above use case, probably using a mix of <a href=""https://brettlangdon.github.io/node-dogapi/#metric-query"" rel=""nofollow noreferrer"">metric.query</a>, <a href=""https://brettlangdon.github.io/node-dogapi/#infrastructure-search"" rel=""nofollow noreferrer"">infrastructure.search</a>, <a href=""https://brettlangdon.github.io/node-dogapi/#search-query"" rel=""nofollow noreferrer"">search.query</a> and <a href=""https://brettlangdon.github.io/node-dogapi/#monitor-getAll"" rel=""nofollow noreferrer"">monitor.getAll</a>.</p>

<p>For example, to get the list of monitors, it would look something like this:</p>

<pre class=""lang-js prettyprint-override""><code>const dogapi = require('dogapi')

dogapi.initialize({
  api_key: 'your_api_key',
  app_key: 'your_app_key'
})

dogapi.monitor.getAll((err, res) =&gt; {
  console.log(res)
})
</code></pre>

<p>Please keep in mind that I didn't test the above code.</p>

<p>If you need something that is not in the library, it should also be fairly easy to wrap the API directly since it's a simple HTTP endpoint.</p>

<p>I hope this helps!</p>
"
Datadog,55188839,55187016,0,"2019/03/15, 20:36:45",True,"2019/03/15, 20:36:45",46,2089382,3,"<p>For each web application that you want to configure with a different <em>Datadog APM service name</em>, you need to set the environment variable <a href=""https://docs.datadoghq.com/tracing/languages/dotnet/?tab=netframeworkonwindows#configuration"" rel=""nofollow noreferrer""><code>DD_SERVICE_NAME</code></a>. If they're all running under the same IIS process, that's not possible.</p>

<p>In IIS there's a feature named <a href=""https://docs.microsoft.com/en-us/iis/configuration/system.applicationhost/applicationpools/"" rel=""nofollow noreferrer"">Application Pool</a>, which can be used to isolate multiple web applications by running them under different processes.</p>

<p>The first thing you need to do is to create a separate application pool for each web application. Once you're done with that, you can set a different <code>DD_SERVICE_NAME</code> for each application pool. The <a href=""https://docs.microsoft.com/en-us/iis/configuration/system.applicationhost/applicationpools/add/environmentvariables/#appcmdexe"" rel=""nofollow noreferrer"">command</a> to set an environment variable scoped to a specific application pool is</p>

<pre><code>appcmd.exe set config -section:system.applicationHost/applicationPools /+""[name='MyAppPool'].environmentVariables.[name='DD_SERVICE_NAME',value='my-service']"" /commit:apphost
</code></pre>

<p>where <code>MyAppPool</code> is the name of the application pool, and <code>my-service</code> is the service name that you want to use for the Datadog APM.</p>

<p>After running the above command, you have to restart IIS for the changes to take effect:</p>

<pre><code>net stop was /y

net start w3svc
</code></pre>
"
Datadog,55543982,55187016,3,"2019/04/06, 01:10:37",False,"2019/04/06, 01:10:37",16566,24231,2,"<p>Starting with version 1.0 of Datadog's .NET Tracer, you can set most settings in your application's <code>app.config</code>/<code>web.config</code> file. For example, to set <code>DD_SERVICE_NAME</code>:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;configuration&gt;
  &lt;appSettings&gt;
    &lt;add key=""DD_SERVICE_NAME"" value=""my-service""/&gt;
  &lt;/appSettings&gt;
&lt;/configuration&gt;
</code></pre>

<p>[Disclaimer: I am a Datadog employee]</p>
"
Datadog,52066429,52066057,0,"2018/08/29, 00:28:16",False,"2018/08/29, 00:28:16",1371,5540166,0,"<p>Maybe you could use the Datadog respective dashboard apis to update your metric names in all dashboards? Should theoretically be not too complicated to script out. </p>

<p><a href=""https://docs.datadoghq.com/api/?lang=python#screenboards"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/?lang=python#screenboards</a>
<a href=""https://docs.datadoghq.com/api/?lang=python#timeboards"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/?lang=python#timeboards</a></p>
"
Datadog,50827960,50827869,0,"2018/06/13, 04:31:37",False,"2018/06/13, 04:31:37",470,951739,0,"<p>Use the <a href=""http://docs.python-requests.org/en/master/"" rel=""nofollow noreferrer"">requests</a> library its a lot simpler</p>

<p>Generate a request header like this</p>

<pre><code>def headers(apikey):
    return {'Authorization': 'Bearer {}'.format(apikey),
            'Content-Type': 'application/json'}
</code></pre>

<p>Send the request like this</p>

<pre><code>result = get(url, headers=headers(apikey))
</code></pre>
"
Datadog,50827969,50827869,0,"2018/06/13, 04:33:31",True,"2018/06/13, 04:33:31",631,9933041,1,"<p>While searching through this <a href=""https://github.com/micrometer-metrics/micrometer/issues/415"" rel=""nofollow noreferrer"">other issue</a>, I found that all that is needed to fix this issue is to specify the API key and the application key within the URL. Consider the following.</p>

<pre><code>def getSkeleton(self):
    api_key = 'your api key';
    app_key = 'your application key';
    boards = self.getAll(); # utilizing the api.ScreenBoards.get_all() function
    boardList = boards['screenboards'];
    for x in boardList:
        url = self.target + x['resource'] + ""?api_key="" + api_key +""&amp;application_key="" + app_key;
        data = urllib.urlopen(url).read();
        print data
</code></pre>
"
Datadog,50957805,50460608,0,"2018/06/21, 01:13:11",True,"2018/06/21, 01:13:11",3956,2417043,1,"<p>Yes, kind of.</p>

<p>It's possible to show single value on a dashboard (just use ""Query Value"" visualization), but it must be based on some metric reported to Datadog.</p>

<p>This is how it looks like:
<a href=""https://i.stack.imgur.com/cpEwt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cpEwt.png"" alt=""enter image description here""></a></p>
"
Datadog,49465784,49443977,0,"2018/03/24, 16:23:41",False,"2018/03/24, 16:23:41",2077,2796894,-1,"<p>It only applies to produce requests.</p>
"
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371,5540166,1,"<p>First you may need to download the MSI file:</p>

<pre><code>$image_url = ""https://s3.amazonaws.com/ddagent-windows-stable/ddagent-cli-latest.msi""
$destin = ""C:\path\to\downloaded\ddagent-cli-latest.msi""
(New-Object System.Net.WebClient).DownloadFile($image_url, $destin)
</code></pre>

<p>The actual powershell command for installation (with extra optional arguments included as arguments):</p>

<pre><code>msiexec /i C:\path\to\downloaded\ddagent-cli-latest.msi /l*v C:\path\to\installation_log.txt /quiet APIKEY=""$DD_API_KEY"" HOSTNAME=""$HOSTNAME"" TAGS=`""$TAGS,COMMA,DELIMITED`
</code></pre>

<p>It's been a while since i've done this (8 months or so?), so it could be outdated, but it used to work :). </p>

<p>Note, if you're running this from a remote provisioning script, you'll probly have to schedule this to be executed not-remotely so that the installation command can be run with heightened permissions, which i believe is required. And you <em>may</em> need to make sure the computer is plugged into the power source (i remember hitting some infuriating issue where that was an arbitrary requirement for Windows scheduled tasks to run, and Windows didn't allow me to configure around that).</p>
"
Datadog,48325027,48314382,2,"2018/01/18, 17:39:30",False,"2018/01/18, 17:39:30",1371,5540166,0,"<p>is your <code>activemq_58.yaml</code> all in one line like that? You probably want it to be more like this:</p>

<pre><code>instances:
    - host: localhost
      port: 8161
      user: admin
      password: admin
</code></pre>
"
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035,5825844,1,"<p>There are a variety of issues here.</p>

<p><strong>1. You've misconfigured the scope formats. (metrics.scope.operator)</strong></p>

<p>For one the configuration doesn't make sense since you specify ""metrics.scope.operator"" multiple times; only the last config entry is honored.</p>

<p>Second, and more importantly, you have misunderstood for scope formats are used for.</p>

<p>Scope formats configure which context information (like the ID of the task) is included in the reported metric's name.</p>

<p>By setting it to a constant (""latency"") you've told Flink to not include anything. As a result, the numRecordsIn metrics for every operator is reported as ""latency.numRecordsIn"".</p>

<p>I suggest to just remove your scope configuration.</p>

<p><strong>2. You've misconfigured the Datadog Tags</strong></p>

<p>I do not understand what you were trying to do with your tags configuration.</p>

<p>The tags configuration option can only be used to provide <strong>global</strong> tags, i.e. tags that are attached to every single metrics, like ""Flink"".</p>

<p>By <strong>default</strong> every metric that the Datadog reports has tags attached to it for every available scope variable available.</p>

<p>So, if you have an operator name A, then the numRecordsIn metric will be reported with a tag ""operator_name:A"".</p>

<p>Again, I would suggest to just remove your configuration.</p>
"
Datadog,46846772,46846059,0,"2017/10/20, 13:03:32",False,"2017/10/20, 13:03:32",148,6815223,1,"<p>For the container agent, you'll want to run <code>sudo docker exec -it dd-agent /etc/init.d/datadog-agent status</code> from your unix based box. If, however, you are using the alpine image the command is: <code>docker exec -it dd-agent /opt/datadog-agent/bin/agent status</code> (different path). More here in this KB from Datadog: <a href=""https://help.datadoghq.com/hc/en-us/articles/203764635-Agent-Status-and-Information"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/203764635-Agent-Status-and-Information</a></p>
"
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223,43973,1,"<p>I had issues with it too. The agent is most likely sending a value for 100 incorrectly as 10000 instead, as that's the highest I've seen it go. To make it usable, ensure the graph type is 'line', click ""Advanced"" on the metric, and make the equation <code>a / 100</code>.</p>

<p>I didn't find any guidance on this either in my search, but this seems to go along with what I've seen logged locally into a Windows box and watching the performance counters locally.</p>
"
Datadog,33961640,33960552,0,"2015/11/27, 18:39:07",True,"2015/11/27, 18:39:07",5244,1991579,1,"<p>Just should set hostname for <code>org.coursera.metrics.datadog.DatadogReporter.Builder</code>:</p>

<pre><code>.withHost(InetAddress.getLocalHost().getCanonicalHostName())
</code></pre>
"
Datadog,66430636,66416385,1,"2021/03/02, 00:30:18",False,"2021/03/02, 00:30:18",156,2676108,0,"<p>Set is almost never the right custom metric type to use. It will send a count of the number of unique items per a given tag. The underlying items details will be stripped from the metric, meaning that from one time slice to the next, you will have no idea that actual true number of items over time.</p>
<p>For example</p>
<pre><code>3:00:07-3:00:32 | 5 second bucket:[device1,device4,device7] -&gt; 3 values
3:00:32-3:00:47 | 5 second bucket:[device1,device3] -&gt; 2 values
</code></pre>
<p>Your time series to datadog will report <code>3</code>, and then <code>2</code>. But because the underlying device info is stripped you have no idea how to combine that 2 and 3 if you to zoom out in time and roll up the numbers to show 1 data point per minute. It could be any number from 3 to 5, but the Datadog backend has no idea. (even though we know that across those 30 seconds there were 4 unique values total)</p>
<p>Plus even if it was accurate somehow, you can't create an alert of it or notify anyone, because you won't know which device is having issues if you see a spike of devices in the 60 second bucket.</p>
<p>So let's go through other metric options.</p>
<hr />
<p>The only metric types that are ever worth using are usually <a href=""https://docs.datadoghq.com/metrics/distributions/#overview"" rel=""nofollow noreferrer"">distributions</a> or <a href=""https://docs.datadoghq.com/developers/metrics/types/?tab=gauge#definition"" rel=""nofollow noreferrer"">gauges</a>, or [counts].</p>
<p>A gauge metric is just a measurement of the latency at a point in time, it's usually good for things like CPU or Memory of a computer, or temperature in a room. Numbers that are impossible to actually collect all dat a points for so you just take measurements every 10 seconds, or every minute, or however often you never to get an idea of the behavior.</p>
<p>A count metric is more exact, it's the number of things that happened. Usually good for number of requests to a server, or number of files processed. Even something like the amount of bytes flowing through something, although that usually is treated like a gauge by most people.</p>
<p>Distributions are good for when you want to create a gauge metric, but you need detailed measurements for every single event that happens. For example a web server is handling hundreds of requests per second and we need to know the latency metrics of that server. It's not possible to send a latency metric for every request as a gauge. Gauges have a built in limit of 1 data point per second (in Datadog). Anything more sent in a 1 second interval gets dropped. But we need stats for every request, so a distribution will summarize the data, it keep a running count, min, max, average, and optionally several percentiles (p50, p75, p99).</p>
<hr />
<p>I haven't seen many good use cases for metric types outside of those 3. For your scenario, it seems like you would want to be sending a distribution metric for that device interval. So device 1 sends a value of 10.14 and device 3 sends a value of 2.3 and so on.</p>
<p>Then you can use a <a href=""https://docs.datadoghq.com/dashboards/widgets/distribution/"" rel=""nofollow noreferrer"">distribution widget</a> in a dashboard to show the number of devices for each interval bucket.</p>
<p>Of course make sure you tag each metric by the device that is generating the metric.</p>
"
Datadog,53458998,53044670,0,"2018/11/24, 16:09:27",True,"2018/11/24, 16:09:27",347,2650254,0,"<p>I was able to do that by using this api call: <a href=""https://docs.datadoghq.com/api/?lang=python#get-a-screenboard"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/?lang=python#get-a-screenboard</a>
and then get it as a son file, which can be passed to cloud formation later.</p>
"
Datadog,50451247,50134871,0,"2018/05/21, 17:41:57",False,"2018/05/21, 17:41:57",2352,8870132,0,"<p>Actually what you need to do is create a generic method.</p>

<pre><code>Create counter is the metric provided by the dataDog. 
val metric:mutable.Map[String, Counter]
def updateCounter(metricName:String, increment:Int, tags:Map[String, String])={
If(metric.isDefinedAt(metricName)){
//update the existing counter in metric map
}else{
//create the counter and update the metric map
}
}
</code></pre>

<p>Now when you are hitting the different different end points just call the updateCounter method will will capture your metric with the specific name of your route.</p>

<p>For example you have route like add and subtract
Then call the update counter method with metric name add and subtract. </p>
"
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261,4172512,1,"<p>You can leverage Datadog's Agent to collect metrics via a JMX connection.  There is documentation found here:</p>

<p><a href=""http://docs.datadoghq.com/integrations/java/"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/integrations/java/</a></p>

<p><a href=""https://www.datadoghq.com/blog/monitoring-jmx-metrics-with-datadog/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/monitoring-jmx-metrics-with-datadog/</a></p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/204501525-Custom-JMX-Integration-s-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/204501525-Custom-JMX-Integration-s-</a></p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/207525586-View-jmx-data-in-jConsole-and-set-up-your-jmx-yaml-to-collect-them"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/207525586-View-jmx-data-in-jConsole-and-set-up-your-jmx-yaml-to-collect-them</a></p>

<p>That should help you get setup and collecting the necessary metrics exposed by your JMX port.</p>

<p>That said, if you encounter any issues reach out to support@datadoghq.com and they can assist you.</p>
"
Datadog,52609589,49671175,1,"2018/10/02, 16:40:17",False,"2018/11/16, 12:34:17",107,4772912,2,"<p>It's possible to solve this by to changing the query by using zero interpolation. You can put "".fill(zero)"" behind your query in the json or choose the option from the UI.
<a href=""https://i.stack.imgur.com/OgTNl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OgTNl.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT:</strong></p>

<p>You're right, the interpolation is not working when no data is available. I had the same problem in the end. Support of Datadog said it isn't possible to show zero when there is no data for a metric. Now there is a feature request made for it. It would be nice if more people will request for this feature, so it will be prioritized. </p>

<p>Nonetheless I have tried to create a workaround by adding a second metric that always has data as a second query and added a formula ((b - b) + a) that negates the second query, but when there is data in the intended query it's show in the graph. This will result in a zero line when there is no data available.</p>

<p><strong>Scenario without data:</strong>
<a href=""https://i.stack.imgur.com/cGykP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cGykP.png"" alt=""GraphWithNoData""></a></p>

<p>The only problem is that when you have a data in the intended query, it looks ugly and the zero line is gone. As you see in the following screenshot.</p>

<p><strong>Scenario with data:</strong>
<a href=""https://i.stack.imgur.com/8Tcux.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Tcux.png"" alt=""GraphWithData""></a></p>

<p><strong>Conclusion:</strong>
The workaround is not perfect, but it will work for some situation. For example, filling up query values with zero instead of (no data). I hope this is a bit better answer to the problem.</p>
"
Datadog,53345774,49671175,0,"2018/11/16, 23:38:15",False,"2018/11/16, 23:38:15",1371,5540166,21,"<p>how about <a href=""https://docs.datadoghq.com/graphing/functions/interpolation/#default"" rel=""noreferrer"">the ""default"" function</a>?</p>

<p>so <code>default(sum:foo.bar{hello:world} by {baz}, 0)</code> or some such?</p>
"
Datadog,58276947,49671175,0,"2019/10/07, 23:49:14",False,"2020/08/28, 05:49:42",452,1655072,6,"<p>There is now a <a href=""https://docs.datadoghq.com/dashboards/functions/interpolation/#default-zero"" rel=""nofollow noreferrer"">default_zero()</a> function that can be used in Datadog by modifying through JSON directly.</p>
"
Datadog,61641189,49671175,0,"2020/05/06, 20:15:31",False,"2020/05/06, 20:15:31",47308,3141234,7,"<p>The <code>default_zero()</code> function does what you're looking for. You can type it in manually, as <a href=""https://stackoverflow.com/a/53345774/3141234"">stephenlechner suggests</a>.</p>

<p>There's another way I found:</p>

<ol>
<li>Click ""Advanced"" in the bottom right: <a href=""https://i.stack.imgur.com/Xc7T5.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xc7T5.png"" alt=""enter image description here""></a></li>
<li>Enter <code>default(a, 0)</code> as the formula, and disable the visibility of metric <code>a</code>: <a href=""https://i.stack.imgur.com/MZeFc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MZeFc.png"" alt=""enter image description here""></a></li>
</ol>

<p>When you save the graph and reopen, you'll see that things have been reshuffled a little, and you'll see a ""default 0"" section tagged onto the end of the metric's definition.</p>

<p><a href=""https://i.stack.imgur.com/t3bFh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/t3bFh.png"" alt=""enter image description here""></a></p>
"
Datadog,66855333,55848522,0,"2021/03/29, 16:43:11",False,"2021/03/29, 16:43:11",336,7243426,0,"<p>Metrics queries now support wildcards.</p>
<p>Example 1: Getting all the requests with a status tag starting with <code>2</code>:
<code>http.server.requests.count{status:2*}</code></p>
<p>Example 1: Getting all the requests with a service tag ending with <code>mongo</code>:
<code>http.server.requests.count{service:*mongo}</code></p>
<p>Example 3 (advanced): Getting all the requests with a service tag starting with <code>blob</code> and ending with <code>postgres</code>:
<code>http.server.requests.count{service:blob*,service:*postgres}</code>
<em>(this will match <code>service:blob-foo-postgres</code> and <code>service:blob_bar_postgres</code> but not <code>service:my_name_postgres</code>)</em></p>
"
Datadog,34400454,34398692,2,"2015/12/21, 18:41:39",True,"2015/12/21, 18:41:39",7984,750117,11,"<p>I've finally found a dropwizzard module that integrates this library with datadog: <a href=""https://github.com/coursera/metrics-datadog"" rel=""noreferrer"">metrics-datadog</a></p>

<p>I've created a Spring configuration class that creates and initializes this Reporter using properties of my YAML.</p>

<p>Just insert this dependency in your pom:</p>

<pre><code>    &lt;!-- Send metrics to Datadog --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.coursera&lt;/groupId&gt;
        &lt;artifactId&gt;dropwizard-metrics-datadog&lt;/artifactId&gt;
        &lt;version&gt;1.1.3&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Add this configuration to your YAML:</p>

<pre><code>yourapp:
  metrics:
    apiKey: &lt;your API key&gt;
    host: &lt;your host&gt;
    period: 10
    enabled: true
</code></pre>

<p>and add this configuration class to your project:</p>

<pre><code>/**
 * This bean will create and configure a DatadogReporter that will be in charge of sending
 * all the metrics collected by Spring Boot actuator system to Datadog.
 *     
 * @see https://www.datadoghq.com/
 * @author jfcorugedo
 *
 */
@Configuration
@ConfigurationProperties(""yourapp.metrics"")
public class DatadogReporterConfig {

  private static final Logger LOGGER = LoggerFactory.getLogger(DatadogReporterConfig.class);

  /** Datadog API key used to authenticate every request to Datadog API */
  private String apiKey;

  /** Logical name associated to all the events send by this application */
  private String host;

  /** Time, in seconds, between every call to Datadog API. The lower this value the more information will be send to Datadog */
  private long period;

  /** This flag enables or disables the datadog reporter */
  private boolean enabled = false;

  @Bean
  @Autowired
  public DatadogReporter datadogReporter(MetricRegistry registry) {

      DatadogReporter reporter = null;
      if(enabled) {
          reporter = enableDatadogMetrics(registry);
      } else {
          if(LOGGER.isWarnEnabled()) {
              LOGGER.info(""Datadog reporter is disabled. To turn on this feature just set 'rJavaServer.metrics.enabled:true' in your config file (property or YAML)"");
          }
      }

      return reporter;
  }

  private DatadogReporter enableDatadogMetrics(MetricRegistry registry) {

      if(LOGGER.isInfoEnabled()) {
          LOGGER.info(""Initializing Datadog reporter using [ host: {}, period(seconds):{}, api-key:{} ]"", getHost(), getPeriod(), getApiKey());
      }

      EnumSet&lt;Expansion&gt; expansions = DatadogReporter.Expansion.ALL;
      HttpTransport httpTransport = new HttpTransport
                                .Builder()
                                .withApiKey(getApiKey())
                                .build();

      DatadogReporter reporter = DatadogReporter.forRegistry(registry)
        .withHost(getHost())
        .withTransport(httpTransport)
        .withExpansions(expansions)
        .build();

      reporter.start(getPeriod(), TimeUnit.SECONDS);

      if(LOGGER.isInfoEnabled()) {
          LOGGER.info(""Datadog reporter successfully initialized"");
      }

      return reporter;
  }

  /**
   * @return Datadog API key used to authenticate every request to Datadog API
   */
  public String getApiKey() {
      return apiKey;
  }

  /**
   * @param apiKey Datadog API key used to authenticate every request to Datadog API
   */
  public void setApiKey(String apiKey) {
      this.apiKey = apiKey;
  }

  /**
   * @return Logical name associated to all the events send by this application
   */
  public String getHost() {
      return host;
  }

  /**
   * @param host Logical name associated to all the events send by this application
   */
  public void setHost(String host) {
      this.host = host;
  }

  /**
   * @return Time, in seconds, between every call to Datadog API. The lower this value the more information will be send to Datadog
   */
  public long getPeriod() {
      return period;
  }

  /**
   * @param period Time, in seconds, between every call to Datadog API. The lower this value the more information will be send to Datadog
   */
  public void setPeriod(long period) {
      this.period = period;
  }

  /**
   * @return true if DatadogReporter is enabled in this application
   */
  public boolean isEnabled() {
      return enabled;
  }

  /**
   * This flag enables or disables the datadog reporter.
   * This flag is only read during initialization, subsequent changes on this value will no take effect 
   * @param enabled
   */
  public void setEnabled(boolean enabled) {
      this.enabled = enabled;
  }
}
</code></pre>
"
Datadog,34400755,34398692,3,"2015/12/21, 18:57:48",False,"2015/12/21, 18:57:48",2073,3239981,2,"<p>If JMX is an option for you, you may use the <a href=""https://dropwizard.github.io/metrics/3.1.0/getting-started/#reporting-via-jmx"" rel=""nofollow"">JMX dropwizrd reporter</a> combined with <a href=""http://docs.datadoghq.com/integrations/java/"" rel=""nofollow"">java datalog integration</a></p>
"
Datadog,49444331,34398692,3,"2018/03/23, 09:18:59",False,"2018/03/23, 12:50:05",3355,607038,6,"<p>It seems that Spring Boot 2.x added several monitoring system into its metrics. DataDog is one of them supported by <a href=""http://micrometer.io/"" rel=""noreferrer"">micrometer.io</a>. See reference documentation: <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-newrelic"" rel=""noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-newrelic</a></p>

<p>For Spring Boot 1.x, you can use back-ported package:</p>

<p><code>compile 'io.micrometer:micrometer-spring-legacy:latest.release'</code></p>
"
Datadog,35113402,35111147,2,"2016/01/31, 13:39:25",False,"2016/01/31, 13:39:25",4968,19892,3,"<p>If <code>dd-agent</code> listens on <code>localhost</code> it can receive data only from localhost (127.0.0.1). Try to change the <code>dd-agent</code> host to <code>0.0.0.0</code> instead of <code>localhost</code>.</p>

<p>We are using <a href=""https://github.com/DataDog/docker-dd-agent"" rel=""nofollow"">docker-dd-agent</a> and it works OOTB.</p>
"
Datadog,35133077,35111147,3,"2016/02/01, 16:31:05",False,"2016/02/01, 16:31:05",111,5868112,11,"<p>You will need to set <code>non_local_traffic: yes</code> in your <code>/etc/dd-agent/datadog.conf</code> file. Otherwise the agent will reject metrics from containers.</p>

<p>After setting, you will need to restart the agent for the change to take effect: <code>sudo /etc/init.d/datadog-agent restart</code> or <code>sudo service datadog-agent restart</code></p>

<p>The <a href=""https://github.com/DataDog/docker-dd-agent"" rel=""noreferrer"">docker-dd-agent</a> image enables <code>non_local_traffic: yes</code> by default.</p>
"
Datadog,39639402,35111147,0,"2016/09/22, 15:43:05",False,"2016/09/22, 15:43:05",296,166816,0,"<p>You don't actually want to use the IP of the host in this case. If you're running the docker dd-agent, there are two environment variables you can tap into:</p>

<p><code>statsd.connect(DOGSTATSD_PORT_8125_UDP_ADDR, DOGSTATSD_PORT_8125_UDP_PORT)</code></p>

<p>That should do the trick. If not, you should be able to find the relevant info to your problem in <a href=""https://github.com/DataDog/docker-dd-agent#dogstatsd-from-the-host"" rel=""nofollow"">this section of the Datadog docs</a>.</p>

<p>Also, I should point out that the only Python library that Datadog shows in their docs is <a href=""https://github.com/DataDog/datadogpy"" rel=""nofollow"">datadogpy</a>.</p>
"
Datadog,36459921,36459827,3,"2016/04/06, 21:52:23",False,"2016/04/06, 21:52:23",12279,596041,2,"<p>What you have is a nil pointer dereference. (Unless you are using package <code>unsafe</code>, which you probably shouldn't touch, so I'm assuming you're not.)</p>

<p>It looks like the <code>e</code> argument to <code>func (c *Client) Event(e *Event) error</code> is <code>nil</code> when called from <code>github.com/some/path/server/http.go:86</code>.</p>
"
Datadog,36461477,36459827,1,"2016/04/06, 23:17:02",True,"2016/04/06, 23:17:02",1631,97094,19,"<p>Thanks to a comment from @twotwotwo, I think I figured this out.</p>

<p>In this line</p>

<pre><code>github.com/DataDog/datadog-go/statsd.(*Client).Event(0x0, 0xc8200c7ec8, 0x0, 0x0)
</code></pre>

<ul>
<li>the first <code>0x0</code> is the <code>*Client</code>, which is indeed nil.</li>
<li><code>0xc8200c7ec8</code> is <code>*Event</code></li>
<li>the following <code>0x0, 0x0</code> represent the return value of type <code>error</code>. <code>error</code>, according to <a href=""http://blog.golang.org/error-handling-and-go"" rel=""noreferrer"">http://blog.golang.org/error-handling-and-go</a>, is an interface. According to <a href=""http://research.swtch.com/interfaces"" rel=""noreferrer"">http://research.swtch.com/interfaces</a>, interfaces are stored as two pointers. The first pointer points to the type information stored in the interface, and the second pointer points to the data stored in the interface.</li>
</ul>

<p>I wrote the following program to demonstrate to myself how different function signatures appear in a stack trace:</p>

<pre><code>package main

import ""errors""

type X struct {
        i int
}

type Y struct {
}

func (y *Y) foo(x *X) {
        panic(""panic in foo"")
}

func (y *Y) bar(x *X) (*Y) {
        panic(""panic in bar"")
        return y
}

func (y *Y) baz(x *X) (error) {
        panic(""panic in baz"")
        return errors.New(""error in baz"")
}

func (y *Y) bam() {
        panic(""panic in bam"")
}

func main() {
        y := new(Y)
        x := new(X)
        // comment out the ones you don't want to check
        y.foo(x)
        y.bar(x)
        y.baz(x)
        y.bam()
}
</code></pre>

<p>When <code>bam</code> is called, which acts on <code>*Y</code> but has no arguments or return value, the output contains: </p>

<pre><code>main.(*Y).bam(0xc82002df48)
</code></pre>

<p>When <code>foo</code> is called, which acts on <code>*Y</code> and takes a <code>*X</code> as argument, but has no return value, the output contains:</p>

<pre><code>main.(*Y).foo(0xc820033f30, 0xc820033f30)
</code></pre>

<p>When <code>bar</code> is called, which acts on <code>*Y</code>, takes a <code>*X</code> as argument, and returns a <code>*Y</code>, the output contains:</p>

<pre><code>main.(*Y).bar(0xc820033f30, 0xc820033f30, 0x40fb46)
</code></pre>

<p>When <code>baz</code> is called, which acts on <code>*Y</code>, takes <code>*X</code> as argument, and returns an <code>error</code> (which is an interface), the output contains:</p>

<pre><code>main.(*Y).baz(0xc820033f38, 0xc820033f38, 0x0, 0x0)
</code></pre>
"
Datadog,42023919,40741803,0,"2017/02/03, 14:05:43",True,"2017/02/03, 14:05:43",1199,2525626,4,"<p>After speaking to the support team at DataDog, I managed to find out the following information relating to what the no_pod pods were.</p>

<blockquote>
  <p>Our Kubernetes check is getting the list of containers from the Kubernetes API, which exposes aggregated data. In the metric explorer configuration here, you can see a couple of containers named /docker and / that are getting picked up along with the other containers. Metrics with pod_name:no_pod that come from container_name:/ and container_name:/docker  are just metrics aggregated across multiple containers. (So it makes sense that these are the highest values in your graphs.) If you don't want your graphs to show these aggregated container metrics though, you can clone the dashboard and then exclude these pods from the query. To do so, on the cloned dashboard, just edit the query in the JSON tab, and in the tag scope, add !pod_name:no_pod.</p>
</blockquote>

<p>So it appears that these pods are the docker and root level containers running outside of the cluster and will always display unless you want to filter them out specifically which I now do.</p>

<p>Many thanks to the support guys at DataDog for looking into the issue for me and giving me a great explanation as to what the pods were and essentially confirming that I can just safely filter these out and not worry about them.</p>
"
Datadog,64519899,35608127,0,"2020/10/25, 04:57:48",False,"2020/10/25, 04:57:48",493,244037,0,"<p>There doesn't appear to be a Crashalytics direct integration yet.</p>
<p>Datadog maintains mobile SDKs that can be included in mobile clients to produce metrics and logs to the platform.</p>
<p>For iOS Logs, see here: <a href=""https://docs.datadoghq.com/logs/log_collection/ios/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/ios/</a></p>
<p>For Android Logs: <a href=""https://docs.datadoghq.com/logs/log_collection/android/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/log_collection/android/</a></p>
<p>There's also a public Android SDK for Real User Monitoring, which can be read here: <a href=""https://docs.datadoghq.com/real_user_monitoring/android/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/real_user_monitoring/android/</a></p>
<p>And the announcement, with a link for the private beta for iOS signup here: <a href=""https://www.datadoghq.com/blog/datadog-mobile-rum/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/datadog-mobile-rum/</a></p>
"
Datadog,42935965,42927552,0,"2017/03/21, 20:59:47",False,"2017/03/21, 20:59:47",2676,1230594,0,"<p>The free text editor you have in the screenshot is for metric queries. Events in graphs are added as overlay to show when events happened over time.</p>

<p>There is no widget, as of now, that shows a single value for the number of times an event occurred. But you can use the event timeline widget, which will show a timeline of events grouped by status and bucketed over a defined period of time. See below:</p>

<p><a href=""https://i.stack.imgur.com/6nyJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6nyJS.png"" alt=""enter image description here""></a></p>
"
Datadog,42999464,42927552,0,"2017/03/24, 14:26:09",False,"2017/03/24, 14:26:09",1929,2599875,3,"<p>Well I realized that the <strong>query value</strong> only works with metrics, so to create a counter we can emit metrics with <code>value: 1</code> and then count them with the <code>rollup(sum, 60)</code> function.</p>

<p><code>dog.emit_point('some.event.name', 1)
</code></p>

<p><code>sum:some.event.name{*}.rollup(sum, 60)</code></p>

<p>The main thing to understand here is that DataDog does not retrieve all the points for a given timeframe. Actually as <a href=""https://help.datadoghq.com/hc/en-us/articles/204526615-What-is-the-rollup-function"" rel=""nofollow noreferrer"">McCloud</a> says, <em>for a given time range we do not return more than 350 points</em>, which is very important to have in mind when you create a counter.</p>

<p>When you query value from a metric in a timeframe, DataDog return a group of points that represents the real stored points, not all the points; the level of how those points are represented (as I understand) is called granurallity, and what you do with this <code>rollup</code> function is to define how those points are going to represent the real points, which in this case is going to be using the <code>sum</code> function. </p>

<p>I hope this helps somebody, I'm still learning about it. Regards</p>
"
Datadog,65565807,42927552,0,"2021/01/04, 18:05:02",False,"2021/01/04, 18:05:02",66,2625807,0,"<p>In Ruby on the client, I use:</p>
<pre class=""lang-rb prettyprint-override""><code>Datadog::Statsd.new(hostname, port)
datadog.increment(metric_name, tags: tag_list)
</code></pre>
<p>I then have a dashboard with a
<a href=""https://docs.datadoghq.com/dashboards/widgets/query_value/"" rel=""nofollow noreferrer"">Query Value</a>
widget, set to &quot;take the [Sum] value from the displayed timeframe&quot;.</p>
<pre class=""lang-sh prettyprint-override""><code>sum:some.event.name{*}.as_count()
</code></pre>
<p>I tested this and it seems to give the right numbers.  The
<a href=""https://docs.datadoghq.com/monitors/guide/as-count-in-monitor-evaluations/"" rel=""nofollow noreferrer"">.as_count()</a>
seems key.</p>
"
Datadog,58493964,45506314,0,"2019/10/21, 23:51:33",True,"2019/10/21, 23:51:33",20407,1017941,1,"<p>Answering just in case someone will spot this question via Google.</p>

<p>You cannot. StatsD protocol do not define tags or comments at all, so there is no possibility for that. You need to use different library like <a href=""https://github.com/lexmag/statix"" rel=""nofollow noreferrer"">Statix</a> for that.</p>
"
Datadog,45359315,45358188,3,"2017/07/27, 22:20:09",False,"2018/10/29, 09:50:33",6670,3625317,7,"<p>Think differently :)</p>

<p>Do bind a nginx-server (vhost) on port 10080 <strong>in addition</strong> - that server does offer the status location and what you need. </p>

<p>Server on 80/443 is also there and ONLY that one is bound/exposed to host ( exposed to the outer world ).</p>

<p>Since datadog is part of your docker-network / service network, it can still access 10080 in the internal network, but nobody else from the outer network.</p>

<p>Bulletproof, easy - no strings attached.</p>
"
Datadog,45359614,45358188,0,"2017/07/27, 22:38:08",True,"2017/07/27, 22:38:08",123680,2830850,4,"<p>Since we are running the service through <code>docker-compose</code> and our issue being we don't know the IP of the agent. So the simple solution is to know the IP before starting. And that means assigning our agent a specific IP </p>

<p>Here is a update <code>docker-compose</code> to do that</p>

<pre><code>version: '2'
services:
  flask:
    restart: always
    image: me/flask-app
    command: /home/app/flask/start_app.sh
    expose:
      - ""8080""

  nginx:
    restart: always
    build: ./nginx
    command: /runtime/start_nginx.sh
    ports:
      - ""80:80""
      - ""443:443""
    expose:
      - ""81""
    volumes:
      - app-static:/app-static:ro
    links:
      - flask:flask
    networks:
      agent:
        ipv4_address: 172.25.0.101
      default:

  datadog-agent:
    image: me/datadog-agent
    env_file: ./datadog-agent/dev.env
    links:
        - flask
        - nginx
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /proc/mounts:/host/proc/mounts:ro
      - /sys/fs/cgroup:/host/sys/fs/cgroup:ro
    networks:
      agent:
        ipv4_address: 172.25.0.100
networks:
  agent:
    driver: bridge
    ipam:
      config:
      - subnet: 172.25.0.0/24
</code></pre>

<p>Now you can do two possible things</p>

<pre><code>server {
  listen 172.25.0.101:81;

  location /nginx_status {
    stub_status on;
    access_log off;
    allow 127.0.0.1;
    allow 172.25.0.100;
    deny all;
  }
}
</code></pre>

<p>You can listen only on <code>172.25.0.101</code> which is accessible only container running on agent network. Also you can add <code>allow 172.25.0.100</code> to only allow the agent container to be able to access this.</p>
"
Datadog,61277277,45358188,1,"2020/04/17, 20:27:04",False,"2020/04/17, 20:27:04",515,2075004,3,"<p>There are two (easier) ways to go about it.</p>

<p>First one is <code>docker-compose</code> but since I already have a setup running since 2 years which doesn't use docker-compose, I went for the 2nd way.</p>

<p>Second way is <code>Allow</code> Directive with a range of IPs.</p>

<p>Eg:</p>

<pre><code>    location /stub_status {
        stub_status;

        allow 172.18.0.0/16;   # This is my local docker IP range
        allow 192.168.0.0/16;  $ This is my production server IP range
        deny all;              # deny all other hosts   
 }
</code></pre>

<p>I am not security expert, but mostly <code>192.168.*</code> IP range is for local networks, not sure about <code>172.18.*</code> range though.</p>

<p>To get more idea about this IP range thing and CIDR stuff, refer below links
<a href=""http://nginx.org/en/docs/http/ngx_http_access_module.html"" rel=""nofollow noreferrer"">http://nginx.org/en/docs/http/ngx_http_access_module.html</a></p>

<p><a href=""https://www.ripe.net/about-us/press-centre/understanding-ip-addressing"" rel=""nofollow noreferrer"">https://www.ripe.net/about-us/press-centre/understanding-ip-addressing</a></p>
"
Datadog,34031282,33929394,0,"2015/12/02, 00:18:15",False,"2015/12/02, 00:18:15",117,5379516,0,"<p>As the err info said dh key is too small, a larger one might help.  Replace the default dh512.pem file with dh4096.pem</p>

<p><code>sudo wget ""https://git.openssl.org/gitweb/?p=openssl.git;a=blob_plain;f=apps/dh4096.pem"" -O dh4096.pem
</code></p>

<p>Ref: <a href=""http://www.alexrhino.net/jekyll/update/2015/07/14/dh-params-test-fail.html"" rel=""nofollow"">http://www.alexrhino.net/jekyll/update/2015/07/14/dh-params-test-fail.html</a></p>
"
Datadog,60844436,60842629,1,"2020/03/25, 09:24:11",False,"2020/03/25, 09:59:29",71285,3558960,3,"<p>This is actually a lot harder than it seems.</p>

<p>Representing big integers in JavaScript can be done using the <a href=""https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt"" rel=""nofollow noreferrer""><code>BigInt</code></a> data type (by suffixing the number with <code>n</code>), which is fairly widely supported at this point.</p>

<p>This would make your object look like this:</p>

<pre><code>const o = {
  span_id: 16956440953342013954n,
  trace_id: 13756071592735822010n
};
</code></pre>

<p>The problem presents itself in the JSON serialization, as there is currently no support for the serialization of <code>BigInt</code> objects. And when it comes to JSON serialization, your options for customization are very limited:</p>

<ul>
<li>The <code>replacer</code> function that can be used with <a href=""https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify"" rel=""nofollow noreferrer""><code>JSON.stringify()</code></a> will let you customize the serialization behavior for <code>BigInt</code> objects, but will not allow you to serialize them as a raw (unquoted) string.</li>
<li>For the same reason, implementing the <code>toJSON()</code> method on the <code>BigInt</code> prototype will also not work.</li>
<li>Due to the fact that <code>JSON.stringify()</code> does not seem to recursively call itself internally, solutions that involve wrapping it in a <a href=""https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Proxy"" rel=""nofollow noreferrer"">proxy</a> also will not work.</li>
</ul>

<p>So the only option that I can find is to (at least partially) implement your own JSON serialization mechanism.</p>

<p>This is a <em>very</em> poor man's implementation that calls <a href=""https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt/toString"" rel=""nofollow noreferrer""><code>toString()</code></a> for object properties that are of type <code>BigInt</code>, and delegates to <code>JSON.stringify()</code> otherwise: </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const o = {
  ""span_id"": 16956440953342013954n,
  ""trace_id"": 13756071592735822010n
};

const stringify = (o) =&gt; '{'
  + Object.entries(o).reduce((a, [k, v]) =&gt; ([
      ...a, 
      `""${k}"": ${typeof v === 'bigint' ? v.toString() : JSON.stringify(v)}`
    ])).join(', ')
  + '}';

console.log(stringify(o));</code></pre>
</div>
</div>
</p>

<p>Note that the above will not work correctly in a number of cases, most prominently nested objects and arrays. If I were to do this for real-world usage, I would probably base myself on <a href=""https://github.com/douglascrockford/JSON-js/blob/master/json2.js"" rel=""nofollow noreferrer"">Douglas Crockford's JSON implementation</a>. It should be sufficient to add an additional case around <a href=""https://github.com/douglascrockford/JSON-js/blob/594c8fa5f8e3fb38b0977f1ff8a87e9d709e7db1/json2.js#L276"" rel=""nofollow noreferrer"">this line</a>:</p>

<pre><code>case ""bigint"":
  return value.toString();
</code></pre>
"
Datadog,59354863,58897099,0,"2019/12/16, 12:36:35",False,"2019/12/16, 12:36:35",394,1602433,-1,"<p>You could try to use <a href=""https://clinicjs.org/doctor/"" rel=""nofollow noreferrer"">clinic</a> in order to debug and profile the app. pretty good tool for nodeJS.</p>
"
Datadog,59406939,58897099,1,"2019/12/19, 11:41:00",False,"2019/12/19, 11:41:00",388,4546641,-1,"<p>You could user <a href=""https://www.npmjs.com/package/memwatch-next"" rel=""nofollow noreferrer"">node-memwatch</a> to detect where is memory leak.</p>

<p>It also might be a known issue, here is the <a href=""https://github.com/apollographql/apollo-client/issues/3982"" rel=""nofollow noreferrer"">link</a> with a similar issue.</p>
"
Datadog,59433027,58897099,0,"2019/12/21, 04:16:20",False,"2019/12/21, 04:16:20",3025,3154872,0,"<p>You are on the right path. The guide I'm about to link to begins by following a similar approach to the one you've taken. I'll link to the section that talks about monitoring memory in real time, which is available when you <code>Record allocation timeline</code> in chrome://inspect </p>

<p><a href=""https://marmelab.com/blog/2018/04/03/how-to-track-and-fix-memory-leak-with-nodejs.html#watching-memory-allocation-in-real-time"" rel=""nofollow noreferrer"">https://marmelab.com/blog/2018/04/03/how-to-track-and-fix-memory-leak-with-nodejs.html#watching-memory-allocation-in-real-time</a></p>
"
Datadog,65950961,47299307,0,"2021/01/29, 10:19:44",False,"2021/01/29, 10:19:44",21,6351378,2,"<p>This question is quite old, however, still might be useful for new users of Google Cloud.</p>
<p>In 'Metrics Explorer' in Google Cloud Console there is an option to write a query with MQL (click <code>Query Editor</code> button).
MQL supports expressions which are described in detail <a href=""https://cloud.google.com/monitoring/mql/reference#expression"" rel=""nofollow noreferrer"">here</a>.</p>
<p>The simplest example for dividing one metric by another would look like this:</p>
<pre><code>{ fetch
    your_resource_type ::
    your_metric_1
; fetch
    your_resource_type ::
    your_metric_2 
}
| join
| div
</code></pre>
"
Datadog,55916769,45954769,0,"2019/04/30, 11:27:17",False,"2019/04/30, 11:27:17",11,11431260,1,"<p>In your question, I looked for your purpose in terms of using the port 8125 or 8126 ports. 8125 port is used for stasd metrics, and 8126 is used for APM (trace) data.</p>

<p>So if you want to use 8125 the important thing to do is having <code>non_local_traffic : yes</code>. So there must be another problem which I don't know yet.</p>

<p>But if your purpose is using APM/trace port: 8126 is only bound to localhost by default. You should make it listen to any network interface by the <code>bind_host: 0.0.0.0</code> configuration. Currently, it will refuse the requests from your containers since they are not coming from localhost.</p>

<p>I had a similar problem and this page helped me: <a href=""https://github.com/DataDog/ansible-datadog/issues/149"" rel=""nofollow noreferrer"">https://github.com/DataDog/ansible-datadog/issues/149</a></p>
"
Datadog,44207761,42588899,1,"2017/05/26, 21:08:43",False,"2017/05/26, 21:08:43",1,1976300,-1,"<p>Yes it is possible to emit metrics to DataDog from a AWS Lambda function.</p>

<p>If you were using node.js you could use <a href=""https://www.npmjs.com/package/datadog-metrics"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/datadog-metrics</a> to emit metrics to the API.  It supports counters, gauges and histograms.  You just need to pass in your app/api key as environment variables.</p>

<p>Matt</p>
"
Datadog,54256464,42588899,0,"2019/01/18, 16:57:13",False,"2019/01/18, 16:57:13",809,1647226,0,"<p>The easier way is using this library: <a href=""https://github.com/marceloboeira/aws-lambda-datadog"" rel=""nofollow noreferrer"">https://github.com/marceloboeira/aws-lambda-datadog</a></p>

<p>It has runtime no dependencies, doesn't require authentication and reports everything to cloud-watch too. You can read more about it here: <a href=""https://www.datadoghq.com/blog/how-to-monitor-lambda-functions/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/how-to-monitor-lambda-functions/</a></p>
"
Datadog,53449880,53435248,2,"2018/11/23, 18:09:47",True,"2019/07/29, 16:36:02",1371,5540166,5,"<p><strong>UPDATED ANSWER:</strong></p>

<p>Still yes. </p>

<p>Docs for new Dashboard endpoint <a href=""https://docs.datadoghq.com/api/?lang=bash#dashboards"" rel=""nofollow noreferrer"">here</a>.</p>

<hr>

<p><strong>ORIGINAL ANSWER:</strong></p>

<p>Yes.</p>

<p>Docs for screenboards <a href=""https://docs.datadoghq.com/api/?lang=python#screenboards"" rel=""nofollow noreferrer"">here</a>. </p>

<p>Docs for timeboards <a href=""https://docs.datadoghq.com/api/?lang=python#timeboards"" rel=""nofollow noreferrer"">here</a>. </p>
"
Datadog,51941286,42722353,0,"2018/08/21, 06:48:18",False,"2018/08/21, 06:57:14",1474,10109833,0,"<p>In most cases, the datadog agent retrieves metrics from an integration by connecting to a URL endpoint. This would be the case for services such as nginx, mysql etc. </p>

<p>This means that you can run just one datadog agent on the host, and configure it to listen to URL endpoints of services exposed from each container.</p>

<p>For example, assuming a mysql docker container is run with the following command:</p>

<pre><code>docker run -d \
  --name mysql \
  -p 3306:3306 \
  -e MYSQL_ROOT_PASSWORD=secret \
  -e MYSQL_DATABASE=mySchema \
  mysql
</code></pre>

<p>You can instruct the agent running on the host to connect to the container IP in the <code>mysql.yaml</code> agent configuration:</p>

<pre><code>init_config:

instances:
- server: &lt;container IP&gt;
    user: datadog
    pass: secret

    tags:
        - optional_tag1
        - optional_tag2
    options:
</code></pre>

<p>Varnish is slightly different as the agent retrieves metrics using the <code>varnishstat</code> binary. According to the example template:</p>

<blockquote>
  <p>In order to support monitoring a Varnish instance which is running as a Docker container we need to wrap commands (varnishstat) with scripts which perform a docker exec on the running container.</p>
</blockquote>

<p>To do this, on the host, create a wrapper script for the container:</p>

<pre><code>echo ""/usr/bin/docker exec varnish_container_name varnishstat ""$@"""" &gt; /home/myuser/docker_varnish
</code></pre>

<p>Then specify the script location in the <code>varnish.yaml</code> agent configuration:</p>

<pre><code>init_config:

instances:
    - varnishstat: /home/myuser/docker_varnish
</code></pre>
"
Datadog,54194264,53880368,2,"2019/01/15, 09:19:00",True,"2019/01/15, 09:19:00",747,2461574,1,"<p>I doubt the Datadog agent will ever work on App Services web app as you do not have access to the running host, it was designed for VMs.
Have you tried this <a href=""https://www.datadoghq.com/blog/azure-monitoring-enhancements/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/azure-monitoring-enhancements/</a> ? They say they support AppServices</p>
"
Datadog,55897902,53880368,2,"2019/04/29, 09:16:35",False,"2019/04/29, 09:16:35",27,5404159,0,"<p>To respond to your comment on wanting custom metrics, this is still possible without the agent at the same location. After installing the nuget package of datadog called statsdclient you can then configure it to send the custom metrics to an agent located elsewhere. Example below: </p>

<pre><code>using StatsdClient;

var dogstatsdConfig = new StatsdConfig
{
    StatsdServerName = ""127.0.0.1"", // Optional if DD_AGENT_HOST environment variable set
    StatsdPort = 8125, // Optional; If not present takes the DD_DOGSTATSD_PORT environment variable value, else default is 8125
    Prefix = ""myTestApp"", // Optional; by default no prefix will be prepended
    ConstantTags = new string[1] { ""myTag:myTestAppje"" } // Optional
};

StatsdClient.DogStatsd.Configure(dogstatsdConfig);
StatsdClient.DogStatsd.Increment(""fakeVisitorCountByTwo"", 2); //Custom metric itself
</code></pre>
"
Datadog,56799311,53880368,0,"2019/06/28, 01:59:38",False,"2019/06/28, 01:59:38",55,11661354,0,"<p>I have written a app service extension for sending Datadog APM metrics with .NET core and provided instructions for how to set it up here: <a href=""https://github.com/payscale/datadog-app-service-extension"" rel=""nofollow noreferrer"">https://github.com/payscale/datadog-app-service-extension</a></p>

<p>Let me know if you have any questions or if this doesn't apply to your situation.</p>
"
Datadog,56808294,53880368,0,"2019/06/28, 16:56:58",False,"2019/06/28, 16:56:58",11,11713986,1,"<p>Logs from App Services can also be sent to Blob storage and forwarded from there via an Azure Function. Unlike traces and custom metrics from App Services, this does not require a VM running the agent. Docs and code for the Function are available here:</p>

<p><a href=""https://github.com/DataDog/datadog-serverless-functions/tree/master/azure/blobs_logs_monitoring"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-serverless-functions/tree/master/azure/blobs_logs_monitoring</a></p>
"
Datadog,58550213,53880368,0,"2019/10/25, 02:03:41",False,"2019/10/25, 02:03:41",1630,1246590,1,"<p>If you want to use DataDog for logging from Azure Function of App Service you can use Serilog and DataDog Sink to the log files:</p>

<pre><code>        services
            .AddLogging(loggingBuilder =&gt;
                loggingBuilder.AddSerilog(
                    new LoggerConfiguration()
                        .WriteTo.DatadogLogs(
                            apiKey: ""REPLACE - DataDog API Key"",
                            host: Environment.MachineName,
                            source: ""REPLACE - Log-Source"",
                            service: GetServiceName(),
                            configuration: new DatadogConfiguration(),
                            logLevel: LogEventLevel.Infomation
                        )
                        .CreateLogger())
            );
</code></pre>

<p><a href=""https://druss.co/2019/10/logging-datadog-azure-function-serilog/"" rel=""nofollow noreferrer"">Full source code and required NuGet packages</a> are here: </p>
"
Datadog,61118441,59500017,0,"2020/04/09, 12:53:48",False,"2020/04/09, 12:53:48",385,1068531,0,"<p>You can deploy the Datadog agent in a container / instance that you manage and the configure it according to <a href=""https://docs.datadoghq.com/integrations/elastic/"" rel=""nofollow noreferrer"">these instructions</a> to gather metrics from the remote ElasticSearch cluster that is hosted on Elastic Cloud. You need to create a <code>conf.yaml</code> file in the <code>elastic.d/</code> directory and provide the required information (Elasticsearch endpoint/URL, username, password, port, etc) for the agent to be able to connect to the cluster. You may find a sample configuration file <a href=""https://github.com/DataDog/integrations-core/blob/master/elastic/datadog_checks/elastic/data/conf.yaml.example"" rel=""nofollow noreferrer"">here</a>.</p>
"
Datadog,61276030,59500017,0,"2020/04/17, 19:13:14",True,"2020/04/17, 19:13:14",2287,466390,0,"<p>As George Tseres mentioned above, the way I had to get this working was to set up collection on a separate instance (through docker) and then to configure it to read the specific Elastic Cloud instances.</p>

<p>I ended up making this: <a href=""https://github.com/crwang/datadog-elasticsearch"" rel=""nofollow noreferrer"">https://github.com/crwang/datadog-elasticsearch</a>, building that docker image, and then pushing it up to AWS ECR.</p>

<p>Then, I spun up a Fargate service / task to run the container.</p>

<p>I also set it to run locally with <code>docker-compose</code> as a test.</p>
"
Datadog,47167529,45188547,0,"2017/11/07, 23:12:04",False,"2017/11/07, 23:12:04",6992,902415,2,"<p>There are 2 relevant options in <code>/etc/dd-agent/conf.d/docker_daemon.yaml</code>:</p>

<ul>
<li><p><strong>collect_disk_stats</strong><br>
If you use devicemapper-backed storage (which is default in ECS but not in vanilla Docker or Kubernetes), docker.data.* and docker.metadata.* statistics should do what you are looking for.</p></li>
<li><p><strong>collect_container_size</strong><br>
A generic way, using the docker API but virtually running df in every container. This enables the docker.container.* metrics.</p></li>
</ul>

<p>See more here:
<a href=""https://help.datadoghq.com/hc/en-us/articles/115001786703-How-to-report-host-disk-metrics-when-dd-agent-runs-in-a-docker-container-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/115001786703-How-to-report-host-disk-metrics-when-dd-agent-runs-in-a-docker-container-</a></p>

<p>and here:
<a href=""https://github.com/DataDog/docker-dd-agent/blob/master/conf.d/docker_daemon.yaml#L46"" rel=""nofollow noreferrer"">https://github.com/DataDog/docker-dd-agent/blob/master/conf.d/docker_daemon.yaml#L46</a></p>
"
Datadog,63340327,63049334,1,"2020/08/10, 15:35:10",True,"2020/08/10, 15:35:10",632,5197466,2,"<p>It is probably caused by a regression between .NET Core 2.2 and .NET Core 3.0
Apparently it will be fixed in version 3.1.7</p>
<p>Just starting the process causes the memory leak on linux, because of a non released handle</p>
<p>Issue has been tracked here <a href=""https://github.com/dotnet/runtime/issues/36661"" rel=""nofollow noreferrer"">https://github.com/dotnet/runtime/issues/36661</a></p>
"
Datadog,60253636,60237664,2,"2020/02/17, 00:04:53",True,"2020/02/17, 23:43:26",37971,1237575,2,"<p>The problem is that the advice class will be loaded on the system class loader as a part of the agent whereas the actual application code is loaded on a sub-class loader that is not visible to the system class loader. This situation does not change if you load your agent on the boot loader either. Therefore, the agent cannot load the <code>HttpServletRequest</code> class which is part of the uber-jar.</p>

<p>This is a typical problem with agents and Byte Buddy has a standard way to circumvent it by using a <code>Transformer.ForAdvice</code> instance instead of using the <code>Advice</code> class directly. Byte Buddy then creates a virtual class loader hierarchy that considers classes represented by both class loaders.</p>

<p><strong>Update</strong>: The problem is that you are calling down to your interceptor that is defined in the system class loader where the class in question is not available. The annotated code will be inlined but the invoked method will not. If you copy-pasted the code into the annotated method, the behavior is as you'd expect it. Byte Buddy uses the annotated code as template and reuses a lot of information emitted by javac to guarantee a speedy conversion. Therefore, the library cannot simply copy the method and should rather feed the entire method body to javac.</p>
"
Datadog,56645177,54185496,0,"2019/06/18, 11:49:21",True,"2019/06/18, 11:49:21",36,11663155,2,"<p>The reason for this error is because apache is listening to port 80 on IPv4 &amp; IPv6.
This will explicitly tell apache to listen to IPv4.</p>

<p>In apache config change: </p>

<p><code>Listen 80</code></p>

<p>to </p>

<p><code>Listen 0.0.0.0:80</code></p>

<p>Make sure the file is being copied in to your docker container and being used in apache.</p>

<p>Or add an extra step in the Dockerfile:</p>

<p><code>&amp;&amp; sed -i 's/^Listen 80$/Listen 0.0.0.0:80/' /etc/apache2/httpd.conf</code></p>
"
Datadog,52392480,52390678,0,"2018/09/18, 21:18:13",True,"2018/09/18, 21:18:13",3537,8993347,3,"<p>Containers are about isolation so in container ""localhost"" means inside container  so ddtrace-test cannot find ddagent inside his container. You have 2 ways to fix that:</p>

<ol>
<li>Put <code>network_mode: host</code> in ddtrace-test so he will bind to host's network interface, skipping network isolation</li>
<li>Change ddtrace-test to use ""ddagent"" host instead of localhost as in docker-compose services can be accessed using theirs names </li>
</ol>
"
Datadog,50097560,50096290,2,"2018/04/30, 12:24:42",True,"2018/04/30, 12:52:06",18198,1765189,6,"<p>I'm guessing you're talking about ""metrics"" instead of matrix!</p>

<p>On the Producer, you have <code>kafka.producer:type=producer-metrics,client-id=""{client-id}""</code>. That metric has 2 interesting attributes:</p>

<ul>
<li><p>request-latency-avg: The average request latency in ms</p></li>
<li><p>request-latency-max: The maximum request latency in ms</p></li>
</ul>

<p>On the broker side, there are a few metrics you want to check to investigate your issue:</p>

<ul>
<li>Message conversion time: Down convertion happens if the producer is using a older message format than the broker. <code>kafka.network:type=RequestMetrics,name=MessageConversionsTimeMs,request=Produce</code></li>
<li><p>Request total time: Total time Kafka took to process the request. <code>kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce</code></p>

<p>In case this is high, you can check the break down metrics:</p>

<ul>
<li>Time the request waits in the request queue: <code>kafka.network:type=RequestMetrics,name=RequestQueueTimeMs,request=Produce</code></li>
<li>Time the request is processed at the leader: <code>kafka.network:type=RequestMetrics,name=LocalTimeMs,request=Produce</code></li>
<li>Time the request waits in the response queue: <code>kafka.network:type=RequestMetrics,name=ResponseQueueTimeMs,request={Produce|FetchConsumer|FetchFollower}</code></li>
<li>Time to send the response: <code>kafka.network:type=RequestMetrics,name=ResponseSendTimeMs,request=Produce</code></li>
</ul></li>
</ul>

<p>These are all listed in the metrics recommended to monitor list in the Kafka documentation: <a href=""http://kafka.apache.org/documentation/#monitoring"" rel=""noreferrer"">http://kafka.apache.org/documentation/#monitoring</a></p>
"
Datadog,44857071,44855220,2,"2017/07/01, 06:28:14",False,"2017/07/01, 06:28:14",19457,684908,3,"<p>Some preliminary Google searching lands me on <a href=""https://github.com/kubernetes/kubernetes/pull/42717"" rel=""nofollow noreferrer"">https://github.com/kubernetes/kubernetes/pull/42717</a> by way of <a href=""https://github.com/kubernetes/kubernetes/issues/24657"" rel=""nofollow noreferrer"">https://github.com/kubernetes/kubernetes/issues/24657</a>. It looks like the pull request was merged in time to be in Kubernetes 1.7. This should mean that you can use the Downward API to expose <code>status.hostIP</code> as an environment variable (<a href=""https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/</a>) or a file in a volume (<a href=""https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a>). Your application would then need to read the environment variable or file to get the value of the actual host IP address.</p>
"
Datadog,44867146,44855220,2,"2017/07/02, 06:15:59",False,"2017/07/04, 10:35:08",9153,172265,0,"<p>If you agent is written by yourself, you can open and listen on a Unix domain socket and let the other pod send data through it. </p>

<p>If not, you can write a small data proxy that listens on a Unix socket for data. On the other end, by sharing a pod with the daemon, you can easily send data to the local container</p>
"
Datadog,44927321,44855220,3,"2017/07/05, 16:14:08",True,"2017/07/05, 16:14:08",1123,1595197,6,"<p>I use the exact same setup, <code>dd-agent</code> running as a DaemonSet in my kubernetes cluster. Using the same port mapping you commented <a href=""https://stackoverflow.com/questions/44855220/sending-data-from-one-pod-to-another-pod-running-specifically-on-the-same-host#comment76761846_44857071"">here</a>, you can just send metrics to the hostname of the node an application is running on.</p>

<p>You can add the node name to the pods environment using the downward api in your pod spec:</p>

<pre><code>env:
- name: NODE_NAME
  valueFrom:
    fieldRef:
      fieldPath: spec.nodeName 
</code></pre>

<p>Then, you can just open an UDP connection to <code>${NODE_NAME}:8125</code> to connect to the datadog agent.</p>
"
Datadog,43669352,42538664,1,"2017/04/28, 02:11:48",False,"2017/04/28, 02:11:48",162,2254902,2,"<p>There are two error in your code:</p>

<ol>
<li>The <code>type</code> used is wrong. It should be <code>service check</code> instead of <code>metric alert</code>.</li>
<li>You need to enclose <code>process.up</code> in a pair of <code>''</code>. </li>
</ol>

<p>Once done, your code will run flawlessly.</p>
"
Datadog,60213083,60064427,1,"2020/02/13, 19:02:26",True,"2020/02/14, 09:17:50",27576,1839482,3,"<p><a href=""https://i.stack.imgur.com/MNeRh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MNeRh.png"" alt=""enter image description here""></a>Datadog has two agents.</p>

<ol>
<li>Cluster agent which is a proxy between Kubernetes API Server and Datadog node agents. The cluster agent is deployed as deployment to one of the kubernetes node.</li>
<li>Node agents which is deployed in each and every Kubernetes node as Daemonset.</li>
</ol>

<p>And yes for DogStatsD the node agents need to be deployed as Daemonset.</p>

<p>Here is the the deployment manifest for <a href=""https://github.com/DataDog/datadog-agent/blob/master/Dockerfiles/manifests/cluster-agent/cluster-agent.yaml"" rel=""nofollow noreferrer"">cluster agent</a> and <a href=""https://github.com/DataDog/datadog-agent/blob/master/Dockerfiles/manifests/agent.yaml"" rel=""nofollow noreferrer"">node agent</a>.</p>
"
Datadog,42481675,42441120,3,"2017/02/27, 11:09:46",True,"2017/02/27, 11:09:46",6657,86,1,"<p>So, while trying to debug this I deleted the deployment + dameonset and service and recreated it. Afterwards it worked....</p>
"
Datadog,44038527,42441120,0,"2017/05/18, 07:21:01",False,"2017/05/18, 07:21:01",9,7901388,0,"<p>Have you seen the <a href=""https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services"" rel=""nofollow noreferrer"">Discovering Services</a> docs? I'd recommend using DNS for service discovery rather than environment variables since environment variables require services to come up in a particular order.</p>
"
Datadog,61972253,61900949,0,"2020/05/23, 15:46:33",False,"2020/05/23, 15:46:33",493,244037,4,"<p>In Datadog Logs, there's a difference between the Tags associated with the execution environment, and Attributes set on Log entry content.</p>

<p>From <a href=""https://docs.datadoghq.com/logs/explorer/#log-structured-information"" rel=""nofollow noreferrer"">this section in the docs</a>:</p>

<blockquote>
  <p><strong>Context</strong> refers to the infrastructure and application context in which the log has been generated. Information is gathered from tags—whether automatically attached (host name, container name, log file name, serverless function name, etc.)—or added through custom tags (team in charge, environment, application version, etc.) on the log by the Datadog Agent or Log Forwarder.</p>
</blockquote>

<p>And looking into the <a href=""https://github.com/DataDog/browser-sdk/blob/723100bf032b6340a0002f51f6cd8af0d84467bd/packages/core/src/configuration.ts#L144-L153"" rel=""nofollow noreferrer"">source for the browser SDK</a>, we can see: </p>

<pre class=""lang-js prettyprint-override""><code>...
  const tags =
    `sdk_version:${conf.sdkVersion}` +
    `${conf.env ? `,env:${conf.env}` : ''}` +
    `${conf.service ? `,service:${conf.service}` : ''}` +
    `${conf.version ? `,version:${conf.version}` : ''}`
  const datadogHost = `${type}-http-intake.logs.${domain}`
  const host = conf.proxyHost ? conf.proxyHost : datadogHost
  const proxyParameter = conf.proxyHost ? `ddhost=${datadogHost}&amp;` : ''

  return `https://${host}/v1/input/${conf.clientToken}?${proxyParameter}ddsource=${source || 'browser'}&amp;ddtags=${tags}`
...
</code></pre>

<p>This shows us that the <code>tags</code> query string parameter being submitted is being calculated based on configuration, and only provides a small amount of user-configurable entries, like <code>env</code>, <code>service</code> - these were released very recently in version 1.11.5 - <a href=""https://github.com/DataDog/browser-sdk/pull/392"" rel=""nofollow noreferrer"">here's the change</a> introducing them.</p>

<p>So you may not be able to set <strong>tags</strong> for a specific log entry - rather you can set <strong>attributes</strong> per log entry, like in the example you shared, which is setting <strong>Attributes</strong> for the logger instance as a whole.
Attributes are part of the log <strong>Content</strong> - which will be viewable in the body of the log entry. </p>

<p>Yes, this is confusing since the function used is named <code>addContext</code>/<code>setContext</code> - and these don't set the same thing as the documentation's ""Context"" - rather they modify the Attributes that are associated with the log entry.</p>

<p>In that event, you may want to have either custom logger instances that provide specific attributes for that logger, or add context inline to the log entry, like this:</p>

<pre class=""lang-js prettyprint-override""><code>DD_LOGS.logger.info('Page Viewed', { referrer: document.referrer });
</code></pre>

<p><a href=""https://docs.datadoghq.com/logs/log_collection/javascript/?tab=bundle#send-a-custom-log-entry"" rel=""nofollow noreferrer"">Here's the docs</a> on this approach which show what other default attributes are being set per log entry.</p>
"
Datadog,58681226,58476988,0,"2019/11/03, 16:41:58",True,"2019/11/03, 16:41:58",2206,712543,3,"<p>According to the documentation, this can be achieved using following properties in telegraf.conf:</p>

<pre><code>[[outputs.datadog]]
apikey = ""&lt;datadog api key&gt;"" # required.
namepass = [""metric_1"",""metric_2""...etc.]
</code></pre>

<p><a href=""https://docs.influxdata.com/telegraf/v1.12/administration/configuration/#measurement-filtering"" rel=""nofollow noreferrer"">https://docs.influxdata.com/telegraf/v1.12/administration/configuration/#measurement-filtering</a></p>

<p>where <strong>namepass</strong> defines pattern list of points which will be emitted.</p>
"
Datadog,35509278,35224366,1,"2016/02/19, 17:47:27",True,"2016/02/19, 17:47:27",455,4676932,10,"<p>These system.io metrics are reported from a <a href=""https://github.com/DataDog/dd-agent/blob/5.6.3/checks/system/unix.py#L115-L133"" rel=""noreferrer"">system agent check</a> that uses <code>iostat</code> under the hood.</p>

<p>According to the <a href=""http://linux.die.net/man/1/iostat"" rel=""noreferrer"">iostat manpage</a> one of the metrics <code>%util</code> (reported as <code>system.io.util</code> within Datadog) seems to do the job:</p>

<blockquote>
  <p>%util: Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%.</p>
</blockquote>

<p>You can create a monitor, as a multi alert on host/device, when this metric is over 90 on the last 30 minutes on average, here is a current screenshot of such an example:</p>

<p><a href=""https://i.stack.imgur.com/bxVvc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bxVvc.png"" alt=""example monitor in Datadog""></a></p>

<p>Of course one can also monitor other iostat metrics to identify other I/O performance failure modes.</p>
"
Datadog,44193701,42958937,4,"2017/05/26, 07:35:57",True,"2017/05/26, 07:35:57",1371,5540166,5,"<p>So from that log line, it appears as though <a href=""https://github.com/DataDog/datadogpy/blob/v0.6.x/datadog/util/hostname.py#L51"" rel=""noreferrer"">this <code>try</code> is excepting</a> in the library's <code>hostname.py</code>. So either...</p>

<ul>
<li><p>(A) The <a href=""https://github.com/DataDog/datadogpy/blob/v0.6.x/datadog/util/hostname.py#L53"" rel=""noreferrer"">hostname line</a> is where it's excepting, and (weirdly) the
library requires that a <code>hostname</code> option be set in your
<code>datadog.conf</code> file. Maybe worth setting a hostname there if you
haven't already. Or,</p></li>
<li><p>(B) The <a href=""https://github.com/DataDog/datadogpy/blob/v0.6.x/datadog/util/hostname.py#L52"" rel=""noreferrer"">get_config() line</a> is where it's excepting, and so the
library isn't able to correctly identify the configuration file
location (or access it, possibly related to permissions). Based on
the directory structure in your question, I think you're working on
an OSX / mac environment, which means the library will be using the
function <code>_mac_config_path()</code> in <code>config.py</code> to try to identify the
configuration path, which from <a href=""https://github.com/DataDog/datadogpy/blob/v0.6.x/datadog/util/config.py#L83"" rel=""noreferrer"">this line in the function</a> would
make it <em>seem</em> as though the library were looking for the
configuration file in <code>~/.datadog-agent/agent/datadog.conf</code> instead
of the appropriate <code>~/.datadog-agent/datadog.conf</code>. Which might be a
legitimate bug...</p></li>
</ul>

<p>So if I were you, and if all this seemed right, I'd try adding a <a href=""https://github.com/DataDog/dd-agent/blob/5.13.x/datadog.conf.example#L25"" rel=""noreferrer"">hostname in the <code>datadog.conf</code></a> to see if that helped, and if it didn't, then I'd try making a <code>~/.datadog-agent/agent/</code> directory and copying your <code>datadog.conf</code> file there as well, just to see if that got things working.</p>

<p>This answer assumes you are working in an OSX / mac environment, and will likely not be correct otherwise. </p>

<p>If either (A) or (B) are the case, then that's a problem with the library and should be updated--it would be kind of you to open an issue on <a href=""https://github.com/DataDog/datadogpy"" rel=""noreferrer"">the library itself</a> to bring this up so the Datadog team that supports that library can be made aware. I suspect not many people end up this library in OSX / mac environments to begin with, so that could explain all this. </p>
"
Datadog,60667516,60662893,0,"2020/03/13, 11:01:40",False,"2020/03/13, 11:01:40",336,7243426,1,"<p>If you want to remove the warning, you can try adding <code>none</code> and <code>shm</code> to the <code>excluded_filesystems</code> in disk.yaml.  This file should exist or be created in the Agent's conf.d directory.</p>

<p>Otherwise, you'll find more options <a href=""https://github.com/DataDog/dd-agent/issues/2932"" rel=""nofollow noreferrer"">here</a>.</p>

<p>If you are looking to exclude the logs from the agent within the platform you can look at excluding the agent (<a href=""https://docs.datadoghq.com/agent/autodiscovery/management/?tab=containerizedagent#exclude-containers"" rel=""nofollow noreferrer"">doc</a>)</p>
"
Datadog,60677166,60662893,0,"2020/03/13, 22:56:50",True,"2020/03/13, 22:56:50",787,1363715,2,"<p>Found the answer here: <a href=""https://github.com/DataDog/datadog-agent/issues/3329"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-agent/issues/3329</a></p>

<p>The field is <code>mount_point_blacklist</code></p>
"
Datadog,56620105,52390804,0,"2019/06/16, 18:14:27",False,"2019/06/16, 18:14:27",301,2708541,0,"<p>You installed the Datadog agent &amp; trace agent on a Mac, listening on localhost. </p>

<p>You installed the flask application and ddtrace library in a docker container on a linux vm sending traffic to localhost. </p>

<p>Those two localhosts are describing two different machines. The easiest option is going to be running both the Agent &amp; flask app on the Mac, or running both in docker. The latter is most similar to an eventual production deployment. Do that. </p>
"
Datadog,59890601,59881685,0,"2020/01/24, 07:03:49",False,"2020/01/24, 07:03:49",390,1467579,1,"<p>There are a number of different ways you could handle this:</p>

<ul>
<li>In the past I've configured a telemetry DAG which would collect the current state of all tasks/DAGs by querying the metadata tables. I'd collect these metrics and push them up to CloudWatch. This became problematic as these internal fields change often so we would run into issues when trying to upgrade to newer versions of Airflow.</li>
<li>There are also some well-maintained <a href=""https://github.com/search?q=airflow+prometheus"" rel=""nofollow noreferrer"">Prometheus exporters</a> that some companies have open sourced. By setting these up you could poll the exposed export path as frequently as you wanted to (DataDog <a href=""https://docs.datadoghq.com/integrations/prometheus/"" rel=""nofollow noreferrer"">supports Prometheus</a>).</li>
</ul>

<p>These are just some of your options. Since the Airflow webserver is just a Flask app you can really expose metrics in whatever way you see fit.</p>
"
Datadog,63754701,59881685,0,"2020/09/05, 16:57:34",False,"2020/09/05, 18:28:32",65,11246025,0,"<p>As I understand, you can monitor running tasks in DAGs using DataDog, refer the integration with Airflow <a href=""https://docs.datadoghq.com/integrations/airflow/#overview"" rel=""nofollow noreferrer"">docs</a></p>
<p>You may refer metrics via DogStatD <a href=""https://docs.datadoghq.com/integrations/airflow/#data-collected"" rel=""nofollow noreferrer"">docs</a>. Also, look at this <a href=""https://www.datadoghq.com/blog/monitor-airflow-with-datadog/"" rel=""nofollow noreferrer"">page</a> would be useful to understand what to monitor.</p>
<p>E.g., the metrics as below:</p>
<ul>
<li><code>airflow.operator_failures</code>: monitor the failed operator.</li>
<li><code>airflow.operator_successes</code>: monitor succeed operator.</li>
<li><code>airflow.dag_processing.processes</code>: Number of currently running DAG parsing (processes).</li>
<li><code>airflow.scheduler.tasks.running </code>: Number of tasks running in executor
Shown as task.</li>
</ul>
"
Datadog,65918401,59879331,0,"2021/01/27, 13:40:25",False,"2021/01/27, 13:40:25",2115,2247740,0,"<p>HikariCP is a connection pool and JDBC is the API for managing a connection.
So it can be thought that Spring thinks about separating connection-pool-manager metrics from connection metrics.</p>
"
Datadog,59679096,59678757,7,"2020/01/10, 11:44:26",True,"2020/01/10, 12:04:43",39071,2291321,3,"<p>You need to set the <a href=""https://www.terraform.io/docs/providers/aws/r/ecs_task_definition.html#network_mode"" rel=""nofollow noreferrer""><code>aws_ecs_task_definition</code>'s <code>network_mode</code></a> to <code>awsvpc</code> if you are defining the <code>network_configuration</code> of the service that uses that task definition.</p>

<p>This is mentioned in the <a href=""https://www.terraform.io/docs/providers/aws/r/ecs_service.html#network_configuration"" rel=""nofollow noreferrer"">documentation for the <code>network_configuration</code> parameter of the <code>aws_ecs_service</code> resource</a>:</p>

<blockquote>
  <p><code>network_configuration</code> - (Optional) The network configuration for the
  service. This parameter is required for task definitions that use the
  <code>awsvpc</code> network mode to receive their own Elastic Network Interface,
  and it is not supported for other network modes.</p>
</blockquote>

<p>In your case you've added the <code>network_mode</code> parameter to the <em>container</em> definition instead of the <em>task</em> definition (a task is a collection of n containers and are grouped together to share some resources). The <a href=""https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ContainerDefinition.html"" rel=""nofollow noreferrer"">container definition schema</a> doesn't allow for a <code>network_mode</code> parameter.</p>
"
Datadog,52935718,52934832,1,"2018/10/22, 21:35:34",False,"2018/10/22, 21:35:34",922,8622323,2,"<p>You can use a conditional with count to override if a resource is to be created. The example below will only create the resource when the variable environment is not = production. If Count = 0 then the resource won't be created,</p>

<p>Regards,</p>

<pre><code>resource ""azurerm_network_security_rule"" ""web_server_nsg_rule_rdp"" {
  name                        = ""RDP Inbound""
  priority                    = 100
  direction                   = ""Inbound""
  access                      = ""Allow""
  protocol                    = ""Tcp""
  source_port_range           = ""*""
  destination_port_range      = ""3389""
  source_address_prefix       = ""*""
  destination_address_prefix  = ""*""
  resource_group_name         = ""${azurerm_resource_group.web_server_rg.name}""
  network_security_group_name = ""${azurerm_network_security_group.web_server_nsg.name}""
  count                       = ""${var.environment == ""production"" ? 0 : 1}""  
}
</code></pre>
"
Datadog,48836870,48807945,0,"2018/02/17, 02:52:20",False,"2018/02/17, 02:52:20",3131,1314028,-1,"<p>I don't think there is something that does this for you automatically. You have to reset the counter yourself at each reporting interval. Something like this should work:</p>

<pre><code>long count = registry.counter(""buttonA"").getCount();
dataDogReporter.report(""buttonA"", count);
registry.counter(""buttonA"").dec(count);
</code></pre>
"
Datadog,45324581,45324189,1,"2017/07/26, 13:41:18",False,"2017/07/26, 13:41:18",173,8047168,6,"<p>Actually, It is quite simple. This is called <strong>Packaging namespace packages</strong>.
<a href=""https://packaging.python.org/guides/packaging-namespace-packages/"" rel=""noreferrer"">https://packaging.python.org/guides/packaging-namespace-packages/</a></p>

<p>All you need is to separate all packages to sub - packages and after install it with a namespace.</p>

<pre><code># for all packages
pip install super_plugins

# for specific
pip install super_plugins.slack super_plugins.datadog
</code></pre>
"
Datadog,42717056,42708258,1,"2017/03/10, 13:15:07",False,"2017/03/10, 13:15:07",136,3403240,0,"<p>2 questions that'll be helpful:</p>

<ol>
<li>What's your timeout set to</li>
<li>What's the query?</li>
</ol>

<p>Now some clarification on where I think you're going wrong here:</p>

<ol>
<li>the resolution is too coarse to diagnose a single query, I could have a server doing nothing, do one expensive query that pegs some bottleneck for the entire time and on that scale look like nothing was bottlenecked, run iostat -x 1 on the servers at the same time and you may find something drastically different than what the charts say at that resolution.</li>
<li>If I'm looking at your CPU usage chart correctly there it looks like 50% usage. On modern servers that's actually fully busy because of hyperthreading and how aggregate CPU usage works see <a href=""https://www.percona.com/blog/2015/01/15/hyper-threading-double-cpu-throughput/"" rel=""nofollow noreferrer"">https://www.percona.com/blog/2015/01/15/hyper-threading-double-cpu-throughput/</a></li>
</ol>
"
Datadog,42718833,42708258,2,"2017/03/10, 14:48:54",False,"2017/03/10, 14:57:25",761,2521248,0,"<p>I suggest tracing the problematic query to see what cassandra was doing.</p>

<p><a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tracing_r.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tracing_r.html</a></p>

<p>Open cql shell, type <code>TRACING ON</code> and execute your query. If everything seems fine, there is a chance that this problem happens occasionally, in which case I'd suggest tracing the queries using nodetool settraceprobablilty for some time, until you manage to catch the problem.</p>

<p>You enable it on each node separately using <code>nodetool settraceprobability &lt;param&gt;</code> where param is the probability (between 0 and 1) that the query will get traced. Careful: this WILL cause increased load, so start with a very low number and go up.</p>

<p>If this problem is occasional there is a chance that this might be caused by long garbage collections, in which case you need to analyse the GC logs. Check how long your GC's are.</p>

<p>edit: just to be clear, if this problem is caused by GC's you will NOT see it with tracing. So first check your GC's, and if its not the problem then move on to tracing.</p>
"
Datadog,28831358,28436960,0,"2015/03/03, 14:03:18",False,"2015/03/03, 14:03:18",579,4531116,1,"<p>Here you can find how to add a new webhook to your mandrill account: <a href=""https://mandrillapp.com/api/docs/webhooks.php.html#method=add"" rel=""nofollow"">https://mandrillapp.com/api/docs/webhooks.php.html#method=add</a></p>

<p>tha main thing here is this:
<code>$url = 'http://example/webhook-url';</code>
this is your webhook URL what will process the data sent by mandrill and forward the information to Datadog.</p>

<p>and this is a description about what mandrill will send to your webhook URL: <a href=""http://help.mandrill.com/entries/21738186-Introduction-to-Webhooks"" rel=""nofollow"">http://help.mandrill.com/entries/21738186-Introduction-to-Webhooks</a></p>
"
Datadog,28835739,28436960,0,"2015/03/03, 17:35:33",False,"2015/03/03, 17:35:33",516,1988301,1,"<p>a listener for webhooks is nothing else then a website/app which triggers an action if a request comes in. Usually you keep it secret or secure it with (http basic) authentication. E.g. create a website called <a href=""http://yourdomain.com/hooklistener.php"" rel=""nofollow"">http://yourdomain.com/hooklistener.php</a>. You can then call it with HTTP POST or GET and pass some data like hooklistener.php?event=triggerDataDog or with POST and send data along with the body. You then run a script or anything you want to process that event.</p>
"
Datadog,28919358,28436960,0,"2015/03/07, 22:02:50",False,"2015/03/07, 22:02:50",544,1428713,0,"<p>A ""listener"" is just any URL that you host where you can receive data that is posted to it. Keep in mind, since you mentioned Zapier, you can set up a trigger that receives the webhook data - in this case the listener URL is provided by Zapier, and you can then send that data into any application (or even post to another webhook). Using Zapier is nice because it doesn't require you to write the listener code that receives the hook data and does something with it.</p>
"
Datadog,21529327,21527387,2,"2014/02/03, 16:00:31",False,"2014/02/03, 16:00:31",25180,1849664,7,"<h3>tl;dr : <a href=""http://aws.amazon.com/cloudwatch"" rel=""noreferrer"">Amazon CloudWatch</a> will do what you want and probably much much more.</h3>
<p>I believe that Amazon actually offers a service that would accomplish your goal - <a href=""http://aws.amazon.com/cloudwatch"" rel=""noreferrer"">CloudWatch</a> <sup><a href=""http://aws.amazon.com/cloudwatch/pricing/"" rel=""noreferrer"">(pricing)</a></sup>. I'm going to take your points one by one. Note that I haven't actually <em>used</em> it before, but the documentation is fairly clear.</p>
<blockquote>
<p>One server had high CPU for over 5 mins when the alert should be triggered after 1 minute</p>
</blockquote>
<p>It looks like CloudWatch can be configured to send an alert (which I'll get to) after one minute of a condition being met:</p>
<p><img src=""https://i.stack.imgur.com/UwGch.png"" alt=""enter image description here"" /></p>
<p>One can actually set conditions for many other metrics as well - this is what I see on one of my instances, and I think that detailed monitoring (I use free), might have even more:</p>
<p><img src=""https://i.stack.imgur.com/KEiDX.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>What else is out there that can do the same job and <strong>will also integrate with Pager Duty?</strong></p>
</blockquote>
<p>I'm assuming you're talking about <a href=""http://www.pagerduty.com/"" rel=""noreferrer"">this</a>. It turns out the Pager Duty has a <a href=""http://www.pagerduty.com/docs/guides/aws-cloudwatch-integration-guide/"" rel=""noreferrer"">helpful guide</a> just for integrating CloudWatch. How nice!</p>
<h2>Pricing</h2>
<p><a href=""http://aws.amazon.com/cloudwatch/pricing/"" rel=""noreferrer"">Here's the pricing page</a>, as you would probably like to parse it instead of me telling you. I'll give a brief overview, though:</p>
<p>You don't want basic monitoring, as it only gives you metrics once per five minutes (which you've indicated is unacceptable.) Instead, you want detailed monitoring (once every minute).</p>
<p>For an EC2 instance, the price for detailed monitoring is $3.50 per instance <strong>per month</strong>. Additionally, every alarm you make is $0.10 per month. This is actually very cheap if compared to <a href=""http://copperegg.com/pricing/"" rel=""noreferrer"">CopperEgg's pricing</a> - $70/mo versus <em>maybe</em> $30 per month for 9 instances and copious amounts of alarms. In reality, you'll probably be paying more like $10/mo.</p>
<p>Pager Duty's tutorial suggests you use SNS, which is another cost. The good thing: <a href=""http://aws.amazon.com/sns/pricing/"" rel=""noreferrer"">it's dirt cheap</a>. $0.60 per million notifications. If you ever get above a dollar in a year for SNS, you need to perform some serious reliability improvements on your servers.</p>
<h2>Other shiny things!</h2>
<p>You're not just limited to Amazon's pre-packaged metrics! You can actually send custom metrics (time it took to complete a cronjob, whatever) to Cloudwatch via a PUT request. Quite handy.</p>
<blockquote>
<p>Submit Custom Metrics generated by your own applications (or by AWS resources not mentioned above) and have them monitored by Amazon CloudWatch. You can submit these metrics to Amazon CloudWatch via a simple Put API request.</p>
</blockquote>
<p>(from <a href=""http://aws.amazon.com/cloudwatch/details/"" rel=""noreferrer"">here</a>)</p>
<h2>Conclusion</h2>
<p>So all in all: CloudWatch is quite cheap, can do 1-minute frequency stats, and will integrate with Pager Duty.</p>
"
Datadog,29150848,21527387,0,"2015/03/19, 19:10:30",False,"2015/03/19, 19:10:30",7448,554903,3,"<h3>tl;dr: <a href=""https://www.serverdensity.com/"" rel=""nofollow noreferrer"">Server Density</a> will do what you want, on top of that it has web checks and custom metrics too.</h3>

<p>In short Server Density is a monitoring tool that will monitor all the relevant server metrics. You can take a look at this page <a href=""https://www.serverdensity.com/server-monitoring/"" rel=""nofollow noreferrer"">where it’s all described</a>. </p>

<blockquote>
  <p>One server had high CPU for over 5 mins when the alert should be triggered after 1 minute</p>
</blockquote>

<p>Server Density’s open source agent collects and posts the data to their server every minute and you can decide yourself when that alert should be triggered. In the alert below you can see that the alert will alert 1 person after 1 minute and then repeatedly alert every 5 minutes. </p>

<p><img src=""https://i.stack.imgur.com/fr4KH.png"" alt=""How an alert looks like""></p>

<p>There is a lot of other metrics that you can alert on too. </p>

<p><img src=""https://i.stack.imgur.com/CYHtQ.png"" alt=""enter image description here""></p>

<blockquote>
  <p>What else is out there that can do the same job and will also integrate with Pager Duty?</p>
</blockquote>

<p>Server Density also integrates with PagerDuty. The only thing you need to do is to <a href=""https://support.pagerduty.com/hc/en-us/articles/202829310-Generating-an-API-Key"" rel=""nofollow noreferrer"">generate an api key at PagerDuty</a> and then provide that in the settings. </p>

<p>Just provide the API key in the settings and you can then in check pagerduty as one of the alert recipients. </p>

<p><img src=""https://i.stack.imgur.com/lJqw8.png"" alt=""Settings""></p>

<h2>Pricing</h2>

<p>You can find the <a href=""https://www.serverdensity.com/pricing/"" rel=""nofollow noreferrer"">pricing page here</a>. I’ll give you a brief overview of it. The pricing starts at $10 for one server plus one web check and then get’s cheaper per server the more servers you add. </p>

<p>Everything will be monitored once every minute and there is no fees added for the amount of alerts added or triggered, even if that is an SMS to your phone number. The cost is slightly more expensive than the Cloudwatch example, but the support is good. If you used copperegg before they have a <a href=""https://github.com/serverdensity/Copperegg-migration"" rel=""nofollow noreferrer"">migration tool</a> too. </p>

<h2>Other shiny things!</h2>

<p>Server Density allows you to monitor all the things! Then only thing you need to do is to send us custom metrics which you can do with a plugin written by yourself or by someone else. </p>

<p>I have to say that the graphs that Server Density provides is somewhat akin to eye candy too. Most other monitoring solutions I’ve seen out there have quite dull dashboards. </p>

<h2>Conclusion</h2>

<p>It will do the job for you. Not as cheap as CloudWatch, but doesn’t lock you in into AWS. It’ll give you 1 minute frequency metrics and integrate with pagerduty + a lot more stuff. </p>
"
Datadog,51896300,47577288,2,"2018/08/17, 16:18:03",False,"2018/08/18, 20:06:12",11,7120456,0,"<p>For now I see only two possibilities:</p>

<ol>
<li>Use GCP custom metrics using client from google-cloud-clients/google-cloud-monitoring and stackdriver integration with Datadog</li>
<li>Use datadog agent deployed in the cloud and connect to it using Datadog StatsD client (Java, Python, Go)</li>
</ol>

<hr>

<ol>
<li><p>Use GCP custom metrics
<a href=""https://cloud.google.com/monitoring/custom-metrics/creating-metrics"" rel=""nofollow noreferrer"">https://cloud.google.com/monitoring/custom-metrics/creating-metrics</a>
and datadog integration with GCP
<a href=""https://www.datadoghq.com/product/integrations/#cat-google-cloud"" rel=""nofollow noreferrer"">https://www.datadoghq.com/product/integrations/#cat-google-cloud</a></p>

<pre class=""lang-java prettyprint-override""><code>final MetricServiceClient client = MetricServiceClient.create();
ProjectName name = ProjectName.of(projectId);

MetricDescriptor descriptor = MetricDescriptor.newBuilder()
    .setType(metricType)
    .setDescription(""This is a simple example of a custom metric."")
    .setMetricKind(MetricDescriptor.MetricKind.GAUGE)
    .setValueType(MetricDescriptor.ValueType.DOUBLE)
    .build();

CreateMetricDescriptorRequest request = CreateMetricDescriptorRequest.newBuilder()
    .setName(name.toString())
    .setMetricDescriptor(descriptor)
    .build();

client.createMetricDescriptor(request);
</code></pre></li>
<li><p>Use datadog statsd client, java one -
<a href=""https://github.com/DataDog/java-dogstatsd-client"" rel=""nofollow noreferrer"">https://github.com/DataDog/java-dogstatsd-client</a> so you can deploy
datadog agent on GCP and connect through it. Sample with kubernetes.
<a href=""https://docs.datadoghq.com/tracing/setup/kubernetes/#deploy-agent-daemonset"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/setup/kubernetes/#deploy-agent-daemonset</a></p>

<pre class=""lang-java prettyprint-override""><code>import com.timgroup.statsd.ServiceCheck;
import com.timgroup.statsd.StatsDClient;
import com.timgroup.statsd.NonBlockingStatsDClient;

public class Foo {

  private static final StatsDClient statsd = new NonBlockingStatsDClient(
    ""my.prefix"",                          /* prefix to any stats; may be null or empty string */
    ""statsd-host"",                        /* common case: localhost */
    8125,                                 /* port */
    new String[] {""tag:value""}            /* Datadog extension: Constant tags, always applied */
  );

  public static final void main(String[] args) {
    statsd.incrementCounter(""foo"");
    statsd.recordGaugeValue(""bar"", 100);
    statsd.recordGaugeValue(""baz"", 0.01); /* DataDog extension: support for floating-point gauges */
    statsd.recordHistogramValue(""qux"", 15);     /* DataDog extension: histograms */
    statsd.recordHistogramValue(""qux"", 15.5);   /* ...also floating-point */
    statsd.recordDistributionValue(""qux"", 15);     /* DataDog extension: global distributions */
    statsd.recordDistributionValue(""qux"", 15.5);   /* ...also floating-point */

    ServiceCheck sc = ServiceCheck
          .builder()
          .withName(""my.check.name"")
          .withStatus(ServiceCheck.Status.OK)
          .build();
    statsd.serviceCheck(sc); /* Datadog extension: send service check status */

    /* Compatibility note: Unlike upstream statsd, DataDog expects execution times to be a
     * floating-point value in seconds, not a millisecond value. This library
     * does the conversion from ms to fractional seconds.
     */
    statsd.recordExecutionTime(""bag"", 25, ""cluster:foo""); /* DataDog extension: cluster tag */
  }
}
</code></pre>

<p>datadog deployment.yaml for kubernetes</p>

<pre class=""lang-yaml prettyprint-override""><code>apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: datadog-agent
spec:
  template:
    metadata:
      labels:
        app: datadog-agent
      name: datadog-agent
    spec:
      serviceAccountName: datadog-agent
      containers:
      - image: datadog/agent:latest
        imagePullPolicy: Always
        name: datadog-agent
        ports:
          - containerPort: 8125
            # hostPort: 8125
            name: dogstatsdport
            protocol: UDP
          - containerPort: 8126
            # hostPort: 8126
            name: traceport
            protocol: TCP
        env:
          - name: DD_APM_ENABLED
            value: ""true""
          - name: DD_API_KEY
            value: ""&lt;YOUR_API_KEY&gt;""
          - name: DD_COLLECT_KUBERNETES_EVENTS
            value: ""true""
          - name: DD_LEADER_ELECTION
            value: ""true""
          - name: KUBERNETES
            value: ""yes""
          - name: DD_KUBERNETES_KUBELET_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
        resources:
          requests:
            memory: ""256Mi""
            cpu: ""200m""
          limits:
            memory: ""256Mi""
            cpu: ""200m""
        volumeMounts:
          - name: dockersocket
            mountPath: /var/run/docker.sock
          - name: procdir
            mountPath: /host/proc
            readOnly: true
          - name: cgroups
            mountPath: /host/sys/fs/cgroup
            readOnly: true
        livenessProbe:
          exec:
            command:
            - ./probe.sh
          initialDelaySeconds: 15
          periodSeconds: 5
      volumes:
        - hostPath:
            path: /var/run/docker.sock
          name: dockersocket
        - hostPath:
            path: /proc
          name: procdir
        - hostPath:
            path: /sys/fs/cgroup
          name: cgroups
</code></pre></li>
</ol>

<p>Currently I'm investigating this so I'm not sure how to do this, yet. </p>
"
Datadog,41995839,41992681,1,"2017/02/02, 08:48:29",True,"2017/02/02, 08:48:29",1981,2473382,4,"<h1>Solution 1, dirty:</h1>

<p>Duplicate the definition of <code>event.sent</code> in <code>event.failed</code>. As soon as you restart the agent, any <code>sent</code> event will be seen as <code>sent</code> <strong>and</strong> <code>failed</code>. After a minute or so you revert the definition of <code>failed</code> to the proper definition, but you now have a few (admittedly bogus) failed event in datadog, allowing you to use the metric.</p>

<h1>Solution 2, clean:</h1>

<p>On your datadog dashboard, edit the relevant metric, and instead of using the graphical interface (tab <code>edit</code>) got to the <code>JSON</code> tab, where you can manually enter your metric name (probably a slightly updated cut &amp; paste of your <code>sent</code> metric), no matter if an event exists or not.</p>
"
Datadog,34909299,34893203,2,"2016/01/20, 21:54:55",False,"2016/01/20, 21:54:55",261,4172512,4,"<p>Just a few items to note to get this working:</p>

<p><code>
dogstatsd = Statsd.new('MY_API_KEY')
</code></p>

<p>This line of code is trying to use your API key to establish a statsD connection, but this should actually be trying to establish the statsD connection via the statsD port currently configured on your Agent as seen here:</p>

<p><code>
Create a stats instance.
statsd = Statsd.new('localhost', 8125)
</code></p>

<blockquote>
  <p>The easiest way to get your custom metrics into Datadog is to send them to DogStatsD, a metrics aggregation server bundled with the Datadog Agent (in versions 3.0 and above). DogStatsD implements the StatsD protocol, along with a few extensions for special Datadog features.</p>
</blockquote>

<p><a href=""http://docs.datadoghq.com/guides/dogstatsd/"" rel=""nofollow"">http://docs.datadoghq.com/guides/dogstatsd/</a></p>

<p>If you would not like to deploy an Agent on the host running the RoR application, you can utilize DogAPI gem:</p>

<p><a href=""https://github.com/DataDog/dogapi-rb"" rel=""nofollow"">https://github.com/DataDog/dogapi-rb</a></p>

<p>Which has additional documentation to get this custom metrics submitted:</p>

<pre><code>require 'rubygems'
require 'dogapi'

api_key = ""abcdef123456""

dog = Dogapi::Client.new(api_key)

dog.emit_point('some.metric.name', 50.0, :host =&gt; ""my_host"", :device =&gt; ""my_device"")
</code></pre>

<p>If you have additional questions, please reach out to support@datadoghq.com</p>
"
Datadog,55661412,55588976,0,"2019/04/13, 05:13:40",True,"2019/04/13, 05:13:40",1371,5540166,1,"<p><a href=""https://docs.datadoghq.com/integrations/#cat-log-collection"" rel=""nofollow noreferrer"">This doc will show you a comprehensive list</a> of all integrations that involve log collection. Some of these include other common log shippers, which can also be used to forward logs to Datadog. Among these you'd find...</p>

<ul>
<li><a href=""https://docs.datadoghq.com/integrations/fluentd/#log-collection"" rel=""nofollow noreferrer"">Fluentd</a></li>
<li><a href=""https://docs.datadoghq.com/integrations/logstash/#log-collection"" rel=""nofollow noreferrer"">Logstash</a></li>
<li><a href=""https://docs.datadoghq.com/integrations/rsyslog/?tab=datadogussite"" rel=""nofollow noreferrer"">Rsyslog</a> (for linux)</li>
<li><a href=""https://docs.datadoghq.com/integrations/syslog_ng/?tab=datadogussite"" rel=""nofollow noreferrer"">Syslog-ng</a> (for linux, windows)</li>
<li><a href=""https://docs.datadoghq.com/integrations/nxlog/?tab=datadogussite"" rel=""nofollow noreferrer"">nxlog</a> (for windows)</li>
</ul>

<p>That said, you <a href=""https://docs.datadoghq.com/agent/faq/the-datadog-agent-for-logs-or-traces-only/?tab=logs"" rel=""nofollow noreferrer"">can still just use the Datadog agent to collect logs only</a> (they want you to collect everything with their agent, that's why they warn you against collecting just their logs).</p>

<p>If you want to collect logs from docker containers, the Datadog agent is an easy way to do that, and it has the benefit of adding lots of relevant docker-metadata as tags to your logs. (<a href=""https://docs.datadoghq.com/logs/log_collection/docker/?tab=containerinstallation"" rel=""nofollow noreferrer"">Docker log collection instructions here</a>.)</p>

<p>If you don't want to do that, I'd look at Fluentd first on the list above -- it has a good reputation for containerized log collection, promotes JSON log formatting (for easier processing), and scales reasonably well. </p>
"
Datadog,29994613,29949571,0,"2015/05/01, 23:26:19",False,"2015/05/01, 23:26:19",455,4676932,2,"<p>The exception you are seeing is related to the IO system metrics collection and has nothing to do with your custom dogstream parser.</p>

<p>If you look at the stack trace it says that it wasn't able to apply the <code>_parse_linux2</code> function. To troubleshoot that further you should take a look at the output of </p>

<pre><code>/opt/datadog-agent/embedded/bin/iostat -d 1 2 -x -k
</code></pre>

<p>which is the command launched by the agent. Feel free to open a bug on the agent GitHub repository.</p>

<p>References:</p>

<ul>
<li><a href=""https://github.com/DataDog/dd-agent/blob/5.3.2/checks/system/unix.py#L184-L185"" rel=""nofollow"">https://github.com/DataDog/dd-agent/blob/5.3.2/checks/system/unix.py#L184-L185</a></li>
<li><a href=""https://github.com/DataDog/dd-agent/blob/5.3.2/checks/system/unix.py#L264-L284"" rel=""nofollow"">https://github.com/DataDog/dd-agent/blob/5.3.2/checks/system/unix.py#L264-L284</a></li>
</ul>
"
Datadog,58548229,58505214,1,"2019/10/24, 22:47:27",False,"2019/10/24, 22:47:27",336,7243426,1,"<p>Do you have the ability to add some parameters in the logs sent. From <a href=""https://docs.datadoghq.com/tracing/advanced/connect_logs_and_traces/?tab=python#manual-trace-id-injection"" rel=""nofollow noreferrer"">the documentation</a> you should be able to inject the trace id into your logs in a way that Datadog will interpret them.</p>

<p>You can also look at a parser to extract the trace id and span id from the raw log. <a href=""https://docs.datadoghq.com/tracing/faq/why-cant-i-see-my-correlated-logs-in-the-trace-id-panel/?tab=withlogintegration"" rel=""nofollow noreferrer"">This documentation</a> should help you out on that.</p>
"
Datadog,61929834,58505214,1,"2020/05/21, 10:55:37",False,"2020/05/21, 10:55:37",8464,85348,1,"<p>From the documentation, if you don't have JSON logs, you need to include <code>dd.trace_id</code> and <code>dd.span_id</code> in your formatter: </p>

<blockquote>
  <p>If your logs are raw formatted, update your formatter to include
  <code>dd.trace_id</code> and <code>dd.span_id</code> in your logger configuration:</p>

<pre><code>&lt;Pattern&gt;""%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L -
%X{dd.trace_id:-0} %X{ dd.span_id:-0} - %m%n""&lt;/Pattern&gt; ```
</code></pre>
</blockquote>

<p>So if you add <code>%X{dd.trace_id:-0} %X{ dd.span_id:-0}</code>, it should work.</p>
"
Datadog,49867829,49690040,0,"2018/04/17, 02:53:52",True,"2018/04/17, 02:53:52",356,2750290,1,"<p>It has support to CURL means you can make REST API calls. Try using some Http libraries like <strong>HttpURLConnection</strong> in java to make those POST requests. </p>

<p>I don't think we need a java based SDK for that as you can frame your own SDK on top of REST api's.</p>
"
Datadog,66307900,64126426,0,"2021/02/22, 00:39:04",False,"2021/02/22, 00:44:46",1272,1563480,0,"<p>After reading this documentation, <a href=""https://docs.datadoghq.com/tracing/setup_overview/setup/ruby/#resque"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/setup_overview/setup/ruby/#resque</a>, did you try making the options a hash with curly braces surrounding? Options is specified as being a hash. So everything after <code>c.use :resque, </code> should be a <strong>hash</strong>.</p>
"
Datadog,52615071,51975736,0,"2018/10/02, 22:23:00",True,"2018/10/04, 04:20:38",899,4386440,1,"<p>I figured out how to do this using the datadog api <a href=""https://docs.datadoghq.com/api/?lang=python#post-timeseries-points"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/?lang=python#post-timeseries-points</a>. The following python script takes in the jtl file (jmeter results) and posts the transaction name, response time, and status (pass/fail) to datadog. </p>

<pre><code>#!/usr/bin/env python3
import sys
import pandas as pd
from datadog import initialize, api

options = {
    'api_key': '&lt;API_KEY&gt;',
    'app_key': '&lt;APPLICATION_KEY&gt;'
}
metrics = []

def get_current_metric(timestamp, label, elapsed, success):
    metric = {}
    metric.update({'metric': 'jmeter'})
    metric.update({'points': [(timestamp, elapsed)]})
    curtags = {}
    curtags.update({'testcase': label})
    curtags.update({'success': success})
    metric.update({'tags': curtags})
    return metric

initialize(**options)

jtl_file = sys.argv[1]
df = pd.read_csv(jtl_file)

for index, row in df.iterrows():
    timestamp = row['timeStamp']/1000
    label = row['label']
    elapsed = row['elapsed']
    success = str(row['success'])
    metric = get_current_metric(timestamp, label, elapsed, success)
    metrics.append(metric)

api.Metric.send(metrics)
</code></pre>
"
Datadog,51125051,51124961,2,"2018/07/01, 19:25:56",False,"2018/07/02, 02:14:02",10159,4907630,2,"<p>First step is to install the DataDog agent on the server in which you are running your application:</p>

<p><a href=""https://docs.datadoghq.com/agent/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/</a></p>

<p>You then need to enable the <code>DogStatsD</code> service in the DataDog agent:</p>

<p><a href=""https://docs.datadoghq.com/developers/dogstatsd/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/developers/dogstatsd/</a></p>

<p>After that, you can send metrics to the <code>statsd</code> agent using any Go library that connects to <code>statsd</code>.</p>

<p>For example:</p>

<p><a href=""https://github.com/DataDog/datadog-go"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-go</a></p>

<p><a href=""https://github.com/go-kit/kit/tree/master/metrics/statsd"" rel=""nofollow noreferrer"">https://github.com/go-kit/kit/tree/master/metrics/statsd</a></p>

<p>Here's an example program sending some counts using the first library:</p>

<pre><code>import (
    ""github.com/DataDog/datadog-go/statsd""
    ""log""
)

func main() {
    // Create the client
    c, err := statsd.New(""127.0.0.1:8125"")
    if err != nil {
        log.Fatal(err)
    }
    // Prefix every metric with the app name
    c.Namespace = ""myapp.""
    // Count two events
    err = c.Count(""my_counter"", 2, nil, 1)
    if err != nil {
        log.Fatal(err)
    }
    // Close the client
    err = c.Close()
    if err != nil {
        log.Fatal(err)
    }
}
</code></pre>
"
Datadog,51145348,51124961,0,"2018/07/03, 04:00:56",False,"2018/07/03, 04:00:56",3174,4639336,0,"<p>Here is a convenience wrapper for DD. It uses ENV vars to configure it, but it's a nice utility to package out common DD calls once you have the agent running in the background.</p>

<pre><code>package datadog

import (
    ""errors""
    ""log""
    ""os""
    ""regexp""
    ""sort""
    ""sync""
    ""time""

    ""github.com/DataDog/datadog-go/statsd""
)

var (
    mu     sync.RWMutex
    client = newClient()

    GlobalTags Tags = Tags{}
)

// Sometimes, the connection to datadog can fail, but because UDP is connectionless, we don't get insight into
// those failures. This loop just makes sure that once a minute, the client is refreshed.
func init() {
    go func() {
        for range time.Tick(time.Minute) {
            c := newClient()
            if c != nil {
                mu.Lock()
                client = c
                mu.Unlock()
            }
        }
    }()
}

func newClient() *statsd.Client {
    hp := os.Getenv(""DOGSTATSD_HOST_PORT"")
    if hp == """" {
        hp = ""127.0.0.1:8125""
    }

    c, err := statsd.New(hp)
    if err != nil {
        log.Println(""stat/datadog"", ""Could not create a datadog statsd client."", ""error"", err)
    }

    return c
}

type Tags map[string]string

func (tags Tags) StringSlice() []string {
    var stringSlice []string
    for k, v := range tags {
        if k != """" &amp;&amp; v != """" {
            stringSlice = append(stringSlice, formatName(k)+"":""+formatName(v))
        }
    }
    sort.Strings(stringSlice)
    return stringSlice
}

func mergeTags(tagsSlice []Tags) Tags {
    merged := Tags{}
    for k, v := range GlobalTags {
        merged[formatName(k)] = formatName(v)
    }
    for _, tags := range tagsSlice {
        for k, v := range tags {
            merged[formatName(k)] = formatName(v)
        }
    }
    return merged
}

func Gauge(name string, value float64, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Gauge(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

func Count(name string, value int64, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Count(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

func Histogram(name string, value float64, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Histogram(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

func Incr(name string, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Incr(formatName(name), mergeTags(tags).StringSlice(), 1)
}

func Decr(name string, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Decr(formatName(name), mergeTags(tags).StringSlice(), 1)
}

func Set(name string, value string, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Set(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

func Timing(name string, value time.Duration, tags ...Tags) error {
    mu.RLock()
    defer mu.RUnlock()
    if client == nil {
        return errors.New(""datadog: nil client"")
    }
    return client.Timing(formatName(name), value, mergeTags(tags).StringSlice(), 1)
}

// Datadog allows '.', '_' and alphas only.
// If we don't validate this here then the datadog error logs can fill up disk really quickly
var nameRegex = regexp.MustCompile(`[^\._a-zA-Z0-9]+`)

func formatName(name string) string {
    return nameRegex.ReplaceAllString(name, ""_"")
}
</code></pre>
"
Datadog,31682034,29697464,0,"2015/07/28, 19:24:53",False,"2015/07/28, 19:24:53",455,4676932,1,"<p>The Monitor section of Datadog now includes a ""Monitor status"" page for each of the monitor you define (for example the URL monitoring you already have). On this page, you can see by group/scope the monitor history and it shows you the uptime for that monitor.</p>

<p>More to read about this new feature <a href=""https://www.datadoghq.com/blog/monitor-alert-status/"" rel=""nofollow"">here</a></p>

<p>It's not yet available as a ""report"" but that's a good idea!</p>
"
Datadog,62551284,62549173,1,"2020/06/24, 11:56:43",False,"2020/06/24, 11:56:43",8138,2385808,2,"<p>When installing Datadog in your K8s Cluster, you install a <a href=""https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-node-logging-agent"" rel=""nofollow noreferrer"">Node Logging Agent</a> as a Daemonset with various volume mounts on the hosting nodes. Among other things, this gives Datadog access to the Pod logs at /var/log/pods and the container logs at /var/lib/docker/containers.</p>
<p>Kubernetes and the underlying Docker engine will only include output from stdout and stderror in those two locations (see <a href=""https://kubernetes.io/docs/concepts/cluster-administration/logging/#logging-at-the-node-level"" rel=""nofollow noreferrer"">here</a> for more information). Everything that is written by containers to log files residing inside the containers, will be invisible to K8s, unless more configuration is applied to extract that data, e.g. by applying the <a href=""https://kubernetes.io/docs/concepts/cluster-administration/logging/#streaming-sidecar-container"" rel=""nofollow noreferrer"">side care container pattern</a>.</p>
<p>So, to get things working in your setup, <strong>configure logback to log to stdout rather than /var/app/logs/myapp.log</strong></p>
<p>Also, if you don't use APM there is no need to instrument your code with the datadog.jar and do all that tracing setup (setting up ports etc).</p>
"
Datadog,58421772,58414654,1,"2019/10/17, 00:11:59",True,"2019/10/17, 00:11:59",6375,1803990,2,"<p>You need to tell Datadog to pull custom metric. Go to AWS integrations configuration page (Integrations side menu -> Integrations -> Amazon Web Services).</p>

<p>You will see a list of services to integrate with, custom metrics is the last option on list. Make sure it's ticked. Takes a while for Datadog to actually start pulling the metric.</p>

<p><a href=""https://i.stack.imgur.com/1rdOY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1rdOY.png"" alt=""enter image description here""></a></p>
"
Datadog,60719420,54508636,0,"2020/03/17, 11:20:42",False,"2020/03/17, 11:20:42",23,6286219,0,"<p>AFAIK it is not possible at the Moment to use micrometer to send events to datadog. Micrometer states that <em>""Micrometer is not a distributed tracing system or an event logger.</em>"" on its section ""1. Purpose"" on its <a href=""https://micrometer.io/docs/concepts"" rel=""nofollow noreferrer"">concepts page</a>.</p>
"
Datadog,43699066,43146756,0,"2017/04/29, 21:21:56",False,"2017/04/29, 21:21:56",1371,5540166,0,"<p>Datadog monitors evaluate every minute, I think. So in your <code>sum(last_30m){X}</code> example, every minute, the monitor would sum the values of <code>{X}</code> over the last 30 minutes, and if that value was above whatever threshold you set, then it would trigger an alert. Same thing for <code>sum(last_1h){X}</code>, but every minute it would evaluate the sum over the last 1 hour.</p>
"
Datadog,61783745,61769846,0,"2020/05/13, 22:59:10",True,"2020/05/13, 23:10:16",2172,3514300,1,"<p>A better way would be to use a sidecar container with a logging agent, it won't increase the load on the API server.</p>

<p><a href=""https://i.stack.imgur.com/AE8K9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AE8K9.png"" alt=""enter image description here""></a></p>

<p>Reference: <a href=""https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent</a></p>

<p>Datadog agent looks like doesn't support /suggest running as a sidecar (<a href=""https://github.com/DataDog/datadog-agent/issues/2203#issuecomment-416180642"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-agent/issues/2203#issuecomment-416180642</a>)</p>

<p>I suggest looking at using other logging agent and pointing the backend to datadog.</p>

<p>Some options are:</p>

<ul>
<li>fluentd: <a href=""https://blog.powerupcloud.com/kubernetes-pod-management-using-fluentd-as-a-sidecar-container-and-prestop-lifecycle-hook-part-iv-428b5f4f7fc7"" rel=""nofollow noreferrer"">https://blog.powerupcloud.com/kubernetes-pod-management-using-fluentd-as-a-sidecar-container-and-prestop-lifecycle-hook-part-iv-428b5f4f7fc7</a></li>
<li>fluentd-bit: <a href=""https://github.com/leahnp/fluentbit-sidecar"" rel=""nofollow noreferrer"">https://github.com/leahnp/fluentbit-sidecar</a></li>
<li>filebeat: <a href=""https://www.elastic.co/beats/filebeat"" rel=""nofollow noreferrer"">https://www.elastic.co/beats/filebeat</a></li>
</ul>

<p>Datadog supports them</p>

<ul>
<li><a href=""https://docs.datadoghq.com/integrations/fluentd/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/fluentd/</a></li>
<li><a href=""https://docs.datadoghq.com/integrations/filebeat/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/filebeat/</a></li>
</ul>
"
Datadog,63974474,63650568,0,"2020/09/20, 03:18:54",False,"2020/09/20, 03:18:54",3852,1601506,2,"<p>You need to tell Datadog that you're interested in that content by creating a facet from the field. Click a log message, mouse over the attribute name, click the gear on the left, then Create facet for @...</p>
<p>For logs indexed after you create the facet, you can search with <code>@fieldName:text*</code>, where <code>fieldName</code> is the name of your field. You'll need to re-hydrate (reprocess) earlier logs to make them searchable.</p>
<p>You won't need to create a facet if you use fields from the <a href=""https://docs.datadoghq.com/logs/processing/attributes_naming_convention/#default-standard-attribute-list"" rel=""nofollow noreferrer"">standard attributes list</a>.</p>
"
Datadog,66879609,63650568,0,"2021/03/31, 02:09:46",False,"2021/04/01, 21:13:00",11,6062224,1,"<p>The error message itself is not a good fit to be defined as a facet.</p>
<p>If you are using JSON and want the main message (say from a <code>msg</code> json field) to be searchable in the Datadog <code>content</code> field. Instead of making
facet for <code>msg</code>, you can define a &quot;Message Remapper&quot; in the log configuration to map it to the <code>Content</code>. And then you can do wildcard searches.</p>
<p><a href=""https://i.stack.imgur.com/or0ld.png"" rel=""nofollow noreferrer"">log config screenshot</a></p>
"
Datadog,62818348,62818107,3,"2020/07/09, 18:25:43",True,"2020/07/13, 17:33:38",6777,7875623,3,"<pre><code>public class MyLoggingSentEventNotifer extends EventNotifierSupport {

  public void notify(EventObject event) throws Exception {

    if (event instanceof ExchangeCompletedEvent) {;
      ExchangeCompletedEvent completedEvent = (ExchangeCompletedEvent) event;
      Exchange exchange = completedEvent.getExchange();
      String routeId = exchange.getFromRouteId();
      Date created = ((ExchangeCompletedEvent) event).getExchange()
                        .getProperty(Exchange.CREATED_TIMESTAMP, Date.class);
      // calculate elapsed time
      Date now = new Date();
      long elapsed = now.getTime() - created.getTime();
      log.info(&quot;Took &quot; + elapsed + &quot; millis on the route : &quot; + routeId);
    }

 }

 public boolean isEnabled(EventObject event) {
        // we only want the sent events
        return event instanceof ExchangeSentEvent;
 }

 protected void doStart() throws Exception {
        // noop
 }

 protected void doStop() throws Exception {
        // noop
 }

}

context.getManagementStrategy().addEventNotifier(new MyLoggingSentEventNotifer());
</code></pre>
<p><strong>Reference</strong></p>
<p><a href=""https://people.apache.org/%7Edkulp/camel/eventnotifier-to-log-details-about-all-sent-exchanges.html"" rel=""nofollow noreferrer"">https://people.apache.org/~dkulp/camel/eventnotifier-to-log-details-about-all-sent-exchanges.html</a></p>
<p><strong>Update</strong></p>
<p>The <code>Exchange.CREATED_TIMESTAMP</code> is no longer stored as exchange property, but you should use the <code>getCreated</code> method on Exchange.</p>
"
Datadog,63516418,62298190,0,"2020/08/21, 07:12:56",False,"2020/08/21, 07:12:56",11,4392334,1,"<p>If you put in ECS Task Definition (sample from json version, but in UI also possible to setup), you should be able to configure container logs:</p>
<pre><code>&quot;logConfiguration&quot;: {
   &quot;logDriver&quot;: &quot;json-file&quot;,
   &quot;options&quot;: {
       &quot;max-size&quot;: &quot;10m&quot;,
       &quot;max-file&quot;: &quot;3&quot;
   }
}
</code></pre>
"
Datadog,62069822,62056153,2,"2020/05/28, 19:31:43",False,"2020/05/28, 19:38:33",1119,6490744,0,"<p>Since your question says <em>is there a way to inspect inside/after each task completes</em> - I'm assuming you haven't tried this celery-result-backend stuff. So you could check out this feature which is provided by Celery itself : <em><code>Celery-Result-Backend / Task-result-Backend</code></em> .
It is very useful for storing results of your celery tasks.
Read through this => <a href=""https://docs.celeryproject.org/en/stable/userguide/configuration.html#task-result-backend-settings"" rel=""nofollow noreferrer"">https://docs.celeryproject.org/en/stable/userguide/configuration.html#task-result-backend-settings</a> </p>

<hr>

<p>Once you get an idea of how to setup this result-backend, Search for <code>result_extended</code> key (in the same link) to be able to add <code>queue-names</code> in your task return values.</p>

<p>Number of options are available - Like you can setup these results to go to any of these :</p>

<pre><code>Sql-DB / NoSql-DB / S3 / Azure / Elasticsearch / etc 
</code></pre>

<hr>

<p>I have made use of this <em><code>Result-Backend</code></em> feature with <em><code>Elasticsearch</code></em> and this how my task results are stored :</p>

<p><a href=""https://i.stack.imgur.com/hnn3S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hnn3S.png"" alt=""enter image description here""></a></p>

<p>It is just a matter of adding few configurations in <code>settings.py</code> file as per your requirements. Worked really well for my application. And I have a weekly cron that clears only <code>successful results</code> of tasks - since we don't need the results anymore - and I can see only <code>failed results</code> <em>(like the one in image).</em></p>

<p>These were main keys for my requirement : <code>task_track_started</code> and <code>task_acks_late</code> along with <code>result_backend</code></p>
"
Datadog,60517002,60516923,1,"2020/03/04, 01:27:31",True,"2020/03/04, 01:27:31",25945,11923999,1,"<p>Try <code>recover</code> to catch all panics and log them. Without that, it'll write the panic msg to stderr:</p>

<pre><code>func main() {
   defer func() {
        if r := recover(); r != nil {
            logger.Errorf(""Panic: %v"", r)
            os.Exit(1)
        }
    }()
    // rest of main here
}
</code></pre>
"
Datadog,59605166,59549130,2,"2020/01/06, 01:37:29",True,"2020/01/06, 01:37:29",5837,9090751,3,"<p>You can override the default <code>TracingInstrumentation</code> with your own implementation. It will be picked automatically due to the @ConditionalOnMissingBean annotation in the <a href=""https://github.com/graphql-java-kickstart/graphql-spring-boot/blob/08395c9c458f00cdc68076ed4926162f063b1251/graphql-spring-boot-autoconfigure/src/main/java/com/oembedler/moon/graphql/boot/GraphQLInstrumentationAutoConfiguration.java#L45"" rel=""nofollow noreferrer"">GraphQLInstrumentationAutoConfiguration</a> class. Here is a simple example that adds two custom metrics: <strong>graphql.counter.query.success</strong> and <strong>graphql.counter.query.error</strong>:</p>

<pre class=""lang-java prettyprint-override""><code>@Component
public class CustomMetricsInstrumentation extends TracingInstrumentation {

    private static final String QUERY_STATUS_COUNTER_METRIC_NAME = ""graphql.counter.query"";
    private static final String OPERATION_NAME_TAG = ""operationName"";
    private static final String UNKNOWN_OPERATION_NAME = ""unknown"";

    private MeterRegistry meterRegistry;

    public CustomMetricsInstrumentation(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
    }

    @Override
    public CompletableFuture&lt;ExecutionResult&gt; instrumentExecutionResult(ExecutionResult executionResult,
                                                                        InstrumentationExecutionParameters parameters) {

        String status = CollectionUtils.isEmpty(executionResult.getErrors()) ? ""success"" : ""error"";
        String operation = parameters.getOperation() != null ? parameters.getOperation() : UNKNOWN_OPERATION_NAME;
        Collection&lt;Tag&gt; tags = Arrays.asList(Tag.of(OPERATION_NAME_TAG, operation));

        meterRegistry.counter(QUERY_STATUS_COUNTER_METRIC_NAME + ""."" + status, tags).increment();

        return super.instrumentExecutionResult(executionResult, parameters);
    }
}
</code></pre>

<p>My application.yaml, just in case:</p>

<pre><code>graphql:
  servlet:
    tracing-enabled: true
    actuator-metrics: true
management:
  endpoint:
  metrics:
    enabled: true
  endpoints:
    web:
      exposure:
        include: health,metrics
</code></pre>

<p>I'm using spring-boot-starter-parent:2.2.2.RELEASE, graphql-spring-boot-starter:6.0.0</p>

<p>I hope it helps.</p>
"
Datadog,66324527,59102943,0,"2021/02/23, 00:44:33",False,"2021/02/23, 00:44:33",1,15263108,0,"<p>Avg CPU usage may not give better view. Check if max CPU utilization is getting around 100%. If so, you may need to optimize on ES side.</p>
"
Datadog,55834007,55832300,2,"2019/04/24, 18:48:15",False,"2019/04/24, 18:48:15",5656,11374921,1,"<p>You should understand how cluster autoscaler works. It is responsible <strong>only</strong> for adding or removing nodes. It is not responsible for creating or destroying pods. So in your case cluster autoscaler is not doing anything because it's useless. Even if you add one more node - there will be still a requirement to run DaemonSet pods on nodes where is not enough CPU. That's why it is not adding nodes.</p>

<p>What you should do is to manually remove some pods from occupied nodes. Then it will be able to schedule DaemonSet pods.</p>

<p>Alternatively you can reduce CPU requests of Datadog to, for example, 100m or 50m. This should be enough to start those pods.</p>
"
Datadog,57509622,55832300,3,"2019/08/15, 15:24:55",True,"2019/08/19, 13:40:01",1425,1837991,4,"<p>You can add priorityClassName to point to a high priority PriorityClass to your DaemonSet. Kubernetes will then remove other pods in order to run the DaemonSet's pods. If that results in unschedulable pods, cluster-autoscaler should add a node to schedule them on.</p>

<p>See <a href=""https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/"" rel=""nofollow noreferrer"">the docs</a> (Most examples based on that) (For some pre-1.14 versions, the apiVersion is likely a beta (1.11-1.13) or alpha version (1.8 - 1.10) instead)</p>

<pre class=""lang-yaml prettyprint-override""><code>apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: ""High priority class for essential pods""
</code></pre>

<p>Apply it to your workload</p>

<pre class=""lang-yaml prettyprint-override""><code>---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: datadog-agent
spec:
  template:
    metadata:
      labels:
        app: datadog-agent
      name: datadog-agent
    spec:
      priorityClassName: high-priority
      serviceAccountName: datadog-agent
      containers:
      - image: datadog/agent:latest
############ Rest of template goes here
</code></pre>
"
Datadog,54660782,54660692,0,"2019/02/13, 02:35:57",True,"2019/02/13, 03:15:24",185548,152948,1,"<p>Here are two ways that work:</p>

<p>1.</p>

<pre><code>jq '.services
    | . as $services
    | keys_unsorted
    | map( select($services[.] | has(""build"")) )'
</code></pre>

<p>(Drill down to <code>.services</code>, remember it as <code>$services</code> for later use, get the list of keys, and select the ones such that the corresponding value in <code>$services</code> has a <code>build</code> key).</p>

<p>2.</p>

<pre><code>jq '.services
    | to_entries
    | map( select(.value | has(""build"")) | .key)'
</code></pre>

<p>(Drill down to <code>.services</code>, convert to a list of <code>{""key"": ..., ""value"": ...}</code> objects, select the ones where the <code>.value</code> has a <code>build</code> key, and return the <code>.key</code> for each).</p>

<p>The second is probably more idiomatic jq, but the first provides an interesting way to think about the problem as well.</p>
"
Datadog,54660903,54660692,0,"2019/02/13, 02:51:04",False,"2019/02/13, 08:06:26",70657,997358,2,"<p>Here's a third approach, notable for being oblivious to the upper reaches:</p>

<pre><code>[(paths(scalars) 
  | select(.[-1] == ""build"")) as $p
 | getpath($p)]
</code></pre>
"
Datadog,53328483,53323761,1,"2018/11/15, 23:58:41",False,"2018/11/15, 23:58:41",1283,10657880,0,"<p>Have you tried to mock the <code>datalog</code> module inside your function <code>test</code>? As long as your other scripts are not running concurrently with your test, this may work. That way the mock itself will be set only when the function is called, instead of being set in your script scope.</p>
"
Datadog,53363259,53323761,0,"2018/11/18, 18:50:17",False,"2018/11/18, 18:50:17",1223,1939996,0,"<p>You could use <code>unittest.mock.patch</code>. If you are using pytest you can do the same with the <code>monkeypatch</code> fixture. </p>

<pre><code>from datadog import statsd
from unittest.mock import Mock, patch

def some_function():
    statsd.increment()

def test_some_function():
    with patch('datadog.statsd', Mock()) as mock_statsd:
        some_function()
    mock_statsd.increment.assert_called()

test_some_function()
</code></pre>
"
Datadog,52703220,52703024,0,"2018/10/08, 16:16:52",False,"2018/10/08, 16:16:52",9507,1312478,0,"<p>I have used many of solutions you mentioned. Splunk is good but it becomes really expensive if you have huge amount of data. You could have always used Cloudwatch Logs but it doesn't give you so much on visual part..</p>

<p>I will recommend ELK (ElasticSearch, Logstash, Kibana) stack. It is a very standard solution; in which logs are stored in Elastic Search. Kibana is used for visualization of logs. This works in almost real time.</p>

<p>If you have very specific dashboards; then you can always create custom dashboards using some front end technologies like AngularJS etc. but if visual part is really huge and very flexible then I feel ELK is better.</p>
"
Datadog,52703399,52703024,0,"2018/10/08, 16:27:05",False,"2018/10/08, 16:27:05",1508,1623047,0,"<p>ELK (ElasticSearch, Logstash, Kibana) stack is a really good solution for what you are looking for, but in some cases ELK is not going to be able to get some metrics, in this case you have some solutions like create your own <strong>beat</strong> program to get the information or use another program to gather this metrics like Apache NiFi.</p>
"
Datadog,52703411,52703024,0,"2018/10/08, 16:27:49",False,"2018/10/08, 16:27:49",3533,6619626,1,"<p>You can use AWS CloudWatch, create a log stream for each of your application or service.
Define your custom metrics, create a dashboard and alert.</p>

<p>It's not limited to AWS things; you can use CloudWatch log agent for On-premises services or software on your local network.</p>

<p>For more information read the following article by Jeff Barr</p>

<p><a href=""https://aws.amazon.com/blogs/aws/cloudwatch-log-service/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/aws/cloudwatch-log-service/</a></p>

<p>and </p>

<p><a href=""https://aws.amazon.com/blogs/aws/improvements-to-cloudwatch-logs-dashboards/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/aws/improvements-to-cloudwatch-logs-dashboards/</a></p>

<p>FYI: we already monitor a lot of application and service inside and outside of AWS by CloudWatch, and it works like a charm.</p>
"
Datadog,52692334,52689144,5,"2018/10/07, 22:57:28",True,"2018/10/08, 13:07:16",3260,950762,0,"<p>It seems normal because <code>Ansible</code> does not refer to Python virtual environment in your case:</p>

<pre><code>ansible python module location = /usr/local/Cellar/ansible/2.6.3/libexec/lib/python2.7/site-packages/ansible
executable location = /usr/local/bin/ansible-playbook
</code></pre>

<p>In <code>virtualenv</code> non-installed packages are initialized from real system environment. So you can achieve it by setting up <code>Ansible</code> within <code>virtualenv</code>  </p>

<p>Have a look at this example:</p>

<pre><code>my_user@my_machine:~$ ansible --version
ansible 2.6.3
  ansible python module location = /usr/local/lib/python2.7/dist-packages/ansible
  executable location = /usr/local/bin/ansible
  python version = 2.7.15rc1 (default, Apr 15 2018, 21:51:34) [GCC 7.3.0]
</code></pre>

<p>After installation of <code>Ansible</code> in <code>virtualenv</code></p>

<pre><code>(py_venv) my_user@my_machine:~$ pip install ansible==2.5.5
</code></pre>

<p><code>Ansible</code> refers to the paths of Python virtual environment:</p>

<pre><code>(py_venv) my_user@my_machine:~$ ansible --version
ansible 2.5.5
  ansible python module location = /home/my_user/py_venv/local/lib/python2.7/site-packages/ansible
  executable location = /home/my_user/py_venv/bin/ansible
  python version = 2.7.15rc1 (default, Apr 15 2018, 21:51:34) [GCC 7.3.0]
</code></pre>

<p>ps: Need to deactivate and activate again the <code>virtualenv</code> once to load the <code>Ansible</code> from virtual environment after the installation.</p>
"
Datadog,52834860,51690297,0,"2018/10/16, 14:52:14",False,"2018/10/16, 14:52:14",160,9716385,2,"<p>To get you started: Create a timelion expression: </p>

<pre><code>.es(index=metricbeat-*, metric='avg:system.process.cpu.total.pct',split='beat.name:10').trim().fit(scale).multiply(1000).movingaverage(5).label(label='cpu usage % : ',regex='.*name.*:').label(label='',regex='&gt;.*')
</code></pre>
"
Datadog,52959391,51690297,1,"2018/10/24, 03:10:30",False,"2018/10/24, 03:10:30",2567,7983309,4,"<p>What you are looking for is achievable using Visual Builder visualization</p>

<p>See <a href=""https://www.elastic.co/guide/en/kibana/6.1/time-series-visualizations.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/kibana/6.1/time-series-visualizations.html</a></p>

<p>Aggregate <code>Max</code> or <code>Avg</code> on <code>system.cpu.total.pct</code></p>

<p>Group By <code>Terms</code> By <code>beat.hostname.keyword</code></p>

<p>The visualization will show CPU usage in % for all hosts sending metrics to your cluster. If you add more hosts those will show up too!</p>
"
Datadog,55875339,51690297,0,"2019/04/27, 00:53:24",False,"2019/04/27, 00:53:24",12414,33204,0,"<p>This is just to illustrate @ben5556's answer with an image.</p>

<p><a href=""https://i.stack.imgur.com/gaZEB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gaZEB.png"" alt=""ss""></a></p>

<h3>Legend</h3>

<ol>
<li>Aggregation set to <code>Max</code></li>
<li>Field to collect data from <code>system.process.summary.total</code></li>
<li>How to group by <code>Term</code></li>
<li>What to group by (the ""Term"") - <code>beat.hostname</code></li>
<li>How to order these terms - order by the <code>Term</code></li>
</ol>

<p><strong>NOTE:</strong> The ""Term"" is <code>beat.hostname</code>.</p>
"
Datadog,53874603,50794695,2,"2018/12/20, 21:06:35",False,"2018/12/20, 21:06:35",101,4771526,0,"<p>sorry for the delay.</p>

<p>From your error log I can't see any issues on recovery being thrown but I either don't see any connection attempts. I wonder if you have some issues with data in the group replication relay logs... </p>

<p>I suggest you open a bug if the problem still persists.
As a workaround you can try to reset the applier channel before ""START GROUP_REPLICATION"" </p>

<blockquote>
  <p>RESET SLAVE ALL FOR CHANNEL ""group_replication_applier"";</p>
</blockquote>
"
Datadog,47014916,47013899,0,"2017/10/30, 14:10:13",False,"2017/10/30, 14:10:13",3148,7925197,0,"<p>The telegraf/influxdb/grafana stack can monitor space left on disc. Kapacitor can also be added if you want alerts. If you want to specify a limit, you have to use a dedicated partition / mount point or a btrfs subvolume with quotas.</p>
"
Datadog,47016362,47013899,0,"2017/10/30, 15:22:49",False,"2017/10/30, 15:22:49",21,7688163,0,"<p>Another option is to make cron job to clean up unused docker images, unused docker volume, and exited docker container. I use this method myself.</p>
"
Datadog,46739062,46716705,0,"2017/10/14, 01:56:40",False,"2017/10/14, 01:56:40",3994,505196,0,"<p>A better approach than using DaemonSets to run your application would be to use a Deployment so that you don't tie your application to the number of nodes in your cluster.</p>

<p>You can then deploy the datadog agent image as a DaemonSet with a set <code>spec.template.spec.affinity</code> that selects nodes with a pod of your application running. This will make sure you have a datadog agent in every node where your application runs.</p>

<p>Another option is to deploy the datadog agent container in the same pod as your application container. In this case you can reach the agent through localhost and scale together, but might end up with more than an agent per node, hence my preference for a DaemonSet with an affinity.</p>
"
Datadog,46740957,46716705,0,"2017/10/14, 07:44:41",False,"2017/10/14, 07:44:41",31231,242493,0,"<p>My team ran it as a daemon set for the purposes of collecting node metrics, but only exposed it as a normal cluster IP service for the purposes of programmatically sending it data from other apps in the cluster. You don't need to expose it on a node port unless you need to access it from outside the cluster and don't have a service-aware load balancer like an ingress controller. (That would be quite a strange use case, so chances are you don't need to expose it on a node port.)</p>
"
Datadog,46411356,46407772,1,"2017/09/25, 20:56:47",True,"2017/09/25, 20:56:47",17772,1494519,3,"<pre><code>queue:default
</code></pre>

<p>is the name of the default queue.  As you state, it is ""queue:$NAME"" but namespaces (if you use them (please don't)) will also prefix the key.</p>
"
Datadog,44550799,44550138,0,"2017/06/14, 20:06:35",True,"2017/06/14, 20:06:35",1028,8020142,0,"<p>You can not set a name on the instances of docker that manages amazon. The namespaces it uses are to be able to handle the scaling of the service. Think that if you write the name and then the service you ask for more than one instance of your application, amazon could not instantiate it on the same node.</p>

<p>I hope the explanation has served.</p>
"
Datadog,44551679,44550138,0,"2017/06/14, 21:02:08",False,"2017/06/14, 21:02:08",3575,112821,4,"<p>No, there is not a way to control the name used for the container in Amazon ECS.  ECS picks a random name designed to avoid conflicts (since names must be unique in Docker; you can't have two containers with the same name) and you can see the code <a href=""https://github.com/aws/amazon-ecs-agent/blob/v1.14.3/agent/engine/docker_task_engine.go#L581-L590"" rel=""nofollow noreferrer"">here</a>.</p>

<p>However, ECS does give you a few things that might be able to help you.  There are automatically-assigned Docker labels for the task ARN, the container name in your task definition, the task definition family, the task definition revision, and the cluster; see <a href=""https://github.com/aws/amazon-ecs-agent/blob/v1.14.3/agent/engine/docker_task_engine.go#L575-L579"" rel=""nofollow noreferrer"">here</a>.  Additionally, you can assign your own custom Docker labels through the task definition.</p>
"
Datadog,39333248,39323306,9,"2016/09/05, 17:58:10",True,"2016/09/05, 17:58:10",2479,5027078,2,"<p>I just had some difficulties to determine what is exactly your second separator. you text example shows '·', but when I checked what is just after 'Elberg"" and before '2nd...', I found 4 characters : code 32 (space), code 194 (¬), code 183 (∑), code 32 (space).</p>

<p>In the script bellow, I have used the code 194. it works when I cut/paste your text example into a file. Here is the script :</p>

<pre><code>set theFile to (""/Users/RaquelBianca/Desktop/ExtractTextOutput2.txt"")
-- your separator seems to be code 32 (space), code 194 (¬), code 183 (∑), code 32 (space)
set Separator to ASCII character 194 -- is it correct ?

set theFileContents to read theFile
set myAuthor to """"
set AppleScript's text item delimiters to {""Job posted by ""}
if (count of text item of theFileContents) is 2 then
set Part2 to text item 2 of theFileContents -- this part starts just after ""Job posted by ""
set AppleScript's text item delimiters to {Separator}
set myAuthor to text item 1 of Part2
end if

log ""result=//"" &amp; myAuthor &amp; ""//"" -- show the result in variable myAuthor
</code></pre>

<p>Note : if the text does not contain ""Job posted by "", then myAuthor is ''.</p>
"
Datadog,39334390,39323306,0,"2016/09/05, 19:17:04",False,"2016/09/05, 19:17:04",1031,5390105,0,"<p>You had the right idea to use <code>AppleScript's text item delimiters</code>, but the way you tried to extract the name was giving you trouble. First, though, I'll go through some things you can do to improve your script:</p>

<pre><code>set all_lines to every text item of theFileContents
repeat with the_line in all_lines
    if ""Job posted by"" is not in the_line then
    set output to output &amp; the_line
else
    …
end repeat
</code></pre>

<p>There's no need to break the file contents into lines; AppleScript can operate on entire paragraphs or more, if desired.</p>

<p>Removing these unnecessary steps (and adding new ones to make it work on the entire file) shrinks the script considerably:</p>

<pre><code>set theFile to (""/Users/RaquelBianca/Desktop/ExtractTextOutput2.txt"")
set theFileContents to read theFile

set output to {}
set od to AppleScript's text item delimiters

if ""Job posted by"" is in theFileContents
    set AppleScript's text item delimiters to {""Job posted by""}
    set latter_part to last text item of theFileContents
    set AppleScript's text item delimiters to {"" ""}
    set last_word to last text item of latter_part
    set output to output &amp; (""$ "" &amp; last_word as string)
else
    display alert ""Poster of job listing not found""
    set output to theFileContents
end if

set AppleScript's text item delimiters to od
return output
</code></pre>

<p>This right here is what's giving you wrong output:</p>

<pre><code>set last_word to last text item of latter_part
set output to output &amp; (""$ "" &amp; last_word as string)
</code></pre>

<p>This is incorrect. It's not the <em>last</em> word you want; that's the last word of the file! To extract the poster of the job listing, change it to the following:</p>

<pre><code>repeat with theWord in latterPart
    if the first character in theWord is ""¬"" then exit repeat
    set output to output &amp; theWord
end repeat
</code></pre>

<p>Due to AppleScript's weird Unicode handling, for whatever reason the dot (·) that separates the name from the other text is converted to ""¬∑"" when run though the script. So, we look for ""¬"" instead.</p>

<p>Some last code fixes:</p>

<p>Some of your variable names use <code>the_snake_case</code>, while others use <code>theCamelCase</code>. It's generally a good idea to use one convention or another, so I fixed that, too.</p>

<p>I assumed you wanted that dollar sign in the output for whatever reason, so I kept it in. If you don't want it, just replace <code>set output to ""$ ""</code> with <code>set output to """"</code>.</p>

<p>So, your final, working script looks like this:</p>

<pre><code>set theFile to ""/Users/RaquelBianca/Desktop/ExtractTextOutput2.txt""
set theFileContents to read theFile as text

set output to ""$ ""
set od to AppleScript's text item delimiters

if ""Job posted by"" is in theFileContents then
    set AppleScript's text item delimiters to {""Job posted by""}
    set latterPart to last text item of theFileContents
    set AppleScript's text item delimiters to {"" ""}
    repeat with theWord in latterPart
        if the first character in theWord is ""¬"" then exit repeat
        set output to output &amp; theWord
    end repeat
else
    display alert ""Poster of job listing not found""
    set output to theFileContents
end if

set AppleScript's text item delimiters to od
return output
</code></pre>
"
Datadog,38986680,38986190,0,"2016/08/17, 04:08:24",False,"2016/08/17, 04:08:24",2571,2149974,0,"<p>I agree it's hard to find, the closest one I can find is this</p>

<pre><code>google.appengine.api.modules.modules.get_num_instances(module=None, version=None)source
</code></pre>

<blockquote>
  <p>Return the number of instances that are set for the given module
  version.</p>
</blockquote>

<p>This is only valid for fixed modules, an error will be raised for automatically-scaled modules. Support for automatically-scaled modules may be supported in the future.</p>

<p><a href=""https://cloud.google.com/appengine/docs/python/refdocs/google.appengine.api.modules.modules"" rel=""nofollow"">https://cloud.google.com/appengine/docs/python/refdocs/google.appengine.api.modules.modules</a></p>

<p>Btw you can also have monitoring from the StackDriver which has metric for total instance</p>
"
Datadog,38999762,38986190,5,"2016/08/17, 17:42:16",False,"2016/08/17, 17:42:16",7229,178236,4,"<p>You should also be able to use the recently GA'd App Engine Admin API to figure this out.  The nice thing about the admin API is that it's going to work for both standard and flexible:
<a href=""https://cloud.google.com/appengine/docs/admin-api/"" rel=""nofollow"">https://cloud.google.com/appengine/docs/admin-api/</a></p>

<p>Here's the endpoint that returns all of the instances for a given service/version:</p>

<p><a href=""https://cloud.google.com/appengine/docs/admin-api/reference/rest/v1/apps.services.versions.instances/list"" rel=""nofollow"">https://cloud.google.com/appengine/docs/admin-api/reference/rest/v1/apps.services.versions.instances/list</a></p>

<p>Depending on the language you're using, there's usually a nice wrapper in the form of a ""Google API client"" + language library.  </p>

<p>Hope this helps!</p>
"
Datadog,39003441,38986190,0,"2016/08/17, 21:02:18",False,"2016/08/17, 21:02:18",2646,1543502,0,"<p>If you're trying to collect stats, you might want to use the <a href=""https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.timeSeries/list"" rel=""nofollow"">Stackdriver Monitoring API</a> to collect the timeseries values that Google has already aggregated.</p>

<p>In particular, the list of <a href=""https://cloud.google.com/monitoring/api/metrics#gcp-appengine"" rel=""nofollow"">App Engine Metrics is here</a>. For example, <code>system/instance_count</code> is the metric indicating the number of instances App Engine is running.</p>
"
Datadog,39007294,38986190,0,"2016/08/18, 01:36:25",True,"2016/08/19, 08:04:25",10698,30997,1,"<p>I hate it when SO questions only end up with partial answers, so here's a complete, working example. If you paste it into your interactive console, it should work for you. (Don't forget to set the <code>versionsId</code> to whatever your default app version is. If you know how I can get it to use the default version, please post a comment. 'default', '*', 'any', etc. all no da workie.) </p>

<p>Strictly achieved by trial and error:</p>

<pre><code>import httplib2
import logging
import time
import webapp2

from google.appengine.api.app_identity import app_identity
from googleapiclient.discovery import build
from oauth2client.client import GoogleCredentials

credentials = GoogleCredentials.get_application_default()
service =  build('appengine', 'v1', credentials=credentials)
appsId = app_identity.get_application_id()
version_list = service.apps().services().versions().list(
        servicesId='default', appsId=appsId).execute()

for version in version_list['versions']:
    if not version['id'].startswith('ah-builtin'):
        rpc_result = service.apps().services().versions().instances().list(
                versionsId=version['id'], servicesId='default',
                appsId=appsId).execute()

        if rpc_result:
            instance_list = rpc_result['instances']
        else:
            instance_list = []
        print version['id'], len(instance_list)
</code></pre>
"
Datadog,35230909,35219961,0,"2016/02/05, 20:20:41",True,"2016/02/05, 20:20:41",94,2207886,2,"<p>The easiest way to proceed is to create a custom check. You can read up on this here: <a href=""http://docs.datadoghq.com/guides/agent_checks/"" rel=""nofollow"">http://docs.datadoghq.com/guides/agent_checks/</a>. There isn't a way to take a pre-existing Nagios or Sensu plugin and have it work as is with Datadog, but looking at one of the delayed_job plugins on Github, looks like it should be pretty easy to convert to a Datadog check. If you have any issues, reach out to support either via email or #datadog on IRC.</p>
"
Datadog,43698915,43224591,0,"2017/04/29, 21:05:40",False,"2017/04/29, 21:05:40",1371,5540166,1,"<ol>
<li><p>The first step will be to make sure you have the <a href=""https://app.datadoghq.com/account/settings#agent"" rel=""nofollow noreferrer"">datadog agent running</a>, and that the APM component of it is running and ready to receive trace data from your applications (<a href=""https://github.com/DataDog/dd-agent/blob/5.13.x/datadog.conf.example#L28"" rel=""nofollow noreferrer"">this option in your datadog.conf</a>, which must be set to ""true"").</p></li>
<li><p>Second, you'll want to install the appropriate library(ies) for the languages your applications are written in. You can find them all listed in your datadog account on this page: <a href=""https://app.datadoghq.com/apm/docs"" rel=""nofollow noreferrer"">https://app.datadoghq.com/apm/docs</a></p></li>
<li><p>Third, once the trace libraries are installed, you'll want to add trace integrations for the tools you're interested in collecting APM data on, and the instructions for those will be found in each library's docs. (E.g, <a href=""http://pypi.datadoghq.com/trace/docs/"" rel=""nofollow noreferrer"">Python</a>, <a href=""http://www.rubydoc.info/github/DataDog/dd-trace-rb/"" rel=""nofollow noreferrer"">Ruby</a>, and <a href=""https://godoc.org/github.com/DataDog/dd-trace-go/tracer"" rel=""nofollow noreferrer"">Go</a>)</p></li>
</ol>

<p>The integraitons will be a fairly quick way to get pretty granular spans on where your applications have higher latency, errors, etc. If from there you'd like to go even further, each library's docs also have instructions on how you can write your own custom tracing functions to expose more info on your custom applications--that's a little more work, but is fairly straight-forward. You'll probably want to add those bit-by-bit as you go.</p>

<p>Then you'd be all set, I think. You'll be tracing services, resources to get the latency, request-count, and error-count of your application requests, and you can drill down to the flame-graphs to further understand what requests spend the most time where in your applications.</p>

<p>Looking back now, seems like they made some recent changes to the setup process that makes it even easier to get the web framework and database integrations added if you're using Python. They've even got a command line tool in their <a href=""http://pypi.datadoghq.com/trace/docs/#get-started"" rel=""nofollow noreferrer"">get-started section now</a>.</p>

<p>Hope this helps! And reach out to their support team (support@datadoghq.com) if you run into issues along the way--they're always happy to lend a hand.</p>
"
Datadog,52828393,52828258,1,"2018/10/16, 08:20:06",False,"2018/10/16, 08:44:48",21,10510929,2,"<p>You can actually just use:</p>

<pre><code>EOF
@slack-datadog-{{environment.name}}
EOF
</code></pre>

<p>Datadog's monitor templating feature will fill in the blank and forward to the relevant channel as long as you whitelisted it in the integrations tile for Slack.</p>
"
Datadog,61282340,52828258,0,"2020/04/18, 02:32:29",False,"2020/04/18, 02:32:29",5547,296829,1,"<p>Starts to get messy, but you could nest two ""does not"" conditional variables, like this:</p>

<pre><code> message = &lt;&lt;EOF
{{#is_match ""environment.name"" ""production""}}
   {{#is_alert}} @slack-datadog-production {{/is_alert}}
{{/is_match}}
{{#is_match ""environment.name"" ""uat""}}
   {{#is_alert}} @slack-datadog-uat {{/is_alert}}
{{/is_match}}

{{^is_match ""environment.name"" ""production""}}
   {{^is_match ""environment.name"" ""uat""}}
      {{#is_alert}} @slack-datadog {{/is_alert}}
   {{/is_match}}
{{/is_match}}
EOF
</code></pre>
"
Datadog,66324527,59102943,0,"2021/02/23, 00:44:33",False,"2021/02/23, 00:44:33",1,15263108,0,"<p>Avg CPU usage may not give better view. Check if max CPU utilization is getting around 100%. If so, you may need to optimize on ES side.</p>
"
Datadog,62680072,62436021,3,"2020/07/01, 18:20:46",True,"2020/07/01, 18:20:46",844,11921495,3,"<p>This was a issue with deployed DataDog daemonset for me:</p>
<p>What I did to resolve:</p>
<ol>
<li><p>Check daemonset if it exists or not:</p>
<pre><code>kubectl get ds -n datadog
</code></pre>
</li>
<li><p>Edit the datadog daemonset:</p>
<pre><code>kubectl edit ds datadog -n datadog
</code></pre>
</li>
<li><p>In the opened yaml, add</p>
<pre><code>- name: DD_KUBELET_TLS_VERIFY
  value: &quot;false&quot;
</code></pre>
<p>Add this in <strong>env:</strong> tag for all places. For me there were 4 places which are having DD tags in the yaml.</p>
</li>
<li><p>Save and close it. The daemonset will restart. And the application will start getting traced.</p>
</li>
</ol>
"
Datadog,67104589,62436021,0,"2021/04/15, 11:08:06",False,"2021/04/15, 11:08:06",1929,2179157,0,"<p>If you are using the Helm chart, you can overwrite on the values:</p>
<pre class=""lang-yaml prettyprint-override""><code>## https://github.com/DataDog/helm-charts/blob/master/charts/datadog/values.yaml

datadog:  
  # kubelet configuration
  kubelet:
    # datadog.kubelet.tlsVerify -- Toggle kubelet TLS verification
    # @default -- true
    tlsVerify:  false
</code></pre>
"
Datadog,64803224,60491872,0,"2020/11/12, 13:39:22",False,"2020/11/12, 13:39:22",129,6103623,1,"<p>hey!</p>
<p>Sorry in advance if my answer isn't correct because <strong>I'm a complete newby</strong> in kuber and helm and I can't make sure that it will help, but maybe it helps.</p>
<p>So, the problem, as I can understand, in the resulting <strong>ConfigMap</strong> configuration. From my expirience, I faced the same with the following config:</p>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
  labels:
    group: mock
data:
  APP_NAME: my-mock
  APP_PORT: 8080
  APP_PATH: /api
</code></pre>
<p>And I could solve it only by surrounding with quotes all the values:</p>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
  labels:
    group: mock
data:
  APP_NAME: &quot;my-mock&quot;
  APP_PORT: &quot;8080&quot;
  APP_PATH: &quot;/api&quot;
</code></pre>
"
Datadog,59159712,59145932,0,"2019/12/03, 16:50:16",False,"2019/12/03, 16:50:16",1038,796064,2,"<p>Per Yuri's suggestion, I found the culprit, and this is how (thanks to Google Support for walking me through this):</p>

<ul>
<li>In GCP Cloud Console, navigate to 'APIs &amp; Services' -> Library</li>
<li>Search for 'Strackdriver Monitoring Api' and click</li>
<li>Click 'Manage' on the next screen</li>
<li>Click 'Metrics' from the left-hand side menu</li>
<li>In the 'Select Graphs' dropdown, select ""Traffic by Credential"" and click 'OK'</li>
</ul>

<p>This showed me a graph making it clear just about all of my requests were coming from a credential named <code>datadog-metrics-collection</code>, a service account I'd set up previously to collect GCP metrics and emit to Datadog. </p>
"
Datadog,59164889,59145932,2,"2019/12/03, 22:32:13",False,"2019/12/03, 22:32:13",136,11980517,0,"<p>Considering the answer posted and question, If we think we do not need Stackdriver monitoring, we can disable stackdriver monitoring API using bellow steps:</p>

<ol>
<li>From the Cloud Console,go to APIs &amp; Services. </li>
<li>Select Stackdriver Monitoring API. </li>
<li>Click Disable API.</li>
</ol>

<p>In addition you can view Stackdriver usage by billing account and also can estimate cost using Stackdriver pricing calculator [a] [b].</p>

<p>View Stackdriver usage by billing account:</p>

<ol>
<li>From anywhere in the Cloud Console, click Navigation menu and select Billing.</li>
<li>If you have more than one billing account, select Go to linked billing account to 
view the current project's billing account. To locate a different billing account, 
select Manage billing accounts and choose the account for which you'd like to get 
usage reports.</li>
<li>Select Reports.</li>
</ol>

<p>4.Select Group By > SKU. This menu might be hidden; you can access it by clicking Show 
  Filters.</p>

<ol start=""5"">
<li>From the SKUs drop-down list, make the following selections:

<ol>
<li>Log Volume (Stackdriver Logging usage)</li>
<li>Spans Ingested (Stackdriver Trace usage)</li>
<li>Metric Volume and Monitoring API Requests (Stackdriver Monitoring usage)</li>
</ol></li>
<li>Your usage data, filtered by the SKUs you selected, will appear.</li>
</ol>

<p>You can also select just one or some of these SKUs if you don't want to group your usage data.</p>

<p>Note: If your usage of any of these SKUs is 0, they don't appear in the Group By > SKU pull-down menu. For example, who use only the Cloud console might never generate API requests, so Monitoring API Requests doesn't appear in the list.</p>

<p>Use the Stackdriver pricing calculator [b]:</p>

<ol>
<li>Add your current or projected Monitoring usage data to the Metrics section and click Add to estimate.</li>
<li>Add your current or projected Logging usage data to the Logs section and click Add to estimate.</li>
<li>Add your current Trace usage data to the Trace spans section and click Add to estimate.</li>
<li>Once you have input your usage data, click Estimate.
Estimates of your future Stackdriver bills appear. You can also Email Estimate or Save Estimate.</li>
</ol>

<p>[a] <a href=""https://cloud.google.com/stackdriver/estimating-bills#billing-acct-usage"" rel=""nofollow noreferrer"">https://cloud.google.com/stackdriver/estimating-bills#billing-acct-usage</a></p>

<p>[b] <a href=""https://cloud.google.com/products/calculator/#tab=google-stackdriver"" rel=""nofollow noreferrer"">https://cloud.google.com/products/calculator/#tab=google-stackdriver</a></p>
"
Datadog,59055123,59046195,0,"2019/11/26, 18:14:07",True,"2019/11/26, 18:14:07",11561,970308,2,"<p>You need to <code>start</code> the publishing. Compare with the <a href=""https://github.com/micrometer-metrics/micrometer/blob/master/micrometer-core/src/main/java/io/micrometer/core/instrument/logging/LoggingMeterRegistry.java#L75"" rel=""nofollow noreferrer""><code>LoggingMeterRegistry</code></a></p>

<p>In your constructor something like:</p>

<pre><code>start(new NamedThreadFactory(""vw-metrics-publisher""))
</code></pre>
"
Datadog,38089941,38088122,0,"2016/06/29, 06:35:15",False,"2016/06/29, 06:35:15",5780,6163736,3,"<p>Why not try?</p>

<pre><code> var os = require(“os”);
 var hostname = os.hostname();
</code></pre>

<p>It will return the docker container's hostname. If you haven't set a hostname explicitly, using something like <code>docker run -h hostname image command</code> then it will return the docker host's hostname.</p>

<p>Alternatively, you could do this using a deployment tool like puppet, ansible, etc. and template the file when you deploy the container.</p>
"
Datadog,65920537,63314162,2,"2021/01/27, 15:50:52",True,"2021/01/27, 15:50:52",48,8161041,1,"<p>Try this</p>
<pre><code>&lt;encoder class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;&gt;
        &lt;customFields&gt;
            {&quot;service&quot;:&quot;ServiceName&quot;,&quot;ddsource&quot;:&quot;java&quot;}
        &lt;/customFields&gt;...
&lt;/encoder&gt;
</code></pre>
"
Datadog,58963300,58564144,0,"2019/11/20, 23:00:45",False,"2019/11/20, 23:00:45",107,10913713,1,"<p>I got the answer to the second question. Now, I can get all tables from one database that I specified. All I needed to do; relation_regex: '.*' and disabled relation_name.</p>

<p>Answer to the first question I got from datadog is that there is no way to monitor all the DBs without listing them individually. They may change this in future, but for now we have to add blocks for each and every database that we want to monitor</p>
"
Datadog,61927265,56382266,0,"2020/05/21, 07:25:17",False,"2020/05/21, 07:25:17",728,2218580,0,"<p>This works for me:</p>

<pre><code>const { createLogger, format, transports } = require('winston')
const { combine, timestamp, json } = format

function dataDogLogger(options) 
{
    const logger = createLogger({
        exitOnError: false,
        format: combine(
            // add a timestamp to all logs
            timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),
            json()
        ),
        transports: [ ...whatever you need here... ]
    })

    return logger
}

module.exports = dataDogLogger
</code></pre>
"
Datadog,51324294,51323878,0,"2018/07/13, 14:39:40",False,"2018/07/13, 14:39:40",61,9763778,1,"<p>I found the solution: the user <code>datadog</code> didnt have permission to read connections that wasnt form him. So it was just getting a single row.</p>

<p>I gave permissions for that user to read <code>pg_stat_activity</code></p>
"
Datadog,51933718,51866333,0,"2018/08/20, 18:11:48",True,"2018/08/20, 18:11:48",3212,282172,2,"<p>Apparently there is a datadog/agent:latest-jmx that should be used that contains the java image... I just missed it in the docs.</p>
"
Datadog,58760072,58725596,0,"2019/11/08, 05:46:30",False,"2019/11/08, 05:46:30",156,2676108,1,"<p>You mostly have to wait for it all to fill in over time.</p>

<blockquote>
  <p>Metric timestamps cannot be more than 10 minutes in the future or more than 1 hour in the past.</p>
</blockquote>

<p><a href=""https://docs.datadoghq.com/developers/metrics/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/developers/metrics/</a></p>
"
Datadog,63815085,63814864,11,"2020/09/09, 18:41:28",False,"2020/09/09, 18:41:28",30563,605153,2,"<p>Usually metrics exposed to <code>/actuator/metrics</code> are sent to the metrics system like datadog.</p>
<p>You can try to check what exactly gets sent to datadog by examining the source code of <a href=""https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java"" rel=""nofollow noreferrer"">DatadogMeterRegistry</a></p>
<p>Put a breakpoint in the publish method and see what gets sent, or, alternatively set the logger of the class to &quot;trace&quot; so that it will print the information that gets sent to the datadog (line 131 in the linked source code).</p>
<p>Another possible direction to check is usage of filters (see <a href=""https://www.javadoc.io/doc/io.micrometer/micrometer-core/1.0.3/io/micrometer/core/instrument/config/MeterFilter.html"" rel=""nofollow noreferrer"">MeterFilter</a>) that can filter out some metrics.</p>
"
Datadog,63835825,63814864,0,"2020/09/10, 21:39:35",True,"2020/09/10, 22:25:47",205,8534030,2,"<p>This did the trick : Thanks to @MarkBramnik</p>
<pre><code>    @Bean
    @Primary
    CompositeMeterRegistry compositeMeterRegistry(DatadogMeterRegistry datadogMeterRegistry, LoggingMeterRegistry loggingMeterRegistry) {
        CompositeMeterRegistry compositeMeterRegistry = new CompositeMeterRegistry();
        compositeMeterRegistry.add(datadogMeterRegistry);
        compositeMeterRegistry.add(loggingMeterRegistry);
        return compositeMeterRegistry;
    }
</code></pre>
"
Datadog,60171576,60170609,3,"2020/02/11, 16:50:50",False,"2020/02/11, 16:50:50",1371,5540166,0,"<p>Yes, it is possible. You can do that in a <a href=""https://app.datadoghq.com/logs/pipelines"" rel=""nofollow noreferrer"">processing pipeline</a> with a grok parser, but you'll want to configure which attribute the grok parser applies to in the advanced settings (<a href=""https://docs.datadoghq.com/logs/processing/parsing/?tab=matcher#advanced-settings"" rel=""nofollow noreferrer"">docs here</a>). (By default grok parsers apply to the ""message"" attribute, but you can configure them to parse any attribute.)</p>

<p>In this case, you'd set the <code>Extract From</code> field to <code>requestUri</code>. The <code>Helper Rules</code> section is not necessary for this. And then in the main <code>Define Parsing Rules</code> section, you'll plug in a rule similar to this:</p>

<pre><code>parse_customer_id \/customers\/%{notSpace:customerId}\/users
</code></pre>

<p>or even further</p>

<pre><code>parse_customer_id \/%{notSpace}\/%{notSpace:customerId}\/%{notSpace}
</code></pre>
"
Datadog,47085123,47069283,1,"2017/11/02, 23:45:28",False,"2017/11/02, 23:45:28",5158,7147666,2,"<p>Just use a find me Twimlet. Enter up to 10 numbers and a timeout between moving on to the next number. Twilio will do the rest.</p>

<p><a href=""https://www.twilio.com/labs/twimlets/findme"" rel=""nofollow noreferrer"">https://www.twilio.com/labs/twimlets/findme</a></p>
"
Datadog,47730118,47069283,0,"2017/12/09, 17:29:38",False,"2017/12/09, 17:29:38",1150,1394755,1,"<p>If you are looking for a more full featured paid solution I'd recommend PagerDuty. DataDog has an integration for PagerDuty. Any monitor that gets triggered that mentions <code>@pagerduty-myteamname</code>(as example) in the monitor message will cause PagerDuty to page the on call person.  If that person does not acknowledge the page you can configure it to go through list of people to contact next until it is acknowledged by someone.</p>
"
Datadog,41220782,40933155,0,"2016/12/19, 12:42:07",True,"2016/12/19, 12:42:07",1981,2473382,0,"<p>It is very much possible, just use the alias property of the <a href=""http://docs.datadoghq.com/integrations/java/#the-attribute-filter"" rel=""nofollow noreferrer"">attribute</a> filter:</p>

<pre><code>- include:
    domain: data
    attribute:
      success:
        alias: jmx.com.abc.reporting.successCount
      error:
        alias: jmx.com.abc.reporting.errorCount
</code></pre>
"
Datadog,39235805,39235163,0,"2016/08/30, 22:38:33",True,"2016/08/30, 22:40:17",496,2050873,6,"<p>Verify if the datadog package is installed in your environment.</p>

<p>You can do this with this command: </p>

<pre><code>$ pip freeze | grep datadog
</code></pre>

<p>If it's not installed, you can install it with this command:</p>

<pre><code>$ pip install datadog
</code></pre>
"
Datadog,41008234,41007369,0,"2016/12/07, 04:05:10",False,"2016/12/07, 04:05:10",261,4172512,1,"<p>Frank,</p>

<p>Your use case follows the standard ""custom metric"" submission that is common within Datadog.  Using one of the supported libraries:</p>

<p><a href=""http://docs.datadoghq.com/libraries/#java"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/libraries/#java</a></p>

<p>You can leverage the statsD port of an Agent running on your host to submit these custom metrics:</p>

<p><a href=""http://docs.datadoghq.com/guides/dogstatsd/"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/guides/dogstatsd/</a></p>

<p>You will want to install the Agent on either the host running this function or point your statsD connection towards an accepting host:</p>

<p><a href=""http://docs.datadoghq.com/guides/basic_agent_usage/"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/guides/basic_agent_usage/</a></p>

<p>There are additional docs found here that should help you understand how custom metrics work in Datadog:</p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-</a></p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/203765485-How-do-I-submit-custom-metrics-What-s-their-overhead-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/203765485-How-do-I-submit-custom-metrics-What-s-their-overhead-</a></p>

<p>Usually when troubleshooting custom metric submissions, we try to implement some form of local printing/logging to ensure the statsD connection is being made and that the custom function is being called and submitted.  Once you can confirm the metric is being sent to the Agent, use the Metric Summary page to check for the custom metric:</p>

<p><a href=""https://app.datadoghq.com/metric/summary"" rel=""nofollow noreferrer"">https://app.datadoghq.com/metric/summary</a></p>

<p>If all else fails, reach out to Datadog at support@datadoghq.com</p>
"
Datadog,51451079,51443070,0,"2018/07/21, 01:20:42",True,"2018/07/21, 01:20:42",46,9872725,3,"<p>I discussed this with Datadog support, and they confirmed that the <code>awslogs</code> logging driver prevents the Datadog agent container from accessing a container's logs. Since <code>awslogs</code> is currently the only logging driver available to tasks using the Fargate launch type, getting logs into Datadog will require another method.</p>

<p>Since the <code>awslogs</code> logging driver emits logs to CloudWatch, one method that I have used is to create a subscription to stream those log groups to Datadog's Lambda function as configured <a href=""https://docs.datadoghq.com/integrations/amazon_web_services/#log-collection"" rel=""nofollow noreferrer"">here</a>.  You can do that from the <a href=""https://docs.datadoghq.com/integrations/amazon_web_services/#manually-set-up-triggers"" rel=""nofollow noreferrer"">Lambda side</a> using CloudWatch logs as the trigger, or from the CloudWatch Logs side, by clicking <code>Actions</code>> <code>Stream to AWS Lambda</code>.</p>

<p>I chose the Lambda option because it was quick and easy and required no code changes to our applications (since we are still in the evaluation stage).  Datadog support advised me that it was necessary to modify the Lambda function in order to attribute logs to the corresponding service:</p>

<p>In <a href=""https://github.com/DataDog/dd-aws-lambda-functions/blob/master/Log/lambda_function.py#L168-L179"" rel=""nofollow noreferrer"">this block</a>, modify it to something like:</p>

<pre><code>structured_line = merge_dicts(log, {
    ""syslog.hostname"": logs[""logStream""],
    ""syslog.path"": logs[""logGroup""],
    ""syslog.appname"": logs[""logGroup""],
    ""aws"": {
        ""awslogs"": {
            ""logGroup"": logs[""logGroup""],
            ""logStream"": logs[""logStream""],
            ""owner"": logs[""owner""]
        }
    }
})
</code></pre>

<p>According to Datadog support:</p>

<ol>
<li><code>syslog.appname</code> needs to match an existing APM service in order to correlate logs to the service.</li>
<li>This solution is not fully supported at the moment and they are working on documenting this more thoroughly.</li>
</ol>

<p>I had to make further modifications to set the value of the <code>syslog.*</code> keys in a way that made sense for our applications, but it works great.</p>
"
Datadog,62545412,62545185,0,"2020/06/24, 02:43:20",False,"2020/06/24, 02:43:20",1371,5540166,1,"<p>You might be able to get this if you...</p>
<ol>
<li>Start creating a metric type monitor</li>
<li>To the far right of the metric definition, select &quot;advanced&quot;</li>
<li>Select &quot;Add Query&quot;</li>
<li>Input your metrics</li>
<li>In the field called &quot;Express these queries as:&quot;, input <code>(a-b)/b</code> or some such</li>
<li>Trigger when the metric is <code>above or equal to</code> the threshold <code>in total</code> during the last <code>24 hours</code></li>
<li>Set Alert threshold &gt;= <code>0.05</code></li>
</ol>
<p>If you start having trouble as you start setting it up, you may want to reach out to support@datadoghq.com to get their assistance.</p>
"
Datadog,38752963,38746266,1,"2016/08/03, 23:23:06",False,"2016/08/03, 23:23:06",15334,3348604,1,"<p>This is not correct graph to detect correct resource limit. You graph shows CPU usage of your app in the cluster, but resource limit is per pod (container). We (and you as well) don't know from the graph how many containers were up and running. You can determinate right CPU limit from the container CPU usage graph(s). You will need Datadog-Docker integration:</p>

<blockquote>
  <p>Please be aware that Kubernetes relies on Heapster to report metrics,
  rather than the cgroup file directly. The collection interval for
  Heapster is unknown which can lead to innacurate time-related data,
  such as CPU usage. If you require more precise metrics, we recommend
  using the Datadog-Docker Integration.</p>
</blockquote>

<p>Then it will depends how Datadog measure CPU utilization per container. If container CPU utilization has max 100%, then 100% CPU container utilization ~ 1000m ~ 1. </p>

<p>I recommend you to read how and when cgroup limits CPU - <a href=""https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html"" rel=""nofollow"">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html</a></p>

<p>You will need a deep knowledge to set proper CPU limits. If you don't need to prioritize any container, then IMHO the best practice is to set 1 (<code>resources.requests.cpu</code>) for all your containers - they will have always equal CPU times.</p>
"
Datadog,42839492,42815252,1,"2017/03/16, 18:18:52",True,"2017/03/16, 18:35:39",455,4676932,3,"<p>The format you're using to send the data does not comply with the <a href=""http://docs.datadoghq.com/api/?lang=console#tags-update"" rel=""nofollow noreferrer"">documentation</a> and your call fails to complete.</p>

<p>The call would work if you change your <code>$data</code> to:</p>

<pre><code>$data = [ 'tags' =&gt; ['env:prod'] ];
</code></pre>

<p>Agreed that the error returned by the API is not really helpful, filed an issue in Datadog to correct that  behavior and return the proper error code and not a 500 (it's actually a 500 and you can see it by printing <code>curl_getinfo($ch)</code> after you executed your curl session).</p>
"
Datadog,54813734,54811591,0,"2019/02/21, 20:18:42",False,"2019/02/21, 20:18:42",790,1958107,1,"<p>By introducing space in datadog.json.j2 template definition .i.e.</p>

<pre><code> [{""source"":""{{ sourcea }}""{{ ',' }} ""service"":""{{ serviceb }}""}] (space at start)
</code></pre>

<p>and running deployment again I got the working config as below </p>

<pre><code>template:
    metadata:
      annotations:
        ad.datadoghq.com/yps.logs: ' [{""source"":""test"", ""service"":""test""}]'
</code></pre>

<p>However I am not able to understand the behaviour if anyone could help me understand it</p>
"
Datadog,44193846,44155361,6,"2017/05/26, 07:50:45",True,"2017/05/26, 07:50:45",1371,5540166,2,"<p>Why not use dogstatsd instead of threadstats? If you're already running the dd-agent on your node in a way that's reachable by your containers, you can use the <code>datadog.statsd.increment()</code> method instead to send the metric over statsd to the agent, and from there it would get forwarded to your datadog account. </p>

<p>Dogstatsd has the benefits of being more straight-forward and of being somewhat easier to troublehsoot, at least with debug-level logging. Threadstats sometimes has the benefit of not requiring a dd-agent, but it does very little (if any) error logging, so makes it difficult to troubleshoot cases like these.</p>

<p>If you went the dogstatsd route, you'd use the following code:</p>

<pre><code>from datadog import initialize
from datadog import statsd
statsd.increment('api.request_count', tags=['environment:' + environment])
</code></pre>

<p>And from there you'd find your metric metadata with the ""rate"" type and with an interval of ""10"", and you could use the ""as_count"" function to translate the values to counts.</p>
"
Datadog,44406163,44155361,1,"2017/06/07, 10:26:03",False,"2017/06/07, 10:26:03",2644,4279006,0,"<p>In the python script I was initializing with an api key:</p>

<pre><code>from datadog import api
from datadog import initialize
from datadog import statsd
options = {
    'api_key':'#######'
}

initialize(**options)
</code></pre>

<p>And sending some events</p>

<pre><code>api.Event.create(title=title, text=text, tags=tags)
</code></pre>

<p>When I changed it to initialize like this, it started working with the dd-agent:</p>

<pre><code>initialize(statsd_use_default_route=True)
</code></pre>

<p>I didn't need the link command (--link dogstatsd:dogstastd).</p>

<p>With that setup it now works in the staging environment, but not in production. :/</p>
"
Datadog,47394149,47142664,0,"2017/11/20, 16:17:28",True,"2017/11/20, 16:17:28",1629,1970882,1,"<p>I am on the same boat. I found this link: <a href=""https://docs.datadoghq.com/tracing/#instrument-your-application"" rel=""nofollow noreferrer"">datadog instrumentation</a>.
So, currently (20.11.2017) there are not agents for C#. Only Go, python and ruby are available.</p>

<p><a href=""https://i.stack.imgur.com/GoVcW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GoVcW.png"" alt=""screenshot 20.11.2017""></a></p>
"
Datadog,54639737,47142664,0,"2019/02/11, 23:52:44",False,"2019/03/01, 18:33:14",16566,24231,0,"<p>(Disclosure: I'm a software developer at Datadog.)</p>

<p>Datadog's open-source .NET Tracer is currently (2019-02-11) in open beta. It supports manual and automatic instrumentation and <a href=""https://opentracing.io/guides/csharp/"" rel=""nofollow noreferrer"">OpenTracing</a>.</p>

<ul>
<li><a href=""https://github.com/DataDog/dd-trace-dotnet"" rel=""nofollow noreferrer"">GitHub</a></li>
<li><a href=""https://docs.datadoghq.com/tracing/languages/dotnet"" rel=""nofollow noreferrer"">Official docs</a></li>
</ul>

<p>For a list of currently supported languages/frameworks, see the <a href=""https://docs.datadoghq.com/tracing/languages/"" rel=""nofollow noreferrer"">updated documentation</a>.</p>

<p>Happy tracing!</p>
"
Datadog,47769427,47739117,1,"2017/12/12, 11:41:56",True,"2017/12/12, 11:41:56",2084,5156990,2,"<p>Looks like I found the problem - <a href=""https://github.com/DataDog/jenkins-datadog-plugin/issues/101"" rel=""nofollow noreferrer"">https://github.com/DataDog/jenkins-datadog-plugin/issues/101</a></p>

<p>current Datadog version 0.6.1. has a bug , after change the Jenkins main config ( any change , not related to Datadog configuration) it stop works.</p>

<p><strong>I downgrade it to 0.5.7 and it works OK</strong></p>
"
Datadog,50768198,50768038,0,"2018/06/08, 23:48:11",False,"2018/06/08, 23:48:11",1371,5540166,0,"<p>Not possible today, but that is in Datadog's plans for development.</p>

<p>What you can do as a workaround, though, is add a link to your logs explorer with the query that triggered the monitor alert, so you can get a quick reference to what were the logs that triggered it. </p>

<p>This link, for example, would quickly scope you to the error logs over the last 15 minutes: <code>https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc</code></p>

<p>And markdown is supported, so you can keep your monitor messages prettier without long links in the messages. Like so:
<code>[Check the error logs here](https://app.datadoghq.com/logs?live=true&amp;query=status%3Aerror&amp;stream_sort=desc)</code></p>
"
Datadog,53290808,52891518,0,"2018/11/14, 01:03:15",False,"2018/11/14, 01:03:15",51,6111276,1,"<p>This might be a problem that <a href=""https://github.com/DataDog/integrations-core/issues/1829"" rel=""nofollow noreferrer"">other people are running into too</a>. kubelet is no longer listening on the ReadOnlyPort in newer Kubernetes versions, and the port is being deprecated. Samuel Cormier-Iijima reports that the issue can be solved by adding adding <code>KUBELET_EXTRA_ARGS=--read-only-port=10255</code> in <code>/etc/default/kubelet</code> on the node host.</p>
"
Datadog,51045835,51033962,0,"2018/06/26, 17:52:41",False,"2018/06/26, 17:52:41",23418,4379151,0,"<p>It looks to me that these are Datadog specific configuration parameters. So you first need to install the Datadog app to your Slack workspace, which you find on the Slack App Directory.</p>

<p>Here is how the process is described in the official documentation:</p>

<blockquote>
  <p><strong>Installation</strong></p>
  
  <p>The Slack integration is installed via its integration tile in the
  Datadog application.</p>
  
  <p><strong>Configuration</strong></p>
  
  <ol>
  <li>In your Slack account go to the Applications page and search for
  Datadog.</li>
  <li>Click Install, followed by Add Integration.</li>
  <li>Copy the Slack Service Hook and paste in the service hook field for
  Slack in Datadog.</li>
  <li>Add the channels you want to be able to post to.</li>
  <li>If you would like to be notified for every comment on a graph, tick
  the check box “Transfer all user comments” by each channel. If left
  unchecked (default) you will need to use the @slack-channel_name
  syntax for comments to be posted to slack.</li>
  </ol>
</blockquote>

<p>Source: <a href=""https://docs.datadoghq.com/integrations/slack/"" rel=""nofollow noreferrer"">official documentation</a> from Datadog</p>
"
Datadog,61108577,61091978,0,"2020/04/08, 22:31:05",False,"2020/04/08, 22:31:05",61,6722990,2,"<p>I think the disconnect here is that the pattern needs to be in the Jboss log manager, and then they're encoded to JSON.</p>

<p>Have you tried puttinng <code>%X{dd.trace_id:-0} %X{dd.span_id:-0}</code> into your Jboss logging pattern? </p>

<p>If not, I'd also recommend opening a ticket at support@datadoghq.com and we can work this through with you.</p>
"
Datadog,55806500,55632833,0,"2019/04/23, 10:46:35",True,"2019/04/23, 10:46:35",410,5254815,0,"<p>Installed it as ""Run with Admin rights"" and it fixed the issue.</p>
"
Datadog,57805506,55632833,0,"2019/09/05, 15:28:28",False,"2019/09/05, 15:28:28",971,3271599,0,"<p>For me, I had to manually give the <code>ddagentuser</code> account read access to the file</p>

<blockquote>
  <p>C:\ProgramData\Datadog\auth_token</p>
</blockquote>
"
Datadog,65583978,65580606,0,"2021/01/05, 19:47:25",False,"2021/01/05, 19:47:25",45878,180368,1,"<p>Answering my own question.</p>
<p>The DataDog logging page has a Configuration section.  On that page the &quot;Pre processing for JSON logs&quot; section allows you to specify alternate property names for a few of the major log message properties.  If you add @m to the Message attributes section and @l to the Status attributes section you will correctly ingest JSON messages from the <code>RenderedCompactJsonFormatter</code> formatter.  If you add RenderedMessage and Level respectively you will correctly ingest <code>JsonFormatter(renderMessage: true)</code> formatter.  You can specify multiple attributes in each section, so you can simultaneously support both formats.</p>
"
Datadog,53029359,52989371,0,"2018/10/28, 09:39:29",False,"2018/10/28, 09:39:29",113,4742614,1,"<p>It is in fact possible to send an alert if a metric shows the same value for a fix period of time. 
You can do this by using the diff() function to your query to produce delta values from consecutive delta points and then apply the abs() function to take absolute values of these deltas.</p>

<p>To do this we use the Arithmetic functions available, which can be applied using the '+' button to your query in UI. </p>

<p>For alert conditions in the metric monitor itself, configure as follows:
Select threshold alert
Set the “Trigger when the metric is…” dropdown selector to below or equal to
Set the “Alert Threshold” field to 0 (zero)</p>

<p>This configuration will trigger an alert event when no change in value has been registered over the selected timeframe.</p>

<p>Here is a link to datadog article: <a href=""https://docs.datadoghq.com/monitors/faq/how-can-i-configure-a-metric-monitor-to-alert-on-no-change-in-value/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/monitors/faq/how-can-i-configure-a-metric-monitor-to-alert-on-no-change-in-value/</a></p>
"
Datadog,41968804,41964906,0,"2017/02/01, 00:39:07",True,"2017/02/01, 00:39:07",26,4313641,1,"<p>You need to pass <code>Content-Type</code> as a header with the request, as shown <a href=""http://docs.datadoghq.com/api/?lang=console#timeboards"" rel=""nofollow noreferrer"">in the docs</a></p>

<pre><code>$ curl -X POST -H ""Content-type: application/json"" 'https://app.datadoghq.com/api/v1/dash?api_key=&lt;key&gt;&amp;application_key=&lt;key&gt;' -d '{""dash"":{""title"":""Foo"",""description"":""bar"",""graphs"":[]}}'
</code></pre>

<p>Response:</p>

<pre><code>{""errors"": [""The parameter 'title' is required""]}
</code></pre>

<p>Your data is also not formatted according to the docs (there should be no <code>dash</code> field at the top level, for starters).</p>
"
Datadog,39751062,39750699,0,"2016/09/28, 17:55:57",False,"2016/09/28, 18:05:43",4213,5802417,-1,"<p>I am not familiar with the products and libraries that you are using, but there is an open source library <strong>MgntUtils</strong> that can extract full or filtered stacktrace from exception as a String. Since you mentioned that you can pass the text (i.e. String) this library may help you. Here are the links to <strong>MgntUtils</strong> library: </p>

<ol>
<li><a href=""https://www.linkedin.com/pulse/open-source-java-library-some-useful-utilities-michael-gantman?trk=pulse_spock-articles"" rel=""nofollow"">https://www.linkedin.com/pulse/open-source-java-library-some-useful-utilities-michael-gantman?trk=pulse_spock-articles</a> Detailed article that explains what utilities are available in <strong>MgntUtils</strong> library and how to use them</li>
<li><a href=""http://search.maven.org/#search%7Cga%7C1%7Cmichaelgantman"" rel=""nofollow"">http://search.maven.org/#search%7Cga%7C1%7Cmichaelgantman</a> - Here you can get it as Maven resource (also library, source and Javadoc available as a separate resource)</li>
<li>Github: <a href=""https://github.com/michaelgantman/Mgnt"" rel=""nofollow"">https://github.com/michaelgantman/Mgnt</a> - Here you can get this library as a git project as well as Jars for library itself, sources and javadoc (each in separate jar)</li>
</ol>

<p>I hope this helps</p>
"
Datadog,64447994,64395922,0,"2020/10/20, 18:05:44",False,"2020/10/20, 18:05:44",23,9053059,1,"<p>Found the answer:</p>
<p>The metric <code>kubernetes.kubelet.volume.stats.used_bytes</code> will allow you to get the disk usage on PersistentVolumes. This can be achieved using the tag <code>persistentvolumeclaim</code> on this metric.</p>
<p>You can view this metric and tag combination in your account here: <a href=""https://app.datadoghq.com/metric/summary?filter=kubernetes.kubelet.volume.stats.used_bytes&amp;metric=kubernetes.kubelet.volume.stats.used_bytes"" rel=""nofollow noreferrer"">https://app.datadoghq.com/metric/summary?filter=kubernetes.kubelet.volume.stats.used_bytes&amp;metric=kubernetes.kubelet.volume.stats.used_bytes</a></p>
<p>Documentation on this metric can be found here: <a href=""https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubelet"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/kubernetes/data_collected/#kubelet</a></p>
"
Datadog,49818303,46300932,0,"2018/04/13, 16:34:59",True,"2018/04/13, 16:56:36",939,2506172,2,"<blockquote>
  <p>However if you look at this metric carefully it appears to be
  calculating these percentiles on a short range (not sure what) and for
  each tuple of the tags that exist.</p>
</blockquote>

<p>The short range that you have noticed is actually the flush interval which defaults to 10 seconds. As per <a href=""https://help.datadoghq.com/hc/en-us/articles/205638045-What-is-the-histogram-metric-type-"" rel=""nofollow noreferrer"" title=""this"">this</a> article on histogram metric by datadog, </p>

<blockquote>
  <p>It aggregates the values that are sent during the flush interval
  (usually defaults to 10 seconds). So if you send 20 values for a
  metric during the flush interval, it'll give you the aggregation of
  those values for the flush interval</p>
</blockquote>

<p>For your query - </p>

<blockquote>
  <p>Ideally what I'd like to get is a 95th percentile of the ResponseTime
  metric over all the tags (maybe I filter it down by 1 or 2 and have a
  couple of different graphs) but over the last week or so. Is there an
  easy way to do this?</p>
</blockquote>

<p>as per my reading of the datadog docs, there isn't a way to get this done at the moment. It might be a good idea to check with datadog support regarding this.</p>

<p>More details <a href=""https://help.datadoghq.com/hc/en-us/articles/211545826-Why-histogram-stats-are-all-the-same-inaccurate-Characteristics-of-Datadog-histograms-"" rel=""nofollow noreferrer"" title=""here"">here</a>.</p>
"
Datadog,61247522,61148607,1,"2020/04/16, 13:00:50",True,"2020/04/16, 13:00:50",408,3754710,1,"<p>If you are trying to connect to an HTTPS URL for Datadog (<code>https://app.datadoghq.com</code> in your example), then you will need to set the <code>https.proxyHost</code> system property for it to have effect - <code>http.proxyHost</code> is for HTTP URLs[1]. These are system-wide settings that will be used by the default <code>HttpSender</code> (<code>HttpUrlConnectionSender</code>) if a <code>Proxy</code> is not passed to its constructor.</p>

<blockquote>
  <p>The micrometer doc says</p>

<pre><code>management.metrics.export.datadog.uri=https://app.datadoghq.com # URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Datadog, you can define the location of the proxy with this.
</code></pre>
  
  <p>But I dont understand what it means? should I replace this url with my
  proxy url or is there any specific uri pattern with the proxy?</p>
</blockquote>

<p>This is referring to a different kind of proxy that you would configure to receive your Datadog traffic on your internal network, and it would forward it to Datadog outside of your network. If you are using an HTTP proxy then you should use the system properties or an <code>HttpSender</code> with your HTTP proxy configured (e.g. an <code>HttpUrlConnectionSender</code> and passing a <code>Proxy</code> to its constructor).</p>

<p>You can configure a custom <code>HttpSender</code> with a <code>DatadogMeterRegistry</code> using its <code>Builder</code>. If you expose this as a <code>Bean</code> in a <code>@Configuration</code> class, Spring Boot will use it in its auto-configuration. For example:</p>

<pre><code>@Bean
public DatadogMeterRegistry datadogMeterRegistry(DatadogConfig config, Clock clock) {
    HttpSender httpSender = new HttpUrlConnectionSender(config.connectTimeout(), config.readTimeout(), new Proxy(Proxy.Type.HTTP, new InetSocketAddress(""myproxy"", 8080)));
    return DatadogMeterRegistry.builder(config).clock(clock).httpClient(httpSender).build();
}
</code></pre>

<hr>

<p>[1] <a href=""https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/8/docs/technotes/guides/net/proxies.html</a></p>
"
Datadog,66284080,66113635,0,"2021/02/19, 21:45:06",False,"2021/02/19, 21:45:06",383,2032722,1,"<p>After few days of research and follow up with datadog support team, I am able to get the APM logs on datadog portal.</p>
<blockquote>
<p>Below is my <code>docker-compose.yml</code> file configuration,  I believe it helps someone in future</p>
</blockquote>
<pre><code>version: &quot;3&quot;
services:
  web:
    build: web
    command: ddtrace-run python standalone_api.py 
    volumes:
      - .:/usr/src/app
    depends_on: 
      datadog-agent:
        condition: service_healthy         
    image: pythonbusinessservice:ICDNew
    ports: 
     - 5000:5000
    environment:     
    - DATADOG_HOST=datadog-agent
    - DD_TRACE_AGENT_PORT=8126
    - DD_AGENT_HOST=datadog-agent
  datadog-agent:
    build: datadog
    image: gcr.io/datadoghq/agent:latest
    ports: 
     - 8126:8126          
    environment: 
     - DD_API_KEY=9e3rfg*****************adf3
     - DD_SITE=datadoghq.com
     - DD_HOSTNAME=pythonbusinessservice
     - DD_TAGS=env:dev      
     - DD_APM_ENABLED=true
     - DD_APM_NON_LOCAL_TRAFFIC=true
     - DD_DOGSTATSD_NON_LOCAL_TRAFFIC=true     
     - DD_SERVICE=pythonbusinessservice 
     - DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL=true 
     - DD_CONTAINER_EXCLUDE=&quot;name:datadog-agent&quot;      
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock
     - /proc/:/host/proc/:ro
     - /opt/datadog-agent/run:/opt/datadog-agent/run:rw
     - /sys/fs/cgroup:/host/sys/fs/cgroup:ro
</code></pre>
<blockquote>
<p>The <code>Dockerfile</code> for my python long running application</p>
</blockquote>
<pre><code>FROM python:3.7

COPY . /app
WORKDIR /app

RUN pip install -r requirements.txt

CMD [&quot;ddtrace-run python&quot;, &quot;/app/standalone_api.py&quot;]
</code></pre>
<p>Please note, on the requirements.txt file I have  <code>ddtrace</code> package listed</p>
"
Datadog,55152101,55087322,0,"2019/03/14, 00:27:39",False,"2019/03/14, 00:27:39",1371,5540166,0,"<p>it appears to me like you're missing a couple environment vars in your docker-compose <code>datadog</code> service configuration. And also the volume that adds the registry for tailing the logs from the docker socket. Maybe try something like this if you haven't?</p>

<pre><code>  # agent section
  datadog:
    build: datadog
    links:
     - redis # ensures that redis is a host that the container can find
     - web # ensures that the web app can send metrics
    environment:
     - DD_API_KEY=34f-------63c
     - DD_LOGS_ENABLED=true
     - DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL=true
     - DD_AC_EXCLUDE=""name:datadog-agent""
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock:ro
     - /proc/:/host/proc/:ro
     - /sys/fs/cgroup/:/host/sys/fs/cgroup:ro
     - /opt/datadog-agent/run:/opt/datadog-agent/run:rw
</code></pre>

<p>from there, if you still end up with trouble, you may want to reach out to support@datadoghq.com to ask for help. They're pretty quick to reply. </p>
"
Datadog,64484497,64482767,4,"2020/10/22, 17:32:13",False,"2020/10/22, 19:17:02",156,2676108,1,"<p>This answer doesn't solve the problem, because postgres is not running in the cluster, it's running in Azure. I'll leave it up since it might be interesting, but I posted another answer for the actually environment setup.</p>
<hr />
<p>For containerized setups, it's not usually recommended to set up a configmap or try giving the agent a yaml file. Instead the recommended configuration is to put annotations on the postgres pod: <a href=""https://docs.datadoghq.com/integrations/postgres/?tab=containerized#containerized"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/postgres/?tab=containerized#containerized</a>.</p>
<p>This concept of placing the config on the application pod, not with the datadog agent, is called autodiscovery. This blog post does a good job explaining the benefits of this solution: <a href=""https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/#autodiscovery"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/monitoring-kubernetes-with-datadog/#autodiscovery</a></p>
<p>Here is a picture diagram showing how the agent goes out to the pods on the same node and would pull the configuration from them:</p>
<p><a href=""https://i.stack.imgur.com/XIZkM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XIZkM.png"" alt=""kubernetes and daemonset agent diagram"" /></a></p>
<p>To configure this, you'd take each of the sections of the yaml config, convert them to json, and set them as annotations on the postgres manifest. An example of how to set up pod annotations is provided for redis, apache, and http here: <a href=""https://docs.datadoghq.com/agent/kubernetes/integrations/?tab=kubernetes#examples"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/kubernetes/integrations/?tab=kubernetes#examples</a></p>
<p>For your scenario I would do something like:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: mypostgres
  annotations:
    ad.datadoghq.com/mypostgres.check_names: '[&quot;postgres&quot;]'
    ad.datadoghq.com/mypostgres.init_configs: '[{}]'
    ad.datadoghq.com/mypostgres.instances: |
      [
        {
          &quot;host&quot;:&quot;%%host%%&quot;, 
          &quot;port&quot;:5432,
          &quot;username&quot;:&quot;my-user&quot;,
          &quot;password&quot;:&quot;some-password&quot;
        }
      ]
  labels:
    name: mypostgres
spec:
  containers:
    - name: mypostgres
      image: postgres:latest
      ports:
        - containerPort: 5432
</code></pre>
<p>notice how the folder name <code>postgres.d/conf.yaml</code> maps to the <code>check_names</code> annotation, the <code>init_configs</code> section maps to <code>init_configs</code> annotation, etc.</p>
<hr />
<p>For the section on custom metrics, since I personally am more familiar with the yaml config, and it's easier to just fill out, I'll usually go to a yaml to json converter, and copy the json from there</p>
<p><a href=""https://i.stack.imgur.com/qkU0W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qkU0W.png"" alt=""yaml to json converter screeenshot"" /></a></p>
<pre><code>metadata:
  name: mypostgres
  annotations:
    ad.datadoghq.com/mypostgres.instances: |
      [
        {
          &quot;host&quot;: &quot;%%host%%&quot;,
          &quot;port&quot;: 5432,
          &quot;username&quot;: &quot;my-user&quot;,
          &quot;password&quot;: &quot;some-password&quot;,
          &quot;dbname&quot;: &quot;some-database&quot;,
          &quot;ssl&quot;: true,
          &quot;tags&quot;: [
            &quot;some_tag&quot;
          ],
          &quot;custom_queries&quot;: [
            {
              &quot;metric_prefix&quot;: &quot;some.prefix&quot;,
              &quot;query&quot;: &quot;SELECT COUNT(*) FROM bla WHERE timestamp &gt; NOW() - INTERVAL '1 hour';&quot;,
              &quot;columns&quot;: [
                {
                  &quot;name&quot;: &quot;countLastHour&quot;,
                  &quot;type&quot;: &quot;count&quot;
                }
              ]
            }
          ]
        }
      ]
</code></pre>
<hr />
<p>A key thing to notice for all those configs is that I never set the hostname. That is automatically discovered by the agent as it scans through containers.</p>
<p>However you may have set <code>my-postgres-host.com</code> because this postgres instance is not actually running in your kubernetes cluster, and is instead living on its own, and not in a container. If that is the case, I would recommend trying to just put the agent on the postgres node directly, all the yaml stuff you've done would work just fine, if that db and the agent are both directly on the vm.</p>
"
Datadog,64486131,64482767,0,"2020/10/22, 19:02:54",False,"2020/10/22, 19:11:25",156,2676108,1,"<p>It looks like the question has been updated to say that this postgres db you are trying to monitor is not actually running the the cluster. And you are not able to put an agent directly on the postgres server since it's a managed service in Azure, so you don't have access to the underlying host.</p>
<p>In those situations it is common to have a random datadog agent on some other host set up the postgres integration anyway, but instead of having <code>host: localhost</code> in the yaml config, put the hostname you would put to access the db externally. In your example it was <code>host: my-postgres-host.com</code>. This provides all the same benefits of the normal integration (except you won't have cpu/disk/resource metrics available obviously)</p>
<p>This is all fine and makes sense, but what if all of the agents you have installed are the agents in the kubernetes daemonset you created? You don't have any hosts directly on VMs to run this check. But we definitely don't recommend configuring the daemonset to run this check directly. If you did, that would mean you are collecting duplicate metrics from that one postgres db in every single node in your cluster. Since every agent is a copy, they'd each be running the same check on the same db you define.</p>
<p><a href=""https://i.stack.imgur.com/TLiQh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TLiQh.png"" alt=""enter image description here"" /></a></p>
<p>Luckily I notice that you are running the Datadog Cluster Agent. This is a separate Datadog tool that is deployed as a single service once per cluster, instead of a daemonset running once per node. It is possible to have the cluster agent configured to run 'cluster level' checks. Perfect for things like databases, message queues, or http checks.</p>
<p>The basic idea is that (in addition to it's other jobs) the cluster agent will also schedule checks. the DCA (datadog cluster agent) will choose one agent from the daemonset to run the check, and if that node agent pod dies, the DCA will find a new one to run the cluster check.</p>
<p><a href=""https://i.stack.imgur.com/dEH0y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dEH0y.png"" alt=""cluster agent scheduling node agent to query postgres"" /></a></p>
<p>Here are the docs on how to set up the DCA to run cluster checks: <a href=""https://docs.datadoghq.com/agent/cluster_agent/clusterchecks/#how-it-works"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/cluster_agent/clusterchecks/#how-it-works</a></p>
<p>To configure it you would enable some flags, and give the DCA the yaml file you created with a config map, or just mounting the file directly. The DCA will pass along that config to whichever node agent it chooses to run the check.</p>
"
Datadog,66775560,66775454,2,"2021/03/24, 08:05:50",False,"2021/03/24, 08:05:50",2752,2672947,2,"<p>See <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/Status"" rel=""nofollow noreferrer"">HTTP response status codes</a></p>
<p>The categories are generally:</p>
<ul>
<li>Informational responses (100–199)</li>
<li>Successful responses (200–299)</li>
<li>Redirects (300–399)</li>
<li>Client errors (400–499)</li>
<li>Server errors (500–599)</li>
</ul>
<p>So 4XX errors are errors, but they indicate the client is likely at fault. E.g. The user went to a page or user agent made a request to a page that does not exist. The server responds with 404 because &quot;Everything on my end is fine, but that page isn't real.&quot;</p>
<p>Is it an error? Sure. Could you potentially identify issues (e.g. typos in links, missing pages, misspellings, malformed API requests, etc..) by routing these to your logs? Sure.</p>
<p>Are you obligated to take action on it? Not if you don't want to.</p>
<p>You're probably best to determine why you feel they are not actionable. Most likely are actionable.</p>
<ul>
<li>Is there lots of traffic to a page that doesn't exist? Maybe a redirect should be put in place.</li>
<li>Is there some activity being rate limited? Perhaps the frequency of those requests needs adjusted or content should be cached, etc..</li>
</ul>
"
Datadog,65953918,65951118,0,"2021/01/29, 13:51:14",True,"2021/01/29, 13:51:14",84586,1384297,2,"<p>Micrometer uses <code>MeterFilter</code>s registered with a <code>MeterRegistry</code> to modified the meters that are registered. The modifications include the ability to map a meter's ID to something different.</p>
<p>In Spring Boot, you can use a <code>MeterRegistryCustomizer</code> bean to add a <code>MeterFilter</code> to a registry. You can use generics to work with a registry of a specific type, for example <code>MeterRegistryCustomizer&lt;DatadogMeterRegistry&gt;</code> for a customizer that is only interested in customizing the Datadog registry.</p>
<p>Putting this together, you can map the ID of the <code>http.server.request</code> meter to <code>i.want.to.be.different</code> using the following bean:</p>
<pre class=""lang-java prettyprint-override""><code>@Bean
MeterRegistryCustomizer&lt;DatadogMeterRegistry&gt; datadogMeterIdCustomizer() {
    return (registry) -&gt; registry.config().meterFilter(new MeterFilter() {

        @Override
        public Id map(Id id) {
            if (&quot;http.server.request&quot;.equals(id.getName())) {
                return id.withName(&quot;i.want.to.be.different&quot;);
            }
            return id;
        }

    });
}
</code></pre>
"
Datadog,63631475,63626179,0,"2020/08/28, 12:52:58",True,"2020/08/28, 12:52:58",4922,11560878,1,"<p>There are some options you should consider:</p>
<ul>
<li><p>don't update anything and just stick to Kubernetes 1.15 (not recommended as it is 4 main versions behind the latest one)</p>
</li>
<li><p><code>git clone</code> your repo and change <code>apiVersion</code> to <code>apps/v1</code> in all your resources</p>
</li>
<li><p>use <a href=""https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#convert"" rel=""nofollow noreferrer"">kubectl convert</a> in order to change the <code>apiVersion</code>, for example: <code>kubectl convert -f deployment.yaml --output-version apps/v1</code></p>
</li>
</ul>
<p>It is worth to mention that stuff gets deprecated for a reason and it is strongly not recommended to stick to old ways if they are not supported anymore.</p>
"
Datadog,63266155,63259337,0,"2020/08/05, 16:22:30",False,"2020/08/05, 16:22:30",434,13958041,0,"<p>If you are doing the setup in an organisation, datadog or prometheus is probably the way to go. You can capture other Kafka related metrics as well. These agents also have integrations with many other tools beside Kafka and will be a good common choice for monitoring.</p>
<p>If you are just doing it for personal POC type of a project and you just want to <strong>view</strong> the lag, I find CMAK very useful (<a href=""https://github.com/yahoo/CMAK"" rel=""nofollow noreferrer"">https://github.com/yahoo/CMAK</a>). This does <strong>not</strong> have historical data, but provides a good <strong>current</strong> visual state of Kafka cluster including lag.</p>
"
Datadog,63279458,63259337,0,"2020/08/06, 11:15:49",False,"2020/08/06, 11:15:49",106,12612778,0,"<p>For cluster wide metrics you can use kafka_exporter (<a href=""https://github.com/danielqsj/kafka_exporter"" rel=""nofollow noreferrer"">https://github.com/danielqsj/kafka_exporter</a>) which exposes some very useful cluster metrics(including consumer lag) and is easy to integrate with prometheus and visualize using grafana.</p>
"
Datadog,65170331,63259337,0,"2020/12/06, 18:24:17",False,"2020/12/06, 18:31:54",81,5501217,0,"<p>Burrow is extremely effective and specialised in monitoring consumer lag.Burrow is good at caliberating consumer offset and more importantly validate if the lag is malicious or not. It has integrations with pagerduty so that the alerts are pushed to the necessary parties.</p>
<p><a href=""https://community.cloudera.com/t5/Community-Articles/Monitoring-Kafka-with-Burrow-Part-1/ta-p/245987"" rel=""nofollow noreferrer"">https://community.cloudera.com/t5/Community-Articles/Monitoring-Kafka-with-Burrow-Part-1/ta-p/245987</a></p>
<p>What burrow has:</p>
<ul>
<li>Non-threshold based lag monitoring algorithm capable to evaluate potential slow downs.</li>
<li>Integration with pagerduty</li>
<li>Exporters for prometheus, AppD etc for historical metrics</li>
<li>Pluggable UI</li>
</ul>
<p>If you are looking for quick solution you can deploy burrow followed by the burrow front end <a href=""https://github.com/GeneralMills/BurrowUI"" rel=""nofollow noreferrer"">https://github.com/GeneralMills/BurrowUI</a></p>
"
Datadog,62246112,62177611,1,"2020/06/07, 16:32:56",False,"2020/06/09, 00:40:01",927,2188893,0,"<p>You could use the Java Admin Kafka API quite easily to expose this over command line or as an HTTP call quite quickly with Spring Boot (could avoid any metrics for each app individually since Admin API could do it for any group).</p>

<p>See link for an example: <a href=""https://gquintana.github.io/2020/01/16/Retrieving-Kafka-lag.html"" rel=""nofollow noreferrer"">https://gquintana.github.io/2020/01/16/Retrieving-Kafka-lag.html</a></p>

<p>Code taken from example above.</p>

<p>Get all groups:</p>

<pre class=""lang-java prettyprint-override""><code>adminClient
.listConsumerGroups()
.valid().thenApply(r -&gt; r.stream()
.map(ConsumerGroupListing::groupId).collect(toList())).get();
</code></pre>

<p>Get highest offset in a set of partitions:</p>

<pre class=""lang-java prettyprint-override""><code>consumer.endOffsets(partitions);
</code></pre>

<p>High level on connecting them together to get lag:</p>

<pre class=""lang-java prettyprint-override""><code>Map&lt;TopicPartition, OffsetAndMetadata&gt; consumerGroupOffsets = getConsumerGroupOffsets(groupId);

Map&lt;TopicPartition, Long&gt; topicEndOffsets = getTopicEndOffsets(groupId, consumerGroupOffsets.keySet());

Map&lt;Object, Object&gt; consumerGroupLag = consumerGroupOffsets
.entrySet()
.stream()
.map(entry -&gt; mapEntry(entry.getKey(), 
new OffsetAndLag(topicEndOffsets.get(entry.getKey()), entry.getValue().offset())));
</code></pre>
"
Datadog,62137876,62123685,0,"2020/06/01, 20:43:26",False,"2020/06/01, 20:43:26",471,967088,0,"<p>Use the supplied jconsole.sh script in bin, don't try and build up the classpath by hand. You also need to use the custom service url. See the docs for details</p>
"
Datadog,63924760,61119886,0,"2020/09/16, 19:57:11",False,"2020/09/16, 19:57:11",460,2810489,0,"<p>You can run datadog tracing on AWS Elastic Beanstalk with Flask by configure tracing manually as defined <a href=""http://pypi.datadoghq.com/trace/docs/web_integrations.html#flask"" rel=""nofollow noreferrer"">here</a>:</p>
<pre><code>from ddtrace import patch_all
patch_all()

from flask import Flask

app = Flask(__name__)


@app.route('/')
def index():
    return 'hello world'


if __name__ == '__main__':
    app.run()
</code></pre>
"
Datadog,61063468,61043878,0,"2020/04/06, 18:37:38",False,"2020/04/06, 18:37:38",9394,573153,2,"<p>Custom metrics are on the roadmap for the APM agent, but we're still working on the exact schedule.</p>

<p>In the meantime you could either use the <a href=""https://www.elastic.co/guide/en/apm/agent/java/current/config-jmx.html"" rel=""nofollow noreferrer"">JMX config options</a> of the agent with custom JMX key properties. Or use the Elasticsearch output of Micrometer. Maybe just change the Micrometer output as an interim solution and potentially switch to custom APM metrics once they are available?</p>

<p>There's also the option to get metrics with <a href=""https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-module-jolokia.html"" rel=""nofollow noreferrer"">Metricbeat from JMX / Jolokia</a>, but that sounds like an even bigger change and not really a long-term upside.</p>
"
Datadog,60050555,59961483,0,"2020/02/04, 06:24:29",False,"2020/02/05, 03:52:39",1201,8673695,2,"<p>One straight forward way I can think of in this case is to use the <code>.env</code> file for your docker-compose.</p>

<p><strong>docker-compose.yaml</strong> file will look something like this</p>

<pre><code>...

ports:
 - ${NGINX_PORT}:80

...

ports:
 - ${API_PORT}:80

</code></pre>

<p><strong>.env</strong> file for each stack will look something like this</p>

<pre><code>NGINX_PORT=30000
API_PORT=30001
</code></pre>

<p>and</p>

<pre><code>NGINX_PORT=30100
API_PORT=30101
</code></pre>

<p>for different projects.</p>

<p>Note:</p>

<ul>
<li><code>.env</code> must be in the same folder as your <code>docker-compose.yaml</code>.</li>
<li>Make sure that all the ports inside <code>.env</code> files will not be conflicting with each other. You can have some kind of conventions like having prefix for features like feature1 will have port starting with <code>301</code> i.e. <code>301xx</code>.</li>
<li>In this way, your <code>docker-compose.yaml</code> can be as generic as you may like.</li>
</ul>
"
Datadog,60068032,59961483,1,"2020/02/05, 04:00:43",False,"2020/02/05, 04:00:43",7900,2259934,0,"<p>You're making things harder than they have to be. Your app is containerized- use a container system.</p>

<p>ECS is <em>very</em> easy to get going with. It's a json file that defines your deployment- basically analogous to docker-compose (they actually supported compose files at some point, not sure if that feature stayed around). You can deploy an arbitrary number of services with different container images. We like to use a terraform module with the image tag as a parameter, but easy enough to write a shell script or whatever.</p>

<p>Since you're trying to save money, create a single application load balancer. each app gets a hostname, and each container gets a subpath. For short lived feature branch deployments, you can even deploy on Fargate and not have an ongoing server cost.</p>
"
Datadog,60405918,59961483,0,"2020/02/26, 04:28:04",False,"2020/02/26, 04:28:04",971,8379207,0,"<p>It turns out the solution involved capabilities from docker-compose. In docker docs the concept is called <a href=""https://docs.docker.com/compose/#multiple-isolated-environments-on-a-single-host"" rel=""nofollow noreferrer"">Multiple Isolated environments on a single host</a></p>

<p>to achieve this:</p>

<ol>
<li><p>I used an .env file with so many env vars. The main one is <code>CONTAINER_IMAGE_TAG</code> that defines the git branch ID to identify the stack.</p></li>
<li><p>A separate docker-compose-dev file defines ports, image tags, extra metadata that is dev related</p></li>
<li><p>Finally the use of <code>--project-name</code> in the docker-compose command allows to have different stacks. </p></li>
</ol>

<p>an example docker-compose Bash function that uses the docker-compose command </p>

<pre><code>docker_compose() {
    docker-compose -f docker/docker-compose.yaml -f docker/docker-compose-dev.yaml --project-name ""project${CONTAINER_IMAGE_TAG}"" --project-directory . ""$@""
}
</code></pre>

<p>The separation should be done in the image tags, container names, network names, volume names and project name.</p>
"
Datadog,58576495,58576189,1,"2019/10/27, 05:54:22",False,"2019/10/27, 05:54:22",1190,5621569,1,"<p>The issue is that your library depends on <code>gcc</code> to run.</p>

<h1>1. Linux/Containers</h1>

<p>If you are running in a container, you can try two options:</p>

<ol>
<li>you can build your app without <code>CGO</code> with the following command: </li>
</ol>

<pre><code>RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o yourBinary
</code></pre>

<ol start=""2"">
<li>You can try to install <code>gcc</code> into your container. If it is an alpine based container, run </li>
</ol>

<pre><code>RUN apk update &amp;&amp; apk add --no-cache gcc
</code></pre>

<p>You could also need <code>musl-dev</code> package, but you should try without it first.</p>

<h1>2. Windows</h1>

<p>Since MacOS and most Linux distros come with GCC, I guess you could be using Windows. In this case, you need to install <a href=""https://sourceforge.net/projects/mingw-w64/"" rel=""nofollow noreferrer"">MinGW</a>.</p>
"
Datadog,59060540,58576189,0,"2019/11/27, 00:50:03",False,"2019/11/27, 01:25:23",671,445891,1,"<p>I know this is old but I ran into this problem too, About Alexey answer, on windows, you should install MinGW and add the path to win environment. You should follow <a href=""http://www.codebind.com/cprogramming/install-mingw-windows-10-gcc/"" rel=""nofollow noreferrer"">this</a>. In case MinGW did not work, you can install <a href=""http://tdm-gcc.tdragon.net/"" rel=""nofollow noreferrer"">this</a> one which worked perfectly for me on windows.</p>
"
Datadog,59958891,57207906,0,"2020/01/29, 02:05:05",False,"2020/01/29, 02:05:05",1,12802667,0,"<p>I had the same error and installing the .NET Framework 4.6.1 SDK (<a href=""https://dotnet.microsoft.com/download/visual-studio-sdks"" rel=""nofollow noreferrer"">https://dotnet.microsoft.com/download/visual-studio-sdks</a>) and restarting the Datadog Agent solved the problem</p>

<ul>
<li>.net tracer log: [w3wp.exe] 3584: [info] Profiler attached.</li>
</ul>
"
Datadog,56117644,56116856,0,"2019/05/13, 21:01:29",False,"2019/05/13, 21:01:29",4287,9073130,1,"<p>Use <code>context.Request.Path</code> conditionally if your <code>routeData</code> is null. It is the closest I can think of since Identity Server 4 middleware has internal routing logic for the standard OAuth protocol routes.</p>
"
Datadog,55054296,55009193,0,"2019/03/08, 01:11:52",True,"2019/03/08, 01:11:52",923,1181073,1,"<p>After a few more days of research, discovered that:</p>

<ul>
<li>StatsD client libraries like <a href=""https://github.com/brightcove/hot-shots"" rel=""nofollow noreferrer"">hot-shots</a> allow you to connect the cloud function to a StatsD daemon on the public internet, but not internally to a daemon on your GCP cloud; so this is not an ideal option.</li>
<li>Alternatively, libraries like <a href=""https://github.com/brettlangdon/node-dogapi"" rel=""nofollow noreferrer"">node-dogapi</a> can bypass StatsD daemons and report directly to DataDog. This works if you're ok with providing your DataDog API and App keys to the library so that it can generate the messages to DataDog for you.</li>
</ul>
"
Datadog,55159583,54729245,0,"2019/03/14, 12:00:21",False,"2019/03/14, 12:00:21",523,145725,0,"<p>The errors you are getting are coming from the remote computer, that is, the Heroku dyno. You can't follow the instructions in the warning (to update bundler) as you can't run arbitrary instructions on their servers. Heroku only support limited ""<a href=""https://devcenter.heroku.com/articles/bundler-version"" rel=""nofollow noreferrer"">carefully curated</a>"" versions of bundler.</p>

<p>Normally when the bundler versions don't match it just gives a warning, not an error, so you can <strong>potentially</strong> just ignore it. Personally I like to eliminate warnings (or supress them if elimination isn't possible) so that when new warnings pop up I am more likely to notice them and deal with them.</p>

<p>That being said, I was not able to ""downgrade"" my Gemfile.lock from 2.0.1 to 1.15.2. I had to first delete Gemfile.lock and then recreate it (presumably there are potentially breaking changes across these major versions). I suspect this is the second problem you encountered.</p>

<p>The best way around these warnings/errors is to match your local version of Bundler to Heroku's carefully curated version. That page above links to another page with the currently supported versions:<br>
<a href=""https://devcenter.heroku.com/articles/ruby-support#libraries"" rel=""nofollow noreferrer"">https://devcenter.heroku.com/articles/ruby-support#libraries</a></p>

<p>As of today that's version 2.0.1 for Gemfile.locks bundled with 2.x and 1.15.2 for everything else.</p>

<pre class=""lang-sh prettyprint-override""><code># To check which version(s) of bundler you have installed:
$ gem list | grep bundler
bundler (1.17.1)

# To install an older version
$ gem install bundler -v 1.15.2
Fetching: bundler-1.15.2.gem (100%)
...
1 gem installed

# To install supported version 2
$ gem install bundler -v 2.0.1
Fetching: bundler-2.0.1.gem (100%)
...
1 gem installed

# Check again:
$ gem list | grep bundler
bundler (2.0.1, 1.17.1, 1.15.2)

# Bundle with the latest installed version
$ bundle install

# Try to bundle with an older version (may break)
$ bundle _1.15.2_ install
Traceback...
Could not find 'bundler' (2.0.1) required by your Gemfile.lock (Gem::GemNotFoundException)

# Actually bundle with an older version
$ rm Gemfile.lock
$ bundle _1.15.2_ install
</code></pre>
"
Datadog,56572059,54717464,0,"2019/06/13, 03:54:58",False,"2019/06/13, 03:54:58",2578,1235057,0,"<p>Could be merge conflicts in the Gemfile.lock. Try running <code>bundle install</code> locally and see if it works before committing and pushing to heroku.</p>
"
Datadog,52758714,52755069,3,"2018/10/11, 14:16:24",False,"2018/10/11, 14:16:24",1,7850738,-2,"<p>add bash script as userparameters in zabbix-agent.</p>
"
Datadog,52590134,52588302,4,"2018/10/01, 14:29:23",True,"2018/10/01, 14:29:23",941,5488567,2,"<p>Since it requires admin permissions, we can not give out UAA clients for the firehose.
However, there are different ways to get metrics in context of a user.</p>

<ol>
<li><p><strong>CF API</strong></p>

<p>You can obtain basic metrics of a specific app by polling the CF API:
<a href=""https://apidocs.cloudfoundry.org/5.0.0/apps/get_detailed_stats_for_a_started_app.html"" rel=""nofollow noreferrer"">https://apidocs.cloudfoundry.org/5.0.0/apps/get_detailed_stats_for_a_started_app.html</a></p>

<p>However, since you have to poll (and for each app), it's not the recommended way.</p></li>
<li><p><strong>Metrics in syslog drain</strong></p>

<p>CF allows devs to forward their logs to syslog drains; in more recent versions, CF also sends metrics to this syslog drain (see <a href=""https://docs.cloudfoundry.org/devguide/deploy-apps/streaming-logs.html#container-metrics"" rel=""nofollow noreferrer"">https://docs.cloudfoundry.org/devguide/deploy-apps/streaming-logs.html#container-metrics</a>). 
For example, you could use Swisscom's Elasticsearch service to store these metrics and then analyze it using Kibana.</p></li>
<li><p><strong>Metrics using loggregator (firehose)</strong></p>

<p>The firehose allows streaming logs to clients for two types of roles:
Streaming <em>all</em> logs to admins (which requires a UAA client with admin permissions) and streaming <em>app</em> logs and metrics to devs with permissions in the app's space. This is also what the <code>cf logs</code> command uses. <code>cf top</code> also works this way <a href=""https://github.com/ECSTeam/cloudfoundry-top-plugin/blob/v0.9.3/top/client.go#L177-L201"" rel=""nofollow noreferrer"">(it enumerates all apps and streams the logs of each app).</a>
However, you will find out that most open source tools that leverage the firehose only work in admin mode, since they're written for the platform operator.</p></li>
</ol>

<p>Of course you also have the possibility to monitor your app by instrumenting it (white box approach), for example by configuring Spring actuator in a Spring boot app or by including an agent of your favourite APM vendor (Dynatrace, AppDynamics, ...)</p>

<p>I guess this is the most common approach; we've seen a lot of teams having success by instrumenting their applications. Especially since advanced monitoring anyway requires you to create your own metrics as the firehose provided cpu/memory metrics are not that powerful in a microservice world.</p>

<p>However, option 2. would be worth a try as well, especially since the ELK's stack metric support is getting better and better.</p>
"
Datadog,51209988,51097821,2,"2018/07/06, 14:58:20",False,"2018/07/08, 06:33:21",4349,5030709,0,"<p>How are you spinning up ecs-agent container? What is docker run command?. Did you try like below?.</p>

<pre><code>sudo docker run --name ecs-agent \
--detach=true \
--restart=on-failure:10 \
--volume=/var/run:/var/run \
--volume=/var/log/ecs/:/log \
--volume=/var/lib/ecs/data:/data \
--volume=/etc/ecs:/etc/ecs \
--net=host \
--env-file=/etc/ecs/ecs.config \
--label=myLabelName=amazon-ecs-agent \
amazon/amazon-ecs-agent:latest
</code></pre>
"
Datadog,50266462,50266331,4,"2018/05/10, 09:02:11",False,"2018/05/10, 09:02:11",162400,174777,0,"<p>Yes, you can pass a script to the instance that will be executed on the first boot (but not thereafter). It is often referred to as a <strong>User Data script</strong>.</p>

<p>See:</p>

<ul>
<li><a href=""https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2-windows-user-data.html"" rel=""nofollow noreferrer"">Running Commands on Your Windows Instance at Launch - Amazon Elastic Compute Cloud</a></li>
<li><a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html"" rel=""nofollow noreferrer"">Running Commands on Your Linux Instance at Launch - Amazon Elastic Compute Cloud</a></li>
</ul>

<p>If you wish to install <em>after</em> the instance has started, use the <a href=""https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html"" rel=""nofollow noreferrer"">AWS Systems Manager Run Command</a>.</p>
"
Datadog,54622604,49892339,0,"2019/02/11, 02:38:36",False,"2019/02/11, 02:38:36",385,912643,1,"<p>The simplest way to get access to data in Cloudyn is to configure a report and schedule data to be pushed to a storage account. From there, you can use standard storage account APIs to access the data.</p>

<p>Instead of using Cloudyn APIs, however, I would recommend using the <a href=""https://aka.ms/costanalysis/api"" rel=""nofollow noreferrer"">Cost Management Query API</a> for aggregated cost/usage data or <a href=""https://docs.microsoft.com/rest/api/consumption/usagedetails"" rel=""nofollow noreferrer"">UsageDetails API</a> for raw usage.</p>
"
Datadog,48502902,48475616,0,"2018/01/29, 15:54:07",False,"2018/01/29, 15:54:07",914,1679567,0,"<p>If you want to secure access to docker socket, <a href=""https://docs.docker.com/engine/security/https/"" rel=""nofollow noreferrer"">this docker documents</a> is a good start.</p>
"
Datadog,48576285,48457894,0,"2018/02/02, 07:34:44",True,"2019/03/29, 04:35:10",253,607808,1,"<p>Spoke with Datadog support. Very helpful but the short answer is that there is currently no option to add additional tags to specify the specific proc_name in the individual <code>gunicorn.yaml</code> file. </p>

<p>As a workaround to enable grouping we enabled unique prefixes for each application but the trade-off is that the metrics are no longer sharing the same namespace. </p>

<p>I've submitted a new feature request on the Github project which will hopefully be considered. </p>

<pre><code>https://github.com/DataDog/integrations-core/issues/1062
</code></pre>
"
Datadog,46154565,46154416,0,"2017/09/11, 14:24:24",False,"2017/09/11, 14:24:24",31125,460802,0,"<p>Response time is in <strong>time</strong> field.</p>

<p>There is an additional metric <strong>latency</strong> which provides time to first byte.</p>

<p>See:</p>

<ul>
<li><a href=""http://jmeter.apache.org/usermanual/glossary.html"" rel=""nofollow noreferrer"">http://jmeter.apache.org/usermanual/glossary.html</a></li>
</ul>

<p>You might also want to read :</p>

<ul>
<li><a href=""http://jmeter.apache.org/usermanual/generating-dashboard.html"" rel=""nofollow noreferrer"">http://jmeter.apache.org/usermanual/generating-dashboard.html</a></li>
<li><a href=""http://jmeter.apache.org/usermanual/realtime-results.html"" rel=""nofollow noreferrer"">http://jmeter.apache.org/usermanual/realtime-results.html</a></li>
</ul>
"
Datadog,46079393,46079056,1,"2017/09/06, 18:42:48",True,"2017/09/06, 18:42:48",2688,2718295,4,"<p>Shared buffers are used for postgres memory cache (at a lower level closer to postgres as compared to OS cache). Setting it to 7gb means that pg will cache to 7gb of data. So if you are doing a lot of full table scans or (recursive) CTEs that may improve performance. Note that <code>postgres</code> master process will allocate this entire amount at database startup, which is why you are seeing your OS use 10GB of ram now.</p>

<p><code>work_mem</code> is memory used for sorts and <em>each</em> concurrent sort allocates a bucket of this size. Therefore this is only bounded by <code>max_connections</code> * concurrent sorts, so effectively it is <em>only</em> bounded by the sort complexity of your queries, so increasing this poses the most risk to system stability. (That is, if you have a single query that the query planner executes with 8 merge sorts, you will use 8*<code>work_mem</code> every time the query is executed).</p>

<p><code>maintenance_work_mem</code> is the memory used by <code>VACUUM</code> and friends (including <code>ALTER TABLE ADD FOREIGN KEY</code>! Increasing this may increase VACUUM speed.</p>

<p><code>wal_buffers</code> has no benefit beyond 16MB, which is the largest WAL chunk the server will write at one time. This can help with slow write i/o.</p>

<p>See also: <a href=""https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server"" rel=""nofollow noreferrer"">https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server</a></p>
"
Datadog,45147281,45147143,2,"2017/07/17, 17:47:26",False,"2017/07/17, 17:47:26",677,6803997,1,"<p><strong>It depends. Also, your image doesn't display as of my answer.</strong></p>

<p>If your machines are super memory hungry and you are an individual without unlimited income, I think your approach would be fine. I would recommend a slightly higher arbitrary percentage to start with, such as 50%, to provide a bit of wiggle room. Continue to analyze the memory usage and adjust your maximum accordingly. I don't see any reason to set memory usage below default.</p>

<p>Otherwise, you can be much more gratuitous and provide 100-200% extra memory, in case your application experiences sudden heavy load.</p>
"
Datadog,41028352,41025255,0,"2016/12/08, 00:14:48",False,"2016/12/08, 00:14:48",643,4214037,1,"<p>As you can see <a href=""http://docs.ansible.com/ansible/playbooks_roles.html#role-dependencies"" rel=""nofollow noreferrer"">here</a>, Ansible provides role dependecies. 
You may create in <code>Datadog.datadog</code> role new directory named meta with main.yml file. In <code>meta/main.yml</code> write </p>

<pre><code>--- 
dependencies:
    - { role: role1 }
</code></pre>

<p>After that, when you call <code>Datadog.datadog</code> role, Ansible will run <code>role1</code> automatically before <code>Datadog.datadog</code> role. </p>

<p>If you create another role named <code>Datadog.datadog1</code> with the same <code>meta/main.yml</code> file and call roles <code>Datadog.datadog</code> and <code>Datadog.datadog1</code>, then Ansible will run <code>role1</code> only once, before running Datadogs roles.</p>
"
Datadog,38387725,38359717,0,"2016/07/15, 07:08:42",True,"2016/07/15, 07:08:42",1048,3778976,1,"<p>For your data model, I would suggest adding  <strong>time</strong>  as a clustering column:</p>

<pre><code>CREATE TABLE metrics(
id uuid,
time timeuuid,
assetid int,
cpuload int,
cpuusage int,
memusage int,
diskusage int,
PRIMARY KEY (id, time) WITH CLUSTERING ORDER BY (time DESC))
</code></pre>

<p>Use descending order to keep the latest metrics first.  You can then query using the LIMIT clause to get the most recent hour:</p>

<pre><code>SELECT * FROM metrics WHERE id = &lt;UUID&gt; LIMIT 60
</code></pre>

<p>Or day:</p>

<pre><code>SELECT * FROM metrics WHERE id = &lt;UUID&gt; LIMIT 1440
</code></pre>

<p>Depending upon how long you plan to keep the data, you may want to add a  column for year, month, or days to the table to limit your partition size.  For example, if you wish to keep data for 3 months,  a <strong>month</strong> column can be added to partition your keys by id and month:</p>

<pre><code>CREATE TABLE metrics(
id uuid,
time timeuuid,
month text,
assetid int,
cpuload int,
cpuusage int,
memusage int,
diskusage int,
PRIMARY KEY ((id, month), time) WITH CLUSTERING ORDER BY (time DESC))
</code></pre>

<p>If you keep data for several years, use year + month or a date value.</p>

<p>Regarding your final question, about separate tables or a single table.  Cassandra supports sparse columns, so you can make multiple inserts in a common table for each metric without updating any data.  However, it's always faster to write just once per row.  </p>

<p>You may need separate tables if you have to query for different metrics by an alternative key.  For example, query for disk usage by id and disk name.  You'd need a separate table or a materialized view to support that query pattern.</p>

<p>Finally, your schema defines an <strong>assetid</strong>, but this isn't defined in your primary key so with your current schema you can't query using assetid.</p>
"
Datadog,36528782,35866315,0,"2016/04/10, 13:51:31",True,"2016/04/10, 13:51:31",56,355476,3,"<p>Try</p>

<pre><code>{
    ""AWSEBDockerrunVersion"": ""1"",
    ""Ports"": [
        {
            ""ContainerPort"": ""8125""
        }
    ]
}
</code></pre>
"
Datadog,61524549,61521576,0,"2020/04/30, 16:28:56",False,"2020/04/30, 16:28:56",1824,944768,3,"<p>this seem to be working:
<code>-@userId:*?*</code> do not forget the minus at the start.</p>
"
Datadog,50793070,50792558,0,"2018/06/11, 11:09:13",True,"2018/06/11, 11:09:13",7647,1958151,2,"<p>It looks like you created <em>some</em> policy, but not the policy of required type. When you create the role for Datadog, you have to choose a very specific role type:</p>

<blockquote>
  <p>Select Another AWS account for the Role Type.</p>
</blockquote>

<p>and then create a policy for that role. Also, don't forget to</p>

<blockquote>
  <p>Check off Require external ID</p>
</blockquote>

<p>You shouldn't have any problems as long as you follow the guideline step by step: <a href=""https://docs.datadoghq.com/integrations/amazon_web_services/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/amazon_web_services/</a></p>
"
Datadog,61044468,50792558,1,"2020/04/05, 17:51:15",False,"2020/04/05, 17:51:15",1,12211192,0,"<p>I had this problem, when I tried to both use the role-assumption role as an assumption role on the <code>assume_role_policy</code>, as well as trying to attach it.</p>

<p>Once I got rid of the aws_iam_policy that I created with the role-assumption policy doc as well as the role-policy attachment, it worked.</p>

<p>Hope this helps.</p>
"
Datadog,45124573,45104434,1,"2017/07/16, 06:03:15",True,"2017/07/16, 06:03:15",1171,6826691,2,"<p>The ""ReadTimeout: HTTPConnectionPool"" error can be corrected by adding a timeout parameter under instances in the elasticsearch.yaml</p>

<pre><code> timeout: 8
</code></pre>
"
Datadog,55850182,55794663,0,"2019/04/25, 16:15:12",False,"2019/04/25, 16:15:12",4059,8763847,4,"<p>What you are doing is correct only, however, the common mistake is not following the below.</p>

<blockquote>
  <p>This library MUST be imported and initialized before any instrumented
  module. When using a transpiler, you MUST import and initialize the
  tracer library in an external file and then import that file as a
  whole when building your application. This prevents hoisting and
  ensures that the tracer library gets imported and initialized before
  importing any other instrumented module.</p>
</blockquote>

<p>Basically, you cannot have <code>require(any instrumented lib)</code> (e.g. http, express, etc) before calling init() tracing function. </p>

<p><a href=""https://docs.datadoghq.com/tracing/setup/nodejs/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/tracing/setup/nodejs/</a></p>
"
Datadog,66329196,66172172,1,"2021/02/23, 10:00:35",False,"2021/02/23, 10:00:35",871,7779815,0,"<p>In the <a href=""https://medium.com/ni-tech-talk/monitoring-confluent-cloud-kafka-with-datadog-natural-intelligence-e0fed5df535"" rel=""nofollow noreferrer"">instructions</a> it refers to http://ccloudexporter_ccloud_exporter_1:2112/metrics in the open metrics file, but in my setup docker-compose gave the ccloud exporter container the name ccloud_ccloud_exporter_1.
To prevent this I added &quot;container_name: ccloud_ccloud_exporter_1&quot; in the docker compose file and used &quot;http://ccloud_ccloud_exporter_1:2112/metrics&quot; as prometheus url in the openmetrics file.</p>
"
Datadog,53585633,52176297,0,"2018/12/03, 01:28:11",False,"2018/12/03, 01:28:11",41,1970447,4,"<p>It's probably too late, but it may be useful to others.
You can set tags by using the environment variables of the application container (not the agent container) by using DD_TAGS.</p>
"
Datadog,59625224,59622491,2,"2020/01/07, 11:08:08",True,"2020/01/08, 11:23:00",336,7243426,1,"<p>What about using the template variables <a href=""https://docs.datadoghq.com/dashboards/template_variables/#pagetitle"" rel=""nofollow noreferrer"">doc</a>? </p>

<p>You could select: </p>

<ul>
<li>Name: Name, </li>
<li>Tag or Attribute: name, </li>
<li>Default Value: dev-db-master</li>
</ul>

<p>Then you'll be able to replace your <code>{name:$flavor-db-master}</code> with <code>{$Name}</code></p>

<p>Otherwise, if you actually wants the value of the template variable you have to use <code>$flavor.value</code>. I advise to use a not widget to check the actual behavior.</p>

<p>EDIT:</p>

<p>This kind of setup is not the recommended. It would be better to set two tags on your database: </p>

<ul>
<li><code>env:dev</code> or <code>env:prod</code> </li>
<li><code>dbname:db1-master</code> or <code>dbname:db2-master</code>. </li>
</ul>

<p>You would then have a unique selection of tags, <code>env:dev,dbname:db1-master</code>. It would then be easy to have a query such as:</p>

<pre><code>""q"": ""avg:aws.rds.bin_log_disk_usage{$Env,dbname:db1-master}""
</code></pre>
"
Datadog,55032271,49022133,0,"2019/03/06, 23:16:09",False,"2019/03/06, 23:16:09",141,8721010,1,"<p>One of Dockers main features is portability and it makes sense to bind datadog into that environment. That way they are packaged and deployed together and you don't have the overhead of installing datadog manually everywhere you choose to deploy.</p>

<p>What they are also implying is that you should use <strong><a href=""https://docs.docker.com/compose/overview/"" rel=""nofollow noreferrer"">docker-compose</a></strong> and turn your application / docker container into an multi-container Docker application, running your image(s) alongside the docker agent. Thus you will not need to write/build/run/manage a container via Dockerfile, but rather add the agent image to your <strong>docker-compose.yml</strong> along with its configuration. Starting your multi-container application will still be easy via:</p>

<pre><code>docker-compose up
</code></pre>

<p>Its really convenient and gives you additional features like their <a href=""https://docs.datadoghq.com/agent/autodiscovery/?tab=docker"" rel=""nofollow noreferrer"">autodiscovery</a> service.</p>
"
Datadog,66513856,66005706,0,"2021/03/07, 08:31:28",False,"2021/03/07, 08:31:28",11,15128440,0,"<p>Finally, found the solution by examining services logs.</p>
<ol>
<li><p>w32time service was configured as &quot;manual start&quot; instead of delay-auto or auto. And though &quot;Set time automatically&quot; was &quot;On&quot;, clock synchronization only happened after starting the service and resyncing. <strong>So I've changed the Startup type of the Windows time service from Manual to Automatic (Delayed Start) using the following command:</strong></p>
<p>&amp;sc.exe config w32time start= delayed-auto</p>
</li>
<li><p><strong>Disabled Time Synchronization (service task) in Task Scheduler.</strong></p>
<p>Disable-ScheduledTask -TaskName &quot;SynchronizeTime&quot; -TaskPath &quot;\Microsoft\Windows\Time Synchronization&quot;</p>
</li>
</ol>
<p>Time Skews are being fixed automatically and occur less often.</p>
"
Datadog,65786188,65785913,6,"2021/01/19, 07:57:08",True,"2021/01/19, 08:18:40",5019,10186808,1,"<p>Typical work flow will look like this (there are other methods)</p>
<ul>
<li>Choose a pattern when creating an index. Like staff-202001, staff-202002, etc</li>
<li>Add each index to an alias. Like staff</li>
</ul>
<p>This can be achieved in multiple ways, easiest is to create a template with index pattern , alias and mapping.
Example: Any new index created matching the pattern <code>staff-*</code> will be assigned with given mapping and attached to alias <code>staff</code>  and we can query <code>staff</code> instead of individual indexes and setup alerts.</p>
<p>We can use cwl--aws-containerinsights-eks-cluster-for-test-host to run queries.</p>
<pre><code>POST _template/cwl--aws-containerinsights-eks-cluster-for-test-host
{
  &quot;index_patterns&quot;: [
    &quot;cwl--aws-containerinsights-eks-cluster-for-test-host-*&quot;
  ],
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;id&quot;: {
        &quot;type&quot;: &quot;keyword&quot;
      },
      &quot;firstName&quot;: {
        &quot;type&quot;: &quot;text&quot;
      },
      &quot;lastName&quot;: {
        &quot;type&quot;: &quot;text&quot;
      }
    }
  },
  &quot;aliases&quot;: {
    &quot;cwl--aws-containerinsights-eks-cluster-for-test-host&quot;: {}
  }
}
</code></pre>
<p>Note: If unsure of mapping, we can remove mapping section.</p>
"
Datadog,66059109,63338416,0,"2021/02/05, 09:17:00",False,"2021/02/05, 09:17:00",1,15150258,0,"<p>FetchFollower acting like &quot;long poll&quot; request. It waits till it gets <em>replica.fetch.min.bytes</em> data for replication or <em>replica.fetch.wait.max.ms</em> timeout (which is by default 500ms).
So it's basically ok, it's just means that most of FetchFollower requests are waiting for data</p>
"
Datadog,58132394,58094024,3,"2019/09/27, 13:06:50",True,"2019/09/27, 13:06:50",26,5036870,1,"<p>This is likely related to this <a href=""https://github.com/containernetworking/plugins/issues/123"" rel=""nofollow noreferrer"">issue in the portmap plugin</a>. The current working theory is that a conntrack entry is created when the client pod reaches out for the UDP host port, and that entry becomes stale when the server pod is deleted, but it's not deleted, so clients keep hitting it, essentially blackholing the traffic.</p>

<p>You can try removing the conntrack entry with something like <code>conntrack -D -p udp --dport 8125</code> on one of the impacted host. If that solves the issue then that was the root cause of your problem.</p>

<p>This workaround described in the GitHub issue should mitigate the issue until a fix is merged:</p>

<p>You can add an initContainer to the server's pod to run the conntrack command when it starts:</p>

<pre><code>initContainers: 
        - image: &lt;conntrack-image&gt;
          imagePullPolicy: IfNotPresent 
          name: conntrack 
          securityContext: 
            allowPrivilegeEscalation: true 
            capabilities: 
              add: [""NET_ADMIN""] 
          command: ['sh', '-c', 'conntrack -D -p udp']
</code></pre>
"
Datadog,51727825,51727504,2,"2018/08/07, 16:19:11",False,"2018/08/07, 16:19:11",554,6529322,2,"<p>Some things I'd consider indicative of the health of the cluster are as follows:</p>

<p><strong>Offline/Under Replicated Partitions</strong>: This is a good indicator as to whether all the nodes in a cluster are even online. If one goes offline, you will almost certainly see some under-replication, and if several are offline, you might even see some offline partitions.</p>

<p><strong>Active Controller</strong>: If this keeps changing, then it means that the cluster is potentially unstable. The controller should not change regularly; if it does, then something is wrong with your cluster.</p>

<p><strong>Bytes In/Out</strong>: These show that your cluster is able to send and receive data. If these are lower than you'd expect, then it might imply that the cluster is undergoing some sort of network issue which would possibly impact the cluster health.</p>

<p>Hope this helps!</p>
"
Datadog,41252693,41252657,4,"2016/12/21, 01:13:34",False,"2016/12/21, 01:13:34",110,6938240,-1,"<p><code>.strip()</code> for removing whitespace characters.</p>

<p><code>.replace('offset=', '')</code> for removing that string.</p>

<p>You should be able to chain them too.</p>
"
Datadog,41252760,41252657,0,"2016/12/21, 01:20:55",False,"2016/12/21, 01:31:54",26687,5352399,0,"<blockquote>
  <p>How to extract the numeric value appeared after <code>offset=</code>?</p>
</blockquote>

<pre><code>import re
regex = re.compile('offset=([\d+\.\d+]+)')
string = 'offset=1.3682'

match = re.search(regex, string)
if match:
    print(match.group(0)) # prints - offset=1.3682
    print(match.group(1)) # prints - 1.3682
</code></pre>

<p><strong>Why i prefer regular expression?</strong> Because even if the string contains other keywords, regular expression will extract the numeric value which appeared after the <code>offset=</code> expression. For example, check for the following cases with my given example.</p>

<pre><code>string = 'welcome to offset=1.3682 Stackoverflow'
string = 'offset=abcd'
</code></pre>

<blockquote>
  <p>How to remove leading and trailing whitespace characters?  </p>
</blockquote>

<pre><code>string.strip()
</code></pre>

<p>will remove all the leading and trailing whitespace characters such as \n, \r, \t, \f, space.</p>

<p>For more flexibility use the following</p>

<ul>
<li>Removes only <strong>leading</strong> whitespace chars: <code>myString.lstrip()</code></li>
<li>Removes only <strong>trailing</strong> whitespace chars: <code>myString.rstrip()</code></li>
<li>Removes <strong>specific</strong> whitespace chars: <code>myString.strip('\n')</code> or <code>myString.lstrip('\n\r')</code> or <code>myString.rstrip('\n\t')</code> and so on.</li>
</ul>

<p>Reference: see this SO <a href=""https://stackoverflow.com/a/6039813/5352399"">answer</a>.</p>
"
Datadog,41252788,41252657,0,"2016/12/21, 01:23:06",False,"2016/12/21, 01:23:06",17322,763269,1,"<p>The straightforward way is:</p>

<pre><code>i.strip().split('offset=')[1]
</code></pre>

<p>For example:</p>

<pre><code>def scrape(line):
    return line.strip().split('offset=')[1]
</code></pre>

<p>Example:</p>

<pre><code>&gt;&gt;&gt; scrape('offset=1.3682')
'1.3682'
</code></pre>

<p>Up to you if you need to convert the output.</p>
"
Datadog,39060683,39060624,1,"2016/08/21, 06:47:56",True,"2016/08/21, 06:47:56",95948,1079354,1,"<p>Since it looks like <code>StatsDClient</code> is an interface of some kind, it would make your testing effort easier to simply inject this dependency into your object.  Even if you're not using an IoC container like Spring or Guice, you can still somewhat control this simply by passing an instance of it in through the constructor.</p>

<pre><code>public MetricRecorder(String namespace, StatsDClient client) {
    Preconditions.checkNotNull(namespace);
    Preconditions.checkNotNull(client);
    this.namespace = namespace;
    this.client = client;
}
</code></pre>

<p>This will make your testing simpler since all you realistically need to do is mock the object passed in during test.</p>

<p>Right now, the reason it's failing is because you're <code>new</code>ing up the instance, and Mockito (in this current configuration) isn't equipped to mock the newed instance.  In all honesty, this set up will make testing simpler to conduct, and you should only need your client configured in one area.</p>

<pre><code>@RunWith(MockitoJUnitRunner.class)
public class MetricsRecorderTest {

    @Test
    public void metricsRecorderTest() {
        StatsDClient dClientMock = Mockito.mock(StatsDClient.class);
        MetricRecorder recorder = new MetricRecorder(""dev"", dClientMock);
        recorder.inc(""foo"", 1);
        verify(recorder.metrics).recordHistogramValue(eq(""dev.foo""), 1);
    }
}
</code></pre>
"
Datadog,39061963,39060624,0,"2016/08/21, 10:42:11",False,"2016/08/21, 10:42:11",126469,1531124,0,"<p>You are getting things wrong here. You don't use a <strong>mocking</strong> framework to test your ""class under test"".</p>

<p>You use the mocking framework to create <strong>mocked</strong> objects; which you then pass to your ""class under test"" within a test case. Then your ""code under test"" calls methods on the mocked object; and by controlling returned values (or by verifying what happens to your mock); that is how you write your testcases.</p>

<p>So, your testcase for a MetricRecorder doesn't mock a MetricRecorder; it should mock the StatsDClient class; and as Makoto suggests; use <strong>dependency</strong> injection to put an object of that class into MetricRecorder.</p>

<p>Besides: basically writing ""test-able"" code is something that needs to be practiced. I wholeheartedly recommend you to watch these <a href=""https://www.youtube.com/playlist?list=PLD0011D00849E1B79"" rel=""nofollow"">videos</a> if you are serious about getting in this business. All of them; really (worth each second!).</p>
"
Datadog,66955358,66943560,0,"2021/04/05, 18:20:48",False,"2021/04/05, 18:20:48",156,2676108,1,"<p>You could tag your metrics with the name or ID of the agent it is collecting metrics from (if you aren't already). Then in Datadog you could write a query that groups by the agent ID and applies a count_not_null function: <a href=""https://docs.datadoghq.com/dashboards/functions/count/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/dashboards/functions/count/</a></p>
<p>This basically hijacks a random metric to extract the unique count of agents reporting that metric to assume the total count of agents. You wouldn't be able to easily group by queue though, so idk if it would be a good solution to your use case.</p>
<hr />
<p>Your idea around using gauges sounds good to me. You can send a new metric called something like <code>myagent.running</code> which sends a value of 1 for each of your agents and does a sum of all gauges in order to get a count. That is actually how the metric <code>datadog.agent.running</code> is implemented: <a href=""https://docs.datadoghq.com/integrations/agent_metrics/#metrics"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/agent_metrics/#metrics</a></p>
"
Datadog,66355760,66271979,0,"2021/02/24, 19:20:38",False,"2021/02/24, 19:20:38",156,2676108,0,"<p>That endpoint is used to send events to Datadog, if your one account is sending them to the logs product, it is most likely because that account has a special flag that converts all your incoming events into logs. This is common for users that use the Security Monitoring product.</p>
<p>I would recommend reaching out to support@datadoghq.com to see if this flag is enabled in that one account. And if you want to replicate that behavior in another account, you can ask them to enable that feature.</p>
"
Datadog,59757720,59757258,2,"2020/01/15, 20:38:32",False,"2020/01/15, 20:38:32",336,7243426,1,"<p>If you have an ever increasing counter, you can use the a function called <a href=""https://docs.datadoghq.com/dashboards/functions/rate/"" rel=""nofollow noreferrer"">rate</a>. You'll be able to select it with the <code>+</code> on the query line. With that you'll be able to have a rate of increase per seconds, minutes or hours.</p>

<p>If you are looking to get a difference between the same metric but at another point in the past, you have a function called <a href=""https://docs.datadoghq.com/dashboards/functions/timeshift/"" rel=""nofollow noreferrer"">timeshift</a> that could also help. This is also accessible with the small <code>+</code> on the right of the query line.</p>

<p>Finally, if you are looking at comparing two different metrics, you  have a button called <a href=""https://docs.datadoghq.com/dashboards/querying/#arithmetic-between-two-metrics"" rel=""nofollow noreferrer"">Advanced</a> that will enable you to write more complex queries such as a difference between two metrics.</p>
"
Datadog,62228894,62094698,0,"2020/06/06, 10:54:35",False,"2020/06/06, 10:54:35",21,7314273,0,"<p>Maybe you can ask them to add it by opening a feature request:
<a href=""https://github.com/DataDog/documentation/issues/new/choose"" rel=""nofollow noreferrer"">https://github.com/DataDog/documentation/issues/new/choose</a></p>
"
Datadog,62008387,61971889,0,"2020/05/25, 21:21:06",True,"2020/05/25, 21:21:06",2771,842302,0,"<blockquote>
  <p>but it looks like I'm missing an important part here.</p>
</blockquote>

<p>That was the point. The lambda itself has not much todo with particular <code>statusCodes</code>. So I either may log each status code and let datadog parse it accordingly. </p>

<p>Or, that's the solution I went for, I can leverage API-Gateway for monitoring status codes per lambda.</p>
"
Datadog,58689932,58607248,0,"2019/11/04, 10:54:36",False,"2019/11/04, 10:54:36",336,7243426,0,"<p>Not sure I fully grasp the issue. Here are some steps to collect your traces:</p>

<ol>
<li>Enable trace collection on Kubernetes and open relevant port (8126) <a href=""https://docs.datadoghq.com/agent/kubernetes/daemonset_setup/?tab=k8sfile#apm-and-distributed-tracing"" rel=""nofollow noreferrer"">doc</a></li>
<li>Configure your app to send traces to the right container. Here is an example to adapt based on your situation. <a href=""https://docs.datadoghq.com/tracing/setup/java/"" rel=""nofollow noreferrer"">doc on java
instrumentation</a></li>
</ol>

<pre><code>        env:
          - name: DD_AGENT_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
</code></pre>

<p>Just in case, more info on Open Tracing <a href=""https://docs.datadoghq.com/tracing/advanced/opentracing/?tab=java"" rel=""nofollow noreferrer"">here</a></p>
"
Datadog,58547485,58502004,6,"2019/10/24, 21:47:16",True,"2019/10/24, 21:47:16",336,7243426,1,"<p>Have you tried <a href=""https://docs.datadoghq.com/monitors/monitor_types/composite/#pagetitle"" rel=""nofollow noreferrer"">composite monitors</a>? You should be able to combine your low CPU Credit monitor with another monitor that looks at events from RDS.</p>

<p>Two monitors such as:</p>

<ul>
<li>A: CPU Credit &lt; 10</li>
<li>B: Number of event received about RDS creation > 1 in past hour</li>
</ul>

<p>A composite monitor: A &amp;&amp; !B</p>

<p>(I hope my example makes sense)</p>
"
Datadog,64437131,60326963,0,"2020/10/20, 04:12:43",False,"2020/10/20, 04:12:43",11,7000092,0,"<p>I solved this by adding 'squashfs' to the list of filesystem types to be ignored by the datadog agent.</p>
<p>Create a file <code>/etc/datadog-agent/conf.d/disk.d/conf.yaml</code>:</p>
<pre><code>init_config:
    file_system_global_blacklist:
      - iso9660$
      - squashfs

instances:
  - use_mount: false

</code></pre>
<p>Restart datadog agent (<code>systemctl restart datadog-agent</code>).</p>
"
Datadog,59747217,59088171,0,"2020/01/15, 10:05:27",False,"2020/01/15, 10:05:27",111,5054074,0,"<p>I am not a fan of helm, but you can accomplish this in 2 ways:</p>

<ul>
<li><p>via env vars: make use of <code>DD_AC_EXCLUDE</code> variable to exclude the Redis containers: eg <code>DD_AC_EXCLUDE=name:prefix-redis</code></p></li>
<li><p>via a config map: mount an empty config map in <code>/etc/datadog-agent/conf.d/redisdb.d/</code>, below is an example where I renamed the <code>auto_conf.yaml</code> to <code>auto_conf.yaml.example</code>.</p></li>
</ul>

<pre><code>apiVersion: v1
data:
  auto_conf.yaml.example: |
    ad_identifiers:
      - redis    init_config:    instances:
        ## @param host - string - required
        ## Enter the host to connect to.
        #
      - host: ""%%host%%""        ## @param port - integer - required
        ## Enter the port of the host to connect to.
        #
        port: ""6379""
  conf.yaml.example: |
    init_config:    instances:        ## @param host - string - required
        ## Enter the host to connect to.
        # [removed content]
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: redisdb-d
</code></pre>

<p>alter the daemonset/deployment object: </p>

<pre><code>[....]
        volumeMounts:
        - name: redisdb-d
          mountPath: /etc/datadog-agent/conf.d/redisdb.d
[...]
      volumes:
      - name: redisdb-d
        configMap:
          name: redisdb-d

[...]
</code></pre>
"
Datadog,64407940,64108531,1,"2020/10/18, 01:24:48",True,"2020/10/18, 01:24:48",493,244037,0,"<p>In these kinds of checks, the response order matters, since the columns returned from the DB are going to be mapped back to the names specified in the YAML.</p>
<p>Reading the error message:</p>
<blockquote>
<p>Error: postgres:953578488181a512 | (postgres.py:398) | non-numeric value <code>cldtx</code> for metric column <code>active_connections</code> of metric_prefix <code>postgresql</code></p>
</blockquote>
<p>We can see that the value of <code>cldtx</code> is being returned for the <code>active_connections</code> column, which in the YAML is declared as a gauge, and this is a string.</p>
<p>The fix should be straightforward, by reordering the YAML, like so:</p>
<pre class=""lang-yaml prettyprint-override""><code>...
     columns:
       - name: db_name
         type: tag
       - name: active_connections
         type: gauge
</code></pre>
<p>Alternately, if you want to keep the YAML ordered, change the query to:</p>
<pre class=""lang-yaml prettyprint-override""><code>...
     query: SELECT count(pid) as active_connections, datname as db_name FROM pg_stat_activity where state = 'active' group by db_name;
...
</code></pre>
"
Datadog,64484665,64476688,0,"2020/10/22, 17:40:57",True,"2020/10/22, 17:40:57",156,2676108,0,"<p>The datadog agent you deployed has no power to run scripts or take action. It is purely a monitoring/data collection tool.</p>
<hr />
<p>However one of the things your monitors in the Datadog application can do is trigger events when they go into an alert state. There are <a href=""https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#integrations"" rel=""nofollow noreferrer"">lots of integrations</a>: creating a ticket in Jira, posting a message to Slack, triggering an SNS topic.</p>
<p>What I recommend you try to do, is to create some kind of job or script that can be triggered externally, like a lambda function, or a jenkins job, or anything really. When the monitor goes off you can use a <a href=""https://docs.datadoghq.com/integrations/webhooks/"" rel=""nofollow noreferrer"">webhook</a> to trigger that script to do whatever you define. Here is a blog post showing <a href=""https://www.datadoghq.com/blog/send-alerts-sms-customizable-webhooks-twilio/"" rel=""nofollow noreferrer"">how twilio sent out a text message by connecting their api to a webhook</a>.</p>
"
Datadog,58708338,58579323,0,"2019/11/05, 11:33:57",False,"2021/02/05, 00:01:35",4059,8763847,1,"<p>Seems like there is a typo in your command. <code>DD_DOGSTATD_NON_LOCAL_TRAFFIC </code> is used instead of <code>DD_DOGSTATSD_NON_LOCAL_TRAFFIC</code></p>
<p>I usually used the below command for testing with Datadog:</p>
<pre><code>DOCKER_CONTENT_TRUST=1 docker run -d \
    --name dd-agent \
    -v /var/run/docker.sock:/var/run/docker.sock:ro \
    -v /proc/:/host/proc/:ro \
    -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \
    -e DD_API_KEY=&lt;api-key&gt; \
    -e DD_DOGSTATSD_NON_LOCAL_TRAFFIC=&quot;true&quot; \
    -p 8125:8125/udp \
    -p 8126:8126/tcp \
    datadog/agent:latest
</code></pre>
"
Datadog,50785165,47436429,0,"2018/06/10, 18:22:21",True,"2018/06/10, 18:22:21",518,3605831,0,"<p>Yes it does have this functionality in the Audit Association entity. The entity stored with the blame_id in the Audit Log entity contains information regarding the user. The one with source_id contains information regarding the entity itself and thus the ID of the entity in the <code>fk</code> field.</p>
"
Datadog,24537072,24536971,1,"2014/07/02, 20:08:18",True,"2014/07/07, 19:45:33",29,3794458,0,"<p>Kamon was being built for Java 1.7 by default. Now, it will support 1.6</p>
"
Datadog,62747357,62739543,0,"2020/07/06, 02:24:28",True,"2020/07/06, 02:24:28",251,5477963,1,"<p>It looks from the above snippets that the <code>combined</code> Morgan format is sent directly sent to Winston, and then parsed within a log pipeline in Datadog. Since the <code>combined</code> format doesn't include the body and there is no built-in token for it, you would have to use a custom format with your own tokens and then update your pipeline accordingly.</p>
<p>For example, to create a custom format in Morgan that includes the status code and the body:</p>
<pre class=""lang-js prettyprint-override""><code>morgan((tokens, req, res) =&gt; [
  tokens.status(req, res),
  req.body // assuming body-parser middleware is used
].join(' '))
</code></pre>
<p>You can also create a token to achieve the same result with a simpler format definition:</p>
<pre class=""lang-js prettyprint-override""><code>morgan.token('body', (req, res) =&gt; req.body
morgan(':status :body')
</code></pre>
<p>You can find the documentation for custom Morgan formats <a href=""http://expressjs.com/en/resources/middleware/morgan.html#using-format-string-of-predefined-tokens"" rel=""nofollow noreferrer"">here</a>, creating tokens <a href=""http://expressjs.com/en/resources/middleware/morgan.html#creating-new-tokens"" rel=""nofollow noreferrer"">here</a>, and Datadog log pipeline parsing <a href=""https://docs.datadoghq.com/logs/processing/parsing/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Hope this helps!</p>
"
Datadog,66453250,66443203,2,"2021/03/03, 10:27:46",False,"2021/03/04, 10:26:16",1249,13336642,0,"<p>Datadog’s IIS integration queries the Web Service performance counters automatically and sends the results to Datadog. The Web Service performance counter class collects information from the World Wide Web Publishing Service.</p>
<p>You can enable the IIS integration by creating a configuration file either manually or through the Datadog Agent GUI. To create a configuration file through the GUI, navigate to the “Checks” tab, choose “Manage Checks,” and select the iis check from the “Add a Check” menu. You can also manually create a conf.yaml file in C:\ProgramData\Datadog\conf.d\iis.d.</p>
<p>There is a sites attribute in the conf.yaml file. This attribute represents the IIS site you want to monitor. You only need to delete the sites you want to exclude.</p>
<p>More information you can refer to this link: <a href=""https://www.datadoghq.com/blog/iis-monitoring-datadog/"" rel=""nofollow noreferrer"">IIS monitoring with Datadog</a>.</p>
"
Datadog,66642805,66443203,0,"2021/03/15, 19:33:06",True,"2021/03/15, 19:33:06",16566,24231,0,"<p>If you are using Datadog's .NET Tracer, you can set <code>DD_TRACE_ENABLED=false</code> in the <code>appSettings</code> section of the <code>web.config</code> file (<a href=""https://docs.datadoghq.com/tracing/setup_overview/setup/dotnet-framework/?tab=webconfig#configuration"" rel=""nofollow noreferrer"">docs</a>). For example:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;configuration&gt;
  &lt;appSettings&gt;
    &lt;add key=&quot;DD_TRACE_ENABLED&quot; value=&quot;false&quot;/&gt;
  &lt;/appSettings&gt;
&lt;/configuration&gt;
</code></pre>
<p>Another option is to deploy a <code>datadog.json</code> file (<a href=""https://docs.datadoghq.com/tracing/setup_overview/setup/dotnet-framework/?tab=jsonfile#configuration"" rel=""nofollow noreferrer"">docs</a>) in the root of your app that contains:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;DD_TRACE_ENABLED&quot;: &quot;false&quot;
}
</code></pre>
<p>(Disclaimer: I work at Datadog)</p>
"
Datadog,59177979,59177043,0,"2019/12/04, 16:10:33",False,"2019/12/04, 16:10:33",807,1506396,2,"<p>I believe you are looking for <code>clusterName</code>:
<a href=""https://github.com/helm/charts/blob/master/stable/datadog/values.yaml#L75"" rel=""nofollow noreferrer"">https://github.com/helm/charts/blob/master/stable/datadog/values.yaml#L75</a></p>

<p>You can add it in your <code>values.yaml</code> under the <code>datadog</code> section like this:</p>

<pre><code>datadog:
  clusterName: myexamplename
</code></pre>
"
Datadog,59178230,59177043,0,"2019/12/04, 16:24:18",False,"2019/12/04, 16:24:18",9281,3270785,2,"<p>refer below command</p>

<pre><code>helm install --name datadog-monitoring \
    --set datadog.apiKey=&lt;DATADOG_API_KEY&gt; \
    --set datadog.appKey=&lt;DATADOG_APP_KEY \
    --set clusterAgent.enabled=true \
    --set clusterAgent.metricsProvider.enabled=true \
    --set datadog.clusterName=&lt;CLUSTER_NAME&gt; \
    stable/datadog
</code></pre>
"
Datadog,62812732,61405563,0,"2020/07/09, 13:23:09",False,"2020/07/09, 13:23:09",877,1570636,0,"<p>Just add this in the startup.cs.</p>
<pre><code>var config = new DatadogConfiguration(&quot;https://http-intake.logs.datadoghq.eu&quot;);
        Serilog.Log.Logger = new LoggerConfiguration()
       .WriteTo.DatadogLogs(&quot;&lt;Datadog_API_KEY&gt;&quot;, configuration: config)
       .CreateLogger();
// Call this from anywhere in the app
Serilog.Log.Logger.Information(&quot;This is a test from session-start&quot;);
Serilog.Log.Logger.Error(&quot;This is a test from session-start&quot;);
</code></pre>
"
Datadog,65504306,61405563,0,"2020/12/30, 10:52:27",False,"2020/12/30, 10:52:27",365,5666309,0,"<p>I'm very sorry for this late answer.</p>
<p>After exchanging a bunch of emails with the Datadog support guys, it turned out that the solution is straightforward:</p>
<pre><code>[OneTimeTearDown]
public virtual void Cleanup()
{
    logger.Dispose();
}
</code></pre>
<p>The <code>Dispose()</code> method force the sinks to gracefully close up and send the logs stored in cache.</p>
<p>Please note that the logs do not appear instantly in the Datadog console: allow a few seconds (to some minutes) for their systems to process the logs you send.</p>
"
Datadog,58695216,58694140,1,"2019/11/04, 16:13:25",False,"2019/11/04, 16:13:25",336,7243426,1,"<p>On the bottom of the infrastructure list, you should see a link called ""JSON API permalink"". If you <a href=""https://app.datadoghq.com/reports/v2/overview"" rel=""nofollow noreferrer"">query it</a>, this should give you a JSON of all your hosts with their agent version. You can then query it with a quick Python script.</p>
"
Datadog,58647078,58641893,0,"2019/10/31, 17:55:11",False,"2019/10/31, 17:55:11",14987,90580,0,"<p>Your metrics should have a common prefix like <code>myapp.metric1</code>, <code>myapp.metric2</code>, etc. 
Then you can disable all metrics and enable explicitly all <code>myapp.*</code> metrics like so:</p>

<p>application.properties:</p>

<pre><code>management.metrics.enable.all=false
management.metrics.enable.myapp=true
</code></pre>

<p>the <code>management.metrics.enable.&lt;your_custom_prefix&gt;</code> will enable all <code>&lt;your_custome_prefix&gt;.*</code> metrics. </p>

<p>If you want to enable some of the built-in core metrics again, for example reenabling <code>jvm.*</code>, you can do: </p>

<pre><code>management.metrics.enable.all=false
management.metrics.enable.myapp=true
management.metrics.enable.jvm=true
</code></pre>

<p>I've created a sample project <a href=""https://github.com/ecerulm/spring-boot-app-with-metrics"" rel=""nofollow noreferrer"">in github</a> that disables core metrics, enables custom metrics, and <code>jvm.*</code> metrics and sends to Datadog. </p>
"
Datadog,60754931,60715172,4,"2020/03/19, 12:10:42",False,"2020/03/19, 12:10:42",37971,1237575,0,"<p>Spring is scanning your classpath that seems incomplete. Maybe it is related to Spring's class loading mechanisms. The <a href=""https://github.com/DataDog/dd-trace-java/blob/7df5cfe3e3f1710d8840522232f18496dc6be530/dd-java-agent/instrumentation/spring-scheduling-3.1/src/main/java/datadog/trace/instrumentation/springscheduling/SpringSchedulingInstrumentation.java"" rel=""nofollow noreferrer"">class in question</a> exists and seems to be part of the agent.</p>

<p>Possibly, you are using an outdated version of the agent.</p>
"
Datadog,47294550,41578245,0,"2017/11/14, 22:22:41",False,"2017/11/14, 22:22:41",51,2454643,0,"<p>Once the datadog is properly installed on your server, you can use the custom metric feature to let datadog read your query result into a custom metric and then use that metric to create a dashboard.</p>

<p>You can find more on custom metric on datadog <a href=""https://help.datadoghq.com/hc/en-us/articles/208385813-Postgres-custom-metric-collection-explained"" rel=""nofollow noreferrer"">here</a></p>

<p>They work with yaml file so be cautious with the formatting of the yaml file that will hold your custom metric.</p>
"
Datadog,53935650,53222715,0,"2018/12/26, 20:21:57",True,"2018/12/26, 20:21:57",347,2650254,0,"<p>so, I had to install a Datadog agent on ec2 instance and configure it to be able to access all mysql dbs and collect the mysql performance schema metrics. And surely with correct security groups for the ec2.
I followed this <a href=""https://docs.datadoghq.com/integrations/amazon_rds/#native-database-integration-1"" rel=""nofollow noreferrer"">docs</a> and <a href=""https://www.datadoghq.com/blog/monitoring-rds-mysql-performance-metrics/"" rel=""nofollow noreferrer"">this</a> and contacted the support.</p>
"
Datadog,51871692,51707255,1,"2018/08/16, 10:24:11",True,"2018/08/16, 10:24:11",76,4481158,0,"<p>If the Windows OS is D drive, the setting is installed in <code>D:\ProgramData\Datadog</code>.
Copying it to <code>C:\ProgramData\Datadog</code> will work, but I submitted an improvement request to Datadog Support.</p>
"
Datadog,60660835,60615996,1,"2020/03/12, 21:36:18",False,"2020/03/12, 21:36:18",28832,281848,1,"<p>I'm not particularly familiar with this Datadog JSON format but the general pattern I would propose here has multiple steps:</p>

<ul>
<li>Decode the serialized data into a normal Terraform value. In this case that would be using <a href=""https://www.terraform.io/docs/configuration/functions/jsondecode.html"" rel=""nofollow noreferrer""><code>jsondecode</code></a>, because the data is JSON-serialized.</li>
<li>Transform and normalize that raw data into a consistent shape that is more convenient to use in a declarative Terraform configuration. This will usually involve at least one <a href=""https://www.terraform.io/docs/configuration/locals.html"" rel=""nofollow noreferrer"">named local value</a> containing an expression that uses <a href=""https://www.terraform.io/docs/configuration/expressions.html#for-expressions"" rel=""nofollow noreferrer""><code>for</code> expressions</a> and <a href=""https://www.terraform.io/docs/configuration/functions/try.html"" rel=""nofollow noreferrer"">the <code>try</code> function</a>, along with the type conversion functions, to try to force the raw data into a more consistent shape.</li>
<li>Use the transformed/normalized result with Terraform's resource and block repetition constructs (resource <code>for_each</code> and <code>dynamic</code> blocks) to describe how the data maps onto physical resource types.</li>
</ul>

<p>Here's a basic example of that to show the general principle. It will need more work to capture all of the details you included in your initial example.</p>

<pre><code>variable ""datadog_json"" {
  type = string
}

locals {
  raw = jsondecode(var.datadog_json)
  screenboard = {
    title       = local.raw.title
    description = try(local.raw.description, tostring(null))
    widgets = [
      for w in local.raw.widgets : {
        type   = w.definition.type

        title       = w.definition.title
        title_size  = try(w.definition.title_size, 16)
        title_align = try(w.definition.title_align, ""center"")

        x      = try(w.definition.x, tonumber(null))
        y      = try(w.definition.y, tonumber(null))
        width  = try(w.definition.x, tonumber(null))
        height = try(w.definition.y, tonumber(null))

        requests = [
          for r in w.definition.requests : {
            q            = r.q
            display_type = r.display_type
            style        = tomap(try(r.style, {}))
          }
        ]
      }
    ]
  }
}

resource ""datadog_screenboard"" ""acceptance_test"" {
  title       = local.screenboard.title
  description = local.screenboard.description
  read_only   = true

  dynamic ""widget"" {
    for_each = local.screenboard.widgets
    content {
      type = widget.value.type

      title       = widget.value.title
      title_size  = widget.value.title_size
      title_align = widget.value.title_align

      x      = widget.value.x
      y      = widget.value.y
      width  = widget.value.width
      height = widget.value.height

      tile_def {
        viz = widget.value.type

        dynamic ""request"" {
          for_each = widget.value.requests
          content {
            q            = request.value.q
            display_type = request.value.display_type
            style        = request.value.style
          }
        }
      }
    }
  }
}
</code></pre>

<p>The separate normalization step to build <code>local.screenboard</code> here isn't strictly necessary: you could instead put the same sort of normalization expressions (using <code>try</code> to set defaults for things that aren't set) directly inside the <code>resource ""datadog_screenboard""</code> block arguments if you wanted. I prefer to treat normalization as a separate step because then this leaves a clear definition in the configuration for what we're expecting to find in the JSON and what default values we'll use for optional items, separate from defining how that result is then mapped onto the physical <code>datadog_screenboard</code> resource.</p>

<p>I wasn't able to test the example above because I don't have a Datadog account. I'm sorry if there are minor typos/mistakes in it that lead to errors. My hope was to show the general principle of mapping from a serialized data file to a resource rather than to give a ready-to-use solution, so I hope the above includes enough examples of different situations that you can see how to extend it for the remaining Datadog JSON features you want to support in this module.</p>

<hr>

<p>If this JSON format is a interchange format formally documented by Datadog, it could make sense for Terraform's Datadog provider to have the option of accepting a single JSON string in this format as configuration, for easier exporting. That may require changes to the Datadog provider itself, which is beyond what I can answer here but might be worth raising in the GitHub issues for that provider to streamline this use-case.</p>
"
Datadog,66632382,66606637,0,"2021/03/15, 06:26:59",True,"2021/03/15, 06:26:59",902,10709519,0,"<p>After testing different queries I found that running this query groups the results by query statements and will return the count of each.</p>
<pre><code>SELECT COUNT(*) as Query_Count, DB, INFO as Query FROM 
INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query' AND TIME &gt; (5 * 60 * 1000) 
GROUP BY QUERY;
</code></pre>
<p>In Datadogs mysql configuration I added tags for the query statement and database name. Now since they are grouped, I can see information per different statement in datadog.</p>
<pre><code>    custom_queries:
      - query: SELECT COUNT(*) as Query_Count, DB, INFO as Query FROM 
        INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND='Query' GROUP BY QUERY;
        columns:
         - name: mysql.processlist.Slow_Query.Query_Count
           type: count
         - name: mysql.processlist.Slow_Query.DB
           type: tag
         - name: mysql.processlist.Slow_Query.Query
           type: tag
</code></pre>
"
Datadog,61225485,61191225,0,"2020/04/15, 12:32:12",False,"2020/04/15, 12:32:12",21,7314273,1,"<p>You might want to use datadog service-check which is also can be sent by the StatsDClient, and then add it to your monitor/dashboard page.
<a href=""https://docs.datadoghq.com/developers/service_checks/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/developers/service_checks/</a></p>
"
Datadog,61232317,61191225,0,"2020/04/15, 18:18:06",False,"2020/04/15, 18:18:06",1371,5540166,0,"<p>Another option is to use <a href=""https://docs.datadoghq.com/integrations/java/?tab=host"" rel=""nofollow noreferrer"">Datadog's Java / JMX integration</a> to capture the health data that's exposed over JMX -- this can probably give you up/down health data, and can certainly give you a lot more granular health metrics too. </p>
"
Datadog,61822822,61817555,1,"2020/05/15, 18:27:58",False,"2020/05/15, 18:27:58",1371,5540166,2,"<p>You can use most any of the common open source log shippers to send server logs to Datadog without using the Datadog agent, for example <a href=""https://docs.datadoghq.com/integrations/fluentd/"" rel=""nofollow noreferrer"">fluentd</a>. But there can be several benefits to using the Datadog agent to collect server logs, such as:</p>

<ul>
<li>If you are using the Datadog agent for other monitoring data already, it saves you having to run/manage more software for log collection</li>
<li>It's the safest way to make sure you get all the right tags applied to both your logs and other monitoring data (like metrics, traces, etc.) for better correlation of data when you're investigating stuff. </li>
</ul>

<p>There are other ways to collect logs in Datadog, among those is the <a href=""https://docs.datadoghq.com/api/v1/logs/#get-a-list-of-logs"" rel=""nofollow noreferrer"">HTTP API</a>. Since this API uses a POST method, I bet you could configure Datadog's <a href=""https://docs.datadoghq.com/integrations/webhooks/"" rel=""nofollow noreferrer"">webhook integration</a> to generate log events from Datadog events and alerts. That said, before you go through the trouble of doing this, if you have a use-case or reason you're interested in doing this, you may want to reach out to Datadog support to see if they have some features coming / in beta that would get you what you want without the extra work on your end. (What <em>is</em> your use-case? I'm curious)</p>
"
Datadog,64847039,64847038,3,"2020/11/15, 18:43:14",False,"2020/11/15, 18:43:14",1072,252340,2,"<p>In Datadog, create an API key in Integrations, APIs. Give the API key a name.</p>
<p>In NLog.config create a target. The url is either datadoghq.com or datadoghq.eu (for europe).</p>
<pre><code> &lt;target xsi:type=&quot;WebService&quot;
        name=&quot;datadog&quot;
        url=&quot;https://http-intake.logs.datadoghq.com/v1/input&quot;
        encoding=&quot;utf-8&quot;
        protocol=&quot;JsonPost&quot;
        preAuthenticate=&quot;false&quot; &gt;
        &lt;parameter name='date' type='System.String' layout='${longdate}'/&gt; 
        &lt;parameter name='ipaddress' type='System.String' layout='${aspnet-request-ip}'/&gt;
        &lt;parameter name=&quot;userid&quot; type='System.String' layout=&quot;${aspnet-User-Identity}&quot; /&gt;
        &lt;parameter name=&quot;level&quot;  type='System.String'  layout=&quot;${level:upperCase=true}&quot;/&gt;
        &lt;parameter name=&quot;version&quot; type='System.String'  layout=&quot;${configsetting:name=VersionSettings.Version:default=?}&quot; /&gt;
        &lt;parameter name=&quot;threadid&quot;  type='System.String' layout=&quot;${threadid}&quot; /&gt;
        &lt;parameter name=&quot;controller&quot;  type='System.String' layout=&quot;${aspnet-mvc-controller}&quot; /&gt;
        &lt;parameter name=&quot;class&quot; type='System.String'  layout=&quot;${callsite:className=True:includeNamespace=False:fileName=False:includeSourcePath=False:methodName=True:cleanNamesOfAnonymousDelegates=True:cleanNamesOfAsyncContinuations=True}&quot; /&gt;
        &lt;parameter name=&quot;message&quot; type='System.String'  layout=&quot;${message}&quot; /&gt;
        &lt;parameter name=&quot;elapsed&quot;  type='System.String' layout =&quot;${event-properties:item=elapsed}&quot; /&gt;
        &lt;parameter name=&quot;service&quot;  type='System.String' layout=&quot;${configsetting:name=Nlog.Component}&quot; /&gt;
        &lt;parameter name=&quot;hostname&quot;  type='System.String' layout=&quot;${configsetting:name=Nlog.HostName}&quot; /&gt;
        &lt;parameter name=&quot;exception&quot;  type='System.String' layout=&quot;${exception:format=ToString}&quot; /&gt;
        &lt;parameter name=&quot;ddsource&quot;  type='System.String' layout=&quot;csharp&quot; /&gt;
        &lt;header name=&quot;DD-API-KEY&quot; layout=&quot;${configsetting:name=Nlog.datadog}&quot;/&gt;
        &lt;header name=&quot;Content-Type&quot; layout=&quot;application/json&quot;/&gt;
&lt;/target&gt;       
</code></pre>
<p>Now create a rule to write to the target and you are done!</p>
<p>All of the parameters can be configured to become columns in Datadog, and/or facets to select on. I am using a date parameter so that the date matches other logs, rather than displaying the built in date.</p>
"
Datadog,67184949,67161917,0,"2021/04/20, 22:09:14",False,"2021/04/20, 22:09:14",116,9839284,0,"<p>As told by @TRW in the comments, using this should do the trick:</p>
<pre><code>- hosts: servers
  roles:
    - { role: datadog.datadog, become: yes }
  vars:
    datadog_api_key: &quot;{{ DD_API_KEY }}&quot;
    datadog_config:
      tags: &quot;{{ ['AID: ']|product(AID.split())|map('join')|list }}&quot;
</code></pre>
"
Datadog,66871585,66761816,0,"2021/03/30, 16:09:45",True,"2021/03/30, 16:09:45",329,6301287,0,"<p>I had to open a ticket asking the Heroku CS team to apply the &quot;pg_monitor&quot; role to my user. They've granted the role and now everything is working fine</p>
"
Datadog,65746430,65744466,0,"2021/01/16, 06:12:41",False,"2021/01/16, 06:12:41",13378,1430810,1,"<p>Maybe it's a complicated idea but I think you can make your own cache store wrapper that decides which cache store to use if I understand your question correctly.</p>
<p>When calling the cache method, it eventually calls <code>read_fragment</code> and <code>write_fragment</code> on your controller <a href=""https://apidock.com/rails/AbstractController/Caching/Fragments/read_fragment"" rel=""nofollow noreferrer"">https://apidock.com/rails/AbstractController/Caching/Fragments/read_fragment</a> and those methods call <code>cache_store.read</code> and <code>cache_store.write</code>.</p>
<p>Then you could have a custom cache store class with custom <code>read</code> and <code>write</code> method that, depending on an option, delegates the read and write to real cache stores.</p>
<pre class=""lang-rb prettyprint-override""><code>class MyCacheStore &lt; ...
  def initialize
    @my_catchall_store = ActiveSupport::Cache::MemCacheStore.new
    @my_order_store = ActiveSupport::Cache::MemCacheStore.new
  end

  def read(key, options)
    case options[:store]
    when :order_store then @my_order_store.read(key, options)
    else
      @my_catchall_store.read(key, options)
    end
  end

  # similar for .write
</code></pre>
<p>Then you use it like...</p>
<pre class=""lang-rb prettyprint-override""><code>cache(:order, store: :order_store) do
  # some code
end

cache(:something_else) do
  # some code
end
</code></pre>
<p>I'm not sure if that's what you are asking sorry.</p>
"
Datadog,65026062,65009092,2,"2020/11/26, 18:39:43",True,"2020/11/26, 18:39:43",782,1953079,1,"<p>Easy solution is to fetch this library directly and do <code>add_subdirectory</code>. But this requires cmake &gt;= 3.11.</p>
<p>Create dir <code>cmake</code> and file <code>cmake/cpp-datadogstatsd.cmake</code></p>
<p><code>cpp-datadogstatsd.cmake</code>:</p>
<pre><code>FetchContent_Declare(
        datadogstatsd
        GIT_REPOSITORY https://github.com/BoardiesITSolutions/cpp-datadogstatsd
        # try v1.1.0.5 if this does not work
        GIT_TAG        1.1.0.5
)

FetchContent_GetProperties(datadogstatsd)
if(NOT datadogstatsd_POPULATED)
    message(STATUS &quot;Downloading datadogstatsd...&quot;)

    FetchContent_Populate(datadogstatsd)
    add_subdirectory(${datadogstatsd_SOURCE_DIR} ${datadogstatsd_BINARY_DIR} EXCLUDE_FROM_ALL)
endif()

</code></pre>
<p>Then, include this cmake file, and link <code>DataDogStatsD_static</code> to your lib/exe:</p>
<pre><code>include(cmake/cpp-datadogstatsd.cmake)

add_executable(test main.cpp)
target_link_libraries(test DataDogStatsD_static)
</code></pre>
"
Datadog,62864281,62819086,0,"2020/07/12, 20:27:18",False,"2020/07/12, 20:27:18",449,12318748,0,"<p>As you have limited requirements, you could achieve this without a bot.</p>
<p>MS Teams has income and outgoing webhooks. You could create a <a href=""https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook"" rel=""nofollow noreferrer"">Incoming webhook</a> inside a Teams channel. It provides an URL which you could use inside the monitoring remote server and POST the message in JSON format to the webhook url. It will be posted in teams channel like below
<a href=""https://i.stack.imgur.com/ITohF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ITohF.png"" alt=""enter image description here"" /></a></p>
<p>For sending message back to the server you need to configure the <a href=""https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-outgoing-webhook"" rel=""nofollow noreferrer"">Outgoing webhook</a> in the channel.</p>
"
Datadog,62710050,62699424,6,"2020/07/03, 09:57:59",True,"2020/07/03, 09:57:59",3786,2509773,1,"<p>Spring Cloud Data Flow and Skipper servers are <code>Spring Boot</code> applications and hence you can configure/customize logging system based on your requirements.</p>
<p>Here are some of the references to configure logging system for a Spring Boot app:</p>
<ul>
<li><a href=""https://docs.spring.io/spring-boot/docs/2.2.7.RELEASE/reference/html/spring-boot-features.html#boot-features-custom-log-configuration"" rel=""nofollow noreferrer"">Custom log configuration</a></li>
<li><a href=""https://docs.spring.io/spring-boot/docs/2.2.7.RELEASE/reference/html/howto.html#howto-configure-logback-for-logging"" rel=""nofollow noreferrer"">Logback configuration</a></li>
</ul>
"
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309,10008173,2,"<p><code>docker logs</code> and similar just collect the stdout and stderr streams from the main process running inside the container.  There's not a ""log level"" associated with that, though some systems might treat or highlight the two streams differently.</p>

<p>As a basic example, you could run</p>

<pre><code>docker run -d --name lister --rm busybox ls /
docker logs lister
</code></pre>

<p>The resulting file listing isn't especially ""error"" or ""debug"" level.</p>

<p>The production-oriented setups I'm used to include the log level in log messages (in a Node context, I've used the <a href=""https://www.npmjs.com/package/winston"" rel=""nofollow noreferrer"">Winston</a> logging library), and then use a tool like <a href=""https://www.fluentd.org"" rel=""nofollow noreferrer"">fluentd</a> to collect and parse those messages.</p>
"
Datadog,59973857,59973631,8,"2020/01/29, 21:00:41",False,"2020/01/29, 21:00:41",126283,14660,0,"<p>Make sure <code>CreatedDate</code> is indexed.</p>

<p>Make sure <code>CreatedDate</code> is using the <a href=""https://www.postgresql.org/docs/12/datatype-datetime.html"" rel=""nofollow noreferrer""><code>date</code> column type</a>. This will be more efficient on storage (just 4 bytes), performance, and you can use all the built in <a href=""https://www.postgresql.org/docs/12/functions-formatting.html"" rel=""nofollow noreferrer"">date formatting</a> and <a href=""https://www.postgresql.org/docs/12/functions-datetime.html"" rel=""nofollow noreferrer"">functions</a>.</p>

<p>Avoid <code>select *</code> and only select the columns you need.</p>

<p>Use <a href=""https://en.wikipedia.org/wiki/ISO_8601"" rel=""nofollow noreferrer""><code>YYYY-MM-DD</code> ISO 8601 format</a>. This has nothing to do with performance, but it will avoid a lot of ambiguity.</p>

<hr>

<p>The real problem is likely that you have thousands of tables with which you regularly make unions of hundreds of tables. This indicates a need to redesign your schema to simplify your queries and get better performance. </p>

<p>Unions and date change checks suggest a lot of redundancy. Perhaps you've partitioned your tables by date. Postgres has its own built in <a href=""https://www.postgresql.org/docs/current/ddl-partitioning.html"" rel=""nofollow noreferrer"">table partitioning</a> which might help.</p>

<p>Without more detail that's all I can say. Perhaps ask another question about your schema.</p>
"
Datadog,59974435,59973631,7,"2020/01/29, 21:40:37",False,"2020/01/29, 21:40:37",18091,1721239,0,"<p>Without seeing <code>EXPLAIN (ANALYZE, BUFFERS)</code>, all we can do is speculate.</p>

<p>But we can do some pretty good speculation.</p>

<p>Cluster the tables on the index on CreatedDate.  This will allow the data to be accessed more sequentially, allowing more read-ahead (but this might not help much for some kinds of storage).  If the tables have high write load, they may not stay clustered and so you would have recluster them occasionally.  If they are static, this could be a one-time event.</p>

<p>Get more RAM.  If you want to perform as if all the data was in memory, then get all the data into memory.</p>

<p>Get faster storage, like top-notch SSD.  It isn't as fast as RAM, but much faster than HDD.</p>
"
Datadog,58858157,58625365,0,"2019/11/14, 15:40:57",True,"2019/11/14, 15:40:57",14336,1251219,0,"<p>The answer to the question is found in the comments to it. Hence, this question should not go unanswered.</p>

<p>The code from the question works as expected, however, the path where the named pipe resides is a special path and this is the reason why the data that is being sent to it never reaches the script. The corresponding special casing in Bash for instance can be found in <a href=""https://github.com/bminor/bash/blob/d894cfd104086ddf68c286e67a5fb2e02eb43b7b/redir.c#L537"" rel=""nofollow noreferrer""><code>redir.c</code></a>.</p>

<p>The solution to the problem is to use a real UDP server on that port:</p>

<pre><code>socat -u -v -x udp-listen:8125,fork /dev/null &amp;&gt;/var/log/datadog-agent.log
</code></pre>
"
Datadog,57463587,57423102,0,"2019/08/12, 17:56:44",False,"2019/08/12, 17:56:44",149,5059173,0,"<p>It turns out that someone had turned on a scheduled job that was sending a super expensive query that was supposed to be a singleton onto the query queue every 5 min.  The query takes 20 min to run, so eventually the system bogs down and falls over.</p>
"
Datadog,57014014,56993743,0,"2019/07/13, 00:03:23",False,"2019/07/13, 00:03:23",8484,1224827,0,"<p>Apparently this was / is an issue with <code>rpy2</code>, which was a dependency of our project. It was being imported by a utility module that was imported on startup. This caused it to be called on every single request to our REST API endpoints. Putting the import inside the actual function that was using it fixed this issue.</p>
"
Datadog,56173299,56172624,0,"2019/05/16, 19:55:12",True,"2019/05/16, 19:55:12",1371,5540166,2,"<p>What you want is either the <code>is_match</code> or <code>is_exact_match</code> conditional variable, which are <a href=""https://docs.datadoghq.com/monitors/notifications/?tab=is_matchis_exact_match#conditional-variables"" rel=""nofollow noreferrer"">documented here</a> (with examples).</p>

<p>The idea is that you can nest your messages <em>and notifications</em> in conditional logic arguments so that only when the monitor alerts/warns/resolves, or only when the evaluated tag scope matches certain conditions, will certain messages or notification channels be part of the alert.  </p>

<p>So in your case you want your message to include something like this:</p>

<blockquote>
  <p>{{#is_exact_match ""environment.name"" ""prod""}}</p>
  
  <p>Add special prod message here</p>
  
  <p>and @pagerduty or @pagerduty-foo </p>
  
  <p>{{/is_exact_match}}</p>
  
  <p>Add message that should always show up here</p>
  
  <p>and @slack-bar</p>
</blockquote>

<p>In this case, only when the ""environment"" tag's value is ""prod"" will the bracketed content be included (which includes the pagerduty notification). The non-bracketed part will always be included (which includes the slack notification). </p>
"
Datadog,53732835,53732707,0,"2018/12/11, 23:49:57",False,"2018/12/11, 23:49:57",1455,2301088,0,"<p><code>(?:\/private\/toolbox\/)(.+)</code> ought to match your route path, capturing the wildcard as the first group:</p>

<pre><code>const path  = '/private/toolbox/whichever';
const match = path.match(/(?:\/private\/toolbox\/)(.+)/);

console.log(match);
</code></pre>

<p>I cannot speak to that RegExp's performance, however.</p>
"
Datadog,53624583,53469205,1,"2018/12/05, 05:07:49",False,"2018/12/05, 05:07:49",3223,43973,0,"<p>I can't speak specifically for the Java implementation, but in the CSharp client, the ability to send this data to Datadog is done to 127.0.0.1 via UDP port 8125. It's on the same thread as your executing code and not asynchronous. The whole effort by your process is finished once the UDP message is sent - it's fired and immediately forgotten.</p>

<p>The thread overhead you mention occurs in the separate Datadog agent process which is listening on the other end of UDP 8125, and has it's own thread pool and ability to buffer some data before sending up to Datadog's servers.</p>

<p>Do you have additional information that shows this behavior? Based on what I know, this doesn't sound like a side effect of the Datadog/StatsD stuff.</p>
"
Datadog,54224944,53469205,0,"2019/01/16, 22:34:23",True,"2019/01/16, 22:34:23",2723,5885013,0,"<p>I found the answer on Datadog's help forum: <a href=""https://help.datadoghq.com/hc/en-us/articles/204588979-How-to-graph-percentiles-in-Datadog"" rel=""nofollow noreferrer"">""How to graph percentiles in Datadog""</a>.</p>

<blockquote>
  <ul>
  <li>Making a change to increase tag complexity (<strong>adding additional tags to be more specific</strong>) will lead to changes in the behavior of a rolled up metric visualization
  
  <ul>
  <li>EX: Whereas before the change METRIC_NAME.avg (without any tags) would be aggregating across all raw points (statsd takes all the raw datapoints, aggregates it and then ships over a single metric stream), adding a tag like region (US, EU) tag causes statsd to bin raw datapoints into two region bins, aggregate them, and ship over two streams. This means when graphing METRIC_NAME.avg AVG by * means an aggregate across the two streams rather than a single one</li>
  </ul></li>
  </ul>
</blockquote>

<p>So the gist is that the latency itself didn't go up, but aggregating over multiple streams (where each stream corresponds to each custom tag) caused the graph to display a different shape.  </p>
"
Datadog,51552249,51524214,0,"2018/07/27, 09:49:35",True,"2018/07/27, 10:22:48",11822,193178,1,"<p>Just use <code>NLog.MappedDiagnosticsLogicalContext.Set(""userid"", ""someValue"")</code> together with <code>${mdlc:item=userid}</code> where needed. See also <a href=""http://github.com/NLog/NLog/wiki/MDLC-Layout-Renderer"" rel=""nofollow noreferrer"">http://github.com/NLog/NLog/wiki/MDLC-Layout-Renderer</a> </p>

<p><code>MappedDiagnosticsLogicalContext</code> uses <code>CallContext</code> (and <code>AsyncLocal</code> on NetCore) which are thread-safe. Settings will also support async Task and follow to the chained tasks, but if scheduling a Time-callback based on a user-request, then the Timer-callback will not see the userid.</p>

<p>You should avoid changing <code>LogManager.Configuration.Variable</code> at runtime, they are global for all concurrent requests, and might get lost during configuration-reload (If autoreload configured). </p>
"
Datadog,52038935,51521838,0,"2018/08/27, 15:02:48",True,"2018/08/27, 15:02:48",1,10134087,0,"<p>If anyone will need the answer, this is how I did this.
It shows the biggest tables that were not last vacuumed in that past 2 weeks, but limits the list for 20 results.</p>

<p>If you want to you can change LIMIT 20 or delete it for shorter/longer list. Also change pg.last_autovacuum for analyze or anything else from pg.stat table you want to check and also 2 week can be changed to whatever time period you want.</p>

<pre><code>SELECT t.nspname || '.' || t.relname AS ""relation"", size, count(DISTINCT t.relname)
from (SELECT nspname, relname, pg_size_pretty(pg_relation_size(C.oid)) AS ""size""
      FROM pg_class C
         LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
      WHERE nspname NOT IN ('pg_catalog', 'information_schema')
      ORDER BY pg_relation_size(C.oid) DESC
      LIMIT 20) t
       left join pg_stat_user_tables pg on pg.relname = t.relname
where pg.last_autovacuum &lt; now() - interval '2 week'
group by relation, size;
</code></pre>
"
Datadog,52039061,51521838,0,"2018/08/27, 15:09:40",False,"2018/08/27, 15:09:40",1,10134087,0,"<p>In datadog under postgres.yaml I added this:</p>

<pre><code>custom_metrics:
- # Last Auto vacuum tables
  query: SELECT t.nspname || '.' || t.relname AS ""relation"", size, %s from (SELECT nspname , relname , pg_size_pretty(pg_relation_size(C.oid)) AS ""size"" FROM pg_class C LEFT JOIN pg_namespace N ON ( N.oid = C.relnamespace) WHERE nspname NOT IN ('pg_catalog', 'information_schema') ORDER BY pg_relation_size(C.oid) DESC LIMIT 20) t left join pg_stat_user_tables pg on pg.relname = t.relname where pg.last_autovacuum &lt; now() - interval '2 week' group by relation, size;
  metrics:
      count(DISTINCT t.relname) last_autovacuum: [postgresql.last_autovacuum, GAUGE]
  relation: false
  descriptors:
      - [relation, relation]
      - [size, size]
</code></pre>

<p>Then I added this as a top-list inside a dashboard. You can also make it as an alert and sum up the counts inside the metrics and decide what's your limit that about it you want to start vacuuming, although it's just recommended to run it regularly and not just when it's just too big, that's why we use this only as a list in our dashboard. To get eyes only. </p>
"
Datadog,49291336,49280325,0,"2018/03/15, 05:58:25",True,"2018/03/15, 05:58:25",1390,8748848,1,"<p>Datadog, like OMS and other monitoring software uses the Azure VM agent to steam the information. Once this agent is installed on the system we are able to gather the info needed. </p>

<p>The VM agent is not something that goes out over the internet like other connections. Hence, you should still see the reporting available. Rather, it should be a direct connection from the Hyper-V manager and the VM itself. This therefore, bypassing any NSG rules you would have in place. </p>
"
Datadog,49292913,49280325,0,"2018/03/15, 08:32:47",False,"2018/03/15, 08:44:04",12857,6851908,0,"<blockquote>
  <p>I have installed data dog agent on one of my virtual machines</p>
</blockquote>

<p>Datadog agent will collect system metrics and <strong>forward</strong> to Datadog.</p>

<p>Datadog agent works like this:</p>

<p><a href=""https://i.stack.imgur.com/wVK1p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wVK1p.png"" alt=""enter image description here""></a></p>

<p>Also you can try to perform a network capture on your Azure VM, then we are able to find the detailed of the agent behavior.</p>

<p>Here is the network capture in my test VM:</p>

<p><a href=""https://i.stack.imgur.com/lSPzj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lSPzj.png"" alt=""enter image description here""></a></p>

<p>We can find that <strong>Datadog agent forward over HTTPS(443) to Datadog HQ</strong>.</p>

<p>After you deny port 443 in NSG outbound rules, the datadog will not get your metrics:</p>

<p><a href=""https://i.stack.imgur.com/XURQB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XURQB.png"" alt=""enter image description here""></a></p>

<p>More information about datadog agent, please refer to this official <a href=""https://help.datadoghq.com/hc/en-us/articles/203034929-What-is-the-Datadog-Agent-What-resources-does-it-consume-"" rel=""nofollow noreferrer"">article</a>.</p>
"
Datadog,48265003,48262289,0,"2018/01/15, 16:28:30",False,"2018/01/15, 16:28:30",9230,63308,0,"<p>Anything printed to STDOUT will be sent to your logging addons like SumoLogic. The options you've shown should take care of that.</p>

<p>The mechanism that addons like SumoLogic use is call <a href=""https://devcenter.heroku.com/articles/log-drains"" rel=""nofollow noreferrer"">Log Drains</a>, and you can tap into that your self to get your log stream over HTTP.</p>
"
Datadog,45557292,45554437,2,"2017/08/08, 02:56:57",False,"2017/08/08, 02:56:57",1371,5540166,1,"<p>hmm, so you're trying to use autodiscovery to find which container the dd-agent should be running the etcd check on? and you're using the auto_conf files approach? And there, you're wondering how to apply the <code>%%host%%</code> template variable?</p>

<p>If that's what you're interested in, I think you'll want to add it into your <code>etcd.yaml</code> on the <code>url</code> line, as shown in <a href=""https://github.com/DataDog/integrations-core/blob/5.15.x/etcd/auto_conf.yaml#L7"" rel=""nofollow noreferrer"">the example file</a> like so:</p>

<pre><code>docker_images:
  - etcd

init_config:

instances:
  - url: ""http://%%host%%:%%port_0%%""
</code></pre>
"
Datadog,43724984,43721365,3,"2017/05/01, 22:11:46",False,"2017/05/01, 22:11:46",261,4172512,1,"<p>When submitting histograms via dogstatsD you should be automatically creating 5 metrics as shown here:</p>

<p>dog.histogram(...)</p>

<p>Usage: Used to track the statistical distribution of a set of values over a statsd flush period.
Actually submits as multiple metrics:</p>

<pre><code>name | Web App type
-----|------------
metric.max | GAUGE
metric.avg | GAUGE
metric.median | GAUGE
metric.95percentile | GAUGE
metric.count | RATE
</code></pre>

<p>Additional details on metric types and their submission sources can be found here:</p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/206955236-Metric-types-in-Datadog"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/206955236-Metric-types-in-Datadog</a></p>

<p>It appears for your use case <code>metric.count</code> would be the closest match for calculating the total length of your word.  Once selected, you can make use of the <code>as_count()</code> modifier which will calculate the total count rather than the average over the flushing period.  More information on this use case can be found here:</p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/204271195-Why-is-a-counter-metric-being-displayed-as-a-decimal-value-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/204271195-Why-is-a-counter-metric-being-displayed-as-a-decimal-value-</a></p>

<p>If you find yourself still running into any issues with this submission feel free to reach out to support@datadoghq.com</p>
"
Datadog,39513221,39513085,0,"2016/09/15, 17:06:29",False,"2016/09/15, 17:06:29",13863,6162307,0,"<p>You can modify the following according to your needs.</p>

<pre><code>from time import sleep

for i in range(12):
    sleep(1)
    print(""\r\t&gt; Progress\t:{:.2%}"".format((i + 1)/12), end='')
</code></pre>

<p>What this basically does, is that it prevents <code>print()</code> from writing the default end character (<code>end=''</code>) and at the same time, it write a carriage return (<code>'\r'</code>) before anything else. In simple terms, you are overwriting the previous <code>print()</code> statement.</p>
"
Datadog,39513348,39513085,1,"2016/09/15, 17:12:09",False,"2016/09/15, 17:16:06",254,3785588,0,"<p>the naive solution would be to just use the total amount of rows in your dataset and the index your are at, then calculate the progress:</p>

<pre><code>size = len(dataset)
for index, element in enumerate(dataset):
    print(index / size * 100)
</code></pre>

<p>This will only be somewhat reliable if every row takes around the same time to complete. Because you have a large dataset, it might average out over time, but if some rows take a millisecond, and another takes 10 minutes, the percentage will be garbage.</p>

<p>Also consider rounding the percentage to one decimal:</p>

<pre><code>size = len(dataset)
for index, element in enumerate(dataset):
    print(round(index / size * 100), 1)
</code></pre>

<p>Printing for every row might slow your task down significantly so consider this improvement:</p>

<pre><code>size       = len(dataset)
percentage = 0
for index, element in enumerate(dataset):
    new_percentage = round(index / size * 100), 1)
    if percentage != new_percentage:
        percentage = new_percentage
        print(percentage)
</code></pre>

<p>There are, of course, also modules for this:</p>

<p><a href=""https://pypi.python.org/pypi/progressbar"" rel=""nofollow"">progressbar</a></p>

<p><a href=""https://pypi.python.org/pypi/progress"" rel=""nofollow"">progress</a></p>
"
Datadog,44937902,44933565,0,"2017/07/06, 03:17:35",False,"2017/07/06, 03:26:50",1,6688988,0,"<p>The solution:</p>

<ul>
<li><p>docker container 0</p>

<ul>
<li>running the application that is outputting the metrics</li>
<li>create a bash script within the application that is outputting the metrics.  </li>
<li>within the script use the set the value of the docker container $HOSTNAME environment variable to the jmxremote.host and the rmi.server.hostname.  </li>
</ul>

<p>#!/bin/sh<br>
    java -Djava.util.logging.config.file=logging.properties -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=9998 -Dcom.sun.management.jmxremote.port=9998 -Djava.rmi.server.hostname=$HOSTNAME -Dcom.sun.management.jmxremote.host=$HOSTNAME -Dcom.sun.management.jmxremote.local.only=false -jar /app/my-streams.jar</p>

<ul>
<li>remember to set chmod +x </li>
<li>set the dockerfile CMD to run the script above like so:<br>
CMD[""./""]</li>
</ul></li>
<li><p>docker container 1  </p>

<ul>
<li>the container running the datadog agent</li>
<li>configure the jmx.yaml file as mentioned above in the question. just set the host to the application name</li>
</ul></li>
<li><p>way more stuff was done that is available from from stack overflow posts. but the above fixes the metrics finding error from datadog-agent.</p></li>
</ul>

<hr>

<p>Here is how to run each component:  </p>

<p>docker container 0<br>
* my-streams<br>
* spin up dependent services in tab<br>
** mvn clean package docker:build<br>
** docker-compose up  </p>

<ul>
<li>another tab spin up my-streams-app<br>
** docker kill my-streams-app<br>
** docker rm my-streams-app<br>
** docker run -d --name my-streams-app -p 9998:9998 --
network=mystreams_default quay.io/myimage/my-streams  </li>
</ul>

<p>docker container 1<br>
* docker build -t dd-agent-my-streams .<br>
* docker run -v /var/run/docker.sock:/var/run/docker.sock:ro   -v /proc/:/host/proc/:ro   -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro -e LOG_LEVEL=DEBUG -e SD_BACKEND=docker --network=mystreams_default  </p>

<p>ssh into docker container 1 to verify if metrics work<br>
* docker ps // to find the name of the container to log into<br>
* docker exec -it  /bin/bash<br>
root@904e6561cc97:/# service datadog-agent configcheck<br>
root@904e6561cc97:/# service datadog-agent jmx list_everything<br>
root@904e6561cc97:/# service datadog-agent jmx collect</p>
"
Datadog,46572247,46449330,1,"2017/10/04, 21:57:04",True,"2017/10/04, 21:57:04",1371,5540166,2,"<p>Well, you <em>could</em> use the Datadog API to script the creation of 20 unique dashboards that all share the same content, but with different hosts. This is the part of the API docs that would help (with examples!) <a href=""https://docs.datadoghq.com/api/#timeboards"" rel=""nofollow noreferrer"">for Timeboards</a>, and this one <a href=""https://docs.datadoghq.com/api/#screenboards"" rel=""nofollow noreferrer"">for Screenboards</a>. </p>

<p>That said, I'd personally find 20 dashboards a bit cluttered / unwieldy in my own Datadog account. Instead, if it was me, I'd try to (A) find clever uses of dashboard template variables (on e.g, cluster tags, host tags, etc.), or (B) group out by each host tag and apply <a href=""https://help.datadoghq.com/hc/en-us/articles/204972439-There-are-too-many-lines-on-my-graph-can-I-only-display-the-most-important-ones-"" rel=""nofollow noreferrer"">the ""top()"" function</a> in some way so that I'd be able to see just the most extreme-value hosts. But that's certainly up to you :)</p>
"
Datadog,55956522,55953321,1,"2019/05/02, 19:18:34",False,"2019/05/02, 19:18:34",2574,4162641,0,"<p>Have you tried using the <code>start</code> and <code>end</code> tags with a 24 hour window?</p>
"
Datadog,53383195,53382882,0,"2018/11/19, 23:55:24",False,"2018/11/19, 23:55:24",1371,5540166,1,"<p>Maybe you could mod the process check to also tag the process number metric by PID (<a href=""https://github.com/DataDog/integrations-core/blob/6.6.0/process/datadog_checks/process/process.py#L415-L418"" rel=""nofollow noreferrer"">this is probly where you'd change that</a>). That way you could group your monitor by your pid tag and the no-data alerts would tell you when the pid switched. </p>

<p>But this would also alert on expected pid changes, so maybe you'd have to schedule downtimes too aggressively for this to be a good idea?</p>

<p>Maybe monitoring some crash logs with <a href=""https://app.datadoghq.com/logs"" rel=""nofollow noreferrer"">their Log Management tool</a> would be a better approach?</p>
"
Datadog,64907275,64453730,0,"2020/11/19, 09:38:41",False,"2020/11/19, 09:38:41",21,7314273,0,"<p>Not sure if this will work but you can give it a try..:</p>
<ol>
<li>create 2 instances of the same monitor mentioned about</li>
<li>create a composite monitor based on them both</li>
<li>trigger the composite only when a.value is not the same as b.value</li>
</ol>
<hr />
<p>{{^is_exact_match a.value b.value }}</p>
<p>@my@mail.com
Alert 2 hosts has passed the threshold</p>
<p>{{/is_exact_match}}</p>
<p>same value - ignore - do nothing</p>
<hr />
<p>The problem is that you probably might get 2 alerts at the same time...</p>
"
Datadog,64396016,64345103,0,"2020/10/16, 23:28:57",True,"2020/10/16, 23:28:57",318,1253272,0,"<p>It turns out that trace id can be set via HTTP endpoint <a href=""https://docs.datadoghq.com/api/v1/tracing/#send-traces"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/v1/tracing/#send-traces</a>. There doesn't seem to be an option for sending traces to the agent directly.</p>
<p>This can still be useful if the performance penalty of making HTTP calls is not a concern, i.e., if you are not working on a real-time system.</p>
"
Datadog,66783806,65717686,0,"2021/03/24, 17:02:55",True,"2021/03/24, 17:02:55",56,10960667,1,"<p>Nothing is wrong there. DataDog conceals that the Kafka integration uses Dogstatsd under the hood. When <code>use_dogstatsd: 'true</code> within /etc/datadog-agent/datadog.yaml is set, metrics do appear in DataDog webUI. If that option is not set the default Broker data is available via JMXFetch using <code>sudo -u dd-agent datadog-agent status</code> as also via <code>sudo -u dd-agent datadog-agent check kafka</code> but not in the webUI.</p>
"
Datadog,60307539,60216953,0,"2020/02/19, 21:07:06",False,"2020/02/19, 21:07:06",571,2019978,0,"<p>No, you cannot install the Datadog agent on a Snowflake host.</p>

<p>We use our separate job scheduling system to monitor Snowflake by running queries (e.g. checks on SYSTEM$CLUSTERING_DEPTH, aggregate queries against the QUERY_HISTORY for timing, etc) via the JDBC connector then relaying the results to our monitoring stack (similar to how Datadog agent would work.)</p>
"
Datadog,58771357,58765810,1,"2019/11/08, 19:36:03",True,"2019/11/08, 19:36:03",336,7243426,1,"<p>I've tried with this query which is similar to yours: </p>

<pre><code>events('sources:rds priority:all tags:event_source:db-instance').by('dbinstanceidentifier').rollup('count').last('1d') &gt; 1
</code></pre>

<p><a href=""https://i.stack.imgur.com/mO06q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mO06q.png"" alt=""enter image description here""></a></p>

<p>And this seems to give me a count by <code>dbinstanceidentifier</code> results.</p>

<p>Do you have more information to provide? Maybe an event list and a monitor result screenshot?</p>

<p><a href=""https://i.stack.imgur.com/quSOj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/quSOj.png"" alt=""enter image description here""></a></p>
"
Datadog,54938650,54920625,0,"2019/03/01, 07:49:07",True,"2019/03/01, 07:49:07",9,11043786,0,"<p>As I posted the issue on GitHub, they give an answer the issue was in source code for dd-trace-php and they will fix and new release.
<a href=""https://github.com/DataDog/dd-trace-php/issues/334"" rel=""nofollow noreferrer"">https://github.com/DataDog/dd-trace-php/issues/334</a></p>

<p>Below response of DatDog in github:</p>

<p>Ah now this is much more clear, thanks for sharing. This is a known problem that we are currently and actively working on. As I cannot commit to that, the fix will be probably come out with the next release.</p>

<p>At an higher level, the cause is an issue we have in some specific cases while invoking private/protected methods and parent::* invocations.</p>

<p>In the meantime, if you are still interested in testing/using the other integrations, the only thing I can recommend is to disable the pdo integration: <strong>fastcgi_param DD_INTEGRATIONS_DISABLED pdo</strong>.</p>

<p>Again, the fix to this is currently in development and will be released very soon.</p>
"
Datadog,45239352,45232107,0,"2017/07/21, 16:46:50",True,"2017/07/21, 16:46:50",10159,4907630,1,"<p>In the code you posted:</p>

<pre><code>c := datadog.Client{}
</code></pre>

<p>This seems to be creating an empty client object.</p>

<p>Shouldn't you be creating a client with your keys using <code>datadog.NewClient(""..."", ""..."")</code> as in the first code snippet you posted?</p>

<pre><code>c := datadog.NewClient(""..."", ""..."")
</code></pre>

<p>Also, you should check the error returned as that will give you more hints to troubleshoot the issue:</p>

<pre><code>_, err := c.PostEvent(&amp;e)
if err != nil {
  log.Fatalf(""fatal: %s\n"", err)
}
</code></pre>

<p>`</p>
"
Datadog,51841340,51840681,1,"2018/08/14, 15:22:29",False,"2018/08/14, 15:22:29",1978,7217896,0,"<p>Even though datadog is being run from the same machine, it is setting up a separate server on your machine. Because of that, it sounds like the datadog agent doesn't have access to your z:/ driver.</p>

<p>Try to put the ""TaskResults"" folder in your root directory (when running from datadog - where the mycheck.yaml file is) and change the path accordingly.</p>

<p>If this works and you still want to have a common drive to be able to share files from your computer to datadog's agent, you have to find a way to mount a drive\folder to the agent. They probably have a way to do that in the <a href=""https://docs.datadoghq.com/agent/"" rel=""nofollow noreferrer"">documentation</a></p>
"
Datadog,51894917,51840681,0,"2018/08/17, 14:54:34",True,"2018/08/17, 14:54:34",410,831608,0,"<p>The solution to this is to create a file share on the network drive and use that path instead of the full network drive path.
May be obvious to some but it didn't occur to me right away since the normal Python code worked without any issue outside of Datadog.</p>

<p>So instead of: </p>

<pre><code>init_config:
taskResultLocation: ""Z:/TaskResults""
</code></pre>

<p>use</p>

<pre><code>init_config:
taskResultLocation: '//FileShareName/d/TaskResults'
</code></pre>
"
Datadog,65631462,65631278,1,"2021/01/08, 17:04:29",False,"2021/01/08, 17:04:29",1530,11866104,0,"<p>Based on the <a href=""https://stackshare.io/stackups/grafana-vs-stackdriver"" rel=""nofollow noreferrer"">doc</a> you can decide which one is good for your use case. Stackdriver is detailed as &quot;Monitoring, logging, and diagnostics for applications on Google Cloud Platform and AWS&quot;. Google Stackdriver provides powerful monitoring, logging, and diagnostics. It equips you with insight into the health, performance, and availability of cloud-powered applications, enabling you to find and fix issues faster.</p>
<p>Grafana can be classified as a tool in the &quot;Monitoring Tools&quot; category, while Stackdriver is grouped under &quot;Cloud Monitoring&quot;.</p>
"
Datadog,61019569,60940393,0,"2020/04/03, 23:00:56",False,"2020/04/03, 23:00:56",1447,554481,0,"<p>After talking with Datadog support, it seems like this is a known issue.</p>

<blockquote>
  <p>Thanks for your patience while we looked into this issue. We we're currently investigating this along with PoolExecutors and will reach out with updates. Right now it looks like those child spans within the async call lose context, so they appear disconnected.</p>
</blockquote>

<p>The workaround for now is to pass in the parent's context. Add this line just before calling the thread pool executor.</p>

<pre><code>current_context = tracer.get_call_context()
</code></pre>

<p>Then pass that context to the function that gets run in the threadpool:</p>

<pre><code>perform_work(
    input_value=input_value,
    parent_context=current_context
)

</code></pre>

<p>And use it to create a span inside the function like this:</p>

<pre><code>span = tracer.start_span('do_something', child_of=parent_context)
seconds = random()
time.sleep(seconds)
span.finish()
</code></pre>

<p>The complete example looks like this:</p>

<pre><code>from concurrent.futures import ThreadPoolExecutor
from ddtrace import tracer
from random import random
import time


def perform_work(input_value, parent_context=None):
    span = tracer.start_span('do_something', child_of=parent_context)
    seconds = random()
    time.sleep(seconds)
    span.finish()
    return input_value ** 2


def sync_work(input_values):
    with tracer.trace('sync_work') as _:
        results = []
        for input_value in input_values:
            result = perform_work(input_value=input_value)
            results.append(result)
        return results


def async_work(input_values):
    with tracer.trace('async_work') as _:
        current_context = tracer.get_call_context()
        thread_pool = ThreadPoolExecutor(max_workers=10)
        futures = thread_pool.map(
            lambda input_value:
            perform_work(
                input_value=input_value,
                parent_context=current_context
            ),
            input_values
        )
        results = list(futures)
        return results


@tracer.wrap(service='ddtrace-example')
def start_work():
    input_values = list(range(15))
    sync_results = sync_work(input_values=input_values)
    print(sync_results)
    async_results = async_work(input_values=input_values)
    print(async_results)


if __name__ == '__main__':
    start_work()
</code></pre>

<p>This will produce a result that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/oYa2M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oYa2M.png"" alt=""enter image description here""></a></p>
"
Datadog,62228982,62221212,3,"2020/06/06, 11:04:38",False,"2020/06/06, 11:04:38",21,7314273,1,"<p>I had something similar issue and chose the first option, but i won't say it is from terraform perspective (since i also had lack in experience in terraform). 
The first hierarchy was more reasonable in segregation aspects, plus would be easier to add/remove/update organizations by demand.</p>
"
Datadog,64542859,64539546,0,"2020/10/26, 20:21:25",True,"2020/10/26, 20:21:25",156,2676108,1,"<p>I don't believe there is any way to graph the historical behavior of the SLI from an SLO.</p>
<p>The closest you could get would be to measure the underlying metric, so if you had <code>good events</code>/<code>bad events</code> you could display that percentage. But the calculation of how often that percentage is above or below a certain threshold would not be possible.</p>
<p>I recommend reaching out to support@datadoghq.com to let them know it's a feature you're interested in. They might be able to provide some updates.</p>
"
Datadog,62698566,62667694,0,"2020/07/02, 17:24:10",False,"2020/07/02, 17:24:10",251,5477963,1,"<p>If you are sending metrics to an actual StatsD server, then tags are not supported by the protocol. You would need to instead send the metrics to the Datadog agent's DogStatsD endpoint which extends StatsD with additional features such as tags. You can find more information about DogStatsD <a href=""https://docs.datadoghq.com/developers/dogstatsd/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>If you are already using the DogStatsD endpoint, then I would suspect an incompatibility with the <code>node-statsd</code> library. The library has not been updated for 6 years and it's possible that something changed since then, causing it to no longer work. In that case I would recommend switching to a more recent DogStatsD client that is still maintained such as <a href=""https://github.com/brightcove/hot-shots"" rel=""nofollow noreferrer"">hot-shots</a>.</p>
<p>Hope this helps!</p>
"
Datadog,65834472,65829133,0,"2021/01/21, 21:29:00",False,"2021/01/21, 21:29:00",17806,119790,0,"<p>Need to use category-processor <a href=""https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor</a></p>
<p>Example:
<a href=""https://i.stack.imgur.com/Oicit.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oicit.png"" alt=""enter image description here"" /></a></p>
<p>I didn't do enough research before posting this question, but the answer for anyone else looking.</p>
"
Datadog,58268784,58267953,0,"2019/10/07, 14:34:45",False,"2019/10/07, 14:34:45",251,5477963,2,"<p>The main issue is that you would have to mount the volume in both the app container and the agent container in order to make it available. It also means you have to find a place to store the log file before it gets picked up by the agent. Doing this for every container could become difficult to maintain and time consuming.</p>

<p>An alternative approach would be to instead send the logs to <code>stdout</code> and let the agent collect them with the Docker integration. Since you configured <code>logsConfigContainerCollectAll</code> to <code>true</code>, the agent is already configured to collect the logs from every container output, so configuring Winston to output to <code>stdout</code> should just work.</p>

<p>See: <a href=""https://docs.datadoghq.com/agent/docker/log/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/docker/log/</a></p>
"
Datadog,58270801,58267953,1,"2019/10/07, 16:41:01",False,"2019/10/07, 16:41:01",336,7243426,2,"<p>To support rochdev comment, here are a few code snippets to help out (if you do not opt in for the STDOUT method which should be simpler). This is only to mount the right volume inside the container agent.</p>

<p>On your app deployment, add:</p>

<pre><code>   spec:
      containers:
      - name: your_nodejs_app
        ...
        volumeMounts:
          - name: abc
            mountPath: /app/logs
      volumes:
        - hostPath:
            path: /app/logs
          name: abc
</code></pre>

<p>And on your agent daemonset:</p>

<pre><code>   spec:
      containers:
      - image: datadog/agent
        ...
        volumeMounts:
          ...
          - name: plop
            mountPath: /app/logs
      volumes:
        ...
        - hostPath:
            path: /app/logs/
          name: plop
</code></pre>
"
Datadog,48946203,48922641,0,"2018/02/23, 12:55:16",True,"2018/02/23, 12:55:16",1365,4894399,0,"<p>As I have mentioned in the comment, you are affected by <a href=""https://coderanger.net/two-pass/"" rel=""nofollow noreferrer"">two pass model</a>. You should remove the keys in the resource added to the end of the chef run or triggered by the DD cookbook resources invoked as the last one in the run.</p>

<pre><code>ruby_block ""clean datadog api attributes"" do
  block do
    node.rm(""datadog"", ""api_key"")
    ....
  end
  subscribes :create, ""template[&lt;some dd template using api keys&gt;]"", :immediately
end
</code></pre>

<p>However, it may not work with all versions of DD cookbook. From few DD cookbook versions, it is possible to store keys in node's run state which is not written to the Chef server.</p>

<pre><code>node.run_state[""datadog""] = {
  ""api_key""         =&gt; datadog[""api_key""],
  ""application_key"" =&gt; datadog[""application_key""]
}
</code></pre>

<p>The above example is preferred solution to your issue.</p>
"
Datadog,51603701,51377442,0,"2018/07/31, 02:52:59",False,"2018/07/31, 02:52:59",1474,10109833,0,"<p>Have a look at the documentation for <a href=""https://docs.datadoghq.com/agent/faq/dogstream/"" rel=""nofollow noreferrer"">Dogstream</a>. It allows you to send metrics to datadog from log files (including summarised metrics).</p>

<p>You may need to write a custom parser for any data that is not in the datadog canonical format in order for datadog to recognize the data. Checkout the example <a href=""https://docs.datadoghq.com/agent/faq/dogstream/#example-for-metrics-collecting"" rel=""nofollow noreferrer"">here</a>.</p>
"
Datadog,45988104,45974396,0,"2017/08/31, 21:40:40",False,"2017/08/31, 21:40:40",148,6815223,0,"<p>Your answer appears to be there in the text -- you're missing a Python package. Try running <code>sudo pip install psutil</code>, then restarting the agent. Can you add your agent version, OS and version, and how you installed the agent to your text as well? It looks like you're also using a <em>very</em> old version of the agent (it's up to 5.17.* for a number of OS's) so there may be better package bundling or critical updates since v. 4.4.0. Try installing a newer version as well.</p>
"
Datadog,45996096,45974396,8,"2017/09/01, 11:27:23",False,"2017/09/01, 12:04:48",1,8547108,-1,"<p>Please find the required</p>

<pre>
[root@mudcsftpup01 ~]# sudo pip install psutil
Requirement already satisfied: psutil in /usr/lib64/python2.7/site-packages
</pre>
"
Datadog,46985445,46984902,0,"2017/10/28, 04:36:56",True,"2017/10/28, 04:36:56",1371,5540166,2,"<p>Two approaches that may work:</p>

<ol>
<li><p>It looks like flink has an <a href=""https://ci.apache.org/projects/flink/flink-docs-release-1.3/monitoring/metrics.html#datadog-orgapacheflinkmetricsdatadogdatadoghttpreporter"" rel=""nofollow noreferrer"">HTTP connector</a> to send metrics to Datadog, which at first glance looks to send over the Datadog metrics API instead of dogstatsd. </p></li>
<li><p>Dogstatsd is not very different from statsd otherwise, so it's often easy to modify statsd libraries to work for dogstatsd. <a href=""https://github.com/drivetribe/flink-metrics-datadog-statsd/blob/master/README.md"" rel=""nofollow noreferrer"">This project on GitHub</a> seems to be such a project, and may come in handy.</p></li>
</ol>
"
Datadog,64256562,63599025,0,"2020/10/08, 08:55:02",False,"2020/10/08, 08:55:02",537,3134333,0,"<p>You may need to set the environment variable <code>DD_APM_NON_LOCAL_TRAFFIC=true</code> in your datadog agent container.</p>
<p>Ref: <a href=""https://docs.datadoghq.com/agent/docker/apm/?tab=linux#docker-apm-agent-environment-variables"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/agent/docker/apm/?tab=linux#docker-apm-agent-environment-variables</a></p>
"
Datadog,65770731,65766741,0,"2021/01/18, 10:05:03",True,"2021/01/18, 10:05:03",756,9629802,0,"<p>You are using the DataDog configuration for the commercial <a href=""https://k6.io/cloud"" rel=""nofollow noreferrer"">k6 Cloud service</a> (<code>k6 cloud</code>), not locally run k6 tests (<code>k6 run</code>). <code>test_run_id</code> is a concept in the cloud service, though it's also easy to emulate locally as a way to distinguish between test runs.</p>
<p>For local tests, you should enable the DataDog output by running k6 with <code>k6 run --out datadog script.js</code>. I assume you did that, otherwise you wouldn't see any metrics in DataDog.</p>
<p>Then, you can use the <a href=""https://k6.io/docs/using-k6/options#tags"" rel=""nofollow noreferrer""><code>tags</code> option</a> to inject a unique extra tag for all metrics generated by a particular k6 run, so you can differentiate them in DataDog. For example:</p>
<pre><code>k6 run --out datadog --tag test_run_id=1 script.js
k6 run --out datadog --tag test_run_id=2 script.js
k6 run --out datadog --tag test_run_id=3 script.js
...
</code></pre>
<p>Of course, you can choose any <code>key=value</code> combination, you are not restricted to <code>test_run_id</code>.</p>
"
Datadog,58270568,58266541,0,"2019/10/07, 16:28:46",False,"2019/10/07, 16:28:46",336,7243426,0,"<p>At first glance, this seems to be a use case more suitable for the APM part of Datadog which would measure the execution time and could be <a href=""https://docs.datadoghq.com/tracing/advanced/adding_metadata_to_spans/?tab=java"" rel=""nofollow noreferrer"">instrumented</a> to measure the execution time of the smaller functions (if it does not picked up this data automatically). You can then create some nice charts with <a href=""https://docs.datadoghq.com/tracing/trace_search_and_analytics/?tab=java"" rel=""nofollow noreferrer"">Trace Search and Analytics</a>.</p>

<p>You could also use a custom metric with tags such as <code>opsize:large</code> and <code>opsize:small</code> which would represent the execution time (a gauge). You can find more details <a href=""https://docs.datadoghq.com/developers/metrics/custom_metrics/#pagetitle"" rel=""nofollow noreferrer"">here</a>.</p>

<p>At the moment, the log module of Datadog does not seem to support the calculation you expect to see. However, the two solutions above and the related logs can be made visible in a dashboard side by side.</p>
"
Datadog,64776832,64772196,0,"2020/11/10, 23:11:22",False,"2020/11/10, 23:11:22",11561,970308,0,"<p>The MeterRegistry already has implemented how to send custom metrics (posting to DataDog) See the code <a href=""https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java#L133"" rel=""nofollow noreferrer"">https://github.com/micrometer-metrics/micrometer/blob/master/implementations/micrometer-registry-datadog/src/main/java/io/micrometer/datadog/DatadogMeterRegistry.java#L133</a></p>
<p>It appears like you are trying to add common tags to each metric. Perhaps you shouldn't implement your own DataDog registry, and instead use the provided one to send the metrics and set the common tags via config:</p>
<pre><code>registry.config().commonTags(Arrays.asList(
  Tag.of(MetricConstants.TENANT_MONIKER, tenant.getTenantMoniker()), 
  Tag.of(MetricConstants.STACK_NAME, stackName)
));
</code></pre>
"
Datadog,62301304,62300720,2,"2020/06/10, 13:27:51",False,"2020/06/10, 13:27:51",12179,1032890,0,"<p>There's a Prometheus endpoint for Tibco EMS:</p>

<p><a href=""https://community.tibco.com/wiki/statistics-logger-tibco-enterprise-message-service#toc-15"" rel=""nofollow noreferrer"">https://community.tibco.com/wiki/statistics-logger-tibco-enterprise-message-service#toc-15</a></p>

<p>I think you can then add the prometheus integration to your datadog agent to send the data do datadog, and build your own dashboard for that:</p>

<p><a href=""https://docs.datadoghq.com/integrations/prometheus/#data-collected"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/prometheus/#data-collected</a></p>
"
Datadog,66880008,66875990,0,"2021/03/31, 03:15:15",False,"2021/03/31, 03:15:15",3,1467883,0,"<p>It looks like Datadog uses zstd compression in order to compress its data before sending it: <a href=""https://github.com/DataDog/datadog-agent/blob/972c4caf3e6bc7fa877c4a761122aef88e748b48/pkg/util/compression/zlib.go"" rel=""nofollow noreferrer"">https://github.com/DataDog/datadog-agent/blob/972c4caf3e6bc7fa877c4a761122aef88e748b48/pkg/util/compression/zlib.go</a></p>
"
Datadog,60381277,60369412,3,"2020/02/24, 19:41:28",False,"2020/02/24, 19:41:28",1371,5540166,0,"<p><strong>First</strong> you'll want to make sure your logs are well structured (which you can control in <a href=""https://docs.datadoghq.com/logs/processing/"" rel=""nofollow noreferrer"">Datadog's processing pipelines</a>). Effectively you'll want to parse out the ""code"" values into some ""error code"" attribute. </p>

<p>If your log events are in this format...</p>

<pre><code>2020-01-01 12:10:10 myservername - Server Error {""error"":{""code"":1001,""type"":""MATCH"",""message"":""Invoke failed: Failed""}}
</code></pre>

<p>...Then all you need is a fairly simple grok parser rule, thanks to the ""json"" filter function. Something like this would get you where you want (note the <code>%{data::json}</code> part, that's what parses the in-log JSON). </p>

<pre><code>myrulename %{date(""yyyy-mm-dd' 'HH:MM:ss""):timestamp} %{notSpace:hostname} - Server Error %{data::json}
</code></pre>

<p>Once you've configured this, your logs will also have an attribute called ""error.code"" with a value of <code>2001</code> or <code>1001</code> or whatever. </p>

<p><strong>Second</strong> you'll want to <a href=""https://docs.datadoghq.com/logs/explorer/?tab=logsearch#setup"" rel=""nofollow noreferrer"">create a facet</a> for that new <code>error.code</code> attribute so that you can <a href=""https://docs.datadoghq.com/logs/explorer/analytics/?tab=timeseries"" rel=""nofollow noreferrer"">make toplist / timeseries / etc. graphs grouped out by your ""error code"" facet</a>.</p>
"
Datadog,44336405,44335719,4,"2017/06/02, 22:48:19",False,"2017/06/02, 22:48:19",1371,5540166,1,"<p>I think what you actually want is the metrics-query API endpoint? <a href=""http://docs.datadoghq.com/api/#metrics-query"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/api/#metrics-query</a></p>

<p>There are also a few Node.JS libraries that may be able to handle this kind of metric querying for you: <a href=""http://docs.datadoghq.com/libraries/#community-node"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/libraries/#community-node</a></p>
"
Datadog,66345892,66326300,0,"2021/02/24, 08:42:37",False,"2021/02/24, 08:55:54",3000,1335245,0,"<p>This is what I've got so far. Everything but the <code>source</code>.</p>
<p><a href=""https://i.stack.imgur.com/WrrVs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WrrVs.png"" alt=""Sending env, hostname, and service to datadog"" /></a></p>
<pre class=""lang-java prettyprint-override""><code>import ch.qos.logback.classic.LoggerContext;

// Add context to logs
LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory();
lc.setPackagingDataEnabled(true);
lc.putProperty(&quot;source&quot;, &quot;java&quot;);
lc.putProperty(&quot;service&quot;, &quot;thing-doer&quot;);
lc.putProperty(&quot;host&quot;, &quot;prd1.do.things&quot;));
lc.putProperty(&quot;env&quot;, &quot;production&quot;);
</code></pre>
"
Datadog,61015284,61010490,2,"2020/04/03, 18:30:55",False,"2020/04/03, 18:30:55",1371,5540166,3,"<p>Yes, you can configure widgets to exclude results by tags. You can do this by applying a tag prepended with a <code>!</code> to signify ""not"". </p>

<p>So in your case, you can set up your widget scoped over <code>importance:ignore</code> and then hit the little <code>&lt;/&gt;</code> button on the right to expose the underlying query, and sneak a <code>!</code> in front to make it <code>!importance:ignore</code>. </p>

<p><a href=""https://docs.datadoghq.com/tagging/using_tags/?tab=assignment#notebooks"" rel=""nofollow noreferrer"">This doc has a nice example</a> (although it's for notebooks, it works the same in dashboards as well). </p>
"
Datadog,47003630,47003531,0,"2017/10/29, 20:17:50",True,"2017/10/29, 20:17:50",2010,6020610,0,"<p>I noticed that the API key of Datadog changes everytime whenever in close the site and open a new instance. so after entering new API key , the issue was solved</p>

<p><a href=""https://i.stack.imgur.com/Jaiam.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jaiam.png"" alt=""enter image description here""></a></p>
"
Datadog,43644895,43641450,0,"2017/04/27, 00:34:11",False,"2017/04/27, 00:34:11",261,4172512,2,"<p>You could use Datadog's Outlier detection to identify instances which exhibit behavior outside the normal for it's peer set.  As an example, you could create an outlier detection monitor:</p>

<p><a href=""http://docs.datadoghq.com/guides/outliers/#alerts"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/guides/outliers/#alerts</a></p>

<p>Which would be scoped to a system metric like <code>aws.ec2.cpuutilization</code> and be alerted if any host spiked abnormally or had very low utilization in comparison to its group.</p>

<p>There are some additional blog posts which discuss the use of the algorithms that can be found here:</p>

<p><a href=""https://www.datadoghq.com/blog/introducing-outlier-detection-in-datadog/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/introducing-outlier-detection-in-datadog/</a></p>

<p><a href=""https://www.datadoghq.com/blog/outlier-detection-algorithms-at-datadog/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/outlier-detection-algorithms-at-datadog/</a></p>

<p><a href=""https://www.datadoghq.com/blog/scaling-outlier-algorithms/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/scaling-outlier-algorithms/</a></p>

<p>That said, if you find yourself needing additional assistance with outlier detection you can always reach out to the Support team at support@datadoghq.com or by using the internal support features found here:</p>

<p><a href=""https://app.datadoghq.com/help"" rel=""nofollow noreferrer"">https://app.datadoghq.com/help</a></p>

<p>Hope this helps!</p>
"
Datadog,62448235,62448098,4,"2020/06/18, 13:49:16",True,"2020/06/18, 13:49:16",64309,10008173,2,"<p><code>docker logs</code> and similar just collect the stdout and stderr streams from the main process running inside the container.  There's not a ""log level"" associated with that, though some systems might treat or highlight the two streams differently.</p>

<p>As a basic example, you could run</p>

<pre><code>docker run -d --name lister --rm busybox ls /
docker logs lister
</code></pre>

<p>The resulting file listing isn't especially ""error"" or ""debug"" level.</p>

<p>The production-oriented setups I'm used to include the log level in log messages (in a Node context, I've used the <a href=""https://www.npmjs.com/package/winston"" rel=""nofollow noreferrer"">Winston</a> logging library), and then use a tool like <a href=""https://www.fluentd.org"" rel=""nofollow noreferrer"">fluentd</a> to collect and parse those messages.</p>
"
Datadog,66203307,66202271,0,"2021/02/15, 07:52:17",False,"2021/02/15, 07:52:17",124,1534712,0,"<p>I have found that <a href=""https://blackfire.io/"" rel=""nofollow noreferrer"">Blackfire</a> is doing the trick. Seems to be relatively easy to install and can run it free locally.</p>
"
Datadog,65834510,65790261,0,"2021/01/21, 21:32:18",True,"2021/01/21, 21:32:18",493,244037,0,"<p>Datadog Tags are generally strings, but also support &quot;key:value&quot; strings, which is most useful, since then the <code>key</code> can act as a dimension.</p>
<p>There's no support that I know of that allows for a single key with multiple values, so I don't think Datadog will support the syntax you're attempting.</p>
<p>You <em>may</em> want to try:</p>
<pre><code>...
  - component:component1
  - component:component2
...
</code></pre>
<p>in your config.</p>
<p>General reference here: <a href=""https://docs.datadoghq.com/getting_started/tagging/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/getting_started/tagging/</a></p>
"
Datadog,65153895,65151286,0,"2020/12/05, 06:52:20",False,"2020/12/05, 06:52:20",17233,9576186,1,"<p><a href=""https://github.com/nestjs/nest/blob/master/packages/core/router/router-explorer.ts#L147"" rel=""nofollow noreferrer"">You're welcome to go look through the source code yourself</a>, but generally my comment is correct. Nest binds all route handlers and enhancers (guards, interceptors, pipes, and filters) as a large anonymous function, in a very abstract way (does the same thing for Fastify as far as I can tell).</p>
"
Datadog,64842339,64821299,4,"2020/11/15, 10:08:00",False,"2020/11/15, 10:08:00",327,2882619,0,"<p>You can use Fluentd as a <code>daemonset</code> on your cluster.</p>
<p>see this repo and docker images -&gt; <a href=""https://github.com/fluent/fluentd-docker-image"" rel=""nofollow noreferrer"">fluent/fluentd-docker-image</a></p>
<p>and use this filter to add <a href=""https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter"" rel=""nofollow noreferrer"">Kubernetes metadata</a> to every log collected by Fluentd and then use a grep filter to exclude logs that are not in your namespaces.</p>
<p>something like this:</p>
<pre><code># Collect pod logs
&lt;source&gt;
  @type tail
  @id in_tail_container_logs
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  exclude_path [&quot;/var/log/containers/*fluent*.log&quot;]
  read_from_head true
  &lt;parse&gt;
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  &lt;/parse&gt;
&lt;/source&gt;

# add Kubernetes metadata
&lt;filter kubernetes.**&gt;
  @type kubernetes_metadata
  @id filter_kube_metadata
  kubernetes_url 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'
  verify_ssl true
&lt;/filter&gt;

# filter and exclude logs that are not in the namespaces `ns1` and `ns2`
&lt;filter kubernetes.**&gt;
  @type grep
  &lt;exclude&gt;
    key $.kubernetes.namespace_
    pattern /^(?!(ns1|ns2))/
  &lt;/exclude&gt;
&lt;/filter&gt;
</code></pre>
"
Datadog,60992355,60987316,3,"2020/04/02, 16:08:59",False,"2020/04/02, 16:08:59",239,2268923,0,"<p>did u try bulk publish?</p>

<p>publish(topic,[]Message{1,2,3,4,.....})</p>
"
Datadog,59738942,59734858,0,"2020/01/14, 19:30:22",False,"2020/01/14, 19:30:22",2803,10809602,1,"<p>I believe your requirement can be accomplished using cmdlet <a href=""https://docs.microsoft.com/en-us/powershell/module/az.compute/invoke-azvmruncommand?view=azps-3.3.0"" rel=""nofollow noreferrer"">Invoke-AzVMRunCommand</a> / <a href=""https://docs.microsoft.com/en-us/powershell/module/azurerm.compute/invoke-azurermvmruncommand?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">Invoke-AzureRmVMRunCommand</a> or <a href=""https://docs.microsoft.com/en-us/powershell/module/az.compute/set-azvmcustomscriptextension?view=azps-3.3.0"" rel=""nofollow noreferrer"">Set-AzVMCustomScriptExtension</a> / <a href=""https://docs.microsoft.com/en-us/powershell/module/azurerm.compute/set-azurermvmcustomscriptextension?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">Set-AzureRmVMCustomScriptExtension</a>.</p>

<p>Related scripts can be found <a href=""http://saemundsson.se/?p=726"" rel=""nofollow noreferrer"">here</a> and <a href=""https://social.msdn.microsoft.com/Forums/en-US/fa98fbee-8c19-4885-a0ce-a93c92eeb559/change-pagefile?forum=azureautomation"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Just FYI, <a href=""https://social.msdn.microsoft.com/Forums/en-US/f86d31d6-ecb2-4aae-a615-fee08f53ccca/error-on-remoting-vms?forum=azureautomation"" rel=""nofollow noreferrer"">this</a> and <a href=""https://social.msdn.microsoft.com/Forums/en-US/9044cbf4-d0de-4853-88df-b4b94b8badd4/how-to-remote-a-vm-using-azure-runbooks?forum=azureautomation"" rel=""nofollow noreferrer"">this</a> are actual references for the above information.</p>

<p>Hope this update helps!</p>
"
Datadog,60614784,59519717,0,"2020/03/10, 11:34:48",False,"2020/03/10, 11:34:48",57,339202,0,"<p>This is possible by adding the annotation below to nginx ingress:</p>

<pre><code>annotations:
    ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/configuration-snippet: |
      opentracing_trace_locations off;
      opentracing_tag resource.name $uri;
</code></pre>

<p>See full answer at <a href=""https://github.com/DataDog/dd-opentracing-cpp/issues/118"" rel=""nofollow noreferrer"">https://github.com/DataDog/dd-opentracing-cpp/issues/118</a></p>
"
Datadog,57763275,57762582,0,"2019/09/03, 01:43:07",True,"2019/09/03, 01:43:07",104968,1766831,1,"<ol>
<li><p>Turn on the ""general log"" and have it write to a file.</p></li>
<li><p>Wait a finite amount of time.</p></li>
<li><p>Then use <code>pt-query-digest</code> to summarize the results.</p></li>
<li><p>Turn off the general log before it fills up disk.</p></li>
</ol>

<p>The slowlog (with a small value in <code>long_query_time</code>) is more useful for finding naughty queries.</p>
"
Datadog,55752352,55737494,0,"2019/04/18, 22:06:04",False,"2019/04/18, 22:06:04",175,2356037,0,"<p>The issue was the size of the HTTPRequest was to large the higher the parallelism which makes sense. I was getting back ""Request Entity Too Large"" however the exception wasn't logging out correctly so I missed it.</p>

<p>It seems that the Flink <a href=""https://github.com/apache/flink/blob/release-1.6.2/flink-metrics/flink-metrics-datadog/src/main/java/org/apache/flink/metrics/datadog/DatadogHttpReporter.java#L118"" rel=""nofollow noreferrer"">DatadogHttpReporter</a> does not take the size of the request into consideration when building it. I modified the Reporter to limit the number of metrics per request to 1000. Now the metrics are showing up just fine.</p>
"
Datadog,54751580,54735683,2,"2019/02/18, 18:30:16",False,"2019/02/18, 18:30:16",1371,5540166,0,"<p>This becomes pretty easy with Datadog's Log Management product -- you can measure lots of things by endpoint, including hits, unique client-ip count, latency (if you add response time to your nginx logs). </p>

<p>More info on the setup and these use cases <a href=""https://www.datadoghq.com/blog/how-to-monitor-nginx-with-datadog/#capturing-nginx-logs"" rel=""nofollow noreferrer"">in this blogpost</a>.</p>

<p>Documentation on the logs part of the nginx integration <a href=""https://docs.datadoghq.com/integrations/nginx/#log-collection"" rel=""nofollow noreferrer"">here</a>. </p>
"
Datadog,53449811,53441210,1,"2018/11/23, 18:04:49",True,"2018/11/23, 18:04:49",1371,5540166,1,"<p>This is definitely possible, but you will want to change your tag setup a little. You want to take advantage of <code>key:value</code> syntax with your tags, so that you can group out the tags by their common <code>key</code>. </p>

<p>So in your case, instead of tagging by <code>entity.count.payment</code>, you would want to tag by <code>entity.count:payment</code> or better yet <code>entity:payment</code>. That way you can write one query of your metric and use the <code>group by</code> functionality on the shared <code>entity</code> tag key to see it's values for all the different <code>entity</code> tags.</p>

<p>From there, you can use the <code>top</code> function to always see just the top n values, whether that be <code>payment</code> or <code>cart</code> or <code>visit</code> etc.</p>

<p><a href=""https://docs.datadoghq.com/tagging/"" rel=""nofollow noreferrer"">This doc here about tags</a> is definitely worth a read! Tags can make graphing and monitoring much easier and more scalable. </p>
"
Datadog,52971077,50468823,0,"2018/10/24, 17:06:01",False,"2018/10/24, 17:06:01",338,8335684,0,"<p>It is necessary to <a href=""https://kamon.io/documentation/1.x/recipes/adding-the-aspectj-weaver/"" rel=""nofollow noreferrer"">add AspectJ weaver as Java Agent</a> when you're starting your Akka aplication: <code>-javaagent:aspectjweaver.jar</code></p>

<p>You can add the following settings in your project SBT configuration:</p>

<pre><code>.settings(
  retrieveManaged := true,
  libraryDependencies += ""org.aspectj"" % ""aspectjweaver"" % aspectJWeaverV)
</code></pre>

<p>So AspectJ weaver JAR will be copied to <code>./lib_managed/jars/org.aspectj/aspectjweaver/aspectjweaver-[aspectJWeaverV].jar</code> in your project root.</p>

<p>Then you can refer this JAR in your Dockerfile:</p>

<pre><code>COPY ./lib_managed/jars/org.aspectj/aspectjweaver/aspectjweaver-*.jar /app- 
workdir/aspectjweaver.jar
WORKDIR /app-workdir
CMD [""java"", ""-javaagent:aspectjweaver.jar"", ""-jar"", ""app.jar""]
</code></pre>
"
Datadog,49608019,49607708,6,"2018/04/02, 11:42:22",False,"2018/04/02, 11:42:22",1443,4333853,1,"<p>Few things to debug </p>

<ul>
<li>enable the broker logs to trace</li>
<li>Compare the logs of one receiving more request and once receiving less request for some short duration which will have ample produce requests to analyze for comparison </li>
<li>Search for ProducerRequest in the log , it will give you the insight if it partitioning is happening as expected and also give info about from which host it is receiving more requests.</li>
</ul>
"
Datadog,49277969,49274395,0,"2018/03/14, 14:36:12",True,"2018/04/29, 00:26:33",2024,698082,1,"<p>You can easily get all needed data via querying dmv and other resources inside SQL Server. Good start is <a href=""https://www.sqlskills.com/blogs/glenn/sql-server-diagnostic-information-queries-for-march-2018/"" rel=""nofollow noreferrer"">here</a>.</p>
"
Datadog,46871722,46867883,2,"2017/10/22, 10:49:32",False,"2017/10/22, 10:49:32",2208,5259881,1,"<p>I have 35 Cassandra nodes (different clusters) monitored without any problems with graphite + carbon + whisper + grafana. But i have to tell that re-configuring collection and aggregations windows with whisper is a pain. </p>

<p>There's many alternatives today for this job, you can use influxdb (+ telegraf) stack for example. </p>

<p>Also with datadog you don't need grafana, they're also a visualizing platform. I've worked with it some time ago, but they have some misleading names for some metrics in their plugin, and some metrics were just missing. As a pros for this platform, it's really easy to install and use. </p>
"
Datadog,46949745,46867883,2,"2017/10/26, 11:49:53",False,"2017/10/26, 11:49:53",341,529321,1,"<p>We have a cassandra cluster of 36 nodes in production right now (we had 51 but migrated the instance type since then so we need less C* servers now), monitored using a single graphite server. We are also saving data for 30 days but in a 60s resolution. We excluded the internode metrics (e.g. open connections from a to b) because of the scaling of the metric count, but keep all other. This totals to ~510k metrics, each whisper file being ~500kb in size => ~250GB. iostat tells me, that we have write peaks to ~70k writes/s. This all is done on a single AWS i3.2xlarge instance which include 1.9TB nvme instance storage and 61GB of RAM. To fully utilize the power of the this disk type we increased the number of carbon caches. The cpu usage is very low (&lt;20%) and so is the iowait (&lt;1%).</p>

<p>I guess we could get away with a less beefy machine, but this gives us a lot of headroom for growing the cluster and we are constantly adding new servers. For the monitoring: Be prepared that AWS will terminate these machines more often than others, so backup and restore are more likely a regular operation.</p>

<p>I hope this little insight helped you.</p>
"
Datadog,41809571,39172174,0,"2017/01/23, 17:14:15",False,"2017/01/23, 17:14:15",89,4256613,1,"<p>It looks like that you have not set your JMX_PORT for kafka from where your datadog agent can listen information abouot the metrics.</p>

<p>Restart your Kafka with the following additional key/value pair parameter:
'JMX_PORT=9999'     </p>

<p>$ JMX_PORT=9999 ./kafka-server-start.sh ../config/server.properties </p>
"
Datadog,50008438,39172174,0,"2018/04/24, 21:19:55",False,"2018/04/24, 21:19:55",3040,4460737,0,"<p>This error essentially means that the Datadog Agent is unable to connect to the Kafka instance to retrieve metrics from the exposed mBeans over the RMI protocol. This error can be resolved by including the following JVM (Java Virtual Machine) arguments when starting the Kafka instance (required for Producer, Consumer, and Broker as they are all separate Java instances)
please </p>

<p><a href=""https://docs.datadoghq.com/integrations/faq/troubleshooting-and-deep-dive-for-kafka/"" rel=""nofollow noreferrer"">Please read this article</a></p>
"
Datadog,39174290,39163880,0,"2016/08/26, 23:35:25",True,"2016/08/26, 23:35:25",961,247700,2,"<p>Nexus 3.0.1 exposes authenticated access to metrics using <a href=""http://metrics.dropwizard.io/3.1.0/manual/servlets/"" rel=""nofollow"">http://metrics.dropwizard.io/3.1.0/manual/servlets/</a>
You have these endpoints available for different purposes:
<code>
        {host:port}/service/metrics/healthcheck
        {host:port}/service/metrics/data
        {host:port}/service/metrics/ping
        {host:port}/service/metrics/threads
</code></p>
"
Datadog,30168310,30155496,3,"2015/05/11, 15:58:22",True,"2015/05/11, 15:58:22",1802,1368626,2,"<p>I recall the default behavior being that each gear can handle 16 concurrent connections, then auto-scaling would kick in and you would get a new gear. Therefore I would think it makes sense to start by testing that a gear works well with 16 users at once. If not, then you can <a href=""https://blog.openshift.com/customizing-autoscale-functionality-in-openshift/"" rel=""nofollow"">change the scaling policy</a> to what works best for you application.</p>

<p><a href=""https://marketplace.openshift.com/apps/5189#!overview"" rel=""nofollow"">BlazeMeter</a> is a tool that could probably help with creating the connections. They mention 100,000 concurrent users on that main page so I don't think you have to worry about getting banned for this sort of test.</p>
"
Datadog,60749201,60748902,0,"2020/03/19, 02:12:25",False,"2020/03/19, 02:12:25",251,5477963,0,"<p>There is <a href=""https://www.npmjs.com/package/dogapi"" rel=""nofollow noreferrer"">dogapi</a> which wraps the entire Datadog API and should be able to do the above use case, probably using a mix of <a href=""https://brettlangdon.github.io/node-dogapi/#metric-query"" rel=""nofollow noreferrer"">metric.query</a>, <a href=""https://brettlangdon.github.io/node-dogapi/#infrastructure-search"" rel=""nofollow noreferrer"">infrastructure.search</a>, <a href=""https://brettlangdon.github.io/node-dogapi/#search-query"" rel=""nofollow noreferrer"">search.query</a> and <a href=""https://brettlangdon.github.io/node-dogapi/#monitor-getAll"" rel=""nofollow noreferrer"">monitor.getAll</a>.</p>

<p>For example, to get the list of monitors, it would look something like this:</p>

<pre class=""lang-js prettyprint-override""><code>const dogapi = require('dogapi')

dogapi.initialize({
  api_key: 'your_api_key',
  app_key: 'your_app_key'
})

dogapi.monitor.getAll((err, res) =&gt; {
  console.log(res)
})
</code></pre>

<p>Please keep in mind that I didn't test the above code.</p>

<p>If you need something that is not in the library, it should also be fairly easy to wrap the API directly since it's a simple HTTP endpoint.</p>

<p>I hope this helps!</p>
"
Datadog,47014511,47003909,0,"2017/10/30, 13:47:57",True,"2017/10/30, 13:47:57",1035,5825844,1,"<p>There are a variety of issues here.</p>

<p><strong>1. You've misconfigured the scope formats. (metrics.scope.operator)</strong></p>

<p>For one the configuration doesn't make sense since you specify ""metrics.scope.operator"" multiple times; only the last config entry is honored.</p>

<p>Second, and more importantly, you have misunderstood for scope formats are used for.</p>

<p>Scope formats configure which context information (like the ID of the task) is included in the reported metric's name.</p>

<p>By setting it to a constant (""latency"") you've told Flink to not include anything. As a result, the numRecordsIn metrics for every operator is reported as ""latency.numRecordsIn"".</p>

<p>I suggest to just remove your scope configuration.</p>

<p><strong>2. You've misconfigured the Datadog Tags</strong></p>

<p>I do not understand what you were trying to do with your tags configuration.</p>

<p>The tags configuration option can only be used to provide <strong>global</strong> tags, i.e. tags that are attached to every single metrics, like ""Flink"".</p>

<p>By <strong>default</strong> every metric that the Datadog reports has tags attached to it for every available scope variable available.</p>

<p>So, if you have an operator name A, then the numRecordsIn metric will be reported with a tag ""operator_name:A"".</p>

<p>Again, I would suggest to just remove your configuration.</p>
"
Datadog,62074774,62074299,0,"2020/05/29, 00:23:05",False,"2020/05/29, 00:23:05",156,2676108,0,"<p>You probably want to be using the MySQL integration, and configure the 'custom queries' option: <a href=""https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/faq/how-to-collect-metrics-from-custom-mysql-queries</a></p>

<p>You can follow those instructions after you configure the base integration <a href=""https://docs.datadoghq.com/integrations/mysql/#pagetitle"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/integrations/mysql/#pagetitle</a> (This will give you a lot of use metrics in addition to the custom queries you want to run)</p>

<p>As you mentioned, DogStatsD is a library you can import to whatever script or application in order to submit metrics. But it really isn't a common practice in the slightest to modify the underlying code of your database. So instead it makes more sense to externally run a query on the database, take those results, and send them to datadog. You could totally write a python script or something to do this. However the Datadog agent already has this capability built in, so it's probably easier to just use that.</p>

<hr>

<p>I am also just assuming SQL refers to MySQL, there are other integration for things like SQL Server, and PostgreSQL, and pretty much every implementation of sql. And the same pattern applies where you would configure the integration, and then add an extra line to the config file where you have the check run your queries.</p>
"
Datadog,50827960,50827869,0,"2018/06/13, 04:31:37",False,"2018/06/13, 04:31:37",470,951739,0,"<p>Use the <a href=""http://docs.python-requests.org/en/master/"" rel=""nofollow noreferrer"">requests</a> library its a lot simpler</p>

<p>Generate a request header like this</p>

<pre><code>def headers(apikey):
    return {'Authorization': 'Bearer {}'.format(apikey),
            'Content-Type': 'application/json'}
</code></pre>

<p>Send the request like this</p>

<pre><code>result = get(url, headers=headers(apikey))
</code></pre>
"
Datadog,50827969,50827869,0,"2018/06/13, 04:33:31",True,"2018/06/13, 04:33:31",631,9933041,1,"<p>While searching through this <a href=""https://github.com/micrometer-metrics/micrometer/issues/415"" rel=""nofollow noreferrer"">other issue</a>, I found that all that is needed to fix this issue is to specify the API key and the application key within the URL. Consider the following.</p>

<pre><code>def getSkeleton(self):
    api_key = 'your api key';
    app_key = 'your application key';
    boards = self.getAll(); # utilizing the api.ScreenBoards.get_all() function
    boardList = boards['screenboards'];
    for x in boardList:
        url = self.target + x['resource'] + ""?api_key="" + api_key +""&amp;application_key="" + app_key;
        data = urllib.urlopen(url).read();
        print data
</code></pre>
"
Datadog,50957805,50460608,0,"2018/06/21, 01:13:11",True,"2018/06/21, 01:13:11",3956,2417043,1,"<p>Yes, kind of.</p>

<p>It's possible to show single value on a dashboard (just use ""Query Value"" visualization), but it must be based on some metric reported to Datadog.</p>

<p>This is how it looks like:
<a href=""https://i.stack.imgur.com/cpEwt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cpEwt.png"" alt=""enter image description here""></a></p>
"
Datadog,45742428,45370569,1,"2017/08/17, 21:19:43",True,"2017/08/17, 21:19:43",3223,43973,1,"<p>I had issues with it too. The agent is most likely sending a value for 100 incorrectly as 10000 instead, as that's the highest I've seen it go. To make it usable, ensure the graph type is 'line', click ""Advanced"" on the metric, and make the equation <code>a / 100</code>.</p>

<p>I didn't find any guidance on this either in my search, but this seems to go along with what I've seen logged locally into a Windows box and watching the performance counters locally.</p>
"
Datadog,64825884,64720852,0,"2020/11/13, 20:07:25",True,"2020/11/24, 23:46:04",177,872145,0,"<p>An issue with IE11 is fixed in v1.26.1
See the fix here: <a href=""https://github.com/DataDog/browser-sdk/pull/633/files"" rel=""nofollow noreferrer"">[RUMF-791] prevent IE11 performance entry error #633</a></p>
"
Datadog,65396545,65396200,1,"2020/12/21, 18:34:57",False,"2020/12/21, 18:34:57",64,6668901,1,"<p>You can look in to other datadog kube metrics like kubernetes.replicas.available / total to alert if no of available - total &lt; 0. Same can be done or for daemonset pods also there is a specific metric exposed. [Datadog docs-kube metrics][1]
[1]: https://docs.datadoghq.com/agent/kubernetes/data_collected/</p>
"
Datadog,52066429,52066057,0,"2018/08/29, 00:28:16",False,"2018/08/29, 00:28:16",1371,5540166,0,"<p>Maybe you could use the Datadog respective dashboard apis to update your metric names in all dashboards? Should theoretically be not too complicated to script out. </p>

<p><a href=""https://docs.datadoghq.com/api/?lang=python#screenboards"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/?lang=python#screenboards</a>
<a href=""https://docs.datadoghq.com/api/?lang=python#timeboards"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/api/?lang=python#timeboards</a></p>
"
Datadog,49328630,49328198,2,"2018/03/16, 21:59:02",False,"2018/03/16, 21:59:02",1371,5540166,1,"<p>First you may need to download the MSI file:</p>

<pre><code>$image_url = ""https://s3.amazonaws.com/ddagent-windows-stable/ddagent-cli-latest.msi""
$destin = ""C:\path\to\downloaded\ddagent-cli-latest.msi""
(New-Object System.Net.WebClient).DownloadFile($image_url, $destin)
</code></pre>

<p>The actual powershell command for installation (with extra optional arguments included as arguments):</p>

<pre><code>msiexec /i C:\path\to\downloaded\ddagent-cli-latest.msi /l*v C:\path\to\installation_log.txt /quiet APIKEY=""$DD_API_KEY"" HOSTNAME=""$HOSTNAME"" TAGS=`""$TAGS,COMMA,DELIMITED`
</code></pre>

<p>It's been a while since i've done this (8 months or so?), so it could be outdated, but it used to work :). </p>

<p>Note, if you're running this from a remote provisioning script, you'll probly have to schedule this to be executed not-remotely so that the installation command can be run with heightened permissions, which i believe is required. And you <em>may</em> need to make sure the computer is plugged into the power source (i remember hitting some infuriating issue where that was an arbitrary requirement for Windows scheduled tasks to run, and Windows didn't allow me to configure around that).</p>
"
Datadog,46846772,46846059,0,"2017/10/20, 13:03:32",False,"2017/10/20, 13:03:32",148,6815223,1,"<p>For the container agent, you'll want to run <code>sudo docker exec -it dd-agent /etc/init.d/datadog-agent status</code> from your unix based box. If, however, you are using the alpine image the command is: <code>docker exec -it dd-agent /opt/datadog-agent/bin/agent status</code> (different path). More here in this KB from Datadog: <a href=""https://help.datadoghq.com/hc/en-us/articles/203764635-Agent-Status-and-Information"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/203764635-Agent-Status-and-Information</a></p>
"
Datadog,48325027,48314382,2,"2018/01/18, 17:39:30",False,"2018/01/18, 17:39:30",1371,5540166,0,"<p>is your <code>activemq_58.yaml</code> all in one line like that? You probably want it to be more like this:</p>

<pre><code>instances:
    - host: localhost
      port: 8161
      user: admin
      password: admin
</code></pre>
"
Datadog,33961640,33960552,0,"2015/11/27, 18:39:07",True,"2015/11/27, 18:39:07",5244,1991579,1,"<p>Just should set hostname for <code>org.coursera.metrics.datadog.DatadogReporter.Builder</code>:</p>

<pre><code>.withHost(InetAddress.getLocalHost().getCanonicalHostName())
</code></pre>
"
Datadog,66357269,66354474,1,"2021/02/24, 21:03:12",False,"2021/02/24, 21:03:12",60467,86611,0,"<p>There is a preview feature that allows you to graph your SNAT port usage and allocation, see:</p>
<p><a href=""https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics#how-do-i-check-my-snat-port-usage-and-allocation"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-diagnostics#how-do-i-check-my-snat-port-usage-and-allocation</a></p>
"
Datadog,62016448,61888648,0,"2020/05/26, 10:07:33",False,"2020/05/26, 10:07:33",744,10300113,0,"<p>Answer from Datadog Support to this:</p>

<blockquote>
  <p>Thanks again for reaching out to Datadog!</p>
  
  <p>From looking further into this, there does not seem to be a way we can package the JDBC  driver with the Datadog Agent. I understand that this is not desirable as you would prefer to use a standard image but I believe the best way to have these bundled together would be to have a custom image for your deployment.</p>
  
  <p>Apologies for any inconveniences that this may cause. </p>
</blockquote>
"
Datadog,65403420,65320127,0,"2020/12/22, 06:52:25",False,"2020/12/22, 06:52:25",1,14869489,0,"<p>Window option available... Go to integration and  click on agents.. There is an option available windows ( left side ).. Click on windows then u get link..... Just copy the link and paste it on ur windows Server.... Then datadog starts monitoring... It takes 5 minutes to monitor</p>
"
Datadog,58689757,58630932,1,"2019/11/04, 10:41:42",True,"2019/11/04, 11:05:42",336,7243426,0,"<ol>
<li>First, I would check what metrics are being stored in Datadog with the <a href=""https://docs.datadoghq.com/api/?lang=python#query-timeseries-points"" rel=""nofollow noreferrer"">API</a></li>
<li>Second, I would check the type of metric you are sending <code>https://app.datadoghq.com/metric/summary?metric=&lt;my-metric&gt;</code> Not sure if a counter type can have decimal values, maybe a gauge would be more appropriate.</li>
<li>Finally, to display a value you can use a query value widget such as the one in the snippet below. Make sure to:

<ul>
<li>Take the last value (here aggregator: last)</li>
<li>Select the number of decimals of interest (here precision: 3)</li>
<li>Select the right space aggregation if you receive this metric from multiple places (i.e. multiple tags) (here avg:my_metric{*})</li>
</ul></li>
</ol>

<pre><code>{
  ""viz"": ""query_value"",
  ""requests"": [
    {
      ""q"": ""avg:nginx.logs.request.count{*}.as_count()"",
      ""type"": null,
      ""style"": {
        ""palette"": ""dog_classic"",
        ""type"": ""solid"",
        ""width"": ""normal""
      },
      ""aggregator"": ""last"",
      ""conditional_formats"": [
        {
          ""comparator"": ""&gt;"",
          ""palette"": ""white_on_red"",
          ""value"": null
        },
        {
          ""comparator"": ""&gt;="",
          ""palette"": ""white_on_yellow"",
          ""value"": null
        },
        {
          ""comparator"": ""&lt;"",
          ""palette"": ""white_on_green"",
          ""value"": null
        }
      ]
    }
  ],
  ""autoscale"": true,
  ""precision"": ""3""
}
</code></pre>

<p><strong>Side note</strong>: I also use this for Kafka (just as a reference) but it should not be required in your case:</p>

<pre><code>ENTRYPOINT [""java"",""-javaagent:dd-java-agent.jar"",""-Ddd.agent.host=localhost"",""-Ddd.jmxfetch.statsd.host=localhost"",""-Ddd.trace.global.tags=env:kafka"",""-Ddd.agent.port=8126"",""-Ddd.service.name=KafkaProducer"",""-Ddd.logs.injection=true"",""-Ddd.trace.analytics.enabled=true"",""-Ddd.kafka.analytics.enabled=true"",""-Ddd.servlet.analytics.enabled=true"",""-Ddd.spring-web.analytics.enabled=true"",""-jar"",""target/KafkaConsumer-0.0.1-SNAPSHOT.jar""]
</code></pre>
"
Datadog,50155727,50148430,3,"2018/05/03, 15:59:44",False,"2018/05/04, 19:46:07",11561,970308,0,"<p>This sounds like a bug. It is possible the Datadog exporter is running in a non-daemon thread. The JVM views non-daemon threads as application critical work. </p>

<p>So essentially the JVM thinks it shouldn't shutdown until the non-daemon thread finishes. In the case of the Datadog exporter thread, that probably won't happen.</p>

<p>To verify there are non-daemon threads, use <code>jstack</code> to generate a thread dump. (command: <code>jstack &lt;pid&gt;</code>) or dump all threads in your <code>close</code> method:</p>

<pre><code>ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();
for (ThreadInfo ti : threadMxBean.dumpAllThreads(true, true)) {
  System.out.print(ti.toString());
}
</code></pre>

<p>An example thread dump output is below. Notice the word 'daemon' on the first line:</p>

<pre><code>""pool-1-thread-1"" #13 prio=5 os_prio=31 tid=0x00007fe885aa5000 nid=0xa907 waiting on condition [0x000070000d67b000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000006c07e9720&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
"
Datadog,44034370,44034151,0,"2017/05/17, 23:37:34",True,"2017/05/17, 23:37:34",261,4172512,2,"<p>The recommendation of:</p>

<blockquote>
  <p>""Please don't include endlessly growing tags in your metrics, like timestamps or user ids. Please limit each metric to 1000 tags.""</p>
</blockquote>

<p>Is more of a warning against using infinitely expanding values as they can drastically increase your custom metric usage.  As mentioned in the following article:</p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/204271775-What-is-a-custom-metric-and-what-is-the-limit-on-the-number-of-custom-metrics-I-can-have-</a></p>

<blockquote>
  <p>""By default customers are allotted 100 custom metrics per host across their entire infrastructure rather than on a per-host basis. For example if you were licensed for 3 hosts, you would have 300 custom metrics by default - these 300 metrics may be divided equally amongst each individual host, or all 300 metrics could be sent from a single host.""</p>
</blockquote>

<p>You will want to keep in mind when configuring your metrics/tags of your current allotment of custom metrics and any billing implications that may have.  That said, if having these tags is important to your team please reach out to support@datadoghq.com and we can sync up with the Sales Team to determine what is best for your team and use case.</p>
"
Datadog,63606465,63326724,0,"2020/08/27, 01:15:40",True,"2020/08/27, 01:15:40",797,2135,0,"<p>Allowing 'triggered' (but not 'recovery') monitor notifications is not a configurable option for the integration on either Opsgenie or Datadog.</p>
<p>You <em>can</em>, however, control this within the Datadog Monitor message body where you reference opsgenie</p>
<blockquote>
<p>Lorem ipsum dolor sit amet @opsgenie-oncall @slack-somechannel</p>
</blockquote>
<p>You can wrap the opsgenie reference within the message body with conditional tags (datadog actually calls them variables) documented here:
<a href=""https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/monitors/notifications/?tab=is_alert#conditional-variables</a>
Then the message body might look something like this:</p>
<blockquote>
<p>Lorem ipsum dolor sit amet {{#is_alert}}@opsgenie-oncall{{/is_alert}} @slack-somechannel</p>
</blockquote>
<p>Now the alert to opsgenie will only occur on the 'trigger' of the monitor but not on 'recovery'</p>
"
Datadog,63971375,63909996,0,"2020/09/19, 20:06:40",True,"2020/09/19, 20:06:40",3852,1601506,1,"<p>Datadog keeps the logs for a period of time according to the billing plan you've selected: <a href=""https://www.datadoghq.com/pricing/#section-log"" rel=""nofollow noreferrer"">https://www.datadoghq.com/pricing/#section-log</a> If you choose the 7 day plan, logs will be dropped from Datadog after 7 days.</p>
<p>The default plan seems to be 15 days, but there are other options between 3-60 days.</p>
"
Datadog,49818706,49381672,0,"2018/04/13, 16:56:49",False,"2018/04/13, 16:56:49",939,2506172,3,"<p><a href=""https://docs.datadoghq.com/developers/metrics/#gauges"" rel=""nofollow noreferrer"" title=""Gauge metric"">Gauge metric</a> types will do the job here given that your query does not run more than once within 10 seconds. If that is not the case, go for <a href=""https://docs.datadoghq.com/developers/metrics/#count"" rel=""nofollow noreferrer"" title=""count metric"">count metric</a> </p>

<p>The flush interval in datadog by default is 10 seconds, if you use a <strong>gauge metric</strong> and the metric is reported more than once in a flush interval, datadog agent only sends the last value ignoring the previous ones. For <strong>count metric</strong> in contrast, the agent sums up all the values reported in the flush interval.</p>

<p>More details about flush interval <a href=""https://help.datadoghq.com/hc/en-us/articles/211545826-Why-histogram-stats-are-all-the-same-inaccurate-Characteristics-of-Datadog-histograms-"" rel=""nofollow noreferrer"" title=""here"">here</a>.</p>
"
Datadog,65533013,49381672,0,"2021/01/01, 21:35:13",False,"2021/01/01, 21:35:13",1718,555329,0,"<p>The best metric type would be a <code>histogram</code> metric.  This will take multiple values, and pre-aggregate them within a flush window, so you will be able to get things like min/max/sum/avg and various percentiles.</p>
<p>If you run multiple times within a flush window:</p>
<ul>
<li><code>count</code> would combine multiple values together, so you would lose the individual numbers, meaning you couldn't easily tell between the process returning a lot of documents, or it returning only a few, but being called a lot</li>
<li><code>gauge</code>, as mentioned in @narayan's answer, would only keep the latest, making it harder to get thins like the max/min count.</li>
</ul>
"
Datadog,53624609,53459133,0,"2018/12/05, 05:10:35",True,"2018/12/05, 05:10:35",3223,43973,1,"<p>Use the Query Value widget. It can only show a single value, which is the average for the current time window that has been chosen.</p>
"
Datadog,58877543,58873873,2,"2019/11/15, 14:56:44",False,"2019/11/19, 14:49:29",336,7243426,0,"<p>One solution would be to setup in <code>logs &gt; configuration &gt; pipelines</code> a <a href=""https://docs.datadoghq.com/logs/processing/processors/?tab=ui#category-processor"" rel=""nofollow noreferrer"">category processor</a> to add a new attribute that could be made searchable.</p>

<p><a href=""https://i.stack.imgur.com/0l7ZJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0l7ZJ.png"" alt=""category processor example""></a></p>

<hr>

<p><em>edit: 19th Nov 2019</em></p>

<p>Step 1:</p>

<p>Add grok parser to extract sign:</p>

<p>rule: <code>detect_dollar .*%{regex(""[$]+""):dollarSign}.*</code>
<a href=""https://i.stack.imgur.com/aJwhy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aJwhy.png"" alt=""enter image description here""></a></p>

<p>Step 2:</p>

<p>Then you can setup a category processor as indicated above. This could look for the attribute <code>@dollarSign:$</code> and set the attribute <code>hasDollarSign</code> to True and set it to false otherwise.</p>

<p><a href=""https://i.stack.imgur.com/wmivb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wmivb.png"" alt=""enter image description here""></a></p>

<p>Step 3:</p>

<p>Create a facet on <code>dollarSign</code> attribute. Following logs can then be searched for.</p>

<p>For logs with no <code>$</code></p>

<pre><code>-@dollarSign:$
</code></pre>

<p>For logs with <code>$</code></p>

<pre><code>@dollarSign:$
</code></pre>

<p>You can do the same with the <code>hasDollarSign</code> attribute and set it as a facet.</p>
"
Datadog,66554834,66554063,2,"2021/03/09, 23:11:35",False,"2021/03/09, 23:11:35",1693,971735,2,"<h2>TL;DR</h2>
<p><code>spring.sleuth.baggage.correlation-fields</code> automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.</p>
<h2>Longer version with background</h2>
<p>I suppose you use Sleuth out of the box (uses Brave):</p>
<ol>
<li>Fist get familiar with <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#getting-started-terminology"" rel=""nofollow noreferrer"">tag</a>, <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-baggage"" rel=""nofollow noreferrer"">baggage</a> and their <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-baggage-vs-tags"" rel=""nofollow noreferrer"">differences</a>; you can also read about them in <a href=""https://github.com/openzipkin/brave/tree/master/brave#baggage"" rel=""nofollow noreferrer"">Brave docs</a></li>
<li>Check Brave's <a href=""https://github.com/openzipkin/brave/blob/5be287b91c3d18da9fc7a597d71b12b27be6043c/brave/src/main/java/brave/propagation/CurrentTraceContext.java#L176"" rel=""nofollow noreferrer""><code>ScopeDecorator</code></a>, <a href=""https://github.com/openzipkin/brave/blob/5be287b91c3d18da9fc7a597d71b12b27be6043c/brave/src/main/java/brave/baggage/CorrelationScopeDecorator.java#L28"" rel=""nofollow noreferrer""><code>CorrelationScopeDecorator</code></a> and <a href=""https://github.com/openzipkin/brave/blob/5be287b91c3d18da9fc7a597d71b12b27be6043c/context/slf4j/src/main/java/brave/context/slf4j/MDCScopeDecorator.java#L25"" rel=""nofollow noreferrer""><code>MDCScopeDecorator</code></a></li>
</ol>
<p>The <code>spring.sleuth.baggage.correlation-fields</code> property automatically sets baggage values to Slf4j’s MDC so you only need to set the baggage field.</p>
<p>Also, using <code>MDCScopeDecorator</code>, you can set the baggage values to Slf4j’s MDC programmatically, you can see how to do it in <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-baggage"" rel=""nofollow noreferrer"">Sleuth docs</a>:</p>
<pre class=""lang-java prettyprint-override""><code>// configuration
@Bean
BaggageField countryCodeField() {
    return BaggageField.create(&quot;country-code&quot;);
}

@Bean
ScopeDecorator mdcScopeDecorator() {
    return MDCScopeDecorator.newBuilder()
            .clear()
            .add(SingleCorrelationField.newBuilder(countryCodeField())
                    .flushOnUpdate()
                    .build())
            .build();
}

// service
@Autowired
BaggageField countryCodeField;

countryCodeField.updateValue(&quot;new-value&quot;);
</code></pre>
"
Datadog,66538157,66533080,0,"2021/03/09, 00:29:39",True,"2021/03/09, 00:29:39",1693,971735,1,"<p>We figured this out in the comments, I'm posting an answer that summarizes it all up: it seems the root cause was using different versions of different spring-boot modules.</p>
<p>It is a good rule of thumb to not define the versions yourself but use BOMs and let them define the versions for you, e.g. see: <a href=""https://search.maven.org/artifact/org.springframework.boot/spring-boot-dependencies/2.4.3/pom"" rel=""nofollow noreferrer"">spring-boot-dependencies</a>. This way you will use the compatible (and tested) versions.</p>
<p><code>management.metrics.tags.your-tag</code> is the way to add tags to all of your metrics. A good way to check this is looking at <code>/actuator/metrics</code>.</p>
"
Datadog,65636279,65619895,3,"2021/01/08, 22:56:19",True,"2021/01/08, 22:56:19",385,2051074,2,"<p>I ended up going with the <code>sbt-javaagent</code> plugin to avoid extra code to exclude the agent jar from the classpath, which the plugin handles automatically.</p>
<p>The trick/hack was to filter out the default <a href=""https://github.com/sbt/sbt-javaagent/blob/v0.1.6/src/main/scala/com/lightbend/sbt/javaagent/JavaAgentPackaging.scala#L41"" rel=""nofollow noreferrer""><code>addJava -javaagent</code> line the <code>sbt-javaagent</code> plugin adds automatically</a>, and then appending a new script snippent to only enable the javaagent when a certain env. variable is set.</p>
<pre class=""lang-scala prettyprint-override""><code>lazy val dataDogAgentName = &quot;dd-java-agent&quot;
lazy val dataDogAgentVersion = &quot;0.70.0&quot;

lazy val distProject = project
  .enablePlugins(JavaAgent, JavaAppPackaging)
  .settings(
    javaAgents += &quot;com.datadoghq&quot; % dataDogAgentName % dataDogAgentVersion,
    bashScriptExtraDefines := bashScriptExtraDefines.value.filterNot(_.contains(&quot;javaagent&quot;)) :+ s&quot;&quot;&quot;
      |if [[ &quot;$$DD_PROFILING_ENABLED&quot; = &quot;true&quot; ]]; then
      |  addJava &quot;-javaagent:$${app_home}/../$dataDogAgentName/$dataDogAgentName-$dataDogAgentVersion.jar&quot;;
      |fi
      |&quot;&quot;&quot;.stripMargin,
  )
</code></pre>
"
Datadog,63061286,63054587,2,"2020/07/23, 21:56:23",False,"2020/07/23, 21:56:23",2661,1184752,2,"<p>You can use simple <code>jcmd</code> command line tool</p>
<pre><code>jcmd &lt;PID&gt; GC.class_histogram | less
</code></pre>
<p>As an example of running this on my simple Clojure application:</p>
<pre><code> num     #instances         #bytes  class name (module)
-------------------------------------------------------
   1:         40131        2944504  [B (java.base@14.0.1)
   2:         38953        1666008  [Ljava.lang.Object; (java.base@14.0.1)
   3:          7610         961920  java.lang.Class (java.base@14.0.1)
   4:         36134         867216  java.lang.String (java.base@14.0.1)
   5:          2353         762216  [I (java.base@14.0.1)
   6:         25076         601824  clojure.lang.PersistentHashMap$BitmapIndexedNode
   7:         13950         446400  java.util.concurrent.ConcurrentHashMap$Node (java.base@14.0.1)
...
</code></pre>
"
Datadog,62333814,62331584,0,"2020/06/12, 00:23:58",False,"2020/06/12, 00:23:58",3,5605817,0,"<p>As shown in the documentation in <a href=""https://docs.aws.amazon.com/glue/latest/dg/add-job-python.html#python-shell-supported-library"" rel=""nofollow noreferrer"">your link</a>, WHL files are also supported. It says:</p>

<blockquote>
  <p>You might already have one or more Python libraries packaged as an .egg or a .whl file.</p>
</blockquote>

<p>There is a .whl file available for the DataDog python library here: <a href=""https://pypi.org/project/datadog/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/datadog/#files</a>. You might try downloading that file, uploading it to your S3 bucket, and using that as your Python library for your Glue job. You might be more successful using that than trying to build your own .egg file. </p>
"
Datadog,62032453,62030485,0,"2020/05/27, 01:30:00",False,"2020/05/27, 01:30:00",101971,248823,0,"<p>The AWS API calls to start a task are:</p>
<ul>
<li><p><a href=""https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_StartTask.html"" rel=""nofollow noreferrer"">StartTask</a>:</p>
<blockquote>
<p>Starts a new task from the specified task definition on the specified container instance or instances.</p>
</blockquote>
</li>
<li><p><a href=""https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_RunTask.html"" rel=""nofollow noreferrer"">RunTask</a>:</p>
<blockquote>
<p>Starts a new task using the specified task definition. You can allow Amazon ECS to place tasks for you, or you can customize how Amazon ECS places tasks using placement constraints and placement strategies.</p>
</blockquote>
</li>
</ul>
<p>Since this is AWS API calls, there are equivalent calls in CLI and SDK.</p>
"
Datadog,61873777,61872153,0,"2020/05/18, 18:57:42",False,"2020/05/18, 18:57:42",19520,2332336,0,"<p>A colleaque of mine informed me that, since we're using docker, we can by-pass supervisor and just run the horizon artisan command directly as the entrypoint of the container.</p>

<p>So, I removed all things related to supervisor and my service yaml is simplified to the following and the logs are coming into datadog:</p>

<pre><code>version: '2'

    services:

      my-app-horizon:
        container_name: my-app-horizon
        image: ${DOCKER_IMAGE}
        mem_limit: 1024M
        command: ""/app/artisan horizon""
        ...snipped...
</code></pre>
"
Datadog,61912692,61644174,0,"2020/05/20, 15:05:31",False,"2020/05/20, 15:05:31",23,12932875,0,"<p>you can open it by navigating to the directory it is in, and then typing </p>

<pre><code>sudo nano datadog.yaml. 
</code></pre>

<p>You need root permissions to view the file as far as I know. </p>
"
Datadog,61590972,61588585,6,"2020/05/04, 14:18:01",True,"2020/05/04, 14:34:48",204,11981028,0,"<ol>
<li>download <a href=""https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-annotations/2.10.3"" rel=""nofollow noreferrer"">Jackson Annotations</a> and import it because <code>JsonLayout</code> use <code>Jackson Annotations</code> internal code.

<ul>
<li>in my case, download and import these</li>
</ul></li>
</ol>

<pre><code>jackson-annotations-2.10.3.jar
jackson-core-2.10.3.jar
jackson-databind-2.10.3.jar
</code></pre>

<ol start=""2"">
<li>add <code>&lt;JsonLayout&gt;...&lt;/JsonLayout&gt;</code> in your config(in my case log4j2.xml) like below</li>
</ol>

<pre><code>&lt;JsonLayout&gt;
  &lt;KeyValuePair key=""testKey"" value=""testValue""/&gt;
&lt;/JsonLayout&gt;
</code></pre>

<p>Replace <code>key</code> and <code>value</code> to what you want to use. In my case <code>key</code> is ""testKey"" and <code>value</code> is ""testValue""</p>

<ol start=""3"">
<li>run and check your log!</li>
</ol>

<p>it it my full sample code and xml configuration info.
code</p>

<pre><code>import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.apache.logging.log4j.ThreadContext;

public class LogTest{
    private static final Logger logger = LogManager.getLogger(LogTest.class.getName());


    public static void main(String[] args) {
        ThreadContext.put(""logFileName"", ""testFile1"" );
        logger.info(""log printed! - testFile1"");

        ThreadContext.put(""logFileName"", ""testFile2"" );
        logger.info(""log printed! - testFile2"");

        ThreadContext.remove(""logFileName"");
    }

}
</code></pre>

<p>log4j2.xml</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;Configuration status=""WARN""&gt;
    &lt;Appenders&gt;
        &lt;Routing name=""RoutingAppender""&gt;
            &lt;Routes pattern=""${ctx:logFileName}""&gt;
                &lt;Route&gt;
                    &lt;RollingFile name=""Rolling-${ctx:logFileName}""
                                 fileName=""./logs/${ctx:logFileName}.log""
                                 filePattern=""./logs/${ctx:logFileName}.%i.log.gz""&gt;
                        &lt;JsonLayout&gt;
                            &lt;KeyValuePair key=""testKey"" value=""testValue""/&gt;
                        &lt;/JsonLayout&gt;
                        &lt;SizeBasedTriggeringPolicy size=""512"" /&gt;
                    &lt;/RollingFile&gt;
                &lt;/Route&gt;
            &lt;/Routes&gt;
        &lt;/Routing&gt;
    &lt;/Appenders&gt;

    &lt;Loggers&gt;
        &lt;Root level=""all""&gt;
            &lt;AppenderRef ref=""RoutingAppender"" /&gt;
        &lt;/Root&gt;
    &lt;/Loggers&gt;
&lt;/Configuration&gt;
</code></pre>

<p>output</p>

<pre><code>{
  ""instant"" : {
    ""epochSecond"" : 1588590944,
    ""nanoOfSecond"" : 469000000
  },
  ""thread"" : ""main"",
  ""level"" : ""INFO"",
  ""loggerName"" : ""com.test.LogTest"",
  ""message"" : ""log printed! - testFile2"",
  ""endOfBatch"" : false,
  ""loggerFqcn"" : ""org.apache.logging.log4j.spi.AbstractLogger"",
  ""threadId"" : 1,
  ""threadPriority"" : 5,
  ""testKey"" : ""testValue""
}

</code></pre>
"
Datadog,60951103,60933143,2,"2020/03/31, 15:56:50",False,"2020/03/31, 15:56:50",11561,970308,1,"<p>You are configuring the filter on a new <code>StatsdMeterRegistry</code>. When using a <code>MeterRegistryCustomizer</code> you need to operate on the registry that was passed in.</p>

<pre><code>@Bean
public MeterRegistryCustomizer&lt;StatsdMeterRegistry&gt; meterRegistryCustomizer() {
    return (registry) -&gt; registry.config().meterFilter(MeterFilter.denyNameStartsWith(""jvm""));
   }   
</code></pre>

<p>Since the customizer will be used against all registries, you also would need to add an if statement to only filter against the registry you want filtered.</p>

<pre><code>@Bean
public MeterRegistryCustomizer&lt;StatsdMeterRegistry&gt; meterRegistryCustomizer() {
   return (registry) -&gt; { 
      if(registry instanceof StatsdMeterRegistry) {
        registry.config().meterFilter(MeterFilter.denyNameStartsWith(""jvm""));
      }   
   }
}   

</code></pre>
"
Datadog,62521519,60896119,0,"2020/06/22, 21:52:24",False,"2020/06/22, 21:52:24",374,5117002,0,"<p>It doesn't have Grafana support yet (coming in a week or 2) but Questdb supports traditional SQL on a Time Series database and might be able to do what you want. <a href=""https://questdb.io"" rel=""nofollow noreferrer"">https://questdb.io</a> or <a href=""https://github.com/questdb"" rel=""nofollow noreferrer"">https://github.com/questdb</a> on GitHub.</p>
"
Datadog,60471660,60465436,0,"2020/03/01, 05:14:06",False,"2020/03/01, 05:14:06",4659,3709060,0,"<p>Datadog can process logs through their pipeline fitering feature</p>

<p><a href=""https://docs.datadoghq.com/logs/processing/pipelines/"" rel=""nofollow noreferrer"">https://docs.datadoghq.com/logs/processing/pipelines/</a></p>
"
Datadog,60493178,60465436,0,"2020/03/02, 18:42:31",True,"2020/03/02, 18:42:31",1371,5540166,1,"<p>If you already have an attribute that contains the url, a really easy way to do this would be to use the <a href=""https://docs.datadoghq.com/logs/processing/"" rel=""nofollow noreferrer"">processing pipelines</a> and add a processor of the ""<a href=""https://docs.datadoghq.com/logs/processing/processors/?tab=ui#url-parser"" rel=""nofollow noreferrer"">url parser</a>"" type to these logs. You just plug in the attribute that contains the url and an attribute path that you'd like to contain all the outputs from it (usually <code>http.url_details</code>), and then all new logs will get the extra url parsing applied.</p>

<p>If your logs have the ""source:nginx"" applied to them (configured in the log shipper), then you'll already have an out-of-the-box Nginx processing pipeline that Datadog has for structuring standard Nginx syntax logs. You can clone that and then just add your new url parser there. Or, if your syntax is similar to the standard syntax, you can just modify their default suggested parsers (in the cloned pipeline). In any case, it'd be worth looking at that default pipeline for inspiration for other valuable things to do beyond url parsing. </p>
"
Datadog,59633846,59632924,0,"2020/01/07, 20:08:11",True,"2020/01/07, 20:08:11",13615,1061413,1,"<p>Assuming you'd like the output to look like the following:</p>

<pre><code>$ helm template efs-provisioner stable/efs-provisioner &lt;flags&gt; \
  | grep -m 1 -C 4 datadog
    chart: efs-provisioner-0.10.0
    release: ""efs-provisioner""
    heritage: ""Helm""
  annotations:
    ad.datadoghq.com/tags: '{""env"": ""staging""}'
</code></pre>

<p>you need to escape the <code>{</code> and and use <code>\""</code> instead of <code>\'</code>:</p>

<pre><code>$ helm template efs-provisioner stable/efs-provisioner \
  --set efsProvisioner.efsFileSystemId=fs-a1b2c3d4 \
  --set efsProvisioner.awsRegion=us-east-1 \
  --set annotations.""ad\.datadoghq\.com/tags""=""\{\""env\"": \""staging\""\}"" 
</code></pre>
"
Datadog,57978792,57978390,9,"2019/09/17, 19:52:43",False,"2019/09/17, 19:52:43",2237,11486670,0,"<p>Unfortuanetly I can not propose an exact solution/workaround to you but you might have a look at the following documentations/API's:</p>

<p><a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html"" rel=""nofollow noreferrer"">Indices Stats API</a></p>

<p><a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html"" rel=""nofollow noreferrer"">Cluster Stats API</a></p>

<p><a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html"" rel=""nofollow noreferrer"">Nodes Stats API</a></p>

<p>The cpu usage is not included in the exported fields but maybe you can derive a high cpu usage behaviour from the other fields.</p>

<p>I hope I could help you in some way.</p>
"
Datadog,56311741,56277311,0,"2019/05/26, 11:01:46",False,"2019/05/26, 11:01:46",2347,1271502,0,"<pre><code>aws ec2 describe-instances --filters ""Name=tag:Department,Values=RnD"" --filters ""Name=tag:Name,Values=Server1"" 
</code></pre>
"
Datadog,55661352,55621848,0,"2019/04/13, 04:58:31",True,"2019/04/13, 04:58:31",1371,5540166,1,"<p>You can reference <a href=""https://wiki.jenkins.io/display/JENKINS/Logging"" rel=""nofollow noreferrer"">this doc</a> to find where the default logging path is for Jenkins depending on your OS. (For linux, it's <code>/var/log/jenkins/jenkins.log</code> if you don't configure it to be something else.</p>

<p>Then as long as your <a href=""https://docs.datadoghq.com/agent/?tab=agentv6"" rel=""nofollow noreferrer"">Datadog agent</a> is v6+ you can use the Datadog agent to tail your jenkins.log file by following <a href=""https://docs.datadoghq.com/logs/log_collection/?tab=tailexistingfiles#tail-existing-files"" rel=""nofollow noreferrer"">this doc</a>. </p>

<p>Specifically, you'd add this line to your <code>dadatod.yaml</code>:</p>

<pre><code>logs_enabled: true
</code></pre>

<p>and add this content to any old <code>conf.yaml</code> file nested in your <code>conf.d/</code> directory, such as <code>conf.d/jenkins.d/conf.yaml</code>:</p>

<pre><code>logs:
  - type: file
    path: /var/log/jenkins/jenkins.log
    service: jenkins
    source: jenkins
</code></pre>

<p>Then the agent will tail your log file as it's written to, and will forward it to your Datadog account so you can query, graph, and monitor on your log data there. </p>

<p>Once you have the logs coming in, you may want to write a <a href=""https://docs.datadoghq.com/logs/processing/"" rel=""nofollow noreferrer"">processing pipeline</a> to get the critical attributes parsed out, but that would be material for a new question :) . </p>
"
Datadog,54284969,54280115,2,"2019/01/21, 09:09:17",True,"2019/01/21, 09:14:28",125340,6464308,1,"<p>This command will only take a split second.</p>

<p>You must have spent 35 minutes waitung for the <code>ACCESS EXCLUSIVE</code> lock on the table to be granted (all the while blocking any transaction unfortunate enough to be queued behind you).</p>

<p>You probably have a problem with long transactions. Normally they should be as short as possible, otherwise they hold locks for a long time and also keep <code>VACUUM</code> from cleaning up dead row versions.</p>

<p>The lock is necessary, but is should not pose a problem with a well behaved database workload.</p>
"
Datadog,52703484,52594973,0,"2018/10/08, 16:32:20",False,"2018/10/08, 16:32:20",66,10121786,0,"<p>You can create one PowerShell script to execute your Batch scripts remotely.
And Even you can schedule your PowerShell script using Windows Task Scheduler which will run as per your settings.</p>
"
Datadog,52065295,52064818,0,"2018/08/28, 22:53:03",False,"2018/08/28, 22:53:03",892,1757329,0,"<p>Rubber Duck.  Turns out because we changed <code>.set</code> to <code>.default</code>, we lost the ability to have the variables properly set during the first run.  <code>.normal</code> will do it for us.</p>
"
Datadog,50526751,50526252,1,"2018/05/25, 13:10:38",True,"2018/05/25, 13:10:38",629,7202531,2,"<p>Lots of threads are in WAITING state, and it's absolutely ok for them. For example, there are thread which have the following stack trace:</p>

<pre><code>...
at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:104)
at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:32)
...
</code></pre>

<p>This only means threads are waiting for any tasks to do.</p>

<p>However, other stacks do not look good.</p>

<pre><code>java.lang.Thread.State: WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
at com.mchange.v2.resourcepool.BasicResourcePool.**awaitAvailable**(BasicResourcePool.java:1414)
at com.mchange.v2.resourcepool.BasicResourcePool.prelimCheckoutResource(BasicResourcePool.java:606)
- locked &lt;0x000000055c2d3ce0&gt; (a com.mchange.v2.resourcepool.BasicResourcePool)
at 
com.mchange.v2.resourcepool.BasicResourcePool.checkoutResource(BasicResourcePool.java:526)
at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool.checkoutAndMarkConnectionInUse(C3P0PooledConnectionPool.java:755)
</code></pre>

<p>Those threads are waiting for connection to be free in the pool. C3P0 is a pool of database connections. Instead of creating a new connection every time, they are cached in the pool. Upon closing, the connection itself is not closed, but only returned to the pool. So, if hibernate for some reason (or other user) do not close connection after releasing it, then pool can get exhausted. </p>

<p>In order to resolve an issue, you have to find out why some connections are not closed after using. Try to look at your code to do this.</p>

<p>The other option is to temporarily go without C3P0 (pooling). This is not forever, but at least you can check whether this guess is right.</p>
"
Datadog,49475152,49468467,1,"2018/03/25, 14:11:43",True,"2018/03/25, 14:11:43",16823,1075289,1,"<p>In Grails 3, You should put the below code to <code>grails-app/conf/spring/resources.groovy</code>:</p>

<pre><code>import io.micrometer.core.instrument.MeterRegistry
import io.micrometer.spring.autoconfigure.MeterRegistryCustomizer

class CommonTagCustomizer implements MeterRegistryCustomizer&lt;MeterRegistry&gt; {

    void customize(MeterRegistry registry) {
        registry.config().commonTags(""host"", ""myapp-dev"")
    }
}

beans = {
    commonTags(CommonTagCustomizer) {}
}
</code></pre>
"
Datadog,48892885,48892401,0,"2018/02/20, 21:33:25",True,"2018/02/20, 21:33:25",43078,78722,2,"<p>Neither, you want something like this I think:</p>

<pre><code>api_key = data_bag_item('datadog', 'datadog_keys')['api_key']
                        ^ bag name ^ item name    ^ accessing something from the item hash
</code></pre>

<p>Also putting the key into node attributes like that is very unsafe and kind of defeats the point of encrypted bags since node attributes are all written back to the Chef Server and so the key will be sent unencrypted.</p>
"
Datadog,47779901,47779576,0,"2017/12/12, 21:09:55",False,"2017/12/12, 21:09:55",3545,1831118,0,"<p>I still don't know why it makes a difference, but adding the <code>-4</code> option made it work</p>

<pre><code>echo ""test_metric:1|c"" | nc -u -4 -w 1 localhost 8125
</code></pre>

<p>Here's the man page on the option:</p>

<blockquote>
  <p>-4      Forces nc to use IPv4 addresses only.</p>
</blockquote>
"
Datadog,36027695,36024447,2,"2016/03/16, 07:49:29",False,"2016/03/16, 19:40:27",824,2493338,0,"<p>Problem in this case is not running scripts via JMeter GUI. Instead it is related to network.</p>

<p>I had a similar distributed setup in EC2-environment and I successfully executed heavy load tests in GUI mode. In my case, all my JMeter (master/slaves) were running on EC2 instances (windows environment). So, I will recommend you to setup your <strong>JMeter</strong> <strong>(Master)</strong> on EC2 and run scripts via GUI mode.</p>

<p>If you still want to run in command line mode then you simply need to pass command to create jtl file while the script runs on command line. Later on you can use this JTL to generate any JMeter report as per requirement. For more details check.. </p>

<p><a href=""https://stackoverflow.com/questions/24537638/jmeter-run-jmx-file-through-command-line-and-get-the-summary-report-in-a-exce"">Jmeter - Run .jmx file through command line and get the summary report in a excel</a></p>

<blockquote>
  <p>jmeter -n -t /path/to/your/test.jmx <strong>-l /path/to/results/file.jtl</strong></p>
</blockquote>

<p>Please refer to Dmitri answer in following question to reduce JTL size.</p>

<p><a href=""https://stackoverflow.com/questions/34986314/how-can-we-control-size-of-jtl-file-while-running-test-from-non-gui-mode"">How can we control size of JTL file while running test from Non GUI Mode</a></p>
"
Datadog,31340312,30227260,0,"2015/07/10, 15:03:45",False,"2015/07/10, 15:03:45",491,4923204,0,"<p>Before implementing the code, you need to look around in Widows ""registry"" using ""regedit"" and find the exact registry key value for the software.</p>

<p>Below example shows, how to fetch the version number of ""internet explorer"".</p>

<p>Also recommended to have basic knowledge on Ruby array and hash, to understand the code</p>

<p>I've used registry_key_XXXXX Chef methods.</p>

<pre><code>if registry_key_exists?('HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Internet Explorer')
  subkey_array = registry_get_values('HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Internet Explorer')
  Chef::Log.info(""#{subkey_array}"")
  reg_key_hash = subkey_array.at(-3)
  ver = reg_key_hash.values_at(:data)
  ie_version = ver.to_s[2, 2]
  Chef::Log.warn(""IE version #{ie_version}"")
else
  Chef::Log.warn(""IE Registry key not found"")
  return
end
</code></pre>

<p>Note: Registry key entry may differ for Windows 32bit and 64bit</p>
"
Datadog,41618026,41605761,0,"2017/01/12, 18:15:50",False,"2017/01/12, 18:15:50",261,4172512,1,"<p>You can leverage Datadog's Agent to collect metrics via a JMX connection.  There is documentation found here:</p>

<p><a href=""http://docs.datadoghq.com/integrations/java/"" rel=""nofollow noreferrer"">http://docs.datadoghq.com/integrations/java/</a></p>

<p><a href=""https://www.datadoghq.com/blog/monitoring-jmx-metrics-with-datadog/"" rel=""nofollow noreferrer"">https://www.datadoghq.com/blog/monitoring-jmx-metrics-with-datadog/</a></p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/204501525-Custom-JMX-Integration-s-"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/204501525-Custom-JMX-Integration-s-</a></p>

<p><a href=""https://help.datadoghq.com/hc/en-us/articles/207525586-View-jmx-data-in-jConsole-and-set-up-your-jmx-yaml-to-collect-them"" rel=""nofollow noreferrer"">https://help.datadoghq.com/hc/en-us/articles/207525586-View-jmx-data-in-jConsole-and-set-up-your-jmx-yaml-to-collect-them</a></p>

<p>That should help you get setup and collecting the necessary metrics exposed by your JMX port.</p>

<p>That said, if you encounter any issues reach out to support@datadoghq.com and they can assist you.</p>
"
Datadog,65043190,65042688,8,"2020/11/27, 21:56:03",True,"2020/11/28, 01:23:09",14946,190823,1,"<p>The problem is your <code>sendData()</code> function. This function is called in your for loop and has the following line:</p>
<pre><code>c, err := statsd.New(&quot;127.0.0.1:18125&quot;)
</code></pre>
<p>This line will create a new DataDog client, which uses a Unix socket. This explains your error message.</p>
<p>With every iteration of your loop, a new socket is &quot;allocated&quot;. After a sufficient amount of loops no sockets can be opened, resulting in:</p>
<blockquote>
<p>socket: too many open files</p>
</blockquote>
<p>To fix this you should create the client only once and pass it to your method as parameter.</p>
<pre><code>func sendData(client *statsd.Client, name []string, channel chan []string) {
    // do something with client...
}

func main() {
    client, err := statsd.New(&quot;127.0.0.1:18125&quot;)
    if err != nil {
        log.Fatal(err)
    }

    // do something else ...

    for res := range channel {
        go func(client *statsd.Client, appName []string) {
            time.Sleep(5 * time.Second)
            go sendData(client, appName, channel)
        }(client, res)
    }
}
</code></pre>
"
Datadog,49457754,49457370,0,"2018/03/23, 22:21:44",True,"2018/03/24, 05:00:40",2191,6084559,3,"<p>The variable <code>$LASTEXITCODE</code> will give you the exit code of the last native command (executable) that was run.</p>
"
Datadog,44265020,44258173,0,"2017/05/30, 17:28:04",False,"2017/05/30, 17:28:04",11,8014459,0,"<p>I'm not familiar with the program, but I've had to solve a problem like this before.  I'm making the assumption that you're always going to start the output you want with a line 'Dogstatsd', and always end with several equals signs.  Based on that, you could script out your output like this:</p>

<pre><code>#!/bin/bash

service datadog-agent info -v &gt; /tmp/DogstatsDump.txt

LinesTotal=$(cat /tmp/DogstatsDump.txt | wc -l)

StartOfOutput=$(grep /tmp/DogstatsDump.txt -ne '^Dogstatsd (v' | cut -d':' -f 1)

LengthOfOutput=$(tail -n $(echo ""$LinesTotal - $StartOfOutput + 1"" | bc)/tmp/DogstatsDump.txt  | grep -ne '^==========' | head -n 1 | cut -d':' -f 1 )

tail -n $(echo ""$LinesTotal - $StartOfOutput + 1"" | bc) /tmp/DogstatsDump.txt  | head -n $(echo ""$LengthOfOutput + 1"" | bc)

rm /tmp/DogstatsDump.txt
</code></pre>

<p>We get the values defining the length of the file, the length until we hit the first line you want, the length where the output ends, and trim accordingly.</p>
"
Datadog,42575657,42556276,4,"2017/03/03, 11:53:34",True,"2017/03/03, 11:53:34",541,1153563,0,"<p>I would strongly suggest to setup a pre-production environment and run load tests (with tools like <a href=""http://jmeter.apache.org/index.html"" rel=""nofollow noreferrer"">JMeter</a>) in conjunction with server-side monitoring.</p>

<p>Tomcat backends can be monitored using the JMX protocol.</p>

<p>You have 2 solutions :</p>

<ul>
<li><strong>Free</strong>: JMeter with <a href=""https://jmeter-plugins.org/wiki/PerfMonAgent/"" rel=""nofollow noreferrer"">Perfmon Agent</a> to monitor CPU, Memory and custom defined JMX Beans,</li>
<li><strong>Freemium</strong> (aka Paid for > 50 concurrent users) : OctoPerf supports <a href=""https://doc.octoperf.com/monitoring/apache-tomcat/"" rel=""nofollow noreferrer"">Apache Tomcat monitoring</a>, and can monitor your aws servers with an on-premise monitoring agent. </li>
</ul>

<p>Like always, free software costs nothing but your time, and paid software gets you straight to the issue in exchanges for some pennies.</p>
"
Elastic APM,55021893,53428661,0,"2019/03/06, 13:22:41",False,"2019/03/06, 13:22:41",89,989723,2,"<p>As it is reported on the official site it could be released in the future.</p>

<blockquote>
  <p>Some of the features we are very excited to provide in the near future
  include distributed tracing and providing framework-specific
  information (e.g., route change times) for some of the frontend
  frameworks such as React, Angular, Vue.js, etc.</p>
</blockquote>

<p><a href=""https://www.elastic.co/blog/elastic-apm-rum-js-agent-is-generally-available"" rel=""nofollow noreferrer"">https://www.elastic.co/blog/elastic-apm-rum-js-agent-is-generally-available</a></p>

<p>For now you can rely on Elastic APM RUM JS Agent using JS tags: </p>

<p><a href=""https://www.elastic.co/guide/en/apm/agent/js-base/3.x/getting-started.html#using-script-tags"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/js-base/3.x/getting-started.html#using-script-tags</a></p>
"
Elastic APM,58086472,54623543,0,"2019/09/24, 21:46:42",False,"2019/09/24, 21:46:42",3095,1532769,3,"<p>You can try to increase:</p>

<ul>
<li>Internal queue size (<strong>queue.mem.events</strong>)</li>
<li>Elasticsearch bulk size (<strong>output.elasticsearch.bulk_max_size</strong>).</li>
</ul>

<p>Please take a look on documentation: <a href=""https://www.elastic.co/guide/en/apm/server/current/tune-apm-server.html"" rel=""nofollow noreferrer"">Tune APM Server</a></p>
"
Elastic APM,60540275,58780229,0,"2020/03/05, 09:37:36",False,"2020/03/05, 09:45:37",1,13010745,0,"<p>You can attach your ElasticApmAttacher.attach() in the Spring Application main class</p>

<p>For a SpringBootApplication packaged as a war file, and deployed to Tomcat server, this can be added to the configure method</p>

<p>Below code might help:</p>

<pre><code>package com.test.main

import co.elastic.apm.attach.ElasticApmAttacher
import org.springframework.boot.Banner
import org.springframework.boot.SpringApplication
import org.springframework.boot.autoconfigure.SpringBootApplication
import org.springframework.boot.builder.SpringApplicationBuilder
import org.springframework.boot.web.servlet.support.SpringBootServletInitializer


@SpringBootApplication(scanBasePackages = [ ""com.test"" ])
class Application extends SpringBootServletInitializer{

    static void main(String[] args) {
        SpringApplication app = new SpringApplication(Application.class)
        app.setBannerMode(Banner.Mode.OFF)
        ElasticApmAttacher.attach();
        app.run(args)
    }

    @Override
    protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {
        ElasticApmAttacher.attach();
        return application.sources(Application.class)
    }
}
</code></pre>
"
Elastic APM,61642930,58780229,0,"2020/05/06, 21:46:03",False,"2020/05/06, 21:46:03",81,1351859,0,"<pre><code>    Map&lt;String, String&gt; apmConfiguration = new HashMap&lt;&gt;();
    apmConfiguration.put(""server_urls"", ""http://localhost:8200"");
    apmConfiguration.put(""service_name"", ""SpringBootApp"");
    ElasticApmAttacher.attach(apmConfiguration);
</code></pre>
"
Elastic APM,61101616,61098374,0,"2020/04/08, 16:15:18",True,"2020/04/08, 16:15:18",36,13258898,2,"<p>unfortunately oracle is not supported by elastic apm agent. you should wrap your <code>oracleQueryRunner</code> in order to start and end agent spans manually. put this code in your <code>main.ts</code> file:</p>

<pre><code>import { OracleQueryRunner } from 'typeorm/driver/oracle/OracleQueryRunner';

const query = OracleQueryRunner.prototype.query;

OracleQueryRunner.prototype.query = async function (...args) {
  const span = apm.startSpan('query');
  if (span) {
    span.type = 'db';
    span.action = args[0];
  }
  const result = await query.bind(this)(...args);
  if (span) { span.end(); }
  return result;
};
</code></pre>
"
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86,6020497,5,"<p>It's true, there isn't an Elixir agent for Elastic APM - you can upvote <a href=""https://github.com/elastic/apm/issues/60"" rel=""noreferrer"">this issue</a> to get the topic more attention.</p>
<p>As you discovered, you can use the OpenTelemetry in the meantime.  To do that, run the OpenTelemetry contrib collector (otel collector) and configure it to export to <code>elastic</code> - there is a full explanation <a href=""https://www.elastic.co/guide/en/apm/get-started/current/open-telemetry-elastic-get-started.html#open-telemetry-elastic-configure"" rel=""noreferrer"">in the docs</a> along with this sample configuration:</p>
<pre class=""lang-yaml prettyprint-override""><code>receivers:
  otlp:
    endpoint: localhost:55680
processors:
  batch:
    timeout: 1s
    send_batch_size: 1024
exporters:
  elastic:
    apm_server_url: &quot;https://elasticapm.example.com&quot;
    secret_token: &quot;ESS_TOKEN&quot;
service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [elastic]
</code></pre>
<p>In your application, configure the tracer use the <a href=""https://github.com/opentelemetry-beam/opentelemetry_exporter#using"" rel=""noreferrer"">opentelemetry exporter</a>.</p>
<p>At that point you'll have a tracer in your application sending traces to the otel collector.  From there, traces will be exported to the Elastic Stack via APM Server.  In summary: <code>your app -&gt; otel collector -&gt; apm-server -&gt; elasticsearch</code></p>
<p>The <a href=""https://github.com/open-telemetry/opentelemetry-erlang-api#registering-and-using-tracers-directly"" rel=""noreferrer"">Erlang/Elixir Agent Docs</a> have sample code for starting and decorating spans.</p>
"
Elastic APM,63573627,63573207,4,"2020/08/25, 10:04:20",True,"2020/08/25, 10:04:20",28262,45691,1,"<p><code>transaction.duration.us</code> should indeed be what you're looking for. It's the duration in microseconds as an integer. Divide it by 1000 to get milliseconds, or by 1'000'000 to get seconds.</p>
<p><a href=""https://www.elastic.co/guide/en/apm/server/7.9/exported-fields-apm-transaction.html#_duration_2"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/server/7.9/exported-fields-apm-transaction.html#_duration_2</a></p>
"
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900,2259934,0,"<p>When you bring up containers using compose, each container has its own networking stack (so they can each talk to themselves on <code>localhost</code>, but they need an ip address or dns name to talk to a different container!).</p>

<p>Compose by default connects each of the containers to a default network and gives each a dns name with the name of the service.</p>

<p>If your compose file looks like</p>

<pre><code>services:
  apm:
    image: apm_image
  elasticsearch:
     image: elasticsearch:latest
</code></pre>

<p>A process in the <code>apm</code> container could access elasticsearch at <code>http://elasticsearch:9200</code></p>
"
Elastic APM,51649953,50282965,0,"2018/08/02, 12:19:26",False,"2018/08/02, 12:19:26",41,8848390,4,"<p>If you are starting all the services with single docker compose file, the app-server.yaml should have the value like this
<code>
output:
  elasticsearch:
    hosts: elasticsearch:9200
</code>
The ""hosts: elasticsearch:9200"" should be service name of the elasticsearch you mentioned in the docker-compose. Like in the followiing </p>

<p><code>
    version: '2'
    services:
      elasticsearch:
          image: elasticsearch:latest
</code>   </p>
"
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528,9619535,1,"<p>Answer to the Errors - I have custom resole.extensions in the <code>webpack.config.js</code>:</p>

<pre><code>resolve: {
  extensions: ['.ts', '.tsx', '.js', '.jsx']
},
</code></pre>

<p>That was missing the default <code>.json</code>:</p>

<pre><code>resolve: {
  extensions: ['.ts', '.tsx', '.js', '.jsx', '.json']
},
</code></pre>

<p>Now only the warnings are left:</p>

<pre><code>WARNING in node_modules/elastic-apm-node/lib/agent.js 152:16-59
Critical dependency: the request of a dependency is an expression
 @ node_modules/elastic-apm-node/index.js

WARNING in node_modules/elastic-apm-node/lib/config.js 25:19-36
Critical dependency: the request of a dependency is an expression
 @ node_modules/elastic-apm-node/lib/agent.js

WARNING in node_modules/elastic-apm-node/lib/instrumentation/index.js 169:16-30
Critical dependency: the request of a dependency is an expression
 @ node_modules/elastic-apm-node/lib/agent.js
</code></pre>

<p>I addressed them to the developer: <a href=""https://github.com/elastic/apm-agent-nodejs/issues/1154"" rel=""nofollow noreferrer"">https://github.com/elastic/apm-agent-nodejs/issues/1154</a></p>
"
Elastic APM,56997868,56989849,1,"2019/07/12, 01:05:09",False,"2019/07/12, 01:05:09",9394,573153,0,"<ol>
<li>Is a firewall blocking the traffic (like Window's built-in one)?</li>
<li><code>10.9.21.91</code> is the IP that hosts are trying to reach? Just to be sure you could bind to <code>host: ""0.0.0.0:8200""</code>, which would cover all possible interfaces.</li>
</ol>
"
Elastic APM,58717230,56989849,0,"2019/11/05, 20:16:57",False,"2019/11/05, 20:16:57",552,1518708,4,"<p>To listen on <code>0.0.0.0</code> try:</p>

<pre><code>  host: "":8200"" 
</code></pre>
"
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445,3834445,3,"<p>Try using a configuration:</p>

<pre><code>version: ""3""

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.8.1
    networks:
      - elastic-jaeger
    ports:
      - ""127.0.0.1:9200:9200""
      - ""127.0.0.1:9300:9300""
    restart: on-failure
    environment:
      - cluster.name=jaeger-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - transport.host=127.0.0.1
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data

  apm-server:
    image: docker.elastic.co/apm/apm-server:6.8.1
    ports:
      - 8200:8200

    volumes:
       - ./apm-server/config/apm-server.yml:/usr/share/apm-server/apm-server.yml

    networks:
      - elastic-jaeger

  kibana:
    image: docker.elastic.co/kibana/kibana:6.8.1
    ports:
      - ""127.0.0.1:5601:5601""
    restart: on-failure
    networks:
      - elastic-jaeger

  jaeger-collector:
    image: jaegertracing/jaeger-collector
    ports:
      - ""14269:14269""
      - ""14268:14268""
      - ""14267:14267""
      - ""9411:9411""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--es.num-shards=1"",
      ""--es.num-replicas=0"",
      ""--log-level=error""
    ]
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    hostname: jaeger-agent
    command: [""--collector.host-port=jaeger-collector:14267""]
    ports:
      - ""5775:5775/udp""
      - ""6831:6831/udp""
      - ""6832:6832/udp""
      - ""5778:5778""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - no_proxy=localhost
    ports:
      - ""16686:16686""
      - ""16687:16687""
    networks:
      - elastic-jaeger
    restart: on-failure
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--span-storage.type=elasticsearch"",
      ""--log-level=debug""
    ]
    depends_on:
      - jaeger-agent

  grafana:
    image: grafana/grafana
    ports:
      - 3999:3000
    volumes:
      - ./grafana-data:/var/lib/grafana
    networks:
      - elastic-jaeger

volumes:
  esdata:
    driver: local

networks:
  elastic-jaeger:
    driver: bridge 

</code></pre>

<p>where the file apm-server/config/apm-server.yml has your config content:</p>

<pre><code>apm-server.rum.enabled: true
apm-server.rum.event_rate.limit: 300
apm-server.rum.event_rate.lru_size: 1000
apm-server.rum.allow_origins: ['*']
apm-server.rum.library_pattern: ""node_modules|bower_components|~""
apm-server.rum.exclude_from_grouping: ""^/webpack""
apm-server.rum.source_mapping.cache.expiration: 5m
apm-server.rum.source_mapping.index_pattern: ""apm-*-sourcemap*""
output.elasticsearch.hosts: [""http://elasticsearch:9200""]
apm-server.host: ""0.0.0.0:8200""
setup.kibana.host: ""kibana:5601""
setup.template.enabled: true
logging.to_files: false

</code></pre>

<p>Note the rum.allow_origins option that you can configure to resolve the CORS issue. <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html</a></p>
"
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36,2155647,1,"<p>you have to modify your elastic APM python agent code. 
In general, you can add labels to your span example</p>

<pre><code>elasticapm.label(key1=value1, key2=value2)
</code></pre>

<p>also, you can add directly to span object. </p>

<p>You will get the ip from request object flask</p>

<p><code>request.remote_addr</code>  set this to the desired key.</p>

<p>more details of APIs on elastic APM python agent can be found here - <a href=""https://www.elastic.co/guide/en/apm/agent/python/current/api.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/python/current/api.html</a></p>

<p>Thanks</p>
"
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576,547452,2,"<p>These metrics come directly from <a href=""https://docs.oracle.com/javase/7/docs/api/java/lang/management/GarbageCollectorMXBean.html"" rel=""nofollow noreferrer"">java.lang.management.GarbageCollectorMXBean</a>. The value of the <code>jvm.gc.time</code> metric is taken from <code>GarbageCollectorMXBean.getCollectionTime</code>, which is indeed accumulating since the process started.</p>

<p>Assuming you're looking at metrics from a single JVM, there are a couple of possible reasons for why the value would appear to have gone backwards:</p>

<ol>
<li>The process restarted.</li>
<li>The values are for two different GC ""memory managers"" (e.g. G1 Young Generation, G1 Old Generation)</li>
</ol>

<p>If the process had restarted (which I expect you would know about anyway), the metrics documents in Elasticsearch would have different values for the field <code>agent.ephemeral_id</code>.</p>

<p>The more likely answer is that you're seeing values for two different memory managers/GC generations, in which case the metrics documents in Elasticsearch would have different values for the field <code>labels.name</code>.</p>

<p><a href=""https://i.stack.imgur.com/NsKAI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NsKAI.png"" alt=""enter image description here""></a></p>
"
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264,1783306,2,"<p>You don't end <code>trans1</code> and <code>trans2</code>.</p>

<p>Just put these 2 lines to the point where these end, and everything should show up fine: </p>

<pre><code>trans1.End();
trans2.End();
</code></pre>

<p>There is the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#convenient-capture-transaction"" rel=""nofollow noreferrer""><code>CaptureTransaction</code></a>, which is a convenient method that can wrap you any code and makes sure the transaction is ended and all exceptions are captured - so you use that method and it does ""everything"" for you.</p>

<p>Then there is the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#api-start-transaction"" rel=""nofollow noreferrer""><code>StartTransaction</code></a> method - this is the one you use in your code -, which starts the transaction and does not do anything else. The advantage here is that you get an <code>ITransaction</code> instance which you can use wherever and whenever you want. But in this case you need to call <code>.End()</code> on it manually once the transaction (aka the code you want to capture) is executed.</p>

<p>Same with <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#convenient-capture-span"" rel=""nofollow noreferrer""><code>CaptureSpan</code></a> and <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#api-transaction-create-span"" rel=""nofollow noreferrer""><code>StartSpan</code></a>.</p>

<p>So you used <code>CaptureSpan</code> for your spans, so those where ended automatically when the lambda with <code>Task.Delay</code> finished, on the other hand you started your transactions with <code>StartTransaction</code> but only called <code>.End()</code> on <code>trans3</code> and not on the 2 other transactions.</p>

<p>There is some explanation with a demo <a href=""https://www.youtube.com/watch?v=4JGqu59DN14&amp;feature=youtu.be&amp;t=3413"" rel=""nofollow noreferrer"">here</a> - sample code of that demo is <a href=""https://github.com/gregkalapos/MicrosoftDevUserGroupGraz_StateOfObservability_Talk/blob/master/Samples/06_DtOverCmd/Program.cs"" rel=""nofollow noreferrer"">here</a>.</p>
"
Elastic APM,61066068,61065570,6,"2020/04/06, 21:03:11",True,"2020/04/06, 21:03:11",3264,1783306,1,"<p>Currently background services are not captured out of the box. </p>

<p>What you can do is to use the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html"" rel=""nofollow noreferrer"">Public Agent API</a> and with a little bit of an additional code you can capture those also as transactions. </p>

<p>Something like this in the background service:</p>

<pre><code>var transaction = Elastic.Apm.Agent
        .Tracer.StartTransaction(""MyTransaction"", ApiConstants.TypeRequest);
try
{
    //background service code that is captured as a transaction
}
catch (Exception e)
{
    transaction?.CaptureException(e);
    throw;
}
finally
{
    transaction?.End();
}
</code></pre>
"
Elastic APM,61196239,61115721,0,"2020/04/13, 23:31:21",False,"2020/04/13, 23:31:21",64,12032134,1,"<p>I run via docker-compose elasticsearch, apm, kibana and tomcat application in docker.
In apm-<em>-transaction-</em> index exist this meta information: <code>container.id</code>.
And in apm-<em>-metrics-</em> index this information is also stored.
Try to look at json structure at Discover tab by index pattern ""apm-*"" </p>

<pre><code>    {
  ""_index"": ""apm-7.6.2-transaction-000001"",
  ""_type"": ""_doc"",
  ""_id"": ""EyeudHEBxv4GJJ2Qs6yk"",
  ""_version"": 1,
  ""_score"": null,
  ""_source"": {
    ""container"": {
      ""id"": ""100b5f0e673337f4381533d8ae9ab47ababf271e422c26f9ecc278a2aa08e3e7""
    },
    ""observer"": {
      ""hostname"": ""6940719de3e8"",
      ""id"": ""7e715843-6a92-4794-99e3-beadc81cc7bc"",
      ""ephemeral_id"": ""a0676d3e-e97f-4179-8b92-88f6214ea035"",
      ""type"": ""apm-server"",
      ""version"": ""7.6.2"",
      ""version_major"": 7
    },
...
</code></pre>

<p><a href=""https://i.stack.imgur.com/FtytQ.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
"
Elastic APM,61509232,61262895,0,"2020/04/29, 21:38:55",False,"2020/04/29, 21:38:55",64,12032134,0,"<p>did you try to give permissions to folder /opt/elastic ?</p>

<pre><code>chmod -R uog+x
</code></pre>
"
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911,4668,1,"<p>It's hard to say without knowing exactly how you're using NestJS and GraphQL together, and without knowing which parts of Apollo Server that NestJS itself uses.  Seeing a small sample of what <em>I am using the graphql feature of nestjs</em> means would be useful.</p>
<p>Here's a few datapoints that might help you narrow things down.  Also, opening <a href=""https://github.com/elastic/apm-agent-nodejs/issues"" rel=""nofollow noreferrer"">an issue</a> in the Agent repository or <a href=""https://discuss.elastic.co/c/observability/apm/58"" rel=""nofollow noreferrer"">a question in their forums</a> might get more of the right eyes on this.</p>
<p>The Elastic APM instrumentation for Apollo Server works by wrapping the <code>runHttpQuery</code> function of the <code>apollo-server-core</code> module, and <a href=""https://github.com/elastic/apm-agent-nodejs/blob/4fe301d66a1ef748a77a0cde7584931e176c618b/lib/instrumentation/modules/apollo-server-core.js#L18"" rel=""nofollow noreferrer"">marking the transaction with <code>trans._graphqlRoute</code></a>.</p>
<p>When the agent <a href=""https://github.com/elastic/apm-agent-nodejs/blob/4fe301d66a1ef748a77a0cde7584931e176c618b/lib/instrumentation/modules/graphql.js#L105"" rel=""nofollow noreferrer"">sees this <code>_graphqlRoute</code> property</a>, it runs some code that will set a default name for the transaction</p>
<pre class=""lang-js prettyprint-override""><code>      if (trans._graphqlRoute) {
        var name = queries.length &gt; 0 ? queries.join(', ') : 'Unknown GraphQL query'
        if (trans.req) var path = getPathFromRequest(trans.req, true)
        var defaultName = name
        defaultName = path ? defaultName + ' (' + path + ')' : defaultName
        defaultName = operationName ? operationName + ' ' + defaultName : defaultName
        trans.setDefaultName(defaultName)
        trans.type = 'graphql'
      }
</code></pre>
<p>In your application, either the <code>_graphqlRoute</code> property isn't getting set, or the renaming code above is doing something weird, or something about NestJS comes along and renames the transaction after it's been named with the above code.</p>
<p>Knowing more specifically <em>what</em> you're doing would help folks narrow in on your problems.</p>
"
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633,13659075,1,"<p>The metrics shown in Kibana are sent by the APM agent that like you said has limited access to your environment. It basically says anything that is collected by the JVM running your JAR.</p>
<p>If you want to get further visibility into the CPU details of your local environment then you must augment your setup using <a href=""https://www.elastic.co/beats/metricbeat"" rel=""nofollow noreferrer"">Elastic MetricBeats</a> that ships O.S level details about your machine that sees beyond what the JVM can see.</p>
<p>In the presentation below I show how to configure logs, metrics, and APM altogether.</p>
<p><a href=""https://www.youtube.com/watch?v=aXbg9pZCjpk"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=aXbg9pZCjpk</a></p>
"
Elastic APM,54385310,52708201,1,"2019/01/27, 07:21:01",False,"2019/01/27, 07:21:01",1,10973722,-1,"<p>currently,we run apm-agent for java application, but after 1 day, server cpu has over 100% and java application killed by system</p>
"
Elastic APM,55333285,55319800,0,"2019/03/25, 09:50:57",False,"2019/03/25, 09:50:57",3915,837717,0,"<p>I am not familiar with what is Elastic APM, but if it says it supports Spring Boot, then it means it supports any spring-boot-based framework which Spring Cloud Stream is.</p>
"
Elastic APM,57318627,57297752,1,"2019/08/02, 03:41:47",False,"2019/08/02, 03:41:47",9394,573153,0,"<p>The <a href=""https://www.elastic.co/guide/en/apm/agent/java/current/metrics.html"" rel=""nofollow noreferrer"">JVM GC metrics tracked</a> right now are <code>jvm.gc.alloc</code>, <code>jvm.gc.time</code>, and <code>jvm.gc.count</code>.</p>

<p>If you are looking for additional ones, which ones would those be? And could you <a href=""https://github.com/elastic/apm-agent-java/issues"" rel=""nofollow noreferrer"">open an issue with the details</a>.</p>
"
Elastic APM,61072797,57297752,1,"2020/04/07, 07:20:17",False,"2020/04/07, 07:20:17",23,6332568,0,"<p>Please import from saved objects option - <a href=""https://github.com/elastic/apm-contrib/blob/master/apm-agent-java/dashboards/java_metrics_dashboard_7.x.json"" rel=""nofollow noreferrer"">https://github.com/elastic/apm-contrib/blob/master/apm-agent-java/dashboards/java_metrics_dashboard_7.x.json</a></p>
"
Elastic APM,57298815,57298727,3,"2019/08/01, 00:04:16",False,"2019/08/01, 00:04:16",151,639333,0,"<p>To include spans into the transactions you should start the spans from the transaction object</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>...
var span = transaction.startSpan('My custom span')
...</code></pre>
</div>
</div>
</p>

<p>And ending the parent transaction object all the nested spans will be also ended in cascade</p>

<p><a href=""https://www.elastic.co/guide/en/apm/agent/js-base/4.x/transaction-api.html#transaction-start-span"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/js-base/4.x/transaction-api.html#transaction-start-span</a></p>
"
Elastic APM,59268282,57782547,8,"2019/12/10, 15:25:10",False,"2019/12/10, 15:25:10",228,4349519,1,"<p>You can start your application with argument <code>active=false</code>.</p>

<p><code>C:\Users\dsm\Documents&gt;java -jar apm-agent-attach-1.9.0-standalone.jar --pid 16832 --args 'service_name=test;server_urls=http://localhost:8200;active=false'</code></p>
"
Elastic APM,58075056,58074198,0,"2019/09/24, 10:22:09",True,"2019/09/24, 10:22:09",163369,4604579,5,"<p>You simply need to change your query to this:</p>

<pre><code>POST apm*/_delete_by_query
{
  ""query"": {
     ""term"": {
       ""service.name"": ""ldap1""
     }
  }
}
</code></pre>
"
Elastic APM,63339911,58074198,0,"2020/08/10, 15:09:15",False,"2020/08/10, 15:09:15",383,10119759,3,"<p>The accepted answer no longer works, you can use the following</p>
<pre><code>POST /apm-*/_delete_by_query
{
  &quot;query&quot;: {
&quot;bool&quot;: {
  &quot;must&quot;: [
    {
      &quot;term&quot;: {
        &quot;service.name&quot;: {
          &quot;value&quot;: &quot;my-application&quot;
        }
      }
    }
  ]
}
  }
}
</code></pre>
"
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394,573153,1,"<p>The key features of APM agents is normally in their framework integrations. The Java APM agent is mostly focussed on web frameworks — see the list of <a href=""https://www.elastic.co/guide/en/apm/agent/java/1.x/supported-technologies-details.html"" rel=""nofollow noreferrer"">supported technologies</a>.</p>

<p>But you already mentioned the <a href=""https://www.elastic.co/guide/en/apm/agent/java/1.x/public-api.html"" rel=""nofollow noreferrer"">public API</a> — if you manually instrument your code with that, you will still be able to use it. It just doesn't automatically understand the framework and you need to help it with that.</p>

<p>Alternatively, if your tool supports OpenTracing then you could use the <a href=""https://www.elastic.co/guide/en/apm/agent/java/1.x/opentracing-bridge.html"" rel=""nofollow noreferrer"">OpenTracing bridge</a> for that.</p>
"
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26,9528014,1,"<p>When routing to a different subpage you have to set the Route Name manually. You can achieve this via a filter on the 'change-route' type. See <code>apm.addFilter()</code> 
docs: <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#apm-add-filter"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#apm-add-filter</a></p>

<p>Something like this should work:</p>

<pre><code>apm.addFilter(payload =&gt; {
  if (payload.data) {
    const mappedData = [];
    payload.data.forEach(tr =&gt; {
      if (tr.type === 'route-change')
        mappedData.push({
          ...tr,
          name: this.$route.name // overwrite unknown with the current route name
        });
      else mappedData.push(tr);
    });
    payload.data = mappedData;
  }
  return payload;
});
</code></pre>
"
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55,3588136,1,"<p>Another way to do it would be to observe for transaction events which is fired whenever transaction starts and ends. </p>

<pre><code>// on transaction start
apm.observe(""transaction:start"", transaction =&gt; {

});

// on transaction end
apm.observe(""transaction:end"", transaction =&gt; {

});
</code></pre>

<p>API docs - <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#observe"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#observe</a></p>

<p>The agent also supports frameworks such as Vue.js, React and Angular out of the box, so the above code should not be necessary. </p>

<p>Vue docs - <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html</a></p>
"
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394,573153,0,"<p>Logging is separate from APM / tracing, but can be integrated.</p>

<p><a href=""https://github.com/elastic/ecs-logging-java"" rel=""nofollow noreferrer"">https://github.com/elastic/ecs-logging-java</a> is a curated logging library that will also correlate the trace IDs, so you can tie both together.</p>

<p>Keep using SLF4J and just add the right logging backend. The output can then be picked up by Filebeat (as described in the repository) and you're ready to go.</p>
"
Elastic APM,61196812,58738337,0,"2020/04/14, 00:09:21",False,"2020/04/14, 00:09:21",64,12032134,1,"<p>Elastic-apm-agent-java automatically capture exceptions when you use slf4j implementation <code>Logger#error(""message"", Throwable)</code>.</p>

<p>More information you can find <a href=""https://www.elastic.co/guide/en/apm/agent/java/master/supported-technologies-details.html#supported-logging-frameworks"" rel=""nofollow noreferrer"">here</a></p>
"
Elastic APM,61801017,59328108,0,"2020/05/14, 18:18:04",False,"2020/05/14, 18:18:04",396,4191904,0,"<p>Try <a href=""https://www.elastic.co/guide/en/apm/get-started/current/quick-start-overview.html"" rel=""nofollow noreferrer"">this</a> official docker-compose set up:</p>

<pre><code>version: '2.2'
services:
  apm-server:
    image: docker.elastic.co/apm/apm-server:7.7.0
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    cap_add: [""CHOWN"", ""DAC_OVERRIDE"", ""SETGID"", ""SETUID""]
    cap_drop: [""ALL""]
    ports:
    - 8200:8200
    networks:
    - elastic
    command: &gt;
       apm-server -e
         -E apm-server.rum.enabled=true
         -E setup.kibana.host=kibana:5601
         -E setup.template.settings.index.number_of_replicas=0
         -E apm-server.kibana.enabled=true
         -E apm-server.kibana.host=kibana:5601
         -E output.elasticsearch.hosts=[""elasticsearch:9200""]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.7.0
    environment:
    - bootstrap.memory_lock=true
    - cluster.name=docker-cluster
    - cluster.routing.allocation.disk.threshold_enabled=false
    - discovery.type=single-node
    - ES_JAVA_OPTS=-XX:UseAVX=2 -Xms1g -Xmx1g
    ulimits:
      memlock:
        hard: -1
        soft: -1
    volumes:
    - esdata:/usr/share/elasticsearch/data
    ports:
    - 9200:9200
    networks:
    - elastic
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '""status"":""red""'

  kibana:
    image: docker.elastic.co/kibana/kibana:7.7.0
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
    - 5601:5601
    networks:
    - elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

volumes:
  esdata:
    driver: local

networks:
  elastic:
    driver: bridge
</code></pre>
"
Elastic APM,60702327,59352981,0,"2020/03/16, 10:08:57",True,"2020/03/16, 10:08:57",1083,3074424,0,"<p>The Elastic RUM agent has support for <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/current/supported-technologies.html#user-interactions"" rel=""nofollow noreferrer"">click user interactions</a>, therefore you shouldn't need to manually start these type of transactions.</p>

<p>Regarding the failure in your code the correct API call is <code>getCurrentTransaction</code> and not <code>currentTransaction</code>.</p>

<p>Hope this helps.</p>
"
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576,547452,0,"<blockquote>
  <p>So, I'm guessing that the handler wrappers are dropping the buffalo.Context information.</p>
</blockquote>

<p>That's correct. The problem is that <code>buffalo.WrapHandler</code> (<a href=""https://github.com/gobuffalo/buffalo/blob/bbdf70341bf44365b8fb0c83689ebe030ce76e5b/wrappers.go#L7-L12"" rel=""nofollow noreferrer"">Source</a>) throws away all of the context other than the underlying <code>http.Request</code>/<code>http.Response</code>:</p>

<pre><code>// WrapHandler wraps a standard http.Handler and transforms it
// into a buffalo.Handler.
func WrapHandler(h http.Handler) Handler {
    return func(c Context) error {
        h.ServeHTTP(c.Response(), c.Request())
        return nil
    }
}
</code></pre>

<blockquote>
  <p>So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?</p>
</blockquote>

<p>I can see two options:</p>

<ul>
<li>Reimplement <code>buffalo.WrapHandler</code>/<code>buffalo.WrapBuffaloHandler</code> to stop throwing away the buffalo.Context. This would involve storing the <code>buffalo.Context</code> in the underlying <code>http.Request</code>'s context, and then pulling it out again on the other side instead of creating a whole new context.</li>
<li>Implement Buffalo-specific middleware for Sentry and Elastic APM without using the <code>Wrap*</code> functions.</li>
</ul>

<p>There's an open issue in the Elastic APM agent for the latter option: <a href=""https://github.com/elastic/apm-agent-go/issues/39"" rel=""nofollow noreferrer"">elastic/apm#39</a>.</p>
"
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264,1783306,1,"<p>There are basically 2 ways the agent captures things:</p>

<ul>
<li>Auto-instrumentation: in this case you don't write any code, the agent just captures things for you - this is what we see on your screenshot</li>
<li>Manual code instrumentation - for this you can use the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html"" rel=""nofollow noreferrer"">Public Agent API</a> and capture things programatically.</li>
</ul>

<p>In a typical ASP.NET Classic MVC application the agent has auto instrumentation for outgoing HTTP calls with <code>HttpClient</code>, Database calls with EF6 (Make sure to <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/setup.html#setup-ef6"" rel=""nofollow noreferrer"">add the interceptor</a>) (<code>SqlClient</code> support is already work-in-progress, hopefully released soon). So unless you have any of these within those requests, the agent won't capture things out of the box.</p>

<p>If you want to capture more things, currently the way to go is to place some agent specific code - so basically manual code instrumentation - into your application and use the public agent API.</p>
"
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33,9358877,0,"<p>Finally noticed issue. </p>

<p>application API is returning service name as Bootstrap because.</p>

<pre><code>in the Response Headers we are getting bootstrapp {
  ""Content-Type"": ""application/json;charset=UTF-8"",
  ""Date"": ""Fri, 10 Apr 2020 20:01:37 GMT"",
  ""Server"": ""Jetty(9.2.13.v20150730)"",
  ""Transfer-Encoding"": ""chunked"",
  ""X-Application-Context"": ""bootstrap:31144""
}

</code></pre>

<pre><code>public static void main(String[] args) throws Exception {
        SpringApplication.run(Application.class, args);
    }
</code></pre>

<p>Id is not set so it is using default value</p>

<p>The Id can be set like this</p>

<pre><code>public static void main(String[] args) throws Exception {
        SpringApplication.run(Application.class, args).setId(""the text that you want to see int the header"");
    }

</code></pre>
"
Elastic APM,61195913,61163723,0,"2020/04/13, 23:09:07",True,"2020/04/13, 23:09:07",64,12032134,1,"<p>You can try the method described here <a href=""https://discuss.elastic.co/t/java-agent-set-environment-programatically/226575"" rel=""nofollow noreferrer"">disscuss-elastic</a>, via ElasticApmAttacher#attach(map of properties).</p>
"
Elastic APM,61524739,61524132,3,"2020/04/30, 16:40:12",True,"2020/04/30, 16:40:12",3264,1783306,1,"<blockquote>
  <p>Is this not yet included in the .NET agent, or is there additional configuration necessary to get this working?</p>
</blockquote>

<p>This is not yet included in the .NET Agent unfortunately.</p>
"
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606,2042827,0,"<p>WebFlux can run on Servlet containers with support for the Servlet 3.1 Non-Blocking IO API as well as on other async runtimes such as <strong>Netty</strong> and Undertow.</p>

<p>Each Spring Boot web application includes an embedded web server.</p>

<p>For reactive stack applications, the <code>spring-boot-starter-webflux</code> includes Reactor Netty by default I guess. And it does not include <code>Servlet API</code> (Netty is non-Servlet runtime), but it looks like your <code>Elastic APM</code> expects this API to be present.</p>

<p>Try to use <code>spring-boot-starter-tomcat</code> instead of Netty. When <a href=""https://docs.spring.io/spring-boot/docs/2.1.1.RELEASE/reference/html/howto-embedded-web-servers.html"" rel=""nofollow noreferrer"">switching to a different HTTP server</a>, you need to exclude the default dependencies in addition to including the one you need. </p>

<p>Here is an example:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;
    &lt;exclusions&gt;
        &lt;!-- Exclude the Netty dependency --&gt;
        &lt;exclusion&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-reactor-netty&lt;/artifactId&gt;
        &lt;/exclusion&gt;
    &lt;/exclusions&gt;
&lt;/dependency&gt;
&lt;!-- Use Tomcat instead --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Tomcat dependency brings Servlet API. Perhaps it will resolve your issue.</p>
"
Elastic APM,66580927,61642883,0,"2021/03/11, 12:42:08",False,"2021/03/11, 12:42:08",47,2800145,0,"<p>Looks like there is no support from elastic for WebFlux yet</p>
<p>Check here <a href=""https://github.com/elastic/apm-agent-java/issues/60"" rel=""nofollow noreferrer"">https://github.com/elastic/apm-agent-java/issues/60</a></p>
<p>They are currently working on it, but there is not a date to be ready yet</p>
"
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45,13409135,0,"<p>I got the answer after posting the same on <strong>Elastic Support Forum</strong>.<br/>
It was a very prompt response.<br/>
This was not a problem from Elastic APM side, and was more of a silly problem from my side.<br/>
Refer the <a href=""https://discuss.elastic.co/t/elastic-apm-java-transactions-spans-are-recorded-but-not-reported-to-elastic-apm-server-or-kibana/239505"" rel=""nofollow noreferrer"">discussion</a> to find the problem and solution.</p>
"
Elastic APM,62903745,62815678,1,"2020/07/14, 23:46:57",True,"2020/07/14, 23:46:57",1274,1370767,1,"<p>This question was cross-posted to discuss.elastic.co, and you can see the answer that was provided there: <a href=""https://discuss.elastic.co/t/elastic-apm-python-system-metrics-dont-show-process-related-metrics-like-memory-on-kibana/240531/2?u=basepi"" rel=""nofollow noreferrer"">https://discuss.elastic.co/t/elastic-apm-python-system-metrics-dont-show-process-related-metrics-like-memory-on-kibana/240531/2?u=basepi</a></p>
"
Elastic APM,63118018,63116459,4,"2020/07/27, 17:58:27",False,"2020/07/27, 17:58:27",163369,4604579,1,"<p>In your ES Cloud console, you need to Edit the cluster configuration, scroll to the APM section and then click &quot;User override settings&quot;. In there you can override the target index by adding the following property:</p>
<pre><code>output.elasticsearch.index: &quot;apm-application-%{[observer.version]}-{type}-%{+yyyy.MM.dd}&quot;
</code></pre>
<p>Note that if you change this setting, you also need to modify the corresponding index template to match the new index name.</p>
"
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64,12032134,0,"<p>currently elastic-apm-agent support natively Quartz framework(since 1.8). If you use it, instrumentation should work. But you should add your packages to <code>application_packages</code>.
It would be good if you can share mini-demo project. And I can reproduce your problem locally.
Information from <a href=""https://www.elastic.co/guide/en/apm/agent/java/current/supported-technologies-details.html"" rel=""nofollow noreferrer"">supported-technologies-details</a></p>
<pre><code>The agent instruments the execute method of any class implementing org.quartz.Job, as well as the executeInternal method of any class extending org.springframework.scheduling.quartz.QuartzJobBean, and creates a transaction with the type scheduled, representing the job execution

NOTE: only classes from the quartz-jobs dependency will be instrumented automatically. For the instrumentation of other jobs the package must be added to the application_packages parameter.
</code></pre>
"
Elastic APM,63351255,63339667,1,"2020/08/11, 06:53:42",True,"2020/08/11, 06:53:42",6576,547452,0,"<p>All of the Elastic APM agents, with the exception of the RUM JavaScript agent, have a <a href=""https://www.elastic.co/guide/en/apm/agent/java/current/config-reporter.html#config-verify-server-cert"" rel=""nofollow noreferrer""><code>verify_server_cert</code></a> configuration variable. You can set this to <code>false</code> to disable server TLS certificate verification.</p>
"
Elastic APM,63353158,63339667,1,"2020/08/11, 10:06:22",False,"2020/08/11, 10:06:22",32819,433570,0,"<p>I am using docker-compose</p>
<p>and the following option doesn't work. I think its a bug for apm..
When I use same option in apm-server.yml it works fine.</p>
<pre><code>environment:
  - output.elasticsearch.protocol=https
  - output.elasticsearch.username=$USERNAME
  - output.elasticsearch.password=$ELASTIC_PASSWORD
  - output.elasticsearch.ssl.verification_mode=none
</code></pre>
"
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264,1783306,1,"<p>One thing you can use is the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#filter-api"" rel=""nofollow noreferrer"">Filter API</a> for this.</p>
<p>With that you have access to all transactions and spans before they are sent to the APM Server.</p>
<p>You can't run through all the spans on a given transaction, so you need some tweaking - for this I use a <code>Dictionary</code> in my sample.</p>
<pre><code>var numberOfSqlQueries = new Dictionary&lt;string, int&gt;();

Elastic.Apm.Agent.AddFilter((ITransaction transaction) =&gt;
{
    if (numberOfSqlQueries.ContainsKey(transaction.Id))
    {
        // We make an assumption here: we assume that all SQL requests on a given transaction end before the transaction ends
        // this in practice means that you don't do any &quot;fire and forget&quot; type of query. If you do, you need to make sure
        // that the numberOfSqlQueries does not leak.
        transaction.Labels[&quot;NumberOfSqlQueries&quot;] = numberOfSqlQueries[transaction.Id].ToString();
        numberOfSqlQueries.Remove(transaction.Id);
    }

    return transaction;
});

Elastic.Apm.Agent.AddFilter((ISpan span) =&gt;
{
    // you can't relly filter whether if it's done by EF Core, or another database library
    // but you have all sorts of other info like db instance, also span.subtype and span.action could be helpful to filter properly
    if (span.Context.Db != null &amp;&amp; span.Context.Db.Instance == &quot;MyDbInstance&quot;)
    {
        if (numberOfSqlQueries.ContainsKey(span.TransactionId))
            numberOfSqlQueries[span.TransactionId]++;
        else
            numberOfSqlQueries[span.TransactionId] = 1;
    }

    return span;
});
</code></pre>
<p>Couple of thing here:</p>
<ul>
<li>I assume you don't do &quot;fire and forget&quot; type of queries, if you do, you need to handle those extra</li>
<li>The counting isn't really specific to EF Core queries, but you have info like db name, database type (mssql, etc.) - hopefully based on that you'll be able filter the queries you want.</li>
<li>With <code>transaction.Labels[&quot;NumberOfSqlQueries&quot;]</code> we add a label to the given transction, and you'll be able to see this data on the transaction in Kibana.</li>
</ul>
"
Elastic APM,65484213,65441133,0,"2020/12/28, 23:19:29",True,"2020/12/28, 23:19:29",109,1175351,0,"<p>Looks like it can be done using drop_event processor in api-server.yml.</p>
<pre><code>processors:
 - drop_event:
     when:
       equals:
         transaction.custom.transactions_sampled: false
</code></pre>
<p>and in code set custom context:</p>
<pre><code>Transaction elasticTransaction = ElasticApm.currentTransaction();
elasticTransaction.addCustomContext(&quot;transactions.sampled&quot;, false);
</code></pre>
"
Elastic APM,66062278,66026533,0,"2021/02/05, 13:15:44",True,"2021/02/05, 13:15:44",59,8278891,0,"<p>Commenting out <code>'SERVER_URL': '127.0.0.1:8200'</code> solved the problem.</p>
"
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264,1783306,1,"<blockquote>
<p>I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.</p>
</blockquote>
<p>Yes, this is possible and there is an API for it. <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#manually-propagating-distributed-tracing-context"" rel=""nofollow noreferrer"">This part of the documentation</a> explains it.</p>
<p>So you'll need to do this when you start your transaction - I imagine in your scenario this will be when you read a message from RabbitMQ.</p>
<p>When you <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#convenient-capture-transaction"" rel=""nofollow noreferrer"">start the transaction</a> there is an optional parameter called <code>distributedTracingData</code> - if you pass it, then the transaction will reuse the traceid which you passed through RabbitMQ, and this way the new transaction will be part of the whole trace. If you don't pass this parameter, a new traceid will be generated and a new trace will be started.</p>
<p>Another comment that may help: you pass the trace id into the method where you start the transaction and each span will inherit this trace id within a transaction - so you control this on the transaction level and accordingly you don't pass it into a span.</p>
<p>Here is a small code snippet on how this would look:</p>
<pre><code>serializedDistributedTracingData = //read this from the message which you get RabbitMq

var transaction2 = Agent.Tracer.StartTransaction(&quot;RadFromQueue&quot;, &quot;RabbitMQRead&quot;,
     DistributedTracingData.TryDeserializeFromString(serializedDistributedTracingData));
</code></pre>
"
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417,2803435,1,"<p>@gregkalapos, again thank you for the information. I checked how to acquire the neccessary trace information as in <a href=""https://www.elastic.co/guide/en/apm/agent/nodejs/master/distributed-tracing.html"" rel=""nofollow noreferrer"">node.js agent documentation</a> and when I debugged noticed that it was the trace id. Next in the C# consumer end I placed a code snippet as mentioned in the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#manually-propagating-distributed-tracing-context"" rel=""nofollow noreferrer"">.Net agent</a> and gave it a run. Kibana displayed the transactions from two different services in a single trace as I hoped it would.</p>
"
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417,2803435,0,"<p>Initially asked this questions because Visual Studio did not show the source as expected in the editor. So what I did was clicked Build after right clicking on the .sln (solution) then noticed that necessary version of .Net SDK was not there and version of MSBuild related to it. Once I updated Visual Studio the sources were visible. Hope this would help if someone faced a similar situation.</p>
"
Elastic APM,66324541,66316731,1,"2021/02/23, 00:45:53",False,"2021/02/23, 00:45:53",633,13659075,1,"<p>Using labels should be the best way to add custom details to the transaction/span but you can also use the <code>addCustomContext()</code> method:</p>
<p><a href=""https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-add-custom-context"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-add-custom-context</a></p>
"
Elastic APM,66782709,66781290,0,"2021/03/24, 16:01:20",False,"2021/03/24, 16:01:20",11,13628343,1,"<p>That error indicates the agent can't connect to apm-server.  <code>SERVER_URL</code> should be <code>ELASTIC_APM_SERVER_URL</code> in the apm-agent-container env.</p>
"
Elastic APM,66798562,66781290,1,"2021/03/25, 13:27:56",False,"2021/03/25, 13:27:56",1,15469419,0,"<p>Thanks for the reply, I'm able to connect apm-server with the agent, but in kibana dashboard, I'm getting &quot; No data has been received from agents yet&quot; . My application is running fine</p>
<pre><code>2021-03-25 08:47:06,605 [main] INFO co.elastic.apm.agent.util.JmxUtils - Found JVM-specific OperatingSystemMXBean interface: com.sun.management.OperatingSystemMXBean
2021-03-25 08:47:06,676 [main] INFO co.elastic.apm.agent.configuration.StartupInfo - Starting Elastic APM 1.19.0 as demo-service1 on Java 10.0.2 Runtime version: 10.0.2+13-Debian-2 VM version: 10.0.2+13-Debian-2 (Oracle Corporation) Linux 5.4.0-1040-azure
2021-03-25 08:47:06,676 [main] INFO co.elastic.apm.agent.configuration.StartupInfo - VM Arguments: [-javaagent:/app/elastic-apm-agent-1.19.0.jar, -Delastic.apm.service_name=demo-service1, -Delastic.apm.application_packages=com.javadeveloperzone.log4j, -Delastic.apm.server_urls=, -Delastic.apm.enable_log_correlation=true]
2021-03-25 08:47:07,836 [main] INFO co.elastic.apm.agent.impl.ElasticApmTracer - Tracer switched to RUNNING state

 . ____ _ __ _ _
/\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
\\/ ___)| |_)| | | | | || (_| | ) ) ) )
' |____| .__|_| |_|_| |_\__, | / / / /
=========|_|==============|___/=/_/_/_/
:: Spring Boot :: (v2.0.3.RELEASE)
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.springframework.cglib.core.ReflectUtils$1 (jar:file:/app/app.jar!/BOOT-INF/lib/spring-core-5.0.7.RELEASE.jar!/) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
WARNING: Please consider reporting this to the maintainers of org.springframework.cglib.core.ReflectUtils$1
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
2021-03-25 08:47:12,189 [main] INFO co.elastic.apm.agent.servlet.ServletVersionInstrumentation - Servlet container info = Apache Tomcat/8.5.31
2021-03-25 08:52:15,083 INFO [Log4JExample] c979238deffe75982fe43844c268c83e - Demobject [productid=qert, productname=qwerty, price=50.0, description=qwerty sdfghj zxcvb]
2021-03-25 08:52:15,083 INFO [Log4JExample] c979238deffe75982fe43844c268c83e - Demobject [productid=qert, productname=qwerty, price=50.0, description=qwerty sdfghj zxcvb]
2021-03-25 08:52:20,706 INFO [Log4JExample] 3cb508519b2d44f9c5cab6da497d0745 - Demobject [productid=qert, productname=qwerty, price=50.0, description=qwerty sdfghj zxcvb]
2021-03-25 08:52:20,706 INFO [Log4JExample] 3cb508519b2d44f9c5cab6da497d0745 - Demobject [productid=qert, productname=qwerty, price=50.0, description=qwerty sdfghj zxcvb]
2021-03-25 10:46:03,993 INFO [Log4JExample] 2828f071583f306e89591a4a2edf2080 - Demobject [productid=qert, productname=qwerty, price=50.0, description=qwerty sdfghj zxcvb]
</code></pre>
"
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369,4604579,1,"<p>Good job so far. Your pipeline is almost good, however, the grok pattern needs some fixing and you have some orphan curly braces. Here is a working example:</p>
<pre><code>POST _ingest/pipeline/_simulate
{
  &quot;pipeline&quot;: {
    &quot;description&quot;: &quot;parse multiple patterns&quot;,
    &quot;processors&quot;: [
      {
        &quot;grok&quot;: {
          &quot;field&quot;: &quot;message&quot;,
          &quot;patterns&quot;: [
            &quot;&quot;&quot;%{TIME:logtime} %{WORD:loglevel} trace.id=%{TRACE_ID:trace.id}(?: transaction.id=%{SPAN_ID:transaction.id})? %{GREEDYDATA:message}&quot;&quot;&quot;
          ],
          &quot;pattern_definitions&quot;: {
            &quot;TRACE_ID&quot;: &quot;[0-9A-Fa-f]{32}&quot;,
            &quot;SPAN_ID&quot;: &quot;[0-9A-Fa-f]{16}&quot;
          }
        }
      },
      {
        &quot;date&quot;: {
          &quot;field&quot;: &quot;logtime&quot;,
          &quot;target_field&quot;: &quot;@timestamp&quot;,
          &quot;formats&quot;: [
            &quot;HH:mm:ss&quot;
          ]
        }
      }
    ]
  },
  &quot;docs&quot;: [
    {
      &quot;_source&quot;: {
        &quot;message&quot;: &quot;08:27:47 INF trace.id=898a7716358b25408d4f193f1cd17831 transaction.id=4f7590e4ba80b64b SOME MSG&quot;
      }
    }
  ]
}
</code></pre>
<p>Response:</p>
<pre><code>{
  &quot;docs&quot; : [
    {
      &quot;doc&quot; : {
        &quot;_index&quot; : &quot;_index&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;_id&quot;,
        &quot;_source&quot; : {
          &quot;trace&quot; : {
            &quot;id&quot; : &quot;898a7716358b25408d4f193f1cd17831&quot;
          },
          &quot;@timestamp&quot; : &quot;2021-01-01T08:27:47.000Z&quot;,
          &quot;loglevel&quot; : &quot;INF&quot;,
          &quot;message&quot; : &quot;SOME MSG&quot;,
          &quot;logtime&quot; : &quot;08:27:47&quot;,
          &quot;transaction&quot; : {
            &quot;id&quot; : &quot;4f7590e4ba80b64b&quot;
          }
        },
        &quot;_ingest&quot; : {
          &quot;timestamp&quot; : &quot;2021-03-30T11:07:52.067275598Z&quot;
        }
      }
    }
  ]
}
</code></pre>
<p>Just note that the exact date is missing so the @timestamp field resolve to January 1st this year.</p>
"
Elastic APM,67065273,67057172,1,"2021/04/12, 23:27:40",True,"2021/04/13, 00:52:22",633,13659075,0,"<p>Try to specify the <code>config_file</code> using the following notation:</p>
<p><code>-Delastic.apm.config_file=elasticapm.properties</code></p>
<p>The attacher can create the log file depending on the settings configured during startup. See the [1] current code for a better understanding.</p>
<p>[1] <a href=""https://github.com/elastic/apm-agent-java/blob/0465d479430172c3e745afd2ef5b62a3da6b60aa/apm-agent-attach-cli/src/main/java/co/elastic/apm/attach/AgentAttacher.java#L79"" rel=""nofollow noreferrer"">https://github.com/elastic/apm-agent-java/blob/0465d479430172c3e745afd2ef5b62a3da6b60aa/apm-agent-attach-cli/src/main/java/co/elastic/apm/attach/AgentAttacher.java#L79</a></p>
"
Elastic APM,61196475,60947830,0,"2020/04/13, 23:48:14",False,"2020/04/13, 23:48:14",64,12032134,0,"<p>Do you mean that you need new ""Transaction type""?</p>

<p>If yes, so you should set <code>type</code> annotation parameter.</p>

<p>But @CaptureTranscation annotation work <a href=""https://www.elastic.co/guide/en/apm/agent/java/master/public-api.html#api-capture-transaction"" rel=""nofollow noreferrer"">in case</a>:</p>

<pre><code>Note that this only works when there is no active transaction on the same thread.

    value: The name of the span. Defaults to ClassName#methodName
    type: The type of the transaction. Defaults to request
</code></pre>
"
Elastic APM,64793314,55457083,0,"2020/11/11, 21:58:43",True,"2020/11/11, 21:58:43",2674,3019008,0,"<p>I worked with the Elastic APM team, who had just rolled out this package: <a href=""https://www.npmjs.com/package/elastic-apm-node"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/elastic-apm-node</a></p>
<p>The directions are pretty self-explanatory, works like a charm.</p>
"
Elastic APM,52714271,52696696,1,"2018/10/09, 09:00:41",False,"2018/10/09, 09:00:41",48155,2989261,0,"<p>Hard to tell without debugging but since some connections are getting dropped when you add more load + concurrency it's likely that you need more replicas on your <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment"" rel=""nofollow noreferrer"">Kubernetes deployments</a> and possibly adjusts the <a href=""https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/"" rel=""nofollow noreferrer"">Resources</a> on your container pod specs.</p>

<p>If this turns out to be the case you can also configure an <a href=""https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"" rel=""nofollow noreferrer"">HPA</a> (Horizontal Pod Autoscaler) to handle your load.</p>
"
Elastic APM,62516533,62511256,2,"2020/06/22, 17:17:56",True,"2020/06/22, 17:17:56",86,6020497,5,"<p>It's true, there isn't an Elixir agent for Elastic APM - you can upvote <a href=""https://github.com/elastic/apm/issues/60"" rel=""noreferrer"">this issue</a> to get the topic more attention.</p>
<p>As you discovered, you can use the OpenTelemetry in the meantime.  To do that, run the OpenTelemetry contrib collector (otel collector) and configure it to export to <code>elastic</code> - there is a full explanation <a href=""https://www.elastic.co/guide/en/apm/get-started/current/open-telemetry-elastic-get-started.html#open-telemetry-elastic-configure"" rel=""noreferrer"">in the docs</a> along with this sample configuration:</p>
<pre class=""lang-yaml prettyprint-override""><code>receivers:
  otlp:
    endpoint: localhost:55680
processors:
  batch:
    timeout: 1s
    send_batch_size: 1024
exporters:
  elastic:
    apm_server_url: &quot;https://elasticapm.example.com&quot;
    secret_token: &quot;ESS_TOKEN&quot;
service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [elastic]
</code></pre>
<p>In your application, configure the tracer use the <a href=""https://github.com/opentelemetry-beam/opentelemetry_exporter#using"" rel=""noreferrer"">opentelemetry exporter</a>.</p>
<p>At that point you'll have a tracer in your application sending traces to the otel collector.  From there, traces will be exported to the Elastic Stack via APM Server.  In summary: <code>your app -&gt; otel collector -&gt; apm-server -&gt; elasticsearch</code></p>
<p>The <a href=""https://github.com/open-telemetry/opentelemetry-erlang-api#registering-and-using-tracers-directly"" rel=""noreferrer"">Erlang/Elixir Agent Docs</a> have sample code for starting and decorating spans.</p>
"
Elastic APM,60540275,58780229,0,"2020/03/05, 09:37:36",False,"2020/03/05, 09:45:37",1,13010745,0,"<p>You can attach your ElasticApmAttacher.attach() in the Spring Application main class</p>

<p>For a SpringBootApplication packaged as a war file, and deployed to Tomcat server, this can be added to the configure method</p>

<p>Below code might help:</p>

<pre><code>package com.test.main

import co.elastic.apm.attach.ElasticApmAttacher
import org.springframework.boot.Banner
import org.springframework.boot.SpringApplication
import org.springframework.boot.autoconfigure.SpringBootApplication
import org.springframework.boot.builder.SpringApplicationBuilder
import org.springframework.boot.web.servlet.support.SpringBootServletInitializer


@SpringBootApplication(scanBasePackages = [ ""com.test"" ])
class Application extends SpringBootServletInitializer{

    static void main(String[] args) {
        SpringApplication app = new SpringApplication(Application.class)
        app.setBannerMode(Banner.Mode.OFF)
        ElasticApmAttacher.attach();
        app.run(args)
    }

    @Override
    protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {
        ElasticApmAttacher.attach();
        return application.sources(Application.class)
    }
}
</code></pre>
"
Elastic APM,61642930,58780229,0,"2020/05/06, 21:46:03",False,"2020/05/06, 21:46:03",81,1351859,0,"<pre><code>    Map&lt;String, String&gt; apmConfiguration = new HashMap&lt;&gt;();
    apmConfiguration.put(""server_urls"", ""http://localhost:8200"");
    apmConfiguration.put(""service_name"", ""SpringBootApp"");
    ElasticApmAttacher.attach(apmConfiguration);
</code></pre>
"
Elastic APM,56835665,56832275,0,"2019/07/01, 15:24:24",False,"2019/07/01, 15:24:24",9394,573153,0,"<p>Are you not building a custom Dockerfile and you could just add it there (using wget or curl probably)?</p>

<p>If you really want a build dependency, <a href=""https://search.maven.org/artifact/co.elastic.apm/elastic-apm-agent/1.7.0/jar"" rel=""nofollow noreferrer"">https://search.maven.org/artifact/co.elastic.apm/elastic-apm-agent/1.7.0/jar</a> should be what you want.</p>

<p>PS: IMO it's a feature that this is only a runtime dependency and you can just add, remove, change it independently of your application; unless you want to do some custom instrumentation.</p>
"
Elastic APM,60892441,60890836,2,"2020/03/27, 20:53:59",True,"2020/03/27, 21:21:59",3264,1783306,2,"<p>You don't end <code>trans1</code> and <code>trans2</code>.</p>

<p>Just put these 2 lines to the point where these end, and everything should show up fine: </p>

<pre><code>trans1.End();
trans2.End();
</code></pre>

<p>There is the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#convenient-capture-transaction"" rel=""nofollow noreferrer""><code>CaptureTransaction</code></a>, which is a convenient method that can wrap you any code and makes sure the transaction is ended and all exceptions are captured - so you use that method and it does ""everything"" for you.</p>

<p>Then there is the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#api-start-transaction"" rel=""nofollow noreferrer""><code>StartTransaction</code></a> method - this is the one you use in your code -, which starts the transaction and does not do anything else. The advantage here is that you get an <code>ITransaction</code> instance which you can use wherever and whenever you want. But in this case you need to call <code>.End()</code> on it manually once the transaction (aka the code you want to capture) is executed.</p>

<p>Same with <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#convenient-capture-span"" rel=""nofollow noreferrer""><code>CaptureSpan</code></a> and <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#api-transaction-create-span"" rel=""nofollow noreferrer""><code>StartSpan</code></a>.</p>

<p>So you used <code>CaptureSpan</code> for your spans, so those where ended automatically when the lambda with <code>Task.Delay</code> finished, on the other hand you started your transactions with <code>StartTransaction</code> but only called <code>.End()</code> on <code>trans3</code> and not on the 2 other transactions.</p>

<p>There is some explanation with a demo <a href=""https://www.youtube.com/watch?v=4JGqu59DN14&amp;feature=youtu.be&amp;t=3413"" rel=""nofollow noreferrer"">here</a> - sample code of that demo is <a href=""https://github.com/gregkalapos/MicrosoftDevUserGroupGraz_StateOfObservability_Talk/blob/master/Samples/06_DtOverCmd/Program.cs"" rel=""nofollow noreferrer"">here</a>.</p>
"
Elastic APM,64329014,64323562,2,"2020/10/13, 08:23:52",False,"2020/10/13, 08:23:52",156911,4668,1,"<p>It's hard to say without knowing exactly how you're using NestJS and GraphQL together, and without knowing which parts of Apollo Server that NestJS itself uses.  Seeing a small sample of what <em>I am using the graphql feature of nestjs</em> means would be useful.</p>
<p>Here's a few datapoints that might help you narrow things down.  Also, opening <a href=""https://github.com/elastic/apm-agent-nodejs/issues"" rel=""nofollow noreferrer"">an issue</a> in the Agent repository or <a href=""https://discuss.elastic.co/c/observability/apm/58"" rel=""nofollow noreferrer"">a question in their forums</a> might get more of the right eyes on this.</p>
<p>The Elastic APM instrumentation for Apollo Server works by wrapping the <code>runHttpQuery</code> function of the <code>apollo-server-core</code> module, and <a href=""https://github.com/elastic/apm-agent-nodejs/blob/4fe301d66a1ef748a77a0cde7584931e176c618b/lib/instrumentation/modules/apollo-server-core.js#L18"" rel=""nofollow noreferrer"">marking the transaction with <code>trans._graphqlRoute</code></a>.</p>
<p>When the agent <a href=""https://github.com/elastic/apm-agent-nodejs/blob/4fe301d66a1ef748a77a0cde7584931e176c618b/lib/instrumentation/modules/graphql.js#L105"" rel=""nofollow noreferrer"">sees this <code>_graphqlRoute</code> property</a>, it runs some code that will set a default name for the transaction</p>
<pre class=""lang-js prettyprint-override""><code>      if (trans._graphqlRoute) {
        var name = queries.length &gt; 0 ? queries.join(', ') : 'Unknown GraphQL query'
        if (trans.req) var path = getPathFromRequest(trans.req, true)
        var defaultName = name
        defaultName = path ? defaultName + ' (' + path + ')' : defaultName
        defaultName = operationName ? operationName + ' ' + defaultName : defaultName
        trans.setDefaultName(defaultName)
        trans.type = 'graphql'
      }
</code></pre>
<p>In your application, either the <code>_graphqlRoute</code> property isn't getting set, or the renaming code above is doing something weird, or something about NestJS comes along and renames the transaction after it's been named with the above code.</p>
<p>Knowing more specifically <em>what</em> you're doing would help folks narrow in on your problems.</p>
"
Elastic APM,61509232,61262895,0,"2020/04/29, 21:38:55",False,"2020/04/29, 21:38:55",64,12032134,0,"<p>did you try to give permissions to folder /opt/elastic ?</p>

<pre><code>chmod -R uog+x
</code></pre>
"
Elastic APM,56741976,56665160,0,"2019/06/24, 21:20:03",False,"2019/06/24, 21:20:03",528,9619535,1,"<p>Answer to the Errors - I have custom resole.extensions in the <code>webpack.config.js</code>:</p>

<pre><code>resolve: {
  extensions: ['.ts', '.tsx', '.js', '.jsx']
},
</code></pre>

<p>That was missing the default <code>.json</code>:</p>

<pre><code>resolve: {
  extensions: ['.ts', '.tsx', '.js', '.jsx', '.json']
},
</code></pre>

<p>Now only the warnings are left:</p>

<pre><code>WARNING in node_modules/elastic-apm-node/lib/agent.js 152:16-59
Critical dependency: the request of a dependency is an expression
 @ node_modules/elastic-apm-node/index.js

WARNING in node_modules/elastic-apm-node/lib/config.js 25:19-36
Critical dependency: the request of a dependency is an expression
 @ node_modules/elastic-apm-node/lib/agent.js

WARNING in node_modules/elastic-apm-node/lib/instrumentation/index.js 169:16-30
Critical dependency: the request of a dependency is an expression
 @ node_modules/elastic-apm-node/lib/agent.js
</code></pre>

<p>I addressed them to the developer: <a href=""https://github.com/elastic/apm-agent-nodejs/issues/1154"" rel=""nofollow noreferrer"">https://github.com/elastic/apm-agent-nodejs/issues/1154</a></p>
"
Elastic APM,65496630,65488128,2,"2020/12/29, 20:01:53",True,"2020/12/29, 20:01:53",633,13659075,1,"<p>The metrics shown in Kibana are sent by the APM agent that like you said has limited access to your environment. It basically says anything that is collected by the JVM running your JAR.</p>
<p>If you want to get further visibility into the CPU details of your local environment then you must augment your setup using <a href=""https://www.elastic.co/beats/metricbeat"" rel=""nofollow noreferrer"">Elastic MetricBeats</a> that ships O.S level details about your machine that sees beyond what the JVM can see.</p>
<p>In the presentation below I show how to configure logs, metrics, and APM altogether.</p>
<p><a href=""https://www.youtube.com/watch?v=aXbg9pZCjpk"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=aXbg9pZCjpk</a></p>
"
Elastic APM,50283028,50282965,0,"2018/05/11, 02:51:21",False,"2018/05/11, 02:51:21",7900,2259934,0,"<p>When you bring up containers using compose, each container has its own networking stack (so they can each talk to themselves on <code>localhost</code>, but they need an ip address or dns name to talk to a different container!).</p>

<p>Compose by default connects each of the containers to a default network and gives each a dns name with the name of the service.</p>

<p>If your compose file looks like</p>

<pre><code>services:
  apm:
    image: apm_image
  elasticsearch:
     image: elasticsearch:latest
</code></pre>

<p>A process in the <code>apm</code> container could access elasticsearch at <code>http://elasticsearch:9200</code></p>
"
Elastic APM,51649953,50282965,0,"2018/08/02, 12:19:26",False,"2018/08/02, 12:19:26",41,8848390,4,"<p>If you are starting all the services with single docker compose file, the app-server.yaml should have the value like this
<code>
output:
  elasticsearch:
    hosts: elasticsearch:9200
</code>
The ""hosts: elasticsearch:9200"" should be service name of the elasticsearch you mentioned in the docker-compose. Like in the followiing </p>

<p><code>
    version: '2'
    services:
      elasticsearch:
          image: elasticsearch:latest
</code>   </p>
"
Elastic APM,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445,3834445,3,"<p>Try using a configuration:</p>

<pre><code>version: ""3""

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.8.1
    networks:
      - elastic-jaeger
    ports:
      - ""127.0.0.1:9200:9200""
      - ""127.0.0.1:9300:9300""
    restart: on-failure
    environment:
      - cluster.name=jaeger-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - transport.host=127.0.0.1
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data

  apm-server:
    image: docker.elastic.co/apm/apm-server:6.8.1
    ports:
      - 8200:8200

    volumes:
       - ./apm-server/config/apm-server.yml:/usr/share/apm-server/apm-server.yml

    networks:
      - elastic-jaeger

  kibana:
    image: docker.elastic.co/kibana/kibana:6.8.1
    ports:
      - ""127.0.0.1:5601:5601""
    restart: on-failure
    networks:
      - elastic-jaeger

  jaeger-collector:
    image: jaegertracing/jaeger-collector
    ports:
      - ""14269:14269""
      - ""14268:14268""
      - ""14267:14267""
      - ""9411:9411""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--es.num-shards=1"",
      ""--es.num-replicas=0"",
      ""--log-level=error""
    ]
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    hostname: jaeger-agent
    command: [""--collector.host-port=jaeger-collector:14267""]
    ports:
      - ""5775:5775/udp""
      - ""6831:6831/udp""
      - ""6832:6832/udp""
      - ""5778:5778""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - no_proxy=localhost
    ports:
      - ""16686:16686""
      - ""16687:16687""
    networks:
      - elastic-jaeger
    restart: on-failure
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--span-storage.type=elasticsearch"",
      ""--log-level=debug""
    ]
    depends_on:
      - jaeger-agent

  grafana:
    image: grafana/grafana
    ports:
      - 3999:3000
    volumes:
      - ./grafana-data:/var/lib/grafana
    networks:
      - elastic-jaeger

volumes:
  esdata:
    driver: local

networks:
  elastic-jaeger:
    driver: bridge 

</code></pre>

<p>where the file apm-server/config/apm-server.yml has your config content:</p>

<pre><code>apm-server.rum.enabled: true
apm-server.rum.event_rate.limit: 300
apm-server.rum.event_rate.lru_size: 1000
apm-server.rum.allow_origins: ['*']
apm-server.rum.library_pattern: ""node_modules|bower_components|~""
apm-server.rum.exclude_from_grouping: ""^/webpack""
apm-server.rum.source_mapping.cache.expiration: 5m
apm-server.rum.source_mapping.index_pattern: ""apm-*-sourcemap*""
output.elasticsearch.hosts: [""http://elasticsearch:9200""]
apm-server.host: ""0.0.0.0:8200""
setup.kibana.host: ""kibana:5601""
setup.template.enabled: true
logging.to_files: false

</code></pre>

<p>Note the rum.allow_origins option that you can configure to resolve the CORS issue. <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html</a></p>
"
Elastic APM,60827426,60820578,0,"2020/03/24, 10:35:00",True,"2020/03/24, 10:35:00",6576,547452,2,"<p>These metrics come directly from <a href=""https://docs.oracle.com/javase/7/docs/api/java/lang/management/GarbageCollectorMXBean.html"" rel=""nofollow noreferrer"">java.lang.management.GarbageCollectorMXBean</a>. The value of the <code>jvm.gc.time</code> metric is taken from <code>GarbageCollectorMXBean.getCollectionTime</code>, which is indeed accumulating since the process started.</p>

<p>Assuming you're looking at metrics from a single JVM, there are a couple of possible reasons for why the value would appear to have gone backwards:</p>

<ol>
<li>The process restarted.</li>
<li>The values are for two different GC ""memory managers"" (e.g. G1 Young Generation, G1 Old Generation)</li>
</ol>

<p>If the process had restarted (which I expect you would know about anyway), the metrics documents in Elasticsearch would have different values for the field <code>agent.ephemeral_id</code>.</p>

<p>The more likely answer is that you're seeing values for two different memory managers/GC generations, in which case the metrics documents in Elasticsearch would have different values for the field <code>labels.name</code>.</p>

<p><a href=""https://i.stack.imgur.com/NsKAI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NsKAI.png"" alt=""enter image description here""></a></p>
"
Elastic APM,61340495,57696669,1,"2020/04/21, 12:44:16",True,"2020/04/21, 12:44:16",36,2155647,1,"<p>you have to modify your elastic APM python agent code. 
In general, you can add labels to your span example</p>

<pre><code>elasticapm.label(key1=value1, key2=value2)
</code></pre>

<p>also, you can add directly to span object. </p>

<p>You will get the ip from request object flask</p>

<p><code>request.remote_addr</code>  set this to the desired key.</p>

<p>more details of APIs on elastic APM python agent can be found here - <a href=""https://www.elastic.co/guide/en/apm/agent/python/current/api.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/python/current/api.html</a></p>

<p>Thanks</p>
"
Elastic APM,65706105,65703890,6,"2021/01/13, 18:38:19",False,"2021/01/13, 18:38:19",633,13659075,0,"<p>If the objective is to attach labels to a transaction over multiple spans then using the <a href=""https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html"" rel=""nofollow noreferrer"">public APIs from the Elastic APM for Java</a> is a better choice instead of instrumenting the JVM with ByteBuddy. You will have much more freedom to do what you want to do without relying on a hacking. FYI, the Elastic APM agent for Java already instrument the JVM with additional bytecode so what you are doing may get even more confusing because of this.</p>
<p>Alternatively, you can also use the <a href=""https://www.elastic.co/guide/en/apm/agent/java/current/opentracing-bridge.html"" rel=""nofollow noreferrer"">OpenTracing Bridge</a> to set labels in a transaction.</p>
"
Elastic APM,65712813,65703890,0,"2021/01/14, 05:02:19",False,"2021/01/14, 05:02:19",49309,1082681,0,"<p><em>Disclaimer: This answer is a stub for now.</em></p>
<p>You should first try to explore a more canonical way of doing things like Ricardo suggested. If for some reason that does not work, then we could explore ways to instrument your agent class - not so much because I think it is a good idea but because it is technically interesting.</p>
<p>Basically, we would have to find out if maybe the class you want to instrument was already loaded before your ByteBuddy agent gets active. Then you would have to use class retransformation rather than redefinition. You would have to make sure the advice you apply can do its job without the need to change the class structure with regard to method signatures and fields.</p>
<pre class=""lang-java prettyprint-override""><code>  .disableClassFormatChanges()
  .with(RETRANSFORMATION)
</code></pre>
<p>You would also need to make sure that the advice and ByteBuddy are visible to the other agent's classloader, e.g. by putting both on the boot class path. But let's not get ahead of ourselves. Explore Ricardo's ideas first, please.</p>
"
Elastic APM,65498418,65484826,1,"2020/12/29, 22:34:40",True,"2020/12/29, 22:34:40",156911,4668,1,"<p>Based on what I've seen it looks like there isn't a &quot;right&quot; way to do this with the stock <code>nuxt</code> command line application.   The problem seems to be that while <code>nuxt.config.js</code> is the first time a user has a chance to add some javascript, that the <code>nuxt</code> command line application bootstraps the Node's HTTP frameworks before this config file is <code>required</code>.  This means the elastic agent (or any APM agent) doesn't have a chance to hook into the modules.</p>
<p>The <a href=""https://github.com/nuxt/cli/issues/4#issuecomment-752230294"" rel=""nofollow noreferrer"">current recommendations</a> from the Nuxt team appears to be</p>
<ol>
<li><p>Invoke <code>nuxt</code> manually via <code>-r</code></p>
<pre><code>     {
       &quot;scripts&quot;: {
         &quot;start&quot;: &quot;node -r elastic-apm-node node_modules/nuxt/.bin/nuxt&quot;
       }
     }  
</code></pre>
</li>
<li><p>Skip <code>nuxt</code> and <a href=""https://nuxtjs.org/docs/2.x/internals-glossary/nuxt/"" rel=""nofollow noreferrer"">use NuxtJS programmatically</a> as a middleware in your framework of choice</p>
<pre><code>     const { loadNuxt } = require('nuxt')
     const nuxtPromise = loadNuxt('start')
     app.use((req, res) =&gt; { nuxtPromise.then(nuxt =&gt; nuxt.render(req, res)) })
</code></pre>
</li>
</ol>
"
Elastic APM,65978053,65484826,0,"2021/01/31, 12:30:39",False,"2021/01/31, 12:36:15",31,5761761,1,"<p>Based on Alan Storm answer (from Nuxt team) I made it work but with a little modification:</p>
<ul>
<li>I created a file named nodeApm.js where I added the following code:
<pre><code>const nodeApm = require('elastic-apm-node')

if (!nodeApm.isStarted()) { ... // configuration magic }
</code></pre>
</li>
<li>In script sections I added:
<pre><code>&quot;start&quot;: &quot;node -r ./nodeApm.js node_modules/nuxt/.bin/nuxt&quot;
</code></pre>
</li>
</ul>
"
Elastic APM,65384228,65380989,4,"2020/12/20, 21:58:43",True,"2020/12/20, 22:08:14",58,14860242,1,"<p>actually there are many reasons why your app not starting depending on how you setup and configured your ELK stack , but for me I did the following and it's working fine :</p>
<ol>
<li>shipped <strong>application.jar</strong> and <strong>apm-agent.jar</strong> via Dockerfile and run them inside container :</li>
</ol>
<pre>
<code>
    FROM openjdk:8-jre-alpine
    
    COPY javaProjects/test-apm/target/test-apm-0.0.1-SNAPSHOT.jar /app.jar 
    
    COPY elastic-apm-agent-1.19.0.jar /apm-agent.jar 
    
    CMD [""/usr/bin/java"",""-javaagent:/apm-agent.jar"", ""-Delastic.apm.service_name=my-cool-service -Delastic.apm.application_packages=main.java -Delastic.apm.server_urls=http://localhost:8200"",""-jar"", ""/app.jar""]

</code>
</pre>
<ol start=""2"">
<li><p>create image from this Dockerfile:</p>
 <pre><code>docker build -t test-apm:latest ./</pre></code>
</li>
<li><p>run the created image :</p>
 <pre><code>docker run  --network host -p 8080:8080 test-apm:latest</pre></code>
</li>
</ol>
<ul>
<li>note my <strong>apm-server</strong> and <strong>ELK-stack</strong> was running on my host machine ,
I think if you do the same and make little changes to mach you environments it should work fine ,</li>
</ul>
"
Elastic APM,59233925,59228679,1,"2019/12/08, 11:07:43",True,"2019/12/08, 11:07:43",37971,1237575,0,"<p>The exception comes from the constructur of the <code>Kernel32</code> class which is a class of the Maven coordinate <em>net.java.dev.jna:jna-platform</em> which itself depends on <em>net.java.dev.jna:jna</em>. It seems to me like you have to incompatible versions of those dependencies on the class path.</p>

<p>I assume that you use version 4 of JNA core and version 5 of JNA platform. Upgrade the first or downgrade the latter and the error should disappear.</p>
"
Elastic APM,58917376,58916834,9,"2019/11/18, 16:59:13",True,"2020/03/24, 22:19:43",18074,4728685,1,"<p>Well, as an option, you can use something like that</p>

<pre class=""lang-cs prettyprint-override""><code>public static class Agent
{
    private static Lazy&lt;Foo&gt; _lazy;

    public static Foo Instance =&gt; _lazy?.Value ?? throw new InvalidOperationException(""Please, setup the instance"");
    public static bool IsInstanceCreated =&gt; _lazy?.IsValueCreated ?? false;

    public static void Setup(Bar bar)
    {
        _lazy = new Lazy&lt;Foo&gt;(() =&gt; new Foo(bar));
    }
}
</code></pre>
"
Elastic APM,56067795,56065263,1,"2019/05/10, 00:25:17",False,"2019/05/10, 00:25:17",2737,7330758,0,"<p>It seems that you are using the oss distribution of elasticsearch but the defaut version of apm.</p>

<p>upgrade the elasticsearch cluster to the default disto or use this oss apm docker image: docker.elastic.co/apm/apm-server-oss:7.0.1</p>
"
Elastic APM,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287,780798,2,"<p><a href=""https://opentracing.io/specification/"" rel=""nofollow noreferrer"">OpenTracing</a> <a href=""https://github.com/opentracing/specification/blob/master/project_organization.md"" rel=""nofollow noreferrer"">is a set of standard APIs that consistently model and describe the behavior of distributed systems</a>)</p>

<p>OpenTracing did not describe how to collect, report, store or represent the data of interrelated traces and spans. It is implementation details (such as <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">jaeger</a> or <a href=""https://www.wavefront.com/"" rel=""nofollow noreferrer"">wavefront</a>).</p>

<p>jaeger-client-csharp is very jaeger-specific. But there is one exception, called <a href=""https://www.jaegertracing.io/docs/1.12/features/#backwards-compatibility-with-zipkin"" rel=""nofollow noreferrer"">zipkin</a> which in turns is not fully OpenTracing compliant, even it has similar terms.</p>

<p>If you are OK with <a href=""https://github.com/opentracing-contrib/csharp-netcore/"" rel=""nofollow noreferrer"">opentracing-contrib/csharp-netcore</a> (hope you are using this library) then if you want to achieve ""no code change"" (in target microservice) in order to configure tracing subsystem, you should use some plug-in model.</p>

<p>Good news that aspnetcore has concept of <a href=""https://docs.microsoft.com/en-us/ASPNET/Core/fundamentals/host/platform-specific-configuration?view=aspnetcore-2.2"" rel=""nofollow noreferrer"">hosted startup assemblies</a>, which allow you to configure tracing system. So, you can have some library called <code>JaegerStartup</code> where you will implement IHostedStartup like follows:</p>

<pre><code>public class JaegerStartup : IHostingStartup
{
    public void Configure(IWebHostBuilder builder)
    {
        builder.ConfigureServices((ctx, services) =&gt;
        {
            services.AddOpenTracing();

            if (ctx.Configuration.IsTracerEnabled()) // implement it by observing your section in configuration.
            {
                services.AddSingleton(serviceProvider =&gt;
                {
                    var loggerFactory = new LoggerFactory();
                    var config = Jaeger.Configuration.FromEnv(loggerFactory);

                    var tracer = config.GetTracer();

                    GlobalTracer.Register(tracer);

                    return tracer;
                });
            }
        });
    }
}
</code></pre>

<p>When you decide to switch the tracing system - you need to create another library, which can be loaded automatically, and target microservice code will not be touched.</p>
"
Elastic APM,60983706,60920952,0,"2020/04/02, 06:15:04",False,"2020/04/02, 06:15:04",6576,547452,0,"<blockquote>
  <p>So, I'm guessing that the handler wrappers are dropping the buffalo.Context information.</p>
</blockquote>

<p>That's correct. The problem is that <code>buffalo.WrapHandler</code> (<a href=""https://github.com/gobuffalo/buffalo/blob/bbdf70341bf44365b8fb0c83689ebe030ce76e5b/wrappers.go#L7-L12"" rel=""nofollow noreferrer"">Source</a>) throws away all of the context other than the underlying <code>http.Request</code>/<code>http.Response</code>:</p>

<pre><code>// WrapHandler wraps a standard http.Handler and transforms it
// into a buffalo.Handler.
func WrapHandler(h http.Handler) Handler {
    return func(c Context) error {
        h.ServeHTTP(c.Response(), c.Request())
        return nil
    }
}
</code></pre>

<blockquote>
  <p>So, what would I need to do to be able to integrate Sentry and Elastic in Buffalo asides from trying to reimplement their wrappers?</p>
</blockquote>

<p>I can see two options:</p>

<ul>
<li>Reimplement <code>buffalo.WrapHandler</code>/<code>buffalo.WrapBuffaloHandler</code> to stop throwing away the buffalo.Context. This would involve storing the <code>buffalo.Context</code> in the underlying <code>http.Request</code>'s context, and then pulling it out again on the other side instead of creating a whole new context.</li>
<li>Implement Buffalo-specific middleware for Sentry and Elastic APM without using the <code>Wrap*</code> functions.</li>
</ul>

<p>There's an open issue in the Elastic APM agent for the latter option: <a href=""https://github.com/elastic/apm-agent-go/issues/39"" rel=""nofollow noreferrer"">elastic/apm#39</a>.</p>
"
Elastic APM,62680831,62677891,0,"2020/07/01, 19:02:17",True,"2020/07/01, 19:02:17",45,13409135,0,"<p>I got the answer after posting the same on <strong>Elastic Support Forum</strong>.<br/>
It was a very prompt response.<br/>
This was not a problem from Elastic APM side, and was more of a silly problem from my side.<br/>
Refer the <a href=""https://discuss.elastic.co/t/elastic-apm-java-transactions-spans-are-recorded-but-not-reported-to-elastic-apm-server-or-kibana/239505"" rel=""nofollow noreferrer"">discussion</a> to find the problem and solution.</p>
"
Elastic APM,58742793,58738337,0,"2019/11/07, 08:00:56",False,"2019/11/07, 08:00:56",9394,573153,0,"<p>Logging is separate from APM / tracing, but can be integrated.</p>

<p><a href=""https://github.com/elastic/ecs-logging-java"" rel=""nofollow noreferrer"">https://github.com/elastic/ecs-logging-java</a> is a curated logging library that will also correlate the trace IDs, so you can tie both together.</p>

<p>Keep using SLF4J and just add the right logging backend. The output can then be picked up by Filebeat (as described in the repository) and you're ready to go.</p>
"
Elastic APM,61196812,58738337,0,"2020/04/14, 00:09:21",False,"2020/04/14, 00:09:21",64,12032134,1,"<p>Elastic-apm-agent-java automatically capture exceptions when you use slf4j implementation <code>Logger#error(""message"", Throwable)</code>.</p>

<p>More information you can find <a href=""https://www.elastic.co/guide/en/apm/agent/java/master/supported-technologies-details.html#supported-logging-frameworks"" rel=""nofollow noreferrer"">here</a></p>
"
Elastic APM,65484213,65441133,0,"2020/12/28, 23:19:29",True,"2020/12/28, 23:19:29",109,1175351,0,"<p>Looks like it can be done using drop_event processor in api-server.yml.</p>
<pre><code>processors:
 - drop_event:
     when:
       equals:
         transaction.custom.transactions_sampled: false
</code></pre>
<p>and in code set custom context:</p>
<pre><code>Transaction elasticTransaction = ElasticApm.currentTransaction();
elasticTransaction.addCustomContext(&quot;transactions.sampled&quot;, false);
</code></pre>
"
Elastic APM,66089252,66088061,1,"2021/02/07, 16:52:15",True,"2021/02/07, 16:52:15",3264,1783306,1,"<blockquote>
<p>I want to know if there is a way to create a transaction (or span) using a traceparent that is being sent from another system not using a HTTP protocol.</p>
</blockquote>
<p>Yes, this is possible and there is an API for it. <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#manually-propagating-distributed-tracing-context"" rel=""nofollow noreferrer"">This part of the documentation</a> explains it.</p>
<p>So you'll need to do this when you start your transaction - I imagine in your scenario this will be when you read a message from RabbitMQ.</p>
<p>When you <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#convenient-capture-transaction"" rel=""nofollow noreferrer"">start the transaction</a> there is an optional parameter called <code>distributedTracingData</code> - if you pass it, then the transaction will reuse the traceid which you passed through RabbitMQ, and this way the new transaction will be part of the whole trace. If you don't pass this parameter, a new traceid will be generated and a new trace will be started.</p>
<p>Another comment that may help: you pass the trace id into the method where you start the transaction and each span will inherit this trace id within a transaction - so you control this on the transaction level and accordingly you don't pass it into a span.</p>
<p>Here is a small code snippet on how this would look:</p>
<pre><code>serializedDistributedTracingData = //read this from the message which you get RabbitMq

var transaction2 = Agent.Tracer.StartTransaction(&quot;RadFromQueue&quot;, &quot;RabbitMQRead&quot;,
     DistributedTracingData.TryDeserializeFromString(serializedDistributedTracingData));
</code></pre>
"
Elastic APM,66094811,66088061,0,"2021/02/08, 03:19:53",False,"2021/02/08, 03:19:53",417,2803435,1,"<p>@gregkalapos, again thank you for the information. I checked how to acquire the neccessary trace information as in <a href=""https://www.elastic.co/guide/en/apm/agent/nodejs/master/distributed-tracing.html"" rel=""nofollow noreferrer"">node.js agent documentation</a> and when I debugged noticed that it was the trace id. Next in the C# consumer end I placed a code snippet as mentioned in the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#manually-propagating-distributed-tracing-context"" rel=""nofollow noreferrer"">.Net agent</a> and gave it a run. Kibana displayed the transactions from two different services in a single trace as I hoped it would.</p>
"
Elastic APM,58075056,58074198,0,"2019/09/24, 10:22:09",True,"2019/09/24, 10:22:09",163369,4604579,5,"<p>You simply need to change your query to this:</p>

<pre><code>POST apm*/_delete_by_query
{
  ""query"": {
     ""term"": {
       ""service.name"": ""ldap1""
     }
  }
}
</code></pre>
"
Elastic APM,63339911,58074198,0,"2020/08/10, 15:09:15",False,"2020/08/10, 15:09:15",383,10119759,3,"<p>The accepted answer no longer works, you can use the following</p>
<pre><code>POST /apm-*/_delete_by_query
{
  &quot;query&quot;: {
&quot;bool&quot;: {
  &quot;must&quot;: [
    {
      &quot;term&quot;: {
        &quot;service.name&quot;: {
          &quot;value&quot;: &quot;my-application&quot;
        }
      }
    }
  ]
}
  }
}
</code></pre>
"
Elastic APM,61067229,61066877,2,"2020/04/06, 22:11:02",True,"2020/04/06, 22:11:02",3264,1783306,1,"<p>There are basically 2 ways the agent captures things:</p>

<ul>
<li>Auto-instrumentation: in this case you don't write any code, the agent just captures things for you - this is what we see on your screenshot</li>
<li>Manual code instrumentation - for this you can use the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html"" rel=""nofollow noreferrer"">Public Agent API</a> and capture things programatically.</li>
</ul>

<p>In a typical ASP.NET Classic MVC application the agent has auto instrumentation for outgoing HTTP calls with <code>HttpClient</code>, Database calls with EF6 (Make sure to <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/setup.html#setup-ef6"" rel=""nofollow noreferrer"">add the interceptor</a>) (<code>SqlClient</code> support is already work-in-progress, hopefully released soon). So unless you have any of these within those requests, the agent won't capture things out of the box.</p>

<p>If you want to capture more things, currently the way to go is to place some agent specific code - so basically manual code instrumentation - into your application and use the public agent API.</p>
"
Elastic APM,55333285,55319800,0,"2019/03/25, 09:50:57",False,"2019/03/25, 09:50:57",3915,837717,0,"<p>I am not familiar with what is Elastic APM, but if it says it supports Spring Boot, then it means it supports any spring-boot-based framework which Spring Cloud Stream is.</p>
"
Elastic APM,66324541,66316731,1,"2021/02/23, 00:45:53",False,"2021/02/23, 00:45:53",633,13659075,1,"<p>Using labels should be the best way to add custom details to the transaction/span but you can also use the <code>addCustomContext()</code> method:</p>
<p><a href=""https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-add-custom-context"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-add-custom-context</a></p>
"
Elastic APM,66221229,66219545,0,"2021/02/16, 10:51:24",False,"2021/02/16, 10:51:24",417,2803435,0,"<p>Initially asked this questions because Visual Studio did not show the source as expected in the editor. So what I did was clicked Build after right clicking on the .sln (solution) then noticed that necessary version of .Net SDK was not there and version of MSBuild related to it. Once I updated Visual Studio the sources were visible. Hope this would help if someone faced a similar situation.</p>
"
Elastic APM,57318627,57297752,1,"2019/08/02, 03:41:47",False,"2019/08/02, 03:41:47",9394,573153,0,"<p>The <a href=""https://www.elastic.co/guide/en/apm/agent/java/current/metrics.html"" rel=""nofollow noreferrer"">JVM GC metrics tracked</a> right now are <code>jvm.gc.alloc</code>, <code>jvm.gc.time</code>, and <code>jvm.gc.count</code>.</p>

<p>If you are looking for additional ones, which ones would those be? And could you <a href=""https://github.com/elastic/apm-agent-java/issues"" rel=""nofollow noreferrer"">open an issue with the details</a>.</p>
"
Elastic APM,61072797,57297752,1,"2020/04/07, 07:20:17",False,"2020/04/07, 07:20:17",23,6332568,0,"<p>Please import from saved objects option - <a href=""https://github.com/elastic/apm-contrib/blob/master/apm-agent-java/dashboards/java_metrics_dashboard_7.x.json"" rel=""nofollow noreferrer"">https://github.com/elastic/apm-contrib/blob/master/apm-agent-java/dashboards/java_metrics_dashboard_7.x.json</a></p>
"
Elastic APM,54385310,52708201,1,"2019/01/27, 07:21:01",False,"2019/01/27, 07:21:01",1,10973722,-1,"<p>currently,we run apm-agent for java application, but after 1 day, server cpu has over 100% and java application killed by system</p>
"
Elastic APM,58087113,58076441,0,"2019/09/24, 22:36:31",True,"2019/09/24, 22:36:31",9394,573153,1,"<p>The key features of APM agents is normally in their framework integrations. The Java APM agent is mostly focussed on web frameworks — see the list of <a href=""https://www.elastic.co/guide/en/apm/agent/java/1.x/supported-technologies-details.html"" rel=""nofollow noreferrer"">supported technologies</a>.</p>

<p>But you already mentioned the <a href=""https://www.elastic.co/guide/en/apm/agent/java/1.x/public-api.html"" rel=""nofollow noreferrer"">public API</a> — if you manually instrument your code with that, you will still be able to use it. It just doesn't automatically understand the framework and you need to help it with that.</p>

<p>Alternatively, if your tool supports OpenTracing then you could use the <a href=""https://www.elastic.co/guide/en/apm/agent/java/1.x/opentracing-bridge.html"" rel=""nofollow noreferrer"">OpenTracing bridge</a> for that.</p>
"
Elastic APM,61171704,61108716,0,"2020/04/12, 15:46:20",True,"2020/04/12, 15:46:20",33,9358877,0,"<p>Finally noticed issue. </p>

<p>application API is returning service name as Bootstrap because.</p>

<pre><code>in the Response Headers we are getting bootstrapp {
  ""Content-Type"": ""application/json;charset=UTF-8"",
  ""Date"": ""Fri, 10 Apr 2020 20:01:37 GMT"",
  ""Server"": ""Jetty(9.2.13.v20150730)"",
  ""Transfer-Encoding"": ""chunked"",
  ""X-Application-Context"": ""bootstrap:31144""
}

</code></pre>

<pre><code>public static void main(String[] args) throws Exception {
        SpringApplication.run(Application.class, args);
    }
</code></pre>

<p>Id is not set so it is using default value</p>

<p>The Id can be set like this</p>

<pre><code>public static void main(String[] args) throws Exception {
        SpringApplication.run(Application.class, args).setId(""the text that you want to see int the header"");
    }

</code></pre>
"
Elastic APM,63934347,63931550,0,"2020/09/17, 11:40:33",True,"2020/09/17, 11:40:33",3264,1783306,1,"<p>One thing you can use is the <a href=""https://www.elastic.co/guide/en/apm/agent/dotnet/current/public-api.html#filter-api"" rel=""nofollow noreferrer"">Filter API</a> for this.</p>
<p>With that you have access to all transactions and spans before they are sent to the APM Server.</p>
<p>You can't run through all the spans on a given transaction, so you need some tweaking - for this I use a <code>Dictionary</code> in my sample.</p>
<pre><code>var numberOfSqlQueries = new Dictionary&lt;string, int&gt;();

Elastic.Apm.Agent.AddFilter((ITransaction transaction) =&gt;
{
    if (numberOfSqlQueries.ContainsKey(transaction.Id))
    {
        // We make an assumption here: we assume that all SQL requests on a given transaction end before the transaction ends
        // this in practice means that you don't do any &quot;fire and forget&quot; type of query. If you do, you need to make sure
        // that the numberOfSqlQueries does not leak.
        transaction.Labels[&quot;NumberOfSqlQueries&quot;] = numberOfSqlQueries[transaction.Id].ToString();
        numberOfSqlQueries.Remove(transaction.Id);
    }

    return transaction;
});

Elastic.Apm.Agent.AddFilter((ISpan span) =&gt;
{
    // you can't relly filter whether if it's done by EF Core, or another database library
    // but you have all sorts of other info like db instance, also span.subtype and span.action could be helpful to filter properly
    if (span.Context.Db != null &amp;&amp; span.Context.Db.Instance == &quot;MyDbInstance&quot;)
    {
        if (numberOfSqlQueries.ContainsKey(span.TransactionId))
            numberOfSqlQueries[span.TransactionId]++;
        else
            numberOfSqlQueries[span.TransactionId] = 1;
    }

    return span;
});
</code></pre>
<p>Couple of thing here:</p>
<ul>
<li>I assume you don't do &quot;fire and forget&quot; type of queries, if you do, you need to handle those extra</li>
<li>The counting isn't really specific to EF Core queries, but you have info like db name, database type (mssql, etc.) - hopefully based on that you'll be able filter the queries you want.</li>
<li>With <code>transaction.Labels[&quot;NumberOfSqlQueries&quot;]</code> we add a label to the given transction, and you'll be able to see this data on the transaction in Kibana.</li>
</ul>
"
Elastic APM,63291099,63129247,0,"2020/08/06, 23:12:47",False,"2020/08/06, 23:12:47",64,12032134,0,"<p>currently elastic-apm-agent support natively Quartz framework(since 1.8). If you use it, instrumentation should work. But you should add your packages to <code>application_packages</code>.
It would be good if you can share mini-demo project. And I can reproduce your problem locally.
Information from <a href=""https://www.elastic.co/guide/en/apm/agent/java/current/supported-technologies-details.html"" rel=""nofollow noreferrer"">supported-technologies-details</a></p>
<pre><code>The agent instruments the execute method of any class implementing org.quartz.Job, as well as the executeInternal method of any class extending org.springframework.scheduling.quartz.QuartzJobBean, and creates a transaction with the type scheduled, representing the job execution

NOTE: only classes from the quartz-jobs dependency will be instrumented automatically. For the instrumentation of other jobs the package must be added to the application_packages parameter.
</code></pre>
"
Elastic APM,58750060,58728047,0,"2019/11/07, 15:43:30",True,"2019/11/07, 15:51:40",26,9528014,1,"<p>When routing to a different subpage you have to set the Route Name manually. You can achieve this via a filter on the 'change-route' type. See <code>apm.addFilter()</code> 
docs: <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#apm-add-filter"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#apm-add-filter</a></p>

<p>Something like this should work:</p>

<pre><code>apm.addFilter(payload =&gt; {
  if (payload.data) {
    const mappedData = [];
    payload.data.forEach(tr =&gt; {
      if (tr.type === 'route-change')
        mappedData.push({
          ...tr,
          name: this.$route.name // overwrite unknown with the current route name
        });
      else mappedData.push(tr);
    });
    payload.data = mappedData;
  }
  return payload;
});
</code></pre>
"
Elastic APM,60703633,58728047,0,"2020/03/16, 11:47:36",False,"2020/03/16, 11:47:36",55,3588136,1,"<p>Another way to do it would be to observe for transaction events which is fired whenever transaction starts and ends. </p>

<pre><code>// on transaction start
apm.observe(""transaction:start"", transaction =&gt; {

});

// on transaction end
apm.observe(""transaction:end"", transaction =&gt; {

});
</code></pre>

<p>API docs - <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#observe"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/current/agent-api.html#observe</a></p>

<p>The agent also supports frameworks such as Vue.js, React and Angular out of the box, so the above code should not be necessary. </p>

<p>Vue docs - <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/current/vue-integration.html</a></p>
"
Elastic APM,66869872,66868273,2,"2021/03/30, 14:09:32",True,"2021/03/30, 15:56:11",163369,4604579,1,"<p>Good job so far. Your pipeline is almost good, however, the grok pattern needs some fixing and you have some orphan curly braces. Here is a working example:</p>
<pre><code>POST _ingest/pipeline/_simulate
{
  &quot;pipeline&quot;: {
    &quot;description&quot;: &quot;parse multiple patterns&quot;,
    &quot;processors&quot;: [
      {
        &quot;grok&quot;: {
          &quot;field&quot;: &quot;message&quot;,
          &quot;patterns&quot;: [
            &quot;&quot;&quot;%{TIME:logtime} %{WORD:loglevel} trace.id=%{TRACE_ID:trace.id}(?: transaction.id=%{SPAN_ID:transaction.id})? %{GREEDYDATA:message}&quot;&quot;&quot;
          ],
          &quot;pattern_definitions&quot;: {
            &quot;TRACE_ID&quot;: &quot;[0-9A-Fa-f]{32}&quot;,
            &quot;SPAN_ID&quot;: &quot;[0-9A-Fa-f]{16}&quot;
          }
        }
      },
      {
        &quot;date&quot;: {
          &quot;field&quot;: &quot;logtime&quot;,
          &quot;target_field&quot;: &quot;@timestamp&quot;,
          &quot;formats&quot;: [
            &quot;HH:mm:ss&quot;
          ]
        }
      }
    ]
  },
  &quot;docs&quot;: [
    {
      &quot;_source&quot;: {
        &quot;message&quot;: &quot;08:27:47 INF trace.id=898a7716358b25408d4f193f1cd17831 transaction.id=4f7590e4ba80b64b SOME MSG&quot;
      }
    }
  ]
}
</code></pre>
<p>Response:</p>
<pre><code>{
  &quot;docs&quot; : [
    {
      &quot;doc&quot; : {
        &quot;_index&quot; : &quot;_index&quot;,
        &quot;_type&quot; : &quot;_doc&quot;,
        &quot;_id&quot; : &quot;_id&quot;,
        &quot;_source&quot; : {
          &quot;trace&quot; : {
            &quot;id&quot; : &quot;898a7716358b25408d4f193f1cd17831&quot;
          },
          &quot;@timestamp&quot; : &quot;2021-01-01T08:27:47.000Z&quot;,
          &quot;loglevel&quot; : &quot;INF&quot;,
          &quot;message&quot; : &quot;SOME MSG&quot;,
          &quot;logtime&quot; : &quot;08:27:47&quot;,
          &quot;transaction&quot; : {
            &quot;id&quot; : &quot;4f7590e4ba80b64b&quot;
          }
        },
        &quot;_ingest&quot; : {
          &quot;timestamp&quot; : &quot;2021-03-30T11:07:52.067275598Z&quot;
        }
      }
    }
  ]
}
</code></pre>
<p>Just note that the exact date is missing so the @timestamp field resolve to January 1st this year.</p>
"
Elastic APM,61652111,61642883,0,"2020/05/07, 10:28:33",False,"2020/05/07, 10:37:54",3606,2042827,0,"<p>WebFlux can run on Servlet containers with support for the Servlet 3.1 Non-Blocking IO API as well as on other async runtimes such as <strong>Netty</strong> and Undertow.</p>

<p>Each Spring Boot web application includes an embedded web server.</p>

<p>For reactive stack applications, the <code>spring-boot-starter-webflux</code> includes Reactor Netty by default I guess. And it does not include <code>Servlet API</code> (Netty is non-Servlet runtime), but it looks like your <code>Elastic APM</code> expects this API to be present.</p>

<p>Try to use <code>spring-boot-starter-tomcat</code> instead of Netty. When <a href=""https://docs.spring.io/spring-boot/docs/2.1.1.RELEASE/reference/html/howto-embedded-web-servers.html"" rel=""nofollow noreferrer"">switching to a different HTTP server</a>, you need to exclude the default dependencies in addition to including the one you need. </p>

<p>Here is an example:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;
    &lt;exclusions&gt;
        &lt;!-- Exclude the Netty dependency --&gt;
        &lt;exclusion&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-reactor-netty&lt;/artifactId&gt;
        &lt;/exclusion&gt;
    &lt;/exclusions&gt;
&lt;/dependency&gt;
&lt;!-- Use Tomcat instead --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Tomcat dependency brings Servlet API. Perhaps it will resolve your issue.</p>
"
Elastic APM,66580927,61642883,0,"2021/03/11, 12:42:08",False,"2021/03/11, 12:42:08",47,2800145,0,"<p>Looks like there is no support from elastic for WebFlux yet</p>
<p>Check here <a href=""https://github.com/elastic/apm-agent-java/issues/60"" rel=""nofollow noreferrer"">https://github.com/elastic/apm-agent-java/issues/60</a></p>
<p>They are currently working on it, but there is not a date to be ready yet</p>
"
Elastic APM,62903745,62815678,1,"2020/07/14, 23:46:57",True,"2020/07/14, 23:46:57",1274,1370767,1,"<p>This question was cross-posted to discuss.elastic.co, and you can see the answer that was provided there: <a href=""https://discuss.elastic.co/t/elastic-apm-python-system-metrics-dont-show-process-related-metrics-like-memory-on-kibana/240531/2?u=basepi"" rel=""nofollow noreferrer"">https://discuss.elastic.co/t/elastic-apm-python-system-metrics-dont-show-process-related-metrics-like-memory-on-kibana/240531/2?u=basepi</a></p>
"
Elastic APM,59268282,57782547,8,"2019/12/10, 15:25:10",False,"2019/12/10, 15:25:10",228,4349519,1,"<p>You can start your application with argument <code>active=false</code>.</p>

<p><code>C:\Users\dsm\Documents&gt;java -jar apm-agent-attach-1.9.0-standalone.jar --pid 16832 --args 'service_name=test;server_urls=http://localhost:8200;active=false'</code></p>
"
Elastic APM,67181477,67173030,2,"2021/04/20, 18:01:19",False,"2021/04/20, 18:01:19",633,13659075,0,"<p>The best way to troubleshoot what is going on is to check if the events from Heartbeat are being collected. The Uptime application only displays events from Heartbeat, and therefore — this is the Beat that you need to check.</p>
<p>First, check the connectivity of Heartbeat and the configured output:</p>
<pre><code>metricbeat test output
</code></pre>
<p>Secondly, check if the events are being generated. You can check this by commenting out your existing output (Likely Elasticsearc/Elastic Cloud) and enabling either the <a href=""https://www.elastic.co/guide/en/beats/heartbeat/current/console-output.html"" rel=""nofollow noreferrer"">Console</a> output or the <a href=""https://www.elastic.co/guide/en/beats/heartbeat/current/file-output.html"" rel=""nofollow noreferrer"">File</a> output. Then start your Metricbeat and check if events are being generated. If they are, then it might be something with the backend side of things; maybe Elasticsearch is rejecting the documents sent and refusing to index them.</p>
<p>Apropos, Elastic is implementing a native <strong>Jenkins</strong> plugin that allows you to observe your CI pipeline using OpenTelemetry compatible backends such as <strong>Elastic APM</strong>. You can learn more about this plugin <a href=""https://github.com/jenkinsci/opentelemetry-plugin"" rel=""nofollow noreferrer"">here</a>.</p>
"
Elastic APM,65005103,64923213,0,"2020/11/25, 14:52:51",True,"2020/11/25, 14:52:51",176,14018385,0,"<p>The only way to get configuration is to check apm-server.yml in your instance, but if you want to check your agent configuration you can use Agent Configuration API, for more information check <a href=""https://www.elastic.co/guide/en/apm/server/current/agent-configuration-api.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/server/current/agent-configuration-api.html</a>.</p>
"
Elastic APM,64854473,64854472,0,"2020/11/16, 10:09:07",False,"2020/11/16, 10:09:07",31,1973221,0,"<p>I can by pass it when I comment entityframework-&gt;Interceptor node in web.config file</p>
<blockquote>
 
</blockquote>
<pre><code>  &lt;interceptor type=&quot;Elastic.Apm.EntityFramework6.Ef6Interceptor, Elastic.Apm.EntityFramework6&quot;/&gt;
&lt;/interceptors&gt;
</code></pre>
<p>And I can continue after uncomment it</p>
"
Elastic APM,63598396,63576369,1,"2020/08/26, 16:09:20",False,"2020/08/26, 16:09:20",15,12391397,1,"<p>About the possibility of using some kind of Proxy between your gke cluster and elastic apm. You can check the following link [1], to see if it can fit your necessities.</p>
<p>[1] <a href=""https://cloud.google.com/vpc/docs/special-configurations#proxyvm"" rel=""nofollow noreferrer"">https://cloud.google.com/vpc/docs/special-configurations#proxyvm</a></p>
"
Elastic APM,63480663,63480314,1,"2020/08/19, 08:40:43",True,"2020/08/19, 08:40:43",6576,547452,1,"<p>Since you didn't mention it above: did you instrument a Go application? The Elastic APM Go &quot;Agent&quot; is a package which you use to instrument your application source code. It is not an independent process, but runs within your application.</p>
<p>So, first (if you haven't already) instrument your application. See <a href=""https://www.elastic.co/guide/en/apm/agent/go/current/getting-started.html#instrumenting-source"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/go/current/getting-started.html#instrumenting-source</a></p>
<p>Here's an example web server using <a href=""https://github.com/labstack/echo"" rel=""nofollow noreferrer"">Echo</a>, and the <a href=""https://www.elastic.co/guide/en/apm/agent/go/current/builtin-modules.html#builtin-modules-apmecho"" rel=""nofollow noreferrer"">apmechov4</a> instrumentation module:</p>
<pre class=""lang-golang prettyprint-override""><code>package main

import (
        &quot;fmt&quot;
        &quot;net/http&quot;

        echo &quot;github.com/labstack/echo/v4&quot;

        &quot;go.elastic.co/apm/module/apmechov4&quot;
)

func main() {
        e := echo.New()
        e.Use(apmechov4.Middleware())
        e.GET(&quot;/hello/:name&quot;, func(c echo.Context) error {
                fmt.Println(c.Param(&quot;name&quot;))
                return nil
        })
        http.ListenAndServe(&quot;:8080&quot;, e)
}
</code></pre>
<p>If you run that and send some requests to <code>http://localhost:8080/hello/world</code>, you should soon see requests in the APM app in Kibana.</p>
<p><a href=""https://i.stack.imgur.com/zgCtv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zgCtv.png"" alt=""enter image description here"" /></a></p>
<p>If you still don't see anything in Kibana, you can follow <a href=""https://www.elastic.co/guide/en/apm/agent/go/current/troubleshooting.html#agent-logging"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/go/current/troubleshooting.html#agent-logging</a> to enable logging. Here's what you can expect to see if the agent is able to successfully send data to the server:</p>
<pre><code>$ ELASTIC_APM_LOG_FILE=stderr ELASTIC_APM_LOG_LEVEL=debug go run main.go
{&quot;level&quot;:&quot;debug&quot;,&quot;time&quot;:&quot;2020-08-19T13:33:28+08:00&quot;,&quot;message&quot;:&quot;sent request with 3 transactions, 0 spans, 0 errors, 0 metricsets&quot;}
{&quot;level&quot;:&quot;debug&quot;,&quot;time&quot;:&quot;2020-08-19T13:33:46+08:00&quot;,&quot;message&quot;:&quot;gathering metrics&quot;}
{&quot;level&quot;:&quot;debug&quot;,&quot;time&quot;:&quot;2020-08-19T13:33:56+08:00&quot;,&quot;message&quot;:&quot;sent request with 0 transactions, 0 spans, 0 errors, 3 metricsets&quot;}
</code></pre>
<p>If on the other hand the server is inaccessible, you would see something like this:</p>
<pre><code>{&quot;level&quot;:&quot;error&quot;,&quot;time&quot;:&quot;2020-08-19T13:38:01+08:00&quot;,&quot;message&quot;:&quot;config request failed: sending config request failed: Get \&quot;http://localhost:8200/config/v1/agents?service.name=main\&quot;: dial tcp 127.0.0.1:8200: connect: connection refused&quot;}
</code></pre>
"
Elastic APM,62871884,62865170,0,"2020/07/13, 11:20:58",True,"2020/07/13, 11:20:58",31,2259926,0,"<p>Sorry my bad, the server urls weren't correctly passed to docker.</p>
"
Elastic APM,60327076,60326859,0,"2020/02/20, 21:16:01",False,"2020/02/20, 21:16:01",1428,2924546,0,"<p>This is by design in Django, and it is intentionally designed in this way. this is a parametrized way.<br>
suppose someone has a column name with spaces like <code>test column name</code> then think what would happen.
it will lead to some unwanted errors, so don't change the underlying logic of the framework.</p>
"
Elastic APM,60327397,60326859,0,"2020/02/20, 21:40:06",True,"2020/02/20, 21:40:06",1,7133255,0,"<p>Thanks @BjarniRagnarsson, The upper case letters was making this behavior of framework as @Sanjay mentioned.</p>

<p>Solution:</p>

<pre><code>class Test1(models.Model):
    key = models.UUIDField('key', db_column='key', editable=False, unique=True, default=uuid.uuid4)
    name = models.CharField('name', db_column='name', max_length=128, null=False)

    class Meta:
        db_table = 'test1'
</code></pre>
"
Elastic APM,59466931,58615241,0,"2019/12/24, 11:52:49",False,"2019/12/24, 11:52:49",2686,1360278,0,"<p>For a high-level overview type of information, have a look at <a href=""https://www.elastic.co/what-is/elasticsearch-monitoring"" rel=""nofollow noreferrer"">Elastic Stack Monitoring</a>.</p>

<p>If you want to look at any monitoring in more detail, have a look at the <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html"" rel=""nofollow noreferrer"">monitoring APIs themselves</a>.</p>

<p>If you want to log this sort of information, you should set thresholds <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-slowlog.html"" rel=""nofollow noreferrer"">for your Elasticsearch slow log</a>.</p>

<p>If you want to index and then view data from the slow log, <a href=""https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-elasticsearch.html"" rel=""nofollow noreferrer"">you can always use Filebeat to ingest that slow log data back into Elasticsearch</a>.</p>
"
Elastic APM,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336,1773866,1,"<p>You're calling one method from another. Spring is creating a proxy around your method. If you call one method from another from the same class then you're not going through the proxy. Extract the method annotated with new span to a separate class and it will work fine.</p>
"
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373,10206966,14,"<p>After searching a solution for some time, I found a docker-compose.yml file which had the Jaeger Query,Agent,collector and Elasticsearch configurations. </p>

<p>docker-compose.yml</p>

<pre> <code> version: ""3""

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.3.1
    networks:
      - elastic-jaeger
    ports:
      - ""127.0.0.1:9200:9200""
      - ""127.0.0.1:9300:9300""
    restart: on-failure
    environment:
      - cluster.name=jaeger-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - transport.host=127.0.0.1
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data

  jaeger-collector:
    image: jaegertracing/jaeger-collector
    ports:
      - ""14269:14269""
      - ""14268:14268""
      - ""14267:14267""
      - ""9411:9411""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--es.num-shards=1"",
      ""--es.num-replicas=0"",
      ""--log-level=error""
    ]
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    hostname: jaeger-agent
    command: [""--collector.host-port=jaeger-collector:14267""]
    ports:
      - ""5775:5775/udp""
      - ""6831:6831/udp""
      - ""6832:6832/udp""
      - ""5778:5778""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - no_proxy=localhost
    ports:
      - ""16686:16686""
      - ""16687:16687""
    networks:
      - elastic-jaeger
    restart: on-failure
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--span-storage.type=elasticsearch"",
      ""--log-level=debug""
    ]
    depends_on:
      - jaeger-agent

volumes:
  esdata:
    driver: local

networks:
  elastic-jaeger:
    driver: bridge </code></pre>

<p>The docker-compose.yml file installs the elasticsearch, Jaeger collector,query and agent.</p>

<p>Install docker and docker compose first
<a href=""https://docs.docker.com/compose/install/#install-compose"" rel=""noreferrer"">https://docs.docker.com/compose/install/#install-compose</a></p>

<p>Then, execute these commands in order</p>

<pre><code> 
1. sudo docker-compose up -d elasticsearch

2. sudo docker-compose up -d 

3. sudo docker ps -a
</code></pre>

<p>start all the docker containers - Jaeger agent,collector,query and elasticsearch.</p>

<p>sudo docker start container-id</p>

<p>access -> <a href=""http://localhost:16686/"" rel=""noreferrer"">http://localhost:16686/</a></p>
"
Jaeger,61147810,51785812,0,"2020/04/10, 23:00:13",False,"2020/04/10, 23:00:13",71,9731647,0,"<p>If Jaeger needs to be set up in Kubernetes cluster as a helm chart, one can use this: <a href=""https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger</a>
It can delploy either Elasticsearch or Cassandara as a storage backend. Which is just a matter of right value being passed in to the chart:</p>

<pre><code>storage:
  type: elasticsearch
</code></pre>

<p>This section shows the helm command as an example:
<a href=""https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger#installing-the-chart-using-a-new-elasticsearch-cluster"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger#installing-the-chart-using-a-new-elasticsearch-cluster</a></p>
"
Jaeger,62418210,51785812,0,"2020/06/17, 00:57:19",False,"2020/06/17, 00:57:19",488,5789008,1,"<p>If you would like to deploy the Jaeger with Elasticsearch and Kibana to quickly validate and check the stack e.g. in kind or Minikube, the following snippet may help you.</p>

<pre><code>#######################
## Add jaegertracing helm repo
#######################
helm repo add jaegertracing
https://jaegertracing.github.io/helm-charts

#######################
## Create a target namespace
#######################
kubectl create namespace observability

#######################
## Check and use the jaegertracing helm chart
#######################
helm search repo jaegertracing
helm install -n observability jaeger-operator jaegertracing/jaeger-operator

#######################
## Use the elasticsearch all-in-one operator
#######################
kubectl apply -f https://download.elastic.co/downloads/eck/1.1.2/all-in-one.yaml

#######################
## Create an elasticsearch deployment
#######################
cat &lt;&lt;EOF | kubectl apply -n observability -f -
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.7.0
  nodeSets:
  - name: default
    count: 1
    config:
      node.master: true
      node.data: true
      node.ingest: true
      node.store.allow_mmap: false
EOF

PASSWORD=$(kubectl get secret -n observability quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode)
kubectl create secret -n observability generic jaeger-secret --from-literal=ES_PASSWORD=${PASSWORD} --from-literal=ES_USERNAME=elastic

#######################
## Kibana to visualize the trace data
#######################

cat &lt;&lt;EOF | kubectl apply -n observability -f -
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: quickstart
spec:
  version: 7.7.0
  count: 1
  elasticsearchRef:
    name: quickstart
EOF

kubectl port-forward -n observability service/quickstart-kb-http 5601
## To get the pw
kubectl get secret quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo

login:
https://localhost:5601
username: elastic
pw: &lt;see above to outcome of the command&gt;

#######################
## Deploy a jaeger tracing application
#######################
cat &lt;&lt;EOF | kubectl apply -n observability -f -
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-prod
spec:
  agent:
    strategy: DaemonSet
  strategy: production
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: https://quickstart-es-http:9200
        tls:
          ca: /es/certificates/ca.crt
        num-shards: 1
        num-replicas: 0
    secretName: jaeger-secret
  volumeMounts:
    - name: certificates
      mountPath: /es/certificates/
      readOnly: true
  volumes:
    - name: certificates
      secret:
        secretName: quickstart-es-http-certs-public
EOF

## to visualize it
kubectl --namespace observability port-forward simple-prod-query-&lt;POP ID&gt; 16686:16686

#######################
## To test the setup
## Of course if you set it up to another namespace it will work, the only thing that matters is the collector URL and PORT
#######################
cat &lt;&lt;EOF | kubectl apply -n observability -f -
apiVersion: v1
kind: List
items:
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: jaeger-k8s-example
    labels:
      app: jaeger-k8s-example
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: jaeger-k8s-example
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          app: jaeger-k8s-example
      spec:
        containers:
          - name: jaeger-k8s-example
            env:
            - name: JAEGER_COLLECTOR_URL
              value: ""simple-prod-collector.observability.svc.cluster.local""
            - name: JAEGER_COLLECTOR_PORT
              value: ""14268""
            image: norbertfenk/jaeger-k8s-example:latest
            imagePullPolicy: IfNotPresent
EOF
</code></pre>
"
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911,2000548,0,"<p>For people who are using OpenTelemetry, Jaeger, and Elasticsearch, here is the way.</p>
<p>Note the image being used are <code>jaegertracing/jaeger-opentelemetry-collector</code> and <code>jaegertracing/jaeger-opentelemetry-agent</code>.</p>
<pre><code>version: '3.8'

services:
  collector:
    image: otel/opentelemetry-collector:latest
    command: [&quot;--config=/conf/opentelemetry-collector.config.yaml&quot;, &quot;--log-level=DEBUG&quot;]
    volumes:
      - ./opentelemetry-collector.config.yaml:/conf/opentelemetry-collector.config.yaml
    ports:
      - &quot;9464:9464&quot;
      - &quot;55680:55680&quot;
      - &quot;55681:55681&quot;
    depends_on:
      - jaeger-collector

  jaeger-collector:
    image: jaegertracing/jaeger-opentelemetry-collector
    command: [&quot;--es.num-shards=1&quot;, &quot;--es.num-replicas=0&quot;, &quot;--es.server-urls=http://elasticsearch:9200&quot;, &quot;--collector.zipkin.host-port=:9411&quot;]
    ports:
      - &quot;14250&quot;
      - &quot;14268&quot;
      - &quot;9411&quot;
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - LOG_LEVEL=debug
    restart: on-failure
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-opentelemetry-agent
    command: [&quot;--config=/config/otel-agent-config.yml&quot;, &quot;--reporter.grpc.host-port=jaeger-collector:14250&quot;]
    volumes:
      - ./:/config/:ro
    ports:
      - &quot;6831/udp&quot;
      - &quot;6832/udp&quot;
      - &quot;5778&quot;
    restart: on-failure
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    command: [&quot;--es.num-shards=1&quot;, &quot;--es.num-replicas=0&quot;, &quot;--es.server-urls=http://elasticsearch:9200&quot;]
    ports:
      - &quot;16686:16686&quot;
      - &quot;16687&quot;
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - LOG_LEVEL=debug
    restart: on-failure
    depends_on:
      - elasticsearch

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.9.0
    environment:
      - discovery.type=single-node
    ports:
      - &quot;9200/tcp&quot;
</code></pre>
<p>Then just need</p>
<pre><code>docker-compose up -d
</code></pre>
<p>Reference: <a href=""https://github.com/jaegertracing/jaeger/blob/master/crossdock/jaeger-opentelemetry-docker-compose.yml"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger/blob/master/crossdock/jaeger-opentelemetry-docker-compose.yml</a></p>
"
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328,1221718,1,"<p>As I mentioned in my comment on the OP's first answer above, I was getting an error when running the docker-compose exactly as given:</p>
<p><code>Error: unknown flag: --collector.host-port</code></p>
<p>I think this CLI flag has been deprecated by the Jaeger folks since that answer was written. So I poked around in the jaeger-agent documentation a bit:</p>
<ol>
<li><a href=""https://www.jaegertracing.io/docs/1.20/deployment/#discovery-system-integration"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.20/deployment/#discovery-system-integration</a></li>
<li><a href=""https://www.jaegertracing.io/docs/1.20/cli/#jaeger-agent"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.20/cli/#jaeger-agent</a></li>
</ol>
<p>And I got this to work with a couple of small modifications:</p>
<ol>
<li>I added port range <code>&quot;14250:14250&quot;</code> to the jaeger-collector ports</li>
<li>I updated the jaeger-agent command input with: <code>command: [&quot;--reporter.grpc.host-port=jaeger-collector:14250&quot;]</code></li>
<li>Finally, I updated the ellastic search version in the <code>image</code> tag to the latest version they have available at this time (though I doubt this was required).</li>
</ol>
<p>The updated docker-compose.yaml:</p>
<pre><code>version: &quot;3&quot;

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.9.3
    networks:
      - elastic-jaeger
    ports:
      - &quot;127.0.0.1:9200:9200&quot;
      - &quot;127.0.0.1:9300:9300&quot;
    restart: on-failure
    environment:
      - cluster.name=jaeger-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - transport.host=127.0.0.1
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data

  jaeger-collector:
    image: jaegertracing/jaeger-collector
    ports:
      - &quot;14269:14269&quot;
      - &quot;14268:14268&quot;
      - &quot;14267:14267&quot;
      - &quot;14250:14250&quot;
      - &quot;9411:9411&quot;
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    command: [
      &quot;--es.server-urls=http://elasticsearch:9200&quot;,
      &quot;--es.num-shards=1&quot;,
      &quot;--es.num-replicas=0&quot;,
      &quot;--log-level=error&quot;
    ]
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    hostname: jaeger-agent
    command: [&quot;--reporter.grpc.host-port=jaeger-collector:14250&quot;]
    ports:
      - &quot;5775:5775/udp&quot;
      - &quot;6831:6831/udp&quot;
      - &quot;6832:6832/udp&quot;
      - &quot;5778:5778&quot;
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - no_proxy=localhost
    ports:
      - &quot;16686:16686&quot;
      - &quot;16687:16687&quot;
    networks:
      - elastic-jaeger
    restart: on-failure
    command: [
      &quot;--es.server-urls=http://elasticsearch:9200&quot;,
      &quot;--span-storage.type=elasticsearch&quot;,
      &quot;--log-level=debug&quot;
    ]
    depends_on:
      - jaeger-agent

volumes:
  esdata:
    driver: local

networks:
  elastic-jaeger:
    driver: bridge 
</code></pre>
"
Jaeger,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231,4158442,7,"<p><a href=""https://github.com/opentracing-contrib/java-spring-cloud"" rel=""noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud</a> project automatically sends standard logging to the active span. Just add the following dependency to your pom.xml</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
   &lt;artifactId&gt;opentracing-spring-cloud-starter&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Or use this <a href=""https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core"" rel=""noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core</a> starter if you want only logging integration.</p>
"
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61,9483992,0,"<p>Then I use <strong>opentracing-spring-jaeger-cloud-starter</strong></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-jaeger-cloud-starter&lt;/artifactId&gt;
    &lt;version&gt;2.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>I got just one line in console with current trace and span
i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - my_method</p>

<pre><code>2019-05-20 16:07:59.549 DEBUG 24428 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [632103eb] HTTP POST ""/api""
2019-05-20 16:07:59.552 DEBUG 24428 --- [ctor-http-nio-2] s.w.r.r.m.a.RequestMappingHandlerMapping : [632103eb] Mapped to public reactor.core.publisher.Mono&lt;org.springframework.http.ResponseEntity&lt;model.Response&gt;&gt; service.controller.method(model.Request)
2019-05-20 16:07:59.559 DEBUG 24428 --- [ctor-http-nio-2] .s.w.r.r.m.a.RequestBodyArgumentResolver : [632103eb] Content-Type:application/json
2019-05-20 16:08:01.450  INFO 24428 --- [ctor-http-nio-2] i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - method
2019-05-20 16:08:01.450 DEBUG 24428 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [632103eb] Completed 200 OK
</code></pre>

<p>Then I use <strong>spring-cloud-starter-sleuth</strong></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>I got the Trace and Spans like [my-service,90e1114e35c897d6,90e1114e35c897d6,false] in each line and it's helpfull for filebeat in ELK</p>

<pre><code>2019-05-20 16:15:38.646 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [3e578505] HTTP POST ""/api""
2019-05-20 16:15:38.662 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Received a request to uri [/api]
2019-05-20 16:15:38.667 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Handled receive of span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:38.713 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] s.w.r.r.m.a.RequestMappingHandlerMapping : [3e578505] Mapped to public reactor.core.publisher.Mono&lt;org.springframework.http.ResponseEntity&lt;model.Response&gt;&gt; service.controller.method(model.Request)
2019-05-20 16:15:38.727 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] .s.w.r.r.m.a.RequestBodyArgumentResolver : [3e578505] Content-Type:application/json
2019-05-20 16:15:39.956 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [gine-1-thread-1] .s.w.r.r.m.a.ResponseEntityResultHandler : Using 'application/json;charset=UTF-8' given [*/*] and supported [application/json;charset=UTF-8, application/*+json;charset=UTF-8, text/event-stream]
2019-05-20 16:15:40.009 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Adding a method tag with value [method] to a span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.009 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Adding a class tag with value [Controller] to a span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.010 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Handled send of NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.021 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [3e578505] Completed 200 OK
</code></pre>

<p>How could I get the same log in console using opentracing-spring-jaeger-cloud-starter ?</p>

<p>my opentracing config</p>

<pre><code>opentracing:
  jaeger:
    enabled: true
    enable-b3-propagation: true
    log-spans: true
    const-sampler:
      decision: true
    http-sender:
      url: http://jaeger-collector:14268/api/traces

</code></pre>
"
Jaeger,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11,10465104,1,"<p>Here is what I did to make jdbc related logs from Logback (Slf4j) write into Jaeger server:</p>

<p>Beginning with Logback config (logback-spring.xml):</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;configuration&gt;
&lt;include resource=""org/springframework/boot/logging/logback/defaults.xml""/&gt;
&lt;springProperty scope=""context"" name=""consoleAppender"" source=""logging.console.enabled"" defaultValue=""false""/&gt;
&lt;property name=""ENV"" value=""${SPRING_PROFILES_ACTIVE:-dev}""/&gt;

&lt;include resource=""org/springframework/boot/logging/logback/console-appender.xml""/&gt;

&lt;jmxConfigurator/&gt;

&lt;appender name=""JSON_CONSOLE"" class=""ch.qos.logback.core.ConsoleAppender""&gt;
    &lt;encoder class=""net.logstash.logback.encoder.LogstashEncoder""&gt;
        &lt;includeMdc&gt;true&lt;/includeMdc&gt;
        &lt;customFields&gt;{""log_type"":""application"",""appname"":""products-rs-load"", ""environment"": ""${ENV}""}
        &lt;/customFields&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;
&lt;appender name=""myAppender"" class=""com.test.MyAppender""&gt;
&lt;/appender&gt;
&lt;root level=""DEBUG""&gt;
    &lt;appender-ref ref=""myAppender""/&gt;
&lt;/root&gt;
&lt;logger name=""org.springframework.boot"" level=""INFO""/&gt;
&lt;logger name=""p6spy"" additivity=""false"" level=""ALL""&gt;
    &lt;appender-ref ref=""myAppender"" /&gt;
&lt;/logger&gt;
&lt;/configuration&gt;
</code></pre>

<p>Here is my appender:</p>

<pre><code>import ch.qos.logback.core.AppenderBase;
public class MyAppender extends AppenderBase {

@Override
protected void append(Object eventObject) {
    LoggingEvent event = (LoggingEvent) eventObject;

    final String loggerName = event.getLoggerName();

    // only DB related operations have to be traced:
    if (!(""p6spy"".equals(loggerName))) {
        return;
    }
    /// Tracer config is straight forward
    Span sp = TracingUtils.buildActiveChildSpan(loggerName, null);

    if (Level.ERROR.equals(event.getLevel())) {
        TracingUtils.setErrorTag(sp);
    }
    Map&lt;String, String&gt; fields = new HashMap&lt;String, String&gt;();
    fields.put(""level"", event.getLevel().toString());
    fields.put(""logger"", loggerName);
    fields.put(""content"", event.getFormattedMessage());
    sp.log(fields);

    sp.finish();
  }
}
</code></pre>
"
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537,2841947,0,"<p>I was facing similar issue. ConnectionInfo was getting traced but not the SQL statements.
In my case, I had to enable traceWithActiveSpanOnly=true.</p>
<p>For e.g, : jdbc:tracing:h2:mem:test?traceWithActiveSpanOnly=true</p>
<p>After that the statements started getting traced.</p>
<p><a href=""https://github.com/opentracing-contrib/java-jdbc"" rel=""nofollow noreferrer"">Check the documentation of opentracing java-jdbc module here</a></p>
"
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451,1432067,4,"<p>Based on my experience and reading online, I found this interesting line in Istio <a href=""https://istio.io/help/faq/mixer/"" rel=""nofollow noreferrer"">mixer faq</a></p>

<blockquote>
  <p>Mixer trace generation is controlled by command-line flags: trace_zipkin_url, trace_jaeger_url, and trace_log_spans. If any of those flag values are set, trace data will be written directly to those locations. If no tracing options are provided, Mixer will not generate any application-level trace information.</p>
</blockquote>

<p>Also, if you go deep into mixer <a href=""https://github.com/istio/istio/blob/d6c3ebcaaffd7e45772beefeeb71708ef1588cb4/install/kubernetes/helm/subcharts/mixer/templates/deployment.yaml"" rel=""nofollow noreferrer"">helm chart</a>, you will find traces of Zipkin and Jaeger signifying that it’s mixer that is passing trace info to Jaeger.</p>

<p>I also got confused which reading this line in one of the articles </p>

<blockquote>
  <p>Istio injects a sidecar proxy (Envoy) in the pod in which your application container is running. This sidecar proxy transparently intercepts (iptables magic) all network traffic going in and out of your application. Because of this interception, the sidecar proxy is in a unique position to automatically trace all network requests (HTTP/1.1, HTTP/2.0 &amp; gRPC).</p>
</blockquote>

<p>On Istio mixer documentation, The Envoy sidecar logically calls Mixer before each request to perform precondition checks, and after each request to report telemetry. The sidecar has local caching such that a large percentage of precondition checks can be performed from cache. Additionally, the sidecar buffers outgoing telemetry such that it only calls Mixer infrequently.</p>

<p><strong>Update:</strong> You can enable tracing to understand what happens to a request in Istio and also the role of mixer and envoy. Read more information <a href=""https://istio.io/help/faq/telemetry/#life-of-a-request"" rel=""nofollow noreferrer"">here</a></p>
"
Jaeger,50230156,50173643,1,"2018/05/08, 12:19:17",True,"2018/05/08, 12:19:17",193,9725840,13,"<p>I found the solution to my problem, in case anybody is facing similar issues.</p>

<p>I was missing the environment variable <em>JAEGER_SAMPLER_MANAGER_HOST_PORT</em>, which is necessary if the (default) remote controlled sampler is used for tracing.</p>

<p>This is the working docker-compose file:</p>

<pre><code>version: '2'

services:           
    demo:
            build: opentracing_demo/.
            ports: 
                    - ""8080:8080""
            environment: 
                    - JAEGER_SERVICE_NAME=hello_service
                    - JAEGER_AGENT_HOST=jaeger
                    - JAEGER_AGENT_PORT=6831     
                    - JAEGER_SAMPLER_MANAGER_HOST_PORT=jaeger:5778
    jaeger: 
            image: jaegertracing/all-in-one:latest
            ports:
                    - ""5775:5775/udp""
                    - ""6831:6831/udp""
                    - ""6832:6832/udp""
                    - ""5778:5778""
                    - ""16686:16686""
                    - ""14268:14268""
                    - ""9411:9411""
</code></pre>
"
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89,668438,0,"<p>For anyone finding themselves on this page looking at a simialr issue for jaeger opentracing in .net core below is a working docker compose snippet and working code in Startup.cs. The piece I was missing was getting jaeger to read the environment variables from docker-compose as by default it was trying to send the traces over udp on localhost.</p>
<pre><code>version: '3.4'

services:
  jaeger-agent:
    container_name: 'tracing.jaeger.agent'
    image: jaegertracing/all-in-one:latest
    networks:
        - jaeger-demo
    ports:
        - &quot;5775:5775/udp&quot;
        - &quot;6831:6831/udp&quot;
        - &quot;6832:6832/udp&quot;
        - &quot;5778:5778/tcp&quot;
        - &quot;16686:16686&quot;
        - &quot;14268:14268&quot;
        - &quot;9411:9411&quot;
    environment:
        - LOG_LEVEL=debug
    labels:
        NAME: &quot;jaeger-agent&quot;

  orderService:
    container_name: 'tracing.orders.api'
    image: ${DOCKER_REGISTRY-}orderservice.api
    build:
      context: .
      dockerfile: OrdersApi/Dockerfile
    networks:
        - jaeger-demo
    ports:
        - &quot;16252:80&quot;
    environment:
        - ASPNETCORE_ENVIRONMENT=Development
        - JAEGER_SERVICE_NAME=OrdersApi
        - JAEGER_AGENT_HOST=jaeger-agent
        - JAEGER_AGENT_PORT=6831
        - JAEGER_SAMPLER_TYPE=const
        - JAEGER_SAMPLER_PARAM=1
    depends_on: 
      - jaeger-agent

  customerService:
    container_name: 'tracing.customers.api'
    image: ${DOCKER_REGISTRY-}customerservice.api
    build:
      context: .
      dockerfile: CustomerApi/Dockerfile
    networks:
        - jaeger-demo
    ports:
        - &quot;17000:80&quot;
    environment:
        - ASPNETCORE_ENVIRONMENT=Development
        - JAEGER_SERVICE_NAME=CustomersApi
        - JAEGER_AGENT_HOST=jaeger-agent
        - JAEGER_AGENT_PORT=6831
        - JAEGER_SAMPLER_TYPE=const
        - JAEGER_SAMPLER_PARAM=1
    depends_on: 
      - jaeger-agent

networks: 
    jaeger-demo:
</code></pre>
<p>Add following nuget packages to your api's.</p>
<pre><code>&lt;PackageReference Include=&quot;OpenTracing.Contrib.NetCore&quot; Version=&quot;0.6.2&quot; /&gt;
&lt;PackageReference Include=&quot;Jaeger&quot; Version=&quot;0.4.2&quot; /&gt;
</code></pre>
<p>Then inside your Startup.cs Configure method, you will need to configure jaeger and opentracing as below.</p>
<pre><code>     services.AddSingleton&lt;ITracer&gt;(serviceProvider =&gt;
     {
        ILoggerFactory loggerFactory = serviceProvider.GetRequiredService&lt;ILoggerFactory&gt;();
     
        Jaeger.Configuration.SenderConfiguration.DefaultSenderResolver = new SenderResolver(loggerFactory).RegisterSenderFactory&lt;ThriftSenderFactory&gt;();
     
        var config = Jaeger.Configuration.FromEnv(loggerFactory);
     
        ITracer tracer = config.GetTracer();
     
        GlobalTracer.Register(tracer);
     
        return tracer;
     });
     
     services.AddOpenTracing();
</code></pre>
"
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192,4275342,2,"<p>The jaeger-agent service should look like</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: jaeger-agent
  namespace: jaeger
  labels:
    app: jaeger
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/component: agent
spec:
  type: LoadBalancer
  ports:
  - name: agent-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  - name: agent-binary
    port: 6832
    protocol: UDP
    targetPort: 6832
  - name: agent-configs
    port: 5778
    protocol: TCP
    targetPort: 5778
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/component: all-in-one
</code></pre>

<p>Don't use IP, use FQDN.
First, try to hardcode value for <code>jaegerHost</code> </p>

<pre><code>string jaegerHost = ""jaeger-agent.jaeger"";
</code></pre>

<p>where <code>jaeger-agent</code> - service name, <code>jaeger</code> - namespace of service</p>

<p>Also, you should create <code>jaeger-collector</code> service </p>
"
Jaeger,59369990,56895100,1,"2019/12/17, 10:11:44",False,"2019/12/17, 10:16:44",156,10129198,2,"<pre><code>from jaeger_client import Config
def get_tracer(service=""Vienna""):

    config = Config(
        config={
            'sampler': {
                'type': 'const',
                'param': 1,
            },
            'logging': True,
        },
        service_name=service,
     )

    tracer = config.initialize_tracer()
    if tracer is None:
        Config._initialized = False
        tracer = config.initialize_tracer()
    return tracer
</code></pre>
"
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313,524946,2,"<p>Are you closing the tracer and the scope? If you are using a version before 0.32.0, you should manually call <code>tracer.close()</code> before your process terminates, otherwise the spans in the buffer might not get dispatched.</p>

<p>As for the scope, it's common to wrap it in a try-with-resources statement:</p>

<pre><code>try (Scope scope = tracer.buildSpan(""parent-span"").startActive(true)) {
  Tags.SAMPLING_PRIORITY.set(scope.span(), 1);
  scope.span().setTag(""this-is-test"", ""YUP"");

  logging.report((JaegerSpan) scope.span());
}
</code></pre>

<p>You might also want to check the OpenTracing tutorial at <a href=""https://github.com/yurishkuro/opentracing-tutorial"" rel=""nofollow noreferrer"">https://github.com/yurishkuro/opentracing-tutorial</a> or the Katacoda-based version at <a href=""https://www.katacoda.com/courses/opentracing"" rel=""nofollow noreferrer"">https://www.katacoda.com/courses/opentracing</a></p>

<p><strong>-- EDIT</strong></p>

<blockquote>
  <p>and is deployed on a different hostname and port</p>
</blockquote>

<p>Then you do need to tell the tracer where to send the traces. Either export the <code>JAEGER_ENDPOINT</code> environment variable, pointing to a collector endpoint, or set <code>JAEGER_AGENT_HOST</code>/<code>JAEGER_AGENT_PORT</code>, with the location of the agent. You can check the available environment variables for your client on the following URL: <a href=""https://www.jaegertracing.io/docs/1.7/client-features/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.7/client-features/</a></p>
"
Jaeger,47756263,47706011,7,"2017/12/11, 17:35:05",True,"2017/12/11, 17:35:05",91,629338,2,"<p>You need to add some more properties to your config options. For reporter deployed on localhost and local sampler strategy :</p>

<pre><code>var config = {
  'serviceName': 'my-awesome-service',
  'reporter': {
    'logSpans': true,
    'agentHost': 'localhost',
    'agentPort': 6832
  },
  'sampler': {
    'type': 'probabilistic',
    'param': 1.0
  }
};
</code></pre>

<p>Replace <code>localhost</code> by server or route name to target another host for Jeager runtime.</p>
"
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51,8394088,0,"<p>This link (<a href=""https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/"" rel=""nofollow noreferrer"">https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/</a>) provides the details of how to enable jaeger traces.</p>
<p>The simplest way to enable jaeger to spring-boot application is add the dependency and the required properties.</p>
<hr />
<p><strong>Dependency:</strong></p>
<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
   &lt;artifactId&gt;opentracing-spring-jaeger-web-starter&lt;/artifactId&gt;
   &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p><strong>Example Properties</strong></p>
<pre><code>opentracing.jaeger.udp-sender.host=localhost
opentracing.jaeger.udp-sender.port=6831
opentracing.jaeger.const-sampler.decision=true
opentracing.jaeger.enabled=true
opentracing.jaeger.log-spans=true
opentracing.jaeger.service-name=ms-name
</code></pre>
"
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36,4216591,1,"<p>Answering to your question about dependencies it is explained here in Dependencies section (<a href=""https://github.com/opentracing-contrib/java-spring-jaeger"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-spring-jaeger</a>):</p>
<blockquote>
<p>The opentracing-spring-jaeger-web-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-web-starter This means that by including it, simple web Spring Boot microservices include all the necessary dependencies to instrument Web requests / responses and send traces to Jaeger.</p>
<p>The opentracing-spring-jaeger-cloud-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-cloud-starter This means that by including it, all parts of the Spring Cloud stack supported by Opentracing will be instrumented</p>
</blockquote>
<p>And by the way:</p>
<ul>
<li>opentracing.jaeger.log-spans is true by default</li>
</ul>
<p>same as:</p>
<ul>
<li>opentracing.jaeger.udp-sender.host=localhost</li>
<li>opentracing.jaeger.udp-sender.port=6831</li>
<li>opentracing.jaeger.const-sampler.decision=true</li>
<li>opentracing.jaeger.enabled=true</li>
</ul>
"
Jaeger,61154134,61154096,3,"2020/04/11, 11:43:10",False,"2020/04/11, 11:43:10",27576,1839482,2,"<p>You can use <a href=""https://www.jaegertracing.io/docs/1.16/operator/"" rel=""nofollow noreferrer"">Jaeger Operator</a> to deploy Jaeger on kubernetes.The Jaeger Operator is an implementation of a Kubernetes Operator. Operators are pieces of software that ease the operational complexity of running another piece of software. More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application</p>
"
Jaeger,61154180,61154096,0,"2020/04/11, 11:47:26",True,"2020/04/11, 22:39:56",382,7847042,1,"<p>Follow this link for steps to deploy JAEGER on kubernetes .</p>

<blockquote>
  <p><a href=""https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/"" rel=""nofollow noreferrer"">https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/</a></p>
</blockquote>

<p>make following changes in application.properties</p>

<pre><code>opentracing.jaeger.udp-sender.host=&lt;load_balancer_ip&gt; of your jaeger service
opentracing.jaeger.http-sender.url=http://&lt;jaeger-collector-service-name&gt;:port/api/traces
</code></pre>
"
Jaeger,61160646,61154096,0,"2020/04/11, 20:07:08",False,"2020/04/11, 20:07:08",181,12530105,1,"<p>You can use this link for a better understanding - <a href=""https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/"" rel=""nofollow noreferrer"">https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/</a></p>
"
Jaeger,61152144,61149872,7,"2020/04/11, 07:27:43",False,"2020/04/11, 10:29:27",27576,1839482,1,"<p>You add <code>opentracing-spring-jaeger-starter</code> <a href=""https://github.com/opentracing-contrib/java-spring-jaeger"" rel=""nofollow noreferrer"">library</a> into the project which simply contains the code needed to provide a Jaeger implementation of the OpenTracing's <code>io.opentracing.Tracer</code> interface.</p>

<p>Since you have the jaeger deployed in Kubernetes and exposed it via a loadbalancer service you can use the loadbalancer IP and port to connect to it from outside the Kubernetes cluster.</p>
"
Jaeger,61154289,61149872,1,"2020/04/11, 11:56:40",False,"2021/04/10, 03:12:05",382,7847042,1,"<p>So the solution that works for me is -
I have made the following changes in my application.properties file of application</p>
<pre><code>opentracing.jaeger.udp-sender.host=http://&lt;load_balancer_ip_of_jaeger_service&gt;:&lt;expose_port&gt;(example &quot;:80&quot;)
opentracing.jaeger.http-sender.url=http://&lt;cluster_ip_or_load_balancer_ip_of jaeger_collector&gt;:&lt;expose_port&gt;/api/traces(example of expose port &quot;:1468&quot;)
</code></pre>
"
Jaeger,62832644,61149872,3,"2020/07/10, 13:45:36",False,"2020/07/10, 13:45:36",9,8694436,0,"<p>You should config:</p>
<pre><code>opentracing.jaeger.udp-sender.host=jaeger-agent
opentracing.jaeger.udp-sender.port=6831
</code></pre>
"
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471,2673284,0,"<p>This is why tracing is a useful tool, it often reveals issues like this that you wouldn't suspect otherwise. If your application is using async framework, these gaps may indicate execution waiting on available threads. Or your application may be CPU throttled during and between the spans. You cannot really explain the gaps from the trace itself, but you surely do have them. Time to whip out the profiler.</p>
"
Jaeger,55138790,53453160,1,"2019/03/13, 11:48:49",True,"2019/03/13, 11:48:49",1678,2104921,1,"<p>Turns out that <strong>Feign</strong> clients are <strong>currently not supported</strong> or to be precise, the spring startes do not configure the Feign clients accordingly. If you want to use Jaeger with your Feign clients, you have to provide an integration of your own. </p>

<p>In my experience so far, the jaeger community is a lesser supportive one, thus you have to gain such knowledge on your own, which in my opinion is a big downside and you should probably consider, using an alternative. </p>
"
Jaeger,63367857,53453160,0,"2020/08/12, 02:50:32",False,"2020/08/12, 02:50:32",1,8149668,0,"<p>In some cases it might be necessary to explicitly expose the <strong>Feign Client</strong> in the Spring configuration, in order to get the traceId propagated. This can be done easily by adding the following into one of your configuration classes</p>
<pre><code>  @Bean
  public Client feignClient() {
    return new Client.Default(null, null);
  }
</code></pre>
"
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463,1749786,1,"<p>The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT! </p>

<p>Jaegar has the ability to collect Zipkin spans:</p>

<p><a href=""https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin</a></p>

<p>You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>The above will send Zipkin spans to <a href=""http://localhost:9411"" rel=""nofollow noreferrer"">http://localhost:9411</a> by default. You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.</p>

<blockquote>
  <p>spring.zipkin.base-url=<a href=""http://your-jaegar-server:9411"" rel=""nofollow noreferrer"">http://your-jaegar-server:9411</a></p>
</blockquote>

<p>Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.</p>

<p>In the log4j2.xml file, all you have to mention is</p>

<blockquote>
  <p>[%X]</p>
</blockquote>

<p>You can find the sample code here:</p>

<p><a href=""https://github.com/anoophp777/spring-webflux-jaegar-log4j2"" rel=""nofollow noreferrer"">https://github.com/anoophp777/spring-webflux-jaegar-log4j2</a></p>
"
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284,4828509,3,"<p>Some web frameworks return empty string if a non-existent header is queried. I have seen this in Spring Boot and KoaJS.</p>

<p>If any of the tracing headers is not sent by Istio, this header logic causes us to send empty string for those non-existent headers which breaks tracing.</p>

<p>My suggestion is after getting the values for headers filter out the ones with empty string as their values and propogate the remaining ones.</p>
"
Jaeger,51929991,51838896,0,"2018/08/20, 14:39:54",False,"2018/08/20, 14:39:54",116,7510189,0,"<ol>
<li>Check if your valid redirect URIs within Keycloak are correct. Add * if you want to make sure, that's not the problem; for security reasons it should be as exact as possible in production.</li>
<li>Your proxy.json constrains access to the role ""application"". Check if that role has been added within Keycloak to the Role Mapping.</li>
</ol>

<p>Also, do you get an Error Message? If so, please post it.</p>
"
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280,1057291,3,"<p>BINGO!</p>

<p>We need to setup the ReporterConfigurations as below. previously my ones were default ones that's why it always connected to local.</p>

<pre><code>return new Configuration(""MyApplication"", 
        new Configuration.SamplerConfiguration(ProbabilisticSampler.TYPE, 1, ""server2.mycompany.com:5778""),
        new Configuration.ReporterConfiguration(false, ""server2.mycompany.com"",6831,1000,100))
        .getTracer();
</code></pre>

<p>Even better, you can create the Configuration from Environment as below providing the environment variables as below</p>

<pre><code>return Configuration.fromEnv().getTracer();
</code></pre>

<p>You can provide this when you run the docker container</p>

<blockquote>
  <p>-e JAVA_OPTS=""</p>

<pre><code> -DJAEGER_SAMPLER_TYPE=probabilistic
 -DJAEGER_SAMPLER_PARAM=1
 -DJAEGER_SAMPLER_MANAGER_HOST_PORT=server2.mycompany.com:5778
 -DJAEGER_REPORTER_LOG_SPANS=false
 -DJAEGER_AGENT_HOST=server2.mycompany.com
 -DJAEGER_AGENT_PORT=6831
 -DJAEGER_REPORTER_FLUSH_INTERVAL=1000
 -DJAEGER_REPORTER_MAX_QUEUE_SIZE=100
 -DJAEGER_SERVICE_NAME=MyApplicationNameX 
</code></pre>
  
  <p>"" ....</p>
</blockquote>
"
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11,12478106,1,"<p>Step 1 : First we need to configure remote host address and port.</p>

<pre><code>    private static final int JAEGER_PORT = HOST_PORT;
    private static final String JAEGER_HOST = ""HOST_IP"";
</code></pre>

<p>Step 2 : configure sender configuration and pass the remote host and port in                           withAgentHost, withAgentPort.</p>

<p>SenderConfiguration senderConfig = Configuration.SenderConfiguration.fromEnv()
                    .withAgentHost(JAEGER_HOST)
                    .withAgentPort(JAEGER_PORT);</p>

<p>Step 3 : Pass sender configuration in reporter configuration</p>

<p>Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv()
                .withLogSpans(true)
                .withSender(senderConfig);</p>

<pre><code>package com.studies.StudyService;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

import io.jaegertracing.Configuration;
import io.jaegertracing.Configuration.SenderConfiguration;
import io.jaegertracing.internal.samplers.ProbabilisticSampler;
import io.opentracing.Tracer;

@SpringBootApplication
//@EnableDiscoveryClient

public class StudyServiceApplication {

    public static void main(String[] args) {
        SpringApplication.run(StudyServiceApplication.class, args);
    }
    /* Configure sender host and port details */
        private static final int JAEGER_PORT = HOST_PORT;
        private static final String JAEGER_HOST = ""HOST_IP"";
    /* End */
    @Bean
    public Tracer getTracer() {

        Configuration.SamplerConfiguration samplerConfig = Configuration.SamplerConfiguration.fromEnv()
                .withType(ProbabilisticSampler.TYPE).withParam(1);

        /* Update default sender configuration with custom host and port */
            SenderConfiguration senderConfig = Configuration.SenderConfiguration.fromEnv()
                    .withAgentHost(JAEGER_HOST)
                    .withAgentPort(JAEGER_PORT);
        /* End */

        Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv()
                .withLogSpans(true)
                .withSender(senderConfig);

        Configuration config = new Configuration(""Service_Name"").withSampler(samplerConfig)
                .withReporter(reporterConfig);

        return config.getTracer();
    }
}
</code></pre>
"
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313,524946,0,"<p>This seems a bit old and it's a bit hard to tell what's wrong. My first guess would be the sampling strategy, as Jaeger samples one trace in one thousand, but looks like you did set it.</p>

<p><code>
JAEGER_SAMPLER_TYPE=const
JAEGER_SAMPLER_PARAM=1
</code></p>

<p>I would recommend you start by using a simple <code>Configuration.fromEnv().getTracer()</code> to get your tracer. Then, control it via env vars, probably setting <code>JAEGER_REPORTER_LOG_SPANS</code> to <code>true</code>. With this option, you should be able to see in the logs whenever Jaeger emits a span.</p>

<p>You can also set the <code>--log-level=debug</code> option to the agent and collector (or all-in-one, in your case) to see when these components receive a span from a client.</p>
"
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190,1733929,1,"<p>You can see that jaeger-query configuration includes: SPAN_STORAGE_TYPE: &quot;kafka&quot;</p>
<p>The error indicates that a kafka client used by jaeger-query to store spans in Kafka cannot in fact reach Kafka, and therefore the jaeger storage factory fails to initialize.</p>
<p>This can be either because Kafka failed to start (did you check)? Or a misconfig of the network in your docker.</p>
"
Jaeger,64572391,64391505,0,"2020/10/28, 13:57:45",True,"2020/10/28, 13:57:45",1101,1141160,1,"<p>I was missing a lot of information. I managed to get it working:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.8'

services:
  
  zookeeper:
    image: confluentinc/cp-zookeeper
    networks:
      - kafka-net
    container_name: zookeeper
    environment:
        ZOOKEEPER_CLIENT_PORT: 2181
    ports:
        - 2181:2181

  cassandra:
    hostname: cassandra
    image: cassandra
    networks:
      - kafka-net
    environment:
      MAX_HEAP_SIZE: 1G
      HEAP_NEWSIZE: 256M
    ports:
     - &quot;9042:9042&quot;

  cassandra-schema:
    image: jaegertracing/jaeger-cassandra-schema
    networks:
      - kafka-net
    depends_on:
      - cassandra

  kafka:
    image: confluentinc/cp-kafka
    networks:
      - kafka-net
    container_name: kafka
    environment:
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        ALLOW_PLAINTEXT_LISTENER: &quot;yes&quot;
        KAFKA_LISTENERS-INTERNAL: //kafka:29092,EXTERNAL://localhost:9092
        KAFKA_ADVERTISED: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
        KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
    ports:
        - 9092:9092
        - 29092:29092
    depends_on:
        - zookeeper
    restart: on-failure
  
  jaeger-collector:
    image: jaegertracing/jaeger-collector
    container_name: jaeger-collector
    networks:
      - kafka-net
    ports:
      - &quot;14250:14250&quot;
      - &quot;14267:14267&quot;
      - &quot;14268:14268&quot; # HTTP collector port to receive spans
      - &quot;14269:14269&quot; # HTTP health check port
    restart: on-failure
    environment:
      LOG_LEVEL: &quot;debug&quot;
      SPAN_STORAGE_TYPE: &quot;kafka&quot;
      KAFKA_BROKERS: &quot;kafka:9092&quot;
      KAFKA_PRODUCER_BROKERS: &quot;kafka:29092&quot;

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    container_name: jaeger-agent
    networks:
      - kafka-net
    command: [&quot;--reporter.grpc.host-port=jaeger-collector:14250&quot;]
    ports:
      - &quot;5775:5775/udp&quot;
      - &quot;6831:6831/udp&quot;
      - &quot;6832:6832/udp&quot;
      - &quot;5778:5778&quot;
    environment:
      LOG_LEVEL: &quot;debug&quot;
      SPAN_STORAGE_TYPE: &quot;kafka&quot;
    restart: on-failure
    depends_on:
      - jaeger-collector

  jaeger-ingester:
    image: jaegertracing/jaeger-ingester
    container_name: jaeger-ingester
    networks:
      - kafka-net
    ports:
      - &quot;14270:14270&quot; # HTTP health check port: http://localhost:14270/
      - &quot;14271:14271&quot; # Metrics port: http://localhost:14270/metrics
    restart: on-failure
    environment:
      LOG_LEVEL: debug
      INGESTER_PARALLELISM: 1
      INGESTER_DEADLOCKINTERVAL: ms
      SPAN_STORAGE_TYPE: cassandra
      CASSANDRA_SERVERS: cassandra
      CASSANDRA_KEYSPACE: jaeger_v1_dc1
      METRICS_BACKEND: expvar
      KAFKA_CONSUMER_BROKERS: kafka:29092
      KAFKA_CONSUMER_TOPIC: jaeger-spans
    depends_on:
      - cassandra-schema

  jaeger-query:
    image: jaegertracing/jaeger-query
    container_name: jaeger-query
    networks:
      - kafka-net
    ports:
      - &quot;16686:16686&quot; # Jaeger UI port
      - &quot;16687:16687&quot; # HTTP health check port: http://localhost:16687/
    restart: on-failure
    depends_on:
      - cassandra-schema
    environment:
      LOG_LEVEL: debug
      SPAN_STORAGE_TYPE: cassandra
      CASSANDRA_SERVERS: cassandra
      CASSANDRA_KEYSPACE: jaeger_v1_dc1
      JAEGER_ENDPOINT: http://jaeger-collector:14268/api/traces

networks:
  kafka-net:
    driver: bridge

</code></pre>
"
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656,3503019,0,"<p><strong>Update</strong>:</p>
<p>I got the same exception (<code>Tracer bean is not configured!..</code>) when I use your version of spring cloud jaeger dependencies. This is irrespective of the <code>RxJava</code> package.</p>
<p>I think you can directly use <code>opentracing-spring-jaeger-cloud-starter</code> which is a combination of <code>opentracing-spring-cloud-starter</code> and <code>opentracing-spring-jaeger-starter</code>. Read <a href=""https://github.com/opentracing-contrib/java-spring-jaeger/blob/master/README.md"" rel=""nofollow noreferrer"">this details</a> for java spring jaeger.</p>
<blockquote>
<p>The opentracing-spring-jaeger-cloud-starter starter is convenience
starter that includes both opentracing-spring-jaeger-starter and
opentracing-spring-cloud-starter This means that by including it, all
parts of the Spring Cloud stack supported by Opentracing will be
instrumented</p>
</blockquote>
<p><strong>Note</strong>:</p>
<p>Maybe RxJava tracing won't work without registering the tracer using the decorators provided by <a href=""https://github.com/opentracing-contrib/java-rxjava/blob/master/README.md"" rel=""nofollow noreferrer"">opentracing-contrib</a>. Please see the working app <a href=""https://github.com/akashsolanki/rxjava-jeager"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I have followed <a href=""https://spring.io/guides/gs/reactive-rest-service/"" rel=""nofollow noreferrer"">this spring guide</a> for Reactive Restful webservice and <code>jaeger</code> worked with below <code>pom.xml</code> without any <code>Tracer bean</code> -</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.spring.opentrace&lt;/groupId&gt;
    &lt;artifactId&gt;jaeger&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;jaeger&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;

    &lt;properties&gt;
        &lt;java.version&gt;11&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Hoxton.SR6&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;
        &lt;/dependency&gt;
        
        &lt;dependency&gt;
          &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
          &lt;artifactId&gt;opentracing-spring-jaeger-cloud-starter&lt;/artifactId&gt;
          &lt;version&gt;3.2.0&lt;/version&gt;
        &lt;/dependency&gt;
       &lt;dependency&gt;
          &lt;groupId&gt;io.reactivex&lt;/groupId&gt;
          &lt;artifactId&gt;rxjava&lt;/artifactId&gt;
          &lt;version&gt;1.3.8&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt;
                    &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>
"
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111,2035350,0,"<p>Thing is, you should use opentelemetry collector if you use opentelemetry exporter.</p>
<p>Pls see schema in attachment <a href=""https://i.stack.imgur.com/i2yfZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i2yfZ.png"" alt=""enter image description here"" /></a></p>
<p>Also I created a gist, which will help you to setup
pls see</p>
<p><a href=""https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f"" rel=""nofollow noreferrer"">https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f</a></p>
<p>(just tune the values, export to jaeger-all-in-one instead of separate + cassandra, etc)</p>
"
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471,2673284,4,"<p>From the official FAQ (<a href=""https://www.jaegertracing.io/docs/latest/faq/#do-i-need-to-run-jaeger-agent"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/latest/faq/#do-i-need-to-run-jaeger-agent</a>):</p>

<p><code>jaeger-agent</code> is not always necessary. Jaeger client libraries can be configured to export trace data directly to <code>jaeger-collector</code>. However, the following are the reasons why running <code>jaeger-agent</code> is recommended:</p>

<ul>
<li>If we want Jaeger client libraries to send trace data directly to collectors, we must provide them with a URL of the HTTP endpoint. It means that our applications require additional configuration containing this parameter, especially if we are running multiple Jaeger installations (e.g. in different availability zones or regions) and want the data sent to a nearby installation. In contrast, when using the agent, the libraries require no additional configuration because the agent is always accessible via localhost. It acts as a sidecar and proxies the requests to the appropriate collectors.</li>
<li>The agent can be configured to enrich the tracing data with infrastructure-specific metadata by adding extra tags to the spans, such as the current zone, region, etc. If the agent is running as a host daemon, it will be shared by all applications running on the same host. If the agent is running as a true sidecar, i.e. one per application, it can provide additional functionality such as strong authentication, multi-tenancy (see <a href=""https://medium.com/jaegertracing/jaeger-and-multitenancy-99dfa1d49dc0"" rel=""nofollow noreferrer"">this blog post</a>), pod name, etc.
If we want Jaeger client libraries to use sampling strategies that are centrally configured in the collectors, this is only possible by using the /sampling HTTP endpoint on the agent. There is no technical reason why this endpoint cannot be implemented directly in the collectors, it's just <a href=""https://github.com/jaegertracing/jaeger/issues/1971"" rel=""nofollow noreferrer"">not done yet</a>.</li>
<li>Agents allow implementing traffic control to the collectors. If we have thousands of hosts in the data center, each running many applications, and each application sending data directly to the collectors, there may be too many open connections for each collector to handle. The agents can load balance this traffic with fewer connections.</li>
</ul>
"
Jaeger,58511794,58511640,0,"2019/10/22, 23:25:32",True,"2019/10/22, 23:39:29",34894,3288890,0,"<p>If you check the your <a href=""https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/Dockerfile"" rel=""nofollow noreferrer"">base image</a> it from scratch.</p>

<pre><code>FROM scratch
EXPOSE 8080 8081 8082 8083
COPY hotrod-linux /go/bin/
ENTRYPOINT [""/go/bin/hotrod-linux""]
CMD [""all""]
</code></pre>

<p>So there is no <strong>Bash, ash</strong> as the image is from scratch so it will only cotnain <code>hotrod-linux</code>. </p>

<p>To get sh or bash in such cases you need to use multi-stage Dockerfile, you can use the base image in Dockerfile and then copy the binaries from the base image in multi-stage in Dockerfile. Here you go</p>

<pre><code>FROM jaegertracing/example-hotrod:latest as base
FROM alpine
COPY --from=base /go/bin/hotrod-linux /go/bin/hotrod-linux
ENTRYPOINT [""/go/bin/hotrod-linux""]
CMD [""all""]
</code></pre>

<p>so now you can build and test and you will able to run command inside container using docker exec, here is the example </p>

<pre><code>docker build -t myimage .

docker run -dit --name test myimage

#now run
docker exec -it test ash
</code></pre>
"
Jaeger,58511799,58511640,0,"2019/10/22, 23:25:50",False,"2019/10/22, 23:25:50",76,12259556,2,"<pre><code>FROM scratch
EXPOSE 8080 8081 8082 8083
COPY hotrod-linux /go/bin/
ENTRYPOINT [""/go/bin/hotrod-linux""]
CMD [""all""]
</code></pre>

<p><a href=""https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/Dockerfile</a></p>

<p>As I can see hotrod image was built from scratch image. And from the docker hub:</p>

<blockquote>
  <p>""an explicitly empty image, especially for building images ""FROM
  scratch""...</p>
  
  <p>""This image is most useful in the context of building base images
  (such as debian and busybox) or super minimal images (that contain
  only a single binary and whatever it requires, such as hello-world).""</p>
</blockquote>

<p><a href=""https://hub.docker.com/_/scratch"" rel=""nofollow noreferrer"">https://hub.docker.com/_/scratch</a></p>

<p>So, I think there is not bash inside this image</p>
"
Jaeger,57986473,57976158,0,"2019/09/18, 09:36:52",True,"2019/09/18, 09:36:52",31,10489752,1,"<p>'reporting_host' must be not 'localhost', but 'jaeger', just as service in docker-compose.yml called.
<code>'reporting_host' =&gt; 'jaeger',</code></p>

<p>Also, I needed to add <code>$tracer-&gt;flush();</code> after all, it closes all the entities and does sending via UDP behind the scenes. </p>
"
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084,2915603,1,"<p>So, answering my own question. Jaeger does not support cross system spans. Every sub-system is responsible for its own span in the whole system. 
For reference, check this answer <a href=""https://github.com/opentracing/specification/issues/143"" rel=""nofollow noreferrer"">https://github.com/opentracing/specification/issues/143</a></p>
"
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678,3349358,5,"<p>If anyone else would like to set up Jaeger in spring project, here's what I did:</p>

<p>Add dependencies to pom:</p>

<pre><code>&lt;properties&gt;
    &lt;opentracing.spring.web.version&gt;0.3.4&lt;/opentracing.spring.web.version&gt;
    &lt;opentracing.jdbc.version&gt;0.0.12&lt;/opentracing.jdbc.version&gt;
    &lt;opentracing.spring.configuration.starter.version&gt;0.1.0&lt;/opentracing.spring.configuration.starter.version&gt;
    &lt;opentracing.spring.jaeger.starter.version&gt;0.2.2&lt;/opentracing.spring.jaeger.starter.version&gt;
&lt;/properties&gt;

&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-web-starter&lt;/artifactId&gt;
    &lt;version&gt;${opentracing.spring.web.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-jdbc&lt;/artifactId&gt;
    &lt;version&gt;${opentracing.jdbc.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-tracer-configuration-starter&lt;/artifactId&gt;
    &lt;version&gt;${opentracing.spring.configuration.starter.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-jaeger-starter&lt;/artifactId&gt;
    &lt;version&gt;${opentracing.spring.jaeger.starter.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Set up you web.xml to register new tracing filter <em>tracingFilter</em> to intercept REST API:</p>

<pre><code>&lt;filter&gt;
    &lt;filter-name&gt;tracingFilter&lt;/filter-name&gt;
    &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;
    &lt;async-supported&gt;true&lt;/async-supported&gt;
&lt;/filter&gt;
&lt;filter-mapping&gt;
    &lt;filter-name&gt;tracingFilter&lt;/filter-name&gt;
    &lt;url-pattern&gt;/api/*&lt;/url-pattern&gt;
&lt;/filter-mapping&gt;
</code></pre>

<p>Register jaeger tracer in spring mvc:</p>

<pre><code>&lt;mvc:interceptors&gt;
    &lt;mvc:interceptor&gt;
        &lt;mvc:mapping path=""/**"" /&gt;
        &lt;bean class=""io.opentracing.contrib.spring.web.interceptor.TracingHandlerInterceptor""&gt;
            &lt;constructor-arg ref=""jaegerTracer"" /&gt;
        &lt;/bean&gt;
    &lt;/mvc:interceptor&gt;
&lt;/mvc:interceptors&gt;
</code></pre>

<p>Set up the <em>tracingFilter</em> bean we described in web.xml:</p>

<pre><code>import io.opentracing.Tracer;
import io.opentracing.contrib.web.servlet.filter.TracingFilter;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;


@Configuration
public class JaegerFilterConfiguration {

    @Bean
    public TracingFilter tracingFilter(Tracer tracer) {
        return new TracingFilter(tracer);
    }
}
</code></pre>

<p>Finally define jaeger tracer spring configuration:</p>

<pre><code>import io.jaegertracing.internal.JaegerTracer;
import io.jaegertracing.internal.metrics.NoopMetricsFactory;
import io.jaegertracing.internal.reporters.RemoteReporter;
import io.jaegertracing.internal.reporters.RemoteReporter.Builder;
import io.jaegertracing.internal.samplers.ProbabilisticSampler;
import io.jaegertracing.thrift.internal.senders.UdpSender;
import io.opentracing.Tracer;
import io.opentracing.util.GlobalTracer;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.ApplicationListener;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.context.event.ContextRefreshedEvent;


@Configuration
public class JaegerConfiguration implements ApplicationListener&lt;ContextRefreshedEvent&gt; {


    private static final int JAEGER_PORT = 6831;
    private static final String JAEGER_HOST = ""localhost"";
    private static final String JAEGER_SERVICE_NAME = ""my-awesome-jaeger"";
    private static final double SAMPLING_RATE = 0.5;
    @Autowired
    private Tracer tracer;

    @Bean
    @Primary
    public Tracer jaegerTracer(RemoteReporter remoteReporter) {
        return new JaegerTracer.Builder(JAEGER_SERVICE_NAME)
                .withReporter(remoteReporter)
                .withMetricsFactory(new NoopMetricsFactory()).withSampler(new ProbabilisticSampler(SAMPLING_RATE))
                .build();
    }

    @Bean
    public RemoteReporter remoteReporter() {
        return new Builder().withSender(new UdpSender(JAEGER_HOST, JAEGER_PORT, 0)).build();
    }

    @Override
    public void onApplicationEvent(ContextRefreshedEvent event) {
        if (!GlobalTracer.isRegistered()) {
            GlobalTracer.register(tracer);
        }
    }
}
</code></pre>
"
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176,1446358,0,"<p>I have got following gradle dependencies working,</p>
<pre><code>    implementation &quot;io.opentracing.contrib:opentracing-spring-cloud-starter:0.1.13&quot;
    implementation &quot;io.jaegertracing:jaeger-client:0.31.0&quot;

</code></pre>
<p>Following tracer bean configuration,</p>
<pre><code>  @Bean
    public io.opentracing.Tracer initTracer() {
        Configuration.SamplerConfiguration samplerConfig = new Configuration.SamplerConfiguration().withType(&quot;const&quot;).withParam(1);
        return Configuration.fromEnv(&quot;sprint-service&quot;).withSampler(samplerConfig).getTracer();
    }
</code></pre>
<p>And then the spans can be recorded as</p>
<pre><code>Span sprintSpan = tracer.buildSpan(&quot;my-span-name&quot;).withTag(&quot;player&quot;, player).start();

// some business functionality

sprintSpan.finish();
</code></pre>
<p>Here is the working example you can refer <a href=""https://github.com/krushnatkhawale/jaeger-with-spring-boot-web-app"" rel=""nofollow noreferrer"">https://github.com/krushnatkhawale/jaeger-with-spring-boot-web-app</a></p>
"
Jaeger,53757860,53754605,0,"2018/12/13, 10:35:24",False,"2018/12/13, 10:35:24",39,10384577,1,"<p>So did you install direct  or created a yaml from the templates ?</p>

<p>I would run the command you used to install but with template function and then add the options for jaeger,Kiali and grafana.</p>
"
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167,9485673,6,"<p>Here is <a href=""https://github.com/istio/istio/tree/master/install/kubernetes/helm/istio"" rel=""noreferrer"">howto</a>: from official repository.
 you need to update <a href=""https://github.com/istio/istio/blob/master/install/kubernetes/helm/istio/values.yaml"" rel=""noreferrer"" title=""values.yaml"">values.yaml</a>.
 and turn on  grafana,  kiali and jaeger. For example with kiali change:</p>

<pre><code>kiali:
    enabled:  false
</code></pre>

<p>to </p>

<pre><code>kiali:
    enabled:  true
</code></pre>

<p>than rebuild the Helm dependencies:</p>

<pre><code>helm dep update install/kubernetes/helm/istio
</code></pre>

<p>than upgrade your istio inside kubernetes:</p>

<pre><code>helm upgrade install/kubernetes/helm/istio
</code></pre>

<p>that's it, hope it was helpful </p>
"
Jaeger,53705114,53654993,0,"2018/12/10, 13:51:09",False,"2018/12/10, 13:51:09",63,326167,0,"<ul>
<li>Yes, it's possible. You can deploy these as you would any typical service.</li>
<li>This is situation dependent. It's generally preferrable to extract metrics/logging to their own instances as the performance requirements are potentially different from your applications.  </li>
<li>This is possible to achieve if you configure your instrumentation to communicate directly to the collectors. The collector can receive spans via HTTP on port 14268 (<a href=""https://www.jaegertracing.io/docs/1.6/deployment"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.6/deployment</a>).</li>
</ul>

<p>Scalabilty is dependent on sampling frequency and volumes. The agent supports adaptive sampling which is a feedback loop from the collector to your instrumented app.</p>

<p>You can statically define this up front in your instrumentation but you lose the adaptive features.</p>
"
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21,10770815,1,"<p>It is possible to bypass agent all together and send metrics directly to collector.</p>

<p>Just define variable JAEGER_ENDPOINT in your app running environment.</p>

<p>This behaviour is documented but buried down in the Jager git repo:</p>

<p><a href=""https://github.com/jaegertracing/jaeger-client-java/blob/master/jaeger-core/README.md"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-java/blob/master/jaeger-core/README.md</a></p>
"
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313,524946,0,"<blockquote>
  <p>Would a single agent colocated with a single collector be possible in a Jaeger deployment?</p>
</blockquote>

<p>It's possible, and that's how the <a href=""https://www.jaegertracing.io/docs/1.8/getting-started/#all-in-one"" rel=""nofollow noreferrer"">""all-in-one""</a> image works.</p>

<blockquote>
  <p>Would it be advisable?</p>
</blockquote>

<p>Depends on your architecture. If you don't expect your Jaeger infra to grow, using the all-in-one is easier from the maintenance perspective. If you need your Jaeger infra to be highly available, then you probably want to place your agents closer to your instrumented applications than to your collector and scale the collectors separately.</p>

<p>More about the Jaeger Agent is discussed in the following blog posts:</p>

<p><a href=""https://medium.com/jaegertracing/running-jaeger-agent-on-bare-metal-d1fc47d31fab"" rel=""nofollow noreferrer"">Running Jaeger Agent on bare metal</a>
<a href=""https://medium.com/jaegertracing/deployment-strategies-for-the-jaeger-agent-1d6f91796d09"" rel=""nofollow noreferrer"">Deployment strategies for the Jaeger Agent</a></p>

<blockquote>
  <p>Is it possible to skip the agent altogether and submit spans directly to the collector over HTTP?</p>
</blockquote>

<p>For some clients (Java, NodeJS and C#), yes. Look for the <a href=""https://www.jaegertracing.io/docs/1.8/client-features/"" rel=""nofollow noreferrer""><code>JAEGER_ENDPOINT</code> option</a>.</p>
"
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313,524946,0,"<p>You can turn on the debugging in the client by setting the option <code>JAEGER_REPORTER_LOG_SPANS</code> to true (or use the related option in the <code>ReporterConfiguration</code>, as it seems that's how you are using it).</p>

<p><a href=""https://www.jaegertracing.io/docs/1.8/client-features/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.8/client-features/</a></p>

<p>Once you confirm the traces are being generated and sent to the agent, set the log-level in the agent to <code>debug</code>: </p>

<p><code>
docker run -p... jaegertracing/jaeger-agent:1.8 --log-level=debug
</code></p>

<p>If you don't see anything in the logs there indicating that the agent received a span (or span batch), then you might need to configure your client with the Agent's address (<code>JAEGER_AGENT_HOST</code> and <code>JAEGER_AGENT_PORT</code>, or related options in the <code>Configuration</code> object).</p>

<p>You mentioned that you are deploying in Azure AKS, so, I guess that the agent isn't available at <code>localhost</code>, which is the default location where the client sends the spans. Typically, the agent would be deployed as a sidecar in such a scenario:</p>

<p><a href=""https://github.com/jaegertracing/jaeger-kubernetes#deploying-the-agent-as-sidecar"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-kubernetes#deploying-the-agent-as-sidecar</a></p>
"
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463,1749786,2,"<p>The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT! Jaegar has the ability to collect Zipkin spans.</p>
<p><a href=""https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin</a></p>
<p>You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.</p>
<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;
    &lt;/dependency&gt;
</code></pre>
<p>The above will send Zipkin spans to http://localhost:9411 by default. You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.</p>
<pre><code>spring.zipkin.base-url=http://your-jaegar-server:9411
</code></pre>
<p>Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.</p>
<p>In the <code>log4j2.xml</code> file, all you have to mention is</p>
<pre><code>[%X]
</code></pre>
<p>I'll be uploading a working example of this approach into my GitHub and sharing the link.</p>
<p><strong>EDIT 1:</strong></p>
<p>You can find the sample code here:</p>
<p><a href=""https://github.com/anoophp777/spring-webflux-jaegar-log4j2"" rel=""nofollow noreferrer"">https://github.com/anoophp777/spring-webflux-jaegar-log4j2</a></p>
"
Jaeger,59675721,50179555,0,"2020/01/10, 06:59:31",True,"2020/01/10, 06:59:31",842,6907909,1,"<p>The Jaeger helm chart is now available <a href=""https://github.com/jaegertracing/helm-charts"" rel=""nofollow noreferrer"">here</a>.</p>

<p>You need to add the helm repo first using the following:</p>

<pre><code>helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
</code></pre>

<p>This can be installed with:</p>

<pre><code>helm install jaegertracing/jaeger
</code></pre>
"
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336,1773866,1,"<p>Yes you can, and I have shown that numerous times during my presentations (<a href=""https://toomuchcoding.com/talks"" rel=""nofollow noreferrer"">https://toomuchcoding.com/talks</a>) and we describe it extensively in the documentation (<a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/</a>). Sleuth will set up your logging pattern which you can then parse and visualize using the ELK stack. Sleuth takes care of tracing context propagation and can send the spans to a span storage (e.g. Zipkin or Jaeger). Sleuth does take care of updating the MDC for you. Please always read the documentation and the project page before filing a question</p>
"
Jaeger,66584227,66527792,0,"2021/03/11, 16:09:56",True,"2021/03/11, 16:09:56",2573,2519395,2,"<p>The solution for this one is to simply increase the memory size in the <code>istio-config.yaml</code> file.<br>
in my case, I'm updating the PVC and it looks like it's already filled with data and decreasing it wasn't an option for istio, so I increased it in the config file instead:<br></p>
<pre><code>tracing:
  jaeger:
    hub: docker.io/jaegertracing
    memory:
      max_traces: 100000
    tag: &quot;1.16&quot;
    persist: true
    spanStorageType: badger
    storageClassName: &quot;gp2&quot;
    accessMode: ReadWriteOnce
  nodeSelector: {}
  opencensus:
    exporters:
      stackdriver:
        enable_tracing: true
    hub: docker.io/omnition
    resources:
      limits:
        cpu: &quot;1&quot;
        memory: 15Gi # I increased this one
      requests:
        cpu: 200m
        memory: 15Gi # and this one
</code></pre>
"
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113,12201084,1,"<p>Based on following example from <a href=""https://www.jaegertracing.io/docs/1.21/operator/#streaming-strategy"" rel=""nofollow noreferrer"">jaeger docs</a>:</p>
<pre><code>apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-streaming
spec:
  strategy: streaming
  collector:
    options:
      kafka: # &lt;1&gt;
        producer:
          topic: jaeger-spans
          brokers: my-cluster-kafka-brokers.kafka:9092
  ingester:
    options:
      kafka: # &lt;1&gt;
        consumer:
          topic: jaeger-spans
          brokers: my-cluster-kafka-brokers.kafka:9092
      ingester:
        deadlockInterval: 5s # &lt;2&gt;
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://elasticsearch:9200
</code></pre>
<p>and on example <a href=""https://www.jaegertracing.io/docs/1.21/cli/"" rel=""nofollow noreferrer"">cli falgs</a>:</p>
<pre><code>--kafka.producer.topic  jaeger-spans
The name of the kafka topic
--kafka.producer.brokers    127.0.0.1:9092
The comma-separated list of kafka brokers. i.e. '127.0.0.1:9092,0.0.0:1234'
--kafka.producer.plaintext.password 
The plaintext Password for SASL/PLAIN authentication
--kafka.producer.plaintext.username 
The plaintext Username for SASL/PLAIN authentication
</code></pre>
<p>I infere that you should be able to do the following:</p>
<pre><code>spec:
     strategy: streaming
  collector:
    options:
      kafka: # &lt;1&gt;
        producer:
          topic: jaeger-spans
          brokers: my-cluster-kafka-brokers.kafka:9092
          plaintext:
            username: &lt;username&gt;
            password: &lt;password&gt; 
</code></pre>
<p>Notice that I split the cli options with the dot and added it as a nested fields in yaml. Do the same to other parameters by analogy.</p>
"
Jaeger,65163350,65138941,0,"2020/12/06, 02:12:58",False,"2020/12/06, 02:12:58",166,13922022,0,"<p>See the answer on this <a href=""https://github.com/jaegertracing/jaeger/issues/1564#issuecomment-667291036"" rel=""nofollow noreferrer"">jaeger issue</a>, you will need to query elastic search or the source where the data is stored.</p>
<p>Alternatively, you should raise an issue on <a href=""https://github.com/jaegertracing/jaeger-ui/issues"" rel=""nofollow noreferrer"">jaeger-ui</a> detailing your case.</p>
"
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471,2673284,0,"<p>When you open an individual trace in the Jaeger UI, there is a View dropdown in the top right corner. One of the options is to view/download the given trace as a JSON file.</p>
<p>You can also programmatically query the Jaeger query service via JSON/Protobuf API, but those endpoints will not result in a data format that you can load back into the UI.</p>
<p><a href=""https://www.jaegertracing.io/docs/latest/apis/#trace-retrieval-apis"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/latest/apis/#trace-retrieval-apis</a></p>
"
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328,1221718,1,"<p>OK, I figured out the issue here which may be obvious to those with more expertise. <a href=""https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/grpc#grpc"" rel=""nofollow noreferrer"">The guide I linked to above that describes how to make an Ingress spec for gRPC</a> is specific to NGINX. Meanwhile, I am using K3S which came out of the box with Traefik as the Ingress Controller. Therefore, the annotations I used in my Ingress spec had no affect:</p>
<pre><code>metadata:
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;GRPC&quot;
</code></pre>
<p>So I found <a href=""https://stackoverflow.com/questions/54562993/traefik-as-an-ingress-controller-server-grpc-unknown-no-status-received"">another Stack Overflow post discussing Traefik and gRPC</a> and modified my original Ingress spec above a bit to include the annotations mentioned there:</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: simple-prod-collector
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: traefik
    ingress.kubernetes.io/protocol: h2c
    traefik.protocol: h2c
spec:
  rules:
  - host: jaeger-collector.my-container-dev
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: simple-prod-collector
            port:
              number: 14250
</code></pre>
<p>These are the changes I made:</p>
<ol>
<li>Changed the <code>metadata/annotations</code> (this was the actual change needed I am sure)</li>
<li>I also updated the spec version to use <code>networking.k8s.io/v1</code> instead of <code>networking.k8s.io/v1beta1</code> so there are some structural changes due to that but none of the actual content changed AFAIK.</li>
</ol>
<p>Hopefully this helps someone else running into this same confusion.</p>
"
Jaeger,64814852,64755896,0,"2020/11/13, 05:00:24",True,"2020/11/13, 05:00:24",121,3792242,1,"<p>It's not clear in the documentation, but I managed to get it working by providing the <code>SPAN_STORAGE_TYPE</code> and the respective connection details to allow the jaeger components to talk to the storage running outside of the all-in-one container.</p>
<p>For instance, I'm running elasticsearch on my Mac, so I used the following command to run all-in-one:</p>
<pre><code>docker run -d --name jaeger-es \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -e SPAN_STORAGE_TYPE=elasticsearch \
  -e ES_SERVER_URLS=http://host.docker.internal:9200 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.20
</code></pre>
"
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749,11977760,0,"<p>According to istio <a href=""https://istio.io/latest/docs/tasks/observability/distributed-tracing/jaeger/#generating-traces-using-the-bookinfo-sample"" rel=""nofollow noreferrer"">documentation</a></p>
<blockquote>
<p>To see trace data, you must send requests to your service. The number of requests depends on Istio’s sampling rate. You set this rate when you install Istio. The default sampling rate is 1%. You need to send at least 100 requests before the first trace is visible. Could you try to send at least 100 requests and check if it works?</p>
</blockquote>
<p>If you wan't to change the default sampling rate then there is istio <a href=""https://istio.io/latest/docs/tasks/observability/distributed-tracing/configurability/#customizing-trace-sampling"" rel=""nofollow noreferrer"">documentation</a> about that.</p>
<blockquote>
<p><strong>Customizing Trace sampling</strong></p>
<p>The sampling rate option can be used to control what percentage of requests get reported to your tracing system. This should be configured depending upon your traffic in the mesh and the amount of tracing data you want to collect. The default rate is 1%.</p>
<p>To modify the default random sampling to 50, add the following option to your tracing.yaml file.</p>
</blockquote>
<pre><code>apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  meshConfig:
    defaultConfig:
      tracing:
        sampling: 50
</code></pre>
<blockquote>
<p>The sampling rate should be in the range of 0.0 to 100.0 with a precision of 0.01. For example, to trace 5 requests out of every 10000, use 0.05 as the value here.</p>
</blockquote>
"
Jaeger,64039073,64036098,0,"2020/09/24, 06:22:45",True,"2020/09/24, 06:22:45",328,1221718,1,"<p>After digging around in the OpenTracing C# .NET Core source (<a href=""https://github.com/opentracing-contrib/csharp-netcore"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/csharp-netcore</a>) I figured out how to override the top level Span.OperationName.</p>
<p>I had to update my <code>Startup.ConfigureServices()</code> call to <code>services.AddOpenTracing()</code> to the following:</p>
<pre><code>services.AddOpenTracingCoreServices(otBuilder =&gt;
{
    otBuilder.AddAspNetCore()
        .ConfigureAspNetCore(options =&gt;
        {
            options.Hosting.OperationNameResolver = (httpContext) =&gt;
            {
                return $&quot;MySuperCoolOperationName ({httpContext.Request.Path.Value})&quot;;
            };
        })
        .AddCoreFx()
        .AddEntityFrameworkCore()
        .AddLoggerProvider();
});
</code></pre>
"
Jaeger,63116714,62992614,0,"2020/07/27, 16:44:44",False,"2020/07/27, 16:44:44",43,7017126,0,"<p>I have resolved it after configuring port as 14250 as JaegerGrpcSpanExporter internally uses grpc port which has been configured to 14250 for jaeger-collector</p>
"
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669,9291851,1,"<p>I was studying Opentracing and Jeager and I've used this tutorial to get familiar with the basic possibilities:
<a href=""https://github.com/yurishkuro/opentracing-tutorial/tree/master/java"" rel=""nofollow noreferrer"">https://github.com/yurishkuro/opentracing-tutorial/tree/master/java</a></p>
<p>If you take a look in the case 1 (Hello World), it explains how to &quot;<a href=""https://github.com/yurishkuro/opentracing-tutorial/tree/master/java/src/main/java/lesson01#annotate-the-trace-with-tags-and-logs"" rel=""nofollow noreferrer"">Annotate the Trace with Tags and Logs</a>&quot;.
That would answer your questions 1, 2 and 3, as with that you can add all the info that you would like within spans and logs.</p>
<p>Here is a snippet from the repository (but I'd recommend checking there, as it has a more detailed explanation):</p>
<pre><code>Span span = tracer.buildSpan(&quot;say-hello&quot;).start();
span.setTag(&quot;hello-to&quot;, helloTo);
</code></pre>
<p>In this case <code>helloTo</code> is a variable containing a name, to whom the app will say hello. It would create a span tag called hello-to with the value that is coming from the execution.</p>
<p>Below we have an example for the logs case, where the whole <code>helloStr</code> message is added to the logs:</p>
<pre><code>// this goes inside the sayHello method
String helloStr = String.format(&quot;Hello, %s!&quot;, helloTo);
span.log(ImmutableMap.of(&quot;event&quot;, &quot;string-format&quot;, &quot;value&quot;, helloStr));

System.out.println(helloStr);
span.log(ImmutableMap.of(&quot;event&quot;, &quot;println&quot;));
</code></pre>
<p>Regarding the last question, that would be easier, you can use the Jaeger UI to search for the trace you would like, there is a field for that on the top left corner:</p>
<p><a href=""https://i.stack.imgur.com/fGxKh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fGxKh.png"" alt=""enter image description here"" /></a></p>
"
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176,1446358,1,"<p>There you go.</p>
<ol>
<li>I want to add application logs to the span so that they are visible in JaegerUI.</li>
</ol>
<pre><code>Span span = tracer.buildSpan(&quot;my-span-name&quot;).start();
span.setTag(&quot;my-tag-name&quot;, &quot;my-tag-value&quot;);
</code></pre>
<p>There are various overloaded methods as follows</p>
<pre><code>Span setTag(String key, String value);
Span setTag(String key, boolean value);
Span setTag(String key, Number value);
</code></pre>
<ol start=""2"">
<li><p>I want to add some fields to span tags so that it's easy to search in JaegerUI.</p>
<p>Jaeger API provides <strong>log</strong> method to log multiple fields that needs to be added to a map, the method signature is as follows,</p>
</li>
</ol>
<p><code>Span log(Map&lt;String, ?&gt; fields);</code></p>
<p>eg:</p>
<pre><code>span.log(
   ImmutableMap.Builder&lt;String, Object&gt;()
     .put(&quot;event&quot;, &quot;soft error&quot;)
     .put(&quot;type&quot;, &quot;cache timeout&quot;)
     .put(&quot;waited.millis&quot;, 1500)
     .build()
); 
</code></pre>
<ol start=""3"">
<li>Also, I want the spanId and traceId to the application log.</li>
</ol>
<p>spanId and traceId are stored in JaegerSpanContext class, which can be obtained from context() method of Span class.</p>
<pre><code>    JaegerSpanContext spanContext = (JaegerSpanContext)sprintSpan.context();
    long spanId = spanContext.getSpanId();
    long traceId = spanContext.getTraceId();
</code></pre>
<ol start=""4"">
<li>Is it possible to search in JaegerUI based on spanId/traceId? If yes, how?</li>
</ol>
<p>There is a search box in the navigation bar of Jaeger UI where you can search traces by trace ID.</p>
<p><a href=""https://i.stack.imgur.com/mrsPB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mrsPB.png"" alt=""enter image description here"" /></a></p>
"
Jaeger,64036080,62420728,0,"2020/09/23, 23:55:37",False,"2020/09/23, 23:55:37",1048,1112106,1,"<p>What you did is for http 1.x, and it doesn't work for http2/grpc. Please dive into grpc impl in springboot doc.</p>
"
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155,1222237,0,"<p>Thanks Yuri. Yes it was a clock issue.
Although the host machine (VM) updated its clock on every unpause, docker for windows did not. The timezones were correct for all containers but the times were all out by the exact same amount. This must be the docker internal clock that seems to only get updated once on launch and not at the launch of every new container. Although all container clocks were out by the same amount, the windows host machine was correct. 
The messages were arriving but the times/dates were outside the time frame window the UI was displaying. If I set a custom date range I'm sure they would appear. The containers must have been out by a few days with the continual stopping and starting and not by just a few hours.</p>
"
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169,3301685,0,"<p>Time drift in docker is a known issue on Mac and Windows OS.</p>
<p>Check the date/time in a docker container using this (apologies in advance)</p>
<pre><code>( ConvertFrom-Json (docker system info --format '{{json .}}') ).SystemTime
</code></pre>
<p>or calculate the drift...</p>
<pre><code>(Get-Date -DisplayHint DateTime) - [DateTime]( ConvertFrom-Json (docker system info --format '{{json .}}') ).SystemTime
</code></pre>
<p>Unfortunately there doesnt seem to be a better solution than occasionally restarting the container.</p>
"
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167,11335868,0,"<p>The problem is that your Jaeger collector is not accessible from outside docker network host as you specified in your docker command. This would only work if your spring boot application is deployed on the host network too.
Try to run Jaeger as follows:</p>

<pre><code>docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.17
</code></pre>

<p>It should trigger.</p>
"
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79,9827874,0,"<p>Have you tried looking at the logs being generated by your pods?</p>

<p>In my case I got the following</p>

<blockquote>
  <p>ERROR Failed to flush spans in reporter: error sending spans over UDP:
  Error: getaddrinfo ENOTFOUND <a href=""http://jaeger-agent"" rel=""nofollow noreferrer"">http://jaeger-agent</a>, packet size: 984,
  bytes sent: undefined</p>
</blockquote>

<p>Changing it to jaeger-agent worked for me.</p>

<p>Also if it helps I have declared this under my jaeger image in docker-compose.yml:</p>

<pre><code> + ports: - ""5775:5775/udp"" - ""6831:6831/udp"" - ""6832:6832/udp"" - ""5778:5778"" - ""16686:16686"" - ""14268:14268"" - ""9411:9411""`
</code></pre>
"
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749,11977760,2,"<p>The answer here is to install istio with <code>--set values.global.tracer.zipkin.address</code> as provided in <a href=""https://istio.io/docs/tasks/observability/distributed-tracing/jaeger/#before-you-begin"" rel=""nofollow noreferrer"">istio documentation</a></p>

<pre><code>istioctl manifest apply --set values.global.tracer.zipkin.address=&lt;jaeger-collector-service&gt;.&lt;jaeger-collector-namespace&gt;:9411
</code></pre>

<hr>

<p><strong>And</strong></p>

<hr>

<p>Use the original TracingService <code>setting: service: ""zipkin.istio-system:9411""</code> as Donato Szilagyi confirmed in comments.</p>

<pre><code>apiVersion: getambassador.io/v2
kind: TracingService
metadata:
  name: tracing
  namespace: {{ .Values.namespace }}
spec:
  service: ""zipkin.istio-system:9411""
  driver: zipkin
  ambassador_id: ambassador-{{ .Values.namespace }}
  config: {}
</code></pre>

<blockquote>
  <p>Great! It works. And this time I used the original TracingService setting: service: ""zipkin.istio-system:9411"" – Donato Szilagy</p>
</blockquote>
"
Jaeger,59678839,59655255,6,"2020/01/10, 11:30:18",False,"2020/01/10, 11:30:18",23024,9773937,0,"<p>Not sure, but it seems you miss setting the TLS for Cassandra Storage in Azure Cosmos DB. When you use the Cassandra client to connect the Cassandra Storage in Azure Cosmos DB, it will give out the time out error, but if you enable the SSL, the connection works well. So I think you can try to enable the TLS for Cassandra in your values.yaml following the steps in the Github which provided.</p>
"
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",nan,nan,1,"<p>A colleague of mine provided the answer... It was hidden in the Makefile, which hadn't worked for me as I don't use Golang (and it had been more complex than just installing Golang and running it, but I digress...).</p>

<p>The following .sh will do the trick. This assumes the query.proto file is a subdirectory from the same location as the script below, under model/proto/api_v2/ (as it appears in the main Jaeger repo). </p>

<pre class=""lang-sh prettyprint-override""><code>#!/usr/bin/env sh
set +x

rm -rf ./js_out 2&gt; /dev/null
mkdir ./js_out

PROTO_INCLUDES=""
    -I model/proto \
    -I idl/proto \
    -I vendor/github.com/grpc-ecosystem/grpc-gateway \
    -I vendor/github.com/gogo/googleapis \
    -I vendor/github.com/gogo/protobuf/protobuf \
    -I vendor/github.com/gogo/protobuf""

python -m grpc_tools.protoc ${PROTO_INCLUDES} --grpc_python_out=./python_out --python_out=./python_out model/proto/api_v2/query.proto
</code></pre>

<p>This will definitely generate the needed Python file, but it will still be missing dependencies. </p>
"
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471,2673284,3,"<p>Jaeger clients implement so-called <strong>head-based sampling</strong>, where a sampling decision is made at the root of the call tree and propagated down the tree along with the trace context. This is done to guarantee consistent sampling of all spans of a given trace (or none of them), because we don't want to make the coin flip at every node and end up with partial/broken traces. Implementing on-error sampling in the head-based sampling system is not really possible. Imaging that your service is calling service A, which returns successfully, and then service B, which returns an error. Let's assume the root of the trace was not sampled (because otherwise you'd catch the error normally). That means by the time you know of an error from B, the whole sub-tree at A has been already executed and all spans discarded because of the earlier decision not to sample. The sub-tree at B has also finished executing. The only thing you can sample at this point is the spans in the current service. You could also implement a reverse propagation of the sampling decision via response to your caller. So in the best case you could end up with a sub-branch of the whole trace sampled, and possible future branches if the trace continues from above (e.g. via retries). But you can never capture the full trace, and sometimes the reason B failed was because A (successfully) returned some data that caused the error later.</p>

<p>Note that reverse propagation is not supported by the OpenTracing or OpenTelemetry today, but it has been discussed in the last meetings of the W3C Trace Context working group.</p>

<p>The alternative way to implement sampling is with <strong>tail-based sampling</strong>, a technique employed by some of the commercial vendors today, such as Lightstep, DataDog. It is also on the roadmap for Jaeger (we're working on it right now at Uber). With tail-based sampling 100% of spans are captured from the application, but only stored in memory in a collection tier, until the full trace is gathered and a sampling decision is made. The decision making code has a lot more information now, including errors, unusual latencies, etc. If we decide to sample the trace, only then it goes to disk storage, otherwise we evict it from memory, so that we only need to keep spans in memory for a few seconds on average. Tail-based sampling imposes heavier performance penalty on the traced applications because 100% of traffic needs to be profiled by tracing instrumentation.</p>

<p>You can read more about head-based and tail-based sampling either in Chapter 3 of my book (<a href=""https://www.shkuro.com/books/2019-mastering-distributed-tracing/"" rel=""nofollow noreferrer"">https://www.shkuro.com/books/2019-mastering-distributed-tracing/</a>) or in the awesome paper <em>""So, you want to trace your distributed system? Key design insights from years of practical experience""</em> by Raja R. Sambasivan, Rodrigo Fonseca, Ilari Shafer, Gregory R. Ganger (<a href=""http://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf"" rel=""nofollow noreferrer"">http://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf</a>).</p>
"
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313,524946,1,"<p>You can bind it to metrics and logging frameworks, but you don't have to. You can simply just call <code>cfg.NewTracer()</code>, like in this example:</p>

<pre class=""lang-golang prettyprint-override""><code>func ExampleFromEnv() {
    cfg, err := jaegercfg.FromEnv()
    if err != nil {
        // parsing errors might happen here, such as when we get a string where we expect a number
        log.Printf(""Could not parse Jaeger env vars: %s"", err.Error())
        return
    }

    tracer, closer, err := cfg.NewTracer()
    if err != nil {
        log.Printf(""Could not initialize jaeger tracer: %s"", err.Error())
        return
    }
    defer closer.Close()

    opentracing.SetGlobalTracer(tracer)
    // continue main()
}
</code></pre>

<p>Source: <a href=""https://github.com/jaegertracing/jaeger-client-go/blob/3585cc566102e0ea2225177423e3fcc3d2e5fd7a/config/example_test.go#L88-L105"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-go/blob/3585cc566102e0ea2225177423e3fcc3d2e5fd7a/config/example_test.go#L88-L105</a></p>

<p>Check the Jaeger Go Client readme for more information on the metrics/logging integration: <a href=""https://github.com/jaegertracing/jaeger-client-go"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-go</a></p>
"
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471,2673284,2,"<p>Jaeger clients are designed to have a minimum set of dependencies. We don't know if your application is using Prometheus metrics or Zap logger. This is why <code>jaeger-client-go</code> (as well as many other Jaeger clients in other languages) provide two lightweight interfaces for a Logger and MetricsFactory that can be implemented for a specific logs/metrics backend that your application is using. Of course, the bindings for Prometheus and Zap are already implemented in the <code>jaeger-lib</code> and can be included optionally.</p>
"
Jaeger,59507206,58947001,1,"2019/12/28, 02:23:31",True,"2019/12/28, 02:23:31",721,1563297,1,"<p>It looks like you have different versions of opentracing. The spring-starter-jaeger version 2.x upgrade the version of opentracing, so you might have introduced this breaking changes when you upgraded the dependency version.</p>
"
Jaeger,64808184,58763972,1,"2020/11/12, 18:51:10",False,"2020/11/12, 18:51:10",163,823789,0,"<p>Unfortunatelly, the reporter interface is used to report FINISHED spans, it is invoked on JaegerSpan.finish. I presume this is why it does not appear in logs.</p>
"
Jaeger,58919712,58642294,0,"2019/11/18, 19:08:56",False,"2019/11/18, 19:08:56",721,1563297,0,"<p>If you are using spring boot with auto configuration, the logs printed using log4j will be instrumented and sent automatically in the span.</p>
"
Jaeger,59260222,58642294,0,"2019/12/10, 06:26:49",False,"2019/12/10, 06:26:49",471,2673284,0,"<p>In Go this is not very straightforward, and largely depends on the logging library you use and the interface it provides. One example is implemented in the <a href=""https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod"" rel=""nofollow noreferrer"">HotROD demo</a> in the Jaeger repository and it is described in the <a href=""https://medium.com/opentracing/take-opentracing-for-a-hotrod-ride-f6e3141f7941"" rel=""nofollow noreferrer"">blog post accompanying the demo</a>. It uses <code>go.uber.org/zap</code> logging library underneath and allows to write log statements accepting the Context argument:</p>

<pre class=""lang-golang prettyprint-override""><code>logger.For(ctx).Info(
    ""Searching for nearby drivers"",
    zap.String(""location"", location),
)
</code></pre>

<p>Behind the scenes <code>logger.For(ctx)</code> captures the current tracing spans and adds all log statements to that span, in addition to sending then to <code>stdout</code> as usual.</p>

<p>To my knowledge, <code>go.uber.org/zap</code> does not yet support this mode of logging natively, and therefore requires a wrapper.</p>

<p>If you use another logging library, than on the high level this is what needs to happen:</p>

<ol>
<li>there must be a way to pass the Context to the logging methods, which could be captured internally by the logging framework in the log message data structure</li>
<li>the ""appender"" (or whatever it's called, I am using log4j terminology) that's responsible for taking log messages and writing them to some output stream can be extended to retrieve the Span from the Context and write the log message to the Span using <code>span.LogKV()</code> or similar API.</li>
</ol>
"
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721,1563297,1,"<p><code>spring-cloud-openfeign</code> since is from the spring-cloud family should be instrumented automatically once you add <code>opentracing-spring-jaeger-cloud-starter</code> the  as stated <a href=""https://github.com/opentracing-contrib/java-spring-jaeger#configuration"" rel=""nofollow noreferrer"">here</a>.
But sometimes (depending on how you create your feign client bean) you need to explicitly expose the bean to the spring context, so that the autoconfiguration can instrument your Feign Client.</p>

<p>Something like this:</p>

<pre><code>@Scope(""prototype"")
fun feignClient() : Client {
    return Client.Default(null, null)
}
</code></pre>

<p>it is kotlin but you can adapt.</p>
"
Jaeger,58440891,58406367,0,"2019/10/18, 00:21:43",True,"2019/10/18, 00:21:43",105,8754016,1,"<p>This was due to the helidon dependency.</p>

<p><a href=""https://helidon.io/docs/latest/#/guides/03_quickstart-mp"" rel=""nofollow noreferrer"">https://helidon.io/docs/latest/#/guides/03_quickstart-mp</a></p>

<p>Also I had to upgrade <code>opentracing-api</code> version to <code>0.33.0</code></p>
"
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471,2673284,1,"<p>If you are already using Istio in the deployment, then enabling tracing in it will provide more complete picture of request processing, such as accounting for the time spent in the network between the proxies. </p>

<p>You also don't need to have full tracing instrumentation in your services as long as they pass through certain headers, then Istio can still provide a pretty accurate picture of the traces (but you cannot capture any business specific data in the traces).</p>

<p>Traces generated by Istio will have standardized span names that you can use to reason about the SLAs across the whole infrastructure, whereas explicit tracing instrumentation inside the services can often use different naming schemes, especially when services are written in different languages and using different frameworks.</p>

<p>For the best of both worlds, I would recommend adding instrumentation inside the services for full fidelity, and also enabling tracing in Istio to capture full picture of request execution (and all network latencies).</p>
"
Jaeger,58058871,57870219,0,"2019/09/23, 11:55:31",False,"2019/09/23, 11:55:31",13313,524946,0,"<p>You most likely have a mismatch with your OpenTracing libraries. It looks like your Servlet Filter integration (io.opentracing.contrib.web.servlet.filter.TracingFilter) is making use of a method that doesn't exist.</p>
"
Jaeger,57855614,57854200,3,"2019/09/09, 17:02:34",True,"2019/09/09, 17:02:34",3892,3603660,2,"<p>You can use opentracing <a href=""https://github.com/opentracing-contrib/java-jdbc"" rel=""nofollow noreferrer"">java-jdbc</a> extension  it will works in Quarkus (I didn't test the native mode).</p>

<p>You need to use the version 0.0.12 as the latest one is based on Opentracing 0.33 but Quarkus use the version 0.31.</p>

<ol>
<li><p>Add the dependency to your pom.xml:</p>

<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
  &lt;artifactId&gt;opentracing-jdbc&lt;/artifactId&gt;
  &lt;version&gt;0.0.12&lt;/version&gt;
&lt;/dependency&gt;
</code></pre></li>
<li><p>Update your application.properties to use the opentracing-jdbc driver, the following are for a Postgres database:</p></li>
</ol>

<pre>
quarkus.datasource.url = jdbc:tracing:postgresql://localhost:5433/mydatabase
quarkus.datasource.driver = io.opentracing.contrib.jdbc.TracingDriver
quarkus.hibernate-orm.dialect = org.hibernate.dialect.PostgreSQLDialect
</pre>

<p>You will then saw the SQL queries in Jaeger as spans.</p>
"
Jaeger,57679113,57608146,1,"2019/08/27, 19:45:11",False,"2019/08/27, 19:45:11",55,11863447,1,"<p>What I eventually did is create a JaegerTraces and annotated with Bean</p>
"
Jaeger,66138678,57608146,0,"2021/02/10, 16:19:25",True,"2021/02/10, 16:19:25",12820,5277820,0,"<p>Apache Camel doesn't provide an implementation of <a href=""https://opentracing.io/"" rel=""nofollow noreferrer"">OpenTracing</a>, so you have to add also an implementation to your dependencies. For example <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">Jaeger</a>.</p>
<p><strong>Maven POM:</strong></p>
<pre class=""lang-xml prettyprint-override""><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.camel.springboot&lt;/groupId&gt;
        &lt;artifactId&gt;camel-opentracing-starter&lt;/artifactId&gt;
        &lt;version&gt;${camel.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
        &lt;artifactId&gt;opentracing-spring-jaeger-starter&lt;/artifactId&gt;
        &lt;version&gt;3.2.2&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<p>Also you have to enable OpenTracing for Apache Camel on your Spring Boot application class, see <a href=""https://camel.apache.org/components/latest/others/opentracing.html#_spring_boot"" rel=""nofollow noreferrer"">Spring Boot</a>:</p>
<blockquote>
<p>If you are using Spring Boot then you can add the <code>camel-opentracing-starter</code> dependency, and turn on OpenTracing by annotating the main class with <code>@CamelOpenTracing</code>.</p>
<p>The Tracer will be implicitly obtained from the camel context’s Registry, or the ServiceLoader, unless a Tracer bean has been defined by the application.</p>
</blockquote>
<p><strong>Spring Boot application class:</strong></p>
<pre class=""lang-java prettyprint-override""><code>@SpringBootApplication
@CamelOpenTracing
public class CamelApplication {

    public static void main(String[] args) {
        SpringApplication.run(CamelApplication.class, args);
    }
}
</code></pre>
"
Jaeger,55247113,55239593,1,"2019/03/19, 19:47:45",False,"2019/03/19, 19:47:45",76,1496147,1,"<p>Could you try using a more recent version of Jaeger: <a href=""https://www.jaegertracing.io/docs/latest/getting-started/#all-in-one"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/latest/getting-started/#all-in-one</a> - actually 1.11 is now out, so could try that.</p>
"
Jaeger,55255528,55239593,0,"2019/03/20, 09:29:23",True,"2019/03/20, 09:29:23",48481,2504224,3,"<p>The problem is that you are using <code>RestTemplate template = new RestTemplate();</code> to get an instance of the <code>RestTemplate</code> to make a REST call.</p>

<p>Doing that means that Opentracing cannot instrument the call to add necessary HTTP headers.</p>

<p>Please consider using <code>@Autowired RestTemplate restTemplate</code></p>
"
Jaeger,55225031,55216969,2,"2019/03/18, 17:41:35",True,"2019/03/18, 17:41:35",48481,2504224,0,"<p>While doing <code>mvnDebug quarkus:dev</code> (without <code>jvm.args</code>) and placing a breakpoint <a href=""https://github.com/quarkusio/quarkus/blob/master/extensions/jaeger/runtime/src/main/java/io/quarkus/jaeger/runtime/JaegerDeploymentTemplate.java#L39"" rel=""nofollow noreferrer"">here</a>, I see that you all your params are being passed except <code>quarkus.jaeger.sampler.parameter</code> which is wrong.
It should be <code>quarkus.jaeger.sampler.param</code></p>
"
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500,9521610,1,"<p>When you are accessing the service from the pod in the <strong>same namespace</strong> you can use just the service name. 
Example:</p>

<pre><code>http://elasticsearch:9200
</code></pre>

<p>If you are accessing the service from the pod in the <strong>different namespace</strong> you should also specify the namespace.
Example: </p>

<pre><code>http://elasticsearch.mynamespace:9200
http://elasticsearch.mynamespace.svc.cluster.local:9200
</code></pre>

<p>To check in what namespace the service is located, use the following command:</p>

<pre><code>kubectl get svc --all-namespaces -o wide
</code></pre>

<p><strong>Note</strong>: Changing ConfigMap does not apply it to deployment instantly. Usually, you need to restart all pods in the deployment to apply new ConfigMap values. There is no rolling-restart functionality at the moment, but you can use the following command as a workaround:<br>
<em>(replace deployment name and pod name with the real ones)</em></p>

<pre><code>kubectl patch deployment mydeployment -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""my-pod-name"",""env"":[{""name"":""START_TIME"",""value"":""'$(date +%s)'""}]}]}}}}'
</code></pre>
"
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313,524946,0,"<p>I don't think it's possible at the moment and you should definitely ask for this feature in the mailing list, Gitter or GitHub issue. The current assumption is that a clear TChannel connection can be made between the agent and collector(s), all being part of the same trusted network.</p>

<p>If you are using the Java, Node or C# client, my recommendation in your situation is to have your Jaeger Client to talk directly to the collector. Look for the env var <code>JAEGER_ENDPOINT</code> in the <a href=""https://www.jaegertracing.io/docs/1.9/client-features/"" rel=""nofollow noreferrer"">Client Features</a> documentation page.</p>
"
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321,4125383,2,"<p>The problem was, that the Report instance used a NoopSender -- thus ignoring the connection settings.</p>

<p>Using</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;io.jaegertracing&lt;/groupId&gt;
        &lt;artifactId&gt;jaeger-thrift&lt;/artifactId&gt;
        &lt;version&gt;0.32.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>in your POM will provide an appropriate Sender to the SenderFactory used by Jaeger's SenderResolver::resolve method.</p>

<p>This solved my problem.</p>
"
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313,524946,0,"<p>The Jaeger tracer (the part that runs along with your application) will send spans via UDP to an agent running on localhost by default. If your agent is somewhere else, set the <code>JAEGER_AGENT_HOST</code>/<code>JAEGER_AGENT_PORT</code> env vars accordingly. If you don't want an agent running on localhost and want to access a Jaeger Collector directly via HTTP, then set the <code>JAEGER_ENDPOINT</code> env var.</p>

<p>More info about these env vars can be found in the <a href=""https://www.jaegertracing.io/docs/1.7/client-features/"" rel=""nofollow noreferrer"">documentation</a> or here:
<a href=""https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core#configuration-via-environment"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core#configuration-via-environment</a></p>
"
Jaeger,52721877,52628054,0,"2018/10/09, 16:09:20",True,"2018/10/09, 16:09:20",13313,524946,0,"<p>No, <a href=""https://github.com/opentracing-contrib/java-spring-cloud/blob/2cc7d1d3666fb8b289f0f6223b662b5e72700895/instrument-starters/opentracing-spring-cloud-zuul-starter/src/main/java/io/opentracing/contrib/spring/cloud/zuul/TracePreZuulFilter.java#L56"" rel=""nofollow noreferrer"">it cannot</a>, but it wouldn't hurt to open an issue there with this suggestion.</p>
"
Jaeger,57364908,52522049,0,"2019/08/05, 22:22:29",False,"2019/12/18, 12:08:53",5287,780798,1,"<p>There is several components which works together and can fully satisfy your requirement.</p>

<ol>
<li><p>Common <a href=""https://github.com/opentracing/opentracing-csharp"" rel=""nofollow noreferrer"">opentracing library</a>, consisted of abstract layer for span, tracer, injectors and extractors, etc.</p></li>
<li><p>Official <a href=""https://github.com/jaegertracing/jaeger-client-csharp"" rel=""nofollow noreferrer"">jaeger-client-csharp</a>. Full list of clients can be found <a href=""https://www.jaegertracing.io/docs/latest/client-libraries/#supported-libraries"" rel=""nofollow noreferrer"">here</a>, which implement <a href=""https://github.com/opentracing/opentracing-csharp"" rel=""nofollow noreferrer"">opentracing abstraction layer</a> mentioned earlier.</p></li>
<li><p>The final piece is the <a href=""https://github.com/opentracing-contrib/csharp-netcore"" rel=""nofollow noreferrer"">OpenTracing API for .NET</a>, which is glue between <a href=""https://github.com/opentracing/opentracing-csharp"" rel=""nofollow noreferrer"">opentracing library</a> and <a href=""https://github.com/dotnet/corefx/blob/master/src/System.Diagnostics.DiagnosticSource/src/DiagnosticSourceUsersGuide.md"" rel=""nofollow noreferrer"">DiagnosticSource</a> concept in dotnet.</p></li>
</ol>

<p>Actually, the final library has <a href=""https://github.com/opentracing-contrib/csharp-netcore/blob/master/samples/Shared/JaegerServiceCollectionExtensions.cs"" rel=""nofollow noreferrer"">sample</a> which uses jaeger csharp implementation of ITracer and configure it as default GlobalTracer.</p>

<p>At the rest in your Startup.cs, you will end up with something like from that sample (services is IServiceCollection):</p>

<pre><code>services.AddSingleton&lt;ITracer&gt;(serviceProvider =&gt;
{
    string serviceName = Assembly.GetEntryAssembly().GetName().Name;

    ILoggerFactory loggerFactory = serviceProvider.GetRequiredService&lt;ILoggerFactory&gt;();

    ISampler sampler = new ConstSampler(sample: true);

    ITracer tracer = new Tracer.Builder(serviceName)
        .WithLoggerFactory(loggerFactory)
        .WithSampler(sampler)
        .Build();

    GlobalTracer.Register(tracer);

    return tracer;
});

// Prevent endless loops when OpenTracing is tracking HTTP requests to Jaeger.
services.Configure&lt;HttpHandlerDiagnosticOptions&gt;(options =&gt;
{
    options.IgnorePatterns.Add(request =&gt; _jaegerUri.IsBaseOf(request.RequestUri));
});
</code></pre>
"
Jaeger,51977735,51878525,0,"2018/08/23, 05:56:58",True,"2018/08/23, 05:56:58",4269,1047335,1,"<p>I resolved this.
It related to the sample rate.
After I configured the <code>JAEGER_SAMPLER_TYPE</code> and <code>JAEGER_SAMPLER_PARAM</code>, I can see the data.</p>
"
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373,10206966,3,"<p>In server 2 , Install jaeger</p>

<pre><code>$ docker run -d --name jaeger \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 9411:9411 \
  jaegertracing/all-in-one:latest
</code></pre>

<p>In server 1, set these environment variables.</p>

<pre><code>JAEGER_SAMPLER_TYPE=probabilistic 
JAEGER_SAMPLER_PARAM=1 
JAEGER_SAMPLER_MANAGER_HOST_PORT=(EnterServer2HostName):5778 
JAEGER_REPORTER_LOG_SPANS=false 
JAEGER_AGENT_HOST=(EnterServer2HostName)
JAEGER_AGENT_PORT=6831 
JAEGER_REPORTER_FLUSH_INTERVAL=1000 
JAEGER_REPORTER_MAX_QUEUE_SIZE=100 
application-server-id=server-x
</code></pre>

<p>Change the tracer registration application code as below in server 1, so that it will get the configurations from the environment variables.</p>

<pre><code>@Produces
@Singleton
public static io.opentracing.Tracer jaegerTracer() {
String serverInstanceId = System.getProperty(""application-server-id"");
if(serverInstanceId == null) {
serverInstanceId = System.getenv(""application-server-id"");
}
return new Configuration(""ApplicationName"" + (serverInstanceId!=null &amp;&amp; !serverInstanceId.isEmpty() ? ""-""+serverInstanceId : """"), 
                Configuration.SamplerConfiguration.fromEnv(),
                Configuration.ReporterConfiguration.fromEnv())
                .getTracer();
}
</code></pre>

<p>Hope this works! </p>

<p>Check this link for integrating elasticsearch as the persistence storage backend so that the traces will not remove once the Jaeger instance is stopped.
<a href=""https://stackoverflow.com/questions/51785812/how-to-configure-jaeger-with-elasticsearch"">How to configure Jaeger with elasticsearch?</a></p>
"
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116,1953109,2,"<p>I finally figured this out after trying out different combinations. This is happening because Jaeger agent is not receiving any UDP packets from my application. </p>

<p>You need to tell the tracer where to send UDP packets, which in this case is <code>docker-machine ip</code> 
I added: </p>

<pre><code>'agentHost': '192.168.99.100',
'agentPort': 6832
</code></pre>

<p>and then I was able to see my services in Jaeger UI. </p>

<pre><code>function initTracer(serviceName, options) {
  var config = {
    'serviceName': serviceName,
    'sampler': {
      'type': 'const',
      'param': 1
    },
    'reporter': {
      'logSpans': true,
      'agentHost': '192.168.99.100',
      'agentPort': 6832
    }
  }
  var options = {
    'logger': {
      'info': function logInfo(msg) {
        console.log('INFO ', msg)
      },
      'error': function logError(msg) {
        console.log('ERROR', msg)
      }
    }
  }

  const tracer = initJaegerTracer(config, options)

  //hook up nodejs process exit event
  process.on('exit', () =&gt; {
    console.log('flush out remaining span')
    tracer.close()
  })
  //handle ctrl+c
  process.on('SIGINT', () =&gt; {
    process.exit()
  })

  return tracer
}

exports.initTracer = initTracer
</code></pre>
"
Jaeger,48432561,48095718,0,"2018/01/25, 00:12:09",False,"2018/01/25, 00:12:09",471,2673284,1,"<p>Service graph data must be generated in Jaeger. Currently it's possible with via a Spark job here: <a href=""https://github.com/jaegertracing/spark-dependencies"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/spark-dependencies</a></p>
"
Jaeger,48432603,46561079,0,"2018/01/25, 00:16:25",True,"2019/12/10, 06:06:47",471,2673284,2,"<p>The Downloads page (<a href=""https://www.jaegertracing.io/download/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/download/</a>) lists both the Docker images and the raw binaries built for various platforms (Linux, macOS, windows). You can also build binaries from source.</p>
"
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169,3301685,0,"<p>Just to add to Yuris answer, you can also download the source from github - <a href=""https://test"" rel=""nofollow noreferrer"">Github - Jaeger</a> This is useful for diagnosing issues, or just getting a better understanding of how it all works.</p>
<p>I have run both the released apps and custom versions on both windows and linux servers without issues. For windows I would recommend running as a service using Nssm. <a href=""https://nssm.cc/usage"" rel=""nofollow noreferrer"">Nssm details</a></p>
"
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169,3301685,0,"<p>Elastic search works fine for this. And Kibana allows you to build nice aggregated views of the traffic.</p>
<p>A recommendation from my experience is to use the <code>--es.tags-as-fields.dot-replacement</code> option and specify a character. This flattens the data structure. Its very useful because ElasticSearch/Kibana struggle with the tags data as an array.</p>
"
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869,3511252,1,"<p>I had the same problem. Found this page that explains how to configure Thrift sender: <a href=""https://github.com/jaegertracing/jaeger-client-csharp/blob/master/src/Senders/Jaeger.Senders.Thrift/README.md"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-csharp/blob/master/src/Senders/Jaeger.Senders.Thrift/README.md</a></p>
<p>The C# tutorial does not mention it though ...</p>
<p>And here is my InitTracer(). Works fine with Jaeger launched from binary:</p>
<pre><code>    private static Tracer InitTracer(string serviceName, ILoggerFactory loggerFactory)
    {
        Configuration.SenderConfiguration.DefaultSenderResolver = new SenderResolver(loggerFactory)
            .RegisterSenderFactory&lt;ThriftSenderFactory&gt;();

        var samplerConfiguration = new Configuration.SamplerConfiguration(loggerFactory)
            .WithType(ConstSampler.Type)
            .WithParam(1);

        var sender = new SenderConfiguration(loggerFactory);

        var reporterConfiguration = new Configuration.ReporterConfiguration(loggerFactory)
            .WithLogSpans(true)
            .WithSender(sender);

        return (Tracer)new Configuration(serviceName, loggerFactory)
            .WithSampler(samplerConfiguration)
            .WithReporter(reporterConfiguration)
            .GetTracer();
    }
</code></pre>
"
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357,3703933,1,"<p>I solved it by using this library instead <a href=""https://github.com/opentracing-contrib/java-spring-cloud"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud</a></p>
<p>It seem to have an option to enable or disable different instrumentation feature. Read about <code>opentracing.spring.cloud.async.enabled</code> for more info.</p>
"
Jaeger,65059275,65059109,1,"2020/11/29, 12:48:21",True,"2020/11/29, 12:48:21",1058,1609014,0,"<p>Looks like your DaemonSet misses the <code>hostNetwork</code> property, to be able to listen on the node IP.
You can check that article for further info: <a href=""https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677"" rel=""nofollow noreferrer"">https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677</a></p>
"
Jaeger,65299081,65056880,0,"2020/12/15, 04:11:35",False,"2020/12/15, 04:11:35",471,2673284,0,"<p>You have two options:</p>
<ol>
<li>Run Jaeger agent (or OpenTelemetry Collector) on each host that runs your applications and let the agent forward trace data to a central Jaeger collector. The Jaeger client can continue emitting data via UDP port in this case.</li>
<li>Configure the Jaeger client with an HTTP endpoint of the Jaeger collector.</li>
</ol>
<p>For (2), you can pass the environment variable to you applications:</p>
<pre><code>JAEGER_ENDPOINT=http://jaeger-collector:14268/api/traces
</code></pre>
<p>Additional references:</p>
<ul>
<li><a href=""https://www.jaegertracing.io/docs/latest/client-features/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/latest/client-features/</a></li>
<li><a href=""https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core</a></li>
</ul>
"
Jaeger,63654371,63634257,3,"2020/08/30, 08:47:00",True,"2020/08/30, 08:47:00",366,10202496,1,"<p>By default, OpenTracing doesn't log automatically into span logs, only important messages that Jaeger feels it needs to be logged and is needed for tracing would be there :). The idea is to separate responsibilities between Tracing and Log management, <a href=""https://github.com/jaegertracing/jaeger/issues/962"" rel=""nofollow noreferrer"">Check this GitHub discussion</a>.</p>
<p>An alternative would be to use centralized log management and print traceId &amp; spanId into your logs for troubleshooting and correlating logs and tracing.</p>
"
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363,977959,0,"<p>As iabughosh said, the main focus on jaeger is traceability, monitoring and performance, not for logging.</p>
<p>Anyway, i found that using the @traced Bean injection you can insert a tag into the current span, that will be printed on Jaeger UI. this example will added the first 4 lines of an excpetion to the Tags seccion. (I use it on my global ExceptionHandler to add more info about the error):</p>
<pre><code>public abstract class MyExceptionHandler {

@Inject
io.opentracing.Tracer tracer;
/**
 * Will trace the message at jaeger metrics service, only for monitoring and profiling use, not for Logger.
 * @param e
 */
protected void monitorTrace(Exception e) {
    if(tracer!= null &amp;&amp; e!=null) {
        StringBuffer sb = new StringBuffer();
        sb.append(e.getCause()+ &quot;\n&quot;);
        int deep = 3;
        
        for(int i=0; i&lt; deep;i++) {
            sb.append(e.getStackTrace()[i]+ &quot;\n&quot;);
        }
        tracer.activeSpan().setTag(&quot;stack &quot;,sb.toString());
    }
}
</code></pre>
<p>}</p>
<p>and you will see the little stacktrace at JaegerUI.</p>
<p><a href=""https://i.stack.imgur.com/xYMzt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xYMzt.png"" alt=""enter image description here"" /></a></p>
<p>Hope helps</p>
"
Jaeger,55568365,55220796,0,"2019/04/08, 10:37:58",False,"2019/04/08, 10:37:58",76,1496147,0,"<p>Did you install the operator on openshift using the instructions listed: <a href=""https://github.com/jaegertracing/jaeger-operator#openshift"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-operator#openshift</a> ?</p>

<p>Did the operator start up ok, if you not were there errors in the log?</p>

<p>The <a href=""https://github.com/jaegertracing/jaeger-operator#creating-a-new-jaeger-instance"" rel=""nofollow noreferrer"">Creating a new Jaeger Instance</a> section starts with a link to some examples, including <a href=""https://github.com/jaegertracing/jaeger-operator/blob/master/deploy/examples/simple-prod.yaml"" rel=""nofollow noreferrer"">simple-prod.yaml</a>, which creates a Jaeger instance that uses an Elasticsearch cluster at the specified URL.</p>

<p>You simply run:</p>

<pre><code>oc create -f simple-prod.yaml
</code></pre>
"
Jaeger,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213,7933630,1,"<p>There's a lot of different questions here, and some of them don't have answers without more information about your specific setup, but I'll try to give you a good overview.</p>
<p><strong>Why Tracing?</strong></p>
<p>You've already intuited that there are a lot of similarities between &quot;APM&quot; and &quot;tracing&quot; - the differences are fairly minimal. Distributed Tracing is a superset of capabilities marketed as APM (application performance monitoring) and RUM (real user monitoring), as it allows you to capture performance information about the work being done in your services to handle a single, logical request both at a per-service level, and at the level of an entire request (or transaction) from client to DB and back.</p>
<p>Trace data, like other forms of telemetry, can be aggregated and analyzed in different ways - for example, unsampled trace data can be used to generate RED (rate, error, duration) metrics for a given API endpoint or function call. Conventionally, trace data is annotated (tagged) with properties about a request or the underlying infrastructure handling a request (things like a customer identifier, or the host name of the server handling a request, or the DB partition being accessed for a given query) that allows for powerful exploratory queries in a tool like Jaeger or a commercial tracing tool.</p>
<p><strong>Sampling</strong></p>
<p>The overall performance impact of generating traces varies. In general, tracing libraries are designed to be fairly lightweight - although there are a lot of factors that influence this overhead, such as the amount of attributes on a span, the log events attached to it, and the request rate of a service. Companies like Google will aggressively sample due to their scale, but to be honest, sampling is more beneficial to consider from a long-term storage perspective rather than an up-front overhead perspective.</p>
<p>While the additional overhead per-request to create a span and transmit it to your tracing backend might be small, the cost to store trace data over time can quickly become prohibitive. In addition, most traces from most systems aren't terribly interesting. This is why dynamic and tail-based sampling approaches have become more popular. These systems move the sampling decision from an individual service layer to some external process, such as the <a href=""https://github.com/open-telemetry/opentelemetry-collector"" rel=""nofollow noreferrer"">OpenTelemetry Collector</a>, which can analyze an entire trace and determine if it should be sampled in or out based on user-defined criteria. You could, for example, ensure that any trace where an error occurred is sampled in, while 'baseline' traces are sampled at a rate of 1%, in order to preserve important error information while giving you an idea of steady-state performance.</p>
<p><strong>Proprietary APM vs. OSS</strong></p>
<p>One important distinction between something like AppDynamics or New Relic and tools like Jaeger is that Jaeger does not rely on proprietary instrumentation agents in order to generate trace data. Jaeger supports OpenTelemetry, allowing you to use open source tools like the <a href=""https://github.com/open-telemetry/opentelemetry-java-instrumentation"" rel=""nofollow noreferrer"">OpenTelemetry Java Automatic Instrumentation</a> libraries, which will automatically generate spans for many popular Java frameworks and libraries, such as Spring. In addition, since OpenTelemetry is available in multiple languages with a shared data format and trace context format, you can guarantee that your traces will work properly in a polyglot environment (so, if you have Node.JS or Golang services in addition to your Java services, you could use OpenTelemetry for each language, and trace context propagation would work seamlessly between all of them).</p>
<p>Even more advantageous, though, is that your instrumentation is decoupled from a specific vendor or tool. You can instrument your service with OpenTelemetry and then send data to one - or more - analysis tools, both commercial and open source. This frees you from vendor lock-in, and allows you to select the best tool for the job.</p>
<p>If you'd like to learn more about OpenTelemetry, observability, and other topics I wrote a longer series that you can find <a href=""https://dev.to/lightstep/opentelemetry-101-what-is-observability-44m"" rel=""nofollow noreferrer"">here</a> (look for the other 'OpenTelemetry 101' posts).</p>
"
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357,3703933,1,"<p>I solved it by using this library instead <a href=""https://github.com/opentracing-contrib/java-spring-cloud"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud</a></p>
<p>It seem to have an option to enable or disable different instrumentation feature. Read about <code>opentracing.spring.cloud.async.enabled</code> for more info.</p>
"
Jaeger,67077099,67003774,0,"2021/04/13, 17:44:16",False,"2021/04/13, 17:44:16",193,3774803,0,"<p>It doesn't work in golang grpc client. I used openTelemetry <a href=""https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/loadbalancingexporter"" rel=""nofollow noreferrer"">load balancing</a> Another option - use kubernetes to balance requests to backends.</p>
"
Jaeger,66621994,66597254,0,"2021/03/14, 09:14:12",False,"2021/03/14, 09:14:12",13,15381526,1,"<p>I realized that I had got into a completely wrong direction. I thought that I have to access the backend storage to get the trace data, which actually make the problem much more complex. I got the answer from github discussion and here is the address <a href=""https://github.com/jaegertracing/jaeger/discussions/2876#discussioncomment-477176"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger/discussions/2876#discussioncomment-477176</a></p>
"
Jaeger,65650224,65595635,0,"2021/01/10, 06:52:21",False,"2021/01/10, 06:52:21",31,3289465,0,"<p>Can you paste the Collector config file? It seems you are using the gRPC protocol and it's not supported on the system where the collector is running.
<a href=""https://github.com/open-telemetry/opentelemetry-collector/blob/master/exporter/otlpexporter/README.md"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-collector/blob/master/exporter/otlpexporter/README.md</a></p>
"
Jaeger,65910568,65595635,1,"2021/01/27, 01:06:48",True,"2021/01/27, 01:06:48",483,4895267,0,"<p>gRPC port isn't enabled in your jaeger instance.</p>
<p>You can try a docker-compose file like this</p>
<pre><code>version: &quot;3.6&quot;
services:
  jaeger:
    image: jaegertracing/all-in-one
    ports:
      - 5775:5775/udp
      - 6831:6831/udp
      - 6832:6832/udp
      - 5778:5778
      - 16686:16686
      - 14268:14268
      - 14250:14250
      - 9411:9411
</code></pre>
<p>And you can connect to it without problems</p>
"
Jaeger,65853125,65202244,0,"2021/01/22, 23:41:07",False,"2021/01/22, 23:41:07",483,4895267,0,"<p>Remove your dependencies and use the following one that will include also the instrumentation you need</p>
<pre><code>&lt;dependency&gt;
 &lt;groupid&gt;io.opentracing.contrib&lt;/groupid&gt;
 &lt;artifactid&gt;opentracing-spring-jaeger-cloud-starter&lt;/artifactid&gt;
 &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
"
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328,1221718,0,"<p>I figured it out... the Jaeger Operator doesn't create a <code>Service</code> exposing the metrics endpoints. These endpoints are just exposed via the pods for the Collector and Query components.</p>
<p>An example from the Collector pod spec:</p>
<pre><code>    ports:
    - containerPort: 9411
      name: zipkin
      protocol: TCP
    - containerPort: 14267
      name: c-tchan-trft
      protocol: TCP
    - containerPort: 14268
      name: c-binary-trft
      protocol: TCP
    - containerPort: 14269
      name: admin-http
      protocol: TCP
    - containerPort: 14250
      name: grpc
      protocol: TCP
</code></pre>
<p>Note the <code>admin-http</code> port there.</p>
<p>So to get the Prometheus Operator to scrape these metrics, I created a <code>PodMonitor</code> which covers both the Collector and Query components because both of them have the <code>labels/app: jaeger</code> and <code>admin-http</code> ports defined:</p>
<pre class=""lang-yaml prettyprint-override""><code>cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: jaeger-components
  namespace: monitoring
  labels:
    release: prometheus
spec:
  podMetricsEndpoints:
  - path: /metrics
    port: admin-http
  namespaceSelector:
    matchNames:
    - monitoring
  selector:
    matchLabels:
        app: jaeger
EOF
</code></pre>
"
Jaeger,64592134,64486524,0,"2020/10/29, 15:30:59",False,"2020/10/29, 15:30:59",51,8394088,0,"<p>referring to the documentation provided in below link helped to resolve the issue.
<a href=""https://stackoverflow.com/questions/56353740/java-lang-illegalstateexception-this-should-not-happen-as-headers-should-only"">java.lang.IllegalStateException: This should not happen as headers() should only be called while a record is processed</a></p>
"
Jaeger,64481993,64465875,0,"2020/10/22, 15:14:22",True,"2020/10/22, 16:39:57",2287,261708,1,"<p>I got it working as mentioned below</p>
<pre><code>kubectl expose service prometheus --type=LoadBalancer --name=prometheus --namespace istio-system
    
export PROMETHEUS_URL=$(kubectl get svc prometheus-svc -n istio-system  -o jsonpath=&quot;{.status.loadBalancer.ingress[0]['hostname','ip']}&quot;):$(kubectl get svc prometheus-svc -n istio-system -o 'jsonpath={.spec.ports[0].port}')


echo http://${PROMETHEUS_URL}
curl http://${PROMETHEUS_URL}
</code></pre>
<p>I would assume that it may not be the right way of exposing the services. Instead</p>
<ol>
<li>Create a Istio Gateway point to <a href=""https://grafana.mycompany.com"" rel=""nofollow noreferrer"">https://grafana.mycompany.com</a></li>
<li>Create a Istio Virtual service to redirect the requuest to the above Internal Service</li>
</ol>
"
Jaeger,64356244,64342820,0,"2020/10/14, 18:06:36",True,"2020/10/14, 18:06:36",39034,785745,0,"<p>This is the simplest working example that I was able to find.</p>
<pre class=""lang-cs prettyprint-override""><code>using Jaeger;
using Jaeger.Reporters;
using Jaeger.Samplers;
using Jaeger.Senders.Thrift;

namespace jaegertest
{
    class Program
    {
        static void Main(string[] args)
        {
            var tracer = new Tracer.Builder(&quot;my-service&quot;)
                .WithSampler(new ConstSampler(true))
                .WithReporter(new RemoteReporter.Builder()
                    .WithSender(new UdpSender())
                    .Build())
                .Build();

            using (var scope = tracer.BuildSpan(&quot;foo&quot;).StartActive(true))
            {
                System.Threading.Thread.Sleep(1000);
            }

            tracer.Dispose();
        }
    }
}
</code></pre>
<p>Here is a more realistic example that builds the tracer from a configuration.</p>
<pre class=""lang-cs prettyprint-override""><code>using Jaeger;
using Jaeger.Samplers;
using Jaeger.Senders;
using Jaeger.Senders.Thrift;
using Microsoft.Extensions.Logging;

namespace jaegertest
{
    class Program
    {
        static void Main(string[] args)
        {
            var loggerFactory = new LoggerFactory();

            var samplerConfiguration = new Configuration.SamplerConfiguration(loggerFactory)
                .WithType(ConstSampler.Type)
                .WithParam(1);

            var senderResolver = new SenderResolver(loggerFactory)
                .RegisterSenderFactory&lt;ThriftSenderFactory&gt;();

            var senderConfiguration = new Configuration.SenderConfiguration(loggerFactory)
                .WithSenderResolver(senderResolver);

            var reporterConfiguration = new Configuration.ReporterConfiguration(loggerFactory)
                .WithSender(senderConfiguration)
                .WithLogSpans(true);

            var tracer = (Tracer)new Configuration(&quot;my-service&quot;, loggerFactory)
                .WithSampler(samplerConfiguration)
                .WithReporter(reporterConfiguration)
                .GetTracer();

            using (var scope = tracer.BuildSpan(&quot;foo&quot;).StartActive(true))
            {
                System.Threading.Thread.Sleep(1000);
            }

            tracer.Dispose();
        }
    }
}
</code></pre>
"
Jaeger,64536471,64266056,0,"2020/10/26, 13:43:36",False,"2020/10/26, 13:43:36",1157,1700378,0,"<p>GitLab Helm charts support tracing, and you can configure it with:</p>
<pre><code>global:
  tracing:
    connection:
      string: 'opentracing://jaeger?http_endpoint=http%3A%2F%2Fjaeger.example.com%3A14268%2Fapi%2Ftraces&amp;sampler=const&amp;sampler_param=1'
    urlTemplate: 'http://jaeger-ui.example.com/search?service={{ service }}&amp;tags=%7B&quot;correlation_id&quot;%3A&quot;{{ correlation_id }}&quot;%7D'
</code></pre>
<p>For more details refer :<a href=""https://docs.gitlab.com/charts/charts/globals.html#tracing"" rel=""nofollow noreferrer"">https://docs.gitlab.com/charts/charts/globals.html#tracing</a></p>
"
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576,12014434,1,"<p>According to <a href=""https://istio.io/latest/docs/ops/integrations/jaeger/#option-2-customizable-install"" rel=""nofollow noreferrer"">istio</a> documentation:</p>
<blockquote>
<h3>Option 2: Customizable install<a href=""https://istio.io/latest/docs/ops/integrations/jaeger/#option-2-customizable-install"" rel=""nofollow noreferrer""></a></h3>
<p>Consult the  <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">Jaeger documentation</a>  to
get started. No special changes are needed for Jaeger to work with
Istio.</p>
<p>Once Jaeger is installed, you will need to point Istio proxies to send
traces to the deployment. This can be configured with  <code>--set values.global.tracer.zipkin.address=&lt;jaeger-collector-address&gt;:9411</code>
at installation time. See the
<a href=""https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#Tracing"" rel=""nofollow noreferrer""><code>ProxyConfig.Tracing</code></a>
for advanced configuration such as TLS settings.</p>
</blockquote>
<p>Istio documentation states to use jaeger collector address in <code>global.tracer.zipkin.address</code>.</p>
<hr />
<p>As for the Jaeger agent host, according to <a href=""https://www.jaegertracing.io/docs/1.20/operator/"" rel=""nofollow noreferrer"">Jaeger</a> Operator documentation:</p>
<blockquote>
<p>&lt;9&gt; By default, the operator assumes that agents are deployed as
sidecars within the target pods. Specifying the strategy as
“DaemonSet” changes that and makes the operator deploy the agent as
DaemonSet. Note that your tracer client will probably have to override
the “JAEGER_AGENT_HOST” environment variable to use the node’s IP.</p>
</blockquote>
<hr />
<blockquote>
<p>Your tracer client will then most likely need to be told where the agent is located. This is usually done by setting the environment variable  <code>JAEGER_AGENT_HOST</code>  to the value of the Kubernetes node’s IP, for example:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: acme/myapp:myversion
        env:
        - name: JAEGER_AGENT_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
</code></pre>
</blockquote>
"
Jaeger,63837122,63835165,0,"2020/09/10, 23:16:48",True,"2020/09/10, 23:16:48",147,1729409,0,"<p>Solved by adding dependency in pom file on jaeger-thrift.</p>
"
Jaeger,64240007,63745859,0,"2020/10/07, 11:18:29",True,"2020/10/07, 11:18:29",391,6452043,1,"<p>I enabled instrumentation on the services using those two dependencies:</p>
<pre><code>        implementation &quot;io.opentracing.contrib:opentracing-jaxrs2:1.0.0&quot;
        implementation &quot;io.jaegertracing:jaeger-client:1.4.0&quot;
</code></pre>
<p>And, I used jaeger-client to configure the tracer using environment variables:</p>
<pre><code>JAEGER_SERVICE_NAME: yourServiceName
</code></pre>
<p>Getting a Tracer Instance:</p>
<pre><code>    public static Tracer jaegerTracer() {
        return Configuration.fromEnv()
                .getTracer();
    }
</code></pre>
<p>Finally, in the dropwizard application, you have to register the tracer like so</p>
<pre><code>   GlobalTracer.registerIfAbsent(jaegerTracer());
   jersey.register(new ServerTracingDynamicFeature());

  
    env.servlets()
           .addFilter(&quot;span-finisher&quot;, new SpanFinishingFilter())
           .addMappingForUrlPatterns(EnumSet.allOf(DispatcherType.class), true, &quot;/*&quot;);
</code></pre>
"
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336,8095979,2,"<p>You need to keep double quotes as it is.</p>
<p>An issue has been identified similar to this [1] and has been fixed recently. Can you try to get the latest WUM updated API Manager 3.1.0 and try enabling Jaeger open tracing?</p>
<p>Alternatively, this issue will not occur when using &quot;localhost&quot; as the hostname.</p>
<p>[1] <a href=""https://github.com/wso2/product-apim/issues/7940"" rel=""nofollow noreferrer"">https://github.com/wso2/product-apim/issues/7940</a></p>
"
Jaeger,63583870,63581052,0,"2020/08/25, 20:08:00",False,"2020/12/03, 10:54:28",2466,3176125,0,"<p>Run Jager using the docker image as follows.</p>
<pre><code>docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.18
</code></pre>
<p>Then add the following config to the deployment.toml.</p>
<pre><code>[apim.open_tracer]
remote_tracer.enable = true
remote_tracer.name = &quot;jaeger&quot;
remote_tracer.properties.hostname = &quot;localhost&quot;
remote_tracer.properties.port = &quot;6831&quot;
</code></pre>
<p>Side Note: For zipkin you can use the following.</p>
<pre><code>[apim.open_tracer]
remote_tracer.enable = true
remote_tracer.name = &quot;zipkin&quot;
remote_tracer.properties.hostname = &quot;localhost&quot;
remote_tracer.properties.port = &quot;9411&quot;
</code></pre>
"
Jaeger,63470131,63221267,0,"2020/08/18, 16:51:58",False,"2020/08/18, 16:51:58",213,7933630,0,"<p>I'm not sure if what you're looking for exists today per se, but you could accomplish this with OpenTelemetry by writing traces through the LogReporter then using a serverless function to read the cloudwatch stream and send it to Jaeger (or to an OpenTelemetry Collector that sends them to Jaeger). You could also write a custom plugin for the OpenTelemetry Collector that read a cloudwatch stream into OTLP then exported it to any endpoint supported by the collector.</p>
"
Jaeger,63218533,63218451,2,"2020/08/02, 19:38:53",False,"2020/08/02, 19:38:53",721,1563297,0,"<p>You are missing the configuration of Jaeger address. Since you did not provided it, it is trying to connect to the default one, which is TCP protocol, <code>127.0.0.1</code> and port 5778.
Check for details the configuration section <a href=""https://github.com/jaegertracing/jaeger-client-cpp#updating-sampling-server-url"" rel=""nofollow noreferrer"">here</a>.</p>
"
Jaeger,63198835,63198673,0,"2020/07/31, 23:41:17",True,"2020/07/31, 23:41:17",7166,3838328,0,"<p>You just need to make use of <code>tags.value</code> instead of <code>value</code> in your match query.</p>
<p>Below query should help:</p>
<pre><code>POST &lt;your_index_name&gt;/_search
{
  &quot;from&quot;: 0,
  &quot;size&quot;: 1,
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;must&quot;: [
        {
          &quot;match&quot;: {
            &quot;process.serviceName&quot;: &quot;transaction-manager&quot;
          }
        },
        {
          &quot;nested&quot;: {
            &quot;path&quot;: &quot;tags&quot;,
            &quot;query&quot;: {
              &quot;match&quot;: {
                &quot;tags.value&quot;: &quot;zipkin&quot;        &lt;---- Note this
              }
            }
          }
        }
      ]
    }
  }
}
</code></pre>
"
Jaeger,62910785,62830150,0,"2020/07/15, 11:34:09",False,"2020/07/15, 11:34:09",184,8575474,0,"<pre><code>#include &lt;jaegertracing/net/IPAddress.h&gt;
#include &lt;jaegertracing/net/Socket.h&gt;
void check(){
try{
jaegertracing::net::Socket socket;
        socket.open(AF_INET, SOCK_STREAM);
        const std::string serverURL = configuration.sampler().kDefaultSamplingServerURL; 
        socket.connect(serverURL);
}catch(...){}
}
</code></pre>
<p>if it throws error then it is unable to reach host, this method is costly I agree but this is the only viable solution I find</p>
"
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113,4573609,0,"<p>so I give it a try and my answer to the question above are:</p>

<ul>
<li><p>Q1) yes it is possible (congrats to the jaeger team, package pretty easy to grasp with a good documentation)</p></li>
<li><p>Q2) I did struggle a bit with this one and thanks to <a href=""https://github.com/CHOMNANP/jaeger-js-text-map-demo"" rel=""nofollow noreferrer"">https://github.com/CHOMNANP/jaeger-js-text-map-demo</a> I implemented a solution by adding a ""textCarrier"" with a ref. to the span context formatted as ""FORMAT_TEXT_MAP"" to the message Component 1 was publishing towards Component 2.</p></li>
</ul>

<p>Code snipper in C1 on the first API invocation</p>

<pre><code>server.post(""/api/vms"", (req, res) =&gt; {
  console.log('Enter /api/vms');
  const span = tracer.startSpan(req.path);
  // Use the log api to capture a log
  span.log({ event: 'request_received' })
  txSpan = span;
  //console.log(""req"",span);
  // Use the setTag api to capture standard span tags for http traces
  span.setTag(opentracing.Tags.HTTP_METHOD, req.method)
  span.setTag(opentracing.Tags.SPAN_KIND, opentracing.Tags.SPAN_KIND_RPC_SERVER)
  span.setTag(opentracing.Tags.HTTP_URL, req.path)
</code></pre>

<p>followed by this part when sending the msg on redis:</p>

<pre><code>   const textCarrier = getTextCarrierBySpanObject(span);
   tracer.inject(span.context(), opentracing.FORMAT_TEXT_MAP, textCarrier)

   var vm = req.body;
   console.log('Creating a new vm: ', vm);
   // Publish a message by specifying a channel name.
   try {
      Object.assign(vm, { textCarrier });
      pub.publish('tasks-queue', JSON.stringify(vm));
   } catch(e) {
        console.log(""App1 Error when publishing task to App2"", e);
   }
</code></pre>

<p>The getTextCarrierBySpanObject function is coming from <a href=""https://github.com/CHOMNANP/jaeger-js-text-map-demo"" rel=""nofollow noreferrer"">https://github.com/CHOMNANP/jaeger-js-text-map-demo</a></p>

<pre><code>function getTextCarrierBySpanObject(_span) {

    const spanContext = _span.context();
    const traceId = spanContext._traceId.toString('hex');
    const spanId = spanContext._spanId.toString('hex');
    let parentSpanId = spanContext._parentId;
    const flag = _.get(spanContext, '_flags', 1);

    if (parentSpanId) {
        parentSpanId = parentSpanId.toString('hex');
    } else {
        parentSpanId = 0;
    }

    const uberTraceId = `${traceId}:${spanId}:${parentSpanId}:${flag}`;
    console.log(""uberTraceId===&gt; "", uberTraceId)

    let textCarrier = {
        ""uber-trace-id"": uberTraceId
    };

    return textCarrier
}
</code></pre>

<p>Code snippet in C2 receiving the msg from redis</p>

<pre><code>sub.on('message', function(channel, message) {
  // message is json string in our case so we are going to parse it.
  try {
    var json = JSON.parse(message)
    console.log(""Task received"", message);

    const tracer = opentracing.globalTracer();
    // Extracting the span context from the message
    var parentSpan = tracer.extract(opentracing.FORMAT_TEXT_MAP, JSON.parse(message).textCarrier);
    console.log(""textCarrier="",JSON.parse(message).textCarrier);
    const span = tracer.startSpan(""/msg"", { childOf: parentSpan });
    // Use the log api to capture a log
    span.log({ event: 'msg_received' })
</code></pre>

<p>I tested with version 1.13</p>

<pre><code>docker run -d --name jaeger   -e COLLECTOR_ZIPKIN_HTTP_PORT=9411   -p 5775:5775/udp   -p 6831:6831/udp   -p 6832:6832/udp   -p 5778:5778   -p 16686:16686   -p 14268:14268   -p 9411:9411   jaegertracing/all-in-one:1.13
</code></pre>

<ul>
<li>Q3) This one is pretty straight forward using FORMAT_HTTP_HEADERS to convey the span from the component C4 invoking a callback on C3 and the from the Component C3 invoking a callback on C1. The only ""problem"" I found was more a ""trace readability issue"" as, in fact, the Spans appear on the Jaeger UI in ""the progagation"" order and not in the ""timing order"" which can be a bit confusing... but the ""trace graph"" experimental feature allowed to actually see the trace in the right order of appearance, so all good.</li>
</ul>

<p>So all in all a pretty convincing prototyping exercise with Jaeger, will most probably pilot it now on a real project before trying it in production.</p>
"
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13,7069613,0,"<p>According to <a href=""https://helm.sh/docs/chart_template_guide/control_structures/"" rel=""nofollow noreferrer"">https://helm.sh/docs/chart_template_guide/control_structures/</a> a string is converted to a boolean of True.  So even a string of false would get evaluated as a Boolean of True by Helm. I was using Spinnaker which handles all overrides as a string unless the ""Raw Overrides"" box is checked.  If that box is checked than it converts the string to primitives where applicable.</p>

<p>My issue was that even though I was overriding with a value of false, Spinnaker would pass that as a string to Helm which would then evaluate that as True.</p>

<p>The solution was to check the ""Raw Overrides"" box in Spinnaker.  </p>
"
Jaeger,65852979,61431244,0,"2021/01/22, 23:26:39",False,"2021/01/22, 23:26:39",483,4895267,1,"<p>You can set a tag to Span creating a new custom Span</p>
<pre><code>Tracer tracer = GlobalTracer.get();
Tracer.SpanBuilder spanBuilder = tracer.buildSpan(&quot;CustomSpan&quot;)
    .withTag(Tags.SPAN_KIND.getKey(), Tags.SPAN_KIND_SERVER);

Span span = spanBuilder.start();
Tags.COMPONENT.set(span, &quot;MyComponent&quot;);
span.setTag(&quot;mytag&quot;, &quot;test&quot;);
span.finish();
</code></pre>
<p>or retrieving the current active Span</p>
<pre><code>Tracer tracer = GlobalTracer.get();
Span span = tracer.activeSpan();
</code></pre>
"
Jaeger,61163147,61073781,0,"2020/04/11, 23:15:30",False,"2020/04/11, 23:15:30",67,1466001,0,"<p>Here is a working example: <a href=""https://github.com/jobinesh/cloud-native-applications/tree/master/helidon-example-mp-jaeger"" rel=""nofollow noreferrer"">https://github.com/jobinesh/cloud-native-applications/tree/master/helidon-example-mp-jaeger</a>. See if that helps you. If you are interested, see the details captured here: <a href=""https://www.jobinesh.com/2020/04/tracing-api-calls-in-your-helidon.html"" rel=""nofollow noreferrer"">https://www.jobinesh.com/2020/04/tracing-api-calls-in-your-helidon.html</a></p>
"
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576,1839482,0,"<p>You need to enable open tracing in nginx ingress controller.</p>

<p>To enable the instrumentation we must enable OpenTracing in the configuration ConfigMap:</p>

<pre><code>data:
  enable-opentracing: ""true""
</code></pre>

<p>To enable or disable instrumentation for a single Ingress, use the enable-opentracing annotation:</p>

<pre><code>kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-opentracing: ""true""
</code></pre>

<p>You must also set the host to use when uploading traces:</p>

<pre><code>jaeger-collector-host: jaeger-agent.default.svc.cluster.local
</code></pre>

<p><a href=""https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/"" rel=""nofollow noreferrer"">https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/</a></p>
"
Jaeger,62828373,60655072,0,"2020/07/10, 09:20:22",False,"2020/07/10, 09:20:22",81,12550162,0,"<p>Are you connecting it to only elasticsearch or stack like ELK/EFK?. I had tried but we cannot configure ELK in jeager-all-one.exe alone in windows without docker.You can do it by running Jeager-collector, Jeager agent and Jeager query individually by mentioning configurations related to ELK.
In Jeager collector and Jeager query you need to set up variables <code>SPAN_STORAGE_TYPES</code> and <code>ES_SERVER_URLS</code>.</p>
"
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829,9428538,2,"<p>To start a jaeger container:</p>

<pre class=""lang-sh prettyprint-override""><code>docker run --rm --name jaeger -d -p 16686:16686 -p 6831:6831/udp jaegertracing/all-in-one
</code></pre>

<p>Then you should by able to access to the Jaeger UI at <a href=""http://localhost:16686"" rel=""nofollow noreferrer"">http://localhost:16686</a></p>

<p>Once you've have a Jaeger up and running, you need to configure a Jaeger exporter to forward spans to Jaeger. This will depends of the language used.</p>

<p><a href=""https://opentelemetry-python.readthedocs.io/en/stable/getting-started.html#configure-exporters-to-emit-spans-elsewhere"" rel=""nofollow noreferrer"">Here</a> is the straightforward documentation to do so in python.</p>
"
Jaeger,59790526,59782209,0,"2020/01/17, 17:42:43",False,"2020/01/17, 17:42:43",2105,2461073,0,"<p>Not out of the box, you have to plug a behaviour into NSB that uses open telemetry
<a href=""https://github.com/open-telemetry/opentelemetry-dotnet"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-dotnet</a>
You will have to write custom code.
Plus you can do push metrics as well as shown in our app insights, Prometheus and other samples.</p>

<p>Let's continue the conversation in our support channels?</p>
"
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970,5253437,0,"<p>Not sure if you are still looking for a solution for this. You should be able to do this currently using the <code>NServiceBus.Extensions.Diagnostics.OpenTelemetry</code> package from <a href=""https://www.nuget.org/packages/NServiceBus.Extensions.Diagnostics.OpenTelemetry"" rel=""nofollow noreferrer"">nuget</a>. This is built by Jimmy Bogard and instruments NServiceBus with the required support for <code>Open Telemetry</code>.</p>
<p>The source for this is available <a href=""https://github.com/jbogard/NServiceBus.Extensions.Diagnostics.OpenTelemetry"" rel=""nofollow noreferrer"">here</a>. You can connect this to any backend of your choice that supports <code>Open Telemetry</code> including but not limited to <code>Jaeger</code> and <code>Zipkin</code>.</p>
<p>Additionally, here is an <a href=""https://github.com/jbogard/nsb-diagnostics-poc"" rel=""nofollow noreferrer"">example</a> that shows this in action.</p>
"
Jaeger,59628491,59561262,0,"2020/01/07, 14:31:30",False,"2020/01/07, 14:31:30",13,4977370,0,"<p>Got it! We need to enable sampling strategy to reach the collector endpoint. </p>

<pre><code>var initTracer = require('jaeger-client').initTracer;

var config = {
  'serviceName': 'Jaeger_Service',
  'reporter': {
    'collectorEndpoint': 'http://jaeger-collector:14268/api/traces',
  },
  'sampler': {
    'type': 'const',
    'param' : 0.1  
  }
};

var options = {
  'logger': {
'info': function logInfo(msg) {
  console.log('INFO ', msg)
},
'error': function logError(msg) {
  console.log('ERROR', msg)
  }
 }
};

var tracer = initTracer(config, options);
var express = require('express');
var app = express();
var http = require('http');
var server = http.createServer(app);

app.get('/', (req, res) =&gt; {
    const span = tracer.startSpan('http_request');
    res.send('Hello Jaeger');
    span.log({'event': 'request_end'});
    span.finish();
});

server.listen(8000);
console.log('Express server started on port %s', server.address().port);
</code></pre>
"
Jaeger,59391314,59372759,0,"2019/12/18, 13:44:56",True,"2019/12/18, 13:44:56",425,9112151,0,"<p>Found out how. I just added one single line of code into tracing.py of django_opentracing lib:</p>

<p><a href=""https://i.stack.imgur.com/wRQ9m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wRQ9m.png"" alt=""enter image description here""></a></p>

<p>And the result:</p>

<p><a href=""https://i.stack.imgur.com/LgBnA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LgBnA.png"" alt=""enter image description here""></a></p>
"
Jaeger,59076610,59076369,1,"2019/11/27, 20:38:47",False,"2019/11/27, 20:38:47",12299,369759,0,"<p>I see..</p>

<p>I thought <strong>@Traced</strong> will be somehow propagated to my db-services/repositories. No, I have to put it explicitly:</p>

<pre><code>import org.eclipse.microprofile.opentracing.Traced;

@Traced        // &lt;&lt; -- here it is
@Singleton
public class MarketPgRepository implements MarketRepository {

    @Inject
    PgPool client;
</code></pre>

<p>That fixes the issue.</p>
"
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049,11207414,0,"<p>According to the documentation <a href=""https://istio.io/docs/tasks/telemetry/gateways/"" rel=""nofollow noreferrer"">Remotely Accessing Telemetry Addons</a>. There are different ways how to acces telemetry. </p>

<p>The Recommended way is to create Secure acces using https instead of http.</p>

<p><strong>Note for both methods:</strong></p>

<blockquote>
  <p>This option covers securing the transport layer only. You should also configure the telemetry addons to require authentication when exposing them externally.</p>
</blockquote>

<p>Please note that jaeger itself doesn't support authentication methods <a href=""https://github.com/jaegertracing/jaeger/issues/219"" rel=""nofollow noreferrer"">github</a> and workaround using Apache httpd server <a href=""https://medium.com/@larsmilland01/secure-architecture-for-jaeger-with-apache-httpd-reverse-proxy-on-openshift-f31983fad400"" rel=""nofollow noreferrer"">here</a>.</p>

<ol>
<li><p>With your recruitments you can use Gateways (SDS) <a href=""https://istio.io/docs/tasks/traffic-management/ingress/secure-ingress-sds/"" rel=""nofollow noreferrer"">with self-signed certificates</a>:</p>

<p><strong>a</strong>.) Make sure your that during istio instalation youe have enabled SDS at ingress gateway <code>--set gateways.istio-ingressgateway.sds.enabled=true</code> and <code>--set tracing.enabled=true</code> for tacing purposes.</p>

<p><strong>b</strong>.) Create self signed certificates for testing purposes you can use this <a href=""https://github.com/nicholasjackson/mtls-go-example"" rel=""nofollow noreferrer"">example and repository</a>.</p>

<p><strong>c</strong>.) Please follow <a href=""https://istio.io/docs/tasks/traffic-management/ingress/secure-ingress-sds/#configure-a-tls-ingress-gateway-using-sds"" rel=""nofollow noreferrer"">Generate client and server certificates and keys</a>  and <a href=""https://istio.io/docs/tasks/traffic-management/ingress/secure-ingress-sds/#configure-a-tls-ingress-gateway-using-sds"" rel=""nofollow noreferrer"">Configure a TLS ingress gateway using SDS</a>.</p></li>
<li><p>Create Virtualservice and Gateway:</p></li>
</ol>

<hr>

<pre><code>apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: mygateway
spec:
  selector:
    istio: ingressgateway # use istio default ingress gateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: ""httpbin-credential"" # must be the same as secret crated in the step 2.
    hosts:
    - ""httpbin.example.com"" ## You can apply ""*"" for all hosts

apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: tracing
spec:
  hosts:
  - ""httpbin.example.com"" ## You can apply ""*"" for all hosts
  gateways:
  - mygateway
  http:
  - match:
    - port: 443
    route:
    - destination:
        port:
          number: 80
        host: tracing.istio-system.svc.cluster.local

curl -kvI https ://xx.xx.xx.xx/
*   Trying xx.xx.xx.xx...
* TCP_NODELAY set
* Connected to xx.xx.xx.xx (xx.xx.xx.xx) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256

* ALPN, server accepted to use h2
&gt; HEAD / HTTP/1.1
&gt; Host: xx.xx.xx.xx
&gt; User-Agent: curl/7.52.1
&gt; Accept: */*
&gt;
* Connection state changed (MAX_CONCURRENT_STREAMS updated)!
&lt; HTTP/2 200
HTTP/2 200
&lt; content-type: text/html; charset=utf-8
content-type: text/html; charset=utf-8
&lt; date: Thu, 07 Nov 2019 10:01:33 GMT
date: Thu, 07 Nov 2019 10:01:33 GMT
&lt; x-envoy-upstream-service-time: 1
x-envoy-upstream-service-time: 1
&lt; server: istio-envoy
server: istio-envoy
</code></pre>

<p>Hope this help</p>
"
Jaeger,58561231,58514716,0,"2019/10/25, 18:12:58",False,"2019/10/25, 18:12:58",721,1563297,1,"<p>The prometheus-es-exporter provides a way to create metrics using queries.</p>

<p>For further details you can check <a href=""https://github.com/braedon/prometheus-es-exporter#query-metrics"" rel=""nofollow noreferrer"">prometheus-es-exporter#query-metrics</a></p>
"
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1,4734979,0,"<p>Great question and a very popular one too. In short, yes, code changes are required. Not just in one service but in all the services that a request will go through. You need to instrument all services to get continuous traces that will be able to tell you the story of a request as it travels through the system. </p>
"
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642,2255344,1,"<p>I believe you can reuse Elasticsearch for multiple purposes - each would use a different set of indices, so separation is good. </p>

<p>from: <a href=""https://www.jaegertracing.io/docs/1.11/deployment/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.11/deployment/</a> :</p>

<blockquote>
  <p>Collectors require a persistent storage backend. Cassandra and Elasticsearch are the primary supported storage backends</p>
</blockquote>

<p>Tying the networking all together, a docker-compose example:
<a href=""https://stackoverflow.com/questions/51785812/how-to-configure-jaeger-with-elasticsearch"">How to configure Jaeger with elasticsearch?</a></p>
"
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433,243104,1,"<p>While this isn't exactly what you asked, it sounds like what you're trying to achieve is seeing tracing for your JMS calls in Jaegar. If that is the case, you could use an OpenTracing tracing solution for JMS or ActiveMQ to report tracing data directly to Jaegar. Here's one potential solution I found with a quick google. There may be others.</p>

<p><a href=""https://github.com/opentracing-contrib/java-jms"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-jms</a></p>
"
Jaeger,57420811,57419866,0,"2019/08/09, 00:26:00",False,"2019/08/09, 00:26:00",121,5799778,0,"<p>Maybe you should check whether the application services which you set up in a hurry are both in the same azure resource group as the VM running the Jaeger all-in-one instance, otherwise the second application might not be able to communicate with the Jaeger instance at all.</p>
"
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313,524946,0,"<p>It doesn't really matter in which language your individual microservices are written, you should see them all in the same trace. Given that you are seeing three traces instead of one trace with three spans, it appears that the context propagation isn't working. Check your HTTP client in your nodejs services, they should perform the <a href=""https://opentracing.io/docs/overview/inject-extract/"" rel=""nofollow noreferrer"">""inject"" operation</a>. Your service ""B"" and ""C"" should then perform the ""extract"" operation.</p>

<p>If you haven't yet, check <a href=""https://github.com/yurishkuro/opentracing-tutorial/tree/master/nodejs"" rel=""nofollow noreferrer"">Yuri Shkuro's OpenTracing Tutorial</a>. The lesson 3 is about the context propagation, including the inject and extract operations.</p>

<p>I'm not quite sure how it works in the NodeJS world, but in Java, it should be sufficient to have the <a href=""https://github.com/opentracing-contrib/java-web-servlet-filter"" rel=""nofollow noreferrer"">opentracing-contrib/java-web-servlet-filter</a> instrumentation library in your classpath, as it would register the necessary pieces in the right hooks and make the trace context available for each incoming HTTP request.</p>
"
Jaeger,57039417,57038416,0,"2019/07/15, 15:06:42",False,"2019/07/15, 15:06:42",4887,6700019,0,"<p>It seems that PyInstaller can't resolve <a href=""https://github.com/jaegertracing/jaeger-client-python"" rel=""nofollow noreferrer""><code>jaeger_client</code></a> import. So an easy way is to just edit your spec file and add the whole <code>jaeger_client</code> library as a <a href=""https://pythonhosted.org/PyInstaller/advanced-topics.html#the-toc-and-tree-classes"" rel=""nofollow noreferrer""><code>Tree</code></a> class:</p>

<pre><code># -*- mode: python -*-

block_cipher = None


a = Analysis(['script.py'],
             ...)
a.datas += Tree('&lt;python_path&gt;/Lib/site-packages/jaeger_client', prefix='./jaeger_client')
pyz = PYZ(a.pure, a.zipped_data,
             cipher=block_cipher)
...
</code></pre>

<p>And generate your executable with <code>pyinstaller script.spec</code>.</p>
"
Jaeger,55241560,55236000,1,"2019/03/19, 14:53:31",False,"2019/03/19, 14:53:31",9399,502575,0,"<p>You can create a <a href=""https://kubernetes.io/docs/concepts/services-networking/service/#nodeport"" rel=""nofollow noreferrer"">NodePort</a> service using the <code>app: jaeger</code> selector to expose the UI outside the cluster.</p>
"
Jaeger,55487339,55236000,0,"2019/04/03, 07:54:28",False,"2019/04/03, 07:54:28",44552,308174,0,"<p><code>kubectl port-forward</code> command default is expose to <code>localhost</code> network only, try to add <code>--address 0.0.0.0</code></p>

<pre><code>$ kubectl port-forward -n istio-system \
 $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath=’{.items[0].metadata.name}’) \
  --address 0.0.0.0 16686:16686 &amp;
</code></pre>

<p>see <a href=""https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#port-forward"" rel=""nofollow noreferrer"">kubectl command reference</a></p>
"
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244,5564578,1,"<p>There are several ways of doing this. The <code>port-forward</code> works fine on Google Cloud Shell. If you are using GKE, then I strongly recommend using Cloud Shell, and <code>port-forward</code> as it is the easiest way. On other clouds, I don't know.</p>

<p>What is suggesting Stefan would work. You can edit the jaeger service with <code>kubectl edit svc jaeger-query</code>, then change the type of the service from <code>ClusterIP</code> to <code>NodePort</code>. Finally, you can access the service with <code>NODE_IP:PORT</code> (any node). If you do <code>kubectl get svc</code>, you will see the new port assigned to the service.
Note: You might need to open a firewall rule for that port.</p>

<p>You can also make the service type <code>LoadBalancer</code>, if you have a control plane to set up an external IP address. This would be a more expensive solution, but you would have a dedicated external IP address for your service.</p>

<p>There are more ways, but I would say these are the appropriate ones.</p>
"
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654,955379,0,"<p>This issue looks more to have to do with Java it self then either Opentracing and Jaeger. as <code>ex.getStackTrace()</code> is more of the problem. As it should be more like </p>

<pre><code>StringWriter errors = new StringWriter();
ex.printStackTrace(new PrintWriter(errors));
span.setTag(""error"", true);
span.log(ImmutableMap.of(""stack"", errors));
</code></pre>

<p>Problem solved.</p>
"
Jaeger,54867389,54675467,0,"2019/02/25, 15:32:18",False,"2019/02/25, 15:32:18",13313,524946,0,"<p>Setting a baggage item is <em>not</em> the same as setting an HTTP header. You should use your HTTP client (not shown in your example) to set the HTTP header.</p>

<p>Baggage items might or not be available as individual HTTP headers: it's a detail implementation of the underlying tracer, such as Jaeger's.</p>
"
Jaeger,54367285,54365010,0,"2019/01/25, 16:30:16",False,"2019/01/25, 16:30:16",13313,524946,0,"<p>Each ""dot"" would be a new child node in the YAML file, like:</p>

<pre><code>es:
  tags-as-fields:
    all: true
  index-prefix: myteam-jaeger
  server-urls: ""http://ip-server:9200""
</code></pre>

<p>Make sure to run the process with the env var SPAN_STORAGE_TYPE set to elasticsearch, like:</p>

<pre><code>SPAN_STORAGE_TYPE=elasticsearch jaeger-all-in-one --config-file=/etc/jaeger-config.yaml
</code></pre>

<p>(as seen on <a href=""https://github.com/jaegertracing/jaeger/issues/1299"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger/issues/1299</a>)</p>
"
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313,524946,1,"<blockquote>
  <p>So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?</p>
</blockquote>

<p>Using the sampler type as <code>const</code> with <code>1</code> as the value means that you are sampling everything.</p>

<blockquote>
  <p>Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes. I would like to understand this configuration spec more but not able to.</p>
</blockquote>

<p>There are several things that might be happening. You might not be closing spans, for instance. I recommend reading the following two blog posts to try to understand what might be happening:</p>

<p><a href=""https://medium.com/jaegertracing/help-something-is-wrong-with-my-jaeger-installation-68874395a7a6"" rel=""nofollow noreferrer"">Help! Something is wrong with my Jaeger installation!</a></p>

<p><a href=""https://medium.com/jaegertracing/the-life-of-a-span-ee508410200b"" rel=""nofollow noreferrer"">The life of a span</a></p>
"
Jaeger,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360,1796107,0,"<p>Your best chance is to get the data from whatever storage the Jaeger collector is using (Cassandra, Elastic.) (<a href=""https://www.jaegertracing.io/docs/1.6/deployment/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.6/deployment/</a> )</p>

<p>My suggestion is to store in Elastic and use Kibana to accomplish what you need.</p>
"
Jaeger,65919685,52145774,0,"2021/01/27, 14:59:44",False,"2021/01/27, 14:59:44",1869,3511252,0,"<p>Old topic but the current version of Jaeger Query UI is a single page app and has an underlying API that allows the same queries capabilities as the UI.</p>
"
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336,1773866,-1,"<p>You're using <code>Camden</code> release train with boot <code>2.0</code> and Sleuth <code>2.0</code>. That's completely incompatible. Please generate a project from start.spring.io from scratch, please don't put any versions manually for spring cloud projects, and please try again. Try using <code>Finchley</code> release train instead of <code>Camden</code></p>
"
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313,524946,1,"<blockquote>
  <p>The problem is that if I kill the Docker running the jaeger collector- systemctl stop docker and later restart docker and jaegertracing/all-in-one, the services are no longer up at <a href=""http://localhost:16686/api/services"" rel=""nofollow noreferrer"">http://localhost:16686/api/services</a></p>
</blockquote>

<p>That's because you are using the in-memory storage. If you stop and start the container, the storage is reset, so, you'll effectively lose your data. For production purposes, you should use a backing storage like Cassandra or Elasticsearch.</p>

<blockquote>
  <p>Does the Jaeger collector needs to be running before starting the Jaeger clients? </p>
</blockquote>

<p>No, but spans reported by clients when the collector isn't available might get dropped. Note that clients will send spans to the agent by default, and will not contact the collector directly. So, if the agent isn't available, spans might get dropped as well.</p>

<blockquote>
  <p>how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?</p>
</blockquote>

<p>Use the configuration option <code>--memory.max-traces</code>. With this option, older traces will get overwritten by new ones once this limit is reached.</p>
"
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313,524946,1,"<blockquote>
  <p>But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces</p>
</blockquote>

<p>That's because the Jaeger client will, by default, send the spans via UDP to an agent at <code>localhost</code>. When your application is running in a Docker container, your <code>localhost</code> there is the container itself, so that the spans are lost.</p>

<p>As you are linking the Jaeger container with your application, you may want to get it solved by exporting the env var <code>JAEGER_AGENT_HOST</code> to <code>jaeger</code>.</p>
"
Jaeger,50339003,49909075,0,"2018/05/15, 00:05:40",True,"2018/05/15, 00:05:40",611,118116,1,"<p>Turns out I don't need neither Zipkin nor Jaeger to have my traces on Stackdriver. All that is needed is a deployment of <a href=""http://gcr.io/stackdriver-trace-docker/zipkin-collector"" rel=""nofollow noreferrer"">zipkin-collector</a> and a service to point to it and all my traces are now reporting as expected on GCP Stackdriver.</p>
"
Jaeger,49626058,49624555,0,"2018/04/03, 12:00:37",False,"2018/04/04, 08:31:53",81,2819181,0,"<p>Not jaeger, able to send traces to zipkin server, using zipkin-simple. 
Related code is in repository <a href=""https://github.com/debmalya/calculator"" rel=""nofollow noreferrer"">https://github.com/debmalya/calculator</a></p>

<pre><code>import zipkinSimple from 'zipkin-simple'
const zipkinTracerSimple = new zipkinSimple({
             debug: true,
             host: ""localhost"",
             port: ""9411"",
             path: ""/api/v2/spans"",
             sampling: 1.0,
})

var zipkinSimpleTraceData

zipkinSimpleTraceData= zipkinTracerSimple.getChild(zipkinSimpleTraceData);
    zipkinSimpleTraceData = 
zipkinTracerSimple.sendClientSend(zipkinSimpleTraceData, {
     service: '&lt;service_name&gt;',
     name: ""&lt;span_name&gt;""
   }) 
</code></pre>
"
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21,9593079,2,"<p>node-jaeger-client currently doesn't run in the browser. There is ongoing <a href=""https://github.com/jaegertracing/jaeger-client-node/issues/109"" rel=""nofollow noreferrer"">work</a> to make jaeger-client browser friendly. This issue: <a href=""https://stackoverflow.com/questions/37418513/readfilesync-is-not-a-function"">readFileSync is not a function</a> contains relevant information to why you're seeing the error message. Essentially, you're trying to run jaeger-client (a nodejs library) using react-scripts which doesn't contain the modules that jaeger-client needs.</p>
"
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471,2673284,0,"<p>There are two issues here. One is that your code sets the port for Jaeger client to 5775. This port expects a different data model than what Node.js client sends, you can remove the <code>agentHost</code> and <code>agentPort</code> parameters and rely on defaults.</p>

<p>The second issue is that you're running the Docker image without exposing the required UDP port. The correct command is shown in the <a href=""http://jaeger.readthedocs.io/en/latest/getting_started/"" rel=""nofollow noreferrer"">documentation</a>, as of today it should be this (one long line):</p>

<pre><code>docker run -d -p5775:5775/udp -p6831:6831/udp -p6832:6832/udp \
    -p5778:5778 -p16686:16686 -p14268:14268 jaegertracing/all-in-one:latest
</code></pre>
"
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324,5600810,0,"<p>Liberty does not have Open Tracing Tracer implementation for Jaeger yet.  We have a sample Tracer implementation for Zipkin.  You can find it at <a href=""https://github.com/WASdev/sample.opentracing.zipkintracer"" rel=""nofollow noreferrer"">https://github.com/WASdev/sample.opentracing.zipkintracer</a>.  Jaegar claims it is backward compatible with Zipkin by accepting spans in Zipkin formats over HTTP.</p>

<p>Feel free to open a RFE at <a href=""https://developer.ibm.com/wasdev/help/submit-rfe/"" rel=""nofollow noreferrer"">https://developer.ibm.com/wasdev/help/submit-rfe/</a></p>
"
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313,524946,0,"<p>This is most likely caused by the static assets <em>not</em> being included in the binary. You can try that out by running the binary you compiled.</p>

<p>Instead of compiling on your own, a better approach would be to get the official binaries from the releases page and build your Docker container using that.</p>

<p><a href=""https://github.com/jaegertracing/jaeger/releases/latest"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger/releases/latest</a></p>
"
Jaeger,56227367,56156809,0,"2019/05/20, 22:55:44",True,"2019/05/20, 22:55:44",386,6692626,9,"<p>This is a limitation in the Serilog logger factory implementation; in particular, Serilog currently ignores added providers and assumes that Serilog Sinks will replace them instead.</p>

<p>So, the solutions is implementaion a simple <code>WriteTo.OpenTracing()</code> method to connect Serilog directly to <code>OpenTracing</code></p>

<pre><code>public class OpenTracingSink : ILogEventSink
{
    private readonly ITracer _tracer;
    private readonly IFormatProvider _formatProvider;

    public OpenTracingSink(ITracer tracer, IFormatProvider formatProvider)
    {
        _tracer = tracer;
        _formatProvider = formatProvider;
    }

    public void Emit(LogEvent logEvent)
    {
        ISpan span = _tracer.ActiveSpan;

        if (span == null)
        {
            // Creating a new span for a log message seems brutal so we ignore messages if we can't attach it to an active span.
            return;
        }

        var fields = new Dictionary&lt;string, object&gt;
        {
            { ""component"", logEvent.Properties[""SourceContext""] },
            { ""level"", logEvent.Level.ToString() }
        };

        fields[LogFields.Event] = ""log"";

        try
        {
            fields[LogFields.Message] = logEvent.RenderMessage(_formatProvider);
            fields[""message.template""] = logEvent.MessageTemplate.Text;

            if (logEvent.Exception != null)
            {
                fields[LogFields.ErrorKind] = logEvent.Exception.GetType().FullName;
                fields[LogFields.ErrorObject] = logEvent.Exception;
            }

            if (logEvent.Properties != null)
            {
                foreach (var property in logEvent.Properties)
                {
                    fields[property.Key] = property.Value;
                }
            }
        }
        catch (Exception logException)
        {
            fields[""mbv.common.logging.error""] = logException.ToString();
        }

        span.Log(fields);
    }
}

public static class OpenTracingSinkExtensions
{
    public static LoggerConfiguration OpenTracing(
              this LoggerSinkConfiguration loggerConfiguration,
              IFormatProvider formatProvider = null)
    {
        return loggerConfiguration.Sink(new OpenTracingSink(GlobalTracer.Instance, formatProvider));
    }
}
</code></pre>
"
Jaeger,62140570,54597464,0,"2020/06/01, 23:33:01",False,"2020/06/01, 23:33:01",1111,898472,0,"<p>I had this problem while using gunicorn with gevent as the worker class. To resolve and get cloud traces working the solution was to monkey patch grpc like so </p>

<pre><code>from gevent import monkey
monkey.patch_all()

import grpc.experimental.gevent as grpc_gevent
grpc_gevent.init_gevent()
</code></pre>

<p>See <a href=""https://github.com/grpc/grpc/issues/4629#issuecomment-376962677"" rel=""nofollow noreferrer"">https://github.com/grpc/grpc/issues/4629#issuecomment-376962677</a></p>
"
Jaeger,32155765,32155133,4,"2015/08/22, 14:48:32",True,"2016/11/03, 17:57:34",nan,nan,13,"<p>For your exact question create a character class</p>

<pre><code># Matches any character that isn't a \ or ""
/[^\\""]/
</code></pre>

<p>And then you can just add * on the end to get 0 or unlimited number of them or alternatively 1 or an unlimited number with +</p>

<pre><code>/[^\\""]*/
</code></pre>

<p>or</p>

<pre><code>/[^\\""]+/
</code></pre>

<p>Also there is this below, found at <a href=""https://regex101.com/"" rel=""noreferrer"">https://regex101.com/</a> under the library tab when searching for json</p>

<pre><code>/(?(DEFINE)
# Note that everything is atomic, JSON does not need backtracking if it's valid
# and this prevents catastrophic backtracking
(?&lt;json&gt;(?&gt;\s*(?&amp;object)\s*|\s*(?&amp;array)\s*))
(?&lt;object&gt;(?&gt;\{\s*(?&gt;(?&amp;pair)(?&gt;\s*,\s*(?&amp;pair))*)?\s*\}))
(?&lt;pair&gt;(?&gt;(?&amp;STRING)\s*:\s*(?&amp;value)))
(?&lt;array&gt;(?&gt;\[\s*(?&gt;(?&amp;value)(?&gt;\s*,\s*(?&amp;value))*)?\s*\]))
(?&lt;value&gt;(?&gt;true|false|null|(?&amp;STRING)|(?&amp;NUMBER)|(?&amp;object)|(?&amp;array)))
(?&lt;STRING&gt;(?&gt;""(?&gt;\\(?&gt;[""\\\/bfnrt]|u[a-fA-F0-9]{4})|[^""\\\0-\x1F\x7F]+)*""))
(?&lt;NUMBER&gt;(?&gt;-?(?&gt;0|[1-9][0-9]*)(?&gt;\.[0-9]+)?(?&gt;[eE][+-]?[0-9]+)?))
)
\A(?&amp;json)\z/x
</code></pre>

<p>This should match any valid json, you can also test it at the website above</p>

<p>EDIT:</p>

<p><a href=""https://regex101.com/library/tA9pM8"" rel=""noreferrer"">Link to the regex</a></p>
"
Jaeger,51790666,51785812,2,"2018/08/10, 19:32:28",True,"2019/07/15, 20:44:47",373,10206966,14,"<p>After searching a solution for some time, I found a docker-compose.yml file which had the Jaeger Query,Agent,collector and Elasticsearch configurations. </p>

<p>docker-compose.yml</p>

<pre> <code> version: ""3""

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.3.1
    networks:
      - elastic-jaeger
    ports:
      - ""127.0.0.1:9200:9200""
      - ""127.0.0.1:9300:9300""
    restart: on-failure
    environment:
      - cluster.name=jaeger-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - transport.host=127.0.0.1
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data

  jaeger-collector:
    image: jaegertracing/jaeger-collector
    ports:
      - ""14269:14269""
      - ""14268:14268""
      - ""14267:14267""
      - ""9411:9411""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--es.num-shards=1"",
      ""--es.num-replicas=0"",
      ""--log-level=error""
    ]
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    hostname: jaeger-agent
    command: [""--collector.host-port=jaeger-collector:14267""]
    ports:
      - ""5775:5775/udp""
      - ""6831:6831/udp""
      - ""6832:6832/udp""
      - ""5778:5778""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - no_proxy=localhost
    ports:
      - ""16686:16686""
      - ""16687:16687""
    networks:
      - elastic-jaeger
    restart: on-failure
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--span-storage.type=elasticsearch"",
      ""--log-level=debug""
    ]
    depends_on:
      - jaeger-agent

volumes:
  esdata:
    driver: local

networks:
  elastic-jaeger:
    driver: bridge </code></pre>

<p>The docker-compose.yml file installs the elasticsearch, Jaeger collector,query and agent.</p>

<p>Install docker and docker compose first
<a href=""https://docs.docker.com/compose/install/#install-compose"" rel=""noreferrer"">https://docs.docker.com/compose/install/#install-compose</a></p>

<p>Then, execute these commands in order</p>

<pre><code> 
1. sudo docker-compose up -d elasticsearch

2. sudo docker-compose up -d 

3. sudo docker ps -a
</code></pre>

<p>start all the docker containers - Jaeger agent,collector,query and elasticsearch.</p>

<p>sudo docker start container-id</p>

<p>access -> <a href=""http://localhost:16686/"" rel=""noreferrer"">http://localhost:16686/</a></p>
"
Jaeger,61147810,51785812,0,"2020/04/10, 23:00:13",False,"2020/04/10, 23:00:13",71,9731647,0,"<p>If Jaeger needs to be set up in Kubernetes cluster as a helm chart, one can use this: <a href=""https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger</a>
It can delploy either Elasticsearch or Cassandara as a storage backend. Which is just a matter of right value being passed in to the chart:</p>

<pre><code>storage:
  type: elasticsearch
</code></pre>

<p>This section shows the helm command as an example:
<a href=""https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger#installing-the-chart-using-a-new-elasticsearch-cluster"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/helm-charts/tree/master/charts/jaeger#installing-the-chart-using-a-new-elasticsearch-cluster</a></p>
"
Jaeger,62418210,51785812,0,"2020/06/17, 00:57:19",False,"2020/06/17, 00:57:19",488,5789008,1,"<p>If you would like to deploy the Jaeger with Elasticsearch and Kibana to quickly validate and check the stack e.g. in kind or Minikube, the following snippet may help you.</p>

<pre><code>#######################
## Add jaegertracing helm repo
#######################
helm repo add jaegertracing
https://jaegertracing.github.io/helm-charts

#######################
## Create a target namespace
#######################
kubectl create namespace observability

#######################
## Check and use the jaegertracing helm chart
#######################
helm search repo jaegertracing
helm install -n observability jaeger-operator jaegertracing/jaeger-operator

#######################
## Use the elasticsearch all-in-one operator
#######################
kubectl apply -f https://download.elastic.co/downloads/eck/1.1.2/all-in-one.yaml

#######################
## Create an elasticsearch deployment
#######################
cat &lt;&lt;EOF | kubectl apply -n observability -f -
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.7.0
  nodeSets:
  - name: default
    count: 1
    config:
      node.master: true
      node.data: true
      node.ingest: true
      node.store.allow_mmap: false
EOF

PASSWORD=$(kubectl get secret -n observability quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode)
kubectl create secret -n observability generic jaeger-secret --from-literal=ES_PASSWORD=${PASSWORD} --from-literal=ES_USERNAME=elastic

#######################
## Kibana to visualize the trace data
#######################

cat &lt;&lt;EOF | kubectl apply -n observability -f -
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: quickstart
spec:
  version: 7.7.0
  count: 1
  elasticsearchRef:
    name: quickstart
EOF

kubectl port-forward -n observability service/quickstart-kb-http 5601
## To get the pw
kubectl get secret quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo

login:
https://localhost:5601
username: elastic
pw: &lt;see above to outcome of the command&gt;

#######################
## Deploy a jaeger tracing application
#######################
cat &lt;&lt;EOF | kubectl apply -n observability -f -
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-prod
spec:
  agent:
    strategy: DaemonSet
  strategy: production
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: https://quickstart-es-http:9200
        tls:
          ca: /es/certificates/ca.crt
        num-shards: 1
        num-replicas: 0
    secretName: jaeger-secret
  volumeMounts:
    - name: certificates
      mountPath: /es/certificates/
      readOnly: true
  volumes:
    - name: certificates
      secret:
        secretName: quickstart-es-http-certs-public
EOF

## to visualize it
kubectl --namespace observability port-forward simple-prod-query-&lt;POP ID&gt; 16686:16686

#######################
## To test the setup
## Of course if you set it up to another namespace it will work, the only thing that matters is the collector URL and PORT
#######################
cat &lt;&lt;EOF | kubectl apply -n observability -f -
apiVersion: v1
kind: List
items:
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: jaeger-k8s-example
    labels:
      app: jaeger-k8s-example
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: jaeger-k8s-example
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          app: jaeger-k8s-example
      spec:
        containers:
          - name: jaeger-k8s-example
            env:
            - name: JAEGER_COLLECTOR_URL
              value: ""simple-prod-collector.observability.svc.cluster.local""
            - name: JAEGER_COLLECTOR_PORT
              value: ""14268""
            image: norbertfenk/jaeger-k8s-example:latest
            imagePullPolicy: IfNotPresent
EOF
</code></pre>
"
Jaeger,63530756,51785812,0,"2020/08/22, 01:23:06",False,"2020/09/02, 13:12:23",30911,2000548,0,"<p>For people who are using OpenTelemetry, Jaeger, and Elasticsearch, here is the way.</p>
<p>Note the image being used are <code>jaegertracing/jaeger-opentelemetry-collector</code> and <code>jaegertracing/jaeger-opentelemetry-agent</code>.</p>
<pre><code>version: '3.8'

services:
  collector:
    image: otel/opentelemetry-collector:latest
    command: [&quot;--config=/conf/opentelemetry-collector.config.yaml&quot;, &quot;--log-level=DEBUG&quot;]
    volumes:
      - ./opentelemetry-collector.config.yaml:/conf/opentelemetry-collector.config.yaml
    ports:
      - &quot;9464:9464&quot;
      - &quot;55680:55680&quot;
      - &quot;55681:55681&quot;
    depends_on:
      - jaeger-collector

  jaeger-collector:
    image: jaegertracing/jaeger-opentelemetry-collector
    command: [&quot;--es.num-shards=1&quot;, &quot;--es.num-replicas=0&quot;, &quot;--es.server-urls=http://elasticsearch:9200&quot;, &quot;--collector.zipkin.host-port=:9411&quot;]
    ports:
      - &quot;14250&quot;
      - &quot;14268&quot;
      - &quot;9411&quot;
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - LOG_LEVEL=debug
    restart: on-failure
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-opentelemetry-agent
    command: [&quot;--config=/config/otel-agent-config.yml&quot;, &quot;--reporter.grpc.host-port=jaeger-collector:14250&quot;]
    volumes:
      - ./:/config/:ro
    ports:
      - &quot;6831/udp&quot;
      - &quot;6832/udp&quot;
      - &quot;5778&quot;
    restart: on-failure
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    command: [&quot;--es.num-shards=1&quot;, &quot;--es.num-replicas=0&quot;, &quot;--es.server-urls=http://elasticsearch:9200&quot;]
    ports:
      - &quot;16686:16686&quot;
      - &quot;16687&quot;
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - LOG_LEVEL=debug
    restart: on-failure
    depends_on:
      - elasticsearch

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.9.0
    environment:
      - discovery.type=single-node
    ports:
      - &quot;9200/tcp&quot;
</code></pre>
<p>Then just need</p>
<pre><code>docker-compose up -d
</code></pre>
<p>Reference: <a href=""https://github.com/jaegertracing/jaeger/blob/master/crossdock/jaeger-opentelemetry-docker-compose.yml"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger/blob/master/crossdock/jaeger-opentelemetry-docker-compose.yml</a></p>
"
Jaeger,64755733,51785812,0,"2020/11/09, 18:47:27",False,"2020/11/09, 18:47:27",328,1221718,1,"<p>As I mentioned in my comment on the OP's first answer above, I was getting an error when running the docker-compose exactly as given:</p>
<p><code>Error: unknown flag: --collector.host-port</code></p>
<p>I think this CLI flag has been deprecated by the Jaeger folks since that answer was written. So I poked around in the jaeger-agent documentation a bit:</p>
<ol>
<li><a href=""https://www.jaegertracing.io/docs/1.20/deployment/#discovery-system-integration"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.20/deployment/#discovery-system-integration</a></li>
<li><a href=""https://www.jaegertracing.io/docs/1.20/cli/#jaeger-agent"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.20/cli/#jaeger-agent</a></li>
</ol>
<p>And I got this to work with a couple of small modifications:</p>
<ol>
<li>I added port range <code>&quot;14250:14250&quot;</code> to the jaeger-collector ports</li>
<li>I updated the jaeger-agent command input with: <code>command: [&quot;--reporter.grpc.host-port=jaeger-collector:14250&quot;]</code></li>
<li>Finally, I updated the ellastic search version in the <code>image</code> tag to the latest version they have available at this time (though I doubt this was required).</li>
</ol>
<p>The updated docker-compose.yaml:</p>
<pre><code>version: &quot;3&quot;

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.9.3
    networks:
      - elastic-jaeger
    ports:
      - &quot;127.0.0.1:9200:9200&quot;
      - &quot;127.0.0.1:9300:9300&quot;
    restart: on-failure
    environment:
      - cluster.name=jaeger-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - transport.host=127.0.0.1
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data

  jaeger-collector:
    image: jaegertracing/jaeger-collector
    ports:
      - &quot;14269:14269&quot;
      - &quot;14268:14268&quot;
      - &quot;14267:14267&quot;
      - &quot;14250:14250&quot;
      - &quot;9411:9411&quot;
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    command: [
      &quot;--es.server-urls=http://elasticsearch:9200&quot;,
      &quot;--es.num-shards=1&quot;,
      &quot;--es.num-replicas=0&quot;,
      &quot;--log-level=error&quot;
    ]
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    hostname: jaeger-agent
    command: [&quot;--reporter.grpc.host-port=jaeger-collector:14250&quot;]
    ports:
      - &quot;5775:5775/udp&quot;
      - &quot;6831:6831/udp&quot;
      - &quot;6832:6832/udp&quot;
      - &quot;5778:5778&quot;
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - no_proxy=localhost
    ports:
      - &quot;16686:16686&quot;
      - &quot;16687:16687&quot;
    networks:
      - elastic-jaeger
    restart: on-failure
    command: [
      &quot;--es.server-urls=http://elasticsearch:9200&quot;,
      &quot;--span-storage.type=elasticsearch&quot;,
      &quot;--log-level=debug&quot;
    ]
    depends_on:
      - jaeger-agent

volumes:
  esdata:
    driver: local

networks:
  elastic-jaeger:
    driver: bridge 
</code></pre>
"
Jaeger,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231,4158442,7,"<p><a href=""https://github.com/opentracing-contrib/java-spring-cloud"" rel=""noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud</a> project automatically sends standard logging to the active span. Just add the following dependency to your pom.xml</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
   &lt;artifactId&gt;opentracing-spring-cloud-starter&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Or use this <a href=""https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core"" rel=""noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core</a> starter if you want only logging integration.</p>
"
Jaeger,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61,9483992,0,"<p>Then I use <strong>opentracing-spring-jaeger-cloud-starter</strong></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-jaeger-cloud-starter&lt;/artifactId&gt;
    &lt;version&gt;2.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>I got just one line in console with current trace and span
i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - my_method</p>

<pre><code>2019-05-20 16:07:59.549 DEBUG 24428 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [632103eb] HTTP POST ""/api""
2019-05-20 16:07:59.552 DEBUG 24428 --- [ctor-http-nio-2] s.w.r.r.m.a.RequestMappingHandlerMapping : [632103eb] Mapped to public reactor.core.publisher.Mono&lt;org.springframework.http.ResponseEntity&lt;model.Response&gt;&gt; service.controller.method(model.Request)
2019-05-20 16:07:59.559 DEBUG 24428 --- [ctor-http-nio-2] .s.w.r.r.m.a.RequestBodyArgumentResolver : [632103eb] Content-Type:application/json
2019-05-20 16:08:01.450  INFO 24428 --- [ctor-http-nio-2] i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - method
2019-05-20 16:08:01.450 DEBUG 24428 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [632103eb] Completed 200 OK
</code></pre>

<p>Then I use <strong>spring-cloud-starter-sleuth</strong></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>I got the Trace and Spans like [my-service,90e1114e35c897d6,90e1114e35c897d6,false] in each line and it's helpfull for filebeat in ELK</p>

<pre><code>2019-05-20 16:15:38.646 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [3e578505] HTTP POST ""/api""
2019-05-20 16:15:38.662 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Received a request to uri [/api]
2019-05-20 16:15:38.667 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Handled receive of span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:38.713 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] s.w.r.r.m.a.RequestMappingHandlerMapping : [3e578505] Mapped to public reactor.core.publisher.Mono&lt;org.springframework.http.ResponseEntity&lt;model.Response&gt;&gt; service.controller.method(model.Request)
2019-05-20 16:15:38.727 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] .s.w.r.r.m.a.RequestBodyArgumentResolver : [3e578505] Content-Type:application/json
2019-05-20 16:15:39.956 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [gine-1-thread-1] .s.w.r.r.m.a.ResponseEntityResultHandler : Using 'application/json;charset=UTF-8' given [*/*] and supported [application/json;charset=UTF-8, application/*+json;charset=UTF-8, text/event-stream]
2019-05-20 16:15:40.009 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Adding a method tag with value [method] to a span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.009 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Adding a class tag with value [Controller] to a span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.010 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Handled send of NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.021 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [3e578505] Completed 200 OK
</code></pre>

<p>How could I get the same log in console using opentracing-spring-jaeger-cloud-starter ?</p>

<p>my opentracing config</p>

<pre><code>opentracing:
  jaeger:
    enabled: true
    enable-b3-propagation: true
    log-spans: true
    const-sampler:
      decision: true
    http-sender:
      url: http://jaeger-collector:14268/api/traces

</code></pre>
"
Jaeger,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11,10465104,1,"<p>Here is what I did to make jdbc related logs from Logback (Slf4j) write into Jaeger server:</p>

<p>Beginning with Logback config (logback-spring.xml):</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;configuration&gt;
&lt;include resource=""org/springframework/boot/logging/logback/defaults.xml""/&gt;
&lt;springProperty scope=""context"" name=""consoleAppender"" source=""logging.console.enabled"" defaultValue=""false""/&gt;
&lt;property name=""ENV"" value=""${SPRING_PROFILES_ACTIVE:-dev}""/&gt;

&lt;include resource=""org/springframework/boot/logging/logback/console-appender.xml""/&gt;

&lt;jmxConfigurator/&gt;

&lt;appender name=""JSON_CONSOLE"" class=""ch.qos.logback.core.ConsoleAppender""&gt;
    &lt;encoder class=""net.logstash.logback.encoder.LogstashEncoder""&gt;
        &lt;includeMdc&gt;true&lt;/includeMdc&gt;
        &lt;customFields&gt;{""log_type"":""application"",""appname"":""products-rs-load"", ""environment"": ""${ENV}""}
        &lt;/customFields&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;
&lt;appender name=""myAppender"" class=""com.test.MyAppender""&gt;
&lt;/appender&gt;
&lt;root level=""DEBUG""&gt;
    &lt;appender-ref ref=""myAppender""/&gt;
&lt;/root&gt;
&lt;logger name=""org.springframework.boot"" level=""INFO""/&gt;
&lt;logger name=""p6spy"" additivity=""false"" level=""ALL""&gt;
    &lt;appender-ref ref=""myAppender"" /&gt;
&lt;/logger&gt;
&lt;/configuration&gt;
</code></pre>

<p>Here is my appender:</p>

<pre><code>import ch.qos.logback.core.AppenderBase;
public class MyAppender extends AppenderBase {

@Override
protected void append(Object eventObject) {
    LoggingEvent event = (LoggingEvent) eventObject;

    final String loggerName = event.getLoggerName();

    // only DB related operations have to be traced:
    if (!(""p6spy"".equals(loggerName))) {
        return;
    }
    /// Tracer config is straight forward
    Span sp = TracingUtils.buildActiveChildSpan(loggerName, null);

    if (Level.ERROR.equals(event.getLevel())) {
        TracingUtils.setErrorTag(sp);
    }
    Map&lt;String, String&gt; fields = new HashMap&lt;String, String&gt;();
    fields.put(""level"", event.getLevel().toString());
    fields.put(""logger"", loggerName);
    fields.put(""content"", event.getFormattedMessage());
    sp.log(fields);

    sp.finish();
  }
}
</code></pre>
"
Jaeger,56941159,56863669,0,"2019/07/08, 22:06:55",False,"2019/07/08, 22:06:55",504,9187876,2,"<p>No, there is no out-of-the-box possibility to change the HTTP header name.
However, you can enable B3 header propagation with <code>opentracing.jaeger.enable-b3-propagation=true</code>.
To configure Traefik to send the trace data as B3 headers, see <a href=""https://github.com/containous/traefik/blob/master/docs/content/observability/tracing/jaeger.md#propagation"" rel=""nofollow noreferrer"">https://github.com/containous/traefik/blob/master/docs/content/observability/tracing/jaeger.md#propagation</a>.
<code>traceContextHeaderName</code> should also be configured as <code>X-B3-TraceId</code> then.</p>
"
Jaeger,53464346,53459759,2,"2018/11/25, 05:17:34",False,"2018/11/26, 04:04:55",2451,1432067,4,"<p>Based on my experience and reading online, I found this interesting line in Istio <a href=""https://istio.io/help/faq/mixer/"" rel=""nofollow noreferrer"">mixer faq</a></p>

<blockquote>
  <p>Mixer trace generation is controlled by command-line flags: trace_zipkin_url, trace_jaeger_url, and trace_log_spans. If any of those flag values are set, trace data will be written directly to those locations. If no tracing options are provided, Mixer will not generate any application-level trace information.</p>
</blockquote>

<p>Also, if you go deep into mixer <a href=""https://github.com/istio/istio/blob/d6c3ebcaaffd7e45772beefeeb71708ef1588cb4/install/kubernetes/helm/subcharts/mixer/templates/deployment.yaml"" rel=""nofollow noreferrer"">helm chart</a>, you will find traces of Zipkin and Jaeger signifying that it’s mixer that is passing trace info to Jaeger.</p>

<p>I also got confused which reading this line in one of the articles </p>

<blockquote>
  <p>Istio injects a sidecar proxy (Envoy) in the pod in which your application container is running. This sidecar proxy transparently intercepts (iptables magic) all network traffic going in and out of your application. Because of this interception, the sidecar proxy is in a unique position to automatically trace all network requests (HTTP/1.1, HTTP/2.0 &amp; gRPC).</p>
</blockquote>

<p>On Istio mixer documentation, The Envoy sidecar logically calls Mixer before each request to perform precondition checks, and after each request to report telemetry. The sidecar has local caching such that a large percentage of precondition checks can be performed from cache. Additionally, the sidecar buffers outgoing telemetry such that it only calls Mixer infrequently.</p>

<p><strong>Update:</strong> You can enable tracing to understand what happens to a request in Istio and also the role of mixer and envoy. Read more information <a href=""https://istio.io/help/faq/telemetry/#life-of-a-request"" rel=""nofollow noreferrer"">here</a></p>
"
Jaeger,50230156,50173643,1,"2018/05/08, 12:19:17",True,"2018/05/08, 12:19:17",193,9725840,13,"<p>I found the solution to my problem, in case anybody is facing similar issues.</p>

<p>I was missing the environment variable <em>JAEGER_SAMPLER_MANAGER_HOST_PORT</em>, which is necessary if the (default) remote controlled sampler is used for tracing.</p>

<p>This is the working docker-compose file:</p>

<pre><code>version: '2'

services:           
    demo:
            build: opentracing_demo/.
            ports: 
                    - ""8080:8080""
            environment: 
                    - JAEGER_SERVICE_NAME=hello_service
                    - JAEGER_AGENT_HOST=jaeger
                    - JAEGER_AGENT_PORT=6831     
                    - JAEGER_SAMPLER_MANAGER_HOST_PORT=jaeger:5778
    jaeger: 
            image: jaegertracing/all-in-one:latest
            ports:
                    - ""5775:5775/udp""
                    - ""6831:6831/udp""
                    - ""6832:6832/udp""
                    - ""5778:5778""
                    - ""16686:16686""
                    - ""14268:14268""
                    - ""9411:9411""
</code></pre>
"
Jaeger,66317822,50173643,0,"2021/02/22, 16:41:31",False,"2021/02/23, 08:30:03",89,668438,0,"<p>For anyone finding themselves on this page looking at a simialr issue for jaeger opentracing in .net core below is a working docker compose snippet and working code in Startup.cs. The piece I was missing was getting jaeger to read the environment variables from docker-compose as by default it was trying to send the traces over udp on localhost.</p>
<pre><code>version: '3.4'

services:
  jaeger-agent:
    container_name: 'tracing.jaeger.agent'
    image: jaegertracing/all-in-one:latest
    networks:
        - jaeger-demo
    ports:
        - &quot;5775:5775/udp&quot;
        - &quot;6831:6831/udp&quot;
        - &quot;6832:6832/udp&quot;
        - &quot;5778:5778/tcp&quot;
        - &quot;16686:16686&quot;
        - &quot;14268:14268&quot;
        - &quot;9411:9411&quot;
    environment:
        - LOG_LEVEL=debug
    labels:
        NAME: &quot;jaeger-agent&quot;

  orderService:
    container_name: 'tracing.orders.api'
    image: ${DOCKER_REGISTRY-}orderservice.api
    build:
      context: .
      dockerfile: OrdersApi/Dockerfile
    networks:
        - jaeger-demo
    ports:
        - &quot;16252:80&quot;
    environment:
        - ASPNETCORE_ENVIRONMENT=Development
        - JAEGER_SERVICE_NAME=OrdersApi
        - JAEGER_AGENT_HOST=jaeger-agent
        - JAEGER_AGENT_PORT=6831
        - JAEGER_SAMPLER_TYPE=const
        - JAEGER_SAMPLER_PARAM=1
    depends_on: 
      - jaeger-agent

  customerService:
    container_name: 'tracing.customers.api'
    image: ${DOCKER_REGISTRY-}customerservice.api
    build:
      context: .
      dockerfile: CustomerApi/Dockerfile
    networks:
        - jaeger-demo
    ports:
        - &quot;17000:80&quot;
    environment:
        - ASPNETCORE_ENVIRONMENT=Development
        - JAEGER_SERVICE_NAME=CustomersApi
        - JAEGER_AGENT_HOST=jaeger-agent
        - JAEGER_AGENT_PORT=6831
        - JAEGER_SAMPLER_TYPE=const
        - JAEGER_SAMPLER_PARAM=1
    depends_on: 
      - jaeger-agent

networks: 
    jaeger-demo:
</code></pre>
<p>Add following nuget packages to your api's.</p>
<pre><code>&lt;PackageReference Include=&quot;OpenTracing.Contrib.NetCore&quot; Version=&quot;0.6.2&quot; /&gt;
&lt;PackageReference Include=&quot;Jaeger&quot; Version=&quot;0.4.2&quot; /&gt;
</code></pre>
<p>Then inside your Startup.cs Configure method, you will need to configure jaeger and opentracing as below.</p>
<pre><code>     services.AddSingleton&lt;ITracer&gt;(serviceProvider =&gt;
     {
        ILoggerFactory loggerFactory = serviceProvider.GetRequiredService&lt;ILoggerFactory&gt;();
     
        Jaeger.Configuration.SenderConfiguration.DefaultSenderResolver = new SenderResolver(loggerFactory).RegisterSenderFactory&lt;ThriftSenderFactory&gt;();
     
        var config = Jaeger.Configuration.FromEnv(loggerFactory);
     
        ITracer tracer = config.GetTracer();
     
        GlobalTracer.Register(tracer);
     
        return tracer;
     });
     
     services.AddOpenTracing();
</code></pre>
"
Jaeger,62995670,57268147,0,"2020/07/20, 15:44:52",False,"2020/07/20, 15:44:52",537,2841947,0,"<p>I was facing similar issue. ConnectionInfo was getting traced but not the SQL statements.
In my case, I had to enable traceWithActiveSpanOnly=true.</p>
<p>For e.g, : jdbc:tracing:h2:mem:test?traceWithActiveSpanOnly=true</p>
<p>After that the statements started getting traced.</p>
<p><a href=""https://github.com/opentracing-contrib/java-jdbc"" rel=""nofollow noreferrer"">Check the documentation of opentracing java-jdbc module here</a></p>
"
Jaeger,44370244,44166525,0,"2017/06/05, 16:35:47",True,"2017/06/05, 16:35:47",8336,1773866,2,"<p>There's an ongoing discussion over here - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/599"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/599</a> . In general we don't explicitly use the OpenTracing API but we are Zipkin compatible in terms of header propagation. You can also manipulate the header names as you wish so if any sort of library you're using requires other header names for span / trace etc. then you can set it yourself as you want to.</p>
"
Jaeger,53370207,44166525,0,"2018/11/19, 09:38:53",False,"2018/11/19, 09:38:53",463,1749786,14,"<p>Spring Sleuth is now OpenTracing compatible. All you have to do is use OpenTracing Jars in your class path. </p>

<p>You can then use Sleuth-Zipkin to send instrumentation data to Jaeger's Zipkin collector.</p>

<p>This way you achieve everything you want with minimal configuration.</p>

<p>You can use my sample program as an example here:</p>

<p><a href=""https://github.com/anoophp777/spring-webflux-jaegar-log4j2"" rel=""noreferrer"">https://github.com/anoophp777/spring-webflux-jaegar-log4j2</a></p>
"
Jaeger,62348886,62216558,0,"2020/06/12, 19:51:43",False,"2020/06/12, 19:51:43",19192,4275342,2,"<p>The jaeger-agent service should look like</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: jaeger-agent
  namespace: jaeger
  labels:
    app: jaeger
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/component: agent
spec:
  type: LoadBalancer
  ports:
  - name: agent-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  - name: agent-binary
    port: 6832
    protocol: UDP
    targetPort: 6832
  - name: agent-configs
    port: 5778
    protocol: TCP
    targetPort: 5778
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/component: all-in-one
</code></pre>

<p>Don't use IP, use FQDN.
First, try to hardcode value for <code>jaegerHost</code> </p>

<pre><code>string jaegerHost = ""jaeger-agent.jaeger"";
</code></pre>

<p>where <code>jaeger-agent</code> - service name, <code>jaeger</code> - namespace of service</p>

<p>Also, you should create <code>jaeger-collector</code> service </p>
"
Jaeger,52894567,52886127,2,"2018/10/19, 17:34:17",True,"2018/10/31, 13:44:01",13313,524946,2,"<p>Are you closing the tracer and the scope? If you are using a version before 0.32.0, you should manually call <code>tracer.close()</code> before your process terminates, otherwise the spans in the buffer might not get dispatched.</p>

<p>As for the scope, it's common to wrap it in a try-with-resources statement:</p>

<pre><code>try (Scope scope = tracer.buildSpan(""parent-span"").startActive(true)) {
  Tags.SAMPLING_PRIORITY.set(scope.span(), 1);
  scope.span().setTag(""this-is-test"", ""YUP"");

  logging.report((JaegerSpan) scope.span());
}
</code></pre>

<p>You might also want to check the OpenTracing tutorial at <a href=""https://github.com/yurishkuro/opentracing-tutorial"" rel=""nofollow noreferrer"">https://github.com/yurishkuro/opentracing-tutorial</a> or the Katacoda-based version at <a href=""https://www.katacoda.com/courses/opentracing"" rel=""nofollow noreferrer"">https://www.katacoda.com/courses/opentracing</a></p>

<p><strong>-- EDIT</strong></p>

<blockquote>
  <p>and is deployed on a different hostname and port</p>
</blockquote>

<p>Then you do need to tell the tracer where to send the traces. Either export the <code>JAEGER_ENDPOINT</code> environment variable, pointing to a collector endpoint, or set <code>JAEGER_AGENT_HOST</code>/<code>JAEGER_AGENT_PORT</code>, with the location of the agent. You can check the available environment variables for your client on the following URL: <a href=""https://www.jaegertracing.io/docs/1.7/client-features/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.7/client-features/</a></p>
"
Jaeger,47756263,47706011,7,"2017/12/11, 17:35:05",True,"2017/12/11, 17:35:05",91,629338,2,"<p>You need to add some more properties to your config options. For reporter deployed on localhost and local sampler strategy :</p>

<pre><code>var config = {
  'serviceName': 'my-awesome-service',
  'reporter': {
    'logSpans': true,
    'agentHost': 'localhost',
    'agentPort': 6832
  },
  'sampler': {
    'type': 'probabilistic',
    'param': 1.0
  }
};
</code></pre>

<p>Replace <code>localhost</code> by server or route name to target another host for Jeager runtime.</p>
"
Jaeger,23347674,23266043,1,"2014/04/28, 20:38:39",True,"2014/04/28, 20:38:39",832,23854,4,"<p>It looks like this is not currently possible with Cordova 3.4 when attempting to read a video file out of the application assets.</p>

<p>See <a href=""https://issues.apache.org/jira/browse/CB-6079"" rel=""nofollow"">https://issues.apache.org/jira/browse/CB-6079</a></p>

<p>It is possible to read the file if its copied to a directory outside the application assets, or the file is stored remotely.  But not in the app assets folder any longer.</p>

<p>I have a similar issue - my application has a welcome screen with a short video explaining the application (~300k) which I cannot play out of the APK itself.</p>
"
Jaeger,17730606,16491312,0,"2013/07/18, 20:49:47",True,"2013/07/18, 20:49:47",3963,2161256,4,"<p>jquery.limitkeypress.js has some issue with ie, I recommend you to use a more powerful library.</p>

<p><a href=""http://github.com/RobinHerbots/jquery.inputmask"">http://github.com/RobinHerbots/jquery.inputmask</a> </p>

<p>With this library you can use something like this:</p>

<pre><code>$("".numbers"").inputmask('Regex', {
    mask: ""9"",
    repeat: 11,
    placeholder: """"
});
</code></pre>

<p>It works perfectly on ie. :)</p>
"
Jaeger,35784198,16491312,0,"2016/03/04, 00:18:17",False,"2016/03/04, 16:45:12",11,6015643,1,"<p>Sorry i have not updated that plugin in a few years but...</p>

<p><a href=""http://brianjaeger.com/process.php"" rel=""nofollow"">jquery.limitkeypress</a> now works with IE9+ there was an issue with how the selection was determined.</p>

<p>IE11 killed support for their document.selection but they kept the document.setSelectionRange which i was using to test what browser was being used...</p>

<p>IE9 enabled document.selectionStart and document.selectionEnd so i now check directly what browser version of IE peoples are using...</p>

<p>I added this to check for IE version:</p>

<pre><code>var ie = (function(){
var undef,
  v = 3,
  div = document.createElement('div'),
  all = div.getElementsByTagName('i');
while (
  div.innerHTML = '&lt;!--[if gt IE ' + (++v) + ']&gt;&lt;i&gt;&lt;/i&gt;&lt;![endif]--&gt;',
  all[0]
);
return v &gt; 4 ? v : undef;
}());
</code></pre>

<p>So my selection functions now look like this:</p>

<pre><code>function getSelectionStart(o) {
  if (ie &lt; 9) {
    var r = document.selection.createRange().duplicate()
    r.moveEnd('character', o.value.length)
    if (r.text == '') return o.value.length
      return o.value.lastIndexOf(r.text)
    } else return o.selectionStart
  }
function getSelectionEnd(o) {
  if (ie &lt; 9) {
    var r = document.selection.createRange().duplicate()
    r.moveStart('character', -o.value.length)
    return r.text.length
  } else return o.selectionEnd
}
</code></pre>
"
Jaeger,52111563,52038025,1,"2018/08/31, 11:36:35",True,"2018/08/31, 11:36:35",284,4828509,3,"<p>Some web frameworks return empty string if a non-existent header is queried. I have seen this in Spring Boot and KoaJS.</p>

<p>If any of the tracing headers is not sent by Istio, this header logic causes us to send empty string for those non-existent headers which breaks tracing.</p>

<p>My suggestion is after getting the values for headers filter out the ones with empty string as their values and propogate the remaining ones.</p>
"
Jaeger,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51,8394088,0,"<p>This link (<a href=""https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/"" rel=""nofollow noreferrer"">https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/</a>) provides the details of how to enable jaeger traces.</p>
<p>The simplest way to enable jaeger to spring-boot application is add the dependency and the required properties.</p>
<hr />
<p><strong>Dependency:</strong></p>
<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
   &lt;artifactId&gt;opentracing-spring-jaeger-web-starter&lt;/artifactId&gt;
   &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p><strong>Example Properties</strong></p>
<pre><code>opentracing.jaeger.udp-sender.host=localhost
opentracing.jaeger.udp-sender.port=6831
opentracing.jaeger.const-sampler.decision=true
opentracing.jaeger.enabled=true
opentracing.jaeger.log-spans=true
opentracing.jaeger.service-name=ms-name
</code></pre>
"
Jaeger,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36,4216591,1,"<p>Answering to your question about dependencies it is explained here in Dependencies section (<a href=""https://github.com/opentracing-contrib/java-spring-jaeger"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-spring-jaeger</a>):</p>
<blockquote>
<p>The opentracing-spring-jaeger-web-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-web-starter This means that by including it, simple web Spring Boot microservices include all the necessary dependencies to instrument Web requests / responses and send traces to Jaeger.</p>
<p>The opentracing-spring-jaeger-cloud-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-cloud-starter This means that by including it, all parts of the Spring Cloud stack supported by Opentracing will be instrumented</p>
</blockquote>
<p>And by the way:</p>
<ul>
<li>opentracing.jaeger.log-spans is true by default</li>
</ul>
<p>same as:</p>
<ul>
<li>opentracing.jaeger.udp-sender.host=localhost</li>
<li>opentracing.jaeger.udp-sender.port=6831</li>
<li>opentracing.jaeger.const-sampler.decision=true</li>
<li>opentracing.jaeger.enabled=true</li>
</ul>
"
Jaeger,59260272,58409078,0,"2019/12/10, 06:32:34",False,"2019/12/10, 06:32:34",471,2673284,0,"<p>This is why tracing is a useful tool, it often reveals issues like this that you wouldn't suspect otherwise. If your application is using async framework, these gaps may indicate execution waiting on available threads. Or your application may be CPU throttled during and between the spans. You cannot really explain the gaps from the trace itself, but you surely do have them. Time to whip out the profiler.</p>
"
Jaeger,61152144,61149872,7,"2020/04/11, 07:27:43",False,"2020/04/11, 10:29:27",27576,1839482,1,"<p>You add <code>opentracing-spring-jaeger-starter</code> <a href=""https://github.com/opentracing-contrib/java-spring-jaeger"" rel=""nofollow noreferrer"">library</a> into the project which simply contains the code needed to provide a Jaeger implementation of the OpenTracing's <code>io.opentracing.Tracer</code> interface.</p>

<p>Since you have the jaeger deployed in Kubernetes and exposed it via a loadbalancer service you can use the loadbalancer IP and port to connect to it from outside the Kubernetes cluster.</p>
"
Jaeger,61154289,61149872,1,"2020/04/11, 11:56:40",False,"2021/04/10, 03:12:05",382,7847042,1,"<p>So the solution that works for me is -
I have made the following changes in my application.properties file of application</p>
<pre><code>opentracing.jaeger.udp-sender.host=http://&lt;load_balancer_ip_of_jaeger_service&gt;:&lt;expose_port&gt;(example &quot;:80&quot;)
opentracing.jaeger.http-sender.url=http://&lt;cluster_ip_or_load_balancer_ip_of jaeger_collector&gt;:&lt;expose_port&gt;/api/traces(example of expose port &quot;:1468&quot;)
</code></pre>
"
Jaeger,62832644,61149872,3,"2020/07/10, 13:45:36",False,"2020/07/10, 13:45:36",9,8694436,0,"<p>You should config:</p>
<pre><code>opentracing.jaeger.udp-sender.host=jaeger-agent
opentracing.jaeger.udp-sender.port=6831
</code></pre>
"
Jaeger,51720029,51719575,0,"2018/08/07, 09:18:44",False,"2018/08/07, 09:34:22",8280,1057291,3,"<p>BINGO!</p>

<p>We need to setup the ReporterConfigurations as below. previously my ones were default ones that's why it always connected to local.</p>

<pre><code>return new Configuration(""MyApplication"", 
        new Configuration.SamplerConfiguration(ProbabilisticSampler.TYPE, 1, ""server2.mycompany.com:5778""),
        new Configuration.ReporterConfiguration(false, ""server2.mycompany.com"",6831,1000,100))
        .getTracer();
</code></pre>

<p>Even better, you can create the Configuration from Environment as below providing the environment variables as below</p>

<pre><code>return Configuration.fromEnv().getTracer();
</code></pre>

<p>You can provide this when you run the docker container</p>

<blockquote>
  <p>-e JAVA_OPTS=""</p>

<pre><code> -DJAEGER_SAMPLER_TYPE=probabilistic
 -DJAEGER_SAMPLER_PARAM=1
 -DJAEGER_SAMPLER_MANAGER_HOST_PORT=server2.mycompany.com:5778
 -DJAEGER_REPORTER_LOG_SPANS=false
 -DJAEGER_AGENT_HOST=server2.mycompany.com
 -DJAEGER_AGENT_PORT=6831
 -DJAEGER_REPORTER_FLUSH_INTERVAL=1000
 -DJAEGER_REPORTER_MAX_QUEUE_SIZE=100
 -DJAEGER_SERVICE_NAME=MyApplicationNameX 
</code></pre>
  
  <p>"" ....</p>
</blockquote>
"
Jaeger,59170296,51719575,2,"2019/12/04, 08:43:41",False,"2019/12/04, 13:31:17",11,12478106,1,"<p>Step 1 : First we need to configure remote host address and port.</p>

<pre><code>    private static final int JAEGER_PORT = HOST_PORT;
    private static final String JAEGER_HOST = ""HOST_IP"";
</code></pre>

<p>Step 2 : configure sender configuration and pass the remote host and port in                           withAgentHost, withAgentPort.</p>

<p>SenderConfiguration senderConfig = Configuration.SenderConfiguration.fromEnv()
                    .withAgentHost(JAEGER_HOST)
                    .withAgentPort(JAEGER_PORT);</p>

<p>Step 3 : Pass sender configuration in reporter configuration</p>

<p>Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv()
                .withLogSpans(true)
                .withSender(senderConfig);</p>

<pre><code>package com.studies.StudyService;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

import io.jaegertracing.Configuration;
import io.jaegertracing.Configuration.SenderConfiguration;
import io.jaegertracing.internal.samplers.ProbabilisticSampler;
import io.opentracing.Tracer;

@SpringBootApplication
//@EnableDiscoveryClient

public class StudyServiceApplication {

    public static void main(String[] args) {
        SpringApplication.run(StudyServiceApplication.class, args);
    }
    /* Configure sender host and port details */
        private static final int JAEGER_PORT = HOST_PORT;
        private static final String JAEGER_HOST = ""HOST_IP"";
    /* End */
    @Bean
    public Tracer getTracer() {

        Configuration.SamplerConfiguration samplerConfig = Configuration.SamplerConfiguration.fromEnv()
                .withType(ProbabilisticSampler.TYPE).withParam(1);

        /* Update default sender configuration with custom host and port */
            SenderConfiguration senderConfig = Configuration.SenderConfiguration.fromEnv()
                    .withAgentHost(JAEGER_HOST)
                    .withAgentPort(JAEGER_PORT);
        /* End */

        Configuration.ReporterConfiguration reporterConfig = Configuration.ReporterConfiguration.fromEnv()
                .withLogSpans(true)
                .withSender(senderConfig);

        Configuration config = new Configuration(""Service_Name"").withSampler(samplerConfig)
                .withReporter(reporterConfig);

        return config.getTracer();
    }
}
</code></pre>
"
Jaeger,53343467,53322900,1,"2018/11/16, 20:27:10",True,"2018/11/16, 20:27:10",463,1749786,1,"<p>The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT! </p>

<p>Jaegar has the ability to collect Zipkin spans:</p>

<p><a href=""https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin</a></p>

<p>You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>The above will send Zipkin spans to <a href=""http://localhost:9411"" rel=""nofollow noreferrer"">http://localhost:9411</a> by default. You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.</p>

<blockquote>
  <p>spring.zipkin.base-url=<a href=""http://your-jaegar-server:9411"" rel=""nofollow noreferrer"">http://your-jaegar-server:9411</a></p>
</blockquote>

<p>Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.</p>

<p>In the log4j2.xml file, all you have to mention is</p>

<blockquote>
  <p>[%X]</p>
</blockquote>

<p>You can find the sample code here:</p>

<p><a href=""https://github.com/anoophp777/spring-webflux-jaegar-log4j2"" rel=""nofollow noreferrer"">https://github.com/anoophp777/spring-webflux-jaegar-log4j2</a></p>
"
Jaeger,55138790,53453160,1,"2019/03/13, 11:48:49",True,"2019/03/13, 11:48:49",1678,2104921,1,"<p>Turns out that <strong>Feign</strong> clients are <strong>currently not supported</strong> or to be precise, the spring startes do not configure the Feign clients accordingly. If you want to use Jaeger with your Feign clients, you have to provide an integration of your own. </p>

<p>In my experience so far, the jaeger community is a lesser supportive one, thus you have to gain such knowledge on your own, which in my opinion is a big downside and you should probably consider, using an alternative. </p>
"
Jaeger,63367857,53453160,0,"2020/08/12, 02:50:32",False,"2020/08/12, 02:50:32",1,8149668,0,"<p>In some cases it might be necessary to explicitly expose the <strong>Feign Client</strong> in the Spring configuration, in order to get the traceId propagated. This can be done easily by adding the following into one of your configuration classes</p>
<pre><code>  @Bean
  public Client feignClient() {
    return new Client.Default(null, null);
  }
</code></pre>
"
Jaeger,52813394,48423910,0,"2018/10/15, 12:16:23",False,"2018/10/15, 12:16:23",13313,524946,0,"<p>This seems a bit old and it's a bit hard to tell what's wrong. My first guess would be the sampling strategy, as Jaeger samples one trace in one thousand, but looks like you did set it.</p>

<p><code>
JAEGER_SAMPLER_TYPE=const
JAEGER_SAMPLER_PARAM=1
</code></p>

<p>I would recommend you start by using a simple <code>Configuration.fromEnv().getTracer()</code> to get your tracer. Then, control it via env vars, probably setting <code>JAEGER_REPORTER_LOG_SPANS</code> to <code>true</code>. With this option, you should be able to see in the logs whenever Jaeger emits a span.</p>

<p>You can also set the <code>--log-level=debug</code> option to the agent and collector (or all-in-one, in your case) to see when these components receive a span from a client.</p>
"
Jaeger,61154134,61154096,3,"2020/04/11, 11:43:10",False,"2020/04/11, 11:43:10",27576,1839482,2,"<p>You can use <a href=""https://www.jaegertracing.io/docs/1.16/operator/"" rel=""nofollow noreferrer"">Jaeger Operator</a> to deploy Jaeger on kubernetes.The Jaeger Operator is an implementation of a Kubernetes Operator. Operators are pieces of software that ease the operational complexity of running another piece of software. More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application</p>
"
Jaeger,61154180,61154096,0,"2020/04/11, 11:47:26",True,"2020/04/11, 22:39:56",382,7847042,1,"<p>Follow this link for steps to deploy JAEGER on kubernetes .</p>

<blockquote>
  <p><a href=""https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/"" rel=""nofollow noreferrer"">https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/</a></p>
</blockquote>

<p>make following changes in application.properties</p>

<pre><code>opentracing.jaeger.udp-sender.host=&lt;load_balancer_ip&gt; of your jaeger service
opentracing.jaeger.http-sender.url=http://&lt;jaeger-collector-service-name&gt;:port/api/traces
</code></pre>
"
Jaeger,61160646,61154096,0,"2020/04/11, 20:07:08",False,"2020/04/11, 20:07:08",181,12530105,1,"<p>You can use this link for a better understanding - <a href=""https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/"" rel=""nofollow noreferrer"">https://www.upnxtblog.com/index.php/2018/07/16/kubernetes-tutorial-distributed-tracing-with-jaeger/</a></p>
"
Jaeger,51929991,51838896,0,"2018/08/20, 14:39:54",False,"2018/08/20, 14:39:54",116,7510189,0,"<ol>
<li>Check if your valid redirect URIs within Keycloak are correct. Add * if you want to make sure, that's not the problem; for security reasons it should be as exact as possible in production.</li>
<li>Your proxy.json constrains access to the role ""application"". Check if that role has been added within Keycloak to the Role Mapping.</li>
</ol>

<p>Also, do you get an Error Message? If so, please post it.</p>
"
Jaeger,63582546,63507749,1,"2020/08/25, 18:46:03",False,"2020/08/25, 18:46:03",61,14137910,3,"<p>After few days of digging I've figured it out. Problem is in the format of the <code>x-request-id</code> header that nginx ingress controller uses.</p>
<p>Envoy proxy expects it to be an UUID (e.g. <code>x-request-id: 3e21578f-cd04-9246-aa50-67188d790051</code>) but ingrex controller passes it as a non-formatted random string (<code>x-request-id: 60e82827a270070cfbda38c6f30f478a</code>). When I pass properly formatted x-request-id header in the request to ingress controller its getting passed down to envoy proxy and request is getting sampled as expected. I also tried to remove
x-request-id header from the request from ingress controller to ServiceA with a simple EnvoyFilter. And it also works as expected. Envoy proxy generates a new x-request-id and request is getting traced.</p>
"
Jaeger,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91,1548178,4,"<p>The demo you tried is using older configuration and opencensus which should be replaced with otlp receiver. Having said that this is a working example
<a href=""https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node/docker"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node/docker</a>
So I'm copying the files from there:</p>
<p>docker-compose.yaml</p>
<pre><code>version: &quot;3&quot;
services:
  # Collector
  collector:
    image: otel/opentelemetry-collector:latest
    command: [&quot;--config=/conf/collector-config.yaml&quot;, &quot;--log-level=DEBUG&quot;]
    volumes:
      - ./collector-config.yaml:/conf/collector-config.yaml
    ports:
      - &quot;9464:9464&quot;
      - &quot;55680:55680&quot;
      - &quot;55681:55681&quot;
    depends_on:
      - zipkin-all-in-one

  # Zipkin
  zipkin-all-in-one:
    image: openzipkin/zipkin:latest
    ports:
      - &quot;9411:9411&quot;

  # Prometheus
  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - &quot;9090:9090&quot;
</code></pre>
<p>collector-config.yaml</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
      http:

exporters:
  zipkin:
    endpoint: &quot;http://zipkin-all-in-one:9411/api/v2/spans&quot;
  prometheus:
    endpoint: &quot;0.0.0.0:9464&quot;

processors:
  batch:
  queued_retry:

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [zipkin]
      processors: [batch, queued_retry]
    metrics:
      receivers: [otlp]
      exporters: [prometheus]
      processors: [batch, queued_retry]
</code></pre>
<p>prometheus.yaml</p>
<pre><code>global:
  scrape_interval: 15s # Default is every 1 minute.

scrape_configs:
  - job_name: 'collector'
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
      - targets: ['collector:9464']
</code></pre>
<p>This should work fine with opentelemetry-js ver. 0.10.2</p>
<p>Default port for traces is 55680 and for metrics 55681</p>
<p>The link I posted previously - you will always find there the latest up to date working example:
<a href=""https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node</a>
And for web example you can use the same docker and see all working examples here:
<a href=""https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/tracer-web/"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/tracer-web/</a></p>
"
Jaeger,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911,2000548,3,"<p>Thank you sooo much for @BObecny's help! This is a complement of @BObecny's answer.</p>
<p>Since I am more interested in integrating with Jaeger. So here is the config to set up with all Jaeger, Zipkin, Prometheus. And now it works on both front end and back end.</p>
<p>First both front end and back end use same exporter code:</p>
<pre><code>import { CollectorTraceExporter } from '@opentelemetry/exporter-collector';

new SimpleSpanProcessor(
  new CollectorTraceExporter({
    serviceName: 'my-service',
  })
)
</code></pre>
<p><strong>docker-compose.yaml</strong></p>
<pre><code>version: &quot;3&quot;
services:
  # Collector
  collector:
    image: otel/opentelemetry-collector:latest
    command: [&quot;--config=/conf/collector-config.yaml&quot;, &quot;--log-level=DEBUG&quot;]
    volumes:
      - ./collector-config.yaml:/conf/collector-config.yaml
    ports:
      - &quot;9464:9464&quot;
      - &quot;55680:55680&quot;
      - &quot;55681:55681&quot;
    depends_on:
      - jaeger-all-in-one
      - zipkin-all-in-one

  # Jaeger
  jaeger-all-in-one:
    image: jaegertracing/all-in-one:latest
    ports:
      - &quot;16686:16686&quot;
      - &quot;14268&quot;
      - &quot;14250&quot;

  # Zipkin
  zipkin-all-in-one:
    image: openzipkin/zipkin:latest
    ports:
      - &quot;9411:9411&quot;

  # Prometheus
  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - &quot;9090:9090&quot;
</code></pre>
<p><strong>collector-config.yaml</strong></p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
      http:

exporters:
  jaeger:
    endpoint: jaeger-all-in-one:14250
    insecure: true
  zipkin:
    endpoint: &quot;http://zipkin-all-in-one:9411/api/v2/spans&quot;
  prometheus:
    endpoint: &quot;0.0.0.0:9464&quot;

processors:
  batch:
  queued_retry:

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [zipkin]
      processors: [batch, queued_retry]
    metrics:
      receivers: [otlp]
      exporters: [prometheus]
      processors: [batch, queued_retry]
</code></pre>
<p><strong>prometheus.yaml</strong></p>
<pre><code>global:
  scrape_interval: 15s # Default is every 1 minute.

scrape_configs:
  - job_name: 'collector'
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
      - targets: ['collector:9464']
</code></pre>
"
Jaeger,59684931,59679605,3,"2020/01/10, 17:51:33",False,"2020/01/10, 17:51:33",519,12405999,3,"<p>Per the log file, there are more than 10,000 started threads. That's <em>a lot</em> even if we don't look at the less that 2 CPUs/cores reserved for the container (limits.cpu = request.cpu = 1600 millicores).</p>

<p>Each thread, and its stack, is allocated in memory separate from the heap. It is quite possible that the large number of started threads is the cause for the OOM problem.</p>

<p>The JVM is started with the Native Memory Tracking related options (<code>-XX:NativeMemoryTracking=detail, -XX:+UnlockDiagnosticVMOptions, -XX:+PrintNMTStatistics)</code> that could help to see the memory usage, including what's consumed by those threads. <a href=""https://docs.oracle.com/en/java/javase/11/vm/native-memory-tracking.html#GUID-56C3FD2E-E227-4902-B361-3EEE3492D70D"" rel=""nofollow noreferrer"">This doc</a> could be a starting point for Java 11.</p>

<p>In any case, it would be highly recommended to <em>not</em> have that many threads started. E.g. use a pool, start and stop them when not needed anymore...</p>
"
Jaeger,59717806,59679605,0,"2020/01/13, 15:39:55",False,"2020/01/13, 15:39:55",2389,12524159,2,"<p>There are two reasons a container is OOM Killed: Container Quota and System Quota.</p>

<p>OOM Killer <strong>only</strong> triggers with memory related issues.</p>

<p>If your system is far from being out of memory, there is probably a limit in your container.
To your process inside the pod, the pod resource limit is like the whole system being OOM.</p>

<ul>
<li><strong>Check your pod manifest, there is possibly a limit set that's being reached in the pods.</strong></li>
</ul>

<p>Also, it's worth checking the Resource Requests because by default they are not set.
Requests must be less than or equal to container limits. That means that containers could be overcommitted on nodes and killed by  OOMK if multiple containers are using more memory than their respective requests at the same time.</p>

<ul>
<li><strong>Check the amount of memory being assigned to each pod and how much memory the process actually uses during how much time. Maybe it's reserving a lot more memory than it really needs and it's causing your Memory usage start at higher levels.</strong></li>
</ul>
"
Jaeger,60649575,59679605,0,"2020/03/12, 09:43:50",True,"2020/03/12, 09:43:50",2227,1004374,1,"<p>In my case the issue was with debugger component that is located in CMD line of Docker file</p>

<pre><code>-agentpath:/opt/cdbg/cdbg_java_agent.so \
            -Dcom.google.cdbg.module=${MODULE} \
            -Dcom.google.cdbg.version=${TAG} \
            -Djava.security.egd=file:/dev/./urandom \
</code></pre>

<p>After removal application stopped leaking. But disappeared only native memory leak. As later investigated there also was heap memory leak induced by jaegger tracer component (luckily here we have much more tools). After its removal application became stable. I don't know if those components were leaky by itself or with combination with other components but fact is that now it is stable.</p>
"
Jaeger,58318625,58249869,0,"2019/10/10, 11:31:45",False,"2019/10/10, 11:31:45",6049,3156333,0,"<p>Istio have this feature called <a href=""https://istio.io/docs/tasks/telemetry/distributed-tracing"" rel=""nofollow noreferrer"">Distributed Tracing</a>, which enables users to track requests in mesh that is distributed across multiple services. This can be used to visualize request latency, serialization and parallelism.</p>

<p>For this to work Istio uses <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/intro/arch_overview/tracing"" rel=""nofollow noreferrer"">Envoy Proxy - Tracing</a> feature.</p>

<p>You can deploy <a href=""https://istio.io/docs/examples/bookinfo/"" rel=""nofollow noreferrer"">Bookinfo Application</a> and see how <a href=""https://istio.io/docs/tasks/telemetry/distributed-tracing/overview/#trace-context-propagation"" rel=""nofollow noreferrer"">Trace context propagation</a> works.</p>
"
Jaeger,58414249,58249869,0,"2019/10/16, 16:14:47",False,"2019/10/16, 16:14:47",334,4734545,0,"<p>If you have the same issue explained in this ticket, you need to wait for the next release of micronaut or use the workaround mentioned by micronaut guys there.</p>

<p><a href=""https://github.com/micronaut-projects/micronaut-core/issues/2209"" rel=""nofollow noreferrer"">https://github.com/micronaut-projects/micronaut-core/issues/2209</a></p>
"
Jaeger,58209862,58209785,0,"2019/10/03, 00:55:41",False,"2019/10/03, 00:55:41",966,324449,0,"<p><code>latest</code> is just a tag like any other -- you will want <code>docker image inspect</code>, which will give you information about the other tags on your image.</p>

<p>In the case of <code>jaegertracing/jaeger-agent:latest</code>, it doesn't look this image has any other tags, so it's probable that this image is tracking something like the master branch of a source control repository, i.e., it doesn't correspond to a published version at all.</p>
"
Jaeger,58244607,58209785,0,"2019/10/05, 04:10:08",False,"2019/10/05, 04:10:08",12605,11934042,1,"<p>As @max-gasner mentioned, it's common for <code>latest</code> to be tracking the <code>master</code> branch of a git repository. This allows the engineers to quickly build and test images before they are released and version tagged. This is one of the reasons why it's not recommended to ever use <code>latest</code> tags for anything critical where you need reproducibility.</p>

<p><code>jaegertracing/jaeger-agent:latest</code> doesn't have any other tags so the only way to determine which ""version"" of <code>latest</code> you are using is to look at the digest. This uniquely identifies the image build. Tags actually resolve to digests. So when a new image is built with the <code>latest</code> tag, that tag will then resolve to the digest of the new image.</p>

<p><a href=""https://i.stack.imgur.com/9UG3y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9UG3y.png"" alt=""enter image description here""></a></p>

<p>DockerHub only shows the short version. You can inspect the full digest like this:</p>

<pre><code>docker image inspect --format '{{.RepoDigests}}' jaegertracing/jaeger-agent:latest
&gt; [jaegertracing/jaeger-agent@sha256:bacc749faba325fbe39dc13294c2222fb0babc9ecd344c25d9d577720a80b5ca]
</code></pre>
"
Jaeger,52908915,52875462,1,"2018/10/20, 21:37:17",True,"2018/10/20, 21:37:17",46,4618624,2,"<p>There are similar ideas in this zipkin/brave repo by @jeqo.</p>

<p><a href=""https://github.com/jeqo/brave/tree/kafka-streams-processor/instrumentation/kafka-streams"" rel=""nofollow noreferrer"">https://github.com/jeqo/brave/tree/kafka-streams-processor/instrumentation/kafka-streams</a></p>

<p>There also seems to be something available in opentracing-contrib repo but it seems to only at trace producer/consumer level.</p>

<p><a href=""https://github.com/opentracing-contrib/java-kafka-client/tree/master/opentracing-kafka-streams"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-kafka-client/tree/master/opentracing-kafka-streams</a></p>

<ul>
<li>lenny</li>
</ul>
"
Jaeger,59260037,59153293,1,"2019/12/10, 06:02:36",True,"2019/12/10, 14:41:51",471,2673284,4,"<p>From the official FAQ (<a href=""https://www.jaegertracing.io/docs/latest/faq/#do-i-need-to-run-jaeger-agent"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/latest/faq/#do-i-need-to-run-jaeger-agent</a>):</p>

<p><code>jaeger-agent</code> is not always necessary. Jaeger client libraries can be configured to export trace data directly to <code>jaeger-collector</code>. However, the following are the reasons why running <code>jaeger-agent</code> is recommended:</p>

<ul>
<li>If we want Jaeger client libraries to send trace data directly to collectors, we must provide them with a URL of the HTTP endpoint. It means that our applications require additional configuration containing this parameter, especially if we are running multiple Jaeger installations (e.g. in different availability zones or regions) and want the data sent to a nearby installation. In contrast, when using the agent, the libraries require no additional configuration because the agent is always accessible via localhost. It acts as a sidecar and proxies the requests to the appropriate collectors.</li>
<li>The agent can be configured to enrich the tracing data with infrastructure-specific metadata by adding extra tags to the spans, such as the current zone, region, etc. If the agent is running as a host daemon, it will be shared by all applications running on the same host. If the agent is running as a true sidecar, i.e. one per application, it can provide additional functionality such as strong authentication, multi-tenancy (see <a href=""https://medium.com/jaegertracing/jaeger-and-multitenancy-99dfa1d49dc0"" rel=""nofollow noreferrer"">this blog post</a>), pod name, etc.
If we want Jaeger client libraries to use sampling strategies that are centrally configured in the collectors, this is only possible by using the /sampling HTTP endpoint on the agent. There is no technical reason why this endpoint cannot be implemented directly in the collectors, it's just <a href=""https://github.com/jaegertracing/jaeger/issues/1971"" rel=""nofollow noreferrer"">not done yet</a>.</li>
<li>Agents allow implementing traffic control to the collectors. If we have thousands of hosts in the data center, each running many applications, and each application sending data directly to the collectors, there may be too many open connections for each collector to handle. The agents can load balance this traffic with fewer connections.</li>
</ul>
"
Jaeger,56927362,56847558,1,"2019/07/08, 04:26:18",True,"2019/07/08, 04:26:18",1084,2915603,1,"<p>So, answering my own question. Jaeger does not support cross system spans. Every sub-system is responsible for its own span in the whole system. 
For reference, check this answer <a href=""https://github.com/opentracing/specification/issues/143"" rel=""nofollow noreferrer"">https://github.com/opentracing/specification/issues/143</a></p>
"
Jaeger,64399197,64391505,1,"2020/10/17, 08:06:03",False,"2020/10/17, 08:06:03",4190,1733929,1,"<p>You can see that jaeger-query configuration includes: SPAN_STORAGE_TYPE: &quot;kafka&quot;</p>
<p>The error indicates that a kafka client used by jaeger-query to store spans in Kafka cannot in fact reach Kafka, and therefore the jaeger storage factory fails to initialize.</p>
<p>This can be either because Kafka failed to start (did you check)? Or a misconfig of the network in your docker.</p>
"
Jaeger,64572391,64391505,0,"2020/10/28, 13:57:45",True,"2020/10/28, 13:57:45",1101,1141160,1,"<p>I was missing a lot of information. I managed to get it working:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.8'

services:
  
  zookeeper:
    image: confluentinc/cp-zookeeper
    networks:
      - kafka-net
    container_name: zookeeper
    environment:
        ZOOKEEPER_CLIENT_PORT: 2181
    ports:
        - 2181:2181

  cassandra:
    hostname: cassandra
    image: cassandra
    networks:
      - kafka-net
    environment:
      MAX_HEAP_SIZE: 1G
      HEAP_NEWSIZE: 256M
    ports:
     - &quot;9042:9042&quot;

  cassandra-schema:
    image: jaegertracing/jaeger-cassandra-schema
    networks:
      - kafka-net
    depends_on:
      - cassandra

  kafka:
    image: confluentinc/cp-kafka
    networks:
      - kafka-net
    container_name: kafka
    environment:
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        ALLOW_PLAINTEXT_LISTENER: &quot;yes&quot;
        KAFKA_LISTENERS-INTERNAL: //kafka:29092,EXTERNAL://localhost:9092
        KAFKA_ADVERTISED: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
        KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
    ports:
        - 9092:9092
        - 29092:29092
    depends_on:
        - zookeeper
    restart: on-failure
  
  jaeger-collector:
    image: jaegertracing/jaeger-collector
    container_name: jaeger-collector
    networks:
      - kafka-net
    ports:
      - &quot;14250:14250&quot;
      - &quot;14267:14267&quot;
      - &quot;14268:14268&quot; # HTTP collector port to receive spans
      - &quot;14269:14269&quot; # HTTP health check port
    restart: on-failure
    environment:
      LOG_LEVEL: &quot;debug&quot;
      SPAN_STORAGE_TYPE: &quot;kafka&quot;
      KAFKA_BROKERS: &quot;kafka:9092&quot;
      KAFKA_PRODUCER_BROKERS: &quot;kafka:29092&quot;

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    container_name: jaeger-agent
    networks:
      - kafka-net
    command: [&quot;--reporter.grpc.host-port=jaeger-collector:14250&quot;]
    ports:
      - &quot;5775:5775/udp&quot;
      - &quot;6831:6831/udp&quot;
      - &quot;6832:6832/udp&quot;
      - &quot;5778:5778&quot;
    environment:
      LOG_LEVEL: &quot;debug&quot;
      SPAN_STORAGE_TYPE: &quot;kafka&quot;
    restart: on-failure
    depends_on:
      - jaeger-collector

  jaeger-ingester:
    image: jaegertracing/jaeger-ingester
    container_name: jaeger-ingester
    networks:
      - kafka-net
    ports:
      - &quot;14270:14270&quot; # HTTP health check port: http://localhost:14270/
      - &quot;14271:14271&quot; # Metrics port: http://localhost:14270/metrics
    restart: on-failure
    environment:
      LOG_LEVEL: debug
      INGESTER_PARALLELISM: 1
      INGESTER_DEADLOCKINTERVAL: ms
      SPAN_STORAGE_TYPE: cassandra
      CASSANDRA_SERVERS: cassandra
      CASSANDRA_KEYSPACE: jaeger_v1_dc1
      METRICS_BACKEND: expvar
      KAFKA_CONSUMER_BROKERS: kafka:29092
      KAFKA_CONSUMER_TOPIC: jaeger-spans
    depends_on:
      - cassandra-schema

  jaeger-query:
    image: jaegertracing/jaeger-query
    container_name: jaeger-query
    networks:
      - kafka-net
    ports:
      - &quot;16686:16686&quot; # Jaeger UI port
      - &quot;16687:16687&quot; # HTTP health check port: http://localhost:16687/
    restart: on-failure
    depends_on:
      - cassandra-schema
    environment:
      LOG_LEVEL: debug
      SPAN_STORAGE_TYPE: cassandra
      CASSANDRA_SERVERS: cassandra
      CASSANDRA_KEYSPACE: jaeger_v1_dc1
      JAEGER_ENDPOINT: http://jaeger-collector:14268/api/traces

networks:
  kafka-net:
    driver: bridge

</code></pre>
"
Jaeger,59675721,50179555,0,"2020/01/10, 06:59:31",True,"2020/01/10, 06:59:31",842,6907909,1,"<p>The Jaeger helm chart is now available <a href=""https://github.com/jaegertracing/helm-charts"" rel=""nofollow noreferrer"">here</a>.</p>

<p>You need to add the helm repo first using the following:</p>

<pre><code>helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
</code></pre>

<p>This can be installed with:</p>

<pre><code>helm install jaegertracing/jaeger
</code></pre>
"
Jaeger,56581518,55256948,2,"2019/06/13, 16:23:02",True,"2019/06/13, 16:23:02",2678,3349358,5,"<p>If anyone else would like to set up Jaeger in spring project, here's what I did:</p>

<p>Add dependencies to pom:</p>

<pre><code>&lt;properties&gt;
    &lt;opentracing.spring.web.version&gt;0.3.4&lt;/opentracing.spring.web.version&gt;
    &lt;opentracing.jdbc.version&gt;0.0.12&lt;/opentracing.jdbc.version&gt;
    &lt;opentracing.spring.configuration.starter.version&gt;0.1.0&lt;/opentracing.spring.configuration.starter.version&gt;
    &lt;opentracing.spring.jaeger.starter.version&gt;0.2.2&lt;/opentracing.spring.jaeger.starter.version&gt;
&lt;/properties&gt;

&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-web-starter&lt;/artifactId&gt;
    &lt;version&gt;${opentracing.spring.web.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-jdbc&lt;/artifactId&gt;
    &lt;version&gt;${opentracing.jdbc.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-tracer-configuration-starter&lt;/artifactId&gt;
    &lt;version&gt;${opentracing.spring.configuration.starter.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-jaeger-starter&lt;/artifactId&gt;
    &lt;version&gt;${opentracing.spring.jaeger.starter.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Set up you web.xml to register new tracing filter <em>tracingFilter</em> to intercept REST API:</p>

<pre><code>&lt;filter&gt;
    &lt;filter-name&gt;tracingFilter&lt;/filter-name&gt;
    &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;
    &lt;async-supported&gt;true&lt;/async-supported&gt;
&lt;/filter&gt;
&lt;filter-mapping&gt;
    &lt;filter-name&gt;tracingFilter&lt;/filter-name&gt;
    &lt;url-pattern&gt;/api/*&lt;/url-pattern&gt;
&lt;/filter-mapping&gt;
</code></pre>

<p>Register jaeger tracer in spring mvc:</p>

<pre><code>&lt;mvc:interceptors&gt;
    &lt;mvc:interceptor&gt;
        &lt;mvc:mapping path=""/**"" /&gt;
        &lt;bean class=""io.opentracing.contrib.spring.web.interceptor.TracingHandlerInterceptor""&gt;
            &lt;constructor-arg ref=""jaegerTracer"" /&gt;
        &lt;/bean&gt;
    &lt;/mvc:interceptor&gt;
&lt;/mvc:interceptors&gt;
</code></pre>

<p>Set up the <em>tracingFilter</em> bean we described in web.xml:</p>

<pre><code>import io.opentracing.Tracer;
import io.opentracing.contrib.web.servlet.filter.TracingFilter;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;


@Configuration
public class JaegerFilterConfiguration {

    @Bean
    public TracingFilter tracingFilter(Tracer tracer) {
        return new TracingFilter(tracer);
    }
}
</code></pre>

<p>Finally define jaeger tracer spring configuration:</p>

<pre><code>import io.jaegertracing.internal.JaegerTracer;
import io.jaegertracing.internal.metrics.NoopMetricsFactory;
import io.jaegertracing.internal.reporters.RemoteReporter;
import io.jaegertracing.internal.reporters.RemoteReporter.Builder;
import io.jaegertracing.internal.samplers.ProbabilisticSampler;
import io.jaegertracing.thrift.internal.senders.UdpSender;
import io.opentracing.Tracer;
import io.opentracing.util.GlobalTracer;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.ApplicationListener;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.context.event.ContextRefreshedEvent;


@Configuration
public class JaegerConfiguration implements ApplicationListener&lt;ContextRefreshedEvent&gt; {


    private static final int JAEGER_PORT = 6831;
    private static final String JAEGER_HOST = ""localhost"";
    private static final String JAEGER_SERVICE_NAME = ""my-awesome-jaeger"";
    private static final double SAMPLING_RATE = 0.5;
    @Autowired
    private Tracer tracer;

    @Bean
    @Primary
    public Tracer jaegerTracer(RemoteReporter remoteReporter) {
        return new JaegerTracer.Builder(JAEGER_SERVICE_NAME)
                .withReporter(remoteReporter)
                .withMetricsFactory(new NoopMetricsFactory()).withSampler(new ProbabilisticSampler(SAMPLING_RATE))
                .build();
    }

    @Bean
    public RemoteReporter remoteReporter() {
        return new Builder().withSender(new UdpSender(JAEGER_HOST, JAEGER_PORT, 0)).build();
    }

    @Override
    public void onApplicationEvent(ContextRefreshedEvent event) {
        if (!GlobalTracer.isRegistered()) {
            GlobalTracer.register(tracer);
        }
    }
}
</code></pre>
"
Jaeger,63082785,55256948,2,"2020/07/25, 02:41:06",False,"2020/07/25, 02:41:06",176,1446358,0,"<p>I have got following gradle dependencies working,</p>
<pre><code>    implementation &quot;io.opentracing.contrib:opentracing-spring-cloud-starter:0.1.13&quot;
    implementation &quot;io.jaegertracing:jaeger-client:0.31.0&quot;

</code></pre>
<p>Following tracer bean configuration,</p>
<pre><code>  @Bean
    public io.opentracing.Tracer initTracer() {
        Configuration.SamplerConfiguration samplerConfig = new Configuration.SamplerConfiguration().withType(&quot;const&quot;).withParam(1);
        return Configuration.fromEnv(&quot;sprint-service&quot;).withSampler(samplerConfig).getTracer();
    }
</code></pre>
<p>And then the spans can be recorded as</p>
<pre><code>Span sprintSpan = tracer.buildSpan(&quot;my-span-name&quot;).withTag(&quot;player&quot;, player).start();

// some business functionality

sprintSpan.finish();
</code></pre>
<p>Here is the working example you can refer <a href=""https://github.com/krushnatkhawale/jaeger-with-spring-boot-web-app"" rel=""nofollow noreferrer"">https://github.com/krushnatkhawale/jaeger-with-spring-boot-web-app</a></p>
"
Jaeger,64090692,64018706,9,"2020/09/27, 19:21:51",False,"2020/09/28, 12:43:21",3656,3503019,0,"<p><strong>Update</strong>:</p>
<p>I got the same exception (<code>Tracer bean is not configured!..</code>) when I use your version of spring cloud jaeger dependencies. This is irrespective of the <code>RxJava</code> package.</p>
<p>I think you can directly use <code>opentracing-spring-jaeger-cloud-starter</code> which is a combination of <code>opentracing-spring-cloud-starter</code> and <code>opentracing-spring-jaeger-starter</code>. Read <a href=""https://github.com/opentracing-contrib/java-spring-jaeger/blob/master/README.md"" rel=""nofollow noreferrer"">this details</a> for java spring jaeger.</p>
<blockquote>
<p>The opentracing-spring-jaeger-cloud-starter starter is convenience
starter that includes both opentracing-spring-jaeger-starter and
opentracing-spring-cloud-starter This means that by including it, all
parts of the Spring Cloud stack supported by Opentracing will be
instrumented</p>
</blockquote>
<p><strong>Note</strong>:</p>
<p>Maybe RxJava tracing won't work without registering the tracer using the decorators provided by <a href=""https://github.com/opentracing-contrib/java-rxjava/blob/master/README.md"" rel=""nofollow noreferrer"">opentracing-contrib</a>. Please see the working app <a href=""https://github.com/akashsolanki/rxjava-jeager"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I have followed <a href=""https://spring.io/guides/gs/reactive-rest-service/"" rel=""nofollow noreferrer"">this spring guide</a> for Reactive Restful webservice and <code>jaeger</code> worked with below <code>pom.xml</code> without any <code>Tracer bean</code> -</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.spring.opentrace&lt;/groupId&gt;
    &lt;artifactId&gt;jaeger&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;jaeger&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;

    &lt;properties&gt;
        &lt;java.version&gt;11&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Hoxton.SR6&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;
        &lt;/dependency&gt;
        
        &lt;dependency&gt;
          &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
          &lt;artifactId&gt;opentracing-spring-jaeger-cloud-starter&lt;/artifactId&gt;
          &lt;version&gt;3.2.0&lt;/version&gt;
        &lt;/dependency&gt;
       &lt;dependency&gt;
          &lt;groupId&gt;io.reactivex&lt;/groupId&gt;
          &lt;artifactId&gt;rxjava&lt;/artifactId&gt;
          &lt;version&gt;1.3.8&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt;
                    &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>
"
Jaeger,53409452,53172758,0,"2018/11/21, 11:57:59",False,"2018/11/21, 11:57:59",13313,524946,0,"<p>You can turn on the debugging in the client by setting the option <code>JAEGER_REPORTER_LOG_SPANS</code> to true (or use the related option in the <code>ReporterConfiguration</code>, as it seems that's how you are using it).</p>

<p><a href=""https://www.jaegertracing.io/docs/1.8/client-features/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.8/client-features/</a></p>

<p>Once you confirm the traces are being generated and sent to the agent, set the log-level in the agent to <code>debug</code>: </p>

<p><code>
docker run -p... jaegertracing/jaeger-agent:1.8 --log-level=debug
</code></p>

<p>If you don't see anything in the logs there indicating that the agent received a span (or span batch), then you might need to configure your client with the Agent's address (<code>JAEGER_AGENT_HOST</code> and <code>JAEGER_AGENT_PORT</code>, or related options in the <code>Configuration</code> object).</p>

<p>You mentioned that you are deploying in Azure AKS, so, I guess that the agent isn't available at <code>localhost</code>, which is the default location where the client sends the spans. Typically, the agent would be deployed as a sidecar in such a scenario:</p>

<p><a href=""https://github.com/jaegertracing/jaeger-kubernetes#deploying-the-agent-as-sidecar"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-kubernetes#deploying-the-agent-as-sidecar</a></p>
"
Jaeger,53341599,51929412,0,"2018/11/16, 18:11:14",False,"2020/12/19, 21:55:48",463,1749786,2,"<p>The best way to move forward in order to use Jaegar is NOT TO USE JAEGAR CLIENT! Jaegar has the ability to collect Zipkin spans.</p>
<p><a href=""https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.8/getting-started/#migrating-from-zipkin</a></p>
<p>You should take advantage of this and use the below Sleuth+Zipkin dependency and exclude Jaegar agent jars in your spring boot app.</p>
<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;
    &lt;/dependency&gt;
</code></pre>
<p>The above will send Zipkin spans to http://localhost:9411 by default. You can override this in your Spring Boot app to point to your Jaegar server easily by overriding the zipkin base URL.</p>
<pre><code>spring.zipkin.base-url=http://your-jaegar-server:9411
</code></pre>
<p>Sleuth will do all the heavy lifting and the default logging will log the span and traceIds.</p>
<p>In the <code>log4j2.xml</code> file, all you have to mention is</p>
<pre><code>[%X]
</code></pre>
<p>I'll be uploading a working example of this approach into my GitHub and sharing the link.</p>
<p><strong>EDIT 1:</strong></p>
<p>You can find the sample code here:</p>
<p><a href=""https://github.com/anoophp777/spring-webflux-jaegar-log4j2"" rel=""nofollow noreferrer"">https://github.com/anoophp777/spring-webflux-jaegar-log4j2</a></p>
"
Jaeger,53757860,53754605,0,"2018/12/13, 10:35:24",False,"2018/12/13, 10:35:24",39,10384577,1,"<p>So did you install direct  or created a yaml from the templates ?</p>

<p>I would run the command you used to install but with template function and then add the options for jaeger,Kiali and grafana.</p>
"
Jaeger,53761233,53754605,3,"2018/12/13, 13:50:31",True,"2018/12/13, 13:50:31",2167,9485673,6,"<p>Here is <a href=""https://github.com/istio/istio/tree/master/install/kubernetes/helm/istio"" rel=""noreferrer"">howto</a>: from official repository.
 you need to update <a href=""https://github.com/istio/istio/blob/master/install/kubernetes/helm/istio/values.yaml"" rel=""noreferrer"" title=""values.yaml"">values.yaml</a>.
 and turn on  grafana,  kiali and jaeger. For example with kiali change:</p>

<pre><code>kiali:
    enabled:  false
</code></pre>

<p>to </p>

<pre><code>kiali:
    enabled:  true
</code></pre>

<p>than rebuild the Helm dependencies:</p>

<pre><code>helm dep update install/kubernetes/helm/istio
</code></pre>

<p>than upgrade your istio inside kubernetes:</p>

<pre><code>helm upgrade install/kubernetes/helm/istio
</code></pre>

<p>that's it, hope it was helpful </p>
"
Jaeger,53705114,53654993,0,"2018/12/10, 13:51:09",False,"2018/12/10, 13:51:09",63,326167,0,"<ul>
<li>Yes, it's possible. You can deploy these as you would any typical service.</li>
<li>This is situation dependent. It's generally preferrable to extract metrics/logging to their own instances as the performance requirements are potentially different from your applications.  </li>
<li>This is possible to achieve if you configure your instrumentation to communicate directly to the collectors. The collector can receive spans via HTTP on port 14268 (<a href=""https://www.jaegertracing.io/docs/1.6/deployment"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.6/deployment</a>).</li>
</ul>

<p>Scalabilty is dependent on sampling frequency and volumes. The agent supports adaptive sampling which is a feedback loop from the collector to your instrumented app.</p>

<p>You can statically define this up front in your instrumentation but you lose the adaptive features.</p>
"
Jaeger,53706156,53654993,0,"2018/12/10, 14:56:58",False,"2018/12/10, 14:56:58",21,10770815,1,"<p>It is possible to bypass agent all together and send metrics directly to collector.</p>

<p>Just define variable JAEGER_ENDPOINT in your app running environment.</p>

<p>This behaviour is documented but buried down in the Jager git repo:</p>

<p><a href=""https://github.com/jaegertracing/jaeger-client-java/blob/master/jaeger-core/README.md"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-java/blob/master/jaeger-core/README.md</a></p>
"
Jaeger,54325898,53654993,0,"2019/01/23, 13:09:57",False,"2019/01/23, 13:09:57",13313,524946,0,"<blockquote>
  <p>Would a single agent colocated with a single collector be possible in a Jaeger deployment?</p>
</blockquote>

<p>It's possible, and that's how the <a href=""https://www.jaegertracing.io/docs/1.8/getting-started/#all-in-one"" rel=""nofollow noreferrer"">""all-in-one""</a> image works.</p>

<blockquote>
  <p>Would it be advisable?</p>
</blockquote>

<p>Depends on your architecture. If you don't expect your Jaeger infra to grow, using the all-in-one is easier from the maintenance perspective. If you need your Jaeger infra to be highly available, then you probably want to place your agents closer to your instrumented applications than to your collector and scale the collectors separately.</p>

<p>More about the Jaeger Agent is discussed in the following blog posts:</p>

<p><a href=""https://medium.com/jaegertracing/running-jaeger-agent-on-bare-metal-d1fc47d31fab"" rel=""nofollow noreferrer"">Running Jaeger Agent on bare metal</a>
<a href=""https://medium.com/jaegertracing/deployment-strategies-for-the-jaeger-agent-1d6f91796d09"" rel=""nofollow noreferrer"">Deployment strategies for the Jaeger Agent</a></p>

<blockquote>
  <p>Is it possible to skip the agent altogether and submit spans directly to the collector over HTTP?</p>
</blockquote>

<p>For some clients (Java, NodeJS and C#), yes. Look for the <a href=""https://www.jaegertracing.io/docs/1.8/client-features/"" rel=""nofollow noreferrer""><code>JAEGER_ENDPOINT</code> option</a>.</p>
"
Jaeger,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111,2035350,0,"<p>Thing is, you should use opentelemetry collector if you use opentelemetry exporter.</p>
<p>Pls see schema in attachment <a href=""https://i.stack.imgur.com/i2yfZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i2yfZ.png"" alt=""enter image description here"" /></a></p>
<p>Also I created a gist, which will help you to setup
pls see</p>
<p><a href=""https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f"" rel=""nofollow noreferrer"">https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f</a></p>
<p>(just tune the values, export to jaeger-all-in-one instead of separate + cassandra, etc)</p>
"
Jaeger,57986473,57976158,0,"2019/09/18, 09:36:52",True,"2019/09/18, 09:36:52",31,10489752,1,"<p>'reporting_host' must be not 'localhost', but 'jaeger', just as service in docker-compose.yml called.
<code>'reporting_host' =&gt; 'jaeger',</code></p>

<p>Also, I needed to add <code>$tracer-&gt;flush();</code> after all, it closes all the entities and does sending via UDP behind the scenes. </p>
"
Jaeger,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441,10020419,4,"<blockquote>
<p>As, you can see there are few missing components - There are few pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc. These components were available in 1.4.2.</p>
</blockquote>
<p>These components where merged with version 1.5 into one service named <code>istiod</code>. See: <a href=""https://istio.io/latest/blog/2020/istiod/"" rel=""nofollow noreferrer"">https://istio.io/latest/blog/2020/istiod/</a></p>
<blockquote>
<p>In 1.4.2 installation I could see grafana, jaeger, kiali, prometheus, zipkin dashboards. But these are now missing.</p>
</blockquote>
<p>These AddonComponents must be installed manually and are not part of <code>istioctl</code> since version 1.7. See: <a href=""https://istio.io/latest/blog/2020/addon-rework/"" rel=""nofollow noreferrer"">https://istio.io/latest/blog/2020/addon-rework/</a></p>
<p>So your installation is not broken. It's just a lot has changed since 1.4. I would suggest to go through the release announcements to read about all changes: <a href=""https://istio.io/latest/news/releases/"" rel=""nofollow noreferrer"">https://istio.io/latest/news/releases/</a></p>
"
Jaeger,62861784,62859856,0,"2020/07/12, 16:39:14",True,"2020/07/12, 16:39:14",654,955379,8,"<p>Iv finally found the solution. It seemed to have to do with how the reporter is started up. Anyhow, I changed my tracer class to this.</p>
<pre><code>    public static class MyTracer
{
    public static ITracer tracer = null;
    public static ITracer InitTracer(IServiceProvider serviceProvider)
    {
        string serviceName = serviceProvider.GetRequiredService&lt;IHostingEnvironment&gt;().ApplicationName;

        Environment.SetEnvironmentVariable(&quot;JAEGER_SERVICE_NAME&quot;, &quot;my-store&quot;);
        //Environment.SetEnvironmentVariable(&quot;JAEGER_AGENT_HOST&quot;, &quot;192.168.2.27&quot;);
        //Environment.SetEnvironmentVariable(&quot;JAEGER_AGENT_PORT&quot;, &quot;6831&quot;);
        //Environment.SetEnvironmentVariable(&quot;JAEGER_SAMPLER_TYPE&quot;, &quot;const&quot;);                     
        //Environment.SetEnvironmentVariable(&quot;JAEGER_REPORTER_LOG_SPANS&quot;, &quot;false&quot;);
        //Environment.SetEnvironmentVariable(&quot;JAEGER_SAMPLER_PARAM&quot;,&quot;1&quot;);
        //Environment.SetEnvironmentVariable(&quot;JAEGER_SAMPLER_MANAGER_HOST_PORT&quot;, &quot;5778&quot;);
        //Environment.SetEnvironmentVariable(&quot;JAEGER_REPORTER_FLUSH_INTERVAL&quot; , &quot;1000&quot;);
        //Environment.SetEnvironmentVariable(&quot;JAEGER_REPORTER_MAX_QUEUE_SIZE&quot; , &quot;100&quot;);
        //application - server - id = server - x

        var loggerFactory = serviceProvider.GetRequiredService&lt;ILoggerFactory&gt;();

        var sampler = new ConstSampler(sample: true);
        var reporter = new RemoteReporter.Builder()
         .WithLoggerFactory(loggerFactory)
         .WithSender(new UdpSender(&quot;192.168.2.27&quot;, 6831, 0))
         .Build();

        tracer = new Tracer.Builder(serviceName)
         .WithLoggerFactory(loggerFactory)
         .WithSampler(sampler)
         .WithReporter(reporter)
         .Build();

        if (!GlobalTracer.IsRegistered())
        {
            GlobalTracer.Register(tracer);
        }
        return tracer;
    }
}
</code></pre>
<p>I know there are several inactive variables here right now. Will see if they still can be of use some how. But none is needed right now to get it rolling.
Hope this might help someone else trying to get the .NET Core working properly together with a remote Jeagertracing server.</p>
"
Jaeger,60706055,60636937,0,"2020/03/16, 14:29:14",False,"2020/03/16, 14:29:14",596,5089120,1,"<p>I use <code>io.opentracing.contrib:opentracing-spring-rabbitmq-starter:3.0.0</code>.
You can find some documentation <a href=""https://github.com/opentracing-contrib/java-spring-rabbitmq"" rel=""nofollow noreferrer"">here</a>. With this mechanism I achieved exactly what you asked for.</p>

<p>I found 2 important things to make sure:  </p>

<ol>
<li>The rabbit template has to be initialized BEFORE the <code>RabbitMqTracingAutoConfiguration</code> is called. In the beginning I had the problem that the bean was not created as the RabbitTemplate was created AFTER the <code>RabbitMqTracingAutoConfiguration</code> was called.   </li>
<li>Not all RestTemplate methods are surrounded with the Aspect. <a href=""https://github.com/opentracing-contrib/java-spring-rabbitmq/blob/release-3.0.0/opentracing-spring-rabbitmq/src/main/java/io/opentracing/contrib/spring/rabbitmq/RabbitMqSendTracingAspect.java#L32"" rel=""nofollow noreferrer"">Here</a> you can find an overview of which methods are currently supported. We have currently the problem, that <code>RabbitTemplate.invoke</code> is not supported.</li>
</ol>

<p>Hopefully you find this helpful.</p>
"
Jaeger,57119911,57119864,0,"2019/07/20, 00:47:14",True,"2019/07/20, 00:47:14",1067,1795610,1,"<p>Ugh. I am an idiot.</p>

<p>Here is what was going wrong for anyone else who might be stuck something like this:</p>

<p>The frontend application <em>is</em> receiving a header, I was just looking in the wrong place.</p>

<p>The request comes from the load balancer to the node frontend microservice which sends its response to the browser.</p>

<p>I was checking the browser for the header, but the node frontend microservice was not forwarding this header to the browser.</p>
"
Jaeger,56143481,56054438,0,"2019/05/15, 10:07:09",True,"2019/05/15, 10:07:09",1520,824434,3,"<p>If anyone is interested; I ended up solving this by creating some publish and consume MassTransit middleware to do the trace propagation via trace injection and extraction respectively.</p>

<p>I've put the solution up on GitHub - <a href=""https://github.com/yesmarket/MassTransit.OpenTracing"" rel=""nofollow noreferrer"">https://github.com/yesmarket/MassTransit.OpenTracing</a></p>

<p>Still interested to hear if there's a better way of doing this...</p>
"
Jaeger,55858211,55506381,1,"2019/04/26, 01:01:46",True,"2019/04/26, 01:01:46",1448,5788941,2,"<p>In istio 1.1, the default sampling rate is 1%, so you need to send at least 100 requests before the first trace is visible. </p>

<p>This can be configured through the <code>pilot.traceSampling</code> option.</p>
"
Jaeger,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313,524946,2,"<p>OpenTracing is the API that <em>your</em> code will interact with directly. </p>

<p>Basically, your application would be ""instrumented"" using the OpenTracing API and a concrete tracer (like Jaeger or Brave/Zipkin) would capture the data and send it somewhere. This allows your application to use a neutral API throughout your code so that you can change from one provider to another without having to change your entire code base.</p>

<p>Another way to think about it is that OpenTracing is like SLF4J in the Java logging world, whereas Jaeger and Zipkin are the concrete implementations, such as Log4j in the Java logging world.</p>
"
Jaeger,59524696,48794097,0,"2019/12/30, 03:11:09",False,"2019/12/30, 03:11:09",2172,3514300,0,"<p>Trace context propagation might be missing. </p>

<p><a href=""https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-context-propagation"" rel=""nofollow noreferrer"">https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-context-propagation</a></p>

<pre><code>Although Istio proxies are able to automatically send spans, they need some hints to tie together the entire trace. Applications need to propagate the appropriate HTTP headers so that when the proxies send span information, the spans can be correlated correctly into a single trace.

To do this, an application needs to collect and propagate the following headers from the incoming request to any outgoing requests:

x-request-id
x-b3-traceid
x-b3-spanid
x-b3-parentspanid
x-b3-sampled
x-b3-flags
x-ot-span-context

Additionally, tracing integrations based on OpenCensus (e.g. Stackdriver) propagate the following headers:

x-cloud-trace-context
traceparent
grpc-trace-bin
</code></pre>
"
Jaeger,25716102,25715304,4,"2014/09/08, 03:49:31",False,"2014/09/08, 03:49:31",194,1478294,0,"<p>I cannot test your code, but the only thing i can think off, is that the order of execution is wrong. You first make a string, then you make it Base64, then you encrypt it. Now you undo the Base64 and afterwards you decrypt the encoded string. These last two must be swapped.</p>
"
Jaeger,25756628,25715304,1,"2014/09/10, 05:56:20",True,"2014/09/10, 08:14:52",22907,2714852,7,"<p>A reminder that <strong>security is unusually treacherous territory</strong>, and if there's a way to call on other well-tested code even more of your toplevel task than just what Go's OpenPGP package is handling for you, consider it. It's good that at least low-level details are outsourced to <code>openpgp</code> because they're nasty and so so easy to get wrong. But tiny mistakes at any level can make crypto features worse than useless; if there's a way to write less security-critical code, that's one of the best things anyone can do for security.</p>

<p>On the specific question: you have to <code>Close()</code> the writer to get everything flushed out (a trait OpenPGP's writer shares with, say, <code>compress/gzip</code>'s). </p>

<p>Unrelated changes: the way you're printing things is a better fit <code>log.Println</code>, which just lets you pass a bunch of values you want printed with spaces in between (like, say, Python <code>print</code>), rather than needing format specifiers like <code>""%s""</code> or <code>""%d""</code>. (The ""EXTRA"" in your initial output is what Go's <code>Printf</code> emits when you pass more things than you had format specifiers for.) It's also best practice to check errors (I dropped <code>if err != nil</code>s where I saw a need, but inelegantly and without much thought, and I may not have gotten all the calls) and to run <code>go fmt</code> on your code. </p>

<p>Again, <strong>I can't testify to the seaworthiness of this code or anything like that.</strong> But now it round-trips all the text. I wound up with:</p>

<pre><code>package main

import (
    ""bytes""
    ""code.google.com/p/go.crypto/openpgp""
    ""encoding/base64""
    ""io/ioutil""
    ""log""
    ""os""
)

// create gpg keys with
// $ gpg --gen-key
// ensure you correct paths and passphrase

const mysecretstring = ""this is so very secret!""
const prefix, passphrase = ""/Users/stuart-warren/"", ""1234""
const secretKeyring = prefix + "".gnupg/secring.gpg""
const publicKeyring = prefix + "".gnupg/pubring.gpg""

func encTest() error {
    log.Println(""Secret:"", mysecretstring)
    log.Println(""Secret Keyring:"", secretKeyring)
    log.Println(""Public Keyring:"", publicKeyring)
    log.Println(""Passphrase:"", passphrase)

    // Read in public key
    keyringFileBuffer, _ := os.Open(publicKeyring)
    defer keyringFileBuffer.Close()
    entitylist, err := openpgp.ReadKeyRing(keyringFileBuffer)
    if err != nil {
        return err
    }

    // encrypt string
    buf := new(bytes.Buffer)
    w, err := openpgp.Encrypt(buf, entitylist, nil, nil, nil)
    if err != nil {
        return err
    }
    _, err = w.Write([]byte(mysecretstring))
    if err != nil {
        return err
    }
    err = w.Close()
    if err != nil {
        return err
    }

    // Encode to base64
    bytesp, err := ioutil.ReadAll(buf)
    if err != nil {
        return err
    }
    encstr := base64.StdEncoding.EncodeToString(bytesp)

    // Output encrypted/encoded string
    log.Println(""Encrypted Secret:"", encstr)

    // Here is where I would transfer the encrypted string to someone else
    // but we'll just decrypt it in the same code

    // init some vars
    var entity2 *openpgp.Entity
    var entitylist2 openpgp.EntityList

    // Open the private key file
    keyringFileBuffer2, err := os.Open(secretKeyring)
    if err != nil {
        return err
    }
    defer keyringFileBuffer2.Close()
    entitylist2, err = openpgp.ReadKeyRing(keyringFileBuffer2)
    if err != nil {
        return err
    }
    entity2 = entitylist2[0]

    // Get the passphrase and read the private key.
    // Have not touched the encrypted string yet
    passphrasebyte := []byte(passphrase)
    log.Println(""Decrypting private key using passphrase"")
    entity2.PrivateKey.Decrypt(passphrasebyte)
    for _, subkey := range entity2.Subkeys {
        subkey.PrivateKey.Decrypt(passphrasebyte)
    }
    log.Println(""Finished decrypting private key using passphrase"")

    // Decode the base64 string
    dec, err := base64.StdEncoding.DecodeString(encstr)
    if err != nil {
        return err
    }

    // Decrypt it with the contents of the private key
    md, err := openpgp.ReadMessage(bytes.NewBuffer(dec), entitylist2, nil, nil)
    if err != nil {
        return err
    }
    bytess, err := ioutil.ReadAll(md.UnverifiedBody)
    if err != nil {
        return err
    }
    decstr := string(bytess)

    // should be done
    log.Println(""Decrypted Secret:"", decstr)

    return nil
}

func main() {
    err := encTest()
    if err != nil {
        log.Fatal(err)
    }
}
</code></pre>
"
Jaeger,64814852,64755896,0,"2020/11/13, 05:00:24",True,"2020/11/13, 05:00:24",121,3792242,1,"<p>It's not clear in the documentation, but I managed to get it working by providing the <code>SPAN_STORAGE_TYPE</code> and the respective connection details to allow the jaeger components to talk to the storage running outside of the all-in-one container.</p>
<p>For instance, I'm running elasticsearch on my Mac, so I used the following command to run all-in-one:</p>
<pre><code>docker run -d --name jaeger-es \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -e SPAN_STORAGE_TYPE=elasticsearch \
  -e ES_SERVER_URLS=http://host.docker.internal:9200 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.20
</code></pre>
"
Jaeger,62300680,62273414,0,"2020/06/10, 12:52:33",True,"2020/06/10, 12:52:33",155,1222237,0,"<p>Thanks Yuri. Yes it was a clock issue.
Although the host machine (VM) updated its clock on every unpause, docker for windows did not. The timezones were correct for all containers but the times were all out by the exact same amount. This must be the docker internal clock that seems to only get updated once on launch and not at the launch of every new container. Although all container clocks were out by the same amount, the windows host machine was correct. 
The messages were arriving but the times/dates were outside the time frame window the UI was displaying. If I set a custom date range I'm sure they would appear. The containers must have been out by a few days with the continual stopping and starting and not by just a few hours.</p>
"
Jaeger,66277887,62273414,0,"2021/02/19, 14:49:58",False,"2021/02/19, 14:49:58",169,3301685,0,"<p>Time drift in docker is a known issue on Mac and Windows OS.</p>
<p>Check the date/time in a docker container using this (apologies in advance)</p>
<pre><code>( ConvertFrom-Json (docker system info --format '{{json .}}') ).SystemTime
</code></pre>
<p>or calculate the drift...</p>
<pre><code>(Get-Date -DisplayHint DateTime) - [DateTime]( ConvertFrom-Json (docker system info --format '{{json .}}') ).SystemTime
</code></pre>
<p>Unfortunately there doesnt seem to be a better solution than occasionally restarting the container.</p>
"
Jaeger,66314862,66299365,0,"2021/02/22, 13:29:44",True,"2021/02/22, 13:29:44",4113,12201084,1,"<p>Based on following example from <a href=""https://www.jaegertracing.io/docs/1.21/operator/#streaming-strategy"" rel=""nofollow noreferrer"">jaeger docs</a>:</p>
<pre><code>apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-streaming
spec:
  strategy: streaming
  collector:
    options:
      kafka: # &lt;1&gt;
        producer:
          topic: jaeger-spans
          brokers: my-cluster-kafka-brokers.kafka:9092
  ingester:
    options:
      kafka: # &lt;1&gt;
        consumer:
          topic: jaeger-spans
          brokers: my-cluster-kafka-brokers.kafka:9092
      ingester:
        deadlockInterval: 5s # &lt;2&gt;
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://elasticsearch:9200
</code></pre>
<p>and on example <a href=""https://www.jaegertracing.io/docs/1.21/cli/"" rel=""nofollow noreferrer"">cli falgs</a>:</p>
<pre><code>--kafka.producer.topic  jaeger-spans
The name of the kafka topic
--kafka.producer.brokers    127.0.0.1:9092
The comma-separated list of kafka brokers. i.e. '127.0.0.1:9092,0.0.0:1234'
--kafka.producer.plaintext.password 
The plaintext Password for SASL/PLAIN authentication
--kafka.producer.plaintext.username 
The plaintext Username for SASL/PLAIN authentication
</code></pre>
<p>I infere that you should be able to do the following:</p>
<pre><code>spec:
     strategy: streaming
  collector:
    options:
      kafka: # &lt;1&gt;
        producer:
          topic: jaeger-spans
          brokers: my-cluster-kafka-brokers.kafka:9092
          plaintext:
            username: &lt;username&gt;
            password: &lt;password&gt; 
</code></pre>
<p>Notice that I split the cli options with the dot and added it as a nested fields in yaml. Do the same to other parameters by analogy.</p>
"
Jaeger,52721877,52628054,0,"2018/10/09, 16:09:20",True,"2018/10/09, 16:09:20",13313,524946,0,"<p>No, <a href=""https://github.com/opentracing-contrib/java-spring-cloud/blob/2cc7d1d3666fb8b289f0f6223b662b5e72700895/instrument-starters/opentracing-spring-cloud-zuul-starter/src/main/java/io/opentracing/contrib/spring/cloud/zuul/TracePreZuulFilter.java#L56"" rel=""nofollow noreferrer"">it cannot</a>, but it wouldn't hurt to open an issue there with this suggestion.</p>
"
Jaeger,64532290,64484169,0,"2020/10/26, 08:17:55",True,"2020/10/26, 08:17:55",5749,11977760,0,"<p>According to istio <a href=""https://istio.io/latest/docs/tasks/observability/distributed-tracing/jaeger/#generating-traces-using-the-bookinfo-sample"" rel=""nofollow noreferrer"">documentation</a></p>
<blockquote>
<p>To see trace data, you must send requests to your service. The number of requests depends on Istio’s sampling rate. You set this rate when you install Istio. The default sampling rate is 1%. You need to send at least 100 requests before the first trace is visible. Could you try to send at least 100 requests and check if it works?</p>
</blockquote>
<p>If you wan't to change the default sampling rate then there is istio <a href=""https://istio.io/latest/docs/tasks/observability/distributed-tracing/configurability/#customizing-trace-sampling"" rel=""nofollow noreferrer"">documentation</a> about that.</p>
<blockquote>
<p><strong>Customizing Trace sampling</strong></p>
<p>The sampling rate option can be used to control what percentage of requests get reported to your tracing system. This should be configured depending upon your traffic in the mesh and the amount of tracing data you want to collect. The default rate is 1%.</p>
<p>To modify the default random sampling to 50, add the following option to your tracing.yaml file.</p>
</blockquote>
<pre><code>apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  meshConfig:
    defaultConfig:
      tracing:
        sampling: 50
</code></pre>
<blockquote>
<p>The sampling rate should be in the range of 0.0 to 100.0 with a precision of 0.01. For example, to trace 5 requests out of every 10000, use 0.05 as the value here.</p>
</blockquote>
"
Jaeger,64036080,62420728,0,"2020/09/23, 23:55:37",False,"2020/09/23, 23:55:37",1048,1112106,1,"<p>What you did is for http 1.x, and it doesn't work for http2/grpc. Please dive into grpc impl in springboot doc.</p>
"
Jaeger,64808184,58763972,1,"2020/11/12, 18:51:10",False,"2020/11/12, 18:51:10",163,823789,0,"<p>Unfortunatelly, the reporter interface is used to report FINISHED spans, it is invoked on JaegerSpan.finish. I presume this is why it does not appear in logs.</p>
"
Jaeger,59578042,59577629,0,"2020/01/03, 13:44:16",False,"2020/01/03, 13:53:57",nan,nan,1,"<p>A colleague of mine provided the answer... It was hidden in the Makefile, which hadn't worked for me as I don't use Golang (and it had been more complex than just installing Golang and running it, but I digress...).</p>

<p>The following .sh will do the trick. This assumes the query.proto file is a subdirectory from the same location as the script below, under model/proto/api_v2/ (as it appears in the main Jaeger repo). </p>

<pre class=""lang-sh prettyprint-override""><code>#!/usr/bin/env sh
set +x

rm -rf ./js_out 2&gt; /dev/null
mkdir ./js_out

PROTO_INCLUDES=""
    -I model/proto \
    -I idl/proto \
    -I vendor/github.com/grpc-ecosystem/grpc-gateway \
    -I vendor/github.com/gogo/googleapis \
    -I vendor/github.com/gogo/protobuf/protobuf \
    -I vendor/github.com/gogo/protobuf""

python -m grpc_tools.protoc ${PROTO_INCLUDES} --grpc_python_out=./python_out --python_out=./python_out model/proto/api_v2/query.proto
</code></pre>

<p>This will definitely generate the needed Python file, but it will still be missing dependencies. </p>
"
Jaeger,51977735,51878525,0,"2018/08/23, 05:56:58",True,"2018/08/23, 05:56:58",4269,1047335,1,"<p>I resolved this.
It related to the sample rate.
After I configured the <code>JAEGER_SAMPLER_TYPE</code> and <code>JAEGER_SAMPLER_PARAM</code>, I can see the data.</p>
"
Jaeger,48432603,46561079,0,"2018/01/25, 00:16:25",True,"2019/12/10, 06:06:47",471,2673284,2,"<p>The Downloads page (<a href=""https://www.jaegertracing.io/download/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/download/</a>) lists both the Docker images and the raw binaries built for various platforms (Linux, macOS, windows). You can also build binaries from source.</p>
"
Jaeger,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169,3301685,0,"<p>Just to add to Yuris answer, you can also download the source from github - <a href=""https://test"" rel=""nofollow noreferrer"">Github - Jaeger</a> This is useful for diagnosing issues, or just getting a better understanding of how it all works.</p>
<p>I have run both the released apps and custom versions on both windows and linux servers without issues. For windows I would recommend running as a service using Nssm. <a href=""https://nssm.cc/usage"" rel=""nofollow noreferrer"">Nssm details</a></p>
"
Jaeger,52813216,52764397,2,"2018/10/15, 12:05:59",False,"2018/10/15, 12:05:59",13313,524946,0,"<p>The Jaeger tracer (the part that runs along with your application) will send spans via UDP to an agent running on localhost by default. If your agent is somewhere else, set the <code>JAEGER_AGENT_HOST</code>/<code>JAEGER_AGENT_PORT</code> env vars accordingly. If you don't want an agent running on localhost and want to access a Jaeger Collector directly via HTTP, then set the <code>JAEGER_ENDPOINT</code> env var.</p>

<p>More info about these env vars can be found in the <a href=""https://www.jaegertracing.io/docs/1.7/client-features/"" rel=""nofollow noreferrer"">documentation</a> or here:
<a href=""https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core#configuration-via-environment"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-core#configuration-via-environment</a></p>
"
Jaeger,63116714,62992614,0,"2020/07/27, 16:44:44",False,"2020/07/27, 16:44:44",43,7017126,0,"<p>I have resolved it after configuring port as 14250 as JaegerGrpcSpanExporter internally uses grpc port which has been configured to 14250 for jaeger-collector</p>
"
Jaeger,53537859,53518480,0,"2018/11/29, 13:19:37",True,"2018/11/29, 13:19:37",321,4125383,2,"<p>The problem was, that the Report instance used a NoopSender -- thus ignoring the connection settings.</p>

<p>Using</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;io.jaegertracing&lt;/groupId&gt;
        &lt;artifactId&gt;jaeger-thrift&lt;/artifactId&gt;
        &lt;version&gt;0.32.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>in your POM will provide an appropriate Sender to the SenderFactory used by Jaeger's SenderResolver::resolve method.</p>

<p>This solved my problem.</p>
"
Jaeger,59678839,59655255,6,"2020/01/10, 11:30:18",False,"2020/01/10, 11:30:18",23024,9773937,0,"<p>Not sure, but it seems you miss setting the TLS for Cassandra Storage in Azure Cosmos DB. When you use the Cassandra client to connect the Cassandra Storage in Azure Cosmos DB, it will give out the time out error, but if you enable the SSL, the connection works well. So I think you can try to enable the TLS for Cassandra in your values.yaml following the steps in the Github which provided.</p>
"
Jaeger,65163350,65138941,0,"2020/12/06, 02:12:58",False,"2020/12/06, 02:12:58",166,13922022,0,"<p>See the answer on this <a href=""https://github.com/jaegertracing/jaeger/issues/1564#issuecomment-667291036"" rel=""nofollow noreferrer"">jaeger issue</a>, you will need to query elastic search or the source where the data is stored.</p>
<p>Alternatively, you should raise an issue on <a href=""https://github.com/jaegertracing/jaeger-ui/issues"" rel=""nofollow noreferrer"">jaeger-ui</a> detailing your case.</p>
"
Jaeger,65299006,65138941,0,"2020/12/15, 04:01:30",False,"2020/12/15, 04:01:30",471,2673284,0,"<p>When you open an individual trace in the Jaeger UI, there is a View dropdown in the top right corner. One of the options is to view/download the given trace as a JSON file.</p>
<p>You can also programmatically query the Jaeger query service via JSON/Protobuf API, but those endpoints will not result in a data format that you can load back into the UI.</p>
<p><a href=""https://www.jaegertracing.io/docs/latest/apis/#trace-retrieval-apis"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/latest/apis/#trace-retrieval-apis</a></p>
"
Jaeger,57257482,57211703,2,"2019/07/29, 18:58:36",True,"2020/05/16, 21:59:41",445,3834445,3,"<p>Try using a configuration:</p>

<pre><code>version: ""3""

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.8.1
    networks:
      - elastic-jaeger
    ports:
      - ""127.0.0.1:9200:9200""
      - ""127.0.0.1:9300:9300""
    restart: on-failure
    environment:
      - cluster.name=jaeger-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - transport.host=127.0.0.1
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data

  apm-server:
    image: docker.elastic.co/apm/apm-server:6.8.1
    ports:
      - 8200:8200

    volumes:
       - ./apm-server/config/apm-server.yml:/usr/share/apm-server/apm-server.yml

    networks:
      - elastic-jaeger

  kibana:
    image: docker.elastic.co/kibana/kibana:6.8.1
    ports:
      - ""127.0.0.1:5601:5601""
    restart: on-failure
    networks:
      - elastic-jaeger

  jaeger-collector:
    image: jaegertracing/jaeger-collector
    ports:
      - ""14269:14269""
      - ""14268:14268""
      - ""14267:14267""
      - ""9411:9411""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--es.num-shards=1"",
      ""--es.num-replicas=0"",
      ""--log-level=error""
    ]
    depends_on:
      - elasticsearch

  jaeger-agent:
    image: jaegertracing/jaeger-agent
    hostname: jaeger-agent
    command: [""--collector.host-port=jaeger-collector:14267""]
    ports:
      - ""5775:5775/udp""
      - ""6831:6831/udp""
      - ""6832:6832/udp""
      - ""5778:5778""
    networks:
      - elastic-jaeger
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
    depends_on:
      - jaeger-collector

  jaeger-query:
    image: jaegertracing/jaeger-query
    environment:
      - SPAN_STORAGE_TYPE=elasticsearch
      - no_proxy=localhost
    ports:
      - ""16686:16686""
      - ""16687:16687""
    networks:
      - elastic-jaeger
    restart: on-failure
    command: [
      ""--es.server-urls=http://elasticsearch:9200"",
      ""--span-storage.type=elasticsearch"",
      ""--log-level=debug""
    ]
    depends_on:
      - jaeger-agent

  grafana:
    image: grafana/grafana
    ports:
      - 3999:3000
    volumes:
      - ./grafana-data:/var/lib/grafana
    networks:
      - elastic-jaeger

volumes:
  esdata:
    driver: local

networks:
  elastic-jaeger:
    driver: bridge 

</code></pre>

<p>where the file apm-server/config/apm-server.yml has your config content:</p>

<pre><code>apm-server.rum.enabled: true
apm-server.rum.event_rate.limit: 300
apm-server.rum.event_rate.lru_size: 1000
apm-server.rum.allow_origins: ['*']
apm-server.rum.library_pattern: ""node_modules|bower_components|~""
apm-server.rum.exclude_from_grouping: ""^/webpack""
apm-server.rum.source_mapping.cache.expiration: 5m
apm-server.rum.source_mapping.index_pattern: ""apm-*-sourcemap*""
output.elasticsearch.hosts: [""http://elasticsearch:9200""]
apm-server.host: ""0.0.0.0:8200""
setup.kibana.host: ""kibana:5601""
setup.template.enabled: true
logging.to_files: false

</code></pre>

<p>Note the rum.allow_origins option that you can configure to resolve the CORS issue. <a href=""https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html"" rel=""nofollow noreferrer"">https://www.elastic.co/guide/en/apm/agent/rum-js/master/configuring-cors.html</a></p>
"
Jaeger,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336,1773866,1,"<p>Yes you can, and I have shown that numerous times during my presentations (<a href=""https://toomuchcoding.com/talks"" rel=""nofollow noreferrer"">https://toomuchcoding.com/talks</a>) and we describe it extensively in the documentation (<a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/</a>). Sleuth will set up your logging pattern which you can then parse and visualize using the ELK stack. Sleuth takes care of tracing context propagation and can send the spans to a span storage (e.g. Zipkin or Jaeger). Sleuth does take care of updating the MDC for you. Please always read the documentation and the project page before filing a question</p>
"
Jaeger,58696749,58504344,2,"2019/11/04, 17:45:56",False,"2019/11/04, 17:45:56",721,1563297,1,"<p><code>spring-cloud-openfeign</code> since is from the spring-cloud family should be instrumented automatically once you add <code>opentracing-spring-jaeger-cloud-starter</code> the  as stated <a href=""https://github.com/opentracing-contrib/java-spring-jaeger#configuration"" rel=""nofollow noreferrer"">here</a>.
But sometimes (depending on how you create your feign client bean) you need to explicitly expose the bean to the spring context, so that the autoconfiguration can instrument your Feign Client.</p>

<p>Something like this:</p>

<pre><code>@Scope(""prototype"")
fun feignClient() : Client {
    return Client.Default(null, null)
}
</code></pre>

<p>it is kotlin but you can adapt.</p>
"
Jaeger,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749,11977760,2,"<p>The answer here is to install istio with <code>--set values.global.tracer.zipkin.address</code> as provided in <a href=""https://istio.io/docs/tasks/observability/distributed-tracing/jaeger/#before-you-begin"" rel=""nofollow noreferrer"">istio documentation</a></p>

<pre><code>istioctl manifest apply --set values.global.tracer.zipkin.address=&lt;jaeger-collector-service&gt;.&lt;jaeger-collector-namespace&gt;:9411
</code></pre>

<hr>

<p><strong>And</strong></p>

<hr>

<p>Use the original TracingService <code>setting: service: ""zipkin.istio-system:9411""</code> as Donato Szilagyi confirmed in comments.</p>

<pre><code>apiVersion: getambassador.io/v2
kind: TracingService
metadata:
  name: tracing
  namespace: {{ .Values.namespace }}
spec:
  service: ""zipkin.istio-system:9411""
  driver: zipkin
  ambassador_id: ambassador-{{ .Values.namespace }}
  config: {}
</code></pre>

<blockquote>
  <p>Great! It works. And this time I used the original TracingService setting: service: ""zipkin.istio-system:9411"" – Donato Szilagy</p>
</blockquote>
"
Jaeger,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287,780798,2,"<p><a href=""https://opentracing.io/specification/"" rel=""nofollow noreferrer"">OpenTracing</a> <a href=""https://github.com/opentracing/specification/blob/master/project_organization.md"" rel=""nofollow noreferrer"">is a set of standard APIs that consistently model and describe the behavior of distributed systems</a>)</p>

<p>OpenTracing did not describe how to collect, report, store or represent the data of interrelated traces and spans. It is implementation details (such as <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">jaeger</a> or <a href=""https://www.wavefront.com/"" rel=""nofollow noreferrer"">wavefront</a>).</p>

<p>jaeger-client-csharp is very jaeger-specific. But there is one exception, called <a href=""https://www.jaegertracing.io/docs/1.12/features/#backwards-compatibility-with-zipkin"" rel=""nofollow noreferrer"">zipkin</a> which in turns is not fully OpenTracing compliant, even it has similar terms.</p>

<p>If you are OK with <a href=""https://github.com/opentracing-contrib/csharp-netcore/"" rel=""nofollow noreferrer"">opentracing-contrib/csharp-netcore</a> (hope you are using this library) then if you want to achieve ""no code change"" (in target microservice) in order to configure tracing subsystem, you should use some plug-in model.</p>

<p>Good news that aspnetcore has concept of <a href=""https://docs.microsoft.com/en-us/ASPNET/Core/fundamentals/host/platform-specific-configuration?view=aspnetcore-2.2"" rel=""nofollow noreferrer"">hosted startup assemblies</a>, which allow you to configure tracing system. So, you can have some library called <code>JaegerStartup</code> where you will implement IHostedStartup like follows:</p>

<pre><code>public class JaegerStartup : IHostingStartup
{
    public void Configure(IWebHostBuilder builder)
    {
        builder.ConfigureServices((ctx, services) =&gt;
        {
            services.AddOpenTracing();

            if (ctx.Configuration.IsTracerEnabled()) // implement it by observing your section in configuration.
            {
                services.AddSingleton(serviceProvider =&gt;
                {
                    var loggerFactory = new LoggerFactory();
                    var config = Jaeger.Configuration.FromEnv(loggerFactory);

                    var tracer = config.GetTracer();

                    GlobalTracer.Register(tracer);

                    return tracer;
                });
            }
        });
    }
}
</code></pre>

<p>When you decide to switch the tracing system - you need to create another library, which can be loaded automatically, and target microservice code will not be touched.</p>
"
Jaeger,54367583,53709029,0,"2019/01/25, 16:46:35",False,"2019/01/25, 16:46:35",13313,524946,0,"<p>I don't think it's possible at the moment and you should definitely ask for this feature in the mailing list, Gitter or GitHub issue. The current assumption is that a clear TChannel connection can be made between the agent and collector(s), all being part of the same trusted network.</p>

<p>If you are using the Java, Node or C# client, my recommendation in your situation is to have your Jaeger Client to talk directly to the collector. Look for the env var <code>JAEGER_ENDPOINT</code> in the <a href=""https://www.jaegertracing.io/docs/1.9/client-features/"" rel=""nofollow noreferrer"">Client Features</a> documentation page.</p>
"
Jaeger,64743914,64743246,1,"2020/11/09, 01:00:09",True,"2020/11/09, 02:02:44",121,3792242,2,"<ol>
<li>To my knowledge, there is no easy way as there is no guarantee that the spans are stored in a specific order. Worth noting though, is if by <code>parentTrace</code>, you mean the <strong>root span</strong> of the trace (the first span), then you can search for spans where <code>refs</code> is <code>null</code> because a root span has no parent. Another way to identify a root span is if the <code>trace_id</code> == <code>span_id</code>.</li>
<li><code>trace_id</code> is stored as a binary blob. What you see from cassandra client is an array of 16 bytes with each octet element represented as two hexadecimal values. To convert it to the hex string you see in cqlsh, you'll need to convert the entire array to a single hex string. See the following python example that does this:</li>
</ol>
<pre><code>from cassandra.cluster import Cluster

cluster = Cluster(['127.0.0.1'])
session = cluster.connect()
rows = session.execute(&quot;select * from jaeger_v1_test.traces&quot;)
trace = rows[0]
hexstr = ''.join('{:02x}'.format(x) for x in trace.trace_id)
print(&quot;hex=%s, byte_arr=%s, len(byte_arr)=%d&quot; % (hexstr, trace.trace_id, len(trace.trace_id)))
cluster.shutdown()
</code></pre>
"
Jaeger,62563034,62560345,1,"2020/06/24, 22:52:11",True,"2020/06/24, 23:07:58",3381,4108803,1,"<p>You could try with <code>StartSpanFromContext</code>, inside your gRPC handlers:</p>
<pre><code>    // import &quot;github.com/opentracing/opentracing-go&quot;
    
    span, _ := opentracing.StartSpanFromContext(ctx, &quot;some_child_span&quot;)
    defer span.Finish()

    span.SetTag(&quot;foo&quot;, &quot;bar&quot;)
</code></pre>
<p>As the documentation of <code>otgrpc.OpenTracingServerInterceptor</code> says:</p>
<blockquote>
<p>[...] the server Span will be embedded in the context.Context for the
application-specific gRPC handler(s) to access.</p>
</blockquote>
<p>If we look at the function implementation:</p>
<pre><code>// import &quot;github.com/opentracing/opentracing-go&quot;

func OpenTracingServerInterceptor(tracer opentracing.Tracer, optFuncs ...Option) grpc.UnaryServerInterceptor {
        // ... a lot omitted
        ctx = opentracing.ContextWithSpan(ctx, serverSpan)
        // ...
        resp, err = handler(ctx, req) // your gRPC handler
        // ...   
}
</code></pre>
<p><br><br>
<strong>Edit</strong>: Given the above, you probably can omit this code:</p>
<pre><code>var span = tracer.StartSpan(&quot;Test Span&quot;)
span.SetTag(&quot;one&quot;, &quot;value&quot;)
span.Finish()
</code></pre>
"
Jaeger,61468536,60753860,0,"2020/04/28, 00:11:21",True,"2020/04/28, 00:11:21",184,8575474,0,"<p>The issue was related to <code>yaml</code> file parsing</p>
"
Jaeger,57877033,57588551,0,"2019/09/10, 22:21:23",False,"2019/09/10, 22:21:23",1040,1688998,0,"<p>Jaeger has a UI to look at your data, but no tools to create statistics. However all your data is being stored in a DB of your choice. Storing it in e.g. Elasticsearch gives you a powerful query language to look at the data as well as many other tools that integrate with it.</p>
"
Jaeger,56349205,56135086,5,"2019/05/28, 22:52:10",False,"2019/05/28, 22:52:10",838,10587554,0,"<p>Correct me, if I'm wrong. If you mean how to find the trace-id on the server side, you can try to access the OpenTracing span by <a href=""https://github.com/opentracing-contrib/python-grpc#integrating-with-other-spans"" rel=""nofollow noreferrer"">get_active_span</a>. The trace-id, I suppose, should be one of the tags in it.</p>
"
Jaeger,56584198,56135086,0,"2019/06/13, 18:46:22",True,"2019/06/13, 18:46:22",5906,474819,1,"<p>I had missed a key piece of documentation. In order to get a trace ID, you have to create a span on the client side. This span will have the trace ID that can be used to examine data in the Jaeger UI. The span has to be added into the GRPC messages via an <code>ActiveSpanSource</code> instance.</p>

<pre><code># opentracing-related imports
from grpc_opentracing import open_tracing_client_interceptor, ActiveSpanSource
from grpc_opentracing.grpcext import intercept_channel
from jaeger_client import Config

# dummy class to hold span data for passing into GRPC channel
class FixedActiveSpanSource(ActiveSpanSource):

    def __init__(self):
        self.active_span = None

    def get_active_span(self):
        return self.active_span

config = Config(
    config={
        'sampler': {
            'type': 'const',
            'param': 1,
        },
        'logging': True,
    },
    service_name='foo')

tracer = config.initialize_tracer()

# ...
# In the method where GRPC requests are sent
# ...
active_span_source = FixedActiveSpanSource()
tracer_interceptor = open_tracing_client_interceptor(
    tracer,
    log_payloads=True,
    active_span_source=active_span_source)

with tracer.start_span('span-foo') as span:
    print(f""Created span: trace_id:{span.trace_id:x}, span_id:{span.span_id:x}, parent_id:{span.parent_id}, flags:{span.flags:x}"")
    # provide the span to the GRPC interceptor here
    active_span_source.active_span = span
    with grpc.insecure_channel(...) as channel:
        channel = intercept_channel(channel, tracer_interceptor)
</code></pre>

<p>Of course, you could switch the ordering of the <code>with</code> statements so that the span is created after the GRPC channel. That part doesn't make any difference.</p>
"
Jaeger,53131562,53131007,1,"2018/11/03, 15:00:17",True,"2018/11/03, 15:00:17",17024,858913,0,"<p>You assumption is correct, the elements are there, but not exactly where you think they are.</p>

<p>To easily check if an element is part of the response html and not being loaded by javascript I normally recommend using a <a href=""https://chrome.google.com/webstore/detail/toggle-javascript/cidlcjdalomndpeagkjpnefhljffbnlo?hl=en"" rel=""nofollow noreferrer"">browser plugin to disable javascript</a>.</p>

<p>If you want the images, they are still part of the html response, you can get them with:</p>

<pre><code>response.css('li.product-images__item')
</code></pre>

<p>the main image appears separately:</p>

<pre><code>response.css('meta[itemprop=image]::attr(content)')
</code></pre>

<p>Hope that helps you.</p>
"
Jaeger,48593245,48592875,1,"2018/02/03, 04:04:40",False,"2018/02/03, 04:14:38",31,7234392,1,"<p>You haven't initialized the variables for the next few iterations.</p>

<p>You need to reinitialize the variables used for while loop's condition check outside their respective while loops. i.e </p>

<pre><code>b = 0;
while(b &lt; exponents.length){
}
</code></pre>

<p>Similarly do it for the while loops which use variables <code>c</code> &amp; <code>d</code>.</p>
"
Jaeger,48594819,48592875,0,"2018/02/03, 08:59:49",False,"2018/02/03, 08:59:49",15431,3992939,0,"<p><a href=""https://stackoverflow.com/a/48593245/3992939"">Daniel's answer is correct</a> : the 
structure of the <code>while</code> loop should be: </p>

<pre><code>    int a=0, b=0, c=0, d=0;
    while (a &lt; length) {

        b=0;
        while (b &lt; length) {
            c=0;
            while (c &lt; length) {
                d=0;
                while (d &lt; length) {
                    d++;
                }
                c++;
            }
            b++;
        }
        a++;
    }
</code></pre>
"
Jaeger,59507206,58947001,1,"2019/12/28, 02:23:31",True,"2019/12/28, 02:23:31",721,1563297,1,"<p>It looks like you have different versions of opentracing. The spring-starter-jaeger version 2.x upgrade the version of opentracing, so you might have introduced this breaking changes when you upgraded the dependency version.</p>
"
Jaeger,51792533,51720184,0,"2018/08/10, 21:53:47",False,"2018/08/10, 22:38:32",373,10206966,3,"<p>In server 2 , Install jaeger</p>

<pre><code>$ docker run -d --name jaeger \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 9411:9411 \
  jaegertracing/all-in-one:latest
</code></pre>

<p>In server 1, set these environment variables.</p>

<pre><code>JAEGER_SAMPLER_TYPE=probabilistic 
JAEGER_SAMPLER_PARAM=1 
JAEGER_SAMPLER_MANAGER_HOST_PORT=(EnterServer2HostName):5778 
JAEGER_REPORTER_LOG_SPANS=false 
JAEGER_AGENT_HOST=(EnterServer2HostName)
JAEGER_AGENT_PORT=6831 
JAEGER_REPORTER_FLUSH_INTERVAL=1000 
JAEGER_REPORTER_MAX_QUEUE_SIZE=100 
application-server-id=server-x
</code></pre>

<p>Change the tracer registration application code as below in server 1, so that it will get the configurations from the environment variables.</p>

<pre><code>@Produces
@Singleton
public static io.opentracing.Tracer jaegerTracer() {
String serverInstanceId = System.getProperty(""application-server-id"");
if(serverInstanceId == null) {
serverInstanceId = System.getenv(""application-server-id"");
}
return new Configuration(""ApplicationName"" + (serverInstanceId!=null &amp;&amp; !serverInstanceId.isEmpty() ? ""-""+serverInstanceId : """"), 
                Configuration.SamplerConfiguration.fromEnv(),
                Configuration.ReporterConfiguration.fromEnv())
                .getTracer();
}
</code></pre>

<p>Hope this works! </p>

<p>Check this link for integrating elasticsearch as the persistence storage backend so that the traces will not remove once the Jaeger instance is stopped.
<a href=""https://stackoverflow.com/questions/51785812/how-to-configure-jaeger-with-elasticsearch"">How to configure Jaeger with elasticsearch?</a></p>
"
Jaeger,64970836,64938228,0,"2020/11/23, 16:58:26",True,"2020/11/23, 16:58:26",328,1221718,1,"<p>OK, I figured out the issue here which may be obvious to those with more expertise. <a href=""https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/grpc#grpc"" rel=""nofollow noreferrer"">The guide I linked to above that describes how to make an Ingress spec for gRPC</a> is specific to NGINX. Meanwhile, I am using K3S which came out of the box with Traefik as the Ingress Controller. Therefore, the annotations I used in my Ingress spec had no affect:</p>
<pre><code>metadata:
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;GRPC&quot;
</code></pre>
<p>So I found <a href=""https://stackoverflow.com/questions/54562993/traefik-as-an-ingress-controller-server-grpc-unknown-no-status-received"">another Stack Overflow post discussing Traefik and gRPC</a> and modified my original Ingress spec above a bit to include the annotations mentioned there:</p>
<pre><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: simple-prod-collector
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: traefik
    ingress.kubernetes.io/protocol: h2c
    traefik.protocol: h2c
spec:
  rules:
  - host: jaeger-collector.my-container-dev
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: simple-prod-collector
            port:
              number: 14250
</code></pre>
<p>These are the changes I made:</p>
<ol>
<li>Changed the <code>metadata/annotations</code> (this was the actual change needed I am sure)</li>
<li>I also updated the spec version to use <code>networking.k8s.io/v1</code> instead of <code>networking.k8s.io/v1beta1</code> so there are some structural changes due to that but none of the actual content changed AFAIK.</li>
</ol>
<p>Hopefully this helps someone else running into this same confusion.</p>
"
Jaeger,59047238,59044026,1,"2019/11/26, 11:05:04",False,"2019/11/26, 11:05:04",13313,524946,1,"<p>You can bind it to metrics and logging frameworks, but you don't have to. You can simply just call <code>cfg.NewTracer()</code>, like in this example:</p>

<pre class=""lang-golang prettyprint-override""><code>func ExampleFromEnv() {
    cfg, err := jaegercfg.FromEnv()
    if err != nil {
        // parsing errors might happen here, such as when we get a string where we expect a number
        log.Printf(""Could not parse Jaeger env vars: %s"", err.Error())
        return
    }

    tracer, closer, err := cfg.NewTracer()
    if err != nil {
        log.Printf(""Could not initialize jaeger tracer: %s"", err.Error())
        return
    }
    defer closer.Close()

    opentracing.SetGlobalTracer(tracer)
    // continue main()
}
</code></pre>

<p>Source: <a href=""https://github.com/jaegertracing/jaeger-client-go/blob/3585cc566102e0ea2225177423e3fcc3d2e5fd7a/config/example_test.go#L88-L105"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-go/blob/3585cc566102e0ea2225177423e3fcc3d2e5fd7a/config/example_test.go#L88-L105</a></p>

<p>Check the Jaeger Go Client readme for more information on the metrics/logging integration: <a href=""https://github.com/jaegertracing/jaeger-client-go"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-go</a></p>
"
Jaeger,59260094,59044026,0,"2019/12/10, 06:10:40",True,"2019/12/10, 06:10:40",471,2673284,2,"<p>Jaeger clients are designed to have a minimum set of dependencies. We don't know if your application is using Prometheus metrics or Zap logger. This is why <code>jaeger-client-go</code> (as well as many other Jaeger clients in other languages) provide two lightweight interfaces for a Logger and MetricsFactory that can be implemented for a specific logs/metrics backend that your application is using. Of course, the bindings for Prometheus and Zap are already implemented in the <code>jaeger-lib</code> and can be included optionally.</p>
"
Jaeger,55247113,55239593,1,"2019/03/19, 19:47:45",False,"2019/03/19, 19:47:45",76,1496147,1,"<p>Could you try using a more recent version of Jaeger: <a href=""https://www.jaegertracing.io/docs/latest/getting-started/#all-in-one"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/latest/getting-started/#all-in-one</a> - actually 1.11 is now out, so could try that.</p>
"
Jaeger,55255528,55239593,0,"2019/03/20, 09:29:23",True,"2019/03/20, 09:29:23",48481,2504224,3,"<p>The problem is that you are using <code>RestTemplate template = new RestTemplate();</code> to get an instance of the <code>RestTemplate</code> to make a REST call.</p>

<p>Doing that means that Opentracing cannot instrument the call to add necessary HTTP headers.</p>

<p>Please consider using <code>@Autowired RestTemplate restTemplate</code></p>
"
Jaeger,62966022,62948787,1,"2020/07/18, 10:50:12",False,"2020/07/18, 10:50:12",669,9291851,1,"<p>I was studying Opentracing and Jeager and I've used this tutorial to get familiar with the basic possibilities:
<a href=""https://github.com/yurishkuro/opentracing-tutorial/tree/master/java"" rel=""nofollow noreferrer"">https://github.com/yurishkuro/opentracing-tutorial/tree/master/java</a></p>
<p>If you take a look in the case 1 (Hello World), it explains how to &quot;<a href=""https://github.com/yurishkuro/opentracing-tutorial/tree/master/java/src/main/java/lesson01#annotate-the-trace-with-tags-and-logs"" rel=""nofollow noreferrer"">Annotate the Trace with Tags and Logs</a>&quot;.
That would answer your questions 1, 2 and 3, as with that you can add all the info that you would like within spans and logs.</p>
<p>Here is a snippet from the repository (but I'd recommend checking there, as it has a more detailed explanation):</p>
<pre><code>Span span = tracer.buildSpan(&quot;say-hello&quot;).start();
span.setTag(&quot;hello-to&quot;, helloTo);
</code></pre>
<p>In this case <code>helloTo</code> is a variable containing a name, to whom the app will say hello. It would create a span tag called hello-to with the value that is coming from the execution.</p>
<p>Below we have an example for the logs case, where the whole <code>helloStr</code> message is added to the logs:</p>
<pre><code>// this goes inside the sayHello method
String helloStr = String.format(&quot;Hello, %s!&quot;, helloTo);
span.log(ImmutableMap.of(&quot;event&quot;, &quot;string-format&quot;, &quot;value&quot;, helloStr));

System.out.println(helloStr);
span.log(ImmutableMap.of(&quot;event&quot;, &quot;println&quot;));
</code></pre>
<p>Regarding the last question, that would be easier, you can use the Jaeger UI to search for the trace you would like, there is a field for that on the top left corner:</p>
<p><a href=""https://i.stack.imgur.com/fGxKh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fGxKh.png"" alt=""enter image description here"" /></a></p>
"
Jaeger,63082657,62948787,0,"2020/07/25, 02:23:11",False,"2020/07/25, 02:23:11",176,1446358,1,"<p>There you go.</p>
<ol>
<li>I want to add application logs to the span so that they are visible in JaegerUI.</li>
</ol>
<pre><code>Span span = tracer.buildSpan(&quot;my-span-name&quot;).start();
span.setTag(&quot;my-tag-name&quot;, &quot;my-tag-value&quot;);
</code></pre>
<p>There are various overloaded methods as follows</p>
<pre><code>Span setTag(String key, String value);
Span setTag(String key, boolean value);
Span setTag(String key, Number value);
</code></pre>
<ol start=""2"">
<li><p>I want to add some fields to span tags so that it's easy to search in JaegerUI.</p>
<p>Jaeger API provides <strong>log</strong> method to log multiple fields that needs to be added to a map, the method signature is as follows,</p>
</li>
</ol>
<p><code>Span log(Map&lt;String, ?&gt; fields);</code></p>
<p>eg:</p>
<pre><code>span.log(
   ImmutableMap.Builder&lt;String, Object&gt;()
     .put(&quot;event&quot;, &quot;soft error&quot;)
     .put(&quot;type&quot;, &quot;cache timeout&quot;)
     .put(&quot;waited.millis&quot;, 1500)
     .build()
); 
</code></pre>
<ol start=""3"">
<li>Also, I want the spanId and traceId to the application log.</li>
</ol>
<p>spanId and traceId are stored in JaegerSpanContext class, which can be obtained from context() method of Span class.</p>
<pre><code>    JaegerSpanContext spanContext = (JaegerSpanContext)sprintSpan.context();
    long spanId = spanContext.getSpanId();
    long traceId = spanContext.getTraceId();
</code></pre>
<ol start=""4"">
<li>Is it possible to search in JaegerUI based on spanId/traceId? If yes, how?</li>
</ol>
<p>There is a search box in the navigation bar of Jaeger UI where you can search traces by trace ID.</p>
<p><a href=""https://i.stack.imgur.com/mrsPB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mrsPB.png"" alt=""enter image description here"" /></a></p>
"
Jaeger,59260378,58134618,0,"2019/12/10, 06:43:54",True,"2019/12/10, 06:43:54",471,2673284,1,"<p>If you are already using Istio in the deployment, then enabling tracing in it will provide more complete picture of request processing, such as accounting for the time spent in the network between the proxies. </p>

<p>You also don't need to have full tracing instrumentation in your services as long as they pass through certain headers, then Istio can still provide a pretty accurate picture of the traces (but you cannot capture any business specific data in the traces).</p>

<p>Traces generated by Istio will have standardized span names that you can use to reason about the SLAs across the whole infrastructure, whereas explicit tracing instrumentation inside the services can often use different naming schemes, especially when services are written in different languages and using different frameworks.</p>

<p>For the best of both worlds, I would recommend adding instrumentation inside the services for full fidelity, and also enabling tracing in Istio to capture full picture of request execution (and all network latencies).</p>
"
Jaeger,66584227,66527792,0,"2021/03/11, 16:09:56",True,"2021/03/11, 16:09:56",2573,2519395,2,"<p>The solution for this one is to simply increase the memory size in the <code>istio-config.yaml</code> file.<br>
in my case, I'm updating the PVC and it looks like it's already filled with data and decreasing it wasn't an option for istio, so I increased it in the config file instead:<br></p>
<pre><code>tracing:
  jaeger:
    hub: docker.io/jaegertracing
    memory:
      max_traces: 100000
    tag: &quot;1.16&quot;
    persist: true
    spanStorageType: badger
    storageClassName: &quot;gp2&quot;
    accessMode: ReadWriteOnce
  nodeSelector: {}
  opencensus:
    exporters:
      stackdriver:
        enable_tracing: true
    hub: docker.io/omnition
    resources:
      limits:
        cpu: &quot;1&quot;
        memory: 15Gi # I increased this one
      requests:
        cpu: 200m
        memory: 15Gi # and this one
</code></pre>
"
Jaeger,64039073,64036098,0,"2020/09/24, 06:22:45",True,"2020/09/24, 06:22:45",328,1221718,1,"<p>After digging around in the OpenTracing C# .NET Core source (<a href=""https://github.com/opentracing-contrib/csharp-netcore"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/csharp-netcore</a>) I figured out how to override the top level Span.OperationName.</p>
<p>I had to update my <code>Startup.ConfigureServices()</code> call to <code>services.AddOpenTracing()</code> to the following:</p>
<pre><code>services.AddOpenTracingCoreServices(otBuilder =&gt;
{
    otBuilder.AddAspNetCore()
        .ConfigureAspNetCore(options =&gt;
        {
            options.Hosting.OperationNameResolver = (httpContext) =&gt;
            {
                return $&quot;MySuperCoolOperationName ({httpContext.Request.Path.Value})&quot;;
            };
        })
        .AddCoreFx()
        .AddEntityFrameworkCore()
        .AddLoggerProvider();
});
</code></pre>
"
Jaeger,59380813,59303550,0,"2019/12/17, 21:23:20",False,"2019/12/17, 21:23:20",471,2673284,3,"<p>Jaeger clients implement so-called <strong>head-based sampling</strong>, where a sampling decision is made at the root of the call tree and propagated down the tree along with the trace context. This is done to guarantee consistent sampling of all spans of a given trace (or none of them), because we don't want to make the coin flip at every node and end up with partial/broken traces. Implementing on-error sampling in the head-based sampling system is not really possible. Imaging that your service is calling service A, which returns successfully, and then service B, which returns an error. Let's assume the root of the trace was not sampled (because otherwise you'd catch the error normally). That means by the time you know of an error from B, the whole sub-tree at A has been already executed and all spans discarded because of the earlier decision not to sample. The sub-tree at B has also finished executing. The only thing you can sample at this point is the spans in the current service. You could also implement a reverse propagation of the sampling decision via response to your caller. So in the best case you could end up with a sub-branch of the whole trace sampled, and possible future branches if the trace continues from above (e.g. via retries). But you can never capture the full trace, and sometimes the reason B failed was because A (successfully) returned some data that caused the error later.</p>

<p>Note that reverse propagation is not supported by the OpenTracing or OpenTelemetry today, but it has been discussed in the last meetings of the W3C Trace Context working group.</p>

<p>The alternative way to implement sampling is with <strong>tail-based sampling</strong>, a technique employed by some of the commercial vendors today, such as Lightstep, DataDog. It is also on the roadmap for Jaeger (we're working on it right now at Uber). With tail-based sampling 100% of spans are captured from the application, but only stored in memory in a collection tier, until the full trace is gathered and a sampling decision is made. The decision making code has a lot more information now, including errors, unusual latencies, etc. If we decide to sample the trace, only then it goes to disk storage, otherwise we evict it from memory, so that we only need to keep spans in memory for a few seconds on average. Tail-based sampling imposes heavier performance penalty on the traced applications because 100% of traffic needs to be profiled by tracing instrumentation.</p>

<p>You can read more about head-based and tail-based sampling either in Chapter 3 of my book (<a href=""https://www.shkuro.com/books/2019-mastering-distributed-tracing/"" rel=""nofollow noreferrer"">https://www.shkuro.com/books/2019-mastering-distributed-tracing/</a>) or in the awesome paper <em>""So, you want to trace your distributed system? Key design insights from years of practical experience""</em> by Raja R. Sambasivan, Rodrigo Fonseca, Ilari Shafer, Gregory R. Ganger (<a href=""http://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf"" rel=""nofollow noreferrer"">http://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf</a>).</p>
"
Jaeger,53949856,53914858,5,"2018/12/27, 21:21:46",True,"2019/01/01, 03:38:13",6500,9521610,1,"<p>When you are accessing the service from the pod in the <strong>same namespace</strong> you can use just the service name. 
Example:</p>

<pre><code>http://elasticsearch:9200
</code></pre>

<p>If you are accessing the service from the pod in the <strong>different namespace</strong> you should also specify the namespace.
Example: </p>

<pre><code>http://elasticsearch.mynamespace:9200
http://elasticsearch.mynamespace.svc.cluster.local:9200
</code></pre>

<p>To check in what namespace the service is located, use the following command:</p>

<pre><code>kubectl get svc --all-namespaces -o wide
</code></pre>

<p><strong>Note</strong>: Changing ConfigMap does not apply it to deployment instantly. Usually, you need to restart all pods in the deployment to apply new ConfigMap values. There is no rolling-restart functionality at the moment, but you can use the following command as a workaround:<br>
<em>(replace deployment name and pod name with the real ones)</em></p>

<pre><code>kubectl patch deployment mydeployment -p '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""my-pod-name"",""env"":[{""name"":""START_TIME"",""value"":""'$(date +%s)'""}]}]}}}}'
</code></pre>
"
Jaeger,61677933,61565012,0,"2020/05/08, 14:18:10",False,"2020/05/08, 14:18:10",167,11335868,0,"<p>The problem is that your Jaeger collector is not accessible from outside docker network host as you specified in your docker command. This would only work if your spring boot application is deployed on the host network too.
Try to run Jaeger as follows:</p>

<pre><code>docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.17
</code></pre>

<p>It should trigger.</p>
"
Jaeger,58058871,57870219,0,"2019/09/23, 11:55:31",False,"2019/09/23, 11:55:31",13313,524946,0,"<p>You most likely have a mismatch with your OpenTracing libraries. It looks like your Servlet Filter integration (io.opentracing.contrib.web.servlet.filter.TracingFilter) is making use of a method that doesn't exist.</p>
"
Jaeger,61171646,61163242,0,"2020/04/12, 15:42:14",False,"2020/04/14, 21:51:42",79,9827874,0,"<p>Have you tried looking at the logs being generated by your pods?</p>

<p>In my case I got the following</p>

<blockquote>
  <p>ERROR Failed to flush spans in reporter: error sending spans over UDP:
  Error: getaddrinfo ENOTFOUND <a href=""http://jaeger-agent"" rel=""nofollow noreferrer"">http://jaeger-agent</a>, packet size: 984,
  bytes sent: undefined</p>
</blockquote>

<p>Changing it to jaeger-agent worked for me.</p>

<p>Also if it helps I have declared this under my jaeger image in docker-compose.yml:</p>

<pre><code> + ports: - ""5775:5775/udp"" - ""6831:6831/udp"" - ""6832:6832/udp"" - ""5778:5778"" - ""16686:16686"" - ""14268:14268"" - ""9411:9411""`
</code></pre>
"
Jaeger,58440891,58406367,0,"2019/10/18, 00:21:43",True,"2019/10/18, 00:21:43",105,8754016,1,"<p>This was due to the helidon dependency.</p>

<p><a href=""https://helidon.io/docs/latest/#/guides/03_quickstart-mp"" rel=""nofollow noreferrer"">https://helidon.io/docs/latest/#/guides/03_quickstart-mp</a></p>

<p>Also I had to upgrade <code>opentracing-api</code> version to <code>0.33.0</code></p>
"
Jaeger,49311159,49179249,0,"2018/03/16, 01:55:29",True,"2018/03/16, 01:55:29",1116,1953109,2,"<p>I finally figured this out after trying out different combinations. This is happening because Jaeger agent is not receiving any UDP packets from my application. </p>

<p>You need to tell the tracer where to send UDP packets, which in this case is <code>docker-machine ip</code> 
I added: </p>

<pre><code>'agentHost': '192.168.99.100',
'agentPort': 6832
</code></pre>

<p>and then I was able to see my services in Jaeger UI. </p>

<pre><code>function initTracer(serviceName, options) {
  var config = {
    'serviceName': serviceName,
    'sampler': {
      'type': 'const',
      'param': 1
    },
    'reporter': {
      'logSpans': true,
      'agentHost': '192.168.99.100',
      'agentPort': 6832
    }
  }
  var options = {
    'logger': {
      'info': function logInfo(msg) {
        console.log('INFO ', msg)
      },
      'error': function logError(msg) {
        console.log('ERROR', msg)
      }
    }
  }

  const tracer = initJaegerTracer(config, options)

  //hook up nodejs process exit event
  process.on('exit', () =&gt; {
    console.log('flush out remaining span')
    tracer.close()
  })
  //handle ctrl+c
  process.on('SIGINT', () =&gt; {
    process.exit()
  })

  return tracer
}

exports.initTracer = initTracer
</code></pre>
"
Jaeger,48432561,48095718,0,"2018/01/25, 00:12:09",False,"2018/01/25, 00:12:09",471,2673284,1,"<p>Service graph data must be generated in Jaeger. Currently it's possible with via a Spark job here: <a href=""https://github.com/jaegertracing/spark-dependencies"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/spark-dependencies</a></p>
"
Jaeger,36821033,36819104,1,"2016/04/24, 11:48:50",True,"2016/04/24, 12:14:06",2648,6230407,0,"<p>It may not be possible to input text with conditional formatting, but you can change the font color. A solution could be to put the word ""LATE in the specified cell(s) beforehand and set the font-color equal to the background-color, which makes the word invisable. When the condition (formula) evaluates true, the new format will change the font-color and the word LATE appears. No VBA requiered.</p>

<p>On the other hand: wouldn't a simple if-formula be better? Something like:</p>

<pre><code>=if(amountpaid?,""PAID"",if(late?, ""LATE"", """"))
</code></pre>

<p>If you wish you can then change the background with conditional formating</p>
"
Jaeger,60859448,60846142,1,"2020/03/26, 02:44:07",False,"2020/03/26, 02:44:07",50169,594589,0,"<p>There's really not many options for other than starting a span in each function you'd like to instrument:</p>

<pre><code>func something(ctx context.Context) {
  ctx, span := trace.StartSpan(ctx, ""something"")
  defer span.End()
}
</code></pre>

<p>If your functions have a common call signature, or you can coalesce your function into a common call signature, you can write a wrapper.  Examples of this can be seen in http <a href=""https://www.alexedwards.net/blog/making-and-using-middleware"" rel=""nofollow noreferrer"">""middleware""</a>.</p>

<p>Consider the http.Handler, you could write a <a href=""https://www.google.com/search?q=dm03514%20decorators&amp;oq=dm03514%20decorators&amp;aqs=chrome..69i57.1985j0j4&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow noreferrer"">decorator</a> for your functions that handles the span lifecycle:</p>

<pre><code>func WithTraced(handler http.Handler, opName string) http.Handler {
    return func(w http.ResponseWriter, r *http.Request) {
        ctx, span := trace.StartSpan(ctx, opName)
        defer span.End()
        handler.ServeHTTP(w, r.WithContext(ctx))

    }

}
</code></pre>

<p>A similar pattern could be applied by <a href=""https://golang.org/doc/effective_go.html#embedding"" rel=""nofollow noreferrer"">embedding</a> structs.</p>
"
Jaeger,58197357,58148102,0,"2019/10/02, 10:55:46",True,"2019/10/02, 10:55:46",109,318536,2,"<blockquote>
  <p>Is it possible to set these when working on OpenShift?</p>
</blockquote>

<p>Yes, you can configure it for the Che master of your installation.</p>

<blockquote>
  <p>OpenShift is the Saas Che offering </p>
</blockquote>

<p>As a user of che.openshift.io you can't leverage from tracing capabilities of Che at this moment.</p>
"
Jaeger,57297860,57292517,0,"2019/07/31, 22:41:03",False,"2019/07/31, 22:41:03",48155,2989261,2,"<p>This 'appears' to be related to the switch from AWS CNI to weave. CNI uses the IP range of your VPC while weave uses its own address range (for pods), so there may be remaining iptables rules from AWS CNI, for example. </p>

<blockquote>
  <p>Internal error occurred: failed calling admission webhook ""pilot.validation.istio.io"": Post <a href=""https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s"" rel=""nofollow noreferrer"">https://istio-galley.istio-system.svc:443/admitpilot?timeout=30s</a>: Address is not allowed</p>
</blockquote>

<p>The message above implies that whatever address <code>istio-galley.istio-system.svc</code> resolves to, internally in your K8s cluster, is not a valid IP address. So I would also try to see what that resolves to. (It may be related to <a href=""https://coredns.io/"" rel=""nofollow noreferrer"">coreDNS</a>).</p>

<p>You can also try the following <a href=""https://github.com/weaveworks/weave/issues/3335#issuecomment-441522517"" rel=""nofollow noreferrer"">these steps</a>;</p>

<p>Basically, (quoted)</p>

<ul>
<li>kubectl delete ds aws-node -n kube-system</li>
<li>delete /etc/cni/net.d/10-aws.conflist on each of the node</li>
<li>edit instance security group to allow UDP, TCP on 6873, 6874 ports</li>
<li>flush iptables, nat, mangle, filter</li>
<li>restart kube-proxy pods</li>
<li>apply weave-net daemonset</li>
<li>delete existing pods so the get recreated in Weave pod CIDR's address-space.</li>
</ul>

<p>Furthermore, you can try reinstalling everything from the beginning using weave.</p>

<p>Hope it helps!</p>
"
Jaeger,54199894,54084447,2,"2019/01/15, 15:32:19",True,"2019/01/15, 15:32:19",294930,208809,2,"<p>OpenTracing is a framework for Distributed Tracing. As such, it is more about performance monitoring and observability than logging (what NLog is about). </p>

<p>OpenTracing allows you to manually instrument your code to generate traces with relevant spans containing information about code execution in your app. This includes annotating spans with errors and arbitrary keys and values, which you <em>could</em> use instead of logging. However, that's not the same as dedicated structured logging. </p>
"
Jaeger,54028704,54021635,1,"2019/01/03, 21:35:20",False,"2019/01/03, 21:35:20",113,9236736,1,"<p>Here's an article/guide on how to work with the limit-ranger and its default values [1]</p>

<p>[1]<a href=""https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b"" rel=""nofollow noreferrer"">https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b</a></p>
"
Jaeger,54200196,53428114,0,"2019/01/15, 15:48:26",True,"2019/01/15, 15:48:26",294930,208809,1,"<p>The <code>span.kind=server</code> tag denotes an entry span, e.g. a span created in the local code in response to an external request. Likewise, <code>span.kind=client</code> denotes an exit span, e.g. a call made from the local code to another server.</p>

<p>In your example, the span generated for Foo is a <code>span.kind=server</code> and the span recording the call to Buzz is a <code>span.kind=client</code>.  </p>
"
Jaeger,51745456,51723902,2,"2018/08/08, 14:22:23",False,"2018/08/08, 14:22:23",4058,9928809,1,"<p>Kubernetes provides quite a big variety of Networking and Load Balancing features from the box. However, the idea to simplify and extend the functionality  of <a href=""https://istio.io/docs/setup/kubernetes/sidecar-injection/#automatic-sidecar-injection"" rel=""nofollow noreferrer"">Istio sidecars</a> is a good choice as they are used for automatic injection into the Pods in order to proxy the traffic between internal Kubernetes services.</p>

<p>You can implement <code>sidecars</code> manually or automatically. If you choose the manual way, make sure to add the appropriate parameter under Pod's annotation field:</p>

<pre><code>annotations:
        sidecar.istio.io/inject: ""true""
</code></pre>

<p>Automatic <code>sidecar</code> injection requires <a href=""https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/"" rel=""nofollow noreferrer"">Mutating Webhook admission controller</a>, available since Kubernetes version 1.9 released, therefore <code>sidecars</code> can be integrated for Pod's creation process as well.</p>

<p>Get yourself familiar with this <a href=""https://medium.com/@timfpark/more-batteries-included-microservices-made-easier-with-istio-on-kubernetes-87c8b76ac2ef"" rel=""nofollow noreferrer"">Article</a> to shed light on using different monitoring and traffic management tools in Istio.</p>
"
Jaeger,52545681,51723902,0,"2018/09/28, 00:56:57",True,"2018/09/28, 00:56:57",363,3463514,1,"<p>Yes - it is possible to use external services with istio. You can disable grafana and prometheus just by setting proper flags in values.yaml of istio helm chart (grafana.enabled=false, etc).
You can check <a href=""https://github.com/kyma-project"" rel=""nofollow noreferrer"">kyma-project</a> project to see how istio is integrated with prometheus-operator, grafana deployment with custom dashboards, and custom jaeger deployment. From your list only certmanager is missing.</p>
"
Jaeger,62141984,62112814,0,"2020/06/02, 01:29:32",False,"2020/06/02, 01:34:32",113,4573609,0,"<p>so I give it a try and my answer to the question above are:</p>

<ul>
<li><p>Q1) yes it is possible (congrats to the jaeger team, package pretty easy to grasp with a good documentation)</p></li>
<li><p>Q2) I did struggle a bit with this one and thanks to <a href=""https://github.com/CHOMNANP/jaeger-js-text-map-demo"" rel=""nofollow noreferrer"">https://github.com/CHOMNANP/jaeger-js-text-map-demo</a> I implemented a solution by adding a ""textCarrier"" with a ref. to the span context formatted as ""FORMAT_TEXT_MAP"" to the message Component 1 was publishing towards Component 2.</p></li>
</ul>

<p>Code snipper in C1 on the first API invocation</p>

<pre><code>server.post(""/api/vms"", (req, res) =&gt; {
  console.log('Enter /api/vms');
  const span = tracer.startSpan(req.path);
  // Use the log api to capture a log
  span.log({ event: 'request_received' })
  txSpan = span;
  //console.log(""req"",span);
  // Use the setTag api to capture standard span tags for http traces
  span.setTag(opentracing.Tags.HTTP_METHOD, req.method)
  span.setTag(opentracing.Tags.SPAN_KIND, opentracing.Tags.SPAN_KIND_RPC_SERVER)
  span.setTag(opentracing.Tags.HTTP_URL, req.path)
</code></pre>

<p>followed by this part when sending the msg on redis:</p>

<pre><code>   const textCarrier = getTextCarrierBySpanObject(span);
   tracer.inject(span.context(), opentracing.FORMAT_TEXT_MAP, textCarrier)

   var vm = req.body;
   console.log('Creating a new vm: ', vm);
   // Publish a message by specifying a channel name.
   try {
      Object.assign(vm, { textCarrier });
      pub.publish('tasks-queue', JSON.stringify(vm));
   } catch(e) {
        console.log(""App1 Error when publishing task to App2"", e);
   }
</code></pre>

<p>The getTextCarrierBySpanObject function is coming from <a href=""https://github.com/CHOMNANP/jaeger-js-text-map-demo"" rel=""nofollow noreferrer"">https://github.com/CHOMNANP/jaeger-js-text-map-demo</a></p>

<pre><code>function getTextCarrierBySpanObject(_span) {

    const spanContext = _span.context();
    const traceId = spanContext._traceId.toString('hex');
    const spanId = spanContext._spanId.toString('hex');
    let parentSpanId = spanContext._parentId;
    const flag = _.get(spanContext, '_flags', 1);

    if (parentSpanId) {
        parentSpanId = parentSpanId.toString('hex');
    } else {
        parentSpanId = 0;
    }

    const uberTraceId = `${traceId}:${spanId}:${parentSpanId}:${flag}`;
    console.log(""uberTraceId===&gt; "", uberTraceId)

    let textCarrier = {
        ""uber-trace-id"": uberTraceId
    };

    return textCarrier
}
</code></pre>

<p>Code snippet in C2 receiving the msg from redis</p>

<pre><code>sub.on('message', function(channel, message) {
  // message is json string in our case so we are going to parse it.
  try {
    var json = JSON.parse(message)
    console.log(""Task received"", message);

    const tracer = opentracing.globalTracer();
    // Extracting the span context from the message
    var parentSpan = tracer.extract(opentracing.FORMAT_TEXT_MAP, JSON.parse(message).textCarrier);
    console.log(""textCarrier="",JSON.parse(message).textCarrier);
    const span = tracer.startSpan(""/msg"", { childOf: parentSpan });
    // Use the log api to capture a log
    span.log({ event: 'msg_received' })
</code></pre>

<p>I tested with version 1.13</p>

<pre><code>docker run -d --name jaeger   -e COLLECTOR_ZIPKIN_HTTP_PORT=9411   -p 5775:5775/udp   -p 6831:6831/udp   -p 6832:6832/udp   -p 5778:5778   -p 16686:16686   -p 14268:14268   -p 9411:9411   jaegertracing/all-in-one:1.13
</code></pre>

<ul>
<li>Q3) This one is pretty straight forward using FORMAT_HTTP_HEADERS to convey the span from the component C4 invoking a callback on C3 and the from the Component C3 invoking a callback on C1. The only ""problem"" I found was more a ""trace readability issue"" as, in fact, the Spans appear on the Jaeger UI in ""the progagation"" order and not in the ""timing order"" which can be a bit confusing... but the ""trace graph"" experimental feature allowed to actually see the trace in the right order of appearance, so all good.</li>
</ul>

<p>So all in all a pretty convincing prototyping exercise with Jaeger, will most probably pilot it now on a real project before trying it in production.</p>
"
Jaeger,49626058,49624555,0,"2018/04/03, 12:00:37",False,"2018/04/04, 08:31:53",81,2819181,0,"<p>Not jaeger, able to send traces to zipkin server, using zipkin-simple. 
Related code is in repository <a href=""https://github.com/debmalya/calculator"" rel=""nofollow noreferrer"">https://github.com/debmalya/calculator</a></p>

<pre><code>import zipkinSimple from 'zipkin-simple'
const zipkinTracerSimple = new zipkinSimple({
             debug: true,
             host: ""localhost"",
             port: ""9411"",
             path: ""/api/v2/spans"",
             sampling: 1.0,
})

var zipkinSimpleTraceData

zipkinSimpleTraceData= zipkinTracerSimple.getChild(zipkinSimpleTraceData);
    zipkinSimpleTraceData = 
zipkinTracerSimple.sendClientSend(zipkinSimpleTraceData, {
     service: '&lt;service_name&gt;',
     name: ""&lt;span_name&gt;""
   }) 
</code></pre>
"
Jaeger,49637708,49624555,0,"2018/04/03, 22:24:22",False,"2018/04/03, 22:24:22",21,9593079,2,"<p>node-jaeger-client currently doesn't run in the browser. There is ongoing <a href=""https://github.com/jaegertracing/jaeger-client-node/issues/109"" rel=""nofollow noreferrer"">work</a> to make jaeger-client browser friendly. This issue: <a href=""https://stackoverflow.com/questions/37418513/readfilesync-is-not-a-function"">readFileSync is not a function</a> contains relevant information to why you're seeing the error message. Essentially, you're trying to run jaeger-client (a nodejs library) using react-scripts which doesn't contain the modules that jaeger-client needs.</p>
"
Jaeger,58138322,58011545,0,"2019/09/27, 19:26:25",False,"2019/09/27, 19:26:25",1,4734979,0,"<p>Great question and a very popular one too. In short, yes, code changes are required. Not just in one service but in all the services that a request will go through. You need to instrument all services to get continuous traces that will be able to tell you the story of a request as it travels through the system. </p>
"
Jaeger,58561231,58514716,0,"2019/10/25, 18:12:58",False,"2019/10/25, 18:12:58",721,1563297,1,"<p>The prometheus-es-exporter provides a way to create metrics using queries.</p>

<p>For further details you can check <a href=""https://github.com/braedon/prometheus-es-exporter#query-metrics"" rel=""nofollow noreferrer"">prometheus-es-exporter#query-metrics</a></p>
"
Jaeger,59391314,59372759,0,"2019/12/18, 13:44:56",True,"2019/12/18, 13:44:56",425,9112151,0,"<p>Found out how. I just added one single line of code into tracing.py of django_opentracing lib:</p>

<p><a href=""https://i.stack.imgur.com/wRQ9m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wRQ9m.png"" alt=""enter image description here""></a></p>

<p>And the result:</p>

<p><a href=""https://i.stack.imgur.com/LgBnA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LgBnA.png"" alt=""enter image description here""></a></p>
"
Jaeger,55241560,55236000,1,"2019/03/19, 14:53:31",False,"2019/03/19, 14:53:31",9399,502575,0,"<p>You can create a <a href=""https://kubernetes.io/docs/concepts/services-networking/service/#nodeport"" rel=""nofollow noreferrer"">NodePort</a> service using the <code>app: jaeger</code> selector to expose the UI outside the cluster.</p>
"
Jaeger,55487339,55236000,0,"2019/04/03, 07:54:28",False,"2019/04/03, 07:54:28",44552,308174,0,"<p><code>kubectl port-forward</code> command default is expose to <code>localhost</code> network only, try to add <code>--address 0.0.0.0</code></p>

<pre><code>$ kubectl port-forward -n istio-system \
 $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath=’{.items[0].metadata.name}’) \
  --address 0.0.0.0 16686:16686 &amp;
</code></pre>

<p>see <a href=""https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#port-forward"" rel=""nofollow noreferrer"">kubectl command reference</a></p>
"
Jaeger,55490575,55236000,0,"2019/04/03, 11:33:16",False,"2019/04/03, 11:33:16",5244,5564578,1,"<p>There are several ways of doing this. The <code>port-forward</code> works fine on Google Cloud Shell. If you are using GKE, then I strongly recommend using Cloud Shell, and <code>port-forward</code> as it is the easiest way. On other clouds, I don't know.</p>

<p>What is suggesting Stefan would work. You can edit the jaeger service with <code>kubectl edit svc jaeger-query</code>, then change the type of the service from <code>ClusterIP</code> to <code>NodePort</code>. Finally, you can access the service with <code>NODE_IP:PORT</code> (any node). If you do <code>kubectl get svc</code>, you will see the new port assigned to the service.
Note: You might need to open a firewall rule for that port.</p>

<p>You can also make the service type <code>LoadBalancer</code>, if you have a control plane to set up an external IP address. This would be a more expensive solution, but you would have a dedicated external IP address for your service.</p>

<p>There are more ways, but I would say these are the appropriate ones.</p>
"
Jaeger,67077099,67003774,0,"2021/04/13, 17:44:16",False,"2021/04/13, 17:44:16",193,3774803,0,"<p>It doesn't work in golang grpc client. I used openTelemetry <a href=""https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/loadbalancingexporter"" rel=""nofollow noreferrer"">load balancing</a> Another option - use kubernetes to balance requests to backends.</p>
"
Jaeger,44337567,43766832,0,"2017/06/03, 00:18:56",True,"2017/06/03, 00:18:56",471,2673284,0,"<p>There are two issues here. One is that your code sets the port for Jaeger client to 5775. This port expects a different data model than what Node.js client sends, you can remove the <code>agentHost</code> and <code>agentPort</code> parameters and rely on defaults.</p>

<p>The second issue is that you're running the Docker image without exposing the required UDP port. The correct command is shown in the <a href=""http://jaeger.readthedocs.io/en/latest/getting_started/"" rel=""nofollow noreferrer"">documentation</a>, as of today it should be this (one long line):</p>

<pre><code>docker run -d -p5775:5775/udp -p6831:6831/udp -p6832:6832/udp \
    -p5778:5778 -p16686:16686 -p14268:14268 jaegertracing/all-in-one:latest
</code></pre>
"
Jaeger,57039417,57038416,0,"2019/07/15, 15:06:42",False,"2019/07/15, 15:06:42",4887,6700019,0,"<p>It seems that PyInstaller can't resolve <a href=""https://github.com/jaegertracing/jaeger-client-python"" rel=""nofollow noreferrer""><code>jaeger_client</code></a> import. So an easy way is to just edit your spec file and add the whole <code>jaeger_client</code> library as a <a href=""https://pythonhosted.org/PyInstaller/advanced-topics.html#the-toc-and-tree-classes"" rel=""nofollow noreferrer""><code>Tree</code></a> class:</p>

<pre><code># -*- mode: python -*-

block_cipher = None


a = Analysis(['script.py'],
             ...)
a.datas += Tree('&lt;python_path&gt;/Lib/site-packages/jaeger_client', prefix='./jaeger_client')
pyz = PYZ(a.pure, a.zipped_data,
             cipher=block_cipher)
...
</code></pre>

<p>And generate your executable with <code>pyinstaller script.spec</code>.</p>
"
Jaeger,62910785,62830150,0,"2020/07/15, 11:34:09",False,"2020/07/15, 11:34:09",184,8575474,0,"<pre><code>#include &lt;jaegertracing/net/IPAddress.h&gt;
#include &lt;jaegertracing/net/Socket.h&gt;
void check(){
try{
jaegertracing::net::Socket socket;
        socket.open(AF_INET, SOCK_STREAM);
        const std::string serverURL = configuration.sampler().kDefaultSamplingServerURL; 
        socket.connect(serverURL);
}catch(...){}
}
</code></pre>
<p>if it throws error then it is unable to reach host, this method is costly I agree but this is the only viable solution I find</p>
"
Jaeger,63470131,63221267,0,"2020/08/18, 16:51:58",False,"2020/08/18, 16:51:58",213,7933630,0,"<p>I'm not sure if what you're looking for exists today per se, but you could accomplish this with OpenTelemetry by writing traces through the LogReporter then using a serverless function to read the cloudwatch stream and send it to Jaeger (or to an OpenTelemetry Collector that sends them to Jaeger). You could also write a custom plugin for the OpenTelemetry Collector that read a cloudwatch stream into OTLP then exported it to any endpoint supported by the collector.</p>
"
Jaeger,52813470,50118186,1,"2018/10/15, 12:20:43",True,"2018/10/15, 12:20:43",13313,524946,1,"<blockquote>
  <p>But when I wrap the Spring-Boot application inside a Docker container with the following docker-compose file and start the Jaeger client again I can't see any traces</p>
</blockquote>

<p>That's because the Jaeger client will, by default, send the spans via UDP to an agent at <code>localhost</code>. When your application is running in a Docker container, your <code>localhost</code> there is the container itself, so that the spans are lost.</p>

<p>As you are linking the Jaeger container with your application, you may want to get it solved by exporting the env var <code>JAEGER_AGENT_HOST</code> to <code>jaeger</code>.</p>
"
Jaeger,65853125,65202244,0,"2021/01/22, 23:41:07",False,"2021/01/22, 23:41:07",483,4895267,0,"<p>Remove your dependencies and use the following one that will include also the instrumentation you need</p>
<pre><code>&lt;dependency&gt;
 &lt;groupid&gt;io.opentracing.contrib&lt;/groupid&gt;
 &lt;artifactid&gt;opentracing-spring-jaeger-cloud-starter&lt;/artifactid&gt;
 &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
"
Jaeger,63218533,63218451,2,"2020/08/02, 19:38:53",False,"2020/08/02, 19:38:53",721,1563297,0,"<p>You are missing the configuration of Jaeger address. Since you did not provided it, it is trying to connect to the default one, which is TCP protocol, <code>127.0.0.1</code> and port 5778.
Check for details the configuration section <a href=""https://github.com/jaegertracing/jaeger-client-cpp#updating-sampling-server-url"" rel=""nofollow noreferrer"">here</a>.</p>
"
Jaeger,65041185,65040519,0,"2020/11/27, 19:06:58",True,"2020/11/27, 19:06:58",328,1221718,0,"<p>I figured it out... the Jaeger Operator doesn't create a <code>Service</code> exposing the metrics endpoints. These endpoints are just exposed via the pods for the Collector and Query components.</p>
<p>An example from the Collector pod spec:</p>
<pre><code>    ports:
    - containerPort: 9411
      name: zipkin
      protocol: TCP
    - containerPort: 14267
      name: c-tchan-trft
      protocol: TCP
    - containerPort: 14268
      name: c-binary-trft
      protocol: TCP
    - containerPort: 14269
      name: admin-http
      protocol: TCP
    - containerPort: 14250
      name: grpc
      protocol: TCP
</code></pre>
<p>Note the <code>admin-http</code> port there.</p>
<p>So to get the Prometheus Operator to scrape these metrics, I created a <code>PodMonitor</code> which covers both the Collector and Query components because both of them have the <code>labels/app: jaeger</code> and <code>admin-http</code> ports defined:</p>
<pre class=""lang-yaml prettyprint-override""><code>cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: jaeger-components
  namespace: monitoring
  labels:
    release: prometheus
spec:
  podMetricsEndpoints:
  - path: /metrics
    port: admin-http
  namespaceSelector:
    matchNames:
    - monitoring
  selector:
    matchLabels:
        app: jaeger
EOF
</code></pre>
"
Jaeger,63117665,62998835,0,"2020/07/27, 17:38:53",False,"2020/07/27, 17:38:53",213,7933630,1,"<p>There's a lot of different questions here, and some of them don't have answers without more information about your specific setup, but I'll try to give you a good overview.</p>
<p><strong>Why Tracing?</strong></p>
<p>You've already intuited that there are a lot of similarities between &quot;APM&quot; and &quot;tracing&quot; - the differences are fairly minimal. Distributed Tracing is a superset of capabilities marketed as APM (application performance monitoring) and RUM (real user monitoring), as it allows you to capture performance information about the work being done in your services to handle a single, logical request both at a per-service level, and at the level of an entire request (or transaction) from client to DB and back.</p>
<p>Trace data, like other forms of telemetry, can be aggregated and analyzed in different ways - for example, unsampled trace data can be used to generate RED (rate, error, duration) metrics for a given API endpoint or function call. Conventionally, trace data is annotated (tagged) with properties about a request or the underlying infrastructure handling a request (things like a customer identifier, or the host name of the server handling a request, or the DB partition being accessed for a given query) that allows for powerful exploratory queries in a tool like Jaeger or a commercial tracing tool.</p>
<p><strong>Sampling</strong></p>
<p>The overall performance impact of generating traces varies. In general, tracing libraries are designed to be fairly lightweight - although there are a lot of factors that influence this overhead, such as the amount of attributes on a span, the log events attached to it, and the request rate of a service. Companies like Google will aggressively sample due to their scale, but to be honest, sampling is more beneficial to consider from a long-term storage perspective rather than an up-front overhead perspective.</p>
<p>While the additional overhead per-request to create a span and transmit it to your tracing backend might be small, the cost to store trace data over time can quickly become prohibitive. In addition, most traces from most systems aren't terribly interesting. This is why dynamic and tail-based sampling approaches have become more popular. These systems move the sampling decision from an individual service layer to some external process, such as the <a href=""https://github.com/open-telemetry/opentelemetry-collector"" rel=""nofollow noreferrer"">OpenTelemetry Collector</a>, which can analyze an entire trace and determine if it should be sampled in or out based on user-defined criteria. You could, for example, ensure that any trace where an error occurred is sampled in, while 'baseline' traces are sampled at a rate of 1%, in order to preserve important error information while giving you an idea of steady-state performance.</p>
<p><strong>Proprietary APM vs. OSS</strong></p>
<p>One important distinction between something like AppDynamics or New Relic and tools like Jaeger is that Jaeger does not rely on proprietary instrumentation agents in order to generate trace data. Jaeger supports OpenTelemetry, allowing you to use open source tools like the <a href=""https://github.com/open-telemetry/opentelemetry-java-instrumentation"" rel=""nofollow noreferrer"">OpenTelemetry Java Automatic Instrumentation</a> libraries, which will automatically generate spans for many popular Java frameworks and libraries, such as Spring. In addition, since OpenTelemetry is available in multiple languages with a shared data format and trace context format, you can guarantee that your traces will work properly in a polyglot environment (so, if you have Node.JS or Golang services in addition to your Java services, you could use OpenTelemetry for each language, and trace context propagation would work seamlessly between all of them).</p>
<p>Even more advantageous, though, is that your instrumentation is decoupled from a specific vendor or tool. You can instrument your service with OpenTelemetry and then send data to one - or more - analysis tools, both commercial and open source. This frees you from vendor lock-in, and allows you to select the best tool for the job.</p>
<p>If you'd like to learn more about OpenTelemetry, observability, and other topics I wrote a longer series that you can find <a href=""https://dev.to/lightstep/opentelemetry-101-what-is-observability-44m"" rel=""nofollow noreferrer"">here</a> (look for the other 'OpenTelemetry 101' posts).</p>
"
Jaeger,64356244,64342820,0,"2020/10/14, 18:06:36",True,"2020/10/14, 18:06:36",39034,785745,0,"<p>This is the simplest working example that I was able to find.</p>
<pre class=""lang-cs prettyprint-override""><code>using Jaeger;
using Jaeger.Reporters;
using Jaeger.Samplers;
using Jaeger.Senders.Thrift;

namespace jaegertest
{
    class Program
    {
        static void Main(string[] args)
        {
            var tracer = new Tracer.Builder(&quot;my-service&quot;)
                .WithSampler(new ConstSampler(true))
                .WithReporter(new RemoteReporter.Builder()
                    .WithSender(new UdpSender())
                    .Build())
                .Build();

            using (var scope = tracer.BuildSpan(&quot;foo&quot;).StartActive(true))
            {
                System.Threading.Thread.Sleep(1000);
            }

            tracer.Dispose();
        }
    }
}
</code></pre>
<p>Here is a more realistic example that builds the tracer from a configuration.</p>
<pre class=""lang-cs prettyprint-override""><code>using Jaeger;
using Jaeger.Samplers;
using Jaeger.Senders;
using Jaeger.Senders.Thrift;
using Microsoft.Extensions.Logging;

namespace jaegertest
{
    class Program
    {
        static void Main(string[] args)
        {
            var loggerFactory = new LoggerFactory();

            var samplerConfiguration = new Configuration.SamplerConfiguration(loggerFactory)
                .WithType(ConstSampler.Type)
                .WithParam(1);

            var senderResolver = new SenderResolver(loggerFactory)
                .RegisterSenderFactory&lt;ThriftSenderFactory&gt;();

            var senderConfiguration = new Configuration.SenderConfiguration(loggerFactory)
                .WithSenderResolver(senderResolver);

            var reporterConfiguration = new Configuration.ReporterConfiguration(loggerFactory)
                .WithSender(senderConfiguration)
                .WithLogSpans(true);

            var tracer = (Tracer)new Configuration(&quot;my-service&quot;, loggerFactory)
                .WithSampler(samplerConfiguration)
                .WithReporter(reporterConfiguration)
                .GetTracer();

            using (var scope = tracer.BuildSpan(&quot;foo&quot;).StartActive(true))
            {
                System.Threading.Thread.Sleep(1000);
            }

            tracer.Dispose();
        }
    }
}
</code></pre>
"
Jaeger,63198835,63198673,0,"2020/07/31, 23:41:17",True,"2020/07/31, 23:41:17",7166,3838328,0,"<p>You just need to make use of <code>tags.value</code> instead of <code>value</code> in your match query.</p>
<p>Below query should help:</p>
<pre><code>POST &lt;your_index_name&gt;/_search
{
  &quot;from&quot;: 0,
  &quot;size&quot;: 1,
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;must&quot;: [
        {
          &quot;match&quot;: {
            &quot;process.serviceName&quot;: &quot;transaction-manager&quot;
          }
        },
        {
          &quot;nested&quot;: {
            &quot;path&quot;: &quot;tags&quot;,
            &quot;query&quot;: {
              &quot;match&quot;: {
                &quot;tags.value&quot;: &quot;zipkin&quot;        &lt;---- Note this
              }
            }
          }
        }
      ]
    }
  }
}
</code></pre>
"
Jaeger,59790526,59782209,0,"2020/01/17, 17:42:43",False,"2020/01/17, 17:42:43",2105,2461073,0,"<p>Not out of the box, you have to plug a behaviour into NSB that uses open telemetry
<a href=""https://github.com/open-telemetry/opentelemetry-dotnet"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-dotnet</a>
You will have to write custom code.
Plus you can do push metrics as well as shown in our app insights, Prometheus and other samples.</p>

<p>Let's continue the conversation in our support channels?</p>
"
Jaeger,66675598,59782209,0,"2021/03/17, 16:55:15",False,"2021/03/17, 17:02:16",970,5253437,0,"<p>Not sure if you are still looking for a solution for this. You should be able to do this currently using the <code>NServiceBus.Extensions.Diagnostics.OpenTelemetry</code> package from <a href=""https://www.nuget.org/packages/NServiceBus.Extensions.Diagnostics.OpenTelemetry"" rel=""nofollow noreferrer"">nuget</a>. This is built by Jimmy Bogard and instruments NServiceBus with the required support for <code>Open Telemetry</code>.</p>
<p>The source for this is available <a href=""https://github.com/jbogard/NServiceBus.Extensions.Diagnostics.OpenTelemetry"" rel=""nofollow noreferrer"">here</a>. You can connect this to any backend of your choice that supports <code>Open Telemetry</code> including but not limited to <code>Jaeger</code> and <code>Zipkin</code>.</p>
<p>Additionally, here is an <a href=""https://github.com/jbogard/nsb-diagnostics-poc"" rel=""nofollow noreferrer"">example</a> that shows this in action.</p>
"
Jaeger,54325706,53885456,1,"2019/01/23, 13:00:46",False,"2019/01/23, 13:00:46",13313,524946,1,"<blockquote>
  <p>So as per this config, does it mean that I'm sampling each and every trace or just the few traces randomly?</p>
</blockquote>

<p>Using the sampler type as <code>const</code> with <code>1</code> as the value means that you are sampling everything.</p>

<blockquote>
  <p>Mysteriously, when I'm passing random inputs to create spans for my microservices, the spans are getting generated only after 4 to 5 minutes. I would like to understand this configuration spec more but not able to.</p>
</blockquote>

<p>There are several things that might be happening. You might not be closing spans, for instance. I recommend reading the following two blog posts to try to understand what might be happening:</p>

<p><a href=""https://medium.com/jaegertracing/help-something-is-wrong-with-my-jaeger-installation-68874395a7a6"" rel=""nofollow noreferrer"">Help! Something is wrong with my Jaeger installation!</a></p>

<p><a href=""https://medium.com/jaegertracing/the-life-of-a-span-ee508410200b"" rel=""nofollow noreferrer"">The life of a span</a></p>
"
Jaeger,57295501,57278976,0,"2019/07/31, 19:41:53",False,"2019/07/31, 19:41:53",13313,524946,0,"<p>It doesn't really matter in which language your individual microservices are written, you should see them all in the same trace. Given that you are seeing three traces instead of one trace with three spans, it appears that the context propagation isn't working. Check your HTTP client in your nodejs services, they should perform the <a href=""https://opentracing.io/docs/overview/inject-extract/"" rel=""nofollow noreferrer"">""inject"" operation</a>. Your service ""B"" and ""C"" should then perform the ""extract"" operation.</p>

<p>If you haven't yet, check <a href=""https://github.com/yurishkuro/opentracing-tutorial/tree/master/nodejs"" rel=""nofollow noreferrer"">Yuri Shkuro's OpenTracing Tutorial</a>. The lesson 3 is about the context propagation, including the inject and extract operations.</p>

<p>I'm not quite sure how it works in the NodeJS world, but in Java, it should be sufficient to have the <a href=""https://github.com/opentracing-contrib/java-web-servlet-filter"" rel=""nofollow noreferrer"">opentracing-contrib/java-web-servlet-filter</a> instrumentation library in your classpath, as it would register the necessary pieces in the right hooks and make the trace context available for each incoming HTTP request.</p>
"
Jaeger,62828373,60655072,0,"2020/07/10, 09:20:22",False,"2020/07/10, 09:20:22",81,12550162,0,"<p>Are you connecting it to only elasticsearch or stack like ELK/EFK?. I had tried but we cannot configure ELK in jeager-all-one.exe alone in windows without docker.You can do it by running Jeager-collector, Jeager agent and Jeager query individually by mentioning configurations related to ELK.
In Jeager collector and Jeager query you need to set up variables <code>SPAN_STORAGE_TYPES</code> and <code>ES_SERVER_URLS</code>.</p>
"
Jaeger,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336,1773866,-1,"<p>You're using <code>Camden</code> release train with boot <code>2.0</code> and Sleuth <code>2.0</code>. That's completely incompatible. Please generate a project from start.spring.io from scratch, please don't put any versions manually for spring cloud projects, and please try again. Try using <code>Finchley</code> release train instead of <code>Camden</code></p>
"
Jaeger,65207511,65149873,0,"2020/12/08, 23:57:48",False,"2020/12/08, 23:57:48",357,3703933,1,"<p>I solved it by using this library instead <a href=""https://github.com/opentracing-contrib/java-spring-cloud"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud</a></p>
<p>It seem to have an option to enable or disable different instrumentation feature. Read about <code>opentracing.spring.cloud.async.enabled</code> for more info.</p>
"
Jaeger,65888740,65490691,0,"2021/01/25, 18:32:56",True,"2021/01/25, 18:32:56",1869,3511252,1,"<p>I had the same problem. Found this page that explains how to configure Thrift sender: <a href=""https://github.com/jaegertracing/jaeger-client-csharp/blob/master/src/Senders/Jaeger.Senders.Thrift/README.md"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-csharp/blob/master/src/Senders/Jaeger.Senders.Thrift/README.md</a></p>
<p>The C# tutorial does not mention it though ...</p>
<p>And here is my InitTracer(). Works fine with Jaeger launched from binary:</p>
<pre><code>    private static Tracer InitTracer(string serviceName, ILoggerFactory loggerFactory)
    {
        Configuration.SenderConfiguration.DefaultSenderResolver = new SenderResolver(loggerFactory)
            .RegisterSenderFactory&lt;ThriftSenderFactory&gt;();

        var samplerConfiguration = new Configuration.SamplerConfiguration(loggerFactory)
            .WithType(ConstSampler.Type)
            .WithParam(1);

        var sender = new SenderConfiguration(loggerFactory);

        var reporterConfiguration = new Configuration.ReporterConfiguration(loggerFactory)
            .WithLogSpans(true)
            .WithSender(sender);

        return (Tracer)new Configuration(serviceName, loggerFactory)
            .WithSampler(samplerConfiguration)
            .WithReporter(reporterConfiguration)
            .GetTracer();
    }
</code></pre>
"
Jaeger,66277685,66236610,0,"2021/02/19, 14:35:11",False,"2021/02/19, 14:35:11",169,3301685,0,"<p>Elastic search works fine for this. And Kibana allows you to build nice aggregated views of the traffic.</p>
<p>A recommendation from my experience is to use the <code>--es.tags-as-fields.dot-replacement</code> option and specify a character. This flattens the data structure. Its very useful because ElasticSearch/Kibana struggle with the tags data as an array.</p>
"
Jaeger,65059275,65059109,1,"2020/11/29, 12:48:21",True,"2020/11/29, 12:48:21",1058,1609014,0,"<p>Looks like your DaemonSet misses the <code>hostNetwork</code> property, to be able to listen on the node IP.
You can check that article for further info: <a href=""https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677"" rel=""nofollow noreferrer"">https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677</a></p>
"
Jaeger,63654371,63634257,3,"2020/08/30, 08:47:00",True,"2020/08/30, 08:47:00",366,10202496,1,"<p>By default, OpenTracing doesn't log automatically into span logs, only important messages that Jaeger feels it needs to be logged and is needed for tracing would be there :). The idea is to separate responsibilities between Tracing and Log management, <a href=""https://github.com/jaegertracing/jaeger/issues/962"" rel=""nofollow noreferrer"">Check this GitHub discussion</a>.</p>
<p>An alternative would be to use centralized log management and print traceId &amp; spanId into your logs for troubleshooting and correlating logs and tracing.</p>
"
Jaeger,63686699,63634257,0,"2020/09/01, 14:23:48",False,"2020/09/01, 14:23:48",363,977959,0,"<p>As iabughosh said, the main focus on jaeger is traceability, monitoring and performance, not for logging.</p>
<p>Anyway, i found that using the @traced Bean injection you can insert a tag into the current span, that will be printed on Jaeger UI. this example will added the first 4 lines of an excpetion to the Tags seccion. (I use it on my global ExceptionHandler to add more info about the error):</p>
<pre><code>public abstract class MyExceptionHandler {

@Inject
io.opentracing.Tracer tracer;
/**
 * Will trace the message at jaeger metrics service, only for monitoring and profiling use, not for Logger.
 * @param e
 */
protected void monitorTrace(Exception e) {
    if(tracer!= null &amp;&amp; e!=null) {
        StringBuffer sb = new StringBuffer();
        sb.append(e.getCause()+ &quot;\n&quot;);
        int deep = 3;
        
        for(int i=0; i&lt; deep;i++) {
            sb.append(e.getStackTrace()[i]+ &quot;\n&quot;);
        }
        tracer.activeSpan().setTag(&quot;stack &quot;,sb.toString());
    }
}
</code></pre>
<p>}</p>
<p>and you will see the little stacktrace at JaegerUI.</p>
<p><a href=""https://i.stack.imgur.com/xYMzt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xYMzt.png"" alt=""enter image description here"" /></a></p>
<p>Hope helps</p>
"
Jaeger,66249819,66213790,0,"2021/02/17, 22:55:27",False,"2021/02/17, 22:55:27",2140,3697695,1,"<p>I am not sure why Istio doesn't automatically trace your calls to external APIs. Perhaps it requires an egress gateway to be used, I'm not sure. Note also that Istio creates traces for http(s) traffic, not TCP.</p>
<p>However, this is something you can still do programmatically. You can use any of the <a href=""https://www.jaegertracing.io/docs/1.21/client-libraries/#supported-libraries"" rel=""nofollow noreferrer"">Jaeger client libraries</a> to augment&quot;the traces already created by Envoy by appending your own spans.</p>
<p>To do so, you need first to extract the trace context from the HTTP headers of the incoming request (assuming that your external API calls are consecutive to an incoming request), and then create a new span as child of that previous span context. A good idea would be to use <a href=""https://github.com/opentracing/specification/blob/master/semantic_conventions.md"" rel=""nofollow noreferrer"">OpenTracing semantic conventions</a> when you tag your new span. Tools like Kiali will be able to leverage some information if it follows this convention.</p>
<p>I've found this blog post that explains how to do it with the nodejs jaeger client: <a href=""https://rhonabwy.com/2019/01/06/adding-tracing-with-jaeger-to-an-express-application/"" rel=""nofollow noreferrer"">https://rhonabwy.com/2019/01/06/adding-tracing-with-jaeger-to-an-express-application/</a></p>
"
Jaeger,65485312,65377607,0,"2020/12/29, 01:18:49",False,"2020/12/29, 01:18:49",166,12249096,0,"<blockquote>
<ol>
<li>Whether OpenCensus collector should be injected by Linkerd.</li>
</ol>
</blockquote>
<p>Yes, the OpenCensus collector should be injected with the Linkerd proxy because the proxies themselves send the span info using mTLS. With mTLS, the sending (client) and receiving (server) sides of the request must present certificates to each other in to <em>verify</em> that identities to each other in a way that validates that the identity was issued by the same trusted source.</p>
<p>The Linkerd service mesh is made up of the control plane and the data plane. The control plane is a set of services that run within the cluster to implement the features of the service mesh. Mutual TLS (mTLS) is one of those features and is implemented by the <code>linkerd-identity</code> component of the control plane.</p>
<p>The data plane is comprised of any number of the Linkerd proxies which are injected into the services in the application, like the OpenCensus collector. Whenever a proxy is started within a pod, it sends a certificate signing request to the <code>linkerd-identity</code> component and receives a certificate in return.</p>
<p>So, when the Linkerd proxies in the control plane send the spans to the collector, they authenticate themselves with those certificates, which must be verified by the proxy injected into the OpenCensus collector Pod. This ensures that all traffic, even distributed traces, are sent securely within the cluster.</p>
<blockquote>
<ol start=""2"">
<li>Should I suffix serviceaccount name by namespace?</li>
</ol>
</blockquote>
<p>In your case, you should suffix the service account with the namespace. By default, Linkerd will use the Pod namespace, so if the service account doesn't exist in the Pod namespace, then the configuration will be invalid. The <a href=""https://github.com/linkerd/linkerd2/blob/main/jaeger/proxy-mutator/mutator/webhook.go#L137"" rel=""nofollow noreferrer"">logic</a> has a function that checks for a namespace in the annotation name and appends it, if it exists:</p>
<pre class=""lang-go prettyprint-override""><code>func ammendSvcAccount(ns string, params *Params) {
    hostAndPort := strings.Split(params.CollectorSvcAddr, &quot;:&quot;)
    hostname := strings.Split(hostAndPort[0], &quot;.&quot;)
    if len(hostname) &gt; 1 {
        ns = hostname[1]
    }
    params.CollectorSvcAccount = fmt.Sprintf(&quot;%s.%s&quot;, params.CollectorSvcAccount, ns)
}
</code></pre>
<p>So, this one is correct:</p>
<pre class=""lang-yaml prettyprint-override""><code>config.alpha.linkerd.io/trace-collector-service-account: my-opencensus-collector-service-account.ops
</code></pre>
"
Jaeger,64175114,64174906,1,"2020/10/02, 19:29:05",False,"2020/10/02, 19:29:05",1,540250,0,"<p>you are using System.Configuration namespace which causes ambiguity.</p>
<p>i would suggest remove the using System.Configuration. And try specifying fully qualified name for the Configuration. visual studio would suggest possible candidates (press Ctrl . on the Class name you want to qualify) provided you have added all required references in project already.</p>
"
Jaeger,64175147,64174906,4,"2020/10/02, 19:31:24",False,"2020/10/02, 19:31:24",85555,880990,0,"<p>If you have a</p>
<pre><code>using System;
</code></pre>
<p>then the C# compiler gets confused with <code>Configuration</code> and thinks it refers to the namespace <code>System.Configuration</code>. You can solve it by using the explicit namespace <code>Jaeger</code>:</p>
<pre><code>var config = new Jaeger.Configuration(
    context.HostingEnvironment.ApplicationName, loggerFactory);
</code></pre>
"
Jaeger,63843385,63843312,1,"2020/09/11, 11:21:58",True,"2020/09/11, 11:21:58",1911,11506391,2,"<p>There is no possibility of doing it in the Dockerfile if you want to keep two separate image. How should you know in advance the name/id of the container you're going to link ?</p>
<p>Below are two solutions :</p>
<ol>
<li>Use Docker compose. This way, Docker will automatically link all the containers together</li>
<li>Create a <code>bridge</code> network and add all the container you want to link inside. This way, you'll have name resolution and you'll be able to contact each container using its name</li>
</ol>
"
Jaeger,63843675,63843312,0,"2020/09/11, 11:43:16",False,"2020/09/11, 11:43:16",206,9641548,0,"<p>I recommend you using <a href=""https://docs.docker.com/engine/reference/commandline/network_create/"" rel=""nofollow noreferrer"">netwoking</a>, by creating:</p>
<pre><code>docker network create [OPTIONS] NETWORK
</code></pre>
<p>and then run with --network=&quot;network&quot;
using docker-compose with <a href=""https://docs.docker.com/compose/networking/#multi-host-networking"" rel=""nofollow noreferrer"">network</a> and link to each other
example:</p>
<pre><code>  version: '3'
    services:
      jaeger:
       network:
        -network1
      other_container:
        network:
         -network1
    networks:
      network1:
        external: true
</code></pre>
"
Jaeger,60463466,60462137,6,"2020/02/29, 10:52:57",False,"2020/03/01, 14:12:35",35494,7582247,1,"<p><code>-I</code> is used for <code>include</code> paths. Use <code>-L</code> for library paths:</p>

<pre><code>g++ -std=c++1z test.cpp -L /usr/local/lib -ljaegertracing -lyaml-cpp
</code></pre>

<p>It also looks like you've linked with a shared library <code>libyaml-cppd.so</code> - not the static library <code>libyaml-cpp.a</code>. I don't recognize the <code>d</code> in <code>libyaml-cppd.so</code> though. I'd check if that's really the library you built.</p>

<p><code>libyaml-cpp</code> will be built as a static library by default (<code>libyaml-cpp.a</code>) and on a 64 bit machine, it will probably default to being installed in <code>/usr/local/lib64</code>.</p>

<hr>

<p>You are only allowed to do very limited things in <code>namespace std</code>. Adding new functions/classes are not allowed (unless as template specializations including user defined types) - so remove <code>namespace std { ... }</code> around your program.</p>

<p>Also. the <code>main</code> function should be in the global namespace. The reason it's not found by the linker is because you put it in a namespace (<code>std</code>).</p>

<p>UPDATE : The issue is resolved follow this link exaclty <a href=""https://github.com/jaegertracing/jaeger-client-cpp/issues/162#issuecomment-565892473"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-cpp/issues/162#issuecomment-565892473</a> (use thrift version 0.11 or 0.11+)</p>
"
Jaeger,59285133,59276096,0,"2019/12/11, 13:47:46",False,"2019/12/11, 13:47:46",2576,12014434,2,"<p>Fisrt of all according to istio <a href=""https://istio.io/docs/concepts/observability/#service-level-metrics"" rel=""nofollow noreferrer"">documentation</a> Prometheus is used as default observation operator in istio mesh by default:</p>

<blockquote>
  <p>The <a href=""https://istio.io/docs/reference/config/policy-and-telemetry/metrics/"" rel=""nofollow noreferrer"">default Istio metrics</a> are defined by a set of configuration artifacts that ship with Istio and are exported to <a href=""https://istio.io/docs/reference/config/policy-and-telemetry/adapters/prometheus/"" rel=""nofollow noreferrer"">Prometheus</a> by default. Operators are free to modify the shape and content of these metrics, as well as to change their collection mechanism, to meet their individual monitoring needs.</p>
</blockquote>

<p>So by having istio injected prometheus operator You end up with two Prometheus operators in Your istio mesh.</p>

<p>Secondly, when you enforce Mutual TLS in Your istio mesh every connection has to be secure (<code>TLS</code>). And as You mentioned it works when there is no istio injection.</p>

<p>So the most likely cause is that the readiness probe fails because it is using <code>HTTP</code> protocol which is insecure (plain text) and this is one of the reason why You would get <code>503</code> error.</p>

<p>If you really need prometheus operator within istio mesh, this could be fixed by creating <code>DestinationRule</code> to <code>Disable</code> tls mode just for the readiness probe.</p>

<p>Example:</p>

<pre><code>$ kubectl apply -f - &lt;&lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
 name: ""readiness-probe-dr""
 namespace: ""prometheus-namespace""
spec:
 host: ""prometheus-prometheus-oper-prometheus.svc.cluster.local""
 trafficPolicy:
   tls:
     mode: DISABLE
EOF
</code></pre>

<p>Note: Make sure to modify it so that it matches Your namespaces and hosts. Also there could be some other prometheus collisions within mesh.</p>

<hr>

<p>The other solution would be not to have prometheus istio injected in the first place. You can disable istio injection in prometheus namespace by using the following commands:</p>

<pre><code>$ kubectl get namespace -L istio-injection
NAME              STATUS   AGE     ISTIO-INJECTION
default           Active   4d22h   enabled
istio-system      Active   4d22h   disabled
kube-node-lease   Active   4d22h   
kube-public       Active   4d22h   
kube-system       Active   4d22h   
prometheus        Active   30s     enabled
</code></pre>

<pre><code>$ kubectl label namespace prometheus istio-injection=disabled --overwrite
namespace/prometheus labeled
</code></pre>

<pre><code>$ kubectl get namespace -L istio-injection
NAME              STATUS   AGE     ISTIO-INJECTION
default           Active   4d22h   enabled
istio-system      Active   4d22h   disabled
kube-node-lease   Active   4d22h   
kube-public       Active   4d22h   
kube-system       Active   4d22h   
prometheus        Active   73s     disabled
</code></pre>
"
Jaeger,59144234,58999288,0,"2019/12/02, 19:50:35",True,"2019/12/02, 19:50:35",3020,1264920,0,"<p>I was able to publish some spans so I could see them on <a href=""http://localhost:16686"" rel=""nofollow noreferrer"">http://localhost:16686</a></p>

<ul>
<li>We should <code>SetOperationName</code> on the Span so we can identify it with a human readable name on the UI</li>
<li>We should also defer the <code>span.Finish()</code> call, it seems this is why I was not able to see anything on the UI</li>
</ul>

<p>This is the updated main function:</p>

<pre><code>func main() {

    tracer, closer := initJaeger(""foo-go-service"")
    defer closer.Close()

    span := tracer.StartSpan(""GoTestSpan"")
    defer span.Finish()
    span.SetOperationName(""opNameGoTestSpan"")
    var myMap = make(map[string]interface{})
    myMap[""foo""] = 42
    myMap[""bar""] = ""42""
    span.LogKV(myMap)

    // time.Sleep(2 * time.Second)

    childSpanRef := opentracing.ChildOf(span.Context())
    childSpan := tracer.StartSpan(""GoChildSpan"", childSpanRef)
    defer childSpan.Finish()
    childSpan.SetOperationName(""opNameGoChildSpan"")
    var myMap2 = make(map[string]interface{})
    myMap2[""foo2""] = 42
    myMap2[""bar2""] = ""42""
    childSpan.LogKV(myMap2)
}
</code></pre>
"
Jaeger,57068778,57068664,0,"2019/07/17, 08:00:18",False,"2019/07/17, 08:00:18",1034326,6309,1,"<p><a href=""https://golang.org/cmd/go/#hdr-Compile_and_run_Go_program"" rel=""nofollow noreferrer""><code>go run</code></a> compiles and runs the named main Go package.</p>

<p>Only <a href=""https://golang.org/cmd/go/#hdr-Compile_packages_and_dependencies"" rel=""nofollow noreferrer""><code>go build</code></a> or <a href=""https://golang.org/cmd/go/#hdr-Compile_and_install_packages_and_dependencies"" rel=""nofollow noreferrer""><code>go install</code></a> would compile the packages named by the import paths, along with their dependencies,</p>
"
Jaeger,54185032,54134885,0,"2019/01/14, 18:04:36",True,"2019/01/14, 18:14:37",147,2560123,1,"<p>I guess if you set sampler to 0 in the configuration then no traces will be captured.
<a href=""https://github.com/jaegertracing/jaeger-client-java#testing"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger-client-java#testing</a></p>

<p>But it's specific to Jaeger.
Otherwise you can use NoopTracer like <code>Tracer tracer = NoopTracerFactory.create();</code> <a href=""https://mvnrepository.com/artifact/io.opentracing/opentracing-noop/0.31.0"" rel=""nofollow noreferrer"">Maven</a> </p>
"
Jaeger,61860648,60506609,0,"2020/05/18, 03:34:12",True,"2020/05/18, 03:34:12",829,9428538,2,"<p>To start a jaeger container:</p>

<pre class=""lang-sh prettyprint-override""><code>docker run --rm --name jaeger -d -p 16686:16686 -p 6831:6831/udp jaegertracing/all-in-one
</code></pre>

<p>Then you should by able to access to the Jaeger UI at <a href=""http://localhost:16686"" rel=""nofollow noreferrer"">http://localhost:16686</a></p>

<p>Once you've have a Jaeger up and running, you need to configure a Jaeger exporter to forward spans to Jaeger. This will depends of the language used.</p>

<p><a href=""https://opentelemetry-python.readthedocs.io/en/stable/getting-started.html#configure-exporters-to-emit-spans-elsewhere"" rel=""nofollow noreferrer"">Here</a> is the straightforward documentation to do so in python.</p>
"
Jaeger,54867389,54675467,0,"2019/02/25, 15:32:18",False,"2019/02/25, 15:32:18",13313,524946,0,"<p>Setting a baggage item is <em>not</em> the same as setting an HTTP header. You should use your HTTP client (not shown in your example) to set the HTTP header.</p>

<p>Baggage items might or not be available as individual HTTP headers: it's a detail implementation of the underlying tracer, such as Jaeger's.</p>
"
Jaeger,59628491,59561262,0,"2020/01/07, 14:31:30",False,"2020/01/07, 14:31:30",13,4977370,0,"<p>Got it! We need to enable sampling strategy to reach the collector endpoint. </p>

<pre><code>var initTracer = require('jaeger-client').initTracer;

var config = {
  'serviceName': 'Jaeger_Service',
  'reporter': {
    'collectorEndpoint': 'http://jaeger-collector:14268/api/traces',
  },
  'sampler': {
    'type': 'const',
    'param' : 0.1  
  }
};

var options = {
  'logger': {
'info': function logInfo(msg) {
  console.log('INFO ', msg)
},
'error': function logError(msg) {
  console.log('ERROR', msg)
  }
 }
};

var tracer = initTracer(config, options);
var express = require('express');
var app = express();
var http = require('http');
var server = http.createServer(app);

app.get('/', (req, res) =&gt; {
    const span = tracer.startSpan('http_request');
    res.send('Hello Jaeger');
    span.log({'event': 'request_end'});
    span.finish();
});

server.listen(8000);
console.log('Express server started on port %s', server.address().port);
</code></pre>
"
Jaeger,55277542,55222810,0,"2019/03/21, 11:44:21",True,"2019/03/21, 11:44:21",654,955379,0,"<p>This issue looks more to have to do with Java it self then either Opentracing and Jaeger. as <code>ex.getStackTrace()</code> is more of the problem. As it should be more like </p>

<pre><code>StringWriter errors = new StringWriter();
ex.printStackTrace(new PrintWriter(errors));
span.setTag(""error"", true);
span.log(ImmutableMap.of(""stack"", errors));
</code></pre>

<p>Problem solved.</p>
"
Jaeger,52722089,50906213,0,"2018/10/09, 16:19:28",True,"2018/10/09, 16:19:28",13313,524946,1,"<blockquote>
  <p>The problem is that if I kill the Docker running the jaeger collector- systemctl stop docker and later restart docker and jaegertracing/all-in-one, the services are no longer up at <a href=""http://localhost:16686/api/services"" rel=""nofollow noreferrer"">http://localhost:16686/api/services</a></p>
</blockquote>

<p>That's because you are using the in-memory storage. If you stop and start the container, the storage is reset, so, you'll effectively lose your data. For production purposes, you should use a backing storage like Cassandra or Elasticsearch.</p>

<blockquote>
  <p>Does the Jaeger collector needs to be running before starting the Jaeger clients? </p>
</blockquote>

<p>No, but spans reported by clients when the collector isn't available might get dropped. Note that clients will send spans to the agent by default, and will not contact the collector directly. So, if the agent isn't available, spans might get dropped as well.</p>

<blockquote>
  <p>how can I flush the memory used by Jaeger OpenTracing so that my host doesn't run out of memory?</p>
</blockquote>

<p>Use the configuration option <code>--memory.max-traces</code>. With this option, older traces will get overwritten by new ones once this limit is reached.</p>
"
Jaeger,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360,1796107,0,"<p>Your best chance is to get the data from whatever storage the Jaeger collector is using (Cassandra, Elastic.) (<a href=""https://www.jaegertracing.io/docs/1.6/deployment/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.6/deployment/</a> )</p>

<p>My suggestion is to store in Elastic and use Kibana to accomplish what you need.</p>
"
Jaeger,65919685,52145774,0,"2021/01/27, 14:59:44",False,"2021/01/27, 14:59:44",1869,3511252,0,"<p>Old topic but the current version of Jaeger Query UI is a single page app and has an underlying API that allows the same queries capabilities as the UI.</p>
"
Jaeger,64592134,64486524,0,"2020/10/29, 15:30:59",False,"2020/10/29, 15:30:59",51,8394088,0,"<p>referring to the documentation provided in below link helped to resolve the issue.
<a href=""https://stackoverflow.com/questions/56353740/java-lang-illegalstateexception-this-should-not-happen-as-headers-should-only"">java.lang.IllegalStateException: This should not happen as headers() should only be called while a record is processed</a></p>
"
Jaeger,61163147,61073781,0,"2020/04/11, 23:15:30",False,"2020/04/11, 23:15:30",67,1466001,0,"<p>Here is a working example: <a href=""https://github.com/jobinesh/cloud-native-applications/tree/master/helidon-example-mp-jaeger"" rel=""nofollow noreferrer"">https://github.com/jobinesh/cloud-native-applications/tree/master/helidon-example-mp-jaeger</a>. See if that helps you. If you are interested, see the details captured here: <a href=""https://www.jobinesh.com/2020/04/tracing-api-calls-in-your-helidon.html"" rel=""nofollow noreferrer"">https://www.jobinesh.com/2020/04/tracing-api-calls-in-your-helidon.html</a></p>
"
Jaeger,65650224,65595635,0,"2021/01/10, 06:52:21",False,"2021/01/10, 06:52:21",31,3289465,0,"<p>Can you paste the Collector config file? It seems you are using the gRPC protocol and it's not supported on the system where the collector is running.
<a href=""https://github.com/open-telemetry/opentelemetry-collector/blob/master/exporter/otlpexporter/README.md"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-collector/blob/master/exporter/otlpexporter/README.md</a></p>
"
Jaeger,65910568,65595635,1,"2021/01/27, 01:06:48",True,"2021/01/27, 01:06:48",483,4895267,0,"<p>gRPC port isn't enabled in your jaeger instance.</p>
<p>You can try a docker-compose file like this</p>
<pre><code>version: &quot;3.6&quot;
services:
  jaeger:
    image: jaegertracing/all-in-one
    ports:
      - 5775:5775/udp
      - 6831:6831/udp
      - 6832:6832/udp
      - 5778:5778
      - 16686:16686
      - 14268:14268
      - 14250:14250
      - 9411:9411
</code></pre>
<p>And you can connect to it without problems</p>
"
Jaeger,58745678,58704937,0,"2019/11/07, 11:30:57",False,"2019/11/07, 12:06:45",2049,11207414,0,"<p>According to the documentation <a href=""https://istio.io/docs/tasks/telemetry/gateways/"" rel=""nofollow noreferrer"">Remotely Accessing Telemetry Addons</a>. There are different ways how to acces telemetry. </p>

<p>The Recommended way is to create Secure acces using https instead of http.</p>

<p><strong>Note for both methods:</strong></p>

<blockquote>
  <p>This option covers securing the transport layer only. You should also configure the telemetry addons to require authentication when exposing them externally.</p>
</blockquote>

<p>Please note that jaeger itself doesn't support authentication methods <a href=""https://github.com/jaegertracing/jaeger/issues/219"" rel=""nofollow noreferrer"">github</a> and workaround using Apache httpd server <a href=""https://medium.com/@larsmilland01/secure-architecture-for-jaeger-with-apache-httpd-reverse-proxy-on-openshift-f31983fad400"" rel=""nofollow noreferrer"">here</a>.</p>

<ol>
<li><p>With your recruitments you can use Gateways (SDS) <a href=""https://istio.io/docs/tasks/traffic-management/ingress/secure-ingress-sds/"" rel=""nofollow noreferrer"">with self-signed certificates</a>:</p>

<p><strong>a</strong>.) Make sure your that during istio instalation youe have enabled SDS at ingress gateway <code>--set gateways.istio-ingressgateway.sds.enabled=true</code> and <code>--set tracing.enabled=true</code> for tacing purposes.</p>

<p><strong>b</strong>.) Create self signed certificates for testing purposes you can use this <a href=""https://github.com/nicholasjackson/mtls-go-example"" rel=""nofollow noreferrer"">example and repository</a>.</p>

<p><strong>c</strong>.) Please follow <a href=""https://istio.io/docs/tasks/traffic-management/ingress/secure-ingress-sds/#configure-a-tls-ingress-gateway-using-sds"" rel=""nofollow noreferrer"">Generate client and server certificates and keys</a>  and <a href=""https://istio.io/docs/tasks/traffic-management/ingress/secure-ingress-sds/#configure-a-tls-ingress-gateway-using-sds"" rel=""nofollow noreferrer"">Configure a TLS ingress gateway using SDS</a>.</p></li>
<li><p>Create Virtualservice and Gateway:</p></li>
</ol>

<hr>

<pre><code>apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: mygateway
spec:
  selector:
    istio: ingressgateway # use istio default ingress gateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: ""httpbin-credential"" # must be the same as secret crated in the step 2.
    hosts:
    - ""httpbin.example.com"" ## You can apply ""*"" for all hosts

apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: tracing
spec:
  hosts:
  - ""httpbin.example.com"" ## You can apply ""*"" for all hosts
  gateways:
  - mygateway
  http:
  - match:
    - port: 443
    route:
    - destination:
        port:
          number: 80
        host: tracing.istio-system.svc.cluster.local

curl -kvI https ://xx.xx.xx.xx/
*   Trying xx.xx.xx.xx...
* TCP_NODELAY set
* Connected to xx.xx.xx.xx (xx.xx.xx.xx) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256

* ALPN, server accepted to use h2
&gt; HEAD / HTTP/1.1
&gt; Host: xx.xx.xx.xx
&gt; User-Agent: curl/7.52.1
&gt; Accept: */*
&gt;
* Connection state changed (MAX_CONCURRENT_STREAMS updated)!
&lt; HTTP/2 200
HTTP/2 200
&lt; content-type: text/html; charset=utf-8
content-type: text/html; charset=utf-8
&lt; date: Thu, 07 Nov 2019 10:01:33 GMT
date: Thu, 07 Nov 2019 10:01:33 GMT
&lt; x-envoy-upstream-service-time: 1
x-envoy-upstream-service-time: 1
&lt; server: istio-envoy
server: istio-envoy
</code></pre>

<p>Hope this help</p>
"
Jaeger,64536471,64266056,0,"2020/10/26, 13:43:36",False,"2020/10/26, 13:43:36",1157,1700378,0,"<p>GitLab Helm charts support tracing, and you can configure it with:</p>
<pre><code>global:
  tracing:
    connection:
      string: 'opentracing://jaeger?http_endpoint=http%3A%2F%2Fjaeger.example.com%3A14268%2Fapi%2Ftraces&amp;sampler=const&amp;sampler_param=1'
    urlTemplate: 'http://jaeger-ui.example.com/search?service={{ service }}&amp;tags=%7B&quot;correlation_id&quot;%3A&quot;{{ correlation_id }}&quot;%7D'
</code></pre>
<p>For more details refer :<a href=""https://docs.gitlab.com/charts/charts/globals.html#tracing"" rel=""nofollow noreferrer"">https://docs.gitlab.com/charts/charts/globals.html#tracing</a></p>
"
Jaeger,50339003,49909075,0,"2018/05/15, 00:05:40",True,"2018/05/15, 00:05:40",611,118116,1,"<p>Turns out I don't need neither Zipkin nor Jaeger to have my traces on Stackdriver. All that is needed is a deployment of <a href=""http://gcr.io/stackdriver-trace-docker/zipkin-collector"" rel=""nofollow noreferrer"">zipkin-collector</a> and a service to point to it and all my traces are now reporting as expected on GCP Stackdriver.</p>
"
Jaeger,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576,12014434,1,"<p>According to <a href=""https://istio.io/latest/docs/ops/integrations/jaeger/#option-2-customizable-install"" rel=""nofollow noreferrer"">istio</a> documentation:</p>
<blockquote>
<h3>Option 2: Customizable install<a href=""https://istio.io/latest/docs/ops/integrations/jaeger/#option-2-customizable-install"" rel=""nofollow noreferrer""></a></h3>
<p>Consult the  <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">Jaeger documentation</a>  to
get started. No special changes are needed for Jaeger to work with
Istio.</p>
<p>Once Jaeger is installed, you will need to point Istio proxies to send
traces to the deployment. This can be configured with  <code>--set values.global.tracer.zipkin.address=&lt;jaeger-collector-address&gt;:9411</code>
at installation time. See the
<a href=""https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#Tracing"" rel=""nofollow noreferrer""><code>ProxyConfig.Tracing</code></a>
for advanced configuration such as TLS settings.</p>
</blockquote>
<p>Istio documentation states to use jaeger collector address in <code>global.tracer.zipkin.address</code>.</p>
<hr />
<p>As for the Jaeger agent host, according to <a href=""https://www.jaegertracing.io/docs/1.20/operator/"" rel=""nofollow noreferrer"">Jaeger</a> Operator documentation:</p>
<blockquote>
<p>&lt;9&gt; By default, the operator assumes that agents are deployed as
sidecars within the target pods. Specifying the strategy as
“DaemonSet” changes that and makes the operator deploy the agent as
DaemonSet. Note that your tracer client will probably have to override
the “JAEGER_AGENT_HOST” environment variable to use the node’s IP.</p>
</blockquote>
<hr />
<blockquote>
<p>Your tracer client will then most likely need to be told where the agent is located. This is usually done by setting the environment variable  <code>JAEGER_AGENT_HOST</code>  to the value of the Kubernetes node’s IP, for example:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: acme/myapp:myversion
        env:
        - name: JAEGER_AGENT_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
</code></pre>
</blockquote>
"
Jaeger,60796655,60793517,1,"2020/03/22, 08:47:00",False,"2020/03/22, 08:47:00",27576,1839482,0,"<p>You need to enable open tracing in nginx ingress controller.</p>

<p>To enable the instrumentation we must enable OpenTracing in the configuration ConfigMap:</p>

<pre><code>data:
  enable-opentracing: ""true""
</code></pre>

<p>To enable or disable instrumentation for a single Ingress, use the enable-opentracing annotation:</p>

<pre><code>kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/enable-opentracing: ""true""
</code></pre>

<p>You must also set the host to use when uploading traces:</p>

<pre><code>jaeger-collector-host: jaeger-agent.default.svc.cluster.local
</code></pre>

<p><a href=""https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/"" rel=""nofollow noreferrer"">https://kubernetes.github.io/ingress-nginx/user-guide/third-party-addons/opentracing/</a></p>
"
Jaeger,61666591,61619122,0,"2020/05/07, 22:54:39",True,"2020/05/07, 22:54:39",13,7069613,0,"<p>According to <a href=""https://helm.sh/docs/chart_template_guide/control_structures/"" rel=""nofollow noreferrer"">https://helm.sh/docs/chart_template_guide/control_structures/</a> a string is converted to a boolean of True.  So even a string of false would get evaluated as a Boolean of True by Helm. I was using Spinnaker which handles all overrides as a string unless the ""Raw Overrides"" box is checked.  If that box is checked than it converts the string to primitives where applicable.</p>

<p>My issue was that even though I was overriding with a value of false, Spinnaker would pass that as a string to Helm which would then evaluate that as True.</p>

<p>The solution was to check the ""Raw Overrides"" box in Spinnaker.  </p>
"
Jaeger,57420811,57419866,0,"2019/08/09, 00:26:00",False,"2019/08/09, 00:26:00",121,5799778,0,"<p>Maybe you should check whether the application services which you set up in a hurry are both in the same azure resource group as the VM running the Jaeger all-in-one instance, otherwise the second application might not be able to communicate with the Jaeger instance at all.</p>
"
Jaeger,66621994,66597254,0,"2021/03/14, 09:14:12",False,"2021/03/14, 09:14:12",13,15381526,1,"<p>I realized that I had got into a completely wrong direction. I thought that I have to access the backend storage to get the trace data, which actually make the problem much more complex. I got the answer from github discussion and here is the address <a href=""https://github.com/jaegertracing/jaeger/discussions/2876#discussioncomment-477176"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger/discussions/2876#discussioncomment-477176</a></p>
"
Jaeger,63837122,63835165,0,"2020/09/10, 23:16:48",True,"2020/09/10, 23:16:48",147,1729409,0,"<p>Solved by adding dependency in pom file on jaeger-thrift.</p>
"
Jaeger,63583340,63581052,1,"2020/08/25, 19:34:35",False,"2020/08/25, 20:14:13",336,8095979,2,"<p>You need to keep double quotes as it is.</p>
<p>An issue has been identified similar to this [1] and has been fixed recently. Can you try to get the latest WUM updated API Manager 3.1.0 and try enabling Jaeger open tracing?</p>
<p>Alternatively, this issue will not occur when using &quot;localhost&quot; as the hostname.</p>
<p>[1] <a href=""https://github.com/wso2/product-apim/issues/7940"" rel=""nofollow noreferrer"">https://github.com/wso2/product-apim/issues/7940</a></p>
"
Jaeger,63583870,63581052,0,"2020/08/25, 20:08:00",False,"2020/12/03, 10:54:28",2466,3176125,0,"<p>Run Jager using the docker image as follows.</p>
<pre><code>docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.18
</code></pre>
<p>Then add the following config to the deployment.toml.</p>
<pre><code>[apim.open_tracer]
remote_tracer.enable = true
remote_tracer.name = &quot;jaeger&quot;
remote_tracer.properties.hostname = &quot;localhost&quot;
remote_tracer.properties.port = &quot;6831&quot;
</code></pre>
<p>Side Note: For zipkin you can use the following.</p>
<pre><code>[apim.open_tracer]
remote_tracer.enable = true
remote_tracer.name = &quot;zipkin&quot;
remote_tracer.properties.hostname = &quot;localhost&quot;
remote_tracer.properties.port = &quot;9411&quot;
</code></pre>
"
Jaeger,57474156,57473539,0,"2019/08/13, 11:45:41",False,"2019/08/13, 11:45:41",1642,2255344,1,"<p>I believe you can reuse Elasticsearch for multiple purposes - each would use a different set of indices, so separation is good. </p>

<p>from: <a href=""https://www.jaegertracing.io/docs/1.11/deployment/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.11/deployment/</a> :</p>

<blockquote>
  <p>Collectors require a persistent storage backend. Cassandra and Elasticsearch are the primary supported storage backends</p>
</blockquote>

<p>Tying the networking all together, a docker-compose example:
<a href=""https://stackoverflow.com/questions/51785812/how-to-configure-jaeger-with-elasticsearch"">How to configure Jaeger with elasticsearch?</a></p>
"
Jaeger,57476852,57473539,0,"2019/08/13, 14:30:22",False,"2019/08/13, 14:30:22",4433,243104,1,"<p>While this isn't exactly what you asked, it sounds like what you're trying to achieve is seeing tracing for your JMS calls in Jaegar. If that is the case, you could use an OpenTracing tracing solution for JMS or ActiveMQ to report tracing data directly to Jaegar. Here's one potential solution I found with a quick google. There may be others.</p>

<p><a href=""https://github.com/opentracing-contrib/java-jms"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-jms</a></p>
"
Jaeger,65852979,61431244,0,"2021/01/22, 23:26:39",False,"2021/01/22, 23:26:39",483,4895267,1,"<p>You can set a tag to Span creating a new custom Span</p>
<pre><code>Tracer tracer = GlobalTracer.get();
Tracer.SpanBuilder spanBuilder = tracer.buildSpan(&quot;CustomSpan&quot;)
    .withTag(Tags.SPAN_KIND.getKey(), Tags.SPAN_KIND_SERVER);

Span span = spanBuilder.start();
Tags.COMPONENT.set(span, &quot;MyComponent&quot;);
span.setTag(&quot;mytag&quot;, &quot;test&quot;);
span.finish();
</code></pre>
<p>or retrieving the current active Span</p>
<pre><code>Tracer tracer = GlobalTracer.get();
Span span = tracer.activeSpan();
</code></pre>
"
Jaeger,64481993,64465875,0,"2020/10/22, 15:14:22",True,"2020/10/22, 16:39:57",2287,261708,1,"<p>I got it working as mentioned below</p>
<pre><code>kubectl expose service prometheus --type=LoadBalancer --name=prometheus --namespace istio-system
    
export PROMETHEUS_URL=$(kubectl get svc prometheus-svc -n istio-system  -o jsonpath=&quot;{.status.loadBalancer.ingress[0]['hostname','ip']}&quot;):$(kubectl get svc prometheus-svc -n istio-system -o 'jsonpath={.spec.ports[0].port}')


echo http://${PROMETHEUS_URL}
curl http://${PROMETHEUS_URL}
</code></pre>
<p>I would assume that it may not be the right way of exposing the services. Instead</p>
<ol>
<li>Create a Istio Gateway point to <a href=""https://grafana.mycompany.com"" rel=""nofollow noreferrer"">https://grafana.mycompany.com</a></li>
<li>Create a Istio Virtual service to redirect the requuest to the above Internal Service</li>
</ol>
"
Jaeger,66493806,66487682,1,"2021/03/05, 15:54:27",False,"2021/03/05, 15:54:27",1,15337079,0,"<p>You can set the service name in the code as follows:</p>
<pre><code>Resource serviceNameResource = Resource.create(Attributes.of(ResourceAttributes.SERVICE_NAME, serviceName));

// Set to process the spans by the Jaeger Exporter
SdkTracerProvider tracerProvider =
    SdkTracerProvider.builder()
        .addSpanProcessor(SimpleSpanProcessor.create(jaegerExporter))
        .setResource(Resource.getDefault().merge(serviceNameResource))
        .build();

OpenTelemetrySdk openTelemetry =
    OpenTelemetrySdk.builder().setTracerProvider(tracerProvider).build();
</code></pre>
"
Jaeger,66519661,66487682,0,"2021/03/07, 19:57:21",True,"2021/03/07, 19:57:21",343,2500390,0,"<p>Based on some of the hints provided by Opentelemetry Java community, created two providers which indeed creates two services. Reusing same exporter, so that multiple connections to the backend can be avoided. I will learn more about  reusing exporter to create two or more provides in the same application in coming days.</p>
<pre><code>OpenTelemetry openTelemetryParent = ExampleConfiguration.initOpenTelemetry(jaegerHostName, jaegerPort,
        &quot;FirstService&quot;);
    Tracer tracerParent = openTelemetryParent.getTracer(&quot;FirstServiceInstrumentation&quot;);

    OpenTelemetry openTelemetryChild = ExampleConfiguration.initOpenTelemetry(jaegerHostName, jaegerPort,
        &quot;SecondService&quot;);
    Tracer tracerChild = openTelemetryChild.getTracer(&quot;SecondServiceInstrumentation&quot;);

    Random randomNum = new Random();

    for (int i = 0; i &lt; 10; i++) {
      Span parentSpan = tracerParent.spanBuilder(&quot;MyService1Span&quot;).startSpan();
      try (Scope scope = parentSpan.makeCurrent()) {
        Thread.sleep(1 + randomNum.nextInt(100));
        secondService(parentSpan, tracerChild, randomNum.nextInt(300));
        Thread.sleep(1 + randomNum.nextInt(100));
      } catch (InterruptedException e) {
        e.printStackTrace();
      } finally {
        parentSpan.end();
      }
    }
  }

  private static void secondService(Span parentSpan, Tracer tracer, int delay) {
    Span childSpan = tracer.spanBuilder(&quot;MyService2Span&quot;).setParent(Context.current().with(parentSpan)).startSpan();
    try {
      Thread.sleep(delay);
    } catch (InterruptedException e) {
      e.printStackTrace();
    }
    childSpan.end();
  }
</code></pre>
<p>Thank you to amazing Opentelemetry java community - @John Watson (jkwatson), Bogdan Drutu, Rupinder Singh, Anuraag Agrawal.</p>
"
Jaeger,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693,971735,0,"<p>Based on this:</p>
<blockquote>
<p>The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.</p>
</blockquote>
<p>it seems that the tracing information is not propagated across services. You can check this by looking into the HTTP headers and check the <code>traceId</code>. In order to make this work the <code>traceId</code> should be the same across the requests. You should see the same <code>traceId</code> in the logs too.</p>
<p>The documentation gives you some pointers how to troubleshoot this:</p>
<ul>
<li><a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#how-to-make-components-work"" rel=""nofollow noreferrer"">How to Make RestTemplate, WebClient, etc. Work?</a></li>
<li><a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#sleuth-http-client-webclient-integration"" rel=""nofollow noreferrer"">WebClient integration</a></li>
</ul>
"
Jaeger,66147115,66048959,2,"2021/02/11, 02:26:16",False,"2021/02/11, 02:26:16",4433,243104,0,"<p>I'm not 100% sure what the problem is you're experiencing, but here's some things to consider.</p>
<p>According to <a href=""https://community.traefik.io/t/tracing-with-elastic/5742/4"" rel=""nofollow noreferrer"">this post on the Traefik forums</a>, that message you're seeing is <code>debug</code> level because it's not something you should be worried about. It's just logging that no trace context was found, so a new one will be created. That second part is not in the message, but apparently that's what happens.</p>
<p>You should check to see if you're getting data appearing in Jaeger. If you are, that message is probably nothing to worry.</p>
<p>If you are getting data in Jaeger, but it's not connected, that will be because Traefik can only only work with trace context that is already in inbound requests, but it can't add trace context to outbound requests. Within your application, you'll need to implement trace propagation so that your outbound requests include the trace context that was received as part of the incoming request. Without doing that, every request will be sent without trace context and will start a new trace when it is received at the next Traefik ingress point.</p>
"
Jaeger,66151385,66048959,0,"2021/02/11, 10:45:37",True,"2021/02/11, 10:45:37",19808,376483,0,"<p>The problem actually was with the <code>traceContextHeaderName</code>. Sadly I can not tell exactly what the problem was as the <code>git diff</code> only shows that nothing changed around traefik and jaeger at the point where I fixed it. I assume config got &quot;stuck&quot; somehow. I tracked down the <a href=""https://github.com/traefik/traefik/blob/714a4d4f2d7a963e3e3dee2bf22d51a8f62bde0b/integration/tracing_test.go#L157-L168"" rel=""nofollow noreferrer"">related lines in source</a>, but as I am no Go-Dev, I can only guess if there's a bug.</p>
<p>What I did was to switch back to <code>uber-trace-id</code>, which magically &quot;fixed&quot; it. After I ran some traces and connected another service (node, npm <code>jaeger-client</code> with <code>process.env.TRACER_STATE_HEADER_NAME</code> set to an equal value), I switched back to <code>traefik-trace-id</code> and things worked.</p>
"
Jaeger,65298980,65219729,1,"2020/12/15, 03:56:21",True,"2020/12/15, 03:56:21",471,2673284,1,"<p>To my knowledge, the design of <code>ForkJoinPool.commonPool()</code> makes it impossible to actually replace that pool programmatically with an instrumented version. So the only workaround is to do it via bytecode manipulation.</p>
<p>The <a href=""https://github.com/open-telemetry/opentelemetry-java-instrumentation"" rel=""nofollow noreferrer"">OpenTelemetry Java Automatic Instrumentation</a> libraries perform a lot of magic to be able to take care of correctly propagating context through async/concurrency primitives, you may want to give them a try.</p>
"
Jaeger,64891826,64878686,1,"2020/11/18, 13:05:30",False,"2020/11/18, 13:05:30",557,615104,2,"<p>Tracing is enabled by default for JAX-RS endpoints only, not for reactive routes at the moment. You can activate tracing by annotating your route with <code>@org.eclipse.microprofile.opentracing.Traced</code>.</p>
"
Jaeger,66257794,64878686,0,"2021/02/18, 12:11:01",True,"2021/02/25, 01:42:56",1332,1687162,1,"<p>Yes, adding @Traced enable to activate tracing on reactive routes.</p>
<p>Unfortunately, using both JAX-RS reactive and reactive routes bugs the tracing on event-loop threads used by JAX-RS reactive endpoint when they get executed.</p>
<p>I only started Quarkus 2 days ago so i don't really the reason of this behavior (and whether it's normal or it's a bug), but obviously switching between two completely mess up the tracing.</p>
<p>Here is an example to easily reproduce it:</p>
<ul>
<li>Create a REST Easy reactive endpoint returning an empty Multi</li>
<li>Create a custom reactive route</li>
<li>set up the IO threads to 2 (easier to quickly reproduce it)</li>
<li>Run the application, and request the two endpoints alternatively</li>
</ul>
<p>Here is a screenshot that show the issue
<a href=""https://i.stack.imgur.com/4WyKL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4WyKL.png"" alt=""enter image description here"" /></a></p>
<p>As you can see, as soon as the JAX-RS resource is it and executed on one of the two threads available, it &quot;corrupts&quot; it, messing the trace_id reported (i don't know if it's the generation or the reporting on logs that is broken) on logs for the next calls of the reactive route.</p>
<p>This does not happen on the JAX-RS resource, as you can notice on the screenshot as well. So it seems to be related to reactive routes only.</p>
<p>Another point here is the fact that JAX-RS Reactive resources are incorrectly reported on Jaeger. (with a mention to a missing root span) Not sure if it's related to the issue but that's also another annoying point.</p>
<p>I'm thinking to completely remove the JAX-RS Reactive endpoint and replace them by normal reactive route to eliminate this bug.
I would appreciate if someone with more experience than me could verify this or tell me what i did wrong :)</p>
<p>EDIT 1: I added a route filter with priority 500 to clear the MDC and the bug is still there, so definitely not coming from MDC.</p>
<p>EDIT 2: I opened a <a href=""https://github.com/quarkusio/quarkus/issues/15182"" rel=""nofollow noreferrer"">bug report</a> on Quarkus</p>
<p>EDIT 3: It seems related to how both implementations works (thread locals versus context propagation in actor based context)
So, unless JAX-RS reactive resources are marked @Blocking (and get executed in a separated thread pool), JAX-RS reactive and Vertx reactive routes are incompatible when it comes to tracing (but also probably the same for MDC related informations since MDC is also thread related)</p>
"
Jaeger,64067421,64051610,0,"2020/09/25, 18:50:14",False,"2020/09/25, 18:50:14",1532,5491213,0,"<p>I tried applying your file on a Kubernetes 1.16 cluster, and there are a couple of issues with it:</p>
<ol>
<li>It uses apiVersions that were <a href=""https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/"" rel=""nofollow noreferrer"">decommed in Kubernetes 1.16</a>. ie- <code>apiVersion: extensions/v1beta1</code> should become <code>apps/v1</code>.</li>
<li>The Deployments don't have any selectors and neither does the Daemonset.</li>
</ol>
<blockquote>
<p>The .spec.selector field defines how the Deployment finds which Pods to manage.</p>
</blockquote>
<p>It seems like you are applying something that is super old.  Kubernetes notes the below in its doc, and so I wonder if this used to work on older versions of Kubernetes where selectors were defaulted.</p>
<blockquote>
<p>As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the .spec.template. The pod selector will no longer be defaulted when left empty.</p>
</blockquote>
<p>It seems like you should take a new approach-- in looking around, I found a couple of good tutorials <a href=""https://www.digitalocean.com/community/tutorials/how-to-implement-distributed-tracing-with-jaeger-on-kubernetes"" rel=""nofollow noreferrer"">here</a> and <a href=""https://thenewstack.io/best-practices-for-deploying-jaeger-on-kubernetes-in-production/"" rel=""nofollow noreferrer"">here</a>, and Jaeger themselves offer a similar approach <a href=""https://www.jaegertracing.io/docs/1.19/operator/"" rel=""nofollow noreferrer"">here</a>.  They all make use of <a href=""https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator#:%7E:text=A%20Kubernetes%20operator%20is%20a,and%20managing%20a%20Kubernetes%20application.&amp;text=A%20Kubernetes%20operator%20is%20an,behalf%20of%20a%20Kubernetes%20user."" rel=""nofollow noreferrer"">Kubernetes Operators</a>.</p>
<blockquote>
<p>A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user</p>
</blockquote>
<p>I don't know what you mean by <em>&quot;So previously I had a binary executable (jaeger-agent) running on a Linux CentOS 8 box alongside a server side application&quot;</em></p>
<p>The file you are applying looks like it <a href=""https://github.com/jaegertracing/jaeger-kubernetes/blob/master/jaeger-production-template.yml#L182-L185"" rel=""nofollow noreferrer"">deploys the agent as a daemonset</a>, which means the agent is run as a pod on each node of your cluster.  If it is running in your k8's cluster, then <a href=""https://stackoverflow.com/questions/63942591/service-cannot-communicate-with-rabbitmq-in-k8s-cluster/63943106#63943106"">this is how I normally approach troubleshooting kubernetes services</a>.  If it is running outside of your cluster entirely, then you need to make sure the Service it talks to is exposed outside of the cluster probably using type LoadBalancer.</p>
"
Jaeger,62430615,62418175,0,"2020/06/17, 16:50:21",True,"2020/06/17, 16:50:21",213,7933630,0,"<p>Metrics in OpenTelemetry are currently undergoing continued development and refinement, so they aren't necessarily available for each language yet (see <a href=""https://github.com/open-telemetry/opentelemetry-js/issues?q=is%3Aissue+is%3Aopen+label%3Afeature-request"" rel=""nofollow noreferrer"">this tag in the OpenTelemetry JS repo</a> for an example of metric instruments that aren't up to date yet with spec), but once they are, I'd expect for metrics to be added to the existing node/web instrumentation packages.</p>

<p>That said, I would still advise you to try out OpenTelemetry for traces at this point, as it's pretty stable for tracing. You can use a Prometheus client to export metrics separately, and once OpenTelemetry metrics are fully supported in the JS library, switch over to that.</p>
"
Jaeger,60989810,60978505,3,"2020/04/02, 13:49:29",False,"2020/04/02, 13:49:29",2576,12014434,0,"<p>There are a few different reasons why You could be experiencing this issue.</p>
<p>From <a href=""https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-sampling"" rel=""nofollow noreferrer"">istio</a> documentation:</p>
<blockquote>
WHY ARE MY REQUESTS NOT BEING TRACED?<a href=""https://istio.io/faq/distributed-tracing/#no-tracing"" rel=""nofollow noreferrer""></a>
<p>Since Istio 1.0.3, the sampling rate for tracing has been reduced to 1% in the  <code>default</code>  <a href=""https://istio.io/docs/setup/additional-setup/config-profiles/"" rel=""nofollow noreferrer"">configuration profile</a>. This means that only 1 out of 100 trace instances captured by Istio will be reported to the tracing backend. The sampling rate in the  <code>demo</code>  profile is still set to 100%. See  <a href=""https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-sampling"" rel=""nofollow noreferrer"">this section</a>  for more information on how to set the sampling rate.</p>
<p>If you still do not see any trace data, please confirm that your ports conform to the Istio  <a href=""https://istio.io/faq/traffic-management/#naming-port-convention"" rel=""nofollow noreferrer"">port naming conventions</a>  and that the appropriate container port is exposed (via pod spec, for example) to enable traffic capture by the sidecar proxy (Envoy).</p>
<p>If you only see trace data associated with the egress proxy, but not the ingress proxy, it may still be related to the Istio  <a href=""https://istio.io/faq/traffic-management/#naming-port-convention"" rel=""nofollow noreferrer"">port naming conventions</a>. Starting with  <a href=""https://istio.io/news/releases/1.3.x/announcing-1.3/#intelligent-protocol-detection-experimental"" rel=""nofollow noreferrer"">Istio 1.3</a>  the protocol for  <strong>outbound</strong>  traffic is automatically detected.</p>
</blockquote>
<p>Hope it helps.</p>
"
Jaeger,59894312,59875985,0,"2020/01/24, 12:11:13",True,"2020/01/24, 12:23:57",4418,127670,0,"<p>The short answer is ""you can't.""</p>

<p>My question was based on a very fundamental misunderstanding of what opentracing does.</p>

<blockquote>
  <p><a href=""https://github.com/opentracing/opentracing.io/issues/102#issuecomment-230031182"" rel=""nofollow noreferrer"">Tracing context is only propagated downstream, not upstream.</a></p>
</blockquote>

<p>From the same discussion thread:</p>

<blockquote>
  <p>On the wire propagation is only meant to carry ""span context"", which
  is a small set of ID fields and possible baggage. Returning the whole
  trace as part of the request is not a use case that was considered.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>The trace collection is meant to be asynchronous and out of process. </p>
</blockquote>

<p>So my understanding is now thus:</p>

<p>Each individual software component creates its own tracing data, bundles it up, and sends it off to the tracing server (e.g. Jaeger). Each software component <strong>must</strong> be configured to use the same tracing provider and the same tracing server - an RPC client cannot tell an RPC server that for a particular trace it should use the Jaeger tracing provider and a Jaeger server at such-and-such an address. (At least, the opentracing standard doesn't provide a way to do this.) The tracing information injected into a RPC request by the client allows the RPC server to embed a 'parent' ID field into the tracing information.</p>

<p>It's then the responsibility of the tracer (e.g. Jaeger) to figure out the relationships between the various traces it has received from various software components by matching up ID codes embedded in them.</p>

<p>So what I wanted to do is not a use case considered by opentracing and is not possible.</p>
"
Jaeger,59841411,59839687,0,"2020/01/21, 14:41:01",False,"2020/01/21, 14:41:01",2576,12014434,0,"<p>My interpretation of this is that We need to keep in mind that communication between services needs to support forwarding/&quot;passing along&quot; the trace ID's so that the tracing works correctly.</p>
<hr />
<p>So it warns us against situations where:</p>
<blockquote>
<p>Client calls -&gt; Service A #using http request with trace ID in header.</p>
<p>Service A -&gt; Service B #using tcp request that does not support headers and the trace ID header is lost.</p>
</blockquote>
<p>This situation could break or limit tracing functionality.</p>
<hr />
<p>On the other hand If we have situation where:</p>
<blockquote>
<p>Client calls -&gt; Service A #using http request with trace ID in header.</p>
<p>Service A -&gt; Service B #using http request the trace ID is forwarded to service B.</p>
</blockquote>
<p>This situation allows for the trace ID header to be present in both connections so the tracing can be logged and then viewed in tracing service dashboard. Then We can explore the path taken by the request and view the latency incurred at each hop.</p>
<p>Hope it helps.</p>
"
Jaeger,60078995,59839687,0,"2020/02/05, 17:23:23",False,"2020/02/05, 17:23:23",461,3707487,3,"<p>When doing tracing in a service mesh, behind proxies, the traceID generated upon the initial client call is propagated automatically only so long as the call goes from proxy->proxy.  </p>

<p>So:</p>

<ol>
<li>Client sends request</li>
<li>Request hits some ingress proxy. A trace ID is generated</li>
<li>Request is routed to proxy A in front of Service A.  The trace ID is propagated from the ingress proxy to proxy A</li>
<li>Request hits microservice A.  The trace ID is propagated to here in the headers.</li>
<li>Microservice A now has full control over the request.  If the service discards all the HTTP headers when making it's outgoing request to Service B, then the traceID will also be discarded.  </li>
</ol>

<p>To get around this, Microservice A just needs to know which headers represent the traceIDs, how to append into it, and some state to make sure it makes it to outgoing requests.  Then you'll get a full transaction chain.</p>

<p>Without the service propagating the headers, tracing basically gives you each path that ends in a microservice.  Still useful, but not as complete of a picture.  </p>
"
Jaeger,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244,5564578,2,"<p>By default, you don't have a logging system on Istio. I mean, besides the native logging of Kubernetes.</p>

<p>Zipkin and Jaeger are tracing systems, meaning for latency, not for logging.</p>

<p>You can definitely get this info through Istio components, but you will have to set it up first. I found <a href=""https://istio.io/docs/tasks/observability/logs/"" rel=""nofollow noreferrer"">this</a> articles; in Istio website about how to collect logs. I would say <code>Fluentd</code> + <code>Elasticsearch</code> would give you something as powerful as you need. Unfortunately I don't have any examples.</p>
"
Jaeger,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576,12014434,1,"<p>According to envoy proxy documentation for envoy <a href=""https://www.envoyproxy.io/docs/envoy/v1.12.0/intro/arch_overview/observability/tracing.html?highlight=traceid#trace-context-propagation"" rel=""nofollow noreferrer""><code>v1.12.0</code></a> used by istio <code>1.3</code>:</p>

<blockquote>
  <h2>Trace context propagation<a href=""https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing.html?highlight=traceid#trace-context-propagation"" rel=""nofollow noreferrer"" title=""Permalink to this headline""></a></h2>
  
  <p>Envoy provides the capability for reporting tracing information regarding communications between services in the mesh. However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests.</p>
  
  <p>Whichever tracing provider is being used, the service should propagate the  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-request-id"" rel=""nofollow noreferrer"">x-request-id</a>  to enable logging across the invoked services to be correlated.</p>
  
  <p>The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood. This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests. This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace.</p>
  
  <p>Alternatively the trace context can be manually propagated by the service:</p>
  
  <ul>
  <li><p>When using the LightStep tracer, Envoy relies on the service to propagate the 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-ot-span-context"" rel=""nofollow noreferrer"">x-ot-span-context</a>
  HTTP header while sending HTTP requests to other services.</p></li>
  <li><p>When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-traceid"" rel=""nofollow noreferrer"">x-b3-traceid</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-spanid"" rel=""nofollow noreferrer"">x-b3-spanid</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-parentspanid"" rel=""nofollow noreferrer"">x-b3-parentspanid</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-sampled"" rel=""nofollow noreferrer"">x-b3-sampled</a>,
  and 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-flags"" rel=""nofollow noreferrer"">x-b3-flags</a>).
  The 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-sampled"" rel=""nofollow noreferrer"">x-b3-sampled</a>
  header can also be supplied by an external client to either enable or
  disable tracing for a particular request. In addition, the single 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-b3"" rel=""nofollow noreferrer"">b3</a>
  header propagation format is supported, which is a more compressed
  format.</p></li>
  <li><p>When using the Datadog tracer, Envoy relies on the service to propagate the Datadog-specific HTTP headers ( 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-datadog-trace-id"" rel=""nofollow noreferrer"">x-datadog-trace-id</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-datadog-parent-id"" rel=""nofollow noreferrer"">x-datadog-parent-id</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-datadog-sampling-priority"" rel=""nofollow noreferrer"">x-datadog-sampling-priority</a>).</p></li>
  </ul>
</blockquote>

<hr>

<p>TLDR: traceId headers need to be manually added to B3 HTTP headers.</p>

<p>Additional information: <a href=""https://github.com/openzipkin/b3-propagation"" rel=""nofollow noreferrer"">https://github.com/openzipkin/b3-propagation</a></p>
"
Jaeger,58269994,58228451,2,"2019/10/07, 15:54:44",True,"2019/10/07, 15:54:44",6049,3156333,1,"<p>If you have sampling rate set to 1% then error will be seen in Jaeger once it occurs 100 times.
This is mentioned at <a href=""https://istio.io/docs/tasks/telemetry/distributed-tracing/jaeger/"" rel=""nofollow noreferrer"">Distributed Tracing - Jaeger</a>:</p>

<blockquote>
  <p>To see trace data, you must send requests to your service. The number of requests depends on Istio’s sampling rate. You set this rate when you install Istio. The default sampling rate is 1%. You need to send at least 100 requests before the first trace is visible. To send a 100 requests to the  <code>productpage</code>  service, use the following command:</p>
  
  <p><code>$ for i in  `seq 1 100`;  do  curl -s -o /dev/null http://$GATEWAY_URL/productpage;  done</code></p>
</blockquote>

<p>If you are not seeing the error in the current sample, I would advice make the sample higher.</p>

<p>You can read about <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/intro/arch_overview/tracing#trace-context-propagation"" rel=""nofollow noreferrer"">Tracing context propagation</a> which is being done by <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/"" rel=""nofollow noreferrer"">Envoy</a>.
Envoy automatically sends spans to tracing collectors</p>

<blockquote>
  <p>Alternatively the trace context can be manually propagated by the service:</p>
  
  <ul>
  <li>When using the LightStep tracer, Envoy relies on the service to propagate the  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-ot-span-context"" rel=""nofollow noreferrer"">x-ot-span-context</a>  HTTP header while sending HTTP requests to other services.</li>
  <li>When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers (  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-b3-traceid"" rel=""nofollow noreferrer"">x-b3-traceid</a>,  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-b3-spanid"" rel=""nofollow noreferrer"">x-b3-spanid</a>,  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-b3-parentspanid"" rel=""nofollow noreferrer"">x-b3-parentspanid</a>,  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-b3-sampled"" rel=""nofollow noreferrer"">x-b3-sampled</a>, and  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-b3-flags"" rel=""nofollow noreferrer"">x-b3-flags</a>). The  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-b3-sampled"" rel=""nofollow noreferrer"">x-b3-sampled</a>  header can also be supplied by an external client to either enable or disable tracing for a particular request. In addition, the single  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-b3"" rel=""nofollow noreferrer"">b3</a>  header propagation format is supported, which is a more compressed format.</li>
  <li>When using the Datadog tracer, Envoy relies on the service to propagate the Datadog-specific HTTP headers (  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-datadog-trace-id"" rel=""nofollow noreferrer"">x-datadog-trace-id</a>,  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-datadog-parent-id"" rel=""nofollow noreferrer"">x-datadog-parent-id</a>,  <a href=""https://www.envoyproxy.io/docs/envoy/v1.10.0/configuration/http_conn_man/headers#config-http-conn-man-headers-x-datadog-sampling-priority"" rel=""nofollow noreferrer"">x-datadog-sampling-priority</a>).</li>
  </ul>
</blockquote>
"
Jaeger,57918375,57913923,1,"2019/09/13, 09:31:04",True,"2019/09/13, 09:31:04",269,12020494,6,"<p>Just mentioning beforehand (you might already know) that a Kubernetes Service is not a ""service"" as in a piece of code. It is a way for Kubernetes components &amp; deployments to communicate with one another through an interface which always stays the same, regardless of how many pods or servers there are. </p>

<p>When Istio deploys it's tracing mechanism, it deploys modular parts so it can deploy them independently, and also scale them independently, very much like micro-services. </p>

<p>Generally a Kubernetes deployed utility will be deployed as a few parts which make up the bigger picture. For instance in your case: </p>

<p>jaeger-agent - This is the components which will collect all the traffic and tracing from your nodes.</p>

<p>jaeger-collector - This is the place where all of the jaeger-agents will push the logs and traces they find on the node, and the collector will aggregate these as a trace may span multiple nodes. </p>

<p>tracing - might be the component which injects the tracing ID's into network traffic for the agent to watch.</p>

<p>zipkin - could be the UI which allows debugging with traces, or replaying requests etc. </p>

<p>The above might not be absolutely correct, but I hope you get the idea of why multiple parts would be deployed. </p>

<p>In the same way we deploy mysql, and our containers separately, Kubernetes projects are generally deployed as a set of deployments or pods. </p>
"
Jaeger,58058841,57913923,0,"2019/09/23, 11:52:53",False,"2019/09/23, 11:52:53",13313,524946,1,"<p>To complement @christiaan-vermeulen's answer: the <code>tracing</code> service is Jaeger's UI (jaeger-query) so that the same URL can be used for alternative backends, whereas the Zipkin service is a convenience service, allowing applications using Zipkin tracers (like Brave) to send data to Jaeger without requiring complex changes. If you look closely, the Zipkin service is backed by the jaeger-collector as well.</p>
"
Jaeger,57474533,57473932,2,"2019/08/13, 12:10:40",True,"2019/08/13, 12:10:40",246,10967175,0,"<p>I hope you have followed the official documentation of the jager with istio.</p>

<p>If you are using the helm chart make the following changes required. </p>

<pre><code>In main values.yaml file
tracing:
  enabled: true

In tracing/values.yaml
provider: jaeger
</code></pre>

<p>Export the dashboard via Kube port-forward or ingress. </p>

<p>Official Documentation.
<a href=""https://istio.io/docs/tasks/telemetry/distributed-tracing/jaeger/"" rel=""nofollow noreferrer"">https://istio.io/docs/tasks/telemetry/distributed-tracing/jaeger/</a></p>

<p>NOTE: The important thing by default jaeger will trace something like 0.1% request i.e. 1 request out of 100 so put a lot of requests only then you can see a trace in UI. </p>
"
Jaeger,57397006,57380205,0,"2019/08/07, 17:43:53",False,"2019/08/07, 17:43:53",527,3966540,0,"<p>I had a wrong opencensus collector configuration.
The docker container network cannot see port 9411 as it was on the host network. I was able to fix the issue after noticing this misconfiguration.</p>
"
Jaeger,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313,524946,0,"<p>OpenTracing does not define the concrete data model, how the data should be collected, nor how it should be transported. As such, there's no specification for the endpoint. This allows implementations like Jaeger to use non-HTTP transport by default when sending data from the client (tracer) to the backend, by sending UDP packets to an intermediate ""Jaeger Agent"".</p>

<p>Given that the base model is pretty much similar among implementations, it's common to have tracing solutions to support each other's endpoints. For instance, Jaeger is able to expose an endpoint with <a href=""https://www.jaegertracing.io/docs/1.13/getting-started/#migrating-from-zipkin"" rel=""nofollow noreferrer"">Zipkin compatibility</a>.</p>

<p>Based on your question, I think you might be interested in the OpenTelemetry project, a successor to the OpenTracing project, as the result of a merge with the OpenCensus project. OpenTelemetry provides its own tracer and is able to ""receive"" data in several formats (including Jaeger), and ""export"" to several backends.</p>
"
Jaeger,56462471,56202494,2,"2019/06/05, 17:23:36",True,"2019/06/05, 17:23:36",3681,11300382,2,"<p>Assuming that your services are  defined in Istio’s internal service registry. If not please configure it according to instruction <a href=""https://istio.io/docs/tasks/traffic-management/egress/"" rel=""nofollow noreferrer""><code>service-defining</code></a>.</p>

<p>In HTTPS all the HTTP-related information like method, URL path, response code, is encrypted so Istio <strong>cannot</strong> see and cannot monitor that information for HTTPS. 
If you need to monitor HTTP-related information in access to external HTTPS services, you may want to let your applications issue HTTP requests and configure Istio to perform TLS origination.</p>

<p>First you have to <strong>redefine</strong> your ServiceEntry and create VirtualService  to rewrite the HTTP request port and add a DestinationRule to perform TLS origination.</p>

<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: external-service1
spec:
  hosts:
  - external-service1.com
  ports:
  - number: 80
    name: http-port
    protocol: HTTP
  - number: 443
    name: http-port-for-tls-origination
    protocol: HTTP
  resolution: DNS
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: external-service1
spec:
  hosts:
  - external-service1.com
  http:
  - match:
    - port: 80
    route:
    - destination:
        host: external-service1.com
        port:
          number: 443
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: external-service1
spec:
  host: external-service1.com
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
    portLevelSettings:
    - port:
        number: 443
      tls:
        mode: SIMPLE # initiates HTTPS when accessing external-service1.com
EOF
</code></pre>

<p>The VirtualService redirects HTTP requests on port 80 to port 443 where the corresponding DestinationRule then performs the TLS origination. Unlike the previous ServiceEntry, this time the protocol on port 443 is HTTP, instead of HTTPS, because clients will only send HTTP requests and Istio will upgrade the connection to HTTPS.</p>

<p>I hope it helps.</p>
"
Jaeger,54367053,54078937,0,"2019/01/25, 16:16:32",True,"2019/01/25, 16:16:32",13313,524946,2,"<p>Note that tracing data (spans) are not the same as ""metrics"", although there could be some overlap in some cases. I recommend the following blog post on what is the purpose of each, including logging:</p>

<p><a href=""https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html"" rel=""nofollow noreferrer"">https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html</a></p>

<p>That said, there is the OpenTracing library mentioned in the blog post you linked, called <a href=""https://github.com/opentracing-contrib/java-metrics"" rel=""nofollow noreferrer"">opentracing-contrib/java-metrics</a>. It allows you to pick specific spans and record them as data points (metrics). It works as a decorator of a concrete tracer, so, your spans would reach a concrete backend like Jaeger and, additionally, create data points based on the configured spans. The data points are then reported via <a href=""https://micrometer.io/"" rel=""nofollow noreferrer"">Micrometer</a>, which can be configured to expose this data in Prometheus format.</p>

<blockquote>
  <p>The problem is, I haven't found any consistent solution to do this, all the examples that I have found so far are outdated, deprecated or lacks documentation.</p>
</blockquote>

<p>Please, open an issue on the <code>java-metrics</code> repository with the problems you are facing.</p>
"
Jaeger,53600710,53095177,1,"2018/12/03, 21:38:34",True,"2018/12/03, 21:38:34",467,1404502,1,"<p>Just to close this question out for the solution to the problem in my instance.  The mistake in configuration started all the way back in the Kubernetes cluster initialisation.  I had applied:</p>

<pre><code>kubeadm init --pod-network-cidr=n.n.n.n/n --apiserver-advertise-address 0.0.0.0
</code></pre>

<p>the pod-network-cidr using the same address range as the local LAN on which the Kubernetes installation was deployed i.e. the desktop for the Ubuntu host used the same IP subnet as what I'd assigned the container network.</p>

<p>For the most part, everything operated fine as detailed above, until the Istio proxy was trying to route packets from an external load-balancer IP address to an internal IP address which happened to be on the same subnet.  Project Calico with Kubernetes seemed to be able to cope with it as that's effectively Layer 3/4 policy but Istio had a problem with it a L7 (even though it was sitting on Calico underneath).</p>

<p>The solution was to tear down my entire Kubernetes deployment.  I was paranoid and went so far as to uninstall Kubernetes and deploy again and redeploy with a pod network in the 172 range which wasn't anything to do with my local lan.  I also made the same changes in the Project Calico configuration file to match pod networks.  After that change, everything worked as expected.</p>

<p>I suspect that in a more public configuration where your cluster was directly attached to a BGP router as opposed to using MetalLB with an L2 configuration as a subset of your LAN wouldn't exhibit this issue either.  I've documented it more in this post:</p>

<p><a href=""https://blooprynt.io/blog/2018/11/12/start-to-finish-net-containers-deployed-in-on-premise-load-balanced-kubernetes-with-istio-mesh"" rel=""nofollow noreferrer"">Microservices: .Net, Linux, Kubernetes and Istio make a powerful combination</a></p>
"
Jaeger,17618211,17616323,0,"2013/07/12, 18:12:11",False,"2013/07/12, 18:12:11",67,1278255,0,"<p>Actually, there is no error. The code was changing the color of a yellow texture to a red tint inline.</p>
"
Jaeger,12318996,12318639,3,"2012/09/07, 16:23:31",True,"2012/09/07, 16:23:31",7480,1566232,1,"<p>Try this : (if you can gives us all the variants of the url it would be better)</p>

<pre><code>RewriteRule ^(.*)-by-([^/]+)/(.*)$ /$1/$3?brand=$2 [L,QSA]
</code></pre>
"
Jaeger,54372644,54371809,0,"2019/01/25, 22:56:14",False,"2019/01/25, 22:56:14",324,5600810,0,"<p>Liberty does not have Open Tracing Tracer implementation for Jaeger yet.  We have a sample Tracer implementation for Zipkin.  You can find it at <a href=""https://github.com/WASdev/sample.opentracing.zipkintracer"" rel=""nofollow noreferrer"">https://github.com/WASdev/sample.opentracing.zipkintracer</a>.  Jaegar claims it is backward compatible with Zipkin by accepting spans in Zipkin formats over HTTP.</p>

<p>Feel free to open a RFE at <a href=""https://developer.ibm.com/wasdev/help/submit-rfe/"" rel=""nofollow noreferrer"">https://developer.ibm.com/wasdev/help/submit-rfe/</a></p>
"
Jaeger,54075526,54061611,0,"2019/01/07, 15:41:12",True,"2019/01/07, 15:41:12",213,7933630,2,"<p>Unlike Jaeger, LightStep is a commercial SaaS offering. If you wanted to try out their service, you'd need to contact their sales team.</p>
"
Jaeger,49580258,49571999,0,"2018/03/30, 22:27:46",False,"2018/03/30, 22:27:46",1,9573945,0,"<p>Managed to create the desired capturing groups:</p>

<pre><code>r'\(\'(\d+)\'\,\s\'(.*?\“|.*?\:)(.*?\”|.*?\.)(.*?\')\)'
</code></pre>

<p>Then I could write out the files, it looks correct as for these few occurences.</p>
"
Jaeger,52894461,52881039,0,"2018/10/19, 17:29:25",False,"2018/10/19, 17:29:25",13313,524946,0,"<p>This is most likely caused by the static assets <em>not</em> being included in the binary. You can try that out by running the binary you compiled.</p>

<p>Instead of compiling on your own, a better approach would be to get the official binaries from the releases page and build your Docker container using that.</p>

<p><a href=""https://github.com/jaegertracing/jaeger/releases/latest"" rel=""nofollow noreferrer"">https://github.com/jaegertracing/jaeger/releases/latest</a></p>
"
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479,715518,0,"<p>You need to add instrumentation rules for your application to ""dig deeper"". The <code>doFilter</code> and <code>service</code> methods are instrumented by default as part of the HTTP instrumentation profile. In addition to the HTTP profile, by default inspectIT instruments SQL statement executions, other entry points into your application (like JMS), the places where external HTTP/JMS calls are done and Java Executors. There are some other common instrumentation profiles (Hibernate, SQL parameters, Spring, etc), but you need to enable them by yourself.</p>

<p>Usually, you would start with the default settings and then, in addition, add instrumentation rules related to your application.</p>
"
InspectIT,54902800,52150469,0,"2019/02/27, 12:03:53",False,"2019/02/27, 12:03:53",479,715518,0,"<p>You need to add instrumentation rules for your application to ""dig deeper"". The <code>doFilter</code> and <code>service</code> methods are instrumented by default as part of the HTTP instrumentation profile. In addition to the HTTP profile, by default inspectIT instruments SQL statement executions, other entry points into your application (like JMS), the places where external HTTP/JMS calls are done and Java Executors. There are some other common instrumentation profiles (Hibernate, SQL parameters, Spring, etc), but you need to enable them by yourself.</p>

<p>Usually, you would start with the default settings and then, in addition, add instrumentation rules related to your application.</p>
"
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930,208809,3,"<p>Disclaimer: I work for Instana.</p>

<p>There is not much to setup. Instana provides out-of-the-box support for Kafka and Zookeeper nodes. So all you need to do is to install the Instana agent on the server(s) you want to monitor. It will automatically detect your Kafka and Zookeeper installations and start reporting metrics for them to your tenant unit. If you don't have a tenant unit yet, you can register for a free trial at <a href=""https://www.instana.com/trial/"" rel=""nofollow noreferrer"">https://www.instana.com/trial/</a> or contact Sales.</p>

<p>If you need additional help, I suggest to open a ticket at <a href=""https://instana.zendesk.com"" rel=""nofollow noreferrer"">https://instana.zendesk.com</a> to get dedicated support. </p>
"
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465,469028,3,"<p>Instana has a <a href=""https://github.com/instana/website-monitoring-examples/tree/master/examples/defining-pages/angular-router/src"" rel=""nofollow noreferrer"">demo application</a> that shows to do this.</p>

<p>To summarize the parts that you would need:</p>

<ol>
<li>Ensure that you have a local directory configured within your TypeScript config that allows definition of custom typings:</li>
</ol>

<pre><code>// common file name: tsconfig.app.json
{
  // other configuration…
  ""typeRoots"": [
    // other configuration…
    ""./custom-typings""
  ]
}
</code></pre>

<ol start=""2"">
<li>Declare the global <code>ineum</code> function in a file within this directory:</li>
</ol>

<pre class=""lang-js prettyprint-override""><code>// common file name globals.d.ts
declare function ineum(s: string, ...parameters: any[]): any;
</code></pre>

<p>The combination of these two steps will make TypeScript aware of the function. Now you can use <code>ineum</code> just like any other global. </p>
"
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628,1847951,2,"<p>Instana will use the same protocol to make the sourcemap request. The documentation example uses http, but it will work with https the same way. 
The most likely reason for your problem is that the sourcemap is not readable from the public internet. In your case, the sourcemap file requires http session authentication and redirects to a login page.</p>
"
Instana,60976067,57292600,0,"2020/04/01, 19:46:26",False,"2020/04/01, 19:46:26",121,2001962,1,"<p>You could remove the location /nginx_status in that server, and add a new server section like this:</p>

<pre><code>server {
    listen 127.0.0.1:80;
    server_name 127.0.0.1;

    # Required by Instana
    location /nginx_status {
        stub_status on;
        access_log off;
    }
}
</code></pre>
"
Instana,58527533,58518001,2,"2019/10/23, 19:43:00",False,"2019/10/23, 19:43:00",51,10645824,3,"<p>That endpoint requires a POST, it appears you are using GET. Hence method not allowed.</p>
"
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11,14562868,1,"<p>Instana offers an agent tailored to React native, which simplifies the integration. The <a href=""https://www.instana.com/docs/mobile_app_monitoring/#react-native"" rel=""nofollow noreferrer"">React native agent</a> is different than the one used for website monitoring.</p>
<p>You can get started with React native monitoring by creating a mobile app within Instana's user interface under <code>Websites &amp; Mobile Apps -&gt; Mobile Apps</code>. For the React native agent you can find <a href=""https://www.instana.com/docs/mobile_app_monitoring/#react-native"" rel=""nofollow noreferrer"">dedicated documentation and installation instructions</a> on Instana's documentation site.</p>
<p>For further questions and support, I suggest leveraging <a href=""https://support.instana.com/hc/en-us/requests/new"" rel=""nofollow noreferrer"">Instana's support portal</a>.</p>
"
Instana,57372845,57354975,0,"2019/08/06, 12:22:05",False,"2019/08/06, 12:22:05",1628,1847951,1,"<p>the Instana repository has been upgraded to support Disco Dingo as well.</p>
"
Instana,55646922,55606472,0,"2019/04/12, 10:53:10",False,"2019/04/12, 10:53:10",294930,208809,3,"<p>Disclaimer: I work for Instana.</p>

<p>There is not much to setup. Instana provides out-of-the-box support for Kafka and Zookeeper nodes. So all you need to do is to install the Instana agent on the server(s) you want to monitor. It will automatically detect your Kafka and Zookeeper installations and start reporting metrics for them to your tenant unit. If you don't have a tenant unit yet, you can register for a free trial at <a href=""https://www.instana.com/trial/"" rel=""nofollow noreferrer"">https://www.instana.com/trial/</a> or contact Sales.</p>

<p>If you need additional help, I suggest to open a ticket at <a href=""https://instana.zendesk.com"" rel=""nofollow noreferrer"">https://instana.zendesk.com</a> to get dedicated support. </p>
"
Instana,59945966,59921305,1,"2020/01/28, 11:46:22",False,"2020/01/28, 14:25:45",1628,1847951,2,"<p>Instana will use the same protocol to make the sourcemap request. The documentation example uses http, but it will work with https the same way. 
The most likely reason for your problem is that the sourcemap is not readable from the public internet. In your case, the sourcemap file requires http session authentication and redirects to a login page.</p>
"
Instana,57088646,55974537,0,"2019/07/18, 09:53:32",True,"2019/07/18, 09:53:32",465,469028,3,"<p>Instana has a <a href=""https://github.com/instana/website-monitoring-examples/tree/master/examples/defining-pages/angular-router/src"" rel=""nofollow noreferrer"">demo application</a> that shows to do this.</p>

<p>To summarize the parts that you would need:</p>

<ol>
<li>Ensure that you have a local directory configured within your TypeScript config that allows definition of custom typings:</li>
</ol>

<pre><code>// common file name: tsconfig.app.json
{
  // other configuration…
  ""typeRoots"": [
    // other configuration…
    ""./custom-typings""
  ]
}
</code></pre>

<ol start=""2"">
<li>Declare the global <code>ineum</code> function in a file within this directory:</li>
</ol>

<pre class=""lang-js prettyprint-override""><code>// common file name globals.d.ts
declare function ineum(s: string, ...parameters: any[]): any;
</code></pre>

<p>The combination of these two steps will make TypeScript aware of the function. Now you can use <code>ineum</code> just like any other global. </p>
"
Instana,64642758,64613844,0,"2020/11/02, 11:18:54",False,"2020/11/02, 11:18:54",11,14562868,1,"<p>Instana offers an agent tailored to React native, which simplifies the integration. The <a href=""https://www.instana.com/docs/mobile_app_monitoring/#react-native"" rel=""nofollow noreferrer"">React native agent</a> is different than the one used for website monitoring.</p>
<p>You can get started with React native monitoring by creating a mobile app within Instana's user interface under <code>Websites &amp; Mobile Apps -&gt; Mobile Apps</code>. For the React native agent you can find <a href=""https://www.instana.com/docs/mobile_app_monitoring/#react-native"" rel=""nofollow noreferrer"">dedicated documentation and installation instructions</a> on Instana's documentation site.</p>
<p>For further questions and support, I suggest leveraging <a href=""https://support.instana.com/hc/en-us/requests/new"" rel=""nofollow noreferrer"">Instana's support portal</a>.</p>
"
Instana,60976067,57292600,0,"2020/04/01, 19:46:26",False,"2020/04/01, 19:46:26",121,2001962,1,"<p>You could remove the location /nginx_status in that server, and add a new server section like this:</p>

<pre><code>server {
    listen 127.0.0.1:80;
    server_name 127.0.0.1;

    # Required by Instana
    location /nginx_status {
        stub_status on;
        access_log off;
    }
}
</code></pre>
"
Instana,58527533,58518001,2,"2019/10/23, 19:43:00",False,"2019/10/23, 19:43:00",51,10645824,3,"<p>That endpoint requires a POST, it appears you are using GET. Hence method not allowed.</p>
"
Instana,62222959,62075759,0,"2020/06/05, 22:35:52",True,"2020/06/05, 22:35:52",4812,4828463,1,"<p>To whoever removed his/her answer: It was a correct answer. I don't know why you deleted it. Anyhow, I am posting again in case someone stumbles here.</p>

<p>You can control frequency and time by using <code>INSTANA_AGENT_UPDATES_FREQUENCY</code> and <code>INSTANA_AGENT_UPDATES_TIME</code> environment variables. </p>

<p>Updating <code>mode</code> via env variable is still unknown at this point.</p>

<p>Look at this page for more info: <a href=""https://www.instana.com/docs/setup_and_manage/host_agent/on/docker/#updates-and-version-pinning"" rel=""nofollow noreferrer"">https://www.instana.com/docs/setup_and_manage/host_agent/on/docker/#updates-and-version-pinning</a></p>
"
Instana,64377786,62075759,1,"2020/10/15, 21:37:15",False,"2020/10/19, 07:30:18",11,13617981,1,"<p>Most agent settings that one may want to change quickly are available as environment variables, see <a href=""https://www.instana.com/docs/setup_and_manage/host_agent/on/docker"" rel=""nofollow noreferrer"">https://www.instana.com/docs/setup_and_manage/host_agent/on/docker</a>. For example, setting the mode via environment variable is supported as well with <code>INSTANA_AGENT_MODE</code>, see e.g., <a href=""https://hub.docker.com/r/instana/agent"" rel=""nofollow noreferrer"">https://hub.docker.com/r/instana/agent</a>. The valid values are:</p>
<ul>
<li><code>APM</code>: the default, the agent monitors everything</li>
<li><code>INFRASTRUCTURE</code>: the agent will collect metrics and entities but not traces</li>
<li><code>OFF</code>: agent runs but collects no telemetry</li>
<li><code>AWS</code>: agent will collect data about AWS managed services in a region and an account, supported on EC2 and Fargate, and with some extra configurations, on hosts outside AWS</li>
</ul>
<p>On Kubernetes, it is also of course possible to use a ConfigMap to override files in the agent container.</p>
"
Instana,57372845,57354975,0,"2019/08/06, 12:22:05",False,"2019/08/06, 12:22:05",1628,1847951,1,"<p>the Instana repository has been upgraded to support Disco Dingo as well.</p>
"
Instana,57736812,57580097,0,"2019/08/31, 12:35:09",True,"2019/08/31, 12:35:09",49,10826472,0,"<p>Solved. Added flags to my run configuration, and increase XMS and XMX twice.</p>
"
Instana,41056601,41053682,3,"2016/12/09, 10:58:46",False,"2016/12/09, 10:58:46",8454,958529,0,"<p><code>pod install</code> is <code>cocoapods</code> command, not part of <code>ruby</code> or <code>gem</code>. The error means there is no <code>pod</code> or <code>install</code> package in <code>ruby</code> package repository. After writing a proper <code>Podfile</code> in your Xcode project dir, just run <code>pod install</code>.</p>
"
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592,1359408,1,"<p>[Disclaimer: I work at <a href=""http://www.lightstep.com"" rel=""nofollow noreferrer"">LightStep]</a></p>

<p>Sorry you're having trouble getting Java and Go to play well together.  I suspect this is caused by time-correction being enabled in Java but not being used in Go.  </p>

<p>You can disable time correction in Java using the <code>withClockSkewCorrection(boolean clockCorrection)</code> 
option to turn off clockCorrection when passing in options to the LightStep tracer</p>

<p>Here is the updated <a href=""https://github.com/lightstep/lightstep-tracer-java#disabling-default-clock-correction"" rel=""nofollow noreferrer"">README</a> and a link to the <a href=""https://github.com/lightstep/lightstep-tracer-java/blob/271ab07d6757c160273d37d3564ea2e8698aa38e/common/src/main/java/com/lightstep/tracer/shared/Options.java#L289"" rel=""nofollow noreferrer"">option code</a></p>

<p>If you contact us via the [Support] button in LightStep, we should be able to get you sorted out.  Please send us a note so that we can confirm that this is solved for you.   </p>

<p>We'll start monitoring SO more carefully so that we catch these things earlier.  </p>

<p>Thanks and happy tracing!  </p>

<p>Will</p>
"
LightStep,63725459,63724073,2,"2020/09/03, 17:13:18",True,"2020/09/03, 17:13:18",91,1548178,2,"<p><code>lightstep-opentelemetry-launcher-node</code> basically bundles the required things for you for easier configuration so this is not an exporter. If you were to simply replace the &quot;LightstepExporter&quot; with &quot;OpenTelemetry Collector Exporter&quot; in your code you can simply do this</p>
<pre><code>  import { CollectorTraceExporter } from '@opentelemetry/exporter-collector';

  tracerProvider.addSpanProcessor(
    new BatchSpanProcessor(
      new CollectorTraceExporter({
        url: 'YOUR_DIGEST_URL',
        headers: {
          'Lightstep-Access-Token': 'YOUR_TOKEN'
        }
      })
    )
  );
</code></pre>
<p>The default <code>YOUR_DIGETS_URL</code> from <a href=""https://github.com/lightstep/otel-launcher-node/"" rel=""nofollow noreferrer"">lightstep/otel-launcher-node</a> is <code>https://ingest.lightstep.com:443/api/v2/otel/trace</code></p>
"
LightStep,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313,524946,0,"<p>OpenTracing does not define the concrete data model, how the data should be collected, nor how it should be transported. As such, there's no specification for the endpoint. This allows implementations like Jaeger to use non-HTTP transport by default when sending data from the client (tracer) to the backend, by sending UDP packets to an intermediate ""Jaeger Agent"".</p>

<p>Given that the base model is pretty much similar among implementations, it's common to have tracing solutions to support each other's endpoints. For instance, Jaeger is able to expose an endpoint with <a href=""https://www.jaegertracing.io/docs/1.13/getting-started/#migrating-from-zipkin"" rel=""nofollow noreferrer"">Zipkin compatibility</a>.</p>

<p>Based on your question, I think you might be interested in the OpenTelemetry project, a successor to the OpenTracing project, as the result of a merge with the OpenCensus project. OpenTelemetry provides its own tracer and is able to ""receive"" data in several formats (including Jaeger), and ""export"" to several backends.</p>
"
LightStep,63725459,63724073,2,"2020/09/03, 17:13:18",True,"2020/09/03, 17:13:18",91,1548178,2,"<p><code>lightstep-opentelemetry-launcher-node</code> basically bundles the required things for you for easier configuration so this is not an exporter. If you were to simply replace the &quot;LightstepExporter&quot; with &quot;OpenTelemetry Collector Exporter&quot; in your code you can simply do this</p>
<pre><code>  import { CollectorTraceExporter } from '@opentelemetry/exporter-collector';

  tracerProvider.addSpanProcessor(
    new BatchSpanProcessor(
      new CollectorTraceExporter({
        url: 'YOUR_DIGEST_URL',
        headers: {
          'Lightstep-Access-Token': 'YOUR_TOKEN'
        }
      })
    )
  );
</code></pre>
<p>The default <code>YOUR_DIGETS_URL</code> from <a href=""https://github.com/lightstep/otel-launcher-node/"" rel=""nofollow noreferrer"">lightstep/otel-launcher-node</a> is <code>https://ingest.lightstep.com:443/api/v2/otel/trace</code></p>
"
LightStep,44529216,43796314,1,"2017/06/13, 21:43:41",True,"2017/06/13, 23:47:13",592,1359408,1,"<p>[Disclaimer: I work at <a href=""http://www.lightstep.com"" rel=""nofollow noreferrer"">LightStep]</a></p>

<p>Sorry you're having trouble getting Java and Go to play well together.  I suspect this is caused by time-correction being enabled in Java but not being used in Go.  </p>

<p>You can disable time correction in Java using the <code>withClockSkewCorrection(boolean clockCorrection)</code> 
option to turn off clockCorrection when passing in options to the LightStep tracer</p>

<p>Here is the updated <a href=""https://github.com/lightstep/lightstep-tracer-java#disabling-default-clock-correction"" rel=""nofollow noreferrer"">README</a> and a link to the <a href=""https://github.com/lightstep/lightstep-tracer-java/blob/271ab07d6757c160273d37d3564ea2e8698aa38e/common/src/main/java/com/lightstep/tracer/shared/Options.java#L289"" rel=""nofollow noreferrer"">option code</a></p>

<p>If you contact us via the [Support] button in LightStep, we should be able to get you sorted out.  Please send us a note so that we can confirm that this is solved for you.   </p>

<p>We'll start monitoring SO more carefully so that we catch these things earlier.  </p>

<p>Thanks and happy tracing!  </p>

<p>Will</p>
"
LightStep,49949656,49949470,0,"2018/04/21, 00:00:46",False,"2018/04/21, 00:00:46",8336,1773866,1,"<p>If you go to the latest snapshot documentation (or milestone) and you search for the word OpenTracing, you would get your answer. It's here <a href=""https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html#_opentracing"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html#_opentracing</a></p>

<blockquote>
  <p>Spring Cloud Sleuth is compatible with OpenTracing. If you have OpenTracing on the classpath, we automatically register the OpenTracing Tracer bean. If you wish to disable this, set <code>spring.sleuth.opentracing.enabled</code> to false</p>
</blockquote>

<p>So it's enough to just have OpenTracing on the classpath and Sleuth will work out of the box</p>
"
SkyWalking,58586629,58314080,0,"2019/10/28, 08:40:49",True,"2019/10/28, 08:40:49",395,9571426,0,"<p>Explanation how to set up <code>skywalking</code> properly:
<a href=""https://github.com/apache/skywalking/issues/3589#issuecomment-543268029"" rel=""nofollow noreferrer"">https://github.com/apache/skywalking/issues/3589#issuecomment-543268029</a></p>
"
SkyWalking,62389599,60476903,0,"2020/06/15, 16:48:11",False,"2020/06/15, 16:48:11",6436,2628868,0,"<p>It is the dashboard default time filter value problem, the time range did not contains data:</p>

<pre><code>curl 'https://skywalking.example.net/graphql' \
  -H 'authority: skywalking.example.net' \
  -H 'pragma: no-cache' \
  -H 'cache-control: no-cache' \
  -H 'accept: application/json, text/plain, */*' \
  -H 'dnt: 1' \
  -H 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36' \
  -H 'content-type: application/json;charset=UTF-8' \
  -H 'origin: https://skywalking.example.net' \
  -H 'sec-fetch-site: same-origin' \
  -H 'sec-fetch-mode: cors' \
  -H 'sec-fetch-dest: empty' \
  -H 'referer: https://skywalking.example.net/' \
  -H 'accept-language: en,zh-CN;q=0.9,zh;q=0.8,zh-TW;q=0.7,fr;q=0.6' \
  --data-binary $'{""query"":""query queryServices($duration: Duration\u0021) {\\n    services: getAllServices(duration: $duration) {\\n      key: id\\n      label: name\\n    }\\n  }"",""variables"":{""duration"":{""start"":""2020-06-03 1015"",""end"":""2020-06-23 1030"",""step"":""MINUTE""}}}' \
  --compressed
</code></pre>

<p>change the time start and end to having collection data area.</p>
"
SkyWalking,62261701,62261285,0,"2020/06/08, 15:06:04",True,"2020/06/08, 15:06:04",6038,11032044,1,"<ol>
<li>Clone the repository to your machine,</li>
</ol>

<pre><code>$ git clone git@github.com:apache/skywalking-kubernetes.git
</code></pre>

<ol start=""2"">
<li>Go to <code>Chart</code> directory, </li>
</ol>

<pre><code>$ cd skywalking-kubernetes/chart
</code></pre>

<ol start=""3"">
<li>Now perform the following commands (from <code>chart</code> dir),</li>
</ol>

<pre class=""lang-sh prettyprint-override""><code>$ helm repo add elastic https://helm.elastic.co

$ helm dep up skywalking

$ helm install &lt;release_name&gt; skywalking -n &lt;namespace&gt; 
</code></pre>
"
SkyWalking,62358989,62358639,0,"2020/06/13, 14:19:26",True,"2020/06/13, 14:19:26",6436,2628868,1,"<p>Finally I build the side car image by myself:</p>

<pre><code>wget https://www.apache.org/dyn/closer.cgi/skywalking/7.0.0/apache-skywalking-apm-7.0.0.tar.gz &amp;&amp; tar -zxvf apache-skywalking-apm-7.0.0.tar.gz
</code></pre>

<p>this is the docker file:</p>

<pre><code>FROM busybox:latest 

ENV LANG=C.UTF-8

RUN set -eux &amp;&amp; mkdir -p /usr/skywalking/agent/

ADD apache-skywalking-apm-bin/agent/ /usr/skywalking/agent/

WORKDIR /
</code></pre>
"
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293,225016,3,"<blockquote>
<p>how to add the jdbc driver jar into the image file?</p>
</blockquote>
<p>One way would be an <code>initContainer:</code> and then artificially inject the jdbc driver via <a href=""https://wiki.openjdk.java.net/display/mlvm/BootClassPath"" rel=""nofollow noreferrer""><code>-Xbootclasspath</code></a></p>
<pre class=""lang-yaml prettyprint-override""><code>initContainers:
- name: download
  image: busybox:latest
  command:
  - wget
  - -O
  - /foo/jdbc.jar
  - https://whatever-the-jdbc-url-jar-is-goes-here
  volumeMounts:
  - name: tmp
    mountPath: /foo
containers:
- env:
  - name: JAVA_OPTS
    value: -Xmx2g -Xbootclasspath/a:/foo/jdbc.jar
  volumeMounts:
  - name: tmp
    mountPath: /foo
volumes:
- name: tmp
  emptyDir: {}
</code></pre>
<p>a similar, although slightly riskier way, is to find a path that is already on the classpath of the image, and attempt to volume mount the jar path into that directory</p>
<p>All of this seems kind of moot given that your image looks like one that is custom built, and therefore the correct action is to update the <code>Dockerfile</code> for it to download the jar at build time</p>
"
SkyWalking,65282846,65259756,0,"2020/12/14, 04:23:10",True,"2020/12/14, 04:23:10",1070,5196039,0,"<p>I created a lifecycle that performs the delete action after a set time, and then I added this configuration to the skywalking application.yml under <code>storage.elasticsearch7</code>:</p>
<pre><code>advanced: ${SW_STORAGE_ES_ADVANCED:&quot;{\&quot;index.lifecycle.name\&quot;:\&quot;sw-policy\&quot;}&quot;}
</code></pre>
<p>SW creates index templates, and now I see that this is part of the template, and indeed the indexes have this sw-policy attached.</p>
"
SkyWalking,62261701,62261285,0,"2020/06/08, 15:06:04",True,"2020/06/08, 15:06:04",6038,11032044,1,"<ol>
<li>Clone the repository to your machine,</li>
</ol>

<pre><code>$ git clone git@github.com:apache/skywalking-kubernetes.git
</code></pre>

<ol start=""2"">
<li>Go to <code>Chart</code> directory, </li>
</ol>

<pre><code>$ cd skywalking-kubernetes/chart
</code></pre>

<ol start=""3"">
<li>Now perform the following commands (from <code>chart</code> dir),</li>
</ol>

<pre class=""lang-sh prettyprint-override""><code>$ helm repo add elastic https://helm.elastic.co

$ helm dep up skywalking

$ helm install &lt;release_name&gt; skywalking -n &lt;namespace&gt; 
</code></pre>
"
SkyWalking,62358989,62358639,0,"2020/06/13, 14:19:26",True,"2020/06/13, 14:19:26",6436,2628868,1,"<p>Finally I build the side car image by myself:</p>

<pre><code>wget https://www.apache.org/dyn/closer.cgi/skywalking/7.0.0/apache-skywalking-apm-7.0.0.tar.gz &amp;&amp; tar -zxvf apache-skywalking-apm-7.0.0.tar.gz
</code></pre>

<p>this is the docker file:</p>

<pre><code>FROM busybox:latest 

ENV LANG=C.UTF-8

RUN set -eux &amp;&amp; mkdir -p /usr/skywalking/agent/

ADD apache-skywalking-apm-bin/agent/ /usr/skywalking/agent/

WORKDIR /
</code></pre>
"
SkyWalking,62389599,60476903,0,"2020/06/15, 16:48:11",False,"2020/06/15, 16:48:11",6436,2628868,0,"<p>It is the dashboard default time filter value problem, the time range did not contains data:</p>

<pre><code>curl 'https://skywalking.example.net/graphql' \
  -H 'authority: skywalking.example.net' \
  -H 'pragma: no-cache' \
  -H 'cache-control: no-cache' \
  -H 'accept: application/json, text/plain, */*' \
  -H 'dnt: 1' \
  -H 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36' \
  -H 'content-type: application/json;charset=UTF-8' \
  -H 'origin: https://skywalking.example.net' \
  -H 'sec-fetch-site: same-origin' \
  -H 'sec-fetch-mode: cors' \
  -H 'sec-fetch-dest: empty' \
  -H 'referer: https://skywalking.example.net/' \
  -H 'accept-language: en,zh-CN;q=0.9,zh;q=0.8,zh-TW;q=0.7,fr;q=0.6' \
  --data-binary $'{""query"":""query queryServices($duration: Duration\u0021) {\\n    services: getAllServices(duration: $duration) {\\n      key: id\\n      label: name\\n    }\\n  }"",""variables"":{""duration"":{""start"":""2020-06-03 1015"",""end"":""2020-06-23 1030"",""step"":""MINUTE""}}}' \
  --compressed
</code></pre>

<p>change the time start and end to having collection data area.</p>
"
SkyWalking,58586629,58314080,0,"2019/10/28, 08:40:49",True,"2019/10/28, 08:40:49",395,9571426,0,"<p>Explanation how to set up <code>skywalking</code> properly:
<a href=""https://github.com/apache/skywalking/issues/3589#issuecomment-543268029"" rel=""nofollow noreferrer"">https://github.com/apache/skywalking/issues/3589#issuecomment-543268029</a></p>
"
SkyWalking,63516070,63509093,0,"2020/08/21, 06:24:43",True,"2020/08/21, 06:24:43",22293,225016,3,"<blockquote>
<p>how to add the jdbc driver jar into the image file?</p>
</blockquote>
<p>One way would be an <code>initContainer:</code> and then artificially inject the jdbc driver via <a href=""https://wiki.openjdk.java.net/display/mlvm/BootClassPath"" rel=""nofollow noreferrer""><code>-Xbootclasspath</code></a></p>
<pre class=""lang-yaml prettyprint-override""><code>initContainers:
- name: download
  image: busybox:latest
  command:
  - wget
  - -O
  - /foo/jdbc.jar
  - https://whatever-the-jdbc-url-jar-is-goes-here
  volumeMounts:
  - name: tmp
    mountPath: /foo
containers:
- env:
  - name: JAVA_OPTS
    value: -Xmx2g -Xbootclasspath/a:/foo/jdbc.jar
  volumeMounts:
  - name: tmp
    mountPath: /foo
volumes:
- name: tmp
  emptyDir: {}
</code></pre>
<p>a similar, although slightly riskier way, is to find a path that is already on the classpath of the image, and attempt to volume mount the jar path into that directory</p>
<p>All of this seems kind of moot given that your image looks like one that is custom built, and therefore the correct action is to update the <code>Dockerfile</code> for it to download the jar at build time</p>
"
SkyWalking,60472081,60465004,0,"2020/03/01, 06:50:19",False,"2020/03/01, 06:50:19",43078,78722,2,"<p>So you can see this more clearly in the output. The pod is Running but the Ready flag is false meaning the container is up but is failing the Readiness Probe.</p>
"
SkyWalking,60463565,60462786,0,"2020/02/29, 11:08:36",True,"2020/02/29, 11:08:36",33843,498256,2,"<p>So you should go throught this document first</p>

<p><a href=""https://kubernetes.io/docs/concepts/storage/volumes/#hostpath"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/storage/volumes/#hostpath</a></p>

<p>use <code>hostPath</code> as sample</p>

<pre><code>  volumes:
  - name: agent
    hostPath:
      # directory location on host
      path: /agent
      # this field is optional
      type: Directory
</code></pre>

<p>You need reference it for both init container and normal container.</p>
"
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936,1125055,1,"<p>Most of the metrics stagemonitor collects are not available via JMX. For example, response time statistics grouped by the endpoint of your application.</p>

<p>Also, stagemonitor is much more than just metrics. It is also a profiler, you can use to see which methods caused a request to be slow. Further more, it can (soon) do distributed tracing which helps you to analyze and debug latency problems in a microservice environment by correlating related requests. It also offers you a Kibana dashboard you can use to drill into the requests your application serves to find out about causes of errors or latency. Another use case is lightweight web analytics to identify which devices and operating systems your customers use to access your site.</p>
"
Stagemonitor,48148146,48147866,0,"2018/01/08, 12:12:45",False,"2018/01/08, 12:12:45",128,7445902,0,"<p>Two possible solutions. </p>

<p>You can have a button 'hide', that will hide the metrics using some javascript code. </p>

<p>Or in the same button you do the following:</p>

<pre><code>private boolean metricsOn = false;

@RequestMapping(value = ""/owners/test"", method = RequestMethod.GET)
public String testing(Map&lt;String, Object&gt; model) {

    if(!metricsOn){
metrics = true;

    Owner owner = new Owner();
    model.put(""owner"", owner);
    return VIEWS_OWNER_CREATE_OR_UPDATE_FORM;
} else{
//return without the given metrics?
metrics = false;
 return VIEWS_OWNER_CREATE_OR_UPDATE_FORM;
}
</code></pre>
"
Stagemonitor,30496430,30493074,0,"2015/05/28, 05:55:25",True,"2015/05/28, 05:55:25",74274,160313,1,"<p>It doesn't appear to be compatible with Grails. If you enable logging</p>

<pre><code>log4j.main = {
   error 'org.codehaus.groovy.grails',
         'org.springframework',
         'org.hibernate',
         'net.sf.ehcache.hibernate'
   info 'org.stagemonitor'
}
</code></pre>

<p>you'll see a bunch of error stacktraces that appear to imply that the way they're using Javassist to wire in tracing code isn't compatible with Groovy and/or the AST transformations that Grails uses:</p>

<pre><code>org.stagemonitor.javassist.NotFoundException: stagemonitortest.PersonController$setErrors
...
org.stagemonitor.javassist.NotFoundException: stagemonitortest.PersonController$getParams$0
...
org.stagemonitor.javassist.NotFoundException: stagemonitortest.Person$count
...
org.stagemonitor.javassist.NotFoundException: stagemonitortest.Person$list$0
...
org.stagemonitor.javassist.NotFoundException: stagemonitortest.PersonController$respond$1
</code></pre>
"
Stagemonitor,38195456,38184669,0,"2016/07/05, 07:59:13",True,"2017/02/13, 14:44:38",3568,1679544,1,"<p>Finally I found out how to disable the browser widget. </p>

<p>Set:</p>

<pre><code>stagemonitor.web.widget.enabled=false
</code></pre>

<p>You can see more information about it <a href=""https://github.com/stagemonitor/stagemonitor/wiki/Configuration-Options#in-browser-widget-enabled"" rel=""nofollow noreferrer"">here</a>.</p>
"
Stagemonitor,43811524,43806990,4,"2017/05/05, 21:23:27",False,"2017/05/05, 21:23:27",104968,1766831,0,"<p>MySQL does not provide anything more than how much data each tenant has.  That can be found in <code>information_schema</code>.</p>

<p>If you need CPU/IO/etc., you need to set up multiple instances of MySQL in VMs or cgroups and have the OS / VM-manager provide the data.  This will cost extra RAM, so it may not be worth it.</p>
"
Stagemonitor,48163586,48162254,0,"2018/01/09, 09:52:48",True,"2018/01/09, 09:52:48",4936,1125055,1,"<p>I'm afraid this is currently not possible. However, stagemonitor offers a ""Custom Metrics"" dashboard for Grafana.</p>

<p>To see the metrics locally, currently the only way is to enable periodic logging of all metrics.</p>
"
Stagemonitor,26004106,25204392,1,"2014/09/23, 23:23:43",False,"2014/09/23, 23:23:43",4936,1125055,3,"<p>Stagemonitor now features a in browser widget that is automatically injected in your web page. You don't need any infrastructure or docker for this and the configuration and set up is easy.</p>

<p>For more information, visit <a href=""http://www.stagemonitor.org/"" rel=""nofollow noreferrer"">http://www.stagemonitor.org/</a>. This is how you enable the widget: <a href=""https://github.com/stagemonitor/stagemonitor/wiki/Step-2%3A-Log-Only-Monitoring-In-Browser-Widget#in-browser-widget"" rel=""nofollow noreferrer"">https://github.com/stagemonitor/stagemonitor/wiki/Step-2%3A-Log-Only-Monitoring-In-Browser-Widget#in-browser-widget</a>.</p>

<p><img src=""https://i.stack.imgur.com/29HxN.png"" alt=""stagemonitor widget""></p>
"
Stagemonitor,48148146,48147866,0,"2018/01/08, 12:12:45",False,"2018/01/08, 12:12:45",128,7445902,0,"<p>Two possible solutions. </p>

<p>You can have a button 'hide', that will hide the metrics using some javascript code. </p>

<p>Or in the same button you do the following:</p>

<pre><code>private boolean metricsOn = false;

@RequestMapping(value = ""/owners/test"", method = RequestMethod.GET)
public String testing(Map&lt;String, Object&gt; model) {

    if(!metricsOn){
metrics = true;

    Owner owner = new Owner();
    model.put(""owner"", owner);
    return VIEWS_OWNER_CREATE_OR_UPDATE_FORM;
} else{
//return without the given metrics?
metrics = false;
 return VIEWS_OWNER_CREATE_OR_UPDATE_FORM;
}
</code></pre>
"
Stagemonitor,44376892,44347364,0,"2017/06/05, 23:09:57",True,"2017/06/06, 08:46:33",4936,1125055,1,"<p>Most of the metrics stagemonitor collects are not available via JMX. For example, response time statistics grouped by the endpoint of your application.</p>

<p>Also, stagemonitor is much more than just metrics. It is also a profiler, you can use to see which methods caused a request to be slow. Further more, it can (soon) do distributed tracing which helps you to analyze and debug latency problems in a microservice environment by correlating related requests. It also offers you a Kibana dashboard you can use to drill into the requests your application serves to find out about causes of errors or latency. Another use case is lightweight web analytics to identify which devices and operating systems your customers use to access your site.</p>
"
Stagemonitor,48163586,48162254,0,"2018/01/09, 09:52:48",True,"2018/01/09, 09:52:48",4936,1125055,1,"<p>I'm afraid this is currently not possible. However, stagemonitor offers a ""Custom Metrics"" dashboard for Grafana.</p>

<p>To see the metrics locally, currently the only way is to enable periodic logging of all metrics.</p>
"
Stagemonitor,30496430,30493074,0,"2015/05/28, 05:55:25",True,"2015/05/28, 05:55:25",74274,160313,1,"<p>It doesn't appear to be compatible with Grails. If you enable logging</p>

<pre><code>log4j.main = {
   error 'org.codehaus.groovy.grails',
         'org.springframework',
         'org.hibernate',
         'net.sf.ehcache.hibernate'
   info 'org.stagemonitor'
}
</code></pre>

<p>you'll see a bunch of error stacktraces that appear to imply that the way they're using Javassist to wire in tracing code isn't compatible with Groovy and/or the AST transformations that Grails uses:</p>

<pre><code>org.stagemonitor.javassist.NotFoundException: stagemonitortest.PersonController$setErrors
...
org.stagemonitor.javassist.NotFoundException: stagemonitortest.PersonController$getParams$0
...
org.stagemonitor.javassist.NotFoundException: stagemonitortest.Person$count
...
org.stagemonitor.javassist.NotFoundException: stagemonitortest.Person$list$0
...
org.stagemonitor.javassist.NotFoundException: stagemonitortest.PersonController$respond$1
</code></pre>
"
Wavefront VMware,63893372,63635003,0,"2020/09/15, 03:17:06",False,"2020/09/15, 03:17:06",5847,3390417,0,"<p>The problem(s) (as noted in <a href=""https://issues.apache.org/jira/browse/GEODE-788"" rel=""nofollow noreferrer"">GEODE-788</a>, <a href=""https://issues.apache.org/jira/browse/GEODE-7665"" rel=""nofollow noreferrer"">GEODE-7665</a>, <a href=""https://issues.apache.org/jira/browse/GEODE-7666"" rel=""nofollow noreferrer"">GEODE-7666</a>, <a href=""https://issues.apache.org/jira/browse/GEODE-7670"" rel=""nofollow noreferrer"">GEODE-7670</a>, <a href=""https://issues.apache.org/jira/browse/GEODE-7672"" rel=""nofollow noreferrer"">GEODE-7672</a> and <a href=""https://issues.apache.org/jira/browse/GEODE-7676"" rel=""nofollow noreferrer"">GEODE-7676</a>) is, is that GemFire/Geode does not support <code>Region.clear()</code> for <code>PARTITION</code> <em>Regions</em> (yet).</p>
<p>When you declare the <a href=""https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/cache/annotation/CacheEvict.html#allEntries--"" rel=""nofollow noreferrer""><code>@CacheEvent(allEntries = true)</code></a> annotation/attribute on your managed application component, for example...</p>
<pre class=""lang-java prettyprint-override""><code>@Component
class MyCacheableSpringManagedApplicationComponent {

  @CacheEvict(allEntries = true)
  void nukeAndPave() {
      repository.deleteAll();
      // ...
  }
}
</code></pre>
<p>This in effect calls <code>Region.clear()</code> (see <a href=""https://github.com/spring-projects/spring-data-geode/blob/2.4.0-M2/spring-data-geode/src/main/java/org/springframework/data/gemfire/cache/GemfireCache.java#L95"" rel=""nofollow noreferrer"">here</a>). This behavior works for <code>REPLICATE</code> and <code>LOCAL</code> <em>Regions</em>, however not for <code>PARTITION</code> <em>Regions</em>, given the numerous GemFire/Geode problems. It is currently a WIP, though.</p>
<p>There was (partly, still is) an intention in Spring Data for Apache Geode &amp; VMware Tanzu (Pivotal) GemFire to handle cache clear operations.</p>
<p><a href=""https://jira.spring.io/browse/DATAGEODE-265"" rel=""nofollow noreferrer"">https://jira.spring.io/browse/DATAGEODE-265</a></p>
<p>However, this is on hold until the above GEODE JIRA tickets get sorted out.</p>
"
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201,1585136,0,"<p>The short answer is no. You really, really want to have DNS set up properly.</p>
<hr />
<p>Here's the long answer that is more nuanced.</p>
<p>All requests to your foundation go through the Gorouter. Gorouter will take the incoming request, look at the <code>Host</code> header and use that to determine where to send the request. This happens the same for system services like CAPI and UAA as it does for apps you deploy to the foundation.</p>
<p>DNS is a requirement because of the <code>Host</code> header. A browser trying to access CAPI or an application on your foundation is going to set the <code>Host</code> header based on the DNS entry you type into your browser's address bar. The cf CLI is going to do the same thing.</p>
<p>There are some ways to work around this:</p>
<ol>
<li><p>If you are strictly using a client like <code>curl</code> where you can set the <code>Host</code> header to arbitrary values. In that way, you could set the host header to <code>api.system_domain</code> and at the same time connect to the IP address of your foundation. That's not a very elegant way to use CF though.</p>
</li>
<li><p>You can manually set entries in your /etc/hosts` (or similar on Windows). This is basically a way to override DNS resolution and supply your own custom IP.</p>
<p>You would need to do this for <code>uaa.system_domain</code>, <code>login.system_domain</code>, <code>api.system_domain</code> and any host names you want to use for apps deployed to your foundation, like <code>my-super-cool-app.apps_domain</code>. These should all point to the IP of the load balancer that's in front of your pool of Gorouters.</p>
<p>If you add enough entries into <code>/etc/hosts</code> you can make the cf CLI work. I have done this on occasion to bypass the load balancer layer for troubleshooting purposes.</p>
<p>Where this won't work is on systems where you can't edit <code>/etc/hosts</code>, like customers or external users of software running on your foundation or if you're trying to deploy apps on your foundation that talk to each other using routes on CF (because you can't edit <code>/etc/hosts</code> in the container). Like if you have <code>app-a.apps_domain</code> and <code>app-b.apps_domain</code> and <code>app-a</code> needs to talk to <code>app-b</code>. That won't work because you have no DNS resolution for <code>apps_domain</code>.</p>
<p>You can probably make app-to-app communication work if you are able to use container-to-container networking and the <code>apps.internal</code> domain though. The resolution for that domain is provided by Bosh DNS. You have to be aware of this difference though when deploying your apps and map routes on the <code>apps.internal</code> domain, as well as setting network policy to allow traffic to flow between the two.</p>
</li>
</ol>
<p>Anyway, there might be other hiccups. This is just off the top of my head. You can see it's a lot better if you can set up DNS.</p>
"
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31,7582877,0,"<p>The most easy way to achieve a portable solution is a service like <a href=""http://xip.io/"" rel=""nofollow noreferrer"">xip.io</a> that will work out of the box. I have setup and run a lot of PoCs that way, when wildcard DNS was something that enterprise IT was still oblivious about.</p>
<p>It works like this (excerpt from their site):</p>
<p>What is xip.io?
xip.io is a magic domain name that provides wildcard DNS
for any IP address. Say your LAN IP address is 10.0.0.1.
Using xip.io,</p>
<pre><code>      10.0.0.1.xip.io   resolves to   10.0.0.1
  www.10.0.0.1.xip.io   resolves to   10.0.0.1
</code></pre>
<p>mysite.10.0.0.1.xip.io   resolves to   10.0.0.1
foo.bar.10.0.0.1.xip.io   resolves to   10.0.0.1</p>
<p>...and so on. You can use these domains to access virtual
hosts on your development web server from devices on your
local network, like iPads, iPhones, and other computers.
No configuration required!</p>
"
Wavefront VMware,67007504,67007310,3,"2021/04/08, 18:47:38",False,"2021/04/08, 18:47:38",5028,5965430,1,"<p>It is unclear whether you're using the SCDF tile or the SCDF OSS (via <code>manfest.yml</code>) on PCF.</p>
<p>Suppose you're using the OSS, AFA. In that case, you are providing the right RMQ service-instance configuration (that you pre-created) in the <code>manifest.yml</code>, then SCDF would automatically propagate that RMQ service instance and bind it to the apps it is deploying to your ORG/Space. You don't need to muck around with connection credentials manually.</p>
<p>On the other hand, if you are using the SCDF Tile, the SCDF service broker will auto-create the RMQ SI and automatically bind it to the apps it deploys.</p>
<p>In summary, there's no reason to manually pass the connection credentials or pack them as application properties inside your apps. You can automate all this provided you're configuring all this correctly.</p>
"
Wavefront VMware,63456882,63327611,0,"2020/08/17, 21:44:26",False,"2020/08/17, 21:44:26",432,6689024,0,"<blockquote>
<p>&quot;<em>In this case it will wait till the processing completes or it
forcibly reduces the instance count when reached threshold.</em>&quot;</p>
</blockquote>
<p><strong>Answer:</strong><br />
No, the App Autoscaler will not force anything, after the decision cycle, it will prepare the instance to be escalated-down (shutdown), so the intention is to avoid lose requests or data during this process.</p>
<p>Please, take a look into the documentation below, it will help you to understand better the App Autoscaler mechanism.</p>
<hr />
<p><strong>How App Autoscaler Determines When to Scale:</strong></p>
<blockquote>
<p>Every 35 seconds, App Autoscaler makes a decision about whether to
scale up, scale down, or keep the same number of instances.</p>
</blockquote>
<blockquote>
<p>To make a scaling decision, App Autoscaler averages the values of a
given metric for the most recent 120 seconds.</p>
</blockquote>
<p>The following diagram provides an example of how App Autoscaler makes scaling decisions:<br />
<a href=""https://i.stack.imgur.com/N57UU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N57UU.png"" alt=""enter image description here"" /></a></p>
<p><strong>Reference:</strong><br />
<a href=""https://docs.pivotal.io/platform/application-service/2-10/appsman-services/autoscaler/about-app-autoscaler.html"" rel=""nofollow noreferrer"">VMWare Tanzu App Autoscaler documentation</a></p>
<p><em>VMWare Tanzu is the former Pivotal Cloud Foundry (PCF).</em></p>
"
Wavefront VMware,67134760,63327611,0,"2021/04/17, 07:49:29",False,"2021/04/17, 07:49:29",11,15668534,1,"<p>I have the same question and as far as I understood from <a href=""https://docs.cloudfoundry.org/devguide/deploy-apps/app-lifecycle.html"" rel=""nofollow noreferrer"">App Container Lifecycle</a> it’s up to your app to gracefully shutdown but that might not be possible in given 10 seconds as some processes might take longer.</p>
<blockquote>
<p><strong>Shutdown</strong>
CF requests a shutdown of your app instance in the following scenarios:
When a user runs <strong>cf scale</strong>, cf stop, cf push, cf delete, or cf restart-app-instance
As a result of a system event, such as the replacement procedure during Diego Cell evacuation or when an app instance stops because of a failed health check probe
To shut down the app, CF sends the app process in the container a SIGTERM. By default, the process has ten seconds to shut down gracefully. If the process has not exited after ten seconds, CF sends a SIGKILL.
By default, apps must finish their in-flight jobs within ten seconds of receiving the SIGTERM before CF terminates the app with a SIGKILL. For instance, a web app must finish processing existing requests  and stop accepting new requests.
Note: One exception to the cases mentioned above is when monit restarts a crashed Diego Cell rep or Garden server. In this case, CF immediately stops the apps that are still running using SIGKILL.</p>
</blockquote>
"
Wavefront VMware,59697948,59047752,0,"2020/01/11, 21:31:36",False,"2020/01/11, 21:31:36",334,1147487,0,"<p>I think there's a workaround for kubernetes versions prior to 1.17.</p>

<hr>

<p>On <strong>kubernetes version v1.16</strong> you can run Sonobuoy (Sonobuoy version v0.16.1 or higher) with providing the test framework flag: <code>--allowed-not-ready-nodes=1</code></p>

<ul>
<li>then the test framework allows 1 node (your tainted node) to be ""not-ready""</li>
<li>this approach works fine for me with kubernetes v1.16.4 and Sonobuoy Version: v0.17.1</li>
<li>see docs here: <a href=""https://github.com/vmware-tanzu/sonobuoy/blob/master/site/docs/master/faq.md#we-have-some-nodes-with-custom-taints-in-our-cluster-and-the-tests-wont-start-how-can-i-run-the-tests"" rel=""nofollow noreferrer"">We have some nodes with custom taints in our cluster and the tests won't start. How can I run the tests?</a></li>
<li>the Sonobuoy invocation looks like:</li>
</ul>

<pre><code>sonobuoy run --plugin-env=e2e.E2E_EXTRA_ARGS=""--allowed-not-ready-nodes=1""
</code></pre>

<hr>

<p>And on <strong>kubernetes version prior to v1.16</strong> it was more complicated. I haven't tested this but according to docs:</p>

<ul>
<li>you should also use the test framework flag: <code>--allowed-not-ready-nodes=1</code></li>
<li>Before kubernetes v1.16, it was necessary to build your own custom image which could execute the tests with the desired options. See docs at:

<ul>
<li><a href=""https://github.com/vmware-tanzu/sonobuoy/blob/master/site/docs/master/faq.md#how-can-i-run-the-e2e-tests-with-certain-test-framework-options-set-what-are-the-available-options"" rel=""nofollow noreferrer"">How can I run the E2E tests with certain test framework options set? What are the available options?</a></li>
<li><a href=""https://sonobuoy.io/custom-e2e-image/#building-your-own-test-image"" rel=""nofollow noreferrer"">Building Your Own Test Image</a></li>
</ul></li>
</ul>
"
Wavefront VMware,66565598,66545598,1,"2021/03/10, 15:19:34",True,"2021/03/10, 15:19:34",10201,1585136,0,"<p>The short answer is no. You really, really want to have DNS set up properly.</p>
<hr />
<p>Here's the long answer that is more nuanced.</p>
<p>All requests to your foundation go through the Gorouter. Gorouter will take the incoming request, look at the <code>Host</code> header and use that to determine where to send the request. This happens the same for system services like CAPI and UAA as it does for apps you deploy to the foundation.</p>
<p>DNS is a requirement because of the <code>Host</code> header. A browser trying to access CAPI or an application on your foundation is going to set the <code>Host</code> header based on the DNS entry you type into your browser's address bar. The cf CLI is going to do the same thing.</p>
<p>There are some ways to work around this:</p>
<ol>
<li><p>If you are strictly using a client like <code>curl</code> where you can set the <code>Host</code> header to arbitrary values. In that way, you could set the host header to <code>api.system_domain</code> and at the same time connect to the IP address of your foundation. That's not a very elegant way to use CF though.</p>
</li>
<li><p>You can manually set entries in your /etc/hosts` (or similar on Windows). This is basically a way to override DNS resolution and supply your own custom IP.</p>
<p>You would need to do this for <code>uaa.system_domain</code>, <code>login.system_domain</code>, <code>api.system_domain</code> and any host names you want to use for apps deployed to your foundation, like <code>my-super-cool-app.apps_domain</code>. These should all point to the IP of the load balancer that's in front of your pool of Gorouters.</p>
<p>If you add enough entries into <code>/etc/hosts</code> you can make the cf CLI work. I have done this on occasion to bypass the load balancer layer for troubleshooting purposes.</p>
<p>Where this won't work is on systems where you can't edit <code>/etc/hosts</code>, like customers or external users of software running on your foundation or if you're trying to deploy apps on your foundation that talk to each other using routes on CF (because you can't edit <code>/etc/hosts</code> in the container). Like if you have <code>app-a.apps_domain</code> and <code>app-b.apps_domain</code> and <code>app-a</code> needs to talk to <code>app-b</code>. That won't work because you have no DNS resolution for <code>apps_domain</code>.</p>
<p>You can probably make app-to-app communication work if you are able to use container-to-container networking and the <code>apps.internal</code> domain though. The resolution for that domain is provided by Bosh DNS. You have to be aware of this difference though when deploying your apps and map routes on the <code>apps.internal</code> domain, as well as setting network policy to allow traffic to flow between the two.</p>
</li>
</ol>
<p>Anyway, there might be other hiccups. This is just off the top of my head. You can see it's a lot better if you can set up DNS.</p>
"
Wavefront VMware,66634194,66545598,0,"2021/03/15, 09:54:59",False,"2021/03/15, 09:54:59",31,7582877,0,"<p>The most easy way to achieve a portable solution is a service like <a href=""http://xip.io/"" rel=""nofollow noreferrer"">xip.io</a> that will work out of the box. I have setup and run a lot of PoCs that way, when wildcard DNS was something that enterprise IT was still oblivious about.</p>
<p>It works like this (excerpt from their site):</p>
<p>What is xip.io?
xip.io is a magic domain name that provides wildcard DNS
for any IP address. Say your LAN IP address is 10.0.0.1.
Using xip.io,</p>
<pre><code>      10.0.0.1.xip.io   resolves to   10.0.0.1
  www.10.0.0.1.xip.io   resolves to   10.0.0.1
</code></pre>
<p>mysite.10.0.0.1.xip.io   resolves to   10.0.0.1
foo.bar.10.0.0.1.xip.io   resolves to   10.0.0.1</p>
<p>...and so on. You can use these domains to access virtual
hosts on your development web server from devices on your
local network, like iPads, iPhones, and other computers.
No configuration required!</p>
"
Wavefront VMware,64468853,64453467,1,"2020/10/21, 20:36:57",False,"2020/10/21, 20:36:57",1170,4462517,1,"<p>Pivotal Web Services is not the same as Pivotal Cloud Foundry. Pivotal Web Services has been sunset, yes. Tanzu Application Service is VMware's enterprise solution that is, if you want to think about it this way, a self-hosted Pivotal Web Services (this is a gross understatement, but works for this situation). Are you looking to test Cloud Foundry for its suitability?</p>
"
Wavefront VMware,65097195,64327529,0,"2020/12/01, 21:27:18",False,"2020/12/01, 21:27:18",43,13300061,0,"<p>Add the AzureIdentity and AzureIdentityBinding roles to cluster as mentioned in docs.</p>
<pre><code># Under AzureIdentityBinding
apiVersion: &quot;aadpodidentity.k8s.io/v1&quot;
kind: AzureIdentityBinding
metadata:
  name: &lt;name&gt; #replace name
spec:
AzureIdentity: &lt;identity&gt; #replace with the name of the identity resource created.
Selector: &lt;label&gt;
</code></pre>
<p>once done, use the selector defined AzureIdentityBinding as label while deploying helm chart.</p>
<pre><code>podLabels: {aadpodidbinding: &lt;selector&gt;} # selector you defined above in AzureIdentityBinding 
</code></pre>
<p>Check for the actual syntax for podLabels using with --set in helm install command. Or you can clone the charts and make changes to values.yaml below and install it from local charts.</p>
<p><a href=""https://github.com/vmware-tanzu/helm-charts/blob/04615803a7d976ce15a6eeb4b1bd6a1cfb9a02c5/charts/velero/values.yaml#L27"" rel=""nofollow noreferrer"">https://github.com/vmware-tanzu/helm-charts/blob/04615803a7d976ce15a6eeb4b1bd6a1cfb9a02c5/charts/velero/values.yaml#L27</a></p>
<p>Just for help:
<a href=""https://medium.com/@kimvisscher/using-aad-pod-identity-in-an-aks-cluster-117c08565692"" rel=""nofollow noreferrer"">https://medium.com/@kimvisscher/using-aad-pod-identity-in-an-aks-cluster-117c08565692</a></p>
"
Wavefront VMware,65372173,64327529,0,"2020/12/19, 18:50:18",False,"2020/12/19, 18:50:18",73,3389881,0,"<p>It seems that you are struggling with how to format the <code>secretContents</code> section of the values.yaml file. If that is so, take a look at a recent update to the documentation. It lays out and documents exactly how to format it:</p>
<p><a href=""https://github.com/vmware-tanzu/helm-charts/blob/1ea07c7fb9c7ba910ba52801c536a6f8dcee096d/charts/velero/values.yaml#L219-L232"" rel=""nofollow noreferrer"">https://github.com/vmware-tanzu/helm-charts/blob/1ea07c7fb9c7ba910ba52801c536a6f8dcee096d/charts/velero/values.yaml#L219-L232</a></p>
"
Zipkin,47672314,47670883,2,"2017/12/06, 12:42:50",True,"2017/12/06, 12:42:50",3566,1358676,11,"<p>I found that I need to add a sampler percentage. By default zero percentage of the samples are sent and that is why the sleuth was not sending anything to zipkin. when I added <code>spring.sleuth.sampler.percentage=1.0</code> in the properties files, it started working.</p>
"
Zipkin,49838749,47670883,0,"2018/04/15, 08:20:04",False,"2018/04/15, 08:20:04",201,7956609,2,"<p>If you are exporting all the span data to Zipkin, sampler can be installed  by creating a bean definition in the Spring boot main class </p>

<pre><code>  @Bean
  public Sampler defaultSampler() {
    return new AlwaysSampler();
  }
</code></pre>
"
Zipkin,54384025,47670883,0,"2019/01/27, 02:18:35",False,"2019/01/27, 02:18:35",339,1405291,9,"<p>For the latest version of cloud dependencies <code>&lt;version&gt;Finchley.SR2&lt;/version&gt;</code><br>
The correct property to send traces to zipkin is: <code>spring.sleuth.sampler.probability=1.0</code>
Which has changed from percentage to probability.</p>
"
Zipkin,42580765,34272334,0,"2017/03/03, 15:55:09",False,"2017/03/03, 15:55:09",1247,1232692,1,"<p>It is a very long time ago, but it looks like it was moved here:</p>

<p><a href=""http://zipkin.io/pages/quickstart"" rel=""nofollow noreferrer"">http://zipkin.io/pages/quickstart</a></p>
"
Zipkin,57852082,34272334,0,"2019/09/09, 13:21:48",False,"2019/09/09, 13:21:48",3190,2987755,0,"<p>Found multiple language examples at <a href=""https://github.com/openzipkin?utf8=%E2%9C%93&amp;q=example"" rel=""nofollow noreferrer"">github</a>.<br>
If you need basic setup steps: <a href=""https://zipkin.io/"" rel=""nofollow noreferrer"">https://zipkin.io/</a><br>
Integrated zipkin with spring boot 2 and mysql 
<a href=""https://github.com/dineshbhagat/mac-configurations/blob/master/apm/zipkin.md"" rel=""nofollow noreferrer"">Steps</a><br>
<a href=""https://github.com/dineshbhagat/Spring-boot-JPA-Two-Level-Cache"" rel=""nofollow noreferrer"">example</a><br>
Here is sample 
<a href=""https://i.stack.imgur.com/yq17A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yq17A.png"" alt=""rest+mysql+hibernate""></a></p>
"
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765,6466540,0,"<p>Lately I have been trying the same and couldn't find that option in initializer. I am just posting this if anyone encounters the same issues and lands on this page. You can refer below sample GitHub project which is consists of four micro services ( zipkin server, client, rest service, and Eureka ) using Edgware release with latest version of sleuth.</p>

<p><a href=""https://github.com/iuprade/slueth-zipkin-server-client"" rel=""nofollow noreferrer""> Sample Zipkin Server/Client </a></p>
"
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767,2498986,3,"<p>Zipkin Server is not part of Spring initializers. You have to use the official release of the Zipkin server </p>

<p><a href=""https://github.com/openzipkin/zipkin#quick-start"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin#quick-start</a></p>

<p>And custom servers are not supported anymore meaning you can't use <code>@EnableZipkinServer</code> anymore since 2.7</p>

<p><a href=""https://github.com/openzipkin/zipkin#quick-start"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin#quick-start</a></p>
"
Zipkin,42403962,42402849,1,"2017/02/23, 01:04:02",False,"2017/02/23, 01:04:02",84586,1384297,3,"<p>I can't explain the error that you got with Dalston.BUILD-SNAPSHOT, but the error with Camden.SR4 is because it's not compatible with Spring Boot 1.5. I'd recommend upgrading to Camden.SR5 <a href=""https://spring.io/blog/2017/02/06/spring-cloud-camden-sr5-is-available"" rel=""nofollow noreferrer"">which is compatible with Spring Boot 1.5</a>.</p>
"
Zipkin,47165778,42402849,0,"2017/11/07, 21:14:23",False,"2017/11/07, 21:14:23",349,961275,1,"<p>Even I got this error while setting up my project. I was using Spring boot 1.5.8 with the Brixton.SR6 release. However, when I consulted the site <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">http://projects.spring.io/spring-cloud/</a> I got to know the issue and I updated my dependency to Dalston.SR4 and then the application started working. </p>
"
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473,2733462,8,"<p>The official documentation was helpful, but I think it didn't include all the dependencies explicitly (at least as of now). I had to do some extra research for samples to get all the required dependencies and configuration together. I wanted to share it, because I believe it could be helpful for someone else.</p>

<p><strong>Spring Boot version:</strong> <code>1.4.0.RELEASE</code></p>

<p><strong>Spring Cloud version:</strong> <code>Brixton.SR4</code></p>

<p><strong>POM:</strong></p>

<pre><code>    ...
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-storage-mysql&lt;/artifactId&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;mysql&lt;/groupId&gt;
        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;
    &lt;/dependency&gt;
   ...
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;Brixton.SR4&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;
</code></pre>

<p><strong>Java:</strong></p>

<pre><code>import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import zipkin.server.EnableZipkinServer;

@SpringBootApplication
@EnableZipkinServer
public class ZipkinServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZipkinServerApplication.class, args);
    }
}
</code></pre>

<p><strong>application.yml:</strong></p>

<pre><code>spring:
  datasource:
    schema: classpath:/mysql.sql
    url: jdbc:mysql://localhost:3306/zipkin?autoReconnect=true
    username: root
    password: admin
    driver-class-name: com.mysql.jdbc.Driver
    initialize: true
    continue-on-error: true
  sleuth:
    enabled: false
zipkin:
  storage:
    type: mysql
</code></pre>

<p><strong>References:</strong></p>

<p><a href=""https://cloud.spring.io/spring-cloud-sleuth/"" rel=""noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/</a></p>
"
Zipkin,63987464,60023762,0,"2020/09/21, 09:54:36",False,"2021/03/27, 13:21:49",1,13026705,0,"<p>Your application starts in Tomcat Server but Zipkin use another server but i dont know that name , i include this code to spring boot starter web dependecy for ignore tomcat server</p>
<pre><code>&lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
&lt;/dependency&gt;
</code></pre>
<p>This worked for me try it</p>
"
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932,1371995,1,"<p>I have tested this with the official <a href=""https://github.com/census-instrumentation/opencensus-node/tree/master/examples/express"" rel=""nofollow noreferrer"">opencensus-node</a> example at github.</p>

<h1>Problem 1:</h1>

<blockquote>
  <p>Zipkin UI displays 2 spans with same name MyApplication(see image),
  but expected 2 different span names</p>
</blockquote>

<p>Just to be clear, <code>MyApplication</code> is the service name you set in your app.js, and the span names are those which you selected on the image <code>/service1</code>, <code>/service1</code>, <code>/external_service_2</code>.</p>

<p>I think this is the intended behavior, you got one service (<code>MyApplication</code>), a root span (<code>/service1</code>) and a child span (<code>/external_service_2</code>).
If you got multiple services connected to the same Zipkin server then you'll have multiple names for the services.</p>

<p><a href=""https://i.stack.imgur.com/g3PnG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g3PnG.png"" alt=""OpenCensus example""></a></p>

<p><strong>From the Zipkin's <a href=""https://zipkin.io/pages/instrumenting.html"" rel=""nofollow noreferrer"">documentation</a>:</strong></p>

<p><strong>Span</strong></p>

<p>A set of Annotations and BinaryAnnotations that correspond to a particular RPC. Spans contain identifying information such as traceId, spanId, parentId, and RPC name.</p>

<p><strong>Trace</strong></p>

<p>A set of spans that share a single root span. Traces are built by collecting all Spans that share a traceId. The spans are then arranged in a tree based on spanId and parentId thus providing an overview of the path a request takes through the system.</p>

<h1>Problem 2:</h1>

<blockquote>
  <p>As far Zipkin UI displays 2 spans with same name, service dependencies
  page contains one Service only(see image)</p>
</blockquote>

<p>Again, this is the intended behavior, since you got only one service and the external request you made goes through it.</p>

<h1>Change span names:</h1>

<p>If you mean the framed names on the first image, at the top it shows only the root span you clicked on the previous screen.
However, you can write custom span names after a little change in your code.</p>

<p>From the <a href=""https://opencensus.io/quickstart/nodejs/tracing"" rel=""nofollow noreferrer"">tracing documentation</a> (with your code):</p>

<pre><code>const options = {
  url: `${ZIPKIN_ENDPOINT}/api/v2/spans`,
  serviceName: 'MyApplication'
}
const tracer = tracing.start({samplingRate: 1}).tracer;
tracer.registerSpanEventListener(new zipkin.ZipkinTraceExporter(options));
</code></pre>

<p>Now you can use <code>tracer.startRootSpan</code>, I used it in the express sample with a request:</p>

<pre><code>tracer.startRootSpan({name: 'main'}, rootSpan =&gt; {
  rp('http://localhost:3000/sample').then(data =&gt; {
    res.send(data);
    rootSpan.end();
  }, err =&gt; {
    console.error(`${err.message}`);
    rootSpan.end();
  });
});
</code></pre>

<p>A span must be closed.</p>

<p>For more information, check the <a href=""https://github.com/census-instrumentation/opencensus-node/blob/master/packages/opencensus-core/test/test-tracer.ts"" rel=""nofollow noreferrer"">test file</a> of the tracer.</p>
"
Zipkin,38537987,35215821,0,"2016/07/23, 06:56:00",False,"2016/07/23, 06:56:00",762,1227937,0,"<p>The best way to ask for a feature is using github issues.</p>

<p>To add a new transport such as RabbitMQ, you'd have to affect Brave (reporter) and Zipkin (collector)</p>

<p><a href=""https://github.com/openzipkin/zipkin/issues"" rel=""nofollow"">https://github.com/openzipkin/zipkin/issues</a>
<a href=""https://github.com/openzipkin/brave/issues"" rel=""nofollow"">https://github.com/openzipkin/brave/issues</a></p>
"
Zipkin,16981952,16981751,0,"2013/06/07, 13:29:08",True,"2018/10/17, 13:15:26",18596,69875,1,"<p>It's not suitable, Zipkin is about tracing in distributed systems. I would say you would want something like a profiler, such as <a href=""http://visualvm.java.net/"" rel=""nofollow noreferrer"">Visual VM</a>,  - free and included with the JDK, or <a href=""http://www.yourkit.com/"" rel=""nofollow noreferrer"">YourKit</a>. Other profilers are available.</p>
"
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071,298389,1,"<p>No, it <strong>is not suitable at all</strong>. Why? Because the architecture of Zipkin have multiple levels of indirection: you have to send your spans into collector service and even if collector is running on your own machine there is a huge overhead -- imagine you are traveling from two neighbour cities somewhere in Europe and somebody proposes you instead of going directly from A to B, fly from A to New York, USA and back to B. That is simply ridiculous.</p>

<p>As for the tools that may suit your needs: VisualVM and Yourkit mentioned by @Jonathan are good for looking at average situation in your program -- if you need to carefully inspect low level paths in your program <a href=""http://mchr3k.github.io/org.intrace/"" rel=""nofollow"">InTrace</a> might be a better choice. </p>
"
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932,433344,1,"<p>Zipkin is foremost intended to provide observability into a complex distributed network of services (aka Microservice Architecture).  It wasn't intended to support just a single application.  </p>

<p>So a profiler can often be a better way to go, particularly if you have an urgent issue to diagnose.</p>

<p>That said, most profilers will only provide realtime data, where Zipkin persists, allowing for analysis over large datasets.</p>

<p>If Zipkin is already in your stack, it's not a bad idea to enrich higher level Zipkin spans (ie a complete REST requests) with specific method calls as child-spans, particularly those that do I/O.  This way you can drill down within Zipkin to more quickly determine bottlenecks.  </p>

<p>Otherwise you'd have to first look at the Zipkin span for high-level bottlenecks, then separately look at other logs or run a profiler, which is inefficient.  </p>

<p>** If it's already in your stack ** </p>
"
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106,3956619,3,"<p>It's hard to tell without more information. But it can be related to <strong>incompatible libraries</strong>. Can you post your dependencies?</p>

<p>In case you are using <strong>older version</strong> of okhttpclient with <strong>latest</strong> spring cloud:greenwich it can cause this issue.</p>

<p>I'm using <strong>Greenwich.RELEASE</strong> with <strong>okhttpclient:10.2.0</strong> which works without problems</p>
"
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11,12647380,1,"<p>Use the below dependency Management for spring-boot to download the suitable versions for cloud version</p>

<pre><code>    &lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;
</code></pre>

<p>I am using Java 10, cloud.version is <strong>Finchley.SR2</strong> and sprinb-boot:2.2.0 and spring-cloud-starter-openfeign :2.1.2.RELEASE. and this combination worked for me to fix the issue.</p>

<p>Acctual problem was 10.x.x feign-core  was not working only  and io.github.openfeign:feign-core:jar:9.7.0:compile was working.</p>
"
Zipkin,62180529,54262815,0,"2020/06/03, 21:51:55",False,"2020/06/03, 21:51:55",116,10783374,1,"<p>I faced this problem using java 11, springboot 2.3.0.RELEASE, and spring-cloud version Greenwich.RELEASE. Adding the following dependences saved me:</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt;
        &lt;artifactId&gt;feign-okhttp&lt;/artifactId&gt;
        &lt;version&gt;10.2.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt;
        &lt;artifactId&gt;feign-core&lt;/artifactId&gt;
        &lt;version&gt;10.2.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Hope this helps someone.</p>
"
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485,1287376,1,"<p>It seems to be a timing issue.</p>

<p>If we add some delay, for instance, between children spans execution like</p>

<pre><code>Thread.sleep(1);
</code></pre>

<p>In between</p>

<pre><code>Span prepare = tracer.newChild(twoPhase.context()).name(""prepare"").start();
try {                                                                
    System.out.print(""prepare"");                                     
} finally {                                                          
    prepare.finish();                                                
}                                                                    
Thread.sleep(1); // &lt;&lt;&lt;                                                                                                                                                         
Span commit = tracer.newChild(twoPhase.context()).name(""commit"").start();
try {                                                                
    System.out.print(""commit"");                                      
} finally {                                                          
    commit.finish();                                                 
}
</code></pre>

<p>Then we get to see spans:</p>

<p><a href=""https://i.stack.imgur.com/8sCjA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8sCjA.png"" alt=""spans""></a></p>

<p>I've faced something like this before when Zipkin dropped spans I was (mistakenly) assigning wrong timestamps to.</p>

<p><em>For reference and ease of reproduction: I've setup a <a href=""https://github.com/embs/brave-example"" rel=""nofollow noreferrer"">project</a> for reproducing this issue / ""fix"".</em></p>
"
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762,1227937,5,"<p>In zipkin lingo, what you are asking about is often called ""local spans"" or ""local tracing"", basically an operation that neither originated, nor resulted in a remote call.</p>

<p>I'm not aware of anything at the syscall level, but many tracers support explicit instrumentation of function calls.</p>

<p>For example, using <a href=""https://github.com/Yelp/py_zipkin#usage-3-log-a-span-inside-an-ongoing-trace"" rel=""noreferrer"">py_zipkin</a>
<code>
@zipkin_span(service_name='my_service', span_name='some_function')
def some_function(a, b):
    return do_stuff(a, b)
</code></p>

<p>Besides explicit instrumentation like this, one could also export data to zipkin. For example, one could convert trace data that is made in another tool to <a href=""http://zipkin.io/zipkin-api/#/paths/%252Fspans/post"" rel=""noreferrer"">zipkin's json format</a>.</p>

<p>This probably doesn't answer your question, but I hope it helps.</p>
"
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007,4255107,0,"<p>The easiest way to get it working is to use the Micrometer library and configure the micrometer to send this data to the Zipkin server.</p>
<p>Enabling metrics using micrometer is very simple, you need to just add a micrometer-core and spring-cloud-starter-zipkin libraries.</p>
<p>See this tutorial for details about configuration and code <a href=""https://www.baeldung.com/tracing-services-with-zipkin"" rel=""nofollow noreferrer"">https://www.baeldung.com/tracing-services-with-zipkin</a></p>
<p>Micrometer will report consumer/producer metrics to Zipkin</p>
"
Zipkin,64096381,64094479,0,"2020/09/28, 08:27:01",True,"2020/09/28, 08:27:01",8336,1773866,2,"<p>If you search for zipkin grafana you'll get this as one of the first results <a href=""https://grafana.com/docs/grafana/latest/features/datasources/zipkin/"" rel=""nofollow noreferrer"">https://grafana.com/docs/grafana/latest/features/datasources/zipkin/</a></p>
"
Zipkin,61609752,61570149,1,"2020/05/05, 12:19:05",False,"2020/05/05, 12:19:05",2015,1398228,0,"<p>You need to reload after adding the subsystem:</p>

<pre><code>[standalone@localhost:9990 /] /extension=org.wildfly.extension.microprofile.opentracing-smallrye:add
{""outcome"" =&gt; ""success""}

[standalone@localhost:9990 /] /subsystem=microprofile-opentracing-smallrye:add
{
    ""outcome"" =&gt; ""success"",
    ""response-headers"" =&gt; {
        ""operation-requires-reload"" =&gt; true,
        ""process-state"" =&gt; ""reload-required""
    }
}

[standalone@localhost:9990 /] reload
[standalone@localhost:9990 /] /subsystem=microprofile-opentracing-smallrye/jaeger-tracer=my-tracer:add()
{""outcome"" =&gt; ""success""}
</code></pre>
"
Zipkin,61636753,61570149,6,"2020/05/06, 16:43:39",False,"2020/05/06, 16:43:39",2015,1398228,1,"<pre><code>embed-server --admin-only=true
/extension=org.wildfly.extension.microprofile.opentracing-smallrye:add()
/subsystem=microprofile-opentracing-smallrye:add()
/subsystem=microprofile-opentracing-smallrye/jaeger-tracer=my-tracer:add()
stop-embedded-server
</code></pre>

<p>This jboss-cli script should enable opentracing before starting the server properly. I'm not sure how/when you can execute that with keycloack image</p>
"
Zipkin,53209830,53208598,0,"2018/11/08, 16:31:04",False,"2018/11/08, 16:31:04",129048,1240763,1,"<blockquote>
  <p><code>Connection refused: connect</code></p>
</blockquote>

<p>Simply means that RabbitMQ is not running on <code>localhost:5672</code> (which is the default if you don't provide a host/port, or addresses, for it in your <code>application.yml</code>).</p>
"
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201,7956609,2,"<p>If service 2 is getting traceId from service 1, you can take the traceId from requestHeader in your java code. Otherwise sleuth generate a new traceId in service 2.</p>

<p>To get the trace Id In java </p>

<pre><code>    @Autowired
    private Tracer tracer;
</code></pre>

<p>Just do </p>

<pre><code>    tracer.getCurrentSpan().traceIdString();
</code></pre>
"
Zipkin,59216676,52759855,0,"2019/12/06, 18:13:45",False,"2019/12/06, 18:13:45",859,4513218,1,"<p>Hello you can also get the x-b3-traceid header information from the request, I created a Util class for that -> <a href=""https://gist.github.com/walterwhites/067dd635986e564aafdb5ac559073b0f"" rel=""nofollow noreferrer"">https://gist.github.com/walterwhites/067dd635986e564aafdb5ac559073b0f</a></p>

<pre><code>public final class DebugUtils {

    private static String PURPLE = ""\033[0;35m"";  // PURPLE
    private static String RED = ""\u001B[31m"";  // RED
    private static String RESET = ""\u001B[0m"";

    public static class ZipkinDebug {

        private static String url = ""http://localhost:9411/zipkin/traces/"";

        public static void displayTraceUrl(HttpServletRequest request) {
            String traceId = request.getHeader(""x-b3-traceid"");
            System.out.println(PURPLE + ""DebugUtils:ZipkinDebug -&gt; "" + url + traceId + RESET);
        }
    }

    public static class RequestInfo {

        public static void displayAllRequestHeaders(HttpServletRequest request) {
            Enumeration&lt;String&gt; headerNames = request.getHeaderNames();
            System.out.println(RED + ""DebugUtils:RequestInfo -&gt; "" + RESET);
            headerNames.asIterator().forEachRemaining(header -&gt; {
                System.out.println(""Header Name:"" + header + ""   "" + ""Header Value:"" + request.getHeader(header));
            });
        }

        public static void displayRequestHeader(HttpServletRequest request, String headerName) {
            System.out.println(RED + ""DebugUtils:RequestInfo -&gt; Header Name:"" + headerName + ""   "" + ""Header Value:"" + request.getHeader(headerName) + RESET);
        }
    }
}
</code></pre>

<p>Then in your main class you just need to call</p>

<pre><code>ZipkinDebug.displayTraceUrl(request);
</code></pre>
"
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391,6452043,3,"<p>after many searches, i found that there are a version conflicts between the dependencies.
and thanks for <em>vladimir-vagaytsev</em> </p>

<p>so, i see that <code>spring-cloud-starter-sleuth</code> imported as a different version.</p>

<p>to fix it i have added  <code>sleuth.version</code> to properties  in pom.xml like so.</p>

<pre><code>&lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Finchley.SR1&lt;/spring-cloud.version&gt;
        &lt;sleuth.version&gt;2.0.1.RELEASE&lt;/sleuth.version&gt;
    &lt;/properties&gt; 
</code></pre>

<p>then in dependency management we need to specify the version like so</p>

<pre><code>&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
            &lt;version&gt;${sleuth.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;
</code></pre>

<p>after then remove unused dependencies build and run.  </p>
"
Zipkin,52338780,52029459,0,"2018/09/14, 23:20:40",False,"2018/09/14, 23:20:40",373,1037492,-1,"<p>This class comes from zipkin-2. You can try adding this dependency.</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-sleuth-zipkin2&lt;/artifactId&gt;
    &lt;version&gt;2.0.0.M3&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
"
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589,5587542,2,"<p>Hello seeing your screenshot maybe you are using a spring boot 2.x version, I had the same problem with spring boot 2.0.3 with Finchley.RELEASE.</p>

<p>I found that Zipkin custom Server are not any more supported and deprecated for this reason it is not possible to use @EnableZipkinServer in Spring Cloud code and you have the ui but not the server side configured, api endpoint and so on.</p>

<p>form the Zipkin base code:</p>

<pre><code>/**
 * @deprecated Custom servers are possible, but not supported by the community. Please use our
 * &lt;a href=""https://github.com/openzipkin/zipkin#quick-start""&gt;default server build&lt;/a&gt; first. If you
 * find something missing, please &lt;a href=""https://gitter.im/openzipkin/zipkin""&gt;gitter&lt;/a&gt; us about
 * it before making a custom server.
 *
 * &lt;p&gt;If you decide to make a custom server, you accept responsibility for troubleshooting your
 * build or configuration problems, even if such problems are a reaction to a change made by the
 * OpenZipkin maintainers. In other words, custom servers are possible, but not supported.
 */
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Import(InternalZipkinConfiguration.class)
@Deprecated
public @interface EnableZipkinServer {

}
</code></pre>

<p>the code is available on <a href=""https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin/server/EnableZipkinServer.java"" rel=""nofollow noreferrer"">official repo of Zipkin</a>
I solve the my problem using the official docker image with a compose</p>

<pre><code>version: '3.1'

services:
  rabbitmq:
    image: rabbitmq:3-management
    restart: always
    ports:
      - 5672:5672
      - 15671:15671
      - 15672:15672
    networks:
      - messaging

  zipkin-server:
    image: openzipkin/zipkin
    ports:
      - 9065:9411
    environment:
      - zipkin.collector.rabbitmq.uri=amqp://guest:guest@rabbitmq:5672
    networks:
      - messaging

networks:
  messaging:
    driver: bridge
</code></pre>

<p>How you can see i use the streaming version. it for me work</p>

<p>I hope that is can help you</p>
"
Zipkin,54355078,50027127,0,"2019/01/24, 22:47:42",False,"2019/01/24, 22:47:42",21,9286335,0,"<p>try with this:</p>

<pre><code>&lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
          &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
          &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
          &lt;version&gt;2.12.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;version&gt;2.12.0&lt;/version&gt;
        &lt;/dependency&gt;
</code></pre>
"
Zipkin,50877783,49676752,0,"2018/06/15, 17:39:40",False,"2018/06/15, 17:39:40",140,7980867,0,"<p>I believe you should be able to as long as you use the fully qualified domain name. For instance, <code>zipkin.mynamespace.svc.cluster.local</code>.</p>
"
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818,1490322,6,"<p>Ok we found the problem and also a work around.</p>

<p>It looks like all the documentation out there is wrong, at least for the version of Spring Cloud Sleuth we are using.  The correct property is not <code>spring.sleuth.sampler.percentage</code>.  The correct property is <code>spring.sleuth.sampler.probability</code></p>

<p>And here is a workaround we found right before noticing that the property was wrong.</p>

<pre><code>@Bean
public Sampler alwaysSampler() {
    return Sampler.ALWAYS_SAMPLE;
}
</code></pre>

<p>Here are some official documentation from Spring Cloud that contain the wrong property.</p>

<p><a href=""https://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M5/single/spring-cloud-sleuth.html"" rel=""noreferrer"">https://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M5/single/spring-cloud-sleuth.html</a></p>

<p><a href=""https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html"" rel=""noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html</a></p>

<p>Here is the source code that is being used and it is using <code>probability</code> not <code>percentage</code>.</p>

<p><a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/sampler/SamplerProperties.java"" rel=""noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/sampler/SamplerProperties.java</a></p>
"
Zipkin,47777632,47764295,0,"2017/12/12, 18:47:54",True,"2017/12/12, 18:47:54",81,2232476,1,"<p>Creating custom zipkin servers is an unsupported configuration, but if you must all of the configuration options are documented in the project readme: <a href=""https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md</a></p>
"
Zipkin,50200855,47764295,0,"2018/05/06, 17:35:24",False,"2018/05/06, 17:35:24",69,2629308,1,"<p>For the dependencies part, the most important one is <strong>zipkin-autoconfigure-storage-elasticsearch-http</strong>, here's an full maven pom.xml example:</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
&lt;project
xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""
xmlns=""http://maven.apache.org/POM/4.0.0""
xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

&lt;groupId&gt;com.example&lt;/groupId&gt;
&lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
&lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;
&lt;name&gt;zipkin-server&lt;/name&gt;
&lt;url&gt;http://maven.apache.org&lt;/url&gt;

&lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;version&gt;1.5.2.RELEASE&lt;/version&gt;
    &lt;relativePath/&gt;
&lt;/parent&gt;

&lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;spring-cloud.version&gt;Dalston.SR1&lt;/spring-cloud.version&gt;
    &lt;zipkin.version&gt;1.23.2&lt;/zipkin.version&gt;
&lt;/properties&gt;

&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-collector-kafka10&lt;/artifactId&gt;
        &lt;version&gt;1.26.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-storage-elasticsearch-http&lt;/artifactId&gt;
        &lt;version&gt;${zipkin.version}&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p></p>

<p>For the configuration part, you will need the following in you <strong>application.yml</strong>:</p>

<pre><code>zipkin:
  storage:
    type: elasticsearch
    elasticsearch:
      hosts: localhost:9200
</code></pre>
"
Zipkin,64168840,47764295,0,"2020/10/02, 11:59:59",False,"2020/10/02, 11:59:59",511,2564032,0,"<p>I configured zipkin to use ES as a data storage on top of kubernetes. If it fits your requirement feel free to download and use <a href=""https://github.com/handysofts/zipkin-on-kubernetes"" rel=""nofollow noreferrer"">https://github.com/handysofts/zipkin-on-kubernetes</a></p>
"
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909,3067542,3,"<p>I found that these traces are actually generated by <a href=""https://github.com/spring-cloud/spring-cloud-consul/blob/master/spring-cloud-consul-discovery/src/main/java/org/springframework/cloud/consul/discovery/ConsulCatalogWatch.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-consul/blob/master/spring-cloud-consul-discovery/src/main/java/org/springframework/cloud/consul/discovery/ConsulCatalogWatch.java</a> </p>

<p>And since this is a class annotated with @scheduled , this Sleuth aspect applies : </p>

<p><a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/TraceSchedulingAspect.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/TraceSchedulingAspect.java</a></p>

<p>And therefore, the property to control the skipped regexp is not spring.sleuth.instrument.web.skipPattern , but spring.sleuth.instrument.<strong>scheduled</strong>.skip-pattern</p>
"
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336,1773866,3,"<p>Of course - you have to just provide your own logging format (e.g. via  <code>logging.pattern.level</code> - check <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html</a> for more info). Then you have to register your own <code>SpanLogger</code> bean implementation where you will take care of adding the value of a spring profile via MDC  (you can take a look at this as an example <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/log/Slf4jSpanLogger.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/log/Slf4jSpanLogger.java</a> )</p>

<p><strong>UPDATE:</strong></p>

<p>There's another solution for more complex approach that seems much easier than rewriting Sleuth classes. You can try the <code>logback-spring.xml</code> way like here - <a href=""https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/service1/src/main/resources/logback-spring.xml#L5-L11"" rel=""nofollow noreferrer"">https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/service1/src/main/resources/logback-spring.xml#L5-L11</a> . I'm resolving the application name there so maybe you could do the same with active profiles and won't need to write any code?</p>
"
Zipkin,40861860,40860284,0,"2016/11/29, 11:14:50",True,"2016/11/29, 11:14:50",42504,936832,2,"<p>As long as you have the ability to specify VM parameters, you can add the monitoring agent, regardless of the whether the JVM is started as a Windows service. For perfino, that VM parameter is</p>

<pre><code>-javaagent:[path to perfino.jar]
</code></pre>
"
Zipkin,43888530,40654863,0,"2017/05/10, 12:29:15",False,"2017/05/11, 04:27:33",130,1427954,3,"<p>please ensure config your zipkin sever correctly in your spring boot config file.
just like this:</p>

<pre><code>    logging:
      level.org.springframework.cloud: DEBUG
    spring:
      application:
        name: service-tracking
      sleuth:
        enabled: false
    zipkin:
        storage:
            type: mem
    server:
        port: 9411
</code></pre>

<p>And add below config in your zipkin client spring boot config file:</p>

<pre><code>  sleuth:
        enabled: true
        sampler:
          percentage: 1
      zipkin:
        enabled: true
        baseUrl: http://${tracking-host:tracking}:9411
</code></pre>
"
Zipkin,39597817,39597545,0,"2016/09/20, 18:15:25",True,"2016/09/20, 18:15:25",8336,1773866,2,"<p>We have a <code>LazyTraceExecutor</code> that you can use - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/async/LazyTraceExecutor.java"" rel=""nofollow"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/async/LazyTraceExecutor.java</a> . </p>
"
Zipkin,41629061,38738337,0,"2017/01/13, 09:02:26",False,"2017/01/13, 09:02:26",762,1227937,2,"<p>There are a bunch of ways to answer this, but I'll answer it from the ""one-way"" perspective. The short answer though, is I think you have to roll your own right now!</p>

<p>While Kafka can be used in many ways, it can be used as a transport for unidirectional single producer single consumer messages. This action is similar to normal one-way RPC, where you have a request, but no response.</p>

<p>In Zipkin, an RPC span is usually request-response. For example, you see timing of the client sending to the server, and also the way back to the client. One-way is where you leave out the other side. The span starts with a ""cs"" (client send) and ends with a ""sr"" (server received).</p>

<p>Mapping this to Kafka, you would mark client sent when you produce the message and server received when the consumer receives it.</p>

<p>The trick to Kafka is that there is no nice place to stuff the trace context. That's because unlike a lot of messaging systems, there are no headers in a Kafka message. Without a trace context, you don't know which trace (or span for that matter) you are completing!</p>

<p>The ""hack"" approach is to stuff trace identifiers as the message key. A less hacky way would be to coordinate a body wrapper which you can nest the trace context into.</p>

<p>Here's an example of the former:</p>

<p><a href=""https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30"" rel=""nofollow noreferrer"">https://gist.github.com/adriancole/76d94054b77e3be338bd75424ca8ba30</a></p>
"
Zipkin,42805876,38738337,0,"2017/03/15, 11:36:01",False,"2017/03/15, 11:36:01",181,6708214,0,"<p>I meet the same problem too.Here is my solution, a less hacky way as above said.</p>

<pre><code>ServerSpan serverSpan = brave.serverSpanThreadBinder().getCurrentServerSpan();
TraceHeader traceHeader = convert(serverSpan);

//in kafka producer,user KafkaTemplete to send
String wrapMsg = ""wrap traceHeader with originMsg "";
kafkaTemplate.send(topic, wrapMsg).get(10, TimeUnit.SECONDS);// use synchronization


//then in kafka consumer
 ConsumerRecords&lt;String, String&gt; records = consumer.poll(5000);
 // for loop 
 for (ConsumerRecord&lt;String, String&gt; record : records) {
     String topic = record.topic();
     int partition = record.partition();
     long offset = record.offset();
     String val = record.value();
     //parse val to json
     Object warpObj = JSON.parseObject(val);
     TraceHeader traceHeader = warpObj.getTraceHeader();
     //then you can do something like this
     MyRequest myRequest = new MyRequest(traceHeader, ""/esb/consumer"", ""POST"");

     brave.serverRequestInterceptor().handle(new HttpServerRequestAdapter(new MyHttpServerRequest(myRequest), new DefaultSpanNameProvider()));

    //then some httprequest within brave-apache-http-interceptors
    //http.post(url,content)
 }
</code></pre>

<p><strong>you must implements MyHttpServerRequest and MyRequest.It is easy,you just return something a span need,such as uri,header,method.</strong>
This is a rough and ugly code example,just offer an idea.</p>
"
Zipkin,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336,1773866,1,"<p>Yes you can, and I have shown that numerous times during my presentations (<a href=""https://toomuchcoding.com/talks"" rel=""nofollow noreferrer"">https://toomuchcoding.com/talks</a>) and we describe it extensively in the documentation (<a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/</a>). Sleuth will set up your logging pattern which you can then parse and visualize using the ELK stack. Sleuth takes care of tracing context propagation and can send the spans to a span storage (e.g. Zipkin or Jaeger). Sleuth does take care of updating the MDC for you. Please always read the documentation and the project page before filing a question</p>
"
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66,12105655,1,"<p><code>server.address=&lt;ip&gt;</code> does not work?</p>
<p><code>java -jar zipkin.jar --server.address=192.168.0.7</code></p>
<p>If it's not working you can add a property file and connects to it when the server starts:</p>
<p><code>java -jar zipkin.jar --spring.config.location=./application.properties</code></p>
<p>in application.properties:</p>
<pre><code>armeria:
  ports:
    - port: 8080
      ip: 192.168.0.7
      protocol: HTTP
</code></pre>
"
Zipkin,66808417,66777772,2,"2021/03/26, 00:23:10",False,"2021/03/26, 00:23:10",46,14172753,0,"<p>I'm not entirely sure if that's what you mean, but you can use Jeager <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/</a>  which checks if trace-id already exist in the invocation metadata and in it generate child trace id. Based on all trace ids call diagrams are generated</p>
"
Zipkin,66654542,66517888,1,"2021/03/16, 13:40:16",False,"2021/03/16, 13:40:16",19,12338209,0,"<p>In your command please try the following -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin</p>
"
Zipkin,66782647,66517888,1,"2021/03/24, 15:58:38",True,"2021/03/24, 15:58:38",16,15470262,0,"<p>i had the same issue, but i solved with this jvm arguments:</p>
<p>-Dotel.traces.exporter=zipkin -Dotel.metrics.exporter=none -Dotel.exporter.zipkin.endpoint=http://localhost:9411/api/v2/spans
Maybe the error is on zipkin.endpoint, try to write the entire url.</p>
<p>Regards,
Marco</p>
"
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11,15274583,1,"<p>I get the same problem and below command did the trick.</p>
<pre><code>java -javaagent:tools/opentelemetry-javaagent-all.jar \
-Dotel.traces.exporter=zipkin \
-jar target/*.jar
</code></pre>
<p>I checked the source code. It looks the property name has been changed:</p>
<p><a href=""https://github.com/open-telemetry/opentelemetry-java/blob/14ace1ec32dbb194b8990763beb3ab6935849547/sdk-extensions/autoconfigure/src/main/java/io/opentelemetry/sdk/autoconfigure/TracerProviderConfiguration.java#L43"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-java/blob/14ace1ec32dbb194b8990763beb3ab6935849547/sdk-extensions/autoconfigure/src/main/java/io/opentelemetry/sdk/autoconfigure/TracerProviderConfiguration.java#L43</a></p>
"
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433,243104,3,"<p>Getting a handle on the distributed tracing space can be a bit confusing. Here's a quick summary...</p>
<p><strong>Open Source Tracers</strong></p>
<p>There are a number of popular open source tracers, which is where Zipkin sits:</p>
<ul>
<li>Zipkin</li>
<li>Jaeger</li>
<li>Haystack</li>
</ul>
<p><strong>Commercial Tracers</strong></p>
<p>There are also a lot of vendors offering commercial monitoring/observability tools which are either centred around or include distributed tracing:</p>
<ul>
<li>Appdynamics</li>
<li>AWS X-Ray</li>
<li>Azure Application Insights</li>
<li>Datadog</li>
<li>Dynatrace</li>
<li>Google Cloud Trace</li>
<li>Honeycomb</li>
<li>Lightstep</li>
<li>New Relic</li>
<li>SignalFX</li>
<li>(probably 100 more...)</li>
</ul>
<p><strong>Standardisation Efforts</strong></p>
<p>Alongside all these products are numerous attempts at creating standards around distributed tracing. These typically start by creating a standard API for the trace-recording side of the architecture, and sometimes extend to become prescriptive about the content of traces or even the wire format. This is where OpenTracing fits in. So it is not a tracing solution itself, but an API that can be implemented by the trace recording SDKs of multiple tracers, allowing you to swap between vendors more easily. The most common standards are:</p>
<ul>
<li>OpenTracing</li>
<li>OpenCensus</li>
<li>OpenTelemetry</li>
</ul>
<p>Note that the first two in the list have been abandoned, with their contributors joining forces to create the third one together.[1]</p>
<p>[1] <a href=""https://opensource.googleblog.com/2019/05/opentelemetry-merger-of-opencensus-and.html"" rel=""nofollow noreferrer"">https://opensource.googleblog.com/2019/05/opentelemetry-merger-of-opencensus-and.html</a></p>
"
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301,1423685,0,"<p>I was using it with default storage which is discouraged in production use, it can handle only small amount of data and can be treated only as a demo version.</p>
<p>What helped a little was setting</p>
<p><code>spring.sleuth.sampler.probability: 0.01 </code></p>
<p>-- by default it logs all spans.</p>
"
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29,9544181,1,"<p>You should create application.properties file and after that you should add the following</p>
<p>Your application.properties :</p>
<pre><code>server.port:9411 // Not Required
management.metrics.web.server.auto-time-requests=false //Required
</code></pre>
<p>Your main class :</p>
<pre><code>import zipkin.server.EnableZipkinServer;

@EnableZipkinServer
@SpringBootApplication
public class ZipkinServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZipkinServerApplication.class, args);

    }

}
</code></pre>
<p>Your Pom.xml :</p>
<pre><code> &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;

    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.7.RELEASE&lt;/version&gt;
        &lt;relativePath /&gt;
    &lt;/parent&gt;

    &lt;dependencies&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
            &lt;version&gt;2.11.7&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
            &lt;version&gt;2.11.7&lt;/version&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>
<p>Your zipkin port :</p>
<pre><code>http://localhost:9411/zipkin
</code></pre>
"
Zipkin,61807946,61800994,1,"2020/05/15, 00:44:40",False,"2020/05/15, 00:44:40",985,598932,1,"<p>It looks like it is related to <a href=""https://github.com/openzipkin/zipkin-js/pull/498"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin-js/pull/498</a>, could you try with zipkin-context-cls@0.19.2-alpha.7 and change <code>ctxImpl</code> into <code>ctxImpl = new CLSContext('zipkin', true);</code>?</p>
"
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911,4668,0,"<p>The problem ended up not being on Zipkin's end, but instead in how I was instrumenting the express server.  </p>

<pre><code>app.get('/main', async function (req, res) {
    //...
})

app.use(zipkinMiddleware({tracer}));
</code></pre>

<p>I had added the zipkin middleware <em>after</em> my call to <code>app.get</code>.  Express executes middlwares in order and makes no distinction between middleware for a named route vs. something added via <code>app.use</code>. </p>

<p>Doing things like this</p>

<pre><code>app.use(zipkinMiddleware({tracer}));

app.get('/main', async function (req, res) {
    //...
})
</code></pre>

<p>Gave me the result I was looking for.     </p>
"
Zipkin,61401330,61368689,0,"2020/04/24, 07:46:24",True,"2020/04/24, 07:46:24",21,11657025,1,"<p>The reason behind error was that i forgot to add the kafka dependency in pom.xml
After adding the dependency, error was gone.</p>
"
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46,10967262,0,"<p>According to Sleuth documentation, AWS SQS is &quot;natively&quot; supported only on the consumer's side:</p>
<p><a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current-SNAPSHOT/reference/html/#spring-cloud-aws-messaging-sqs"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/current-SNAPSHOT/reference/html/#spring-cloud-aws-messaging-sqs</a></p>
<p>In order to add seamless tracing over AWS SQS I resorted to Brave SQS instrumentation (aka SqsMessageTracing) and had to add another dependency:</p>
<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;io.zipkin.aws&lt;/groupId&gt;
      &lt;artifactId&gt;brave-instrumentation-aws-java-sdk-sqs&lt;/artifactId&gt;
      &lt;version&gt;0.21.2&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<p>and have the following configuration:</p>
<pre><code>@Configuration
public class SQSConfig {

  @Autowired
  private Tracing tracing;

  @Autowired
  private AWSCredentialsProvider awsCredentialsProvider;

  @Autowired
  private RegionProvider regionProvider;

  @Bean
  public AmazonSQSAsync amazonSQS() {
    SqsMessageTracing sqsMessageTracing = SqsMessageTracing.create(tracing);

    return AmazonSQSAsyncClientBuilder.standard()
      .withRegion(regionProvider.getRegion().getName())
      .withCredentials(awsCredentialsProvider)
      .withRequestHandlers(sqsMessageTracing.requestHandler())
      .build();
  }

  @Bean
  public QueueMessagingTemplate queueMessagingTemplate(AmazonSQSAsync sqsClient) {
    QueueMessagingTemplate template = new QueueMessagingTemplate(sqsClient);
    template.setMessageConverter(getMappingJackson2MessageConverter());
    return template;
  }
}
</code></pre>
<p>This is just because I didn't want to do the SQS producer instrumentation myself nor add the tracing headers programmatically.
Little reference for the Brave instrumentation can be found here:</p>
<p><a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/1550#issuecomment-589686583"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/1550#issuecomment-589686583</a></p>
<p>My SQS message producer looks like this:</p>
<pre><code>@Component
public class MessageAdapter {

  @Autowired
  private final QueueMessagingTemplate queueMessagingTemplate;


  public void sendSqsMessage(Object payload) {
    HashMap&lt;String, Object&gt; headers = new HashMap&lt;&gt;();
    headers.put(&quot;message-group-id&quot;, UUID.randomUUID().toString());
    queueMessagingTemplate.convertAndSend(&quot;sqs-queue-1&quot;, payload, headers);
  }
}
</code></pre>
<p><strong>FINAL NOTE</strong></p>
<p>Not required but I also excluded the</p>
<pre><code>org.springframework.cloud.aws.autoconfigure.context.ContextInstanceDataAutoConfiguration
</code></pre>
<p>Since it performs an AWS environment configuration scan at application startup which wasn't required for me (and also raised a lengthy error log)</p>
<pre><code>@SpringBootApplication
// Unwanted autoconfiguration, which raises a lengthy warning at startup,
// brought in by Brave AWS SQS instrumentation
@EnableAutoConfiguration(exclude = ContextInstanceDataAutoConfiguration.class)
public class MainSpringBootApplication {

  public static void main(String[] args) {
    SpringApplication.run(MainSpringBootApplication.class);
  }

}
</code></pre>
"
Zipkin,66417136,60631319,0,"2021/03/01, 07:37:26",False,"2021/03/01, 07:37:26",438,3404777,0,"<p>I think you must have found your answer by now. But I am posting this for future reference.</p>
<p>Take a look at this <a href=""https://github.com/openzipkin/zipkin/issues/1870"" rel=""nofollow noreferrer"">Github issue</a>, it basically explains everything and provides a few workarounds.</p>
"
Zipkin,58216010,58214695,0,"2019/10/03, 12:18:37",True,"2019/10/03, 12:18:37",381,10371480,1,"<p>According to <a href=""https://zipkin.io/pages/tracers_instrumentation.html"" rel=""nofollow noreferrer"">this</a> Spring Cloud Sleuth is the only tracer that supports messaging. </p>
"
Zipkin,58220753,58214695,0,"2019/10/03, 16:58:34",False,"2019/10/03, 16:58:34",81,2232476,1,"<p>Brave is the library spring cloud sleuth is built on, therefore you could make it work without sleuth: <a href=""https://github.com/openzipkin/brave"" rel=""nofollow noreferrer"">https://github.com/openzipkin/brave</a></p>

<p>Just to clarify though, Sleuth doesn't force you to use any of the rest of the spring-cloud components. It is <code>spring-cloud</code> because it is one of the ""cloud native"" spring technologies</p>
"
Zipkin,57911578,57892994,0,"2019/09/12, 20:10:36",True,"2019/09/12, 20:10:36",99,7474991,2,"<p>ok I finally realized whats the mistake that I have done when I was starting my zipkin server with this command</p>

<pre><code> java -jar zipkin-server-2.16.2-exec.jar
</code></pre>

<p>but I was not specifying my Zipkin server where my Kafka is running so when I did </p>

<pre><code> set KAFKA_BOOTSTRAP_SERVERS=localhost:9092
 java -jar zipkin-server-2.16.2-exec.jar 
</code></pre>

<p>it worked </p>
"
Zipkin,58040373,57703884,0,"2019/09/21, 15:42:54",False,"2019/09/21, 15:42:54",1477,4051158,0,"<p>I am new to zipkin and golang, If you want to trace internal process, then you can create span from context</p>

<p>example: say you have api called Login, inside login you might perform database operation or any other operations</p>

<pre><code>func Login(res http.ResponseWriter, req *http.Request) {
    span, _ := tracer.StartSpanFromContext(req.Context(), ""database"")
    databaseOperation()
    span.Finish()
}
</code></pre>
"
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287,8631898,0,"<p>On my project, we generated the spans manually before sending the events.</p>
<blockquote>
<p>var span = tracing.tracer().nextSpanWithParent(req -&gt; true,
Void.class, ctx.get(Span.class).context());</p>
<p>span.name(&quot;yourSpanName&quot;).start();</p>
<p>return sendEventPublisher.doOnError(span::error).doOnTerminate(span::finish);</p>
</blockquote>
<p>This way, we also link the span to the publisher lifecycle as we had problems with webflux sharing spans between threads.</p>
<p>Basically, we create a span and link it to the parent context created by Spring for the request (either from an incoming B3 HTTP header, or generated if absent). &quot;ctx&quot; is the subscriber context here.</p>
<p>This also implied to tell sleuth not to generate the spans for async operations in application.properties:</p>
<blockquote>
<p>spring.sleuth.async.enabled=false</p>
</blockquote>
"
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524,746528,1,"<p>For basic authentication, the username and password are required to be sent as part of the HTTP Header <code>Authorization</code>. The header value is computed as Base64 encoding of the string <code>username:password</code>.So if the username is <code>abcd</code> and password is <code>1234</code>, the header will look something like this (Chatset used: UTF-8).</p>

<blockquote>
  <p>Authorization: Basic YWJjZDoxMjM0</p>
</blockquote>

<p>Sleuth cloud project provides <code>ZipkinRestTemplateCustomizer</code> to configure the <code>RestTemplate</code> used to communicate with the Zipkin server.</p>

<p>Refer to the documentation for the same: 
<a href=""https://cloud.spring.io/spring-cloud-sleuth/reference/html/#sending-spans-to-zipkin"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/reference/html/#sending-spans-to-zipkin</a></p>

<p>Note: Base64 encoding is reversible and hence Basic auth credentials are not secured. HTTPS communication should be used along with Basic Authentication. </p>
"
Zipkin,63461779,54986635,0,"2020/08/18, 07:12:18",False,"2020/08/18, 07:12:18",3,12639023,0,"<p>I got same problem. Seem Spring boot <strong>2.1.2.RELEASE</strong> not work with Zipkin. Please upgrade to Spring boot version &gt; <strong>2.1.2.RELEASE</strong>.</p>
"
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568,5238035,2,"<p>For those who could come across with a same scenario like this,</p>

<p><strong>github</strong> has given <strong>APIs</strong> to get details on the repository tag set of each project release as a json object (<a href=""https://api.github.com/repos/openzipkin/zipkin/tags"" rel=""nofollow noreferrer"">https://api.github.com/repos/openzipkin/zipkin/tags</a> ). So that can be used to get the latest version of zipkin.</p>

<p>To get the currently running version of my system, zipkin has given an actuator/info end point (<a href=""http://localhost:9411/actuator/info"" rel=""nofollow noreferrer"">http://localhost:9411/actuator/info</a>).</p>
"
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767,2498986,1,"<p>Yes. You have to use Brave. In fact, Spring cloud sleuth (V2) uses Brave under the hood. Check the brave web-mvc example to get started.</p>

<p><a href=""https://github.com/openzipkin/brave-webmvc-example"" rel=""nofollow noreferrer"">https://github.com/openzipkin/brave-webmvc-example</a></p>
"
Zipkin,51042639,51040315,0,"2018/06/26, 15:11:31",True,"2018/06/26, 15:11:31",1273,4035426,1,"<p>Try to change all properties with this:</p>

<pre><code>#Sleuth
spring.sleuth.sampler.percentage=1.0
#Zipkin
spring.zipkin.sender.type=web
</code></pre>

<p>spring.sleuth.sampler.percentage=1.0 is Edgware
so you need that</p>

<p>baseUrl by default is localhost</p>
"
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728,5254103,0,"<p>When I change the project root log level to ""debug"", I saw some error report from zipkin. Then I realized that the zipkin server I used was very very old. And the zipkin API call returned 404.</p>

<p>When I updated my zipkin server to latest version. It worked.</p>
"
Zipkin,50599592,50599433,4,"2018/05/30, 11:14:23",False,"2018/05/30, 11:14:23",294930,208809,0,"<p>According to <a href=""https://istio.io/docs/tasks/telemetry/distributed-tracing.html"" rel=""nofollow noreferrer"">the section titled Cleanup in the Istio docs</a>:</p>

<pre><code>kubectl delete -f install/kubernetes/addons/zipkin.yaml
</code></pre>
"
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336,1773866,1,"<blockquote>
  <p>However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans). If I use org.slf4j.Logger to simply LOG.info(""something""), I see the INFO message in console output, with the exportable flag set to true:</p>
</blockquote>

<p>You can't send logs to Zipkin. You can send log statements to ELK. You can check out the sample <a href=""https://github.com/marcingrzejszczak/vagrant-elk-box"" rel=""nofollow noreferrer"">https://github.com/marcingrzejszczak/vagrant-elk-box</a> that has a vagrant box with ELK, uses Sleuth for log correlation and uses ELK to visualize the logs</p>
"
Zipkin,50527821,50506432,0,"2018/05/25, 14:08:47",False,"2018/05/25, 14:08:47",201,7956609,1,"<p>To log only request with particular error you can add the log in your exception mapper where you are handling those error. </p>

<p>To show the log for error response you can set like below,</p>

<pre><code>    @Autowired
    private Tracer tracer;
</code></pre>

<p>and set </p>

<pre><code>    tracer.addTag(""error"",""Your message"")
</code></pre>
"
Zipkin,50478519,50389884,0,"2018/05/23, 04:20:43",False,"2018/05/23, 04:20:43",201,7956609,1,"<p>You can add the trace id in your logback.xml</p>

<p>""request_id"":
                {""trace_id"":""%X{X-B3-TraceId}"",""span_id"":""%X{X-B3-SpanId}"",""parent_span_id"":""%X{X-B3-ParentSpanId}""},</p>
"
Zipkin,49407790,49396204,0,"2018/03/21, 15:35:06",False,"2018/03/21, 15:35:06",11,7472851,0,"<p>I found the solution I think. I changed it to this:</p>

<p>RABBIT_URI=amqp://user:password@tracing:5672</p>
"
Zipkin,48628487,48626548,8,"2018/02/05, 19:47:34",True,"2018/02/05, 19:47:34",8336,1773866,0,"<p>Please use latest snapshots. Sleuth in latest snapshots uses brave internally so integration will be extremely simple.</p>
"
Zipkin,47363721,47343439,2,"2017/11/18, 09:34:23",False,"2017/11/18, 09:34:23",8336,1773866,3,"<p>This feature is available in edgware release train. That corresponds to version 1.3.x of sleuth </p>
"
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81,2232476,0,"<p>The Zipkin UI is making an AJAX request to the API in order to retrieve the data that is displayed. You can find the API definition for zipkin at:</p>

<p><a href=""https://github.com/openzipkin/zipkin-api"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin-api</a></p>

<p>I believe you are looking for the URL: <a href=""http://zipkin.iamplus.xyz/api/v1/traces"" rel=""nofollow noreferrer"">http://zipkin.iamplus.xyz/api/v1/traces</a></p>

<p>From there you will get the traces matching your filter</p>
"
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86,7146447,0,"<p>I have the same config running on my ingress 9.0-beta.11. I guess it's just a misconfiguration.</p>

<p>First I'll recommend you to not change the template and use the default values and just change when the basic-auth works.</p>

<p>What the logs of ingress show to you? Did you create the basic-auth file in the same namespace of the ingress resource?</p>
"
Zipkin,43843859,43790619,0,"2017/05/08, 12:12:55",True,"2017/05/08, 12:12:55",8336,1773866,1,"<p>The issue got fixed - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/585"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/585</a> . In the upcoming releases 1.1.5 and 1.2.1 it should work</p>
"
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336,1773866,4,"<p>Spring Cloud Zipkin Stream is using Spring Cloud Stream underneath. You need to provide how do you want to send the spans to Zipkin - thus you need a binder. One possible binder is the RabbitMQ binder. Check out this: <a href=""https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/zipkin-server/build.gradle#L6"" rel=""nofollow noreferrer"">https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/zipkin-server/build.gradle#L6</a></p>
"
Zipkin,37867634,37842256,0,"2016/06/16, 22:15:19",True,"2016/06/16, 22:15:19",911,1822028,1,"<p>It seems that Brave does not support this. An issue has been reported on their GitHub page. <a href=""https://github.com/openzipkin/brave/issues/166"" rel=""nofollow"">https://github.com/openzipkin/brave/issues/166</a></p>
"
Zipkin,17684615,17017657,0,"2013/07/16, 21:48:17",False,"2019/09/02, 10:04:21",1124,819626,1,"<p>I don't know much about ActiveMQ but you need to pass the zipkin trace information along.</p>

<p>Review the <em>ActiveMQ Collector</em> section in this doc 
<a href=""https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md</a> </p>

<p>I just set up Zipkin tracing for a stack that includes RabbitMQ. I added the parent_span_id, and span_id to the message header before the message is placed on the queue. Then the applications that read the messages get the trace information from the header.</p>

<p>And if you need more help, I recommend jumping on IRC #zipkin.</p>
"
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693,971735,1,"<p>I was not able to reproduce your issue with the <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-zipkin"" rel=""nofollow noreferrer"">spring-cloud-sleuth-sample-zipkin</a> app (it worked to me), here's what I did:</p>
<ol>
<li>Added <code>org.springframework.amqp:spring-rabbit</code> to the <code>pom.xml</code></li>
<li>Added <code>spring.zipkin.sender.type: rabbit</code> to the <code>application.yml</code></li>
<li>Started RabbitMQ using <a href=""https://github.com/jonatan-ivanov/local-services/blob/main/rabbit/docker-compose.yml"" rel=""nofollow noreferrer"">this docker-compose.yml</a></li>
<li>Manually created a queue (named <code>zipkin</code>) on the Rabbit Management UI</li>
<li>Started the app and hit it with a request</li>
<li>Manually get the messages out of the queue and checked if they have the right payload (they did)</li>
</ol>
<p>A few pointers to troubleshoot this:</p>
<ol>
<li>You should see this log event at startup</li>
</ol>
<pre><code>Created new connection: rabbitConnectionFactory#21917b6f:0/SimpleConnection@40803682 [delegate=amqp://guest@127.0.0.1:5672/, localPort= 60265]
</code></pre>
<ol start=""2"">
<li>The connection is created by <a href=""https://github.com/spring-projects/spring-amqp/blob/master/spring-rabbit/src/main/java/org/springframework/amqp/rabbit/connection/AbstractConnectionFactory.java#L561"" rel=""nofollow noreferrer"">AbstractConnectionFactory</a>, you can debug it</li>
<li>Telnet (<code>telnet localhost 5672</code>) can help to troubleshoot connectivity issues</li>
</ol>
<p>Try to make it work using the <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-zipkin"" rel=""nofollow noreferrer"">sample</a> and try to bring the working example closer to your app (by adding dependencies) and see what is the difference between the two and where will it break. If you can create a minimal sample app (e.g.: based on the zipkin sample) that reproduces the issue, please feel free to create an issue on GH: <a href=""https://github.com/spring-cloud/spring-cloud-sleuth"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth</a> and tag me (<code>@jonatan-ivanov</code>), I can take a look.</p>
"
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830,7349864,1,"<h2>AUTORESPONSE</h2>
<p>Finally I found it. I had 2 problemas</p>
<p>1 - I was using zipkin-slim docker image for my zip container. This image doesn't contain the rabbitmq collector <a href=""https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq"" rel=""nofollow noreferrer"">rabbitmq collector</a>. I have replaces by standar zipkin image</p>
<p>2 - I do not know why, but the connection from sleuth/zipkin to RabbitMQ is not retrying (I will investigate further). So, if I was in a hurry and test very early (when RabbitMQ is not yet available) it fails, and not retries.</p>
<p>My docker-compose relevant sections now are like this:</p>
<pre><code>  rabbitmq:  
    image: rabbitmq:3.7.8-management  
    mem_limit: 350m
    expose:
      - &quot;5672&quot;
      - &quot;15672&quot;
    ports:    
      - 5672:5672    
      - 15672:15672  
    healthcheck:    
      test: [&quot;CMD&quot;, &quot;rabbitmqctl&quot;, &quot;status&quot;]    
      interval: 10s    
      timeout: 5s    
      retries: 10


  # https://hub.docker.com/r/openzipkin/zipkin
  zipkin:
    #image: openzipkin/zipkin-slim
    image: openzipkin/zipkin
    mem_limit: 512m
    expose:
      - &quot;9411&quot;
    ports:
      - &quot;9411:9411&quot;
    environment:
    - RABBIT_ADDRESSES=rabbitmq
    - STORAGE_TYPE=mem
    depends_on:
      rabbitmq:
        condition: service_healthy

</code></pre>
<p>Thanks again to <a href=""https://stackoverflow.com/users/971735/jonatan-ivanov"">Jonatan Ivanov</a> for helping me!</p>
"
Zipkin,58554326,58551796,0,"2019/10/25, 10:49:15",True,"2019/10/25, 10:49:15",762,1227937,1,"<p>probably best to have the issue you raised in github vs cross posting. it is a bug <a href=""https://github.com/honeycombio/honeycomb-opentracing-proxy/issues/37"" rel=""nofollow noreferrer"">https://github.com/honeycombio/honeycomb-opentracing-proxy/issues/37</a></p>
"
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83,2702332,0,"<p>There are 2 entries in mysql zipkin_spans table </p>

<ol>
<li><strong>trace_id_high</strong> -> corresponds to decimal representation of first 16 hex character </li>
<li><strong>id</strong> -> corresponds to decimal representation of lower 16 hex character</li>
</ol>

<p><strong>Example</strong></p>

<p>32 character hex trace id <strong>5ec92d0240cd9dee0421f4763e9f674f</strong> displayed in zipkin ui corresponds to </p>

<p><strong>trace_id_high = 6830039797584469486 in mysql</strong> (5EC92D0240CD9DEE -> upper 16 hex character)</p>

<p><strong>id = 297787839077115727 in mysql</strong> (421F4763E9F674F -> lower 16 hex charecter)</p>
"
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167,486631,0,"<p>After continue efforts and going throgh core api of spring boot application I got my solution:)</p>
<p>Root cause of my issue is below :</p>
<blockquote>
<p>MY application using Spring boot RabitMQ integration and due that
zipkin taking 1st prefrance to RabitMQ sender and my trace are ignored
my zipkin server.</p>
</blockquote>
<p>So use below configration is anyone has same issue to avoid lot painless efforts , even we are not getting in logs of server root cause of it</p>
<p>*---</p>
<pre><code>spring:
  application:
    name: 'active-listener'
  profiles: 'dev'
  sleuth:
    async:
      enabled: false
    annotation:
      enabled: true
  enabled: true
  sampler:
    probability: 1.0
  zipkin:
    baseUrl: http://localhost:9411
    enabled: true
    sender:
      type: web*
</code></pre>
"
Zipkin,53370111,53369694,0,"2018/11/19, 09:32:06",True,"2018/11/19, 09:32:06",463,1749786,2,"<p>I was using Finchley.SR2 train of releases. Once I upgraded to the latest Spring Boot and Spring Cloud versions, the issue fixed itself. </p>

<p>I removed the opentracing-spring-cloud-starter dependency and am now just using </p>

<pre><code>   &lt;dependency&gt;
        &lt;groupId&gt;io.opentracing.brave&lt;/groupId&gt;
        &lt;artifactId&gt;brave-opentracing&lt;/artifactId&gt;
    &lt;/dependency&gt;
</code></pre>
"
Zipkin,52739988,52377663,0,"2018/10/10, 15:12:13",False,"2018/10/10, 15:12:13",201,7956609,0,"<p>Check your configuration file and make sure the baseUrl is given properly here</p>

<pre><code>    spring.zipkin.baseUrl
</code></pre>
"
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301,2756547,0,"<p>OK! I see now the problem!</p>

<p>So, you say that your HTTP request has these tracing headers: <code>X-B3-TraceId</code>, <code>X-B3-SpanId</code>, <code>X-B3-Sampled</code>, <code>X-Span-Name</code>, <code>X-B3-ParentSpanId</code>.</p>

<p>Then you have this code:</p>

<pre><code>rabbitTemplate.convertAndSend(""shipping-task"", shipment);
</code></pre>

<p>And that's absolutely natural that your tracing header are not transferred to the RabbitMQ: there is just no those headers to send. </p>

<p>I believe that you can extract those headers in this <code>@RequestMapping</code> method and populate them to the AMQP message before sending. See <code>org.springframework.amqp.core.MessageBuilder</code>.</p>

<p>I also think that Spring Cloud Sleuth should have some mechanism to obtain those headers, e.g. <code>Tracer.currentSpan()</code>: <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.RC1/single/spring-cloud-sleuth.html#_current_span"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.RC1/single/spring-cloud-sleuth.html#_current_span</a></p>
"
Zipkin,48063504,47992456,0,"2018/01/02, 17:22:45",False,"2018/01/02, 17:22:45",81,2232476,0,"<p>Yes, they are both stateless. You can deploy them using whatever horizontal-scalability construct is available to you. </p>
"
Zipkin,47630471,47630294,3,"2017/12/04, 11:45:28",True,"2017/12/04, 13:10:19",32952,6603816,3,"<p>When connecting to the mysql container while using links, you need to use the container name as a hostname.</p>

<p>Change the connection string to:</p>

<pre><code>url: jdbc:mysql://${mysqlHost:0.0.0.0}:3306/zipkin?autoReconnect=true
</code></pre>

<p>And when starting the zipkin container, set the env variable:</p>

<pre><code>docker run -p 9411:9411 --name zipkinServer --link mysqlDocker -d -e mysqlHost=mysqlDocker zipkin
</code></pre>
"
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336,1773866,1,"<p>Why are you mocking a span? This makes absolutely no sense. Also a Span is never a bean. You already create a normal span via a builder and you should leave that. Assuming that you have set up the Boot context property and  you want to mock out <code>tracer</code> bean you should do the following</p>

<pre><code>public class AbstractSpanAccessorTest {
@MockBean
private Tracer tracer;

private Random random = new Random();

@Before
public void mockSpan() {
  long id = createId();
  Span spanMock = Span.builder().name(""mock"").traceId(id).spanId(id).build();
  doReturn(spanMock).when(tracer).getCurrentSpan();
  doReturn(spanMock).when(tracer).createSpan(anyString());
}

private long createId() {
  return random.nextLong();
}
}
</code></pre>
"
Zipkin,44636339,44611370,0,"2017/06/19, 20:13:49",False,"2017/06/19, 20:13:49",35,440061,0,"<p>*sigh, so it turns out, that someone had turned off zipking tracing in a properties file, for no good reason.
*sigh</p>
"
Zipkin,43401515,43394714,2,"2017/04/13, 23:52:30",True,"2017/04/13, 23:52:30",81,2232476,2,"<p>You are trying to run 2 different applications.</p>

<p>To run the <code>zipkin</code> application with with ElasticSearch and Kafka you will need to run it with both sets of environment variables:</p>

<pre><code>KAFKA_ZOOKEEPER=kafka1:2181,kafka2:2181 KAFKA_GROUP_ID=zipkin STORAGE_TYPE=elasticsearch ES_HOSTS=es5_1:9200 java -jar /opt/zipkin/bin/zipkin.jar --logging.level.zipkin=DEBUG
</code></pre>

<p>Once you have the <code>zipkin</code> server running with ES, then you can use your second command to generate the data for the dependency graph view</p>
"
Zipkin,41842532,41823536,0,"2017/01/25, 04:43:12",False,"2017/01/25, 04:43:12",762,1227937,0,"<p>At the moment, there's no replacement for the ""thread binder"" apis. There will be in the coming months. This is indeed needed to renovate existing instrumentation.</p>

<p>Until then, you can re-use thread binders via TracerAdapter or use a different in-process propagation library. The following link includes a working example <a href=""https://github.com/openzipkin/brave/tree/master/brave#upgrading-from-brave-3"" rel=""nofollow noreferrer"">https://github.com/openzipkin/brave/tree/master/brave#upgrading-from-brave-3</a></p>
"
Zipkin,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474,4682632,3,"<p>Zipkin only does tracing. APM tools like Appdynamics do other monitoring (browser, mobile, database, server, network). Code-level diagnostics with automated overhead controls and limiters. Don't forget log analytics and transaction analytics. It also collects metrics. </p>

<p>There is a lot more to APM than just tracing, which is what Zipkin does. You could do this with a stack of 20 open source tools, but you'd have to deal with disjointed UIs and data models not to mention the work associated with keeping them all working.</p>
"
Zipkin,64209717,64201190,0,"2020/10/05, 16:34:52",False,"2020/10/05, 16:34:52",2576,12014434,1,"<p>According to <a href=""https://istio.io/latest/docs/ops/integrations/jaeger/#option-2-customizable-install"" rel=""nofollow noreferrer"">istio</a> documentation:</p>
<blockquote>
<h3>Option 2: Customizable install<a href=""https://istio.io/latest/docs/ops/integrations/jaeger/#option-2-customizable-install"" rel=""nofollow noreferrer""></a></h3>
<p>Consult the  <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">Jaeger documentation</a>  to
get started. No special changes are needed for Jaeger to work with
Istio.</p>
<p>Once Jaeger is installed, you will need to point Istio proxies to send
traces to the deployment. This can be configured with  <code>--set values.global.tracer.zipkin.address=&lt;jaeger-collector-address&gt;:9411</code>
at installation time. See the
<a href=""https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#Tracing"" rel=""nofollow noreferrer""><code>ProxyConfig.Tracing</code></a>
for advanced configuration such as TLS settings.</p>
</blockquote>
<p>Istio documentation states to use jaeger collector address in <code>global.tracer.zipkin.address</code>.</p>
<hr />
<p>As for the Jaeger agent host, according to <a href=""https://www.jaegertracing.io/docs/1.20/operator/"" rel=""nofollow noreferrer"">Jaeger</a> Operator documentation:</p>
<blockquote>
<p>&lt;9&gt; By default, the operator assumes that agents are deployed as
sidecars within the target pods. Specifying the strategy as
“DaemonSet” changes that and makes the operator deploy the agent as
DaemonSet. Note that your tracer client will probably have to override
the “JAEGER_AGENT_HOST” environment variable to use the node’s IP.</p>
</blockquote>
<hr />
<blockquote>
<p>Your tracer client will then most likely need to be told where the agent is located. This is usually done by setting the environment variable  <code>JAEGER_AGENT_HOST</code>  to the value of the Kubernetes node’s IP, for example:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: acme/myapp:myversion
        env:
        - name: JAEGER_AGENT_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
</code></pre>
</blockquote>
"
Zipkin,45014834,45012331,0,"2017/07/10, 17:32:57",True,"2017/07/10, 17:32:57",8336,1773866,1,"<p>Why are you mocking a span? This makes absolutely no sense. Also a Span is never a bean. You already create a normal span via a builder and you should leave that. Assuming that you have set up the Boot context property and  you want to mock out <code>tracer</code> bean you should do the following</p>

<pre><code>public class AbstractSpanAccessorTest {
@MockBean
private Tracer tracer;

private Random random = new Random();

@Before
public void mockSpan() {
  long id = createId();
  Span spanMock = Span.builder().name(""mock"").traceId(id).spanId(id).build();
  doReturn(spanMock).when(tracer).getCurrentSpan();
  doReturn(spanMock).when(tracer).createSpan(anyString());
}

private long createId() {
  return random.nextLong();
}
}
</code></pre>
"
Zipkin,66524575,66517644,2,"2021/03/08, 07:07:48",True,"2021/03/08, 07:25:55",1693,971735,1,"<p>It was removed in Sleuth 3.0, though it seems the docs were not updated, I'm going to update the docs soon.</p>
<p>To fix the rest with your logs, you can check the logging config <a href=""https://github.com/spring-projects/spring-boot/tree/5fc49aa485a68664dcdea83eaa366cb1142bcb32/spring-boot-project/spring-boot/src/main/resources/org/springframework/boot/logging"" rel=""nofollow noreferrer"">here</a>, the <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-log-integration"" rel=""nofollow noreferrer"">log integration in the docs</a> and <a href=""https://stackoverflow.com/a/65851232/971735"">this answer</a>.</p>
"
Zipkin,66481285,66479206,1,"2021/03/04, 20:52:36",False,"2021/03/04, 20:52:36",1693,971735,0,"<p>This should be done out of the box: <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/2.2.7.RELEASE/reference/html/#feign"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/2.2.7.RELEASE/reference/html/#feign</a></p>
<p>You can take a look at the feign sample (you need to go back in the history, currently it is for 3.x): <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-feign"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-feign</a></p>
<p>In order to see if propagation works, look into the outgoing request, it should contain the tracing-related headers.</p>
"
Zipkin,66384663,66383029,4,"2021/02/26, 13:04:31",True,"2021/02/26, 13:04:31",8336,1773866,2,"<p>You could create your own <code>SpanHandler</code> bean that takes the <code>FinishedSpan</code>, converts into JSON and stores it somewhere on your drive. Then you could just iterate over jsons and upload them to the Zipkin server</p>
"
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331,2133695,1,"<p>No, the tracing SPI will not be backported to Vert.x 3.</p>
<p>I would recommend to check out <a href=""https://vertx.io/blog/from-vert-x-3-to-vert-x-4/"" rel=""nofollow noreferrer"">Migrate from Vert.x 3 to Vert.x 4</a>:</p>
<blockquote>
<p>When­ever pos­si­ble, Vert.x 4 APIs have been made avail­able in
Vert.x 3 with a dep­re­ca­tion of the old API, giv­ing the
op­por­tu­nity to im­prove a Vert.x 3 ap­pli­ca­tion with a bet­ter
API while al­low­ing the ap­pli­ca­tion to be ready for a Vert.x 4
mi­gra­tion.</p>
</blockquote>
<p>In other words, one of the Vert.x 4 goals was to minimize the upgrading effort.</p>
"
Zipkin,65171650,65169398,0,"2020/12/06, 20:34:26",False,"2020/12/06, 20:34:26",8336,1773866,0,"<p>You should use e.g. openzipkin Brave project or Opentelemetry projects directly. Sleuth works only with boot based projects</p>
"
Zipkin,65347921,65064734,0,"2020/12/17, 22:30:30",True,"2020/12/17, 22:30:30",1693,971735,0,"<p>I think what you call correlationId is in fact the traceId, if you are new to distributed tracing, I highly recommend reading the docs of <code>spring-cloud-sleuth</code>, the <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/#introduction"" rel=""nofollow noreferrer"">introduction</a> section will give you a basic understanding while the <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/#propagation"" rel=""nofollow noreferrer"">propagation</a> will tell you well, how your fields are propagated across services.<br />
I also recommend this talk: <a href=""https://tanzu.vmware.com/content/springone-platform-2017/distributed-tracing-latency-analysis-for-your-microservices-grzejszczak-krishna"" rel=""nofollow noreferrer"">Distributed Tracing: Latency Analysis for Your Microservices - Grzejszczak, Krishna</a>.</p>
<p>To answer your exact questions:</p>
<blockquote>
<p>How the correlation id will be passed to Kafka messages?</p>
</blockquote>
<p>Kafka has headers, I assume the fields are propagated through Kafka headers.</p>
<blockquote>
<p>How the correlation id will be passed to Http requests?</p>
</blockquote>
<p>Through HTTP Headers.</p>
<blockquote>
<p>Is it possible to use existing tracedId from other service?</p>
</blockquote>
<p>Not just possible, Sleuth does this for you out of the box. If there is a traceId in the incoming request/message/event/etc. Sleuth will not create a new one but it will use it (see the docs I linked above).</p>
"
Zipkin,63739507,63737361,2,"2020/09/04, 13:37:27",False,"2020/09/04, 13:37:27",4113,12201084,0,"<p>Your service is expecting following labels on pod:</p>
<pre><code>selector:
  app.kubernetes.io/name: zipkin
  app.kubernetes.io/instance: zipkin
  app: zipkin
</code></pre>
<p>Although it looks like you have only one label on zipkin pods:</p>
<pre><code>labels:
    app: zipkin
</code></pre>
<p>Label selector uses logical AND (&amp;&amp;), and this means that all labels specified must be on pod to match it.</p>
"
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351,10196632,0,"<p>The following worked. Sorry, I cannot provide info on all details since I don't know them :( Maybe somebody else can.</p>
<p>deployment.yaml</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: zipkin
  labels:
    app.kubernetes.io/name: zipkin
    app.kubernetes.io/instance: zipkin
    app: zipkin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zipkin
  template:
    metadata:
      name: zipkin
      labels:
        app: zipkin
    spec:
      containers:
        - name: zipkin
          image: openzipkin/zipkin:2.21
          imagePullPolicy: Always
          ports:
            - containerPort: 9411
              protocol: TCP
          env:
          - name: STORAGE_TYPE
            value: elasticsearch
          - name: ES_HOSTS
            value: https://my-es-host:9243
          - name: ES_USERNAME
            value: myUser
          - name: ES_PASSWORD
            value: myPassword
          - name: ES_HTTP_LOGGING
            value: HEADERS
          readinessProbe:
            httpGet:
              path: /api/v2/services
              port: 9411
            initialDelaySeconds: 5
            timeoutSeconds: 3
            
</code></pre>
<p>service.yaml</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: zipkin
  labels:
    app.kubernetes.io/name: zipkin
    app.kubernetes.io/instance: zipkin
    app: zipkin
spec:
  type: ClusterIP
  ports:
    - port: 9411
      targetPort: 9411
      protocol: TCP
      name: http  &lt;-- DELETED
  selector:
    app.kubernetes.io/name: zipkin    &lt;-- DELETED
    app.kubernetes.io/instance: zipkin    &lt;-- DELETED
    app: zipkin
    
            
</code></pre>
<p>ingress.yaml</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: zipkin
  labels:
    app.kubernetes.io/name: zipkin
    app.kubernetes.io/instance: zipkin
    app.kubernetes.io/managed-by: zipkin
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1    &lt;-- DELETED
    nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;3600&quot;
    nginx.ingress.kubernetes.io/proxy-body-size: &quot;0&quot;
    nginx.ingress.kubernetes.io/cors-allow-methods: &quot;PUT, GET, POST, OPTIONS, DELETE&quot;
    nginx.ingress.kubernetes.io/cors-allow-origin: &quot;*&quot;
    nginx.ingress.kubernetes.io/enable-cors: &quot;true&quot;
spec:
  tls:
    - hosts:
        - ns-zipkin.my-host
      secretName: .my-host
  rules:
    - host: ns-zipkin.my-host
      http:
        paths:
          - path: /  &lt;-- CHANGED
            backend:
              serviceName: zipkin
              servicePort: 9411   &lt;-- CHANGED
</code></pre>
"
Zipkin,63384808,63384688,0,"2020/08/13, 00:15:25",True,"2020/08/13, 00:15:25",8336,1773866,1,"<p>It's because of sampling. Please create a bean of sampler type whose value can be Sampler.ALWAYS or set the probability property to 1.0</p>
"
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11,11424150,1,"<p>Hi I just resolved this issue ..</p>
<p>Step1.verify C:\Programfiles\Err(version) folder including bin folder is created or not. Otherwise try to download and install Erlang again. reinstall RabbitMQ and try connecting Zipkin. Make sure Erlang version and RabbitMQ version is compatible.</p>
<p>Step2.Check ERLANG_HOME is set to proper location in environment variables.</p>
<p>at this point if RabbitMQ windows installer pointing to any old erlang version installed earlier  try to install RabitMQ windows manually
follow the steps mentioned in below link for manual installation</p>
<p><a href=""https://www.rabbitmq.com/install-windows-manual.html"" rel=""nofollow noreferrer"">https://www.rabbitmq.com/install-windows-manual.html</a></p>
"
Zipkin,61710527,61710421,0,"2020/05/10, 13:32:48",True,"2020/05/10, 13:32:48",nan,nan,0,"<p>That's an old implementation. Below I have modified your code to work:</p>

<pre><code>import brave.Tracing;
import brave.Span;

@Component 
public class  Test {

  @Autowired
  Tracing tracing;

  public void test (){
     Span span = tracing.tracer().nextSpan().name(""name"");
     //business logic
     span.finish();
  }
}
</code></pre>

<p>For more information check this link: <a href=""https://gist.github.com/marcingrzejszczak/d3c15a0c11dda71970e42c513c9c0e09"" rel=""nofollow noreferrer"">https://gist.github.com/marcingrzejszczak/d3c15a0c11dda71970e42c513c9c0e09</a></p>
"
Zipkin,61332491,61198731,0,"2020/04/21, 00:41:04",False,"2020/04/21, 00:41:04",985,598932,0,"<p>From <a href=""https://github.com/openzipkin/zipkin/tree/master/zipkin-storage/elasticsearch#indexes"" rel=""nofollow noreferrer"">zipkin docs</a>:</p>

<blockquote>
  <p>There is no support for TTL through this SpanStore. It is recommended instead to use Elastic Curator to remove indices older than the point you are interested in.</p>
</blockquote>
"
Zipkin,62824310,60598519,0,"2020/07/10, 00:55:28",False,"2020/07/16, 00:09:39",1,13324871,0,"<p>I had the same problem and solved it by:
Open windows task manger and kill all java instances java.exe or javaw.exe</p>
"
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576,12014434,0,"<p>I have finally figured out what could be the cause of this issue:</p>

<p>The install option:</p>

<p><code>--set values.tracing.provider=zipkin --set values.global.tracer.zipkin.address</code></p>

<p>requires <code>&lt;zipkin-collector-service&gt;.&lt;zipkin-collector-namespace&gt;:9411</code> according to <a href=""https://istio.io/docs/tasks/observability/distributed-tracing/zipkin/#before-you-begin"" rel=""nofollow noreferrer"">istio</a> documentation. While You have just IP address and port of external server. </p>

<p>This most likely means that the install option requires existing name that is in istio service mesh registry.</p>

<p>So if Your zipkin collector is outside cluster We need to add <code>ServiceEntry</code>, <code>VirtualService</code> and maybe <code>DestinationRule</code> and so the external service can be used within mesh.</p>

<p>You can follow <a href=""https://istio.io/docs/reference/config/networking/service-entry/"" rel=""nofollow noreferrer"">istio</a> documentation to see how to create these objects for external service. <a href=""https://istiobyexample.dev/external-services/"" rel=""nofollow noreferrer"">Here</a> is another guide.</p>

<p>After that We need to update the tracer address value with the <code>VirtualService</code> as an endpoint.</p>

<pre><code>--set values.global.tracer.zipkin.address=zipkin-external-virtualservice.egress-zipkin-namespace:9411
</code></pre>

<p>Hope this helps.</p>
"
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576,12014434,1,"<p>By using the following <a href=""https://istio.io/docs/setup/install/istioctl/#show-differences-in-profiles"" rel=""nofollow noreferrer"">commands</a> I was able to generate the manifests using <a href=""https://istio.io/docs/setup/getting-started/#download"" rel=""nofollow noreferrer""><code>istioctl</code></a> with parameters You mentioned:</p>

<pre><code>$ istioctl manifest generate --set profile=demo --set values.tracing.enabled=true --set values.tracing.provider=zipkin &gt; istio-demo-with-zipkin.yaml
</code></pre>

<pre><code>$ istioctl manifest generate --set profile=demo &gt; istio-demo.yaml
</code></pre>

<p>Then compared them to see differences made with those parameter modifications.</p>

<pre><code>$ istioctl manifest diff istio-demo.yaml istio-demo-with-zipkin.yaml
Differences of manifests are:


Object ConfigMap:istio-system:istio-sidecar-injector has diffs:

data:
  values:
    tracing:
      provider: jaeger -&gt; zipkin


Object Deployment:istio-system:istio-tracing has diffs:

metadata:
  labels:
    app: jaeger -&gt; zipkin
spec:
  selector:
    matchLabels:
      app: jaeger -&gt; zipkin
  template:
    metadata:
      annotations:
        prometheus.io/port: 14269 -&gt;
        prometheus.io/scrape: true -&gt;
      labels:
        app: jaeger -&gt; zipkin
    spec:
      containers:
        '[?-&gt;0]': -&gt; map[env:[map[name:POD_NAMESPACE valueFrom:map[fieldRef:map[apiVersion:v1
          fieldPath:metadata.namespace]]] map[name:QUERY_PORT value:9411] map[name:JAVA_OPTS
          value:-XX:ConcGCThreads=2 -XX:ParallelGCThreads=2 -Djava.util.concurrent.ForkJoinPool.common.parallelism=2
          -Xms700M -Xmx700M -XX:+UseG1GC -server] map[name:STORAGE_METHOD value:mem]
          map[name:ZIPKIN_STORAGE_MEM_MAXSPANS value:500000]] image:docker.io/openzipkin/zipkin:2.14.2
          imagePullPolicy:IfNotPresent livenessProbe:map[initialDelaySeconds:200 tcpSocket:map[port:9411]]
          name:zipkin ports:[map[containerPort:9411]] readinessProbe:map[httpGet:map[path:/health
          port:9411] initialDelaySeconds:200] resources:map[limits:map[cpu:300m memory:900Mi]
          requests:map[cpu:150m memory:900Mi]]]
        '[0-&gt;?]': map[env:[map[name:POD_NAMESPACE valueFrom:map[fieldRef:map[apiVersion:v1
          fieldPath:metadata.namespace]]] map[name:BADGER_EPHEMERAL value:false] map[name:SPAN_STORAGE_TYPE
          value:badger] map[name:BADGER_DIRECTORY_VALUE value:/badger/data] map[name:BADGER_DIRECTORY_KEY
          value:/badger/key] map[name:COLLECTOR_ZIPKIN_HTTP_PORT value:9411] map[name:MEMORY_MAX_TRACES
          value:50000] map[name:QUERY_BASE_PATH value:/jaeger]] image:docker.io/jaegertracing/all-in-one:1.14
          imagePullPolicy:IfNotPresent livenessProbe:map[httpGet:map[path:/ port:14269]]
          name:jaeger ports:[map[containerPort:9411] map[containerPort:16686] map[containerPort:14250]
          map[containerPort:14267] map[containerPort:14268] map[containerPort:14269]
          map[containerPort:5775 protocol:UDP] map[containerPort:6831 protocol:UDP]
          map[containerPort:6832 protocol:UDP]] readinessProbe:map[httpGet:map[path:/
          port:14269]] resources:map[requests:map[cpu:10m]] volumeMounts:[map[mountPath:/badger
          name:data]]] -&gt;
      volumes: '[map[emptyDir:map[] name:data]] -&gt;'


Object Service:istio-system:jaeger-agent is missing in B:



Object Service:istio-system:jaeger-collector is missing in B:



Object Service:istio-system:jaeger-query is missing in B:



Object Service:istio-system:tracing has diffs:

metadata:
  labels:
    app: jaeger -&gt; zipkin
spec:
  ports:
    '[0]':
      targetPort: 16686 -&gt; 9411
  selector:
    app: jaeger -&gt; zipkin


Object Service:istio-system:zipkin has diffs:

metadata:
  labels:
    app: jaeger -&gt; zipkin
spec:
  selector:
    app: jaeger -&gt; zipkin
</code></pre>

<p>You can try to manually modify those applied settings or apply it to Your cluster.</p>

<p>Istioctl I used to generate these manifests:</p>

<pre><code>$ istioctl version
client version: 1.4.3
control plane version: 1.4.3
data plane version: 1.4.3 (4 proxies)
</code></pre>

<p>Hope it helps.</p>
"
Zipkin,58754849,58619789,0,"2019/11/07, 20:21:08",False,"2019/11/07, 20:21:08",21,9936877,0,"<p>Still with using 2.2.0 parent, I still face the whitelable error.
I will check on this latter but by changing the pom defination the Zipkin server work</p>
<h1>pom.xml</h1>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; 
&lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;!-- &lt;version&gt;2.2.0.RELEASE&lt;/version&gt; --&gt;
    &lt;version&gt;1.5.10.RELEASE&lt;/version&gt;
    &lt;!-- &lt;relativePath/&gt; lookup parent from repository  --&gt;
&lt;/parent&gt;
&lt;groupId&gt;edu.rohit&lt;/groupId&gt; 
&lt;artifactId&gt;ZipkinServer&lt;/artifactId&gt;  
&lt;version&gt;1&lt;/version&gt;  
&lt;name&gt;ZipkinServer&lt;/name&gt; 
&lt;description&gt;Demo project for Spring Boot Slueth-Zipkin&lt;/description&gt;

&lt;properties&gt;
    &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;!-- &lt;spring-cloud.version&gt;Hoxton.RC1&lt;/spring-cloud.version&gt; --&gt;
    &lt;!-- &lt;spring-cloud.version&gt;2.1.3.RELEASE&lt;/spring-cloud.version&gt; --&gt;
    &lt;spring-cloud.version&gt;Edgware.RELEASE&lt;/spring-cloud.version&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;!-- &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-web-services&lt;/artifactId&gt;
    &lt;/dependency&gt; --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
        &lt;!-- &lt;version&gt;${spring-cloud.version}&lt;/version&gt; --&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
        &lt;!-- &lt;version&gt;2.11.7&lt;/version&gt; --&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
        &lt;!-- &lt;version&gt;2.11.7&lt;/version&gt; --&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;
    
&lt;/dependencies&gt;

&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;

&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;

&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;spring-milestones&lt;/id&gt;
        &lt;name&gt;Spring Milestones&lt;/name&gt;
        &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt;
        &lt;snapshots&gt;
            &lt;enabled&gt;false&lt;/enabled&gt;
        &lt;/snapshots&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
&lt;/project&gt;
</code></pre>
<p>And in zipkinserverapplication we need the @Enablezipkinserver</p>
<h1>ZipkinServerApplication</h1>
<pre><code>package edu.rohit.ZipkinServer;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;
import zipkin.server.EnableZipkinServer;

@EnableZipkinServer
@SpringBootApplication(scanBasePackages={&quot;edu.rohit.ZipkinServer&quot;})

public class ZipkinServerApplication {

 public static void main(String[] args) {
     SpringApplication.run(ZipkinServerApplication.class, args);
 }
}
</code></pre>
"
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589,5587542,0,"<p>Hi if your spring cloud app target is a Spring Boot 2.x base I suggest to do not try to use the @EnableZipkinServer because it is not a raccomanded way as the java doc suggest:</p>

<p>form the Zipkin base code:</p>

<pre><code>/**
 * @deprecated Custom servers are possible, but not supported by the community. Please use our
 * &lt;a href=""https://github.com/openzipkin/zipkin#quick-start""&gt;default server build&lt;/a&gt; first. If you
 * find something missing, please &lt;a href=""https://gitter.im/openzipkin/zipkin""&gt;gitter&lt;/a&gt; us about
 * it before making a custom server.
 *
 * &lt;p&gt;If you decide to make a custom server, you accept responsibility for troubleshooting your
 * build or configuration problems, even if such problems are a reaction to a change made by the
 * OpenZipkin maintainers. In other words, custom servers are possible, but not supported.
 */
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Import(InternalZipkinConfiguration.class)
@Deprecated
public @interface EnableZipkinServer {

}
</code></pre>

<p>I spoke for personal experience with spring boot application 2.x family. 
The my solution,for development, was use a docker compose like below(consider that my application use spring cloud stream in order to push on zipkin the tracing information:</p>

<pre><code>version: '3.1'

services:
  rabbitmq:
    image: rabbitmq:3-management
    restart: always
    ports:
      - 5672:5672
      - 15671:15671
      - 15672:15672
    networks:
      - messaging

  zipkin-server:
    image: openzipkin/zipkin
    ports:
      - 9065:9411
    environment:
      - zipkin.collector.rabbitmq.uri=amqp://guest:guest@rabbitmq:5672
    networks:
      - messaging

networks:
  messaging:
    driver: bridge
</code></pre>

<p>On the other hands if your target is a spring boot 1.5.x you can use the legacy embedded zipkin server like below:</p>

<p>POM:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;it.valeriovaudi.emarket&lt;/groupId&gt;
    &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;

    &lt;name&gt;zipkin-server&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;

    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;1.5.3.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Dalston.RELEASE&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- start web dependencies --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- end web dependencies --&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;


        &lt;!-- start distributed tracing dependencies --&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- EXAMPLE FOR RABBIT BINDING --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- end distributed tracing dependencies --&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;!-- start test dependencies --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;!-- end test dependencies --&gt;
    &lt;/dependencies&gt;

    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;build&gt;
        &lt;finalName&gt;zipkin-server&lt;/finalName&gt;

        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>

<p>Application:</p>

<p>package it.valeriovaudi.emarket;</p>

<pre><code>@SpringBootApplication
@EnableZipkinStreamServer
public class ZipkinServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZipkinServerApplication.class, args);
    }
}
</code></pre>

<p>both solution works for me, spring boot 1.5.x was the base spring boot version for my master thesis and spring boot 2.x version for a my personal distributed system project that I use every day for my persona family budget management.</p>

<p>I hope that this can help you</p>
"
Zipkin,58799041,58619789,0,"2019/11/11, 11:50:52",False,"2019/11/11, 11:50:52",21,9936877,0,"<pre><code>I did some study in actuator use in spring boot 2.x.
The actuator use have significant changes. The default endpoint root path is '/actuator' and not'/' as in 1.x. By default only two endpoint(/health, /info) is enable.

</code></pre>

<pre><code>#To make all endpoints enable set
    management.endpoints.web.exposure.include=*   
#To specifically enable one endpoint(/beans) set
    management.endpoint.beans.enabled=true 
#Or we can create a WebSecurity bean like this:
    @Bean
    public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http){
          return http.authorizeExchange()
         .pathMatchers(""/actuator/**"").permitAll()
         .anyExchange().authenticated()
         .and().build();
    }  
#Remember in spring boot 1.x all this issues was resolved by setting the bellow but it is #depreciated in 2.x
    management.security.enabled=false
#By default the base path is '/actuator' but can be changed by setting
    management.endpoints.web.base-path= /&lt;Mypath&gt;
</code></pre>

<p>For more details can read the document <a href=""https://www.baeldung.com/spring-boot-actuators#boot-2x-actuator"" rel=""nofollow noreferrer"">https://www.baeldung.com/spring-boot-actuators#boot-2x-actuator</a></p>
"
Zipkin,58298923,58294974,2,"2019/10/09, 10:18:06",True,"2019/10/09, 10:18:06",2382,4497840,2,"<p>I have a working project with spring cloud stream and zipkin using the following configuration (maybe you should set the sender.type): </p>

<pre><code>spring:
  zipkin:
    enabled: true
    service.name: my-service
    sender.type: web
    base-url: http://localhost:9411
  sleuth:
    enabled: true
    sampler:
      probability: 1.0
</code></pre>

<p>Hope this can help.</p>
"
Zipkin,58172731,58170335,2,"2019/09/30, 20:39:51",False,"2019/09/30, 20:39:51",81,2232476,0,"<p>The problem lies in your <code>ES_HOSTS</code> variable, from the docs <a href=""https://github.com/openzipkin/zipkin/tree/844c43b9cde02b84583f0e3641fc20e670a3b25f/zipkin-server#elasticsearch-storage"" rel=""nofollow noreferrer"">here</a>:</p>

<blockquote>
  <ul>
  <li><code>ES_HOSTS</code>: A comma separated list of elasticsearch base urls to connect to ex. <a href=""http://host:9200"" rel=""nofollow noreferrer"">http://host:9200</a>.
            Defaults to ""<a href=""http://localhost:9200"" rel=""nofollow noreferrer"">http://localhost:9200</a>"".</li>
  </ul>
</blockquote>

<p>So you will need: <code>ES_HOSTS=http://storage:9200</code></p>
"
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35,9795454,0,"<p>Finally I have this file:</p>

<pre><code>version: '3.7'

services:
  storage:
    image: openzipkin/zipkin-elasticsearch7
    container_name: elasticsearch
    ports:
      - 9200:9200

  zipkin:
    image: openzipkin/zipkin
    container_name: zipkin
    environment: 
      - STORAGE_TYPE=elasticsearch
      - ""ES_HOSTS=elasticsearch:9300""
    ports:
      - 9411:9411
    depends_on: 
      - storage

  dependencies:
    image: openzipkin/zipkin-dependencies
    container_name: dependencies
    entrypoint: crond -f
  depends_on:
    - storage
  environment:
    - STORAGE_TYPE=elasticsearch
    - ""ES_HOSTS=elasticsearch:9300""
    - ""ES_NODES_WAN_ONLY=true""

prometheus:
  image: prom/prometheus:latest
  container_name: prometheus
  volumes:
    - $PWD/prometheus:/etc/prometheus/
    - /tmp/prometheus:/prometheus/data:rw
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
    - '--web.console.libraries=/usr/share/prometheus/console_libraries'
    - '--web.console.templates=/usr/share/prometheus/consoles'
  ports:
    - ""9090:9090""

grafana:
  image: grafana/grafana
  container_name: grafana
  depends_on:
    - prometheus
  ports:
    - ""3000:3000""
</code></pre>

<p>Main differences are the usage of </p>

<blockquote>
  <p>""ES_HOSTS=elasticsearch:9300""</p>
</blockquote>

<p>instead of </p>

<blockquote>
  <p>""ES_HOSTS=storage:9300""</p>
</blockquote>

<p>and in the dependencies configuration I add the entrypoint in dependencies:</p>

<blockquote>
  <p>entrypoint: crond -f
  This one is really the key to not have the exception when I start docker-compose. </p>
</blockquote>

<p>To solve this issue, I check the this project: <a href=""https://github.com/openzipkin/docker-zipkin"" rel=""nofollow noreferrer"">https://github.com/openzipkin/docker-zipkin</a></p>

<p>The remaining question is: why do I need to use entrypoint: crond -f</p>
"
Zipkin,57287189,57284146,0,"2019/07/31, 12:12:05",False,"2019/07/31, 12:12:05",11,11602721,1,"<p>I found examples from:
<a href=""https://github.com/openzipkin/zipkin/tree/master/zipkin-lens/testdata"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/tree/master/zipkin-lens/testdata</a></p>

<p>It works well.</p>
"
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1,11841253,0,"<p>Set the datasource setting in the application.yml file of the application as follows,</p>

<pre><code>spring:
  dataSource:
    url: jdbc: mariadb://localhost:3306/test
    driverClassName: org.mariadb.jdbc.Driver
    username: test
    password: test
</code></pre>

<p>You can add the zipkin attribute to POM.xml</p>

<pre><code>&lt;dependency&gt;
      &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
      &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Problems can occur due to Spring's auto configuration property.</p>

<p>Therefore, modify the datasource setting as follows and modify the datasource configuration related source to make the application work normally.</p>

<pre><code>myApp:
   dataSource:
     jdbc-url: jdbc:mariadb://localhost:3306/test
     driverClassName: org.mariadb.jdbc.Driver
     username: test
     password: test
</code></pre>
"
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864,1413240,0,"<p>It looks like the trace is triggering an old-fashioned inverted-lock-order deadlock freezing threads that are attempting to acquire new Connections.</p>

<p>The last of the three deadlocked threads is <a href=""https://github.com/spring-projects/spring-framework/blob/master/spring-beans/src/main/java/org/springframework/beans/factory/support/DefaultSingletonBeanRegistry.java#L204"" rel=""nofollow noreferrer"">trying to get a lock on some singleton or bean</a>. It has already passed through and presumably acquired a lock on a <code>GenericScope</code>.</p>

<p>The other two threads are <a href=""https://github.com/spring-cloud/spring-cloud-commons/blob/master/spring-cloud-context/src/main/java/org/springframework/cloud/context/scope/GenericScope.java#L387-L388"" rel=""nofollow noreferrer"">trying to acquire a lock on a <code>GenericScope</code></a>, which presumably the first thread has.</p>

<p>An unexpected reentrance from the <code>zipkin</code> code into spring is generating a deadlock. <code>c3p0</code> has a fixed-size thread pool that notices when all its threads (just 3 here, <code>c3p0</code>'s default) are persistently frozen, then (pretty correctly in this case) declares a deadlock and replaces the blocked threads in hopes of recovering.</p>

<p>Does c3p0 recover? Is this a rare or frequent deadlock? There's not much you can easily do to prevent this deadlock, I think either you'll have to tolerate it or do without the instrumentation.</p>
"
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630,10676678,2,"<p>Application Insights users would also be able to leverage the distributed tracing offered through Zipkin by instrumenting their services using existing libraries. To use the Application Insights back-end store, configure your Zipkin server instance to use the Application Insights <a href=""https://github.com/openzipkin-attic/zipkin-azure/pulls"" rel=""nofollow noreferrer"">plug-in</a>. This integration makes monitoring and debugging your overall end-to-end applications much easier.</p>

<p>Once you have the data in Application Insights, you can always perform <a href=""https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/cross-workspace-query"" rel=""nofollow noreferrer"">cross-resource log queries</a> between Application Insights and Log Analytics.</p>

<p>Additional Documentation Reference - </p>

<p><a href=""https://github.com/openzipkin-attic/zipkin-azure/issues/33"" rel=""nofollow noreferrer"">Zipkin to Application Insights Module</a></p>

<p><a href=""https://blogs.msdn.microsoft.com/microsoftrservertigerteam/2017/05/10/introducing-zipkin-azure/"" rel=""nofollow noreferrer"">Zipkin-Azure</a></p>

<p><a href=""https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-collector-api"" rel=""nofollow noreferrer"">Send Log Data to Azure Monitor with HTTP Data Collector API (public preview)</a></p>

<p>Hope the above information helps.</p>
"
Zipkin,55535480,55364711,0,"2019/04/05, 15:32:51",True,"2019/04/05, 15:32:51",8336,1773866,1,"<p>No you can't. You can use tools like Elasticsearch Logstash Kibana to visualize it. You can go to my repo <a href=""https://github.com/marcingrzejszczak/docker-elk"" rel=""nofollow noreferrer"">https://github.com/marcingrzejszczak/docker-elk</a> and run <code>./   getReadyForConference.sh</code>, it will start docker containers with the ELK stack, run the apps, curl the request to the apps so that you can then check them in ELK.</p>
"
Zipkin,54758616,54758615,0,"2019/02/19, 05:51:06",True,"2019/02/19, 05:51:06",3909,3067542,1,"<p>Alright, so after a few hours struggling, I made some progress, and now the app starts - even though the root cause of the issue is not fully clear to me at this time. 
Below are my findings :</p>

<ul>
<li><p>one strange thing I noticed : if I change the <code>sender.type</code> from <code>web</code> to <code>rabbit</code>, then the application starts with no error. </p></li>
<li><p>I also found this Spring Boot <a href=""https://github.com/spring-projects/spring-boot/issues/12854"" rel=""nofollow noreferrer"">issue report</a>, very similar to mine, that was pointing at a JDK bug. And indeed, upgrading from <code>jdk1.8.0_25</code> to <code>jdk1.8.0_201</code> .</p></li>
<li><p>Finally, I also found that if I was using <code>jdk1.8.0_25</code> and wasn't providing the <code>sender.type</code> at all, then the app was also starting with no issue. </p></li>
</ul>

<p>For some reason, in the other app that I have and that works, I am able to use <code>jdk1.8.0_25</code> and <code>sender.type: web</code></p>

<p>If anyone has a methodology to figure out this kind of issue quickly, don't hesitate to add it in the comment or edit this answer. </p>
"
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336,1773866,0,"<p>It makes perfect sense that it's <code>null</code>. That's because YOU control the way what happens with the caught exception. In your case, nothing, cause you swallow that exception. </p>

<p>If you want to do sth better, just add the error tag manually via the <code>SpanCustomizer</code>. That way you'll add the exception to the given span. It will then automatically get closed and reported to Zipkin (you can do sth else than <code>ex.toString()</code> of course.</p>

<pre><code>@Slf4j
@RestControllerAdvice
@Order(Ordered.HIGHEST_PRECEDENCE)
public class ExceptionHanders {

    private final SpanCustomizer customizer;

    public ExceptionHanders(SpanCustomizer customizer) {
        this.customizer = customizer;
    }

    @ExceptionHandler({RuntimeException.class})
    @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)
    public String handleRuntimeException(Exception ex) throws Exception {
        this.customizer.tag(""error"", ex.toString());
        return ""testabcd"";
    }
}
</code></pre>
"
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802,2810730,1,"<p><a href=""https://zipkin.io/"" rel=""nofollow noreferrer"">Zipkin</a> is a solution for distributed tracing. Specifically it allows to track latency problems in distributed system. Also it's a greate tool for debugging/investigating problems in your application. So by definition it requires to collect successful and failed traces. However <a href=""https://zipkin.io/pages/architecture.html"" rel=""nofollow noreferrer"">traces</a> have nothing to do with logging.</p>

<p>Assuming you mean controlling the logging level of Zipkin server, then you can just set it using <a href=""https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#logging"" rel=""nofollow noreferrer"">--logging.level.zipkin2=INFO</a>.</p>
"
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336,1773866,0,"<p>I don't understand the problem. You don't send logs to Zipkin. You send spans to Zipkin. Zipkin has nothing to do with logs. </p>
"
Zipkin,54468173,54466528,0,"2019/01/31, 21:45:59",True,"2019/01/31, 21:45:59",2296,1575416,2,"<p>Seems to work once I added the Web package. Though I don't recall it being needed previously.</p>
"
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767,2498986,0,"<p>Zipkin currently supports four types of backend storage to store spans in-memory, MySQl, ElasticSearch, Cassandra. Although for production it is recommended to use ES or Cassandra. The other two can be used for learning and understanding. Traces stored in the in-memory is ephemeral and won't be available after the restart. </p>

<p>In the zipkin UI there is an option to see the trace and download it, which can be used to view at a later point in time. If you still have further questions drop in to the zipkin <a href=""https://gitter.im/openzipkin/zipkin"" rel=""nofollow noreferrer"">gitter</a> channel.</p>
"
Zipkin,53221841,53218692,0,"2018/11/09, 10:01:09",False,"2018/11/09, 10:01:09",136,1433218,0,"<p>we also use use zipkin but can't query with zipkin as elk. we can just click on each services which are display on zipkin and get more info as below image.  </p>

<p><a href=""https://i.stack.imgur.com/BuFa6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BuFa6.png"" alt=""after click the service""></a></p>
"
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767,2498986,0,"<p>Zipkin is not a business transaction tracking system and it should not be used that way because it is not built for this purpose. There are other tools which are built specifically to cater the needs to business operations which you must consider.</p>

<p>P.S. I am a Zipkin contributor.</p>
"
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071,2979435,0,"<p>This is not an answer to how achieve this with zipkin but yes for the whole problem.</p>

<p>If you have a  transaction that didn't complete it's steps then you probably have two of following issues</p>

<p><strong>Some microservice failed to deliver the event to the next one and didn't figure it out</strong></p>

<p>You have to make sure delivery at least once here, using Kafka you have to wait until message get flushed to the server for example</p>

<p><strong>The destiny microservice received the message and is not processing it</strong></p>

<p>You have to make sure you application is processing what it's supposed to,  you can monitor the database if the transactions are there or use some tool like LinkedIn burrow to monitor your Kafka message group if you are integrating by using Kafka.</p>

<p>Conclusion is, instead to try monitor all the thing once it looks like creating specialist monitors for every step will be more assertive and simple to develop.</p>
"
Zipkin,52975826,52972425,3,"2018/10/24, 21:33:48",False,"2018/10/24, 21:33:48",8295,4550110,0,"<p>Here is the related issue:</p>

<pre><code>we also mentioned recently that for data to appear, applications need to be
sending traces

https://github.com/openzipkin/zipkin#quick-start

you can tell also by hitting the /metrics endpoint and look at stats named
collector
</code></pre>

<p><a href=""https://github.com/openzipkin/zipkin/issues/1939"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/issues/1939</a></p>
"
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1,10552245,0,"<p>I opened a issue on the zipkin github, a theme already being treated as a bug.</p>

<p>Initial thread:
<a href=""https://github.com/openzipkin/zipkin/issues/2218#issuecomment-432876510"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/issues/2218#issuecomment-432876510</a></p>

<p>Bug track:
<a href=""https://github.com/openzipkin/zipkin/issues/2219"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/issues/2219</a></p>

<p>Tks for all!</p>
"
Zipkin,52815354,52706088,1,"2018/10/15, 14:07:16",False,"2018/10/15, 14:07:16",201,7956609,2,"<p>You have to use <strong>spring.sleuth.web.skipPattern</strong></p>

<p>sample you will get here <a href=""https://www.baeldung.com/tracing-services-with-zipkin"" rel=""nofollow noreferrer"">https://www.baeldung.com/tracing-services-with-zipkin</a></p>
"
Zipkin,52739617,52424864,0,"2018/10/10, 14:51:01",False,"2018/10/10, 14:51:01",201,7956609,0,"<p>I think to remove the service names from zipkin you have to Re-deploy the zipkin service</p>
"
Zipkin,52776732,52424864,0,"2018/10/12, 12:41:59",True,"2018/10/12, 12:41:59",131,6236211,0,"<p>You will need to remove the spring.zipkin.base-url property from the corresponding applications to remove it from zipkin server list.</p>
"
Zipkin,52608036,51661009,0,"2018/10/02, 15:08:15",True,"2018/10/02, 15:08:15",153,2999097,0,"<p>finally got working after spring verison updated to <code>5.x</code>
It already have <code>Brave Instrument for zipkin trace</code> </p>
"
Zipkin,51580557,51578263,2,"2018/07/29, 15:58:11",True,"2018/07/29, 15:58:11",8336,1773866,2,"<p>If you read the docs or any information starting from edgware you would see that we've removed that support. You should use native zipkin rabbit / kafka dependencies. Everything is there in the docs.</p>
"
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336,1773866,1,"<p>If it comes from the <code>@Scheduled</code> method then you can use <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/SleuthSchedulingProperties.java#L38"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/SleuthSchedulingProperties.java#L38</a> (<code>spring.sleuth.scheduled.skipPattern</code>) to find the thread and disable it. If you say its name is <code>async</code> then it means that it comes from a <code>TraceRunnable</code> or <code>TraceCallable</code>. That can be problematic to get rid off. You can file an issue in Sleuth to allow <code>SpanAdjuster</code> to actually not send spans to Zipkin (by for example returning <code>null</code>). You can also try to disable async at all <code>spring.sleuth.async.enabled</code>. If you're not using any other features of async that should not interfere.</p>
"
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767,2498986,0,"<p>Brave will work regardless of the server that you choose to use. Remove the jetty configuration from the pom file and use the Tomcat.</p>

<pre><code>&lt;plugin&gt;
  &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt;
  &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt;
  &lt;version&gt;2.2&lt;/version&gt;
  &lt;configuration&gt;
    &lt;port&gt;${tomcat.port}&lt;/port&gt;
    &lt;path&gt;/&lt;/path&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>

<p>If you still have trouble or want to know more about zipkin/brave connect to the community via the gitter channel.</p>

<p>P.S. I contribute to OpenZipkin (Zipkin)</p>
"
Zipkin,51141657,51068201,0,"2018/07/02, 21:12:12",False,"2018/07/02, 21:12:12",81,2232476,0,"<p>The UI cannot be password protected without also password protecting the API endpoints, including the one you would send your spans to.</p>

<p>PS: The <code>@EnableZipkinServer</code> annotation has been deprecated</p>
"
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336,1773866,1,"<p><strong>EDGWARE</strong></p>

<p>Have you read the documentation? If you use Spring Cloud Sleuth in Edgware version if you read the Sleuth section you would find this piece of the documentation <a href=""https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_custom_sa_tag_in_zipkin"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_custom_sa_tag_in_zipkin</a></p>

<p>Let me copy that for you</p>

<blockquote>
  <p>54.5 Custom SA tag in Zipkin Sometimes you want to create a manual Span that will wrap a call to an external service which is not
  instrumented. What you can do is to create a span with the
  peer.service tag that will contain a value of the service that you
  want to call. Below you can see an example of a call to Redis that is
  wrapped in such a span.</p>
</blockquote>

<pre><code>org.springframework.cloud.sleuth.Span newSpan = tracer.createSpan(""redis"");
try {
    newSpan.tag(""redis.op"", ""get"");
    newSpan.tag(""lc"", ""redis"");
    newSpan.logEvent(org.springframework.cloud.sleuth.Span.CLIENT_SEND);
    // call redis service e.g
    // return (SomeObj) redisTemplate.opsForHash().get(""MYHASH"", someObjKey);
} finally {
    newSpan.tag(""peer.service"", ""redisService"");
    newSpan.tag(""peer.ipv4"", ""1.2.3.4"");
    newSpan.tag(""peer.port"", ""1234"");
    newSpan.logEvent(org.springframework.cloud.sleuth.Span.CLIENT_RECV);
    tracer.close(newSpan);
}
</code></pre>

<blockquote>
  <p>[Important]   Important Remember not to add both peer.service tag and
  the SA tag! You have to add only peer.service.</p>
</blockquote>

<p><strong>FINCHLEY</strong></p>

<p>The <code>SA</code> tag will not work for Finchley. You have to do it in the following manner using the <code>remoteEndpoint</code> on the span.</p>

<pre><code>    Span span = tracer.newTrace().name(""redis""); 
    span.remoteEndpoint(Endpoint.newBuilder().serviceName(""redis"").build()); 
    span.kind(CLIENT);
    try(SpanInScope ws = tracer.withSpanInScope(span.start())) {
          // add any tags / annotations on the span
          // return (SomeObj) redisTemplate.opsForHash().get(""MYHASH"", someObjKey);
    } finally {
      span.finish();
    }
</code></pre>
"
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336,1773866,1,"<p>That was a bug in Spring Cloud Sleuth in Edgware. The Stream Kafka Binder in Edgware required explicit passing of headers that should get propagated. The side effect of adding <code>sleuth-stream</code> on the classpath was exactly that feature. By fixing the <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005</a> issue we're adding back the missing feature to core. This is not ported to Finchley since Stream Kafka Binder in Finchley passes all headers by default.</p>

<p>The workaround for Edgware is to pass a list of headers in the following manner:</p>

<pre><code>spring:
  cloud:
    stream:
      kafka:
        binder:
          headers:
            - spanId
            - spanSampled
            - spanProcessId
            - spanParentSpanId
            - spanTraceId
            - spanName
            - messageSent
</code></pre>
"
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141,553720,0,"<p>The Istio sidecar proxy (Envoy) generates the first headers. According to <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#x-request-id"" rel=""nofollow noreferrer"">https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#x-request-id</a>: </p>

<blockquote>
  <p>Envoy will generate an x-request-id header for all external origin requests (the header is sanitized). It will also generate an x-request-id header for internal requests that do not already have one.</p>
</blockquote>
"
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336,1773866,3,"<p>You've mixed almost everything you could have mixed. On the app side you're using both the deprecated zipkin server and the deprecated client. On the server side you're using deprecated zipkin server. </p>

<p>My suggestion is that you go through the documentation <a href=""https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_spring_cloud_sleuth"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_spring_cloud_sleuth</a> and read that the <code>stream servers</code> are deprecated and you should use the openzipkin zipkin server with rabbitmq support (<a href=""https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq</a>).</p>

<p>On the consumer side use <a href=""https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka</a> . It really is as simple as that. Also don't forget to turn on the sampling percentage to 1.0</p>
"
Zipkin,53221956,49280873,0,"2018/11/09, 10:11:00",False,"2018/11/09, 10:11:00",136,1433218,0,"<p>Just add below, it need to be working,</p>

<pre><code> zipkin:
    sender:
      type: web
    baseUrl:  http://192.168.0.207:9411
  sleuth:
    sampler:
      percentage: 1.0
</code></pre>
"
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181,6708214,0,"<p>Yeah,you should use different libraries for different languages.<br>
Brave for Java,Zipkin4net for C# and so on.<br>
For more details,you can visit Zipkin official site: <a href=""https://zipkin.io/pages/existing_instrumentations.html"" rel=""nofollow noreferrer"">Zipkin Existing instrumentations</a>.</p>

<p>Then all you shoud do is following the librarie guide.
Have fun!</p>
"
Zipkin,48346769,48160588,0,"2018/01/19, 19:46:45",False,"2018/01/19, 19:46:45",1207,7376337,0,"<p>The first request uses v1 of the Zipkin api while the second uses v2 (see <a href=""https://github.com/openzipkin/zipkin/issues/1499"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/issues/1499</a> for the v2 specification). Spans are broken up by kind (SERVER and CLIENT) instead of having client receive, server receive, client send, and server send annotations (hence why there are more spans).</p>
"
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336,1773866,1,"<blockquote>
  <p>I have a client application with multiple channels as SOURCE/SINK. I want to send logs to Zipkin server.</p>
</blockquote>

<p>Zipkin is not a tool to store logs</p>

<blockquote>
  <p>According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP.</p>
</blockquote>

<p>No - you need the <code>sleuth-stream</code> dependency on the client side and the <code>zipkin-stream</code> dependency on the server side (which got deprecated and you should start using the inbuilt rabbitmq support from Zipkin).</p>

<blockquote>
  <p>At client side: Q1. Is there an automatic configuration for zipkin rabbit binding in such scenario? If not, what is default channel name of zipkin SOURCE channel?</p>
</blockquote>

<p>Yes, there is. The channel is <code>sleuth</code></p>

<blockquote>
  <p>Q2. Do I need to configure defaultSampler to AlwaysSampler()?</p>
</blockquote>

<p>No, you have the <code>PercentageBasedSampler</code> (I'm pretty sure it's written in the docs). You can tweak its values.</p>

<blockquote>
  <p>At Server side: Q1. Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using: wget -O zipkin.jar '<a href=""https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec"" rel=""nofollow noreferrer"">https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec</a>' ...as stated on <a href=""https://zipkin.io/pages/quickstart.html"" rel=""nofollow noreferrer"">https://zipkin.io/pages/quickstart.html</a> ?</p>
</blockquote>

<p>You should do the wget. If you want to use the legacy stream support then you should create a zipkin server yourself.</p>

<blockquote>
  <p>Q2. How do I configure zipkin SINK channel to destination?</p>
</blockquote>

<p>If you're using the legacy zipkin stream app then it's automatically configured to point to proper destination. You can tweak the destination as you please in the standard way Spring Cloud Stream supports it.</p>
"
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693,2901325,0,"<p>I'm not sure I fully understand your question but I can elaborate a bit on how istio works with respect to tracing:</p>

<p>Tracing means identifying every span or node that is part of the original request, so typically an Id is generated by the istio-ingress and your application should <a href=""https://istio.io/docs/tasks/telemetry/distributed-tracing.html#understanding-what-happened"" rel=""nofollow noreferrer"">propagate it</a> so each istio-proxy can capture and forward that information to istio-mixer which then lets you use Zipkin or Jaeger to visualize it.</p>

<p>Istio can't know when you make outcalls from your application for which original request it was for unless you do copy the headers.</p>

<p>Does that help/makes sense ?</p>
"
Zipkin,47592674,47591972,4,"2017/12/01, 13:44:22",False,"2017/12/01, 13:44:22",1176,2269535,0,"<p>It looks a incompatibility between version in my opinion, something is overridden when you inject the spring-cloud-starter-zipkin dependency</p>

<p>What i don't understand from your question is:</p>

<p><strong>Do you need this dependency ""spring-cloud-starter-zipkin"", are you using it?</strong></p>

<p>If no obviously just put it out of the pom, if yes, check which version are you using:</p>

<p>mvn dependency:tree and try to align spring-cloud-starter-zipkin with the Spring version you are using.</p>

<p>Playing a bit with the version of your artifacts you will find the solution.</p>

<p>Hope it helped.</p>
"
Zipkin,52585722,47026664,0,"2018/10/01, 09:37:07",False,"2020/09/02, 11:35:57",61,3548002,0,"<p>I use &quot;TraceCallable&quot; class from &quot;<a href=""http://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-sleuth-core"" rel=""nofollow noreferrer"">spring-cloud-sleuth</a>&quot; lib to solve it in my code.</p>
<p>My code example is:</p>
<pre class=""lang-kotlin prettyprint-override""><code>@Component
class TracingCallableSupplier(
    private val tracing: Tracing,
    private val spanNamer: SpanNamer
) {

    /**
     * Supply callable which will use tracing from parent while performing jobs.
     */
    fun &lt;T : Any?&gt; supply(function: () -&gt; T): Callable&lt;T&gt; {
        return TraceCallable(tracing, spanNamer, Callable(function))
    }
}

@Service
class MyBean{
    @Autowired
    lateinit var traceSupplier : TracingCallableSupplier

    fun myMethod {
        val callable = tracingCallableSupplier.supply {
            // ... some code to be called asynchronous...
        }

        // simplest coroutine...
        GlobalScope.launch {
            callable.call()
        }

        // more advanced coroutine usage...
        val deferred = (0 until 10).map {
            async {
                callable.call()
            }
        }

        runBlocking {
            deferred.map {
                val result = it.await()
                // ... some processing ...
            }
        }
    }
}
</code></pre>
"
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145,10936956,1,"<p><em><strong>Attention: This solution does works for logging purposes, but does not work for other Sleuth features like instrumenting RestTemplates to send the tracing headers to other services. So unfortunately, this is not a fully working solution. :(</strong></em></p>
<p>Some time after adopting @Baca's solution, I discovered that Kotlin Coroutines offer direct integration with slf4j, which is what Spring Sleuth builds on.
Sleuth adds properties <code>X-B3-TraceId</code>, <code>traceId</code>, <code>X-B3-SpanId</code>, and <code>spanId</code> to the thread's MDC.</p>
<p>You can retain the parent thread's MDC for a coroutine with the code shown below. The coroutine framework will take care of restoring the MDC context on the worker thread whenever the coroutine is executed/resumed. This is the easiest solution I could discover so far. :)</p>
<pre class=""lang-kotlin prettyprint-override""><code>// add your own properties or use the ones already added by Sleuth
MDC.put(&quot;someLoggerProperty&quot;, &quot;someValue&quot;)

GlobalScope.launch(MDCContext()) {
    // your code goes here
}
</code></pre>
<p>The launch method takes an optional CoroutineContext and the coroutine-slf4j integration implements the MDCContext. This class captures the calling thread's MDC context (creates a copy) and uses that for the coroutine execution.</p>
<p>Add this dependency to your build.gradle:</p>
<pre><code>implementation group: 'org.jetbrains.kotlinx', name: 'kotlinx-coroutines-slf4j', version: '1.3.9'
</code></pre>
<p>Project: <a href=""https://github.com/Kotlin/kotlinx.coroutines/tree/master/integration/kotlinx-coroutines-slf4j"" rel=""nofollow noreferrer"">https://github.com/Kotlin/kotlinx.coroutines/tree/master/integration/kotlinx-coroutines-slf4j</a>
Documentation: <a href=""https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-slf4j/index.html"" rel=""nofollow noreferrer"">https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-slf4j/index.html</a></p>
"
Zipkin,46454753,46432583,1,"2017/09/27, 21:22:45",False,"2017/09/27, 21:22:45",2661,1813696,0,"<p>With Spring boot <strong>Dalston.SR3</strong> (which uses open zipkin 1.28) you can achieve this by setting property <strong>zipkin.storage.mem.max-spans=xxx</strong> This will limit the number of spans and discard old ones.</p>

<p>pom.xml</p>

<pre><code>&lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;version&gt;1.5.4.RELEASE&lt;/version&gt;
    &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
&lt;/parent&gt;

&lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
    &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;spring-cloud.version&gt;Dalston.SR3&lt;/spring-cloud.version&gt;
&lt;/properties&gt;
</code></pre>
"
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26,5772061,1,"<p>The best way to trace OpenStack project is to use Osprofiler library. If you just want to understand the workflow or just know about the types of calls being made inside OpenStack then Osprofiler is the best and easiest way to get the trace. Now Osprofiler is an accepted OpenStack project and the official project to get traces for OpenStack.</p>

<p>Instead of having to go through the whole code and adding instrumentation points near HTTP request or RPC calls, osprofiler is already integrated in all of the main projects of OpenStack (Nova, Neutron, Keystone, Glance etc..). You just have to enable osprofiler in the configuration files of each project in OpenStack to get a trace of that particular project.</p>

<p>You can go through this link - <a href=""https://docs.openstack.org/osprofiler/latest/"" rel=""nofollow noreferrer"">https://docs.openstack.org/osprofiler/latest/</a></p>

<p>Enabling of Osprofiler in the configuration files can be done by adding these lines at the end of the configuration file (nova.conf or neutron.conf) :</p>

<pre><code>[profiler]
enabled = True
trace_sqlalchemy = True
hmac_keys = SECRET_KEY
connection_string = messaging://
</code></pre>

<p>The connection_string parameter indicates the collector (where the trace information is stored). By default it uses Ceilometer.
You can actually redirect the trace information to other collectors like Elasticsearch by changing the connection_string parameter in the conf file to the elasticsearch server.</p>

<p>This is by far the easiest way to get a trace in OpenStack with just minimal effort. </p>
"
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45,4107097,0,"<p>After some discussion with Marcin Grzejszczak and Adrien Cole (zipkin and sleuth creators/active developers) I ended up creating a Jersey filter that acts as bridge between sleuth and brave. Regarding AMQP integration, added a new @StreamListener with a conditional for zipkin format spans (using headers). Sending messages to the sleuth exchange with zipkin format will then be valid and consumed by the listener. For javascript (zipkin-js), I ended up creating a new AMQP Logger that sends zipkin spans to a determined exchange. If someone ends up reading this and needs more detail, you're welcome to reach out to me.</p>
"
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434,7122593,0,"<p>Take a look at Sampling interval in the docs : </p>

<blockquote>
  <p>In distributed tracing the data volumes can be very high so sampling can be important (you usually don’t need to export all spans to get a good picture of what is happening). Spring Cloud Sleuth has a Sampler strategy that you can implement to take control of the sampling algorithm. Samplers do not stop span (correlation) ids from being generated, but they do prevent the tags and events being attached and exported. By default you get a strategy that continues to trace if a span is already active, but new ones are always marked as non-exportable. If all your apps run with this sampler you will see traces in logs, but not in any remote store. For testing the default is often enough, and it probably is all you need if you are only using the logs (e.g. with an ELK aggregator). If you are exporting span data to Zipkin or Spring Cloud Stream, there is also an AlwaysSampler that exports everything and a PercentageBasedSampler that samples a fixed fraction of spans.</p>
</blockquote>

<p><a href=""http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling</a></p>
"
Zipkin,44395594,44382633,1,"2017/06/06, 19:47:13",False,"2017/06/06, 19:47:13",37971,1237575,0,"<p>You are blending Zipkin autoconfigure version 1.2 with Zipkin 1.26. This results in a version missmatch.</p>
"
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1,7608262,0,"<p>The problem is casued by two reasons. First:just as Rafael Winterhalter said the version dismatched; second: you are lacking dependency for zipkin. Finally i fixed the problem by adding the whole 'io.zipkin.java:zipkin"" library. Here is my final pom file with the storage type of elasticsearch:</p>

<pre><code> &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin&lt;/artifactId&gt;
            &lt;version&gt;1.21.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
            &lt;version&gt;1.21.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
            &lt;version&gt;1.21.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-storage-elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;1.21.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.integration&lt;/groupId&gt;
            &lt;artifactId&gt;spring-integration-jmx&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
</code></pre>
"
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1,7983801,0,"<p>Appears that a sleuth span is not the same as a Zipkin span.
Hence, in the above code there is no way to instantiate a default tracer with zipkin span reporter.
I converted the sleuth span into a zipkin span and then reported it to zipkin. The class to convert it is available in spring-cloud-sleuth-stream. I used pretty much the same class with some tweaks.</p>
"
Zipkin,44062121,44027882,0,"2017/05/19, 08:32:21",False,"2017/05/19, 08:32:21",8336,1773866,2,"<p>Here you have a very basic example of Sleuth &amp; HTTP communication. <a href=""https://github.com/openzipkin/sleuth-webmvc-example"" rel=""nofollow noreferrer"">https://github.com/openzipkin/sleuth-webmvc-example</a> You can set your dependencies in a similar manner and everything should work fine. In your example you've got Stream but I don't think you're using it so it's better to remove it.</p>
"
Zipkin,45815790,44027882,0,"2017/08/22, 13:58:17",False,"2017/08/22, 13:58:17",253,5751473,0,"<p>As M.Deinum said remove <code>stream</code> and <code>stream-rabbit</code> dependencies what if you do not need some AMQP server to store the trace message.</p>

<p>or</p>

<p>config the AMQP(rabbitMQ in your code) from application-configuration(both) and add <code>zipkin-stream</code> &amp; <code>stream-rabbit</code> in <code>zipkin-server</code> side, so this time your app(<code>zipkin-client</code>) will not direct connect with <code>zipkin-server</code> 
and it will be: </p>

<pre><code>zipkin-client &lt;==&gt; AMQP(rabbitMQ) &lt;==&gt; zipkin-server
</code></pre>
"
Zipkin,47431400,43913962,0,"2017/11/22, 11:49:20",False,"2017/11/22, 11:49:20",215,3702774,1,"<p>You may define all needed params via ENV options.
Here is a cmd for running zipkin in docker:</p>

<pre><code>docker run  -d -p 9411:9411 -e STORAGE_TYPE=elasticsearch -e ES_HOSTS=http://172.17.0.3:9200 -e ES_USERNAME=elastic -e ES_PASSWORD=changeme openzipkin/zipkin
</code></pre>

<p>All these params can be defined in Deployment (see <a href=""https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/"" rel=""nofollow noreferrer"">Expose Pod Information to Containers Through Environment Variables</a>)</p>
"
Zipkin,48982440,43782954,0,"2018/02/26, 08:10:42",False,"2018/02/26, 08:10:42",181,6708214,1,"<p>1.You should check if your Zipkin Server is on.  </p>

<p>2.You should check if the Span transfering is async.</p>

<p>In HTTP,Zipkin uses in-band transfer,all the information carried in HTTP headers.The cost time of generating Span is about 200 nanosecond.</p>
"
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762,1227937,1,"<p>The TL;DR; is that <a href=""https://github.com/openzipkin/b3-propagation"" rel=""nofollow noreferrer"">B3 propagation</a> was initially designed for fixed size data: carrying data ancillary to tracing isn't in scope, and for this reason any solution that extends B3 in such a fashion wouldn't be compatible with existing code.</p>

<p>So, that means any solution like this will be an extension which means custom handling in the <a href=""http://zipkin.io/pages/architecture.html"" rel=""nofollow noreferrer"">instrumented apps</a> which are the things passing headers around. The server won't care as it never sees these headers anyway.</p>

<p>Ways people usually integrate other things like flags with zipkin is to add a tag aka binary annotation including its value (usually in the root span). This would allow you to query or retrieve these offline, but it doesn't address in-flight lookups from applications.</p>

<p>Let's say that instead of using an intermediary like linkerd, or a platform-specific propagated context, we want to dispatch the responsibility to the tracing layer. Firstly, what sort of data could work alright? The easiest is something set-once (like zipkin's trace id). Anything set and propagated without mutating it is the least mechanics. Next in difficulty is appending new entries mid-stream, and most difficult is mutating/merging entries.</p>

<p>Let's assume this is for inbound flags which never change through the request/trace tree. We see a header when processing trace data, we store it and forward it downstream. If this value doesn't need to be read by the tracing system, it is easiest, as it is largely a transport/propagation concern. For example, maybe other middleware read that header and it is only a ""side job"" we are adding to the tracer to remember certain things to pass along. If this was done in a single header, it would be less code than a pattern in each of the places this would be to added. It would be even less code if the flags could be encoded in a number, however unrealistic that may be.</p>

<p>There are libraries with apis to manipulate the propagated context manually, for example, <a href=""https://github.com/JonathanMace/tracingplane"" rel=""nofollow noreferrer"">""baggage"" from brownsys</a> and OpenTracing (of which some libraries support zipkin). The former aims to be a generic layer for any instrumentation (ex monitoring, chargeback, tracing etc) and the latter is specific to tracing. OpenTracing has defines abstract types like <a href=""http://opentracing.io/documentation/pages/api/cross-process-tracing.html"" rel=""nofollow noreferrer"">injector and extractor</a> which could be customized to carry other fields. However, you still would need a concrete implementation (which knows your header format etc) in order to do this. Unless you want applications to read this data, it would need to be a secret detail of that implementation (specifically the trace context).</p>

<p>Certain zipkin-specific libraries like Spring Cloud Sleuth and Brave have means to <a href=""https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/Propagation.java"" rel=""nofollow noreferrer"">customize how headers are parsed</a>, to support variants of B3 or new or site-specific trace formats. Not all support this at the moment, but I would expect this type of feature to become more common. This means you may need to do some surgery in order to support all platforms you may need to support.</p>

<p>So long story short is that there are some libraries which are pluggable with regards to propagation, and those will be easiest to modify to support this use case. Some code will be needed regardless as B3 doesn't currently define an expression like this.</p>
"
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762,1227937,0,"<p>In general it is better to use the zipkin's http variant of Elasticsearch as it cannot conflict with Spring Boot's elasticsearch library versions.</p>

<p>I would set everything in zipkin's group id to latest (currently 1.21.0 which is Spring Boot 1.4.x) and use zipkin-autoconfigure-storage-elasticsearch-http (plus.. the one you are using <a href=""https://github.com/openzipkin/zipkin/issues/1511"" rel=""nofollow noreferrer"">will be dropped</a>)</p>

<p>Make sure your es hosts is specified in url syntax ex. <a href=""http://host1:9200"" rel=""nofollow noreferrer"">http://host1:9200</a></p>
"
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971,1237575,0,"<p>Zipkin generates traces and communicates them back to a Zipkin server. The latter action is typically performed via HTTP but Zipkin is agnostic to how a span is started and ended.</p>

<p>If you want to measure the execution time of a single method, you would normally create a local span that is nested inside a server span. Zipkin is a distributrd tracing tool for discovering machine-to-machine interaction which is why spans often cover HTTP. If you want to measure execution time of a method, a tool like metrics might be more suited.</p>
"
Zipkin,41798515,40954585,1,"2017/01/23, 04:38:41",True,"2017/01/23, 04:38:41",762,1227937,1,"<p>This was an issue with MySQL 5.7 and more recently resolved. You can try latest Zipkin.</p>
"
Zipkin,39601988,39600581,2,"2016/09/20, 22:06:46",True,"2016/09/20, 22:06:46",8336,1773866,1,"<p>You'd have to implement your own ZipkinSpanReporter that would look more or less like <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/v1.0.8.RELEASE/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java"" rel=""nofollow"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/v1.0.8.RELEASE/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java</a> . In the next version of Sleuth you will be able to register a bean of ZipkinSpanReporter that can you a custom version of a publisher - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/1.0.x/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java"" rel=""nofollow"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/1.0.x/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java</a></p>
"
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762,1227937,1,"<p>Instrumenting a library is something that sometimes folks have to do for one reason or another. There are several tracer libraries in Java but the salient points about creating a tracer are either on the website, or issues on the website.</p>

<p><a href=""http://zipkin.io/pages/instrumenting.html"" rel=""nofollow"">http://zipkin.io/pages/instrumenting.html</a>
<a href=""https://github.com/openzipkin/openzipkin.github.io/issues/11"" rel=""nofollow"">https://github.com/openzipkin/openzipkin.github.io/issues/11</a></p>

<p>OpenTracing also has some nice fundamentals to look at <a href=""http://opentracing.io/"" rel=""nofollow"">http://opentracing.io/</a></p>

<p>This isn't a one-answer type of question, in my experience, as you'll learn you'll need to address other things that tracer libraries address out-of-box. For that reason I'd highly suggest joining gitter so that you
can have a dialogue through your journey <a href=""https://gitter.im/openzipkin/zipkin"" rel=""nofollow"">https://gitter.im/openzipkin/zipkin</a></p>
"
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13,3491416,1,"<p>I was recording wrong annotation i.e client instead of server. </p>

<p>Just a simple change did the trick.</p>

<p><code>Trace.traceService(""Function1"",""Test"")</code></p>

<p>Sample working Zipkin example: <a href=""https://gist.github.com/AkhilJ876/3e38757c28d43924f296dd2d147c0bd9#file-zipkintracing_example-L34"" rel=""nofollow"">https://gist.github.com/AkhilJ876/3e38757c28d43924f296dd2d147c0bd9#file-zipkintracing_example-L34</a></p>
"
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91,3623286,1,"<p>Same problem here with the docker images using Cassandra (1.40.1, 1.40.2, 1.1.4).</p>

<p>This is a problem specific to using Cassandra as the storage tier.  Mysql and the in-memory storage generate the dependency graph on-demand as expected.</p>

<p>There are references to the following project to generate the Cassandra graph data for the UI to display.  </p>

<ul>
<li><a href=""https://github.com/openzipkin/zipkin-dependencies-spark"" rel=""nofollow"">https://github.com/openzipkin/zipkin-dependencies-spark</a></li>
</ul>

<p>This looks to be superseded by ongoing work mentioned here</p>

<ul>
<li><a href=""https://github.com/openzipkin/zipkin-dependencies-spark/issues/22"" rel=""nofollow"">https://github.com/openzipkin/zipkin-dependencies-spark/issues/22</a></li>
</ul>
"
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592,7251967,0,"<blockquote>
  <p>If storage type is other than inline storage, for zipkin dependencies graph you have to start separate cron job/scheduler which reads the storage database and builds the graph. Because zipkin dependencies is separate spark job .
  For reference : <a href=""https://github.com/openzipkin/docker-zipkin-dependencies"" rel=""nofollow noreferrer"">https://github.com/openzipkin/docker-zipkin-dependencies</a></p>
  
  <p>I have used zipkin with elastic search as storage type. I will share the steps for setting up the zipkin dependencies with elastic search and cron job for the same: </p>
</blockquote>

<pre><code>1.cd ~/
2. curl -sSL https://zipkin.io/quickstart.sh | bash -s io.zipkin.dependencies:zipkin-dependencies:LATEST zipkin-dependencies.jar
3. touch  cron.sh or vi cron.sh  
4. paste this content  : 
STORAGE_TYPE=elasticsearch ES_HOSTS=https:172.0.0.1:9200 ES_HTTP_LOGGING=BASIC ES_NODES_WAN_ONLY=true java -jar zipkin-dependencies.jar
5.chmode a+x cron.sh //make file executable
6.crontab -e   
window will open paste  below content 
0 * * * * cd &amp;&amp; ./cron.sh //every one hour it will start the cron job if you need every 5 minutes change the commmand to '*/5 * * * * cd &amp;&amp; ./cron.sh'
7. to check cron job is schedule run commant crontab -l
</code></pre>

<blockquote>
  <p>Other solution is to start a separate service and run the cron job
  using docker</p>
  
  <p>Steps to get the latest zipkin-dependencies jar try running given
  command on teminal</p>
</blockquote>

<pre><code>cd /zipkindependencies // where your Dockerfile is available
curl -sSL https://zipkin.io/quickstart.sh | bash -s io.zipkin.dependencies:zipkin-dependencies:LATEST
</code></pre>

<blockquote>
  <p>you will get jar file at above mention directory </p>
  
  <p>Dockerfile</p>
</blockquote>

<pre><code>FROM openjdk:8-jre-alpine
ENV STORAGE_TYPE=elasticsearch
ENV ES_HOSTS=http://172.0.0.1:9200
ENV ES_NODES_WAN_ONLY=true
ADD crontab.txt /crontab.txt
ADD script.sh /script.sh
COPY entry.sh /entry.sh
COPY zipkin-dependencies.jar  /
RUN chmod a+x /script.sh /entry.sh
RUN /usr/bin/crontab /crontab.txt
CMD [""/entry.sh""]
EXPOSE 8080
</code></pre>

<blockquote>
  <p>entry.sh</p>
</blockquote>

<pre><code>#!/bin/sh
# start cron
/usr/sbin/crond -f -l 8
</code></pre>

<blockquote>
  <p>script.sh</p>
</blockquote>

<pre><code>#!/bin/sh
java ${JAVA_OPTS} -jar /zipkin-dependencies.jar
</code></pre>

<blockquote>
  <p>crontab.txt</p>
</blockquote>

<pre><code>0 * * * * /script.sh &gt;&gt; /var/log/script.log
</code></pre>
"
Zipkin,21582785,21551629,3,"2014/02/05, 18:27:05",True,"2014/02/05, 18:27:05",26,2070089,1,"<p>This is due to not having an instance of the query server running.</p>

<p>I'm in the middle of a re-write that'll simplify all of this. Until then, you need to spin up a query server.</p>
"
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336,1773866,1,"<p>That was a bug in Spring Cloud Sleuth in Edgware. The Stream Kafka Binder in Edgware required explicit passing of headers that should get propagated. The side effect of adding <code>sleuth-stream</code> on the classpath was exactly that feature. By fixing the <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005</a> issue we're adding back the missing feature to core. This is not ported to Finchley since Stream Kafka Binder in Finchley passes all headers by default.</p>

<p>The workaround for Edgware is to pass a list of headers in the following manner:</p>

<pre><code>spring:
  cloud:
    stream:
      kafka:
        binder:
          headers:
            - spanId
            - spanSampled
            - spanProcessId
            - spanParentSpanId
            - spanTraceId
            - spanName
            - messageSent
</code></pre>
"
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396,4641689,1,"<p>I would say you can have fluentd in multiple namespaces and Elasticsearch in one namespace and fluentd can discover Elasticsearch via K8s internal DNS A/AAAA record e.g. <code>elasticsearch.${namespace}.svc.cluster.local</code>.</p>

<p>I don't have any link to the best practice, but I would show you a practice I saw from the community.</p>

<ul>
<li><p>If you are not familiar with configuring K8s cluster, I recommend to deploy ELK by Helm. It will save you a lot of time and give you enough configuration options.
<a href=""https://github.com/helm/charts/tree/master/stable/elastic-stack"" rel=""nofollow noreferrer"">https://github.com/helm/charts/tree/master/stable/elastic-stack</a>.</p></li>
<li><p>Install your ELK helm release on a <code>separate</code> namespace, for example: <code>logging</code>.</p></li>
<li><p>Install fluentd in any namespaces in your cluster and configure elasticsearch host <a href=""https://github.com/helm/charts/tree/master/stable/fluentd-elasticsearch"" rel=""nofollow noreferrer"">https://github.com/helm/charts/tree/master/stable/fluentd-elasticsearch</a></p></li>
</ul>
"
Zipkin,57200810,57143690,0,"2019/07/25, 14:23:59",False,"2019/07/25, 14:23:59",9,4762878,1,"<p>I used an Aspect and from the returned ResponseEntity object, decided whether or not to programatically add an error tag to span. With this tag, zipkin will identify and highlight the trace in red colour. 
Below is the code snippet to add error tag to span.</p>

<pre><code>import org.springframework.cloud.sleuth.Span;
import org.springframework.cloud.sleuth.Tracer;
----
@Autowired
private Tracer tracer;

public void addErrorTag(String message) {
    Span currentSpan = tracer.getCurrentSpan();
    currentSpan.logEvent(""ERROR: "" + message);
    tracer.addTag(""error"", message);
}
</code></pre>
"
Zipkin,48765679,48668426,0,"2018/02/13, 13:23:09",False,"2018/02/13, 13:23:09",681,4255878,0,"<p>i think i found a suitable way to do this. After further thinking my idea went into the direction of using annotations and aspects for intercepting HTTP requests from/to thrifts http client which seems to be quite some work.</p>

<p>after further search, i found this library for spring-boot serving exactly my needs:
<a href=""https://github.com/aatarasoff/spring-thrift-starter"" rel=""nofollow noreferrer"">https://github.com/aatarasoff/spring-thrift-starter</a></p>
"
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1,12382349,0,"<p>I dont kown can you see my pic and I put my code:</p>

<p><strong>pom.xml:</strong></p>

<pre><code>&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;configuration&gt;
                &lt;mainClass&gt;com.test.itoken.zipkin.ZipKinApplication&lt;/mainClass&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p><strong>ZipkinApplication.java:</strong></p>

<pre><code>package com.test.itoken.zipkin;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.EnableEurekaClient;
import zipkin.server.internal.EnableZipkinServer;

@SpringBootApplication
@EnableEurekaClient
@EnableZipkinServer
public class ZipkinApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZipkinApplication.class, args);
    }

}
</code></pre>

<p><strong>The error:</strong></p>

<pre><code>Exception in thread ""main"" java.lang.ClassNotFoundException: com.test.itoken.zipkin.ZipKinApplication
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    at org.springframework.boot.loader.LaunchedURLClassLoader.loadClass(LaunchedURLClassLoader.java:93)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:46)
    at org.springframework.boot.loader.Launcher.launch(Launcher.java:87)
    at org.springframework.boot.loader.Launcher.launch(Launcher.java:50)
    at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51)
</code></pre>
"
Zipkin,53692930,53692437,3,"2018/12/09, 15:41:18",False,"2018/12/09, 15:41:18",81,2232476,0,"<p>I can say your YAML has some bad indentation and things are not in the right sections even. Otherwise though, you are trying to run Zipkin in an unsupported configuration. Please check out our quickstart documentation: <a href=""https://zipkin.io/pages/quickstart.html"" rel=""nofollow noreferrer"">https://zipkin.io/pages/quickstart.html</a></p>
"
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963,3625215,1,"<p>There are 2 approaches to this </p>

<ol>
<li>Start Zipkin server with SpringBootApplication</li>
<li>Start Zipkin server as a standalone and add url in SpringBootServer</li>
</ol>

<p>Looking at your yml file you have added </p>

<pre><code>zipkin:
   base-url: http://localhost:8082
</code></pre>

<p>which means your approach is 2.</p>

<p>But then in your pom, you have added <code>zipkin-server</code> and <code>zipkin-autoconfigure-ui</code> dependencies which is not required.</p>

<p>I will try to separate both setups</p>

<p><strong>1. To Start Zipkin server with SpringBootApplication</strong> </p>

<p><strong>pom.xml</strong></p>

<pre><code> &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
    &lt;/dependency&gt;


  &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;
</code></pre>

<p><strong>application.properties</strong></p>

<pre><code>spring.application.name=zipkin-server
server.port=9411
</code></pre>

<p><strong>Application.java</strong></p>

<pre><code>@SpringBootApplication
@EnableZipkinStreamServe
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(ZipkinServerApplication.class, args);
    }
}
</code></pre>

<p><strong>2. To Start Zipkin server as a standalone and use SpringBootApplication as Zipkin Client</strong></p>

<p><a href=""https://zipkin.io/pages/quickstart.html"" rel=""nofollow noreferrer"">Start Zipkin server</a> </p>

<p><strong>pom.xml</strong></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p><strong>application.properties</strong></p>

<pre><code>spring.zipkin.base-url=http://localhost:9411/
spring.sleuth.sampler.probability=1
</code></pre>

<p><strong>Edit 1:</strong></p>

<p><code>@EnableZipkinServer</code> is deprecated and unsupported as per Brian Devins's comment. So, please go through the <a href=""https://zipkin.io/pages/existing_instrumentations.html"" rel=""nofollow noreferrer"">doc</a> for more detail info.</p>
"
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336,1773866,0,"<p>You're using an ancient version of Spring CLoud. Please upgrade to latest Edgware. The RxJava support is very basic so we suggest that you use Project Reactor. To do that just migrate to Finchley and it should work out of the box with WebFlux.</p>
"
Zipkin,50268263,50267380,6,"2018/05/10, 11:01:52",False,"2018/05/10, 11:01:52",8336,1773866,1,"<p>You're using an ancient version of Sleuth, can you please upgrade? Why do you provide Zipkin's version manually? Also as far as I see you're using the Sleuth's Zipkin server (that is deprecated in Edgware and removed in Finchley). My suggestion is that you stop using the Sleuth's Stream server (you can read more about this here <a href=""https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka</a>). </p>

<pre><code>&lt;dependencyManagement&gt; (1)
         &lt;dependencies&gt;
             &lt;dependency&gt;
                 &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                 &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                 &lt;version&gt;${release.train.version}&lt;/version&gt;
                 &lt;type&gt;pom&lt;/type&gt;
                 &lt;scope&gt;import&lt;/scope&gt;
             &lt;/dependency&gt;
         &lt;/dependencies&gt;
   &lt;/dependencyManagement&gt;

   &lt;dependency&gt; (2)
       &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
       &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt; (3)
       &lt;groupId&gt;org.springframework.amqp&lt;/groupId&gt;
       &lt;artifactId&gt;spring-rabbit&lt;/artifactId&gt;
   &lt;/dependency&gt;
</code></pre>

<p>1) In order not to pick versions by yourself it’s much better if you add the dependency management via the Spring BOM</p>

<p>2) Add the dependency to spring-cloud-starter-zipkin - that way all dependent dependencies will be downloaded</p>

<p>3) To automatically configure rabbit, simply add the spring-rabbit dependency</p>
"
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93,6816012,0,"<p>the simplest solution i have found for solving broken ui for zipkin thru gateway
is by changing following property of zipkin-server-shared.yml file inside zipkin server </p>

<p><code>zipkin:
  ui:
   base-path: /zipkin
</code>
change above property to </p>

<p><code>zipkin:
  ui:
   base-path: /api/tracing/zipkin
</code></p>

<p>and change ur zuul path to following
<code>zuul.routes.zipkin.path=/api/tracing/*</code></p>

<p>and than access zipkin using follwing url</p>

<p><code>https://gatewayhost:port/api/tracing/zipkin/</code></p>

<p>give attention to small details in config and dont forget to put trailing ""/"" after zipkin  in url</p>
"
Zipkin,46093745,46092526,2,"2017/09/07, 13:12:45",False,"2017/09/07, 13:12:45",8336,1773866,0,"<p>It has nothing to do with Spring Cloud Sleuth or Zipkin. @SpringBootApplication automatically does @ComponentScan so all @RestController classes will get registered as beans if they are in the same package as your @SpringBootApplication annotated class or if it's in the child packages. Please read and try to understand how Spring Boot works by reading this chapter of the docs - <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-using-springbootapplication-annotation.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-using-springbootapplication-annotation.html</a></p>
"
Zipkin,62140570,54597464,0,"2020/06/01, 23:33:01",False,"2020/06/01, 23:33:01",1111,898472,0,"<p>I had this problem while using gunicorn with gevent as the worker class. To resolve and get cloud traces working the solution was to monkey patch grpc like so </p>

<pre><code>from gevent import monkey
monkey.patch_all()

import grpc.experimental.gevent as grpc_gevent
grpc_gevent.init_gevent()
</code></pre>

<p>See <a href=""https://github.com/grpc/grpc/issues/4629#issuecomment-376962677"" rel=""nofollow noreferrer"">https://github.com/grpc/grpc/issues/4629#issuecomment-376962677</a></p>
"
Zipkin,59260266,48857181,0,"2019/12/10, 06:31:59",False,"2019/12/10, 06:31:59",907,3941521,0,"<p><code>Object.keys(headers).filter((key) =&gt; TRACING_HEADERS.includes(key)).map((key) =&gt; headers[key])</code> returns an <em>array</em>.</p>

<p>What you want is:</p>

<pre><code>Object.keys(headers)                                                                                                                                                                              
  .filter(key =&gt; TRACING_HEADERS.includes(key))                                                                                                                                                                    
  .reduce((obj, key) =&gt; {                                                                                                                                                                                          
    obj[key] = headers[key];                                                                                                                                                                                       
    return obj;                                                                                                                                                                                                    
  }, {})
</code></pre>

<p>I'm pretty sure this isn't an istio / distributed tracing issue ;-)</p>
"
Zipkin,65147869,48857181,0,"2020/12/04, 19:17:32",False,"2020/12/04, 19:17:32",36,4216591,0,"<p>b3-propagation of x-b3-parentspanid (<a href=""https://github.com/openzipkin/b3-propagation"" rel=""nofollow noreferrer"">https://github.com/openzipkin/b3-propagation</a>) can be configured in your application.yml by adding:</p>
<pre><code>opentracing:
  jaeger:
    enable-b3-propagation: true
</code></pre>
"
Zipkin,50985064,50855480,0,"2018/06/22, 12:38:46",True,"2018/06/22, 12:38:46",231,4158442,7,"<p><a href=""https://github.com/opentracing-contrib/java-spring-cloud"" rel=""noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud</a> project automatically sends standard logging to the active span. Just add the following dependency to your pom.xml</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
   &lt;artifactId&gt;opentracing-spring-cloud-starter&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Or use this <a href=""https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core"" rel=""noreferrer"">https://github.com/opentracing-contrib/java-spring-cloud/tree/master/instrument-starters/opentracing-spring-cloud-core</a> starter if you want only logging integration.</p>
"
Zipkin,56222158,50855480,2,"2019/05/20, 16:39:29",False,"2019/05/20, 16:39:29",61,9483992,0,"<p>Then I use <strong>opentracing-spring-jaeger-cloud-starter</strong></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
    &lt;artifactId&gt;opentracing-spring-jaeger-cloud-starter&lt;/artifactId&gt;
    &lt;version&gt;2.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>I got just one line in console with current trace and span
i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - my_method</p>

<pre><code>2019-05-20 16:07:59.549 DEBUG 24428 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [632103eb] HTTP POST ""/api""
2019-05-20 16:07:59.552 DEBUG 24428 --- [ctor-http-nio-2] s.w.r.r.m.a.RequestMappingHandlerMapping : [632103eb] Mapped to public reactor.core.publisher.Mono&lt;org.springframework.http.ResponseEntity&lt;model.Response&gt;&gt; service.controller.method(model.Request)
2019-05-20 16:07:59.559 DEBUG 24428 --- [ctor-http-nio-2] .s.w.r.r.m.a.RequestBodyArgumentResolver : [632103eb] Content-Type:application/json
2019-05-20 16:08:01.450  INFO 24428 --- [ctor-http-nio-2] i.j.internal.reporters.LoggingReporter   : Span reported: f1a264bbe2c7eae9:f1a264bbe2c7eae9:0:1 - method
2019-05-20 16:08:01.450 DEBUG 24428 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [632103eb] Completed 200 OK
</code></pre>

<p>Then I use <strong>spring-cloud-starter-sleuth</strong></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>I got the Trace and Spans like [my-service,90e1114e35c897d6,90e1114e35c897d6,false] in each line and it's helpfull for filebeat in ELK</p>

<pre><code>2019-05-20 16:15:38.646 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [3e578505] HTTP POST ""/api""
2019-05-20 16:15:38.662 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Received a request to uri [/api]
2019-05-20 16:15:38.667 DEBUG [my-service,,,] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Handled receive of span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:38.713 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] s.w.r.r.m.a.RequestMappingHandlerMapping : [3e578505] Mapped to public reactor.core.publisher.Mono&lt;org.springframework.http.ResponseEntity&lt;model.Response&gt;&gt; service.controller.method(model.Request)
2019-05-20 16:15:38.727 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] .s.w.r.r.m.a.RequestBodyArgumentResolver : [3e578505] Content-Type:application/json
2019-05-20 16:15:39.956 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [gine-1-thread-1] .s.w.r.r.m.a.ResponseEntityResultHandler : Using 'application/json;charset=UTF-8' given [*/*] and supported [application/json;charset=UTF-8, application/*+json;charset=UTF-8, text/event-stream]
2019-05-20 16:15:40.009 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Adding a method tag with value [method] to a span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.009 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Adding a class tag with value [Controller] to a span NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.010 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.c.s.instrument.web.TraceWebFilter    : Handled send of NoopSpan(90e1114e35c897d6/90e1114e35c897d6)
2019-05-20 16:15:40.021 DEBUG [my-service,90e1114e35c897d6,90e1114e35c897d6,false] 12548 --- [ctor-http-nio-2] o.s.w.s.adapter.HttpWebHandlerAdapter    : [3e578505] Completed 200 OK
</code></pre>

<p>How could I get the same log in console using opentracing-spring-jaeger-cloud-starter ?</p>

<p>my opentracing config</p>

<pre><code>opentracing:
  jaeger:
    enabled: true
    enable-b3-propagation: true
    log-spans: true
    const-sampler:
      decision: true
    http-sender:
      url: http://jaeger-collector:14268/api/traces

</code></pre>
"
Zipkin,58798104,50855480,0,"2019/11/11, 10:48:18",False,"2019/11/11, 10:48:18",11,10465104,1,"<p>Here is what I did to make jdbc related logs from Logback (Slf4j) write into Jaeger server:</p>

<p>Beginning with Logback config (logback-spring.xml):</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;configuration&gt;
&lt;include resource=""org/springframework/boot/logging/logback/defaults.xml""/&gt;
&lt;springProperty scope=""context"" name=""consoleAppender"" source=""logging.console.enabled"" defaultValue=""false""/&gt;
&lt;property name=""ENV"" value=""${SPRING_PROFILES_ACTIVE:-dev}""/&gt;

&lt;include resource=""org/springframework/boot/logging/logback/console-appender.xml""/&gt;

&lt;jmxConfigurator/&gt;

&lt;appender name=""JSON_CONSOLE"" class=""ch.qos.logback.core.ConsoleAppender""&gt;
    &lt;encoder class=""net.logstash.logback.encoder.LogstashEncoder""&gt;
        &lt;includeMdc&gt;true&lt;/includeMdc&gt;
        &lt;customFields&gt;{""log_type"":""application"",""appname"":""products-rs-load"", ""environment"": ""${ENV}""}
        &lt;/customFields&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;
&lt;appender name=""myAppender"" class=""com.test.MyAppender""&gt;
&lt;/appender&gt;
&lt;root level=""DEBUG""&gt;
    &lt;appender-ref ref=""myAppender""/&gt;
&lt;/root&gt;
&lt;logger name=""org.springframework.boot"" level=""INFO""/&gt;
&lt;logger name=""p6spy"" additivity=""false"" level=""ALL""&gt;
    &lt;appender-ref ref=""myAppender"" /&gt;
&lt;/logger&gt;
&lt;/configuration&gt;
</code></pre>

<p>Here is my appender:</p>

<pre><code>import ch.qos.logback.core.AppenderBase;
public class MyAppender extends AppenderBase {

@Override
protected void append(Object eventObject) {
    LoggingEvent event = (LoggingEvent) eventObject;

    final String loggerName = event.getLoggerName();

    // only DB related operations have to be traced:
    if (!(""p6spy"".equals(loggerName))) {
        return;
    }
    /// Tracer config is straight forward
    Span sp = TracingUtils.buildActiveChildSpan(loggerName, null);

    if (Level.ERROR.equals(event.getLevel())) {
        TracingUtils.setErrorTag(sp);
    }
    Map&lt;String, String&gt; fields = new HashMap&lt;String, String&gt;();
    fields.put(""level"", event.getLevel().toString());
    fields.put(""logger"", loggerName);
    fields.put(""content"", event.getFormattedMessage());
    sp.log(fields);

    sp.finish();
  }
}
</code></pre>
"
Zipkin,47672314,47670883,2,"2017/12/06, 12:42:50",True,"2017/12/06, 12:42:50",3566,1358676,11,"<p>I found that I need to add a sampler percentage. By default zero percentage of the samples are sent and that is why the sleuth was not sending anything to zipkin. when I added <code>spring.sleuth.sampler.percentage=1.0</code> in the properties files, it started working.</p>
"
Zipkin,49838749,47670883,0,"2018/04/15, 08:20:04",False,"2018/04/15, 08:20:04",201,7956609,2,"<p>If you are exporting all the span data to Zipkin, sampler can be installed  by creating a bean definition in the Spring boot main class </p>

<pre><code>  @Bean
  public Sampler defaultSampler() {
    return new AlwaysSampler();
  }
</code></pre>
"
Zipkin,54384025,47670883,0,"2019/01/27, 02:18:35",False,"2019/01/27, 02:18:35",339,1405291,9,"<p>For the latest version of cloud dependencies <code>&lt;version&gt;Finchley.SR2&lt;/version&gt;</code><br>
The correct property to send traces to zipkin is: <code>spring.sleuth.sampler.probability=1.0</code>
Which has changed from percentage to probability.</p>
"
Zipkin,60284574,60190181,1,"2020/02/18, 17:41:23",False,"2020/02/18, 17:41:23",593,2750758,0,"<p>Details of error (Java stack trace) would be really useful here.</p>

<p>By error message I assume, you are using <a href=""https://qpid.apache.org/components/jms/index.html"" rel=""nofollow noreferrer"">qpid JMS client</a>, that is performing check of message properties' names. These names can contain only characters, that are valid <a href=""https://docs.oracle.com/javase/specs/jls/se11/html/jls-3.html#jls-IdentifierChars"" rel=""nofollow noreferrer"">Java identifier characters</a>.</p>

<p>In string 'queue-name' there is a '-' character, that is not Java identifier. To fix, you need to change 'queue-name' into something with valid characters, for example 'queue_name' (with underscore), or 'queueName' (camel case).</p>
"
Zipkin,60330998,60190181,0,"2020/02/21, 03:40:33",False,"2020/02/21, 03:40:33",16250,8381946,4,"<p>Section 3.5.1 of the JMS 2 specification states this about message properties:</p>

<blockquote>
  <p>Property names must obey the rules for a message selector identifier. See
  Section 3.8 “Message selection” for more information.</p>
</blockquote>

<p>In regards to identifiers, section 3.8.1.1 states, in part:</p>

<blockquote>
  <p>An identifier is an unlimited-length character sequence that must begin with a Java identifier start character; all following characters must be Java identifier part characters. An identifier start character is any character for which the method <a href=""https://docs.oracle.com/javase/7/docs/api/java/lang/Character.html#isJavaIdentifierStart(char)"" rel=""nofollow noreferrer""><code>Character.isJavaIdentifierStart</code></a> returns <code>true</code>. This includes '_' and '$'. An identifier part character is any character for which the method  <a href=""https://docs.oracle.com/javase/7/docs/api/java/lang/Character.html#isJavaIdentifierPart(char)"" rel=""nofollow noreferrer""><code>Character.isJavaIdentifierPart</code></a> returns <code>true</code>.</p>
</blockquote>

<p>If you pass the character <code>-</code> into either <code>Character.isJavaIdentifierStart</code> or <code>Character.isJavaIdentifierPart</code> the return value is <code>false</code>. In other words, <strong>the <code>-</code> character in the name of a message property violates the JMS specification</strong> and therefore will cause an error.</p>
"
Zipkin,60392406,60190181,0,"2020/02/25, 12:26:42",True,"2020/02/25, 12:26:42",98,11377504,1,"<p>From the error message its obvious that you are using qpid JMS client for communication through queues. 
qpid client won’t allow any keys which violates java variable naming convention e.g. you won’t be able to send x-request-id in a queue’s header
which qpid jms client is consuming as it’ll throw error. 
You need to take care of istio/zipkin to not to add certain headers (id you don’t need them actually) with the queue when its trying to communicate on azure bus. 
So you have to disable the istio/zipkin libraries  to intercept the request for queues so that request to/from queue can be made without headers. This will fix the issue.</p>
"
Zipkin,42580765,34272334,0,"2017/03/03, 15:55:09",False,"2017/03/03, 15:55:09",1247,1232692,1,"<p>It is a very long time ago, but it looks like it was moved here:</p>

<p><a href=""http://zipkin.io/pages/quickstart"" rel=""nofollow noreferrer"">http://zipkin.io/pages/quickstart</a></p>
"
Zipkin,57852082,34272334,0,"2019/09/09, 13:21:48",False,"2019/09/09, 13:21:48",3190,2987755,0,"<p>Found multiple language examples at <a href=""https://github.com/openzipkin?utf8=%E2%9C%93&amp;q=example"" rel=""nofollow noreferrer"">github</a>.<br>
If you need basic setup steps: <a href=""https://zipkin.io/"" rel=""nofollow noreferrer"">https://zipkin.io/</a><br>
Integrated zipkin with spring boot 2 and mysql 
<a href=""https://github.com/dineshbhagat/mac-configurations/blob/master/apm/zipkin.md"" rel=""nofollow noreferrer"">Steps</a><br>
<a href=""https://github.com/dineshbhagat/Spring-boot-JPA-Two-Level-Cache"" rel=""nofollow noreferrer"">example</a><br>
Here is sample 
<a href=""https://i.stack.imgur.com/yq17A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yq17A.png"" alt=""rest+mysql+hibernate""></a></p>
"
Zipkin,57331090,57319678,1,"2019/08/02, 20:28:15",False,"2019/08/02, 20:28:15",103,4522858,2,"<p>It the application.properties file for each eureka client ,  I added/changed</p>

<p>------------------ client</p>

<pre><code>......

ci2.srvhost = my hostname

#to find this list,run ipconfig in command prompt
spring.cloud.inetutils.preferredNetworks=146.122,10.0

eureka.client.serviceUrl.defaultZone= http://${ci2.srvhost}:8761/eureka/
eureka.instance.hostname=${ci2.srvhost}
spring.cloud.client.hostname=${ci2.srvhost}
</code></pre>

<p>-------------------- eureka server application.property--------------------</p>

<pre><code># host to set of ci2 services
ci2.srvhost = ${COMPUTERNAME}


# on windows 10 boxes, running docker, we have to include preferred networks, 
# this is not needed on linux, or windows boxes not running docker
spring.cloud.inetutils.preferredNetworks=146.122,10.0
</code></pre>

<h1>-------------------- standard config, should not need to change below -------------------</h1>

<pre><code>server.port=8761


eureka.client.register-with-eureka=false
eureka.client.fetch-registry=false

eureka.instance.hostname=${ci2.srvhost}
eureka.instance.prefer-ip-address=true

logging.level.com.netflix.eureka=OFF
logging.level.com.netflix.discovery=OFF

serviceUrl.defaultZone= http://${eureka.instance.hostname}:${server.port}/eureka/
</code></pre>
"
Zipkin,59930544,57319678,0,"2020/01/27, 13:47:06",False,"2020/09/28, 22:28:46",121,5897631,12,"<p>I was facing a similar issue where the eureka server was registering the services at host.docker.internal instead of localhost.</p>
<p>The issue in my case was an altered host file at location C:\Windows\System32\Drivers\etc\hosts. I deleted all the lines in the host file and saved it using npp with admin privilege. Restart the server post this change.</p>
<p>Looks like 'Docker Desktop' was changing the hostfile.</p>
"
Zipkin,61323578,57319678,1,"2020/04/20, 16:24:19",False,"2020/04/20, 16:24:19",21,2579322,1,"<p>""message"": ""Connection refused: no further information: host.docker.internal in eureka gateway error</p>

<p>Resolution:</p>

<p>check ping host.docker.internal
response is some ip addresses apart form local host i,e 127.0.0.1
remove the C:\Windows\System32\Drivers\etc\hosts.file entries , make it empty</p>

<p>then restart eureka and your microservice instance. </p>

<p>also will find the message like below in the log this ensures you are registered in eureka</p>

<p>DiscoveryClient_BEER-SERVICE/DESKTOP-G2AIGG1:beer-service:
splitting the above log message which denotes discovery client 
BEER-SERVICE is my service and 
DESKTOP-G2AIGG1 is my pc name
beer-service is the service registered.</p>
"
Zipkin,63259763,57319678,1,"2020/08/05, 09:43:08",False,"2020/08/05, 09:43:08",3449,387417,0,"<p>I was also facing the same issue, when I was loadbalancing my restTemplate. Something like this</p>
<pre><code>@Bean
@LoadBalanced
RestTemplate restTemplate() {
return new RestTemplate();}
</code></pre>
<p>This is because of the ribbon client.
So, without making any changes in the host file, when i deleted this code and made use of <code>RestTemplateBuilder</code> to get restTemplate, everything was working fine. Code Example:</p>
<pre><code>@Autowired
private RestTemplateBuilder restTemplateBuilder;

RestTemplate restTemplate = restTemplateBuilder.build();
</code></pre>
<p>You can try this approach as well.</p>
"
Zipkin,63283687,57319678,0,"2020/08/06, 15:30:53",True,"2020/08/06, 15:30:53",103,4522858,1,"<p>Thanks for the tip on the host file on windows</p>
<p>I found that docker adds aliases  in the host file for host.docker.internal and gateway.docker.internal.</p>
<p>I am guessing that Eureka does a host lookup from the IP and host.docker.internal is returned.</p>
<p>I am not an expert at hosts files, but I added an alias for my actual host name to my IP (note: we use static ip's).   After doing this, docker did not change my host file on reboot and the reverse lookup of the ip-&gt;host now returns my machine name instead of host.docker.internal</p>
<pre><code>------------------ modified hosts file for windows

#I added an alias from my machine name to the IP before the docker added aliases
#
146.122.145.71 mymachine.net.foo.com
# Added by Docker Desktop
146.122.145.71 host.docker.internal
146.122.145.71 gateway.docker.internal
</code></pre>
"
Zipkin,66593805,57319678,0,"2021/03/12, 05:34:31",False,"2021/03/12, 05:34:31",11,13366251,1,"<p>Solution for Window 10:
You don't have to remove all the lines from hosts files.
Just comment this if exists (#192.168.1.4 host.docker.internal) (as we use this when playing with docker)
And paste this (127.0.0.1   host.docker.internal)
It worked for me.</p>
"
Zipkin,42982623,42982050,4,"2017/03/23, 19:08:47",True,"2017/03/23, 19:08:47",8336,1773866,1,"<p>You can use the new Dalston feature of using annotations on Spring Data repositories. You can check out this for more info <a href=""http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_managing_spans_with_annotations"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_managing_spans_with_annotations</a></p>
"
Zipkin,39174580,39174579,2,"2016/08/27, 00:02:27",True,"2016/08/27, 00:02:27",473,2733462,8,"<p>The official documentation was helpful, but I think it didn't include all the dependencies explicitly (at least as of now). I had to do some extra research for samples to get all the required dependencies and configuration together. I wanted to share it, because I believe it could be helpful for someone else.</p>

<p><strong>Spring Boot version:</strong> <code>1.4.0.RELEASE</code></p>

<p><strong>Spring Cloud version:</strong> <code>Brixton.SR4</code></p>

<p><strong>POM:</strong></p>

<pre><code>    ...
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-storage-mysql&lt;/artifactId&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;mysql&lt;/groupId&gt;
        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;
    &lt;/dependency&gt;
   ...
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;Brixton.SR4&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;
</code></pre>

<p><strong>Java:</strong></p>

<pre><code>import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import zipkin.server.EnableZipkinServer;

@SpringBootApplication
@EnableZipkinServer
public class ZipkinServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZipkinServerApplication.class, args);
    }
}
</code></pre>

<p><strong>application.yml:</strong></p>

<pre><code>spring:
  datasource:
    schema: classpath:/mysql.sql
    url: jdbc:mysql://localhost:3306/zipkin?autoReconnect=true
    username: root
    password: admin
    driver-class-name: com.mysql.jdbc.Driver
    initialize: true
    continue-on-error: true
  sleuth:
    enabled: false
zipkin:
  storage:
    type: mysql
</code></pre>

<p><strong>References:</strong></p>

<p><a href=""https://cloud.spring.io/spring-cloud-sleuth/"" rel=""noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/</a></p>
"
Zipkin,49724310,45046528,0,"2018/04/09, 04:06:02",False,"2018/04/09, 04:06:02",765,6466540,0,"<p>Lately I have been trying the same and couldn't find that option in initializer. I am just posting this if anyone encounters the same issues and lands on this page. You can refer below sample GitHub project which is consists of four micro services ( zipkin server, client, rest service, and Eureka ) using Edgware release with latest version of sleuth.</p>

<p><a href=""https://github.com/iuprade/slueth-zipkin-server-client"" rel=""nofollow noreferrer""> Sample Zipkin Server/Client </a></p>
"
Zipkin,50127150,45046528,0,"2018/05/02, 07:24:17",False,"2018/05/02, 07:24:17",1767,2498986,3,"<p>Zipkin Server is not part of Spring initializers. You have to use the official release of the Zipkin server </p>

<p><a href=""https://github.com/openzipkin/zipkin#quick-start"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin#quick-start</a></p>

<p>And custom servers are not supported anymore meaning you can't use <code>@EnableZipkinServer</code> anymore since 2.7</p>

<p><a href=""https://github.com/openzipkin/zipkin#quick-start"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin#quick-start</a></p>
"
Zipkin,42403962,42402849,1,"2017/02/23, 01:04:02",False,"2017/02/23, 01:04:02",84586,1384297,3,"<p>I can't explain the error that you got with Dalston.BUILD-SNAPSHOT, but the error with Camden.SR4 is because it's not compatible with Spring Boot 1.5. I'd recommend upgrading to Camden.SR5 <a href=""https://spring.io/blog/2017/02/06/spring-cloud-camden-sr5-is-available"" rel=""nofollow noreferrer"">which is compatible with Spring Boot 1.5</a>.</p>
"
Zipkin,47165778,42402849,0,"2017/11/07, 21:14:23",False,"2017/11/07, 21:14:23",349,961275,1,"<p>Even I got this error while setting up my project. I was using Spring boot 1.5.8 with the Brixton.SR6 release. However, when I consulted the site <a href=""http://projects.spring.io/spring-cloud/"" rel=""nofollow noreferrer"">http://projects.spring.io/spring-cloud/</a> I got to know the issue and I updated my dependency to Dalston.SR4 and then the application started working. </p>
"
Zipkin,49212158,49211771,2,"2018/03/10, 19:47:36",True,"2018/03/10, 19:53:21",6010,5210117,5,"<p>This is really strange because you are using latest relase and in the GitHub spring-cloud-sleuth depends to <code>&lt;brave.version&gt;4.17.2&lt;/brave.version&gt;</code>. And I think 4.16.3-SNAPSHOT version is not exists in the maven repo. (just checked 2.0.0.M8 depends to this version)</p>

<p>If you change to <code>&lt;sleuth.version&gt;2.0.0.M7&lt;/sleuth.version&gt;</code> it does find the required dependencies. </p>

<p><a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/pom.xml"" rel=""noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/pom.xml</a></p>
"
Zipkin,49212538,49211771,1,"2018/03/10, 20:24:25",False,"2018/03/10, 22:18:03",8336,1773866,2,"<p>The M8 for sleuth was broken. That issue will be fixed in M9.</p>

<p>You can use M8 but you have to explicitly change the brave version to some release one.</p>
"
Zipkin,43232907,43230619,1,"2017/04/05, 16:39:23",True,"2017/04/05, 16:39:23",399,6921715,13,"<p>For 1, 2, 3 it was because I was doing a new RestTemplate.</p>

<p>The doc says :</p>

<blockquote>
  <p>You have to register RestTemplate as a bean so that the interceptors will get injected. If you create a RestTemplate instance with a new keyword then the instrumentation WILL NOT work.</p>
</blockquote>

<p>So RTFM for myself, and this solved my 3 first problems :</p>

<pre><code>@Bean
public RestTemplate template() {
    return new RestTemplate();
}
@Autowired
private RestTemplate template;
</code></pre>
"
Zipkin,63987464,60023762,0,"2020/09/21, 09:54:36",False,"2021/03/27, 13:21:49",1,13026705,0,"<p>Your application starts in Tomcat Server but Zipkin use another server but i dont know that name , i include this code to spring boot starter web dependecy for ignore tomcat server</p>
<pre><code>&lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
&lt;/dependency&gt;
</code></pre>
<p>This worked for me try it</p>
"
Zipkin,16981952,16981751,0,"2013/06/07, 13:29:08",True,"2018/10/17, 13:15:26",18596,69875,1,"<p>It's not suitable, Zipkin is about tracing in distributed systems. I would say you would want something like a profiler, such as <a href=""http://visualvm.java.net/"" rel=""nofollow noreferrer"">Visual VM</a>,  - free and included with the JDK, or <a href=""http://www.yourkit.com/"" rel=""nofollow noreferrer"">YourKit</a>. Other profilers are available.</p>
"
Zipkin,18345991,16981751,0,"2013/08/21, 01:31:07",False,"2013/08/21, 01:31:07",60071,298389,1,"<p>No, it <strong>is not suitable at all</strong>. Why? Because the architecture of Zipkin have multiple levels of indirection: you have to send your spans into collector service and even if collector is running on your own machine there is a huge overhead -- imagine you are traveling from two neighbour cities somewhere in Europe and somebody proposes you instead of going directly from A to B, fly from A to New York, USA and back to B. That is simply ridiculous.</p>

<p>As for the tools that may suit your needs: VisualVM and Yourkit mentioned by @Jonathan are good for looking at average situation in your program -- if you need to carefully inspect low level paths in your program <a href=""http://mchr3k.github.io/org.intrace/"" rel=""nofollow"">InTrace</a> might be a better choice. </p>
"
Zipkin,31348098,16981751,1,"2015/07/10, 21:43:33",False,"2015/07/10, 21:43:33",17932,433344,1,"<p>Zipkin is foremost intended to provide observability into a complex distributed network of services (aka Microservice Architecture).  It wasn't intended to support just a single application.  </p>

<p>So a profiler can often be a better way to go, particularly if you have an urgent issue to diagnose.</p>

<p>That said, most profilers will only provide realtime data, where Zipkin persists, allowing for analysis over large datasets.</p>

<p>If Zipkin is already in your stack, it's not a bad idea to enrich higher level Zipkin spans (ie a complete REST requests) with specific method calls as child-spans, particularly those that do I/O.  This way you can drill down within Zipkin to more quickly determine bottlenecks.  </p>

<p>Otherwise you'd have to first look at the Zipkin span for high-level bottlenecks, then separately look at other logs or run a profiler, which is inefficient.  </p>

<p>** If it's already in your stack ** </p>
"
Zipkin,59544246,59444729,0,"2019/12/31, 14:36:17",True,"2019/12/31, 19:46:58",3932,1371995,1,"<p>I have tested this with the official <a href=""https://github.com/census-instrumentation/opencensus-node/tree/master/examples/express"" rel=""nofollow noreferrer"">opencensus-node</a> example at github.</p>

<h1>Problem 1:</h1>

<blockquote>
  <p>Zipkin UI displays 2 spans with same name MyApplication(see image),
  but expected 2 different span names</p>
</blockquote>

<p>Just to be clear, <code>MyApplication</code> is the service name you set in your app.js, and the span names are those which you selected on the image <code>/service1</code>, <code>/service1</code>, <code>/external_service_2</code>.</p>

<p>I think this is the intended behavior, you got one service (<code>MyApplication</code>), a root span (<code>/service1</code>) and a child span (<code>/external_service_2</code>).
If you got multiple services connected to the same Zipkin server then you'll have multiple names for the services.</p>

<p><a href=""https://i.stack.imgur.com/g3PnG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g3PnG.png"" alt=""OpenCensus example""></a></p>

<p><strong>From the Zipkin's <a href=""https://zipkin.io/pages/instrumenting.html"" rel=""nofollow noreferrer"">documentation</a>:</strong></p>

<p><strong>Span</strong></p>

<p>A set of Annotations and BinaryAnnotations that correspond to a particular RPC. Spans contain identifying information such as traceId, spanId, parentId, and RPC name.</p>

<p><strong>Trace</strong></p>

<p>A set of spans that share a single root span. Traces are built by collecting all Spans that share a traceId. The spans are then arranged in a tree based on spanId and parentId thus providing an overview of the path a request takes through the system.</p>

<h1>Problem 2:</h1>

<blockquote>
  <p>As far Zipkin UI displays 2 spans with same name, service dependencies
  page contains one Service only(see image)</p>
</blockquote>

<p>Again, this is the intended behavior, since you got only one service and the external request you made goes through it.</p>

<h1>Change span names:</h1>

<p>If you mean the framed names on the first image, at the top it shows only the root span you clicked on the previous screen.
However, you can write custom span names after a little change in your code.</p>

<p>From the <a href=""https://opencensus.io/quickstart/nodejs/tracing"" rel=""nofollow noreferrer"">tracing documentation</a> (with your code):</p>

<pre><code>const options = {
  url: `${ZIPKIN_ENDPOINT}/api/v2/spans`,
  serviceName: 'MyApplication'
}
const tracer = tracing.start({samplingRate: 1}).tracer;
tracer.registerSpanEventListener(new zipkin.ZipkinTraceExporter(options));
</code></pre>

<p>Now you can use <code>tracer.startRootSpan</code>, I used it in the express sample with a request:</p>

<pre><code>tracer.startRootSpan({name: 'main'}, rootSpan =&gt; {
  rp('http://localhost:3000/sample').then(data =&gt; {
    res.send(data);
    rootSpan.end();
  }, err =&gt; {
    console.error(`${err.message}`);
    rootSpan.end();
  });
});
</code></pre>

<p>A span must be closed.</p>

<p>For more information, check the <a href=""https://github.com/census-instrumentation/opencensus-node/blob/master/packages/opencensus-core/test/test-tracer.ts"" rel=""nofollow noreferrer"">test file</a> of the tracer.</p>
"
Zipkin,38537987,35215821,0,"2016/07/23, 06:56:00",False,"2016/07/23, 06:56:00",762,1227937,0,"<p>The best way to ask for a feature is using github issues.</p>

<p>To add a new transport such as RabbitMQ, you'd have to affect Brave (reporter) and Zipkin (collector)</p>

<p><a href=""https://github.com/openzipkin/zipkin/issues"" rel=""nofollow"">https://github.com/openzipkin/zipkin/issues</a>
<a href=""https://github.com/openzipkin/brave/issues"" rel=""nofollow"">https://github.com/openzipkin/brave/issues</a></p>
"
Zipkin,30204797,23032944,4,"2015/05/13, 05:49:26",False,"2015/05/13, 05:49:26",15576,677561,1,"<p>The first step to good searching in elasticsearch is to create fields from your data.  With logs, logstash is the proper tool.  The grok{} filter uses patterns (existing or user-defined regexps) to split the input into fields.</p>

<p>You would need to make sure that it was mapped to an integer (e.g. %{INT:duration:int} in your pattern).  You could then query elasticsearch for ""duration:>1000"" to get the results.</p>

<p>Elasticsearch uses the lucene query engine, so you can find sample queries based on that.</p>
"
Zipkin,19034042,19032203,5,"2013/09/26, 19:49:25",False,"2013/09/26, 20:56:50",15073,14032,8,"<p>Zipkin is the best solution.</p>

<p><em>--zipkin developer</em></p>

<p><strong>EDIT</strong> - Ok ok, here's a serious answer:</p>

<p>Zipkin is a distributed tracing system developed by Twitter because our service-oriented-architecture is so goddamned big that it's often hard to understand WTF is happening in any given request.  Seriously, here's a visualization in Zipkin of all the services dependencies at twitter:</p>

<p><img src=""https://i.stack.imgur.com/V0pUG.png"" alt=""Holy shit that&#39;s a lot of services""></p>

<p>Is your platform this intense?  You should use zipkin.  Did I mention it's one of the best scaling systems I've ever seen?  It has zero problem keeping up with twitter-level load, and that might be important to you if you're that big.</p>

<p>What's that you say?  You're not as big as twitter?  You only have three services: a web frontend, some kind of middleware, and your database backend?  Maybe zipkin is a bit overkill for you.  We've done some work to make it a bit easier to setup, but really my job isn't to make zipkin easy for you, it's to make zipkin awesome for Twitter.</p>

<p>Still, if you plan on scaling scala, the twitter stack with Finagle etc is insanely good. Don't let all the evangelists from Typesafe fool you.  Their stack has some serious deficiencies when you try to deploy it in massive-scale architectures.  But again, our job isn't to tell you how good our stack is, or even help you use it.  It's to make our stack awesome.</p>
"
Zipkin,64818358,63407052,0,"2020/11/13, 11:22:51",False,"2020/11/13, 11:22:51",51,8394088,0,"<p>This link (<a href=""https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/"" rel=""nofollow noreferrer"">https://objectpartners.com/2019/04/25/distributed-tracing-with-apache-kafka-and-jaeger/</a>) provides the details of how to enable jaeger traces.</p>
<p>The simplest way to enable jaeger to spring-boot application is add the dependency and the required properties.</p>
<hr />
<p><strong>Dependency:</strong></p>
<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;
   &lt;artifactId&gt;opentracing-spring-jaeger-web-starter&lt;/artifactId&gt;
   &lt;version&gt;3.2.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p><strong>Example Properties</strong></p>
<pre><code>opentracing.jaeger.udp-sender.host=localhost
opentracing.jaeger.udp-sender.port=6831
opentracing.jaeger.const-sampler.decision=true
opentracing.jaeger.enabled=true
opentracing.jaeger.log-spans=true
opentracing.jaeger.service-name=ms-name
</code></pre>
"
Zipkin,65029618,63407052,0,"2020/11/26, 23:49:03",True,"2020/11/26, 23:49:03",36,4216591,1,"<p>Answering to your question about dependencies it is explained here in Dependencies section (<a href=""https://github.com/opentracing-contrib/java-spring-jaeger"" rel=""nofollow noreferrer"">https://github.com/opentracing-contrib/java-spring-jaeger</a>):</p>
<blockquote>
<p>The opentracing-spring-jaeger-web-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-web-starter This means that by including it, simple web Spring Boot microservices include all the necessary dependencies to instrument Web requests / responses and send traces to Jaeger.</p>
<p>The opentracing-spring-jaeger-cloud-starter starter is convenience starter that includes both opentracing-spring-jaeger-starter and opentracing-spring-cloud-starter This means that by including it, all parts of the Spring Cloud stack supported by Opentracing will be instrumented</p>
</blockquote>
<p>And by the way:</p>
<ul>
<li>opentracing.jaeger.log-spans is true by default</li>
</ul>
<p>same as:</p>
<ul>
<li>opentracing.jaeger.udp-sender.host=localhost</li>
<li>opentracing.jaeger.udp-sender.port=6831</li>
<li>opentracing.jaeger.const-sampler.decision=true</li>
<li>opentracing.jaeger.enabled=true</li>
</ul>
"
Zipkin,54816556,54262815,1,"2019/02/21, 23:32:10",True,"2019/02/21, 23:32:10",106,3956619,3,"<p>It's hard to tell without more information. But it can be related to <strong>incompatible libraries</strong>. Can you post your dependencies?</p>

<p>In case you are using <strong>older version</strong> of okhttpclient with <strong>latest</strong> spring cloud:greenwich it can cause this issue.</p>

<p>I'm using <strong>Greenwich.RELEASE</strong> with <strong>okhttpclient:10.2.0</strong> which works without problems</p>
"
Zipkin,59579899,54262815,0,"2020/01/03, 16:02:23",False,"2020/01/03, 16:02:23",11,12647380,1,"<p>Use the below dependency Management for spring-boot to download the suitable versions for cloud version</p>

<pre><code>    &lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;
</code></pre>

<p>I am using Java 10, cloud.version is <strong>Finchley.SR2</strong> and sprinb-boot:2.2.0 and spring-cloud-starter-openfeign :2.1.2.RELEASE. and this combination worked for me to fix the issue.</p>

<p>Acctual problem was 10.x.x feign-core  was not working only  and io.github.openfeign:feign-core:jar:9.7.0:compile was working.</p>
"
Zipkin,62180529,54262815,0,"2020/06/03, 21:51:55",False,"2020/06/03, 21:51:55",116,10783374,1,"<p>I faced this problem using java 11, springboot 2.3.0.RELEASE, and spring-cloud version Greenwich.RELEASE. Adding the following dependences saved me:</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt;
        &lt;artifactId&gt;feign-okhttp&lt;/artifactId&gt;
        &lt;version&gt;10.2.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt;
        &lt;artifactId&gt;feign-core&lt;/artifactId&gt;
        &lt;version&gt;10.2.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Hope this helps someone.</p>
"
Zipkin,41628945,41508180,0,"2017/01/13, 08:52:38",True,"2017/01/13, 08:52:38",762,1227937,5,"<p>In zipkin lingo, what you are asking about is often called ""local spans"" or ""local tracing"", basically an operation that neither originated, nor resulted in a remote call.</p>

<p>I'm not aware of anything at the syscall level, but many tracers support explicit instrumentation of function calls.</p>

<p>For example, using <a href=""https://github.com/Yelp/py_zipkin#usage-3-log-a-span-inside-an-ongoing-trace"" rel=""noreferrer"">py_zipkin</a>
<code>
@zipkin_span(service_name='my_service', span_name='some_function')
def some_function(a, b):
    return do_stuff(a, b)
</code></p>

<p>Besides explicit instrumentation like this, one could also export data to zipkin. For example, one could convert trace data that is made in another tool to <a href=""http://zipkin.io/zipkin-api/#/paths/%252Fspans/post"" rel=""noreferrer"">zipkin's json format</a>.</p>

<p>This probably doesn't answer your question, but I hope it helps.</p>
"
Zipkin,63486777,63485673,2,"2020/08/19, 15:18:24",True,"2020/08/19, 15:18:24",91,1548178,4,"<p>The demo you tried is using older configuration and opencensus which should be replaced with otlp receiver. Having said that this is a working example
<a href=""https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node/docker"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node/docker</a>
So I'm copying the files from there:</p>
<p>docker-compose.yaml</p>
<pre><code>version: &quot;3&quot;
services:
  # Collector
  collector:
    image: otel/opentelemetry-collector:latest
    command: [&quot;--config=/conf/collector-config.yaml&quot;, &quot;--log-level=DEBUG&quot;]
    volumes:
      - ./collector-config.yaml:/conf/collector-config.yaml
    ports:
      - &quot;9464:9464&quot;
      - &quot;55680:55680&quot;
      - &quot;55681:55681&quot;
    depends_on:
      - zipkin-all-in-one

  # Zipkin
  zipkin-all-in-one:
    image: openzipkin/zipkin:latest
    ports:
      - &quot;9411:9411&quot;

  # Prometheus
  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - &quot;9090:9090&quot;
</code></pre>
<p>collector-config.yaml</p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
      http:

exporters:
  zipkin:
    endpoint: &quot;http://zipkin-all-in-one:9411/api/v2/spans&quot;
  prometheus:
    endpoint: &quot;0.0.0.0:9464&quot;

processors:
  batch:
  queued_retry:

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [zipkin]
      processors: [batch, queued_retry]
    metrics:
      receivers: [otlp]
      exporters: [prometheus]
      processors: [batch, queued_retry]
</code></pre>
<p>prometheus.yaml</p>
<pre><code>global:
  scrape_interval: 15s # Default is every 1 minute.

scrape_configs:
  - job_name: 'collector'
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
      - targets: ['collector:9464']
</code></pre>
<p>This should work fine with opentelemetry-js ver. 0.10.2</p>
<p>Default port for traces is 55680 and for metrics 55681</p>
<p>The link I posted previously - you will always find there the latest up to date working example:
<a href=""https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/collector-exporter-node</a>
And for web example you can use the same docker and see all working examples here:
<a href=""https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/tracer-web/"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-js/tree/master/examples/tracer-web/</a></p>
"
Zipkin,63489195,63485673,2,"2020/08/19, 17:27:00",False,"2020/08/19, 18:54:58",30911,2000548,3,"<p>Thank you sooo much for @BObecny's help! This is a complement of @BObecny's answer.</p>
<p>Since I am more interested in integrating with Jaeger. So here is the config to set up with all Jaeger, Zipkin, Prometheus. And now it works on both front end and back end.</p>
<p>First both front end and back end use same exporter code:</p>
<pre><code>import { CollectorTraceExporter } from '@opentelemetry/exporter-collector';

new SimpleSpanProcessor(
  new CollectorTraceExporter({
    serviceName: 'my-service',
  })
)
</code></pre>
<p><strong>docker-compose.yaml</strong></p>
<pre><code>version: &quot;3&quot;
services:
  # Collector
  collector:
    image: otel/opentelemetry-collector:latest
    command: [&quot;--config=/conf/collector-config.yaml&quot;, &quot;--log-level=DEBUG&quot;]
    volumes:
      - ./collector-config.yaml:/conf/collector-config.yaml
    ports:
      - &quot;9464:9464&quot;
      - &quot;55680:55680&quot;
      - &quot;55681:55681&quot;
    depends_on:
      - jaeger-all-in-one
      - zipkin-all-in-one

  # Jaeger
  jaeger-all-in-one:
    image: jaegertracing/all-in-one:latest
    ports:
      - &quot;16686:16686&quot;
      - &quot;14268&quot;
      - &quot;14250&quot;

  # Zipkin
  zipkin-all-in-one:
    image: openzipkin/zipkin:latest
    ports:
      - &quot;9411:9411&quot;

  # Prometheus
  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - &quot;9090:9090&quot;
</code></pre>
<p><strong>collector-config.yaml</strong></p>
<pre><code>receivers:
  otlp:
    protocols:
      grpc:
      http:

exporters:
  jaeger:
    endpoint: jaeger-all-in-one:14250
    insecure: true
  zipkin:
    endpoint: &quot;http://zipkin-all-in-one:9411/api/v2/spans&quot;
  prometheus:
    endpoint: &quot;0.0.0.0:9464&quot;

processors:
  batch:
  queued_retry:

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [zipkin]
      processors: [batch, queued_retry]
    metrics:
      receivers: [otlp]
      exporters: [prometheus]
      processors: [batch, queued_retry]
</code></pre>
<p><strong>prometheus.yaml</strong></p>
<pre><code>global:
  scrape_interval: 15s # Default is every 1 minute.

scrape_configs:
  - job_name: 'collector'
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
      - targets: ['collector:9464']
</code></pre>
"
Zipkin,49450864,43025795,0,"2018/03/23, 15:33:24",False,"2018/03/23, 15:33:24",485,1287376,1,"<p>It seems to be a timing issue.</p>

<p>If we add some delay, for instance, between children spans execution like</p>

<pre><code>Thread.sleep(1);
</code></pre>

<p>In between</p>

<pre><code>Span prepare = tracer.newChild(twoPhase.context()).name(""prepare"").start();
try {                                                                
    System.out.print(""prepare"");                                     
} finally {                                                          
    prepare.finish();                                                
}                                                                    
Thread.sleep(1); // &lt;&lt;&lt;                                                                                                                                                         
Span commit = tracer.newChild(twoPhase.context()).name(""commit"").start();
try {                                                                
    System.out.print(""commit"");                                      
} finally {                                                          
    commit.finish();                                                 
}
</code></pre>

<p>Then we get to see spans:</p>

<p><a href=""https://i.stack.imgur.com/8sCjA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8sCjA.png"" alt=""spans""></a></p>

<p>I've faced something like this before when Zipkin dropped spans I was (mistakenly) assigning wrong timestamps to.</p>

<p><em>For reference and ease of reproduction: I've setup a <a href=""https://github.com/embs/brave-example"" rel=""nofollow noreferrer"">project</a> for reproducing this issue / ""fix"".</em></p>
"
Zipkin,56529683,56525260,0,"2019/06/10, 18:54:49",False,"2019/06/10, 18:54:49",525,4504053,10,"<p>You can add the following setting on your properties key to disable zipkin, <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin2/ZipkinProperties.java#L48"" rel=""noreferrer"">source</a>.</p>

<pre><code>spring.zipkin.enabled=false
</code></pre>

<p>Better yet, create separate development properties (like <code>application-dev.properties</code>) to avoid changing above setting everytime you want to run in your machine: <a href=""https://stackoverflow.com/a/34846351/4504053"">https://stackoverflow.com/a/34846351/4504053</a></p>
"
Zipkin,47010922,47008485,4,"2017/10/30, 10:38:04",False,"2017/10/30, 10:38:04",8336,1773866,2,"<p>Most likely your code is broken. You can check out the <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/ZipkinAutoConfiguration.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/ZipkinAutoConfiguration.java</a> class where for Edgware we've added load balanced zipkin server resolution.</p>
"
Zipkin,47139081,47008485,1,"2017/11/06, 16:23:51",True,"2017/11/06, 16:23:51",1260,4864870,0,"<p>Is it correct that you are using the code example from the baeldung tutorial? (<a href=""http://www.baeldung.com/tracing-services-with-zipkin"" rel=""nofollow noreferrer"">http://www.baeldung.com/tracing-services-with-zipkin</a> - 3.2. Spring Config)</p>

<p>I think there is a mistake with line 34 and 35 (the closing curly brace).</p>

<p>I've fixed the problem by modifing the method like this:</p>

<pre><code>@Bean
public ZipkinSpanReporter makeZipkinSpanReporter() {
    return new ZipkinSpanReporter() {
        private HttpZipkinSpanReporter delegate;
        private String baseUrl;

        @Override
        public void report(Span span) {

            InstanceInfo instance = eurekaClient
                    .getNextServerFromEureka(""zipkin"", false);
            if (!(baseUrl != null &amp;&amp;
                    instance.getHomePageUrl().equals(baseUrl))) {
                baseUrl = instance.getHomePageUrl();
                RestTemplate restTemplate = new RestTemplate();
                zipkinRestTemplateCustomizer.customize(restTemplate);
                delegate = new HttpZipkinSpanReporter(
                        restTemplate,
                        baseUrl,
                        zipkinProperties.getFlushInterval(),
                        spanMetricReporter);
            }

            if (!span.name.matches(skipPattern)) {
                delegate.report(span);
            }
        }
    };
}
</code></pre>

<p>Maybe this helps someone or maybe someone from baeldung reads this and could verify and correct the code example. ;)</p>
"
Zipkin,44664880,44643259,3,"2017/06/21, 03:09:57",True,"2017/06/23, 08:31:36",nan,nan,4,"<p>Problem solved. <code>tracer.withSpanInScope(clientSpan)</code> would do the work.</p>

<p>Note that, <code>withSpanInScope(...)</code> has not been called before sending messages .</p>
"
Zipkin,40358966,39870535,0,"2016/11/01, 13:16:38",False,"2016/11/01, 13:16:38",762,1227937,1,"<p>Some people use zipkin to identify dead services, but probably metrics/stats would be the better route if you are trying to break down and report by thrift method.</p>
"
Zipkin,19022596,19022191,0,"2013/09/26, 11:03:01",False,"2013/09/26, 11:03:01",4556,1097599,5,"<p>Well before starting digging into JVM stuff or setting up all the infrastructure needed by Zipkin you could simply start by measuring some application-level metrics.</p>

<p>You could try the library <a href=""https://github.com/codahale/metrics"">metrics</a> via this <a href=""https://github.com/erikvanoosten/metrics-scala"">scala api</a>.</p>

<p>Basically you manually set up counters and gauges at specific points of your application that will help you diagnose your bottleneck problem.</p>
"
Zipkin,52814922,52759855,2,"2018/10/15, 13:43:25",True,"2018/10/17, 09:29:03",201,7956609,2,"<p>If service 2 is getting traceId from service 1, you can take the traceId from requestHeader in your java code. Otherwise sleuth generate a new traceId in service 2.</p>

<p>To get the trace Id In java </p>

<pre><code>    @Autowired
    private Tracer tracer;
</code></pre>

<p>Just do </p>

<pre><code>    tracer.getCurrentSpan().traceIdString();
</code></pre>
"
Zipkin,59216676,52759855,0,"2019/12/06, 18:13:45",False,"2019/12/06, 18:13:45",859,4513218,1,"<p>Hello you can also get the x-b3-traceid header information from the request, I created a Util class for that -> <a href=""https://gist.github.com/walterwhites/067dd635986e564aafdb5ac559073b0f"" rel=""nofollow noreferrer"">https://gist.github.com/walterwhites/067dd635986e564aafdb5ac559073b0f</a></p>

<pre><code>public final class DebugUtils {

    private static String PURPLE = ""\033[0;35m"";  // PURPLE
    private static String RED = ""\u001B[31m"";  // RED
    private static String RESET = ""\u001B[0m"";

    public static class ZipkinDebug {

        private static String url = ""http://localhost:9411/zipkin/traces/"";

        public static void displayTraceUrl(HttpServletRequest request) {
            String traceId = request.getHeader(""x-b3-traceid"");
            System.out.println(PURPLE + ""DebugUtils:ZipkinDebug -&gt; "" + url + traceId + RESET);
        }
    }

    public static class RequestInfo {

        public static void displayAllRequestHeaders(HttpServletRequest request) {
            Enumeration&lt;String&gt; headerNames = request.getHeaderNames();
            System.out.println(RED + ""DebugUtils:RequestInfo -&gt; "" + RESET);
            headerNames.asIterator().forEachRemaining(header -&gt; {
                System.out.println(""Header Name:"" + header + ""   "" + ""Header Value:"" + request.getHeader(header));
            });
        }

        public static void displayRequestHeader(HttpServletRequest request, String headerName) {
            System.out.println(RED + ""DebugUtils:RequestInfo -&gt; Header Name:"" + headerName + ""   "" + ""Header Value:"" + request.getHeader(headerName) + RESET);
        }
    }
}
</code></pre>

<p>Then in your main class you just need to call</p>

<pre><code>ZipkinDebug.displayTraceUrl(request);
</code></pre>
"
Zipkin,49182914,49182626,0,"2018/03/08, 23:38:09",False,"2018/03/08, 23:38:09",9818,1490322,6,"<p>Ok we found the problem and also a work around.</p>

<p>It looks like all the documentation out there is wrong, at least for the version of Spring Cloud Sleuth we are using.  The correct property is not <code>spring.sleuth.sampler.percentage</code>.  The correct property is <code>spring.sleuth.sampler.probability</code></p>

<p>And here is a workaround we found right before noticing that the property was wrong.</p>

<pre><code>@Bean
public Sampler alwaysSampler() {
    return Sampler.ALWAYS_SAMPLE;
}
</code></pre>

<p>Here are some official documentation from Spring Cloud that contain the wrong property.</p>

<p><a href=""https://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M5/single/spring-cloud-sleuth.html"" rel=""noreferrer"">https://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.M5/single/spring-cloud-sleuth.html</a></p>

<p><a href=""https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html"" rel=""noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/single/spring-cloud-sleuth.html</a></p>

<p>Here is the source code that is being used and it is using <code>probability</code> not <code>percentage</code>.</p>

<p><a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/sampler/SamplerProperties.java"" rel=""noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/sampler/SamplerProperties.java</a></p>
"
Zipkin,47195615,47195614,4,"2017/11/09, 08:50:24",True,"2017/11/09, 08:50:24",3909,3067542,3,"<p>I found that these traces are actually generated by <a href=""https://github.com/spring-cloud/spring-cloud-consul/blob/master/spring-cloud-consul-discovery/src/main/java/org/springframework/cloud/consul/discovery/ConsulCatalogWatch.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-consul/blob/master/spring-cloud-consul-discovery/src/main/java/org/springframework/cloud/consul/discovery/ConsulCatalogWatch.java</a> </p>

<p>And since this is a class annotated with @scheduled , this Sleuth aspect applies : </p>

<p><a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/TraceSchedulingAspect.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/TraceSchedulingAspect.java</a></p>

<p>And therefore, the property to control the skipped regexp is not spring.sleuth.instrument.web.skipPattern , but spring.sleuth.instrument.<strong>scheduled</strong>.skip-pattern</p>
"
Zipkin,52031368,52029459,0,"2018/08/27, 03:52:08",True,"2018/09/16, 13:54:14",391,6452043,3,"<p>after many searches, i found that there are a version conflicts between the dependencies.
and thanks for <em>vladimir-vagaytsev</em> </p>

<p>so, i see that <code>spring-cloud-starter-sleuth</code> imported as a different version.</p>

<p>to fix it i have added  <code>sleuth.version</code> to properties  in pom.xml like so.</p>

<pre><code>&lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Finchley.SR1&lt;/spring-cloud.version&gt;
        &lt;sleuth.version&gt;2.0.1.RELEASE&lt;/sleuth.version&gt;
    &lt;/properties&gt; 
</code></pre>

<p>then in dependency management we need to specify the version like so</p>

<pre><code>&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
            &lt;version&gt;${sleuth.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;
</code></pre>

<p>after then remove unused dependencies build and run.  </p>
"
Zipkin,52338780,52029459,0,"2018/09/14, 23:20:40",False,"2018/09/14, 23:20:40",373,1037492,-1,"<p>This class comes from zipkin-2. You can try adding this dependency.</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-sleuth-zipkin2&lt;/artifactId&gt;
    &lt;version&gt;2.0.0.M3&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
"
Zipkin,39597817,39597545,0,"2016/09/20, 18:15:25",True,"2016/09/20, 18:15:25",8336,1773866,2,"<p>We have a <code>LazyTraceExecutor</code> that you can use - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/async/LazyTraceExecutor.java"" rel=""nofollow"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/async/LazyTraceExecutor.java</a> . </p>
"
Zipkin,54325585,54325234,2,"2019/01/23, 12:54:00",True,"2019/01/23, 12:54:00",13313,524946,2,"<p>OpenTracing is the API that <em>your</em> code will interact with directly. </p>

<p>Basically, your application would be ""instrumented"" using the OpenTracing API and a concrete tracer (like Jaeger or Brave/Zipkin) would capture the data and send it somewhere. This allows your application to use a neutral API throughout your code so that you can change from one provider to another without having to change your entire code base.</p>

<p>Another way to think about it is that OpenTracing is like SLF4J in the Java logging world, whereas Jaeger and Zipkin are the concrete implementations, such as Log4j in the Java logging world.</p>
"
Zipkin,50877783,49676752,0,"2018/06/15, 17:39:40",False,"2018/06/15, 17:39:40",140,7980867,0,"<p>I believe you should be able to as long as you use the fully qualified domain name. For instance, <code>zipkin.mynamespace.svc.cluster.local</code>.</p>
"
Zipkin,64052707,64050154,0,"2020/09/24, 21:55:46",False,"2020/09/24, 22:01:08",2441,10020419,4,"<blockquote>
<p>As, you can see there are few missing components - There are few pods missing istio-citadel, istio-pilot, istio-policy, istio-sidecar, istio-telemetry, istio-tracing etc. These components were available in 1.4.2.</p>
</blockquote>
<p>These components where merged with version 1.5 into one service named <code>istiod</code>. See: <a href=""https://istio.io/latest/blog/2020/istiod/"" rel=""nofollow noreferrer"">https://istio.io/latest/blog/2020/istiod/</a></p>
<blockquote>
<p>In 1.4.2 installation I could see grafana, jaeger, kiali, prometheus, zipkin dashboards. But these are now missing.</p>
</blockquote>
<p>These AddonComponents must be installed manually and are not part of <code>istioctl</code> since version 1.7. See: <a href=""https://istio.io/latest/blog/2020/addon-rework/"" rel=""nofollow noreferrer"">https://istio.io/latest/blog/2020/addon-rework/</a></p>
<p>So your installation is not broken. It's just a lot has changed since 1.4. I would suggest to go through the release announcements to read about all changes: <a href=""https://istio.io/latest/news/releases/"" rel=""nofollow noreferrer"">https://istio.io/latest/news/releases/</a></p>
"
Zipkin,47777632,47764295,0,"2017/12/12, 18:47:54",True,"2017/12/12, 18:47:54",81,2232476,1,"<p>Creating custom zipkin servers is an unsupported configuration, but if you must all of the configuration options are documented in the project readme: <a href=""https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md</a></p>
"
Zipkin,50200855,47764295,0,"2018/05/06, 17:35:24",False,"2018/05/06, 17:35:24",69,2629308,1,"<p>For the dependencies part, the most important one is <strong>zipkin-autoconfigure-storage-elasticsearch-http</strong>, here's an full maven pom.xml example:</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
&lt;project
xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""
xmlns=""http://maven.apache.org/POM/4.0.0""
xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

&lt;groupId&gt;com.example&lt;/groupId&gt;
&lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
&lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;
&lt;name&gt;zipkin-server&lt;/name&gt;
&lt;url&gt;http://maven.apache.org&lt;/url&gt;

&lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;version&gt;1.5.2.RELEASE&lt;/version&gt;
    &lt;relativePath/&gt;
&lt;/parent&gt;

&lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;spring-cloud.version&gt;Dalston.SR1&lt;/spring-cloud.version&gt;
    &lt;zipkin.version&gt;1.23.2&lt;/zipkin.version&gt;
&lt;/properties&gt;

&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-collector-kafka10&lt;/artifactId&gt;
        &lt;version&gt;1.26.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-storage-elasticsearch-http&lt;/artifactId&gt;
        &lt;version&gt;${zipkin.version}&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p></p>

<p>For the configuration part, you will need the following in you <strong>application.yml</strong>:</p>

<pre><code>zipkin:
  storage:
    type: elasticsearch
    elasticsearch:
      hosts: localhost:9200
</code></pre>
"
Zipkin,64168840,47764295,0,"2020/10/02, 11:59:59",False,"2020/10/02, 11:59:59",511,2564032,0,"<p>I configured zipkin to use ES as a data storage on top of kubernetes. If it fits your requirement feel free to download and use <a href=""https://github.com/handysofts/zipkin-on-kubernetes"" rel=""nofollow noreferrer"">https://github.com/handysofts/zipkin-on-kubernetes</a></p>
"
Zipkin,64096381,64094479,0,"2020/09/28, 08:27:01",True,"2020/09/28, 08:27:01",8336,1773866,2,"<p>If you search for zipkin grafana you'll get this as one of the first results <a href=""https://grafana.com/docs/grafana/latest/features/datasources/zipkin/"" rel=""nofollow noreferrer"">https://grafana.com/docs/grafana/latest/features/datasources/zipkin/</a></p>
"
Zipkin,59524696,48794097,0,"2019/12/30, 03:11:09",False,"2019/12/30, 03:11:09",2172,3514300,0,"<p>Trace context propagation might be missing. </p>

<p><a href=""https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-context-propagation"" rel=""nofollow noreferrer"">https://istio.io/docs/tasks/observability/distributed-tracing/overview/#trace-context-propagation</a></p>

<pre><code>Although Istio proxies are able to automatically send spans, they need some hints to tie together the entire trace. Applications need to propagate the appropriate HTTP headers so that when the proxies send span information, the spans can be correlated correctly into a single trace.

To do this, an application needs to collect and propagate the following headers from the incoming request to any outgoing requests:

x-request-id
x-b3-traceid
x-b3-spanid
x-b3-parentspanid
x-b3-sampled
x-b3-flags
x-ot-span-context

Additionally, tracing integrations based on OpenCensus (e.g. Stackdriver) propagate the following headers:

x-cloud-trace-context
traceparent
grpc-trace-bin
</code></pre>
"
Zipkin,40861860,40860284,0,"2016/11/29, 11:14:50",True,"2016/11/29, 11:14:50",42504,936832,2,"<p>As long as you have the ability to specify VM parameters, you can add the monitoring agent, regardless of the whether the JVM is started as a Windows service. For perfino, that VM parameter is</p>

<pre><code>-javaagent:[path to perfino.jar]
</code></pre>
"
Zipkin,61609752,61570149,1,"2020/05/05, 12:19:05",False,"2020/05/05, 12:19:05",2015,1398228,0,"<p>You need to reload after adding the subsystem:</p>

<pre><code>[standalone@localhost:9990 /] /extension=org.wildfly.extension.microprofile.opentracing-smallrye:add
{""outcome"" =&gt; ""success""}

[standalone@localhost:9990 /] /subsystem=microprofile-opentracing-smallrye:add
{
    ""outcome"" =&gt; ""success"",
    ""response-headers"" =&gt; {
        ""operation-requires-reload"" =&gt; true,
        ""process-state"" =&gt; ""reload-required""
    }
}

[standalone@localhost:9990 /] reload
[standalone@localhost:9990 /] /subsystem=microprofile-opentracing-smallrye/jaeger-tracer=my-tracer:add()
{""outcome"" =&gt; ""success""}
</code></pre>
"
Zipkin,61636753,61570149,6,"2020/05/06, 16:43:39",False,"2020/05/06, 16:43:39",2015,1398228,1,"<pre><code>embed-server --admin-only=true
/extension=org.wildfly.extension.microprofile.opentracing-smallrye:add()
/subsystem=microprofile-opentracing-smallrye:add()
/subsystem=microprofile-opentracing-smallrye/jaeger-tracer=my-tracer:add()
stop-embedded-server
</code></pre>

<p>This jboss-cli script should enable opentracing before starting the server properly. I'm not sure how/when you can execute that with keycloack image</p>
"
Zipkin,51011690,50027127,0,"2018/06/24, 19:10:20",False,"2018/06/24, 22:07:28",3589,5587542,2,"<p>Hello seeing your screenshot maybe you are using a spring boot 2.x version, I had the same problem with spring boot 2.0.3 with Finchley.RELEASE.</p>

<p>I found that Zipkin custom Server are not any more supported and deprecated for this reason it is not possible to use @EnableZipkinServer in Spring Cloud code and you have the ui but not the server side configured, api endpoint and so on.</p>

<p>form the Zipkin base code:</p>

<pre><code>/**
 * @deprecated Custom servers are possible, but not supported by the community. Please use our
 * &lt;a href=""https://github.com/openzipkin/zipkin#quick-start""&gt;default server build&lt;/a&gt; first. If you
 * find something missing, please &lt;a href=""https://gitter.im/openzipkin/zipkin""&gt;gitter&lt;/a&gt; us about
 * it before making a custom server.
 *
 * &lt;p&gt;If you decide to make a custom server, you accept responsibility for troubleshooting your
 * build or configuration problems, even if such problems are a reaction to a change made by the
 * OpenZipkin maintainers. In other words, custom servers are possible, but not supported.
 */
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Import(InternalZipkinConfiguration.class)
@Deprecated
public @interface EnableZipkinServer {

}
</code></pre>

<p>the code is available on <a href=""https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/java/zipkin/server/EnableZipkinServer.java"" rel=""nofollow noreferrer"">official repo of Zipkin</a>
I solve the my problem using the official docker image with a compose</p>

<pre><code>version: '3.1'

services:
  rabbitmq:
    image: rabbitmq:3-management
    restart: always
    ports:
      - 5672:5672
      - 15671:15671
      - 15672:15672
    networks:
      - messaging

  zipkin-server:
    image: openzipkin/zipkin
    ports:
      - 9065:9411
    environment:
      - zipkin.collector.rabbitmq.uri=amqp://guest:guest@rabbitmq:5672
    networks:
      - messaging

networks:
  messaging:
    driver: bridge
</code></pre>

<p>How you can see i use the streaming version. it for me work</p>

<p>I hope that is can help you</p>
"
Zipkin,54355078,50027127,0,"2019/01/24, 22:47:42",False,"2019/01/24, 22:47:42",21,9286335,0,"<p>try with this:</p>

<pre><code>&lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
          &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
          &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
          &lt;version&gt;2.12.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;version&gt;2.12.0&lt;/version&gt;
        &lt;/dependency&gt;
</code></pre>
"
Zipkin,65881898,65873809,0,"2021/01/25, 11:19:25",False,"2021/01/25, 11:19:25",4007,4255107,0,"<p>The easiest way to get it working is to use the Micrometer library and configure the micrometer to send this data to the Zipkin server.</p>
<p>Enabling metrics using micrometer is very simple, you need to just add a micrometer-core and spring-cloud-starter-zipkin libraries.</p>
<p>See this tutorial for details about configuration and code <a href=""https://www.baeldung.com/tracing-services-with-zipkin"" rel=""nofollow noreferrer"">https://www.baeldung.com/tracing-services-with-zipkin</a></p>
<p>Micrometer will report consumer/producer metrics to Zipkin</p>
"
Zipkin,43888530,40654863,0,"2017/05/10, 12:29:15",False,"2017/05/11, 04:27:33",130,1427954,3,"<p>please ensure config your zipkin sever correctly in your spring boot config file.
just like this:</p>

<pre><code>    logging:
      level.org.springframework.cloud: DEBUG
    spring:
      application:
        name: service-tracking
      sleuth:
        enabled: false
    zipkin:
        storage:
            type: mem
    server:
        port: 9411
</code></pre>

<p>And add below config in your zipkin client spring boot config file:</p>

<pre><code>  sleuth:
        enabled: true
        sampler:
          percentage: 1
      zipkin:
        enabled: true
        baseUrl: http://${tracking-host:tracking}:9411
</code></pre>
"
Zipkin,53209830,53208598,0,"2018/11/08, 16:31:04",False,"2018/11/08, 16:31:04",129048,1240763,1,"<blockquote>
  <p><code>Connection refused: connect</code></p>
</blockquote>

<p>Simply means that RabbitMQ is not running on <code>localhost:5672</code> (which is the default if you don't provide a host/port, or addresses, for it in your <code>application.yml</code>).</p>
"
Zipkin,63455905,63443921,4,"2020/08/17, 20:36:56",True,"2020/08/17, 20:36:56",111,2035350,0,"<p>Thing is, you should use opentelemetry collector if you use opentelemetry exporter.</p>
<p>Pls see schema in attachment <a href=""https://i.stack.imgur.com/i2yfZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i2yfZ.png"" alt=""enter image description here"" /></a></p>
<p>Also I created a gist, which will help you to setup
pls see</p>
<p><a href=""https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f"" rel=""nofollow noreferrer"">https://gist.github.com/AndrewGrachov/11a18bc7268e43f1a36960d630a0838f</a></p>
<p>(just tune the values, export to jaeger-all-in-one instead of separate + cassandra, etc)</p>
"
Zipkin,42764225,42764069,1,"2017/03/13, 14:56:55",True,"2017/03/13, 17:25:56",8336,1773866,3,"<p>Of course - you have to just provide your own logging format (e.g. via  <code>logging.pattern.level</code> - check <a href=""https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html</a> for more info). Then you have to register your own <code>SpanLogger</code> bean implementation where you will take care of adding the value of a spring profile via MDC  (you can take a look at this as an example <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/log/Slf4jSpanLogger.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/log/Slf4jSpanLogger.java</a> )</p>

<p><strong>UPDATE:</strong></p>

<p>There's another solution for more complex approach that seems much easier than rewriting Sleuth classes. You can try the <code>logback-spring.xml</code> way like here - <a href=""https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/service1/src/main/resources/logback-spring.xml#L5-L11"" rel=""nofollow noreferrer"">https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/service1/src/main/resources/logback-spring.xml#L5-L11</a> . I'm resolving the application name there so maybe you could do the same with active profiles and won't need to write any code?</p>
"
Zipkin,66205972,66167917,5,"2021/02/15, 12:01:11",True,"2021/03/01, 10:16:15",8336,1773866,2,"<p>The problem might be related to the fact that you're creating the Feign builder manually via <code>Feign.builder()</code> factory method. We're unable to instrument that call. You should create a bean (via <code>SleuthFeignBuilder.builder</code>) and inject that into your code.</p>
"
Zipkin,62871239,62870927,19,"2020/07/13, 10:38:42",True,"2020/07/13, 10:38:42",26220,927493,3,"<p>Dependencies are resolved before plugins are executed.</p>
<p>So the properties you read with the properties-maven-plugin are not available in the <code>&lt;dependencies&gt;</code> section.</p>
<p>If you want to set a dependency version by property, this property must be set inside the POM, on the command line or in the <code>settings.xml</code>.</p>
"
Zipkin,62871339,62870927,0,"2020/07/13, 10:45:40",False,"2020/07/13, 10:53:28",1709,8484783,-1,"<p>You are using an old version of the plugin (<code>1.0-alpha-2</code>), update it to the <a href=""https://www.mojohaus.org/properties-maven-plugin/usage.html"" rel=""nofollow noreferrer"">latest</a> <code>1.0.0</code>.</p>
<p>Then make sure that the file <code>version.properties</code> is in the folder <code>C:\Workspace</code>. Anyway, with the latest version of the plugin you should get a proper error message if it can't find the file.</p>
<p>One more suggestion: <code>spring-cloud-starter-zipkin</code> belongs to the <code>org.springframework.cloud</code> group which follows another version. The suggested way to declare that dependency is like the following:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;properties&gt;
    &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;spring-cloud.version&gt;Hoxton.SR6&lt;/spring-cloud.version&gt;
&lt;/properties&gt;

&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;

&lt;dependencies&gt;
    &lt;!-- ... --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;!-- ... --&gt;
&lt;/dependencies&gt;
</code></pre>
"
Zipkin,62756345,62674846,2,"2020/07/06, 15:42:41",False,"2020/07/06, 15:42:41",50,11866103,1,"<p>I was able to setup OpenCensus in GCP (in an instance on the project) by following the steps <a href=""https://cloud.google.com/monitoring/custom-metrics/open-census#prereqs"" rel=""nofollow noreferrer"">mentioned here</a>.</p>
<p>To set it up quickly, here are the commands I ran in a brand new Ubuntu instance</p>
<pre><code>sudo apt-get install python3
sudo apt install python3-pip
wget https://raw.githubusercontent.com/GoogleCloudPlatform/python-docs-samples/master/opencensus/requirements.txt
pip3 install -r requirements.txt
wget https://raw.githubusercontent.com/GoogleCloudPlatform/python-docs-samples/master/opencensus/metrics_quickstart.py
python3 metrics_quickstart.py
</code></pre>
"
Zipkin,59925933,59924890,2,"2020/01/27, 08:15:42",False,"2020/01/27, 08:20:48",26898,1192728,0,"<p>You should use <a href=""https://istio.io/docs/tasks/traffic-management/egress/egress-gateway/"" rel=""nofollow noreferrer"">egress-gateway</a>. When all external calls go to the gateway, istio can get the metadata and does some tracing works. There are many advantages when using ingress/egress gateway:</p>

<ul>
<li>Increasing security:  We can set up all security rules at the gateway.</li>
<li>Abstraction the application logic: Instead of configuring settings at each microservices.</li>
<li>TLS processing: Like the above example, envoy can have all the necessary data in HTTPS requests.</li>
</ul>
"
Zipkin,59985374,59924890,0,"2020/01/30, 13:47:03",False,"2020/01/30, 13:47:03",5749,11977760,0,"<p>Based on <a href=""https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/trace/v2/trace.proto#config-trace-v2-tracing"" rel=""nofollow noreferrer"">envoy documentation</a> it doesn't support https tracing.</p>

<blockquote>
  <p>The tracing configuration specifies global settings for the HTTP tracer used by Envoy. The configuration is defined by the Bootstrap tracing field. Envoy may support other tracers in the future, but right now the HTTP tracer is the only one supported.</p>
</blockquote>

<p>And this post on <a href=""https://stackoverflow.com/questions/187655/are-https-headers-encrypted"">stackoverflow</a></p>

<blockquote>
  <p>HTTPS (HTTP over SSL) sends all HTTP content over a SSL tunel, so HTTP content and headers are encrypted as well.</p>
</blockquote>

<p>I have even tried to reproduce that, but like in your case zipkin worked only for http.</p>

<p>Based on that I would say it's not possible to use zipkin to track https.</p>
"
Zipkin,58562527,58562126,2,"2019/10/25, 19:38:58",False,"2019/10/25, 19:38:58",6038,11032044,0,"<p>It's because you haven't mentioned the <code>host</code> here:</p>

<pre><code>spec:
  rules:
  -  host: hostname.com // here
     http:
      paths:
        - path: /zipkin
          backend:
            serviceName: zipkin
            servicePort: 9411
</code></pre>
"
Zipkin,58581451,58562126,1,"2019/10/27, 18:52:20",False,"2019/10/27, 18:52:20",1017,6713869,0,"<p>First, previous answer is wrong, you don't need to specify <code>host</code> it is not mandatory unless you want to set up a DNS. </p>

<p>Second, the backend <code>zipkin</code> requires the <code>/zipkin</code> URI to respond right? If this is the case, then the rewrite annotation is removing the URI. So you would need to change your yaml like this to pass <code>/zipkin</code> to your backend. </p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
  namespace: nginx-ingress
spec:
  rules:
  - http:
      paths:
        - path: /zipkin
          backend:
            serviceName: zipkin
            servicePort: 9411
</code></pre>
"
Zipkin,58591046,58562126,0,"2019/10/28, 14:35:42",False,"2019/10/28, 14:35:42",2049,11207414,0,"<p>Just to clarify the OP problem.</p>

<p>There are different <a href=""https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/"" rel=""nofollow noreferrer"">ingress Controllers</a> </p>

<p>Note:</p>

<blockquote>
  <p>When you create an ingress, you should annotate each ingress with the appropriate ingress.class to indicate which ingress controller should be used if more than one exists within your cluster.</p>
  
  <p>If you do not define a class, your cloud provider may use a default ingress controller.</p>
  
  <p>Ideally, all ingress controllers should fulfill this specification, but the various ingress controllers operate slightly differently.</p>
</blockquote>

<p>Using this annotation: </p>

<pre><code>nginx.org/rewrites: &gt;
      serviceName=zipkin rewrite=/;
</code></pre>

<p>It looks like you are using NGINX Ingress Controller provided by nginxinc.</p>

<p>You can find more information about <strong>Rewrites Support</strong> for NGINX Ingress Controller provided by <a href=""https://github.com/nginxinc/kubernetes-ingress/tree/master/examples/rewrites"" rel=""nofollow noreferrer"">nginxinc here</a>.</p>

<p>example:</p>

<pre><code>nginx.org/rewrites: ""serviceName=service1 rewrite=rewrite1[;serviceName=service2 rewrite=rewrite2;...]""
</code></pre>

<p>It's different from the kubernetes community at <a href=""https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md#rewrite-target"" rel=""nofollow noreferrer"">kubernetes/ingress-nginx repo</a>.
Different ingress controllers have different configs and annotations.</p>

<p>So for this example:</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.org/rewrites: ""serviceName=my-nginx rewrite=/""
  name: test-ingress
  namespace: default
spec:
  rules:
  - host: hostname.com
    http:
      paths:
      - backend:
          serviceName: my-nginx
          servicePort: 80
        path: /zipkin
status:
  loadBalancer:
    ingress:
    - ip: xx.xxx.xxx.xx
</code></pre>

<p>Test it:</p>

<pre><code>curl -vH 'Host:hostname.com' xx.xxx.xxx.xx/zipkin


* Connected to xx.xxx.xxx.xx (xx.xxx.xxx.xx) port 80 (#0)
&gt; GET /zipkin HTTP/1.1
&gt; Host:hostname.com
&gt; User-Agent: curl/7.52.1
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Server: nginx/1.17.5
&lt; Date: Mon, 28 Oct 2019 12:21:48 GMT
&lt; Content-Type: text/html
&lt; Content-Length: 612
&lt; Connection: keep-alive
&lt; Last-Modified: Tue, 22 Oct 2019 14:30:00 GMT
&lt; ETag: ""5daf1268-264""
&lt; Accept-Ranges: bytes
&lt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</code></pre>
"
Zipkin,57856470,57856249,2,"2019/09/09, 17:52:26",True,"2019/09/10, 12:40:49",8336,1773866,3,"<p>If S1 sends a request to S2 it's the S1 that sets the sampler value and S2 just continues the result. In other words S1 will export to zipkin 100 * 0.1 = 10 requests, and S2 will export 100 * 0.1 = 10 requests. S1 makes a decision and S2 will continue it.</p>
"
Zipkin,55661357,55575721,0,"2019/04/13, 04:59:36",False,"2019/04/13, 04:59:36",123,10593981,1,"<p>FYI, this worked after generating dummy X-B3-SpanId; it works as long as X-B3-TraceId is unique.</p>

<p>e.g.</p>

<pre><code>map $http_x_b3_traceid $_request_id {
    default   $http_x_b3_traceid;
    """"        $request_id;
}
map $http_x_b3_spanid $_span_id {
    default   $http_x_b3_spanid;
    """"        ""1111111111111111"";
}
</code></pre>
"
Zipkin,59222522,55571957,3,"2019/12/07, 04:36:36",True,"2019/12/07, 05:27:09",26,12494357,1,"<p>I know this is old but I have just had exactly the same problem, and have just worked out it's being caused by the appmetrics libraries. </p>

<p>Once I figure out how to get it working I'll update this.</p>

<p>EDIT:</p>

<p>OK managed to get it working with appmetrics-dash. You need to use monitor() instead of attach() and move the monitor to the end of your routes as so. I have investigated appmetrics-prometheus and it only has an attach() at this stage so can't be used:</p>

<pre><code>const dash = require('appmetrics-dash');

const express = require('express');
const history = require('connect-history-api-fallback');

const app = express();
const server = require('http').Server(app);

require('./routers/index')(app, server);
require('./services/index')(app);

// Add your code here

dash.monitor({server});

// catch 404 and forward to error handler
app.use(function(req, res) {
    res.status(404).send('Not Found');
});

// error handler
app.use(function(err, req, res) {
    res.status(err.status || 500).send('Error');
});

server.listen(env.port, () =&gt; {
    logger.info(`${env.name} listening on ${env.url}/appmetrics-dash`);
    logger.info(`${env.name} listening on ${env.url}`);
});
</code></pre>
"
Zipkin,55173328,54702777,0,"2019/03/15, 01:07:05",False,"2019/03/15, 01:14:36",nan,nan,1,"<p>Alright, so I'm going to answer this based on what you said here:</p>

<blockquote>
  <p><em>Or a better approach</em> if there aint any support/ plugin for the same.</p>
</blockquote>

<p>The way that I do it us through <a href=""https://prometheus.io/"" rel=""nofollow noreferrer"">Prometheus</a>, in combination with <a href=""https://github.com/Technofy/cloudwatch_exporter"" rel=""nofollow noreferrer""><code>cloudwatch_exporter</code></a>, and <a href=""https://prometheus.io/docs/alerting/alertmanager/"" rel=""nofollow noreferrer""><code>alertmanager</code></a>.</p>

<p>The configuration for <code>cloudwatch_exporter</code> to monitor SQS is going to be something like (this is only two metrics, you'll need to add more based on what you're looking to monitor):</p>

<pre><code>tasks:
 - name: ec2_cloudwatch
   default_region: us-west-2
   metrics:
    - aws_namespace: ""AWS/SQS""
      aws_dimensions: [QueueName]
      aws_metric_name: NumberOfMessagesReceived
      aws_statistics: [Sum]
      range_seconds: 600
    - aws_namespace: ""AWS/SQS""
      aws_dimensions: [QueueName]
      aws_metric_name: ApproximateNumberOfMessagesDelayed
      aws_statistics: [Sum]
</code></pre>

<p>You'll then need to configure prometheus to scrape the <code>cloudwatch_exporter</code> endpoint at an interval, for ex what I do:</p>

<pre><code>  - job_name: 'somename'
    scrape_timeout: 60s
    dns_sd_configs:
    - names:
        - ""some-endpoint""
    metrics_path: /scrape
    params:
      task: [ec2_cloudwatch]
      region: [us-east-1]
    relabel_configs:
      - source_labels: [__param_task]
        target_label: task
      - source_labels: [__param_region]
        target_label: region
</code></pre>

<p>You would then configure <code>alertmanager</code> to alert based on those scraped metrics; I do not alert on those metrics so I cannot give you an example. But, to give you an idea how of this architecture, a diagram is below:</p>

<p><a href=""https://i.stack.imgur.com/xTwv0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xTwv0.png"" alt=""enter image description here""></a></p>

<p>If you need to use something like <code>statsd</code> you can use <a href=""https://github.com/prometheus/statsd_exporter"" rel=""nofollow noreferrer""><code>statsd_exporter</code></a>. And, just in-case you were wondering, yes <a href=""https://grafana.com/plugins/prometheus"" rel=""nofollow noreferrer"">Grafana supports prometheus</a>.</p>
"
Zipkin,53845941,53765439,0,"2018/12/19, 08:47:57",False,"2018/12/19, 09:03:57",533,5736574,2,"<p>As there is a bug in Spring AMQP, which will be fixed in Release 2.1.3 
<strong><a href=""https://jira.spring.io/browse/AMQP-846"" rel=""nofollow noreferrer"">Issue link</a></strong></p>

<p>For a tempory fix, you can enable retry properties to create advice chain.</p>

<pre><code>spring.rabbitmq.listener.direct.retry.enabled=true
spring.rabbitmq.listener.simple.retry.enabled=true
</code></pre>

<p>Hope this resolves your problem.</p>
"
Zipkin,53967904,53765439,0,"2018/12/29, 10:21:00",False,"2018/12/29, 10:27:20",53,2842281,1,"<p>I had this same issue, changing Spring Boot version to 2.1.0.RELEASE did the trick for me. You should try it too. There must be something wrong with  RabbitMQ in Spring Boot version 2.1.1.RELEASE.</p>
"
Zipkin,55292618,53765439,0,"2019/03/22, 05:48:23",False,"2019/03/22, 05:48:23",11,9568260,1,"<p>add build.gradle </p>

<blockquote>
  <p>apply plugin: 'org.springframework.boot'</p>
</blockquote>

<p>springBootVersion=2.1.3.RELEASE
springCloudVersion=Greenwich.RELEASE</p>
"
Zipkin,52505155,52486463,1,"2018/09/25, 22:03:18",False,"2018/09/25, 22:03:18",5434,7059,0,"<p>Finally got it to work removing <code>@AutoConfigureAfter</code>, <code>@CondtionnalOnBean</code> and <code>@ConditionnalOnMissingBean</code>, using instead <code>@ConditionalOnClass</code>, <code>@ConditionnalOnMissingClass</code> and reproducing other <code>@Conditionnals</code> from <code>TraceAutoConfiguration</code>. Not great, but at least working.</p>
"
Zipkin,51938178,51931755,0,"2018/08/20, 23:37:41",False,"2018/08/20, 23:37:41",8855,9705485,1,"<p>I think that Christian Posta article you refer to is very good. As he says, you can deal with the most common use-cases with the out of the box Kubernetes solutions for discovery (kub dns), load-balancing (with Services) and edge services/gateway (Ingress). </p>

<p>As Christian also points out, if you need to dynamically discover services by actively querying rather than knowing what you are looking for then Spring Cloud Kubernetes can be better than going directly to Kubernetes Apis. If you need to refresh your app from a config change and see it update quickly without going through a rolling update (which would be needed if you were mounting the configmap as a volume) then Spring cloud Kubernetes config client could be of value. The ribbon integration could also be of value if you need client-side load-balancing. So you could start out without Spring Cloud Kubernetes and add parts of it if and when you find that it would help. I think it is better to think of the project as adding extra options and conveniences rather than alternatives to Kubernetes-native solutions.</p>

<p>It is also worth noting that you can deploy a Netflix stack app to Kubernetes (including using Zuul and eureka) and there isn't necessarily anything wrong with that. It has the advantage that you can work with it outside Kubernetes and it might be more convenient for your particular team if it's Java team. The main downside is that the Netflix stack is very tied to Java, whereas Kubernetes is language neutral.</p>
"
Zipkin,50941229,50940162,3,"2018/06/20, 08:59:58",False,"2018/06/21, 08:11:08",491,2389992,0,"<p>We had this very similar issue with Akka. We observed huge delay in ask pattern to deliver messages to the target actor on peek load.</p>

<p>Most of these issues are related to heap memory consumption and not because of usages of dispatchers.</p>

<p>Finally we fixed these issues by tuning some of the below configuration and changes.</p>

<p>1) Make sure you stop entities/actors which are no longer required. If its a persistent actor then you can always bring it back when you need it.
   Refer : <a href=""https://doc.akka.io/docs/akka/current/cluster-sharding.html#passivation"" rel=""nofollow noreferrer"">https://doc.akka.io/docs/akka/current/cluster-sharding.html#passivation</a></p>

<p>2) If you are using cluster sharding then check the akka.cluster.sharding.state-store-mode. By changing this to persistence we gained 50% more TPS.</p>

<p>3) Minimize your log entries (set it to info level).</p>

<p>4) Tune your logs to publish messages frequently to your logging system. Update the batch size, batch count and interval accordingly. So that the memory is freed. In our case huge heap memory is used for buffering the log messages and send in bulk. If the interval is more then you may fill your heap memory and that affects the performance (more GC activity required).</p>

<p>5) Run blocking operations on a separate dispatcher.</p>

<p>6) Use custom serializers (protobuf) and avoid JavaSerializer.</p>

<p>7) Add the below JAVA_OPTS to your jar</p>

<p>export JAVA_OPTS=""$JAVA_OPTS -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 -Djava.security.egd=file:/dev/./urandom""</p>

<p>The main thing is XX:MaxRAMFraction=2 which will utilize more than 60% of available memory. By default its 4 means your application will use only one fourth of the available memory, which might not be sufficient.</p>

<p>Refer : <a href=""https://blog.csanchez.org/2017/05/31/running-a-jvm-in-a-container-without-getting-killed/"" rel=""nofollow noreferrer"">https://blog.csanchez.org/2017/05/31/running-a-jvm-in-a-container-without-getting-killed/</a></p>

<p>Regards,</p>

<p>Vinoth</p>
"
Zipkin,50585875,50562296,0,"2018/05/29, 16:41:00",True,"2018/05/30, 12:06:27",2002,9524052,1,"<p>As @Bal Chua and @Pär Nilsson mentioned, for environmental variables you can use only string variables because Linux environmental variables can be only strings. </p>

<p>So, if you use yaml, you need to place value into quotes to force Kubernetes to use string.</p>

<p>For example:</p>

<pre><code>- name: RABBITMQ_PORT
  value: '31503'
</code></pre>
"
Zipkin,49881877,49880941,2,"2018/04/17, 18:17:50",True,"2018/07/27, 14:22:52",5965,1235935,8,"<p>Even when you use Spring Cloud, 100 services do NOT mean 100 servers. In Spring Cloud the packaging unit is Spring Boot application and a single server may host many such Spring Boot applications. If you want, you can containerize the Spring Boot applications and other Spring Cloud infrastructure support components. But that is not Kubernetes.</p>

<p>If you move to Kubernetes, you don't need the infrastructure support services like Zuul, Ribbon etc. because Kubernetes has its own components for service discovery, gateway, load balancer etc. In Kubernetes, the packaging unit is Docker images and one or more Docker containers can be put inside one pod which is the minimal scaling unit. So, Kubernetes has a different set of components to manage the Microservices.</p>

<p>Kubernetes is a different platform than Spring cloud. Both have the same objectives. However, Kubernetes has some additional features like self healing, auto-scaling, rolling updates, compute resource management, deployments etc.</p>
"
Zipkin,50084521,49880941,0,"2018/04/29, 10:57:09",False,"2018/04/29, 10:57:09",8855,9705485,2,"<p>Just to add to saptarshi basu's answer, you might want to look at <a href=""https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes"" rel=""nofollow noreferrer"">https://dzone.com/articles/deploying-microservices-spring-cloud-vs-kubernetes</a> as it walks through the comparison and asks which responsibilities you might want to be handled by which components when using Spring cloud on kubernetes</p>
"
Zipkin,49687046,49686743,0,"2018/04/06, 09:52:45",True,"2018/04/06, 09:52:45",8336,1773866,1,"<p>If you're using Sleuth 2.0 you can call on the <code>Tracer</code> a method to create a new trace. In the older version of sleuth I guess what I'd do is to use an executor that is <strong>NOT</strong> a bean. That way you would lose the trace and it would get restarted at some point (by rest template or sth like that).</p>
"
Zipkin,45369348,45368147,4,"2017/07/28, 12:08:22",False,"2017/07/28, 12:08:22",8336,1773866,3,"<p>Thanks for the kind words! In Sleuth Edgware we will support Reactor - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-reactor"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-reactor</a> and in Sleuth Finchley we will support reactor and webflux <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/2.0.x/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceWebFluxAutoConfiguration.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/2.0.x/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceWebFluxAutoConfiguration.java</a>. In other words it's already possible to use Sleuth in the reactive context.</p>
"
Zipkin,43982611,43937265,1,"2017/05/15, 17:54:11",False,"2017/05/15, 17:54:11",473,2733462,1,"<p>It seems like you are using <a href=""https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_http"" rel=""nofollow noreferrer"">Sleuth with Zipkin via HTTP</a>. You can try the <a href=""https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_spring_cloud_stream"" rel=""nofollow noreferrer"">Sleuth with Zipkin via Spring Cloud Stream</a> approach. I haven't done the benchmark myself, but it should improve the performance in theory.</p>

<p>Please see the documentation at: <a href=""https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_spring_cloud_stream"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sleuth_with_zipkin_via_spring_cloud_stream</a></p>
"
Zipkin,44062172,43937265,3,"2017/05/19, 08:36:59",False,"2017/05/19, 08:36:59",8336,1773866,1,"<p>I wonder what kind of a benchmarking method you have picked. Which version of Sleuth are you using? Also is this one single benchmark that you're doing? Is it on your computer? Has the JVM gotten heated up? Are there any other processes executed? Doing benchmarking is not that easy... You can use tools like JMH to do it better.</p>

<p>BTW try turning off the DEBUG logging level and check the results again. </p>

<p>We are performing benchmark tests of Sleuth and from what we see when adding Sleuth the latency gets increased by around 20 ms. Definitely not 600 ms. </p>
"
Zipkin,59895808,43937265,0,"2020/01/24, 13:46:12",True,"2020/01/24, 13:46:12",131,9437494,2,"<p>I belive you had problem with: org.springframework.cloud.sleuth.zipkin.ServerPropertiesEndpointLocator.</p>

<p>It uses InetUtils.findFirstNonLoopbackAddress() to determine instance address. Method is called synchronously when each span is close (in ZipkinSpanListener#convert). The workaround is to create custom org.springframework.cloud.sleuth.zipkin.EndpointLocator. You can use something like that:</p>

<pre><code>class CachingEndpointLocator implements EndpointLocator {

    private final Endpoint endpoint;

    CachingEndpointLocator(EndpointLocator delegate) {
      this.endpoint = delegate.local();
    }

    @Override
    public Endpoint local() {
      return endpoint;
    }
  }
</code></pre>

<p>And combine it with one of existing EndpointLocators. You can find them in: org.springframework.cloud.sleuth.zipkin.ZipkinAutoConfiguration.</p>

<p>This issue is already fixed in sleuth 2.X.X. Where: org.springframework.cloud.sleuth.zipkin2.DefaultEndpointLocator caches server address:</p>

<pre><code>    public DefaultEndpointLocator(Registration registration,
            ServerProperties serverProperties, Environment environment,
            ZipkinProperties zipkinProperties, InetUtils inetUtils) {
        this.registration = registration;
        this.serverProperties = serverProperties;
        this.environment = environment;
        this.zipkinProperties = zipkinProperties;
        this.firstNonLoopbackAddress = findFirstNonLoopbackAddress(inetUtils);
    }
</code></pre>
"
Zipkin,40117668,40090893,1,"2016/10/18, 23:39:29",True,"2016/10/18, 23:39:29",257711,526535,2,"<p>The web app is trying to access <code>config.json</code> at root (accessing as <code>/config.json</code> vs just <code>config.json</code> ) - that is <a href=""http://localhost:8001/config.json"" rel=""nofollow"">http://localhost:8001/config.json</a> . This would obviously be wrong as it should be <a href=""http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json"" rel=""nofollow"">http://localhost:8001/api/v1/proxy/namespaces/default/services/zipkin-http/config.json</a> </p>

<p>There is a very simple solution for this - just run:</p>

<pre><code>kubectl port-forward &lt;name of the pod&gt; 9411
</code></pre>

<p>Now just go to <a href=""http://localhost:9411"" rel=""nofollow"">http://localhost:9411</a> and the UI should be up (tried and verified.)</p>

<p>You can get the name of the pod by doing <code>kubectl get pods</code></p>

<p>PS: <code>kubectl proxy</code> is generally meant to access the Kubernetes API, and <code>kube port-forward</code> is the right tool in this case. </p>
"
Zipkin,38021120,38019422,1,"2016/06/24, 22:52:36",False,"2016/06/24, 22:52:36",31,6509824,0,"<pre><code>import (
    ""github.com/opentracing/opentracing-go""
    ""github.com/openzipkin/zipkin-go-opentracing""
)
func IdFromSpan(aspan interface{}) uint64 {
    zspan := aspan.(zipkintracer.Span)
    return zspan.Context().TraceID
}
</code></pre>
"
Zipkin,45769658,38019422,0,"2017/08/19, 11:59:32",False,"2017/08/19, 11:59:32",117,916394,0,"<p>I'm not sure this is the right way to do it but this should normally works</p>

<pre><code>stdopentracing ""github.com/opentracing/opentracing-go""
zipkin ""github.com/openzipkin/zipkin-go-opentracing""

[...]

var traceID string

sp := stdopentracing.SpanFromContext(ctx)

if sp != nil {
    traceID = sp.Context().(zipkin.SpanContext).TraceID.ToHex()
}
</code></pre>
"
Zipkin,66808417,66777772,2,"2021/03/26, 00:23:10",False,"2021/03/26, 00:23:10",46,14172753,0,"<p>I'm not entirely sure if that's what you mean, but you can use Jeager <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/</a>  which checks if trace-id already exist in the invocation metadata and in it generate child trace id. Based on all trace ids call diagrams are generated</p>
"
Zipkin,66417136,60631319,0,"2021/03/01, 07:37:26",False,"2021/03/01, 07:37:26",438,3404777,0,"<p>I think you must have found your answer by now. But I am posting this for future reference.</p>
<p>Take a look at this <a href=""https://github.com/openzipkin/zipkin/issues/1870"" rel=""nofollow noreferrer"">Github issue</a>, it basically explains everything and provides a few workarounds.</p>
"
Zipkin,66917361,66916739,0,"2021/04/02, 12:39:37",True,"2021/04/02, 12:39:37",8336,1773866,1,"<p>Yes you can, and I have shown that numerous times during my presentations (<a href=""https://toomuchcoding.com/talks"" rel=""nofollow noreferrer"">https://toomuchcoding.com/talks</a>) and we describe it extensively in the documentation (<a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/html/</a>). Sleuth will set up your logging pattern which you can then parse and visualize using the ELK stack. Sleuth takes care of tracing context propagation and can send the spans to a span storage (e.g. Zipkin or Jaeger). Sleuth does take care of updating the MDC for you. Please always read the documentation and the project page before filing a question</p>
"
Zipkin,66913758,66911879,3,"2021/04/02, 05:29:08",False,"2021/04/02, 05:29:08",66,12105655,1,"<p><code>server.address=&lt;ip&gt;</code> does not work?</p>
<p><code>java -jar zipkin.jar --server.address=192.168.0.7</code></p>
<p>If it's not working you can add a property file and connects to it when the server starts:</p>
<p><code>java -jar zipkin.jar --spring.config.location=./application.properties</code></p>
<p>in application.properties:</p>
<pre><code>armeria:
  ports:
    - port: 8080
      ip: 192.168.0.7
      protocol: HTTP
</code></pre>
"
Zipkin,66350164,66284701,0,"2021/02/24, 13:40:02",False,"2021/02/24, 13:52:09",11,15274583,1,"<p>I get the same problem and below command did the trick.</p>
<pre><code>java -javaagent:tools/opentelemetry-javaagent-all.jar \
-Dotel.traces.exporter=zipkin \
-jar target/*.jar
</code></pre>
<p>I checked the source code. It looks the property name has been changed:</p>
<p><a href=""https://github.com/open-telemetry/opentelemetry-java/blob/14ace1ec32dbb194b8990763beb3ab6935849547/sdk-extensions/autoconfigure/src/main/java/io/opentelemetry/sdk/autoconfigure/TracerProviderConfiguration.java#L43"" rel=""nofollow noreferrer"">https://github.com/open-telemetry/opentelemetry-java/blob/14ace1ec32dbb194b8990763beb3ab6935849547/sdk-extensions/autoconfigure/src/main/java/io/opentelemetry/sdk/autoconfigure/TracerProviderConfiguration.java#L43</a></p>
"
Zipkin,61247710,57180252,0,"2020/04/16, 13:11:53",False,"2020/04/16, 13:11:53",287,8631898,0,"<p>On my project, we generated the spans manually before sending the events.</p>
<blockquote>
<p>var span = tracing.tracer().nextSpanWithParent(req -&gt; true,
Void.class, ctx.get(Span.class).context());</p>
<p>span.name(&quot;yourSpanName&quot;).start();</p>
<p>return sendEventPublisher.doOnError(span::error).doOnTerminate(span::finish);</p>
</blockquote>
<p>This way, we also link the span to the publisher lifecycle as we had problems with webflux sharing spans between threads.</p>
<p>Basically, we create a span and link it to the parent context created by Spring for the request (either from an incoming B3 HTTP header, or generated if absent). &quot;ctx&quot; is the subscriber context here.</p>
<p>This also implied to tell sleuth not to generate the spans for async operations in application.properties:</p>
<blockquote>
<p>spring.sleuth.async.enabled=false</p>
</blockquote>
"
Zipkin,60500953,60489344,0,"2020/03/03, 08:01:57",True,"2020/03/03, 08:01:57",5749,11977760,2,"<p>The answer here is to install istio with <code>--set values.global.tracer.zipkin.address</code> as provided in <a href=""https://istio.io/docs/tasks/observability/distributed-tracing/jaeger/#before-you-begin"" rel=""nofollow noreferrer"">istio documentation</a></p>

<pre><code>istioctl manifest apply --set values.global.tracer.zipkin.address=&lt;jaeger-collector-service&gt;.&lt;jaeger-collector-namespace&gt;:9411
</code></pre>

<hr>

<p><strong>And</strong></p>

<hr>

<p>Use the original TracingService <code>setting: service: ""zipkin.istio-system:9411""</code> as Donato Szilagyi confirmed in comments.</p>

<pre><code>apiVersion: getambassador.io/v2
kind: TracingService
metadata:
  name: tracing
  namespace: {{ .Values.namespace }}
spec:
  service: ""zipkin.istio-system:9411""
  driver: zipkin
  ambassador_id: ambassador-{{ .Values.namespace }}
  config: {}
</code></pre>

<blockquote>
  <p>Great! It works. And this time I used the original TracingService setting: service: ""zipkin.istio-system:9411"" – Donato Szilagy</p>
</blockquote>
"
Zipkin,61807946,61800994,1,"2020/05/15, 00:44:40",False,"2020/05/15, 00:44:40",985,598932,1,"<p>It looks like it is related to <a href=""https://github.com/openzipkin/zipkin-js/pull/498"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin-js/pull/498</a>, could you try with zipkin-context-cls@0.19.2-alpha.7 and change <code>ctxImpl</code> into <code>ctxImpl = new CLSContext('zipkin', true);</code>?</p>
"
Zipkin,61862789,61800994,0,"2020/05/18, 08:08:34",True,"2020/05/18, 19:34:48",156911,4668,0,"<p>The problem ended up not being on Zipkin's end, but instead in how I was instrumenting the express server.  </p>

<pre><code>app.get('/main', async function (req, res) {
    //...
})

app.use(zipkinMiddleware({tracer}));
</code></pre>

<p>I had added the zipkin middleware <em>after</em> my call to <code>app.get</code>.  Express executes middlwares in order and makes no distinction between middleware for a named route vs. something added via <code>app.use</code>. </p>

<p>Doing things like this</p>

<pre><code>app.use(zipkinMiddleware({tracer}));

app.get('/main', async function (req, res) {
    //...
})
</code></pre>

<p>Gave me the result I was looking for.     </p>
"
Zipkin,50599592,50599433,4,"2018/05/30, 11:14:23",False,"2018/05/30, 11:14:23",294930,208809,0,"<p>According to <a href=""https://istio.io/docs/tasks/telemetry/distributed-tracing.html"" rel=""nofollow noreferrer"">the section titled Cleanup in the Istio docs</a>:</p>

<pre><code>kubectl delete -f install/kubernetes/addons/zipkin.yaml
</code></pre>
"
Zipkin,50478519,50389884,0,"2018/05/23, 04:20:43",False,"2018/05/23, 04:20:43",201,7956609,1,"<p>You can add the trace id in your logback.xml</p>

<p>""request_id"":
                {""trace_id"":""%X{X-B3-TraceId}"",""span_id"":""%X{X-B3-SpanId}"",""parent_span_id"":""%X{X-B3-ParentSpanId}""},</p>
"
Zipkin,66654542,66517888,1,"2021/03/16, 13:40:16",False,"2021/03/16, 13:40:16",19,12338209,0,"<p>In your command please try the following -Dotel.traces.exporter=zipkin instead of -Dotel.exporter=zipkin</p>
"
Zipkin,66782647,66517888,1,"2021/03/24, 15:58:38",True,"2021/03/24, 15:58:38",16,15470262,0,"<p>i had the same issue, but i solved with this jvm arguments:</p>
<p>-Dotel.traces.exporter=zipkin -Dotel.metrics.exporter=none -Dotel.exporter.zipkin.endpoint=http://localhost:9411/api/v2/spans
Maybe the error is on zipkin.endpoint, try to write the entire url.</p>
<p>Regards,
Marco</p>
"
Zipkin,57365645,55962977,3,"2019/08/05, 23:23:09",True,"2019/08/05, 23:23:09",5287,780798,2,"<p><a href=""https://opentracing.io/specification/"" rel=""nofollow noreferrer"">OpenTracing</a> <a href=""https://github.com/opentracing/specification/blob/master/project_organization.md"" rel=""nofollow noreferrer"">is a set of standard APIs that consistently model and describe the behavior of distributed systems</a>)</p>

<p>OpenTracing did not describe how to collect, report, store or represent the data of interrelated traces and spans. It is implementation details (such as <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">jaeger</a> or <a href=""https://www.wavefront.com/"" rel=""nofollow noreferrer"">wavefront</a>).</p>

<p>jaeger-client-csharp is very jaeger-specific. But there is one exception, called <a href=""https://www.jaegertracing.io/docs/1.12/features/#backwards-compatibility-with-zipkin"" rel=""nofollow noreferrer"">zipkin</a> which in turns is not fully OpenTracing compliant, even it has similar terms.</p>

<p>If you are OK with <a href=""https://github.com/opentracing-contrib/csharp-netcore/"" rel=""nofollow noreferrer"">opentracing-contrib/csharp-netcore</a> (hope you are using this library) then if you want to achieve ""no code change"" (in target microservice) in order to configure tracing subsystem, you should use some plug-in model.</p>

<p>Good news that aspnetcore has concept of <a href=""https://docs.microsoft.com/en-us/ASPNET/Core/fundamentals/host/platform-specific-configuration?view=aspnetcore-2.2"" rel=""nofollow noreferrer"">hosted startup assemblies</a>, which allow you to configure tracing system. So, you can have some library called <code>JaegerStartup</code> where you will implement IHostedStartup like follows:</p>

<pre><code>public class JaegerStartup : IHostingStartup
{
    public void Configure(IWebHostBuilder builder)
    {
        builder.ConfigureServices((ctx, services) =&gt;
        {
            services.AddOpenTracing();

            if (ctx.Configuration.IsTracerEnabled()) // implement it by observing your section in configuration.
            {
                services.AddSingleton(serviceProvider =&gt;
                {
                    var loggerFactory = new LoggerFactory();
                    var config = Jaeger.Configuration.FromEnv(loggerFactory);

                    var tracer = config.GetTracer();

                    GlobalTracer.Register(tracer);

                    return tracer;
                });
            }
        });
    }
}
</code></pre>

<p>When you decide to switch the tracing system - you need to create another library, which can be loaded automatically, and target microservice code will not be touched.</p>
"
Zipkin,58040373,57703884,0,"2019/09/21, 15:42:54",False,"2019/09/21, 15:42:54",1477,4051158,0,"<p>I am new to zipkin and golang, If you want to trace internal process, then you can create span from context</p>

<p>example: say you have api called Login, inside login you might perform database operation or any other operations</p>

<pre><code>func Login(res http.ResponseWriter, req *http.Request) {
    span, _ := tracer.StartSpanFromContext(req.Context(), ""database"")
    databaseOperation()
    span.Finish()
}
</code></pre>
"
Zipkin,61367030,56328934,0,"2020/04/22, 17:01:23",False,"2020/04/22, 17:01:23",2849,797243,0,"<p>I have seen this issue a lot. From my experience the most common cause is, that the base64 string was encoded on the commandline using <code>echo '$mypw' | base64</code> which will create newlines in the encoded string. You need to use the <code>-n</code> switch to echo: <code>echo -n '$mypw' | base64</code>.</p>
"
Zipkin,52117392,51187274,1,"2018/08/31, 17:16:40",False,"2018/08/31, 18:03:37",1,1380632,0,"<p>What logging framework are you using? I was using log4j2 in my project. It started working when I removed log4j2 and left it to the default logging of spring-cloud-sleuth.</p>
"
Zipkin,48763174,48759442,0,"2018/02/13, 11:15:18",True,"2018/02/13, 11:15:18",8336,1773866,2,"<p>It's a bug - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/855"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/855</a> . I've fixed it ATM. A workaround is to start it manually either in each method that uses <code>@NewSpan</code> by calling <code>start()</code> method on current span (that doesn't scale too nicely)</p>

<pre><code>@Autowired SpanCustomizer customizer;

@NewSpan
void foo() {
    this.customizer.start();
}
</code></pre>

<p>You can also create a bean of <code>SpanCreator</code> (you can check the fixed version here <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/annotation/DefaultSpanCreator.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/annotation/DefaultSpanCreator.java</a>)</p>

<pre><code>class MySpanCreator implements SpanCreator {

    private static final Log log = LogFactory.getLog(MySpanCreator.class);

    private final Tracing tracer;

    MySpanCreator(Tracing tracer) {
        this.tracer = tracer;
    }

    @Override public Span createSpan(MethodInvocation pjp, NewSpan newSpanAnnotation) {
        String name = StringUtils.isEmpty(newSpanAnnotation.name()) ?
                pjp.getMethod().getName() : newSpanAnnotation.name();
        String changedName = SpanNameUtil.toLowerHyphen(name);
        if (log.isDebugEnabled()) {
            log.debug(""For the class ["" + pjp.getThis().getClass() + ""] method ""
                    + ""["" + pjp.getMethod().getName() + ""] will name the span ["" + changedName + ""]"");
        }
        return this.tracer.tracer().nextSpan().name(changedName).start();
    }

}
</code></pre>

<p>Notice the <code>.start()</code> at the end of the method.</p>
"
Zipkin,48679791,48679359,13,"2018/02/08, 09:23:24",False,"2018/02/08, 09:23:24",2367,3860531,0,"<p>Try this </p>

<pre><code>DELETE t1,t2 FROM t1
        INNER JOIN
    t2 ON t2.ref = t1.id 
WHERE
    t1.id = 1;
</code></pre>

<p>I hope this will help you.</p>
"
Zipkin,51354529,51151547,3,"2018/07/16, 07:19:17",True,"2018/07/16, 07:19:17",1767,2498986,1,"<p>Yes. You have to use Brave. In fact, Spring cloud sleuth (V2) uses Brave under the hood. Check the brave web-mvc example to get started.</p>

<p><a href=""https://github.com/openzipkin/brave-webmvc-example"" rel=""nofollow noreferrer"">https://github.com/openzipkin/brave-webmvc-example</a></p>
"
Zipkin,47363721,47343439,2,"2017/11/18, 09:34:23",False,"2017/11/18, 09:34:23",8336,1773866,3,"<p>This feature is available in edgware release train. That corresponds to version 1.3.x of sleuth </p>
"
Zipkin,63969131,63489826,0,"2020/09/19, 16:03:50",True,"2020/09/19, 16:03:50",4433,243104,3,"<p>Getting a handle on the distributed tracing space can be a bit confusing. Here's a quick summary...</p>
<p><strong>Open Source Tracers</strong></p>
<p>There are a number of popular open source tracers, which is where Zipkin sits:</p>
<ul>
<li>Zipkin</li>
<li>Jaeger</li>
<li>Haystack</li>
</ul>
<p><strong>Commercial Tracers</strong></p>
<p>There are also a lot of vendors offering commercial monitoring/observability tools which are either centred around or include distributed tracing:</p>
<ul>
<li>Appdynamics</li>
<li>AWS X-Ray</li>
<li>Azure Application Insights</li>
<li>Datadog</li>
<li>Dynatrace</li>
<li>Google Cloud Trace</li>
<li>Honeycomb</li>
<li>Lightstep</li>
<li>New Relic</li>
<li>SignalFX</li>
<li>(probably 100 more...)</li>
</ul>
<p><strong>Standardisation Efforts</strong></p>
<p>Alongside all these products are numerous attempts at creating standards around distributed tracing. These typically start by creating a standard API for the trace-recording side of the architecture, and sometimes extend to become prescriptive about the content of traces or even the wire format. This is where OpenTracing fits in. So it is not a tracing solution itself, but an API that can be implemented by the trace recording SDKs of multiple tracers, allowing you to swap between vendors more easily. The most common standards are:</p>
<ul>
<li>OpenTracing</li>
<li>OpenCensus</li>
<li>OpenTelemetry</li>
</ul>
<p>Note that the first two in the list have been abandoned, with their contributors joining forces to create the third one together.[1]</p>
<p>[1] <a href=""https://opensource.googleblog.com/2019/05/opentelemetry-merger-of-opencensus-and.html"" rel=""nofollow noreferrer"">https://opensource.googleblog.com/2019/05/opentelemetry-merger-of-opencensus-and.html</a></p>
"
Zipkin,61401330,61368689,0,"2020/04/24, 07:46:24",True,"2020/04/24, 07:46:24",21,11657025,1,"<p>The reason behind error was that i forgot to add the kafka dependency in pom.xml
After adding the dependency, error was gone.</p>
"
Zipkin,51042639,51040315,0,"2018/06/26, 15:11:31",True,"2018/06/26, 15:11:31",1273,4035426,1,"<p>Try to change all properties with this:</p>

<pre><code>#Sleuth
spring.sleuth.sampler.percentage=1.0
#Zipkin
spring.zipkin.sender.type=web
</code></pre>

<p>spring.sleuth.sampler.percentage=1.0 is Edgware
so you need that</p>

<p>baseUrl by default is localhost</p>
"
Zipkin,60081185,55001316,0,"2020/02/05, 19:34:06",True,"2020/02/05, 19:34:06",524,746528,1,"<p>For basic authentication, the username and password are required to be sent as part of the HTTP Header <code>Authorization</code>. The header value is computed as Base64 encoding of the string <code>username:password</code>.So if the username is <code>abcd</code> and password is <code>1234</code>, the header will look something like this (Chatset used: UTF-8).</p>

<blockquote>
  <p>Authorization: Basic YWJjZDoxMjM0</p>
</blockquote>

<p>Sleuth cloud project provides <code>ZipkinRestTemplateCustomizer</code> to configure the <code>RestTemplate</code> used to communicate with the Zipkin server.</p>

<p>Refer to the documentation for the same: 
<a href=""https://cloud.spring.io/spring-cloud-sleuth/reference/html/#sending-spans-to-zipkin"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/reference/html/#sending-spans-to-zipkin</a></p>

<p>Note: Base64 encoding is reversible and hence Basic auth credentials are not secured. HTTPS communication should be used along with Basic Authentication. </p>
"
Zipkin,51073739,50753384,0,"2018/06/28, 05:12:27",True,"2018/06/28, 05:12:27",1728,5254103,0,"<p>When I change the project root log level to ""debug"", I saw some error report from zipkin. Then I realized that the zipkin server I used was very very old. And the zipkin API call returned 404.</p>

<p>When I updated my zipkin server to latest version. It worked.</p>
"
Zipkin,50539945,50533178,1,"2018/05/26, 09:21:55",True,"2018/05/26, 09:21:55",8336,1773866,1,"<blockquote>
  <p>However, I'd also like to send log messages to Zipkin (either as new spans or annotations to existing spans). If I use org.slf4j.Logger to simply LOG.info(""something""), I see the INFO message in console output, with the exportable flag set to true:</p>
</blockquote>

<p>You can't send logs to Zipkin. You can send log statements to ELK. You can check out the sample <a href=""https://github.com/marcingrzejszczak/vagrant-elk-box"" rel=""nofollow noreferrer"">https://github.com/marcingrzejszczak/vagrant-elk-box</a> that has a vagrant box with ELK, uses Sleuth for log correlation and uses ELK to visualize the logs</p>
"
Zipkin,50527821,50506432,0,"2018/05/25, 14:08:47",False,"2018/05/25, 14:08:47",201,7956609,1,"<p>To log only request with particular error you can add the log in your exception mapper where you are handling those error. </p>

<p>To show the log for error response you can set like below,</p>

<pre><code>    @Autowired
    private Tracer tracer;
</code></pre>

<p>and set </p>

<pre><code>    tracer.addTag(""error"",""Your message"")
</code></pre>
"
Zipkin,66499651,63385866,0,"2021/03/05, 23:09:02",True,"2021/03/05, 23:09:02",1301,1423685,0,"<p>I was using it with default storage which is discouraged in production use, it can handle only small amount of data and can be treated only as a demo version.</p>
<p>What helped a little was setting</p>
<p><code>spring.sleuth.sampler.probability: 0.01 </code></p>
<p>-- by default it logs all spans.</p>
"
Zipkin,37867634,37842256,0,"2016/06/16, 22:15:19",True,"2016/06/16, 22:15:19",911,1822028,1,"<p>It seems that Brave does not support this. An issue has been reported on their GitHub page. <a href=""https://github.com/openzipkin/brave/issues/166"" rel=""nofollow"">https://github.com/openzipkin/brave/issues/166</a></p>
"
Zipkin,63034460,61208227,0,"2020/07/22, 15:42:19",False,"2020/07/22, 15:42:19",46,10967262,0,"<p>According to Sleuth documentation, AWS SQS is &quot;natively&quot; supported only on the consumer's side:</p>
<p><a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current-SNAPSHOT/reference/html/#spring-cloud-aws-messaging-sqs"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/current-SNAPSHOT/reference/html/#spring-cloud-aws-messaging-sqs</a></p>
<p>In order to add seamless tracing over AWS SQS I resorted to Brave SQS instrumentation (aka SqsMessageTracing) and had to add another dependency:</p>
<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;io.zipkin.aws&lt;/groupId&gt;
      &lt;artifactId&gt;brave-instrumentation-aws-java-sdk-sqs&lt;/artifactId&gt;
      &lt;version&gt;0.21.2&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<p>and have the following configuration:</p>
<pre><code>@Configuration
public class SQSConfig {

  @Autowired
  private Tracing tracing;

  @Autowired
  private AWSCredentialsProvider awsCredentialsProvider;

  @Autowired
  private RegionProvider regionProvider;

  @Bean
  public AmazonSQSAsync amazonSQS() {
    SqsMessageTracing sqsMessageTracing = SqsMessageTracing.create(tracing);

    return AmazonSQSAsyncClientBuilder.standard()
      .withRegion(regionProvider.getRegion().getName())
      .withCredentials(awsCredentialsProvider)
      .withRequestHandlers(sqsMessageTracing.requestHandler())
      .build();
  }

  @Bean
  public QueueMessagingTemplate queueMessagingTemplate(AmazonSQSAsync sqsClient) {
    QueueMessagingTemplate template = new QueueMessagingTemplate(sqsClient);
    template.setMessageConverter(getMappingJackson2MessageConverter());
    return template;
  }
}
</code></pre>
<p>This is just because I didn't want to do the SQS producer instrumentation myself nor add the tracing headers programmatically.
Little reference for the Brave instrumentation can be found here:</p>
<p><a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/1550#issuecomment-589686583"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/1550#issuecomment-589686583</a></p>
<p>My SQS message producer looks like this:</p>
<pre><code>@Component
public class MessageAdapter {

  @Autowired
  private final QueueMessagingTemplate queueMessagingTemplate;


  public void sendSqsMessage(Object payload) {
    HashMap&lt;String, Object&gt; headers = new HashMap&lt;&gt;();
    headers.put(&quot;message-group-id&quot;, UUID.randomUUID().toString());
    queueMessagingTemplate.convertAndSend(&quot;sqs-queue-1&quot;, payload, headers);
  }
}
</code></pre>
<p><strong>FINAL NOTE</strong></p>
<p>Not required but I also excluded the</p>
<pre><code>org.springframework.cloud.aws.autoconfigure.context.ContextInstanceDataAutoConfiguration
</code></pre>
<p>Since it performs an AWS environment configuration scan at application startup which wasn't required for me (and also raised a lengthy error log)</p>
<pre><code>@SpringBootApplication
// Unwanted autoconfiguration, which raises a lengthy warning at startup,
// brought in by Brave AWS SQS instrumentation
@EnableAutoConfiguration(exclude = ContextInstanceDataAutoConfiguration.class)
public class MainSpringBootApplication {

  public static void main(String[] args) {
    SpringApplication.run(MainSpringBootApplication.class);
  }

}
</code></pre>
"
Zipkin,63672504,63073886,0,"2020/08/31, 17:15:00",False,"2020/08/31, 17:15:00",29,9544181,1,"<p>You should create application.properties file and after that you should add the following</p>
<p>Your application.properties :</p>
<pre><code>server.port:9411 // Not Required
management.metrics.web.server.auto-time-requests=false //Required
</code></pre>
<p>Your main class :</p>
<pre><code>import zipkin.server.EnableZipkinServer;

@EnableZipkinServer
@SpringBootApplication
public class ZipkinServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZipkinServerApplication.class, args);

    }

}
</code></pre>
<p>Your Pom.xml :</p>
<pre><code> &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;

    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.7.RELEASE&lt;/version&gt;
        &lt;relativePath /&gt;
    &lt;/parent&gt;

    &lt;dependencies&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
            &lt;version&gt;2.11.7&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
            &lt;version&gt;2.11.7&lt;/version&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>
<p>Your zipkin port :</p>
<pre><code>http://localhost:9411/zipkin
</code></pre>
"
Zipkin,49407790,49396204,0,"2018/03/21, 15:35:06",False,"2018/03/21, 15:35:06",11,7472851,0,"<p>I found the solution I think. I changed it to this:</p>

<p>RABBIT_URI=amqp://user:password@tracing:5672</p>
"
Zipkin,46289584,46287877,0,"2017/09/19, 02:30:27",False,"2017/09/19, 02:30:27",81,2232476,0,"<p>The Zipkin UI is making an AJAX request to the API in order to retrieve the data that is displayed. You can find the API definition for zipkin at:</p>

<p><a href=""https://github.com/openzipkin/zipkin-api"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin-api</a></p>

<p>I believe you are looking for the URL: <a href=""http://zipkin.iamplus.xyz/api/v1/traces"" rel=""nofollow noreferrer"">http://zipkin.iamplus.xyz/api/v1/traces</a></p>

<p>From there you will get the traces matching your filter</p>
"
Zipkin,52585202,52550644,0,"2018/10/01, 08:47:20",True,"2018/10/01, 08:47:20",568,5238035,2,"<p>For those who could come across with a same scenario like this,</p>

<p><strong>github</strong> has given <strong>APIs</strong> to get details on the repository tag set of each project release as a json object (<a href=""https://api.github.com/repos/openzipkin/zipkin/tags"" rel=""nofollow noreferrer"">https://api.github.com/repos/openzipkin/zipkin/tags</a> ). So that can be used to get the latest version of zipkin.</p>

<p>To get the currently running version of my system, zipkin has given an actuator/info end point (<a href=""http://localhost:9411/actuator/info"" rel=""nofollow noreferrer"">http://localhost:9411/actuator/info</a>).</p>
"
Zipkin,41723986,40701663,3,"2017/01/18, 18:04:16",True,"2017/01/18, 18:04:16",8336,1773866,4,"<p>Spring Cloud Zipkin Stream is using Spring Cloud Stream underneath. You need to provide how do you want to send the spans to Zipkin - thus you need a binder. One possible binder is the RabbitMQ binder. Check out this: <a href=""https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/zipkin-server/build.gradle#L6"" rel=""nofollow noreferrer"">https://github.com/spring-cloud-samples/sleuth-documentation-apps/blob/master/zipkin-server/build.gradle#L6</a></p>
"
Zipkin,63461779,54986635,0,"2020/08/18, 07:12:18",False,"2020/08/18, 07:12:18",3,12639023,0,"<p>I got same problem. Seem Spring boot <strong>2.1.2.RELEASE</strong> not work with Zipkin. Please upgrade to Spring boot version &gt; <strong>2.1.2.RELEASE</strong>.</p>
"
Zipkin,57911578,57892994,0,"2019/09/12, 20:10:36",True,"2019/09/12, 20:10:36",99,7474991,2,"<p>ok I finally realized whats the mistake that I have done when I was starting my zipkin server with this command</p>

<pre><code> java -jar zipkin-server-2.16.2-exec.jar
</code></pre>

<p>but I was not specifying my Zipkin server where my Kafka is running so when I did </p>

<pre><code> set KAFKA_BOOTSTRAP_SERVERS=localhost:9092
 java -jar zipkin-server-2.16.2-exec.jar 
</code></pre>

<p>it worked </p>
"
Zipkin,43843859,43790619,0,"2017/05/08, 12:12:55",True,"2017/05/08, 12:12:55",8336,1773866,1,"<p>The issue got fixed - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/585"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/585</a> . In the upcoming releases 1.1.5 and 1.2.1 it should work</p>
"
Zipkin,48628487,48626548,8,"2018/02/05, 19:47:34",True,"2018/02/05, 19:47:34",8336,1773866,0,"<p>Please use latest snapshots. Sleuth in latest snapshots uses brave internally so integration will be extremely simple.</p>
"
Zipkin,48432603,46561079,0,"2018/01/25, 00:16:25",True,"2019/12/10, 06:06:47",471,2673284,2,"<p>The Downloads page (<a href=""https://www.jaegertracing.io/download/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/download/</a>) lists both the Docker images and the raw binaries built for various platforms (Linux, macOS, windows). You can also build binaries from source.</p>
"
Zipkin,66277779,46561079,0,"2021/02/19, 14:43:21",False,"2021/02/19, 14:43:21",169,3301685,0,"<p>Just to add to Yuris answer, you can also download the source from github - <a href=""https://test"" rel=""nofollow noreferrer"">Github - Jaeger</a> This is useful for diagnosing issues, or just getting a better understanding of how it all works.</p>
<p>I have run both the released apps and custom versions on both windows and linux servers without issues. For windows I would recommend running as a service using Nssm. <a href=""https://nssm.cc/usage"" rel=""nofollow noreferrer"">Nssm details</a></p>
"
Zipkin,45632418,45492904,0,"2017/08/11, 13:02:33",False,"2017/08/11, 13:02:33",86,7146447,0,"<p>I have the same config running on my ingress 9.0-beta.11. I guess it's just a misconfiguration.</p>

<p>First I'll recommend you to not change the template and use the default values and just change when the basic-auth works.</p>

<p>What the logs of ingress show to you? Did you create the basic-auth file in the same namespace of the ingress resource?</p>
"
Zipkin,66808417,66777772,2,"2021/03/26, 00:23:10",False,"2021/03/26, 00:23:10",46,14172753,0,"<p>I'm not entirely sure if that's what you mean, but you can use Jeager <a href=""https://www.jaegertracing.io/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/</a>  which checks if trace-id already exist in the invocation metadata and in it generate child trace id. Based on all trace ids call diagrams are generated</p>
"
Zipkin,66064768,66064081,3,"2021/02/05, 16:09:01",True,"2021/02/05, 18:51:05",7249,8340997,1,"<p>According to <a href=""https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready-process-monitoring-programmatically"" rel=""nofollow noreferrer"">Spring Boot Reference Docs</a> :</p>
<p>To enable <code>/httptrace</code> in the actuator, then you have to create a bean of  <code>InMemoryHttpTraceRepository</code> class in the custom <code>@Configuration</code> class which provides the trace of the request and response.</p>
<pre><code>@Bean
public HttpTraceRepository htttpTraceRepository() {
  return new InMemoryHttpTraceRepository();
}
</code></pre>
<p>To enable <code>/auditevents</code> in the actuator, then you have to create a bean of <code>InMemoryAuditEventRepository</code> class in the custom <code>@Configuration</code> class which exposes audit events information.</p>
<pre><code>@Bean
public AuditEventRepository auditEventRepository() {
  return new InMemoryAuditEventRepository();
}
</code></pre>
<p>To enable <code>/integrationgraph</code> in actuator, you have to add <code>spring-integration-core dependency</code> in the pom.xml (as per documentation) :</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.integration&lt;/groupId&gt;
    &lt;artifactId&gt;spring-integration-core&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>
<p>or if you are having a spring-boot project, then add this :</p>
<pre><code>  &lt;dependency&gt;
      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
      &lt;artifactId&gt;spring-boot-starter-integration&lt;/artifactId&gt;
  &lt;/dependency&gt;
</code></pre>
<p><code>/actuator/sessions</code> are by-default enabled. But still you can add this explicitly to check the behaviour.</p>
<p>Add this in application.properties.</p>
<pre><code>management.endpoint.sessions.enabled = true
</code></pre>
"
Zipkin,64439121,64434436,0,"2020/10/20, 08:42:11",True,"2020/10/20, 08:42:11",66,12105655,1,"<p>Thanks Jorg Heymans for the question.
Yeah, it's a bug and should be fixed by <a href=""https://github.com/line/armeria/pull/3120"" rel=""nofollow noreferrer"">https://github.com/line/armeria/pull/3120</a>
Thank you!</p>
"
Zipkin,64043957,64040747,1,"2020/09/24, 12:53:37",False,"2020/09/24, 12:53:37",1554,3719412,1,"<p>Ribbon is a client side load balancer which means there is no any other hop in between your client and service. Basically you keep and maintain a list of service on your client.</p>
<p>In AWS load balancer case you need to make another hop in between the client and server.</p>
<p>Both have advanges and disadvantages. Former has the advantage of not having any dependency to any specific external solution. Basically with ribbon and service discovery like eureka you can deploy your product to any cloud provider or on-premise setup without additional effort. Latter has advantage of not needing an extra component of service discovery or keeping the cache of service list on client. But it has that additional hop which might be an issue if you are trying to run an very high-load system.</p>
<p>Although I don't have much experience with AWS CloudWatch what I know is it helps you to collect logs to a central place from different AWS components. And that is what you are trying to do with your solution.</p>
"
Zipkin,60258813,60258661,0,"2020/02/17, 10:53:24",False,"2020/02/17, 10:53:24",159,4502707,3,"<blockquote>
  <p>kubectl exec -it ""pod-name"" -c ""container-name"" -n ""namespace""</p>
</blockquote>

<p>Here only the container name is needed. In your case it will be:</p>

<blockquote>
  <p>kubectl exec -it my-api-XXX -c my-api  -- /bin/bash</p>
</blockquote>

<p>You can exec to Zipkin because <code>exec</code> is taking zipkin as the default container.</p>
"
Zipkin,56657881,56655528,0,"2019/06/19, 01:35:50",False,"2019/06/19, 01:35:50",694,3800106,0,"<p>It is solved now; all I had to do was port forwarding.</p>

<pre><code>kubectl port-forward zipkin-774cc77659-g929n 9411:9411
</code></pre>

<p>Thanks,</p>
"
Zipkin,56734998,56655528,0,"2019/06/24, 13:49:27",False,"2019/06/24, 13:49:27",2049,11207414,0,"<p>By default you service is exposed as <strong>ClusterIP</strong>, in this case your service will be accessible from within your cluster.</p>

<p>You can use port forwarding ""<strong>With this connection in place you can use your local workstation to debug your application that is running in the pod</strong>"" as described in the answer above.</p>

<p>Another approach is to use other <em>""service types""</em> like <strong>NodePort</strong>.</p>

<p>You can find more information here <a href=""https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types"" rel=""nofollow noreferrer"">Publishing services (ServiceTypes)</a></p>
"
Zipkin,55274713,55274572,0,"2019/03/21, 08:15:19",False,"2019/03/21, 08:15:19",3915,837717,2,"<p>Sleuth will do the same for messaging by using message headers to propagate <em>span id, trace id</em> and other relevant information. It does so by registering special channel interceptor.</p>
"
Zipkin,54914623,54845174,0,"2019/02/27, 23:15:56",True,"2019/02/27, 23:15:56",504,9187876,1,"<p>The configuration you're referring to is for the instrumentation of messaging systems, not for sending traces to zipkin using a messaging system.</p>

<p>You should look at this <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin2/ZipkinAutoConfiguration.java"" rel=""nofollow noreferrer"">auto-configuration</a>, and especially this <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin2/sender/ZipkinRabbitSenderConfiguration.java"" rel=""nofollow noreferrer"">sender config</a>.</p>

<p>What you want to do has also been documented here: <a href=""https://cloud.spring.io/spring-cloud-sleuth/2.0.x/single/spring-cloud-sleuth.html#_sleuth_with_zipkin_over_rabbitmq_or_kafka"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-sleuth/2.0.x/single/spring-cloud-sleuth.html#_sleuth_with_zipkin_over_rabbitmq_or_kafka</a></p>

<p>You should only need to add <code>spring-cloud-starter-zipkin</code> and <code>spring-rabbit</code> to your dependencies. If you want to change the default queue (which is <code>zipkin</code>), then you'll need to add <code>spring.zipkin.rabbitmq.queue</code> to your properties.</p>
"
Zipkin,53655726,53653606,3,"2018/12/06, 18:27:44",False,"2018/12/07, 23:02:30",81,2232476,0,"<p>You will need your own <code>PropagationFactory</code> implementation. Here is the default one: <a href=""https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/B3Propagation.java"" rel=""nofollow noreferrer"">https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/B3Propagation.java</a></p>

<p>You can create a bean and sleuth should use that instead of this one.</p>

<p>More specifically you will need an implementation with a custom <code>TraceContext.Extractor&lt;C&gt;</code> implementation. This can then pull the trace ID from your header, and add return the appropriate <code>TraceContext</code>. Then it can pass it along using the normal headers. If you'd like to use the same correlation header when sending downstream then you will also have to implement <code>TraceContext.Injector&lt;C&gt;</code>.</p>
"
Zipkin,53184979,53174880,3,"2018/11/07, 09:16:13",True,"2018/11/07, 09:16:13",8336,1773866,1,"<p>The option is to disable Slf4j integration as you mentioned. When a new span / scope is created, we go through Slf4j to put data in MDC and it takes time unfortunately. Disabling that will save it.</p>
"
Zipkin,51526235,51526164,6,"2018/07/25, 22:21:47",True,"2018/07/26, 03:24:14",91301,2756547,0,"<p>This is indeed possible with the mentioned <code>executor channel</code>. All you recipient flows must really start from the <code>ExecutorChannel</code>. In your case you have to modify all of them to something like this:</p>

<pre><code>public IntegrationFlow flow2() {
    return IntegrationFlows.from(MessageChannels.executor(taskExexecutor()))
            .enrichHeaders(
                    h -&gt; h.header(TransportConstants.HEADER_CONTENT_TYPE, MediaType.APPLICATION_XML_VALUE))
            .transform(ele -&gt; createRequest1(ele))                  
            .wireTap(""asyncXMLLogging"")
            .handle(wsGateway.applyAsHandler(endpoint1))
            .transform(
                    ele -&gt; response2(ele))
            .get();
}
</code></pre>

<p>Pay attention to the <code>IntegrationFlows.from(MessageChannels.executor(taskExexecutor()))</code>. That's exactly how you can make each sub-flow async.</p>

<p><strong>UPDATE</strong></p>

<p>For the older Spring Integration version without <code>IntegrationFlow</code> improvement for the sub-flows we can do like this:</p>

<pre><code>public IntegrationFlow flow2() {
    return integrationFlowDefinition -&gt; integrationFlowDefinition
            .channel(c -&gt; c.executor(Executors.newCachedThreadPool()))
            .enrichHeaders(
                    h -&gt; h.header(TransportConstants.HEADER_CONTENT_TYPE, MediaType.APPLICATION_XML_VALUE))
            .transform(ele -&gt; createRequest1(ele))                  
            .wireTap(""asyncXMLLogging"")
            .handle(wsGateway.applyAsHandler(endpoint1))
            .transform(
                    ele -&gt; response2(ele));
}
</code></pre>

<p>This is similar to what you show in the comment above.</p>
"
Zipkin,51442735,51442393,1,"2018/07/20, 15:38:54",False,"2018/07/20, 15:38:54",51827,1259109,0,"<p>Works for me with 1.4.0.RELEASE (2.0.0.RELEASE isn't out yet, but should be soon). You probably have a bad jar file in your local maven cache (e.g. the one that it complains about).</p>
"
Zipkin,49357777,49326587,1,"2018/03/19, 09:31:43",False,"2018/03/19, 09:31:43",8336,1773866,0,"<p>You have to provide a different logging pattern to make it work with PCF Metrics AFAIR. You need the parent span to be present in logs. Set the property <code>logging.pattern.level: ""%clr(%5p) %clr([${spring.application.name:-},%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-B3-ParentSpanId:-},%X{X-Span-Export:-}]){yellow}""</code>. Check this example: <a href=""https://github.com/pivotal-cf/pcf-metrics-trace-example-spring"" rel=""nofollow noreferrer"">https://github.com/pivotal-cf/pcf-metrics-trace-example-spring</a> </p>
"
Zipkin,49903075,49326587,1,"2018/04/18, 18:02:24",False,"2018/04/18, 18:02:24",1,9665105,0,"<p>PCF metrics does  not support custom spans, it only shows the respomse time distribution span that corresponds to http request routed by goRouter. </p>
"
Zipkin,49066640,48940831,1,"2018/03/02, 11:51:31",True,"2018/03/02, 11:51:31",8336,1773866,1,"<p>It was a bug that got fixed with this commit - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/commit/d7a0747907f4ab7201f67e7d0c762a324fbe0668"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/commit/d7a0747907f4ab7201f67e7d0c762a324fbe0668</a> . Please check out the latest snapshots</p>
"
Zipkin,48762766,48756959,9,"2018/02/13, 10:51:34",True,"2018/02/13, 21:29:25",8336,1773866,2,"<p>Why are you setting the values of dependencies manually? Please use the Edgware.SR2 BOM. You have to add the kafka dependency, ensure that rabbit is not on the classpath. If you have both kafka and rabbit on the classpath you need to set the <code>spring.zipkin.sender.type=kafka</code></p>

<p>UPDATE:</p>

<p>As we describe in the documentation, the Sleuth Stream support is deprecated in Edgware and removed in FInchley. If you've decided to go with the new approach of using native Zipkin messaging support, then you have to use the Zipkin Server with Kafka as described here <a href=""https://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/tree/master/zipkin-autoconfigure/collector-kafka10</a> .  Let me copy part of the docs here</p>

<hr>

<h2>Configuration</h2>

<p>The following configuration points apply apply when <code>KAFKA_BOOTSTRAP_SERVERS</code> or
<code>zipkin.collector.kafka.bootstrap-servers</code> is set. They can be configured by setting an environment
variable or by setting a java system property using the <code>-Dproperty.name=value</code> command line
argument. Some settings correspond to ""New Consumer Configs"" in
<a href=""https://kafka.apache.org/documentation/#newconsumerconfigs"" rel=""nofollow noreferrer"">Kafka documentation</a>.</p>

<p>Environment Variable | Property | New Consumer Config | Description</p>

<p><code>KAFKA_BOOTSTRAP_SERVERS</code> | <code>zipkin.collector.kafka.bootstrap-servers</code> | bootstrap.servers | Comma-separated list of brokers, ex. 127.0.0.1:9092. No default</p>

<p><code>KAFKA_GROUP_ID</code> | <code>zipkin.collector.kafka.group-id</code> | group.id | The consumer group this process is consuming on behalf of. Defaults to <code>zipkin</code></p>

<p><code>KAFKA_TOPIC</code> | <code>zipkin.collector.kafka.topic</code> | N/A | Comma-separated list of topics that zipkin spans will be consumed from. Defaults to <code>zipkin</code></p>

<p><code>KAFKA_STREAMS</code> | <code>zipkin.collector.kafka.streams</code> | N/A | Count of threads consuming the topic. Defaults to <code>1</code></p>
"
Zipkin,51743845,48639074,0,"2018/08/08, 13:00:10",False,"2018/08/08, 13:00:10",26,5772061,0,"<p>OpenStack does not have Zipkin as an inbuilt tracer. Hence OSProfiler was adopted as a standard project for tracing in OpenStack.</p>

<p>As far as i can see from the documentation, Nova should have OSProfiler support for Mitaka. Although i have not used OSProfiler with Mitaka, I have worked with OSProfiler with Newton and subsequent releases.</p>

<p>You can post the issue that you are facing so that it will be easier to debug.</p>
"
Zipkin,48216934,48216192,0,"2018/01/12, 00:38:24",True,"2018/01/12, 00:38:24",8336,1773866,3,"<p>If you're using Edgware release train, just set <code>spring.zipkin.sender.type=web</code>. That way you force the HTTP based span sending</p>
"
Zipkin,48170238,48169950,2,"2018/01/09, 16:22:14",True,"2018/01/09, 16:22:14",129048,1240763,1,"<p>Embedded headers are not pluggable, but you can disable them with <code>...producer.header-mode=raw</code>.</p>

<p>With Ditmars (1.3.x) you can use the kafka11 artifact, which supports native headers - you have to override a bunch of dependencies (kafka-clients, SK, SIK and kafka itself if you are using the <code>KafkaEmbedded</code> broker for testing. See <a href=""https://github.com/spring-cloud/spring-cloud-stream-starters/releases/tag/vDitmars.SR2"" rel=""nofollow noreferrer"">the relesae notes</a>).</p>

<p>There's <a href=""https://gitter.im/spring-cloud/spring-cloud-stream?at=5a42e02ce43a7a150cabb613"" rel=""nofollow noreferrer"">a discussion on Gitter</a> about overriding the versions.</p>

<p>spring-kafka 1.3.x natively uses 0.11 but 1.3.1 and higher (1.3.2 is current) also supports the 1.0.0 client.</p>

<p>Elmhurst (2.0) - currently in milestones - uses SK 2.1.0 which natively uses 1.0.0 kafka.</p>
"
Zipkin,47925684,47925555,1,"2017/12/21, 15:07:02",False,"2017/12/21, 15:07:02",28734,1153938,1,"<p>We implemented this on our microservices platform</p>

<ul>
<li>incoming requests get an id</li>
<li>this id persists through all calls</li>
<li>data is logged from each microservice with the id attached</li>
<li>by knowing which services call which the latency can be observed by looking at the requests filtered by id</li>
</ul>

<p>A lot of the logging is done by pushing requests onto a RabbitMQ queue and then getting logstash to consume that.</p>

<p>Other data is obtained via filebeat transmitting the logs to logstash</p>

<p>Both the logs and the RabbitMQ data has the id attached so can be correlated</p>

<p>An alternative approach would be to build instrumentation into each microservice that specifically monitored latency and then record that directly into logstash</p>

<p>You might like to read <a href=""https://medium.com/devopslinks/how-to-monitor-the-sre-golden-signals-1391cadc7524"" rel=""nofollow noreferrer"">https://medium.com/devopslinks/how-to-monitor-the-sre-golden-signals-1391cadc7524</a> for a general guide to essential monitoring that is applicable to microservices</p>
"
Zipkin,46489881,46479182,0,"2017/09/29, 16:30:12",False,"2017/09/29, 16:30:12",691,1029971,1,"<p>I figured out how to disable the bean that was injecting LogbackAccess.  This resolved the issue so that Zipkin is now accepting requests.</p>
"
Zipkin,46188800,46185813,1,"2017/09/13, 07:21:00",True,"2017/09/13, 07:21:00",12382,707451,1,"<p>To log internal HTTP request sent from a Node.js server, you can create a Proxy Node.js server and log all requests there using <a href=""https://www.npmjs.com/package/morgan"" rel=""nofollow noreferrer"">Morgan</a>.</p>

<p>First, define 3 constants (or read from your project config file):</p>

<pre><code>// The real API endpoint, such as ""another micro-service"" in your network
const API = http://&lt;real_server&gt;
// Proxy Node.js server running on localhost
const LOGGER_ENDPOINT=http://localhost:3010
// Flag, decide whether logger is enabled.
const ENABLE_LOGGER=true
</code></pre>

<p>Second, When your Node.js server is launched, start the logger server at the same time if <code>ENABLE_LOGGER</code> is true. The logger server only do one thing: log the request and forward it to the real API server using <code>request</code> module. You can use <a href=""https://www.npmjs.com/package/morgan"" rel=""nofollow noreferrer"">Morgan</a> to provide more readable format.</p>

<pre><code>const request = require('request');
const morgan = require('morgan')(':method :url :status Cookie: :req[Cookie] :res[content-length] - :response-time ms');
...
if (ENABLE_LOGGER &amp;&amp; LOGGER_ENDPOINT) {
  let loggerPort = 3010;
  const logger = http.createServer((req, res) =&gt; {
    morgan(req, res, () =&gt; {
      req.pipe(request(API + req.url)).pipe(res);
    });
  });
  logger.listen(loggerPort);
}
</code></pre>

<p>Third, in your Node.js server, send API request to logger server when <code>ENABLE_LOGGER</code> is true, and send API directly to the real server when <code>ENABLE_LOGGER</code> is false.</p>

<pre><code>let app = express(); // assume Express is used, but this strategy can be easily applied to other Node.js web framework.
...
let API_Endpoint = ENABLE_LOGGER ? LOGGER_ENDPOINT : API;
app.set('API', API_Endpoint);
...
// When HTTP request is sent internally
request(app.get('API') + '/some-url')... 
</code></pre>
"
Zipkin,44763002,44762961,4,"2017/06/26, 18:19:01",True,"2017/06/26, 18:19:01",8336,1773866,1,"<p>No - we haven't added any instrumentation around Webservicetemplate. You'd have to add an interceptor similar to the one we add for RestTemplate. You'd have to pass all the tracing headers to the request so that the other side can properly parse it.</p>
"
Zipkin,41045080,41040727,0,"2016/12/08, 19:11:10",True,"2016/12/08, 19:11:10",1825,499395,3,"<p>We have an internal OkHttpClient wrapper implementing Call.Factory which adds an initial interceptor: </p>

<pre><code>public class HttpClient implements Call.Factory {

private final OkHttpClient ok;

HttpClient(final OkHttpClient ok) {
    this.ok = ok;
}

/**
 * Implements Call.Factory, uses implicit (thread local) Ctx
 */
public Call newCall(final Request request) {
    return newCall(Ctx.fromThread().orElseGet(Ctx::empty), request);
}


public Call newCall(Ctx ctx, final Request request) {
    OkHttpClient.Builder b = this.ok.newBuilder();
    b.interceptors().add(0, new CtxInterceptor(ctx));
    return b.build().newCall(request);
}
</code></pre>

<p>to solve this problem. It is not transparent, however, so may not be good for Brave. It works fine, because in practice once a client is configured, you only really use the <code>Call.Factory</code> interface :-)</p>

<p>The <code>Ctx</code> bit is just the context with stuff we want to propagate, we can do it implicit or explicit, hence the extra method to explicitly take it.</p>
"
Zipkin,40895971,40694955,0,"2016/11/30, 20:49:26",True,"2016/11/30, 20:49:26",3732,560814,2,"<p>Thanks for trying out HTrace!  Sorry that the version issue is such a pain right now.</p>

<p>It is much easier to configure HTrace with the version in cloudera's CDH5.5 distribution of Hadoop and later.  There is a good description of how to do it here: <a href=""http://blog.cloudera.com/blog/2015/12/new-in-cloudera-labs-apache-htrace-incubating/"" rel=""nofollow noreferrer"">http://blog.cloudera.com/blog/2015/12/new-in-cloudera-labs-apache-htrace-incubating/</a>  If you want to stick with an Apache release of the source code rather than a vendor release, try Hadoop 3.0.0-alpha1. <a href=""http://hadoop.apache.org/releases.html"" rel=""nofollow noreferrer"">http://hadoop.apache.org/releases.html</a></p>

<p>The HTrace libraries shippped in Hadoop 2.6 and 2.7 are very old... we never backported HTrace 4.x to those branches.  They were stability branches, so new features like tracing was out of scope.  There is some functionality there, but not much.  I recommend using the newer HTrace 4.x library which is actively developed.  The HTrace 4.x branch also has a stable API, so hopefully breakage will be minimized in the future.</p>
"
Zipkin,42159646,40694955,0,"2017/02/10, 14:39:46",False,"2017/02/10, 16:03:38",9,7545887,0,"<p>Exactly, in the code, I see the configuration key's prefix is <code>dfs.htrace</code>, not the <code>hadoop.htrace</code>.  And in dfsclient, it's <code>dfs.client.htrace</code>. You can change the prefix to <code>dfs.htrace</code>, then restart the cluster and it take effect. The code is in class <code>org.apache.hadoop.tracing.SpanReceiverHost</code>. Hope this help!</p>
"
Zipkin,40534423,40525453,4,"2016/11/10, 20:16:52",False,"2016/11/10, 20:16:52",8336,1773866,0,"<p>The sampling decision is taken for a trace. That means that when the first request comes in and the span is created you have to take a decision. You don't have any tags / baggage at that point so you must not depend on the contents of tags to take this decision. That's a wrong approach.</p>

<p>You are taking a very custom approach. If you want to go that way (which is not recommended) you can create a custom implementation of a <code>SpanReporter</code> - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/SpanReporter.java#L30"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/SpanReporter.java#L30</a> . <code>SpanReporter</code> is the one that is sending spans to zipkin. You can create an implementation that will wrap an existing <code>SpanReporter</code> implementation and will delegate the execution to it only when some values of tags match. But from my perspective it doesn't sound right.</p>
"
Zipkin,40151801,39862738,0,"2016/10/20, 13:26:02",False,"2016/10/20, 13:26:02",8336,1773866,5,"<p>If I'm not mistaken (and I guess I'm not) no wonder that you're not sending the Spans to Zipkin cause you didn't add the Zipkin dependency. Check the <em>Sleuth with Zipkin via HTTP</em> section of the docs: <a href=""http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html"" rel=""noreferrer"">http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html</a> .  </p>

<pre><code>dependencyManagement {
    imports {
        mavenBom ""org.springframework.cloud:spring-cloud-dependencies:Brixton.RELEASE""
    }
}
dependencies {
    compile ""org.springframework.cloud:spring-cloud-starter-zipkin""
}
</code></pre>
"
Zipkin,42608836,39862738,1,"2017/03/05, 15:21:05",False,"2017/03/05, 15:21:05",167,3814829,0,"<p>This config worked for me in 1 of my application: </p>

<pre><code>  spring.zipkin.baseUrl = localhost:9411 
  spring.sleuth.enabled = true 
  spring.sleuth.sampler.percentage = 1.0 
</code></pre>

<p>Enabling property might do the trick!</p>
"
Zipkin,38528610,38523705,2,"2016/07/22, 17:11:03",True,"2016/07/22, 17:11:03",91301,2756547,1,"<p>First of all the main feature of Spring Integration is <code>MessageChannel</code>, but it still isn't clear to me why people are missing <code>.channel()</code> operator in between endpoint definitions.</p>

<p>I mean that for your case it must be like:</p>

<pre><code>.split(ShoppingCart.class, ShoppingCart::getLineItems)
.channel(c -&gt; c.executor(executor()))
.enrich(e -&gt; e.requestChannel(ADD_LINE_ITEM_CHANNEL))
</code></pre>

<p>Now about your particular problem.</p>

<p>Look, <code>ContentEnricher</code> (<code>.enrich()</code>) is request-reply component: <a href=""http://docs.spring.io/spring-integration/reference/html/messaging-transformation-chapter.html#payload-enricher"" rel=""nofollow"">http://docs.spring.io/spring-integration/reference/html/messaging-transformation-chapter.html#payload-enricher</a>.</p>

<p>Therefore it sends request to its <code>requestChannel</code> and waits for reply. And it is done independently of the <code>requestChannel</code> type.</p>

<p>I raw Java we can demonstrate such a behavior with this code snippet:</p>

<pre><code>for (Object item: items) {
    Data data = sendAndReceive(item);
}
</code></pre>

<p>where you should see that <code>ADD_LINE_ITEM_CHANNEL</code> as an <code>ExecutorChannel</code> doesn't have much value because we are blocked within loop for the reply anyway.</p>

<p>A <code>.split()</code> does exactly similar loop, but since by default it is with the <code>DirectChannel</code>, an iteration is done in the same thread. Therefore each next item waits for the reply for the previous.</p>

<p>That's why you definitely should parallel exactly as an input for the <code>.enrich()</code>, just after <code>.split()</code>.</p>
"
Zipkin,28202807,28187275,1,"2015/01/28, 23:32:37",True,"2015/01/28, 23:32:37",1050,748140,1,"<p>The Apache Thrift TSocketTransport (almost certainly what you are using) uses TCP on a configurable port. Cassandra usually uses port 9160 for thrift. When using Thrift/TCP no HTTP setup is necessary. Just open 9160 (and any other ports your custom thrift servers may be listening on). </p>

<p>Though you can use Thrift over HTTP, Thrift is RPC, not REST, so proxy caching will cause problems, the client needs a direct comm channel with the server.</p>
"
Zipkin,48832878,28187275,0,"2018/02/16, 20:31:16",False,"2018/02/16, 20:31:16",1620,1401124,0,"<p>If you do need to access a thrift service via a proxy, something like this would work:</p>

<p><a href=""https://github.com/totally/thrift_goodies/blob/master/transport.py"" rel=""nofollow noreferrer"">https://github.com/totally/thrift_goodies/blob/master/transport.py</a></p>

<p>You can kill the kerberos stuff if you don't need that.</p>
"
Zipkin,47431400,43913962,0,"2017/11/22, 11:49:20",False,"2017/11/22, 11:49:20",215,3702774,1,"<p>You may define all needed params via ENV options.
Here is a cmd for running zipkin in docker:</p>

<pre><code>docker run  -d -p 9411:9411 -e STORAGE_TYPE=elasticsearch -e ES_HOSTS=http://172.17.0.3:9200 -e ES_USERNAME=elastic -e ES_PASSWORD=changeme openzipkin/zipkin
</code></pre>

<p>All these params can be defined in Deployment (see <a href=""https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/"" rel=""nofollow noreferrer"">Expose Pod Information to Containers Through Environment Variables</a>)</p>
"
Zipkin,41798515,40954585,1,"2017/01/23, 04:38:41",True,"2017/01/23, 04:38:41",762,1227937,1,"<p>This was an issue with MySQL 5.7 and more recently resolved. You can try latest Zipkin.</p>
"
Zipkin,55801244,55512192,0,"2019/04/23, 00:16:47",False,"2019/04/23, 00:16:47",630,10676678,2,"<p>Application Insights users would also be able to leverage the distributed tracing offered through Zipkin by instrumenting their services using existing libraries. To use the Application Insights back-end store, configure your Zipkin server instance to use the Application Insights <a href=""https://github.com/openzipkin-attic/zipkin-azure/pulls"" rel=""nofollow noreferrer"">plug-in</a>. This integration makes monitoring and debugging your overall end-to-end applications much easier.</p>

<p>Once you have the data in Application Insights, you can always perform <a href=""https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/cross-workspace-query"" rel=""nofollow noreferrer"">cross-resource log queries</a> between Application Insights and Log Analytics.</p>

<p>Additional Documentation Reference - </p>

<p><a href=""https://github.com/openzipkin-attic/zipkin-azure/issues/33"" rel=""nofollow noreferrer"">Zipkin to Application Insights Module</a></p>

<p><a href=""https://blogs.msdn.microsoft.com/microsoftrservertigerteam/2017/05/10/introducing-zipkin-azure/"" rel=""nofollow noreferrer"">Zipkin-Azure</a></p>

<p><a href=""https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-collector-api"" rel=""nofollow noreferrer"">Send Log Data to Azure Monitor with HTTP Data Collector API (public preview)</a></p>

<p>Hope the above information helps.</p>
"
Zipkin,63198835,63198673,0,"2020/07/31, 23:41:17",True,"2020/07/31, 23:41:17",7166,3838328,0,"<p>You just need to make use of <code>tags.value</code> instead of <code>value</code> in your match query.</p>
<p>Below query should help:</p>
<pre><code>POST &lt;your_index_name&gt;/_search
{
  &quot;from&quot;: 0,
  &quot;size&quot;: 1,
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;must&quot;: [
        {
          &quot;match&quot;: {
            &quot;process.serviceName&quot;: &quot;transaction-manager&quot;
          }
        },
        {
          &quot;nested&quot;: {
            &quot;path&quot;: &quot;tags&quot;,
            &quot;query&quot;: {
              &quot;match&quot;: {
                &quot;tags.value&quot;: &quot;zipkin&quot;        &lt;---- Note this
              }
            }
          }
        }
      ]
    }
  }
}
</code></pre>
"
Zipkin,62824310,60598519,0,"2020/07/10, 00:55:28",False,"2020/07/16, 00:09:39",1,13324871,0,"<p>I had the same problem and solved it by:
Open windows task manger and kill all java instances java.exe or javaw.exe</p>
"
Zipkin,50626312,50599475,0,"2018/05/31, 17:09:32",True,"2018/05/31, 17:09:32",3141,553720,0,"<p>The Istio sidecar proxy (Envoy) generates the first headers. According to <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#x-request-id"" rel=""nofollow noreferrer"">https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers#x-request-id</a>: </p>

<blockquote>
  <p>Envoy will generate an x-request-id header for all external origin requests (the header is sanitized). It will also generate an x-request-id header for internal requests that do not already have one.</p>
</blockquote>
"
Zipkin,44636339,44611370,0,"2017/06/19, 20:13:49",False,"2017/06/19, 20:13:49",35,440061,0,"<p>*sigh, so it turns out, that someone had turned off zipking tracing in a properties file, for no good reason.
*sigh</p>
"
Zipkin,52167852,52104427,6,"2018/09/04, 16:48:04",False,"2018/09/04, 16:48:04",8336,1773866,-1,"<p>You're using <code>Camden</code> release train with boot <code>2.0</code> and Sleuth <code>2.0</code>. That's completely incompatible. Please generate a project from start.spring.io from scratch, please don't put any versions manually for spring cloud projects, and please try again. Try using <code>Finchley</code> release train instead of <code>Camden</code></p>
"
Zipkin,43401515,43394714,2,"2017/04/13, 23:52:30",True,"2017/04/13, 23:52:30",81,2232476,2,"<p>You are trying to run 2 different applications.</p>

<p>To run the <code>zipkin</code> application with with ElasticSearch and Kafka you will need to run it with both sets of environment variables:</p>

<pre><code>KAFKA_ZOOKEEPER=kafka1:2181,kafka2:2181 KAFKA_GROUP_ID=zipkin STORAGE_TYPE=elasticsearch ES_HOSTS=es5_1:9200 java -jar /opt/zipkin/bin/zipkin.jar --logging.level.zipkin=DEBUG
</code></pre>

<p>Once you have the <code>zipkin</code> server running with ES, then you can use your second command to generate the data for the dependency graph view</p>
"
Zipkin,61975479,56923548,0,"2020/05/23, 19:50:21",False,"2020/05/23, 19:50:21",83,2702332,0,"<p>There are 2 entries in mysql zipkin_spans table </p>

<ol>
<li><strong>trace_id_high</strong> -> corresponds to decimal representation of first 16 hex character </li>
<li><strong>id</strong> -> corresponds to decimal representation of lower 16 hex character</li>
</ol>

<p><strong>Example</strong></p>

<p>32 character hex trace id <strong>5ec92d0240cd9dee0421f4763e9f674f</strong> displayed in zipkin ui corresponds to </p>

<p><strong>trace_id_high = 6830039797584469486 in mysql</strong> (5EC92D0240CD9DEE -> upper 16 hex character)</p>

<p><strong>id = 297787839077115727 in mysql</strong> (421F4763E9F674F -> lower 16 hex charecter)</p>
"
Zipkin,47630471,47630294,3,"2017/12/04, 11:45:28",True,"2017/12/04, 13:10:19",32952,6603816,3,"<p>When connecting to the mysql container while using links, you need to use the container name as a hostname.</p>

<p>Change the connection string to:</p>

<pre><code>url: jdbc:mysql://${mysqlHost:0.0.0.0}:3306/zipkin?autoReconnect=true
</code></pre>

<p>And when starting the zipkin container, set the env variable:</p>

<pre><code>docker run -p 9411:9411 --name zipkinServer --link mysqlDocker -d -e mysqlHost=mysqlDocker zipkin
</code></pre>
"
Zipkin,52739988,52377663,0,"2018/10/10, 15:12:13",False,"2018/10/10, 15:12:13",201,7956609,0,"<p>Check your configuration file and make sure the baseUrl is given properly here</p>

<pre><code>    spring.zipkin.baseUrl
</code></pre>
"
Zipkin,48063504,47992456,0,"2018/01/02, 17:22:45",False,"2018/01/02, 17:22:45",81,2232476,0,"<p>Yes, they are both stateless. You can deploy them using whatever horizontal-scalability construct is available to you. </p>
"
Zipkin,50298803,50295992,5,"2018/05/11, 22:10:23",False,"2018/05/11, 22:10:23",91301,2756547,0,"<p>OK! I see now the problem!</p>

<p>So, you say that your HTTP request has these tracing headers: <code>X-B3-TraceId</code>, <code>X-B3-SpanId</code>, <code>X-B3-Sampled</code>, <code>X-Span-Name</code>, <code>X-B3-ParentSpanId</code>.</p>

<p>Then you have this code:</p>

<pre><code>rabbitTemplate.convertAndSend(""shipping-task"", shipment);
</code></pre>

<p>And that's absolutely natural that your tracing header are not transferred to the RabbitMQ: there is just no those headers to send. </p>

<p>I believe that you can extract those headers in this <code>@RequestMapping</code> method and populate them to the AMQP message before sending. See <code>org.springframework.amqp.core.MessageBuilder</code>.</p>

<p>I also think that Spring Cloud Sleuth should have some mechanism to obtain those headers, e.g. <code>Tracer.currentSpan()</code>: <a href=""http://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.RC1/single/spring-cloud-sleuth.html#_current_span"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-static/spring-cloud-sleuth/2.0.0.RC1/single/spring-cloud-sleuth.html#_current_span</a></p>
"
Zipkin,65962429,65943408,2,"2021/01/30, 00:17:20",False,"2021/01/30, 00:17:20",1693,971735,1,"<p>I was not able to reproduce your issue with the <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-zipkin"" rel=""nofollow noreferrer"">spring-cloud-sleuth-sample-zipkin</a> app (it worked to me), here's what I did:</p>
<ol>
<li>Added <code>org.springframework.amqp:spring-rabbit</code> to the <code>pom.xml</code></li>
<li>Added <code>spring.zipkin.sender.type: rabbit</code> to the <code>application.yml</code></li>
<li>Started RabbitMQ using <a href=""https://github.com/jonatan-ivanov/local-services/blob/main/rabbit/docker-compose.yml"" rel=""nofollow noreferrer"">this docker-compose.yml</a></li>
<li>Manually created a queue (named <code>zipkin</code>) on the Rabbit Management UI</li>
<li>Started the app and hit it with a request</li>
<li>Manually get the messages out of the queue and checked if they have the right payload (they did)</li>
</ol>
<p>A few pointers to troubleshoot this:</p>
<ol>
<li>You should see this log event at startup</li>
</ol>
<pre><code>Created new connection: rabbitConnectionFactory#21917b6f:0/SimpleConnection@40803682 [delegate=amqp://guest@127.0.0.1:5672/, localPort= 60265]
</code></pre>
<ol start=""2"">
<li>The connection is created by <a href=""https://github.com/spring-projects/spring-amqp/blob/master/spring-rabbit/src/main/java/org/springframework/amqp/rabbit/connection/AbstractConnectionFactory.java#L561"" rel=""nofollow noreferrer"">AbstractConnectionFactory</a>, you can debug it</li>
<li>Telnet (<code>telnet localhost 5672</code>) can help to troubleshoot connectivity issues</li>
</ol>
<p>Try to make it work using the <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-zipkin"" rel=""nofollow noreferrer"">sample</a> and try to bring the working example closer to your app (by adding dependencies) and see what is the difference between the two and where will it break. If you can create a minimal sample app (e.g.: based on the zipkin sample) that reproduces the issue, please feel free to create an issue on GH: <a href=""https://github.com/spring-cloud/spring-cloud-sleuth"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth</a> and tag me (<code>@jonatan-ivanov</code>), I can take a look.</p>
"
Zipkin,65966458,65943408,0,"2021/01/30, 11:30:34",False,"2021/01/30, 11:30:34",830,7349864,1,"<h2>AUTORESPONSE</h2>
<p>Finally I found it. I had 2 problemas</p>
<p>1 - I was using zipkin-slim docker image for my zip container. This image doesn't contain the rabbitmq collector <a href=""https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq"" rel=""nofollow noreferrer"">rabbitmq collector</a>. I have replaces by standar zipkin image</p>
<p>2 - I do not know why, but the connection from sleuth/zipkin to RabbitMQ is not retrying (I will investigate further). So, if I was in a hurry and test very early (when RabbitMQ is not yet available) it fails, and not retries.</p>
<p>My docker-compose relevant sections now are like this:</p>
<pre><code>  rabbitmq:  
    image: rabbitmq:3.7.8-management  
    mem_limit: 350m
    expose:
      - &quot;5672&quot;
      - &quot;15672&quot;
    ports:    
      - 5672:5672    
      - 15672:15672  
    healthcheck:    
      test: [&quot;CMD&quot;, &quot;rabbitmqctl&quot;, &quot;status&quot;]    
      interval: 10s    
      timeout: 5s    
      retries: 10


  # https://hub.docker.com/r/openzipkin/zipkin
  zipkin:
    #image: openzipkin/zipkin-slim
    image: openzipkin/zipkin
    mem_limit: 512m
    expose:
      - &quot;9411&quot;
    ports:
      - &quot;9411:9411&quot;
    environment:
    - RABBIT_ADDRESSES=rabbitmq
    - STORAGE_TYPE=mem
    depends_on:
      rabbitmq:
        condition: service_healthy

</code></pre>
<p>Thanks again to <a href=""https://stackoverflow.com/users/971735/jonatan-ivanov"">Jonatan Ivanov</a> for helping me!</p>
"
Zipkin,56032785,56032752,0,"2019/05/08, 05:37:47",False,"2020/08/19, 12:00:44",167,486631,0,"<p>After continue efforts and going throgh core api of spring boot application I got my solution:)</p>
<p>Root cause of my issue is below :</p>
<blockquote>
<p>MY application using Spring boot RabitMQ integration and due that
zipkin taking 1st prefrance to RabitMQ sender and my trace are ignored
my zipkin server.</p>
</blockquote>
<p>So use below configration is anyone has same issue to avoid lot painless efforts , even we are not getting in logs of server root cause of it</p>
<p>*---</p>
<pre><code>spring:
  application:
    name: 'active-listener'
  profiles: 'dev'
  sleuth:
    async:
      enabled: false
    annotation:
      enabled: true
  enabled: true
  sampler:
    probability: 1.0
  zipkin:
    baseUrl: http://localhost:9411
    enabled: true
    sender:
      type: web*
</code></pre>
"
Zipkin,26486619,26016714,0,"2014/10/21, 15:21:43",True,"2014/10/21, 15:21:43",583,1372349,0,"<p>i get the same messages but still be able to collect messages and view them with the web service. I dont know why the [error] prefix is in front of it but if you read the chars behind you see INF/DEB and so on... It stays for INFO and DEBUG.</p>

<p>Greets</p>
"
Zipkin,62188352,62188289,1,"2020/06/04, 09:39:36",False,"2020/06/04, 09:39:36",4038,809423,0,"<p>yes, you can have multiple express running in the same node process (thats how clustering works in node as well)</p>

<p>but you will need to have them running on different ports.;</p>

<pre><code># const express = require('express')
const app1 = express()
app1.listen(3001, () =&gt; { //... })

//...

const app2 = express()
app2.listen(3002, () =&gt; { //... })
</code></pre>
"
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501,1272149,1,"<p>It is possible to create two separate tracer providers. Only one of them will be the global tracer provider, which the API will use if you call API methods. You can't use the plugins in this configuration, which means you will have to manually instrument your application. If this is a use-case which is important to you, I suggest you create an issue on the github repo.</p>
"
Zipkin,61028338,61027513,1,"2020/04/04, 15:50:12",True,"2020/04/04, 15:50:12",1162,4383264,0,"<pre><code>&gt;&gt; kubectl get deployment zipkin 
NAME READY UP-TO-DATE AVAILABLE AGE 
zipkin 0/1 1 0 14m
</code></pre>

<p>This indicates that the pod is not ready, hence the service will not add that pod's IP in the list of endpoints. Check the readiness probe of your pod by describing it and debug the issue that's making it non-ready. Once this if fixed, you'll start seeing some endpoints populated when you describe the service &amp; that will enable you to access the service by DNS name.</p>
"
Zipkin,54493070,54492867,3,"2019/02/02, 14:27:06",False,"2019/02/02, 14:27:06",12094,1556338,2,"<p>First specify your own overlay network (see bottom of code below) and use it for your services.</p>

<pre><code>version: '3'

services:
  myservice1:
    image: myImage1
    depends_on:
      - myService2
      - myService3
    ports:
      - ""8081:80""
    environment:
      - TZ=Europe/Warsaw
    networks:
      - backbone

  myservice3:
    image: myImage2
    environment:
      - profile=${MY_PROFILE}
      - TZ=Europe/Warsaw
    networks:
      - backbone

networks:
  backbone:
   driver: overlay
</code></pre>

<p>Then in your compose file for your other services like ZIpkin, add the <code>backbone</code> network to its list. Eg:</p>

<pre><code>version: '3'

services:
  ZIpkin:
    image: myZImage
    networks:
      - ZIPN
      - backbone

networks:
  backbone:
   external:
     name:  PROJ_backbone
</code></pre>

<p>Note that outside of the first compose file, you'll need to prefix the <em>project</em> name for your network. Unless you set the environment variable <code>COMPOSE_PROJECT_NAME</code> it will be the name of the directory that the compose file is in. Do a <code>docker network ls</code> to find out the full name of the network to use.</p>
"
Zipkin,54494741,54492867,0,"2019/02/02, 17:56:24",False,"2019/02/02, 17:56:24",1285,6022333,0,"<p>OK, so I solved my problem using only docker-compose files. 
By Portainer I pasted my second docker-compose file in Stack section (I creaated new Stack):</p>

<pre><code>version: '3'
services:

  my_name_zipkin: --&gt; THIS NAME SHOULD WE USE WHEN WE'D LIKE TO COMMUNICATE WITH CONTAINER
    image: openzipkin/zipkin
    ports:
      - ""9411:9411""
    networks:
      - zipkin
      - my_old_network_with_services
networks:
  zipkin:
  my_old_network_with_services:
    external: true
</code></pre>

<p>Now we should use 'my_name_zipkin' name to communicate with this service. Service name is the name we should use to communicate between containers.</p>

<p>So in properties file I set:</p>

<pre><code>spring.zipkin.base-url=http://zipkin:9411/
</code></pre>
"
Zipkin,51306145,51305899,0,"2018/07/12, 15:49:09",False,"2018/07/12, 15:49:09",18670,340088,0,"<p>The slf4j API only takes <code>String</code> as the input to the <code>info</code>, <code>debug</code>, <code>warn</code>, <code>error</code> messages. </p>

<p>What you could do is create your own JsonLogger wrapper, which takes a normal <code>Logger</code> (maybe wraps around it), which you could include at the top of your classes like:</p>

<p><code>private static final JsonLogger logger = new JsonLogger(LoggerFactory.getLogger(MyClass.class))</code>;</p>

<p>You can then use Jackson, GSON or your favourite object to JSON mapper inside your JsonLogger so that you could do what you want. It can then offer the <code>info</code>, <code>debug</code>, <code>warn</code>, <code>error</code> methods like a normal logger. </p>

<p>You can also create your own <code>JsonLoggerFactory</code> which encapsulates this for you so that the line to include in each class is more concise.</p>
"
Zipkin,58298923,58294974,2,"2019/10/09, 10:18:06",True,"2019/10/09, 10:18:06",2382,4497840,2,"<p>I have a working project with spring cloud stream and zipkin using the following configuration (maybe you should set the sender.type): </p>

<pre><code>spring:
  zipkin:
    enabled: true
    service.name: my-service
    sender.type: web
    base-url: http://localhost:9411
  sleuth:
    enabled: true
    sampler:
      probability: 1.0
</code></pre>

<p>Hope this can help.</p>
"
Zipkin,57397006,57380205,0,"2019/08/07, 17:43:53",False,"2019/08/07, 17:43:53",527,3966540,0,"<p>I had a wrong opencensus collector configuration.
The docker container network cannot see port 9411 as it was on the host network. I was able to fix the issue after noticing this misconfiguration.</p>
"
Zipkin,53468275,53428788,0,"2018/11/25, 16:03:42",False,"2018/11/25, 16:03:42",1767,2498986,0,"<p>Zipkin currently supports four types of backend storage to store spans in-memory, MySQl, ElasticSearch, Cassandra. Although for production it is recommended to use ES or Cassandra. The other two can be used for learning and understanding. Traces stored in the in-memory is ephemeral and won't be available after the restart. </p>

<p>In the zipkin UI there is an option to see the trace and download it, which can be used to view at a later point in time. If you still have further questions drop in to the zipkin <a href=""https://gitter.im/openzipkin/zipkin"" rel=""nofollow noreferrer"">gitter</a> channel.</p>
"
Zipkin,56070984,56070700,5,"2019/05/10, 08:05:46",True,"2019/05/10, 08:05:46",12864,1413240,0,"<p>It looks like the trace is triggering an old-fashioned inverted-lock-order deadlock freezing threads that are attempting to acquire new Connections.</p>

<p>The last of the three deadlocked threads is <a href=""https://github.com/spring-projects/spring-framework/blob/master/spring-beans/src/main/java/org/springframework/beans/factory/support/DefaultSingletonBeanRegistry.java#L204"" rel=""nofollow noreferrer"">trying to get a lock on some singleton or bean</a>. It has already passed through and presumably acquired a lock on a <code>GenericScope</code>.</p>

<p>The other two threads are <a href=""https://github.com/spring-cloud/spring-cloud-commons/blob/master/spring-cloud-context/src/main/java/org/springframework/cloud/context/scope/GenericScope.java#L387-L388"" rel=""nofollow noreferrer"">trying to acquire a lock on a <code>GenericScope</code></a>, which presumably the first thread has.</p>

<p>An unexpected reentrance from the <code>zipkin</code> code into spring is generating a deadlock. <code>c3p0</code> has a fixed-size thread pool that notices when all its threads (just 3 here, <code>c3p0</code>'s default) are persistently frozen, then (pretty correctly in this case) declares a deadlock and replaces the blocked threads in hopes of recovering.</p>

<p>Does c3p0 recover? Is this a rare or frequent deadlock? There's not much you can easily do to prevent this deadlock, I think either you'll have to tolerate it or do without the instrumentation.</p>
"
Zipkin,51580557,51578263,2,"2018/07/29, 15:58:11",True,"2018/07/29, 15:58:11",8336,1773866,2,"<p>If you read the docs or any information starting from edgware you would see that we've removed that support. You should use native zipkin rabbit / kafka dependencies. Everything is there in the docs.</p>
"
Zipkin,21036699,21035841,1,"2014/01/10, 07:20:28",True,"2014/01/10, 07:20:28",136,1982812,1,"<p>Yes, you can use BAM/CEP for this. If you need real time monitoring you can use CEP and you can use BAM for batch process. From BAM 2.4.0 onwards, CEP features have been added inside BAM also hence you can use BAM and do real time analytics.</p>

<p>What type of services are involved with your scenario? Depends on this you can use already existing data publisher or write new data publisher for BAM/CEP to publish your request details. For example if you are having chain of axis2 webservice calls for a request from client, and you want to monitor where the bottle neck/more time consumed, then you may use the service stats publishing, and monitor the average time take to process the message which will help you to see where the actual delay has been introduced. For this you can use existing service statistics publisher feature. Also BAM will allow you to create your own dashboard to visualize, hence you can customize the dashboard. </p>

<p>Also with BAM 2.4.0 we have introduced notifications feature also which you can define some threshold value and configure to send notification if that cross that threshold value.</p>
"
Zipkin,62430615,62418175,0,"2020/06/17, 16:50:21",True,"2020/06/17, 16:50:21",213,7933630,0,"<p>Metrics in OpenTelemetry are currently undergoing continued development and refinement, so they aren't necessarily available for each language yet (see <a href=""https://github.com/open-telemetry/opentelemetry-js/issues?q=is%3Aissue+is%3Aopen+label%3Afeature-request"" rel=""nofollow noreferrer"">this tag in the OpenTelemetry JS repo</a> for an example of metric instruments that aren't up to date yet with spec), but once they are, I'd expect for metrics to be added to the existing node/web instrumentation packages.</p>

<p>That said, I would still advise you to try out OpenTelemetry for traces at this point, as it's pretty stable for tracing. You can use a Prometheus client to export metrics separately, and once OpenTelemetry metrics are fully supported in the JS library, switch over to that.</p>
"
Zipkin,21582785,21551629,3,"2014/02/05, 18:27:05",True,"2014/02/05, 18:27:05",26,2070089,1,"<p>This is due to not having an instance of the query server running.</p>

<p>I'm in the middle of a re-write that'll simplify all of this. Until then, you need to spin up a query server.</p>
"
Zipkin,66524575,66517644,2,"2021/03/08, 07:07:48",True,"2021/03/08, 07:25:55",1693,971735,1,"<p>It was removed in Sleuth 3.0, though it seems the docs were not updated, I'm going to update the docs soon.</p>
<p>To fix the rest with your logs, you can check the logging config <a href=""https://github.com/spring-projects/spring-boot/tree/5fc49aa485a68664dcdea83eaa366cb1142bcb32/spring-boot-project/spring-boot/src/main/resources/org/springframework/boot/logging"" rel=""nofollow noreferrer"">here</a>, the <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-log-integration"" rel=""nofollow noreferrer"">log integration in the docs</a> and <a href=""https://stackoverflow.com/a/65851232/971735"">this answer</a>.</p>
"
Zipkin,53221841,53218692,0,"2018/11/09, 10:01:09",False,"2018/11/09, 10:01:09",136,1433218,0,"<p>we also use use zipkin but can't query with zipkin as elk. we can just click on each services which are display on zipkin and get more info as below image.  </p>

<p><a href=""https://i.stack.imgur.com/BuFa6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BuFa6.png"" alt=""after click the service""></a></p>
"
Zipkin,53236126,53218692,1,"2018/11/10, 07:02:05",False,"2018/11/10, 07:02:05",1767,2498986,0,"<p>Zipkin is not a business transaction tracking system and it should not be used that way because it is not built for this purpose. There are other tools which are built specifically to cater the needs to business operations which you must consider.</p>

<p>P.S. I am a Zipkin contributor.</p>
"
Zipkin,53932587,53218692,0,"2018/12/26, 15:12:50",False,"2018/12/26, 15:12:50",3071,2979435,0,"<p>This is not an answer to how achieve this with zipkin but yes for the whole problem.</p>

<p>If you have a  transaction that didn't complete it's steps then you probably have two of following issues</p>

<p><strong>Some microservice failed to deliver the event to the next one and didn't figure it out</strong></p>

<p>You have to make sure delivery at least once here, using Kafka you have to wait until message get flushed to the server for example</p>

<p><strong>The destiny microservice received the message and is not processing it</strong></p>

<p>You have to make sure you application is processing what it's supposed to,  you can monitor the database if the transactions are there or use some tool like LinkedIn burrow to monitor your Kafka message group if you are integrating by using Kafka.</p>

<p>Conclusion is, instead to try monitor all the thing once it looks like creating specialist monitors for every step will be more assertive and simple to develop.</p>
"
Zipkin,53221956,49280873,0,"2018/11/09, 10:11:00",False,"2018/11/09, 10:11:00",136,1433218,0,"<p>Just add below, it need to be working,</p>

<pre><code> zipkin:
    sender:
      type: web
    baseUrl:  http://192.168.0.207:9411
  sleuth:
    sampler:
      percentage: 1.0
</code></pre>
"
Zipkin,46454753,46432583,1,"2017/09/27, 21:22:45",False,"2017/09/27, 21:22:45",2661,1813696,0,"<p>With Spring boot <strong>Dalston.SR3</strong> (which uses open zipkin 1.28) you can achieve this by setting property <strong>zipkin.storage.mem.max-spans=xxx</strong> This will limit the number of spans and discard old ones.</p>

<p>pom.xml</p>

<pre><code>&lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;version&gt;1.5.4.RELEASE&lt;/version&gt;
    &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
&lt;/parent&gt;

&lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
    &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;spring-cloud.version&gt;Dalston.SR3&lt;/spring-cloud.version&gt;
&lt;/properties&gt;
</code></pre>
"
Zipkin,58172731,58170335,2,"2019/09/30, 20:39:51",False,"2019/09/30, 20:39:51",81,2232476,0,"<p>The problem lies in your <code>ES_HOSTS</code> variable, from the docs <a href=""https://github.com/openzipkin/zipkin/tree/844c43b9cde02b84583f0e3641fc20e670a3b25f/zipkin-server#elasticsearch-storage"" rel=""nofollow noreferrer"">here</a>:</p>

<blockquote>
  <ul>
  <li><code>ES_HOSTS</code>: A comma separated list of elasticsearch base urls to connect to ex. <a href=""http://host:9200"" rel=""nofollow noreferrer"">http://host:9200</a>.
            Defaults to ""<a href=""http://localhost:9200"" rel=""nofollow noreferrer"">http://localhost:9200</a>"".</li>
  </ul>
</blockquote>

<p>So you will need: <code>ES_HOSTS=http://storage:9200</code></p>
"
Zipkin,58181776,58170335,0,"2019/10/01, 12:23:56",False,"2019/10/01, 12:23:56",35,9795454,0,"<p>Finally I have this file:</p>

<pre><code>version: '3.7'

services:
  storage:
    image: openzipkin/zipkin-elasticsearch7
    container_name: elasticsearch
    ports:
      - 9200:9200

  zipkin:
    image: openzipkin/zipkin
    container_name: zipkin
    environment: 
      - STORAGE_TYPE=elasticsearch
      - ""ES_HOSTS=elasticsearch:9300""
    ports:
      - 9411:9411
    depends_on: 
      - storage

  dependencies:
    image: openzipkin/zipkin-dependencies
    container_name: dependencies
    entrypoint: crond -f
  depends_on:
    - storage
  environment:
    - STORAGE_TYPE=elasticsearch
    - ""ES_HOSTS=elasticsearch:9300""
    - ""ES_NODES_WAN_ONLY=true""

prometheus:
  image: prom/prometheus:latest
  container_name: prometheus
  volumes:
    - $PWD/prometheus:/etc/prometheus/
    - /tmp/prometheus:/prometheus/data:rw
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
    - '--web.console.libraries=/usr/share/prometheus/console_libraries'
    - '--web.console.templates=/usr/share/prometheus/consoles'
  ports:
    - ""9090:9090""

grafana:
  image: grafana/grafana
  container_name: grafana
  depends_on:
    - prometheus
  ports:
    - ""3000:3000""
</code></pre>

<p>Main differences are the usage of </p>

<blockquote>
  <p>""ES_HOSTS=elasticsearch:9300""</p>
</blockquote>

<p>instead of </p>

<blockquote>
  <p>""ES_HOSTS=storage:9300""</p>
</blockquote>

<p>and in the dependencies configuration I add the entrypoint in dependencies:</p>

<blockquote>
  <p>entrypoint: crond -f
  This one is really the key to not have the exception when I start docker-compose. </p>
</blockquote>

<p>To solve this issue, I check the this project: <a href=""https://github.com/openzipkin/docker-zipkin"" rel=""nofollow noreferrer"">https://github.com/openzipkin/docker-zipkin</a></p>

<p>The remaining question is: why do I need to use entrypoint: crond -f</p>
"
Zipkin,43106901,43095404,0,"2017/03/30, 04:24:13",True,"2017/03/30, 04:24:13",762,1227937,0,"<p>In general it is better to use the zipkin's http variant of Elasticsearch as it cannot conflict with Spring Boot's elasticsearch library versions.</p>

<p>I would set everything in zipkin's group id to latest (currently 1.21.0 which is Spring Boot 1.4.x) and use zipkin-autoconfigure-storage-elasticsearch-http (plus.. the one you are using <a href=""https://github.com/openzipkin/zipkin/issues/1511"" rel=""nofollow noreferrer"">will be dropped</a>)</p>

<p>Make sure your es hosts is specified in url syntax ex. <a href=""http://host1:9200"" rel=""nofollow noreferrer"">http://host1:9200</a></p>
"
Zipkin,52608036,51661009,0,"2018/10/02, 15:08:15",True,"2018/10/02, 15:08:15",153,2999097,0,"<p>finally got working after spring verison updated to <code>5.x</code>
It already have <code>Brave Instrument for zipkin trace</code> </p>
"
Zipkin,37799358,37757200,0,"2016/06/14, 00:23:23",False,"2016/07/26, 21:58:32",13,3491416,1,"<p>I was recording wrong annotation i.e client instead of server. </p>

<p>Just a simple change did the trick.</p>

<p><code>Trace.traceService(""Function1"",""Test"")</code></p>

<p>Sample working Zipkin example: <a href=""https://gist.github.com/AkhilJ876/3e38757c28d43924f296dd2d147c0bd9#file-zipkintracing_example-L34"" rel=""nofollow"">https://gist.github.com/AkhilJ876/3e38757c28d43924f296dd2d147c0bd9#file-zipkintracing_example-L34</a></p>
"
Zipkin,59877579,59859303,0,"2020/01/23, 13:31:55",True,"2020/01/23, 13:31:55",2576,12014434,1,"<p>By using the following <a href=""https://istio.io/docs/setup/install/istioctl/#show-differences-in-profiles"" rel=""nofollow noreferrer"">commands</a> I was able to generate the manifests using <a href=""https://istio.io/docs/setup/getting-started/#download"" rel=""nofollow noreferrer""><code>istioctl</code></a> with parameters You mentioned:</p>

<pre><code>$ istioctl manifest generate --set profile=demo --set values.tracing.enabled=true --set values.tracing.provider=zipkin &gt; istio-demo-with-zipkin.yaml
</code></pre>

<pre><code>$ istioctl manifest generate --set profile=demo &gt; istio-demo.yaml
</code></pre>

<p>Then compared them to see differences made with those parameter modifications.</p>

<pre><code>$ istioctl manifest diff istio-demo.yaml istio-demo-with-zipkin.yaml
Differences of manifests are:


Object ConfigMap:istio-system:istio-sidecar-injector has diffs:

data:
  values:
    tracing:
      provider: jaeger -&gt; zipkin


Object Deployment:istio-system:istio-tracing has diffs:

metadata:
  labels:
    app: jaeger -&gt; zipkin
spec:
  selector:
    matchLabels:
      app: jaeger -&gt; zipkin
  template:
    metadata:
      annotations:
        prometheus.io/port: 14269 -&gt;
        prometheus.io/scrape: true -&gt;
      labels:
        app: jaeger -&gt; zipkin
    spec:
      containers:
        '[?-&gt;0]': -&gt; map[env:[map[name:POD_NAMESPACE valueFrom:map[fieldRef:map[apiVersion:v1
          fieldPath:metadata.namespace]]] map[name:QUERY_PORT value:9411] map[name:JAVA_OPTS
          value:-XX:ConcGCThreads=2 -XX:ParallelGCThreads=2 -Djava.util.concurrent.ForkJoinPool.common.parallelism=2
          -Xms700M -Xmx700M -XX:+UseG1GC -server] map[name:STORAGE_METHOD value:mem]
          map[name:ZIPKIN_STORAGE_MEM_MAXSPANS value:500000]] image:docker.io/openzipkin/zipkin:2.14.2
          imagePullPolicy:IfNotPresent livenessProbe:map[initialDelaySeconds:200 tcpSocket:map[port:9411]]
          name:zipkin ports:[map[containerPort:9411]] readinessProbe:map[httpGet:map[path:/health
          port:9411] initialDelaySeconds:200] resources:map[limits:map[cpu:300m memory:900Mi]
          requests:map[cpu:150m memory:900Mi]]]
        '[0-&gt;?]': map[env:[map[name:POD_NAMESPACE valueFrom:map[fieldRef:map[apiVersion:v1
          fieldPath:metadata.namespace]]] map[name:BADGER_EPHEMERAL value:false] map[name:SPAN_STORAGE_TYPE
          value:badger] map[name:BADGER_DIRECTORY_VALUE value:/badger/data] map[name:BADGER_DIRECTORY_KEY
          value:/badger/key] map[name:COLLECTOR_ZIPKIN_HTTP_PORT value:9411] map[name:MEMORY_MAX_TRACES
          value:50000] map[name:QUERY_BASE_PATH value:/jaeger]] image:docker.io/jaegertracing/all-in-one:1.14
          imagePullPolicy:IfNotPresent livenessProbe:map[httpGet:map[path:/ port:14269]]
          name:jaeger ports:[map[containerPort:9411] map[containerPort:16686] map[containerPort:14250]
          map[containerPort:14267] map[containerPort:14268] map[containerPort:14269]
          map[containerPort:5775 protocol:UDP] map[containerPort:6831 protocol:UDP]
          map[containerPort:6832 protocol:UDP]] readinessProbe:map[httpGet:map[path:/
          port:14269]] resources:map[requests:map[cpu:10m]] volumeMounts:[map[mountPath:/badger
          name:data]]] -&gt;
      volumes: '[map[emptyDir:map[] name:data]] -&gt;'


Object Service:istio-system:jaeger-agent is missing in B:



Object Service:istio-system:jaeger-collector is missing in B:



Object Service:istio-system:jaeger-query is missing in B:



Object Service:istio-system:tracing has diffs:

metadata:
  labels:
    app: jaeger -&gt; zipkin
spec:
  ports:
    '[0]':
      targetPort: 16686 -&gt; 9411
  selector:
    app: jaeger -&gt; zipkin


Object Service:istio-system:zipkin has diffs:

metadata:
  labels:
    app: jaeger -&gt; zipkin
spec:
  selector:
    app: jaeger -&gt; zipkin
</code></pre>

<p>You can try to manually modify those applied settings or apply it to Your cluster.</p>

<p>Istioctl I used to generate these manifests:</p>

<pre><code>$ istioctl version
client version: 1.4.3
control plane version: 1.4.3
data plane version: 1.4.3 (4 proxies)
</code></pre>

<p>Hope it helps.</p>
"
Zipkin,63739507,63737361,2,"2020/09/04, 13:37:27",False,"2020/09/04, 13:37:27",4113,12201084,0,"<p>Your service is expecting following labels on pod:</p>
<pre><code>selector:
  app.kubernetes.io/name: zipkin
  app.kubernetes.io/instance: zipkin
  app: zipkin
</code></pre>
<p>Although it looks like you have only one label on zipkin pods:</p>
<pre><code>labels:
    app: zipkin
</code></pre>
<p>Label selector uses logical AND (&amp;&amp;), and this means that all labels specified must be on pod to match it.</p>
"
Zipkin,63778418,63737361,0,"2020/09/07, 16:19:45",True,"2020/09/07, 18:08:33",351,10196632,0,"<p>The following worked. Sorry, I cannot provide info on all details since I don't know them :( Maybe somebody else can.</p>
<p>deployment.yaml</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: zipkin
  labels:
    app.kubernetes.io/name: zipkin
    app.kubernetes.io/instance: zipkin
    app: zipkin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zipkin
  template:
    metadata:
      name: zipkin
      labels:
        app: zipkin
    spec:
      containers:
        - name: zipkin
          image: openzipkin/zipkin:2.21
          imagePullPolicy: Always
          ports:
            - containerPort: 9411
              protocol: TCP
          env:
          - name: STORAGE_TYPE
            value: elasticsearch
          - name: ES_HOSTS
            value: https://my-es-host:9243
          - name: ES_USERNAME
            value: myUser
          - name: ES_PASSWORD
            value: myPassword
          - name: ES_HTTP_LOGGING
            value: HEADERS
          readinessProbe:
            httpGet:
              path: /api/v2/services
              port: 9411
            initialDelaySeconds: 5
            timeoutSeconds: 3
            
</code></pre>
<p>service.yaml</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: zipkin
  labels:
    app.kubernetes.io/name: zipkin
    app.kubernetes.io/instance: zipkin
    app: zipkin
spec:
  type: ClusterIP
  ports:
    - port: 9411
      targetPort: 9411
      protocol: TCP
      name: http  &lt;-- DELETED
  selector:
    app.kubernetes.io/name: zipkin    &lt;-- DELETED
    app.kubernetes.io/instance: zipkin    &lt;-- DELETED
    app: zipkin
    
            
</code></pre>
<p>ingress.yaml</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: zipkin
  labels:
    app.kubernetes.io/name: zipkin
    app.kubernetes.io/instance: zipkin
    app.kubernetes.io/managed-by: zipkin
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: &quot;true&quot;
    nginx.ingress.kubernetes.io/rewrite-target: /$1    &lt;-- DELETED
    nginx.ingress.kubernetes.io/proxy-read-timeout: &quot;3600&quot;
    nginx.ingress.kubernetes.io/proxy-body-size: &quot;0&quot;
    nginx.ingress.kubernetes.io/cors-allow-methods: &quot;PUT, GET, POST, OPTIONS, DELETE&quot;
    nginx.ingress.kubernetes.io/cors-allow-origin: &quot;*&quot;
    nginx.ingress.kubernetes.io/enable-cors: &quot;true&quot;
spec:
  tls:
    - hosts:
        - ns-zipkin.my-host
      secretName: .my-host
  rules:
    - host: ns-zipkin.my-host
      http:
        paths:
          - path: /  &lt;-- CHANGED
            backend:
              serviceName: zipkin
              servicePort: 9411   &lt;-- CHANGED
</code></pre>
"
Zipkin,50813959,50813714,8,"2018/06/12, 12:39:34",True,"2018/06/26, 16:13:12",8336,1773866,1,"<p><strong>EDGWARE</strong></p>

<p>Have you read the documentation? If you use Spring Cloud Sleuth in Edgware version if you read the Sleuth section you would find this piece of the documentation <a href=""https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_custom_sa_tag_in_zipkin"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_custom_sa_tag_in_zipkin</a></p>

<p>Let me copy that for you</p>

<blockquote>
  <p>54.5 Custom SA tag in Zipkin Sometimes you want to create a manual Span that will wrap a call to an external service which is not
  instrumented. What you can do is to create a span with the
  peer.service tag that will contain a value of the service that you
  want to call. Below you can see an example of a call to Redis that is
  wrapped in such a span.</p>
</blockquote>

<pre><code>org.springframework.cloud.sleuth.Span newSpan = tracer.createSpan(""redis"");
try {
    newSpan.tag(""redis.op"", ""get"");
    newSpan.tag(""lc"", ""redis"");
    newSpan.logEvent(org.springframework.cloud.sleuth.Span.CLIENT_SEND);
    // call redis service e.g
    // return (SomeObj) redisTemplate.opsForHash().get(""MYHASH"", someObjKey);
} finally {
    newSpan.tag(""peer.service"", ""redisService"");
    newSpan.tag(""peer.ipv4"", ""1.2.3.4"");
    newSpan.tag(""peer.port"", ""1234"");
    newSpan.logEvent(org.springframework.cloud.sleuth.Span.CLIENT_RECV);
    tracer.close(newSpan);
}
</code></pre>

<blockquote>
  <p>[Important]   Important Remember not to add both peer.service tag and
  the SA tag! You have to add only peer.service.</p>
</blockquote>

<p><strong>FINCHLEY</strong></p>

<p>The <code>SA</code> tag will not work for Finchley. You have to do it in the following manner using the <code>remoteEndpoint</code> on the span.</p>

<pre><code>    Span span = tracer.newTrace().name(""redis""); 
    span.remoteEndpoint(Endpoint.newBuilder().serviceName(""redis"").build()); 
    span.kind(CLIENT);
    try(SpanInScope ws = tracer.withSpanInScope(span.start())) {
          // add any tags / annotations on the span
          // return (SomeObj) redisTemplate.opsForHash().get(""MYHASH"", someObjKey);
    } finally {
      span.finish();
    }
</code></pre>
"
Zipkin,44062121,44027882,0,"2017/05/19, 08:32:21",False,"2017/05/19, 08:32:21",8336,1773866,2,"<p>Here you have a very basic example of Sleuth &amp; HTTP communication. <a href=""https://github.com/openzipkin/sleuth-webmvc-example"" rel=""nofollow noreferrer"">https://github.com/openzipkin/sleuth-webmvc-example</a> You can set your dependencies in a similar manner and everything should work fine. In your example you've got Stream but I don't think you're using it so it's better to remove it.</p>
"
Zipkin,45815790,44027882,0,"2017/08/22, 13:58:17",False,"2017/08/22, 13:58:17",253,5751473,0,"<p>As M.Deinum said remove <code>stream</code> and <code>stream-rabbit</code> dependencies what if you do not need some AMQP server to store the trace message.</p>

<p>or</p>

<p>config the AMQP(rabbitMQ in your code) from application-configuration(both) and add <code>zipkin-stream</code> &amp; <code>stream-rabbit</code> in <code>zipkin-server</code> side, so this time your app(<code>zipkin-client</code>) will not direct connect with <code>zipkin-server</code> 
and it will be: </p>

<pre><code>zipkin-client &lt;==&gt; AMQP(rabbitMQ) &lt;==&gt; zipkin-server
</code></pre>
"
Zipkin,54468173,54466528,0,"2019/01/31, 21:45:59",True,"2019/01/31, 21:45:59",2296,1575416,2,"<p>Seems to work once I added the Web package. Though I don't recall it being needed previously.</p>
"
Zipkin,44193861,44147933,0,"2017/05/26, 07:52:41",False,"2017/05/26, 07:52:41",1,7983801,0,"<p>Appears that a sleuth span is not the same as a Zipkin span.
Hence, in the above code there is no way to instantiate a default tracer with zipkin span reporter.
I converted the sleuth span into a zipkin span and then reported it to zipkin. The class to convert it is available in spring-cloud-sleuth-stream. I used pretty much the same class with some tweaks.</p>
"
Zipkin,65231143,65228873,0,"2020/12/10, 10:43:15",True,"2020/12/10, 10:43:15",6331,2133695,1,"<p>No, the tracing SPI will not be backported to Vert.x 3.</p>
<p>I would recommend to check out <a href=""https://vertx.io/blog/from-vert-x-3-to-vert-x-4/"" rel=""nofollow noreferrer"">Migrate from Vert.x 3 to Vert.x 4</a>:</p>
<blockquote>
<p>When­ever pos­si­ble, Vert.x 4 APIs have been made avail­able in
Vert.x 3 with a dep­re­ca­tion of the old API, giv­ing the
op­por­tu­nity to im­prove a Vert.x 3 ap­pli­ca­tion with a bet­ter
API while al­low­ing the ap­pli­ca­tion to be ready for a Vert.x 4
mi­gra­tion.</p>
</blockquote>
<p>In other words, one of the Vert.x 4 goals was to minimize the upgrading effort.</p>
"
Zipkin,54766304,54645785,3,"2019/02/19, 14:25:08",True,"2019/02/19, 14:25:08",8336,1773866,0,"<p>It makes perfect sense that it's <code>null</code>. That's because YOU control the way what happens with the caught exception. In your case, nothing, cause you swallow that exception. </p>

<p>If you want to do sth better, just add the error tag manually via the <code>SpanCustomizer</code>. That way you'll add the exception to the given span. It will then automatically get closed and reported to Zipkin (you can do sth else than <code>ex.toString()</code> of course.</p>

<pre><code>@Slf4j
@RestControllerAdvice
@Order(Ordered.HIGHEST_PRECEDENCE)
public class ExceptionHanders {

    private final SpanCustomizer customizer;

    public ExceptionHanders(SpanCustomizer customizer) {
        this.customizer = customizer;
    }

    @ExceptionHandler({RuntimeException.class})
    @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)
    public String handleRuntimeException(Exception ex) throws Exception {
        this.customizer.tag(""error"", ex.toString());
        return ""testabcd"";
    }
}
</code></pre>
"
Zipkin,52815354,52706088,1,"2018/10/15, 14:07:16",False,"2018/10/15, 14:07:16",201,7956609,2,"<p>You have to use <strong>spring.sleuth.web.skipPattern</strong></p>

<p>sample you will get here <a href=""https://www.baeldung.com/tracing-services-with-zipkin"" rel=""nofollow noreferrer"">https://www.baeldung.com/tracing-services-with-zipkin</a></p>
"
Zipkin,48982608,48518928,0,"2018/02/26, 08:26:46",False,"2018/02/26, 08:26:46",181,6708214,0,"<p>Yeah,you should use different libraries for different languages.<br>
Brave for Java,Zipkin4net for C# and so on.<br>
For more details,you can visit Zipkin official site: <a href=""https://zipkin.io/pages/existing_instrumentations.html"" rel=""nofollow noreferrer"">Zipkin Existing instrumentations</a>.</p>

<p>Then all you shoud do is following the librarie guide.
Have fun!</p>
"
Zipkin,57287189,57284146,0,"2019/07/31, 12:12:05",False,"2019/07/31, 12:12:05",11,11602721,1,"<p>I found examples from:
<a href=""https://github.com/openzipkin/zipkin/tree/master/zipkin-lens/testdata"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/tree/master/zipkin-lens/testdata</a></p>

<p>It works well.</p>
"
Zipkin,65171650,65169398,0,"2020/12/06, 20:34:26",False,"2020/12/06, 20:34:26",8336,1773866,0,"<p>You should use e.g. openzipkin Brave project or Opentelemetry projects directly. Sleuth works only with boot based projects</p>
"
Zipkin,52585722,47026664,0,"2018/10/01, 09:37:07",False,"2020/09/02, 11:35:57",61,3548002,0,"<p>I use &quot;TraceCallable&quot; class from &quot;<a href=""http://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-sleuth-core"" rel=""nofollow noreferrer"">spring-cloud-sleuth</a>&quot; lib to solve it in my code.</p>
<p>My code example is:</p>
<pre class=""lang-kotlin prettyprint-override""><code>@Component
class TracingCallableSupplier(
    private val tracing: Tracing,
    private val spanNamer: SpanNamer
) {

    /**
     * Supply callable which will use tracing from parent while performing jobs.
     */
    fun &lt;T : Any?&gt; supply(function: () -&gt; T): Callable&lt;T&gt; {
        return TraceCallable(tracing, spanNamer, Callable(function))
    }
}

@Service
class MyBean{
    @Autowired
    lateinit var traceSupplier : TracingCallableSupplier

    fun myMethod {
        val callable = tracingCallableSupplier.supply {
            // ... some code to be called asynchronous...
        }

        // simplest coroutine...
        GlobalScope.launch {
            callable.call()
        }

        // more advanced coroutine usage...
        val deferred = (0 until 10).map {
            async {
                callable.call()
            }
        }

        runBlocking {
            deferred.map {
                val result = it.await()
                // ... some processing ...
            }
        }
    }
}
</code></pre>
"
Zipkin,64439116,47026664,0,"2020/10/20, 08:41:48",False,"2020/10/22, 13:48:08",145,10936956,1,"<p><em><strong>Attention: This solution does works for logging purposes, but does not work for other Sleuth features like instrumenting RestTemplates to send the tracing headers to other services. So unfortunately, this is not a fully working solution. :(</strong></em></p>
<p>Some time after adopting @Baca's solution, I discovered that Kotlin Coroutines offer direct integration with slf4j, which is what Spring Sleuth builds on.
Sleuth adds properties <code>X-B3-TraceId</code>, <code>traceId</code>, <code>X-B3-SpanId</code>, and <code>spanId</code> to the thread's MDC.</p>
<p>You can retain the parent thread's MDC for a coroutine with the code shown below. The coroutine framework will take care of restoring the MDC context on the worker thread whenever the coroutine is executed/resumed. This is the easiest solution I could discover so far. :)</p>
<pre class=""lang-kotlin prettyprint-override""><code>// add your own properties or use the ones already added by Sleuth
MDC.put(&quot;someLoggerProperty&quot;, &quot;someValue&quot;)

GlobalScope.launch(MDCContext()) {
    // your code goes here
}
</code></pre>
<p>The launch method takes an optional CoroutineContext and the coroutine-slf4j integration implements the MDCContext. This class captures the calling thread's MDC context (creates a copy) and uses that for the coroutine execution.</p>
<p>Add this dependency to your build.gradle:</p>
<pre><code>implementation group: 'org.jetbrains.kotlinx', name: 'kotlinx-coroutines-slf4j', version: '1.3.9'
</code></pre>
<p>Project: <a href=""https://github.com/Kotlin/kotlinx.coroutines/tree/master/integration/kotlinx-coroutines-slf4j"" rel=""nofollow noreferrer"">https://github.com/Kotlin/kotlinx.coroutines/tree/master/integration/kotlinx-coroutines-slf4j</a>
Documentation: <a href=""https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-slf4j/index.html"" rel=""nofollow noreferrer"">https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-slf4j/index.html</a></p>
"
Zipkin,54555743,54555618,0,"2019/02/06, 16:19:12",False,"2019/02/06, 23:57:39",2802,2810730,1,"<p><a href=""https://zipkin.io/"" rel=""nofollow noreferrer"">Zipkin</a> is a solution for distributed tracing. Specifically it allows to track latency problems in distributed system. Also it's a greate tool for debugging/investigating problems in your application. So by definition it requires to collect successful and failed traces. However <a href=""https://zipkin.io/pages/architecture.html"" rel=""nofollow noreferrer"">traces</a> have nothing to do with logging.</p>

<p>Assuming you mean controlling the logging level of Zipkin server, then you can just set it using <a href=""https://github.com/openzipkin/zipkin/blob/master/zipkin-server/README.md#logging"" rel=""nofollow noreferrer"">--logging.level.zipkin2=INFO</a>.</p>
"
Zipkin,54555923,54555618,0,"2019/02/06, 16:29:05",False,"2019/02/06, 16:29:05",8336,1773866,0,"<p>I don't understand the problem. You don't send logs to Zipkin. You send spans to Zipkin. Zipkin has nothing to do with logs. </p>
"
Zipkin,51141657,51068201,0,"2018/07/02, 21:12:12",False,"2018/07/02, 21:12:12",81,2232476,0,"<p>The UI cannot be password protected without also password protecting the API endpoints, including the one you would send your spans to.</p>

<p>PS: The <code>@EnableZipkinServer</code> annotation has been deprecated</p>
"
Zipkin,66481285,66479206,1,"2021/03/04, 20:52:36",False,"2021/03/04, 20:52:36",1693,971735,0,"<p>This should be done out of the box: <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/2.2.7.RELEASE/reference/html/#feign"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/2.2.7.RELEASE/reference/html/#feign</a></p>
<p>You can take a look at the feign sample (you need to go back in the history, currently it is for 3.x): <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-feign"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/tree/master/spring-cloud-sleuth-samples/spring-cloud-sleuth-sample-feign</a></p>
<p>In order to see if propagation works, look into the outgoing request, it should contain the tracing-related headers.</p>
"
Zipkin,51438611,51438273,1,"2018/07/20, 11:46:33",False,"2018/07/20, 11:46:33",8336,1773866,1,"<p>If it comes from the <code>@Scheduled</code> method then you can use <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/SleuthSchedulingProperties.java#L38"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/scheduling/SleuthSchedulingProperties.java#L38</a> (<code>spring.sleuth.scheduled.skipPattern</code>) to find the thread and disable it. If you say its name is <code>async</code> then it means that it comes from a <code>TraceRunnable</code> or <code>TraceCallable</code>. That can be problematic to get rid off. You can file an issue in Sleuth to allow <code>SpanAdjuster</code> to actually not send spans to Zipkin (by for example returning <code>null</code>). You can also try to disable async at all <code>spring.sleuth.async.enabled</code>. If you're not using any other features of async that should not interfere.</p>
"
Zipkin,45048596,45046700,1,"2017/07/12, 07:41:40",False,"2017/07/12, 07:41:40",4434,7122593,0,"<p>Take a look at Sampling interval in the docs : </p>

<blockquote>
  <p>In distributed tracing the data volumes can be very high so sampling can be important (you usually don’t need to export all spans to get a good picture of what is happening). Spring Cloud Sleuth has a Sampler strategy that you can implement to take control of the sampling algorithm. Samplers do not stop span (correlation) ids from being generated, but they do prevent the tags and events being attached and exported. By default you get a strategy that continues to trace if a span is already active, but new ones are always marked as non-exportable. If all your apps run with this sampler you will see traces in logs, but not in any remote store. For testing the default is often enough, and it probably is all you need if you are only using the logs (e.g. with an ELK aggregator). If you are exporting span data to Zipkin or Spring Cloud Stream, there is also an AlwaysSampler that exports everything and a PercentageBasedSampler that samples a fixed fraction of spans.</p>
</blockquote>

<p><a href=""http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling"" rel=""nofollow noreferrer"">http://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_sampling</a></p>
"
Zipkin,63219184,62894286,0,"2020/08/02, 20:39:09",False,"2020/08/02, 20:46:19",11,11424150,1,"<p>Hi I just resolved this issue ..</p>
<p>Step1.verify C:\Programfiles\Err(version) folder including bin folder is created or not. Otherwise try to download and install Erlang again. reinstall RabbitMQ and try connecting Zipkin. Make sure Erlang version and RabbitMQ version is compatible.</p>
<p>Step2.Check ERLANG_HOME is set to proper location in environment variables.</p>
<p>at this point if RabbitMQ windows installer pointing to any old erlang version installed earlier  try to install RabitMQ windows manually
follow the steps mentioned in below link for manual installation</p>
<p><a href=""https://www.rabbitmq.com/install-windows-manual.html"" rel=""nofollow noreferrer"">https://www.rabbitmq.com/install-windows-manual.html</a></p>
"
Zipkin,62188352,62188289,1,"2020/06/04, 09:39:36",False,"2020/06/04, 09:39:36",4038,809423,0,"<p>yes, you can have multiple express running in the same node process (thats how clustering works in node as well)</p>

<p>but you will need to have them running on different ports.;</p>

<pre><code># const express = require('express')
const app1 = express()
app1.listen(3001, () =&gt; { //... })

//...

const app2 = express()
app2.listen(3002, () =&gt; { //... })
</code></pre>
"
Zipkin,62196735,62188289,1,"2020/06/04, 17:08:54",True,"2020/06/04, 17:08:54",501,1272149,1,"<p>It is possible to create two separate tracer providers. Only one of them will be the global tracer provider, which the API will use if you call API methods. You can't use the plugins in this configuration, which means you will have to manually instrument your application. If this is a use-case which is important to you, I suggest you create an issue on the github repo.</p>
"
Zipkin,39601988,39600581,2,"2016/09/20, 22:06:46",True,"2016/09/20, 22:06:46",8336,1773866,1,"<p>You'd have to implement your own ZipkinSpanReporter that would look more or less like <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/v1.0.8.RELEASE/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java"" rel=""nofollow"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/v1.0.8.RELEASE/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java</a> . In the next version of Sleuth you will be able to register a bean of ZipkinSpanReporter that can you a custom version of a publisher - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/1.0.x/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java"" rel=""nofollow"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/1.0.x/spring-cloud-sleuth-zipkin/src/main/java/org/springframework/cloud/sleuth/zipkin/HttpZipkinSpanReporter.java</a></p>
"
Zipkin,50010347,50010083,9,"2018/04/24, 23:34:00",True,"2018/04/24, 23:34:00",8336,1773866,3,"<p>You've mixed almost everything you could have mixed. On the app side you're using both the deprecated zipkin server and the deprecated client. On the server side you're using deprecated zipkin server. </p>

<p>My suggestion is that you go through the documentation <a href=""https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_spring_cloud_sleuth"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_spring_cloud_sleuth</a> and read that the <code>stream servers</code> are deprecated and you should use the openzipkin zipkin server with rabbitmq support (<a href=""https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/tree/master/zipkin-collector/rabbitmq</a>).</p>

<p>On the consumer side use <a href=""https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Edgware.SR3/single/spring-cloud.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka</a> . It really is as simple as that. Also don't forget to turn on the sampling percentage to 1.0</p>
"
Zipkin,66037294,65957785,0,"2021/02/04, 01:32:16",False,"2021/02/04, 01:32:16",3,6206060,0,"<p>I contributed to Micronaut and submitted a PR, which is now merged.
<a href=""https://github.com/micronaut-projects/micronaut-core/pull/4873"" rel=""nofollow noreferrer"">Pull request</a></p>
"
Zipkin,64845200,64818612,0,"2020/11/15, 15:54:52",True,"2020/11/15, 15:54:52",160,3244615,0,"<p>So it was application B which was not passing the header along. Turns out that the queue uri had a property <code>targetClient</code> which was set to 1. The uri is something like</p>
<pre><code>queue:///DESTINATION_QUEUE?targetClient=1
</code></pre>
<p>Now I am not an IBM MQ expert by far, but the <a href=""https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.2.0/com.ibm.mq.dev.doc/q032240_.htm"" rel=""nofollow noreferrer"">documentation</a> states that setting this property to 1 means that <code>Messages do not contain an MQRFH2 header.</code> I toggled it to 0 and voila, all spans fall into place.</p>
"
Zipkin,64215147,64214980,6,"2020/10/05, 22:40:51",False,"2020/10/05, 22:40:51",425,8214791,0,"<p>You must tell the containers the network &quot;foo_network&quot;. The External flag says that the containers are not accessible from outside. Of course you don't have to bet, but I thought as an example it might be quite good.</p>
<p>And because of the &quot;links&quot; function look here <a href=""https://docs.docker.com/compose/compose-file/#links"" rel=""nofollow noreferrer"">Link</a></p>
<pre><code>version: '2.4'

services:
  zipkin:
    image: openzipkin/zipkin-slim
    container_name: zipkin
    environment:
      - STORAGE_TYPE=mem
    ports:
      # Port used for the Zipkin UI and HTTP Api
      - 9411:9411
    depends_on:
      - storage
    networks:
      - foo_network

  storage:
    image: busybox:1.31.0
    container_name: fake_storage
    networks:
      - foo_network

  myfastapi:
    build: .
    ports:
      - 8000:8000
    links:
      - zipkin
    depends_on:
      - zipkin
    networks:
      - foo_network

  dependencies:
    image: busybox:1.31.0
    container_name: fake_dependencies
    networks:
     - foo_network

networks:
  foo_network:
    external: false
</code></pre>
"
Zipkin,63661339,63331441,0,"2020/08/30, 22:28:14",False,"2020/08/30, 22:28:14",11,6284144,0,"<p>I think I found the problem.</p>
<pre><code>@EnableEurekaClient
@EnableScheduling
@SpringBootApplication
@EnableAsync
public class AccountApplication {

    public static void main(String[] args) {
        SpringApplication.run(AccountApplication.class, args);
    }

    @Bean(&quot;threadPoolTaskExecutor&quot;)
    public Executor asyncExecutor() {
        final ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(20);
        executor.setMaxPoolSize(1000);
        executor.setThreadNamePrefix(&quot;AsyncThread-&quot;);
        executor.initialize();
        return executor;
    }

}
</code></pre>
<p>Than I used the Exceutor in my code:</p>
<pre><code>   @Async(&quot;threadPoolTaskExecutor&quot;)
   public CompletableFuture&lt;ServiceAccountDTO&gt; registerAccountInService(final String uuid, final ServiceEnum serviceEnum,
                                                                             final Date creationDate, final Date realCreationDate) {
    ..
    }
</code></pre>
<p>With this configuration the service was not starting correctly.</p>
<p>Now the @Bean(&quot;threadPoolTaskExecutor&quot;) configuration is removed and I'm using only @Async.</p>
<p>But why it's not working with Spring Boot Starter 2.3.x? And there was no error message in  the log.</p>
"
Zipkin,61571674,61571538,0,"2020/05/03, 11:35:32",True,"2020/05/03, 11:35:32",nan,nan,1,"<p>Zipkin is a Spring-Boot-based project, the @EnableZipkinServer is not a Spring Cloud annotation. It’s an annotation that’spart of the Zipkin project. This often confuses people who are new to the Spring Cloud Sleuth and Zipkin, because the Spring Cloud team did write the @EnableZipkinStreamServer annotation as part of Spring Cloud Sleuth. The @EnableZipkinStreamServer annotation simplifies the use of Zipkin with RabbitMQ and Kafka.</p>

<p>Advantages of  @EnableZipkinServer is simplicity in setup. With the @EnableZipkinStream server you need to set up and configure the services being traced and the Zipkin server to publish/listen to RabbitMQ or Kafka for tracing data.The advantage of the @EnableZipkinStreamServer annotation is that you can continue to collect trace data even if the Zipkin server is unavailable. This is because the trace messages will accumulate the trace data on a message queue until the Zipkin server is available for processing the records. If you use the @EnableZipkinServer annotation and the Zipkin server is unavailable,the trace data that would have been sent by the service(s) to Zipkin will be lost.</p>
"
Zipkin,60987725,60974758,2,"2020/04/02, 11:56:59",False,"2020/04/02, 11:56:59",8336,1773866,0,"<p>Please don't use field injection, use constructor injection. Also new span there doesn't make sense cause you already have a new span created by the framework.</p>
"
Zipkin,59405789,59405521,1,"2019/12/19, 10:24:24",False,"2019/12/19, 10:24:24",8336,1773866,1,"<p>Your versions are wrong. Please don't set the versions by yourself, please use the Spring Cloud BOM (spring-cloud-dependencies) dependency management like presented below</p>

<pre><code>buildscript {
    dependencies {
        classpath ""io.spring.gradle:dependency-management-plugin:0.5.2.RELEASE""
    }
}

apply plugin: ""io.spring.dependency-management""

dependencyManagement {
     imports {
          mavenBom ""org.springframework.cloud:spring-cloud-sleuth:${springCloudSleuthVersion}""
     }
}
dependencies {
    compile 'org.springframework.cloud:spring-cloud-starter-sleuth'
}
</code></pre>

<p>Also - it's enough for you to add the starters. You've added a starter in a given version and then you've added the core dependency in another one, that makes no sense.</p>

<p>Last thing - versions 1.x are deprecated and no longer maintained. The current version is 2.2.0.RELEASE and release train version is Hoxton.RELEASE</p>
"
Zipkin,58357995,58349450,1,"2019/10/12, 23:01:05",False,"2019/10/12, 23:01:05",1164,113183,0,"<p>This can be done using Finagle's <a href=""https://twitter.github.io/finagle/guide/Contexts.html"" rel=""nofollow noreferrer"">Contexts</a>.</p>

<blockquote>
  <p>Contexts give you access to request-scoped state, such as a request’s deadline, throughout the logical life of a request without requiring them to be explicitly passed</p>
</blockquote>
"
Zipkin,55535572,55090908,0,"2019/04/05, 15:37:34",False,"2019/04/05, 15:37:34",8336,1773866,0,"<p>Unfortunately the best answer to this issue is to upgrade to the latest version of Sleuth where we've migrated to Brave as an internal tracer and fixed a lot of issues.</p>
"
Zipkin,54445765,54445701,1,"2019/01/30, 19:00:50",False,"2019/01/30, 19:00:50",760,4178894,0,"<p>You can connect your existing container to another network</p>

<pre><code>docker network connect [OPTIONS] NETWORK CONTAINER
</code></pre>
"
Zipkin,54001979,53985744,0,"2019/01/02, 08:11:22",False,"2019/01/02, 08:11:22",1,10856416,-1,"<p>The error-code implies an error on the other end - 400-errors are not located on your end. Have you tried dumping the response (including headers)? Also, did you try to re-authenticate, perhaps reset cookings, etc? Did you contact the other end? How did they respond to it?</p>
"
Zipkin,53391639,53391503,0,"2018/11/20, 13:06:05",False,"2018/11/20, 13:06:05",26220,927493,0,"<p>Add a dependencyManagement entry for io.zipkin.zipkin2:zipkin:2.7.1</p>
"
Zipkin,53393882,53391503,0,"2018/11/20, 15:18:12",True,"2018/11/20, 15:18:12",330,1270045,0,"<p>For me or anybody finding this thread:
Solved it by upgrading from Camden to Edgware which contains 1.3.5 (and resolving everything around that switch).</p>
"
Zipkin,53185008,53154813,1,"2018/11/07, 09:18:45",True,"2018/11/07, 09:18:45",8336,1773866,1,"<p>You can create your own custom <code>SpanAdjuster</code> that will modify the span name. You can also use <code>FinishedSpanHandler</code> to operate on finished spans to tweak them.</p>
"
Zipkin,52481820,52156749,2,"2018/09/24, 17:42:00",False,"2018/09/24, 17:42:00",8336,1773866,0,"<p>Yes, when you create a span you can set the service name. Just call <code>newSpan.remoteServiceName(...)</code></p>
"
Zipkin,52784486,52156749,0,"2018/10/12, 20:33:48",False,"2018/10/12, 20:33:48",2970,4296607,0,"<p>Taking the input of @MarcinGrzejszczak as reference, I resolved using a custom span:</p>

<pre><code>Span remoteDependency = tracer.nextSpan()
                              .name(""dependency_name"") 
                              .start();
</code></pre>

<p>Where <code>tracer</code> is an autowired object from <code>Trace</code>:</p>

<pre><code>@Autowired
private Tracer tracer;
</code></pre>

<p>Both classes are in <code>brave</code> package</p>

<pre><code>import brave.Span;
import brave.Tracer;
</code></pre>

<p><strong>Result:</strong></p>

<p><a href=""https://i.stack.imgur.com/KSRXn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KSRXn.png"" alt=""Sample""></a></p>

<p>If you want to take a look at the implementation in more detail, here is the sample: <a href=""https://github.com/juanca87/sample-traceability-microservices"" rel=""nofollow noreferrer"">https://github.com/juanca87/sample-traceability-microservices</a></p>
"
Zipkin,51934542,51934410,2,"2018/08/20, 19:02:24",True,"2018/08/20, 19:02:24",874,1753823,0,"<p>add this under your  section at the end of your pom.xml. you may need to add for all the dependencies.</p>

<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
  &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
  &lt;version&gt;what-ever-version-number&lt;/version&gt;&lt;/dependency&gt;
</code></pre>
"
Zipkin,52823986,51534836,0,"2018/10/15, 23:04:45",False,"2018/10/15, 23:04:45",17842,7862821,0,"<p>Maybe, I didn't get your question right, but with almost your <code>docker-compose.yaml</code> file:</p>

<pre><code>version: '3'
services:
  storage:
    image: openzipkin/zipkin-mysql
    container_name: mysql

  zipkin:
    image: openzipkin/zipkin
    container_name: zipkin
    environment:
      - STORAGE_TYPE=mysql
      - MYSQL_HOST=mysql
    ports:
      - 9411:9411
    depends_on:
      - storage
</code></pre>

<p><code>prometheus</code> metrics are available on <code>localhost:9411/metrics</code> both inside container and on host system:</p>

<pre><code>$ curl localhost:9411/metrics
{""counter.zipkin_collector.messages.http"":0.0,""counter.zipkin_collector.spans_dropped.http"":0.0,""gauge.zipkin_collector.message_bytes.http"":0.0,""counter.zipkin_collector.bytes.http"":0.0,""gauge.zipkin_collector.message_spans.http"":0.0,""counter.zipkin_collector.spans.http"":0.0,""counter.zipkin_collector.messages_dropped.http"":0.0}
</code></pre>
"
Zipkin,51494483,51444018,0,"2018/07/24, 11:56:29",False,"2018/07/24, 11:56:29",402,4298311,0,"<p>I finally got it working. I just changed the logstash config file and added:</p>

<pre><code>input
{  
   tcp   
   {  
      port =&gt; 5000 
      type =&gt; ""java"" 
      codec =&gt; ""json""
   }
}
filter
{  
   if [type]== ""java""   
   {  
      json
      {  
         source =&gt; ""message""
      }
   }
}
</code></pre>

<p>The filter part was missing earlier.</p>
"
Zipkin,50922654,50922384,0,"2018/06/19, 10:19:26",False,"2018/06/19, 10:19:26",8336,1773866,0,"<p>Can you follow the guidelines described here <a href=""https://stackoverflow.com/help/how-to-ask"">https://stackoverflow.com/help/how-to-ask</a> and the next question you ask, ask it with more details? E.g. I have no idea how exactly you use Sleuth? Anyways I'll try to answer...</p>

<p>You can create a <code>SpanAdjuster</code> bean, that will analyze the span information (e.g. span tags) and basing on that information you will change the sampling decision so as not to send it to Zipkin.</p>

<p>Another option is to wrap the default span reporter in a similar logic. </p>

<p>Yet another option is to verify what kind of a thread it is that is creating this span and toggle it off (assuming that it's a <code>@Scheduled</code> method) - <a href=""https://cloud.spring.io/spring-cloud-static/Finchley.RC2/single/spring-cloud.html#__literal_scheduled_literal_annotated_methods"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Finchley.RC2/single/spring-cloud.html#__literal_scheduled_literal_annotated_methods</a></p>
"
Zipkin,50709747,50691266,0,"2018/06/06, 01:22:16",False,"2018/06/06, 02:13:47",1,9897847,-2,"<pre><code>apiVersion: v1
kind: Service
metadata:
  name: gearbox-rack-eureka-server
  labels:
    name: gearbox_rack_eureka_server
spec:
  selector:
    app: gearbox-rack-eureka-server
  type: ClusterIP
  ports:
    - port: 8761
      name: tcp
</code></pre>

<hr>

<p>name: tcp -> protocol: TCP ?</p>
"
Zipkin,49899755,49899572,0,"2018/04/18, 15:29:29",True,"2018/04/18, 15:29:29",8336,1773866,0,"<blockquote>
  <p>I tried to back to use the stable version(1.3.3) of spring cloud sleuth, but when i use the bom for the project its make conflict in the spring boot version that i am using(2.0). Its compactible with the spring boot version 2?</p>
</blockquote>

<p>You can't use the Sleuth 1.3 with Boot 2.0. </p>

<blockquote>
  <p>I am using the spring cloud sleuth to make trace of services on my company, but i have a version of tracing on others services that is not compactible with the opentracing headers, so i want to change the headers of http messages to make the new services compactibles with the current tracing headers that i have in the others components.</p>
</blockquote>

<p>Yeah, that's the Brave change. For http you can define your own parses. <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceHttpAutoConfiguration.java#L57-L68"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/TraceHttpAutoConfiguration.java#L57-L68</a> . </p>

<p><code>
@Autowired HttpClientParser clientParser;
    @Autowired HttpServerParser serverParser;
    @Autowired @ClientSampler HttpSampler clientSampler;
    @Autowired(required = false) @ServerSampler HttpSampler serverSampler;
</code></p>

<p>These are the samplers that you can register.</p>

<p>For messaging you'd have to create your own version of the global channel interceptor - like the one we define here - <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/messaging/TraceSpringIntegrationAutoConfiguration.java#L49-L53"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/messaging/TraceSpringIntegrationAutoConfiguration.java#L49-L53</a> .</p>

<p>If that's not acceptable for you, go ahead and file an issue in Sleuth so we can discuss it there.</p>
"
Zipkin,49402195,49393630,0,"2018/03/21, 11:13:30",True,"2018/03/21, 11:13:30",762,1227937,0,"<p>There's no concept of a trace finishing in zipkin. A span within a trace can start and finish. We don't start and stop spans on different hosts, so unfinished spans probably are accidental. You can chat more here if you like <a href=""https://gitter.im/spring-cloud/spring-cloud-sleuth"" rel=""nofollow noreferrer"">https://gitter.im/spring-cloud/spring-cloud-sleuth</a></p>
"
Zipkin,49242542,49241122,0,"2018/03/12, 20:53:14",False,"2018/03/12, 20:53:14",8336,1773866,1,"<p>In Sleuth <code>1.3.x</code> you can create a custom <code>SpanReporter</code> that, before sending a span to Zipkin, would analyze the URL and would not report that span. In Sleuth <code>2.0.x</code> you can create a custom <code>HttpSampler</code> for the client side (with name <code>sleuthClientSampler</code>)</p>
"
Zipkin,49202659,49201687,1,"2018/03/09, 23:47:19",True,"2018/03/09, 23:47:19",13548,33404,1,"<p>Turns out the issue was a corrupt Maven package. Deleting my <code>.m2\repository</code> folder and running <code>mvn spring-boot:run</code> to downloaded dependencies and run my app resolved the issue.</p>
"
Zipkin,48900919,48893982,0,"2018/02/21, 10:12:42",True,"2018/02/21, 10:12:42",8336,1773866,0,"<p>Please read this page from Zipkin - <a href=""https://zipkin.io/pages/instrumenting.html"" rel=""nofollow noreferrer"">https://zipkin.io/pages/instrumenting.html</a> . It's all written there how things should work.</p>

<p>HTTP Tracing</p>

<p>HTTP headers are used to pass along trace information.</p>

<p>The B3 portion of the header is so named for the original name of Zipkin: BigBrotherBird.</p>

<pre><code>Ids are encoded as hex strings:

X-B3-TraceId: 128 or 64 lower-hex encoded bits (required)
X-B3-SpanId: 64 lower-hex encoded bits (required)
X-B3-ParentSpanId: 64 lower-hex encoded bits (absent on root span)
X-B3-Sampled: Boolean (either “1” or “0”, can be absent)
X-B3-Flags: “1” means debug (can be absent)
For more information on B3, please see its specification.
</code></pre>

<p>Also please check the B3 specification page - <a href=""https://github.com/openzipkin/b3-propagation"" rel=""nofollow noreferrer"">https://github.com/openzipkin/b3-propagation</a></p>
"
Zipkin,48874125,48873756,13,"2018/02/19, 23:02:35",True,"2018/02/19, 23:02:35",8336,1773866,0,"<blockquote>
  <p>people are scared about the overhead which sleuth can have by adding @NewSpan on the methods.</p>
</blockquote>

<p>Do they have any information about the overhead? Have they turned it on and the application started to lag significantly? What are they scared of? Is this a high-frequency trading application that you're doing where every microsecond counts? </p>

<blockquote>
  <p>I need to decide on runtime whether the Trace should be added or not (Not talking about exporting). Like for actuator trace is not getting added at all. I assume this will have no overhead on the application. Putting X-B3-Sampled = 0 is not exporting but adding tracing information. Something like skipPattern property but at runtime.</p>
</blockquote>

<p>I don't think that's possible. The instrumentation is set up by adding interceptors, aspects etc. They are started upon application initialization.</p>

<blockquote>
  <p>Always export the trace if service exceeds a certain threshold or in case of Exception.</p>
</blockquote>

<p>With the new Brave tracer instrumentation (Sleuth 2.0.0) you will be able to do it in a much easier way. Prior to this version you would have to implement your own version of a <code>SpanReporter</code> that verifies the tags (if it contains an <code>error</code> tag), and if that's the case send it to zipkin, otherwise not.</p>

<blockquote>
  <p>If I am not exporting Spans to zipkin then will there be any overhead by tracing information?</p>
</blockquote>

<p>Yes, there is cause you need to pass tracing data. However, the overhead is small.</p>
"
Zipkin,48006082,47934052,2,"2017/12/28, 12:49:29",False,"2017/12/28, 12:49:29",8336,1773866,0,"<p>The code you've provided is not related to Sleuth but opentracing. In Sleuth you would call <code>Tracer.createSpan(""name"")</code> and that way a child span od your current trace would be created.</p>
"
Zipkin,48013962,47934052,0,"2017/12/28, 22:29:13",False,"2017/12/28, 22:29:13",21,6203524,0,"<p>I've also managed to get it working by using just the cloud trace api by doing this before I create a span.</p>

<pre><code>SpanContext spanContext = Trace.getSpanContextFactory().fromHeader(traceId);
Trace.getSpanContextHandler().attach(spanContext);
</code></pre>

<p>Not sure if there is a negative of doing this.</p>
"
Zipkin,42800431,42499468,0,"2017/03/15, 05:08:37",True,"2017/03/15, 05:08:37",91,5460458,0,"<p>I succeeded to send gcp's trace api in php client via REST. It can see trace set by php client parameters , but my endpoint for trace api has stopped though I don't know why.Maybe ,it is not still supported well because the document have many ambiguous expression so, I realized watching server response by BigQuery with fluentd and DataStudio and it seem best solution because auto span can be set by table name with yyyymmdd and we can watch arbitrary metrics with custom query or calculation field.</p>
"
Zipkin,41441905,41441661,1,"2017/01/03, 12:56:57",True,"2017/01/03, 12:56:57",762,1227937,2,"<p>Zipkin's connection to cassandra is independent from the normal spring setup. We use some very specific setup. you'll want to set properties in the namespace of zipkin.storage.cassandra</p>

<p><a href=""https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/resources/zipkin-server-shared.yml#L40"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/blob/master/zipkin-server/src/main/resources/zipkin-server-shared.yml#L40</a></p>
"
Zipkin,61514513,61514326,0,"2020/04/30, 04:37:29",True,"2020/04/30, 04:37:29",396,4641689,1,"<p>I would say you can have fluentd in multiple namespaces and Elasticsearch in one namespace and fluentd can discover Elasticsearch via K8s internal DNS A/AAAA record e.g. <code>elasticsearch.${namespace}.svc.cluster.local</code>.</p>

<p>I don't have any link to the best practice, but I would show you a practice I saw from the community.</p>

<ul>
<li><p>If you are not familiar with configuring K8s cluster, I recommend to deploy ELK by Helm. It will save you a lot of time and give you enough configuration options.
<a href=""https://github.com/helm/charts/tree/master/stable/elastic-stack"" rel=""nofollow noreferrer"">https://github.com/helm/charts/tree/master/stable/elastic-stack</a>.</p></li>
<li><p>Install your ELK helm release on a <code>separate</code> namespace, for example: <code>logging</code>.</p></li>
<li><p>Install fluentd in any namespaces in your cluster and configure elasticsearch host <a href=""https://github.com/helm/charts/tree/master/stable/fluentd-elasticsearch"" rel=""nofollow noreferrer"">https://github.com/helm/charts/tree/master/stable/fluentd-elasticsearch</a></p></li>
</ul>
"
Zipkin,59577425,59576942,0,"2020/01/03, 13:02:50",False,"2020/01/03, 13:02:50",5244,5564578,2,"<p>By default, you don't have a logging system on Istio. I mean, besides the native logging of Kubernetes.</p>

<p>Zipkin and Jaeger are tracing systems, meaning for latency, not for logging.</p>

<p>You can definitely get this info through Istio components, but you will have to set it up first. I found <a href=""https://istio.io/docs/tasks/observability/logs/"" rel=""nofollow noreferrer"">this</a> articles; in Istio website about how to collect logs. I would say <code>Fluentd</code> + <code>Elasticsearch</code> would give you something as powerful as you need. Unfortunately I don't have any examples.</p>
"
Zipkin,48765679,48668426,0,"2018/02/13, 13:23:09",False,"2018/02/13, 13:23:09",681,4255878,0,"<p>i think i found a suitable way to do this. After further thinking my idea went into the direction of using annotations and aspects for intercepting HTTP requests from/to thrifts http client which seems to be quite some work.</p>

<p>after further search, i found this library for spring-boot serving exactly my needs:
<a href=""https://github.com/aatarasoff/spring-thrift-starter"" rel=""nofollow noreferrer"">https://github.com/aatarasoff/spring-thrift-starter</a></p>
"
Zipkin,57200810,57143690,0,"2019/07/25, 14:23:59",False,"2019/07/25, 14:23:59",9,4762878,1,"<p>I used an Aspect and from the returned ResponseEntity object, decided whether or not to programatically add an error tag to span. With this tag, zipkin will identify and highlight the trace in red colour. 
Below is the code snippet to add error tag to span.</p>

<pre><code>import org.springframework.cloud.sleuth.Span;
import org.springframework.cloud.sleuth.Tracer;
----
@Autowired
private Tracer tracer;

public void addErrorTag(String message) {
    Span currentSpan = tracer.getCurrentSpan();
    currentSpan.logEvent(""ERROR: "" + message);
    tracer.addTag(""error"", message);
}
</code></pre>
"
Zipkin,52329845,52145774,0,"2018/09/14, 13:15:30",True,"2018/09/14, 13:15:30",360,1796107,0,"<p>Your best chance is to get the data from whatever storage the Jaeger collector is using (Cassandra, Elastic.) (<a href=""https://www.jaegertracing.io/docs/1.6/deployment/"" rel=""nofollow noreferrer"">https://www.jaegertracing.io/docs/1.6/deployment/</a> )</p>

<p>My suggestion is to store in Elastic and use Kibana to accomplish what you need.</p>
"
Zipkin,65919685,52145774,0,"2021/01/27, 14:59:44",False,"2021/01/27, 14:59:44",1869,3511252,0,"<p>Old topic but the current version of Jaeger Query UI is a single page app and has an underlying API that allows the same queries capabilities as the UI.</p>
"
Zipkin,50798776,50796087,4,"2018/06/11, 16:23:06",True,"2018/06/11, 16:39:48",8336,1773866,1,"<p>That was a bug in Spring Cloud Sleuth in Edgware. The Stream Kafka Binder in Edgware required explicit passing of headers that should get propagated. The side effect of adding <code>sleuth-stream</code> on the classpath was exactly that feature. By fixing the <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/issues/1005</a> issue we're adding back the missing feature to core. This is not ported to Finchley since Stream Kafka Binder in Finchley passes all headers by default.</p>

<p>The workaround for Edgware is to pass a list of headers in the following manner:</p>

<pre><code>spring:
  cloud:
    stream:
      kafka:
        binder:
          headers:
            - spanId
            - spanSampled
            - spanProcessId
            - spanParentSpanId
            - spanTraceId
            - spanName
            - messageSent
</code></pre>
"
Zipkin,66147291,66141019,0,"2021/02/11, 02:50:17",False,"2021/02/11, 02:50:17",1693,971735,0,"<p>Based on this:</p>
<blockquote>
<p>The problem is each service is capturing trace without issue but I can't see service to service communication and architectural design.</p>
</blockquote>
<p>it seems that the tracing information is not propagated across services. You can check this by looking into the HTTP headers and check the <code>traceId</code>. In order to make this work the <code>traceId</code> should be the same across the requests. You should see the same <code>traceId</code> in the logs too.</p>
<p>The documentation gives you some pointers how to troubleshoot this:</p>
<ul>
<li><a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#how-to-make-components-work"" rel=""nofollow noreferrer"">How to Make RestTemplate, WebClient, etc. Work?</a></li>
<li><a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#sleuth-http-client-webclient-integration"" rel=""nofollow noreferrer"">WebClient integration</a></li>
</ul>
"
Zipkin,47120862,46164976,1,"2017/11/05, 13:08:17",True,"2017/11/05, 13:08:17",26,5772061,1,"<p>The best way to trace OpenStack project is to use Osprofiler library. If you just want to understand the workflow or just know about the types of calls being made inside OpenStack then Osprofiler is the best and easiest way to get the trace. Now Osprofiler is an accepted OpenStack project and the official project to get traces for OpenStack.</p>

<p>Instead of having to go through the whole code and adding instrumentation points near HTTP request or RPC calls, osprofiler is already integrated in all of the main projects of OpenStack (Nova, Neutron, Keystone, Glance etc..). You just have to enable osprofiler in the configuration files of each project in OpenStack to get a trace of that particular project.</p>

<p>You can go through this link - <a href=""https://docs.openstack.org/osprofiler/latest/"" rel=""nofollow noreferrer"">https://docs.openstack.org/osprofiler/latest/</a></p>

<p>Enabling of Osprofiler in the configuration files can be done by adding these lines at the end of the configuration file (nova.conf or neutron.conf) :</p>

<pre><code>[profiler]
enabled = True
trace_sqlalchemy = True
hmac_keys = SECRET_KEY
connection_string = messaging://
</code></pre>

<p>The connection_string parameter indicates the collector (where the trace information is stored). By default it uses Ceilometer.
You can actually redirect the trace information to other collectors like Elasticsearch by changing the connection_string parameter in the conf file to the elasticsearch server.</p>

<p>This is by far the easiest way to get a trace in OpenStack with just minimal effort. </p>
"
Zipkin,58754849,58619789,0,"2019/11/07, 20:21:08",False,"2019/11/07, 20:21:08",21,9936877,0,"<p>Still with using 2.2.0 parent, I still face the whitelable error.
I will check on this latter but by changing the pom defination the Zipkin server work</p>
<h1>pom.xml</h1>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; 
&lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;!-- &lt;version&gt;2.2.0.RELEASE&lt;/version&gt; --&gt;
    &lt;version&gt;1.5.10.RELEASE&lt;/version&gt;
    &lt;!-- &lt;relativePath/&gt; lookup parent from repository  --&gt;
&lt;/parent&gt;
&lt;groupId&gt;edu.rohit&lt;/groupId&gt; 
&lt;artifactId&gt;ZipkinServer&lt;/artifactId&gt;  
&lt;version&gt;1&lt;/version&gt;  
&lt;name&gt;ZipkinServer&lt;/name&gt; 
&lt;description&gt;Demo project for Spring Boot Slueth-Zipkin&lt;/description&gt;

&lt;properties&gt;
    &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;!-- &lt;spring-cloud.version&gt;Hoxton.RC1&lt;/spring-cloud.version&gt; --&gt;
    &lt;!-- &lt;spring-cloud.version&gt;2.1.3.RELEASE&lt;/spring-cloud.version&gt; --&gt;
    &lt;spring-cloud.version&gt;Edgware.RELEASE&lt;/spring-cloud.version&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;!-- &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-web-services&lt;/artifactId&gt;
    &lt;/dependency&gt; --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
        &lt;!-- &lt;version&gt;${spring-cloud.version}&lt;/version&gt; --&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
        &lt;!-- &lt;version&gt;2.11.7&lt;/version&gt; --&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
        &lt;!-- &lt;version&gt;2.11.7&lt;/version&gt; --&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;
    
&lt;/dependencies&gt;

&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
            &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
            &lt;type&gt;pom&lt;/type&gt;
            &lt;scope&gt;import&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;

&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;

&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;spring-milestones&lt;/id&gt;
        &lt;name&gt;Spring Milestones&lt;/name&gt;
        &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt;
        &lt;snapshots&gt;
            &lt;enabled&gt;false&lt;/enabled&gt;
        &lt;/snapshots&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
&lt;/project&gt;
</code></pre>
<p>And in zipkinserverapplication we need the @Enablezipkinserver</p>
<h1>ZipkinServerApplication</h1>
<pre><code>package edu.rohit.ZipkinServer;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;
import zipkin.server.EnableZipkinServer;

@EnableZipkinServer
@SpringBootApplication(scanBasePackages={&quot;edu.rohit.ZipkinServer&quot;})

public class ZipkinServerApplication {

 public static void main(String[] args) {
     SpringApplication.run(ZipkinServerApplication.class, args);
 }
}
</code></pre>
"
Zipkin,58755169,58619789,2,"2019/11/07, 20:47:05",False,"2019/11/07, 20:47:05",3589,5587542,0,"<p>Hi if your spring cloud app target is a Spring Boot 2.x base I suggest to do not try to use the @EnableZipkinServer because it is not a raccomanded way as the java doc suggest:</p>

<p>form the Zipkin base code:</p>

<pre><code>/**
 * @deprecated Custom servers are possible, but not supported by the community. Please use our
 * &lt;a href=""https://github.com/openzipkin/zipkin#quick-start""&gt;default server build&lt;/a&gt; first. If you
 * find something missing, please &lt;a href=""https://gitter.im/openzipkin/zipkin""&gt;gitter&lt;/a&gt; us about
 * it before making a custom server.
 *
 * &lt;p&gt;If you decide to make a custom server, you accept responsibility for troubleshooting your
 * build or configuration problems, even if such problems are a reaction to a change made by the
 * OpenZipkin maintainers. In other words, custom servers are possible, but not supported.
 */
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Import(InternalZipkinConfiguration.class)
@Deprecated
public @interface EnableZipkinServer {

}
</code></pre>

<p>I spoke for personal experience with spring boot application 2.x family. 
The my solution,for development, was use a docker compose like below(consider that my application use spring cloud stream in order to push on zipkin the tracing information:</p>

<pre><code>version: '3.1'

services:
  rabbitmq:
    image: rabbitmq:3-management
    restart: always
    ports:
      - 5672:5672
      - 15671:15671
      - 15672:15672
    networks:
      - messaging

  zipkin-server:
    image: openzipkin/zipkin
    ports:
      - 9065:9411
    environment:
      - zipkin.collector.rabbitmq.uri=amqp://guest:guest@rabbitmq:5672
    networks:
      - messaging

networks:
  messaging:
    driver: bridge
</code></pre>

<p>On the other hands if your target is a spring boot 1.5.x you can use the legacy embedded zipkin server like below:</p>

<p>POM:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;it.valeriovaudi.emarket&lt;/groupId&gt;
    &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;

    &lt;name&gt;zipkin-server&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;

    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;1.5.3.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Dalston.RELEASE&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- start web dependencies --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- end web dependencies --&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;


        &lt;!-- start distributed tracing dependencies --&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- EXAMPLE FOR RABBIT BINDING --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- end distributed tracing dependencies --&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;!-- start test dependencies --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;!-- end test dependencies --&gt;
    &lt;/dependencies&gt;

    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;build&gt;
        &lt;finalName&gt;zipkin-server&lt;/finalName&gt;

        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>

<p>Application:</p>

<p>package it.valeriovaudi.emarket;</p>

<pre><code>@SpringBootApplication
@EnableZipkinStreamServer
public class ZipkinServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZipkinServerApplication.class, args);
    }
}
</code></pre>

<p>both solution works for me, spring boot 1.5.x was the base spring boot version for my master thesis and spring boot 2.x version for a my personal distributed system project that I use every day for my persona family budget management.</p>

<p>I hope that this can help you</p>
"
Zipkin,58799041,58619789,0,"2019/11/11, 11:50:52",False,"2019/11/11, 11:50:52",21,9936877,0,"<pre><code>I did some study in actuator use in spring boot 2.x.
The actuator use have significant changes. The default endpoint root path is '/actuator' and not'/' as in 1.x. By default only two endpoint(/health, /info) is enable.

</code></pre>

<pre><code>#To make all endpoints enable set
    management.endpoints.web.exposure.include=*   
#To specifically enable one endpoint(/beans) set
    management.endpoint.beans.enabled=true 
#Or we can create a WebSecurity bean like this:
    @Bean
    public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http){
          return http.authorizeExchange()
         .pathMatchers(""/actuator/**"").permitAll()
         .anyExchange().authenticated()
         .and().build();
    }  
#Remember in spring boot 1.x all this issues was resolved by setting the bellow but it is #depreciated in 2.x
    management.security.enabled=false
#By default the base path is '/actuator' but can be changed by setting
    management.endpoints.web.base-path= /&lt;Mypath&gt;
</code></pre>

<p>For more details can read the document <a href=""https://www.baeldung.com/spring-boot-actuators#boot-2x-actuator"" rel=""nofollow noreferrer"">https://www.baeldung.com/spring-boot-actuators#boot-2x-actuator</a></p>
"
Zipkin,57295589,57273076,2,"2019/07/31, 19:48:01",False,"2019/07/31, 19:48:01",13313,524946,0,"<p>OpenTracing does not define the concrete data model, how the data should be collected, nor how it should be transported. As such, there's no specification for the endpoint. This allows implementations like Jaeger to use non-HTTP transport by default when sending data from the client (tracer) to the backend, by sending UDP packets to an intermediate ""Jaeger Agent"".</p>

<p>Given that the base model is pretty much similar among implementations, it's common to have tracing solutions to support each other's endpoints. For instance, Jaeger is able to expose an endpoint with <a href=""https://www.jaegertracing.io/docs/1.13/getting-started/#migrating-from-zipkin"" rel=""nofollow noreferrer"">Zipkin compatibility</a>.</p>

<p>Based on your question, I think you might be interested in the OpenTelemetry project, a successor to the OpenTracing project, as the result of a merge with the OpenCensus project. OpenTelemetry provides its own tracer and is able to ""receive"" data in several formats (including Jaeger), and ""export"" to several backends.</p>
"
Zipkin,44395594,44382633,1,"2017/06/06, 19:47:13",False,"2017/06/06, 19:47:13",37971,1237575,0,"<p>You are blending Zipkin autoconfigure version 1.2 with Zipkin 1.26. This results in a version missmatch.</p>
"
Zipkin,44403836,44382633,0,"2017/06/07, 07:58:45",False,"2017/06/07, 07:58:45",1,7608262,0,"<p>The problem is casued by two reasons. First:just as Rafael Winterhalter said the version dismatched; second: you are lacking dependency for zipkin. Finally i fixed the problem by adding the whole 'io.zipkin.java:zipkin"" library. Here is my final pom file with the storage type of elasticsearch:</p>

<pre><code> &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin&lt;/artifactId&gt;
            &lt;version&gt;1.21.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
            &lt;version&gt;1.21.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
            &lt;version&gt;1.21.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
            &lt;artifactId&gt;zipkin-autoconfigure-storage-elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;1.21.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.integration&lt;/groupId&gt;
            &lt;artifactId&gt;spring-integration-jmx&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
</code></pre>
"
Zipkin,52975826,52972425,3,"2018/10/24, 21:33:48",False,"2018/10/24, 21:33:48",8295,4550110,0,"<p>Here is the related issue:</p>

<pre><code>we also mentioned recently that for data to appear, applications need to be
sending traces

https://github.com/openzipkin/zipkin#quick-start

you can tell also by hitting the /metrics endpoint and look at stats named
collector
</code></pre>

<p><a href=""https://github.com/openzipkin/zipkin/issues/1939"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/issues/1939</a></p>
"
Zipkin,52987258,52972425,0,"2018/10/25, 13:38:05",False,"2018/10/25, 13:38:05",1,10552245,0,"<p>I opened a issue on the zipkin github, a theme already being treated as a bug.</p>

<p>Initial thread:
<a href=""https://github.com/openzipkin/zipkin/issues/2218#issuecomment-432876510"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/issues/2218#issuecomment-432876510</a></p>

<p>Bug track:
<a href=""https://github.com/openzipkin/zipkin/issues/2219"" rel=""nofollow noreferrer"">https://github.com/openzipkin/zipkin/issues/2219</a></p>

<p>Tks for all!</p>
"
Zipkin,66384663,66383029,4,"2021/02/26, 13:04:31",True,"2021/02/26, 13:04:31",8336,1773866,2,"<p>You could create your own <code>SpanHandler</code> bean that takes the <code>FinishedSpan</code>, converts into JSON and stores it somewhere on your drive. Then you could just iterate over jsons and upload them to the Zipkin server</p>
"
Zipkin,43153293,43151368,0,"2017/04/01, 08:46:41",True,"2017/04/01, 08:46:41",762,1227937,1,"<p>The TL;DR; is that <a href=""https://github.com/openzipkin/b3-propagation"" rel=""nofollow noreferrer"">B3 propagation</a> was initially designed for fixed size data: carrying data ancillary to tracing isn't in scope, and for this reason any solution that extends B3 in such a fashion wouldn't be compatible with existing code.</p>

<p>So, that means any solution like this will be an extension which means custom handling in the <a href=""http://zipkin.io/pages/architecture.html"" rel=""nofollow noreferrer"">instrumented apps</a> which are the things passing headers around. The server won't care as it never sees these headers anyway.</p>

<p>Ways people usually integrate other things like flags with zipkin is to add a tag aka binary annotation including its value (usually in the root span). This would allow you to query or retrieve these offline, but it doesn't address in-flight lookups from applications.</p>

<p>Let's say that instead of using an intermediary like linkerd, or a platform-specific propagated context, we want to dispatch the responsibility to the tracing layer. Firstly, what sort of data could work alright? The easiest is something set-once (like zipkin's trace id). Anything set and propagated without mutating it is the least mechanics. Next in difficulty is appending new entries mid-stream, and most difficult is mutating/merging entries.</p>

<p>Let's assume this is for inbound flags which never change through the request/trace tree. We see a header when processing trace data, we store it and forward it downstream. If this value doesn't need to be read by the tracing system, it is easiest, as it is largely a transport/propagation concern. For example, maybe other middleware read that header and it is only a ""side job"" we are adding to the tracer to remember certain things to pass along. If this was done in a single header, it would be less code than a pattern in each of the places this would be to added. It would be even less code if the flags could be encoded in a number, however unrealistic that may be.</p>

<p>There are libraries with apis to manipulate the propagated context manually, for example, <a href=""https://github.com/JonathanMace/tracingplane"" rel=""nofollow noreferrer"">""baggage"" from brownsys</a> and OpenTracing (of which some libraries support zipkin). The former aims to be a generic layer for any instrumentation (ex monitoring, chargeback, tracing etc) and the latter is specific to tracing. OpenTracing has defines abstract types like <a href=""http://opentracing.io/documentation/pages/api/cross-process-tracing.html"" rel=""nofollow noreferrer"">injector and extractor</a> which could be customized to carry other fields. However, you still would need a concrete implementation (which knows your header format etc) in order to do this. Unless you want applications to read this data, it would need to be a secret detail of that implementation (specifically the trace context).</p>

<p>Certain zipkin-specific libraries like Spring Cloud Sleuth and Brave have means to <a href=""https://github.com/openzipkin/brave/blob/master/brave/src/main/java/brave/propagation/Propagation.java"" rel=""nofollow noreferrer"">customize how headers are parsed</a>, to support variants of B3 or new or site-specific trace formats. Not all support this at the moment, but I would expect this type of feature to become more common. This means you may need to do some surgery in order to support all platforms you may need to support.</p>

<p>So long story short is that there are some libraries which are pluggable with regards to propagation, and those will be easiest to modify to support this use case. Some code will be needed regardless as B3 doesn't currently define an expression like this.</p>
"
Zipkin,42901789,42884672,0,"2017/03/20, 12:55:54",False,"2017/03/20, 12:55:54",37971,1237575,0,"<p>Zipkin generates traces and communicates them back to a Zipkin server. The latter action is typically performed via HTTP but Zipkin is agnostic to how a span is started and ended.</p>

<p>If you want to measure the execution time of a single method, you would normally create a local span that is nested inside a server span. Zipkin is a distributrd tracing tool for discovering machine-to-machine interaction which is why spans often cover HTTP. If you want to measure execution time of a method, a tool like metrics might be more suited.</p>
"
Zipkin,47665630,47655299,2,"2017/12/06, 03:58:02",False,"2017/12/06, 03:58:02",693,2901325,0,"<p>I'm not sure I fully understand your question but I can elaborate a bit on how istio works with respect to tracing:</p>

<p>Tracing means identifying every span or node that is part of the original request, so typically an Id is generated by the istio-ingress and your application should <a href=""https://istio.io/docs/tasks/telemetry/distributed-tracing.html#understanding-what-happened"" rel=""nofollow noreferrer"">propagate it</a> so each istio-proxy can capture and forward that information to istio-mixer which then lets you use Zipkin or Jaeger to visualize it.</p>

<p>Istio can't know when you make outcalls from your application for which original request it was for unless you do copy the headers.</p>

<p>Does that help/makes sense ?</p>
"
Zipkin,45568932,45174992,0,"2017/08/08, 15:53:02",True,"2017/08/08, 15:53:02",45,4107097,0,"<p>After some discussion with Marcin Grzejszczak and Adrien Cole (zipkin and sleuth creators/active developers) I ended up creating a Jersey filter that acts as bridge between sleuth and brave. Regarding AMQP integration, added a new @StreamListener with a conditional for zipkin format spans (using headers). Sending messages to the sleuth exchange with zipkin format will then be valid and consumed by the listener. For javascript (zipkin-js), I ended up creating a new AMQP Logger that sends zipkin spans to a determined exchange. If someone ends up reading this and needs more detail, you're welcome to reach out to me.</p>
"
Zipkin,63384808,63384688,0,"2020/08/13, 00:15:25",True,"2020/08/13, 00:15:25",8336,1773866,1,"<p>It's because of sampling. Please create a bean of sampler type whose value can be Sampler.ALWAYS or set the probability property to 1.0</p>
"
Zipkin,59580066,59574885,1,"2020/01/03, 16:14:30",True,"2020/01/03, 17:50:28",2576,12014434,1,"<p>According to envoy proxy documentation for envoy <a href=""https://www.envoyproxy.io/docs/envoy/v1.12.0/intro/arch_overview/observability/tracing.html?highlight=traceid#trace-context-propagation"" rel=""nofollow noreferrer""><code>v1.12.0</code></a> used by istio <code>1.3</code>:</p>

<blockquote>
  <h2>Trace context propagation<a href=""https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing.html?highlight=traceid#trace-context-propagation"" rel=""nofollow noreferrer"" title=""Permalink to this headline""></a></h2>
  
  <p>Envoy provides the capability for reporting tracing information regarding communications between services in the mesh. However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests.</p>
  
  <p>Whichever tracing provider is being used, the service should propagate the  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-request-id"" rel=""nofollow noreferrer"">x-request-id</a>  to enable logging across the invoked services to be correlated.</p>
  
  <p>The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood. This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests. This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace.</p>
  
  <p>Alternatively the trace context can be manually propagated by the service:</p>
  
  <ul>
  <li><p>When using the LightStep tracer, Envoy relies on the service to propagate the 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-ot-span-context"" rel=""nofollow noreferrer"">x-ot-span-context</a>
  HTTP header while sending HTTP requests to other services.</p></li>
  <li><p>When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-traceid"" rel=""nofollow noreferrer"">x-b3-traceid</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-spanid"" rel=""nofollow noreferrer"">x-b3-spanid</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-parentspanid"" rel=""nofollow noreferrer"">x-b3-parentspanid</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-sampled"" rel=""nofollow noreferrer"">x-b3-sampled</a>,
  and 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-flags"" rel=""nofollow noreferrer"">x-b3-flags</a>).
  The 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-b3-sampled"" rel=""nofollow noreferrer"">x-b3-sampled</a>
  header can also be supplied by an external client to either enable or
  disable tracing for a particular request. In addition, the single 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-b3"" rel=""nofollow noreferrer"">b3</a>
  header propagation format is supported, which is a more compressed
  format.</p></li>
  <li><p>When using the Datadog tracer, Envoy relies on the service to propagate the Datadog-specific HTTP headers ( 
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-datadog-trace-id"" rel=""nofollow noreferrer"">x-datadog-trace-id</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-datadog-parent-id"" rel=""nofollow noreferrer"">x-datadog-parent-id</a>,
  <a href=""https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers#config-http-conn-man-headers-x-datadog-sampling-priority"" rel=""nofollow noreferrer"">x-datadog-sampling-priority</a>).</p></li>
  </ul>
</blockquote>

<hr>

<p>TLDR: traceId headers need to be manually added to B3 HTTP headers.</p>

<p>Additional information: <a href=""https://github.com/openzipkin/b3-propagation"" rel=""nofollow noreferrer"">https://github.com/openzipkin/b3-propagation</a></p>
"
Zipkin,57232063,57219354,0,"2019/07/27, 15:16:24",False,"2019/07/27, 15:16:24",1,11841253,0,"<p>Set the datasource setting in the application.yml file of the application as follows,</p>

<pre><code>spring:
  dataSource:
    url: jdbc: mariadb://localhost:3306/test
    driverClassName: org.mariadb.jdbc.Driver
    username: test
    password: test
</code></pre>

<p>You can add the zipkin attribute to POM.xml</p>

<pre><code>&lt;dependency&gt;
      &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
      &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>Problems can occur due to Spring's auto configuration property.</p>

<p>Therefore, modify the datasource setting as follows and modify the datasource configuration related source to make the application work normally.</p>

<pre><code>myApp:
   dataSource:
     jdbc-url: jdbc:mariadb://localhost:3306/test
     driverClassName: org.mariadb.jdbc.Driver
     username: test
     password: test
</code></pre>
"
Zipkin,55535480,55364711,0,"2019/04/05, 15:32:51",True,"2019/04/05, 15:32:51",8336,1773866,1,"<p>No you can't. You can use tools like Elasticsearch Logstash Kibana to visualize it. You can go to my repo <a href=""https://github.com/marcingrzejszczak/docker-elk"" rel=""nofollow noreferrer"">https://github.com/marcingrzejszczak/docker-elk</a> and run <code>./   getReadyForConference.sh</code>, it will start docker containers with the ELK stack, run the apps, curl the request to the apps so that you can then check them in ELK.</p>
"
Zipkin,48982440,43782954,0,"2018/02/26, 08:10:42",False,"2018/02/26, 08:10:42",181,6708214,1,"<p>1.You should check if your Zipkin Server is on.  </p>

<p>2.You should check if the Span transfering is async.</p>

<p>In HTTP,Zipkin uses in-band transfer,all the information carried in HTTP headers.The cost time of generating Span is about 200 nanosecond.</p>
"
Zipkin,47664992,47660900,0,"2017/12/06, 02:38:32",True,"2017/12/06, 02:38:32",8336,1773866,1,"<blockquote>
  <p>I have a client application with multiple channels as SOURCE/SINK. I want to send logs to Zipkin server.</p>
</blockquote>

<p>Zipkin is not a tool to store logs</p>

<blockquote>
  <p>According to my understanding, if spring finds spring cloud stream in classpath, Zipkin client defaults to messaging instead of sending logs through HTTP.</p>
</blockquote>

<p>No - you need the <code>sleuth-stream</code> dependency on the client side and the <code>zipkin-stream</code> dependency on the server side (which got deprecated and you should start using the inbuilt rabbitmq support from Zipkin).</p>

<blockquote>
  <p>At client side: Q1. Is there an automatic configuration for zipkin rabbit binding in such scenario? If not, what is default channel name of zipkin SOURCE channel?</p>
</blockquote>

<p>Yes, there is. The channel is <code>sleuth</code></p>

<blockquote>
  <p>Q2. Do I need to configure defaultSampler to AlwaysSampler()?</p>
</blockquote>

<p>No, you have the <code>PercentageBasedSampler</code> (I'm pretty sure it's written in the docs). You can tweak its values.</p>

<blockquote>
  <p>At Server side: Q1. Do I need to create Zipkin server as a spring boot application for my use case or can I use the jar retrieved using: wget -O zipkin.jar '<a href=""https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec"" rel=""nofollow noreferrer"">https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec</a>' ...as stated on <a href=""https://zipkin.io/pages/quickstart.html"" rel=""nofollow noreferrer"">https://zipkin.io/pages/quickstart.html</a> ?</p>
</blockquote>

<p>You should do the wget. If you want to use the legacy stream support then you should create a zipkin server yourself.</p>

<blockquote>
  <p>Q2. How do I configure zipkin SINK channel to destination?</p>
</blockquote>

<p>If you're using the legacy zipkin stream app then it's automatically configured to point to proper destination. You can tweak the destination as you please in the standard way Spring Cloud Stream supports it.</p>
"
Zipkin,37648120,37459261,0,"2016/06/06, 03:30:28",False,"2016/06/06, 16:49:03",91,3623286,1,"<p>Same problem here with the docker images using Cassandra (1.40.1, 1.40.2, 1.1.4).</p>

<p>This is a problem specific to using Cassandra as the storage tier.  Mysql and the in-memory storage generate the dependency graph on-demand as expected.</p>

<p>There are references to the following project to generate the Cassandra graph data for the UI to display.  </p>

<ul>
<li><a href=""https://github.com/openzipkin/zipkin-dependencies-spark"" rel=""nofollow"">https://github.com/openzipkin/zipkin-dependencies-spark</a></li>
</ul>

<p>This looks to be superseded by ongoing work mentioned here</p>

<ul>
<li><a href=""https://github.com/openzipkin/zipkin-dependencies-spark/issues/22"" rel=""nofollow"">https://github.com/openzipkin/zipkin-dependencies-spark/issues/22</a></li>
</ul>
"
Zipkin,58025941,37459261,0,"2019/09/20, 12:50:09",False,"2019/09/20, 13:20:39",1592,7251967,0,"<blockquote>
  <p>If storage type is other than inline storage, for zipkin dependencies graph you have to start separate cron job/scheduler which reads the storage database and builds the graph. Because zipkin dependencies is separate spark job .
  For reference : <a href=""https://github.com/openzipkin/docker-zipkin-dependencies"" rel=""nofollow noreferrer"">https://github.com/openzipkin/docker-zipkin-dependencies</a></p>
  
  <p>I have used zipkin with elastic search as storage type. I will share the steps for setting up the zipkin dependencies with elastic search and cron job for the same: </p>
</blockquote>

<pre><code>1.cd ~/
2. curl -sSL https://zipkin.io/quickstart.sh | bash -s io.zipkin.dependencies:zipkin-dependencies:LATEST zipkin-dependencies.jar
3. touch  cron.sh or vi cron.sh  
4. paste this content  : 
STORAGE_TYPE=elasticsearch ES_HOSTS=https:172.0.0.1:9200 ES_HTTP_LOGGING=BASIC ES_NODES_WAN_ONLY=true java -jar zipkin-dependencies.jar
5.chmode a+x cron.sh //make file executable
6.crontab -e   
window will open paste  below content 
0 * * * * cd &amp;&amp; ./cron.sh //every one hour it will start the cron job if you need every 5 minutes change the commmand to '*/5 * * * * cd &amp;&amp; ./cron.sh'
7. to check cron job is schedule run commant crontab -l
</code></pre>

<blockquote>
  <p>Other solution is to start a separate service and run the cron job
  using docker</p>
  
  <p>Steps to get the latest zipkin-dependencies jar try running given
  command on teminal</p>
</blockquote>

<pre><code>cd /zipkindependencies // where your Dockerfile is available
curl -sSL https://zipkin.io/quickstart.sh | bash -s io.zipkin.dependencies:zipkin-dependencies:LATEST
</code></pre>

<blockquote>
  <p>you will get jar file at above mention directory </p>
  
  <p>Dockerfile</p>
</blockquote>

<pre><code>FROM openjdk:8-jre-alpine
ENV STORAGE_TYPE=elasticsearch
ENV ES_HOSTS=http://172.0.0.1:9200
ENV ES_NODES_WAN_ONLY=true
ADD crontab.txt /crontab.txt
ADD script.sh /script.sh
COPY entry.sh /entry.sh
COPY zipkin-dependencies.jar  /
RUN chmod a+x /script.sh /entry.sh
RUN /usr/bin/crontab /crontab.txt
CMD [""/entry.sh""]
EXPOSE 8080
</code></pre>

<blockquote>
  <p>entry.sh</p>
</blockquote>

<pre><code>#!/bin/sh
# start cron
/usr/sbin/crond -f -l 8
</code></pre>

<blockquote>
  <p>script.sh</p>
</blockquote>

<pre><code>#!/bin/sh
java ${JAVA_OPTS} -jar /zipkin-dependencies.jar
</code></pre>

<blockquote>
  <p>crontab.txt</p>
</blockquote>

<pre><code>0 * * * * /script.sh &gt;&gt; /var/log/script.log
</code></pre>
"
Zipkin,38537946,38527466,1,"2016/07/23, 06:47:14",False,"2016/07/23, 06:47:14",762,1227937,1,"<p>Instrumenting a library is something that sometimes folks have to do for one reason or another. There are several tracer libraries in Java but the salient points about creating a tracer are either on the website, or issues on the website.</p>

<p><a href=""http://zipkin.io/pages/instrumenting.html"" rel=""nofollow"">http://zipkin.io/pages/instrumenting.html</a>
<a href=""https://github.com/openzipkin/openzipkin.github.io/issues/11"" rel=""nofollow"">https://github.com/openzipkin/openzipkin.github.io/issues/11</a></p>

<p>OpenTracing also has some nice fundamentals to look at <a href=""http://opentracing.io/"" rel=""nofollow"">http://opentracing.io/</a></p>

<p>This isn't a one-answer type of question, in my experience, as you'll learn you'll need to address other things that tracer libraries address out-of-box. For that reason I'd highly suggest joining gitter so that you
can have a dialogue through your journey <a href=""https://gitter.im/openzipkin/zipkin"" rel=""nofollow"">https://gitter.im/openzipkin/zipkin</a></p>
"
Zipkin,52739617,52424864,0,"2018/10/10, 14:51:01",False,"2018/10/10, 14:51:01",201,7956609,0,"<p>I think to remove the service names from zipkin you have to Re-deploy the zipkin service</p>
"
Zipkin,52776732,52424864,0,"2018/10/12, 12:41:59",True,"2018/10/12, 12:41:59",131,6236211,0,"<p>You will need to remove the spring.zipkin.base-url property from the corresponding applications to remove it from zipkin server list.</p>
"
Zipkin,47149111,47135535,0,"2017/11/07, 04:37:05",False,"2017/11/07, 04:37:05",474,4682632,3,"<p>Zipkin only does tracing. APM tools like Appdynamics do other monitoring (browser, mobile, database, server, network). Code-level diagnostics with automated overhead controls and limiters. Don't forget log analytics and transaction analytics. It also collects metrics. </p>

<p>There is a lot more to APM than just tracing, which is what Zipkin does. You could do this with a stack of 20 open source tools, but you'd have to deal with disjointed UIs and data models not to mention the work associated with keeping them all working.</p>
"
Zipkin,51354519,51244941,1,"2018/07/16, 07:17:28",False,"2018/07/16, 07:17:28",1767,2498986,0,"<p>Brave will work regardless of the server that you choose to use. Remove the jetty configuration from the pom file and use the Tomcat.</p>

<pre><code>&lt;plugin&gt;
  &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt;
  &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt;
  &lt;version&gt;2.2&lt;/version&gt;
  &lt;configuration&gt;
    &lt;port&gt;${tomcat.port}&lt;/port&gt;
    &lt;path&gt;/&lt;/path&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;
</code></pre>

<p>If you still have trouble or want to know more about zipkin/brave connect to the community via the gitter channel.</p>

<p>P.S. I contribute to OpenZipkin (Zipkin)</p>
"
Zipkin,50339003,49909075,0,"2018/05/15, 00:05:40",True,"2018/05/15, 00:05:40",611,118116,1,"<p>Turns out I don't need neither Zipkin nor Jaeger to have my traces on Stackdriver. All that is needed is a deployment of <a href=""http://gcr.io/stackdriver-trace-docker/zipkin-collector"" rel=""nofollow noreferrer"">zipkin-collector</a> and a service to point to it and all my traces are now reporting as expected on GCP Stackdriver.</p>
"
Zipkin,54616102,54559465,0,"2019/02/10, 13:55:02",False,"2019/02/10, 13:55:02",8336,1773866,1,"<p>You're calling one method from another. Spring is creating a proxy around your method. If you call one method from another from the same class then you're not going through the proxy. Extract the method annotated with new span to a separate class and it will work fine.</p>
"
Zipkin,60213881,60170816,0,"2020/02/13, 19:56:57",False,"2020/02/13, 19:56:57",2576,12014434,0,"<p>I have finally figured out what could be the cause of this issue:</p>

<p>The install option:</p>

<p><code>--set values.tracing.provider=zipkin --set values.global.tracer.zipkin.address</code></p>

<p>requires <code>&lt;zipkin-collector-service&gt;.&lt;zipkin-collector-namespace&gt;:9411</code> according to <a href=""https://istio.io/docs/tasks/observability/distributed-tracing/zipkin/#before-you-begin"" rel=""nofollow noreferrer"">istio</a> documentation. While You have just IP address and port of external server. </p>

<p>This most likely means that the install option requires existing name that is in istio service mesh registry.</p>

<p>So if Your zipkin collector is outside cluster We need to add <code>ServiceEntry</code>, <code>VirtualService</code> and maybe <code>DestinationRule</code> and so the external service can be used within mesh.</p>

<p>You can follow <a href=""https://istio.io/docs/reference/config/networking/service-entry/"" rel=""nofollow noreferrer"">istio</a> documentation to see how to create these objects for external service. <a href=""https://istiobyexample.dev/external-services/"" rel=""nofollow noreferrer"">Here</a> is another guide.</p>

<p>After that We need to update the tracer address value with the <code>VirtualService</code> as an endpoint.</p>

<pre><code>--set values.global.tracer.zipkin.address=zipkin-external-virtualservice.egress-zipkin-namespace:9411
</code></pre>

<p>Hope this helps.</p>
"
Zipkin,58298923,58294974,2,"2019/10/09, 10:18:06",True,"2019/10/09, 10:18:06",2382,4497840,2,"<p>I have a working project with spring cloud stream and zipkin using the following configuration (maybe you should set the sender.type): </p>

<pre><code>spring:
  zipkin:
    enabled: true
    service.name: my-service
    sender.type: web
    base-url: http://localhost:9411
  sleuth:
    enabled: true
    sampler:
      probability: 1.0
</code></pre>

<p>Hope this can help.</p>
"
Zipkin,66896684,66766936,0,"2021/04/01, 02:45:08",True,"2021/04/01, 02:45:08",1693,971735,0,"<p><a href=""https://docs.spring.io/spring-kafka/docs/current/reference/html/#overview"" rel=""nofollow noreferrer"">As the documentation suggests</a>, you need to create a <code>ProducerFactory</code> bean if you want to use your own <code>KafkaTemplate</code>:</p>
<pre class=""lang-java prettyprint-override""><code>@Configuration
public class KafkaProducerConfig {

    @Bean
    public ProducerFactory&lt;String, Object&gt;producerFactory(KafkaProperties kafkaProperties) {
        return new DefaultKafkaProducerFactory&lt;&gt;(kafkaProperties.buildProducerProperties());
    }

    @Bean
    public KafkaTemplate&lt;String, Object&gt; kafkaTemplate(ProducerFactory&lt;String, Object&gt;producerFactory) {
        return new KafkaTemplate&lt;&gt;(producerFactory);
    }
}
</code></pre>
"
Zipkin,66704380,66687298,1,"2021/03/19, 09:57:02",False,"2021/03/19, 09:57:02",2568,8225898,0,"<p>The Spring Cloud project is moving to their own solutions.</p>
<p>Ribbon is replaced by the Spring Cloud Load Balancer, Hysterix by the Spring Cloud Circuit Breaker, Zuul by the Spring Cloud Gateway.</p>
<p>This is a good read, including examples, about this topic: <a href=""https://piotrminkowski.com/2020/05/01/a-new-era-of-spring-cloud/"" rel=""nofollow noreferrer"">https://piotrminkowski.com/2020/05/01/a-new-era-of-spring-cloud/</a></p>
"
Zipkin,66481119,66475457,0,"2021/03/04, 20:41:58",False,"2021/03/04, 20:41:58",1693,971735,0,"<p>Sleuth does this for you by default in 3.x too: <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-log-integration"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#features-log-integration</a></p>
<p>You can break this functionality by misconfiguring your log pattern or <code>logging.pattern.level</code> or your classpath.</p>
<p>What I would suggest is going to <a href=""https://start.spring.io"" rel=""nofollow noreferrer"">https://start.spring.io</a>, generate a new project using sleuth and web/webflux, write a single controller and check the logs (do not create any log config file, just leave everything on default).</p>
"
Zipkin,65718763,65669868,4,"2021/01/14, 14:14:20",True,"2021/01/14, 14:14:20",2294,10347794,1,"<p>Let me add to this thread my three bits. Speaking of Envoy, yes, when attached to your application it adds a lot of useful features from observability bucket, e.g. network level statistics and tracing.</p>
<p>Here is the question, have you considered running your legacy apps inside service mesh, like Istio ?.</p>
<p>Istio simplifies deployment and configuration of Envoy for you. It injects sidecar container (istio-proxy, in fact Envoy instance) to your Pod application, and gives you these extra features like a set of service metrics out of the box*.</p>
<p>Example: Stats produced by Envoy in Prometheus format, like <code>istio_request_bytes</code> are visualized in Kiali Metrics dashboard for inbound traffic as <code>request_size</code> (check screenshot)</p>
<p>*as mentioned by @David Kruk, you still needs to have Prometheus server deployed in your cluster to be able to pull these metrics to Kiali dashboards.</p>
<p>You can learn more about Istio <a href=""https://istio.io/latest/docs/examples/microservices-istio/"" rel=""nofollow noreferrer"">here</a>. There is also a dedicated section on how to <a href=""https://istio.io/latest/docs/tasks/observability/kiali/"" rel=""nofollow noreferrer"">visualize metrics</a> collected by Istio (e.g. request size).</p>
<p><a href=""https://i.stack.imgur.com/w4SNP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w4SNP.png"" alt=""enter image description here"" /></a></p>
"
Zipkin,65602072,65600807,5,"2021/01/06, 21:17:20",False,"2021/01/06, 21:17:20",1693,971735,1,"<p>You can use a <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#using-annotations-advanced-tag-setting"" rel=""nofollow noreferrer""><code>TagValueResolver</code></a> or a <a href=""https://docs.spring.io/spring-cloud-sleuth/docs/current/reference/htmlsingle/#customizing-messaging-spans"" rel=""nofollow noreferrer""><code>MessageSpanCustomizer</code></a>, see the docs for the details.</p>
"
Zipkin,63699177,63699141,4,"2020/09/02, 08:23:29",False,"2020/09/02, 08:23:29",6381,2528083,0,"<p>Better approach will be using Kafka instead of Redis.</p>
<blockquote>
<p>Create a topic for every microservice &amp; keep moving the packet from
one topic to another after processing.</p>
</blockquote>
<pre><code>topic(raw-data) - |MS One| - topic(processed-data-1) - |MS Two| - topic(processed-data-2) ... etc 
</code></pre>
<p>Keep appending the results to same object and keep moving it down the line, untill every micro-service has processed it.</p>
"
Zipkin,61648502,61646918,2,"2020/05/07, 04:50:02",False,"2020/05/07, 04:50:02",43078,78722,0,"<p>What you have is correct, your issue is likely not DNS. You can confirm by doing just a DNS lookup and comparing that to the IP of the Service.</p>
"
Zipkin,59963724,59867690,1,"2020/01/29, 11:09:14",False,"2020/01/29, 11:09:14",7644,259167,0,"<p>The <code>doubleName</code> method is private. Micronaut cannot apply AOP annotations (like <code>ContinueSpan</code> to private methods.</p>
"
Zipkin,61001724,59867690,0,"2020/04/03, 00:47:14",False,"2020/04/03, 00:47:14",289,45693,0,"<p>Are these methods on a bean (<code>Singleton</code>, etc.)? I have found the span annotations only get applied on beans properly. I had to refactor some of my code to create beans from <code>Factory</code>s or such.</p>
"
Zipkin,58272901,58272315,0,"2019/10/07, 18:41:12",True,"2019/10/07, 18:41:12",8336,1773866,0,"<p>You've passed in the <code>B3Propagation.FACTORY</code> as the implementation of the propagation factory so you're explicitly stating that you want the default B3 headers. You've said that you want some other field that is alphanumeric to be also propagated. Then in a log parsing tool you can define that you want to use your custom field as the trace id, but it doesn't mean that the deafult X-B3-TraceId field will be changed. If you want to use your custom field as trace id that Sleuth understands, you need to change the logging format and implement a different propagation factory bean.</p>
"
Zipkin,58436544,58272315,0,"2019/10/17, 18:56:45",False,"2019/10/17, 19:03:55",3,9052240,0,"<p>One  of the  way which   worked for me is 
using ExtraFieldPropagation
and adding those   keys in sleuth  properties  under   propagation-keys
and  whitelisted-keys</p>

<p>sample code 
  '  @Autowired Tracer tracer;</p>

<pre><code>Span currentSpan = tracer.nextSpan().start();
ExtraFieldPropagation.set(
      ""customkey"", ""customvalue"");
sleuth:
    log:
      slf4j:
        whitelisted-mdc-key : customkey
propagation:
      tag:
        enabled: true
    propagation-keys : customkey '
</code></pre>
"
Zipkin,56349083,56293312,0,"2019/05/28, 22:43:26",False,"2019/05/28, 22:43:26",129048,1240763,0,"<p>With Edgware (SCSt Ditmars), you have to specify which headers will be transferred.</p>
<p><a href=""https://docs.spring.io/spring-cloud-stream/docs/Ditmars.SR5/reference/htmlsingle/#_kafka_binder_properties"" rel=""nofollow noreferrer"">See Kafka Binder Properties</a>.</p>
<p>This is because Edgware was based on Kafka before it supported headers natively and we encode the headers into the payload.</p>
<blockquote>
<p><strong>spring.cloud.stream.kafka.binder.headers</strong></p>
<p>The list of custom headers that will be transported by the binder.</p>
<p>Default: empty.</p>
</blockquote>
<p>You should also be sure to upgrade spring-kafka to 1.3.9.RELEASE and kafka-clients to 0.11.0.2.</p>
<p>Preferably, though, upgrade to Finchley or Greemwich. Those versions support headers natively.</p>
"
Zipkin,55039063,55036782,0,"2019/03/07, 10:20:10",False,"2019/03/07, 10:20:10",181,5436378,0,"<p>Well, without seeing any code, I could only give you a sample of how you should achieve this. So an http call, for example if you use node-fetch or axios will return a promise. To wait for promises paralelly, you can do the following:</p>

<pre><code>async function myParallelRequests() {
    const requestOne = fetch(urlOne);
    const requestTwo = fetch(urlTwo);
    const requestThree = fetch(urlThree);
    const [responseOne, responseTwo, responseThree] = await Promise.all([
        requestOne,
        requestTwo,
        requestThree,
    ]);
}
</code></pre>

<p>Note that I use fetch API here, provided in node by the node-fetch package. Fetch returns a <code>Promise</code>. Then I call <code>Promise.all(promises)</code> where <code>promises</code> is a <code>Promise</code> array. You can then do whatever you would like to do with the 3 responses and your requests were made paralelly.</p>

<p>Hope this helps, good luck!</p>
"
Zipkin,54945201,54943755,7,"2019/03/01, 15:00:07",False,"2019/03/01, 15:00:07",26220,927493,0,"<p>You have </p>

<pre><code>io.zipkin.reporter2:zipkin-reporter:jar:2.2.0:compile (version managed from 2.7.14)
</code></pre>

<p>This means that there is a <code>&lt;dependencyManagement&gt;</code> entry in your POM or your Parent POM that sets the version to <code>2.2.0</code>.</p>
"
Zipkin,51854323,51851541,5,"2018/08/15, 10:07:09",False,"2018/08/22, 18:07:28",8336,1773866,0,"<p>You can have s custom span reporter that before sending spans to zipkin will dump the span as a json structure to logs.</p>

<p>UPDATE:</p>

<p>With this PR merged <a href=""https://github.com/spring-cloud/spring-cloud-sleuth/pull/1068"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-sleuth/pull/1068</a>, in 2.1.0 you'll have an easy way to implement your own MDC entries</p>
"
Zipkin,48660632,48659538,2,"2018/02/07, 11:49:39",True,"2018/02/07, 11:49:39",8336,1773866,1,"<p>Usage of Sleuth Stream is deprecated. Please use the <code>zipkin</code> starter, add the <code>Kafka</code> dependency and set things as presented here <a href=""https://cloud.spring.io/spring-cloud-static/Edgware.SR1/multi/multi__introduction.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/Edgware.SR1/multi/multi__introduction.html#_sleuth_with_zipkin_via_rabbitmq_or_kafka</a></p>
"
Zipkin,48140158,48137375,1,"2018/01/07, 20:14:38",False,"2018/01/07, 20:14:38",111,2639742,1,"<p>fran, in Edgware.RELEASE the <code>&lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;</code> will resolve Zipkin 2 dependencies try using <code>&lt;artifactId&gt; spring-cloud-starter-zipkin-legacy&lt;/artifactId&gt;</code> instead</p>
"
Zipkin,52525191,48137375,0,"2018/09/26, 22:55:47",False,"2018/09/26, 22:55:47",1,7391489,0,"<p>To define the primary connection factory for RabbitMQ in XML files, you can do something like this:</p>

<pre><code>&lt;!-- Here the primary connection --&gt;
&lt;bean id=""rabbitConnectionFactory"" primary=""true"" class=""org.springframework.amqp.rabbit.connection.CachingConnectionFactory""&gt;
    &lt;constructor-arg value=""${spring.rabbitmq.host}""/&gt;
    &lt;property name=""username"" value=""${spring.rabbitmq.username}""/&gt;
    &lt;property name=""password"" value=""${spring.rabbitmq.password}""/&gt;
    &lt;property name=""virtualHost"" value=""${spring.rabbitmq.virtual-host}""/&gt;
&lt;/bean&gt;

&lt;!-- RabbitMQ configuration --&gt;
&lt;rabbit:connection-factory
        id=""rabbitConnectionFactory_2""
        host=""${queuing.events.host}""
        port=""${queuing.events.port}""
        username=""${queuing.events.username}""
        password=""${queuing.events.password}""
        virtual-host=""${queuing.events.virtual-host}""
        publisher-returns=""true""/&gt;

&lt;rabbit:template id=""amqpTemplate_2"" connection-factory=""rabbitConnectionFactory_2"" /&gt;
&lt;rabbit:admin id=""rabbitAdmin_2"" connection-factory=""rabbitConnectionFactory_2""/&gt;
&lt;rabbit:listener-container connection-factory=""rabbitConnectionFactory_2"" auto-startup=""true"" requeue-rejected=""false"" /&gt;

&lt;bean id=""rabbitListenerContainerFactory_2"" class=""org.springframework.amqp.rabbit.config.SimpleRabbitListenerContainerFactory""&gt;
    &lt;property name=""connectionFactory"" ref=""rabbitConnectionFactory_2""/&gt;
&lt;/bean&gt;
</code></pre>
"
Zipkin,47297460,47293445,0,"2017/11/15, 02:20:26",False,"2017/11/15, 02:20:26",67,8942125,0,"<p>I do not have much knowledge with hysterix, but if you are trying to pass some contextual info like trace IDs around, then io.grpc.Context is the correct class to use. You would need to call <code>context.withValue</code> to create a new context with the traceID. In the places where you want the data, you need to attach the context. Also be sure to detach the context when done, which I do not see happening in your snippet.</p>
"
Zipkin,47352178,47293445,6,"2017/11/17, 15:52:06",False,"2017/11/17, 15:52:06",58,2174913,0,"<p>You need to use ...</p>

<p><code>HystrixPlugins.getInstance().registerConcurrencyStrategy(...)</code></p>

<p>... to register a custom <code>HystrixConcurrencyStrategy</code> that uses your own <code>Callable</code> ...</p>

<pre><code>public class ConcurrencyStrategy extends HystrixConcurrencyStrategy {    
    @Override
    public &lt;K&gt; Callable&lt;K&gt; wrapCallable(Callable&lt;K&gt; c) {
        return new ContextCallable&lt;&gt;(c);
    }
}
</code></pre>

<p>... that applies context preservation around the circuit ...</p>

<pre><code>public class ContextCallable&lt;K&gt; implements Callable&lt;K&gt; {

    private final Callable&lt;K&gt; callable;
    private final PreservableContexts contexts;

    public ContextCallable(Callable&lt;K&gt; actual) {
        this.callable = actual;
        this.contexts = new PreservableContexts();
    }

    @Override
    public K call() throws Exception {
        contexts.set();
        try {
            return callable.call();
        } finally {
            contexts.clear();
        }
    }
}
</code></pre>

<p>... via is a helper class capable of preserving the Zipkin context ...</p>

<pre><code>public class PreservableContexts {

    private final TraceContext traceContext;

    public PreservableContexts() {
        this.traceContext = TraceContextHolder.getContext();
    }

    public void set() {
        if (traceContext != null) {
            TraceContextHolder.setContext(traceContext);
        }
    }

    public void clear() {
        TraceContextHolder.clearContext();
    }

}
</code></pre>

<p>... and allowing an easy method of adding other contexts you may wish to preserve e.g. MDC, SecurityContext etc ...</p>
"
Zipkin,42395388,42386925,3,"2017/02/22, 17:14:57",False,"2017/02/22, 17:14:57",129048,1240763,0,"<p>This is a guess.</p>
<p>Kafka has no concept of message headers (where spans are stored).</p>
<p>SCSt therefore has to embed message headers in the payload.</p>
<p>The current version requires you to &quot;opt-in&quot; which headers you want transported in this way.</p>
<p><a href=""http://docs.spring.io/spring-cloud-stream/docs/Brooklyn.SR1/reference/htmlsingle/#_kafka_producer_properties"" rel=""nofollow noreferrer"">Documentation here</a>.</p>
<blockquote>
<p>spring.cloud.stream.kafka.binder.headers</p>
<p>The list of custom headers that will be transported by the binder.</p>
<p>Default: empty.</p>
</blockquote>
<p>Unfortunately, patterns are not currently supported, you have to list the headers individually. We are <a href=""https://github.com/spring-cloud/spring-cloud-stream/issues/769"" rel=""nofollow noreferrer"">considering adding support for patterns</a> and/or transporting all headers by default.</p>
"
Zipkin,42409200,42386925,1,"2017/02/23, 09:11:02",False,"2017/02/23, 09:11:02",31,6503007,0,"<p>Finally, I found 2 issues related with my applications.
1. The application with @EnalbeZipkinStreamServer could not be traced. This looks like a by design.
2. If kafka is used as the binder, the applications should specify the headers as the following:</p>

<pre><code>    spring.cloud.stream.kafka.binder.headers[0]=spanId
    spring.cloud.stream.kafka.binder.headers[1]=spanSampled
    spring.cloud.stream.kafka.binder.headers[2]=spanProcessId
    spring.cloud.stream.kafka.binder.headers[3]=spanParentSpanId
    spring.cloud.stream.kafka.binder.headers[4]=spanTraceId
    spring.cloud.stream.kafka.binder.headers[5]=spanName
    spring.cloud.stream.kafka.binder.headers[6]=spanFlags
</code></pre>
"
Zipkin,52815924,41086850,0,"2018/10/15, 14:37:50",False,"2018/10/15, 17:52:20",1,10507091,0,"<p>I'm not sure this is what you are expecting, you can add this dependancy in <code>pom.xml</code> if you are using maven:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p>and a <code>AlwaysSampler @Bean</code> in your <code>SpringBootApplication</code> class</p>

<pre><code>@Bean
public AlwaysSampler defaultSampler(){
    return new AlwaysSampler();
}
</code></pre>

<p>This will help you to sample your inputs in zipkin all time.</p>
"
Zipkin,37706728,37642783,0,"2016/06/08, 18:23:49",True,"2016/06/08, 18:23:49",8336,1773866,1,"<p>The best thing to do would be to show your sample project. Another is to check if you don't have a custom logback.xml or any other type of logging configuration that breaks the current set up (most likely you do cause I can see that the pattern is different).</p>
"
Zipkin,37608801,37608464,0,"2016/06/03, 10:47:09",True,"2016/06/03, 10:47:09",8336,1773866,2,"<p>So you have an ip and port of your app so that could give you a hint. Also if you want a custom span of yours to have that information, then it's enough to add a custom tag to it. Actually you can always call the <code>tracer.addTag(""key"", ""value"")</code> to put the additional information that you need.</p>
"
Zipkin,32429851,32362058,1,"2015/09/07, 04:04:28",False,"2015/09/07, 04:04:28",154,2023873,0,"<p>I am not sure about the <code>dcat</code> distribution itself, but your error may be because you have:</p>

<pre><code>b3[i] ~ dcat(p[i,])#WIND Data
</code></pre>

<p>however when you specify <code>p</code> in the model, it is a three dimensional array <code>p[j,k,i].</code>
I think you need:</p>

<pre><code>b3[i] ~ dcat(p[,,i])#WIND Data
</code></pre>

<p>Note, the <code>i</code> is the last index for <code>p</code>, and the two commas.
Hope his helps...</p>
"
Zipkin,58889653,58889547,0,"2019/11/16, 12:16:29",False,"2019/11/16, 12:16:29",1,12382349,0,"<p>I dont kown can you see my pic and I put my code:</p>

<p><strong>pom.xml:</strong></p>

<pre><code>&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;configuration&gt;
                &lt;mainClass&gt;com.test.itoken.zipkin.ZipKinApplication&lt;/mainClass&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p><strong>ZipkinApplication.java:</strong></p>

<pre><code>package com.test.itoken.zipkin;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.EnableEurekaClient;
import zipkin.server.internal.EnableZipkinServer;

@SpringBootApplication
@EnableEurekaClient
@EnableZipkinServer
public class ZipkinApplication {

    public static void main(String[] args) {
        SpringApplication.run(ZipkinApplication.class, args);
    }

}
</code></pre>

<p><strong>The error:</strong></p>

<pre><code>Exception in thread ""main"" java.lang.ClassNotFoundException: com.test.itoken.zipkin.ZipKinApplication
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    at org.springframework.boot.loader.LaunchedURLClassLoader.loadClass(LaunchedURLClassLoader.java:93)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:46)
    at org.springframework.boot.loader.Launcher.launch(Launcher.java:87)
    at org.springframework.boot.loader.Launcher.launch(Launcher.java:50)
    at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51)
</code></pre>
"
Zipkin,50203216,50203215,0,"2018/05/06, 21:33:41",True,"2018/05/06, 21:33:41",93,6816012,0,"<p>the simplest solution i have found for solving broken ui for zipkin thru gateway
is by changing following property of zipkin-server-shared.yml file inside zipkin server </p>

<p><code>zipkin:
  ui:
   base-path: /zipkin
</code>
change above property to </p>

<p><code>zipkin:
  ui:
   base-path: /api/tracing/zipkin
</code></p>

<p>and change ur zuul path to following
<code>zuul.routes.zipkin.path=/api/tracing/*</code></p>

<p>and than access zipkin using follwing url</p>

<p><code>https://gatewayhost:port/api/tracing/zipkin/</code></p>

<p>give attention to small details in config and dont forget to put trailing ""/"" after zipkin  in url</p>
"
Zipkin,50837359,50836516,3,"2018/06/13, 15:31:21",False,"2018/06/13, 15:31:21",8336,1773866,0,"<p>You're using an ancient version of Spring CLoud. Please upgrade to latest Edgware. The RxJava support is very basic so we suggest that you use Project Reactor. To do that just migrate to Finchley and it should work out of the box with WebFlux.</p>
"
Zipkin,53692930,53692437,3,"2018/12/09, 15:41:18",False,"2018/12/09, 15:41:18",81,2232476,0,"<p>I can say your YAML has some bad indentation and things are not in the right sections even. Otherwise though, you are trying to run Zipkin in an unsupported configuration. Please check out our quickstart documentation: <a href=""https://zipkin.io/pages/quickstart.html"" rel=""nofollow noreferrer"">https://zipkin.io/pages/quickstart.html</a></p>
"
Zipkin,53694198,53692437,3,"2018/12/09, 18:13:01",True,"2018/12/13, 16:58:29",5963,3625215,1,"<p>There are 2 approaches to this </p>

<ol>
<li>Start Zipkin server with SpringBootApplication</li>
<li>Start Zipkin server as a standalone and add url in SpringBootServer</li>
</ol>

<p>Looking at your yml file you have added </p>

<pre><code>zipkin:
   base-url: http://localhost:8082
</code></pre>

<p>which means your approach is 2.</p>

<p>But then in your pom, you have added <code>zipkin-server</code> and <code>zipkin-autoconfigure-ui</code> dependencies which is not required.</p>

<p>I will try to separate both setups</p>

<p><strong>1. To Start Zipkin server with SpringBootApplication</strong> </p>

<p><strong>pom.xml</strong></p>

<pre><code> &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
    &lt;/dependency&gt;


  &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt;
        &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;
</code></pre>

<p><strong>application.properties</strong></p>

<pre><code>spring.application.name=zipkin-server
server.port=9411
</code></pre>

<p><strong>Application.java</strong></p>

<pre><code>@SpringBootApplication
@EnableZipkinStreamServe
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(ZipkinServerApplication.class, args);
    }
}
</code></pre>

<p><strong>2. To Start Zipkin server as a standalone and use SpringBootApplication as Zipkin Client</strong></p>

<p><a href=""https://zipkin.io/pages/quickstart.html"" rel=""nofollow noreferrer"">Start Zipkin server</a> </p>

<p><strong>pom.xml</strong></p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<p><strong>application.properties</strong></p>

<pre><code>spring.zipkin.base-url=http://localhost:9411/
spring.sleuth.sampler.probability=1
</code></pre>

<p><strong>Edit 1:</strong></p>

<p><code>@EnableZipkinServer</code> is deprecated and unsupported as per Brian Devins's comment. So, please go through the <a href=""https://zipkin.io/pages/existing_instrumentations.html"" rel=""nofollow noreferrer"">doc</a> for more detail info.</p>
"
Zipkin,59356961,59356270,2,"2019/12/16, 14:47:29",False,"2019/12/16, 14:47:29",8336,1773866,1,"<p>You can use spring cloud sleuth. Please check the documentation for examples of using elk stack to harvest the logs. The zipkin server can be fetched as a standalone jar, you don't need to create your custom version </p>
"
Zipkin,52852525,52772025,0,"2018/10/17, 13:19:16",True,"2018/10/17, 13:19:16",71,7561583,1,"<p>It is indeed a cluster problem. There is a problem with the <code>__consumer_offsets</code> topic data of kafka. It is good to restart kafka after deleting.</p>
"
Zipkin,47323502,47318596,0,"2017/11/16, 09:02:10",False,"2017/11/16, 09:02:10",8336,1773866,-1,"<p>I have no knowledge of it being impossible. Maybe you should first try doing it and then asking a question? Also, if for some reason it turns out you can't use it, then if you just google <code>zipkin scala</code> you'll see things like <a href=""https://github.com/lloydmeta/zipkin-futures"" rel=""nofollow noreferrer"">https://github.com/lloydmeta/zipkin-futures</a> , <a href=""https://github.com/bizreach/play-zipkin-tracing"" rel=""nofollow noreferrer"">https://github.com/bizreach/play-zipkin-tracing</a> etc.</p>
"
Zipkin,52798597,52798338,0,"2018/10/14, 03:55:06",False,"2018/10/14, 03:55:06",387,6905597,0,"<p>You'll find all of it on GitHub.</p>

<p><a href=""https://github.com/spring-projects/spring-framework/tree/master/spring-web/src/main/java/org/springframework/web/bind/annotation"" rel=""nofollow noreferrer"">Spring Web annotations</a></p>

<p><a href=""https://github.com/spring-projects/spring-framework/tree/master/spring-beans/src/main/java/org/springframework/beans/factory/annotation"" rel=""nofollow noreferrer"">Spring framework annotations</a></p>

<p><a href=""https://github.com/spring-projects/spring-framework/tree/master/spring-context/src/main/java/org/springframework/context/annotation"" rel=""nofollow noreferrer"">More spring framework annotations</a></p>
"
Zipkin,46836832,46762291,1,"2017/10/19, 21:47:05",False,"2017/10/19, 21:47:05",2102,5486262,1,"<p>You can use Apache NiFi's built-in provenance capabilities to trace how a given flow went through the system.</p>

<p><a href=""https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#data_provenance"" rel=""nofollow noreferrer"">https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#data_provenance</a></p>
"
