Migrating HTTP Tests
How we relocated all of our HTTP synthetics monitors from Pingdom to Datadog
Björn Skoglund
Björn Skoglund
Follow
Jan 14, 2020 · 6 min read





In recent months, we have been on a mission here at Compass to migrate all our observability tools to Datadog. As part of this larger process it had come time to migrate from Pingdom to Datadog Synthetics. This is the story of that migration process.
Problem Overview
The basic principle of a synthetics monitoring is to regularly send an HTTP request to an endpoint from one or multiple AWS regions and then validate that the responses arrive within a timeout and with the right status. In the case of a failure, the test can send out a notification to the owner of the service. These notifications can be sent to one or many on-call services, Slack channels or email addresses.
At Compass we have traditionally used Pingdom checks for validating the availability of our services. At the time of the migration we had built up a collection of hundreds of Pingdom monitors for our services that all had to be migrated to Datadog. Luckily a synthetic user-test in Datadog maps almost perfectly from Pingdom’s Uptime test so the problem space is fairly clear.
What throws a wrench in the machinery are the sometimes small differences between the two systems:
The tag structure in Datadog has a namespace component so a tag like consumer-search in Pingdom can correspond to two separate tags such as team:consumer-search and app:consumer-search in Datadog.
The Slack and on-call integrations in the two systems both operate on internal IDs such that the on-call service consumer-search-backend has integration ID X in Pingdom and Y in Datadog.
The default values in each system are different. While the default timeout in Pingdom is 30 seconds, it is 2 seconds in Datadog.
Best Practices of Technical Migrations
Before we dive into the details, this is a good time to review some basics for migrating data. Here is the master advice for all migration tasks:
Script it! Always. Scripts are not for saving time. They are primarily useful for encoding assumptions and maintaining quality in the migrated dataset. Write a script that you can run multiple times without creating duplicates or other artifacts. As you encode your assumptions you will be testing them and some of them will turn out to need tweaking before the process is over. Being able to re-run the process is absolutely essential.
Now with that said, here is some lower priority advice that we have found useful:
Do not touch the data in the target environment until you have made a formal decision that the migration is complete and any additional changes will be made in the target environment only.
Be liberal with error generation whenever an unhandled case shows up. This will be your guide to determining when you are done.
If need be, write additional validation scripts for any questions that may arise.
Implementation
All scripts were written using Python 3.7 using the standard JSON library and the requests library for HTTP. First party integration libraries for Pingdom, the on-call system and Datadog were foregone in favor of using the plain HTTP API endpoints each service provided. This helped us in debugging which was preferable to any performance gains of the vendor libraries.
Now, ideally you would write a single script that runs the entire process. Read the data from Pingdom, bake it to the Datadog format and upload it to its new home. As we shall see this process was not very handy in our case.
We decided instead to adopt a strategy of creating a set of scripts and caching the in-between results in local JSON files. This made each script simpler and provided good ways of reviewing intermediate results as well as providing better support for manually managed mappings such as the ones for tags and integrations.
Scope of Data Being Migrated
Test URL
Request headers
Test status (live or paused)
Expected response time
Expected response status
Expected response content-type
Tags
On-call notifications
Slack notifications
Email notifications
Most of these were straightforward so we will focus now on a few of the hands-on problems we ran into.
Mapping Tags, One-to-Many
After aggregating all the tags used in Pingdom we could conclude that there were less than 100 tags and the clearest way to proceed was to manually map these tags to tags already used in Datadog. As mentioned before it was not necessarily a 1to1 relation so we ended up with a JSON file with a list of tags and their mapped counterparts in Datadog, such as:
"consumer-search": [
    "team:consumer-search", "app-search"
]
This JSON file could then be reviewed and updated in case a miss was made.
On-call System
The on-call notifications were the trickiest part of the migration. Both systems introduced a separate layer of abstraction between the test and the on-call service ID. Each has a different notion of an Integration. The integration object holds the ID for the on-call service and the test holds only a reference to the integration.

Both systems has one level of indirection when integrating with the on-call service
Luckily Pingdom allowed for exporting these integrations together with the corresponding on-call service ID. After also exporting all services from the on-call service we could start to create a mapping from Pingdom integrations to services. The resulting data looked something like below. The main key in the listing is the Pingdom integration ID and the “oncall_id” field is the raw on-call service id.
"12345": {
    "pingdom”: “[Consumer Search] Critical",
    "oncall_id”: “abcdef0123456789",
    "oncall_service”: “[Consumer Search] React App"
}
This is where the process hit a snag. The natural next step would be to analogously export the integrations from Datadog but this is not possible. Only creating new integrations is allowed.
Here it is worth taking a detour to review how integrations work in Datadog. Each notification integration is an @-mention in the message field of the test. So to email Jane Doe you add @jane.doe@compass.com to your message field and an email will be sent on test failure. Likewise the on-call integrations are @-mentions to the name of the integration.
In the end, after reviewing the list of integrations in need of migration, we could manually add the integrations we know are already available in Datadog. The rest we could script an import for, and then amend the mapping file with a final Datadog integration. Below is the final entry in the mapping. The datadog field is the final Datadog @-mention required in the test message field.
"12345": {
    "pingdom”: “[Consumer Search] Critical",
    "oncall_id”: “abcdef0123456789",
    "oncall_service”: “[Consumer Search] React App"
    "datadog”: @oncall-ConsumerSearch-ReactApp",
}
Shhhhh, don’t tell Mom
Another particular issue with migrating the tests automatically and including the notification settings is that if something goes wrong, everyone and their mother may get a page in the middle of the day, not because the service itself has failed but because our brand new test is not migrated or calibrated properly.
To reduce the number of unnecessary pages going out we took a two-pronged approach. By default we created all new tests as paused and with a single notification policy sending failures to an email address controlled by the migration team. Once the migration of data had been generally validated we could independently validate that first, the test runs successfully and also, while keeping the test paused, add all notifications and validate them in the target environment, all using manually executed scripts. When we were satisfied with both; a single final step pushed all tests live with the on-call notifications enabled.
Validation
Once we had all tests running in Datadog the question arises, are we done now?
This too was answered using a script. We re-exported all checks from Pingdom and using the mapping file that was the result of the entire process we could validate that each check had an online running counterpart in Datadog.
Future Work
The namespace structure of tags in Datadog allows us to validate that all tests are associated with a team and that the correct team is notified in the on-call service for each test. This work is ongoing and will result in more robust on-call notifications being sent out during an incident.
Summary
In working to aggregate all observability tools in one place we decided to migrate HTTP ping tests from Pingdom to Datadog Synthetics. This was done using scripts.
The main crux of the process was migrating third-party integrations as these had multiple levels of indirection. The solution to these problems was achieved through export and aggregation of integration data from all systems into a migration mapping file between the two systems.
In the end, 100% of the tests were migrated from Pingdom to Datadog Synthetics. A major contributor to the project’s success was the application of some basic principles of data migration and to be honest, the magic of a little bit of python.
