A success story on sending application metrics to Datadog
thomas91310
thomas91310

Mar 29, 2018·6 min read




— How Datadog metrics helped us identify and fix a critical production issue
Introduction
There are plenty of monitoring tools (Statsd, Prometheus, Datadog…) that allow engineers to create graphs based on metrics they want to keep an eye on. At SimpleReach, we use Datadog to track system as well as application metrics. For example, something that has a lot of importance for us is the total duration of our main Golang processing pipeline. When the application starts, we call start := time.Now() and just before the application exits, we send time.Now().Sub(start)).Seconds() to Datadog. We can then visualize the duration of the pipeline in Datadog:

a) Pipeline duration in seconds
Importance of key application metrics
Where it becomes a little bit more interesting is when you start sending application metrics that are dependent on other nodes or cluster of nodes. Our application depends on a cluster of Cassandra nodes where we issue select and insert queries using the gocql golang package. The number of attempts to successfully execute queries as well as the latency of queries become critical application metrics for us. We can get these two metrics directly from the gocql package and send them to Datadog:
q.Attempts() where q is a gocql.Query object gives us the number of attempts per query. If we send this number to Datadog, we can then create a graph like the one below:

b) Number of attempts per request and per type (select vs insert)
q.Latency() where q is a gocql.Query object gives us the latency per query. If we send this number to Datadog, we can then create a graph like the one below:

c) Average request latencies (in seconds) by type (select / insert)
As you can see on fig b, we were seeing 1.3 attempts on average on insert queries to Cassandra.
Latencies for inserts could go as high as 5 to 6 seconds and averaging around 3 seconds (fig c)! These numbers do not reflect great performance as you could guess. We would rather have this metric closer to 1 attempt (=0 retries) on insert and select queries. By reducing the number of retries, we would also reduce the average latencies of both operations.
How do I fix this?
Thanks to these graphs, we noticed the problem described above during the first stage of the implementation. However, after we spent a little bit of time investigating the issue, we did not find the root cause. As a consequence, we decided to file a ticket for it to be worked on another sprint.
This has been now 4 to 5 months that the application runs in production and I have worked on it for the past 3.
Unfortunately, my super cool coworker that I was working very closely with decides to leave the company and I am now left out with the software to support by myself.
I am a little concerned about the high latencies and the large number of retries to Cassandra as I do not know much about how the cluster was setup and how the application was currently connecting to it… However, I am excited about finding out the root cause of this issue.
Things getting a little bit out of control
Last day of my coworker that I will miss a lot and we receive our first timeout ever experienced to the Cassandra cluster. It is only one request so it is definitely not critical but I really need to dig into this issue as it is not only retrying but also timing out now. DataDog event:

d) First timeout from the application to the Cassandra cluster
The investigation
Obviously, logs are one of the first things I started looking at and I was surprised to see a lot of failures to connect to a certain number of nodes. Example:
2018/01/03 08:10:04 error: failed to connect to IP:PORT due to error: gocql: no response to connection startup within timeout
Was the cluster in a good shape when we got these failures to connect? From what we can tell, yes it was.
It is when I started looking at the specific node IPs that the application was failing to connect to, that I realized about 80% of these log messages were coming from only a few IPs. I looked their hostnames up and realized they were SOLR nodes and not Cassandra nodes!
Wait a minute, why is the application connecting to a couple of SOLR nodes? We tell the application to connect to the Cassandra nodes we pass in as a cli parameter. I verify the list that we pass as a cli parameter and the list only contains Cassandra hostnames and absolutely no SOLR hostnames.
After asking around and examining our gocql.ClusterConfig, I am finding out that our Cassandra and SOLR nodes belong to the same cluster. Everything starts to make sense as I am looking at the gocql.NewCluster documentation:
The supplied hosts are used to initially connect to the cluster then the rest of the ring will be automatically discovered.
We obviously have to inform the package that we only want to connect to the Cassandra nodes since Cassandra and SOLR nodes are part of the same cluster… We need a gocql.HostFilter!
The one line fix and the happiness that we already had graphs around the Cassandra integration
The fix to the problem is to use a gocql.HostFilter. There are a two ways of creating a gocql.HostFilter. The first one is to whitelist all the Cassandra hostnames you want to connect to and use gocql.WhiteListHostFilter. It does the job perfectly if you have all your Cassandra hostnames / IPs handy.
However, at SimpleReach, we have a good amount of Cassandra nodes and since Cassandra and SOLR nodes are in two separate data centers, we can use a gocql.DataCentreHostFilter which is much more powerful. It basically lets gocql discover all the Cassandra nodes that are part of our Cassandra data center and filter out all the other nodes that are not part of this data center but part of the cluster.
A quick tcpdump -i * dst SOLR_IP on my local computer to verify that the application does not connect to a SOLR node and this is officially ready to get pushed out!
Thanks to the graphs we already had, this is what it looks like before and after deploying:

e) Average request latencies (in seconds) by type (select / insert) before / after deploying :)
Average latencies looking much better after deploying.

f) Number of attempts per request and per type (select vs insert) before / after deploying :)
The number of attempts per query also looks much better after the deployment as well. There is rarely a retry for both operations which is a good sign :).
The main reason why we saw these two drops is because the application was trying to insert to SOLR for a key space that did not exist on the SOLR nodes resulting in at least an extra roundtrip. Basically, if we were lucky, the next attempt would go to a Cassandra node but it could also go to a SOLR node again.
The original timeout I shared was probably caused by a serie of “unfortunate” retries to SOLR causing our go application to timeout.
Conclusion
I am now thrilled that this is fixed. The overall pipeline duration also decreased which we are all happy about and I really do not think I would have been able to identify the performance improvements without application metrics!
When I take into consideration the amount of visibility and the relevance of the discussions that open up from analyzing application and system metrics graphs, I truly believe that they should be required when software is pushed out to production.
Application and system metrics graphs are also a great way to onboard people on a project. Only looking at them can help new engineers understand what has already been done as well as potentially enable them to identify existing application bottlenecks.